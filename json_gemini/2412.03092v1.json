{"title": "Revolve: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization", "authors": ["Peiyan Zhang", "Haibo Jin", "Leyang Hu", "Xinnuo Li", "Liying Kang", "Man Luo", "Yangqiu Song", "Haohan Wang"], "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced the ability of LLM-based systems to perform complex tasks through natural language processing and tool interaction. However, optimizing these LLM-based systems for specific tasks remains challenging, often requiring manual interventions like prompt engineering and hyperparameter tuning. Existing automatic optimization methods, such as textual feedback-based techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to using immediate derivatives in traditional numerical gradient descent. However, relying solely on such feedback can be limited when the adjustments made in response to this feedback are either too small or fluctuate irregularly, potentially slowing down or even stalling the optimization process. To overcome these challenges, more adaptive methods are needed, especially in situations where the system's response is evolving slowly or unpredictably. In this paper, we introduce REVOLVE, an optimization method that tracks how Responses EVOLVE across iterations in LLM systems. By focusing on the evolution of responses over time, REVOLVE enables more stable and effective optimization by making thoughtful, progressive adjustments at each step. We evaluate the effectiveness of REVOLVE across three tasks: prompt optimization, solution optimization, and code optimization. Experimental results demonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8% improvement in prompt optimization, a 20.72% gain in solution refinement, and a 29.17% increase in code optimization. Additionally, REVOLVE converges in fewer iterations, resulting in significant computational savings. These advantages highlight its adaptability and efficiency, positioning REVOLVE as a valuable tool for optimizing LLM-based systems and accelerating the development of next-generation Al technologies.", "sections": [{"title": "1 Introduction", "content": "In recent years, Large Language Models (LLMs) have dramatically advanced AI's ability to handle complex tasks through natural language processing, enabling LLM-based systems, often referred to as language agents, to interact with external tools and solve problems previously considered out of reach (Brown, 2020; Achiam et al., 2023; Team et al., 2023; Anthropic, 2023; Touvron et al., 2023; Zheng et al., 2024). However, the development of these language agents still requires significant manual effort to break down tasks and fine-tune prompts, tools, and APIs, limiting scalability and adaptability (Wei et al., 2022; Lyu et al., 2023; Zhou et al., 2024). This raises the need for automated, scalable optimization techniques that can enhance language agents efficiently.\nTo this end, a number of recent efforts has been made on automatic optimization of language agents. For instance, DSpy (Khattab et al., 2024) uses bootstrapping and random search to optimize LLM prompts"}, {"title": "2 Background", "content": "Our approach draws inspiration from several key areas of research, particularly automated prompt engineering, agent optimization, and gradient-based learning. Below, we highlight foundational works in these areas and situate our method within this broader context.\nFrom Prompt Engineering to Agent Optimization. Prompt engineering has become a key focus in both academia and industry, leading to several methods aimed at automating the process. Early works (Pryzant et al., 2020; Yang et al., 2024) explored the use of structured prompts that enable LLMs to optimize their own inputs. Other approaches (Prasad et al., 2022; Guo et al., 2023) use search algorithms, like genetic algorithms, to automatically refine prompts. Building on the success of automated prompt engineering, researchers have extended these concepts to broader agent optimization. Techniques like Agent-Pro (Zhang et al., 2024) and AgentOptimizer (Zhang et al.) focus on optimizing individual components, such as prompts or tools. However, these methods often treat components in isolation, which can result in local improvements without significantly enhancing the overall system. Search-based approaches, such as DSpy (Khattab et al., 2024) and GPTSwarm (Zhuge et al., 2024), take a more comprehensive view by optimizing across the combinatorial space of agent components. Despite their scope, these methods rely heavily on numerical metrics that are often inadequate for real-world tasks like software development or creative writing. Additionally, they struggle to optimize multiple components simultaneously or adapt dynamically to changes in the agent pipeline.\nGradient-Based Approaches for Agent Optimization. Recent advancements have introduced gradient descent-inspired techniques to optimize prompts more effectively. ProTeGi (Pryzant et al., 2023) is among the first to use natural language feedback\u2014referred to as textual gradients\u2014to iteratively refine prompts. However, as a first-order optimization method, ProTeGi adjusts based only on immediate feedback from a single iteration, limiting its capacity to handle more complex, multi-step tasks. Agent Symbolic Learning (ASL) (Zhou et al., 2024) extended this concept by treating the entire agent system-including prompts, tools, and configurations\u2014as learnable components, much like backpropagation in neural networks. This allows for a more comprehensive optimization but remains dependent on immediate feedback from each iteration. Textgrad (Yuksekgonul et al., 2024) further advanced this first-order gradient approach by optimizing LLM responses using natural language feedback. By treating feedback as a gradient, Textgrad refines responses without directly altering the model's parameters. While effective for simpler tasks, Textgrad struggles with deeper, multi-step optimizations, frequently getting stuck in suboptimal states.\nTo address these limitations, momentum-based methods (Yuksekgonul et al., 2024) have been introduced. These techniques track feedback trends across iterations, adjusting step sizes when feedback becomes repetitive, which helps break stagnation. However, while feedback provides immediate guidance on how inputs should change, it doesn't directly reflect how model responses evolve over time. Feedback may remain constant as responses improve or fluctuate despite stagnation, leading to suboptimal adjustments. In complex tasks, where responses exhibit slow or subtle shifts, this disconnection can make feedback misleading or overly sensitive to temporary fluctuations, causing instability. Therefore, while momentum-based methods provide more variation, they still lack the fine-grained control needed for long-term improvement.\nREVOLVE: Optimization Through Response Evolution. REVOLVE enhances traditional optimization methods by focusing on how responses evolve over multiple iterations, enabling more refined adjustments throughout the process. Instead of relying solely on immediate feedback, our approach tracks the evolving relationship between consecutive prompts and their corresponding responses. This parallels second-order methods in traditional optimization, where the the Hessian matrix is used to capture changes in the gradient to guide more precise adjustments. However, rather than directly computing numerical second derivatives, we model these iterative shifts in responses to inform our adjustments, giving the system a broader understanding of response dynamics over time. The key advantages of our approach include:\n\u2022 Response Evolution Awareness: REVOLVE monitors changes across iterations, allowing for more refined and adaptive optimization, unlike first-order methods that rely only on immediate feedback."}, {"title": "3 REVOLVE: Optimizing AI Systems by Tracking Response Evolution", "content": ""}, {"title": "3.1 Method Overview", "content": "In this work, we extend the TextGrad approach (Yuksekgonul et al., 2024) by tracking the evolution of the LLM responses across iterations, allowing for more effective and precise optimization."}, {"title": "3.2 Overview of Optimization Pipeline", "content": "Our method builds upon the general optimization pipeline used in LLM-based systems (Zhou et al., 2024; Yuksekgonul et al., 2024), introducing natural language feedback (textual gradients) to refine system responses over multiple iterations, instead of relying on numerical gradients.\nForward Pass. In the forward pass, the AI system is modeled as a computation graph where each node represents a specific task. Inputs are processed sequentially through the nodes, with each node generating outputs based on prior results. These intermediate outputs are stored in a trajectory, which is later used in the backward pass.\nLanguage Loss Computation. After the forward pass, an evaluator LLM assesses the system's performance by generating textual feedback, which serves as the loss. This feedback reflects how well the system's outputs align with the task objectives and drives the subsequent optimization process.\nBackward Pass. In the backward pass, similar to numerical gradients in conventional deep learning, textual gradients are backpropagated through the nodes of the system. These gradients, in the form of natural language instructions, indicate how the system's variables\u2014such as prompts, tools, and decisions\u2014should be adjusted to improve the objective function. Starting from the final node, the system computes the necessary updates for these variables as it moves backward. This process mirrors backpropagation in neural networks, but the adjustments are determined by language feedback rather than numerical values.\nWhile the above process sets the stage for optimizing the system, the effectiveness of the optimization depends on how well this feedback is utilized. In this context, different gradient-based optimization methods come into play."}, {"title": "3.3 First-Order Optimization: Textgrad Approach", "content": "TextGrad (Yuksekgonul et al., 2024) computes a first-order gradient based on the language loss provided by the evaluator LLM. The first-order gradient captures the difference in response quality between consecutive iterations. Mathematically, the first-order gradient is expressed as:\n$\\bigtriangledown L(r(p_t)) = \\frac{\\partial L(r(p_t))}{\\partial p_t}$    (1)\nwhere we use r(pt) to denote the response when the model is fed with input prompt pt and t to denote the iteration. Also, we use \u1f43 to denote the TextGrad-style derivative of loss function with respective to the input prompt due to its analogous nature to the actual derivative that is typically denoted as d."}, {"title": "3.4 REVOLVE and Its Analogy to Second-Order Gradient Optimization", "content": "We seek to extend the previous method by extending Eq. 1 to consider the history of previous prompts and their responses. Optimizing based on only the current response can lead to short-term improvements but might results in stagnation, especially in complex tasks where deeper issues arise over time. For example, a LLM might slightly refine responses with each iteration, yet without considering the history of prompts and responses, it risks repeating similar patterns. By factoring in the evolution of responses"}, {"title": "3.5 Discussion on Simulating Second-Order Effects in Textual Optimization", "content": "No Numerical Hessian Computation. REVOLVE focuses on simulating second-order effects, not just computing real second-order derivatives. Therefore, the REVOLVE framework does not compute the traditional Hessian matrix used in numerical optimization. Instead, it simulates the effects of second-order optimization within the textual optimization framework.\nNo Reliance on LLMs' Second-Order Derivatives. REVOLVE combines structured first-order feedback with response trajectory tracking to simulate the second-order optimization effects. This approach enables it to identify stagnation and instability, refining responses without relying on LLMs' ability to compute second-order derivatives, distinguishing it from basic prompting strategies."}, {"title": "4 Experiments - Evaluation and Understanding of Models", "content": "We evaluate REVOLVE on three challenging tasks: Prompt Optimization for Reasoning, Solution Optimization, and Code Optimization. For prompt optimization, we use the Big Bench Hard dataset (Suzgun"}, {"title": "4.1 Prompt optimization for reasoning", "content": "The goal of Prompt Optimization for Reasoning is to refine a basic prompt for a specific reasoning task, enhancing the LLM's effectiveness in reasoning. This task is ideal for evaluating optimization methods, as reasoning tasks often involve large, complex search spaces where subtle prompt adjustments can significantly influence the outcome.\nTask Setup: We evaluate prompt optimization on two reasoning tasks: Object Counting from the Big Bench Hard benchmark (Suzgun et al., 2022; Srivastava et al., 2022) and grade-school math problem solving from the GSM8K dataset (Cobbe et al., 2021). For each task, when using the iterative optimization methods, we use a batch size of 3 across 12 optimization iterations, allowing the model to process a total of 36 training examples, randomly sampled with replacement. After each iteration, we validate the prompt using a validation set, and if the validation accuracy improves, we update the prompt accordingly. We compare the model's accuracy on the test set after all 12 iterations, using prompts generated by different optimization methods. Consistent with (Yuksekgonul et al., 2024), for both tasks, we use the string-based exact match metric, which looks at the final numerical value provided in the answer, and compares it to the ground truth answer. Detailed task setup is provided in Appendix B.\nBaselines and LLM Backends: We evaluate REVOLVE against three key baselines:\n\u2022 Zero-shot Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022): This baseline initializes all prompts using a zero-shot CoT strategy, where the model is prompted to \"think step-by-step\" before generating an answer. This approach is widely regarded as a strong baseline for reasoning tasks.\n\u2022 TextGrad (Yuksekgonul et al., 2024): Textual feedback is treated as a first-order gradient to iteratively optimize prompts.\n\u2022 Momentum-Enhanced TextGrad (Yuksekgonul et al., 2024): This method extends the original TextGrad framework by incorporating momentum. This variant aims to overcome potential stagnation in the optimization process by enlarging updates to the prompt when previous feedbacks on the variable are similar.\nOur experiments perform prompt optimization separately on four LLMs: gpt-3.5-turbo-0125, GPT-4-0125-preview, Gemini 1.5 Pro, and Llama 3.1 8B Instruct, with GPT-40 serving as the backend of the optimization system. This multi-model setup allows us to evaluate the effectiveness of the optimization methods across diverse architectures, ensuring a comprehensive assessment of their capabilities.\nResults: As evidenced by Table 1, in both reasoning tasks, REVOLVE delivers a substantial improvement over the Zero-shot CoT prompt, underscoring its effectiveness across diverse datasets and model architectures. On the Object Counting task, with Llama 3.1 8B Instruct as the base model, REVOLVE outperforms TextGrad by achieving a 6% higher accuracy, demonstrating its superior ability to refine LLM responses. Similarly, on GSM8K, REVOLVE exceeds both TextGrad and M-Textgrad across most LLM backends, with an average performance increase of 2% over TextGrad. These results suggest that REVOLVE not only enhances the optimization process but also addresses the inherent limitations of first-order feedback in TextGrad, leading to more accurate and refined reasoning capabilities.\nUniversality: \"REVOLVE's universality is evidenced by its consistent performance across all LLMs, including gpt-3.5-turbo-0125, GPT-4, and Llama 3.1 8B Instruct, where it delivers the highest accuracy with an average improvement of 5-7% compared to the baselines. However, there is one exception on the Gemini-1.5-Pro model, where REVOLVE slightly trails behind TextGrad. This small performance gap may be due to the use of GPT-40 to guide the Gemini-1.5-pro in the prompt optimization reasoning task. Given that Gemini-1.5-pro may exhibit more sophisticated reasoning capabilities than GPT-40 in this specific scenario, the transfer of guidance from GPT-40 could have introduced suboptimal adjustments,"}, {"title": "4.2 Solution optimization", "content": "We proceed to evaluate REVOLVE on the solution optimization task. This task aims to refine and improve the solution to complex scientific or technical problems, such as questions in quantum mechanics or organic chemistry. The solution will evolve dynamically through self-evaluation and critique, challenging the LLM to continually refine its responses. This process aligns with test-time training (Sun et al., 2020, 2024), where models are refined during testing, as well as with recent progress in self-refinement for reasoning tasks (Yao et al., 2022; Madaan et al., 2024; Shinn et al., 2024), which have demonstrated efficacy in iterative problem-solving.\nTask Setup: We evaluate solution optimization on two challenging benchmarks: Google-proof Question Answering (GPQA) (Rein et al., 2023), which consists of expert-level multiple-choice questions in physics, biology, and chemistry, and two subsets of the MMLU benchmark(Hendrycks et al., 2020), specifically focused on Machine Learning and College Physics. GPQA is a highly difficult benchmark, with experts achieving 81% accuracy and skilled non-experts reaching only 22%, highlighting the challenge of the questions. Performance of LLMs on these benchmarks has not yet saturated, making them ideal for benchmarking solution refinement. We use three iterations of optimization for each question when using the iterative optimization methods. The final answer is determined through majority voting across all iterations for all the iterative optimization methods. Consistent with (Yuksekgonul et al., 2024), we use the string-based exact match metric. Detailed task setup is in Appendix C.\nBaselines and LLM Backends: We compare REVOLVE against three primary baselines for solution optimization: Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022), TextGrad (Yuksekgonul et al., 2024) and Momentum-Enhanced TextGrad. All methods are applied separately on three LLMs: GPT-40, GPT-4-0125-preview, and Llama 3.1 8B Instruct, with themselves serving as the backend of the optimization system.\nDetailed baseline configurations and prompting exemplars can be found in Appendix C.\nResults: As shown in Table 2, across all benchmarks, REVOLVE significantly improves the performance of Llama 3.1 8B Instruct compared to all baselines. On average, across the three benchmarks, REVOLVE achieves a 17.79% relative improvement in final accuracy over TextGrad. This substantial gain highlights the effectiveness of incorporating second-order gradients into the optimization process, enabling more precise adjustments and greater performance gains on solution optimization tasks.\nDeterioration in TextGrad: Interestingly, we observe performance deterioration with TextGrad on the MMLU benchmark, where both intermediate and final results are worse than the initial state. This"}, {"title": "4.3 Code Optimization", "content": "The Code Optimization task aims to refine code snippets to improve their correctness and runtime efficiency, often with limited supervision from local tests and iterative self-evaluation. This task is also well-suited for evaluating optimization techniques as it requires handling intricate problem constraints and optimizing through iterative adjustments.\nTask Setup: We evaluate code optimization using the LeetCode Hard dataset (Shinn et al., 2024), an online platform featuring coding challenges commonly used for technical interview preparation. The primary metric for this task is the Completion Rate, which measures the percentage of problems for which all test cases are passed, calculated as $\\frac{\\text{Number of problems passed}}{\\text{Total number of problems}}$. Since LeetCode test cases are not publicly available, generated code is submitted to the LeetCode platform for evaluation on these unseen test cases. Results are averaged over multiple runs for robustness. Additional details of the task setup are provided in Appendix D.\nBaselines and LLM Backends: We evaluate REVOLVE against four key baselines on the LeetCode Hard dataset using Llama 3.1 8B Instruct model as the backend:"}, {"title": "4.4 Ablation Study", "content": "Given the simplicity of our method, there are no complex components that can be eliminated for a traditional ablation. Instead, we conduct an ablation study by testing different prompt designs to evaluate their impact on performance. Specifically, we compare our REVOLVE prompt, a variant of this prompt, and the prompt used in TextGrad to highlight the effectiveness of our approach.\n\u2022 REVOLVE Prompt: This is the prompt carefully designed for REVOLVE, which takes into account both immediate feedback and the evolution of responses across iterations.\n\u2022 Variant Prompt: It differs from our method by directly instructing the LLM to generate more diverse responses, effectively pushing it towards greater variation with each iteration.\n\u2022 TextGrad Prompt: Serving as the baseline, TextGrad's prompt focuses primarily on immediate feedback, making adjustments based solely on the latest response."}, {"title": "4.5 Comparison of Computational Resources", "content": "To analyze the computational efficiency of REVOLVE, we compare its GPU memory usage and runtime against baseline methods across three task categories. We use Llama 3.1 8B Instruct as the base LLM, running on a setup with 4 NVIDIA 3090 GPUs. The results are shown in Table 5.\nWe observe that while REVOLVE involves slightly higher per-iteration runtime due to its second-order optimization-inspired design, it converges in fewer iterations, resulting in significant overall savings. The detailed findings are as follows:\n\u2022 On Object Counting dataset, REVOLVE reduces total runtime by 50% compared to TextGrad by converging in fewer iterations despite slightly higher per-iteration costs.\n\u2022 For solution optimization task, REVOLVE achieves 26.14% lower total runtime than TextGrad, while M-TextGrad incurs 77.65% higher runtime due to instability.\n\u2022 For code optimization task, REVOLVE reduces total runtime by 16.67% compared to baselines.\n\u2022 For GPU memory usage, REVOLVE demonstrates similar requirements to baseline methods, indicating no significant increase in computational resources."}, {"title": "5 Conclusion", "content": "In this paper, we introduced REVOLVE, an optimization framework that extends traditional methods by considering the evolution of responses over multiple iterations. Instead of focusing only on immediate feedback, REVOLVE incorporates insights from the similarity between consecutive responses, akin to how second-order information is used in classic optimization. This approach allows for more informed adjustments, addressing issues like stagnation and instability seen in earlier methods. By capturing these iterative changes, REVOLVE achieves more stable and consistent improvements across tasks, particularly in complex scenarios where simpler methods fall short. This makes REVOLVE a valuable step forward in effective optimization for AI systems."}, {"title": "A System Prompt Details for REVOLVE", "content": "In REVOLVE, we use system prompts designed to guide iterative response refinement. The prompts focus on comparing the current response with previous iterations, emphasizing gradual, thoughtful evolution. They request the model to provide feedback not only on immediate changes but also on patterns observed across multiple iterations.\nwe use the following glossary to the system prompt:"}, {"title": "OPTIMIZER SYSTEM PROMPT", "content": "\"You are part of an optimization system that improves text (i.e., variable) by analyzing how the responses evolve across multiple iterations. \"\n\"Your goal is not just to make a single improvement, but to ensure that the variable evolves naturally and meaningfully over time. \"\n\"Focus on adjusting the variable in a way that each step introduces thoughtful, measured changes based on past iterations, rather than drastic shifts. \"\n\"The feedback provided will help guide these adjustments, but ensure that your improvements maintain coherence and contextual alignment. \"\n\"You MUST give your response by sending the improved variable between {new_variable_start_tag} {{improved variable}} {new_variable_end_tag} tags. \"\nf\"{GLOSSARY_TEXT}\""}, {"title": "Textual Gradient Descent Prompt Prefix", "content": "\"Here is the role of the variable you will improve: <ROLE>{variable_desc}</ROLE>.\"\n\"The variable is the text within the following span: <VARIABLE> {variable_short} </VARI-ABLE>\"\n\"Here is the context and feedback we received for the variable:\"\n\"<CONTEXT>{variable_grad}</CONTEXT>\"\n\"Additionally, reflect on how the responses to this variable have evolved across iterations:\"\n'<PAST_ITERATIONS>{past_values}</PAST_ITERATIONS>\"\n\"Make nuanced improvements, keeping in mind that too-similar responses suggest insufficient change, but avoid making overly large changes.\"\n\"Ensure that the response evolves in a coherent and thoughtful manner that aligns with the context, feedback, and past responses.\""}, {"title": "GRADIENT TEMPLATE", "content": "\"Here is a conversation:<CONVERSATION>{context}</CONVERSATION>\"\n\"This conversation is part of a larger system. The output is used as {response_desc}. Here is the feedback we received for {variable_desc} in the conversation:<FEEDBACK>{feedback}</FEEDBACK>\"\n\"Additionally, consider how the responses to this variable have changed across previous iterations:\"\n'<PAST_ITERATIONS>{past_values}</PAST_ITERATIONS>\"\n\"Make sure future responses reflect a meaningful, gradual evolution based on these past iterations, encouraging thoughtful progress rather than drastic shifts.\""}, {"title": "B Prompt Optimization", "content": "For the dataset split, we follow the settings used in TextGrad (Yuksekgonul et al., 2024). The Big Bench Hard Object Counting dataset is divided into 50/100/100 samples for train/validation/test, respectively. For GSM8K, we adopt the split from DSPy (Khattab et al., 2024), using 200/300/1399 samples for train/validation/test. In each task, we limit the training set to 36 samples, consistent with the TextGrad setup. Example queries for each dataset are shown below:"}, {"title": "Example Query for Big Bench Hard Object Counting", "content": "I have an apple, three bananas, a strawberry, a peach, three oranges, a plum, a raspberry, two grapes, a nectarine, and a blackberry. How many fruits do I have?"}, {"title": "Example Query for GSM8K", "content": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?"}, {"title": "C Solution Optimization", "content": "For the solution optimization task, we follow the experimental setup outlined by TextGrad (Yuksekgonul et al., 2024). This ensures fair comparisons across all experiments. We evaluate on two benchmarks: Google-Proof Question Answering (GPQA) (Rein et al., 2023) and two subsets from the MMLU benchmark (Hendrycks et al., 2020), Machine Learning and College Physics. Following the simple-evals repository practice, we employ string matching to extract the final answer (one of ABCD) and compare it to the ground truth. The datasets comprise 198 questions in the GPQA Diamond subset, 112 in MMLU Machine Learning, and 92 in MMLU College Physics. We compare REVOLVE against three primary baselines for solution optimization:\n\u2022 Chain-of-Thought (CoT) (Kojima et al., 2022; Wei et al., 2022): This baseline serves as our initial baseline. This method employs a step-by-step reasoning process, providing a strong foundation for comparison in complex problem-solving tasks.\n\u2022 TextGrad (Yuksekgonul et al., 2024): This method leverages textual gradients to iteratively refine solutions. For the solution optimization task, we apply three iterations of test-time updates using TextGrad, refining the solution at each step. The process involves making one call to GPT-40 to evaluate the test-time loss, another call to collect gradients, and a final call to update the solution accordingly."}, {"title": "Example Query for GPQA Diamond", "content": "Answer the following multiple choice question. The last line of your response should be of the following format: 'Answer: $LETTER' (without quotes) where LETTER is one of ABCD. Think step by step before answering.\nA) A = cyclohexane-1,3,5-trione, B = dimethyl fumarate\nB) A = benzoquinone, B = dimethyl fumarate\nC) A = benzoquinone, B = methyl 2-hydroxypropanoate\nD) A = cyclohexane-1,3,5-trione, B = methyl 2-hydroxypropanoate"}, {"title": "Example Comparison of TextGrad and REVOLVE", "content": "Answer the following multiple-choice question... Select the suitable reagents for the follow-ing mentioned reactions. butan-2-one + NaCN + A \u2014> 2-hydroxy-2-methylbutanenitrile 2-(4-benzylphenyl)-2-hydroxybutanenitrile + B (H2O) \u2014> 2-(4-benzylphenyl)-2-hydroxybutanoic acid.\nA) A = NaHSO3, B = CH3C\u041e\u041e\u041d\nB) A = H3O+, B = HCl\nC) A = NaHSO3, B = HCl\nD) A = H3O+, B = CH3COOH"}, {"title": "D Code Optimization", "content": "In the code optimization task, we primarily rely on the settings from previous work, particularly TextGrad Yuksekgonul et al. (2024), to ensure a fair comparison across experiments. Specifically, we adopt"}, {"title": "E Abaltion Study", "content": ""}, {"title": "E.1 Ablation Study", "content": "Given the simplicity of our method, there are no complex components that can be eliminated for a traditional ablation. Instead, we conduct an ablation study by testing different prompt designs to evaluate their impact on performance. Specifically, using Llama-3.1-8B-Instruct as the LLM backend, we compare our REVOLVE prompt, a variant of this prompt, and the prompt used in TextGrad to highlight the effectiveness of our approach.\n\u2022 REVOLVE Prompt: This is the prompt carefully designed for REVOLVE, which takes into account both immediate feedback and the evolution of responses across iterations.\n\u2022 Variant Prompt: It differs from our method by directly instructing the LLM to generate more diverse responses, effectively pushing it towards greater variation with each iteration.\n\u2022 TextGrad Prompt: Serving as the baseline, TextGrad's prompt focuses primarily on immediate feedback, making adjustments based solely on the latest response."}, {"title": "OPTIMIZER SYSTEM PROMPT", "content": "\"You are part of an optimization system that improves text (i.e., variable). \"\n\"You will be asked to creatively and critically improve prompts, solutions to problems, code, or any other text-based variable. \"\n\"You will receive some feedback, and use the feedback to improve the variable. \"\n\"Pay attention to the role description of the variable, and the context in which it is used. \"\n\"Importantly, focus on creating responses that are varied and diverse in nature. \"\n\"You MUST give your response by sending the improved variable between {new_variable_start_tag} {{improved variable}} {new_variable_end_tag} tags. \"\n\"The text you send between the tags will directly replace the variable.\"\nf\"{GLOSSARY_TEXT}\""}, {"title": "Textual Gradient Descent Prompt Prefix", "content": "\"Here is the role of the variable you will improve: <ROLE>{variable_desc}</ROLE>.\"\n\"The variable is the text within the following span: <VARIABLE> {variable_short} </VARI-ABLE>\"\n\"Here is the context and feedback we got for the variable:\"\n''<CONTEXT>{variable_grad}</CONTEXT>\"\n\"Improve the variable ({variable_desc}) using the feedback provided in <FEEDBACK> tags.\"\n\"Ensure that your response introduces new and diverse ways of solving the problem or addressing the prompt.\""}, {"title": "GRADIENT TEMPLATE", "content": "\"Here is a conversation:<CONVERSATION>{context}</CONVERSATION>\"\n\"This conversation is part of a larger system, where varied and creative outputs are important. The output is used as {response_desc}. Here is the feedback we received for {variable_desc} in the conversation:\"\"<FEEDBACK>{feedback}</FEEDBACK>\"\n\"Encourage diversity in your improvements.\""}]}