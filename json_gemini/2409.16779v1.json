{"title": "LLaMa-SciQ: An Educational Chatbot for Answering Science MCQ", "authors": ["Marc-Antoine Allard", "Matin Ansaripour", "Maria Yuffa", "Paul Teiletche"], "abstract": "Large Language Models (LLMs) often struggle with tasks requiring mathematical reasoning, particularly multiple-choice questions (MCQs). To address this issue, we developed LLaMa-SciQ, an educational chatbot designed to assist college students in solving and understanding MCQs in STEM fields. We begin by fine-tuning and aligning the models to human preferences. After comparing the performance of Mistral-7B and LLaMa-8B, we selected the latter as the base model due to its higher evaluation accuracy. To further enhance accuracy, we implement Retrieval-Augmented Generation (RAG) and apply quantization to compress the model, reducing inference time and increasing accessibility for students. For mathematical reasoning, LLaMa-SciQ achieved 74.5% accuracy on the GSM8k dataset and 30% on the MATH dataset. However, RAG does not improve performance and even reduces it, likely due to retriever issues or the model's unfamiliarity with context. Despite this, the quantized model shows only a 5% loss in performance, demonstrating significant efficiency improvements.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are known to perform poorly on questions requiring advanced mathematical reasoning (Wu et al., 2023). This is especially true for the university level problems (Wang et al., 2024). In literature, the failure of current approaches is attributed to inability of LLMs to recognize and correct a wrong answer (Imani et al., 2023) as well as catastrophic forgetting of linguistic skills when trained on maths data (Sharma et al., 2023). The issues cannot be fully addressed with a simple prompting strategy due to data variability (Wang et al., 2024).\nThis project explores state-of-the-art LLMs for creation of an accessible chatbot that assists students in mathematics, physics and computer science. Specifically, we fine-tune LLaMa-3-8B model as well as Mistral-7B on a variety of mathematical and scientific datasets further using Direct Preference Optimization (DPO) to align model's responses to the ones preferred by a student. We compare the performances of the models and demonstrate the significantly superior performance of the LLaMa model with which we proceed. We try to enhance the accuracy of fine-tuned LLaMa-3-8B model by applying Retrieval Augmented Generation (RAG). Finally, we quantize the LLM for more efficient inference, making it suitable for students needs."}, {"title": "2 Related Work", "content": "With recent release of ChatGPT-3.5 and ChatGPT-4, the number of people using LLMs for education has sky-rocketed (F\u00fctterer et al., 2023). To leverage its capabilities while making user-friendly interfaces vast amount of research is dedicated to creation of LLM based Chatbots for academic purposes (Odede and Frommholz, 2024). Despite success of ChatGPT models on lingustic tasks, their performance was limited on problems involving mathematical reasoning. This is especially true for MCQ questions where the answer is not verbal. This is showcased by the work of (Savelka et al., 2023), where GPT model struggled to give the correct answer to the questions that do not contain natural language.\nTo improve the performance of the pre-trained LLM model on mathematical questions while ensuring the alignment of the responses with the intended purposes and human values, we considered both Supervised Fine-tuning on mathematical and scientific datasets as well as DPO on the preference pairs ranked by students. This approach was inspired by InstructGPT (Ouyang et al., 2022) in aligning LLMs with human preferences. We also considered DPO with an offset (Amini et al., 2024). This approach introdues variability in treatment of preference pairs and could be less robust, since"}, {"title": "3 Approach", "content": "Our approach consisted of performing SFT training on both Mistral-7B and LLaMa-3-8B. We then compared the performance of two models with SFT and DPO training and proceeded with LLaMa-3-8B which performed better on the evaluation set (Figure 1). In this section, we outline the details of the models and their fine-tuning strategy with emphasis on LLaMa-3-8B."}, {"title": "3.1 Base Model Architecture", "content": "LLaMa-3-8B (AI@Meta, 2024) is an auto-regressive language model featuring an enhanced transformer architecture with a standard decoder-only design. The model integrates supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to better align with human preferences regarding safety and helpfulness. Llama 3, which uses a tokenizer with a 128K-token vocabulary for more efficient language encoding, shows significant performance improvements over its predecessor. The model, trained on sequences up to 8,192 tokens with boundary-aware self-attention, uses Grouped-Query Attention (GQA) to enhance inference scalability.\nMistral-7B (Jiang et al., 2023), a language model with 7 billion parameters, utilizes a transformer-based architecture comprising multiple transformer blocks. It employs sliding window attention that allows the model to attend to tokens outside of the window, Rolling Buffer Cache to reduce the cache memory usage while keeping the model quality. It also utilizes pre-fill and chunking, which involve loading known parts of a prompt into the (k, v) cache to facilitate token generation. If the prompt is lengthy, it is segmented into smaller chunks, each pre-filled into the cache to enhance processing efficiency during token prediction."}, {"title": "3.2 Training Pipeline", "content": "The training pipeline for LLaMa-3-8B model is demonstrated in Figure 1. We first performed Supervised Fine-tuning on a mix of specialized maths and science datasets. We then performed DPO training using preference data generated and annotated by students via cDPO loss. Finally, we gauged the performance of the model on AQUA-Rat (Ling et al., 2017) dataset which contains STEM-related MCQ questions.\nMistral-7B used the same process as LLaMa, except for the final SFT, since LLaMa showed superior performance (Figure 1). Ultimately, we have not implemented both due to time constraints."}, {"title": "3.2.1 Supervised Fine Tuning", "content": "The results of supervised fine-tuning of two models are demonstrated in Table 1."}, {"title": "3.2.2 Preference Data Collection", "content": "To collect preference data, a cohort of 300 students was asked to generate two responses, a better one and a slightly worse one but preferably still correct, to the question using GPT-wrapper. The students were further asked to rank the responses.\nTo generate answers for the dataset of questions in mathematics, physics, and computer science, we have developed a prompting strategy that incorporates several techniques. Firstly, we create a"}, {"title": "3.2.3 Reward Model", "content": "The reward model is a critical component of the DPO fine-tuning strategy. The reward model is based on the policy that maximizes the reward with KL constraint to the reference policy:\n\u03c0* = arg max \u03c0 \u03a3y [\u03b2y - \u03b2 log (\u03c0(y) / \u03c0_{ref}(y))].\nConsidering a small probability that the preference pair could be flipped, the preferred response is in reality less correct or explicit than the other one, we can derive the following DPO loss to optimise the reward model:\n$L_{DPO}(\\theta, y_w, y_l) = -(1 - \\epsilon) \\log p_\\theta(y_w > y_l)$ (1)\n$e \\log(1 \u2013 p_\\theta(y_\u03c9 > y_l))$ (2)\n$= (1 - \\epsilon)L_{DPO}(\\theta, y_w, y_l)$ (3)\n$+ \\epsilon L_{DPO}(\\theta, y_l, y_w),$ (4)\nwhere $\\epsilon$ indicates the probability of the answer being wrong (or flipping the pair)."}, {"title": "3.3 Retrieval Augmented Generation", "content": "We augment LLaMa-SciQ by incorporating Retrieval-Augmented Generation (RAG) methods. This approach stands out as one of the most effective means to enhance the predictive capabilities of our model. RAG combines the capabilities of generative models, dense vector indices of a racorpus of documents, and pre-trained neural retrievers. Figure 2 summarizes our RAG pipeline, known as the Naive RAG. We use the dataset described in 4.1.4 as our Dense Passage Retrieval (DPR) corpus of documents over which we create an index using Facebook's FAISS library (Douze et al., 2024). Documents are then retrieved using Facebook's DPR question encoder (Karpukhin et al., 2020) and added to the prompt in the format detailed in Appendix D.1.\nWe observed that our model is getting biased by saying to use the provided information. So we changed the prompt for RAG to tell the model to consider the model but not get biased on the information and try to fulfill the objective of the questions."}, {"title": "3.4 Quantization", "content": "We took an alternative root compared to standard quantization techniques. In particular, we specified the bytes-and-bits (bnb) parameter when loading the model using Unlosth package. We, therefore, reduced the weights to 4-bits while sustaining the accuracy.\nWhen you enable \"load_in_4bits\" in the \"from_pretrained\" function of the unsloth repository, the model utilizes a quantization technique facilitated by the bitsandbytes library. This technique allows the model weights to be represented with 4-bit precision, significantly reducing the model's memory footprint while attempting to preserve performance.\nThis 4-bit quantization primarily involves the transformation of model weights, previously in full-precision formats like fp16 or bf16, into a 4-bit format. The process entails creating instances of linear layers designed for 4-bit operations (e.g., \"Linear4bit\"), and then loading the original model's weights into these quantized modules. The actual quantization happens when these modified models are transferred to a computation device like a GPU.\nThis quantization approach can utilize different data types for quantization, like FP4 (Float4) or NF4 (NormalFloat4), which are tailored for different kinds of data distributions and usage scenarios. For example, NF4 is designed for data that naturally follows a normal distribution, offering"}, {"title": "4 Experiments", "content": "This section outlines the datasets created for our model's alignment stages. Samples of the datasets can be found in Appendix B."}, {"title": "4.1 Data", "content": "This section outlines the datasets created for our model's alignment stages. Samples of the datasets can be found in Appendix B."}, {"title": "4.1.1 SFT Dataset", "content": "We first introduce StemQA, a specialized dataset to extend our model's performance on math and coding questions. This dataset is a blend of MetaMathQA (Yu et al., 2023) and CodeFeedback-Filtered-Instruction (Zheng et al., 2024) datasets. It is balanced so that 75% of the questions are math-related, while the remaining 25% are coding-related. Table 2 presents these proportions. The answers now include the rationale followed by \"The answer is: <Maths/Code>\" to simplify future answer extraction."}, {"title": "4.1.2 DPO Dataset", "content": "Then, we introduce StemDPO, a dataset to align our model with human preferences, focusing particularly on STEM questions. This dataset combines our class preference pairs with the PyDPO and MetaMathDPO datasets. Our objective was to expand this dataset to a size of 50,000 samples, maintaining the same distribution proportions as"}, {"title": "4.1.3 MCQ Dataset", "content": "We present StemMCQ, a modified version of the well-known AQUA-RAT dataset (Ling et al., 2017), specifically designed to align the model with its primary purpose: answering STEM multiple-choice questions. The answers include the AQUA-RAT rationale followed by our extraction flag: \"The answer is: <MCQ Letter>\". We chose to include the rationale in our responses, as the Chain-of-Thought approach has demonstrated improved results compared to simply providing the answer (Wei et al., 2022). Table 4 presents the dataset size."}, {"title": "4.1.4 DPR Dataset", "content": "To enable RAG in our model, we developed StemDPR, a DPR corpus of Wikipedia science documents. This dataset is built from WikiStemCorpus\u00b9, a science-focused subset of the well-known RAG dataset wiki_dpr (Karpukhin et al., 2020). We compute the document embeddings of WikiStemCorpus using Facebook's DPR context encoder (Karpukhin et al., 2020)."}, {"title": "4.2 Evaluation", "content": "In this section, we define the evaluation process, which is divided into multiple steps. The initial step involves selecting the best model based on its generation quality. The final step assesses the predictability power of our MCQA model.\nTo select the best generation models, we need to assess the quality of their generation in terms of correctness and reasoning.\n\u2022 DPO Reward Accuracies (Rafailov et al., 2023): This allows us to assess the preference alignment of the model's generation in terms of human alignment.\nTo thoroughly assess our model's performance on STEM QA, we choose diverse datasets that represent various skills the model should have acquired. First, we use benchmark datasets to evaluate the correctness of our first-stage model in answering open STEM questions:\n\u2022 MATH (Hendrycks et al., 2021): This dataset of 5k advanced mathematics questions to assess the model's mathematical step-by-step reasoning skills.\n\u2022 GSM8K (Cobbe et al., 2021): A dataset of 8.5K (1k testing split) high quality linguistically diverse grade school math word problems created by human problem writers. Used to further evaluate the model's mathematical reasoning abilities.\nThen, we use MCQA datasets to assess the MCQA performance of our final specialized model:"}, {"title": "4.3 Baseline", "content": "We compare LLaMa-SciQ with the candidate base models: LLaMa-3-8B (AI, 2024) and Mistral-7B (Jiang et al., 2023). At each step of the training pipeline (described in Section 3), we conduct ablation studies by comparing the newly trained model with the model from the previous step."}, {"title": "4.4 Setup", "content": "We adapt our SFT and DPO procedures to run on a single V-100 GPU with 32 GB of VRAM and a single A-100 GPU with 40 GB of VRAM, respectively. We utilize the Unsloth library (unslothai and contributors, 2024), designed for fast and resource-efficient training of large language models. Combining Unsloth's techniques with LoRa adaptors allows us to efficiently align LLaMa-3-8B and Mistral-7B within our resource constraints. In addition, due to the extended duration of the training process (more than 15 hours), extensive hyperparameter tuning is not practical."}, {"title": "4.4.1 1st SFT \u2013 Mathematical Reasoning", "content": "Therefore, the SFT hyperparameters (see Table 9 in the appendix) are chosen based on the state-of-the-art SFT of the models. For similar reasons, we train our models using two relatively small, random sample sizes from the full SFT dataset (described in Section 2.1): 10,000 and 100,000 examples.\nWe conduct two SFT sessions for each model. The best models are selected from the 100,000-sample-size runs, showing the best results in the generation (see an example in D.2)."}, {"title": "4.4.2 DPO Alignment", "content": "For the DPO training procedure, we split the DPO dataset described in Section 4.1.2 into 45,000 samples for training and the remaining for testing. The hyperparameter settings are described in Table 9."}, {"title": "4.4.3 2nd SFT \u2013 MCQA Reasoning", "content": "Finally, using the same hyperparameter setup as in the first SFT sessions, we perform the final SFT training for MCQA specialization using 97,500 MCQ samples. Figure 3 presents the training loss of the kept run."}, {"title": "4.5 Results", "content": "The intermediate and final results can be found in Table 6, Table 5, and Table 7.\n\u2022 MATH (Hendrycks et al., 2021): On the MATH dataset, known for its complexity and depth, we managed to achieve the performance announced by Meta on their introduction page of LLaMA-3-8B, with a score of 30%. This demonstrates the power of LLaMA-3, especially in comparison to the Mistral-7B, where our results were consistent with Mistral's research, showing a score of around 11%.\n\u2022 GSM8K (Cobbe et al., 2021): For the GSM8K dataset, which is less challenging than MATH, our score was slightly below Meta's results by 5.1%, but still more than 40% higher than Mistral's performance. Note that we used 0-shot prompting for both evaluations, whereas Meta used few-shot prompting.\nFinally, for the evaluation of LLaMa-SciQ on MCQA from the EPFL course, the results were decent but somewhat disappointing compared to the general math benchmarks.\n\u2022 MCQA Examples EPFL: The best score was achieved by the policy model, outperforming the two specializations by around 5%. The RAG and Quantized models showed similar performance, with a difference of approximately 0.555 in accuracy. The RAG system did not improve accuracy and seemed to lead to poorer decisions, possibly due to"}, {"title": "5 Analysis", "content": "We noted that our model exhibits reasonable generation capabilities and demonstrates sound reasoning when answering questions. During our SFT and DPO training, which frequently involved mathematical questions, our model proved particularly adept at handling them. However, as the benchmark (Table 7) included questions from a wide range of disciplines, the results were generally acceptable.\nWe believe our quantized model maintained the accuracy of the LLaMa-SciQ model, as it occasionally achieved higher accuracy in our tests. During development, we experimented with various configuration settings, including adjustments to the temperature, to optimize performance. Table 8 presents the best results of the generation tested on a 10-subsample of the EPFL MCQ dataset; the full test results are presented in Appendix E). Despite these efforts, we think the generation configuration could still benefit from fine-tuning. With a large beam size in the beam search, the quantized model's performance was comparable to that of LLaMa-SciQ. However, due to resource constraints, we reduced the beam size to 1 for our final benchmark.\nThe RAG model did not meet our goal of enhancing accuracy. We attribute this to the encoder used for information retrieval, which was not specifically fine-tuned for our model. Consequently, the encoder sometimes retrieved irrelevant information, potentially biasing the model towards incorrect data. Additionally, our model was trained to adhere to a specific template rather than to utilize the provided information effectively, which likely contributed to its underperformance."}, {"title": "6 Ethical considerations", "content": "In this section, we address the ethical considerations relevant to LLaMa-SciQ.\nLow-Resources Language Performances The high performance of LLaMa-3-8b on high-resource languages (AI, 2024) suggests that LLaMa-SciQ should be capable of handling questions in most of these languages (with the best performance on English MCQs, as the SFT dataset is English-based). However, additional work is needed to extend its capabilities to low-resource languages, such as Urdu and Swahili. This could be achieved by expanding our SFT datasets to teach the model multilingual scientific reasoning. Furthermore, although more challenging and costly, we could extend our DPO dataset to include low-resource"}, {"title": "Accessibility for Deaf Community", "content": "The exclusion of signed languages from modern language technologies marginalizes Deaf communities, who prefer to communicate in signed languages online (Yin et al., 2021). Therefore, it is essential to include signed language compatibility in our model to respect this community and support its communication preferences. One potential approach to achieve this is by harnessing Sign Language Translation (SLT), which has seen advancements through deep learning techniques (Al-Qurishi et al., 2021; Chen et al., 2022), such as the STMC-Transformer model (Yin and Read, 2020). By integrating SLT into LLaMa-SciQ's pipeline, we could easily address signed questions."}, {"title": "Social Bias & Harmful Content", "content": "The model, designed for the MCQA task, should not exhibit more harmful content or social bias than its inherent base model. However, for broader usages, studies indicated that LLM presents vulnerabilities exploitable to output harmful content or social bias (Wei et al., 2024; Deng et al., 2023). Therefore, future work should involve additional training to mitigate LLaMa-SciQ's potential biases or harmful content that may arise from out-of-scope usages. This can be achieved using Meta's Responsible Use Guide (RUG) and LLaMa-Guard (?), an LLM-based safeguard model designed for Human-AI conversation use cases."}, {"title": "7 Conclusion", "content": "In this work, we propose LLaMa-SciQ: an educational chatbot designed for science multiple-choice question answering (MCQA). The model is a fine-tuned LLaMa-3-8B aligned with human preferences using the novel STEM datasets introduced (StemQA, StemDPO, StemMCQ). It also employs cost-reducing training techniques such as Unsloth (unslothai and contributors, 2024) to address limitation in resources. LLaMa-SciQ maintains the performance of state-of-the-art large language models in scientific question answering, achieving up to 74.5% on the GSM8k benchmark and 30% on the MATH benchmark using zero-shot prompting. These results are comparable to the base model using eight-shot prompting on these benchmarks."}, {"title": "A Contribution", "content": "Each member of the group contributed equally to all of the aspects of the project.\nMatin Ansaripour: DPO training, LLaMa-adapter supervised training, Quantization coding, Report Writing.\nPaul Teiletche: Dataset processing, external dataset adaptation, RAG specialisation, Report writing, Evaluation coding.\nMarc-Antoine Allard: Dataset processing, external dataset adaptation, RAG specialisation, Report writing, Evaluation coding.\nMaria Yuffa: DPO training, Quantization coding, Literature review, Report writing."}, {"title": "B Datasets Samples", "content": ""}, {"title": "B.1 SFT Dataset", "content": "{ Sample from the SFT dataset\n\"problem\": \"Determine the sum of the positive factors of 48.\",\n\"solution\": \"To find the sum of the positive factors of 48, we can [...]. The answer is: 124\"\n}"}, {"title": "B.2 DPO Dataset", "content": "{ Sample from the DPO dataset\n\"prompt\": \"Tom eats a pound of carrots [...] how many calories did he eat in total?\",\n\"chosen\": \"Tom eats 1 pound of carrots, which have 51 calories per pound, so he eats 1*51 = 51 calories [...] The answer is: 85\",\n\"rejected\": \"Tom eats 1 pound of carrots, which have 51 calories per pound, so he eats 1*51 = 97 calories [...] the answer is: 85\"\n}"}, {"title": "B.3 MCQ Dataset", "content": "{ Sample from the MCQ dataset\n\"subject\": \"maths\",\n\"question\": \"There are 8 players in a chess group [...] how many total games will be played?\",\n\"options\": [\"10\",\"30\",\"28\",\"60\",\"90\"]\n\"answer\": \"10 players are there. two players [...] The answer is: C.\"\n}"}, {"title": "B.4 DPR Dataset", "content": "{ Sample from the DPR dataset\n\"text\": \"In mathematical analysis, the Cauchy index is [...] the degree of q.\",\n\"title\": \"Cauchy index\"\n\"embeddings\": [-0.6179105639457703, ..., 0.35533231496810913]\n}"}, {"title": "B.5 Intruction for DPO Generation", "content": "Instruction to generate examples for DPO\n\"Imagine you're a teaching assistant for a <course_topic> course. A student has just asked the question above. Your goal is to provide a comprehensive and detailed explanation, similar to how you would guide a student in understanding the concept thoroughly. Use scientific reasoning and relevant examples to clarify the topic and ensure a deep understanding by the student.\""}, {"title": "C Training Details", "content": "Here we present more details for SFT and DPO training."}, {"title": "C.1 Training Hyperparameters", "content": "Table 9 presents the hyperparameters that we used for each training."}]}