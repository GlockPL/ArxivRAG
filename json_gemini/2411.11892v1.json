{"title": "GREEN My LLM: Studying the key factors affecting the energy consumption of code assistants", "authors": ["Tristan Coignion", "Cl\u00e9ment Quinton", "Romain Rouvoy"], "abstract": "In recent years, Large Language Models (LLMs) have significantly improved in generating high-quality code, enabling their integration into developers' Integrated Development Environments (IDEs) as code assistants. These assistants, such as GITHUB COPILOT, deliver real-time code suggestions and can greatly enhance developers' productivity. However, the environmental impact of these tools, in particular their energy consumption, remains a key concern. This paper investigates the energy consumption of LLM-based code assistants by simulating developer interactions with GITHUB COPILOT and analyzing various configuration factors. We collected a dataset of development traces from 20 developers and conducted extensive software project development simulations to measure energy usage under different scenarios. Our findings reveal that the energy consumption and performance of code assistants are influenced by various factors, such as the number of concurrent developers, model size, quantization methods, and the use of streaming. Notably, a substantial portion of generation requests made by GITHUB COPILOT is either canceled or rejected by developers, indicating a potential area for reducing wasted computations. Based on these findings, we share actionable insights into optimizing configurations for different use cases, demonstrating that careful adjustments can lead to significant energy savings.", "sections": [{"title": "1. Introduction", "content": "In recent years, Large Language Models (LLMs) for code have significantly become better at generating code, facilitating their seamless integration into developers' Integrated Development Environments (IDEs) as code assistants. Code assistants offer auto-completion suggestions that developers can either accept or reject. The generation process is typically initiated automatically after a brief pause in typing, but can also be manually triggered via a command or keyboard shortcut.\nNumerous code assistants, like GITHUB COPILOT, TABNINE, and CODE-WHISPERER, including open-source options\u2014like TABBY and CODY-offer IDE extensions and manage inference servers for code suggestions.\nHowever, the energy consumption of software has gained prominence as a significant environmental and societal concern. In the context of Artificial Intelligence (AI), Green AI has been defined by Schwartz et al. [1] as research that yields novel results while considering the computational cost, making practitioners ideally reduce resources spent. Specifically to Green AI applied to LLMs, studies have observed the environmental impact of training and using such LLMs. For instance, Samsi et al. benchmarked the energy consumption of LLM inference and were able to estimate the energy of a single response from an LLM [2]. Other works focused more on the impact of training the model, such as the carbon footprint of the BLOOM model estimated by Luccioni et al. [3]. However, the evaluation of LLMs' energy consumption remains challenging when assessing LLMs dedicated to specific purposes. In the context of LLMs for code, existing studies have only focused on the impact of the generated code [4, 5].\nIn this paper, we aim to determine how much energy an average developer consumes when using a code assistant similar to GITHUB COPILOT and how to reduce it. To the best of our knowledge, our work is one of the first to study the energy consumption of LLMs in code assistants. In particular, we wish to deliver actionable insights into the energy consumption of the code assistant from the perspective of both the service provider (e.g., GITHUB COPILOT) and the end-user interacting with the code assistant. All the more in the context of code assistants, like GITHUB COPILOT, the end-user knows little about the internal workings and impacts of the service: as the LLM inference is executed remotely in the cloud, it is difficult for the developer to perceive the computing impact of using a code assistant. Thus, we aim to answer the following research questions:"}, {"title": "2. Code Assistant Dataset", "content": "To investigate our research questions, it was imperative to measure the energy consumption of a code assistant in a realistic usage setting. To that end, we designed an experiment with participants using GITHUB COPILOT to gather a dataset of development traces, enabling us to simulate developers using a code assistant on an inference server under our control. This approach was necessitated by our inability to access GITHUB COPILOT'S inference server and our objective to measure its power consumption. Moreover, conducting this experiment in two distinct phases\u2014(i) having participants use a code assistant to develop a small application followed by (ii) exploring the energy consumption through traces replay\u2014allowed us to gain"}, {"title": "2.1. Involved participants", "content": "We recruited 20 volunteers among Computer Science (CS) students and CS professionals, using mailing lists and word of mouth. All of the participants were proficient in Java programming. 13 participants were between 18 and 25 years old, 6 were between 26 and 35 years old, and 1 was between 36 and 45 years old. 13 of them were students in CS (12 in a master's degree and 1 in a bachelor's degree), and 7 of them were CS professionals. 1 participant did not know GITHUB COPILOT before the experiment, 7 participants knew GITHUB COPILOT but never used it, 4 already used it a little, and 8 used it regularly. All of the participants were familiar with VSCODE. The participants were compensated for their time with 50\u20ac each. This experiment was approved by our institution's Ethical Board.\nBefore the experiment, the 20 participants filled out a survey with their age, development experience, and familiarity with GITHUB COPILOT. Following the experiment, 19 out of the 20 participants agreed to complete a subsequent survey concerning their experience and feelings during the experiment. This last survey was used to report ideas for future research."}, {"title": "2.2. Assigned task", "content": "The task given to the participants consisted of developing a CLI Connect 4 game2 in Java in one hour using VSCODE and GITHUB COPILOT, it is derived from a project given in an university OOP introductory course. A skeleton of the project and associated unit tests were provided. The participants then had to design and write the game loop and logic and handle the board display to the players using the CLI. The instructions for the participants, the skeleton project, and the projects created by the participants are available in our replication package.\nThe participants used the GITHUB COPILOT VSCODE extension only through inline and panel completions; that is, they could not use other features of GITHUB COPILOT, such as the chat. They were also given access to the Internet while being forbidden from using other AI assistant (e.\u0434., ChatGPT)."}, {"title": "2.3. Dataset description", "content": "The ASSISTANTTRACES dataset consists of all the telemetry sent by GITHUB COPILOT during the experiment in JSON format. There are 119, 774 telemetry messages in total, including 9, 633 generation requests. The dataset is available at https://doi.org/10.5281/zenodo.11503612."}, {"title": "3. Experimental setup", "content": "To estimate the energy consumption of GITHUB COPILOT, we leveraged the collected traces to simulate the developers' behavior and the front end of the code assistant on an inference server. Specifically, the inference servers were run on a cluster, whose nodes consist of AMD EPYC 7513 (Zen 3), with 512 GiB of memory and 4 Nvidia A100-SXM4-40GB (40 GiB). The server's distribution and OS were Debian 6 on Linux 5.10.0-28-amd64.\nAll the artifacts of this study, including our results, code, and datasets, are available in the following public repository: https://doi.org/10.5281/zenodo.13167546."}, {"title": "3.1. Simulation client", "content": "Thanks to the telemetry data from the ASSISTANTTRACES dataset, we could reproduce the API requests that GITHUB COPILOT sent to its inference server. We developed a simulator (the client) to mimic developers' interactions with GITHUB COPILOT by replaying generation requests to the inference server. The main benefit of our approach is that we can make multiple simulations with different scenarios by varying e.g., the number of developers or the behavior of the code assistant.\nWhen performing simulations with concurrent developers, we limited the simulation to the first hour of development to account for some developers"}, {"title": "3.2. Inference server", "content": "To handle generation requests and generate code suggestions, we set up an inference server. Code suggestions are generated using an LLMs. In particular, we used the TEXT GENERATION INFERENCE (TGI) server,3 a server for text generation inference that is easily configurable to operate with different LLMs and that supports sharding between multiple GPUs and multiple parallel requests (using continuous batching). The server was set up with its default parameters, except for the number of shards, quantization method, and the number of concurrent requests which we describe in the next section."}, {"title": "3.3. Studied factors", "content": "Using the aforementioned setup, we performed several simulations with varying elements in the setup's configuration. We chose to study specific factors because we hypothesized they would affect the energy consumption of the code assistant. The factors we studied are as follows:\nNumber of concurrent developers: We varied the number of developers concurrently querying the code assistant ranging from 1 to 500 with discrete values: 1, 2, 5, 10, 20, 30, 50, 75, 100, 150, 200, 300, 400, 500.\nStreaming the requests: We emulated different request-sending behaviors from GITHUB COPILOT by activating and deactivating streaming when"}, {"title": "3.4. Configuration space exploration", "content": "We define a configuration as a set of factors with a specific value. One example of configuration is [20 concurrent developers; streamed requests;"}, {"title": "3.5. Energy consumption measurements", "content": "We measured the energy consumption of the inference server's CPU and GPU using perf and nvidia-smi utilities, respectively. We also collected the time taken for generations to complete (latency), the number of rejected"}, {"title": "4. Data analysis", "content": "In this section, we describe our methodology for analyzing the results we obtained from section 3."}, {"title": "4.1. Simulations analysis", "content": "To get an overview of the data, we computed the mean power consumption (in Watts) and total energy used (in Wh) for every simulation, as well as the mean latency, the number of rejected requests, and the number of completed generations. We also derived the power consumption per developer, which allocates an equal share of the server's energy consumption to every developer using the code assistant. When analyzing a simulation with multiple concurrent developers, only the period in the simulation where all developers were active in parallel was considered.\nServer saturation. When the server receives more requests than it can handle, the number of rejected requests or the latency may increase, depending on the client and server configuration. We consider that a server is saturated when the number of rejected requests exceeds 10% or when the mean latency of the requests exceeds 20 seconds. Note that this saturation threshold was set arbitrarily; searching for an ideal saturation threshold is out of the scope of this paper.\nMeasuring the impact of a factor. To assess the impact of a factor on energy consumption or latency, we compared pairs of configurations that differed only by the factor being studied. This method ensures that any observed differences are solely due to the factor in question."}, {"title": "4.2. Participant analysis", "content": "For each participant, we performed analyses using the gathered telemetry and survey data. Specifically, we calculated the number of generations"}, {"title": "5. Results", "content": "In this section, we summarize the key observations from our experiment and answer our research questions. We make our complete results available in the companion notebook in the replication package."}, {"title": "5.1. Relationship between participant's characteristics and their usage of GitHub Copilot", "content": "Before answering any of the research questions, we wanted to have a look at the relationship between some of the participant's characteristics, such as their experience, familiarity with GitHub Copilot, if they coded in java during the last year, or whether or not they finished the task, and their usage of GitHub Copilot, that is, the rate at which the participants made requests, and the ratio of code suggestions they accepted when presented to them. We"}, {"title": "5.2. RQ1: What is the impact of studied factors on the energy consumption and performance of code assistants ?", "content": "In this section, we report on our findings on the different studied factors and their impacts. In Figure 2 and Figure 3, we depict the impact of switching from one option to another on energy consumption and request latency, respectively. The impacts were calculated using the method described in subsection 4.1. Each hue represents one factor, while each row represents the switch from one option to the next. The X-axis reflects the ratio in measured latency or energy consumption between the first and second options.\nNumber of concurrent developers. When increasing the number of concurrent developers, we observe an increase in the average power con-"}, {"title": "5.3. RQ2: How many generation requests made by GITHUB COPILOT are actually useful?", "content": "Figure 5 illustrates the final state of generation requests sent by GITHUB COPILOT during our experiment. Out of the 9, 634 generation requests made by GITHUB COPILOT over the 20 participants, only 2, 944 finished generating and were displayed to the user (30%). Of these, 1,066 were accepted (11% of all requests), and 816 (8.5%) were kept in the code at the end of the experiment according to GITHUB COPILOT.\nOur analysis reveals that the majority of generation requests made by GITHUB COPILOT were canceled, often because they were started while the user was still typing, leading to new requests that replaced the previous ones. Empty completions typically occur at the end of a sentence, or before a closing parentheses or brackets. The completions that were displayed but not accepted are most likely due to either the users not wanting or needing the suggestion in the first place, or to the suggestions not matching what the user expected.\nPreviously, we discussed the energy implications of these inefficiencies when presenting the energy saving of manually triggering GITHUB COPILOT. By eliminating unnecessary requests, i.e., canceled and empty requests, we could save 15% of energy on average. This relatively modest energy saving, despite removing almost 70% of the generations is due to the lower-than-average computing time used by canceled and empty generations, compared to completed generations. Assessing the energy impact of suggestions displayed, but not accepted, by users is left for future research.\nRQ2: An analysis of GITHUB COPILOT's generation requests reveals a significant share of them are not useful to the end user, indicating that many generation requests are either unnecessary or not helpful to users. This inefficiency highlights the potential for substantial energy savings by optimizing when and how generation requests are made, possibly through more intelligent triggering mechanisms or better user interaction designs."}, {"title": "5.4. RQ3: Under different scenarios and objectives, how much does a developer using a code assistant such as GITHUB COPILOT Cost in energy?", "content": "For the sake of simplicity, this section only focuses on a subset of configurations and their impacts, given the large number of different configurations"}, {"title": "6. Discussion and implications", "content": "In this section, we discuss the results reported in the previous section and their implications."}, {"title": "7. Limitations", "content": "In this section, we address several limitations of our study:\nParticipants and task selection. Selecting participants by word of mouth, and mailing lists from university students and professionals allowed us to easily recruit enough qualified participants for our study. Moreover, the task assigned to the participants was designed to be short and simple for an experimental setting, yet long and complex enough to simulate a realistic development session. One alternative was to perform our dataset collection on developers working for their companies or school projects. While this alternative could have provided us with more realistic development traces, it would have been logistically harder to perform. Another alternative was to assign multiple short tasks, following Vaithilingam et al. [19], which could simplify generalization, while compromising the realism of the full development process, including including design, debugging, and testing.\nHaving a non-representative sample of the population of developers and a not complex-enough task may affect some of our results such as the number of accepted/rejected generation requests, as well as the number of requests"}, {"title": "8. Related Works", "content": "Previous research have investigated various properties of LLMs specialized in code generation: previous research has investigated various aspects of LLMs for code-related tasks, including the security of their suggestions [22, 23, 24], the prevalence of bugs in the generated code [25], how developers interact with them [14, 19, 24] or just the quality and correctness of the code they generate [6, 26, 27, 28]. There have also been efforts to measure the efficiency of LLMs through the creation of benchmarks for comparing them, such as HUMANEVAL [21], MBPP [29], CODEREVAL [30], APPS [31], CODEXGLUE [32] or RECODE [33]. Xu et al. [34] also conducted a comparative evaluation of multiple LLMs for code.\nSpecifically, researchers have also investigated the energy consumption and carbon footprint of various aspects of Deep Learning, such as computer vision [3, 35, 36] and natural language processing [3, 4]. Notably, in the context of LLMs, Vartziotis et al. studied the Green Capacity of LLMs for code [4]. They quantified the sustainability awareness of ChatGPT, GITHUB COPILOT and CodeWhisperer, based on the energy consumption and performance of the solutions they generated for Leetcode problems. Coignion et al. studied the performance of the code generated by multiple LLMs on Leetcode, and found that the LLMs studied had similar code performance [5]."}, {"title": "9. Conclusion", "content": "In this study, we explored the energy consumption of code assistants powered by LLMs, focusing on GITHUB COPILOT. Our findings indicate that various configuration factors, such as the number of concurrent developers, model size, and use of streaming, impact both the energy consumption and performance of these tools. Notably, a substantial amount of energy is spent on generation requests that are ultimately unused, highlighting an area for potential improvement.\nOur results indicate that significant energy savings can be achieved by service providers and end-users if they take the following steps: disable the automatic triggering of code assistant suggestions on the client side, utilize batching techniques to maximize inference server usage and support more concurrent developers, and reduce model resource consumption through effective quantization and the use of more efficient models."}]}