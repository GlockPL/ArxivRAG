{"title": "Vision-Language Model Dialog Games for Self-Improvement", "authors": ["Ksenia Konyushkova", "Christos Kaplanis", "Serkan Cabi", "Misha Denil"], "abstract": "The increasing demand for high-quality, diverse training data poses a significant bottleneck in advancing vision-language models (VLMs). This paper presents VLM Dialog Games, a novel and scalable self-improvement framework for VLMs. Our approach leverages self-play between two agents engaged in a goal-oriented play centered around image identification. By filtering for successful game interactions, we automatically curate a high-quality dataset of interleaved images and text. We demonstrate that fine-tuning on this synthetic data leads to performance gains on downstream tasks and generalises across datasets. Moreover, as the improvements in the model lead to better game play, this procedure can be applied iteratively. This work paves the way for self-improving VLMs, with potential applications in various real-world scenarios especially when the high-quality multimodal data is scarce.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) have achieved remarkable success by training on vast datasets that now include a significant portion of the Internet (Gemini, 2024; OpenAI et al., 2023). Their performance generally scales with training data size (Kaplan et al., 2020), but acquiring new, high-quality data is increasingly challenging, especially for vision-language models (VLMs), which require carefully curated interleaved image and text data. Recent research (Bai et al., 2022; Chen et al., 2024; Huang et al., 2023; Yuan et al., 2024) indicates that self-improvement techniques can use synthetically generated data to overcome this limitation. We introduce a novel self-improvement method based on goal-oriented play between VLMs. This approach provides a scalable way to iteratively generate high-quality synthetic data, which can be used to fine-tune the model for further performance improvement. By carefully designing the game, we can target specific capabilities and domains for improvement, while the goal-oriented nature ensures the quality of the generated data.\n\nWe initiate the process with two VLMs which are assigned the roles of \"Describer\" and \"Guesser\" in a variant of reference game (Das et al., 2017;"}, {"title": "2. Related Work", "content": "Dialog games Various forms of dialog reference games have been known in linguistics since a long time (Krauss and Weinheimer, 1964). In computer science, prior work on multimodal dialog games is primarily focused on collecting datasets of grounded dialogs (Das et al., 2017; De Vries et al., 2017; Haber et al., 2019) or more recently, evaluating the capabilities of VLMs (Chalamalasetti et al., 2023; Hakimov et al., 2024). These existing games vary in design, including the number of images involved (single or multiple), the roles of the agents (symmetric, sharing the same goal, or asymmetric), and the interaction length (single-turn or multi-turn).\n\nIn contrast to these evaluation and data collection efforts, our work leverages dialog games for self-improvement, creating synthetic datasets to enhance VLM capabilities. To the best of our knowledge, this is a novel use of the dialog games.\n\nSelf-Improvement Self-improvement (Chen et al., 2024) techniques have gained significant interest in both language and multimodal learning. A prevalent approach involves using an LLM to critique and refine its own responses (Bai et al., 2022; Yuan et al., 2024). For instance, Huang et al. (2023) demonstrate that fine-tuning on self-generated rationale-augmented answers, without ground truth labels, enhances LLM reasoning. Subramaniam et al. (2025) propose a multi-agent framework where diverse responses from a society of LLMs drive iterative fine-tuning and continuous improvement.\n\nSelf-improvement has also shown promise in enhancing multimodal understanding of VLMs, albeit with fewer existing studies. A prominent technique in VLM self-improvement is cycle consistency, initially developed for image-to-image translation (Zhu et al., 2017). It enforces that a transformation from a source domain to a target domain, and back, yields an output similar to the input. This principle has been successfully extended to the multimodal domain (Li et al., 2023a,b; Sharifzadeh et al., 2024), often exploiting the symmetry between image captioning and text-to-image generation. Cycles such as text"}, {"title": "3. Method", "content": "This section introduces our method for iterative self-improvement through VLM Dialog Games. We first describe the VLM Dialog Game mechanism and its key properties: self-play and goal-oriented nature, which are crucial for self-improvement (Section 3.1). We then detail the complete self-improvement workflow, including game setup, dialog generation and filtering, model finetuning, final evaluation on the target task, and potentially repeating these steps (Section 3.2)."}, {"title": "3.1. VLM Dialog Game Description", "content": "We introduce a VLM Dialog Game which is constructed using unlabelled images and two VLM agents. The first agent, the Describer, is presented with a single target image and is instructed to faithfully answer questions about it. The second agent, the Guesser, receives a set of N images, including the target image and several distractor images. The Guesser's objective is to identify the target image by posing questions to the Describer. The agents' behaviour is controlled by prompting mechanism for VLMs which is described further in Section 3.2.1.\n\nFigure 1 shows an illustrative example of the VLM Dialog Game in action. All images that the Guesser sees contain white and blue objects on an orange background, thus, to identify the target image the Guesser should focus on more specific properties of the images. To disambiguate, the Guesser initiates a series of clarifying questions, such as \"How many objects can you see?\" and \"Are the objects squares or circles?\". Once the responses uniquely define the target image (in this case, by pointing to \"9 square objects\"), the Guesser successfully identifies it. While resembling a classic reference game used for human data collection and VLM evaluation, this specific design features two key elements enabling VLM self-improvement: self-play for data generation and automatic success determination."}, {"title": "3.2. Workflow", "content": "The properties of the VLM Dialog Game enable the following workflow for self-improvement:\n\n\u2022 Game setup: Configure the dialog game with a designated unlabelled image dataset."}, {"title": "3.2.1. Game Setup", "content": "This section details the setup of the VLM Dialog Game, including agent instructions and image selection strategies.\n\nAgent instructions We provide precise instructions to both the Describer and Guesser agents to guide their interaction in the game. The Describer is instructed to answer questions about the target image truthfully and accurately. The Guesser agent operates in two stages:\n\n1. Questioning/Guessing: Initially provided with an empty image description, the Guesser must either:\n\n\u2022 Ask a clarifying question to distinguish the target image from the distractors, or\n\n\u2022 Make a guess identifying image X as the target image where X is the index of the hypothesized target image among the distractors.\n\n2. Summarisation: The Guesser must create a concise summary of the target image description given the initial image description (or the previous summary), a question and an answer.\n\nSpecific prompt texts for both agents are provided in Appendix 6.1.\n\nImage selection and game difficulty The images used in the game can be sourced from various datasets, including general datasets of natural images like OpenImages (Kuznetsova et al.,"}, {"title": "3.2.2. Dialog Generation", "content": "During this stage, the Describer and Guesser agents engage in an interactive dialog. We construct the training dataset from examples of successful behavior by both the Describer and the Guesser:\n\n\u2022 Describer examples. Input: a single image and a question about it; Output: the corresponding answer.\n\n\u2022 Guesser examples. Input: N images (including the target and distractors) and a cumulative summary of the target image description; Output: either a clarifying question or a guess identifying the target image.\n\nEach successful VLM Dialog Game generates multiple training examples of both types."}, {"title": "3.2.3. Dialog Filtering", "content": "The game's design allows us to directly verify the Guesser's final selection. However, to mitigate the possibility of correct guess occurring by chance,"}, {"title": "3.2.4. Model Improvement", "content": "The filtered dataset from successful dialog games is then used to fine-tune the VLM in a standard supervised fine-tuning way. If the gains in playing the VML Dialog Games are large, we can use the improved model in order to collect the new synthetic dataset for further model improvement. While this process directly affects the VLM's performance within the dialog game itself, our primary focus is on evaluating its capabilities on downstream tasks. For instance, if the dialog game utilizes images from a robotics domain, we assess the fine-tuned VLM's performance on tasks such as robotic success detection, and for general images we test the performance on visual question answering (VQA) on the unseen images."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\n\nWe evaluate our method using the Gemini 1.5 Flash model (Gemini, 2024) as the base VLM. Gemini 1.5 Flash is a powerful, instruction-tuned VLM that can take as input interleaved text and images and it provides a strong base model. We use standard supervised fine-tuning procedure (see Appendix 9). We limit the game length to a maximum of three question-answer turns. For conciseness, we refer to the self-improvement method of the fine-tuning on synthetically collected dialogs as \"VLM Dialog Games\"."}, {"title": "4.2. Experiments with General Images in Dialog Games", "content": "This section details our experiments using the DOCCI (Onoe et al., 2024) and the OpenImages datasets (Kuznetsova et al., 2020) to evaluate the effectiveness of our self-improvement method for image understanding through VQA tasks."}, {"title": "4.2.1. Dataset and Game Configuration", "content": "DOCCI dataset contains clusters of images grouped by their category. We randomly sample 1000 image groups, each containing N = 4 images from one of 149 categories. Figure 2 provides an example of a dialog game generated by prompted Gemini using this setup.\n\nOpenImages We select a subset of 1000 random images, forming them into games with N = 4 images. As the dataset does not contain clusters, we select the most similar images (Jia et al., 2021) as distractors. An example of a dialog game produced in this scenario is demonstrated in Figure 3."}, {"title": "4.2.2. Evaluations Tasks", "content": "Dialog success rate Following prior work using dialog games to assess VLM capabilities (Haki-"}, {"title": "4.2.3. Results", "content": "Table 1 compares the performance of the base Gemini 1.5 Flash model with VLM Dialog Games method. Fist, results demonstrate that the VLM Dialog Games method with either the DOCCI or OpenImages datasets improves performance within the game with both training and unseen images (e.g., games played on DOCCI by a model trained with OpenImages). More importantly we also achieve better performance on broader visual understanding tasks as measured by VQA accuracy. Note that evaluation images for it are drawn from a distinct dataset (VQAv2), demonstrating the generalization of our method. Specifically, for DOCCI dialog games, the accuracy on the VQAv2 yes/no and counting subsets increased by 6.8% and 2.3%, respectively. For OpenImages dialog games, yes/no question accuracy increases by 10.4% and remains unchanged for counting questions. We hypothesis that different image sources may be better suited for improving specific tasks. For example, Onoe et al. (2024) note that many DOCCI images contain references to counts, suggesting that this dataset is well-suited for self-improvement on counting task."}, {"title": "4.3. Ablation Studies", "content": "Next, we investigate the impact of key design choices: the number of images per game and the method of image grouping. We test the different options on OpenImages dialog games and VQA yes/no question accuracy.\n\nImpact of the number of images per game We study the effect of N on the game complexity by varying N from 2 to 8 (see Appendix 7 for dialog examples). Table 2 presents the game success rate, the number of question-answer pairs from successful dialogs, and the VQAv2 yes/no accuracy for each N. While fine-tuning with data from any N improves VQAv2 performance compared to the base Gemini 1.5 Flash model, the best result is achieved with N = 4 in this study. With N = 2, the game is relatively simple, leading to a high success rate but potentially less informative data, and a higher probability of erroneous data due to the correct guesses by chance. Conversely, with N = 8, the game becomes too difficult, resulting in few successful dialogs for fine-tuning. These results confirm that balancing game difficulty and the quantity of training data is crucial for generating an optimal dataset for fine-tuning."}, {"title": "Impact of Image Grouping Strategy", "content": "We investigate how image grouping affects model performance by comparing two strategies: 1) similarity-based grouping (Section 4.2.1), which uses visually and conceptually related distractors to elicit more targeted Guesser questions, and 2) random distractor selection. Table 3 compares models using these strategies. Both strategies improve over the initial Gemini 1.5 Flash checkpoint (73.0%) significantly, therefore, the VLM Dialog Game can be effectively implemented even with random image groupings. However, using similar images yields slightly higher accuracy (83.4% vs. 82.6%). While random images produce a larger quantity of successful dialogs (24.7% vs. 18.4%), the increased challenge of similar images in a game likely leads to more informative training data. Thus, we believe that for the best results in fine-tuning, we need to find a right trade off between game difficulty and training data quantity."}, {"title": "4.4. Robotics Dialog Games", "content": "High-quality interleaved data is scarce in specialized domains, potentially limiting base model performance in applications. This section describes our experiments using the VLM Dialog Games on video frames from a robotics manipulation domain where we test VLM success detection in object manipulation tasks."}, {"title": "4.4.1. Dataset and Game Configuration", "content": "We use image frames from videos recorded in the ALOHA setup (A Low-cost Open-source Hardware System for Bimanual Teleoperation) (Zhao et al., 2023). The images feature bimanual robotic arms performing 10 object manipulation tasks (e.g., putting objects in containers). We use images captured from an overhead camera perspective. Our dataset comprises 20 episodes (both successful and unsuccessful) for each of the 10 tasks,"}, {"title": "4.4.2. Evaluation Task: Success Detection in Robotics", "content": "To evaluate the impact of our method on robotic task understanding, we measure the model's ability to perform success detection. Accurate success detection is critical for various robotics applications, including policy training, evaluation, and data curating. We evaluate success detection on the final frame of video episodes, treating it as a zero-shot VQA task (Du et al., 2023). The model is presented with the final frame image and a textual description of the intended task (e.g., \"open the drawer\") and it is prompted with a question on task completion (e.g., \"Is the drawer open?\"). We report the accuracy of the model's yes/no responses."}, {"title": "4.4.3. Baselines", "content": "To isolate the specific contribution of the VLM Dialog Games, we compare our method against the original Gemini 1.5 Flash model and several other baselines."}, {"title": "Description Supervised Fine-Tuning (SFT-Description)", "content": "Since our dialog games design utilizes task descriptions for each robotic episode, we include a baseline fine-tuned directly on image-description pairs. This baseline \"SFT-Description\" helps determine if simply exposing the model to paired image and task descriptions from the target domain is sufficient for improvement."}, {"title": "Self-Improving Question Answering (Self-QA)", "content": "This baseline explores an alternative self-improvement approach based on question answering similar to the approach of Luu et al. (2024) (without the image captioning). The model performs two tasks:\n\n1. Question generation: Given an image from the ALOHA dataset, the model generates a question about the scene.\n2. Answer generation: Given an image and a generated question, the model provides an answer."}, {"title": "VLM Dialog Games (Answers Only)", "content": "Our fine-tuning data includes both Describer and Guesser perspectives. Since the final success detection task closely resembles the Describer's role of answering questions, we include a baseline fine-tuned only on the datapoints from the Describer. This isolates the contribution of the Guesser's questions to the overall improvement."}, {"title": "Multiple Rounds of Self-Improvement", "content": "We expect fine-tuning to improve the model's performance in subsequent games. Thus, we use the improved model to generates a new, higher-quality dataset of synthetic dialogs. These dialogs are filtered and used to fine-tune the next iteration of the model, a process we refer to as \"round 1\" and \"round 2\"."}, {"title": "4.4.4. Results", "content": "Table 4 presents the success detection accuracy and game success rates averaged across the 10 robotic tasks. The initial Gemini 1.5 Flash model achieves a success detection accuracy of 56.5% on this highly specialised domain, only slightly above chance. Both the SFT-Description and Self-QA baselines improve upon this, demonstrating the benefit of domain-specific fine-tuning (65.0% and 67.0% accuracy, respectively).\n\nHowever, fine-tuning on a single round of dialog game data (VLM Dialog Games (round 1)) yields a larger improvement, achieving a success detection accuracy of 69.5% surpassing the baseline Self-QA by 2.5%. Interestingly, although the VLM received no explicit instructions for success detection, the need to distinguish between frames from the same task type lead it to focus on the task progression. In contrast, the Self-QA method primarily generated object-related questions (see Appendix 8 for examples).\n\nImportantly, this initial round of dialog game fine-tuning also substantially increases the game success rate, from 14.39% to 40.15%, thus enabling further improvement. We performed a second round of fine-tuning (VLM Dialog Games (round 2)), using data generated by the round 1 model. This further boosted both the game success rate (to 53.74%) and the success detection accuracy (to 73.0%), a 16.5% absolute improvement over the original base model.\n\nThe VLM Dialog Games (answers only) baseline, which uses only the Describer's answers from the dialog games, achieves a success detection accuracy comparable to VLM Dialog Games (round 1). However, its game success rate remains comparatively low (17.92%) and does not enable further iterative improvement. This suggests that while the Describer's answers are sufficient for improving success detection, the Guesser's questions play a crucial role in improving the model's ability to play the dialog game effectively, which is necessary for continued self-improvement.\n\nTo conclude, our dialog game framework enables significant adaptation to specialized tasks like robotic success detection, where standard VLM pre-training may be less effective due to the lack of the domain-specific data. Crucially, this self-improvement is achieved with minimal task-specific supervision, requiring only video episodes to guide the dialog generation."}, {"title": "5. Discussion, limitations and conclusion", "content": "This paper introduced VML Dialog Games as a novel self-improvement framework. Our approach leverages goal-oriented self-play between two agents engaged in a reference-style dialog game. By automatically filtering for successful game interactions, we generate a high-quality dataset of interleaved image and text"}, {"title": "6. Appendix: VLM Prompts", "content": "6.1. Prompt for VML Dialog Games\n\nPrompt to Describer agent to answer questions about the image faithfully is the same for all datasets and domains:\n\nYou are given an image and your\ntask is to answer a given\nquestion about it. Be precise\nand accurate. Only answer the\nquestion, do not say anything\nelse about the image.\nImage: {image}\nQuestion: {question}\nAnswer:\n\nPrompt for Guesser for general images:\nYou are given several images\nImage 1, Image 2, Image 3,\nImage 4 and image description."}, {"title": "8. Appendix: Question-Answers Generated by Dialog Games and Self-QA", "content": "Self-QA:\n\n\u2022 Question: Is there a yellow object in the image? Answer: yes"}, {"title": "9. Appendix: LLM Inference and Training Details", "content": "We rely on Gemini 1.5 Flash (gemini-1.5-flash-002) model which is available for inference and fine-tuning through the Google Cloud Vertex API. For generating dailog and evaluation, we sample with nucleus sampling selecting the top 0.8 probability mass of tokens. For the evalaution, we use sampling temperature 0. Our batch size for SFT is 16, we use Adam optimizer with learning rate 5 \u00d7 10-7. To prevent overfitting to the small datasets from the dialog games, we use a small unrelated to the any of the tested tasks dataset of images with text and track token loss on it. We select a checkpoint just before the loss starts increasing. This usually corresponds to approximately one epoch of fine-tuning."}]}