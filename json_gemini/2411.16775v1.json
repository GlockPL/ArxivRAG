{"title": "Parameter Efficient Instruction Tuning: An Empirical Study", "authors": ["Pengfei He"], "abstract": "Instruction tuning [27] [25] [16] [24] has become an important step for finetuning\npretrained language models to better follow human instructions and generalize\non various tasks. Nowadays, pretrained language models[2] become increasingly\nlarger, and full parameter finetuning is overwhelmingly costly. Therefore, Parameter\nEfficient Finetuning (PEFT) [3] [14] [6] has arisen as a cost-effective practice for\ninstruction tuning because of significantly smaller computational, memory, and\nstorage cost compared to full finetuning. Despite their widespread adaptations, the\nvast hyperparameter spaces, the number of PEFT methods, the different focus of\ninstruction tuning capabilities make disentangling the impact of each aspect difficult.\nThis study systematically investigates several representative PEFT methods, survey-\ning the effect of hyperparameter choices including training hyperparameters and\nPEFT-specific hyperparameters, how different models sizes and the number of\ninstruction tasks affect the performance, in-task-distribution memorization and\nopen instruction following capability[23]. Our empirical study shows that only\nLORA [8] and adapter [7] can get close to full finetuning with ideal training settings.\nThe ideal training setting includes an appropriate learning rate, largest LoRA rank\nor adapter size allowed and a diverse training tasks. On the other hand, LoRA and\nadapter suffer from training instability if such an ideal training condition is not\nmet. Additionally, LoRA requires a greater number of tasks for effective unseen\ntask generalization, exhibit slower learning speed. Moreover, LoRA has weaker\ntask-level memorization. Lastly, LoRA and adapter falls short in complex reasoning,\ncoding and long-form generation compared to finetuning in open instruction tuning\nsettings but it shows stronger capabilities compared to adapter. We hope our work\ncould guide practitioners through the PEFT optimization and provides the insight\nof further research on what aspects to improve these methods in instruction tuning\nscenarios.", "sections": [{"title": "1 Introduction", "content": "Recent study demonstrates instruction tuning which trains language models on instruction-output\npairs data can enhance model's capability of comprehending human instructions and following these\ninstructions. Efforts are also underway to compile a diverse mixture of high-quality instruction tuning\ndata[22] [23][9] [28] to advance language models in their better generalization on downstream tasks\nand better alignment with user intents. Consequently, instruction tuning has become a standard method\nof aligning LLMs closely with human instructions. However, LLM's vast amounts of parameters often\nconstrains their accessibility through traditional finetuning methods due to the high training, storage,\nmemory cost it incurs. In light of this, PEFT has recently demonstrated remarkable achievements to\naddress the concern. This is primarily due to its cost-efficiency, as PEFT necessitates the training of\nonly a fraction of the model's parameter, and it results in much lower required training memory and\nstorage."}, {"title": "2 Experiment Setups", "content": "Adapter[7], a pinoeer PEFT method, is a bottleneck network inserted between layers within fixed\npretrained model. LoRA [8] trains on a low-rank weight matrix, applied additively to selected matrix\nindependently within the transformer layer. Zaken et al. [26] proposes to finetune the biases of the\nneural network only. Inspired by the effectiveness of text prompting methods in directing LLM,\nPrompt Tuning [10] and Prefix Tuning [11] have been developed, they concatenate a sequence of soft\ntokens(continuous vectors) into the model input or activation and only train these soft tokens.\nIn light of these, our study begins by identifying effective PEFT methods. This involves training\non expert-written instruction tuning dataset SuperNI[22] and conducting a series of experiments.\nThese experiments vary in hyperparameters, data sizes, data distribution and model sizes. To explore\nthe PEFT's effectiveness further, we extend our training and evaluation to a more challenging setup\nT\u00dcLU[23], and it consists of more diverse instruction tuning tasks and more comprehensive evaluation\nwhich covers a range of complex model capabilities (i.e., factual knowledge, reasoning, multilinguality,\ncoding) and open-ended instruction-following abilities.\nOur key findings are as follows:\n1.  We identify LoRA and adapter are most effective PEFT methods for instruction tuning (Sec.\n3.1). Greater LoRA ranks/adapter sizes help improving performance (Sec. 3.2).\n2.  We find that LoRA and adapter training exhibits certain instability compared to finetuning,\nand such an instability correlates with greater rank and higher learning rate (Sec. 3.3).\n3.  We validate that PEFTs with larger base models consistently improve the final performance\n(Sec. 3.4).\n4.  We show that both LoRA and adapter have weaker generalization capability in low-data\ninstruction tuning settings (Sec. 3.5), and LoRA shows weaker task-level memorization\nwhen compared to both adapter and finetuning (Sec. 3.6).\n5.  For open instruction tuning, LoRA has generally better multifaceted capabilities than adapter\nand make it an ideal alternative to finetuning. Plus, both adapter and LoRA demonstrates\nweak coding, complex reasoning and long-form generation capabilities (Sec. 4)."}, {"title": "2.1 Setup 1: T5 finetuning on SuperNI", "content": "In the first experiment, our goal is to identify the instruction following capabilities of PEFTS. We\nlimit the hyperparameter search space along learning rates and PEFT-specific hyperparameter only.\nWe choose to use SuperNI [22] as our dataset for two reasons. First, it comprises of a large amount\nof different tasks, offering an ideal testbed for the cross-task generalization capabilities of LLMs.\nSecond, compared to some latest larger instruction data mixture, it's considerably smaller which\nsignificantly reduces per-experiment runtime and thus overall runtime greatly. For the first setup,\nwe use model T5-3B \"LM-adapted\" language model [18] which is further trained with a language\nmodeling objective.\nIn our standard setup, our full dataset comprises 707 training tasks and 50 validation tasks, all\nrandomly selected from SuperNI [22] original training tasks. The test data compromises 119 unseen\nEnglish tasks adhering the original setup. For each task, we sample 100 instances. In preparing the\ndata examples aligning with instruction data format, we concatenate the task definition and the task\ninput for each data example. Based on the standard setup, we vary model sizes, data sizes, PEFT for\nablation study of the correlation of one or more variables.\nEach PEFT method is trained for 4 epochs to ensure adequate training for each PEFT method. For\nPEFTs, we utilize the AdamW optimizer, paired with a linear learning rate scheduler and a warm-up\nphase that comprises 3% of the total training steps. For finetuning, we use constant learning rate\nscheduler which kept the same training setting as Tk-Instruct [22]. We use RougeL [13] score as\nour evaluation metric, and the best-performing checkpoint is selected for evaluation on the test set.\nOverall, our first experiment design represents a balanced trade-off between testing effectiveness and\nidentifying patterns of PEFT and efficiency of running experiments at scale."}, {"title": "2.2 Setup 2: LLAMA-2 finetuning on T\u00dcLU datasets", "content": "With the selected PEFTs and their best hyperparameters found from the first setup, we want to\ncompare PEFTs based on modern LLMs with finetuning in more comprehensive instruction training\nand evaluation settings. For this purpose, we utilize LLAMA-2 [20] language model as our base\nmodel. We leverage open instruction datasets T\u00dcLU[23] which compromises both human-generated\nand GPT-generated instruction tuning data, and we follow the same multi-faceted evaluation setup\ncovering factual knowledge, reasoning, multilinguality, coding and open-ended instruction following.\nWe train selected PEFT methods for 3 epochs. These were conducted with a maximum input length\n2048, consistent with finetuning setting from T\u00dcLU [23]. Refer to Sec. C for detailed training\nhyperparameters."}, {"title": "3 Empirical Findings", "content": "We derived Finding 1 through 6 from Setup 2.1, and Finding 7 from Setup 2.2. Finding 3 through 6\nprovides a deeper exploration into LoRA."}, {"title": "3.1 Finding 1: LoRA and adapter are effective for instruction tuning.", "content": "Our extensive hyperparameter search reveals that among the five PEFTs, only LoRA and adapter are\nproved close to full finetuning in instruction tuning settings (See Table 1). Prompt tuning shows no\neffective learning due to the difficulty of optimizing soft prompts as claimed in some other findings [8]\nand the cross-task nature of instruction tuning. Prefix tuning shows some learning but still significantly\nunderperforms finetuning. This observation is in line with recent theoretical analyses[17], which\nsuggests that prefix tuning and prompt tuning are less expressive than finetuning. Prefix tuning also\nhas a reparameterization trick that transforms the prefix matrix into MLP, and our experiment results\nindicate such reparameterization improves training stability. Lastly, BitFit also falls short compared\nto finetuning, and we think tuning bias alone also has limited expressivity."}, {"title": "3.2 Finding 2: More PEFT trainable parameters yield better performance.", "content": "We have investigated the impact of LoRA rank and adapter size in instruction tuning. As indicated\nby Fig. 1a, we observe that a higher LoRA rank/adapter size consistently yields better performance\nwhen coupled with the optimal learning rate. The finding aligns with the general principle in machine\nlearning, where an increase in the number of parameters correlates with enhanced model capacity and\nperformance improvements. Also, due to the nature of task diversity in instruction tuning settings,\nmore tasks coupled with more PEFT trainable parameters do improve model performance. However,\nthe performance gain coupled with higher rank tends to diminish, despite the rank increases by the\nfactor of two. See detailed results in Table 4 and Table 5."}, {"title": "3.3 Finding 3: Higher LoRA rank is more sensitive to the learning rate", "content": "As Fig. 1b indicates, too small learning rate could cause underfitting given the same training epochs\nwhile high learning rate could cause training instability. If we also take rank into consideration, a\nlower rank has a better tolerance of high learning rate. For example, at rank 8, learning rate 1e-3 even\nproduces the best result at the same rank. However, as the rank goes higher, relatively lower learning\nrate stabilize training and improves performance. According to the grid search result, the optimal\nlearning rate is 1e-4 among experiments with different LoRA ranks."}, {"title": "3.4 Finding 4: LoRA and adapter with larger base models perform better", "content": "Both Fig. 2a and Fig. 2b illustrates that the performance of both LoRA and adapter is consistently\nimproved from T5-base to T5-3b. This trend observed indicates that as the capacity of the underlying\nmodel increases, the ability to finetune LoRA and adapter also becomes more effective. Consequently,\nthis highlights the importance of selecting an appropriately scaled model to maximize the benefits of\nLoRA."}, {"title": "3.5 Finding 5: LoRA underperforms in low-data setting", "content": "Despite fully trained LoRA is close to or on par with full finetuning, larger data size always benefits\nLORA across ranks as shown in Fig. 3b. On the other hand, in many industrial adaptations, there\ncould only be a limited number of instruction tasks available. The important question is how many\ndifferent instruction tasks required so that it starts to generalize for each method? In our experiments,\nwe reveal that LoRA is actually a \"slow learner\" in a way that it requires more tasks to ramp up\ncross-task generalization than adapter and finetuning (See Fig. 3a), and it suffers from training\ninstability when the number of training tasks is low (See Fig. 3b). Therefore, in low-data instruction\ntuning settings, especially domain specific, finetuning is still an optimal choice, and adapter could be\na PEFT alternative if the slight overhead latency is acceptable."}, {"title": "3.6 Finding 6: LoRA has worse task-level memorization", "content": "Under the ideal downstream tuning setting, it is advantageous to provide training instruction tasks\nsame as testing instruction tasks, and this resemble traditional training and test dataset split on a\ninstance level. This raises a natural research question: \"How do LoRA and adapter perform for test\ndata which is in-distribution in task level but out of distribution in instance level?\" In such scenarios,\na key capability of language models is their memorization ability: how effectively they can learn and\nretain task-level features seen during training and perform well on tasks of the same types during\ntesting. To assess this, we selected 100 tasks instances from each training task type, provided there\nwere sufficient instances outside of training set. Our experiments indicates that LoRA demonstrates\ncomparatively weaker task-level memorization capabilities than both adapter and finetuning. By\ncontrast, adapter is just slightly weaker than finetuning in this aspect. (Fig. ??) We hypothesize that\nthe key reason for memorization capability lies in the number of extra parameters with nonlinearities,\nas pre-trained knowledge is predominantly located in the feed-forward network(FFN) layers [4] rather\nthan attention layers. Consequently, since there is no additional nonlinear layer for LoRA and only\nquery and key projection layer weight delta are tuned in our LoRA configuration, this results in a\nreduced capability to store task-specific knowledge.\nIt's worth noting that the study by Mireshghallah et al. [15] investigate instance-level memorization to\nreduce extraction attack, and suggests that the adapter model exhibits comparatively less memorization\nat this level. In addition to our finding, we consider adapter could be a valuable alternative if both\ntask-level in-distribution memorization and privacy are important for downstream tasks despite the\nslight inference latency."}, {"title": "3.7 Finding 7: PEFTs underperform in open instruction tuning and multifaceted testing", "content": "As Table 3 suggests, both LoRA and adapter exhibits a significantly weaker performance in reasoning\ntasks while LoRA consistently outperforms adapter when it comes to a wide range of challenging\nopen-instruction tasks."}, {"title": "4 Related Work", "content": "Large Language Models (LLMs) have revolutionized the field of natural language processing with\ntheir vast knowledge base and advanced reasoning capabilities. Yet, their extensive parameter sizes\npose challenges for traditional downstream finetuning.\nAs summarized in Lialin et al. [12], there are five distinct categories of PEFT methods with some\nmethods straddling multiple categories. For the initial stage of our survey, we have selected seven PEFT\nmethod that span these categories. We employ LoRA [10] as a representative of reparameterization-\nbased method, building on its proven effectiveness in works such as QLoRA [5]. BitFit [26] serves as\nour selective method. Prompt-Tuning [10] and Prefix-Tuning [11] are selected from the intersection\nof soft prompts and additive methods. Lastly, we include adapter [7] method which overlaps both\nadditive and adapter-based method.\nThe line of research related to instruction tuning has developed in several key ways. Ziegler et al. [30]\nfocused on finetuning language models through the use of human preferences, aiming to produce model\noutputs that better align with human intent and values. SuperNI [22] introduced an expert-written\ninstruction dataset that spans a variety of task types. Wang et al. [21] leveraged pre-trained models to\nautonomously generate instructional data for subsequent finetuning. This approach led to notable\nenhancements in the model's capability to accurately follow instructions, mitigating the need for\ncostly human-annotated instruction datasets. Taori et al. [19] contributed a synthetic dataset generated\nfrom GPT-4 outputs. This dataset was created using self-instruct methods and was subsequently\ndistilled for the purpose of instruction tuning. Wang et al. [23] studies how the combination of human\nannotated and GPT4 generated instruction dataset modulate the performance of trained model, with a\nblend of both proving optimal. In our experiment, we utilize the same experiment setting but with the\nintegration of PEFT methods.\nAs reported in Biderman et al. [1] LoRA learns less but has better regularization effect, and it is\nsensitive to learning rates. Our finding is complementary to theirs with more focus on instruction\ntuning. In subsection 3.6, LoRA also has a weaker in-task-distribution memorization capability\ncompared to finetuning, and in subsection 3.5 LoRA requires more instruction tasks to generalize.\nDifferent aspects of PEFTs for instruction tuning have also been explored. Zhuo et al. [29] investigates\nPEFTs for instruction tuning on coding, and it reveals that full finetuning still surpass all PEFTs in\nterms of downstream performance. Plus, Biderman et al. [1] also tests coding and math reasoning\ncapability with domain dataset, and it shows LoRA is strong at math but weaker at coding. In\nsubsection, with mixed-task dataset in our work, we find LoRA shows inferior performance in a\nwider range of challenging tasks including coding, complex reasoning and open-ended generation by\ncompared to finetuning."}, {"title": "5 Conclusion", "content": "This study has demonstrated that PEFTs, particularly LoRA and adapter, present viable alternatives\nto full fine-tuning in instruction-tuning scenarios, offering a balance between performance and\ncomputational efficiency. Our comprehensive empirical analysis highlighted that greater LoRA\nranks and adapter sizes enhance performance significantly, though they may introduce some training\ninstability. Additionally, while LoRA outperforms adapter in open instruction tuning settings due\nto its robust generalization across diverse tasks, it requires a substantial number of tasks to achieve\neffective unseen task generalization.\nMoreover, the limitations observed in the long-form generation capabilities of both LoRA and\nadapter underscore the ongoing challenges within PEFT methods, necessitating further innovation\nand exploration in this field. These findings advocate for a nuanced application of PEFT methods,\ntailored to specific model sizes, task types, and data availability, aligning them more closely with\nreal-world applications. As the landscape of language model finetuning evolves, the insights from\nthis study will hopefully guide future research towards optimizing the efficiency and effectiveness of\ninstruction tuning across various domains.\nFuture work should focus on refining these methods to enhance their stability and expand their\napplicability to more complex and varied datasets. By continuing to investigate the trade-offs between\ndifferent PEFT strategies and their impacts on model performance, the field can move towards more\nsophisticated and nuanced PEFT techniques that maximize both performance and efficiency."}, {"title": "A Limitations", "content": "Despite the comprehensiveness of our training and evaluations, we do not exhaustively cover all PEFT\nmethods and more fine-grained hyperparameter grid search. In our work, we only select the most\nrepresentative PEFT methods across the broad PEFT categories, and commonly used hyperparameters.\nGiven the computing constraint, our first set of experiments about hyperparameter search is based\non SuperNI and T5. Therefore, the optimal hyperparamter based on them might not reflect latest\nmodel architecture's performance on latest instruction tuning datasets which has a broader coverage\non different topics. Plus, T\u00dcLU suggests incorporating SuperNI in the data mixture is harmful for\nmodel performance."}, {"title": "B Broader Impact", "content": "We believe that a comprehensive validation of a PEFT method is broadly positive. And it could help\npractitioners to narrow the search space of PEFT methods and hyperparameters to save experiment\ntime and efforts."}, {"title": "C Model Training Details and Compute", "content": "We designed the first set of experiments to find the hyperparameter pattern with the considerations\nof each PEFT's feature. For instance, the prompt length in prompt tuning [10] and the prefix length\nin prefix tuning [11] are both constrained by the input length. BitFit [26] features significantly\nfewer trainable parameters (< 1%) and lacks any adjustable PEFT hyperparameters. Considering\nthese complexities, we opted not to standardize the number of trainable parameters for performance\ncomparison. Instead, our experimental design adheres closely to the original configurations as reported\nin the respective foundational works. We select these representative PEFTs and conducted experiments\nwithin a restricted hyperparameter search space to yield meaningful insights.\nFor SuperNI [22] experiments in Sec. 2.1, we trained our models primarily on High-Flyer cluster,\neach node on which contains 8 Nvidia A100 GPUs. We utilize DistributedDataParallel for most\ntraining jobs when GPU memory permits; otherwise, we employ the DeepSpeed library and ZeRO\noptimizer. Our training hyperparameters that are not part of the grid search are as follows:\n\u2022 Precision: FP32\n\u2022 Epochs: 4\n\u2022 Weight decay: 0\n\u2022 Warmup ratio: 0.03\n\u2022 Max. seq. length: 1,024\n\u2022 Effective batch size: 128\n\u2022 Dropout: 0.1\n\u2022 LORA Layers wrapped: all query and key layers\nFor T\u00dcLU experiments in Sec. 2.2, we trained our models primarily on a local cluster, each experiment\nis conducted on a single NVIDIA A6000 GPU without using extra training framework. Our training\nhyperparameters are as follows:\n\u2022 Precision: BFloat16\n\u2022 Epochs: 3"}, {"title": "D Reproducibility", "content": "We cannot guarantee the exact reproduction of all experiment numbers because the experiments are\nconducted on preemptible clusters, which may restart multiple times. However, the overall findings\nremain valid. Plus, there are some experiment runs with unstable training failed to generate non-empty\ncontent, and it leads to near-zero RougeL scores and reduced average RougeL scores reported in the\nresults. Such results are caused by the nature of training instability of PEFTs, high learning rates or a\nlarge volume of trainable parameters."}]}