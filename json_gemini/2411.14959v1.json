{"title": "Design-o-meter: Towards Evaluating and Refining Graphic Designs", "authors": ["Sahil Goyal", "Abhinav Mahajan", "Swasti Mishra", "Prateksha Udhayanan", "Tripti Shukla", "K J Joseph", "Balaji Vasan Srinivasan"], "abstract": "Graphic designs are an effective medium for visual communication. They range from greeting cards to corporate flyers and beyond. Off-late, machine learning techniques are able to generate such designs, which accelerates the rate of content production. An automated way of evaluating their quality becomes critical. Towards this end, we introduce Design-o-meter, a data-driven methodology to quantify the goodness of graphic designs. Further, our approach can suggest modifications to these designs to improve its visual appeal. To the best of our knowledge, Design-o-meter is the first approach that scores and refines designs in a unified framework despite the inherent subjectivity and ambiguity of the setting. Our exhaustive quantitative and qualitative analysis of our approach against baselines adapted for the task (including recent Multimodal LLM-based approaches) brings out the efficacy of our methodology. We hope our work will usher more interest in this important and pragmatic problem setting. Project Page: sahilg06.github.io/Design-o-meter.", "sections": [{"title": "1. Introduction", "content": "Graphic designs are becoming increasingly ubiquitous: advertisement content, menu cards at restaurants, campaign flyers, and so on. It is a composite of text, images, and shapes that harmoniously intermingle in an aesthetically pleasing way to convey the intended message effectively. A typical workflow of a graphic designer involves ideation, creation, and refinement stages. Each of these stages has its unique characteristics: ideation involves planning the design, creation involves aggregating the design elements and creating the first version, and refinement involves improving the design iteratively. The refinement stage is particularly prone to redundancy, as it involves fine-tuning details, adjusting layouts, and sometimes reworking significant portions of the design to meet the desired standards. Designers often undergo numerous feedback and revision cycles, which can be labor-intensive and time-consuming. Generative AI technologies can work hand-in-hand with designers to supplement them in all phases of their creative workflow. Off-late, such generative models [7, 8, 20, 22, 57] has been used for creating designs and layouts from user intents. Some of these methods can take in design assets from a user and generate designs by composing them [20,21,51], while some others can generate the entire graphic design from a text prompt [23, 27, 60]. These can generate large number of designs with very low latency. Coupled with the increasing online and offline consumption of designs, there is a strong demand for automatic tools to evaluate them.\nThe metrics currently used to evaluate graphic designs, like Fr\u00e9chet Inception Distance (FID) [19], mean IOU and max IOU are often insufficient to capture the nuanced aspects of design quality. Further, these metrics strongly depend on having a high-quality reference design for comparison. Such evaluations might even penalize the creative freedom of the model (a new design might have a completely different location for the constituent elements but still might look good). Reference-free evaluation metrics like alignment and overlap [34] have also been proposed to evaluate the generated layouts. These metrics fail to provide a holistic assessment of overall design quality. These limitations highlight the need for more comprehensive and robust evaluation methods that can assess the effectiveness, usability, and visual appeal of generated designs such that it corroborates with human perception and its use cases.\nTowards this end, we propose Design-o-meter. As shown in Fig. 1, it contains a scorer, which quantitatively evaluates how good a design is, and a refiner, which takes in the score from the scorer and refines the design to improve the score. A design is a composite of components like text, images, shapes, and icons. Each component has its properties like color, location, size, content, opacity, shadow, and so on. A visually appealing design has an optimal value for each property and component. Hence, the search space for good designs is indeed combinatorially large. Further, the design trends also change with time. To effectively model such a complex design space, we propose a data-driven approach for the scorer and a novel meta-heuristic methodology for the refiner. Concretely, the scorer is modeled as a metric learning model that scores good-designs over bad-designs (Sec. 3.1), and the refiner is a genetic algorithm with a novel design-specific crossover operation called SWAN: Design Specific Crossover with Smart Snapping (Sec. 3.2).\nWe conduct thorough experimental analysis to test the mettle of both our scorer and refiner modules. Our experiments reveal that our scorer is able to capture small nuances in design documents that recent Multimodal LLMs fail to capture, while our newly proposed SWAN is able to efficiently navigate the complex design space to refine existing designs, comfortably outperforming recent state-of-the-art approaches. Further, we clearly ablate the different components of the method, and provide sensitivity analysis of the various design choices of our framework.\nOur key contributions are summarized below:\n\u2022 We develop a novel holistic framework, Design-o-meter, that can provide a comprehensive design score and further refine the designs to improve the score.\n\u2022 We propose a metric learning model that learns to disambiguate between good designs and bad designs.\n\u2022 Our novel smart snapping crossover methodology SWAN, refines designs to improve its aesthetic appeal.\n\u2022 Our exhaustive quantitative and qualitative evaluation brings out the efficacy of our proposed Design-o-meter."}, {"title": "2. Related Work", "content": "In the following subsections, we first discuss related methodologies that evaluate designs and then talk about approaches that refine them. Finally, we provide a brief summary of generic algorithms, which is the basis of our proposed refiner module."}, {"title": "2.1. Design Evaluation", "content": "Design evaluations strategies can be grouped into heuristic-based approaches and data-driven approaches:\nDesign Heuristics based Approaches: Design heuristics are cognitive tools that designers and engineers use to measure different design aesthetics. Prior works [4,18,36,37,43,\n50,62] use heuristics as a fixed set of formulas, each formula measuring a particular aspect of the design. Such methods do not consider the overlap between design elements, which is common in graphic designs. Despite the valuable insights provided by the heuristic rules, they don't offer a comprehensive view of the overall design. Also, heuristics are often subjective, and their concrete meaning can deviate, which might lead to inconsistencies.\nData-driven Approaches: Most of the data-driven approaches [14, 49, 56] formulate the problem as a regression or classification task, i.e., mapping images to aesthetic ratings given by human annotators. Dou et al. [14] train a convolution network to predict the aesthetic ratings of webpages. They use a dataset [48] of webpage screenshots with human-annotated aesthetic ratings. Wan et al. [56] obtain global, local, and aesthetic features from the webpage layout to predict the aesthetics. The above-mentioned techniques rely on human-annotated aesthetic ratings, which have several drawbacks. Creating a high-quality human-annotated dataset is time-consuming and expensive. It may also introduce subjectivity and bias.\nAn interesting approach that requires minimal or no human interference is the use of Siamese Networks [5]. Ever since they emerged, Siamese Networks have found application in various diverse fields such as self-supervised representation learning [1, 3, 6, 61], audio-visual synchronization [10, 16, 45], and measuring aesthetics [29, 30, 53, 63], etc. Their popularity is due to several reasons, such as the sharing of parameters among its twin networks, the ability to navigate the search space, and the extraction of distinctive features for downstream tasks. They allow learning in unsupervised settings. They compare the data instances in pairs instead of directly using labels and are robust to data imbalance. Zhao et al. [63] propose a Siamese-based deep learning framework to estimate personality scores of a graphic design. Aesthetics++ [30] train a Siamese-based network to estimate the aesthetics of a design. Kong et al. [29] and Aesthetics++ [30] use comparatively less human annotation to create training pairs but still introduce bias and may not be accurate. Tabata et al. [53] arbitrarily moves the layout elements without any guidance to generate negative examples for training. However, designs generated randomly are not necessarily bad and can sometimes exhibit creative and unique layouts. We follow the successful practice of existing works [30, 63] utilizing Siamese networks. Instead of human annotation and random per-"}, {"title": "2.2. Refining Layouts", "content": "Prior works like [38, 43] minimize energy functions for typical design principles like white space, symmetry, and alignment using simulated annealing. This approach is not scalable and requires significant computational time (up to 40 minutes to generate one optimized layout), making it impractical for real-time or large-scale applications. Pang et al. [44] generate a set of candidate designs by randomly perturbing the existing design and then selecting the best out of the perturbed designs using heuristic rules. However, such an approach is highly inefficient as the design space is vast and complex. Also, relying on a fixed set of heuristic formulas is not a good approach, as discussed in the section Sec. 2.1. Aesthetics++ [30] generates the candidate designs from the input design by traversing a segmentation tree (created using hierarchical segmentation of the graphic design) and leveraging design principles. The candidate designs with the highest aesthetic score is taken as the refinement suggestion. They use a human-annotated dataset for a data-driven approach. However, the dataset used may not represent all user preferences, leading to biased or skewed results. Also, generating multiple candidate designs through tree traversal is computationally intensive and time-consuming.\nRUITE [47] models the refinement as a denoising task and trains a transformer [55] model to denoise the layouts. RUITE only focuses on aligning UI elements; it neglects content and other critical aspects of design, like color schemes and interactive elements. Moreover, reliance on transformer architectures introduces significant latency in the training and inference phases. FlexDM [22] employs multitask learning in a single transformer-based model to solve various design tasks by predicting masked fields in incomplete vector-graphic documents. While this approach allows for flexible design refinement, it requires significant computational resources, making implementation and scaling challenging.\nIn contrast to the above approaches, our method is computationally efficient, providing a more practical solution for design refinement tasks. Our genetic algorithm-based refinement module takes approximately 30 sec to generate a refined design. We also avoid human intervention in our approach to get unbiased results."}, {"title": "2.3. Genetic Algorithms", "content": "Metaheuristic algorithms are applied to address complex real-world problems across domains like engineering, economics, management, etc. A genetic algorithm [28] (GA) is a metaheuristic algorithm inspired by natural selection. It is a population-based search algorithm used for optimization. Classical GA has an objective function (fitness function), chromosome representation of the population, and operators inspired by biology (selection, crossover, and mutation). Population is improved iteratively using the genetic operators and selecting the fittest.\nMultiobjective Genetic Algorithms (MOGAs) differ primarily from standard GAs in how they assign fitness functions, while the remaining steps follow the same procedure as in GAs. They focus mainly on convergence and diversity. NSGA [52] (Non-dominated sorting genetic algorithm) is a multiobjective genetic algorithm that finds multiple pareto-optimal solutions in a single run. It lacks elitism, needs to specify the sharing parameter ($\\sigma_{share}$), and has high computation complexity. To solve these, Deb et al. [12] introduce a fast, elitist, non-dominated sorting genetic algorithm, NSGA-II. It has been applied in various real-world applications, demonstrating its versatility and effectiveness.\nWe remove the dependency on multiple objectives as we represent our model score as our unified objective function. We introduce a novel crossover method for designs SWAN: Design Specific Crossover with Smart Snapping, and refine the mutation functions for our task. With a unified objective function, we bypass the need to filter the pareto-front, which is essential for real user-based scenarios."}, {"title": "3. Methodology", "content": "Our proposed approach Design-o-meter, takes an input a design, and proposes a quantitative score from a scorer module, and in-turn uses this score to refine the design using a refiner module. A design is a composite of a set of elements, which are put together in an aesthetic way. Hence, a design $D$, can be represented as its metadata $D_{meta}$. For instance, a design with a background image and foreground text saying 'Hi!', can be represented as $D_{meta} =[{x_1, y_1, w_1, h_1, image\\_location}, {x_2, y_2, w_2, h_2, \u2018Hi!\u2019}]$, where $x_i, y_i, w_i, h_i$ refers to the location and dimension information, in its most simplistic form. The image and text can have more attributes like opacity, overlay, shadow, font details, emphasis and so on. Using $D_{meta}$, we can render its rendition image $I(D_{meta}) \\in R^{H\u00d7W\u00d73}$ and a color encoded layout $L(D_{meta}) \\in R^{H\u00d7W\u00d73}$, as illustrated in Fig. 3. Our scorer $S(I(D_{meta}), L(D_{meta}))$ learns to measure the goodness and global aesthetics of the input design. The refinement module $R(d_{meta})$, takes as input $d_{meta} \\subset D_{meta}$, which is a set of actionable layout attributes that we are interested in optimizing. It refines these attributes to generate $d'_{meta}$, by maximizing the corresponding design score from the scorer: $d'_{meta} | S(I(d'_{meta}), L(d'_{meta}) > S(I(d_{meta}), L(d_{meta}))\\forall d_{meta}$. $d'_{meta}$ can be rendered to obtain the refined design. We detail about our scorer and refiner in the next sub-sections."}, {"title": "3.1. Design Scorer", "content": "We propose to use a data-driven, self-supervised approach towards learning the scorer module. Different from other works which tries to regress a scalar score from the input design [14, 49, 56], we learn a Siamese model in a contrastive setup, to differentiate between good designs and bad designs.\nModel Architecture: The scorer function $S$ is a composition of a feature extractor network $F$, and a final scoring block $S_{block}$. We use a four layer convolutional network as the feature extractor, and a three layer fully connected network for the scoring block. Given the input design $D_{meta}$, its design score is computed as:\n$S(I(D_{meta}), L(D_{meta})) = S_{block}(F(I(D_{meta}), L(D_{meta}))) \\qquad(1)$\nWe train our entire model from scratch. Experimentally, we find that using pretrained feature extractors like DINO-V2 [42], CLIP [46], BLIP [33] and ViT [55] gives inferior results. This is because these models are trained on natural images, which are distinctly different from the design data, and thus struggle in gauging and extracting important features specific to quantify design aesthetics.\nOur light-weight convolutional network with group-normalization accepts the rendition image $I(D_{meta})$ and the colour-coded layout map $L(D_{meta})$ concatenated across the channel dimension as follows:\n${N, C, H, W} + {N, C, H, W} = {N, 2 * C, H, W}$\nwhere $N, C, H$, and $W$ are batch size, number of channels, height, and width, respectively. The layout map $L(D_{meta})$ effectively warps the multi-layer information of a design into the scorer. This information helps the model to give attention to the constituent components of the design, and its relative positions while proposing a design score. the color coding and Fig. 3 shows layout map and its corresponding designs rendition.\nTraining Details: Similar to the traditional Siamese training setup, the weights of $S$ is shared. We curate good designs $D^{good}_{meta}$ and bad designs $D^{bad}_{meta}$ (explained in the next sub-section) to train the model using the following loss function:\n$L_{scorer} = \\alpha L_{rank} + \\beta L_{sim};\\qquad(2)$\nwhere $L_{rank}$ is hinge loss:\n$L_{rank} = max(0, m - (S(D^{good}_{meta}) - S(D^{bad}_{meta}))) \\qquad(3)$\nand $L_{sim}$ is a similarity loss, formulated as below:\n$L_{sim} = ln(e^{2*P_{sim} (S(D^{good}_{meta}), S(D^{bad}_{meta})} + 1);\\qquad(4)$\n$P_{sim}$ is an embedding similarity computed as the dot product between the tanh-activations of the \"good\" and \"bad\" design pairs as follows:\n$P_{sim} = \\frac{F(D^{good}_{meta}).F(D^{bad}_{meta})}{max(||F(D^{good}_{meta}) || .||F(D^{bad}_{meta})||_2, \\epsilon)}; \\epsilon > 0 \\qquad(5)$\nDataset Creation: Given a set of designs from any design dataset [59], we first filter them on two criteria: 1) total number of elements should be at most 10, 2) text should not overlap with images; to create a list of good designs $D^{good}_{meta}$. Next, we surgically modify these designs to make them bad by altering the location and scale of its constituent elements. We employ 22 such types of transformations to create $D^{bad}_{meta}$. The location based perturbations are:\nNoise addition: In order to simulate imperfect designs, noise from standard normal distribution with mean 0 is added to the center coordinates of the design elements. By varying the standard deviation across 0.05, 0.1, 0.2, and .5, we control the degree of perturbation, and thus the degree of badness in the graphic design.\nMoving specific type and group of elements differently: Not all elements of a bad-design will have equal amount of layout shift. In-order to accommodate this aspect, we arbitrarily move specific elements in the graphic design such as the largest element, smallest element, two largest elements, and two smallest elements.\nClutter: Another aspect of designs that makes them naturally bad is clutter, and hence, we clutter all the elements of a graphic design at different positions such as center, top-left, top-right, bottom-left, and bottom-right.\nScale-based transformations are similar to the position-based transformations. The height and width of the design elements are modified instead of the coordinates of the center. Further, the position-based and scale-based transformations are combined to cover more cases. A critical aspect of"}, {"title": "3.2. Design Refiner", "content": "As a design is a composite of multiple components, the search space that constitutes all the designs is vast. Traversing this space of designs in a meaningful way would enable us to find better aesthetically pleasing designs, which improves over the initial design. Here, we propose an efficient approach based on a meta-heuristic algorithm, equipped with our novel SWAN: Design Specific Crossover with Smart Snapping. Our scorer (Sec. 3.1), guides SWAN, to refine its design aesthetics by acting as the objective function being optimized. Our approach expedites convergence and makes the complex algorithm more deterministic.\nRevisiting Genetic Algorithms: Genetic algorithms are a type of meta-heuristic algorithms used to solve non-linear optimization problems. In our case, we want to optimise the vector $d_{meta}$ on the non-linear objective $S$, such that the score $S(d_{meta})$ is maximised. All genetic algorithms follow a three-step framework: 1) Initialisation: where random vectors in the search space ($d_{meta}$) are sampled, 2) Exploitation: where the \"fitness\" of the samples is carried out using the scorer and only the \u201cfittest\u201d samples are retained, and 3) Exploration: where new samples are created with the knowledge of the fittest samples to explore a diverse search space, through crossover and mutation steps.\nThese steps are carried on until a specified number of iterations or stopping criteria is met. NSGA-II [13] introduces a faster filtering operation for selecting the \"fittest\" samples. Deb et al. [11] was an augmentation of the previous work and it tailor makes an algorithm specifically for multi-objective optimization. However, many works [25, 32] argue that this is at the cost of appropriate search space exploration, which is detrimental to overall performance. Since we are explicitly working on single-objective optimization, we use its predecessor. We adapt NSGA-II to our setup by replacing its crossover mechanism with a novel design-specific crossover approach, explained next.\nSWAN: Design Specific Crossover with Smart Snapping\nAn overview of the approach is presented in Algorithm 1 and Fig. 2. In line 1, two parents (vectors in the search space, $d^{1}_{meta}, d^{2}_{meta} \\in d_{meta}$) are randomly chosen from the set of \"fit\" samples (samples remaining after the exploitation step). The objective of crossover function is to fuse the knowledge of both the parents to create a \u201csmarter\u201d offspring, which becomes a new sample in the population. In regular crossover, there is blind copy-pasting of parent data, and we find that to be unfit for our task at hand, which motivates us to propose SWAN: Design Specific Crossover with Smart Snapping. The first step is to initialize a random mask, $V$, which assigns what parent is responsible for the percolation of which element in the new Child vector ($d^{child}$). Next, we copy element attributes from the first parent $L_1$ using the mask $V$ and build our current canvas $C$ in Line 4 and the third column in Fig. 2. For the remaining image elements from the second parent, we build grid lines from the edges of the design elements inside the current canvas and find all the boxes thus made after the intersection of the lines. We then find the box with the most similar size and proximity to the element from Parent2, and then 'Snap' it inside it and update the Canvas as shown in Fig. 2. For text elements, we find the Euler distances from the center of the text and all the other centers of elements from the current canvas. We find the element with least distance and find out what the cost (distance moved) is of either x-aligning or y-aligning with the element. We choose the minimum of it and update the current canvas to continue.\nThe mutation step in NSGA-II can also be improved. Standard mutation operation, which adds Gaussian noise to elements, might result in partial overflow in the design elements of their canvas size. We identify such cases and add noise only if it is within the canvas area. This allows our method to skip over degenerate cases."}, {"title": "4. Experiments and Results", "content": "Datasets: We utilize the widely-used Crello [59] dataset for our evaluation. It contains 23182 design templates. The data is divided into 18768 / 2315 / 2278 examples for train, validation, and test splits. We use multiple guided perturbations techniques to make the design pairs as discussed in Sec. 3.1. If the evaluation dataset includes the same or similar transformations used in training, we call it a biased setting; otherwise, it is called unbiased. We employ two unbiased setups: color and cross-match. In the color dataset, the transformation used makes the design's color scheme bad by recoloring certain elements of it. We traverse the design, find overlapping elements (say, SVG preceding a Text), we extract the colors: $C_{SVG}, C_{text}$ and randomly choose one of the elements(say, the SVG) and recolor it such that the $CIELAB\\_Distance(C_{text}, C_{SVG}) \\in (2, 3)$. This ensures that there are distinct colours but it makes the design unaesthetic. In the cross-match dataset, we randomly pair a good version of a design x with the bad version of a design y, where x\u2260y.\nBaselines: We compare our scorer with recent Multimodal LLM-based design scorers. Specifically, we compare against GPT-40 [40] and LLaVA-NeXT [35]. We compare the refiner against leading design refinement approaches. Specifically, we compare against SmartText++ [31], FlexDM [22], and COLE [27] for text box placement (refine-text setting); and against Canvas VAE [59], FLexDM [22], DocLap [64], GPT-4 [41], and GPT-4V [39] for full design refinement (refine-all setting).\nImplementation Details: We employ a learning rate of 1e-4, the Adam optimizer [15] with gradient coefficients of [0.5, 0.99], L2 regularization with weight decay as 0.005 and every 5 epochs, we schedule the learning rate to divide by half. We set a hard margin, m = 0.2, and loss parameters $\\alpha$ = 0.8 and $\\beta$ = 0.2 (see Eq. (2)). We add sensitivity analysis on these hyper-parameters in Sec. 5.4. All the convolution layers in the scorer model are as follows (representing the number of filters, kernel size, and stride, respectively): (64,3,1). We use group-normalization [58] (ngroup=2) in our scoring module. For SWAN, we find that population\\_size = 100, n\\_trials = 1500, p = 0.3 gives good performance.\nEvaluation Metrics. We introduce Rank Accuracy (RAcc) to evaluate the design scoring methods. The higher the RAcc, the better the scorer. We define the Rank Accuracy as follows:\n$RAcc = \\frac{1}{N} \\sum_{i=1}^{N} I\\{S(g_i) > S(b_i)\\} ;\\qquad(6)$\nIf the score given by a scorer $S$ to a good design ($g_i$) is more than that of a bad design ($b_i$) in a ($g_i,b_i$) pair, we consider it as correctly classified.\nFollowing FlexDM [22], we use mean Intersection over Union (mIOU) and mean boundary displacement error [31]"}, {"title": "4.1. Evaluating the Scorer", "content": "We use three settings (biased, unbiased color, and unbiased cross-match) for evaluation. Each data instance in these datasets is a pair of designs, where the task of the model is to provide a better score to the good design. \nWe show two designs, and you should give each design a design score between 1-100, which follows design principles and reason, and justify your score briefly and succinctly and then output which design has the higher score.\nDespite having only ~410k parameters compared to the billions in multi-modal LLMs, our model achieves the highest RAcc across all three datasets. This shows that multi-modal LLM-based evaluation of graphic design [9, 26] is not good at gauging micro-aesthetic differences between designs [17]."}, {"title": "4.2. Evaluating the Refiner", "content": "Following earlier works [22, 27], we consider two settings: refining a single text box and refining whole design. \nWe find that it significantly helps to improve performance."}, {"title": "5. Discussion and Analysis", "content": "5.1 Visualizing the scorer: We conduct a simple occlusion based sensitivity analysis to measure the contribution of each region of a design towards the design score. We slide a 60 px \u00d7 60 px occluding window across the design, replacing the pixels within the window with the design's mean pixel value at each location. This modified design is then evaluated to generate a new score. The difference between the new score and the original score is used as the pixel value for the window's center in the sensitivity map. This process creates a pixel-wise sensitivity map highlighting areas of the design that most positively or negatively impact the design score.\n5.2 On the Efficacy of Layout Encoding: Including layout color encoding in the input to the scorer model makes it easy for the model to capture the basic design principles such as overlap and alignment. We experiment with three configurations to verify this hypothesis: only rendering, only layout, rendering + layout; and train the model on Crello dataset. We notice that using rendering + layout as input performs the best with RAcc of 94.97 while the other two configurations give 86.65 and 86.99 RAcc.\n5.3 Effect of Normalization Layers: Every graphic design is unique and complex. Assuming that designs belong to a common distribution and using normalization methods like BatchNorm [24] degrades the performance of the model. serve that GroupNorm performs best along with the rendering+layout as input.\n5.4 Sensitivity Analysis: We experiment with different types of margins in Eq. (3).\nH-Margin for the results. Next, we analyze the contribution of $\\alpha$ and $\\beta$ parameters in Eq. (2). Decreasing $\\alpha$ and increasing $\\beta$, seems to have a positive effect, and hence we use $\\alpha$ = 0.8 and $\\beta$ = 0.2 for all experiments."}, {"title": "6. Conclusion", "content": "In this paper, we propose Design-o-meter, a novel framework which can evaluate and refine graphic designs. It takes an existing user design as input, and provide a unified design score. Further, it refines the input design to improve the design score. Our exhaustive experimental analysis brings out the efficacy of Design-o-meter. It would be interesting to see how our scorer can aid other layout and design generation frameworks as an off-the-shelf discriminator. We leave this for future exploration. We hope Design-o-meter will kindle interest in this practical and relevant setting of evaluating and refining graphic designs."}, {"title": "A. Additional Ablation Experiments", "content": "A.1. Alternate Margins for Eqn. 3\nWe experiment with Hard margin (H-Margin), transformation-based margin (TB-Margin), and adaptive margin (Ada-Margin). For H-Margin, we try low (0.2), medium (0.5), and high (1.0) values. A low value of 0.2 achieves the highest RAcc of 94.97. For TB-Margin, we assign different margins to different transformations, a low value (0.2) for transformations adding noise with a low std deviation, and similarly for medium (0.4) and high (0.6) noises. We define Ada-Margin as the maximum Euclidean distance between feature embeddings of designs in a pair across a batch, represented as:\n$Ada-Margin = max_{(g_i,b_i) \\in batch}(max(||F(g_i) \u2013 F(b_i)||_2), 0.1) (1)$ where $\\lambda$ is a scaling factor, we choose $\\lambda$ = 0.05.\nA.2. Alternate Similarity Loss Formulation\nWe experiment with different similarity losses (exponential, binomial deviance, and square) for Eqn. 4.\n$L^{exp}_{sim} = e^{P_{sim}(S(D^{good}_{meta}), S(D^{bad}_{meta}))} (8)$\n$L^{dev}_{sim} = ln(e^{2*P_{sim} (S(D^{good}_{meta}), S(D^{bad}_{meta})} + 1) (9)$\n$L^{Sq}_{sim} = (P_{sim}(S(D^{good}_{meta}), S(D^{bad}_{meta})) + 1)^2 (10)$\n$P_{sim}$ is an embedding similarity computed as the dot product between the tanh-activations of the \"good\" and \"bad\" design pairs as follows:\n$P_{sim} = \\frac{F(D^{good}_{meta}).F(D^{bad}_{meta})}{max(||F(D^{good}_{meta}) || .||F(D^{bad}_{meta})||_2, \\epsilon)}; \\epsilon > 0 (11)$\nWe get similar RAcc using all the similarity losses with minor differences. We choose $L^{dev}_{sim}$ because of the validation loss decreases most in this case.\nA.3. Use of Classifier Guidance in Scorer\nWe try to guide the scorer model with a binary classification head on top of the siamese model. We additionally introduce a binary cross-entropy loss to differentiate good and bad designs. This setting increases the model size and training time, but doesn't significantly help the scorer model in ranking the designs better."}, {"title": "B. Failure Cases", "content": "We showcase failure cases of Design-o-meter in Fig. 6, ranging from minor to significant failures. Despite being an excellent scorer and refiner, at-times the signals from the input are weak to correctly guide the layout and scale transformations."}, {"title": "C. Perturbations used in Dataset Creation", "content": "In Fig. 9, we show a visualization of the perturbations that we do to the input design to create the dataset to train the scorer module, as explained in Sec. 3.1."}, {"title": "D. Additional Results", "content": "We include more qualitative results on the scores and refined outputs in Figs. 7 and 8."}]}