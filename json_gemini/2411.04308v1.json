{"title": "Improving Bilingual Capabilities of Language Models to Support Diverse Linguistic Practices in Education", "authors": ["ANAND SYAMKUMAR", "NORA TSENG", "KAYCIE BARRON", "SHANGLIN YANG", "SHAMYA KARUMBAIAH", "RHEEYA UPPAL", "JUNJIE HU"], "abstract": "Large language models (LLMs) offer promise in generating educational content, providing instructor feedback, and reducing\nteacher workload on assessments. While prior studies have focused on studying LLM-powered learning analytics, limited\nresearch has examined how effective LLMs are in a bilingual context. In this paper, we study the effectiveness of multilingual\nlarge language models (MLLMs) across monolingual (English-only, Spanish-only) and bilingual (Spanglish) student writing.\nWe present a learning analytics use case that details LLM performance in assessing acceptable and unacceptable explanations\nof Science and Social Science concepts. Our findings reveal a significant bias in the grading performance of pre-trained models\nfor bilingual writing compared to English-only and Spanish-only writing. Following this, we fine-tune open-source MLLMs\nincluding Llama 3.1 and Mistral NeMo using synthetic datasets generated in English, Spanish, and Spanglish. Our experiments\nindicate that the models perform significantly better for all three languages after fine-tuning with bilingual data. This study\nhighlights the potential of enhancing MLLM effectiveness to support authentic language practices amongst bilingual learners.\nIt also aims to illustrate the value of incorporating non-English languages into the design and implementation of language\nmodels in education.", "sections": [{"title": "1 Introduction", "content": "Recent research in learning analytics highlights the importance of addressing diversity, inclusivity, and equity to\nbetter support all learners [15, 24]. The advent of large language models (i.e., algorithms trained on large amounts\nof text to understand and process human language) in learning analytics research and practice raises similar\nquestions about equity [4]. How well can language models trained predominantly on mainstream English data\nserve students from diverse linguistic backgrounds?\nMultilingual large language models (MLLMs) that can process and produce text in multiple languages offer\npromising new directions for supporting authentic multilingual communication. However, despite their impressive\nperformance in individual languages, MLLMs are limited in their ability to switch fluidly across languages (known\nas translanguaging or code-switching [6, 29]) - leading to an inaccurate representation of language use by bilingual\nlearners. Homogeneity in MLLM training [28] due to scarcities in code-switched data further contributes to the\nissue, as well as the lack of safety benchmarks and comprehensive evaluation [23]. As learning analytics powered\nby language models make their way into classrooms, studying the affordances and constraints of using MLLMs\nbecomes increasingly important.Building upon existing research on translanguaging, bi/multilingualism, and\nnatural language processing, the current study necessitates the integration of languages produced by bilingual\nlearners, such as Spanglish (translanguaging in Spanish and English) into the training and evaluation of MLLMs.\nWe investigate the code-switching and multilingual capabilities of MLLMs in an illustrative learning analytics\nuse case of assessing student writing. We ask the following research question: How does MLLM performance vary\nacross assessing monolingual (English, Spanish) and bilingual (Spanglish) student writing?\nWe first evaluate the performance of two MLLMs in assessing Science and Social Science ideas as expressed in\nEnglish, Spanish, and Spanglish. To overcome the data scarcity issue, we create and use a synthetically-generated\ndataset evaluated by humans on both the language and content accuracy. Then, we attempt to improve MLLM\nperformance using techniques such as prompting and fine-tuning. Our hypotheses include:\n\u2022 H1: MLLMs are significantly more accurate when ideas are presented in English or Spanish, but are\nsignificantly less accurate when ideas are in Spanglish.\n\u2022 H2: Fine-tuning with the target language will significantly improve MLLM performance. That is, fine-tuning\na model on Spanglish will maximize its performance in identifying ideas expressed in Spanglish.\nDuring translanguaging, speakers draw from their entire linguistic background and cultural identity to navigate\nand defy the socio-political boundaries of \"proper\" language use, in turn promoting authentic social interaction\nand communication [8]. Additionally, Li Wei [26] defines the term translanguaging space as a place where cultural\nand linguistic boundaries eclipse in bilinguals' daily lives. Within these spaces, both criticality and creativity\nwork simultaneously to inform bilingual learners' perceptions on socio-cultural and linguistic phenomena and\ndetermine what constitutes the norms of language use.\nMuch of the discourse surrounding these practices has shifted towards how to leverage students' linguistic\nresources in learning environments. With increasing globalization, instructional resources such as the CUNY-\nNYSIEB Translanguaging Guides now include pedagogical strategies that use bilingualism as a resource to leverage\nlearning. These guides emphasize collaborative work and provide linguistic resources for EBLs of varying age and\ngrade levels, and have been widely used in the past decade by educational communities working towards putting\ntheory into practice [9]. These resources have useful applications when promoting students' existing language\nbackground, cultural identity, and multi-modal practices (i.e., gesture) as meaning- and sense-making processes.\nAs the learning analytics field consistently showcases diverse learner perspectives, we view translanguaging as\nan increasingly valuable framework that empowers bilingual learners to convey ideas and understanding using\ntheir complete linguistic repertoire."}, {"title": "1.1 LLM Support in Education", "content": "Large Language Models (LLMs), with their ability to generate language from large-scale datasets, have gained\nprominence in the learning analytics field. They are applied in tasks such as essay scoring and feedback generation,\nhighlighting LLMs' growing role in saving instructor time and providing valuable feedback for learners [18].\nFor instance, Automated Essay Scoring (AES) systems reduce examiner workloads in large-scale assessments\nlike TOEFL and GMAT. LLMs like BERT outperformed previous AES methods, establishing new benchmarks\nin the field [10]. Beyond scoring, LLMs are capable of providing personalized feedback for intelligent tutoring\nsystems [22], aiding researchers in developing high-quality educational tools. They also show aptitude in\nclassroom-specific subjects such as mathematics, are capable of generating multiple-choice questions [20],\nplotting figures [7], and provide teacher training via simulations [17]. With these advancements, it is evident that\nLLMs have immense potential in classrooms, making it increasingly important to extend the same support to\nbi/multilingual classrooms."}, {"title": "1.2 Multilingual LLMs", "content": "In general, since LLMs are trained on English-centric data, LLMs perform better in English than in non-English\nlanguages [30]. However, newer MLLMs such as GPT-4 [1] demonstrate increasing multilingual capabilities and\nare outperforming their predecessors on multilingual benchmarks [2]. Despite recent advancements in MLLM\nresearch, further inspection reveals a challenge: many MLLMs still struggle with code-switching due to the lack\nof diverse linguistic resources in training data [29]. Additionally, there is a lack of comprehensive benchmarks to\nsupport and analyze MLLMs recent developments [23]. As a result, many languages with limited digital resources\nare excluded [14]. Code-switching resources in particular are scarce due to the lack of large-scale annotations\nthat require multilingual human-raters [27]. To address this data shortage, researchers have used manually\ncreated datasets using methods such as random replacements and noun-phrase translations [16] or by using\nMLLMs to generate synthetic code-switched texts [12, 28]. In synthetic generation, recent studies show that\nnewer MLLMs such as GPT-4 are more robust for code-switching and cross-lingual understanding [13], posing a\npotential solution for contexts requiring code-switching capabilities. As bilingual learners engage in classroom\ndiscussions in two or more languages, it is crucial to develop fair and reliable datasets for MLLMs to capture\ndynamic bi/multilingual processes."}, {"title": "2 Methods", "content": "To test our hypotheses of MLLM performance across monolingual (Spanish, English) and bilingual (Spanglish)\nsettings, we developed parallel datasets across English, Spanish and Spanglish. We prepared our primary dataset\nin English and then translated it into Spanish and Spanglish. Pre-trained LLMs are models already trained on\nlarge amounts of data and can be fine-tuned for specific tasks. To test whether pre-trained models exhibited worse\nperformance in bilingual languages, as H1 suggests, we evaluated the grading performance of two MLLMs across\nall three languages. For H2, which hypothesizes target-language fine-tuning as the most effective way to improve\nperformance for a language, we performed two experiments: one aimed at improving Spanglish performance\nthrough fine-tuning, and another to explore cross-lingual transfer. We first outline our synthetic data preparation\nprocess, then describe our experimental designs."}, {"title": "2.1 Synthetic Data Preparation", "content": "We used Claude 3.5 Sonnet [5], a large proprietary (closed-source) language model to generate our synthetic\ndatasets, which we used for training and evaluation.\nFirst, we prompted the model to generate question-answer pairs in English along with a binary grade of\nAcceptable or Unacceptable. We focused on Science and Social Science topics for grades 6 through 10. Without"}, {"title": "2.2 Experimental Design", "content": "We performed fine-tuning and evaluation using open-source language models to avoid sending the data to a third-\nparty server a critical factor given the importance of privacy in educational data. Smaller open-source models\nlike Llama 3.1 are free to use and fine-tune, requiring significantly lower computational resources. Specifically,\nwe used the 'instruct' versions of Llama 3.1 (8 billion parameters) [21] and Mistral NeMo (12 billion parameters)\n[3]. A consistent grading prompt was used across both models, all languages and all experimental setups. The\nprompt detailed the task: grade single-sentence answers as acceptable or unacceptable."}, {"title": "2.2.1 Zero-shot Baseline for English, Spanish and Spanglish", "content": "To test H1, we compared the grading performance\nof Llama 3.1 and NeMo across English, Spanish and Spanglish in a zero-shot setting. Zero-shot learning refers to\nthe model's ability to perform a task without having seen any training examples for it. This approach helped\nidentify potential language biases in the two models and served as a baseline for our next experiment."}, {"title": "2.2.2 Improving Spanglish Performance", "content": "To evaluate whether fine-tuning with the target language improves\nSpanglish performance, we compared the zero-shot, few-shot prompting, and fine-tuned performance of Llama\n3.1 and NeMo for Spanglish. Few-shot learning is when we provide the model with a few examples of our task as\npart of the prompt. For few-shot, we provided three Spanglish question-answer-grade examples and evaluated\nperformance using the pre-trained models. Fine-tuning involves adapting a pre-trained model for a particular\ntask by training it with a task-specific dataset. For fine-tuning, we used Unsloth's open-source library [25] to"}, {"title": "2.2.3 Cross-lingual Transfer", "content": "In H2, we hypothesized that fine-tuning with the target language is the best\nway to improve performance for that language. Specifically, we expected Spanglish fine-tuning to be the most\neffective way to improve Spanglish performance compared to English or Spanish fine-tuning. To investigate how\nfine-tuning in each language influences performance in other languages (cross-lingual transfer), we fine-tuned\nLlama 3.1 on each language and evaluated its performance across all three languages (see Figure 1). To improve\nand balance the performance across the three languages, we also fine-tuned Llama 3.1 on three mixed datasets:\nEnglish (100) + Spanglish (50), Spanish (100) + Spanglish (50), English (50) + Spanish (50) + Spanglish (50). The\nmixed language datasets were prepared by sampling and merging data from the three monolingual training\ndatasets, maintaining the same dataset size. All the six fine-tuned models were tested on English, Spanish and\nSpanglish. Additionally, we computed and compared the average performance of each model across languages."}, {"title": "3 Results", "content": "Our analysis investigated potential bias in MLLMs by comparing the assessment of bilingual and monolingual\nwriting. We also focused on methods to enhance bilingual performance and examine cross-lingual transfer after\nf"}]}