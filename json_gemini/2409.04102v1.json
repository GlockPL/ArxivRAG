{"title": "Intelligent Tutoring Systems by Bayesian Nets with Noisy Gates", "authors": ["Alessandro Antonucci", "Francesca Mangili", "Claudio Bonesana", "Giorgia Adorni"], "abstract": "Directed graphical models such as Bayesian nets are often used to implement intelligent tutoring systems able to interact in real-time with learners in a purely automatic way. When coping with such models, keeping a bound on the number of parameters might be important for multiple reasons. First, as these models are typically based on expert knowledge, a huge number of parameters to elicit might discourage practitioners from adopting them. Moreover, the number of model parameters affects the complexity of the inferences, while a fast computation of the queries is needed for real-time feedback. We advocate logical gates with uncertainty for a compact parametrization of the conditional probability tables in the underlying Bayesian net used by tutoring systems. We discuss the semantics of the model parameters to elicit and the assumptions required to apply such approach in this domain. We also derive a dedicated inference scheme to speed up computations.", "sections": [{"title": "Introduction", "content": "Developing intelligent tutoring systems (ITSs) able to support learners without the supervision of a human teacher and provide feedback and customized instructions has been a classical field of application for AI since its early stages. Unsurprisingly, the change of paradigm from rule-based AI systems performing logical reasoning to (probabilistic) uncertain reasoning systems also impacted the ITS design (Mayo 2001). In more recent times the growing need of e-learning technologies brought a renovated interest on ITSs (Guo et al. 2021). While ITS technologies are benefiting from the more recent advances of deep learning to obtain a more flexible human-machine interaction, using probabilistic approaches in the assessment and tutoring part remains essential.\nA typical modeling approach for ITSs includes an assessment step in which the competence profile of the learner is described by a set of latent variables (to be called skills in the following) whose actual value affects some manifest variables such as the answers given to different questions or the reactions to feedback. This is the case of"}, {"title": "Bayesian Nets", "content": "We use uppercase letters to denote variables, lowercase for states and calligraphic for the whole set of states. Thus, for instance, variable X takes its generic value x from set X. In the Boolean case X = {0,1} and notation \u00acx refers to the the negation of state x. A probability mass function (PMF) P(X) over X is a non-negative map X \u2192 R such that \u03a3x\u2208X P(x) = 1. Given a joint PMF P(X1, X2), we can compute conditional probabilities by Bayes rule, e.g., P(x1|x2) = P(x1,x2)/P(x2), where P(x2) is obtained by marginalization, i.e., P(x2) = \u03a3x1\u2208X1 P(x1, x2). Note that the conditional probability is defined only if P(x2) > 0.\nA (conditional) PMFs is denoted as P(X1|x2). Similarly, a conditional probability table (CPT) is a collection of conditional PMFs, e.g., P(X1|X2) = {P(X1|x2)}x2\u2208X2.\nLet G denote a directed acyclic graph whose nodes are in one-to-one correspondence with the variables in X := (X1,..., XN). A Bayesian net (BN) consists in a collection of CPTs {P(X|Pa(X))}X\u2208X where Pa(X) are the parents, i.e., the immediate predecessors of X according to G. A BN defines a joint PMF P(X) such that P(x) = \u03a0i=1N P(xi|pa(Xi)), for each x \u2208 X, where the values of Xi and pa(Xi) are those consistent with x for each i = 1,..., N (Koller and Friedman 2009). Such compact factorization is based on the independence relations induced from G by the Markov condition, i.e., each node is assumed to be independent of its non-descendants non-parents given its parents. BN inference consists in the computation of queries based on the joint PMF. In particular we are interested in updating tasks consisting in the computation of the (posterior) PMF for a (single) variable Xq \u2208 X given some evidential information about other variables XE \u2286 X, that are assumed to be in a state XE. This corresponds to:\n$\\displaystyle P(X_q|X_E) = \\frac{\\Sigma_{x'}\\Pi_{i=1:N} P(x_i|pa(X_i))}{\\Sigma_{x',x_q}\\Pi_{i=1:N} P(x_i|pa(X_i))},$ (1)\nwhere X' := X \\ {XE, Xq} and, here and in the following, the domains of a sum is left implicit for the sake of readability. The task in Eq. (1) is NP-hard in the general case (Koller and Friedman 2009). Exact inference schemes such as message propagation allow to compute exact inference for simple (e.g., singly-connected) topologies of G in polynomial time. In the general case, exact inference is exponential with respect to the treewidth of G, while models with high treewidth should be updated by approximate schemes. The situation is therefore especially critical for models with high maximum indegree (i.e., number of parents) as this involves both large CPTs and treewidth."}, {"title": "Noisy Gates", "content": "We focus on Boolean models. The extension to non-Boolean (but ordinal) variables is discussed in the conclusions.\nDisjunctive Gates. We consider the quantification of the CPT for a variable Y in a BN. Say that (X1,..., Xn) are the parents of Y. The elicitation of P(Y|X1,..., Xn) requires a number of parameters exponential in the number of parents n (namely 2n as we cope with Boolean variables). A noisy-OR gate (Pearl 1988) requires instead the elicitation of n parameters only, say {\u03bbi}i=1n, with, for each i = 1, . . ., n, \u03bbi \u2208 [0, 1]. The corresponding CPT is specified as follows:\n$\\displaystyle P(Y = 0|x_1,...,x_n) = \\Pi_{i=1}^n (I_{x_i=0} + \\lambda_i I_{x_i=1}),$ (2)\nwhere IA is an indicator function returning one if A is true and zero otherwise.\nTo better understand the semantic of such models, let us introduce n auxiliary Boolean variables (X1, . . ., Xn). Consider a BN fragment as in Figure 2 where Y is the common child of these variables being in fact their disjunction, i.e., Y := \u2228i=1n X. Such deterministic relation induces a degenerate (i.e., involving only probabilities equal to zero and one) quantification of the corresponding CPT. We further set the input variable Xi as the unique parent of Xi for each i = 1,...,n. The quantification of the (two-by-two) CPT P(X|Xi) is achieved by setting P(X = 0|Xi = 0) = 1 and P(X = 0|Xi = 1) = \u03bbi. Accordingly, we can regard the auxiliary variable Xi as a inhibition of Xi: if the input Xi is false then Xi is certainly false, while if Xi is true then X can be false with probability \u03bbi. It is straightforward to prove that such BN fragment returns the probabilities in Eq. (2) when the auxiliary variables are summed out. In fact P(Y = 0|x1,..., xn) can be expressed as:\n$\\displaystyle \\Sigma_{(x'_1,...,x'_n)} P(Y = 0|x'_1,...,x'_n) \\Pi_{i=1}^n P(x'_i|x_i) = \\Pi_{i=1}^n P(X = 0|x_i),$ (3)\nand hence as \u03a0i=1n P(X = 0|xi) because the CPT of Y implements a disjunction and the only configuration giving non-zero probability to Y = 0 is the one where all the parents are in their false state. Eq. (2) trivially follows from Eq. (3) by the definition of the CPT P(X|Xi)."}, {"title": "Conjunctive Gates", "content": "The noisy-OR model, as depicted in Figure 2, can be generalized. First, we might replace the disjunction defining the CPT P(Y|X1, ..., Xn) with a different deterministic function f. Regarding the CPT P(X|Xi) we keep the assumption that one of its two PMFs is deterministic, while the other depends on \u03bbi. In particular we want that for \u03bbi = 0 the CPT implements the deterministic constraint X = Xi, while for \u03bbi = 1 the two PMFs are equal and the CPT expresses the irrelevance of Xi to X. This is achieved by setting P(X = x|Xi = x) = 1 and P(X = x|Xi = \u00acx) = \u03bbi. We call x distinguished state. In the above general setup, the noisy-OR gate corresponds to set f as a disjunction and x = 0. Setting f as a conjunction and x = 1 defines instead a noisy-AND. In the latter case \u03bbi can be intended as an activation probability (i.e., if Xi is true also X is certainly true, while if Xi is false it might become true with that probability). The analogous of Eq. (2) for the noisy-AND gate is:\n$\\displaystyle P(Y = 1|x_1,...,x_n) = \\Pi_{i=1}^n [I_{x_i=1} + \\lambda_i I_{x_i=0}].$ (4)\nThe derivation is analogous to that in Eq. (3), with non-zero contributions involving only states such that X = 1. A generalization of Eq. (2) that holds both for the noisy-OR and the noisy-AND gates based on the notion of distinguished state is therefore:\n$\\displaystyle P(Y = \\hat x|x_1,...,x_n) = \\Pi_{i=1}^n [I_{x_i=\\hat x} + \\lambda_i I_{x_i=\\lnot\\hat x}].$ (5)\nA further extension can be achieved by making the gate leaky, i.e., adding a Boolean variable X0 that is a parent of Y and setting P(X = \u00acx) = \u03bb0. A leaky model is equivalent to a standard noisy gate with an additional parent X0 and corresponding parameter \u03bb0, with X0 instantiated in its non-distinguished state. This allows to obtain a leaky version of Eq. (5) by simply adding on the right-hand side an additional term involving \u03bb0 and such that X0 = x."}, {"title": "Exploiting noisy gates in BN-based ITS", "content": "Noisy gates allow for a compact specification of the CPTs in a BN. For a compact model specification we adopt a simple structure in the graph underlying the BN: the skills are parentless nodes X1,..., Xn and Xi = 1 means that the learner possesses skill i, the questions are childless nodes Y1,..., Ym whose parents are the skills relevant to answer that particular question (see, e.g., the BN in Figure 1) and Yj = 1 describes a correct answer. At the elicitation level, the domain expert should asses the prior PMFs P(Xi) for the different skills and, if we use noisy gates for the questions, n \u00d7 m parameters \u03bbi,j, one for each parent/skill i of each question j. In the previous section, the BN in Fig. 2 has been shown to represent both a noisy-OR and a noisy-AND provided that Yj is either the conjunction or the disjunction of all auxiliary nodes X1,j, and that, given x, then P(Xi,j = x|Xi = x) = 1 and P(Xi,j = x|Xi = \u00acx) = \u03bbi,j. A disjunction with x = 0 implements a noisy OR, a conjunction with x = 1 a noisy AND."}, {"title": "Interchangeable Skills (Or)", "content": "For disjunctive gates, Yj changes to the non-distinguished state corresponding to a correct answer if at least one of the auxiliary variables is in the non-distinguished state Xi,j = 1. This model can be used to describe a situation where a single skill is sufficient to answer a specific question (e.g., in Fig. 1, multiplications are solved either by fingers or by long multiplication). If the learner has the skill i, the probability of answering j correctly by means of that skill depends on the question and is equal to 1 \u2212 \u03bbi,j. E.g., in Fig. 1, 1 \u2212 \u03bb1,1 describes the probability that counting with fingers allows to solve the first multiplication. The fact that the second multiplication is too hard to be solved by fingers is graphically represented by the absence of a direct arc between X1 and Y2. In the noisy-OR model, this translates in the constraint 1 \u2212 \u03bb1,2 = 0. In general, setting 1 \u2212 \u03bbi,j = 0, implies that having skill Xi has no effects on the capability of the learner to solve question Yj and that the inability to answer question Yj does not change the prior opinion about the learner possessing skill Xi. On the other hand, 1 \u2212 \u03bbi,j = 1 means that having skill Xi ensures that the learner correctly answers question Yj.\nIn a noisy-OR gate, for a learner missing all skills, all auxiliary variables are in the false state and, therefore, the learner is expected to be always wrong. This can be easily solved by making the gate leaky. The probability P(X = 1|X0 = 1) = 1 \u2212 \u03bb0 describes the chances of a random guess; for instance, in a multiple choice question with four options, one of which is correct, one should set the probability 1 \u2013 \u03bb0 = 1/4."}, {"title": "Complementary Skills (And)", "content": "In disjunctive gates, all auxiliary variables must be in their distinguished state Xi = 1 for the answer to be correct. Therefore, a noisy-AND is useful to describe complementary skills, which must be mastered at the same time in order to answer properly.\nIn the noisy-AND model, possessing skill Xi implies the ability to apply it to any question Yj, whereas a weakness in skill Xi alone will compromise the success in task Yj with probability 1 \u2212 \u03bbi,j. When 1 \u2212 \u03bbi,j = 0, it follows that lacking Xi in no way can compromise the success in task Yj. On the other hand, when 1 \u2013 \u03bbi,j = 1, task Yj is such that the lack of skill Xi alone causes the student to fail.\nIn a noisy AND, if a learner has all skills, she/he can certainly solve any task, thus excluding the possibility of doing wrong by other reasons than the lack of one of the modeled skills. Again, this can be easily solved by a leaky gate where P(X = 0|X0 = 0) = 1 \u2212 \u03bb0 is the probability of failing the task despite the presence of all skills.\nIn both cases, 1 \u2212 \u03bbi,j represents the importance of skill Xi with respect to question Yj, being the probability that the non distinguished state of the i-th skill will alone lead question Yj to be in the non distinguished state as well. The extreme cases where 1-\u03bbi,j = 0 and 1-\u03bbi,j = 1 represent, respectively, a situation where the skill and the question nodes are conditionally independent, that is, skill Xi is not relevant to question Yj, and a situation where the non-distinguished state of the skill Xi alone implies the non-distinguished state of question Yj."}, {"title": "Posterior Probabilities", "content": "Here we derive analytical expressions for the inferences of interest in ITSs based on BNs. We focus on the posterior probabilities for the skill of interest updated after the gathering of the answers from the learners. We initially consider a BN as in Fig. 2 and derive expressions for the probabilities of a particular skill, say Xk, given that the answer is the distinguished state (i.e., P(Xk|Y = x)) or the negation of the distinguished state (i.e., P(Xk|Y = \u00acx)). In this way we obtain expressions valid for both noisy-OR and noisy-AND gates. We also characterize the dependence of these expressions with respect to the corresponding parameter \u03bbk and their relation with the marginal PMF P(Xk).\nThe answer is the distinguished state. In a conjunctive (resp. disjunctive) model, Y is true (resp. false) if and only if all its parents (X1, ..., Xn) are in their true (resp. false) states. In other words Y = x implies Xi = x for each i = 1,..., n. Such induced evidence allows in particular to drop the arc connecting Xk with Yk. By Markov condition:\n$\\displaystyle P(X_k|Y = \\hat x) = P(X_k|X_i = \\hat x).$ (6)\nThe posterior queries of Xk can be therefore obtained by local computations involving only the elements of the CPT P(Xk|Xi) and the marginal \u03c0k := P(Xk = \u00acx). In fact, by Bayes rule, we can obtain from Eq. (6) that:\n$\\displaystyle P(X_k = \\lnot\\hat x|Y = \\hat x) = \\frac{\\pi_k \\lambda_k}{\\pi_k \\lambda_k + (1 - \\pi_k)}$ (7)\nEq. (7) is a monotonically increasing function of \u03bbk, that is always greater or equal than \u03c0k.\nThe answer is the non distinguished state. If Y = \u00acx, it is not possible to directly propagate the evidence as in the previous case. To compute P(Xk = \u00acx|Y = \u00acx) = P(Xk = \u00acx,Y = \u00acx)/P(Y = \u00acx) first notice that:\n$\\displaystyle P(X_k = \\lnot\\hat x, Y = \\lnot\\hat x) = \\pi_k - P(X_k = \\lnot\\hat x, Y = \\hat x).$ (8)\nIn principle the computation of P(Y = \u00acx) requires a sum over all the 2n joint states of the parents of Y. In practice this can be achieved in linear time by a recursion (Pearl 1988). In fact, by the total probability theorem:\n$\\displaystyle P(Y = \\hat x) = \\Sigma_{(x_1,...,x_n)} P(Y = \\hat x|x_1,...,x_n) \\Pi_{i=1}^n P(x_i).$ (9)\nEq. (5) allows to rewrite the r.h.s. of Eq. (9) as\n$\\displaystyle \\Sigma_{(x_1,...,x_n)} \\Pi_{i=1}^n [(1 - \\pi_i)I_{x_i=\\hat x} + \\lambda_i \\pi_i I_{x_i=\\lnot\\hat x}],$ (10)\nand, finally, P(Y = \u00acx) = \u03a0i=1n (1 \u2013 \u03c0i(1 \u2013 \u03bbi)). Thus, by Eqs. (7) and (8), P(Xk = \u00acx|Y = \u00acx) becomes:\n$\\displaystyle \\frac{\\pi_k - \\pi_k \\lambda_k \\Pi_{i\\neq k}(1 - \\pi_i(1 - \\lambda_i))}{1 - \\Pi_{i=1}^n (1 - \\pi_i (1 - \\lambda_i))}.$ (11)\nConcerning monotonicity with respect to \u03bbk, it can be shown that the derivative of (11) is always negative, and therefore P(Xk = \u00acx|Y = \u00acx) is a monotonically decreasing function of \u03bbk, being also greater than or equal to \u03c0k."}, {"title": "Summary and general case", "content": "Eqs. (7) and (11) allow for an analytical characterization of the effects of an answer on ITSs based on BNs with noisy gates. The monotonicity results with respect to \u03bbk are consistent with an elicitation strategy that intends 1 \u2013 \u03bbk as the relative importance of the k-th skill to properly answer question Y. The relation with \u03c0k implies that, for both noisy-OR and noisy-AND gates, inequality P(Xk = 1|Y = 0) < P(Xk = 1) < P(Xk = 1|Y = 1), i.e., the qualitative behavior we expect in ITSs when the learner provides wrong or right answers.\nRegarding complexity, our formulae can be computed in linear time. Yet, this refers to the updating of a single answer/evidence, that is equivalent to cope with a BN as in Fig. 2. Coping with multiple evidences does not make any difference if the answers are the distinguished states of their gates. In those cases, by Eq. (6), we can move the evidence to the auxiliary variables of the gate and disconnect them from the question. In practice, the evidence can be embedded in the model by re-defining the marginal PMF of Xk with the posterior values in Eq. (7) separately for each evidence and each k. As a result, we might focus on the updating of the non-distinguished answers by considering a BN where only the corresponding gates are present."}, {"title": "Conclusions", "content": "We advocate BNs with noisy (conjunctive or disjunctive) gates for the implementation of ITSs. A natural outlook is the extension to ordinal variables. The has been already proposed in (Diez 1993), but providing an analogous characterization requires a dedicated effort. Following the ideas in (Antonucci et al. 2021), it seems possible to extend the procedure derived in this paper from Bayesian to credal nets, i.e., BNs with interval parameters. The model in (Antonucci 2011) might be considered for that."}]}