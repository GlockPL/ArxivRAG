{"title": "SocialMind: LLM-based Proactive AR Social Assistive System with Human-like Perception for In-situ Live Interactions", "authors": ["BUFANG YANG", "YUNQI GUO", "LILIN XU", "ZHENYU YAN", "HONGKAI CHEN", "GUOLIANG XING", "XIAOFAN JIANG"], "abstract": "Social interactions are fundamental to human life. The recent emergence of large language models (LLMs)-based virtual\nassistants has demonstrated their potential to revolutionize human interactions and lifestyles. However, existing assistive\nsystems mainly provide reactive services to individual users, rather than offering in-situ assistance during live social\ninteractions with conversational partners. In this study, we introduce SocialMind, the first LLM-based proactive AR social\nassistive system that provides users with in-situ social assistance. SocialMind employs human-like perception leveraging\nmulti-modal sensors to extract both verbal and nonverbal cues, social factors, and implicit personas, incorporating these\nsocial cues into LLM reasoning for social suggestion generation. Additionally, SocialMind employs a multi-tier collaborative\ngeneration strategy and proactive update mechanism to display social suggestions on Augmented Reality (AR) glasses,\nensuring that suggestions are timely provided to users without disrupting the natural flow of conversation. Evaluations on\nthree public datasets and a user study with 20 participants show that SocialMind achieves 38.3% higher engagement compared\nto baselines, and 95% of participants are willing to use SocialMind in their live social interactions.", "sections": [{"title": "1 INTRODUCTION", "content": "Social interactions are a crucial determinant of quality of life, significantly impacting both physical and mental\nhealth by enhancing communication skills and alleviating stress [3]. However, over 15 million individuals in\nAmerica experience social anxiety when anticipating or engaging in social interactions [5]. Even people without\nsocial anxiety may feel anxious when interacting with certain individuals, such as senior managers and unfamiliar\ncolleagues, which can reduce their overall well-being. With the surge of Large Language Models (LLMs) and\ntheir reasoning capabilities [7, 64], numerous LLM-based personal virtual assistants have been developed to\nenhance individuals' overall well-being. However, existing LLM-based personal virtual assistants, such as writing\nassistants [24, 57], fitness assistants [65, 74], and coding assistants [21], focus solely on serving individual users\nrather than supporting live social interactions with conversational partners.\nIn addition to general virtual assistants, LLM-based social assistive systems have been proposed recently, aiming\nto support autistic patients [39], provide knowledge consultation on social etiquette [6], and resolve cultural\nconflicts in communication [35, 77, 79]. These assistive systems either function in a reactive \u201cquery-response\u201d\nmanner to address users' explicit social-related questions [6, 39], or act as post-processing modules to detect and\nremediate norm violations in social conversations [34, 35], rather than providing in-situ assistance during live\nsocial interactions with conversational partners. However, human face-to-face social interactions are complex"}, {"title": "2 RELATED WORK", "content": "Voice assistants are widely used in daily lives on various commercial mobile devices, such as Apple's Siri [1] and\nGoogle Assistant [28]. Recently, LLM-based virtual assistants have been developed, such as fitness assistants\n[65, 73, 74], writing assistants [24, 57], and coding assistants [21]. OS-1 [71] is a virtual companion on smart glasses\noffering companionship by recording daily activities and chatting with users. UbiPhysio [65] is a fitness assistant\nthat provides natural language feedback for daily fitness and rehabilitation exercises, improving workout quality.\nMoreover, recent studies develop personal assistants for older adults [25, 75] and individuals with impairments\n[41]. EasyAsk [25] is a search assistant for older adults, accepting both audio and text inputs to provide app\ntutorials based on their queries. Talk2Care [75] is a voice assistant designed to engage in conversations with\nolder adults to gather health information for healthcare providers. Additionally, studies like PEARL [57] and\nPRELUDE [24] develop LLM-based writing assistants that adapt outputs to user preferences using retrieval\naugmentation [57] or interactive learning [24]. However, these systems focus solely on single-user human-to-\ncomputer interactions, considering only the user's unilateral goals and inputs. SocialMind takes a further step by\nproviding users with social assistance during live, face-to-face interactions involving other parties."}, {"title": "3 A SURVEY ON SOCIAL ASSISTANCE NEEDS", "content": "To understand the demand for social assistants during interactions, we conduct a survey exploring user experience,\npreferences, and needs regarding live social interactions. The results and findings guide the design of our system."}, {"title": "3.1 Design of Questionnaire", "content": "The questionnaire comprises three sections, totaling 14 questions. The questions are summarized as follows:\n\u2022 P1: This section is designed to capture participants' social experiences, including their experiences of social\nawkwardness, awkwardness sources, and attention to nonverbal behaviors during social interactions.\n\u2022 P2: The second section assesses the needs and preferences for virtual social assistance technologies. It includes\nquestions about participants' attitudes toward social assistance during live interaction, preferred devices,\ndesired social situations, desired content of suggestion, and assistive information format. It also examines\nparticipants' preferred information display methods and tolerance for system latency.\n\u2022 P3: The final section explores participants' attitudes toward privacy and comfort in the context of virtual\nsocial assistance technologies, assessing their willingness to interact with users utilizing such assistants and\nconcerns about potential personal data capture during interactions.\nWe collect 60 questionnaires in total, and summarize the results and findings as follows."}, {"title": "3.2 Social Experience and Awkwardness", "content": "Among the participants, 18.3% consider themselves to enjoy interacting with others, while the remaining\nparticipants describe themselves as not enjoying it as much. Besides, only less than 10.0% of the participants\nreport being completely at ease during social interactions. As shown in Figure 2a, 91.7% claim that they experience\nsome level of awkwardness in social situations, indicating that social awkwardness is pretty common in daily life.\nThe survey results indicate social awkwardness comes from various sources. Specifically, more than 60.0%\nreport experiencing awkwardness when interacting with workplace superiors or professors. This trend extends\nto formal events like meetings. Peer interactions contribute as well, with 40.0% feeling nervous when interacting\nwith colleagues or fellow students, particularly in initial encounters. Furthermore, 31.7% report awkwardness\nwhen interacting with long-lost acquaintances, and nearly half feel anxious when communicating with unfamiliar\nrelatives. Moreover, as Figure 2c shows, 65.0% experience stress in formal settings. Besides, over half also\nregard conversational partners as a key factor, indicating that personal relationships are vital in shaping social\nawkwardness. These results indicate that social awkwardness is most pronounced in situations involving authority\nfigures, formal settings, and unfamiliarity.\nFurthermore, the results reveal that nonverbal social behaviors play an important role in social interactions,\nparticularly facial expressions, tone of voice, personal distance, and gestures. Figure 2b shows that only 8.3%\noverlook nonverbal behaviors while the majority consider them essential during interactions. Specifically, facial\nexpressions are noted by nearly 80.0% of the participants, and tone of voice is noted by 65.0%. Besides, 38.3%\nare attentive to personal distance, and 31.7% regard gestures as supplementary cues. Despite these nonverbal\nbehaviors' significance, their indirect nature presents challenges, suggesting a need for support in interpreting\nnonverbal cues."}, {"title": "3.3 The Demand and Preference for a Social Assistant", "content": "The questionnaire's second section reveals that the preference for a virtual social assistant aligns with the social\nawkwardness experienced by participants. Notably, 70.0% believe that a virtual assistant offering instant social\nsuggestions during interactions would be beneficial, indicating a clear demand for social assistance technology."}, {"title": "3.4 Privacy and User Comfort", "content": "Privacy and user comfort are critical factors in the adoption and acceptance of virtual social assistance technologies.\nResults reveal strong openness to such technologies, with 88.3% willing to engage with users employing such\nassistants. However, when confronted with specific privacy concerns, such as image capture during interactions,\nuser comfort levels decrease. Despite this, 63.3% are willing to continue conversations. This indicates that while\nprivacy concerns are present, they do not significantly deter interest and demand for social assistance technologies,\nhighlighting a generally positive reception."}, {"title": "3.5 Findings Summary", "content": "We summarize our key findings as follows:\n\u2022 Social awkwardness is pretty common in daily life, particularly in interactions with authority figures, formal\nsettings, and unfamiliar situations. This reveals the potential benefits of virtual social assistance.\n\u2022 Nonverbal behaviors like gestures, facial expressions, and personal distance are essential in interactions, as\npeople naturally perceive and focus on these cues during conversations. An effective virtual assistant should\ntherefore provide assistance with human-like perception for nonverbal cues."}, {"title": "4 SYSTEM DESIGN", "content": "SocialMind is an LLM-based proactive AR social assistive system capable of human-like perception, providing\nusers with in-situ assistance during live social interactions. Figure 3 shows the system overview of SocialMind.\nSocialMind first leverages the multi-modal sensor data, including audio, video, and head motion, to achieve\nhuman-like perception in social contexts (\u00a7 4.2). It automatically extracts nonverbal and verbal behaviors, and\nparses social factor cues. Meanwhile, SocialMind identifies implicit persons from social contexts and performs\nimplicit persona adaptation (\u00a7 4.3). The extracted verbal and nonverbal behaviors, social factors, and implicit\npersona cues are then integrated into the LLMs for reasoning (\u00a7 4.4). Finally, SocialMind employs a multi-tier\ncollaborative reasoning strategy with a social factor-aware cache and intention infer-based reasoning approach\nto generate in-situ social suggestions (\u00a7 4.5). These suggestions are displayed on AR glasses through a proactive\nresponse mechanism to assist users in live social interactions without disrupting the natural flow of conversations.\nWe chose AR glasses for social assistance over devices like smartphones or smartwatches for three main\nreasons. First, AR glasses for daily wear are increasingly accepted, as seen in applications like captioning and\ntranslation [31, 37, 59, 62]. Second, AR glasses offer a non-distracting solution, allowing users to maintain\neye contact during social interactions without disrupting the natural flow of conversation. Finally, our survey\nindicates that most participants favor glasses as the ideal hardware for embedding a social assistive system in\nlive interactions over other devices."}, {"title": "4.2 Human-like Perception in Social Context", "content": "Existing studies on social assistive systems focus solely on single-user human-to-computer interactions and follow\na reactive paradigm, conducting either question-answering [6, 39] or remediating cultural violations [35, 77, 79].\nHowever, live social interactions involve multi-modal cues such as nonverbal behaviors and social factors, posing\nchallenges to existing text-only LLMs in providing comprehensive social suggestions. SocialMind employs a"}, {"title": "4.2.1 Nonverbal Cues Perception", "content": "Nonverbal behaviors play a crucial role in face-to-face social interactions\n[20]. For example, facial expressions like confusion and frowning can indicate a person's emotional state during\nface-to-face social conversations [22]. Additionally, gestures can reveal a person's implicit perspectives, such as\ntheir understanding, intentions, or agreement, during social interaction.\nSocialMind proactively perceives the nonverbal behaviors of the conversational partners and leverages these\nimplicit cues to adjust social strategies and assist the user. However, nonverbal behaviors such as facial expressions,\ngestures, and physical proximity are captured by multi-modal sensors. Directly offloading the raw multi-modal\ndata to the cloud server incurs significant bandwidth usage, high latency, and raises privacy concerns. To\naddress these challenges, SocialMind employs multiple lightweight yet specialized small models on AR glasses\nto efficiently process raw data locally. Specifically, we first employ MediaPipe Holistic [29] in SocialMind to\ngenerate human poses, including facial mesh and hand poses. These facial mesh and hand poses are then further\nprocessed by different specialized models to generate nonverbal cues (\u00a7 5.1.1). Finally, these nonverbal cues are\nincorporated into the LLMs to generate nonverbal cues-aware social suggestions (\u00a7 4.4). Table 3 shows the details\nof the nonverbal cues detected in SocialMind, including facial expressions, gestures, and physical proximity [52].\nWe selected these nonverbal cues based on feedback from our user survey in \u00a7 3 and because existing studies\nindicate that they are the most representative forms of nonverbal communication during face-to-face social\ninteractions [20]."}, {"title": "4.2.2 Efficient Primary User Identification", "content": "Since SocialMind focuses on live face-to-face social interactions with\nconversational partners, it requires efficient and robust identification of the primary user and other participants.\nVoice fingerprinting [26] can be used for speaker identification, but it introduces additional overhead from\nregistration and raises security concerns, such as voice synthesis and replay attacks [45]. This is evidenced\nby Microsoft's recent closure of its speaker recognition service [55]. Volume-based solutions [16] utilize low-\nfrequency energy to differentiate the primary user's speech from that of nearby individuals, but their robustness\nis limited by environmental noise and variations in the user's speaking volume. To address these challenges,\nSocialMind leverages a lightweight primary user identification approach leveraging the vibration signals on the\nsmart glasses as indicators.\nWe first conduct real-world measurements where the primary user wears smart glasses and engages in\nconversations with different partners. The smart glasses record the audio and vibration signals simultaneously.\nFigure 4 shows the waveform of the audio and vibration signals on the smart glasses during live social interactions.\nThe primary user speaks during the first 6 seconds, while the conversational partner speaks during the last 6\nseconds. Compared to the audio, the amplitude of the vibration exhibits a clear difference between the primary\nuser's speaking period and the partner's speaking period. Therefore, we leverage the signal energy vibration\nsignals as an indicator to detect the primary user. Specifically, we calculate the vibration signal's energy within\nthe 3 ~ 10 Hz range and use it as the indicator. Energies exceeding a certain threshold are detected as the primary\nuser. We employ a grid search to determine the optimal threshold. The sample rate of the vibration signal in\nSocialMind is set to 466 Hz, which is significantly lower than the audio sample rate, thereby reducing bandwidth\nusage. Additionally, SocialMind transmits the vibration signal from the glasses to the server at regular intervals\nof 300 ms and sets the threshold for primary user detection at 1.1 on the server. Details on the threshold search\nand the overall detection performance of our approach compared to audio-based solutions can be found in \u00a7 5.3.3."}, {"title": "4.2.3 Social Factor Cues Parsing", "content": "Existing studies show that social factors play a vital role in social communication\n[14]. Social behaviors and speech content considered acceptable or unacceptable can vary significantly depending\non different social factors such as social relation and formality [33]. For example, when making a request, the tone"}, {"title": "4.3 Implicit Persona Adaptation", "content": "Every individual has unique backgrounds, life experiences, and personal interests, which are abstracted into\npersonas [10]. A social topic that connects the personal interests and backgrounds of both parties can enhance the\nengagement of both parties. An ideal social assistive system should proactively identify the implicit personas of\nboth parties and incorporate these personas into the strategies for social suggestion generation. However, natural\nsocial conversations often lack explicit queries to initiate the knowledge retrieval of personal historical databases,\nposing challenges for system personalization. SocialMind employs an implicit persona adaptation approach to\ngenerate customized social suggestions, enhancing the engagement of both parties."}, {"title": "4.3.1 Implicit Persona Extraction", "content": "Existing personal assistant systems employ the user's explicit queries to\nretrieve historical data and provide personalized responses [17, 24, 74]. However, systems like writing assistants\n[24], emotional support assistants [17], and medical assistants [74] primarily function in a question-answering\nmanner, relying on explicit queries to initiate the retrieval of the personal historical database. These explicit\nqueries allow them to utilize the standard RAG techniques [44] to retrieve responses with high semantic similarity.\nHowever, natural social conversations lack explicit queries to initiate the retrieval of personal historical data,\nposing challenges in generating social suggestions that incorporate implicit personas.\nTo address this challenge, SocialMind employs an additional LLM to extract the implicit personas of both parties\nfrom historical conversations in advance, maintaining a persona database. Figure 5 shows the pipeline of the\nimplicit persona adaptation in SocialMind. Specifically, the persona extraction occurs during the post-interaction\nphase, where an LLM extracts the implicit persona cues from the social conversations. These persona cues"}, {"title": "4.3.2 Persona Management", "content": "Since live experiences and personal interests evolve over time, SocialMind employs a\npersona management strategy to adapt to these emerging personas. Specifically, for new conversational partners,\ntheir persona cues will be directly registered in the persona database. For the user and previously met partners,\nSocialMind first utilizes LLMs to determine if any contradictory or similar cues already exist within the persona\ndatabase for that particular identity. If no such cases are found, the incoming persona cues are registered and\nstored in the memory. If the incoming persona cues are semantically similar to existing ones, the two sets of\ncues are merged. Conversely, if the incoming persona cues are contradictory, the historical cues are removed and\nreplaced by the new incoming ones."}, {"title": "4.3.3 Persona Retrieval", "content": "During live social interactions, SocialMind first performs persona retrieval using face\nID matching to determine whether the conversational partners are in the database. This facial recognition is\nexecuted locally on the glasses to protect privacy. If the partners are found, the personas of both the user and the\npartner are loaded and integrated into LLMs as a knowledge source (\u00a74.4). Otherwise, only the user's persona\nwill be used. These implicit persona cues can help LLMs identify shared interests or experiences and generate\ncustomized social suggestions, thereby enhancing the engagement of both parties."}, {"title": "4.4 Multi-source Social Knowledge Reasoning", "content": "Existing social assistive systems primarily focus on text-based question answering and remediating social viola-\ntions. SocialMind leverages multi-modal sensors to gather multi-source knowledge during live social interactions.\nThis section introduces the knowledge integration in SocialMind, which guides the LLMs to understand and\nutilize this knowledge in live social interactions, ensuring improved instruction-following performance and\nenhanced user Quality of Experience (QoE)."}, {"title": "4.4.1 Knowledge Source", "content": "The knowledge source in SocialMind contains nonverbal cues (\u00a7 4.2.1), the context of\nlive social conversations (\u00a7 4.2.2), social factors (\u00a7 4.2.3), and implicit persona cues from both parties (\u00a7 4.3.1).\nSocialMind also integrates external tools that provide the latest weather updates and trending social news, which\nare valuable sources for conversation topics. Since this information is updated daily and does not need to be\nretrieved during online interactions, it is pre-retrieved and incorporated into SocialMind's knowledge source."}, {"title": "4.4.2 Knowledge Integration", "content": "This subsection introduces the prompt used in SocialMind, which enables LLMs to\nintegrate multi-source and multi-modal information for social suggestion generation. Specifically, the prompt"}, {"title": "4.4.3 Concise CoT Reasoning", "content": "The aforementioned subsections introduce the knowledge source used in Social-\nMind. However, integrating this information into LLMs and generating social suggestions that enhance the user's\nQoE remain several challenges. First, the user survey in \u00a7 3 shows that more than 67% of participants prefer\nsocial suggestions presented as summarized key points in bullet form, followed by a sample sentence. However,\nlengthy and redundant social suggestions may exceed the average human reading speed of approximately 200\nwords per minute and may not fully display on the glasses' screen [13]. Additionally, the output format of\nsocial suggestions should be specifically designed for readability, comfort, and quick comprehension. Second, the\nabundance of information and instructions makes it challenging for LLMs to accurately follow instructions and\ngenerate appropriate social suggestions.\nTo address these challenges, we employ a concise Chain-of-Thought (CoT) [66] reasoning strategy for social\nsuggestion generation. Specifically, we first add the instruction \u201cLet's think step by step\u201d into the prompt. The\nCoT reasoning strategy enhances LLMs' complex task reasoning and instruction-following capabilities [66]. Next,\nwe set constraints on the generation length of the social suggestions by including \u201cLimit your total response to\nN words or less\" in the prompt. Based on our measurement experiments, we set N to 70, as this length is optimal\nfor full display on the eye screen. Finally, according to the user survey, the final display format of the suggestions\non the glasses includes summarized suggestions in bullet points, followed by a sample sentence. This format\nallows users to choose their preference, whether referring to the summarized bullets or reading the example.\""}, {"title": "4.5 Multi-tier Collaborative Social Suggestion Generation", "content": "Social assistance during user in-situ interactions with other parties requires providing instant social suggestions,\nenabling the user to refer to them and talk with others without disrupting the natural flow of the conversation. To\naddress this challenge, SocialMind employs a multi-tier collaborative suggestion generation approach, as shown\nin Figure 6. It includes a social factor-aware cache and an intention infer-based reasoning strategy, to provide\ninstant social suggestions. Additionally, SocialMind employs a proactive response update mechanism to control\nthe refresh of social suggestions displayed on AR glasses."}, {"title": "4.5.1 Social Factor-aware Cache", "content": "Cache has been widely utilized in existing studies to avoid redundant computa-\ntions [19, 30, 69] and to reduce serving costs of LLMs [11, 27]. However, cache-based conversational systems\nrely on semantic retrieval mechanisms, generating semantically similar responses but often struggling with\nlogical consistency [67]. It can be more challenging in the context of live social assistance since conversational\nnorms vary with social factors [78]. Even for the same utterance, assistive systems should offer different social\nsuggestions based on varying social factors and non-verbal cues, posing challenges to the robustness of cache. To\naddress these challenges, SocialMind leverages the social factor priors to construct and manage the cache."}, {"title": "4.5.2 Intention Infer-based Suggestion Generation", "content": "Although the social factor-aware cache can significantly\nreduce inference latency, it faces challenges in providing logically consistent social suggestions. Therefore,\nSocialMind employs LLM reasoning to generate in-depth social suggestions. However, directly using LLMs in\nlive social interactions can cause significant system latency, which may disrupt the natural flow of live social\nconversations and reduce QoE. To address this challenge, SocialMind employs an intention infer-based reasoning\nstrategy inspired by human behaviors in social interactions."}, {"title": "4.5.3 Proactive Response Update", "content": "Frequent refreshing of the social suggestions displayed on AR glasses can\nsignificantly reduce the QoE and usability of SocialMind, as users do not have enough time to read and grasp the\ninformation. To address this challenge, SocialMind employs a proactive response update mechanism. Specifically,\nwe set the suggestion display refresh interval to 3 seconds, considering that the average human reading speed is\n200 words per minute [49] and concise CoT reasoning (\u00a7 4.4.3) limits responses to 70 words. Additionally, we set\nan additional instruction in the LLM to determine whether there is any change in the semantics of the other\nparty's two consecutive utterances. If the semantic similarity between the two utterances is high, SocialMind will\nnot update the social suggestions on the AR glasses."}, {"title": "5 EVALUATION", "content": "This section introduces the experimental setup, evaluation of SocialMind, and a real-world user study."}, {"title": "5.1 Experimental Setup", "content": "We selected the off-the-shelf RayNEO X2 [4] smart glasses as our hardware\nplatform. These glasses run on the Android 12 operating system, boasting 6GB of RAM and 128GB of storage.\nThey come equipped with a front-facing camera, dual-eye waveguide color displays, and three microphones\n(Figure 7a). Our solution is also compatible with other AR glasses, including models from INMO [37].\nFigure 7b illustrates the glass-view suggestions presented to the user. Our implementation consists of an\non-glass app and a Python-based server. The glasses app is developed using 4,038 lines of Java and Kotlin code.\nTo ensure user privacy, we process video and audio locally. We use GPU-accelerated MediaPipe [29] models for\nefficient pose and facial landmark tracking. For voice recognition, we employ Azure Voice Recognition with local\nvoice feature extraction. Notably, speaker recognition features have been discontinued by major voice recognition\nplatforms due to privacy concerns [56], which motivates our use of vibration-based primary user identification.\nThe server, built with Python, handles social cue recognition and proactive suggestions using lightweight\nScikit-learn models [58] and Langchain [15] for LLM coordination. The glasses communicate with the server"}, {"title": "5.1.2 Experiments on Public Datasets", "content": "We first validate the effectiveness of SocialMind using public multi-\nturn dialogue datasets [38, 47, 78]. However, to the best of our knowledge, no public datasets contain social\nconversations that include comprehensive nonverbal cues and personas. Additionally, the conversations in\nexisting datasets remain fixed, and cannot be dynamically steered by the social suggestions generated by assistive\nsystems. Therefore, we use two LLM agents for role-playing social interactions with the help of social assistive\nsystems. This subsection details the datasets and LLM agent settings."}, {"title": "Dialogue Datasets", "content": "We use three public multi-turn dialogue datasets to validate the effectiveness of SocialMind.\n\u2022 DailyDialog [47] dataset contains 13,118 multi-turn conversations covering a wide range of daily topics.\n\u2022 Synthetic-Persona-Chat [38] dataset is a conversational dataset featuring personas for both parties. Compared\nwith DailyDialog, conversations in Synthetic-Persona-Chat are persona-conditioned. Each sample includes the\npersonas of the two parties and their conversations. The dataset comprises 20,000 conversations and 5,000\npersonas in total. The personas in this dataset are used to construct the LLM agents.\n\u2022 SocialDial [78] dataset comprises more than 6.4K multi-turn dialogues with social factor annotations, each\nannotated with social factors such as social relations and social norms.\nFor all datasets, we randomly select one speaker as the primary user in the experiment and the other as the\nconversational partner. However, since the conversations in the datasets remain fixed and are not influenced by the\ngenerated social suggestions, we set up two LLM agents to role-play the social interactions. Existing studies have\ndemonstrated the effectiveness of using LLMs as agents for role-playing, such as negotiations [35] and medical\ndiagnosis simulations [74]. We extend the role-playing capabilities of LLMs to live social interaction settings.\nSpecifically, we set up two separate LLMs to create agents for role-playing as the user and the conversational\npartner during live social conversations. The user agent interacts with the partner agent while incorporating\nthe social suggestions generated by the assistive systems. We randomly select 50 samples in each dataset for\nexperiments."}, {"title": "Setup of Simulated Agents", "content": "First, we set instructions in the prompts to enable the user agent and the conver-\nsational partner agent to conduct multi-turn social conversations. Additionally, the agent contains nonverbal\nbehavior and persona attributes. For nonverbal behaviors, we use a series of nonverbal behaviors that encompass\ntypical cues in face-to-face social conversations, including facial expressions, gestures, and physical proximity [20].\nEach type of behavior contains multiple subcategories, such as confusion and frowning for facial expressions, and\nnodding and head shaking for gestures. Table 3 details the categories of nonverbal behaviors in our experiments.\nDuring each turn of the social conversation, we randomly select one subcategory from the nonverbal behaviors\ncategory as the partner LLM agent's current simulated nonverbal behaviors.\nFurthermore, since Synthetic-Persona-Chat is the only dialogue dataset that contains the personas of both\nparties, we use the personas as personal profiles to incorporate into the prompt for the LLM and set up the\nsimulated user and conversational partner agents. Additionally, the conversations corresponding to the user and\nthe conversational partner are used as historical data for implicit persona extraction"}, {"title": "Two Role-play Paradigms", "content": "The LLM role-play of social interactions are conducted using two paradigms:\ndialogue-based and social factor-based. In the dialogue-based role-play, both LLM agents initiate interactions\nusing dialogues from three datasets. In the social factor-based paradigm, the agents start interactions guided by\nsocial factors defined in [78], including social norms, social relations, formality, and location see Figure 22 and\nFigure 23. We randomly select subcategories of these social factors to create diverse social scenarios and conduct\nexperiments."}, {"title": "5.1.3 Real-world Evaluation", "content": "To further validate the effectiveness of SocialMind, we recruited 20 volunteers\nto participate in real-world evaluation. Each participant wears glasses and engages in face-to-face live social\nconversations with the conversational partner, assisted by SocialMind. The study has received IRB approval, and\nall participants have given consent for data collection. Figure 7 shows the settings of the real-world tests and the\nsystem prototype of SocialMind. After the experiments, each participant is required to fill out a questionnaire\nand participate in a user study regarding their experience with SocialMind."}, {"title": "5.1.4 Evaluation Metrics", "content": "Since generating social suggestions in multi-turn conversations is an open-ended\ntask [80] without explicit standard answers, existing metrics used for question-answering and classification tasks\nare not suitable for evaluation. In this study, we propose the following criteria to validate the effectiveness of\nsocial assistive systems. First, we assess the content of the social suggestions provided by the assistive systems.\nSecond, we evaluate their effectiveness when users employ these systems during social interactions.\n\u2022 Personalization. This metric evaluates the quality of social suggestions from a personalized perspective,\nassessing whether the social suggestions incorporate users' implicit personas, including personal interests\nand backgrounds. It is similar to the score introduced in studies [71]. A higher personalization score for social\nsuggestions can enhance user engagement in social interactions, as users are more familiar with the content.\n\u2022 Engagement. This metric has been widely used in previous studies [43] to evaluate user engagement in\nconversational systems. However, in the context of a social assistive system, we extend engagement to the\nconversational partner's perspective, assessing whether the social suggestions consider the conversational\npartner's implicit personas. Higher engagement in social suggestions indicates the conversational partner's\nincreased willingness and enhanced participation in social interactions.\n\u2022 Nonverbal Cues Utilization. Given the importance of nonverbal cues in social interactions, we propose this\nmetric to assess whether social suggestions take into account the conversational partner's nonverbal cues.\nExisting studies have demonstrated that LLMs can be utilized to assess the quality of multi-turn conversa-\ntions [74] and open-ended tasks [71, 80], known as LLM-as-a-Judge [80]. We adopt the LLM-as-a-Judge [80] and\nextend it to evaluate the quality of open-ended social suggestions, guided by the aforementioned criteria. We use\nGPT-40 as the base model for LLM evaluation throughout this paper."}, {"title": "5.1.5 Baselines", "content": "Since no previous studies have developed assistive systems that provide social suggestions\nduring live interactions between two parties, we established several baseline approaches to evaluate our system.\n\u2022 Zero-shot. This is a prompt-based solution. We include instructions in the prompts of LLMs to provide social\nsuggestions during conversations between the user and conversational partner. We use GPT-40 as the base\nLLM. Figure 26 shows the prompt of Zero-shot.\n\u2022 CoT [66]. The settings are the same as in Zero-shot, but employ the CoT reasoning strategy."}, {"title": "5.2 Overall Performance", "content": "This section shows the overall performance of SocialMind under scenarios with varying social factors."}, {"title": "5.2.1 Quantitative Results", "content": "We first compare the performance of SocialMind and baseline approaches using\nquantitative evaluation metrics, including personalization, engagement, and nonverbal cues utilization."}, {"title": "5.2.2 Qualitative Results", "content": "To better understand SocialMind's performance", "follows": "nObservation 1: Intention infer-based reasoning strategy enables SocialMind to provide logically consistent\nand instant social suggestions. Figure 12 shows an example of the intention infer-based social suggestion\ngeneration in SocialMind. SocialMind utilizes the partially spoken sentences of the conversational partner\n(marked as red in Figure 12) to generate instant suggestions by inferring the partner's intentions, providing\nthe user with early preparation for their response. Results show that SocialMind can comprehend the partner's\nintentions, draft preliminary social suggestions from incomplete utterances, and deliver these suggestions to\nthe user. This enables the user to promptly begin considering their response strategy. Once the partner finishes\nspeaking, SocialMind also utilizes the complete utterances for further reasoning, offering the user in-depth social\nsuggestions that the user can incorporate to refine their current response or use in the next round of conversation"}]}