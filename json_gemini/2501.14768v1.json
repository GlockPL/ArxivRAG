{"title": "Equation discovery framework EPDE: Towards a better\nequation discovery", "authors": ["Mikhail Maslyaev", "Alexander Hvatova"], "abstract": "Equation discovery methods hold promise for extracting knowledge from\nphysics-related data. However, existing approaches often require substan-\ntial prior information that significantly reduces the amount of knowledge\nextracted. In this paper, we enhance the EPDE algorithm an evolution-\nary optimization-based discovery framework. In contrast to methods like\nSINDy, which rely on pre-defined libraries of terms and linearities, our ap-\nproach generates terms using fundamental building blocks such as elementary\nfunctions and individual differentials. Within evolutionary optimization, we\nmay improve the computation of the fitness function as is done in gradient\nmethods and enhance the optimization algorithm itself. By incorporating\nmulti-objective optimization, we effectively explore the search space, yield-\ning more robust equation extraction, even when dealing with complex ex-\nperimental data. We validate our algorithm's noise resilience and overall\nperformance by comparing its results with those from the state-of-the-art\nequation discovery framework SINDy.", "sections": [{"title": "1. Introduction", "content": "Originating from SINDy [1] and PDE-FIND [2], equation discovery has\nevolved into a valuable machine learning tool for extracting knowledge from\nobservational data. Extensions such as [3] and [4] have addressed the chal-\nlenges posed by the experimental data. In SINDy, a predetermined term\nlibrary is employed, incorporating a priori knowledge that the equation can\nbe constructed using these terms. Often, the library is minimized to enhance\nits effectiveness. However, this approach raises concerns about the potential\nto extract unknown equations due to the limitations of the term library.\nUsing fixed functional structures in regression-based techniques, such as\nthe form $u_t = F(x, u_x, u_{xx}, ...)$, tends to limit the discovery of entirely new\nequations, leading primarily to coefficient adjustments. This phenomenon\nextends to methods such as PDENet [5], PDE-READ [6], physics-informed\nneural networks (PINNs) [7], and DeepONet [8]. These approaches combine\ndata and differential operators into a single loss function, which could intro-\nduce biases when addressing the challenge of discovering unknown equations.\nAll these limitations are detailed in a survey [9].\nWe may propose evolutionary optimization as a more flexible yet slow\nalternative. Evolutionary optimization-based frameworks [10, 11, 12, 13] offer\nan expanded exploration arena, facilitating dynamic term library generation.\nThis broader search scope, encompassing an extended high-order linear term\nset and additional combinations, notably amplifies the optimization time.\nFrom the initial genetic algorithm introduced in [13], efforts have been made\nto accelerate computations and attain concise models. Techniques such as\nemploying local regression with neural networks in the DLGA algorithm [10],\nsparse regression using candidate equation terms set in previous versions of\nEPDE [12], and representing each term as an independent computational\ngraph within a forest in the SGA-PDE algorithm [11], have been proposed to\ndecrease the search space, thus accelerating the equation discovery process.\nEffective equation discovery hinges not only on the equation representa-\ntion but also on the optimization algorithm employed. For example, the supe-\nriority of multi-objective evolutionary algorithms over their single-objective\ncounterparts is evident, even when these algorithms share a common ob-\njective [14]. Moreover, population-based algorithms present an advantage\nregarding dynamic search space alteration. To illustrate, using a term pref-\nerence distribution can improve the stability and efficacy of the evolutionary\nalgorithm search [15].\nComparative algorithm evaluation is complex due to several factors, in-\ncluding various noise quantification methodologies, disparate end-result as-\nsessment criteria, and variations in input data quantity. In particular, the\nintricacies are exemplified by the distinct treatments of noise levels across"}, {"title": "2. Overview of the proposed method", "content": "Generally, in the approach based on the evolutionary algorithm, discover-\ning differential equations can be reinstated as the construction of expression,\nconnecting various elementary function derivatives, which is optimal from\nthe standpoint of selected criteria on the input data. The main idea of our\napproach is provided in scheme Fig. 1\nIt is assumed that the observational data $u(x) = {U_1(x), ..., U_k (x)}$ have\nthe form of a grid function on a discrete grid $x = x_1, ..., x_{dim}$, where $dim$ is\nthe dimensionality of the problem, $x_i$ are parameters (spatial and temporal\ncoordinates, particle sizes) and $k$ is the number of observed values. For the\ndifferential equation discovery problem, it is also assumed that the function\nrepresented by the observational data $u(x) = {\\varphi_1(x), ..., \\varphi_k (x)}$ is differen-\ntiable up to the maximal order of the differential equation that the algorithm\ncan obtain.\nTo define the equation discovery algorithm, one should consider three\naspects of the problem, which are:\n\u2022 approach to the definition of elementary functions and equation encod-\ning,"}, {"title": "2.1. Representation of differential equations", "content": "In the discovery process, the elementary functions $t_j = t_j (p_{j1}, p_{2}, ...)$\nare used as factors in the equations. For convenience, they are grouped into\nparametrized classes (families). The basic class (family) of elementary func-\ntions is the input variable derivatives in the form of $F_{deriv} = {\\frac{\\partial^{n}u}{\\partial x_1^{n_1} ... \\partial x_{dim}^{n_k}}}$,\nwith the $n_1 + ... + n_k = n \u2208 1,2,..., N_d$. With $N_d$, we denote the hyperpa-\nrameter of the maximal order of the equation. The equation may be of lesser\norder $N \u2264 N_d$. All selected families of elementary functions are combined\ninto a pool $F = F_{deriv} \u222a F_1 \u222a ...$ from which factors are chosen during the\nrandomized equation generation process and in evolutionary operators.\nEquation factors are combined into terms by product capped by the num-\nber of terms hyperparameter $N_{terms}$. The equation may contain fewer terms"}, {"title": "2.2. Optimization criteria", "content": "The desire to control the complexity of the obtained models and minimize\nthe error manifests itself in making preferences for parsimonious equations.\nIn practice, the parsimony of data-driven differential equations is connected\nto the number of terms in the equation structure, the order of derivatives in-\nside of them, and its process representation ability. The criteria for equation\ndiscovery used in the algorithm include the complexity metric Eq. (3), where\nby $n$ we define the order of the derivative. Corresponding tokens are given\na base complexity of 0.5 to avoid equation overfitting (for example, creating\noverly complex non-homogeneity) with elementary functions not represented\nby derivatives.\n$C(L'u) = \\Sigma_i \\Sigma_j compl(t_{ij});$\n$compl(t_{ij}) = $\n$\\{$\\begin{array}{ll}\nn & \\text{, if } t_{ij} = \\frac{\\partial^n u}{\\partial x_1^{n_1} ... \\partial x_{k_{dim}}}, n\\ge 1 \\\\\n0.5 & \\text{, otherwise} \\end{array}$$\nWe propose using error metrics to evaluate the quality of a candidate\nequation. The first one includes an estimation of the discrepancy of the\ndifferential equation operator. Under ideal conditions, the equation repre-\nsents an identity of the expression containing unknown modeled functions\nand their derivatives to zero. While trivial combinations, utilizing zero co-\nefficients, of any predictors can form a correct identity in a homogeneous\nequation case, they present no interest for the model creation. To avoid\nconvergence to a zero coefficient vector, it is feasible to designate a random\ncandidate term with index target_idx (left-hand side) to have a fictive coef-\nficient of $b_{target-idx} = -1$, so the problem is restated as the detection of the\noptimal real-valued coefficients for the other terms (right-hand side) with\nrespect to this condition, as in Eq. (4)."}, {"title": "2.3. Evolutionary optimization", "content": "For single equation discovery with objective criteria $Q(L'u)$ and $C'(L'u)$,\nintroduced in the previous section, the problem of multi-objective optimiza-\ntion is stated as the search for the correct terms of the equation ${a_i, c_i}, i =$\n1, ..., $N_{terms}$, paired with real-valued coefficients & bias coefficient ${b_i}, i =$\n1,... $N_{terms+1}$ in respect to the metrics $Q(L'u)$ and $C'(L'u)$. Previous research\nhas proved that the multi-objective approach, where optimization is held si-\nmultaneously for both metrics, has faster and more reliable convergence to\nthe true governing equation than the single-objective approach. In case of\nalgorithm operation on the data, describing processes that involve multiple\nvariables, complexity and quality metrics are introduced for each equation\nindividually, as $Q_i(S(u)) = Q(L_iu), C_i(S(u)) = C(L_iu)$. The general vec-\ntor of objectives can be constructed as $F(S(u)) = (Q_1(S(u)), C_1(S(u)), ...,$\n$Q_{n_var}(S(u)), C_{n_var}(S(u))).$\nFor two candidate systems, the dominance condition can be introduced:\nwe say that the system $S_1(u)$ dominates the system $S_2(u)$ (and is denoted as\n$S_1(u) \\preceq S_2(u)$), if for every equation $i$ in the system $Q_i(S_1(u)) \u2264 Q_i(S_2(u))$,"}, {"title": "3. Validation of the algorithm", "content": "The following validation study is conducted on the varied differential\nequations, representing partial differential equations, an ordinary differen-\ntial equation, and a system of ordinary differential equations. To better\nunderstand the strong and weak sides of the algorithm and its limitations,\nwe have examined the performance of the SINDy (Sparse Identification of\nNon-linear Dynamics) framework on the same problems and compared their\nperformance. The solutions of the equations with artificially added noise of\nvarying magnitudes are used as the synthetic input data. Due to the stochas-\ntic nature of the evolutionary search, we have conducted ten independent runs\nof EPDE in each case, while SINDy operates on a more deterministic basis.\nThe experiments' results with sparse identifications depend on explicitly\ndefined sparsity parameters. Therefore, a set of experiments has to be con-\nducted with different values of the defining parameter. The models obtained\ncan be compared to represent a validation dataset, and the best was selected\nas the optimal one.\nThe algorithm performance was analyzed from the point of view of eval-\nuating the frequency of the algorithm and discovering the correct equations\namong $n$ independent algorithm launches. The term-by-term comparison"}, {"title": "3.1. Partial differential equations", "content": "First, we examine the case of partial differential equation discovery. Meth-\nods based on sparse regression among second-order PDEs can operate only\nwith parabolic-like equations. The first case to be examined is Burgers' equa-\ntion Eq.(14). It represents the momentum balance for the one-dimensional\nNavier-Stokes equation, with v representing the viscosity. With the viscosity\nvalues close to zero, the equation devolves into the non-linear homogeneous\nfirst-order PDE $u_t + u \u2022 u'_x = 0$.\n$\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \u03bd \\frac{\\partial^2 u}{\\partial x^2}$$\nIn this example, we will study the system with parameter $v = 0.1$, pro-\nvided by SINDy developers. The dataset comprises 101 time points and 256\nspatial points with steps of 0.1 and 0.0625, respectively.\nThe execution time for the EPDE framework runs 91 seconds on average,\nwhile a search with SINDy takes 0.032 seconds. This time discrepancy can\nbe attributed to the algorithmic simplicity of the execution and the smaller"}, {"title": "3.2. Ordinary differential equations", "content": "The ability of the proposed framework to derive ordinary differential equa-\ntions can be demonstrated on the Van der Pol oscillator. Initially introduced\nto describe the relaxation-oscillation cycle produced by the electromagnetic\nfield, the model has found applications in other spheres of science, such as\nbiology or seismology. The model takes the form of an ordinary differential\nequation of the second order as in Eq. (16) with E - positive constant (in the\nexperiments, E = 0.2).\n$u\" + E(u\u00b2 \u2013 1)u' + u = 0$\nAlthough the proposed evolutionary approach can handle the approach\ndirectly, the equation can be transformed into a system of two first-order\ndifferential equations Eq. (17), which in theory can be processed with a sparse\nidentification of dynamics.\n$\\begin{cases}\nu' = -E(u^2 - 1)v - u\\end{cases}$$\nThe dataset included the solution of the equation with initial conditions of\n$u = \\sqrt{3}/2; u' = 1/2$ for a domain of 320 points with the step of 0.05 starting\nfrom a conventional point $t = 0$. The numerical solution was computed using\nthe Runge-Kutta method of order 4."}, {"title": "3.3. Systems of differential equations", "content": "Next, we shall discuss the performance of the proposed algorithm on prob-\nlems of discovering differential equation systems. A simple example of such\na system is the hunter-prey model, also known as the Lotka-Volterra system,\npresented in Eq. (18). It describes the dynamics of a primitive ecosystem\nwith a single hunter species with population size v and a single prey of size\nu. Non-negative parameters \u03b1, \u03b2, \u03b4, and y control the behavior of the system.\n$\\begin{cases}\nu' = \\delta u v - \\gamma v\\end{cases}$$\nWe used the system with the following parameters in the experiment:\n\u03b1 = \u03b2 = \u03b3 = \u03b4 = 20. The initial condition required by the system was\nset as u = 4, v = 2, and the equations were numerically solved with the\nRunge-Kutta fourth-order method.\nThe evaluated non-linear system has a relatively simple yet illustrative\nstructure and can be discovered by an evolutionary approach and sparse\nidentification."}, {"title": "4. Experimental summary", "content": "The experimental results are summarized in Tab. 7. The central aggre-\ngated values are the frequency of successfully obtaining the correct equations\nin the experiments and the errors of the calculated coefficients over all ex-\nperiments shown above. In the summarizing table, in columns of positive\ndiscoveries (Pos., %), we present the frequencies of the correct equation on\ndata with the noise of value in the noise magnitude column (noise level, NL\n%). For the evolutionary approach, even a single successful equation dis-\ncovery can be beneficial and prove that the algorithm can converge to the\ncorrect structure. Although the correct structure may not be optimal on\nnoisy training data, it can be identified on the held-out sample so that addi-\ntional data can be used for model rectification. Thus, the frequency column\nof one positive equation discovery is introduced in the results (one pos. in\nexp., %)."}, {"title": "5. Conclusion", "content": "In this study, we have explored the opportunities to employ an evolu-\ntionary approach to data-driven differential equation discovery instead of\nconventional sparsity promotion. The main results of the comparison can be\nsummarized in the following points:\n\u2022 The evolutionary method achieves its primary objective of providing\na solution to the central issue of sparse regression: to adapt the term\nlibrary to the problem. Using elementary functions as the building\nblocks of the equations allows the factors of the equation to contain\nparameters subject to optimization. This advantage can be vital in\nthe case of unknown dependent variable representations. However, the\nsparse identification method can be optimal in some cases due to the\nissues linked with the increased computational cost of getting the model\nand the consequent longer optimization time. Such scenarios include\nthose we can be sure of in the presence of time dynamics in the form\nof first-order derivatives.\n\u2022 Although for the integrity of the comparison, we have examined only\nthe first-order partial differential equations from the time axis point\nthat can be discovered by sparse identification, it is necessary to point\nout the abundance of equations without the first partial derivative\nalong the time axis. Fundamental cases such as Poisson and the wave\nequation do not contain u and, therefore, cannot be discovered with\nsparse regression. With ordinary differential equations, when we expect\nhigher-order derivatives, we can introduce additional variables repre-\nsenting the derivatives of the main variable.\n\u2022 In the evolutionary approach, significant attention shall be paid to se-\nlecting the preprocessing tool, i.e. noise-resistant derivative calculation\nmethod, and the fitness function. Correctly selecting these elements can\nsignificantly increase the noise threshold to derive the correct equations,\nalbeit further increasing the demand for the required calculations."}]}