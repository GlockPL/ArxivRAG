{"title": "The Switch, the Ladder, and the Matrix: Models for Classifying AI Systems", "authors": ["Jakob M\u00f6kander", "Margi Sheth", "David S. Watson", "Luciano Floridi"], "abstract": "Organizations that design and deploy artificial intelligence (AI) systems increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. One major obstacle organizations face when attempting to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the purpose of implementing AI governance in practice. We find that attempts to classify AI systems found in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, data input, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organizations that design, deploy, or regulate AI systems with the conceptual tools needed to operationalise AI governance in practice.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) is increasingly reshaping societies and transforming economies (AlgorithmWatch, 2019). This is understandable: the delegation of tasks to AI systems holds great promise to improve efficiency, reduce costs, and enable new solutions to complex problems (Taddeo & Floridi, 2018). For example, AI systems can improve health outcomes (Grote & Berens, 2020; Schneider, 2019) and help mitigate environmental risks (Rolnick et al., 2019; Vinuesa et al., 2019). However, the use of AI systems is coupled with ethical challenges. A particular AI system may be poorly designed, leaving individuals and groups vulnerable to poor quality outcomes, bias and discrimination, and invasion of privacy (Leslie, 2019). Further, AI systems can enable human wrongdoing, reduce human control, and erode human self-determination (Tsamados et al., 2020). At the same time, fear and misplaced concerns could hamper the adoption of well-designed AI systems, thereby leading to significant social opportunity costs (Cookson, 2018). These and other similar ethical challenges cannot be ignored if one wishes to reap the benefits brought by AI systems.\nMany governments, research institutes, and NGOs have proposed ethical principles that provide normative guidance to organisations that design and deploy AI systems (Fjeld, 2020; Jobin et al., 2019).\u00b9 Although differing in terminology, these guidelines tend to converge on five principles: beneficence, non-maleficence, autonomy, justice, and explicability (Floridi & Cowls, 2019). In parallel, numerous organisations have adopted AI ethical principles of their own (de Laat, 2021). Notable examples include Google (2018), Microsoft (2019), IBM (Cutler et al., 2018), BMW Group (2020) and AstraZeneca (2020). Collectively, these efforts constitute a step in the right direction. However, the adoption of (and subsequent adherence to) AI ethical principles remains voluntary (Cath et al., 2018) and often unchecked. Moreover, the industry lacks both incentives and useful tools to translate abstract principles into verifiable criteria (Morley et al., 2020; Raji et al., 2020).\nLegislation has only recently begun to change this picture. The Artificial Intelligence Act (published by the European Commission on 24 April 2021) was the first comprehensive legislative framework for AI proposed by any major global economy. During the last year, many other countries and regions have followed suit. For example, the U.S. Senate and House are currently considering the Algorithmic Accountability act of 2022 (Office of U.S. Senator Ron Wyden, 2022). Yet whether and when this bill will pass into law remains uncertain."}, {"title": "2 Conceptualising 'Al systems'", "content": "At a high LoA, AI systems can be viewed as interactive, autonomous, and self-learning systems that can perform tasks that would otherwise require human intelligence and intervention to be executed successfully (Floridi et al., 2018). Still, to help AI practitioners understand whether an AI governance framework applies in a particular case, it must be complemented by a classification of AI systems at lower LoAs.\nNote that the term 'AI system' here indicates that we are talking about a class of systems that differs from others, as opposed to some kind of intelligence that differs from human intelligence (Kostopoulos, 2021). To capture this distinction, a wide range of terms like \u2018AI-based systems' (Gasser & Almeida, 2017; Saleiro et al., 2018), 'algorithmic systems' (Ananny & Crawford, 2018; Rahwan, 2018), 'automated decision-making systems' (AlgorithmWatch, 2019; Whittaker et al., 2018), and 'autonomous/intelligent systems' (Bryson & Winfield, 2017; IEEE SA, 2020) are often used interchangeably in the existing literature. For the sake of simplicity, we shall use the term 'AI system' consistently throughout this article. In doing so, we follow the (OECD, 2020) and the Alan Turing Institute (Leslie, 2019). However, nothing hinges on the choice.\nPrevious work has shown that AI systems can be classified according to several different dimensions. A distinction is often made between narrow and general AI systems (Russell et al., 2015). While narrow (or weak) AI refers to systems that can outperform humans on specific cognitive tasks, general (or strong) AI refers to systems that demonstrate human-level intelligence across a broad range of cognitive tasks (Goldstein, 2018). So defined, most current AI systems are narrow, although there is a growing body of research on transfer learning (Weiss et al., 2016) and meta-learning (Vanschoren, 2018) explicitly devoted to building models that generalize across tasks.\nAnother distinction is often made between different AI paradigms. While symbolic approaches are based on logic programming and symbol manipulation, adaptive methods rely on statistical techniques to solve specific problems without being explicitly programmed to do so (Russell & Norvig, 2015). This latter class includes machine learning algorithms, such as decision trees and deep neural networks (Samoili et al., 2020). However, the two approaches are not necessarily mutually exclusive. So-called hybrid architectures attempt to combine the large-scale learning abilities of neural networks with symbolic knowledge representation (Marcus, 2020).\nWithin the realm of adaptive approaches, practitioners distinguish between supervised-, unsupervised-, and reinforcement learning. Supervised learning involves inferring a relationship from inputs to outputs, e.g., classifying image labels from pixels or predicting economic demand from time-series data (Hastie et al., 2009). In contrast, unsupervised learning is about finding patterns (e.g., clusters or latent variables) hidden in collections of unlabelled data without any predetermined target (Frankish & Ramsey, 2014). Finally, reinforcement learning occurs when an agent attempts to maximise rewards by interacting within some structured environment (Sutton & Barto, 2018). Policies are gradually improved through repeated trials, as when AlphaGo (Silver et al., 2016) became the world's greatest master of the ancient Chinese game \u2018Go' by playing against itself millions of times.\nAI systems can also be classified with respect to the type of cognitive tasks they attempt to emulate (Feigenbaum & Feldman, 1963). Traditionally, AI research has focused on the following problem domains: perception, i.e., the ability to transform sensory inputs into usable information;\nreasoning, i.e., the capability to solve problems; knowledge, i.e., the ability to represent and understand the world; planning, i.e., the capability of setting and achieving goals; and communication, i.e., the ability to understand and produce language (Corea, 2019). An alternative and complementary way of classifying Al systems is based on the type of analytics they perform. These include descriptive analytics (what happened?), diagnostic analytics (why did something happen?), predictive analytics (what is going to happen?), prescriptive analytics (what should happen?), and automated analytics (performing actions) (Corea, 2019).\nHere, it is worth mentioning that in the proposed European regulation (the Artificial Intelligence Act, or AIA), AI systems are defined by the combination of the technical approaches that underpin a system and the cognitive tasks that the system is designed to perform (European Commission, 2021b). More specifically, in Annex 1 to the AIA, AI systems are defined as:\nsoftware that [i] is developed with one or more of the [following] techniques and approaches: (a) Machine learning approaches, [...]; (b) Logic- and knowledge-based approaches, [...]; and (c) Statistical approaches, [...], and [ii] can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with.\nThe definition of Al system provided in the AIA is broad by any standard.\u201d Further, the diverse nature of the technical approaches it encapsulates-and the wide range of applications they enable -shows that it is often necessary to consider LoA-dependent factors when classifying Al systems. In practice, Al systems are not isolated technologies but integrated into larger socio-technical systems that encompass organisations, people, infrastructures, and processes (Chopra & Singh, 2018). Put differently, information processing-from the collection of input data to the final decision or classification-typically consists of several interconnected (and often iterative) steps performed by both human operators and computational systems (Chen & Golan, 2016). Hence, the decisions made by Al systems are never just a reflection of their technical properties but also of the socio-technical environment surrounding their use (Eubanks, 2019).\nHowever, all technical artefacts (Al systems included) are value-laden insofar as they alter the cost-benefit ratio of the actions undertaken by humans and thus influence their decision-making (Danaher, 2012). Moreover, some Al systems can adapt their behaviour based on external inputs and evolve over time. This ability of Al systems 'to learn', i.e., to update continuously their internal decision-making logic, is one of the reasons why it is difficult to assign accountability when harm occurs (Burrell, 2016; Floridi, 2016). From a socio-technical perspective, it is this combination of relative autonomy and learning skills that underpin both beneficial and problematic uses of Al systems. To determine the risk level, or harm potential, it is often necessary to take several factors into account, including data access, i.e., the extent to which a system has complete and accurate knowledge about its environment; model stability, i.e., the extent to which a system may alter its own control structure to perform its task; and goal freedom, i.e., the extent to which the system's goals are known and stable.\nTo summarise, previous work in the field suggests that separating Al systems from other systems is a LoA-dependent, multi-variable problem, and, as we shall see in Sections 4-6, this problem can be approached in different ways. Before discussing these different approaches in greater detail, let us explore the needs of an effective classification system and establish criteria for the same."}, {"title": "3 Criteria for good classifications of Al systems", "content": "To implement Al governance in practice, the concept \u2018Al systems' must be operationalised for three main reasons. First, organisations are under constant pressure to innovate. By having a clearly defined material scope for their Al governance, organisations can take care not to unduly burden systems or projects from which no Al-specific risks arise (AIEIG, 2020).\nSecond, governance is most effective when rules and norms are applied fairly, transparently, and consistently (Hodges, 2015). Without a shared understanding of what constitutes Al systems within a specific organisation, systems, and processes (henceforth use cases) are likely to be subject to additional scrutiny only on an ad hoc basis. Such a procedure undermines the legitimacy of the Al governance framework in question and hamper its ability to systematically identify ethical risks.\nThird, and most importantly, not all ethical risks that organisations face stem from the use of Al systems. The inherent technical opacity of Al systems, for example, is often dwarfed by the opacity stemming from state secrecy or intellectual property rights (Burrell, 2016). Further, human decision-makers also make mistakes and produce discriminatory or inconsistent outcomes (Kahneman, 2011). As a result, organisations already have processes in place to oversee human decision-making and enforce commitments related to environmental sustainability, corporate social responsibility, and data management (e.g., in line with the General Data Protection Regulation). Therefore, a well-defined material scope for Al governance frameworks should complement or refine existing governance structures, not duplicate, or generate inconsistencies within them.\nWhat constitutes a good classification of Al systems? Drawing on best practices from the legal tradition (Baldwin & Cave, 1999) and attempts to create working definitions within the philosophy of science (Carnap, 1950), we argue that good classifications should be:\n1) Fit for purpose: the classification should help organisations demarcate the material scope of Al governance in ways that are neither over- nor underinclusive. A classification of Al systems is overinclusive when it includes systems or use cases that do not require additional oversight with respect to the normative goals of the Al governance framework in question (e.g., items already covered under data management and governance principles). In contrast, a classification is underinclusive when systems or use cases that pose the specific ethical risks defined in the Al governance framework are not included.\n2) Simple and clear: to be practicable, classifications must be easy to understand and apply in practice. This implies that Al practitioners should be able to determine, with little effort, how to classify a specific use case. Ideally, the classification should be based on conditions that are discrete, i.e., which are either met or not. Finally, usefulness also implies that people without expert knowledge should be able to apply the classification.\n3) Stable over time: to allow for predictable governance, classifications must be resilient. Since Al research is a rapidly progressing field, Al systems' technical features and potential applications are subject to constant change. Hence, good classifications should not be based on elements that are likely to become obsolete too quickly.\nTaken together, these criteria require that good models for classifying Al systems should help organisations specify to which systems or use cases their Al ethics principles apply, enable Al practitioners determine to which class of Al systems (if any) a particular use case belongs, and provide a stable basis for Al governance over time. These criteria also constitute the LoA on which we will evaluate the strengths and weaknesses of different models for classifying Al systems in Section 7.\nBefore proceeding, however, it is worth stressing that how Al systems are classified is an integral part of the design of the Al governance framework itself. For example, classifications that simply establish minimum thresholds for what constitutes Al systems demand flexible Al governance frameworks that can handle a wide range of use cases in proportionate and effective ways. In contrast, more fine-grained classifications afford layered Al governance frameworks that can specify both the risks and potential remedies associated with different use cases.\nBuilding on this insight, it is possible to cluster pairs of classifications of Al systems and Al governance frameworks into three types. Using mental models, we have chosen to call these the Switch, the Ladder, and the Matrix. These are (of course) ideal types. In practice, many organisations use a combination of these approaches to demarcate the material scope of their Al governance frameworks. Nevertheless, as we shall see, the Switch, the Ladder and the Matrix are based on different logics. Hence, there is merit in describing, exemplifying, and evaluating them separately."}, {"title": "4 The Switch", "content": "Most Al governance frameworks include working definitions of Al systems. The aim is to capture the most relevant features of the systems under investigation in a single sentence or paragraph, providing policymakers and Al practitioners with a simple rule of thumb for when to apply the Al governance framework. Some of these working definitions are, as we shall see, too abstract to help establish a material scope. However, others are also useful for the practical purpose of implementing Al governance frameworks without complicating matters. Consider the approach taken by the IEEE. In 2020, the IEEE published its Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-Being. For the purpose of this framework, the IEEE (2020) defined Al systems as follows:\n[An Al system is]10 a semi-autonomous or autonomous computer-controlled system programmed to carry out some task with or without limited human intervention capable of decision making by independent inference and successfully adapting to its context.\nThis working definition highlights two central features of Al systems: the level of autonomy and the ability to adapt. Of course, both are a matter of degree. In some cases, Al systems act with complete autonomy, whereas in other cases, Al systems only provide recommendations to a human operator who has the final say (Cummings, 2004). Although it is a simplification, the IEEE's working definition is based on features that are directly linked to the specific ethical concerns posed by Al systems. It also enables Al practitioners to determine what is not within the Al governance framework's scope. For example, this definition does not cover non-adaptive expert systems that structure information for the convenience of human decision-makers.\nThe logic behind the IEEE's working definition of Al systems can be abstracted into what we call the Switch. The Switch is a model for binary classifications: something either is or is not an Al system. To establish such a threshold, the Switch consists of one or more essential requirements. These requirements can concern technical features (i.e., referring to what the system is) and functional aspects (i.e., referring to what the system does). Under ideal circumstances, simple yes/no questions are enough to determine whether a specific system satisfies the relevant requirement(s). Essential requirements are individually necessary and jointly sufficient. For our purposes, this means that an Al governance framework should apply to any use case that meets the requirements constituting the Switch."}, {"title": "5 The Ladder", "content": "A central function of Al governance frameworks is to put mechanisms in place that ensure accountability for Al systems and their outcomes, both before and after their implementation (European Commission, 2019). Because Al systems may exacerbate existing risks and introduce new ones, Al governance is closely linked to risk management, i.e., processes that allow different risks to be identified, understood, and managed (Leslie, 2019). According to ISO 31000 risk management guidelines (ISO 31000 - Risk Management - Guidelines, 2018), risk is the effect of uncertainty on objectives. So understood, risks can be ethical, legal, or technical. Faced with constant pressures to reduce uncertainty (Luhmann, 2018), most large organisations already have risk management frameworks in place (Currie, 2019). Hence, the use of Al systems does not necessarily require a complete overhaul of existing governance structures but rather an awareness of how Al systems may increase, or complicate the detection of, risks as they manifest themselves in unfamiliar ways (Lee et al., 2020). In short, successfully implementing Al governance entails the adoption of adequate measures to mitigate the ethical risks posed by a specific system in a manner proportionate to the magnitude of those risks (AI HLEG, 2019).\nBuilding on this rationale, a growing number of proposals have advocated a risk-based approach to Al governance and hence to the classification of Al systems (Krafft et al., 2020).\u00b9\u00b2 Most notable amongst these proposals is the draft European regulation on Al, which takes an explicitly risk-based approach to Al governance (European Commission, 2020, 2021b). However, to exemplify the logic behind the approach, we will focus our analysis on the recommendation of the German Data Ethics Commission (DEK). The reason for this is that the DEK, as we shall see, outlined the risk-based approach to classifying Al systems in an outright and pedagogical manner.\nIn 2018, the DEK called for a risk-based approach to Al governance that would range from no regulation for the most innocuous Al systems to a complete ban for the most dangerous ones (DEK"}]}