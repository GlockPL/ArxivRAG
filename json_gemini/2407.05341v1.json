{"title": "The Switch, the Ladder, and the Matrix: Models for Classifying Al Systems", "authors": ["Jakob M\u00f6kander", "Margi Sheth", "David S. Watson", "Luciano Floridi"], "abstract": "Organizations that design and deploy artificial intelligence (AI) systems increasingly commit themselves to high-level, ethical principles. However, there still exists a gap between principles and practices in AI ethics. One major obstacle organizations face when attempting to operationalise AI Ethics is the lack of a well-defined material scope. Put differently, the question to which systems and processes AI ethics principles ought to apply remains unanswered. Of course, there exists no universally accepted definition of AI, and different systems pose different ethical challenges. Nevertheless, pragmatic problem-solving demands that things should be sorted so that their grouping will promote successful actions for some specific end. In this article, we review and compare previous attempts to classify AI systems for the purpose of implementing AI governance in practice. We find that attempts to classify AI systems found in previous literature use one of three mental models: the Switch, i.e., a binary approach according to which systems either are or are not considered AI systems depending on their characteristics; the Ladder, i.e., a risk-based approach that classifies systems according to the ethical risks they pose; and the Matrix, i.e., a multi-dimensional classification of systems that take various aspects into account, such as context, data input, and decision-model. Each of these models for classifying AI systems comes with its own set of strengths and weaknesses. By conceptualising different ways of classifying AI systems into simple mental models, we hope to provide organizations that design, deploy, or regulate AI systems with the conceptual tools needed to operationalise AI governance in practice.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) is increasingly reshaping societies and transforming economies (AlgorithmWatch, 2019). This is understandable: the delegation of tasks to Al systems holds great promise to improve efficiency, reduce costs, and enable new solutions to complex problems (Taddeo & Floridi, 2018). For example, Al systems can improve health outcomes (Grote & Berens, 2020; Schneider, 2019) and help mitigate environmental risks (Rolnick et al., 2019; Vinuesa et al., 2019). However, the use of Al systems is coupled with ethical challenges. A particular Al system may be poorly designed, leaving individuals and groups vulnerable to poor quality outcomes, bias and discrimination, and invasion of privacy (Leslie, 2019). Further, Al systems can enable human wrongdoing, reduce human control, and erode human self-determination (Tsamados et al., 2020). At the same time, fear and misplaced concerns could hamper the adoption of well-designed Al systems, thereby leading to significant social opportunity costs (Cookson, 2018). These and other similar ethical challenges cannot be ignored if one wishes to reap the benefits brought by Al systems.\nMany governments, research institutes, and NGOs have proposed ethical principles that provide normative guidance to organisations that design and deploy Al systems (Fjeld, 2020; Jobin et al., 2019).\u00b9 Although differing in terminology, these guidelines tend to converge on five principles: beneficence, non-maleficence, autonomy, justice, and explicability (Floridi & Cowls, 2019). In parallel, numerous organisations have adopted Al ethical principles of their own (de Laat, 2021). Notable examples include Google (2018), Microsoft (2019), IBM (Cutler et al., 2018), BMW Group (2020) and AstraZeneca (2020). Collectively, these efforts constitute a step in the right direction. However, the adoption of (and subsequent adherence to) Al ethical principles remains voluntary (Cath et al., 2018) and often unchecked. Moreover, the industry lacks both incentives and useful tools to translate abstract principles into verifiable criteria (Morley et al., 2020; Raji et al., 2020).\nLegislation has only recently begun to change this picture. The Artificial Intelligence Act (published by the European Commission on 24 April 2021) was the first comprehensive legislative framework for Al proposed by any major global economy. During the last year, many other countries and regions have followed suit. For example, the U.S. Senate and House are currently considering the Algorithmic Accountability act of 2022 (Office of U.S. Senator Ron Wyden, 2022). Yet whether and when this bill will pass into law remains uncertain."}, {"title": "", "content": "Outside the domain of hard governance, however, much has already been done to bridge the gap between principles and practice in Al ethics (Ib\u00e1\u00f1ez & Olmeda, 2021; Morley et al., 2021; Schiff et al., 2021b). Institutions have produced detailed assessment lists (AI HLEG, 2020; Reisman et al., 2018); researchers have developed translational tools like model cards (Mitchell et al., 2019) and datasheets (Gebru et al., 2018; Holland et al., 2018); industry-specific initiatives have drafted standardised protocols and reporting guidelines for the use of Al systems (Cruz Rivera et al., 2020; Liu et al., 2020), and private organisations are increasingly adopting ethics-based auditing procedures (Brundage et al., 2020; Deloitte, 2020; M\u00f6kander et al., 2021a; PwC, 2019; Sandvig et al., 2014). All these efforts serve the overarching purpose of enabling effective Al governance, i.e., to provide organisations with the tools needed to ensure that the Al systems they design are legal, ethical, and technically robust.\nHowever, the lack of a clear material scope\u00b3\u2014that is, to which technological systems the ethical and legal considerations may or may not apply-continues to make it difficult to implement and enforce Al governance frameworks in practice (Kritikos, 2019; Scherer, 2016). For example, in a recent industry case study (which was based on interviews with managers and software developers), we found that one of the main challenges organisations face when attempting to operationalise Al governance is the lack of procedures for demarcating the material scope of such initiatives (M\u00f6kander & Floridi, 2022). As a result, organisations are often unable to produce an inventory of the Al systems they develop or use. Despite such difficulties, however, both organisations that commit themselves to Al ethics principles and regulators that develop Al governance frameworks inevitably face the question 'to which systems and processes ought these additional layers of governance to apply?' (Aiken, 2021).\nOf course, there is no one way to demarcate the material scope of Al governance. Different Al systems pose different ethical and legal challenges (Oxborough et al., 2018). Moreover, Al systems are often embedded in larger socio-technical systems (Lauer, 2020; van de Poel, 2020) in which human- and machine-centric processes overlap and co-evolve (Di Maio, 2014; Tam et al., 2017). Admittedly, this ontological underdetermination is not unique to the problem of Al governance. Grouping things into neat categories seldom works, given the messy and continuous boundaries of the natural world (Smith, 2019). However, for the purpose of pragmatic inquiry and practical problem solving, things have to be sorted so that their grouping can promote successful actions for some specific end (Dewey, 1957). In short, every policy needs to define its material scope (Schuett, 2021).\u2074"}, {"title": "", "content": "In this article, we analyse the material scope of existing Al governance frameworks. Through a systematised literature review (Grant & Booth, 2009), we identify and compare previous attempts to classify Al systems for the practical purpose of operationalising corporate Al governance.\u2075 We find that they follow one of three approaches. According to the binary approach, systems either are or are not considered Al systems, depending on their intrinsic characteristics. According to the risk-based approach, systems are classified into different categories depending on the types of ethical risks they pose. Finally, according to the multi-dimensional approach, various aspects-such as context, data input, and decision-model type-need to be considered when classifying systems. Using mental models (Johnson-Laird, 1983), we call these approaches the Switch, the Ladder, and the Matrix, respectively. In the following sections, we discuss each of these models in detail and provide several concrete examples.\nBefore proceeding, three limitations help demarcate the scope of this article. First, we do not undertake any normative evaluation of different Al ethics guidelines. Previous research has pointed out that the apparent consensus around high-level principles may hide tensions, for example, in terms of priorities or how concepts like justice or fairness should be interpreted (Schiff et al., 2021a). As highlighted by (Whittlestone et al., 2019), different ethical principles sometimes give rise to tensions for which there are no fixed solutions. Therefore, organisations are expected to strike justifiable trade-offs within the limits of legal permissibility and operational viability. However, we assume normative clarity, i.e., that an organisation seeking to implement an Al governance framework has already committed itself to a coherent set of principles, e.g., to the EU or OECD ethical guidelines.\nSecond, for the purpose of our argument, we do not need to engage with questions concerning good intent. Ideally, organisations that commit to Al ethics principles also seek to live by these values. In practice, however, commitments to ethical principles can be undermined by unethical practices like 'ethics blue washing', i.e., making unsubstantiated claims about Al systems to appear more ethical than they are, or 'ethics lobbying', i.e., exploiting ethics to delay or avoid necessary legislation (Ferretti, 2021; Floridi, 2019). While important, these considerations lie outside the scope of this article. Instead, we take as our starting point the premise that good classification of Al systems may facilitate but never guarantee morally good outcomes.\nFinally, our review does not encompass abstract definitions of what 'artificial intelligence' really is. As is well-known, there exists no universally accepted definition of Al (Wang, 2019).\u2076"}, {"title": "", "content": "Discussions concerning the merits of different universal definitions of Al remain outside this article's scope. Instead, we focus on classifications that help organisations implement and enforce their Al governance frameworks. To quote John Dewey (Dewey, 1957), \u201cTo have an aim is to limit, select, concentrate, and group\". And we have an aim in mind: to unlock the potential of autonomous and self-learning systems to serve as a force for good while managing the ethical challenges they pose.\nTo describe and discuss different models for classifying Al systems, we rely on the method of levels of abstraction (Floridi, 2008). Abstraction is an appropriate method for analysing and understanding complex phenomena since it allows for the creation of concepts and objects at different levels of thinking and language (van Leeuwen, 2014). Only within a level of abstraction (LoA) can comparison between objects make sense. However, note that this is not a relativist approach: a question is always asked for a purpose, and, for that specific purpose, there is an appropriate LoA that can be compared to others in terms of \"fitting\" the purpose more or less successfully.\nThe remainder of this article is structured as follows. In Section 2, we build on previous work to showcase how Al systems can be classified in many different ways based on their technical features, the socio-technical contexts in which they are applied, and on the LoA for which a classification is sought. In Section 3, we argue that, to establish the material scope of Al governance, good classifications of Al systems should be fit for purpose, simple and clear, and stable over time. We then introduce three models for how to classify Al systems. In Sections 4-6, we describe and exemplify the Switch, the Ladder, and the Matrix, respectively. In Section 7, we evaluate these models according to the criteria set out in Section 3. Finally, in Section 8, we conclude by discussing how classifying Al systems is an LoA-dependent question. Hence, none of the models discussed in this article should be viewed as applicable absolutely, that is, independently of the choice of the LoA deemed to be most appropriate for the given purpose. Instead, we suggest that the models for classifying Al systems outlined in this article collectively constitute a useful set of tools for technology providers or regulators that wish to clarify the material scope of their Al governance frameworks."}, {"title": "Conceptualising 'Al systems'", "content": "At a high LoA, Al systems can be viewed as interactive, autonomous, and self-learning systems that can perform tasks that would otherwise require human intelligence and intervention to be executed successfully (Floridi et al., 2018). Still, to help Al practitioners understand whether an Al governance framework applies in a particular case, it must be complemented by a classification of Al systems at lower LoAs."}, {"title": "", "content": "Note that the term 'Al system' here indicates that we are talking about a class of systems that differs from others, as opposed to some kind of intelligence that differs from human intelligence (Kostopoulos, 2021). To capture this distinction, a wide range of terms like \u2018Al-based systems' (Gasser & Almeida, 2017; Saleiro et al., 2018), 'algorithmic systems' (Ananny & Crawford, 2018; Rahwan, 2018), 'automated decision-making systems' (AlgorithmWatch, 2019; Whittaker et al., 2018), and 'autonomous/intelligent systems' (Bryson & Winfield, 2017; IEEE SA, 2020) are often used interchangeably in the existing literature. For the sake of simplicity, we shall use the term 'Al system' consistently throughout this article. In doing so, we follow the (OECD, 2020) and the Alan Turing Institute (Leslie, 2019). However, nothing hinges on the choice.\nPrevious work has shown that Al systems can be classified according to several different dimensions. A distinction is often made between narrow and general Al systems (Russell et al., 2015). While narrow (or weak) Al refers to systems that can outperform humans on specific cognitive tasks, general (or strong) Al refers to systems that demonstrate human-level intelligence across a broad range of cognitive tasks (Goldstein, 2018). So defined, most current Al systems are narrow, although there is a growing body of research on transfer learning (Weiss et al., 2016) and meta-learning (Vanschoren, 2018) explicitly devoted to building models that generalize across tasks.\nAnother distinction is often made between different Al paradigms. While symbolic approaches are based on logic programming and symbol manipulation, adaptive methods rely on statistical techniques to solve specific problems without being explicitly programmed to do so (Russell & Norvig, 2015). This latter class includes machine learning algorithms, such as decision trees and deep neural networks (Samoili et al., 2020). However, the two approaches are not necessarily mutually exclusive. So-called hybrid architectures attempt to combine the large-scale learning abilities of neural networks with symbolic knowledge representation (Marcus, 2020).\nWithin the realm of adaptive approaches, practitioners distinguish between supervised-, unsupervised-, and reinforcement learning. Supervised learning involves inferring a relationship from inputs to outputs, e.g., classifying image labels from pixels or predicting economic demand from time-series data (Hastie et al., 2009). In contrast, unsupervised learning is about finding patterns (e.g., clusters or latent variables) hidden in collections of unlabelled data without any predetermined target (Frankish & Ramsey, 2014). Finally, reinforcement learning occurs when an agent attempts to maximise rewards by interacting within some structured environment (Sutton & Barto, 2018). Policies are gradually improved through repeated trials, as when AlphaGo (Silver et al., 2016) became the world's greatest master of the ancient Chinese game \u2018Go' by playing against itself millions of times.\nAl systems can also be classified with respect to the type of cognitive tasks they attempt to emulate (Feigenbaum & Feldman, 1963). Traditionally, Al research has focused on the following problem domains: perception, i.e., the ability to transform sensory inputs into usable information;"}, {"title": "", "content": "reasoning, i.e., the capability to solve problems; knowledge, i.e., the ability to represent and understand the world; planning, i.e., the capability of setting and achieving goals; and communication, i.e., the ability to understand and produce language (Corea, 2019). An alternative and complementary way of classifying Al systems is based on the type of analytics they perform. These include descriptive analytics (what happened?), diagnostic analytics (why did something happen?), predictive analytics (what is going to happen?), prescriptive analytics (what should happen?), and automated analytics (performing actions) (Corea, 2019).\nHere, it is worth mentioning that in the proposed European regulation (the Artificial Intelligence Act, or AIA), Al systems are defined by the combination of the technical approaches that underpin a system and the cognitive tasks that the system is designed to perform (European Commission, 2021b). More specifically, in Annex 1 to the AIA, Al systems are defined as:\nsoftware that [i] is developed with one or more of the [following] techniques and approaches: (a) Machine learning approaches, [...]; (b) Logic- and knowledge-based approaches, [...]; and (c) Statistical approaches, [...], and [ii] can, for a given set of human-defined objectives, generate outputs such as content, predictions, recommendations, or decisions influencing the environments they interact with.\nThe definition of Al system provided in the AIA is broad by any standard.\u201d Further, the diverse nature of the technical approaches it encapsulates-and the wide range of applications they enable -shows that it is often necessary to consider LoA-dependent factors when classifying Al systems. In practice, Al systems are not isolated technologies but integrated into larger socio-technical systems that encompass organisations, people, infrastructures, and processes (Chopra & Singh, 2018). Put differently, information processing-from the collection of input data to the final decision or classification-typically consists of several interconnected (and often iterative) steps performed by both human operators and computational systems (Chen & Golan, 2016). Hence, the decisions made by Al systems are never just a reflection of their technical properties but also of the socio-technical environment surrounding their use (Eubanks, 2019).\nHowever, all technical artefacts (Al systems included) are value-laden insofar as they alter the cost-benefit ratio of the actions undertaken by humans and thus influence their decision-making (Danaher, 2012). Moreover, some Al systems can adapt their behaviour based on external inputs and evolve over time. This ability of Al systems 'to learn', i.e., to update continuously their internal decision-making logic, is one of the reasons why it is difficult to assign accountability when harm occurs (Burrell, 2016; Floridi, 2016). From a socio-technical perspective, it is this combination of relative"}, {"title": "", "content": "autonomy and learning skills that underpin both beneficial and problematic uses of Al systems. To determine the risk level, or harm potential, it is often necessary to take several factors into account, including data access, i.e., the extent to which a system has complete and accurate knowledge about its environment; model stability, i.e., the extent to which a system may alter its own control structure to perform its task; and goal freedom, i.e., the extent to which the system's goals are known and stable.\nTo summarise, previous work in the field suggests that separating Al systems from other systems is a LoA-dependent, multi-variable problem, and, as we shall see in Sections 4-6, this problem can be approached in different ways. Before discussing these different approaches in greater detail, let us explore the needs of an effective classification system and establish criteria for the same."}, {"title": "Criteria for good classifications of Al systems", "content": "To implement Al governance in practice, the concept \u2018Al systems' must be operationalised for three main reasons. First, organisations are under constant pressure to innovate. By having a clearly defined material scope for their Al governance, organisations can take care not to unduly burden systems or projects from which no Al-specific risks arise (AIEIG, 2020).\nSecond, governance is most effective when rules and norms are applied fairly, transparently, and consistently (Hodges, 2015). Without a shared understanding of what constitutes Al systems within a specific organisation, systems, and processes (henceforth use cases) are likely to be subject to additional scrutiny only on an ad hoc basis. Such a procedure undermines the legitimacy of the Al governance framework in question and hamper its ability to systematically identify ethical risks.\nThird, and most importantly, not all ethical risks that organisations face stem from the use of Al systems. The inherent technical opacity of Al systems, for example, is often dwarfed by the opacity stemming from state secrecy or intellectual property rights (Burrell, 2016). Further, human decision-makers also make mistakes and produce discriminatory or inconsistent outcomes (Kahneman, 2011). As a result, organisations already have processes in place to oversee human decision-making and enforce commitments related to environmental sustainability, corporate social responsibility, and data management (e.g., in line with the General Data Protection Regulation). Therefore, a well-defined material scope for Al governance frameworks should complement or refine existing governance structures, not duplicate, or generate inconsistencies within them.\nWhat constitutes a good classification of Al systems? Drawing on best practices from the legal tradition (Baldwin & Cave, 1999) and attempts to create working definitions within the philosophy of science (Carnap, 1950), we argue that good classifications should be:\n1) Fit for purpose: the classification should help organisations demarcate the material scope of Al governance in ways that are neither over- nor underinclusive. A classification of Al systems"}, {"title": "", "content": "is overinclusive when it includes systems or use cases that do not require additional oversight with respect to the normative goals of the Al governance framework in question (e.g., items already covered under data management and governance principles). In contrast, a classification is underinclusive when systems or use cases that pose the specific ethical risks defined in the Al governance framework are not included.\n2) Simple and clear: to be practicable, classifications must be easy to understand and apply in practice. This implies that Al practitioners should be able to determine, with little effort, how to classify a specific use case. Ideally, the classification should be based on conditions that are discrete, i.e., which are either met or not. Finally, usefulness also implies that people without expert knowledge should be able to apply the classification.\n3) Stable over time: to allow for predictable governance, classifications must be resilient. Since Al research is a rapidly progressing field, Al systems' technical features and potential applications are subject to constant change. Hence, good classifications should not be based on elements that are likely to become obsolete too quickly.\nTaken together, these criteria require that good models for classifying Al systems should help organisations specify to which systems or use cases their Al ethics principles apply, enable Al practitioners determine to which class of Al systems (if any) a particular use case belongs, and provide a stable basis for Al governance over time. These criteria also constitute the LoA on which we will evaluate the strengths and weaknesses of different models for classifying Al systems in Section 7.\nBefore proceeding, however, it is worth stressing that how Al systems are classified is an integral part of the design of the Al governance framework itself. For example, classifications that simply establish minimum thresholds for what constitutes Al systems demand flexible Al governance frameworks that can handle a wide range of use cases in proportionate and effective ways. In contrast, more fine-grained classifications afford layered Al governance frameworks that can specify both the risks and potential remedies associated with different use cases.\nBuilding on this insight, it is possible to cluster pairs of classifications of Al systems and Al governance frameworks into three types. Using mental models, we have chosen to call these the Switch, the Ladder, and the Matrix. These are (of course) ideal types. In practice, many organisations use a combination of these approaches to demarcate the material scope of their Al governance frameworks. Nevertheless, as we shall see, the Switch, the Ladder and the Matrix are based on different logics. Hence, there is merit in describing, exemplifying, and evaluating them separately."}, {"title": "The Switch", "content": "Most Al governance frameworks include working definitions of Al systems. The aim is to capture the most relevant features of the systems under investigation in a single sentence or paragraph, providing policymakers and Al practitioners with a simple rule of thumb for when to apply the Al governance framework. Some of these working definitions are, as we shall see, too abstract to help establish a material scope. However, others are also useful for the practical purpose of implementing Al governance frameworks without complicating matters. Consider the approach taken by the IEEE. In 2020, the IEEE published its Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-Being. For the purpose of this framework, the IEEE (2020) defined Al systems as follows:\n[An Al system is]10 a semi-autonomous or autonomous computer-controlled system programmed to carry out some task with or without limited human intervention capable of decision making by independent inference and successfully adapting to its context.\nThis working definition highlights two central features of Al systems: the level of autonomy and the ability to adapt. Of course, both are a matter of degree. In some cases, Al systems act with complete autonomy, whereas in other cases, Al systems only provide recommendations to a human operator who has the final say (Cummings, 2004). Although it is a simplification, the IEEE's working definition is based on features that are directly linked to the specific ethical concerns posed by Al systems. It also enables Al practitioners to determine what is not within the Al governance framework's scope. For example, this definition does not cover non-adaptive expert systems that structure information for the convenience of human decision-makers.\nThe logic behind the IEEE's working definition of Al systems can be abstracted into what we call the Switch. The Switch is a model for binary classifications: something either is or is not an Al system. To establish such a threshold, the Switch consists of one or more essential requirements. These requirements can concern technical features (i.e., referring to what the system is) and functional aspects (i.e., referring to what the system does). Under ideal circumstances, simple yes/no questions are enough to determine whether a specific system satisfies the relevant requirement(s). Essential requirements are individually necessary and jointly sufficient. For our purposes, this means that an Al governance framework should apply to any use case that meets the requirements constituting the Switch."}, {"title": "The Ladder", "content": "A central function of Al governance frameworks is to put mechanisms in place that ensure accountability for Al systems and their outcomes, both before and after their implementation (European Commission, 2019). Because Al systems may exacerbate existing risks and introduce new ones, Al governance is closely linked to risk management, i.e., processes that allow different risks to be identified, understood, and managed (Leslie, 2019). According to ISO 31000 risk management guidelines (ISO 31000 - Risk Management - Guidelines, 2018), risk is the effect of uncertainty on objectives. So understood, risks can be ethical, legal, or technical. Faced with constant pressures to reduce uncertainty (Luhmann, 2018), most large organisations already have risk management frameworks in place (Currie, 2019). Hence, the use of Al systems does not necessarily require a complete overhaul of existing governance structures but rather an awareness of how Al systems may increase, or complicate the detection of, risks as they manifest themselves in unfamiliar ways (Lee et al., 2020). In short, successfully implementing Al governance entails the adoption of adequate measures to mitigate the ethical risks posed by a specific system in a manner proportionate to the magnitude of those risks (AI HLEG, 2019).\nBuilding on this rationale, a growing number of proposals have advocated a risk-based approach to Al governance and hence to the classification of Al systems (Krafft et al., 2020).\u00b9\u00b2 Most notable amongst these proposals is the draft European regulation on Al, which takes an explicitly risk-based approach to Al governance (European Commission, 2020, 2021b). However, to exemplify the logic behind the approach, we will focus our analysis on the recommendation of the German Data Ethics Commission (DEK). The reason for this is that the DEK, as we shall see, outlined the risk-based approach to classifying Al systems in an outright and pedagogical manner.\nIn 2018, the DEK called for a risk-based approach to Al governance that would range from no regulation for the most innocuous Al systems to a complete ban for the most dangerous ones (DEK,"}, {"title": "The Matrix", "content": "As discussed in Section 2, Al systems-although hard to define-typically share several characteristics. These characteristics include the ability to perceive the environment through input data, process information to interpret the data, make decisions based on the data, and, ultimately, act to achieve pre-defined goals (Samoili et al., 2020). Because Al systems are embedded in conditions that include and exceed them (Reddy et al., 2019), how they are used and their effects on society are not determined solely by the design of their internal decision-making models (M\u00f6kander & Axente, 2021). The extent to which an Al system's behaviour is perceived as ethical also depends on the input data, which could be incomplete or biased (Kim, 2017), and the context in which the system is applied (Lauer, 2020). Hence, the impact an Al system has on its environment is not always intuitive and may not be consistent over time. Rather, according to this perspective, every individual use case poses a unique set of ethical challenges.\nTo deal with this complexity, many organisations use multiple dimensions to classify Al systems. For example, the OECD (2022) has recently published a Framework for the Classification of Al"}, {"title": "", "content": "systems.14 The purpose this framework is to help organisations and policymakers assess and classify Al systems according to their potential ethical implications.\nAs a starting point, the OECD uses a socio-technical conceptualisation of Al systems. The OECD's framework thus requires organisations to analyse Al systems according to four key dimensions:\n1) Context: the socio-economic environment in which the Al system is deployed, notably the sector and the potential impact the system may have;\n2) Data and input: the data used by the Al model to build a representation of its environment;\n3) Al model: real-world processes in the system's internal environment, constituting the core of the Al system; and,\n4) Task and output: resulting actions taken by the system to influence its environment.\nEach of these four key dimensions is broken down into subdimensions (see Table 2, below).\nThe approach taken by the OECD represents a model to classify Al systems that we, in this article, have chosen to call the Matrix.15 In essence, the Matrix refers to classifications of Al systems that take several different dimensions into account to identify the specific ethical risks associated with a particular use case. Of course, the number and scope of these dimensions can vary between different"}, {"title": "Discussion", "content": "Let us now evaluate the three models introduced in this article according to the criteria for good classifications of Al systems stipulated in Section 3. A significant advantage of the Switch is that it is simple and clear, i.e., easy to grasp and use even for non-experts. This is because a few essential requirements are easy to bear in mind yet still provide a meaningful basis for identifying the systems to which a specific Al governance framework applies. At the same time, the Switch may not always be fit for purpose, since all binary approaches to classifying Al systems\u2014which attempt to draw a line across a continuous spectrum-struggle to strike the right balance between over- and under-inclusiveness. At first glance, it may appear better to err on the side of caution, i.e., to employ an overinclusive Switch. Doing so would increase the probability of identifying and managing high-risk use cases. At the same time, however, care must also be taken not to cause unjustifiable administrative"}, {"title": "", "content": "burdens.16 Finally, the extent to which a Switch is stable over time depends on which elements are included in the essential requirements. While requirements that refer to a particular design structure (e.g., neural networks) or a specific use case (e.g., facial recognition) are likely to change over time, essential requirements that refer to capabilities would be more permanent (Schuett, 2021).\nCompared with the Switch, the Ladder has several conceptual advantages. First, it builds on well-established mechanisms like risk assessments and risk labels. This procedural continuity helps organisations integrate Al governance frameworks into their existing governance structures and quality management processes (Raji et al., 2020). Second, the Ladder is fit for purpose insofar as it demands that organisations assess the specific ethical challenges associated with the technical systems they design and the use cases for which these systems are deployed. This is important given that-even where they are technically similar-the consequences of the decisions informed or determined by Al systems may differ considerably depending on the concrete setting in which they are applied (Krafft et al., 2020). Finally, the fact that it is technology-agnostic makes the Ladder a highly stable model for classifying Al systems. As a result, Al governance frameworks that classify Al systems according to the Ladder are well equipped to handle both rapid technological innovation and social change. However, these advantages come at a cost. It can be very challenging to assess all the ethical risks posed by a specific use case in practice. Moreover, it remains challenging to quantify the externalities that occur due to indirect causal chains over time (Dafoe, 2017).\nThe Matrix is fit for purpose insofar as it informs the policy considerations associated with different types of Al systems. Classifications based on the Matrix help also help organisations identify which precautionary measures are appropriate when designing or implementing a specific Al system. However, the Matrix is not a simple model for classifying Al systems. In other words, it does not help Al practitioners to determine with little effort whether a particular use case should be subjected to the Al governance framework. Instead, the Matrix front-loads both the administrative burden-and the process of ethical deliberation-to the initial stages of software development processes. Finally, classifications based on the Matrix may not be stable over time. Many subdimensions consist of variables whose categories or ranges may change due to future technological advances. Al research is a quickly moving landscape and speculating about what technical breakthroughs may occur is beyond the scope of this article. The point is that any model that attempts to exhaust all possible combinations of available technologies, on the one hand, and potential areas of application, on the other, will struggle to stay relevant and useful."}, {"title": "Conclusions", "content": "Al governance frameworks need to define their material scope if they are to make a difference in how organisations design and deploy Al systems. However, throughout this article, we have argued for a pragmatic approach: It is less important to define what Al is in abstract terms and more important to establish processes for identifying those systems or processes that require additional layers of governance. We have also shown that existing attempts to classify Al systems for the purpose of Al governance tend to follow one (or a combination) of the following three models: the Switch, the Ladder, or the Matrix.\nAll of these models come with their own set of strengths and weaknesses. The Switch is simple and clear-and can thus be easily communicated and applied. However, classifications of Al systems based on the Switch are often under- or overinclusive. In contrast, the Ladder provides a technology-neutral model for classifying Al systems that is fit for purpose and stable over time. However, although the risk-based approach, on which the Ladder is built, facilitates the integration of Al governance framework into existing governance structures, it also adds complexity that makes it more difficult to implement than the Switch. Finally, the Matrix offers the most comprehensive model for how to classify Al systems. As such, it is well-suited to inform policy decisions. However, classifications based on the Matrix add significant administrative burdens on organisations and Al practitioners and are also less stable over time.\nIn short, there is a three-way trade-off between how fit for purpose a model for classifying Al system is, how simple it is to apply, and how stable it is over time. When to apply classifications based on the Switch, the Ladder, or the Matrix thus remains a question to be evaluated locally, case by case. Moreover, it is important to remember that how best to classify Al systems depends on the design of the Al governance framework as a whole. Flexible and progressive Al governance frameworks must accompany simple classifications of Al systems. Highly detailed classifications of Al systems, on the other hand, allow for Al governance frameworks that proactively provide suggestions for how to manage the specific ethical challenges associated with a particular use case.\nIn this article, we have deliberately avoided any discussion about legal classifications of Al systems. However, it is worth stressing that the expectations on legal classifications of Al systems differ significantly from those intended for other purposes. Organisations that wish to improve their internal processes can afford some ambiguity concerning the material scope of their Al governance frameworks as long as they operate within the space of legal permissibility and operational viability. In contrast, courts need to be able to determine precisely which systems or processes are affected by a specific law. We therefore anticipate the debate about how to classify Al system to intensify as the focus of the discourse concerning Al ethics increasingly shifts from \u2018soft' ethics principles to the emergence of 'hard' regulations (Floridi, 2018)."}, {"title": "", "content": "The analysis in this article has shown that it may neither be feasible nor desirable to define the material scope of Al governance frameworks at a cross-sectorial, technology neutral LoA. Rather than attempting to define Al, regulators should consider the role information processing play (and ought to play) in shaping social systems. This is partly because both human decision-makers and Al systems come with their own sets of strengths and weaknesses (Baum, 2017), and partly because ethical tensions do not emerge from the use of specific technologies alone but can also be intrinsic to the decision-making task (Danks & London, 2017). As a result, overly simplistic measures such as introducing a blanket ban on specific technologies are often unhelpful. A more fruitful approach would be to understand how different information processing systems shape their environments and subject the decision-making process itself to appropriate and proportionate quality assurance and transparency obligations.\nTo conclude, we hope that this article may serve as a map for those who seek to design or implement Al governance frameworks in practice. Our hope rests on the old idea that new insights can be gained through a multiplicity of perspectives. Models help us organise information to grasp complex phenomena, and the many-model approach helps illuminate the blind spots inherent in each model (Page, 2018). By drawing on the models outlined in this article, we hope that technology providers and regulators will be better equipped to find the classifications of Al systems that work best for them.\nFinally, some may remain sceptical about whether it is necessary to classify Al systems at all. They may rightly point to the fact that Al systems are just a variation of other systems, with which they share many characteristics and ethical risks. However, human organisation is made possible by articulated conceptual representations of the world taken at a relatively high level of abstraction rather than the way the world really is. To quote John (Dewey, 1957) once more, \u201cA classification is not a bare transcript or duplicate of some finished and done-for arrangement pre-existing in nature. It is rather a repertory of weapons for attack upon the future and the unknown.\""}]}