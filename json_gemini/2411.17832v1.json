{"title": "SVGDreamer++: Advancing Editability and Diversity in Text-Guided SVG Generation", "authors": ["Ximing Xing", "Qian Yu", "Chuang Wang", "Haitao Zhou", "Jing Zhang", "Dong Xu"], "abstract": "Recently, text-guided scalable vector graphics (SVG) synthesis has demonstrated significant potential in domains such as iconography and sketching. However, SVGs generated from existing Text-to-SVG methods often lack editability and exhibit deficiencies in visual quality and diversity. In this paper, we propose a novel text-guided vector graphics synthesis method to address these limitations. To enhance the editability of output SVGs, we introduce a Hierarchical Image Vectorization (HIVE) framework that operates at the semantic object level and supervises the optimization of components within the vector object. This approach facilitates the decoupling of vector graphics into distinct objects and component levels. Our proposed HIVE algorithm, informed by image segmentation priors, not only ensures a more precise representation of vector graphics but also enables fine-grained editing capabilities within vector objects. To improve the diversity of output SVGs, we present a Vectorized Particle-based Score Distillation (VPSD) approach. VPSD addresses over-saturation issues in existing methods and enhances sample diversity. A pre-trained reward model is incorporated to re-weight vector particles, improving aesthetic appeal and enabling faster convergence. Additionally, we design a novel adaptive vector primitives control strategy, which allows for the dynamic adjustment of the number of primitives, thereby enhancing the presentation of graphic details. Extensive experiments validate the effectiveness of the proposed method, demonstrating its superiority over baseline methods in terms of editability, visual quality, and diversity. We also show that our new method supports up to six distinct vector styles, capable of generating high-quality vector assets suitable for stylized vector design and poster design.", "sections": [{"title": "1 INTRODUCTION", "content": "SCALABLE Vector Graphics (SVGs) represent visual con-cepts using geometric primitives such as B\u00e9zier curves, polygons, and lines. Due to their inherent nature, SVGs are highly suitable for visual design applications, such as posters and logos. Secondly, compared to raster images, vector images can maintain compact file sizes, making them more efficient for storage and transmission purposes. More importantly, vector images offer greater editability, allowing designers to easily select, modify, and compose elements. This attribute is particularly crucial in the design process, as it allows for seamless adjustments and creative exploration.\nIn recent years, there has been a growing interest in general vector graphics generation. Several optimization-based methods have been proposed [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], building upon the differentiable rasterizer DiffVG [13]. These methods, such as CLIPDraw [1] and VectorFusion [7], differ primarily in their approach to su-pervision. Some works [1, 2, 3, 4, 5, 6] combine the CLIP model [14] with DiffVG [13], using CLIP as a source of supervision. More recently, the significant progress achieved by Text-to-Image (T2I) diffusion models [15, 16, 17, 18, 19] has inspired the task of Text-to-SVGs. Both VectorFusion [7] and DiffSketcher [8] attempted to utilize T2I diffusion mod-els for supervision. These models make use of the high-"}, {"title": "2 RELATED WORK", "content": "Scalable Vector Graphics (SVGs) provide a declarative for-mat for visual concepts articulated through primitives. SVGs are extensively utilized in the design domain owing to their manipulable geometric composition, resolution in-dependence, and compact file size. One approach to gen-erating SVG content entails training a neural network to generate predefined SVG commands and attributes [25, 26, 27, 28, 29, 30, 31]. Neural networks designed for learning SVG representations typically include architectures such as RNNs [25, 28], VAEs [26], and Transformers [27, 29, 30, 31].\nThe training of these networks is heavily dependent on datasets in vector form. However, the limited availability of large-scale vector datasets significantly constrains their gen-eralization capability and their ability to synthesize intricate vector graphics. To date, the domain of vector graphics has not benefited from datasets of a scale comparable to Ima-geNet [32]. The existing datasets are predominantly focused on specific, narrow areas, such as monochromatic (black-and-white) vector icons [27, 33], emojis [34] and fonts [26]. Instead of directly learning an SVG generation network, an alternative method of vector synthesis is to optimize towards a matching image during evaluation time.\nLi et al. [13] introduce a differentiable rasterizer that bridges the vector graphics and raster image domains. While image generation methods that traditionally operate over vector graphics require a vector-based dataset, recent works has demonstrated the use of differentiable rasterizer"}, {"title": "3 THE SVGDREAMER APPROACH", "content": "In this section, we introduce SVGDreamer, an optimization-based method that creates a variety of vector graphics based on text prompts. A vector graphic is defined as a set of paths, {Pi}i=1n, and color attributes, {Ci}i=1n. Each path is com-prised of m control points, Pi = {Pj}j=1m = {(xj, yj)j}j=1m and one color attribute, Ci = {r, g, b, a}i. In this paper, we will optimize the SVG parameters to progressively evolve their initial state into a more refined and accurate graphical representation. We optimize an SVG by backpropagating gradients of rasterized images to the SVG path parameters, \u03b8 = {Pi, Ci}i=1n, utilizing a differentiable renderer [13] R(\u03b8).\nOur approach leverages the pre-trained text-to-image diffusion model prior to guide the differentiable renderer R and optimize the parametric graphic path \u03b8, resulting in the synthesis of vector graphs that match the description of the text prompt y. Our pipeline consists of two parts: semantic-driven image vectorization (Fig. 2) and SVG syn-thesis through VPSD optimization (Fig. 3). The first part is Semantic-driven Image VEctorization (SIVE), consisting of two stages: primitive initialization and semantic-aware optimization. We rethink the application of attention mech-anisms in synthesizing vector graphics. We extract the cross-attention maps corresponding to different objects in the diffusion model and apply it to initialize control points and consolidate object vectorization. This process allows us to decompose the foreground objects from the background. Consequently, the SIVE process generates vector objects which are independently editable. It separates vector objects by aggregating the curves that form them, which in turn simplifies the combination of vector graphics.\nIn section 3.2, we propose the Vectorized Particle-based Score Distillation (VPSD) to generate diverse high-quality text-matching vector graphics. VPSD is designed to model the distribution of vector path control points and colors for approximating the vector parameter distribution, thus obtaining vector results of diversity."}, {"title": "3.1 SIVE: Semantic-driven Image Vectorization", "content": "Image rasterization is a mature technique in computer graphics, while image vectorization, the reverse path of rasterization, remains a major challenge. Given an arbitrary input image, LIVE [37] recursively learns the visual con-cepts by adding new optimizable closed B\u00e9zier paths and"}, {"title": "3.1.1 Primitive Initialization", "content": "Vectorizing visual objects often involves assigning numer-ous paths, which leads to object-layer confusion in LIVE-based methods. To address this issue, we suggest organizing vector graphic elements semantically and assigning paths to objects based on their semantics. We initialize O groups of object-level control points according to the cross-attention map corresponding to different objects in the text prompt. And we represent them as the foreground MG, where i indicates the i-th token in the text prompt. Correspondingly, the rest will be treated as background. Such design allows us to represent the attention maps of background and fore-ground as,\nMBG = Inv(\u2211i=1O MFiG); MFG = softmax(QKTi)/\u221ad (1)\nwhere MBG indicates the attention map of the background. Inv(\u00b7) indicates the reverse operation of the sum of MFiG. MGiG indicates cross-attention score, where Ki indicates i-th token keys from text prompt, Q is pixel queries features, and d is the latent projection dimension of the keys and queries.\nThen, inspired by DiffSketcher [8], we normalize the attention maps using softmax and treat it as a distribution map to sample m positions for the first control point pj=1 of each B\u00e9zier curve. The other control points ({pi}i=2m) are sampled within a small radius (0.05% of image size) around pj=1 to define the initial set of paths. In the following section, we will explain how to consolidate object semantics during the synthesis of vector graphics using the mask."}, {"title": "3.1.2 Semantic-aware Optimization", "content": "In this stage, we utilize an attention-based mask loss to separately optimize the objects in the foreground and back-ground. This ensures that control points remain within their respective regions, aiding in object decomposition. Namely, the hierarchy only exists within the designated object and does not get mixed up with other objects. This strategy fuels the permutations and combinations between objects that form different vector graphics, and enhances the editability of the objects themselves.\nSpecifically, we convert the attention map obtained during the initialization stage into masks M = {{MFGiG}i=1O, MBG}, O foregrounds and one background mask in total. This is accomplished by assigning the atten-tion score a value of 1 if it exceeds the predefined threshold,"}, {"title": "3.2 VPSD: Vectorized Particle-based Score Distillation", "content": "The Diversity of SVG Generation. While vectorizing a rasterized diffusion sample is lossy, recent techniques [7, 8] have identified the SDS loss [20] as beneficial for our task of generating vector graphics. To synthesize a vector image that matches a given text prompt y, they directly optimize the parameters \u03b8 = {Pi, Ci}i=1n of a differentiable rasterizer R(\u03b8) via SDS loss. At each iteration, the differentiable rasterizer is used to render a raster image x = R(\u03b8), which is then data augmented to obtain xa. Then, the pretrained latent diffusion model (LDM) \u03f5\u03c6 uses a VAE encoder [58] to encode xa into a latent representation z = E(xa), where z\u2208 R(H/f)\u00d7(W/f)\u00d74 and f is the VAE encoder downsample factor. Finally, the gradient of SDS is estimated by,\nVoLVPSD(\u03c6, x = R(\u03b8)) \u225c\nEt,\u03f5,a w(t)(\u03f5\u03c6(zt; y, t) \u2212 \u03f5)\u2202z\u2202xa\u2202xa\u2202\u03b8 (3)\nwhere w(t) is the weighting function. And noised to form zt = atxa + \u03c3t\u03f5.\nUnfortunately, SDS-based methods often suffer from is-sues such as shape over-smoothing, color over-saturation, limited diversity in results, and slow convergence in syn-thesis results [7, 8, 20, 21]. Inspired by the principled vari-ational score distillation framework [56], we propose vec-torized particle-based score distillation (VPSD) to address the aforementioned issues. Instead of modeling SVGs as a set of control points and corresponding colors like SDS, we model SVGs as the distributions of control points and colors respectively. In principle, given a text prompt y, there exists a probabilistic distribution \u03bc of all possible vector"}, {"title": "4 SVGDREAMER++", "content": "In this section, we introduce the enhanced SVGDreamer++ approach. The original SVGDreamer exhibits two primary limitations: (1) it may produce vector graphics with in-accurate boundaries, and its editability is limited to the object level. (2) The number of primitives used to com-pose a vector graphic must be preset and remain fixed during optimization, which can lead to slow convergence or insufficient detail in the resultant vector graphics. To address these limitations, we introduce two improvements in SVGDreamer++. First, we propose a Hierarchical Image VEctorization (HIVE), an advanced version of SIVE, to enhance the quality of boundaries in vector graphics and extend the model's editability to both object-level and part-level (Sec. 4.1). Second, we design an adaptive vector prim-itive control strategy that dynamically adjusts the number of primitives during optimization, leading to faster conver-gence and improved visual quality (Sec. 4.2). The remaining components of SVGDreamer++ are identical to those of the original SVGDreamer."}, {"title": "4.1 HIVE: Hierarchical Image Vectorization", "content": "In the SVGDreamer framework, SIVE is utilized to segre-gate foreground objects from the background using masks derived from the attention maps of a pre-trained diffusion model, as detailed in Sec. 3.1.2. However, these attention-based masks can introduce inaccuracies in boundaries dur-ing the optimization process. This issue stems from the res-olution limitations of the attention features extracted from the diffusion model's cross-attention layers. As illustrated in Fig. 5, this limitation becomes evident when the resolution of the attention map is significantly lower than that of the target image. Furthermore, as SIVE operates at the object level, it lacks the capability to manage local or fine-grained elements, such as the helmet of a space suit.\nIn SVGDreamer++, we introduce a Hierarchical Image VEctorization (HIVE) approach to enhance both the quality and editability of the generated vector graphics. The core distinction between HIVE and SIVE lies in the method of generating masks, which are employed as guidance during image vectorization. HIVE utilizes segmentation priors to obtain masks, ensuring both accurate boundaries and fine-grained control. The pipeline is shown in Fig. 4. Specifically, HIVE adopts the primitive initialization method from SIVE, as discussed in Sec. 3.1.1. Subsequently, the user selects O nouns from the text prompt as the trigger condition for Grounded-SAM [59] to generate O object-level masks. Then, the coordinates of control points within each object are used as conditions to drive the SAM model [24] to produce F masks, corresponding to fine-grained details. This results in two sets of masks: object-level masks {Mio}i=1O and fine-grained masks {Mxi}i=1F for individual object regions. These masks supervise the image vectorization process, as delineated in Eq. 8.\nCLHIVE = \u2211i(MoiI \u2212 Mioxi)2\nO\u2211i=1\n+ \u2211\u2211(MxijI \u2212 Mxoixj)2, (8)\ni xj\nwhere I and Io are the target image and the i-th object, {Mio}i=1O is the set of object-level masks, with Mio being the i-th mask predicted by Grounded-SAM, Mxoij is the j-th fine-grained mask of the i-th object predicted by SAM, xi = R(\u03b8i) is the i-th rendering.\nBy employing this new mask generation strategy, HIVE can effectively reduce vector path interweaving and cou-pling across objects or parts, significantly enhancing the visual quality and editability of the vector graphics."}, {"title": "4.2 Adaptive Vector Primitives Control", "content": "The number of paths significantly affects the visual quality of generated SVGs. Intuitively, complex content, such as a zebra, requires more paths than simple content, like an apple. More paths often lead to better results by captur-ing more delicate details. However, an insufficient num-ber of paths can lead to geometric feature degradation, such as missing details, while an excessive number can slow down the optimization process. Consequently, setting a proper number of paths is a challenging task, and this problem remains largely unexplored. Here we introduce a novel Adaptive Vector Primitive Control strategy that can dynamically adjust the number of primitives during optimization. The core idea is to eliminate redundant paths and add additional paths in regions with geometric fea-ture degradation. As depicted in Fig. 6, we identify two scenarios that necessitate additional paths. In regions with complex structures, a path might cover an adequate area but be too simplistic to accurately represent the structure (termed \u201cOver-Represented\u201d). In another scenario, a path might cover an insufficient area to represent the structure adequately (\u201cUnder-Represented\u201d). Both cases can lead to geometric degradation, thus requiring more paths. Our Adaptive Vector Primitives Control algorithm is detailed in Algorithm 1. This module is designed as a plug-and-play component, capable of seamless integration into image vec-torization algorithms that utilize gradient optimization.\nThe algorithm includes two key components: path prun-ing (Lines 5 to 7) and path control (Lines 8 to 14). Path"}, {"title": "5 VECTOR PRIMITIVES REPRESENTATION", "content": "In addition to text prompts, we provide a variety of vector representations for style control. These vector representa-tions are achieved by limiting primitive types and their parameters. Users can control the art style by modifying the input text or by constraining the set of primitives and parameters. Unlike existing text-to-image and text-to-SVG methods, we provide users a variety of flexible ways to build vector graphics, opening up potential in the field of generative vector design. We explore six settings:\n1) Iconography is the most common SVG style, consisting of several paths and their fill colors. This style allows for a wide range of compositions while maintaining a minimal-istic expression. We utilize closed-form B\u00e9zier curves with trainable control points and fill colors (including opacity).\n2) Pixel Art is a widely used style that draws inspiration from the low-resolution, 8-bit graphics characteristic of early video games. To emulate this style, we employ square SVG polygons with variable fill colors and opacity, enabling precise control over the pixelated aesthetic.\n3) Low-Poly Art involves the deliberate cutting and ar-rangement of simple geometric shapes according to the modeling principles of objects. To achieve this style, we utilize square SVG polygons with trainable control points and variable fill colors (including opacity), which enables precise control over the composition and aesthetic of the low-poly representation.\n4) Painting Style vector art seeks to replicate a painter's brush strokes within the vector domain. This is achieved through the use of open-form B\u00e9zier curves with trainable control points, variable stroke color (including opacity), and adjustable stroke width, allowing for precise emulation of traditional painting techniques.\n5) Sketching employs black strokes to delineate objects, serving as a method to convey information with minimalis-tic expression. To replicate this style, we utilize open-form B\u00e9zier curves with trainable control points and adjustable opacity, allowing for precise control over the sketch-like appearance.\n6) Ink and Wash Painting is a traditional Chinese art form characterized by the use of varying concentrations of black ink to create nuanced and expressive imagery. To emulate this style in our work, we employ open-form B\u00e9zier curves with trainable control points, adjustable opacity, and variable stroke widths, enabling precise control over the rendering of ink-like effects."}, {"title": "6 EXPERIMENTS", "content": "Overview. In this section, we first explain the dataset and evaluation metrics we used, as well as the implementa-tion details of our experiments. We then provide exper-imental results to demonstrate the effectiveness of our proposed method. Specifically, Section 6.1 offers a quali-tative (Sec. 6.1.1) and quantitative (Sec. 6.1.2) comparison with state-of-the-art methods, accompanied by a flowchart (Sec. 6.1.3) illustrating the SVG editing process. Section 6.2 presents ablation studies and analytical results for deeper insights. Section 6.3 demonstrates the practical applications of the proposed SVGDreamer++ in vector design, particu-larly in designing posters (Sec. 6.3.1) and generating vector assets (Sec. 6.3.2).\nDataset. Current text-to-SVG approaches perform well on prompts with a single simple portrait object but struggle with prompts that include environmental surroundings or multiple objects due to inaccurate 2D supervision. To eval-uate these methods, we design three prompt sets: Single object, Single object with surroundings, and Multiple ob-jects. The Single object set establishes a baseline, while the other two sets increase complexity. We then use these three prompt sets to conduct a thorough evaluation of text-to-SVG methods.\nEvaluation Metrics. To evaluate our proposed method and baseline methods, we employed six quantitative indicators across four dimensions: (1) Visual quality of the generated SVGs, assessed by FID (Fr\u00e9chet Inception Distance) [60]; (2) Fidelity of color representation, evaluated by PSNR (Peak Signal-to-Noise Ratio) [61]; (3) Alignment with the input text prompt, assessed by CLIP score [14] and BLIP score [62], and (4) Aesthetic appeal of the generated SVGs, measured by Aesthetic score [63] and HPS (Human Prefer-ence Score) [64].\nImplementation Details. In our implementation, we lever-age the pre-trained Stable Diffusion [16]. For SVG parameter optimization \u03b8 = {Pi, Ci}i=1n, we use the Adam optimizer with settings \u03b21 = 0.9, \u03b22 = 0.9, \u03f5 = 1e-6. We use a learning rate warm-up strategy where the control point learning rate starts at 0.01 and increases to 0.9 over the first 50 iterations, followed by an exponential decay from 0.8 to 0.4 over the subsequent 650 iterations, totaling 700 iterations. The color learning rate is set to 0.1 and the stroke width learning rate to 0.01. For the training of LoRA [22] parameters, We adopt the AdamW optimizer with parame-ters \u03b21 = 0.9, \u03b22 = 0.999, \u03f5 = 1e \u2212 10, and lr = 1e \u2212 5. In the HIVE experiment, to counteract the vacant background regions caused by segmentation, we integrate the LaMa model [65] to fill these areas before processing with HIVE for vectorization. In most experiments, we set the particle"}, {"title": "6.1.3 Editability", "content": "With our newly proposed HIVE module (Sec. 4.1), SVG-Dreamer++ is capable of generating high-quality vector graphics that are editable at both the object-level and part-level. As shown in Fig. 9, this capability empowers users to efficiently reuse synthesized vector elements and create new vector compositions. Two SVGs generated by SVG-Dreamer++ (SVG1 and SVG2), can be decoupled at the object level into components including BG1, FG11, BG2, FG2 and FG3. These foreground objects and background elements can be recombined to form new SVGs, as demon-strated in the fourth box of the Fig. 9 (BG1+FG2+FG3, FG1+BG2, BG1+FG4 and BG2+FG5). Furthermore, as shown in the fifth box, local elements can also be editied. For example, the cloak of the character can be changed from a black one to a red one, while his weapon is changed from a lightsaber to a golden longsword. Finally, after editing, we put the character back into the background. In summary, this example demonstrates that the results generated by SVGDreamer++ are editable at both the object level and the local level."}, {"title": "6.2 Ablation Study", "content": "Figure 10 presents a comparative analysis of the three vectorization techniques. As illustrated in the 3rd row of Fig. 10, LIVE [37] encounters considerable challenges in accurately capturing and differentiating discrete subject ele-ments within images. This frequently results in the overlay of identical paths across varying visual subjects, such as fonts, astronauts, and backgrounds, leading to significant path confusion. When addressing complex vector graphic tasks that involve multiple paths, LIVE often produces"}, {"title": "4.1 HIVE v.s. SIVE v.s. LIVE", "content": "hierarchical path overlays across different objects. This in-troduces additional complexity into SVG representations, thereby complicating subsequent editing processes. The 2nd row of Fig. 10 demonstrates that SIVE (Sec. 3.1) assigns paths to vector objects, facilitating object-level vectorization. However, the limitations in resolution of cross-attention maps contribute to inaccuracies in boundary delineation, with vector boundaries for elements such as astronauts and planets occasionally blending into the background. The HIVE (Sec. 4.1) methodology proposed in this paper effec-tively mitigates these limitations by offering precise super-visory signals for vector objects throughout the optimization process. Moreover, HIVE provides advanced support for both object-level and part-level vectorization, thereby en-abling detailed local editing of vector objects and extending the capabilities beyond those of previous approaches."}, {"title": "6.2.2 The Impact of Adaptive Vector Primitives Control", "content": "As shown in Fig. 11. We input text prompt \u201cA tree, color palette: light pink and purple. minimalism. flat 2d\" into the Latent Diffusion Model (LDM) [16] to generate raster images"}, {"title": "6.2.3 VPSD v.s. LSDS v.s. ASDS", "content": "The development of text-to-SVG [7, 8] was inspired by DreamFusion [20], but the resulting vector graphics have limited quality and exhibit a similar over-smoothness as the"}, {"title": "6.2.4 The Impact of the Number of Vector Particles", "content": "We investigate the impact of the number of particles on the generated results. We vary the number of particles in 1, 4, 8, 16 and analyze how this variation affects the outcomes. As shown in Fig. 13, the diversity of the generated results is slightly larger as the number of particles increases. Mean-while, the quality of generated results is not significantly affected by the number of particles. Considering the high computation overhead associated with optimizing vector primitive representations and the limitations imposed by available computation resources, we limit our testing to a maximum of 6 particles."}, {"title": "6.2.5 The Impact of Reward Feedback Learning (ReFL)", "content": "In [56], only selected particles update the LoRA network in each iteration. However, this approach neglects the learning progression of LoRA networks, which are used to represent variational distributions. These networks typically require numerous iterations to approximate the optimal distribu-tion, resulting in slow convergence. Unfortunately, the ran-domness introduced by particle initialization can lead to early learning of sub-optimal particles, which adversely affects the final convergence result. In VPSD, we introduce a Reward Feedback Learning (ReFL) method. This method leverages a pre-trained reward model [23] to assign reward scores to samples collected from LoRA model. Then LoRA model subsequently updates from these reweighted sam-ples. As indicated in Tab. 2, this led to a significant reduction"}, {"title": "6.2.6 SVG Diversity Generation", "content": "As depicted in Fig. 15, we offer a diverse array of vector representations to facilitate style control, extending beyond mere text prompts to include constraints on primitive types and their parameters. In Sec. 5, we delineate six vector styles, each characterized by unique combinations of vector primitives. This diverse definition enables a more flexible and precise representation of styles in the domain of vector graphics. Users can manipulate the artistic style by adjust-ing the input text or restricting the set of primitives and their associated parameters. Distinct from existing text-to-image and text-to-SVG methods [1, 4, 8, 11, 12], our ap-proach affords users flexible and varied means to generate vector graphics, thereby broadening the scope of gener-ative vector design. Notably, VF [7] initially offers three vector styles-iconography, sketch, and pixel-art. We have expanded this repertoire to six by incorporating ink-and-wash, low-polygon, and painting styles."}, {"title": "6.3 Applications of SVGDreamer++", "content": "A poster is a large sheet used for advertising events, films, or conveying messages to people. It usually contains text"}, {"title": "6.3.2 Creative Vector Assets", "content": "The creation of vector assets is a time-intensive process for designers, and the acquisition of these assets is often costly due to intellectual property protections. We investigate the application of SVGDreamer++ in generating vector assets across various styles. The proposed SVGDreamer++ frame-work is capable of generating vector graphics at both the object level and part-level, offering exceptional editability. Consequently, vector objects are extracted from the nouns identified in the text descriptions to compose vector graphic assets. As illustrated in Fig. 17, all graphical elements in the four examples are generated using SVGDreamer++. We present a curated collection of vector assets encompass-ing four distinct styles: character portraits, graphic por-traits, video game items, and vector stickers. In contrast to diffusion-based [15, 16, 17, 18, 48] raster objects, vector objects generated by our approach support localized editing, are not constrained by resolution, and feature a compact file representation. The vector assets generated by SVG-Dreamer++ are characterized by their exceptional versatility and precision, making them particularly suitable for com-plex design tasks that require scalable and editable graphics. These vector elements can be seamlessly integrated into design applications, such as web and advertising design, thereby enhancing the efficiency and creativity of the design process."}, {"title": "7 CONCLUSION & DISCUSSION", "content": "In this work, we have introduced SVGDreamer++, an in-novative model for text-guided vector graphics synthesis. SVGDreamer++ improves on the previous state-of-the-art SVGDreamer in two ways. Firstly, we introduce an ad-vanced Hierarchical Image VEctorization algorithm, termed HIVE. This algorithm integrates an image segmentation prior to ensure more precise vectorization supervision, thereby rectifying the inaccurate boundaries observed in vector objects generated by SIVE. Secondly, we propose a novel Adaptive Vector Primitives Control algorithm during the optimization phase to address and improve regions with deficient geometric features. These empower our model to generate vector graphics with high editability, superior visual quality, and notable diversity. SVGDreamer++ is ex-pected to significantly advance the application of text-to-SVG models in the design field.\nLimitations. The editability of our method, which depends on the text-to-image (T2I) model used, is currently limited. However, future advancements in T2I diffusion models could enhance the decomposition capabilities of our ap-proach, thereby extending its editability. Moreover, explor-ing ways to automatically determine the number of control points at the SIVE object level is valuable."}]}