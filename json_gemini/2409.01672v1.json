{"title": "Enhancing Fine-Grained Visual Recognition in the Low-Data Regime Through Feature Magnitude Regularization", "authors": ["Avraham Chapman", "Haiming Xu", "Lingqiao Liu"], "abstract": "Training a fine-grained image recognition model with limited data presents a significant challenge, as the subtle differences between categories may not be easily discernible amidst distracting noise patterns. One commonly employed strategy is to leverage pretrained neural networks, which can generate effective feature representations for constructing an image classification model with a restricted dataset. However, these pretrained neural networks are typically trained for different tasks than the fine-grained visual recognition (FGVR) task at hand, which can lead to the extraction of less relevant features. Moreover, in the context of building FGVR models with limited data, these irrelevant features can dominate the training process, overshadowing more useful, generalizable discriminative features. Our research has identified a surprisingly simple solution to this challenge: we introduce a regularization technique to ensure that the magnitudes of the extracted features are evenly distributed. This regularization is achieved by maximizing the uniformity of feature magnitude distribution, measured through the entropy of the normalized features. The motivation behind this regularization is to remove bias in feature magnitudes from pretrained models, where some features may be more prominent and, consequently, more likely to be used for classification. Additionally, we have developed a dynamic weighting mechanism to adjust the strength of this regularization throughout the learning process. Despite its apparent simplicity, our approach has demonstrated significant performance improvements across various fine-grained visual recognition datasets.", "sections": [{"title": "1. Introduction", "content": "Fine-grained visual recognition (FGVR) involves the classification of a large number of groups that differ only subtly from each other. Differentiating these classes often requires sensitivity to specific features in small regions of an image. For a bird, the difference between the two species may lie in the subtle differences between their beaks or feather-tips [34]. Training a model to discover these features is further complicated by the fact that there is often a dearth of data available for training. The more specialized the dataset, the more difficult it is to find the expertise to label the images [40, 31, 33]. A model trained on a limited FGVR dataset can often be sidetracked by irrelevant details, such as background features in an image.\nMany foundational vision models exist and have demonstrated success across various downstream tasks [20, 2, 5, 4, 13]. When applied to FGVR tasks, these models can deliver reasonable results even though they are not specifically tailored for FGVR [30, 35, 18]. The challenge arises because these models, while robust, are not optimized for FGVR\u2019s unique requirements. Consequently, when these models are fine-tuned using a limited number of labeled samples for FGVR, there is a risk that the most transferable and distinctive features crucial for FGVR may not be effectively emphasized. Additionally, there is a concern that these models may inherit bias from their pretraining phase. One particular form of bias manifests in the feature magnitudes, where certain dimensions of the feature space are more likely to exhibit significant values. Consequently, these dimensions are more likely to be utilized if they exhibit discriminative patterns within the training dataset. However, when dealing with a small training dataset, the identified discriminative features may not generalize well to unseen test images. For instance, these features may overly focus on background regions, which is not conducive to accurate FGVR, as presented in Figure 1.\nIn this study, we propose a compellingly straightforward approach to enhance fine-grained image recognition when working with sparse data. Our method introduces a regularization strategy known as Feature Magnitude Regularization (FMR), which aims to equalize the distribution of feature magnitudes across the model. By computing the entropy of normalized features and striving to maximize this entropy, we ensure a more balanced feature representation. This approach is specifically designed to encourage an equitable importance among all features during the training process, thus mitigating potential biases in feature magnitudes. An important consideration when applying this regularization is how to adjust its strength effectively. Instead of employing a fixed weight for the regularization, we have developed a dynamic weighting mechanism that adapts the strength of regularization as the learning process unfolds. To achieve this, we set the regularization strength in proportion to the disparity between the current entropy of feature magnitude distribution and its maximum value. This encourages stronger regularization when the feature magnitude distribution deviates significantly from uniformity, ensuring that our approach remains effective throughout the whole training procedure.\nWe performed extensive experimental evaluations on several popular fine-grained visual recognition benchmarks. Our experiments clearly demonstrate that the proposed method yields substantial improvements over conventional fine-tuning techniques when working with limited data. Furthermore, our approach exhibits favorable performance compared to other methods specifically designed to enhance the fine-tuning of pretrained models with a limited amount of data."}, {"title": "2. Related Works", "content": "FGVR is concerned with the classification of multiple fine-subcategories of a larger group. There have been attempts to address this problem as far back as 25 years ago [37, 17, 34]. The advent of deep learning [21] provided a powerful tool to address this problem.\nApproaches tend to be grouped into the following two areas: Recognition by Localization-Classification Subnetworks [29, 36] and Recognition by End-to-End Feature Encoding [25, 39, 24].\nRecognition by Localization-Classification Subnetworks work [29, 36] by attempting to locate key parts of an image, such as a bird's beak, and extracting feature vectors describing each part. These feature vectors, along with feature vectors describing global aspects of the image, are then passed to sub-networks, whose job is to perform classification. Examples include R-CNN [12], FCN [26], and Faster R-CNN [28]. Another more recent example is SAM-Bilinear [30], which uses a self-boosting mechanism to build up an understanding of which regions of an image are relevant for the FGVR task.\nRecognition by End-to-End Feature Encoding is about guiding convolutional neural networks (CNNs) to learn features from an input that provides enough discriminative information to allow for distinguishing subtle differences between similar classes. Methods of achieving this include higher-order feature interactions and novel loss functions. Higher-order feature interaction-based methods involve mining higher-order feature statistics from deeper convolution layers to extract useful descriptions of object parts [25, 39]. Bilinear Convolutional Neural Networks (B-CNNs) [24] use two CNNs whose outputs at each location combined to form a bilinear feature representation.\nAnother popular method for boosting FGVR performance is through the introduction of loss functions. These functions may attempt to reduce the confidence of predictions by the model [9] or to learn correlations between feature regions [32, 11, 42]. In addition, there are techniques like MC-Loss [3], which attempt to locate harder classes and boost their gradients to encourage learning of the harder classes. Finally, there are losses that attempt to do a better job exploiting the knowledge already contained in a pre-trained model. L2-SP [22] uses a simple L2 penalty to encourage similarity between the final weights"}, {"title": "3. Method", "content": "Our method tackles the challenge of training a finegrained image classification model when the available dataset has a limited number of training samples.\nThe overall structure of our technique is depicted in Figure 2. As illustrated, our approach involves the introduction of an extra loss term alongside the standard cross-entropy loss typically used in supervised learning. After extracting features from the network backbone \u03a8, we begin by applying softmax normalization to these features, transforming them into a representation resembling a probability distribution. Subsequently, we calculate the negative entropy of this distribution-like representation and employ it as a form of regularization loss, using a dynamically calculated weighting. The following sub-sections describe our proposed network and go into more detail about the feature magnitude regularization (FMR) training process."}, {"title": "3.2. Feature Magnitude Bias", "content": "Utilizing pretrained models has become a common practice when developing image classification systems with limited training data [1, 15, 30]. These pretrained models provide high-quality feature representations and effectively capture the visual content of images. Nevertheless, it is important to notice that pretrained models are typically trained on image datasets that may differ significantly from the specific fine-grained recognition task at hand. This disparity can potentially introduce bias into the feature representations, where certain visual elements are more prominently represented in the resulting features. However, these visually dominant elements may not necessarily be relevant or useful for the downstream task. It might be expected that these features would either go unused by a classifier trained on the downstream task data or be suppressed during training. However, our initial investigations indicate otherwise.\nFigure 1 illustrates a specific scenario to highlight our observations. The upper portion of Figure 1 displays a feature magnitude histogram derived from the pretrained model (i.e., ResNet-50). It is evident that certain feature dimensions exhibit significantly higher magnitudes compared to others. When we employ class activation mapping (CAM) [41] to investigate the image regions contributing to these features, we discover that some of these features (highlighted in red dashed boxes) do not correspond to regions of interest on the object. However, when we proceed to train a linear classifier using these features (a.k.a linear probing), we notice that the classifier does not heavily downweight these particular features, as shown in the middle row of Figure 1. This suggests that despite these features being less relevant to the intended concept for recognition, they can still appear to be discriminative in the context of a small-size training dataset. This situation can mislead the model into relying on these less pertinent features for making final predictions."}, {"title": "3.3. Feature Magnitude Regularization", "content": "To mitigate the feature magnitude bias inherent in a pretrained model, we introduce a feature magnitude regularization loss. Initially, we normalize the features using the Softmax operation, which can be expressed as:\n$P_i = \\frac{exp((\\Psi(I)_i))}{\\sum_j exp((\\Psi(I)_j))}$,\nwhere $\\Psi(I)$ indicates the value of the i-th dimension of the feature $\\Psi(I) \\in R^D$. This operation produces a pseudo-probability distribution $p = [P_1, P_2,\u2026,P_D] \\in R^D$. We then apply the negative entropy to form a loss term $L_{fmr}$:\n$L_{fmr} = \\sum_{i=1}^{D} p_i \\log(p_i)$,\nwhere X is a weighting coefficient. Please note that when we minimize $L_{fmr}$, we are essentially encouraging the pseudo-distribution p to closely resemble a uniform distribution. In other words, this optimization aims to ensure that the distribution of magnitudes for the unnormalized features becomes as uniform as possible."}, {"title": "3.3.1 Dynamic Coefficient Tuning", "content": "The choice of A for $L_{fmr}$ is very important to the successful application of FMR when fine-tuning. AA that is too strong will clobber even useful features, resulting in an uninteresting uniform feature distribution. Moreover, the optimal A value varies from dataset to dataset and throughout the training process itself. In Subsection 4.3.2 below, we explore this in more detail."}, {"title": "4. Experiments", "content": "In this section, we evaluate the performance of FMR for three fine-grained visual recognition datasets, as well as on"}, {"title": "4.1. Datasets and Experimental Details", "content": null}, {"title": "4.1.1 Datasets", "content": "We applied FMR to four popular FGVR datasets: CUB200 [34], Stanford Cars [19], FGVC-Aircraft [27] and iNaturalist [16]. Due to limited computing resources, we used a subset of iNaturalist consisting of the Order Passeriformes. Please see Table 1 for details. To explore the applicability of FMR in low data regimes, we used subsets of the datasets consisting of 15%, 30%, 50% and 100% of the data."}, {"title": "4.1.2 Implementation Details", "content": "Our experiments were conducted using PyTorch, employing a ResNet-50 [14] pretrained on ImageNet [8] as the backbone network denoted as \u03a8. Each experimental configuration was repeated three times with and without utilizing the FMR loss. The trade-off parameter \u1e9e for the dynamic loss is set to 50 for all datasets and experiments.\nFollowing [30], the training images were resized to 256x256 pixels and randomly cropped into 224x224 pixel patches. These patches were then subjected to random horizontal flips and RandAugment [7]. We utilized an SGD Optimizer with a batch size of 24, a learning rate of 0.001, a momentum of 0.9, and a weight decay of 0.0001.\nDuring testing, we followed the approach of [30], which involved taking five patches and their horizontal reflections, subsequently averaging the predictions obtained from all ten patches."}, {"title": "4.1.3 Compared Algorithms", "content": "We conducted a performance comparison between FMR and several popular methods for supervised fine-grained visual recognition techniques: SAM - Bilinear [30], described above, is the state-of-the-art method for FGVR in the low data regime\u00b9. Bilinear Convolutional Neural Networks (B-CNNs) [24] involve passing an image through two Convolutional Neural Networks (CNNs). We compared our results with those reported by Shu et al. [30] who reimplemented this technique using ResNet-50. We also compared against L2-SP [22], DELTA [23] and Batch Spectral Shrinkage (BSS) [6], which are all described above. Co-Tuning [38] establishes a relationship between the source dataset classes and the target dataset classes. It converts one-hot vectors across the logits for one dataset into probability distributions across the logits for the other dataset and trains both tasks simultaneously. MaxEnt [10] is most similar to our work. We implemented their technique for comparison with ours. Meanwhile, the performance of naive Fine-Tuning of a pretrained model on the training data is also reported for a reference base and denoted as FT Baseline."}, {"title": "4.2. Standard FGVR Benchmarks", "content": "Our experimental results demonstrate the efficacy of FMR. We first present our results on CUB200, Stanford Cars and FGVC-Aircraft in Table 2.\nAs seen, the proposed methods achieve superior performance compared to existing approaches. For example, at the 15% training set size for CUB200, FMR achieves a significant lead with an accuracy of 61.30%, outperforming the next best method (MaxEnt) by nearly +7%. This trend of superior performance continues across all training set sizes. These results highlight the exceptional ability of the proposed FMR to enhance FGVR with various degrees of data availability, demonstrating its robustness and effectiveness in different training contexts.\nWe also set out to demonstrate the use of FMR on a much larger dataset. We tested FMR on the subset order Passeriformes in the iNaturalist Dataset [16] with the same label percentages as we used above. The results can be found in the right-most columns of Table 2. FMR again demonstrates superior performance. These results show that FMR works in datasets with larger scale as well."}, {"title": "4.3. Ablation Studies", "content": "This subsection conducts several ablation studies to investigate the impact of various factors in the proposed method."}, {"title": "4.3.1 The Impact of Pretraining Paradigm", "content": "Given the effectiveness of the proposed FMR in mitigating bias in pretrained models, it's pertinent to explore its impact in relation to the pretraining paradigm used for initial model training. We conducted two experimental trials to examine this: one where FMR was applied to DINO [2], a widely recognized self-supervised (unsupervised) method for pretraining a model, and another involving a model with randomly initialized weights, which lacks pretraining and, theoretically, any inherent feature magnitude bias from such a process. To prevent overfitting, especially given our smaller dataset compared to ImageNet, we opted for ResNet-18 for training from scratch.\nThe results, as illustrated in Table 3, are revealing. FMR demonstrated a comparable level of improvement in both supervised and DINO pretrained models, suggesting that the issue of feature magnitude bias might be present even in self-supervised learning models. Intriguingly, when applied to the model trained from scratch, FMR's contribution was minimal, leading to similar outcomes regardless of its use. This aligns with our hypothesis that FMR effectively counters feature magnitude bias inherent in the pretrained models; absent such pretraining, this bias diminishes, rendering FMR less impactful."}, {"title": "4.3.2 Dynamic Weighting vs. Static Weighting", "content": "In this section, we explore the impact of employing a dynamic weighting scheme for the proposed feature magnitude regularization module. Specifically, we compare it with an alternative approach that employs a static weighting scheme. We conduct experiments on two datasets, CUB200 and FGVC Aircraft, using two different pretrained backbones. For these experiments, we focus on the scenario where only 10% of the labeled training data is available. In total, we perform four experiments, varying the FMR weighting coefficient A from 10 to 1000. We record the performance achieved under each A value, generating a performance curve. Additionally, we plot a dash line representing the accuracy obtained by using our proposed dynamic weighting scheme (with B = 50) for comparison. The results are depicted in Figure 3.\nFigure 3 clearly illustrates that the choice of A significantly impacts the performance. Interestingly, we observe that irrespective of the static A value chosen, its highest performance consistently falls below that achieved using the dynamic weighting scheme. The performance gap can be"}, {"title": "4.4. Analysis of FMR", "content": "We present two analyses to explore how FMR works. We first conduct experiments to quantify the quality of the feature learned from FMR and then present some visualization results of the learned features."}, {"title": "4.4.1 Encouraging Learning of Generalizable Features", "content": "The motivation behind the proposed method is to address the feature magnitude bias problem commonly encountered in pretrained models. The underlying expectation is that by incorporating the proposed Feature Magnitude Regularization, the model can focus on utilizing more generalizable features while filtering out distracting ones. To quantitatively assess the impact of FMR on the acquisition of more generalizable features, we devise the following experiment.\nFor both the baseline fine-tuning method and the FMR method, we fix the feature extractor after training on the downstream task dataset. Subsequently, we train two linear classifiers using the features extracted from the backbone: one is trained on the training set, and the other is trained on the testing set. This approach allows us to assess the significance of each feature in distinguishing between data points in the training set and the testing set. Specifically, the weight of the first linear classifier indicates the feature's importance in separating data from the training set, while the weight of the second linear classifier reflects its importance in separating data from the testing set. If a feature exhibits a high degree of generalizability, both classifier weights should have large values, indicating that the feature is deemed important for distinguishing both training and testing data.\nWe introduce a measurement that assesses the percentage of top-k weighted features from the training set that also appear in the top-k weighted features of the testing set. A higher percentage indicates the identification of more generalizable features. We present the results for various values of k, and these results are visualized in Figure 5. Observing the results, it is evident that the curve associated with the FMR consistently remains above that of the fine-tuning baseline. This trend implies that FMR contributes to the model's ability to recognize more generalizable features\nWe therefore selected the top-k features by magnitude (averaged across the classes) from the classifier trained on the training set and counted the number n of features that also appeared in the top-k for the testing set. This gave us a percentage - n/k. We repeated this exercise with many values of k.\nThe results can be seen in Figure 5. This shows that FMR consistently results in the selection of more generalizable features with higher weightings."}, {"title": "4.4.2 Visualization of the contribution of the top features", "content": "Finally, we employ visualization techniques to gain insights into the image regions that influence the top features learned through different methods. To quantitatively measure the contribution of these top features, we establish the following approach.\nFor a sample belonging to a specific class, we compute the element-wise product between the feature and its corresponding class weight. This element-wise product reveals the contribution of each dimension to the logit score for that particular class, effectively creating a dimension-wise contribution vector (DCV). Subsequently, we calculate the class-wise mean of the DCV and rank the top-k dimensions within this mean vector. For each sample within the class, we compute the average of the top 5 DCVs to assess the contribution of the top features to the prediction score. We then apply CAM using this average of the top 5 DCVs to identify the corresponding image regions.\nFigure 4 displays the heat maps visualizing the corresponding image regions obtained from both the fine-tuning baseline approach and our FMR approach. It is evident that our FMR method frequently attend object region whereas the fine-tuned counterpart occasionally directs attention to areas outside of the object.\nCombining the outcomes presented in Figure 5 with the visualizations in Figure 4, these findings provide insights into the characteristics of FMR and its effectiveness in enhancing the generalization performance for fine-grained visual recognition."}, {"title": "5. Conclusions", "content": "In this study, a novel approach named Feature Magnitude Regularization (FMR) was introduced to improve finegrained image recognition, especially in low-data scenarios. FMR effectively equalizes feature magnitudes, addressing issues arising from dominant features in pre-trained models. This method dynamically adjusts regularization strength based on feature magnitude distribution, leading to more balanced feature representations and improved model performance. Experimental results across various datasets confirmed FMR's superiority over traditional fine-tuning methods, showcasing its potential to enhance image recognition accuracy and generalizability in challenging data-limited environments."}]}