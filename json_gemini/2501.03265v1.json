{"title": "Optimizing Edge AI: A Comprehensive Survey on Data, Model, and System Strategies", "authors": ["Xubin Wang", "Weijia Jia"], "abstract": "The emergence of 5G and edge computing hardware has brought about a significant shift in artificial intelligence, with edge AI becoming a crucial technology for enabling intelligent applications. With the growing amount of data generated and stored on edge devices, deploying AI models for local processing and inference has become increasingly necessary. However, deploying state-of-the-art AI models on resource-constrained edge devices faces significant challenges that must be addressed. This paper presents an optimization triad for efficient and reliable edge AI deployment, including data, model, and system optimization. First, we discuss optimizing data through data cleaning, compression, and augmentation to make it more suitable for edge deployment. Second, we explore model design and compression methods at the model level, such as pruning, quantization, and knowledge distillation. Finally, we introduce system optimization techniques like framework support and hardware acceleration to accelerate edge AI workflows. Based on an in-depth analysis of various application scenarios and deployment challenges of edge AI, this paper proposes an optimization paradigm based on the data-model-system triad to enable a whole set of solutions to effectively transfer ML models, which are initially trained in the cloud, to various edge devices for supporting multiple scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "From AlphaGo to ChatGPT, the rapid progress of AI technology in recent years makes us marvel at its huge potential. Simultaneously, media coverage of the threat of artificial intelligence challenging human intelligence is increasing. In fact, while the potential of AI has been shown, there are still some gaps between AI applications and the real world. To achieve better performance, larger datasets and more parameters are often used to train AI models, which usually requires a large amount of training consumption and also makes the model complex. A typical example is the large-scale language model GPT-3, which has 175 billion parameters and requires about 800GB of storage [1]. Unfortunately, the high cost of training makes it out of reach for the average person, and only exists in labs like OpenAI with a deep accumulation. Meanwhile, these cumbersome models are difficult to deploy to small, resource-constrained models such as edge devices, which are widely distributed in life. Therefore, there is an urgent need to design efficient AI models and frameworks for use on small devices.\nWith the development of communication technologies such as 5G and the Internet of Things, the edge of the network can reach a variety of devices in multiple settings, including schools, hospitals, factories, shops and homes [2]. These widely distributed edge devices generate huge amounts of data. According to Gartner, by 2025, about 75% of enterprise-generated data will not come from traditional data centers or the cloud, but from edge devices [3]. Storing and processing these large amounts of data in the traditional cloud will bring great system overhead and transmission bandwidth requirements. Meanwhile, processing data at the edge is an important requirement for some applications (eg. Smart Cities [4], Autonomous driving [5]), even as rapid advances in network technologies such as 5G bring increased communication capabilities. Edge computing is a kind of computing mode which is close to data source and strives to reduce transmission delay through local computation [6]. By putting computing on the edge, it relieves the real-time requirements that cloud computing cannot meet in some scenarios [7].\nEdge AI refers to AI algorithms deployed on edge devices for local processing, which can process data without a network connection. As more and more devices cannot rely on the cloud to process data, the emergence and development of edge AI can help alleviate such problems [8]. Especially with the advent of the era of big data, the use of artificial intelligence technology to improve the level of automatic processing equipment is particularly important. For example, a prominent feature of Industry 4.0 is smart automation, where industrial robots need to process data at high speed with minimal latency [9]. With the help of AI, industrial robots can process and infer a large amount of multi-modal data from mobile devices, sensors and Internet of Things platforms with an efficiency beyond the reach of human beings, so as to find potential risks and deal with them in a timely and effective manner, thus improving the intelligence of factories [10]. In recent years, deep learning has brought about breakthroughs in artificial intelligence technology [11]. However, these models usually need to consider a large number of parameters during training and rely on high-performance multi-core processing devices such as GPUs. For Al models to cover real-life scenario problems, it is essential to deploy them on devices with limited resources. At the same time, the deployment of the model on the local device can also avoid data leakage during the transmission process to the server, so as to meet the increasing importance of security and privacy needs of people. Therefore, it is necessary and practical to study efficient models that can be deployed to resource-constrained edge devices."}, {"title": "A. Related Surveys", "content": "Our main work in this survey is to provide a comprehensive overview of the current state-of-the-art techniques and approaches for developing efficient models for resource-constrained devices. Compared with the previous edge AI survey, Shi et al. [12] discussed from the perspective of efficient communication, Dai et al. [20] introduced from the perspective of computation offloading, Zhang et al. talked [21] mobile edge AI for the Internet of Vehicles, Park et al. [18] provided an overview for wireless network intelligence, and Letaief et al. [14] discussed edge AI for 6G. While these surveys are essential and cover various aspects of edge AI, they do not provide a comprehensive discussion of deploying Al models on edge devices.\nDeng et al. [2] discussed edge AI from the perspectives of AI on edge and AI for edge, while Zhou et al. [8] provided a comprehensive overview of relevant frameworks, technologies, and structures for deep learning models involved in training and reasoning on the network's edge. Similarly, Xu et al. [22] conducted an extensive review of edge intelligence, including edge inference, edge training, edge caching, and edge offload-ing. On the other hand, Hua et al. [23] introduced their survey from the viewpoint of AI-assisted edge computing, while Murshed et al. [17] reviewed edge AI from the perspective of practical applications. Additionally, surveys [13] and [19] also touched on edge inference and model deployment. Finally, survey [24] and [25] covered the topics of model compression and acceleration. Although some of these surveys have briefly discussed the deployment of AI models on edge devices, none has provided a comprehensive discussion of this crucial aspect. Therefore, this survey aims to fill this gap and provide a detailed and in-depth analysis of AI model deployment on edge devices."}, {"title": "B. Our Contributions", "content": "In this survey, our focus is to provide an academic response to the following research questions (RQs):\n\u2022 RQ1: What are the data challenges for building and deploying machine learning (ML) models on edge devices and how can we address them?\n\u2022 RQ2: How can we optimize ML models for efficient edge deployment without significantly compromising accuracy?\n\u2022 RQ3: What system infrastructure and tools can best support edge Al workflows and seamless model deployment on heterogeneous edge hardware?\n\u2022 RQ4: What are the applications of edge AI in daily life?\n\u2022 RQ5: What are the challenges of edge AI and how can they be mitigated and addressed?\n\u2022 RQ6: What are the future trends of edge AI?"}, {"title": "II. FUNDAMENTAL CONCEPTS", "content": "This section provides the fundamental concepts of edge computing and edge artificial intelligence, and Figure 4 shows the intersection between edge computing and artificial intelli-gence and the focus of this survey.\nCloud computing offers many benefits, including flexibility, scalability, enhanced collaboration, and reduced costs for modern enterprises [26]. However, cloud computing systems are completely dependent on the Internet, and without a valid Internet connection, users will not be able to access services. Additionally, since the cloud infrastructure is provided by the cloud provider, the cloud user has limited control over appli-cations, resources, and services. The risk of user data being leaked in the cloud and during transmission is also noteworthy [27]. Despite the many advantages of cloud computing, when edge devices have real-time requirements for data processing, the response time of modes that transport output from edge to cloud for processing and then return may be too long, especially in the case of an unstable network. Edge computing, a distributed computing architecture, has been proposed to address this issue. It moves data processing to the edge node where the data is generated, addressing the issue of slow response and high delay that can occur in cloud processing [7].\nEdge application services reduce the transfer of data and aim to keep processing locally, which alleviates problems such as network latency and transmission overhead. Since the data is stored and processed locally, the user has absolute ownership of the data, which also avoids the risk of data leakage during transmission between edge nodes and servers [28]. Edge computing brings computing closer to the end-user and speeds up the response time of services, which is necessary and essential in services such as autonomous driving [5]. When local resources are limited, the local device transmits data to the edge network server instead of to the cloud server, which can avoid long-distance transmission and response and thus improve efficiency [29]. Moreover, the deployment and access of edge devices and their ability to continue service even when communication is slow or temporarily interrupted ensure the scalability and reliability of edge computing [7]. The application of edge computing has been greatly successful in many aspects, for example, IoT [30], autonomous driving [5], smart cities [4], robotics [31], and so on.\nEdge artificial intelligence, or edge AI, is a combination of edge computing and artificial intelligence. With the pro-liferation of IoT devices, a large amount of multi-modal data (such as audio, video, pictures, etc.) is continuously generated. Advances in edge computing allow data on these edge devices to be processed locally in real-time without being sent back to the cloud, reducing latency and providing more efficient and timely responses [7]. Artificial intelligence is an automated technology that quickly analyzes large amounts of data to extract information for further prediction and decision-making, which makes it suitable for application on edge devices in many scenarios [15]. As the computing power of edge devices improves without a significant increase in hardware costs and advances in algorithm-optimization techniques enable compu-tationally demanding Al models to run on edge devices, the generation and development of edge AI technology that meets the requirements of real-time response is made possible [19].\nAs shown in Figure 6, edge AI allows data to be processed at the local level, which greatly reduces latency between cloud and local data processing. With less data being transferred, the system's bandwidth requirements and cost are reduced. More importantly, because data is stored and processed locally, data security is improved, and there is less risk of data leakage. Along with the application of AI technology, this increases the level of automation of tasks handled by edge devices. Furthermore, edge AI enables model training and reasoning on edge devices, which allows real-time decisions to be made. It also enables local decision making, which is independent of network quality and cloud systems, further improving the reliability of edge task execution. Edge AI is used in a wide range of applications, including autonomous cars, virtual reality games, smart factories, security cameras, and wearable healthcare devices [32]. Enabled by AI technology, the automation and intelligence level of edge equipment is enhanced."}, {"title": "III. DATA \u039f\u03a1\u03a4\u0399\u039c\u0399\u0396\u0391\u03a4ION FOR EDGE AI DEPLOYMENT", "content": "Garbage in, garbage out (GIGO) is a commonly used idiom in the computer world, which implies that poor quality data entering a computer system will produce poor results. In industry, it is widely recognized that data and features determine the upper limit of ML, and data preprocessing methods, such as feature engineering, play a crucial role in industrial processes."}, {"title": "A. Data Cleaning", "content": "Data cleaning is a crucial step in data preprocessing that involves removing or correcting noisy or incorrect data and removing irrelevant or redundant observations. In ML, the presence of label noise in data can significantly impact the accuracy of the trained model. However, re-labeling large datasets accurately can be a challenging task particularly in situations where resources are limited. To address this issue, recent research has proposed innovative approaches, such as active label cleaning, which identifies and prioritizes visibly mislabeled samples to clean up the noisy data [34]. Mishra et al. [35] have developed an ensemble method based on three deep learning models to handle noise labels of different concentrations of human movement activities collected by smartphones, which can alleviate the problem of label noise arising from crowdsourcing or rapid labeling on the Internet.\nWith the proliferation of smart sensors in the IoT, vast amounts of data are being collected. However, the harsh sensor environment tends to introduce noise into the collected data. To mitigate the problem that traditional sensor nodes are not enough to handle big data, researchers have proposed various innovative approaches. For instance, Wang et al. [36] proposed a method of data cleaning during data collection and optimized the model through online learning. Ma et al. [37] proposed a federated data cleaning approach for future edge-based distributed IoT applications while protecting data privacy. Sun et al. [38] developed a data stream cleaning system with the help of both the cloud servers and edge devices. Additionally, Sun et al. [39] also proposed an adaptive data cleaning method based on intelligent data collaboration for filtering noise data. The work of Gupta et al. [40] proposed a ProtoNN compression approach to reduce the model size further by learning a small number of prototypes to represent the training set to enable deployment on resource-scarce de-vices. These approaches offer promising solutions for cleaning data and enabling efficient processing of big data in resource-constrained environments.\nDiscussion: While innovative approaches like active label cleaning and ensemble methods based on deep learning models can alleviate the problem of label noise, they may have some disadvantages. For instance, active label cleaning depends on the availability of a small set of labeled samples, and the performance of the approach may suffer if the labeled samples are not representative of the entire dataset. Similarly, ensemble methods can be computationally expensive and may increase the risk of overfitting. In addition, some of the proposed approaches for data cleaning in IoT environments, such as data cleaning during data collection and federated data cleaning, may require significant computational resources and may not be feasible in resource-constrained environments. Furthermore, intelligent data collaboration for filtering noise data may require significant communication overhead, which can be a challenge in IoT environments. Therefore, while these approaches offer promising solutions for cleaning data and enabling efficient processing of big data in resource-constrained environments, they also have limitations that need to be carefully considered."}, {"title": "B. Feature Compression", "content": "Feature compression is a common technique used in ML to reduce the dimensionality of high-dimensional feature space. Two popular methods of feature compression are feature selection and feature extraction, which aim to remove re-dundant and irrelevant features while retaining the necessary information [41]. Feature selection involves choosing a subset of relevant features from the original set while maintaining maximum usefulness, resulting in improved model accuracy, reduced complexity, and enhanced interpretability [42]. In contrast, feature extraction creates new features based on the functions of the original ones, ensuring that the newly created features contain useful information while being non-redundant [43]. By leveraging these techniques, researchers can compress the feature space and improve the efficiency and performance of their models.\nWith the increasing popularity and growth of computation-ally constrained devices such as smartphones, wearables, and IoT devices, there is a growing need to develop efficient and effective ML algorithms for on-device analysis on these plat-forms. Feature selection has emerged as a popular technique for reducing the dimensionality of high-dimensional feature spaces and improving the efficiency and accuracy of ML models. In recent years, researchers have applied feature se-lection methods to various resource-constrained applications. For example, Do et al. [44] proposed an accessible melanoma detection method using smartphones, where they designed a feature selection module to select the most discriminative features for classification. Similarly, Fasih et al. [45] adopted feature selection methods to reduce memory and computa-tional requirements in their Active Feature Selection approach for emotion recognition. Summerville et al. [46] designed an ultra-lightweight deep approach based on feature selection for anomaly detection in IoT devices. Sudhakar et al. [47] proposed ActID, a framework for user identification based on activity sensors, where the feature selection method is used to evaluate and select discriminative high-quality features, thus reducing the complexity of the algorithm and making it better adapt to the requirements of resource-limited devices. Laddha et al. [48] proposed a method for selecting features with high invariance and robustness based on descriptor score to achieve the required pose precision for real-time simultaneous localization and mapping (SLAM) on resource-constrained platforms.\nIn addition, feature selection has also been applied in other edge environments to enhance the performance and efficiency of ML algorithms. Several studies have demonstrated the use-fulness of this technique in various edge-based applications, such as Parkinson's disease classification [49], atrial fibrilla-tion recognition [50], data dimensionality reduction [51], fault diagnosis on the edge of IoT [52], COVID-19 detection [53], and more. For instance, swarm intelligence-based methods [51], pre-training models [53], and social learning particle swarm optimization [53] have been employed to select the most informative features. Feature selection appears to be a promising approach to reducing the computational complexity of ML algorithms, enhancing their accuracy, and enabling real-time analysis on resource-constrained devices. Therefore, its broader adoption is expected to facilitate on-device analysis in constrained environments and accelerate the development of efficient and effective edge-based ML solutions.\nThe increasing demand for intelligent sensing and analysis on edge has led to a growing need for efficient and effective methods to reduce the energy and memory cost of deep learning algorithms in resource-constrained edge computing"}, {"title": "C. Data Augmentation", "content": "Data augmentation is a commonly used technique in ML to increase the amount of a dataset by generating new data through slight modifications of existing data. This technique can be particularly useful when dealing with smaller datasets and can help alleviate overfitting problems. In the field of image processing, data augmentation can be achieved through various techniques such as rotation, edge enhancement, de-noising, and scaling of images [64]. By applying these mod-ifications to existing images, new and diverse images can be generated, thereby increasing the size of the dataset and improving the performance of the model. In natural language processing (NLP) tasks, data augmentation can be realized by various techniques such as randomly adding or deleting words, adjusting the order of words, auxiliary task utilization [65], translating samples into a second language and then translating back to form new samples, among others [66].\nThese techniques help in generating new and diverse data, which can be used to train better models and improve the performance of the model on the test data.\nTo address the challenge of limited data availability in edge devices, researchers have proposed various data augmentation methods that generate new and diverse data for training ML models. For instance, Wang et al. [67] designed a traffic prediction method based on a 5G cellular infrastructure that incorporates data augmentation to alleviate data shortages and privacy issues on edge devices. Similarly, Liao et al. [68] proposed three data augmentation methods to accelerate the creation of a multi-user enhanced PHY layer authentication system model. Another example is the work of Liu et al. [69], who improved the prediction accuracy of a KITTI road detection model by introducing appropriate data augmentation strategies, such as adding road edge labels to small training samples. In addition, data augmentation has been employed by researchers to improve the generalization of images in complex scenes, as demonstrated by Jiao et al. [70] in their method for litchi monitoring. Gu et al. [71] proposed a line segment detection method named M-LSD that leverages data augmen-tation to provide auxiliary line data for the training process. Furthermore, Liu et al. [72] utilized the data augmentation technique to improve the performance of intrusion detection systems in the industrial IoT by addressing the problem of data imbalance. Pan et al. [73] expanded the amount of training data for 1D tracking through the use of data augmentation, which reduced the pressure of collecting more data from users.\nDiscussion: Although data augmentation is a powerful technique for generating new and diverse data and improving the performance of ML models, it has some limitations and disadvantages. One major disadvantage is that data augmen-tation may introduce bias or unrealistic assumptions into the training data, which can negatively affect the performance of the model on the test data. Moreover, the effectiveness of data augmentation depends on the choice of augmentation tech-niques and the specific application domain. For instance, some augmentation techniques may not be suitable for certain types of data, such as medical images, where introducing artificial modifications can be risky. Additionally, data augmentation can be computationally expensive, especially for large datasets and complex models. Therefore, while data augmentation is a valuable technique for improving the performance of ML models, it is important to carefully consider its limitations and potential drawbacks in different application scenarios."}, {"title": "IV. MODEL OPTIMIZATION FOR EDGE AI DEPLOYMENT", "content": "Model optimization is a critical step in deploying ML models to edge devices where computational resources are limited. There are two main approaches to model optimiza-tion: model design and model compression (as shown in Figure 8). The former involves developing compact model architectures and using automated neural architecture search techniques to achieve superior performance while minimizing the computational burden and number of model parameters. The latter involves using methods such as pruning, parameter sharing, quantization, knowledge distillation, and low-rank factorization to shrink the size of deep learning models without significantly affecting their accuracy or performance. These techniques are crucial for deploying complicated models on devices with limited resources or in large-scale distributed systems with constrained processing, memory, and storage."}, {"title": "A. Model Design", "content": "Developing optimal model architectures is critical for achieving superior performance across a range of ML applica-tions. In this section, we will explore two approaches for ad-dressing this challenge: the design of compact model structures and the use of automated neural architecture search (NAS) techniques. These strategies aim to achieve superior model performance while minimizing the computational burden and number of model parameters, enabling practical deployment on various computational devices.\nCompact neural network architectures are typically characterized by their lower require-ment for computing resources and fewer parameters. Due to the limited computing power of edge devices, it is increasingly important to develop neural network models that are both efficient and compact. Therefore, in this section, we will introduce some of the noteworthy lightweight neural network models that have been proposed in the literature.\nThe rise of areas such as the IoT and edge computing, which require processing huge amounts of data and the ability to perform real-time analysis on edge devices, has boosted the de-velopment of lightweight neural networks. These lightweight neural networks typically use techniques such as convolu-tional grouping, depth-separable convolution, width-separable convolution, channel pruning, network pruning, and others to compact the network architecture [74], resulting in higher computational efficiency and lower memory consumption. For example, the MobileNets series [75] [76] [77] is a collection of lightweight neural networks built for mobile vision applica-tions. These networks were developed by Google researchers and have gained a lot of traction in the computer vision world due to their high accuracy and minimal computing complexity, making them perfect for usage on mobile and embedded devices with limited resources. Moreover, Zhou et al. [78] proposed to invert the structure and introduce a novel bottleneck design, referred to as the \"sandglass block,\" which conducts identity mapping and spatial transformation at higher dimensions, thereby reducing information loss and gradient confusion more effectively. In Tan et al. [79]'s research, they introduced an automated approach for mobile NAS that incor-porates model latency as a crucial optimization objective to solve the difficulty of manual solution for so many architecture possibilities in CNN.\nShuffleNets series is a lightweight CNN proposed by MegVII, which aims to solve the balance problem between the accuracy and efficiency of lightweight neural networks. The core idea of ShuffleNets is to enhance the information flow of the network and improve its accuracy by performing channel shuffling within groups. In ShuffleNetV1 [80], channel shuffling is introduced, which divides the input group into multiple sub-groups along the channel dimension and performs convolution operations on each sub-group. The results are then concatenated along the channel dimension. Through this operation, ShuffleNetV1 can effectively reduce computational complexity while improving accuracy. Based on ShuffleNetV1, ShuffleNetV2 [81] employs a novel ShuffleNetV2 unit struc-ture, which incorporates designs such as channel shuffling and pointwise convolutions. This unit structure significantly improves information flow, thereby further enhancing the accuracy of the network. In OneShot proposed by Guo et al. [82], it alleviates the weight adaptive problem by building a simplified super network in which all architectures are single paths.\nSqueezeNet [83] achieves efficient information transfer with few parameters by introducing a component named \"Fire module,\" which consists of a 1 x 1 convolutional layer called squeeze layer and a 1 x 1 and 3 x 3 convolutional layer called expand layer. Squeeze layer compresses the number of channels in the input feature graph, and expand layer increases the number of channels in the compressed feature graph. The subsequent version of SqueezeNet, SqueezeNext, using hardware simulation results of power consumption and inference speed on embedded systems, showed that compared to SqueezeNet, the model is 2.59x faster, 2.25x more energy-efficient, and without any accuracy degradation [84]. Han et al. [85] proposed a plug-and-play Ghost module, which tends to generate more feature graphs through low-cost operations to enhance feature extraction, and at the same time, it uses a Ghost bottleneck structure to enhance the representation ability of models.\nThe EfficientNet series [86] [87] [88], proposed by Tan et al. of the Google Brain Group, are also famous efficient CNNs. EfficientNet employs a technique called \"compound scaling,\" which adjusts not only the depth, width, and resolution of the network when scaling it up but also the interdependent relationships between these parameters. This results in a more efficient and accurate network [86]. EfficientNetV2 is an upgraded version of EfficientNet, which further improves the performance of the network by using more efficient network structure design and optimized training strategies, and proposes an improved progressive learning method to adaptively adjust the learning strategy [87]. EfficientDet is based on EfficientNet as the backbone network and achieves higher detection accuracy and faster inference speed through innovative designs such as the introduction of the BiFPN structure, carefully designed feature network hierarchy and feature fusion mechanism, as well as optimized loss function [88].\nHuang et al. [89] proposed CondenseNet, an efficient CNN architecture that encourages feature reuse through dense con-nectivity and prunes filters associated with redundant feature reuse through learned group convolutions. The pruned network can be efficiently converted into a network with regular group convolutions for efficient inference, which can be easily implemented with limited computational costs during training. Yang et al. [90] proposed an alternative scheme named Con-denseNetV2 to improve the reuse efficiency of features. In this approach, each layer has the capability to selectively utilize a specific set of highly significant features from the previous layer while concurrently updating a set of earlier features to enhance their relevance to subsequent layers. Mehta et al. proposed ESPNet [91] and ESPNetV2 [92], where ESPNet reduces computation and learns representations with large receptive fields by using point-wise convolutions and spatial pyramid of dilated convolutions. ESPNetV2 is an extension of ESPNet that uses depth-separable convolution and outper-forms ESPNet by 4-5%. FBNets (Facebook-Berkeley-Nets), a series of lightweight networks created by Facebook and UC Berkeley, FBNet [93] uses a differentiable NAS framework to optimize neural architecture using a gradient-based method, while the second version FBNetV2 [94] focuses on the small DNAS search space. The third version FBNetV3 [95], takes into account the fact that the other approaches overlooked a better architecture-recipe combination. PeleeNet, unlike recent lightweight networks that heavily rely on depthwise separable convolutions, utilizes conventional convolutions and is primar-ily designed for deployment on mobile devices [96].\nThe Inception series is also a classic network proposed by Google. The idea is to use multiple convolution kernels of different sizes to process input data in parallel and then concatenate their outputs along the channel dimension to form the network output [97]. InceptionV2 uses batch normalization and replaces large convolution kernels with small convolution kernels [98]. In InceptionV3, the factorization into smaller convolutions is introduced, the larger two-dimensional con-volution is split into smaller one-dimensional convolutions, and the Inception module structure is optimized [99]. Incep-tionV4 introduced stem modules and reduction Blocks [100]. Xception primarily achieved complete separation of learning spatial correlation and learning inter-channel correlation tasks through the introduction of depthwise separable convolutions [101].\nMehta et al. [102] proposed MobileViT to learn the global representation of networks, which combines the advantages of CNNs and ViTs and is superior to both. Wu et al. [103] designed a lightweight NLP architecture, Lite-Transformer, where the key is a Long Short Range of Attention, with one set of heads focused on local context modeling and another on long-distance relationship modeling. Recently, lightweight networks based on attention have been proposed. For instance, Hou et al. [104] took into account that some channel attention studies ignored location information and embedded it into channel attention to enhance network performance. To avoid the complexity of the model caused by the sophisticated atten-tion module, Wang et al. [105] designed an Efficient Channel Attention (ECA) module that can bring clear performance improvement with only a handful parameters involved. In attention mechanisms, there are two types: spatial attention and channel attention. Combining the two can improve perfor-mance, but it inevitably leads to increased model complexity. To address this, Zhang et al. [106] designed the Shuffle At-tention (SA) module, which combines the advantages of both types of attention while avoiding excessive model complexity. Misra et al. [107] proposed triplet attention, a novel method for efficient attention weight calculation by using a three-branch structure to capture cross-dimensional interactions. In the study by Zhang et al. [108], they designed a Split-Attention block for use in ResNet, which allows attention to span feature groups while ensuring the simplicity and ease of use of ResNet. In addition, to help readers better understand these lightweight networks, we summarized their characteristics in Table 3.\nDiscussion: The advantages of compact architecture design are that it produces efficient and compact neural network models with higher computational efficiency, lower memory consumption, and improved accuracy. These models are suit-able for deployment on edge devices with limited resources, making them ideal for IoT and edge computing applications. However, designing optimal model architectures can be a time-consuming and resource-intensive process. Additionally, some model design techniques, such as depthwise separable convolutions and pointwise convolutions, may be less effective in capturing complex features compared to traditional con-volutional layers, which may negatively impact the model's accuracy."}, {"title": "2) Neural Architecture Search:", "content": "NAS aims to automate the process of designing neural network architectures, which can be a time-consuming and resource-intensive process when done manually. NAS typically employs different optimization methods such as evolutionary algorithms, reinforcement learn-ing, or gradient-based optimization to search the space of neu-ral architectures and identify the one that performs best on a specific task while satisfying certain computational constraints. Recently, with the rise of IoT and AI of Things (AIoT), there has been a growing demand for intelligent devices with low energy consumption, high computing efficiency, and low resource usage. NAS has emerged as a promising approach to design efficient and lightweight neural networks that can be deployed on edge devices. In this section, we will discuss various recent studies that have used NAS to design efficient neural networks for edge computing.\nOne notable subject area in current studies is the employ-ment of advanced search algorithms and multi-objective opti-mization approaches to identify neural network architectures that optimize different performance metrics such as accuracy, resource consumption, and power efficiency. This work has become increasingly important in recent years as edge com-puting applications require efficient yet accurate models. To this end, researchers have proposed various approaches for multi-objective architecture search. Lu et al. [109] and Lyu et al. [110] are two such studies that employed multi-objective optimization to identify efficient and accurate neural network architectures for edge computing applications. Similarly, Chen et al. [111] used performance-based strategies to search ef-ficiently for architectures that are optimal with regard to multiple objectives. These studies underscore the significance of considering different objectives during the architecture search process to ensure that the neural architectures are both accurate and efficient. By using advanced search algorithms and multi-objective optimization techniques, researchers can design models that are effective in resource-constrained en-vironments while still maintaining high accuracy, which is crucial for edge computing applications.\nThe innovative techniques and algorithms to improve the efficiency and effectiveness of NAS are often employed in the research. For example, Mendis et al. [112] incorporated intermittent execution behavior into the search process to find accurate network architectures that can safely and efficiently execute under intermittent power, which is a common chal-lenge in edge computing applications. Similarly, Ning et al. [113] identified factors that affect the fault resilience capability of neural network models and used this knowledge to design fault-tolerant CNN architectures for edge devices. These stud-ies demonstrate the importance of considering unique chal-lenges and constraints of edge computing applications when designing neural networks through NAS. By incorporating innovative techniques and algorithms, researchers can develop architectures that are not only efficient and accurate but also robust and reliable.\nFurthermore, several studies proposed hardware-efficient primitives and frameworks to optimize the search process and improve the performance of the resulting networks. For in-stance, Liu et al. [114] proposed the Point-Voxel Convolution (PVConv) primitive, which combines the best of point-based and voxel-based models for efficient NAS. This hardware-efficient primitive achieves state-of-the-art performance with significant speedup and has been successfully deployed in real-world edge computing scenarios, such as an autonomous racing vehicle. Donegan et al. [115] proposed the use of a differentiable NAS method to find efficient CNNs for Intel Movidius Vision Processing Unit (VPU), achieving state-of-the-art classification accuracy on ImageNet. These studies highlight the importance of considering hardware efficiency when designing neural networks for edge computing applica-tions. By leveraging hardware-efficient primitives and frame-works, researchers can not only optimize the search process but also develop neural architectures that are efficient and effective in resource-constrained environments.\nMoreover, some studies propose novel NAS approaches that strictly adhere to resource constraints, while others fo-cus on addressing non-i.i.d. data distribution in federated learning scenarios. For instance, Nayman et al. [116] intro-duced HardCoRe-NAS, which is a constrained NAS approach that strictly adheres to multiple resource constraints such as latency, energy, and memory, without compromising the accuracy of the resulting networks. Similarly, MemNAS [117] is a framework that optimizes both the performance and memory usage of NAS by considering memory usage as an optimization objective during the search process. On the other hand, Zhang et al. [118] focused on addressing non-i.i.d. data distribution in federated learning scenarios by proposing the Federated Direct NAS (FDNAS) and Cluster Federated Direct NAS (CFDNAS) frameworks. These frameworks lever-age advanced proxylessNAS and meta-learning techniques to achieve device-aware NAS, tailored to the particular data distribution and hardware constraints of each device. These studies demonstrate the importance of considering various constraints and challenges in the design of neural networks for edge computing applications.\nDiscussion: While NAS has shown promise in designing ef-ficient and lightweight neural networks for edge computing ap-plications, there are still some disadvantages to this approach. One major limitation is that NAS can be computationally expensive, especially when searching through a large space of possible architectures. This can make it challenging to deploy NAS-based models in resource-constrained environments, par-ticularly those with limited computational power. Additionally, while NAS can optimize for multiple objectives, it can be difficult to find a balance between accuracy and efficiency, especially when dealing with complex tasks or non-i.i.d. data distribution. Furthermore, the resulting neural architectures may not be easily interpretable, making it challenging to understand how they work or explain their decisions."}, {"title": "B. Model Compression", "content": "Model compression is a group of methods (such as pruning, parameter sharing, quantization, knowledge distillation and low-rank factorization) for shrinking the size of deep learn-ing models without significantly affecting their accuracy or performance by eliminating extraneous components, such as duplicated parameters or layers. Due to deep learning models' high computational and storage needs, model compression has become more and more crucial. These methods are created to make it possible to deploy complicated models on devices with limited resources or in large-scale distributed systems with constrained processing, memory, and storage. To enhance the efficacy and efficiency of deep learning models in various applications, it is possible to employ these techniques either in isolation or in conjunction. For instance, a classic example of combining multiple techniques is Deep Compression, which combines techniques such as pruning, quantization, and Huff-man coding to achieve significant compression of deep"}]}