{"title": "The GigaMIDI Dataset with Features for Expressive Music Performance Detection", "authors": ["Keon Ju Maverick Lee", "Jeff Ens", "Sara Adkins", "Pedro Sarmento", "Mathieu Barthet", "Philippe Pasquier"], "abstract": "The Musical Instrument Digital Interface (MIDI), introduced in 1983, revolutionized music pro- duction by allowing computers and instruments to communicate efficiently. MIDI files encode musical instructions compactly, facilitating convenient music sharing. They benefit Music Infor- mation Retrieval (MIR), aiding in research on music understanding, computational musicology, and generative music. The GigaMIDI dataset contains over 1.4 million unique MIDI files, encom- passing 1.8 billion MIDI note events and over 5.3 million MIDI tracks. GigaMIDI is currently the largest collection of symbolic music in MIDI format available for research purposes under fair dealing. Distinguishing between non-expressive and expressive MIDI tracks is challenging, as MIDI files do not inherently make this distinction. To address this issue, we introduce a set of innovative heuristics for detecting expressive music performance. These include the Distinctive Note Velocity Ratio (DNVR) heuristic, which analyzes MIDI note velocity; the Distinctive Note Onset Deviation Ratio (DNODR) heuristic, which examines deviations in note onset times; and the Note Onset Median Metric Level (NOMML) heuristic, which evaluates onset positions relative to metric levels. Our evaluation demonstrates these heuristics effectively differentiate between non-expressive and expressive MIDI tracks. Furthermore, after evaluation, we create the most substantial expressive MIDI dataset, employing our heuristic, NOMML. This curated iteration of GigaMIDI encompasses expressively-performed instrument tracks detected by NOMML, con- taining all General MIDI instruments, constituting 31% of the GigaMIDI dataset, totalling 1,655,649 tracks.", "sections": [{"title": "1. Introduction", "content": "The representation of digital music can be categorized into two main forms: audio and symbolic domains. Audio representations of musical signals characterize sounds produced by acoustic or digital sources (e.g. acoustic musical instruments, vocals, found sounds, virtual instruments, etc.) in an uncompressed or com- pressed way. In contrast, symbolic representation of music relies on a notation system to characterize the musical structures created by a composer or resulting from a performance (e.g., scores, tablatures, MIDI per- formance). While audio representations intrinsically encode signal aspects correlated to timbre, it is not the case for symbolic representations; however, symbolic representations may refer to timbral identity (e.g. cello staff) and expressive features correlated with timbre (e.g. pianissimo or forte dynamics) through notations. Multiple encoding formats are employed for the representation of music. WAV is frequently utilized to store uncompressed audio, thereby retaining nuanced timbral attributes. In contrast, MIDI serves as a preva- lent format for the symbolic storage of music data. MIDI embraces a multitrack architecture to represent musical information, enabling the generation of a score representation through score editor software. This pro- cess encompasses diverse onset timings and velocity levels, facilitating quantification and encoding of these musical events. The choice of training dataset significantly influ- ences deep learning models, particularly highlighted in the development of symbolic music generation models. Consequently, MIDI datasets have gained increased atten-"}, {"title": "2. Background", "content": "Before exploring the GigaMIDI dataset, we examine symbolic music datasets in existing literature. This sets the stage for our discussion on MIDI's musical expres- sion and performance aspects, laying the groundwork for understanding our heuristics in detecting expres- sive music performance from MIDI data."}, {"title": "2.1 Symbolic Music Data", "content": "Symbolic formats refer to the representation of music through symbolic data, such as MIDI files, rather than audio recordings. Symbolic mu- sic understanding involves analyzing and interpreting music based on its symbolic data, namely information about musical notation, music theory and formalized music concepts."}, {"title": "2.2 Music Expression and Performance Representa- tions of MIDI", "content": "We use the terms expressive MIDI, human-performed MIDI, and expressive machine-generated MIDI in- terchangeably to describe MIDI files that capture expressively-performed (EP) tracks, as illustrated in Figure 1. EP-class MIDI tracks capture performances by human musicians or producers, emulate the nuances of live performance, or are generated by machines trained with deep learning algorithms. These tracks incorpo- rate variations of features, such as timing, dynamics, and articulation, to convey musical expression. From the perspective of music psychology, ana- lyzing expressive music performance involves under- standing how variations of, e.g. timing, dynamics and timbre relate to performers' in- tentions and influence listeners' perceptions. Repp's research demonstrates that expressive timing deviations, like rubato, enhance listeners' perception of natural- ness and musical quality by aligning with their cogni- tive expectations of flow and structure. Palmer's work further reveals that expressive timing and dynamics are not random but result from skilled motor planning, as musicians use mental represen- tations of music to execute nuanced timing and dy- namic changes that reflect their interpretive intentions. Our focus lies on two main types of MIDI tracks: non-expressive and expressive. Non-expressive MIDI tracks exhibit relatively fixed velocity levels and on- set deviations, resulting in metronomic and mechani- cal rhythms. In contrast, expressive MIDI tracks fea- ture subtle temporal deviations (non-quantized but hu- manized or human-performed) and greater variations in velocity levels associated with dynamics."}, {"title": "2.2.1 Non-expressive and expressively-performed MIDI tracks", "content": "MIDI files are typically produced in two ways (exclud- ing synthetic data from generative music systems): us- ing a score/piano roll editor or recording a human per- formance. MIDI controllers and instruments, such as a keyboard and pads, can be utilized to adjust the param-"}, {"title": "2.2.2 Music expression and performance datasets", "content": "The aligned scores and performances (ASAP) dataset has been developed specifically for annotating non- expressive and expressively-performed MIDI tracks. Comprising 222 digital musi- cal scores synchronized with 1068 performances, ASAP encompasses over 92 hours of Western classical piano music. This dataset provides paired MusicXML and quantized MIDI files for scores, along with paired MIDI files and partial audio recordings for performances. The alignment of ASAP includes annotations for down- beat, beat, time signature, and key signature, making it notable for its incorporation of music scores aligned with MIDI and audio performance data. However, it primarily focuses on creating expressive dynamics via MIDI Control Change #1 (modulation wheel) and lacks velocity variations, featuring predom- inantly constant velocities as verified by our manual in- spection. In contrast, the GigaMIDI dataset focuses on expressive performance detection through variations of micro-timings and velocity levels. datasets focus on singular instruments, specifically piano and drums, respectively. Despite their narrower scope, these datasets are note- worthy for including MIDI files exclusively performed by human musicians. and the Batik-Plays-Mozart MIDI dataset both provide valuable resources for study- ing classical piano performance. performance data such as note timing and velocity. Together, these datasets support research in performance analysis and machine learning applications in music. was devised for capturing performer-induced expres- siveness by transcribing audio piano performances into MIDI format. ATEPP addresses inaccuracies inherent in the automatic music transcription process. Similarly, comprises Al-transcribed piano tracks that encapsulate expressive performance nuances. How- ever, we excluded the ATEPP and GiantMIDI piano datasets from our expressive music performance de- tection task. State-of-the-art transcription models are known to overfit the MAESTRO dataset is a limited, due to its recordings originating from a controlled piano competition setting. These perfor- mances, all played on similar Yamaha Disklavier pi- anos under concert hall conditions, result in consis- tent acoustic and timbral characteristics. This unifor- mity restricts the models' ability to generalize to out- of-distribution data, contributing to the observed over- fitting."}, {"title": "3. GigaMIDI Data Collection", "content": "We present the GigaMIDI dataset in this section and its descriptive statistics, such as the MIDI instrument group, the number of MIDI notes, ticks per quarter notes, and musical style. Additional descriptive statis- tics are in Supplementary file 1: Appendix.A.1."}, {"title": "3.1 Overview of GigaMIDI Dataset", "content": "The GigaMIDI dataset is a superset of the MetaMIDI dataset and it contains 1,437,304 unique MIDI files with 5,334,388 MIDI in- strument tracks, and 1,824,536,824 (over 109; hence, the prefix \"Giga\") MIDI note events. The GigaMIDI dataset includes 56.8% single-track and 43.2% multi- track MIDI files. It contains 996,164 drum tracks and 4,338,224 non-drum tracks. The initial version of the dataset consisted of 1,773,996 MIDI files. Approxi- mately 20% of the dataset was subjected to a cleaning process, which included deduplication achieved by veri- fying and comparing the MD5 checksums of the files. While we integrated certain publicly accessible MIDI datasets from previous research endeavours, it is note- worthy that over 50% of the GigaMIDI dataset was ac- quired through web-scraping and organized by the au- thors. The GigaMIDI dataset includes per-track loop de- tection, adapting the loop detection and extraction al- gorithm presented in (Adkins et al., 2023) to MIDI files. In total, 7,108,181 loops with lengths rang- ing from 1 to 8 bars were extracted from GigaMIDI tracks, covering all types of MIDI instruments. Details and analysis of the extracted loops from the GigaMIDI dataset will be shared in a companion paper report via our GitHub page."}, {"title": "3.2 Collection and Preprocessing of GigaMIDI Dataset", "content": "The authors manually collected and aggregated the GigaMIDI dataset, applying our heuristics for MIDI- based expressive music performance detection. This aggregation process was designed to make large-scale symbolic music data more accessible to music re- searchers. Regarding data collection, we manually gathered freely available MIDI files from online sources like Zenodo, GitHub, and public MIDI repositories by web scraping. The source links for each subset are provided via our GitHub webpage. During aggregation, files were organized and deduplicated by comparing MD5 hash values. We also standardized each subset to the General MIDI (GM) specification, ensuring coherence; for example, non-GM drum tracks were remapped to GM. Manual curation was employed to assess the suit- ability of the files for expressive music performance de- tection, with particular attention to defining ground truth tracks for expressive and non-expressive cate- gories. This process involved systematically identify- ing the characteristics of expressive and non-expressive MIDI track subsets by manually checking the charac-"}, {"title": "3.3 Descriptive Statistics of the GigaMIDI Dataset", "content": "The GigaMIDI dataset analysis reveals that most MIDI note events (77.6%) are found in two instru- ment groups: piano and drums. The piano instru- ment group has more MIDI note events (60.2%) be- cause most piano-based tracks are longer. The higher number of MIDI notes in piano tracks compared to other instrumental tracks can be attributed to several factors. The inherent nature of piano playing, which involves ten fingers and frequently includes simulta- neous chords due to its dual-staff layout, naturally in- creases note density. Additionally, the piano's wide pitch range, polyphonic capabilities, and versatility in musical roles allow it to handle melodies, harmonies, and accompaniments simultaneously. Piano tracks are often used as placeholders or sketches during compo- sition, and MIDI input is typically performed using a keyboard defaulting to a piano timbre. These charac- teristics, combined with the cultural prominence of the piano and the practice of condensing multiple parts into a single piano track for convenience, result in a higher density of notes in MIDI datasets. The GigaMIDI dataset includes a significant pro- portion of drum tracks (17.4%), which are generally shorter and contain fewer note events compared to pi- ano tracks. This is primarily because many drum tracks are designed for drum loops and grooves rather than for full-length musical compositions. Sound effects, including breath noise, bird tweets, telephone rings, applause, and gunshot sounds, exhibit minimal usage, account- ing for only 0.249% of the dataset. Chromatic per- cussion (2.4%) stands for pitched percussions, such as glockenspiel, vibraphone, marimba, and xylophone."}, {"title": "4. Heuristics for MIDI-based Expressive Mu- sic Performance Detection", "content": "Our heuristic design centers on analyzing variations in velocity levels and onset time deviations from a met- ric grid. MIDI velocity replicates the hammer velocity in acoustic pianos, where the force applied to the keys determines the speed of the hammers, subsequently af- fecting the energy transferred to the strings and, conse- quently, the amplitude of the resulting vibrations. This concept is integrated into MIDI keyboards, which repli- cate hammer velocity by using MIDI velocity levels to control the dynamics of the sound. A velocity value of 0 produces no sound, while 127 indicates maximum intensity. Higher velocity values yield louder notes, while lower ones result in softer tones, analogous to dynamics markings like pianissimo or fortissimo in tra- ditional performance. Onset time deviations in MIDI represent the difference between the actual note tim-"}, {"title": "4.1 Baseline Heuristic: Distinct Number of Velocity Levels and Onset Time Deviations", "content": "This baseline heuristic focuses solely on analyzing the count of distinct velocity levels (\"distinct velocity\") and unique onset time deviations (\"distinct onset\") without considering the MIDI track length. Generally, longer MIDI tracks show more distinct velocities and onset deviations than shorter ones. Designed as a simpler alternative to the more sophisticated Heuristics 1 and 2, this baseline has limited accuracy for MIDI tracks of varying lengths, as it does not adjust for track dura- tion. However, this was not a significant issue during heuristic evaluation in Section 5.2, as most tracks in the evaluation set are longer and have a limited vari- ance in terms of length. Our baseline heuristic design counts the number of unique velocity levels and onset time deviations present in a MIDI track. For example, consider a MIDI track where v = [64, 72, 72, 80, 64, 88] represents the MIDI velocity values, and o = [-5, 0, 5, -5, 10, 0] repre- sents the onset time deviations in MIDI ticks. Applying our heuristic, we first store only the unique values in each list: for v, the distinct velocity levels are {64, 72, 80, 88}, and for o, the distinct onset time deviations are {-5, 0, 5, 10}. By counting these unique values, we identify four distinct velocity levels and four dis- tinct onset time deviations for this MIDI track, with no deviation being treated as a specific occurrence."}, {"title": "4.2 Distinctive Note Velocity/Onset Deviation Ratio (DNVR/DNODR)", "content": "Distinctive note velocity and onset deviation ratios measure the proportion (in %) of unique MIDI note velocities and onset time deviations in each MIDI track. These metrics form a set of heuristics for detect- ing expressive performances, classified into four cate- gories: Non-Expressive (NE), Expressive-Onset (EO), Expressive-Velocity (EV), and Expressively-Performed (EP), as shown in Figure 1. The DNVR metric counts unique velocity levels to differentiate between tracks with consistent velocity and those with expressive ve- locity variation, while the DNODR calculation helps identify MIDI tracks that are either perfectly quantized or have minimal microtiming deviations"}, {"title": "Heuristic 1 Calculation of Distinctive Note Veloc- ity/Onset Deviation Ratio (DNVR/DNODR)", "content": "Heuristic 1 is proposed to analyze the variation in velocity levels and onset time deviations within a MIDI track. Here, Xvelocity holds each track's velocity values, while Xonset contains onset deviations from a quantized MIDI grid based on the track's TPQN. For example, a possible set of values could be Xvelocity = {88,102,...} and Xonset = {-3,2,5,...}, the latter being represented in ticks. The functions Cvelocity and conset return the count of unique velocity levels and onset time devia- tions per track, respectively. Next, Conset-ratio is divided by the track's TPQN to represent the proportion of mi- crotiming positions within each quarter note. Similarly, Cvelocity-ratio is divided by 127 (the range of possible velocity levels). Finally, each ratio is converted to a percentage by multiplying by 100."}, {"title": "4.3 MIDI Note Onset Median Metric Level (NOMML)", "content": "Figure 6 displays the classification of various note on- sets into duple metric levels 0-5. Let us define k as the parameter that controls the metric level's depth. The duple onset metric level (dup) grid divides the beat into even subdivisions, such as halves or quarters, cap- turing rhythms in duple meter. The triplet onset met- ric level (trip) grid divides the beat into three equal parts, aligning with triplet rhythms commonly found in swing and compound meters. Notably, since the grey- coloured note onset (ML < 1/128 note metric level) does not belong to any dupi for 0 \u2264 i \u2264 5, it is assigned to the extra category shown in the bottom row because it is finer than the maximum metric level where k = 6. For example, Figure 6 displays the metric level depth. The duple metric level dups divides each quarter note into 2k equal pulses, while the triplet metric level tripk divides it into 3 \u00d7 2k pulses. For our experiments, we choose k = 6. Consequently, the maximum metric lev- els we consider are dup6 and trip6, corresponding to the 128th notes. Based on our observation of data in MIDI tracks, this provides a sufficient level of granular- ity, given the note durations frequently found in most forms of music."}, {"title": "Heuristic 2 Calculation of Note Onset Median Metric Level (NOMML)", "content": "In Heuristic 2, we propose MIDI note onset median metric level (NOMML), another heuristic for detect- ing non-expressive and expressively-performed MIDI tracks. This heuristic counts the median metric level of note onsets. The metric level ml(x) for a note onset x is the lowest duple or triplet level that aligns with the onset. Since some pulses overlap between duple and triplet levels, we prioritize duple levels before consid- ering triplets. For instance, with 120 ticks per quarter note, a note onset a at tick 60 aligns with pulses on all metric levels dupi for i \u2265 1 and tripj for j \u2265 2. Here, the lowest matching levels are dup1 and trip2, so, by prioritizing duple levels, ml(a) = dup1. Conversely, a note onset b at tick 40 aligns only with triplet levels, resulting in ml(b) = trip1. Given a list of note onset times (o), Heuristic 2 cal- culates the median metric level. The list c is used to store the metric levels for each note onset, so after ex- ecuting lines 4-17, we have c = [ml(o1), ..., ml(on)]. For example, we have a list of metric levels for note on- sets: c = [2,3,4,6,3,7,8,3,4]. To calculate the median, we first sort c as follows: c = [2,3,3,3,4,4,6,7,8]. Since the list contains 9 values, the median is the middle el- ement, which is the 5th value in the sorted list. Thus, the median metric level for c is 4. In lines 4-9, the lowest duple metric level is deter- mined for each note onset oi. The condition in line 10 is met only when oi does not belong to any duple met- ric level. Here, ||c|| denotes the current length of c. If oi does not match a duple level, lines 11-15 determine the lowest triplet metric level. When oi does not be- long to any duple or triplet level, it is assigned to an extra category containing both dupi and tripj for any i \u2265 k (lines 16-17). To calculate the median metric level, each level is assigned a unique numerical value. Duple and triplet metric levels are interleaved to ensure a meaningful median: duple levels are represented by even numbers (dupi = 2i) and triplet levels by odd numbers (tripj = 2i + 1)."}, {"title": "5. Threshold and Evaluation of Heuristics for Expressive Music Performance Detection", "content": "Optimal threshold selection involves a structured ap- proach to determine the best threshold for distinguish- ing between non-expressive (NE) and expressively- performed (EP) tracks. A machine learning regressor aids in identifying this threshold, evaluated using met- rics such as classification accuracy and the P4 metric"}, {"title": "6. Limitations", "content": "In navigating the use of MIDI datasets for research and creative explorations, it is imperative to consider the ethical implications inherent in dataset bias (Born, 2020). Bias in MIDI datasets often mirrors prevailing practices in Western digital music production, where certain instruments, particularly the piano and drums, as illustrated in Figure 7 (b), dominate. This predomi- nance is largely influenced by the widespread availabil- ity and use of MIDI-compatible instruments and con- trollers for these instruments. The piano is a primary compositional tool and a ubiquitous MIDI controller and keyboard, facilitating input for a wide range of virtual instruments and synthesizers. Similarly, drums, whether through drum machines or MIDI drum pads, enjoy widespread use for rhythm programming and beat production. This prevalence arises from their intuitive interface and versatility within digital audio workstations. This may explain why the distribution of MIDI instruments in MIDI datasets is often skewed toward piano and drums, with limited representation of other instruments, particularly those requiring more nuanced interpretation or less commonly played via MIDI controllers or instruments. Moreover, the MIDI standard, while effective for en- coding basic musical information, is limited in repre- senting the complexities of Western music's time signa- tures and meters. It lacks an inherent framework to en- code hierarchical metric structures, such as strong and weak beats, and struggles with the dynamic flexibility of metric changes. Additionally, its reliance on fixed temporal grids often oversimplifies expressive rhyth- mic nuances like rubato, leading to a loss of critical mu- sical details. These constraints necessitate supplemen- tary metadata or advanced techniques to accurately capture the temporal intricacies of Western music. Furthermore, a constraint emerges from the inad- equate accessibility of ground truth data that clearly demarcates the differentiation between non-expressive and expressive MIDI tracks across all MIDI instruments for expressive performance detection. Presently, such data predominantly originates from piano and drum instruments in the GigaMIDI dataset."}, {"title": "7. Conclusion and Future Work", "content": "Analyzing MIDI data may benefit symbolic music gen- eration, computational musicology, and music data mining. The GigaMIDI dataset may contribute to MIR research by providing consolidated access to extensive MIDI data for analysis. Metadata analyses, data source references, and findings on expressive music perfor- mance detection may enhance nuanced inquiries and foster progress in expressive music performance analy- sis and generation. Our novel heuristics for discerning between non- expressive and expressively-performed MIDI tracks ex- hibit notable efficacy on the presented dataset. The NOMML (Note Onset Median Metric Level) heuristic demonstrates a classification accuracy of 100%, under- scoring its discriminative capacity for expressive music performance detection. Future work on the GigaMIDI dataset could signif- icantly advance symbolic music research by using MIR techniques to identify and categorize musical styles systematically across all MIDI files. Currently, only about one-fifth of the dataset includes style meta- data; expanding this would improve its comprehen- siveness. Track-level style categorization, rather than file-level, would better capture the mix of styles in gen- res like rock, jazz, and pop. Additionally, adding meta-"}, {"title": "8. Data Accessibility and Ethical Statements", "content": "The GigaMIDI dataset consists of MIDI files acquired via the aggregation of previously available datasets and web scraping from publicly available online sources. Each subset is accompanied by source links, copyright information when available, and acknowledgments. File names are anonymized using MD5 hash encryp- tion. We acknowledge the work from the previous dataset papers that we aggregate and analyze as part of the GigaMIDI subsets. This dataset has been collected, utilized, and dis- tributed under the Fair Dealing provisions for research and private study outlined in the Canadian Copyright Act . Fair Dealing per- mits the limited use of copyright-protected material without the risk of infringement and without having to seek the permission of copyright owners. It is in- tended to provide a balance between the rights of cre- ators and the rights of users. As per instructions of the Copyright Office of Simon Fraser University, two protective measures have been put in place that are deemed sufficient given the nature of the data (acces- sible online): 1. We explicitly state that this dataset has been col- lected, used, and distributed under the Fair Deal- ing provisions for research and private study out- lined in the Canadian Copyright Act. 2. On the Hugging Face hub, we advertise that the data is available for research purposes only and collect the user's legal name and email as proof of agreement before granting access. We thus decline any responsibility for misuse. The FAIR (Findable, Accessible, Interoperable, Reusable) principles (Jacobsen et al., 2020) serve as a framework to ensure that data is well-managed, easily discoverable, and usable for a broad range of purposes in research. These principles are particularly important in the context of data management to facilitate open science, collaboration, and reproducibility. Findable: Data should be easily discoverable by both humans and machines. This is typi- cally achieved through proper metadata, trace-"}, {"title": "9. Acknowledgements", "content": "We gratefully acknowledge the support and contribu- tions that have directly or indirectly aided this re- search. This work was supported in part by funding from the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Social Sciences and Humanities Research Council of Canada (SSHRC). We also extend our gratitude to the School of Interac- tive Arts and Technology (SIAT) at Simon Fraser Uni- versity (SFU) for providing resources and an enriching research environment. Additionally, we thank the Cen- tre for Digital Music (C4DM) at Queen Mary University of London (QMUL) for fostering collaborative opportu- nities and supporting our engagement with interdisci- plinary research initiatives. We also acknowledge the support of EPSRC UKRI Centre for Doctoral Training in AI and Music (Grant EP/S022694/1) and UKRI - Inno- vate UK (Project number 10102804). Special thanks are extended to Dr. Cale Plut for his meticulous manual curation of musical styles and to Dr. Nathan Fradet for his invaluable assistance in developing the HuggingFace Hub website for the GigaMIDI dataset, ensuring it is accessible and user-friendly for music computing and MIR researchers. We also sincerely thank our research interns, Paul Triana and Davide Rizzotti, for their thorough proofreading of the manuscript, as well as the TISMIR reviewers who helped us improve our manuscript. Finally, we express our heartfelt appreciation to the individuals and communities who generously shared their MIDI files for research purposes. Their contribu- tions have been instrumental in advancing this work and fostering collaborative knowledge in the field."}, {"title": "10. Competing Interests", "content": "The authors have no competing interests to declare."}, {"title": "11. Authors' Contributions", "content": "The authors confirm their contributions to the manuscript as follows: Study Conception and Design: Keon Ju Maver- ick Lee, Jeff Ens, Pedro Sarmento, Mathieu Bar- thet, and Philippe Pasquier. Data Collection and Metadata: Keon Ju Maver- ick Lee, Jeff Ens, Pedro Sarmento, and Sara Ad-"}, {"title": "B. Model Selection and Hyperparameter Settings for Optimal Threshold Selection of Heuristics for Expressive Music Performance Detection", "content": "Following a series of comparative experiments involving logistic regression, decision trees, and random forests each implemented using the scikit-learn library-logistic regression was chosen as the most suitable machine learning algorithm for determining optimal thresholds to differentiate between non-expressive and ex- pressive MIDI tracks. This selection was made based on the ground truth data we manually collected, which informed the model's performance evaluation and final decision. The choice of a machine learning model for identifying optimal thresholds between two classes, such as non-expressive and expressively-performed MIDI tracks, requires careful consideration of the data's specific char- acteristics and the analysis goals. Logistic regression is often favoured when the relationship between the input features and the target class is approximately linear. This model provides a clear, interpretable framework for classification by modelling the probability that a given input belongs to one of the two classes. The output of logistic regression is a continuous probability score between 0 and 1, which allows for straightforward determina- tion and adjustment of the decision threshold. This simplicity and directness make logistic regression particularly appealing when the primary objective is to identify a reliable and easily interpretable threshold. However, logistic regression has limitations, particularly when the true relationship between the features and the outcome is non-linear or complex. In such cases, decision trees and random forests offer more flexibility. Decision trees can capture non-linear interactions between features by partitioning the feature space into distinct regions associated with a specific class. Random forests, as ensembles of decision trees, enhance this flexibility by averaging the predictions of multiple trees, thereby reducing variance and improving generalization. These models can model complex relationships that logistic regression might miss, making them more suitable for datasets where the linear assumption of logistic regression does not hold. Regarding threshold determination, logistic regression has a distinct advantage due to its probabilistic output. The model naturally provides a probability estimate for each instance, and a threshold can be easily applied to classify instances into one of the two classes. This straightforward approach to threshold selection is one of the key reasons logistic regression is often chosen for tasks requiring clear and interpretable decision boundaries. In contrast, decision trees and random forests do not inherently produce probability scores similarly. While they can be adapted to generate probabilities by considering the distribution of classes within the leaf nodes for decision trees or across the trees in the forest for random forests, this process is more complex and can make threshold selection less intuitive. In our computational experiment, the logistic regression machine learning model, combined with manual threshold inspection for validation, was found to be sufficient for identifying the optimal threshold for each heuristic. This approach was particularly effective given the simplicity of the task, which involved a single feature for each of the three key metrics\u2014Distinctive Note Velocity Ratio (DNVR), Distinctive Note Onset Deviation Ratio (DNODR), and Note Onset Median Metric Level (NOMML)\u2014and the classification of data into two categories: non-expressive and expressive tracks. The problem at hand, being a straightforward binary classification task using a supervised learning algorithm, aligned well with the capabilities of logistic regression, thereby rendering it an appropriate choice for our optimal threshold selection."}, {"title": "B.2 Hyperparameter Settings and Training Details", "content": "The process of training a logistic regression model using the leave-one-out cross-validation (LOOCV) method requires a methodical approach to ensure robust model performance. Leave-one-out cross-validation is a special case of k-fold cross-validation where the number of folds equals the number of instances in the dataset. In this method, the model is trained on all data points except one, which is used as the validation set, and this process is repeated for each data point. The advantage of LOOCV lies in its ability to maximize the use of available data for training while providing a nearly unbiased estimate of model performance. However, due to its computational intensity, especially with large datasets, careful consideration is given to the selection and tuning of hyperparameters to optimize the model's performance. In our case, we trained our models with 722 instances using LOOCV, a relatively small amount of data available with the ground truth of non-expressive and expressive tracks due to the scarcity of such ground truth available for expressive music performance detection. The training environment for our experiments was configured on a MacBook Pro, equipped with an Apple M2 CPU and 16GB of RAM, without the use of external GPUs. Our analysis, which included evaluation using the P4 metric alongside basic metrics such as classification accuracy, precision, and recall, did not indicate any significant impact on performance attributable to the computational setup. Furthermore, we share three logistic regression models in .pkl format, each trained on a specific heuristic, accessible via GitHub. These models correspond to the following heuristics: baseline heuristics, Distinctive Note Velocity Ratio (DNVR), trained in less than 10 minutes;"}]}