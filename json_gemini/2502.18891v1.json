{"title": "Dynamic Classification: Leveraging Self-Supervised Classification to Enhance Prediction Performance", "authors": ["ZHONG Ziyuan", "ZHOU Junyang"], "abstract": "In this paper, we propose an innovative dynamic classification algorithm designed to achieve the objective of zero missed detections and minimal false positives. The algorithm partitions the data into N equivalent training subsets and N prediction subsets using a supervised model, followed by independent predictions from N separate predictive models. This enables each predictive model to operate within a smaller data range, thereby improving overall accuracy. Additionally, the algorithm leverages data generated through supervised learning to further refine prediction results, filtering out predictions that do not meet accuracy requirements without the need to introduce additional models. Experimental results demonstrate that, when data partitioning errors are minimal, the dynamic classification algorithm achieves exceptional performance with zero missed detections and minimal false positives, significantly outperforming existing model ensembles. Even in cases where classification errors are larger, the algorithm remains comparable to state-of-the-art models.\nThe key innovations of this study include self-supervised classification learning, the use of small-range subset predictions, and the direct rejection of substandard predictions. While the current algorithm still has room for improvement in terms of automatic parameter tuning and classification model efficiency, it has demonstrated outstanding performance across multiple datasets. Future research will focus on optimizing the classification component to further enhance the algorithm's robustness and adaptability.", "sections": [{"title": "1 Background and Introduction", "content": "The rapid increase in global energy demand has not only exacerbated environmental pollution but also intensified climate change and deepened the energy crisis. As a result, the development of renewable energy sources has become a crucial direction for achieving global sustainable development [1]. Currently, renewable energy sources such as solar, wind, and biomass are gradually becoming key alternatives to fossil fuels. However, as of 2017, fossil energy still accounted for 73.5% of the global electricity supply, while renewable energy made up only 26.5% [3]. Promoting the research, development, and application of new energy technologies has been recognized as a core strategy to address future energy shortages and environmental challenges.\nEnergy storage technology, especially battery technology, plays a critical role in new energy systems. It not only promotes the efficient use of renewable energy but also plays a vital role in transforming energy structures [1]. Lithium-ion batteries, liquid current batteries, sodium-sulfur batteries, and other energy storage technologies have been rapidly developed in recent years, showing significant improvements in energy density, safety, and cost. However, there remains a gap in realizing the goal of \"high safety, low cost, long lifespan, and environmental friendliness\" [2]. Therefore, future technology development in energy storage is expected to feature the coexistence of various technological approaches.\nIn recent years, with the in-depth study of battery materials, new battery technologies have emerged. Research has shown that optimizing material composition and battery design can significantly enhance battery performance, such as increasing energy density and cycle life [1]. Furthermore, the introduction of the circular economy concept has provided new directions for the sustainable development of batteries, such as efficient material recycling and reuse [2]. Nonetheless, battery technology still faces many challenges in achieving the goals of a fully sustainable lifecycle and environmental protection. Future research should focus on discovering new materials and the synergistic application of multiple technological approaches [1, 2].\nThe application of artificial intelligence (AI) in the field of energy storage shows great potential. Using machine learning and big data technologies, AI accelerates the discovery, performance prediction, and optimal design of new materials. For instance, the materials genome program developed using AI has significantly improved the efficiency of battery materials development [4]. Additionally, AI has demonstrated strong capabilities in battery condition monitoring, fault diagnosis, and performance prediction, providing robust support for advancing battery technology [4]. In the future, AI is expected to further drive innovation and widespread adoption of battery technologies, injecting new vitality into energy storage development [4].\nThe rapid advancement of AI technology offers a new opportunity for battery technology innovation. In recent years, machine learning algorithms have been widely applied in battery performance prediction and state-of-health (SOH) estimation, primarily focusing on non-invasive methods to obtain microstate information inside batteries [6]. For example, using data-driven machine learning models, charging and discharging curve characteristics can be analyzed to predict the capacity degradation and remaining lifetime of a battery [5]. Moreover, AI has accelerated the development of novel energy storage materials through high-throughput computing and material genome programs, significantly improving research efficiency and material screening accuracy [4].\nRegarding AI's application in lithium-ion batteries, current research mainly focuses on model-based and data-based approaches [5]. Model-based methods analyze the chemical reactions and degradation mecha-nisms inside the battery. Though theoretically reflective of battery operations, these models often suffer from large errors due to complex internal chemical reactions and environmental factors [6]. Data-based methods, such as convolutional neural networks (CNN), recurrent neural networks (RNN), and support vector ma-chines (SVM), have attracted attention for their flexibility and accuracy in nonlinear problems [5]. For instance, CNN and RNN models optimized by genetic algorithms (GA) have significantly improved battery capacity prediction, achieving a root mean square error (RMSE) of only 0.1176%, representing a considerable advancement over previous models.\nDespite these advances, challenges remain. The predictive performance of data-driven approaches relies heavily on high-quality data, which is costly and limited in quantity[6]. In addition, due to the different internal structures and operating conditions of different batteries, the generalization ability of the models is weak, making it difficult to adapt to complex and changing real-world situations[1]. Meanwhile, traditional methods usually rely on trial and error in parameter setting, a time-consuming and labor-intensive process that limits the practical application of these models[5]. Finally, there is a need in industry for a reliable"}, {"title": "2 Related Work", "content": "Numerous studies have explored methods to improve model prediction accuracy, with a focus on optimization algorithms, machine learning, and hybrid approaches. This section reviews key advancements in the field, highlighting their methodologies, strengths, and limitations, and provides a comparative analysis with the proposed Dynamic Classification Algorithm (DCA).\nAzevedo et al. [7] conducted a systematic review of hybrid optimization and machine learning methods for clustering and classification. Their approach integrated various techniques to overcome the limitations of single methods. However, their framework required significant computational resources and faced challenges in adapting to rapidly evolving data environments. Kotary et al. [8] proposed a joint prediction and optimization learning framework that directly learns optimal solutions from observable features. Despite its theoretical innovation, the approach demands end-to-end training for optimization problems, which can be computationally impractical, especially for nonconvex or discrete scenarios.\nKhan et al. [9] combined neural networks with traditional filters (e.g., Kalman and \u03b1-\u03b2 filters) to enhance the prediction accuracy of dynamic systems under noisy conditions. While this method demonstrated significant improvements in dealing with uncertainty, it relied on complex parameter tuning, limiting its applicability in resource-constrained environments. Ippolito et al. [10] combined supervised and unsupervised learning to improve stratigraphic classification accuracy, offering probability distributions rather than discrete classifications. However, their method required extensive data preprocessing and feature engineering, which increased computational costs.\nSon et al. [11] applied both supervised and unsupervised learning to predict social media user engage-ment in the airline industry. Although innovative in its use of sentiment analysis for classification, the model required intensive data cleaning and customization for different business contexts. Shah and Satyanarayana [12] introduced a predictive range corrector model to enhance reliability by accounting for deviations between observed and predicted outputs. While this approach improved prediction accuracy, its reliance on control sequence optimization made it less scalable for broader applications.\nDalal et al. [13] developed a supply chain optimization model using Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory Networks (BiLSTMs). Their method improved sus-tainability by leveraging CNNs for resource allocation and BiLSTMs for temporal demand forecasting. Similarly, Guill\u00e9n et al. [14] introduced the LSB-MAFS technique, combining Least Squares Boosting (LSB) and Multivariate Adaptive Regression Spline (MARS) to enhance production frontier forecasting. While effective, the method primarily addressed specific regression tasks, limiting its adaptability.\nCurrently, data prediction results are often filtered through additional classification models. However, these models typically struggle to establish a reliable balance between missed detections and false positives in practical applications. This approach not only increases computational overhead but also risks over-filtering or omitting crucial data, thereby compromising the overall reliability of the prediction system. In contrast, the proposed Dynamic Classification Algorithm (DCA) employs self-supervised learning to automatically partition data subsets and, by predicting within small classification errors, directly filters results based on the distributional characteristics of the training set-without the need for additional models. The dynamic classification method ensures that the range for each subset is accurately defined, which allows the algorithm"}, {"title": "3 Abbreviations and Keywords List", "content": ""}, {"title": "4 Algorithm Structure Overview", "content": "The Dynamic Classification Algorithm (DCA) is designed to enhance prediction accuracy by partitioning the data into smaller, well-defined subsets. The algorithm consists of four core components, each contributing significantly to the overall effectiveness:\n\u2022 Routine Operations - Standard preprocessing tasks such as data cleaning, normalization, and feature selection, which are essential in machine learning workflows.\n\u2022 Dynamic Classification Process The core innovation of DCA, where self-supervised learning iteratively refines data segmentation, improving classification boundaries.\n\u2022 Redundant Training and Prediction - A mechanism that enhances stability by incorporating data from neighboring subsets, improving prediction accuracy.\n\u2022 Excluding Predicted Results \u2013 A filtering strategy that removes predictions failing to meet accuracy requirements, ensuring retained results achieve a balanced zero missed detections and minimal false positives.\nThis section provides a detailed breakdown of the algorithm's structure, classification principles, redun-dant training mechanism, and the filtering of predicted results. The overall structure is illustrated in Fig.1, which divides the framework into four main components.\nThe **routine processing module (white section)** consists of fundamental preprocessing tasks such as data cleaning, normalization, and feature selection. These steps ensure data quality before classification and prediction."}, {"title": "4.1 Principle of Dynamic Classification Processes", "content": "The **Dynamic Classification Process** is a fundamental component of the Dynamic Classification Algorithm (DCA). In traditional classification models, the learning process typically begins by extracting feature representations, followed by clustering or classification. However, this approach can lead to the coexistence of high and low values within the same classification, a phenomenon that diminishes the predictive model's ability to generalize effectively. Specifically, when target values span a wide numerical range, this inconsis-tency complicates the model's learning process, as the features may be similar while the predicted values diverge significantly.\nTo address this issue, the DCA leverages **self-supervised learning** to learn from the distribution of predicted target values. The algorithm dynamically partitions the dataset into smaller, more homogenous subsets, iteratively adjusting classification boundaries until an optimal segmentation is found. This ensures that each subset contains target values within a narrower range, thereby improving prediction accuracy.\nThe **Dynamic Classification Process** consists of six key modules: **Division Module**, **Classification Model Module**, **Convergence Judgment Module**, **Penalty Module**, **Correction Module**, and **Evaluation Module**, as illustrated in Fig.2.\nDuring the initialization phase, the training set is divided into $Train_t$ and $Train_p$, using a **1:1 ratio**, ensuring a balanced training and validation setup.\nDivision Module: The first step involves manually defining the number of planned classifications, **N**. The initial classifier applies a **Gaussian kernel function** to measure the distribution of values in the target column of $Train_t$, and the data is then divided based on the degree of fluctuation in the sorted values. Larger fluctuations indicate finer divisions, ensuring more precise segmentation (see Fig.3). This"}, {"title": "4.2 Principle of Redundant Prediction", "content": "The best model identified in the Evaluation Module will classify both the training and prediction sets. The corresponding subsets within their respective intervals are then obtained, as shown in Table 2.\nIn this process, slight perturbations in the classification of the prediction set are inevitable. These may manifest as minor overlaps between prediction intervals, caused by fluctuations in prediction data or minor data entry errors. To resolve this, redundant training is introduced. This technique combines the training data from a target classification interval with data from adjacent intervals. Data from neighboring intervals is selected according to two criteria:"}, {"title": "4.3 Excluding Predicted Results", "content": "The boundaries of each subset are initially determined by the OptimalSegmentationList, which is obtained through supervised learning from the training set. However, these boundaries can be manually fine-tuned according to accuracy requirements to achieve stable performance and higher precision. This ensures that predictions falling within the valid range are retained, while those deemed inaccurate are removed.\nThe exclusion process operates as follows: once the predictions are generated, they are compared against the predefined segmentation boundaries. Predictions that fall outside their corresponding subset's range are flagged as unreliable and subsequently discarded. This ensures that only the most accurate predictions are retained, enhancing the overall reliability of the classification results.\nCritically, the exclusion mechanism is designed to guarantee zero missed detections, meaning that all valid predictions are preserved. Within this constraint, adjustments to the segmentation boundaries focus on reducing false positives as much as possible. By dynamically refining these parameters, the exclusion module contributes to a more precise and robust prediction system, allowing for high accuracy with minimal unnecessary exclusions."}, {"title": "4.4 Algorithm Advantages and Effectiveness", "content": "In practical applications, prediction-based models often face two main challenges. First, balancing the impact of samples with lower and higher target values is difficult. When the training set is sampled with a normal distribution, the predictions tend to concentrate around the middle of the value range, and using uniform sampling only slightly mitigates this issue. The second challenge arises when trying to identify inaccurate predictions. After predictions are made, it is necessary to categorize and re-assess them against an accuracy threshold, which often leads to either missed detections or over-exclusion of data.\nThe Dynamic Classification Algorithm (DCA) effectively addresses these issues by partitioning the data into smaller, well-defined subsets. This segmentation reduces the range within which each model operates, allowing it to focus on narrower, more homogeneous data intervals. As a result, the complexity of predictions is minimized, improving the accuracy of the model. Importantly, this approach ensures that each subset operates independently, reducing errors that might arise from overlapping data distributions or perturbations in predictions.\nWithout dynamic classification, unsupervised models may fail to properly distinguish between features with similar values, leading to inaccurate predictions, particularly when the feature distributions are broad. Moreover, in traditional prediction structures, re-learning and selection of new models after predictions often exacerbate these issues, making them computationally expensive and inefficient.\nThe Dynamic Classification Algorithm (DCA) effectively reduces the complexity of predictions by lim-iting each subset's data range. By segmenting the data into quantifiable intervals, DCA can precisely filter prediction results, ensuring that only predictions that meet the required accuracy are retained. This method is particularly significant in industrial applications, as it eliminates the need for additional models or complex retraining steps, simplifying the prediction process.\nFurthermore, DCA ensures that the model achieves the goal of **zero missed detections** while min-imizing **false positives** as much as possible. By carefully adjusting the segmentation boundaries, the algorithm allows for stable, high-accuracy predictions with minimal exclusion. This balancing act is crucial"}, {"title": "5 Data Testing Situation", "content": "In battery production, consistency is crucial. In production processes involving chemical and sorting stages, cells must undergo discharge testing to verify that their capacity meets the required standards. The test data for this study comes from actual factory production, consisting of three different production batches, and is used to predict cell capacity. The necessary information about these batches has been anonymized and is provided in Table 3.\nThe data for these three batches follows a normal distribution. Feature extraction primarily relies on characteristics from the middle production stages, such as weight, time, density, and voltage features from the formation and capacity sorting stages. Data preprocessing steps include filtering, temperature compensation, and removing outliers using the interquartile range method. The dynamic classification algorithm can be applied to both uniform and normal distributions, although parameter adjustments may vary."}, {"title": "5.1.1 Comparison Plan and Indicator Settings", "content": "Due to confidentiality, the specific numerical values can only be presented approximately. The total number of features is approximately 100, of which about 20% originate from intrinsic battery cell characteristics (such as weight, coating density, etc.), while the remaining 80% are derived from data generated during the production process, including voltage, temperature, and capacity data. In the data preprocessing stage, outliers were handled using the interquartile range method. Specifically, 17% of the outliers were filtered from the 22LA dataset, 19% from the 27LA dataset, and 14% from the 13LA dataset. The remaining data underwent normalization and standardization.\nSince average accuracy alone is not sufficient to effectively differentiate between models, this study selected more detailed indicators to assess model performance. These include the proportion of samples with prediction accuracy between 99% and 100% relative to the total, and the proportion of samples with prediction accuracy between 99.5% and 100%. The higher the percentage of these two indicators, the greater the reliability and precision of the prediction.\nThe experiment compared three methods: KMeans model + prediction, GMM model + prediction, and dynamic classification model. All of these models use linear regression as the underlying prediction method, while the direct prediction results (DP) of the linear model are also recorded as comparison data. The training and prediction sets were divided in a 3:7 ratio, using the same features and outlier detection"}, {"title": "5.1.2 Comparison Analysis", "content": "Table 4 records all test results with the best prediction effect.\nAfter completing these steps, in order to achieve the goal of zero missed detections and minimal false positives, we must exclude predictions that are inaccurate. To evaluate this step, we introduce comparisons with advanced models like XGBoost, Gaussian classification models, and Random Forest. These models, as advanced methods, are more capable of capturing detailed information, which typically results in better performance.\nSince introducing a new model requires a certain amount of data for training, let's assume we know that some data is predicted inaccurately, and this data will serve as the training set. This assumption is made to better illustrate the advantages of the dynamic classification algorithm in its mechanism. Assume that the data with prediction accuracy below 99% is J, and we divide J into 10 parts, supplementing them with randomly selected data that meets the accuracy threshold, ensuring the training set size remains constant at J, with consistent features. Each model will use 9 different training sets, where the ratio of inaccurate to accurate data starts at 1:9 and gradually increases until it reaches 9:1. Finally, we will record the missed detection rate, false positive rate, missed detections count, and false positives count. The results from the three models are shown in Fig.4.\nAmong the experimental results from these three models, all models achieved a good balance with a 4:6 training-to-test data ratio. The corresponding metrics for false positives and false negatives are depicted in the graph. The bar sections represent the miss rate (blue) and overkill rate (red), while the line sections indicate the absolute number of false positives and false negatives. We can observe that as the proportion of non-compliant data in the training set increases, the miss rate significantly decreases, but the overkill rate gradually increases. This result is the combined effect of changes in both the proportion of the training set and the non-compliant data in the prediction set.\nIn terms of achieving the goal of 'zero missed detections and minimal false positives,' XGBoost performed the best. When the proportion of non-compliant to compliant data was set at 8:2, XGBoost successfully excluded all non-compliant data, as seen in the low miss rate. However, the false positive rate remained significant, with the number of misclassified samples also notable. These results illustrate the trade-off in balancing the two metrics: even though misclassified instances were minimized, almost half of the remaining data was misidentified. This discrepancy highlights that, in real-world scenarios, it is hard for model to fully achieve the ideal of zero missed detections and minimal false positives, as production data often contain noise and errors that are difficult to discern. The graph visually represents the model's performance under different training-prediction ratios, showing the progression from a high miss rate to an acceptable level as the proportion of non-compliant data decreases.\nUnder the Dynamic Classification Algorithm (DCA) framework, the model can leverage information derived from effective self-supervised learning to filter out predictions with significant errors. After indepen-dent prediction, each predicted result should fall within its designated range. If a sample exceeds this range by more than \u00b15%, it can be considered inaccurate. If there are specific accuracy requirements, the range"}, {"title": "5.2 Open Source Data", "content": "Open-source datasets are sourced entirely from Kaggle. Four non-sequential datasets, representing various fields, were selected to validate the performance of the proposed model. Comparisons were made with the"}, {"title": "6 Summary and Next Steps", "content": "This paper introduces a dynamic classification algorithm (DCA) designed to address the limitations of existing prediction models. By leveraging self-supervised learning based on the distribution of sample data in the training set, the algorithm effectively narrows down the range of data within each subset, leading to improved prediction accuracy. The algorithm consists of three key components: a regular part, a classification part, and a prediction part. Each categorized subclass, containing both predicted and training data, allows the algorithm to predict smaller ranges independently, thus enhancing overall prediction results. As a consequence, DCA is highly sensitive to classification errors, and when these errors are minimized, the algorithm demonstrates excellent performance.\nIn the case of non-public datasets, particularly factory production data, the DCA achieves a classifi-cation error rate of less than 1%, significantly enhancing prediction accuracy. For public datasets, while the classification errors are slightly higher, the performance of DCA remains comparable to other advanced models, including Random Forest and XGBoost. Notably, in the Estimation Obesity dataset, where the classification error rate is only 1.4%, DCA delivers exceptional results, confirming its ability to utilize the first part of supervised classification to effectively support data filtering. This, in turn, allows the algorithm to outperform existing regression models.\nDespite its promising results, the DCA algorithm still requires optimization, particularly in the classi-fication phase. Future research will focus on three primary optimization areas:\n1. Automated Parameter Tuning: Developing an automated system to dynamically adjust the number of classifications and sensitivity based on the data distribution, ensuring more precise partitioning for improved classification performance.\n2. Classification Model Dependency: Enhancing the algorithm's robustness by reducing its dependence on the classification model's effectiveness. A stronger classification model will lead to better results, particularly when the feature correlation is weak.\n3. Avoiding Local Optima: Introducing mechanisms to prevent the algorithm from falling into local optima, ensuring that the model converges to the most optimal solution.\nThese optimizations will further improve the robustness and applicability of the DCA algorithm, enabling it to handle a wider range of data characteristics and real-world scenarios, and provide even more reliable and accurate predictions."}]}