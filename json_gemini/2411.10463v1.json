{"title": "Unexploited Information Value in Human-AI Collaboration", "authors": ["Ziyang Guo", "Yifan Wu", "Jason Hartline", "Jessica Hullman"], "abstract": "Humans and AIs are often paired on decision tasks with the expectation of achieving\ncomplementary performance \u2013 where the combination of human and AI outper-\nforms either one alone. However, how to improve performance of a human-AI\nteam is often not clear without knowing more about what particular information\nand strategies each agent employs. In this paper, we propose a model based in\nstatistically decision theory to analyze human-AI collaboration from the perspec-\ntive of what information could be used to improve a human or AI decision. We\ndemonstrate our model on a deepfake detection task to investigate seven video-level\nfeatures by their unexploited value of information. We compare the human alone,\nAI alone and human-AI team and offer insights on how the AI assistance impacts\npeople's usage of the information and what information that the AI exploits well\nmight be useful for improving human decisions.", "sections": [{"title": "Introduction", "content": "As the performance of artificial intelligence (AI) models makes remarkable advances, workflows in\nwhich humans and AIs collaborate have been sought for important decisions in medicine, finance, and\nother domains. Designing for human involvement is critical. While an AI model can usually make\npredictions with higher accuracy than the average human when the two use similar information [\u00c6gis-\nd\u00f3ttir et al., 2006, Grove et al., 2000, Meehl, 1954], in some cases a human must retain final control\nover the decision for liability reasons. When humans have access to additional information over the\nAI, there is the potential for a human-AI collaboration to achieve complementary performance, i.e.,\nbetter performance than either the human or AI alone. For example, a physician may have access to\nadditional information that may not be captured in tabular electronic health records or other structured\ndata [Alur et al., 2024]. Others argue that human theory-based causal logic can contribute knowledge\nthat AI data-based predictions can not learn from historic data [Felin and Holweg].\nHowever, evidence supporting complementary performance between humans and AI is limited, with\nmany studies showing that human-AI teams often underperform AI alone in tasks [Bu\u00e7inca et al.,\n2020, Bussone et al., 2015, Green and Chen, 2019, Jacobs et al., 2021, Lai and Tan, 2019, Vaccaro\nand Waldo, 2019, Kononenko, 2001]. Thus, numerous empirical studies attempt to explore design\nstrategies and conditions under which complementary human-AI performance can be achieved. For"}, {"title": "Model Setup", "content": "Information can be considered valuable to a decision-maker to the extent that it is possible in theory to\nincorporate it in their decisions to improve performance. Our approach analyzes the expected marginal\npayoff gain from ideal use of additional information over ideal use of the existing information in\nhuman decisions in decision tasks. In this section, we define the basis of this approach, including a\ndecision problem and associated information structure, following prior decision-theoretic frameworks\nfor studying decisions from statistical information [Wu et al., 2023, Guo et al., 2024, Hullman et al.,\n2024]. Then we define how a rational decision maker would act given a signal and a decision problem\nwith an associated information structure. Using the rational decision maker as a tool, we show how to\ninvestigate the information encoded in behavioral decisions.\nDecision Problem A decision problem consists of three key elements. We illustrate with an\nexample of a weather decision.\n\u2022 A payoff-relevant state \u03c9 from a space \u03a9. For example, \u03c9 \u2208 \u03a9 = {0,1} = {no rain, rain}.\n\u2022 A decision d from the decision space D characterizing the decision-maker (DM)'s choice. For\nexample, d \u2208 D = {0,1} = {not take umbrella, take umbrella}.\n\u2022 The payoff function S : D \u00d7 \u03a9 \u2192 R, used to assess the quality of a decision given a realization\nof the state, e.g., S(d = 0,\u03c9 = 0) = 0, S(d = 0,\u03c9 = 1) = \u2212100, S(d = 1,\u03c9 = 0) = \u221250, S(d =\n1,\u03c9 = 1) = 0, which punishes the DM for selecting an action that does not match the weather.\nInformation Model We cast the information available to a DM as a signal defined within an\ninformation structure. We use the definition of an information structure in Blackwell et al. [1951].\nThe information structure has two elements:\n\u2022 Signals. There are n \u201cbasic signals\" represented as random variables \u03a31,..., \u03a3\u03b7, from the\nsignal spaces \u03a31,..., \u03a3n. These represent information obtained by a decision-maker, e.g., \u03a3\u2081 =\n{cloudy, not cloudy}, \u03a32 \u2208 {0, . . ., 100} for temprature Celsius, etc."}, {"title": "Rational Decision Maker", "content": "We suppose a rational DM who knows the data-generating process, observes a signal realization,\nupdates their prior to arrive at posterior beliefs, and then chooses a decision to maximize their\nexpected payoff based on the posterior belief. Formally, the rational DM's expected payoff given a\n(set of) signals V is\n$R(V) = E_{v\u223c\u03c0}[max_{d\u2208D} E_{\u03c9\u223cPr(\u03c9/v)}[S(d, \u03c9)]]$\nWe use \u00d8 to represent a null signal, such that R(\u00d8) is the expected payoff of a Bayesian rational DM\nwho has no access to a signal but only uses their prior belief to make decisions. In this case, the\nBayesian rational DM will take the best fixed action and their expected payoff is\n$R(0) = max_{d\u2208D} E_{\u03c9\u223c\u03c0}[S(d,\u03c9)]$\nGiven a set of signals V\u2081 and a ground set of signals V2, we can define the information gain from V\u2081\nover V2, the payoff improvement of V\u2081 over the payoff obtainable from V2.\n$Y(V_1; V_2) = R(V_1 \u222a V_2) \u2013 R(V_2).$\n(1)"}, {"title": "Information in Behavioral Decisions", "content": "We use the term \u201cbehavioral DM\" for a human who makes the decision in a decision-making problem\nafter observing the signals. The intuition behind our approach is that any information that is used by\nbehavioral DMs should eventually reveal itself through variation in their behaviors. Therefore, the\ninformation value in behavioral decisions can be recovered by offering the behavioral decisions as\na signal to the Bayesian rational DM, which is equivalent to the information gain from behavioral\ndecisions over a null signal. Similarly, we can look to the information gain from different signals\nover the behavioral decisions alone to test how useful the signals are beyond the information revealed\nin the behavioral decisions.\nWe model the decisions of a behavioral DM as a random variable Db from the action space D, which\nfollows the distribution \u03c0\u266d \u2208 \u0394(\u03a9\u03a7 \u03a31 \u03a7 . . . \u03a7 \u03a3\u1fc3 \u00d7 D) \u2013 the joint behavior of the human correlated\nwith the state and signals. The Bayesian rational DM knows the joint distribution \u03c0\u266d. After observing\nhuman decisions, the rational DM updates to a posterior and selects the decision that maximizes their\nexpected payoff. Their expected payoff is given by the function:\n$R(D) = E_{d\u223c\u03c0}[max_{d\u2208D} E_{\u03c9\u223cPr(\u03c9/Db=db)}[S(d, \u03c9)]]$\nInformation Gain of Signals Over Behavioral Decisions We seek to identify signals that can\npotentially improve behavioral decisions by analyzing their expected information gain (V; Db), the\nimprovement in payoff expected from having the signal V over only having the behavioral action Db.\nIf the information gain of a signal is low over having only the behavioral decisions, this means either\nthat the behavioral DM has already exploited the information, or that the information value to the\ndecision problem of the signal is low. If, however, the information gain of a signal is high, then in\ntheory the behavioral DM can improve their payoff by incorporating the signal's information in their\ndecision making."}, {"title": null, "content": "However, the information value of a basic signal may be overlooked if its value in combination\nwith other signals is not considered. Signals can be complemented [Chen and Waggoner, 2016],\ni.e, they contain no information value by themselves but a considerable value when combined with\nother signals. For example, two signals 21 and 22 ight be uniformly random bits and the state\n\u03c9 = 21 \u2013 22, the XOR of 21 and 22. In this case, neither of the signals offers information value on\nits own but knowing both can lead to the maximum payoff. To consider this complementation between\nsignals, we use the Shapley value $ [Shapley, 1953] to interpret the contribution to information gain\nof each basic signal. The Shapley value calculates the average of the marginal contribution of a basic\nsignal Ei in every combination of signals.\n$\u03c6(\u03a3_i) = \\frac{1}{n} \u03a3_{V\u2286{V_1,...,V_n}/{i}}(( \\binom{n - 1}{\\abs{V}})^{-1}) ((V \u222a {\u03a3i}; D^b) \u2212 \u03b3(V; D^b))$\n(2)\nThe Shapley value suggests how much information value of the basic signal is unexploited by the\nbehavioral DM on average in all combinations.\nInformation Gain of Behavioral Decisions Over Signals We analyze the additional information\ncontained in behavioral decisions beyond what is contained in the other available signals by examining\nthe information gain of the behavioral decision over the signal, denoted as y(Db; V). Suppose the set\nof all signals formalized in the data-generating process are V = {1, ..., \u03a3\u03b7}. \u03b3(Db; V) captures\nthe value of information that is reflected in behavioral decisions beyond the signals formalized by the\ndata-generating process.\nOur framework also offers a way to assess whether humans bring addition relevant information over\nan AI model for a decision task. Denote the AI predictions and human behavioral decisions as random\nvariables DAI and DH. \u03b3(DH; DAI) gives the value of additional information that is reflected in\nhuman decisions beyond AI predictions for the decision task."}, {"title": "Experiment", "content": "We apply our model to a deepfake video detection task studied by Groh et al. [2022], where partic-\nipants are asked to judge whether a video is genuine or has been manipulated by neural network\nmodels. They are given access to predictions from a computer vision model that achieved an accuracy\nscore of 65% on 4,000 videos in heldout data. Participants first review the video and report an initial\ndecision. Then, in a second round, they are told the AI's recommendation and choose whether to\nchange their initial decision. Participants are asked to report their belief that the video is fake in 1%\nincrements: d \u2208 {0%, 1%, ..., 100%}.\nWe use the Brier score as the payoff function in our model: $S(w, d) = 1 \u2212 (w \u2013 d)^2$, with the binary\npayoff-related state: \u03c9 \u2208 {0,1} = {genuine, fake}. We construct the basic signals in our model\nby the seven video-level features proposed by Groh et al. [2022]: graininess, blurriness, darkness,\npresence of a flickering face, presence of two people, presence of a floating distraction, and the\npresence of an individual with dark skin, all of which are hand-labeled as binary indictors. We\nestimate the data-generating process using the realizations of signals, state and behavioral decisions\nin the experiment data of Groh et al. [2022].\nWe show the results in Figure 1, where each distribution shows the distribution of the information\ngain of the signal over behavioral decisions. The signals are on the y axis and behavioral decisions\nare encoded by different colors. The information gain is on the payoff scale, which is bounded by\n[0, 1], where 1 means the signal can improve the payoff of a rational decision maker who performs\nas badly as possible (defined by the scoring rule, e.g., 0 payoff in Brier score) to a rational decision\nmaker who achieves the maximum payoff (e.g., 1 payoff in Brier score).\nUnaided human v.s. AI. We first compare how participants without AI assistance and the AI use\ninformation in the deepfake detection task. Specifically, we calculate the Shapley value of information\ngain, $\u03c6^{DH}$(i) for participants and $\u03c6^{DAI}$(i) for AI, as shown in Equation 2, for each video-level feature\nV. The information gain over behavioral decisions reflects the information value of signals that are\nnot fully redundant with the information in the behavioral decisions.\nFirst, we observe that, in general, the information gain of the features over the AI decisions are lower\nthan those for human decisions. There are several exceptions, such as the presence of an individual"}, {"title": null, "content": "with dark skin. This suggests that, overall, the information in features is better exploited by AI than\nby participants. More specifically, we find that the AI uses certain features much more effectively\nthan participants. For instance, the presence of a flickering face offers the least information gain over\nAl decisions among all the features, whereas it is the feature that offers the largest information gain\nover human decisions. This suggests that one way to improve the current human-AI performance\nis to help the participants better exploit the information that AI exploits well but participants did\nnot. Second, we find that the AI relies on less sensitive information compared to participants. For\nexample, AI uses the presence of an individual with dark skin the least among all features, while for\nparticipants it is the second most important feature.\nUnaided human v.s. human-AI team. We assess the information gain after participants are\npresented with AI recommendations in the deepfake detection task. We calculate the Shapley value\nof information gain $\u03c6^{DHAI}$(i) for human participants with AI recommendations relative to without.\nWe find that simply displaying the AI's predictions to participants does not necessarily help them to\nbetter exploit the potential value of information that they exploited poorly without access to the\nAI. For example, even though the information gain of the presence of a flickering face is reduced\nwhen presenting participants with AI predictions relative to without, the AI's much smaller gain\nfor the signal implies participants could still use it much more effectively. This suggests that better\ninterventions (e.g., explanations for AI predictions) may be needed to help people better incorporate\nsome signals. Second, for the signals that the AI does not exploit well, offering the AI predictions\ndoes not necessarily reduce participants' usage of that information. For example, for the signal\ndenoting the presence of an individual with dark skin, we did not see a significant improvement on\nthe information gain over human-AI decisions compared to the gain over human decisions. Both\nof these findings suggest that simply displaying AI predictions may not change people's usage of\ninformation and improve their decision quality. Other interventions (such as explanations) should be\nexplored to improve the use of potentially valuable by humans in human-AI collaborations."}, {"title": "Discussion", "content": "In this paper, we propose an approach to investigating information value in the context of a human\nand AI paired on a decision task. We identify the unexploited value of information contained in\navailable signals by quantifying the information gain (as the increase in the expected payoff of a\nrational decision maker) of the signals over the behavioral decisions. We also identify the information\nvalue reflected in the behavioral decisions that is not contained in the signals. This approach makes\nit possible to identify signals that could be used to improve behavioral decisions through further\ninterventions. For example, explanations might be designed to focus attention on signals whose\ninformation gain is low over the AI predictions but high over the human decisions.\nOur work has limitations. First, the problem of complementation between signals is still not fully\naddressed. Though we develop a model using Shapley value to calculate the marginal contribution of\nsignals in all combinations, we only account for the complementation and substitution between ob-\nserved signals. Second, our methodology investigates information value in terms of the improvement\nthat an ideal decision maker can achieve given that information."}]}