{"title": "Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative Querying", "authors": ["Federico Castagna", "Isabel Sassoon", "Simon Parsons"], "abstract": "Studies have underscored how, regardless of the recent breakthrough and swift advances in AI research, even state-of-the-art Large Language models (LLMs) continue to struggle when performing logical and mathematical reasoning. The results seem to suggest that LLMs still work as (highly advanced) data pattern identifiers, scoring poorly when attempt-ing to generalise and solve reasoning problems the models have never previously seen or that are not close to samples presented in their training data. To address this compelling concern, this paper makes use of the notion of critical questions from the literature on argumentation theory, focusing in particular on Toulmin's model of argumentation. We show that employing these critical questions can improve the reasoning capa-bilities of LLMs. By probing the rationale behind the models' reasoning process, the LLM can assess whether some logical mistake is occurring and correct it before providing the final reply to the user prompt. The un-derlying idea is drawn from the gold standard of any valid argumentative procedure: the conclusion is valid if it is entailed by accepted premises. Or, to paraphrase such Aristotelian principle in a real-world approxima-tion, characterised by incomplete information and presumptive logic, the conclusion is valid if not proved otherwise. This approach successfully steers the models' output through a reasoning pipeline, resulting in better performance against the baseline and its Chain-of-Thought (CoT) imple-mentation. To this end, an extensive evaluation of the proposed approach on the MT-Bench Reasoning and Math tasks across a range of LLMs is provided.", "sections": [{"title": "1 Introduction", "content": "Despite the remarkable leap forward and recent advancements in AI research, it is a well-known issue that even state-of-the-art Large Language models (LLMs)"}, {"title": "2 Background", "content": "This paper combines several components, each representing an essential building block of the CQoT pipeline introduced herein. In this section, we will unpack the notions and tools underpinning such a pipeline."}, {"title": "2.1 Argumentation", "content": "The term \"argumentation\", as used in computer science, refers to a wide range of work rooted in philosophy and the study of human reasoning. In this tradition, a seminal work is Toulmin's \u201cThe Uses of Argument\u201d [52] which began to formalize the idea that in reasoning it is not just the conclusion that is important, but also the rationale behind the reached conclusion. In Toulmin's view, all conclusions are defeasible in the sense that they can be overturned by subsequent inferences. Conclusions, which Toulmin calls \"claims\", are constructed from data about the world, are supported by a warrant, a motive for thinking that the conclusion holds, and this itself is derived from some backing (experience or experimental data). This whole structure is an argument. All claims may be subject to a rebuttal, and the final truth will only be determined by taking into account all such rebuttals (each of which is itself an argument), examining the warrants, and reaching a verdict on which argument is most plausible.\nAn example of the use of this kind of reasoning, drawn as a formal schema, and taken from [52], is shown in Figure 1. Here, we are reasoning about the nationality of an individual \"Harry\", under the rules for British nationality that held at the time that Toulmin was writing. Here, the data is that Harry was born in Bermuda. From this it was then possible to make the claim that Harry was British, this conclusion warranted by the fact that, at the time, an indi-vidual born in Bermuda was a British citizen. (This warrant was itself backed by the law of the time concerning British nationality.) The schema explicitly records that the claim is not certain a fact denoted by the attachment of a qualifier, such as \"presumably\" and the fact that the claim may be sub-ject to a rebuttal, such as the fact that Harry might have given up his British nationality.\nWe will come back to Toulmin's schema below as a convenient framework to consider how we might bolster the performance of reasoning in LLMs, but"}, {"title": "2.2 Argument Structure & Critical Questions", "content": "Toulmin's schema provides us with part of what we need to augment LLM rea-soning, and we combine that idea with the more recent notions of argument schemes and critical questions [56, 55]. Broadly speaking, the idea behind argu-"}, {"title": "2.3 LLMS", "content": "Large Language Models (LLMs)\u00b9 represent one of the current pinnacles of Deep Learning technology stemming from the transformer architecture [53]. Since the release of ChatGPT in November 2022 [35], the production and deployment of generative AI systems have soared, resulting in new model announcements almost every week. The competition among proprietary companies, the contri-bution of the open source community and the huge investments made in the sector over the past two years have certainly yielded impressive advancements in the AI landscape. The main actors of this progress include tech firms such as Anthropic, OpenAI, Google, Meta, NVIDIA and many others. In particular, Anthropic's Claude 3.5 Sonnet [2], OpenAI's GPT-40 [36] and Google's Gemini 1.5 pro [50] have alternated as the top three models (according to a plethora of different evaluation benchmarks) during 2024. However, despite their out-standing performances, these models are still prone to errors, especially when handling tasks that require complex reasoning capabilities. Some studies claim that LLMs are just proficient at memorizing training data, but still struggle to generalize and undertake tasks which are unknown or unlike anything they have previously seen [46, 32]."}, {"title": "2.3.1 Chain-of-Thought", "content": "To improve LLM capabilities and curtail their shortcomings, several different techniques have been developed. Some of such approaches involve training, fine-tuning or other resource-intensive post-processing stages. Researchers also engaged with lightweight methods which, in some cases, proved even more suc-cessful than the aforementioned ones. Among these examples, prompting en-gineering techniques, i.e., practices to effectively craft prompts that optimize models' output, have been largely employed to enhance LLM logical thinking [43]. Chain of Thought (CoT), probably the most influential of such techniques, consists of a prompting strategy that details a series of intermediate reasoning steps to achieve better performance in arithmetic, symbolic and commonsense inferences [58]. Zero-shot-CoT is a streamlined version of CoT that is task-agnostic and does not require few-shot examples [25]. In the following, we will leverage the first prompt of this (originally twofold) approach, rendered by the well-known phrasing \"Let's think step by step\". For simplicity, we will refer to it just as CoT."}, {"title": "2.4 Benchmarks", "content": "In order to assess the overall capabilities of an LLM, thus ranking the best-performing ones, AI researchers needed to design solutions that were more prac-"}, {"title": "3 Methodology", "content": "Our Critical-Questions-of-Thought (CQoT) approach can be summarised by the four steps illustrated in Figure 2. The chosen name already hints at the main component of this technique, i.e., the critical questions, whose contribution occur in Step 2 of the pipeline. For each CQ we identify which elements of Toulmin's model the question targets. Note that we are not trying to claim that an LLM follows the Toulmin model of argumentation, but rather that LLMs engage in presumptive reasoning, and so a set of CQs that cover all the elements of Toulmin's general schema of presumptive reasoning provide a suitable battery of queries with which to test any conclusions that an LLM comes up with. The leveraged critical questions are:\n1. Does the reasoning process start with clearly defined premises? (Data)\n2. Are the premises supported by evidence or accepted facts? (Data)"}, {"title": "3.1 Critical-Questions-of-Thought pipeline", "content": "Critical-Questions-of-Thought is a simple approach that can be implemented on top of any LLMs. In this section, we introduce and break down each of the four stages that compose the CQoT pipeline, as depicted in Figure 2. In a nutshell, Steps 1, 2 and 4 can be interpreted as different prompting strategies leveraging the outcome of the previous steps. Step 3 operates as a checkpoint. It verifies the model's responses to the CQs and decides whether the pipeline should continue to the final phase or iterate over, effectively starting again from the initial stage. More precisely:\n\u2022 Step 1. Prompt the LLM to reason logically about the input query and sketch a plan to follow. The model must divide each reasoning process into premises and conclusions so that each conclusion derives from the respective premises. No final response should be provided."}, {"title": "4 Evaluation", "content": ""}, {"title": "4.1 Experimental setup", "content": "In this section, we lay out the specifics of how we appraised the CQOT pipeline (Figure 2) against the chosen LLMs in their standard and CoT-augmented ver-"}, {"title": "4.1.1 Parameters", "content": "We previously reported that the experiment has been conducted with both open source and (free plan version of) proprietary LLMs. Where we had access to the full models' inference parameters, i.e., with Llama 3.1-70b-Instruct and Nemotron-51b-Instruct, we tuned them as follows. return_full_text: False; temperature: 0.8 (temperature:0.2 for Step 2 and Step 4, which need to be as objective as possible); top_p:0.95; do_sample: True; max_new_tokens: 2000. The underlying idea of this setting was to control the creativity of the LLM while saving the available compute. When leveraging Gemini 1.5-pro-001 on the Google Vertex AI platform, we were able to adjust fewer parameters: temperature:1 (temperature:0 for Step 2 and Step 4, which need to be as objective as possi-"}, {"title": "4.1.2 MT-Bench", "content": "As mentioned earlier, we opted for MT-Bench math and reasoning questions2 to assess the performances of our pipeline. Unfortunately, it is a well-known issue that popular benchmark data may contaminate the models' training data, thus affecting the validity of the LLMs' output. This problem has led the AI community to devise alternative evaluation methods such as LiveBench, which contains frequently updated questions from recent information sources [59], and Chatbot Arena, an open platform to appraise LLMs via human preferences [11]. Although MT-Bench has been available for about a year at the time of the writing [63] and the risk that the models' training data has been contaminated with MT-Bench prompt exists, we tested the same LLMs with and without CoT and CQoT, thus ensuring a fair comparison of the performances (i.e., if the baseline has been contaminated, so is the CoT and the CQOT version, therefore effectively putting them on equal ground for evaluation)3. For the CoT-augmented version of the underlying model, we employed a specific prompt, as detailed in Figure 9. Overall, each harnessed model has been input with a total of 40 questions, evenly divided between reasoning and math topics. We argue that the collected answers suffice to render a meaningful sample of the differences between the standard LLM, its CoT implementation, and the CQOT pipeline."}, {"title": "4.1.3 Ablation study", "content": "To effectively verify that the CQoT results were indeed dependent on the probing impact of the critical questions, we devised an additional test. We evaluated both Llama 3.1 70b-Instruct and Nemotron-51b-Instruct on a slightly different pipeline composed only of the first and fourth steps of the CQOT method. We opted for these two models out of the five we previously examined due to their being open-source and thus being easier to use for reproducing our findings. The idea was to determine whether the strengths of the CQoT approach lay solely in the envisaged reasoning plan or whether the presence of CQs was granting further improvements. The full pipeline leveraged for this ablation study is described in Figure 4, whereas the respective prompts are presented in Figure 8 in the Appendix."}, {"title": "4.1.4 LLM as a Judge", "content": "Drawing from the research of Zheng et al. [63], where it has been established how state-of-the-art LLMs can achieve an agreement rate on par with human ex-perts when assessing model responses, we followed a similar evaluation strategy. When choosing the judge, we considered both GPT-40 and Claude 3.5 Sonnet as promising options. The former was better at appraising math questions, and the latter was stronger at assessing reasoning queries [48]. Given that, at the time of the experiment, the free tier version of GPT-40 (October 2024 version) was more generous than Claude in granting a larger number of requests per day, we selected the first as the judging model. The prompts provided to evaluate the LLMs' response quality are also taken from the study conducted by Zheng et al. [63], although we slightly reshaped such prompts to better reflect the task of handling only reasoning and math-related queries (see Figure 10 in the Appendix). Such prompts instruct the judging model to assess responses us-ing a 1-10 scoring system, where higher grades mean better performances. To avoid fluctuations in the judge rates and restrict the potential randomness of the outcome, we manually prompted GPT-40 multiple times for each answer to be evaluated until the same score occurred three times. Such a score was then identified as being representative of the response of the appraised model. We argue that this method ensures a more reliable grading of the evaluated answers compared to single scoring. Additionally, there were instances where we further prompted the judge with follow-up inputs. In particular, we employed a specific instruction (see Figure 11 in the Appendix) that proved useful in balancing the assessment whenever the score penalized the lack of conciseness (a criterion, we believe, is not fundamental when testing reasoning capabilities). We also repeated the reference answer when the judge wasn't properly acknowledging it"}, {"title": "4.2 Results", "content": "Results of the laid-out evaluation, obtained by testing the whole pipeline, can be seen in Tables 1, Figure 5 and Figure 6, whereas the outcome of the follow-up ablation experiment, comparing a pipeline composed only by Step 1 and 4 against the baseline and its CQOT version, can be seen in Table 2. Both tables are arranged in a similar way, emphasizing in bold the LLMs' highest performance according to the task (math or logical reasoning) and the technique harnessed (standard baseline, Step1_4, CoT or CQoT). The numbers displayed are the mean of the values scored (i.e., assigned by the LLM judge) by a model under a task with a specific technique. Figure 5 graphically showcases the same average numbers by means of a barplot, focusing on the baseline and CQOT, separating the two tasks of comparison. Figure 6 provides a more detailed visualization of the mean scores (points) per model per task and the respective standard errors (bars). The collected data is examined in the next section."}, {"title": "4.3 Analysis", "content": "Employing the CQoT pipeline creates a significant increase in the performance of models on the MT-Bench Reasoning and Math tasks when compared with both the baseline performance (the raw LLM output) and the performance of the LLMs augmented with CoT. These improvements are particularly marked"}, {"title": "5 Discussion", "content": "As humans take more time to elaborate and solve hard problems [23], it is plausible to assume that, by giving more 'time to think' to LLMs during in-ference, the models' capability to correctly address challenging reasoning issues should likewise increase. This is exactly what the test-time compute paradigm has confirmed, especially through LLMs such as OpenAI's o1 [37], DeepSeek's R1-Lite-Preview [49] and Qwen's QwQ [51]. However, another main feature of human thinking is its argumentative nature and how it is fundamental to formu-late arguments that challenge (or support) specific information to consolidate our knowledge. Indeed, Mercier and Sperber claim that reasoning evolved as a byproduct of the generation and evaluation of arguments [30]. This notion underpins the purpose of the critical questions as means to probe the reliability of the information embedded in the inquired argument. The CQoT approach combines together CQs, inspired by Toulmin's model, and the test-time com-pute paradigm into a pipeline that allows LLMs to enhance their logical and mathematical skills.\nWe noticed that the driving engine of the CQoT pipeline is the reasoning protocol generated in Step 1, which steers the underlying model's thinking pro-cess. The LLM's reply will be reliant on such a protocol, thus possibly leading to an erroneous response if the provided reasoning plan presents any mistakes. The employment of CQs (implemented in Step 2 and verified in Step 3) will mostly filter out each of such wrongly designed protocols, but some may still be able to elude the test, especially for smaller and less performing models4."}, {"title": "5.1 Advantages", "content": "\u2022 The use of the CQoT approach allows open-source (arguably smaller) mod-els such as Nemotron-51b-Instruct and Llama 3.1-70b-Instruct to outper-form notably state-of-the-art proprietary (arguably larger) models such as GPT-40, Gemini 1.5-pro-001 and Claude Sonnet 3.5 on most of their baseline outputs.\n\u2022 The CQOT procedure is highly versatile and works on top of LLMs' re-sponses, meaning that it is independent of specific models or their archi-tecture (e.g., state space model-inspired Mamba [17] or the hybrid Jamba [26]).\n\u2022 The outcome of the pipeline presented herein does not prevent the under-lying models from employing additional techniques (e.g., prompting engi-neering) or strategies (e.g., multiple sampling) to further enhance LLMs' capabilities."}, {"title": "5.2 Limitations", "content": "\u2022 We acknowledge that the judge (GPT-40) committed mistakes, albeit rarely, in its evaluation by erroneously interpreting the reference answers and giving a lower score to the tested models. We correct this by prompt-ing the reference answer once again, thus solving the unbalanced assess-ment."}, {"title": "6 Related and Future Work", "content": "The notion of critical questions for Toulmin's model of argument has been ex-plored by a number of researchers. Among these, we acknowledge the work of Yu and Zenker [61] where the authors build upon Toulmin's schema and articulate a three-part meta-level CQ list to achieve complete and applicable argument evaluation, that is based on three fundamental attack types: against premise, inference, and conclusion. However, in contrast to our approach, some of the CQs are argument scheme-specific and therefore rely on identifying a particu-lar argument scheme to complete the argument evaluation. Additionally, there is no mention of a practical combination of such a meta-level approach with LLMs. The literature also presents studies extending Toulmin's schema to ad-dress argument evaluation [54] and to prompt LLMs to output explicitly named argument components as classified by the schema [18]. Regardless, neither paper leverages critical questions. Unlike the previous two, the EQRbot [5, 7], devised to deliver customised healthcare explanations to patients' requests, makes use of critical questions to assess the validity of a bespoke argument scheme instan-tiation. Such a scheme is meant to succinctly convey relevant information to the user. Nevertheless, although the EQRbot is AI-driven and harnesses CQs, it does not employ LLMs in particular, nor does it specifically enhance their"}, {"title": "7 Conclusion", "content": "While Large Language models represent advanced reasoning tools, they still en-counter a number of issues when faced with logical and mathematical problems. To tackle this issue, we propose a solution that combines argumentation theories and the test-time compute inference paradigm. The latter can be encapsulated by the idea of giving the model more 'time to think' before providing an output. The former refers to Toulmin's tradition of argumentative pattern components and the notion of critical questions that serve to probe arguments' validity. Our solution merges all such elements into a pipeline, denoted as Critical-Questions-of-Thought (CQoT), capable of enhancing LLMs reasoning performances. We assessed CQOT on the MT-Bench Reasoning and Math questions, testing it with five different models, proprietary and open source. The evaluation resulted in CQOT outperforming both the baseline model and its version augmented by Chain-of-Thought, exhibiting an average 5% improvement in response correct-ness and helpfulness."}, {"title": "Appendix", "content": "In this section, we detail all the prompts devised and used within our CQOT pipeline and the experiment conducted to evaluate its performance."}, {"title": "CQoT Pipeline Prompt Templates", "content": "As depicted in Figure 2, the CQOT approach follows a procedural structure organised into 4 different steps. Each step is driven by an operation executed by the underlying LLM which, in turn, is triggered by a specific prompt. Every prompt includes multiple tags enclosing diverse sets of instructions, as shown in Figure 7. Intuitively, the main command is contained within the 'System Instruction' labels and makes use of the content defined by the 'User Prompt' and 'Reasoning Steps' tags. Notice that Step 3 of the pipeline acts as the loop counter for the overarching process, backtracking to the initial stage if the produced reasoning plan does not positively address the critical questions posited in Step 2. Given that this operation is either automated or manually executed, there is no need for any LLM to intervene, hence, no prompt."}, {"title": "Pipeline Prompt Templates", "content": "According to Figure 4, the pattern followed in the ablation study comprises only the first and last steps of the CQoT pipeline. This serves to test the results achieved by the underlying LLM when its thinking process is steered by the preliminary generation of a reasoning plan without any assessment from the critical questions. As such, the structure presented in Figure 8 embeds only the prompts of Step 1 and Step 4, which are composed exactly as the ones introduced in Figure 7, hence designed with the same 'System Instruction', 'User Prompt' and 'Reasoning Steps' tags."}, {"title": "CoT Prompt Template", "content": "Chain-of-Thought is a well-known prompting technique capable of enhancing LLMs output by adding, in its most trivial form, the words \"Let's think step by step\". In our CoT prompt, illustrated in Figure 9, we adopted a similar phrasing within the standard structure specification ('System Instruction', \u2018User Prompt') provided in each prompt of the CQoT pipeline."}, {"title": "LLM Judge Prompt Templates", "content": "The assessment of each model output (either baseline, CoT augmented, Step 1 and 4-only, or CQoT) has been conducted by GPT-40 (October 2024 version), which assumed the role of the judge. Both MT-Bench Reasoning and Math tasks"}]}