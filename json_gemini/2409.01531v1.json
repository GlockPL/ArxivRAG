{"title": "On the Design Space Between Transformers and Recursive Neural Nets", "authors": ["Jishnu Ray Chowdhury", "Cornelia Caragea"], "abstract": "In this paper, we study two classes of models, Recursive Neural Networks (RvNNs) and Transformers, and show that a tight connection between them emerges from the recent development of two recent models - Continuous Recursive Neural Networks (CRvNN) and Neural Data Routers (NDR). On one hand, CRvNN pushes the boundaries of traditional RvNN, relaxing its discrete structure-wise composition and ends up with a Transformer-like structure. On the other hand, NDR constrains the original Transformer to induce better structural inductive bias, ending up with a model that is close to CRVNN. Both models, CRvNN and NDR, show strong performance in algorithmic tasks and generalization in which simpler forms of RvNNs and Transformers fail. We explore these \"bridge\" models in the design space between RvNNs and Transformers, formalize their tight connections, discuss their limitations, and propose ideas for future research.", "sections": [{"title": "1 Introduction", "content": "Consider a task like ListOps (Nangia & Bowman, 2018) where we have to solve a nested list of mathematical operations such as: MAX(1,3,SUM(4,5,MIN(9,7)),4)). Note that in such a task, unconstrained all-to-all (all tokens to all tokens) interactions cannot accommodate for fixed layer depth in Transformers (Vaswani et al., 2017) compared to RNNs where layer depth can extend adaptively based on sequence length. To solve this task, a model has to process the sequence sequentially because the operations in the outer lists can only be executed once the inner list operations are executed. Keeping that in mind, we can list a few potentially necessary conditions or requirements to fulfill for a task like this (and other tasks like program synthesis, program execution, mathematical reasoning, algorithmic reasoning, etc.):\n\u2022 C1: Ability to operate in arbitrary orders (for example, the innermost operations may occur in any position in the input).\n\u2022 C2: Being equipped with a gating mechanism to keep some hidden states unchanged if needed (for example, outermost values may need to remain unchanged for a while to wait for inner list operations to be completed).\n\u2022 C3: Having an adaptive number of layers (according to the input) such that the number of layers can potentially increase indefinitely with higher demand (for example, when a higher depth of reasoning is required according to sequence length as longer sequences are more complex).\nA similar list of requirements was also motivated in Csord\u00e1s et al. (2022b). Transformers (Vaswani et al., 2017) with their fixed layers (failing C3) and lacking a gating mechanism to control information flow (failing C2) tend to struggle in tasks like ListOps (Tran et al., 2018; Shen et al., 2019; Tay et al., 2021; Csord\u00e1s et al.,"}, {"title": "2 General Schema", "content": "At the broadest level, both NDR (a variant of Transformer) and CRVNN (a variant of RvNN) can be formalized in terms of a recursive application of some function $Rec : \\mathbb{R}^{s \\times d} \\rightarrow \\mathbb{R}^{s \\times d}$. The application of such a function at the recursive step t (or equivalently, sharing weights at every layer) can be written as:\n$H^{t+1}, E^t = Rec(H^{t-1}, E^{t-1})$\nIn this chapter, we focus only on the encoder function (not the decoder). Here, $H^t, H^{t-1} \\in \\mathbb{R}^{s \\times d}$ represent a sequence of hidden states (e.g., $H^t = (H_1, H_2, ..., H_s)$) where s is the sequence length and d is the dimension of hidden states. $E^t \\in \\mathbb{R}^{s \\times 1}$ represents a sequence mask. Normally, the sequence mask can be thought of as a mask for sequence padding\u00b9 with 0 for pads and 1 otherwise. However, in the case of CRvNNs, we can treat $E^t$ more generally as a \"soft sequence mask\" with continuous values in [0,1]. Going a little more granular, the general internal structure of Rec can be formalized as:\n$Rec(H^t, E^t) = Compose(Retrieve(H^t, E^t), H^t)$\nThe Retrieve function can be thought to retrieve information for any state $H_i^t$ from the surrounding context $H^t$. The Compose function can be thought to integrate the retrieved information with the original $H_i^t$. Roughly, we will show some analogous functions that fulfill these roles in both NDRs and CRvNNs. In \u00a76, we also discuss how this recursive formalism can also represent vanilla Transformers (Vaswani et al., 2017).\nNext, we show how NDR and CRVNN can be taken as specific instances of this general recursive schema as defined in Eq. 2 and how a strong similarity appears between NDR and CRVNN despite being created from two different perspectives."}, {"title": "3 Neural Data Router (NDR)", "content": "Neural Data Routers (Csord\u00e1s et al., 2022b) can be construed as a modified version of Universal Transformers (UTs) (Dehghani et al., 2019) that similarly employs layer-wise parameter sharing or recursion (repeats the same layer function Rec).\nFormalism of Retrieve Function: The Retrieve function of NDR can be said to constitute the Multi-headed Attention Function (MHA) (Vaswani et al., 2017):\n$X^t, E^t = MHA(H^t, E^t)$\nFor ease of explanation, we concentrate on the single-headed version (although the multi-headed version is used in the experiments). Inside the single-headed attention of NDR, there is a non-traditional form of"}, {"title": "attention called geometric attention:", "content": "$Q = H^tW_Q, K = H^tW_K, V = H^tW_V$\n$C = sigmoid(QK^T) \\odot E^t$\n$A_{ij} = C_{ij} \\prod_{k \\in S_{ij}} (1 \u2013 C_{ik})$\nHere, $W_Q, W_K, W_V \\in \\mathbb{R}^{d \\times d_h}$, $H^t \\in \\mathbb{R}^{s \\times d}$, and $Q, K, V \\in \\mathbb{R}^{s \\times d_h}$. $A \\in \\mathbb{R}^{s \\times s}$ is the attention matrix where $A_{ij}$ represents how much $H_i^t$ attends to $H_j^t$. $S_{ij}$ contains all positions k except i and j such that $|i - k| < |j - i|$ if $i < j$ else $|i - k| < |j - i|$ if $i > j$. Additionally, the diagonals of $A_{ii}$ are set as 0. Plainly said, geometric attention makes each query prefer to attend to the closest matching values (closest in terms of relative distances), suppressing more distant attention proportionately. This is motivated by the fact that algorithmic tasks often benefit from a preference for local operations. Finally, we have:\n$Z = A_{ij}V, Y = ZW_o$\n$X^t = LN_1(Y + H^t)$\nHere, $W_o \\in \\mathbb{R}^{d_h \\times d}$ and $LN_1$ is layer norm. NDR (Csord\u00e1s et al., 2022b) also introduces a mechanism to learn to reduce attention to positions in a specific direction, but currently, we ignore these implementational details for focus.\nFormalism of Compose Function: In the Compose function, NDR introduces a gating mechanism:\n$G = sigmoid(FFN_{gate}(X^t))$\n$FFN_{gate} : \\mathbb{R}^{s \\times d} \\rightarrow \\mathbb{R}^{s \\times d}$ can be any multi-layered position-wise feedforward network, and $G \\in \\mathbb{R}^{s \\times d}$ are the gates (multidimensional). The output of the Compose function is then:\n$H^{t+1} = G \\odot LN_2(FFN_{data}(X^t)) + (1 \u2212 G) \\odot H^t$\n$E^{t+1} = E^t$\nHere $LN_2$ is layer norm and $FFN_{data} : \\mathbb{R}^{s \\times d} \\rightarrow \\mathbb{R}^{s \\times d}$ is another multi-layered position-wise feedforward network.\nRequirements Satisfaction in NDR: NDRs (Csord\u00e1s et al., 2022b) address many of the desiderata de-scribed in the introduction. NDR addresses C1 by using a form of self-attention to process data in any arbitrary order, but unlike Transformers, it uses geometric attention to process data in a more controlled fashion. NDR explicitly addresses C2 by incorporating a gating mechanism in Eq. 10 (although, note that certain other Transformers also incorporate a gating mechanism Chai et al. (2020)). NDR partially addresses C3 by sticking to sharing parameters across layers, similar to Universal Transformers. Thus, it allows us to increase the number of layers during inference indefinitely. However, NDR remains inflexible because it cannot completely adapt according to the input. In the original work, the expected maximum depth has to be considered a priori to set the hyperparameters for the maximum depth of layers during training Csord\u00e1s et al. (2022b). On the other hand, although we can use the sequence size as a heuristic for maximum layer depth without dynamic halting, the model can become exorbitantly expensive if done so because it lacks any early halting mechanism."}, {"title": "4 Continuous Recursive Neural Network (CRvNN)", "content": "We already showed CRVNNs to be a continuous relaxation of traditional Tree-RvNNs. Here, we show that CRVNNs can also, at the same time, be construed as a form of a constrained Transformer (particularly, a constrained NDR).\nFormalism of Retrieve Function: The Retrieve function of CRVNN can be said to constitute an attention-like function (say, NR - neighbor retriever) to retrieve an immediate left neighbor:\n$X^t, E^t = NR(H^t, E^t)$"}, {"title": "Inside NR, we have:", "content": "$A_{ij} = E_j \\prod_{l \\in S_{ij}} (1 \u2013 E_k)$\n$X^t = A H^t$\nHere, the set $S_{ij}$ contains all positions k except i and j such that $i-k < i - j$ and $i - k > 0$. $A \\in \\mathbb{R}^{s \\times s}$ acts like an attention matrix and $A_{ij}$ is 0 if $S_{ij}$ is empty. Eqn. 13 is a reformulation of Eqn. 6 in \u00a73.2.1 from Chowdhury & Caragea (2021). It is one of the two variants proposed for modeling $A_{ij}$ in \u00a73.2.1 (denoted as $P_{ij}$ in \u00a7\u00a73.2.1) in Chowdhury & Caragea (2021).\nA unique aspect of CRVNN is that it allows for the soft deletion of processed information so that it does not confuse future recursive steps. For example once MAX(1,3,SUM(4,5,MIN(9,7)),4)) is simplified to MAX(1,3,SUM(4,5,7),4)),we do not need to keep around information related to MIN(9, 7) explicitly. The information in the corresponding positions can be 'deleted.' This is modeled by $E^t$ (initialized as the pad mask) where $E_i^t$ represents the probability of position i having not been deleted so far (alternatively, we may say $E_i^t$ is the existential probability of position i to use the vocabulary we used in Chowdhury & Caragea (2021)). Now, we can see that $A_{ij}$ represents the probability that the item in j is the first existing item left to i. Thus, $X^t$ represents the softly retrieved left neighbors of $H^t$.\nFormalism of Compose Function: The Compose function can be described as:\n$G = DF(H^t, E^t), L = AG$\n$H^{t+1} = L \\odot Cell(X^t, H^t) + (1 \u2212 L) \\odot H^t$\n$E^{t+1} = E^t \\odot (1 \u2013 G)$\nHere DF: $\\mathbb{R}^{s \\times d} \\times \\mathbb{R}^{s \\times 1} \\rightarrow \\mathbb{R}^{s \\times 1}$ is some arbitrary function which we can refer to as the decision function serving a similar role as $FFN_{gate}$ in NDR. $G, L \\in \\mathbb{R}^{s \\times 1}$ and Cell : $\\mathbb{R}^{s \\times d} \\times \\mathbb{R}^{s \\times d} \\rightarrow \\mathbb{R}^{s \\times d}$ can be any arbitrary recursive cell function like an LSTM (Hochreiter & Schmidhuber, 1997) or the Transformer-inspired Gated Recursive Cell (GRC) (Shen et al., 2019).\nWe can interpret $G_i$ as the probability for the information in i to be pushed leftwards (deleting it from the current position)\u00b2. This interpretation provides intuition for Eq. 16 and Eq. 17. Given this interpretation, $L_i$ (where $L = AG$) can represent the probability of pulling some information from rightwards. This explains Eq. 16, where we update a position i based on its $L_i$ score, which indicates if new information is being pulled. Moreover, $G_i$ can also be simultaneously treated as a deletion probability (if the information in i is pushed leftwards, position i itself can be deleted). Thus, $E^{t+1}$ is updated based on $G$ as in Eq. 17 to reduce existential probability according to the current deletion probability.\nRequirements Satisfaction in CRvNN: Like a Transformer, CRVNN can also process sequences in arbitrary orders satisfying C1. It has a gating mechanism satisfying C2 and a dynamic halting mechanism satisfying C3. However, although CRVNN satisfies these requirements, it does so at the cost of stronger inductive bias and less flexibility. Unlike CRVNN, in principle, NDRs can model more general non-projective structures for information flow."}, {"title": "5 Connections between CRvNN and NDR", "content": "With our new formalism, the connection between NDR and CRVNN (and by extension the connection be-tween Transformers and Tree-RvNNs) becomes much more explicit . In the Retrieve component, note how Eq. 13 to create the attention-like matrix in CRvNN turns out to be of the same form as Eq. 6 for geometric attention in NDR. The main difference is that CRVNN does not use query-key interaction\u00b3 and the set $S_{ij}$ is defined differently in both cases. Particularly given the restrictions in $S_{ij}$, the attention in CRVNN can be said to be a strictly directionally masked variation of geometric attention from NDR. Beyond that, in the Compose component, we again find a striking similarity between 16 for CRVNN and 10 from NDR. Both represent a gating mechanism to control information updates. Besides the similarities, a few differences worth emphasizing include that CRvNN lacks any multi-head setup, and its gates are scalar."}, {"title": "Dynamic Halt:", "content": "One of the most critical differences between NDRs and CRVNNs is dynamic halt. CRVNN's maintenance of existential probabilities makes up for a convenient mechanism for a dynamic halt. First, we can use a threshold to convert the probabilities into binary values, and then we can choose to halt whenever all but one position exists with a binarized value of 1 according to the existential probability. In contrast, NDR lacks any form of dynamic halt (see the end of \u00a73)."}, {"title": "6 Transformers under the Recursive Formalism", "content": "Here, we explain how the vanilla Transformer can fit under the recursive formulation introduced in \u00a72. First, note that the recursive formalism in \u00a72 does at least represent Universal Transformer (UT) (Dehghani et al., 2019). UT repeatedly applies the same function with the same transformer weights in every layer during encoding (equivalent to sharing parameters in every layer). Second, it can also be shown that any vanilla Transformers with 'unshared parameters' can be re-expressed in the form of a UT with recursive layers and repeated parameters by increasing the hidden state size (Bai et al., 2019) (See the supplementary C in Bai et al. (2019)). Thus, by extension, the general schema in \u00a72 also accommodates vanilla Transformers."}, {"title": "7 NDR Empirical Analysis", "content": "We performed some empirical experiments with NDR on two datasets, ListOps and Logical Inference, to see where it stands compared to CRVNN."}, {"title": "7.1 ListOps-DG2", "content": "Dataset Settings: List Ops or List Operations is an arithmetic task for solving a nested list of mathematical operations as exemplified in \u00a71. ListOps-DG is a special split originally set up in Ray Chowdhury & Caragea (2023) to test depth-generalization capacities of models. Depth of a ListOps sequence represents the maxi-mum number of nested operators present in it. The training set of ListOps-DG has \u2264 6 depths, but the test split (DG split) has higher depths (8-10). There are also other splits for length generalization, higher depth generalization, and argument generalization. The test split from Long Range Arena (LRA) Tay et al. (2021) version of ListOps is also used. Details are present in Ray Chowdhury & Caragea (2023), and the main data parameters per split are presented in Table 2. ListOps-DG2 - the exact data used here - is designed to be similar to ListOps-DG. ListOps-DG2 was also originally presented in Ray Chowdhury & Caragea (2023). The main difference between ListOps-DG and ListOps-DG2 is that ListOps-DG2 has 1 million training samples, which is ~ 10 times more than that in ListOps-DG. We did this because originally, NDR was also trained in a similar sized ListOps dataset (Csord\u00e1s et al., 2022b). The training samples have \u2264 100 sequence length, \u2264 5 arguments, and < 6 depth (depth is the maximum number of nested operators).\nModel Settings: We compare three models a relatively vanilla Transformer, NDR, and CRVNN. For the NDR, we use the same hyperparameters as used for ListOps by Csord\u00e1s et al. (Csord\u00e1s et al., 2022b). For CRVNN, we use the same hyperparameters as we used before for List Ops-DG in Ray Chowdhury & Caragea (2023). For the baseline Transformer, we use FlashAttention2 (Dao, 2024) and xPos as positional encoding (Sun et al., 2023). We train NDR with 20 layers. We used the same hyperparameters for Transformer as used for ListOps in the original work presenting Long Range Arena (Tay et al., 2021).\nResults: We present the results . As we can see from the results, the Transformer baseline does not perform as well. NDR, with better inductive biases, performs quite well and generalizes to slightly higher depths (the DG split) consistent with prior results (Csord\u00e1s et al., 2022b). However, it fails to generalize"}, {"title": "7.2 Logical Inference", "content": "Dataset Settings: We also perform a few experiments of the logical inference dataset (Bowman et al., 2015). Logical Inference (Bowman et al., 2015) is another structure-sensitive task where Transformers strug-gle to generalize (Tran et al., 2018). In this task, the model has to classify the logical relationship between two given sequences in propositional logic formalism. For this task, the models are trained on samples with < 6 operators and then tested on samples with a higher number of logical operators (7 \u2013 12).\nModel Settings: Again, we compare three models a relatively vanilla Transformer, NDR, and CRVNN. We share the hyperparameters between ListOps-DG2 and Logical Inference for both CRvNN and NDR. One difference is that NDR was trained with 15 layers since the training samples have lower tree depth. For the baseline Transformer, we use FlashAttention2 (Dao, 2024) and xPos as positional encoding (Sun et al., 2023) as before. We used the same hyperparameters for Transformer as we used for logical inference in Chowdhury & Caragea (2024)."}, {"title": "8 Discussion and Future Directions", "content": "Structural Flexibility: NDRs have a more flexible inductive bias than CRVNN in the retriever function because NDRs use key-value attention to attend to any of the closest matching elements. In principle, NDR could model non-projective or more general structures for information flow. However, in CRVNN, the retriever only allows attention to the closest existing element, which limits it to only approximate information flow through a projective constituency structure. While the stronger inductive bias in CRvNN can be helpful, it can also limit its scalability to more complex tasks when more data is available (Sutton, 2019). At the same time, some degree of inductive bias is, arguably, still necessary for OOD generalization (Mitchell, 1980; Goyal & Bengio, 2022). The ideal is to find a sweet spot where the vicinity of the design space explored here may be a promising region to investigate.\nPermanent Deletion: CRVNN enforces a more controlled information flow compared to NDRs. One interesting distinction that allows this is that CRvNN uses a form of permanent (soft) deletion of token positions through the existential probabilities (which are monotonically decreasing per recursive function). This can allow for a better inductive bias for ignoring processed information in a consistent manner.\nDynamic Halting: One immediate future direction can be to investigate different ways to incorporate dynamic halt mechanisms (Schmidhuber, 2012; Graves, 2016; Banino et al., 2021) in NDR. Another alter-native in a similar vein can be to turn NDR into a Deep Equilibrium Network (Bai et al., 2019), which can implicitly and adaptively increase recursive iterations based on sample difficulty. Promisingly, a re-cent work even shows that Deep Equilibrium models can also help in OOD length generalization in certain synthetic tasks (Liang et al., 2021). As a plus, Deep Equilibrium models can also significantly reduce the memory consumption of these models (Bai et al., 2019). We also explore some novel halting mechanisms in Chowdhury & Caragea (2024). This could be further investigated with NDR or adjacent Transformer models."}]}