{"title": "SIMPLIFYING COMPLEX MACHINE LEARNING BY LINEARLY SEPARABLE NETWORK EMBEDDING SPACES", "authors": ["Alexandros Xenos", "Noel Malod-Dognin", "Natasa Przulj"], "abstract": "Low-dimensional embeddings are a cornerstone in the modelling and analysis of complex networks. However, most existing approaches for mining network embedding spaces rely on computationally in-tensive machine learning systems to facilitate downstream tasks. In the field of NLP, word embedding spaces capture semantic relationships linearly, allowing for information retrieval using simple linear operations on word embedding vectors. Here, we demonstrate that there are structural properties of network data that yields this linearity. We show that the more homophilic the network representation, the more linearly separable the corresponding network embedding space, yielding better downstream analysis results. Hence, we introduce novel graphlet-based methods enabling embedding of networks into more linearly separable spaces, allowing for their better mining. Our fundamental insights into the structure of network data that enable their linear mining and exploitation enable the ML community to build upon, towards efficiently and explainably mining of the complex network data.", "sections": [{"title": "Introduction", "content": "Networks naturally model complex systems in many real-world applications. Examples include social, information, and biological domains. Current state-of-the-art approaches for analyzing these complex data are based on network embedding techniques [1]. These algorithms map a network nodes in a low d-dimensional space, where the geometry of the space reflects the similarities between the nodes [2] in the sense that two nodes are defined to be similar either when they belong to the same network neighbourhood, or have similar topological roles independent of being adjacent, e.g. being hub nodes (also called topological similarity). In some data types, it is unclear how to exploit the structure of the generated embedding spaces, so the vectorial representation of the nodes (termed embedding vectors) are then fed to computationally intensive machine learning (ML) systems to perform tasks, such as node classification, clustering, and link prediction. Interestingly, in another domain, in the field of Natural Language Processing (NLP), the Skip-Gram neural network (NN) based word embeddings (e.g. Word2vec [3]) were shown to capture semantic relationships between words linearly, allowing for downstream analysis tasks using simple linear operations on word embedding vectors.[3] More recently, linear semantic relationships (i.e., the compositionality of embedding vectors) have also been observed in data embeddings from pre-trained vision-language models (VLMs).[4] This poses the question of why in some cases, the embedding techniques lead to a well-stratified embedding space amenable to linear exploitation, while in other cases they do not. This motivates us to explore: (i) if there is an intrinsic property in the structure of the data that yields this linearity and (ii) if the linearity holds in the biological network domain, for embeddings of the systems-level molecular networks. If so, this would simplify the analyses by making them linear, hence alleviating the need for computationally intensive ML models.\nThe main methods for generating network embeddings are using Graph Neural Networks (GNNs) that adapt the Graph Convolutional Networks used in image processing [5, 6], or employ random walks to generate sequences of nodes (e.g., DeepWalk [7], LINE [8] and node2vec [9]) on which the above mentioned Skip-Gram NN architecture"}, {"title": "1.1 Contribution", "content": "In this study, we introduce novel, graphlet-based, random-walk matrix representations to be used for embedding networks that account for both network neighborhood similarity and topological similarity of nodes. The abundance of these network matrix representations allows us to explore the link between the linearity of the resulting embedding space and the intrinsic properties of the given matrix representation. We hypothesize that the more homophilic the input network matrix representation, the more linearly separable (and hence linearly exploitable) the resulting embedding space, hence diminishing the need for complex ML approaches to perform downstream analyses. We demonstrate that for the thirteen networks from multiple domains that we studied (six multi-label biological networks and seven single-label networks from social, citation, and transportation networks domain), there always exists a graphlet-based matrix representation that yields a more homophilic representation compared to the standard adjacency matrix. We generate the embeddings by explicitly factorizing these matrices using an Orthogonal NMTF (ONMTF) framework.[28] We demonstrate that in 9 out of the 13 embedding spaces we can achieve as good node classification F1-scores with linear classifiers (e.g., linear Support Vector Machines (SVM)) as with non-linear classifiers, indicating that the embedding vectors of nodes from different classes are sufficiently linearly separable. We also demonstrate that in three of these networks, the embedding spaces are fully linearly separable, since the node classification F1-scores of linear SVM are very high and not statistically significantly different than those of complex non-linear ML methods. Finally, we also demonstrate that even when the resulting embedding spaces are not linear (i.e. when the non-linear classifier better disentangles the different node classes), our graphlet-based embeddings outperform the state-of-the-art"}, {"title": "2 Methods", "content": ""}, {"title": "2.1 Datasets", "content": ""}, {"title": "Biological multi-labeled networks", "content": "In this study, we focus on the most well-studied molecular networks, modelling protein-protein interactions and gene co-expressions of the following three species: Homo sapiens (human), Saccha-romyces cerevisiae (budding yeast) and Schizosaccharomyces pombe (fission yeast). For each of these three organisms, we collected the experimentally validated protein-protein interactions (PPIs) from BioGRID [29] version 3.5.182. We model these data as PPI networks in which nodes represent protein-coding genes, and edges connect nodes (genes) whose protein products physically bind. Also, we collected the gene co-expressions (COEX) from COXPRESdb [30] version 8 (for all the species, we selected the dataset, u22, that has the highest number of expression samples). In our collected co-expression datasets, the co-expression measure between two genes is standardized using zeta scores. To construct the COEX networks, in which nodes represent genes and edges the co-expressions between the genes, we selected the strongest co-expression values having a zeta score higher than or equal to 3, which is the usual practice. The statistics of the six generated biological networks are presented in Table 1. These networks are multi-labeled, as a gene often has multiple annotations, detailed below."}, {"title": "Gene annotations", "content": "For each gene (or equivalently, protein, as a gene product) in the biological networks, we collected their Reactome Pathway (RP) annotations from Reactome database v83[31] and their KEGG Pathway (KP) annotations from KEGG database v111.1.[32] Also, we collected the most specific experimentally validated Gene Ontology Biological Process (GO BP) annotations [33] from the NCBI database (downloaded on September 2023). The GO BP annotations are back-propagated, using the is-a relationship, to their ancestors in the Gene Ontology tree. Note that a gene can have multiple GO BP, KP and RP annotations."}, {"title": "Single-labeled networks", "content": "We analyze single-labeled networks, beyond biology, that are used as benchmarking datasets in ML studies. We collected from PyTorch Geometric [34] the USA air-traffic network,[35] the Coauthor CS network,[36] the CORA and the CiteSeer citation networks,[37] the two standard heterophilic Wikipedia page-page networks (Chameleon and Squirrel) [38] and the Wiki-CS hyperlinks network. [39] For all the datasets, we treat the networks as undirected and only consider their largest connected component. The statistics of the aforementioned networks are presented in Table 2."}, {"title": "2.2 Network embeddings", "content": "The state-of-the-art approaches to analyze complex data modeled as networks are by using network embedding algorithms.[1, 2] Such algorithms represent nodes as vectors in a d-dimensional space, in which similar nodes are embedded close in the space. The new generation of network embedding algorithms are inspired by the word embedding techniques used in NLP.\nRecently, Levy et al. [40] showed that neural networks (i.e., Skip-Gram with Negative sampling, SGNS), that are used to obtain the word embeddings in NLP, are implicitly factorizing a word-word matrix in which a cell represents the pointwise mutual information (PMI) [41] of the two words, shifted by a global constant; in particular, PMI measures the strength of the association between two words by calculating the log-likelihood ratio of the co-occurrence of two words, compared to what would be expected if they were statistically independent. However, the PMI value of two words, w and c, that do not occur in the lexical corpus is $PMI(w, c) = log \\frac{0}{0} = -\\infty$. To address this issue, in the field of NLP, they replace the negative values with zeros, resulting in a sparser matrix, called the positive PMI (PPMI) matrix.[41] Levy et al. [40] also demonstrated that the exact factorization of this PPMI matrix with Singular Value Decomposition (SVD) leads to equivalent, or better results for word similarity tasks than the embeddings resulting from SGNS architectures.\nFormally, for two words, w and c, PPMI is defined as:\n$PPMI(w, c) = max \\{0, log \\frac{\\#(w, c) x |C|}{\\#w x \\#c}\\},$                                                                             (1)\nwhere C is the size of the corpus, #(w, c) is the number of times the two words co-occur in the corpus and #w and #c are the numbers of times the words w and c occur in the corpus, respectively."}, {"title": "2.3 Graphlets and Graphlet Adjacency", "content": "Graphlets are small, connected, non-isomorphic, induced sub-graphs of a large network, that appear at any frequency in the network.[25] Different topological positions within graphlets are characterized by different symmetry groups of nodes, called automorphism orbits.[43] Orbits are used to generalize the notion of the node degree: the graphlet degree of a node is the number of times the node touches a particular graphlet at a particular orbit. [44] Yaveroglu et al. [45] showed that between the orbits, there exist redundancies, as well as dependencies, and proposed a set of 11 non-redundant orbits of 2- to 4-node graphlets (see Figure 1). Each node in the network is represented by its 11-dimensional vector called Graphlet Degree Vector (GDV), that captures the 11 non-redundant graphlet degrees of the node. To quantify the topological similarity between two nodes, u and v, we compare their GDV vectors using the GDV distance, which is computed as follows. Given two GDV vectors, x and y, the distance between their ith coordinate is defined as:\n$Disti(x, y) = Wi \u00d7 \\frac{log(xi + 1) - log(yi + 1)}{log(max\\{xi, Yi\\} + 2)},$                                                             (5)\nwhere wi is the weight of orbit i that accounts for dependencies between the orbits (detailed by Milenkovic and Przulj [44]). The log-scale is used to control the different orders of magnitude between orbit counts. Then, GDV similarity between nodes u and v is defined as:\n$GDV sim(u, v) = 1 \u2013 \\frac{\\sum_{i=1}^{11}Disti(x, y)}{\\Sigma_{i=1}^{11} Wi}$  (6)\nThe pairwise GDV similarities over all nodes in a network are represented in the GDV similarity matrix, capturing the topological similarities over all nodes in the network.[45] In our recent study, we used the GDV similarity matrix as the input into the DeepWalk closed formula and factorized the resulting matrix (GDV PPMI matrix) with NMTF to generate the network embedding based solely on the topological similarities between the nodes. [11]"}, {"title": "2.4 Novel graphlet-based network matrix representations", "content": "In this study, we generalize the PPMI formula used in NLP, to perform network embeddings that consider local subgraphs. In particular, we use as the corpus all instances of a given graphlet in the network and then compute how frequently two nodes co-appear in these instances of a given graphlet in the network. Recall that a Graphlet Adjacency Matrix captures in how many instances of a given graphlet two nodes co-occur. Thus, to transform the raw co-occurrences to probabilities of co-occurrences, we apply to each Graphlet Adjacency Matrix the PPMI formula from NLP (see equation 1). We call this generalization of the PPMI the Graphlet Pointwise Mutual Information (GPMI). \u03a4\u03bf generate the Graphlet Adjacency Matrices, we use the implementation of Windels et al. [46]\nFormally, for two nodes, u and v, GPMI is defined as:\n$GPMI(u, v) = max \\{0, log \\frac{vol(Ak) \u00d7 Ak(u, v)}{Dk(u) \u00d7 Dk(v)}\\},$ (8)\nwhere vol(Ak) is the total number of instances of the given graphlet k in the network, Ak(u, v) is the number of instances of graphlet k in which nodes u and v co-occur, and Dk(u) and Dk (v) are the numbers of instances of graphlet k in which node u and node v occur, respectively.\nIn the DeepWalk closed-formula, the key parameter is the adjacency matrix of the network, A, in which the random walks of length T are computed. Hence, to incorporate the graphlets into the DeepWalk closed formula, we propose to compute random walks in all different Graphlet Adjacency Matrices corresponding to the nine graphlets with up to 4 nodes (illustrated in Figure 1). In the Graphlet Adjacency Matrix, Ak, the entries represent the numbers of times two nodes simultaneously participate in the given graphlet, Gk. This number can be very big, especially for higher-order (larger) graphlets. To address this, we introduce the binarized Graphlet Adjacency Matrix, \u0100k, by setting all the non-zero values to one. Then, we use as input for the DeepWalk closed-formula any of the nine binarized Graphlet Adjacency Matrices. We name this generalization of the DeepWalk closed formula the DeepGraphlet for graphlet Gk:\n$DeepGraphletk = max \\{0,log(vol(Ak) \\sum_{r=1}^{T}((DAk)^{-1} Ak)^r (DAk)^{-1})\\},$ (9)\nwhere vol(Ak) is the volume of the binarized Graphlet Adjacency Matrix and is computed as $vol(Ak) = \\Sigma_{i} \\Sigma_{j} Ak (i, j)$, Ak is the binarized Graphlet Adjacency Matrix of the network, Dk is the diagonal matrix of degrees of the given binarized Graphlet Adjacency Matrix and T=10 is the length of the random walks.\nIn both of our newly introduced network matrix representations, GPMI and DeepGraphlet, we have nine different graphlet-based representations of a network that correspond to the different connectivity patterns captured by each of the nine 2-node to 4-node graphlets."}, {"title": "2.5 Homophily measures", "content": "To assess if the unweighted matrix representations (Graphlet Adjacency Matrices and binarized Graphlet Adjacency Matrices) of a network G = (V, E), Ak and \u0100k, are homophilic (i.e., if the adjacent nodes share the same label), we use the two standard metrics: the edge homophily index and node homophily index.[21] The edge homophily index,"}, {"title": "2.6 Non-negative matrix tri-factorization based embeddings", "content": "Following our published work[11], we use an Orthonormal NMTF (ONMTF) framework to decompose the different representation of the networks (Adjacency, PPMI, DeepWalk and their graphlet-based extensions) and generate the corresponding network embedding spaces. In particular, given an input network matrix representation, X, our ONMTF framework decomposes it into three non-negative matrix factors, E, S and PT, as X \u2248 ESPT, where E. S contains the vector representations of the entities of X in the embedding space spanned by PT, S is a compressed representation of network X and PT is the orthonormal basis of the embedding space. Importantly, the orthonormality constraint (PTP = I) leads to independent, nonambiguous directions in the embedding space and improves the clustering interpretation of NMTF.[28] The decomposition is done by solving the following:\n$Min_{G,S,P>0}\\{||X \u2013 ESPT ||}\\, PT P = I$                                                                (10)\nwhere F denotes the Frobenius Norm. This optimization problem is NP-hard [28], thus to solve it we use a fixed point method that starts from an initial solution and iteratively uses the multiplicative update rules [28], derived from the Karush-Kuhn-Tucker (KKT) conditions, to converge towards a locally optimal solution (see [11] for details). To generate the initial E, S and P matrices, we use the Singular Value Decomposition (SVD) based strategy.[49] This strategy makes the solver deterministic and also reduces the number of iterations that are needed to achieve convergence (see [49] for more details). We stop the iterative solver after 500 iterations."}, {"title": "2.7 Linear separability of the embedding space", "content": "An embedding space is considered to be linearly separable if the embedding vectors of nodes from different classes can be separated by hyperplanes. If the nodes belonging to different classes in the embedding space are linearly separable, then a linear classifier can classify them into their respective classes as accurately as a non-linear classifier. Hence, to assess the linearity of an embedding space, we perform node classification on single-label and multi-label networks by using Support Vector Machines (SVM) with both linear (Euclidean) and non-linear (Radial Basis Function, RBF) kernel. In addition, we compare their performance with that of the Random Forest (RF) classifier, a state-of-the-art non-linear method.[50] To evaluate the accuracy of the classifiers, we use 10-fold cross-validation and compute the corresponding weighted F1-score for each classifier.\nWe say that an embedding space is sufficiently linearly separable, if the node classification F1-scores of the linear SVM (Euclidean kernel) are on par, or surpass those of the non-linear methods (SVM RBF and RF). To demonstrate that there are no statistically significant differences in the performances of the linear and non-linear classifiers, we compare the distributions of their F1 scores by using the Mann-Whitney U test. If in addition to having no statistically significant differences, these F1-scores are also greater than 0.8, we say that the embedding space is fully linearly separable. Finally, if the F1-scores of the non-linear classifiers are higher than those of the linear methods, we say that the embedding space is non-linearly separable."}, {"title": "2.8 Random partition graph model", "content": "To demonstrate the relationship between the homophily level of the input network matrix representation and the linear separability in the embedding space under ideal conditions, without noisy data, we simulate data using the random partition graph model.[51] The main parameters of this model are the number of partitions (representing communities), the size of each community, the probability of connections between nodes within the same community (Pin, intra-edges), and the probability of connections between nodes from different communities (pout, inter-edges). We set the number of nodes to 1,000 distributed across 5 communities and we vary the pin probability from 0.05 to 1 in increments of 0.05 and the pout from 0 to 1 in increments of 0.05, resulting in the total of 420 networks. Note that starting both probabilities at 0 would result in networks with no edges, making it meaningless to apply the homophily measures. For each simulated network, we compute the homophily level of the input network matrix representation: adjacency matrix, DeepWalk closed matrix formula and LINE closed matrix formula. Then, we use our NMTF-based framework to generate the embedding space, and for each embedding space, we perform node classification by using SVM with the Euclidean kernel. Finally, we compute the Pearson Correlation Coefficient between the homophily levels of the input network matrix representations and the F1-node classification scores."}, {"title": "2.9 Downstream analysis tasks", "content": "To demonstrate that higher linear separability of classes in the embedding space leads to better downstream analysis results, we perform two different experiments: one for single-labeled networks and another for multi-labeled biological networks. For the single-labeled networks, we predict the label of the nodes based on their distances in the embedding space. For the biological networks, where nodes (genes) can have multiple annotations or remain unannotated due to unknown functions, a common downstream analysis task involves uncovering functional network modules\n    sets of genes that collectively perform higher-level biological functions.\nLabel prediction in single label networks: In a single-labeled network, each node is assigned one label, and two nodes that have the same label belong to the same class. A common downstream task in an embedding space is to classify a node based on its cosine similarity from nodes with known labels. The cosine similarity is a measure of similarity between two vectors that considers the angle between them rather than their length. Here, we evaluate if the embedding spaces of single-labeled networks are organized coherently with respect to their node classes. To do so, we formulate the problem as a multi-class classification problem, in which we aim to classify two nodes with the same label based on their cosine similarity. The ability of the cosine similarity between the embedding vectors of nodes to correctly group nodes of the same class is evaluated by using the ROC curve (AUROC) analysis. [52] Since we have more than one node class (label), we use the one-vs-rest strategy to generalize the binary classification task to the multi-class problem. In this strategy, we use one binary classifier for each possible class and compute its corresponding AUROC score. Then, to account for the imbalances between the different classes, we calculate the weighted AUROC score.\nFunctional module discovery in biological networks: In biological networks, a common downstream analysis task is uncovering functional network modules, i.e., sets of genes that together perform higher-level biological functions. After embedding a biological network, this is typically done by clustering the genes based on the proximity of their embedding vectors (Euclidean distance) in the embedding space. Subsequently, the biological relevance of the obtained"}, {"title": "3 Results", "content": "To generate embeddings that simultaneously capture node neighborhood similarity and node topological similarity, we use the nine 2- to 4- node graphlets (Fig. S1) as follows. To randomly walk between similarly wired nodes in the network, from a given node we can visit any other node that simultaneously participates in the selected graphlet (specified by us) that the given node participates in; we do this for all x graphlets that the given node participates in to generate x different matrices and we repeat this for each node of the network. That is, the connectivity information is captured by the family of Graphlet Adjacency matrices (Gadj) [46], where Go is the traditional adjacency matrix (detailed in Methods). We extend the DeepWalk and the Line matrix closed formulae to account for these new graphlet based random walks captured by the Graphlet Adjacency matrices, hence defining the new DeepGraphlets family and the new Graphlet Pointwise Mutual Information (GPMI) family of closed formulae, respectively, defined for each of the 9 graphlets (detailed in Methods). For a given graphlet, Gk, we term the DeepWalk graphlet-based extension as DeepGraphletGr and the LINE graphlet-based extension as GPMIG\u0141 (for details, see \u201cGraphlet-based network embeddings\" in Methods). Note that for all of the new matrix representations, those based on graphlet Go correspond to the original methods, with which we compare our new methods; the rest of our new graphlet-based extensions (termed \"higher-order graphlets\") capture both the neighborhood-based similarity (as in the competing methods) and the topological similarity (beyond only the direct neighborhood). In addition, we compare our new graphlet-based extensions with the baseline Graphlet Adjacency matrices and with two network representations that rely only on the topological similarity between the nodes, the Graphlet Degree Vector (GDV) similarity matrix and the GDV PPMI matrix [11] (for details, see \u201cGraphlets and Graphlet Adjacency\" in Methods).\nWe apply our new graphlet-based network representations on networks in several application domains. In particular, we apply them on systems-level molecular interaction networks, in which nodes corresponds to genes (or equivalently, to proteins, as gene products), which can have multiple annotations (labels). We use the protein-protein interaction (PPI) and gene co-expression (COEX) networks of three species, Homo sapiens, Saccharomyces cerevisiae, and Schizosaccharomyces pombe, resulting in six molecular networks. The sizes of these six molecular networks are presented in Table 1. In addition, we also apply our methodology on seven commonly used single-label networks from different domains: the USA air-traffic network [35], the Coauthor computer science (CS) network [36], the CORA and the CiteSeer [37] citation networks, the Wiki-CS hyperlinks network [39] and the two Wikipedia page-page networks (Chameleon and Squirrel).[38] Note that two of the single-label networks, Chameleon and Squirrel, are heterophilic based on their standard adjacency matrix representation (for details, see \"Single-label networks\" in Datasets in Methods).\nThe statistics of the these networks are presented in Table 2."}, {"title": "3.1 Graphlet random walk-based matrices yield more homophilic network representations", "content": "To demonstrate that our new graphlet-based extensions of LINE and DeepWalk yield more homophilic representations of the networks than the original baseline methods (that also correspond to our Go extension, as detailed above), we measure the level of homophily of the different matrix representations of six molecular multi-label networks and seven single-label networks (detailed above). Traditionally, in unweighted networks represented by their standard adjacency matrices, homophily is measured by the edge homophily index (the proportion of edges connecting two nodes from the same class) and the node homophily index (the proportion of adjacent nodes from the same class).[47] In weighted networks, it is measured by the Geometric Separability Index (GSI) [48], a simplified measure that compares the label of a node only with the label of its closest neighbor. In this study, we extend these measures to quantify the homophily/heterophily in any matrix representation of the network (weighted or unweighted). In the case of the molecular networks, we annotate the genes with level 1 Gene Ontology Biological Process (GO BP) terms [33], which represent higher-level biological functions. Since the functions (annotations) of all genes are not known, we report the results over the set of genes with known annotations. For the number of annotated nodes in each of these networks, see Supplementary Table 3.\nWe observe that for both the multi-label and the single-label networks, there is at least one higher-order graphlet-based network representation that is more homophilic (i.e., having greater GSI, node homophily and edge homophily indices) than the baseline adjacency matrix, DeepWalk and LINE matrix closed formulae (see Figure 1). In addition, over all networks and over all graphlets, our new DeepGraphletG, and GPMIG, representations that capture both topological and neighborhood-based similarity yield more homophilic representations than GDV similarity matrix and GDV PPMI matrix that are based solely on topological similarity (see Figure 1). Among the different network representations, graphlet G2 and graphlet G8 extensions of GPMI and G Adj lead to the most homophilic representations in terms of the node and edge homophily index (see Panels A and B of Figure 1). In addition, our new DeepGraphlets yield the most homophilic representations in terms of the GSI (see Panel C and D of Figure 1): DeepGraphletG\u2081 in case of single-label networks, and DeepGraphletG\u2082 in case of multi-label networks.\nIn summary, for both the multi-label and the single-label networks, our new graphlet-based matrix representations of the networks are more homophilic than the original representations that capture either only the direct neighbourhood similarity, or only the topological similarity. In the following sections, we assess if embedding the networks by factorising these, more homophilic matrix representations of networks leads to more linearly separable network embedding spaces."}, {"title": "3.2 Graphlet random walk-based network representations lead to linearly separable embedding spaces", "content": "We examine if our graphlet-based matrix extensions detailed above can be applied to obtain the state-of-the-art network embedding methods yielding more linearly separable network embedding spaces. That is, we examine if they can be applied to result in the embedding vectors of nodes from different classes that can be separated by hyperplanes. To do that, first we generate the network embedding spaces by factorizing all the different matrix representations (DeepGraphlet, GPMI and Graphlet Adjacency matrices) of the seven single-label networks detailed above. Recall that this involves 9 graphlet-based matrix representations (because there are 9 up to 4-node graphlets) per method (for the 3 methods, DeepGraphlet, GPMI and Graphlet Adjacency matrices) for each of the 7 single-label networks, hence yielding the total of 9 \u00d7 3 \u00d7 7 = 189 embedding spaces. We do the matrix factorizations by utilizing the ONMTF framework (detailed in \u201cNon-negative matrix tri-factorization embeddings\" in Methods). Similarly, we generate the embeddings of the six molecular interaction networks detailed above (i.e., 9 graphlet-based representations per method per network, yielding the total of 9 \u00d7 3 \u00d7 6 = 162 embedding spaces). Recall that if the classes in an embedding space are linearly separable, then a linear classifier should classify the nodes into their respective classes as accurately as a non-linear classifier. Hence, we assess the linearity of an embedding space produced by our methods by comparing the resulting node classification weighted F1-scores to those obtained by Support Vector Machines (SVM) with linear kernel (Euclidean) and non-linear kernel (Radial Basis Function, RBF). In addition, we assess it by comparing with that of the Random Forest (RF) classifier, a state-of-the-art non-linear method [50] (for details, see \u201cLinear organization of the embedding space\" in Methods). We term a space as \u201csufficiently linearly separable\" if the node classification weighted F1-scores of the linear classifiers are on par, or better than those of the non-linear classifiers. In addition, if these weighted F1-scores are also greater than 0.8, we term the space as being \u201cfully linearly separable\". Finally, if the weighted F1-scores of the non-linear classifiers are larger than those of the linear methods, we term the space as being \"non-linear\".\nAs shown in Table 1, in 9 out of the 13 networks, the corresponding embedding spaces are sufficiently linearly separable, since the node classification F1-scores of the linear SVM outperform those of the non-linear SVM (RBF kernel). In 3 of these networks (Cora, Wikipedia CS and CS Co-author), the corresponding embedding spaces are fully linearly separable, since the F1-scores exceed 0.8 (average weighted F1-score of 0.85, see left panel in Supplementary"}, {"title": "3.3 More homophilic network representations lead to linearly separable embedding spaces", "content": "We hypothesize that the more homophilic the input network matrix representation, the more linearly separable are the node classes in the resulting embedding space. To demonstrate this under ideal conditions, without noisy data, we generate synthetic data from the random partition graph model.[51] This model allows the generation of random graphs that exhibit clustering and modular organization akin to real-world networks. We simulate 420 networks, each containing 1,000 nodes distributed across five communities (for details, see \"Random partition graph model\" in Methods). We represent each network with the DeepWalk closed matrix formula, the LINE closed matrix formula and the standard Adjacency matrix. For each network's matrix representation, we compute its node homophily index, edge homophily index and GSI. Subsequently, we generate the network embedding spaces by factorizing their aforementioned matrix representation by using the NMTF framework (see \u201cNon-negative matrix tri-factorization embeddings\" in Methods) and for each embedding space, we compute the node classification F1-score with the linear SVM (Euclidean kernel). We observe a positive correlation between the homophily level of the input matrix representation and the node classification F1-scores: 0.68 Pearson Correlation Coefficient (PCC) for GSI and 0.19 PCC for node and edge homophily indices, all statistically significant (see Table 2). To assess if this observation holds for our real networks, we computed the correlations between the homophily indices of the network matrix representations of our 13 real networks and the node classification F1-scores in the corresponding embedding spaces. In the single-label networks, we observe 0.44 PCC for GSI and approximately 0.36 PCC for node and edge homophily indices, all statistically significant (see Table 2). In the multi-label molecular networks, we observe 0.46 PCC for GSI, 0.53 PCC for the node homophily index and 0.44 PCC for the edge homophily index, all statistically significant (see Table 2).\nHence, we verify our hypothesis in simulated and real world networks that the more homophilic the input network matrix representation, the more linearly separable are the node classes in the resulting embedding space. In the following section, we show that our new graphlet-based matrix representations of networks, which are more homophilic than their traditional matrix representations, also yield network embeddings resulting in better results in downstream analysis tasks."}, {"title": "3.4 Graphlet-based embeddings lead to better results in downstream analysis tasks", "content": "We assess the performance of our new graphlet-based network embeddings in downstream analysis tasks for single-label and multi-label real-world networks. In single-label networks, a common information retrieval task is to predict the label of a node based on the label of its most semantically similar node, defined by the largest cosine similarity of their embedding vectors. Therefore, we evaluate whether our new graphlet-based network embedding spaces improve information retrieval by using cosine similarity of the node embedding vectors in the same way. To do so, we assess if nodes whose embedding vectors have high cosine similarity have the same label. Formally, for each network, we compute its weighted area under the ROC curve (AUROC) [52"}]}