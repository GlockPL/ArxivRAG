{"title": "CURLing the Dream: Contrastive Representations for\nWorld Modeling in Reinforcement Learning", "authors": ["Victor A. Kich", "Jair A. Bottega", "Raul Steinmetz", "Ricardo B. Grando", "Ayano Yorozu", "Akihisa Ohya"], "abstract": "In this work, we present Curled-Dreamer, a novel reinforcement learning algorithm that integrates contrastive\nlearning into the DreamerV3 framework to enhance performance in visual reinforcement learning tasks. By incorporating\nthe contrastive loss from the CURL algorithm and a reconstruction loss from autoencoder, Curled-Dreamer achieves\nsignificant improvements in various DeepMind Control Suite tasks. Our extensive experiments demonstrate that Curled-\nDreamer consistently outperforms state-of-the-art algorithms, achieving higher mean and median scores across a diverse\nset of tasks. The results indicate that the proposed approach not only accelerates learning but also enhances the robustness\nof the learned policies. This work highlights the potential of combining different learning paradigms to achieve superior\nperformance in reinforcement learning applications.", "sections": [{"title": "1. INTRODUCTION", "content": "Reinforcement learning (RL) has achieved promising\nresults in addressing complex tasks through deep neural\nnetworks [1], [2], [3]. However, the challenge of learning\nfrom high-dimensional visual inputs persists. Traditional\napproaches often struggle with sample efficiency and the\nquality of the learned representations. Recent advance-\nments, such as the DreamerV3 algorithm, have shown\npromise by employing latent dynamics models to predict\nfuture states and rewards, thereby enhancing the efficiency\nof visual RL tasks [4]. Despite these improvements, there\nremains potential for enhancing representation learning\nand overall performance in RL algorithms [5], [1].\nContrastive learning has emerged as a powerful tech-\nnique for unsupervised representation learning, demon-\nstrating impressive results across various domains such\nas computer vision and natural language processing. One\nnotable algorithm, CURL, applies contrastive learning\nprinciples to RL by maximizing the agreement between\ndifferent augmentations of the same state while minimiz-\ning the agreement between different states [6], [7], [8], [9],\n[10]. This approach has proven effective in enhancing the\nperformance and robustness of RL algorithms [11], [12].\nWe propose an enhancement to the DreamerV3 algo-\nrithm by integrating the contrastive loss from CURL. Our\nmethod, termed Curled-Dreamer, aims to improve the en-\ncoder's ability to capture informative and discriminative\nfeatures from visual inputs. The Curled-Dreamer\u00b9 algo-\nrithm modifies the training procedure of the encoder in\nDreamerV3 to include a contrastive loss that promotes the\nsimilarity of representations for positive pairs (different\naugmentations of the same state) and dissimilarity for neg-"}, {"title": "2. RELATED WORKS", "content": "Contrastive learning has emerged as a transformative\napproach in RL by enhancing the quality of learned rep-\nresentations and improving sample efficiency. Eysenbach\net al. (2022) demonstrated that contrastive representation\nlearning methods can be directly applied as RL algorithms,\nachieving higher success rates in goal-conditioned tasks\nand outperforming prior methods on image-based tasks\nwithout additional data augmentation or auxiliary objec-\ntives [14]. Yang et al. (2023) proposed a novel unsuper-\nvised skill discovery method through contrastive learning,\nwhich maximizes mutual information between behaviors\nbased on the same skill, leading to the generation of di-\nverse and far-reaching skills [15].\nIn the realm of model-based RL, Ma et al. (2020) in-\ntroduced Contrastive Variational Reinforcement Learning\n(CVRL), which leverages contrastive learning to maxi-\nmize mutual information between latent states and obser-\nvations. This approach has shown robustness in tasks with\ncomplex visual observations, outperforming state-of-the-\nart generative world models [16]. Kim et al. (2022) devel-\noped the Action-Driven Auxiliary Task (ADAT) method,\nwhich focuses on essential features for decision-making\nand significantly enhances feature learning and perfor-\nmance in standard benchmarks [17].\nYuan and Lu (2022) addressed the challenge of distri-\nbution mismatch in offline meta-reinforcement learning\nby proposing a contrastive learning framework that im-\nproves task representation robustness and generalization\nto out-of-distribution behavior policies [18]. Shen et al.\n(2021) introduced Sequential and Dynamic Constraint\nContrastive Reinforcement Learning (SDCRL), incorpo-\nrating sequential information and dynamic transitions to\nimprove sample efficiency and performance on complex\ntasks [19].\nLi et al. (2023) introduced Model-Enhanced Con-\ntrastive Reinforcement Learning (MCRL) for sequential\nrecommendation, addressing data sparsity and overesti-\nmation issues, and significantly outperforming existing\nmethods [20]. Qiu et al. (2022) proposed Contrastive\nUCB, an algorithm integrating contrastive learning with\nUpper Confidence Bound (UCB) methods for improved\nsample efficiency and representation learning in online RL\nsettings [21].\nLastly, Wang and Hu (2023) developed lightweight and\neffective DRL algorithms incorporating contrastive learn-\ning to balance new and old experience trajectories, achiev-\ning state-of-the-art performance in PyBullet robotics en-\nvironments [22]. These studies collectively highlight the\npotential of contrastive learning to advance RL by enhanc-\ning representation quality, sample efficiency, and general-\nization capabilities."}, {"title": "3. METHODOLOGY", "content": "In this work, we propose an enhancement to the Dream-\nerV3 algorithm by integrating the contrastive loss from\nthe CURL algorithm, aiming to improve the encoder's\nability to capture informative and discriminative features\nfrom visual inputs. Curled-Dreamer, modifies the train-\ning procedure of the encoder in DreamerV3 to include a\ncontrastive loss that promotes the similarity of represen-\ntations for positive pairs (different augmentations of the\nsame state) and dissimilarity for negative pairs (augmenta-\ntions of different states). Additionally, we incorporate a\nreconstruction loss in the output of the decoder to further\nenhance the quality of the learned representations.\nThe Curled-Dreamer algorithm operates through the\nfollowing steps. First, we perform data augmentation on\neach state observation by performing random crops. Let\nbe a set of original states, and be\nthe corresponding augmented states. This step aims to\nimprove the robustness of the representations learned by\nthe encoder. Next, the encoder is trained using both\nthe objectives of DreamerV3 and a contrastive loss. The\ncontrastive loss is computed using the InfoNCE (Informa-\ntion Noise Contrastive Estimation) loss [23], defined as\nfollows:\n$\\L_{InfoNCE} = -E_A \\log \\left[\\frac{\\exp(sim(f_\\theta(a_i), f_\\theta(a_i'))/\\tau)}{\\sum_{a_j \\in A} \\exp(sim(f_\\theta(a_i), f_\\theta(a_j))/\\tau)}\\right],$ (1)\nwhere is the positive pair of (i.e., another augmenta-\ntion of the same state), denotes the cosine simi-\nlarity, and is a temperature parameter. This contrastive\nloss encourages the encoder to produce similar represen-\ntations for different augmentations of the same state and\ndissimilar representations for different states.\nThe representations obtained from the en-\ncoder are then used as inputs to the latent dynamics model.\nThe latent dynamics model and reward predictor are\ntrained to predict future states and rewards, respectively.\nThe loss function for training the latent dynamics model\nand the reward predictor is given by:\n$\\L_{dynamics} = E \\left[\\sum_{t=1}^{T} (||g_\\phi(z_t, a_t) - z_{t+1}||^2 + ||r_\\psi (z_t, a_t) \u2013 r_t||^2)\\right]$ (2)\nwhere is the latent state representation at time , is\nthe action taken at time , and is the reward received at\ntime . This loss ensures that the latent dynamics model\naccurately predicts the future latent states and rewards\nbased on the current latent state and action.\nThe encoder's output is also fed into a decoder to\nreconstruct the original input, promoting the learning of\nmore detailed and accurate representations. The recon-\nstruction loss is defined as:\n$\\L_{reconstruction} = E [||d_\\eta(f_\\theta(s_t)) \u2013 s_t||^2] .$ (3)\nThe policy is optimized using the predictions from\nthe latent dynamics model. The policy optimization fol-\nlows the actor-critic method used in DreamerV3. The\npolicy loss is given by:\n$\\L_{policy} = -E_{\\pi_\\theta} \\left[\\sum_{t=1}^{T} \\gamma^t r_\\psi (z_t, a_t)\\right],$ (4)\nwhere is the discount factor. This loss function maxi-\nmizes the expected cumulative reward over time, guiding\nthe policy to select actions that lead to higher rewards."}, {"title": "4. EXPERIMENTS", "content": "In this section, we present the experimental setup and\nthe results obtained from evaluating the proposed Curled-\nDreamer algorithm. We describe the environments used,\nthe hyperparameters adopted, and provide an in-depth\nanalysis of the results.\nWe evaluated the performance of Curled-Dreamer on a\nsuite of 20 tasks from the DeepMind Control Suite (DMC)\n[13]. The DMC provides a diverse set of continuous con-\ntrol tasks designed to test various aspects of reinforcement\nlearning algorithms, including balance, locomotion, and\nmanipulation. The selected tasks offers a comprehensive\nevaluation of the algorithm's capability to handle various"}, {"title": "5. RESULTS", "content": "Overall, Curled-Dreamer demonstrated substantial im-\nprovements across a variety of tasks, showcasing its versa-\ntility and robustness. The algorithm achieved higher mean\nand median scores across the suite of tasks, indicating a\nsignificant enhancement in learning efficiency and policy\nperformance. Some environments are shown in Fig. 2.\nWhile the results are summarized in Table 1 and depicted\nin Fig. 3.\nParticularly outstanding results were observed in\ntasks that involve complex dynamical systems and high-\ndimensional observations. For instance, Curled-Dreamer\nexcelled in the Acrobot Swingup task, achieving a score of"}, {"title": "6. CONCLUSION", "content": "In this paper, we introduced Curled-Dreamer, an en-\nhanced reinforcement learning algorithm that integrates\ncontrastive learning into the DreamerV3 framework. By\nincorporating the contrastive loss from CURL and a re-"}]}