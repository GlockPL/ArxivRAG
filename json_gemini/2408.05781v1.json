{"title": "CURLing the Dream: Contrastive Representations for World Modeling in Reinforcement Learning", "authors": ["Victor A. Kich", "Jair A. Bottega", "Raul Steinmetz", "Ricardo B. Grando", "Ayano Yorozu", "Akihisa Ohya"], "abstract": "In this work, we present Curled-Dreamer, a novel reinforcement learning algorithm that integrates contrastive\nlearning into the DreamerV3 framework to enhance performance in visual reinforcement learning tasks. By incorporating\nthe contrastive loss from the CURL algorithm and a reconstruction loss from autoencoder, Curled-Dreamer achieves\nsignificant improvements in various DeepMind Control Suite tasks. Our extensive experiments demonstrate that Curled-\nDreamer consistently outperforms state-of-the-art algorithms, achieving higher mean and median scores across a diverse\nset of tasks. The results indicate that the proposed approach not only accelerates learning but also enhances the robustness\nof the learned policies. This work highlights the potential of combining different learning paradigms to achieve superior\nperformance in reinforcement learning applications.", "sections": [{"title": "1. INTRODUCTION", "content": "Reinforcement learning (RL) has achieved promising\nresults in addressing complex tasks through deep neural\nnetworks [1], [2], [3]. However, the challenge of learning\nfrom high-dimensional visual inputs persists. Traditional\napproaches often struggle with sample efficiency and the\nquality of the learned representations. Recent advance-\nments, such as the DreamerV3 algorithm, have shown\npromise by employing latent dynamics models to predict\nfuture states and rewards, thereby enhancing the efficiency\nof visual RL tasks [4]. Despite these improvements, there\nremains potential for enhancing representation learning\nand overall performance in RL algorithms [5], [1].\nContrastive learning has emerged as a powerful tech-\nnique for unsupervised representation learning, demon-\nstrating impressive results across various domains such\nas computer vision and natural language processing. One\nnotable algorithm, CURL, applies contrastive learning\nprinciples to RL by maximizing the agreement between\ndifferent augmentations of the same state while minimiz-\ning the agreement between different states [6], [7], [8], [9],\n[10]. This approach has proven effective in enhancing the\nperformance and robustness of RL algorithms [11], [12].\nWe propose an enhancement to the DreamerV3 algo-\nrithm by integrating the contrastive loss from CURL. Our\nmethod, termed Curled-Dreamer, aims to improve the en-\ncoder's ability to capture informative and discriminative\nfeatures from visual inputs. The Curled-Dreamer\u00b9 algo-\nrithm modifies the training procedure of the encoder in\nDreamerV3 to include a contrastive loss that promotes the\nsimilarity of representations for positive pairs (different\naugmentations of the same state) and dissimilarity for neg-"}, {"title": "2. RELATED WORKS", "content": "Contrastive learning has emerged as a transformative\napproach in RL by enhancing the quality of learned rep-\nresentations and improving sample efficiency. Eysenbach\net al. (2022) demonstrated that contrastive representation\nlearning methods can be directly applied as RL algorithms,\nachieving higher success rates in goal-conditioned tasks\nand outperforming prior methods on image-based tasks\nwithout additional data augmentation or auxiliary objec-\ntives [14]. Yang et al. (2023) proposed a novel unsuper-\nvised skill discovery method through contrastive learning,\nwhich maximizes mutual information between behaviors\nbased on the same skill, leading to the generation of di-\nverse and far-reaching skills [15].\nIn the realm of model-based RL, Ma et al. (2020) in-\ntroduced Contrastive Variational Reinforcement Learning\n(CVRL), which leverages contrastive learning to maxi-\nmize mutual information between latent states and obser-\nvations. This approach has shown robustness in tasks with\ncomplex visual observations, outperforming state-of-the-\nart generative world models [16]. Kim et al. (2022) devel-\noped the Action-Driven Auxiliary Task (ADAT) method,\nwhich focuses on essential features for decision-making\nand significantly enhances feature learning and perfor-\nmance in standard benchmarks [17].\nYuan and Lu (2022) addressed the challenge of distri-\nbution mismatch in offline meta-reinforcement learning\nby proposing a contrastive learning framework that im-\nproves task representation robustness and generalization\nto out-of-distribution behavior policies [18]. Shen et al.\n(2021) introduced Sequential and Dynamic Constraint\nContrastive Reinforcement Learning (SDCRL), incorpo-\nrating sequential information and dynamic transitions to\nimprove sample efficiency and performance on complex\ntasks [19].\nLi et al. (2023) introduced Model-Enhanced Con-\ntrastive Reinforcement Learning (MCRL) for sequential\nrecommendation, addressing data sparsity and overesti-\nmation issues, and significantly outperforming existing\nmethods [20]. Qiu et al. (2022) proposed Contrastive\nUCB, an algorithm integrating contrastive learning with\nUpper Confidence Bound (UCB) methods for improved\nsample efficiency and representation learning in online RL\nsettings [21].\nLastly, Wang and Hu (2023) developed lightweight and\neffective DRL algorithms incorporating contrastive learn-\ning to balance new and old experience trajectories, achiev-\ning state-of-the-art performance in PyBullet robotics en-\nvironments [22]. These studies collectively highlight the\npotential of contrastive learning to advance RL by enhanc-\ning representation quality, sample efficiency, and general-\nization capabilities."}, {"title": "3. METHODOLOGY", "content": "In this work, we propose an enhancement to the Dream-\nerV3 algorithm by integrating the contrastive loss from\nthe CURL algorithm, aiming to improve the encoder's\nability to capture informative and discriminative features\nfrom visual inputs. Curled-Dreamer, modifies the train-\ning procedure of the encoder in DreamerV3 to include a\ncontrastive loss that promotes the similarity of represen-\ntations for positive pairs (different augmentations of the\nsame state) and dissimilarity for negative pairs (augmenta-\ntions of different states). Additionally, we incorporate a\nreconstruction loss in the output of the decoder to further\nenhance the quality of the learned representations.\nN\nThe Curled-Dreamer algorithm operates through the\nN\nfollowing steps. First, we perform data augmentation on\neach state observation by performing random crops. Let\nS = {si}=1 be a set of original states, and A = {ai}=1\nbe the corresponding augmented states. This step aims to\nimprove the robustness of the representations learned by\nthe encoder. Next, the encoder fe is trained using both\nthe objectives of DreamerV3 and a contrastive loss. The\ncontrastive loss is computed using the InfoNCE (Informa-\ntion Noise Contrastive Estimation) loss [23], defined as\nfollows:\n$$L_{InfoNCE} = -E_A \\bigg[log \\frac{exp(sim(f_\\theta(a_i), f_\\theta(a_i'))/\\tau)}{\\sum_{a_j \\in A} exp(sim(f_\\theta(a_i), f_\\theta(a_j))/\\tau)} \\bigg],$$\nwhere $a_i'$ is the positive pair of $a_i$ (i.e., another augmenta-\ntion of the same state), sim(\u00b7, \u00b7) denotes the cosine simi-\nlarity, and \u03c4 is a temperature parameter. This contrastive\nloss encourages the encoder to produce similar represen-\ntations for different augmentations of the same state and\ndissimilar representations for different states.\nThe representations $z_t = f_\\theta(s_t)$ obtained from the en-\ncoder are then used as inputs to the latent dynamics model.\nThe latent dynamics model $g_\\phi$ and reward predictor $r_\\psi$ are\ntrained to predict future states and rewards, respectively.\nThe loss function for training the latent dynamics model\nand the reward predictor is given by:\n$$L_{dynamics} = E \\bigg[\\sum_{t=1}^T(||g_\\phi(z_t, a_t) - z_{t+1}||^2 + ||r_\\psi(z_t, a_t) \u2013 r_t||^2)\\bigg]$$\nwhere $z_t$ is the latent state representation at time t, $a_t$ is\nthe action taken at time t, and $r_t$ is the reward received at\ntime t. This loss ensures that the latent dynamics model\naccurately predicts the future latent states and rewards\nbased on the current latent state and action.\nThe encoder's output is also fed into a decoder $d_\\eta$ to\nreconstruct the original input, promoting the learning of\nmore detailed and accurate representations. The recon-\nstruction loss is defined as:\n$$L_{reconstruction} = E [||d_\\eta(f_\\theta(s_t)) \u2013 s_t||^2].$$\nThe policy \u03c0\u03b8 is optimized using the predictions from\nthe latent dynamics model. The policy optimization fol-\nlows the actor-critic method used in DreamerV3. The\npolicy loss is given by:\n$$L_{policy} = -E_{\\pi_\\theta} \\bigg[\\sum_{t=1}^T \\gamma^t r_\\psi(z_t, a_t) \\bigg],$$\nwhere \u03b3 is the discount factor. This loss function maxi-\nmizes the expected cumulative reward over time, guiding\nthe policy to select actions that lead to higher rewards."}, {"title": "4. EXPERIMENTS", "content": "In this section, we present the experimental setup and\nthe results obtained from evaluating the proposed Curled-\nDreamer algorithm. We describe the environments used,\nthe hyperparameters adopted, and provide an in-depth\nanalysis of the results.\nWe evaluated the performance of Curled-Dreamer on a\nsuite of 20 tasks from the DeepMind Control Suite (DMC)\n[13]. The DMC provides a diverse set of continuous con-\ntrol tasks designed to test various aspects of reinforcement\nlearning algorithms, including balance, locomotion, and\nmanipulation. The selected tasks offers a comprehensive\nevaluation of the algorithm's capability to handle various\ncontrol challenges, including high-dimensional observa-\ntions and continuous action spaces.\nTo ensure a fair comparison, we employed the same\nhyperparameters as used in the original DreamerV3 paper\n4. The key hyperparameters are as follows: the learning\nrate for the encoder, dynamics model, reward predictor,\ndecoder, and policy is set to 3 \u00d7 10-4; a batch size of 50\nis used for training; the discount factor (\u03b3) is set to 0.99;\nthe sequence length for training the recurrent models is\nset to 50; the temperature parameter (\u03c4) for the InfoNCE\nloss is set to 0.1; the weights for the contrastive loss (\u03bb2),\nreconstruction loss (\u03bb3), and dynamics loss (\u03bb1) are all\nset to 1.0. These hyperparameters were chosen based on\nextensive tuning performed in the DreamerV3 paper and\nhave been shown to provide robust performance across a\nwide range of tasks.\nWe evaluated the performance of Curled-Dreamer us-\ning the average return over 1 million environment steps.\nThis metric provides a comprehensive assessment of the\nlearning efficiency and policy performance. For each task,\nwe report the mean and median scores achieved by our\nalgorithm, and we compare these results against several\nstate-of-the-art baseline algorithms, including PPO [24],\nSAC [25], CURL [6], DrQ-v2 [26], and previous versions\nof Dreamer V3 [4]."}, {"title": "5. RESULTS", "content": "Overall, Curled-Dreamer demonstrated substantial im-\nprovements across a variety of tasks, showcasing its versa-\ntility and robustness. The algorithm achieved higher mean\nand median scores across the suite of tasks, indicating a\nsignificant enhancement in learning efficiency and policy\nperformance.\nParticularly outstanding results were observed in\ntasks that involve complex dynamical systems and high-\ndimensional observations. For instance, Curled-Dreamer\nexcelled in the Acrobot Swingup task, achieving a score of\n328, which is nearly double the score of the next best base-\nline. This improvement can be attributed to the enhanced\nrepresentation learning facilitated by the contrastive and\nreconstruction losses, enabling the model to better capture\nthe intricate system dynamics.\nIn manipulation tasks such as Finger Turn Easy and\nFinger Turn Hard, Curled-Dreamer achieved scores of\n767 and 896, respectively. These results surpass those\nof the previous best-performing models, indicating that\nthe enhanced representation learning contributes to better\nmanipulation and control capabilities. The ability to cap-\nture fine-grained details from visual inputs likely played a\ncrucial role in these tasks, where precise movements are\nrequired.\nLocomotion tasks, including Cheetah Run and\nQuadruped Walk, also benefited significantly from the\nproposed enhancements. In the Cheetah Run task, Curled-\nDreamer achieved a score of 867, outperforming the pre-\nvious best score of 836. The improved representation\nlearning allowed the model to better understand and pre-\ndict the agent's movements, resulting in more effective\nand coordinated locomotion. Similarly, in the Quadruped\nWalk task, Curled-Dreamer achieved an impressive score\nof 894, showcasing its ability to generalize across different\ntypes of locomotion challenges.\nMoreover, tasks with sparse rewards, such as Cartpole\nBalance Sparse and Cartpole Swingup Sparse, demon-\nstrated the robustness of Curled-Dreamer in learning stable\npolicies despite limited feedback. The algorithm achieved\nnear-perfect scores in these tasks, indicating its proficiency\nin extracting valuable information from high-dimensional\nobservations and learning effective control strategies even\nin challenging environments.\nThe results also highlight the effectiveness of the com-\nbined learning paradigms in enhancing the model's per-\nformance. The contrastive loss improved the encoder's\nability to capture informative and discriminative features\nfrom visual inputs, while the reconstruction loss promoted\nthe learning of more detailed and accurate representations.\nThese components, along with the robust training proce-\ndure and well-tuned hyperparameters, contributed to the\noverall success of Curled-Dreamer.\nIn conclusion, the experimental results provide strong\nevidence of the effectiveness of Curled-Dreamer in ad-\ndressing various reinforcement learning tasks. The combi-\nnation of DreamerV3's predictive capabilities and the en-\nhanced representation learning from CURL makes Curled-\nDreamer a powerful and versatile algorithm for visual\nreinforcement learning."}, {"title": "6. CONCLUSION", "content": "In this paper, we introduced Curled-Dreamer, an en-\nhanced reinforcement learning algorithm that integrates\ncontrastive learning into the DreamerV3 framework. By\nincorporating the contrastive loss from CURL and a re-\nconstruction loss, Curled-Dreamer improves the quality\nof learned representations, resulting in enhanced perfor-\nmance and robustness in visual reinforcement learning\ntasks. Our experiments on the DeepMind Control Suite\ndemonstrated that Curled-Dreamer consistently outper-\nforms or matches the performance of state-of-the-art algo-\nrithms, highlighting its effectiveness across a diverse set\nof tasks.\nThe results indicate that the enhanced representation\nlearning achieved through contrastive and reconstruction\nlosses is key to Curled-Dreamer's performance. These\ncomponents enable the model to capture more informative\nand discriminative features from high-dimensional visual\ninputs, leading to better policy performance. The robust\ntraining procedure and well-tuned hyperparameters further\ncontribute to the algorithm's success, ensuring stability\nand efficiency in learning.\nFurthermore, the addition of parameters, such as the\nuse of two encoders and the matrix W, likely contributed\nto the improved results by allowing the model to capture\nmore complex features and relationships within the data.\nThis increase in model capacity, combined with effective\ntraining strategies, enhanced Curled-Dreamer's ability to\nlearn and generalize from visual inputs.\nFuture work will explore parallelism and remove the\ndecoder from the model to make the algorithm more ro-\nbust. Additionally, implementing Curled-Dreamer using\nJAX instead of PyTorch could make it faster. Although\nthere is no performance comparison in this paper, our ap-\nproach, even with more extensive learning, is currently\nslower than the compared state-of-the-art methods.\nOverall, Curled-Dreamer showcases the potential of\ncombining different learning paradigms to achieve su-\nperior results in reinforcement learning. Investigating\nthe scalability and generalization capabilities of Curled-\nDreamer in more complex environments could provide\nvaluable insights into its applicability in real-world sce-\nnarios."}]}