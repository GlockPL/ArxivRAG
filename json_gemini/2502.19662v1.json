{"title": "HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration", "authors": ["Rohan Juneja", "Shivam Aggarwal", "Safeen Huda", "Tulika Mitra", "Li-Shiuan Peh"], "abstract": "Quantization is critical for realizing efficient inference of LLMs. Traditional quantization methods are hardware-agnostic, limited to bit-width constraints, and lacking circuit-level insights, such as timing and energy characteristics of Multiply-Accumulate (MAC) units. We introduce HALO, a versatile framework that adapts to various hardware through a Hardware-Aware Post-Training Quantization (PTQ) approach. By leveraging MAC unit properties, HALO minimizes critical-path delays and enables dynamic frequency scaling. Deployed on LLM accelerators like TPUs and GPUs, HALO achieves on average 270% performance gains and 51% energy savings, all with minimal accuracy drop.", "sections": [{"title": "I. INTRODUCTION", "content": "Transformer-based large language models (LLMs) have grown exponentially, increasing 100-fold every two years, far outpacing the 3.1\u00d7 improvements in hardware. This widening gap has made inference increasingly costly, as seen with models like LLaMA (65 billion parameters) and GPT-4 (1.76 trillion parameters), which require vast computational resources. Quantization, which reduces model size and cost by lowering bit-width, is crucial for efficiency. However, implementing quantization is challenging because of the diverse, fragmented landscape of hardware accelerators, each optimized for specific quantization techniques and data types. Existing quantization techniques fail to account for these diverse hardware, highlighting the need for adaptable strategies that can optimize model efficiency across different accelerator architectures by leveraging circuit-level characteristics.\nLimitations of hardware-agnostic techniques. Most existing quantization techniques [1] overly focus on reducing bit-width without considering hardware-specific factors, often treating critical components like Multiply-Accumulate (MAC) units as a black box. In ML accelerators like TPUs, MAC units used for matrix multiplications central to LLM inference, occupy 77-80% of the chip area and account for 50-89% of total power consumption [2]. Our key observation is that MAC units exhibit significant variability in performance and power characteristics, depending on specific weight patterns after quantization. Through detailed circuit analysis, we discover that certain weight configurations enable shorter critical paths, allowing the MAC units to operate at higher frequencies, whereas others result in longer critical paths that constrain the system's overall operating speed. This variability highlights an unexplored opportunity to enhance inference performance and energy efficiency through more sophisticated, hardware-aware weight quantization strategies. Yet, current techniques remain oblivious to this crucial quantization-hardware interplay.\nHALO is the answer. Hardware accelerators for LLMs, such as GPUs, use Dynamic Voltage and Frequency Scaling (DVFS) to balance performance and power. NVIDIA GA100 GPUs, for instance, support up to 181 DVFS configurations [3]. Quantization directly influences the critical-path delays in MAC units, which in turn determines how well DVFS can be utilized to optimize both performance and energy efficiency. Our key insight is that by carefully selecting quantization levels that correspond to favorable critical-path delays, we can enable higher operating frequencies while maintaining model accuracy.\nWe present HALO, Hardware-Aware LOw critical-path delay quantization framework, which seamlessly integrates quantization and DVFS into a unified optimization strategy. HALO takes hardware specifications and design objectives as inputs and outputs Pareto-optimal quantized models with optimized DVFS schedules for deployment on target accelerators. HALO employs three key approaches to achieve this:\n1) Weight Sensitivity Analysis: HALO analyzes weight sensitivity to identify critical weights essential for maintaining model accuracy and those that can withstand aggressive quantization.\n2) Sensitivity-aware Uniform Quantization and DVFS Alignment: For weights essential to model accuracy, HALO selects quantization levels with a broader dynamic range to ensure precision. DVFS frequencies are configured so that high-precision components operate efficiently,"}, {"title": "II. MOTIVATION", "content": "The Multiply-Accumulate (MAC) unit is a fundamental component in AI accelerators, playing a significant role in both power consumption and area utilization. As illustrated in Fig.2(a), each MAC unit features three input ports and two output ports. The unit operates by multiplying the weight w with the activation a to produce the product wa. This product is then added to the third input $y_{n-1}$, resulting in the updated partial sum $y_n$.\nThe timing characteristics of a MAC unit are heavily influenced by the specific weight values, as they affect the worst-case critical-path delays, ultimately constraining the operating frequency. To investigate this, we conduct static timing analysis on 8-bit Booth-Wallace Tree MAC (DW02_MAC) unit from the Synopsys DesignWare library using Synopsys PrimeTime [4]. Fig. 3 illustrates the timing profile for two quantized weights, 64 and -127, with the x-axis representing delay and the y-axis showing the frequency of this delay across all activation transitions. The weight value 64 achieves an operating clock frequency of 3.7 GHz, while -127 is limited to 1.9 GHz. This variable delays stem from Booth encoding, which processes bits of the multiplier in pairs (or triplets in modified Booth encoding). Certain bit patterns reduce the number of active signal paths, shortening critical-paths and resulting in faster processing for specific weight values.\nFig.4 shows the achievable operating frequency, based on the maximum delay for each weight value across activation transitions, while Fig.5 depicts the corresponding power consumption for an 8-bit integer MAC unit. The active power consumption is determined by switching activity, which fluctuates with different weight values. Notably, weights associated with shorter critical-path delays tend to exhibit lower power consumption, offering potential energy savings. This observed correlation between timing and power characteristics reveals opportunities for optimizing both frequency and energy by strategically selecting weight values for model inference."}, {"title": "III. QUANTIZATION FRAMEWORK", "content": "Our quantization framework for LLM inference introduces a timing-aware strategy, detailed in Algorithm 1, which prioritizes weights with low critical-path delays to minimize latency while preserving model fidelity. The adaptive method operates across levels and layer sensitivity, optimizing performance by focusing on weights most critical to efficiency. The framework comprises three key components: \u2460 sensitivity-aware uniform quantization to identify and preserve critical weights (Lines 1-3), \u2461 critical-path delay aware non-uniform quantization to optimize weight patterns for hardware efficiency (Lines 4-10), and 3 adaptive DVFS to maximize performance by matching quantization levels with optimal operating frequencies.\nA. Sensitivity-aware Uniform Quantization\nThe framework begins with a sensitivity analysis of model weights, identifying weights values that can tolerate quantization without significantly affecting accuracy, as outlined in Algorithm 1. The framework initially separates outliers (outside the blue lines) and salient weights (in red) from normal values, as shown in Fig.6.\nOutliers & Salient Weights: We incorporate outlier removal to manage extreme weight values based on inter-quartile range scaling. To compute outliers in the weight distribution, we employ the 3\u03c3 rule [5]. Outliers are identified as values lying beyond three standard deviations from the mean.\nFrom the normal values obtained after this distribution, we rely on Taylor series expansion to estimate the most salient weights in the model. Following [6], we use an approximation to the Hessian H based on the Fisher information matrix F, which can be calculated over a sample dataset D as\n$F = \\frac{1}{|D|} \\sum_{d \\in D} g_d g_d^T$, (1)\nwhere g is the gradient and H \u2248 F. This only requires computing the gradient for a set of samples. For each weight tensor W, the weight sensitivity is computed as $A_w$ = F. Weights with higher $A_w$ values are considered more salient due to their significant impact on the model's output. We preserve the top 0.05% of the weights based on this criterion. Cumulatively, both outliers and extremely salient weight values correspond to less than 0.5% of the total weight values. For this reason, we handle these weight values separately and apply per-channel quantization for this set of weight values, isolating them to maintain model precision.\nB. Critical-path delay aware Non-Uniform Quantization\nUniform Quantization discretizes continuous values into $2^b$ evenly spaced levels. On the other hand, non-uniform quantization adapts to the data distribution using variable interval sizes defined by thresholds T, which partition the input range into regions $R_k = [t_{k-1}, t_k)$. Each region $R_k$ is assigned a representation level $y_k$, where $y_k$ is the quantized value corresponding to data points within $R_k$. In this work, we leverage non-uniform quantization to more efficiently map the distribution of weights to specific values that reduce the critical-path delays (as discussed in Sec.II), thereby optimizing frequency and energy consumption.\nTile-Based Sensitivity Analysis: To optimize the model for efficient inference on hardware, the weight tensors are divided into fixed-size tiles (128 \u00d7 128 by default). Specifically, the sensitivity of each tile is evaluated as the sum of the absolute values of the gradients for each tile, normalized by the size of the tile, based on Eq.1. For a given kth tile $T_k$, we compute a per-tile sensitivity score $A_T$ using a diagonal approximation of the Fisher information matrix:\n$A_T = \\frac{\\sum_{i, j} g_{k, i, j}^2}{tile_rows \\times tile_cols}$, (2)"}, {"title": "C. Adaptive DVFS Strategy", "content": "where $g_{k, i, j}$ denotes the gradient of the loss with respect to each weight in the k-th tile, and tile_rows \u00d7 tile_cols represents the total number of elements within the tile. This score captures the average Fisher information across all weights in the tile, providing a quantitative measure of the tile's sensitivity in relation to its influence on the model's output.\nTile Sensitivity Mapping: To balance hardware efficiency and model accuracy, tiles in each layer are classified as low-sensitive or high-sensitive based on their relative importance. Determining a fixed sensitivity threshold for each layer is challenging, as weight distributions vary significantly across layers. To address this, we employ a dynamic tile sensitivity mapping strategy that adapts to the cumulative sensitivity distribution of each layer.\nThe process starts by computing the sensitivity of all tiles in a given layer, derived as the normalized sum of absolute gradient magnitudes within each tile. Sensitivities are then sorted in descending order to rank tiles by importance. A cumulative sum of these sorted sensitivities is calculated and normalized against the total layer sensitivity, generating a cumulative distribution curve from 0 to 1.\nThe mapping threshold k is derived from this curve and represents the fraction of tiles classified as low-sensitive, ensuring a specified percentage of total sensitivity (e.g., 95%) is retained. Tiles contributing most to overall sensitivity are marked high-sensitive, while the rest are classified as low-sensitive. Mathematically, k is the ratio of the index where cumulative sensitivity exceeds the threshold to the total number of tiles, defaulting to 1.0 if no such index exists.\nOnce k is determined, boolean masks separate tiles into low- and high-sensitivity categories. Low-sensitivity tiles are quantized more aggressively, while high-sensitivity tiles retain higher precision to preserve performance. The adaptive quantization and computation flow based on the DVFS characteristics are described in detail in Sec.III-C.\n1) DVFS for Outliers and Salient features\nPackaging of Salient and Outlier Weights: Salient and outlier weights, exhibiting extreme sparsity, are packaged for efficient computation using a Sparse Matrix-Vector Multiplication (SpMV) engine. The hypersparse weight matrix is compactly stored with a value vector val for non-zero elements and an index vector idx for column positions, reducing memory usage and accelerating computations. The matrix-vector multiplication A \u00d7 b for a dense vector $b\u2208 R^k$ is performed as:\nres[i] = val[i] \u00d7 b[idx[i]], for i = 0, 1, . . ., m \u2013 1,\nwhere res is the result vector, efficiently leveraging the matrix's sparsity.\nDVFS Configuration Selection: These uniformly quantized hypersparse weights span the entire 8-bit range, necessitating DVFS settings that respect the critical-path delay for all possible weight values. Guided by MAC characteristic trends, the optimal voltage-frequency (V, f) point is selected to minimize energy consumption while ensuring timing constraints are met:\n(V, f) = arg min [E(Vi, fi)] given(1/fi) \u2265 Critical Path\n(Vi,fi)\nThis approach ensures efficient system performance without sacrificing model fidelity.\n2) DVFS for High- and Low-Sensitivity Weights\nFor non-uniformly quantized weights on the systolic array, weight tensors are divided into 128 \u00d7 128 tiles, with DVFS settings determined by the tile sensitivity mapping strategy as outlined in Sec.III-B. This tile size aligns with the architecture of modern systolic arrays in TPUs and is critical for balancing the tradeoff between accuracy and performance, optimizing energy efficiency while preserving precision. The assumed DVFS levels for both GPUs and systolic arrays are summarized in Table I. We base our DVFS levels on practical hardware parameters: for GPUs, using NVIDIA's publicly available maximum clock frequency of 2.8 GHz [7], and for systolic arrays (similar to those present in TPUs), deriving levels from MAC characteristics (see Sec.II).\nHigh-Sensitivity Tiles: These tiles are composed of critical weights that are crucial for maintaining model accuracy. As discussed in Sec.II, the DW02_MAC unit handles 16 high-sensitivity weights, operating at a frequency of 2.4 GHz. These tiles are exclusively made up of these 16 values to ensure precision. The execution is performed at specific DVFS settings (Vi, fi), where 1/fi \u2265 the critical delay associated with these weights. While this configuration may lead to higher energy consumption, it significantly boosts tile performance by overclocking the accelerator's global clock to meet the precise timing requirements of these weights.\nLow-Sensitivity Tiles: These tiles are subjected to aggressive overclocking to maximize performance. As detailed in Sec.II, they contain only 9 weights, each capable of operating at frequencies up to 3.7 GHz. The DVFS settings are fine-tuned to respect the critical-path delay only for these 9 weights. Despite the high overclocking, the energy increase remains incremental, as switching occurs primarily along the shortest paths in the circuit.\nBy leveraging MAC characteristics, this approach achieves an efficient trade-off between processing speed and energy consumption, optimizing the use of the accelerator's resources."}, {"title": "IV. EVALUATION", "content": "This section evaluates the accuracy of LLM models with HALO quantization and demonstrates its speedup and energy efficiency compared to systolic arrays and GPUs.\nA. Implementation Details\nDatasets and Models. We evaluate the effectiveness of HALO using various models, including LLaMA2 [8] and OPT [9] family of models. We conduct language modeling evaluation using the C4 [10] and WikiText2 [11] datasets. We report perplexity as the measure of the performance of the model.\nHyperparameters & Baselines. We evaluate our approach with state-of-the-art quantization methods, including SmoothQuant [12] and Round-To-Nearest (RTN) quantization, under varying weight precision with activations fixed at 8 bits. Quantization is applied to computationally intensive operators such as attention and linear layers, using per-channel quantization for weights and per-token static quantization for activations. For SmoothQuant, original smoothing factors for activation scaling are used. Additionally, we retain 0.45% of values, comprising 0.05% sensitivity-critical values and 0.4% outliers, with sensitivity measured using 100 random samples from the C4 training set.\nHardware Setup. To evaluate performance of the systolic array, we develop a custom simulator and implement the design, including a global DVFS unit, in SystemVerilog. The design is synthesized in a 22nm commercial technology using topographical mode with parasitics for routing overhead and verified through behavioral simulations. For GPU evaluations, we extend AccelSim [13] to model the NVIDIA 2080 Ti architecture using the DVFS configurations from Table I. Energy estimations are performed using AccelWattch [14] and GPUWattch [15].\nB. Accuracy Results\nOur evaluation benchmarks HALO against 16-bit floating-point (FP16) precision, with both weights and activations in FP16, and integer quantization schemes, denoted as WxA8. Here, WxA8 represents 8-bit integer activations and x-bit integer weights, where x is 8, 4, or 3. We compare our method to Round to Nearest (RTN), which often loses accuracy at lower bit widths, and SmoothQuant, which redistributes quantization difficulty to reduce errors. As shown in Table II, HALO consistently achieves accuracy comparable to FP16, with perplexity increases under 0.5 across models. We specifically include W3A8 and W4A8 baselines because they align with our quantization levels. The MAC characteristics in Sec.II indicate that aggressive quantization of low- and high-sensitivity tiles groups weights into buckets of 9 and 16, making these baselines relevant for evaluating HALO's approach. We introduce three HALO variants: perf-opt for performance, acc-opt for preserving accuracy close to FP16 level, and bal for a balanced trade-off. Results across various systolic array tile sizes in Sec.IV-D highlight HALO's balance of efficiency and model fidelity.\nC. Systolic Array Performance and Energy\nFig.7 shows that HALO delivers significant latency speedup across all models, outperforming traditional methods like FP16 and W8A8, which struggle with outliers. HALO achieves 353% improvement over FP16, 87% over W8A8, 76% over W4A8, and 74% over W3A8, while maintaining better accuracy than W4A8 and W3A8. Fig.8 highlights the trade-off between normalized performance and perplexity, with the knee point marking the optimal balance. The bal variant aligns with this point, making it ideal for performance-critical LLM deployments.\nFig.9 shows energy consumption split into static (or idle energy consumption) and dynamic energy from switching in core, buffer, and memories. FP16 incurs the highest energy usage due to uniform 16-bit operations, while W8A8 sees moderate reductions but lacks flexibility in adapting to weight sensitivity. W4A8 achieves better energy efficiency but rises slightly with model size, and W3A8 has the lowest energy consumption, though with minor fluctuations. HALO variants employ dynamic frequency scaling, optimizing energy efficiency based on weight sensitivity. This results in energy consumption only 12% higher than W3A8 and 10% higher than W4A8, with the bal variant providing a superior performance-to-efficiency trade-off.\nD. Impact of Tile Size on System Efficiency\nFig.10 illustrates the performance of HALO with varying Systolic array tile sizes, presented as HALO-128, HALO-64, and HALO-32. We evaluate tile sizes of 128x128, 64\u00d764, and 32 \u00d7 32, finding that smaller tiles, such as 32 \u00d7 32, achieve on average 15% higher performance than 128\u00d7128 and 7% higher than 64\u00d764. This is because the framework quantizes more tiles into higher frequency ranges, enhancing efficiency. As shown in Table II, notably larger models exhibit better perplexity with smaller tile sizes, highlighting the importance of selecting optimal tile sizes for effective tile-wise quantization.\nE. GPU Performance and Energy\nAs shown in Fig.11, HALO on GPU outperforms W8A8 by effectively managing weight sensitivity, achieving consistent speedups across models through dynamic frequency adjustments that maximize performance. Fig.12 shows the normalized energy comparison, considering constant, static, and dynamic power. Constant power refers to the energy consumed by the GPU's peripheral circuitry, while dynamic power includes contributions from DRAM, caches, register file, and processing elements (CUDA and tensor cores). Although W8A8 achieves the lowest normalized energy consumption, HALO(acc-opt) and HALO(bal) offer a more balanced energy profile. Meanwhile, HALO(perf-opt) delivers higher performance at a modest energy cost. These results underscore HALO's ability to balance energy efficiency and performance based on specific optimization objectives."}, {"title": "V. RELATED WORKS", "content": "Quantization Accelerators. Several approaches aim to handle quantization in neural networks, particularly for LLMs. GOBO [16] and OlAccel [17] use mixed precision to maintain accuracy, while Bitfusion [18] dynamically adapts precision levels to model needs. However, many focus on traditional DNNs, which are easier to quantize than LLMs. Methods like GOBO rely on full-precision compute units, while OlAccel uses coordinate lists for outliers. HALO introduces a framework that is orthogonal to existing strategies and integrates seamlessly with accelerators. When applied to mixed-precision accelerators equipped with encoder/decoder components, this not only boosts performance but also reduces DRAM accesses by 59.06%, thereby enhancing overall system efficiency.\nQuantization Methods. Post-training quantization (PTQ) methods like GPTQ [19] and AWQ [20] focus on reducing LLM weights to 3 or 4 bits while preserving critical weight precision. Techniques such as outlier-aware quantization [21] and SqueezeLLM [6] distinguish between sensitive and non-sensitive weights. Recent methods like SmoothQuant [12] shift quantization challenges to weights, allowing both weights and activations to be compressed to 8 bits. HALO builds on these techniques to maintain LLM accuracy by leveraging underlying hardware DVFS."}, {"title": "VI. CONCLUSION", "content": "We present HALO, a framework that enhances LLM inference efficiency through timing-aware, weight-sensitive quantization. By strategically selecting and quantizing weights, HALO minimizes power consumption and critical path delays, leveraging circuit-level insights. This approach enables efficient dynamic frequency and voltage scaling while remaining compatible with existing hardware, offering a flexible and power-efficient solution for scalable LLM deployments."}]}