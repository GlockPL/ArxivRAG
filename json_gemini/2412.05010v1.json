{"title": "BACKDOORING OUTLIER DETECTION METHODS: A NOVEL ATTACK APPROACH", "authors": ["ZeinabSadat Taghavi", "Hossein Mirzaei"], "abstract": "There have been several efforts in backdoor attacks, but these have primarily focused on the closed-set\nperformance of classifiers (i.e., classification). This has left a gap in addressing the threat to classifiers'\nopen-set performance, referred to as outlier detection in the literature. Reliable outlier detection\nis crucial for deploying classifiers in critical real-world applications such as autonomous driving\nand medical image analysis. First, we show that existing backdoor attacks fall short in affecting the\nopen-set performance of classifiers, as they have been specifically designed to confuse intra-closed-set\ndecision boundaries. In contrast, an effective backdoor attack for outlier detection needs to confuse\nthe decision boundary between the closed and open sets. Motivated by this, in this study, we propose\nBATOD, a novel Backdoor Attack targeting the Outlier Detection task. Specifically, we design two\ncategories of triggers to shift inlier samples to outliers and vice versa. We evaluate BATOD using\nvarious real-world datasets and demonstrate its superior ability to degrade the open-set performance\nof classifiers compared to previous attacks, both before and after applying defenses.", "sections": [{"title": "1 Introduction", "content": "Deep Neural Networks (DNNs) have demonstrated remarkable success in closed-set tasks such as classification, where\nthe model is tasked with recognizing a predefined set of categories that remain consistent during both the training and\ntesting phases. However, the next challenge for DNNs is enhancing their performance in open-set tasks, commonly\nreferred to as outlier detection. In outlier detection, the objective changes to differentiating between the closed set\n(or the inliers) and the open set (or the outliers), requiring the model to determine whether an input image belongs to\nthe closed set. The conventional baseline for outlier detection involves a model trained using cross-entropy loss on\nclosed-set classes. At test time, a confidence scoring method like the maximum value of the softmax probability (MSP)\nvector is utilized to ascertain whether an input belongs to one of the known classes. The intuition behind MSP, as well\nas other post-hoc scoring methods, is that classifiers behave relatively more certainly [1, 2, 3, 4, 5, 6, 7].\nDespite significant advancements in DNN-based classifiers, their trustworthiness faces challenges due to emerging\nthreats, notably backdoor attacks [8]. In such attacks, an adversary could introduce poisoned samples into the training\ndataset by embedding a special trigger on incorrectly labelled images. This results in a backdoored model that performs\nnormally on clean data but consistently yields incorrect predictions on poisoned samples. To combat these threats,\nvarious strategies for attack and defense have been proposed [9, 10, 11, 12].\nStudies on backdoor attacks have primarily focused on the closed-set performance of a classifier [9, 10, 13, 14], where\nthe adversary's objective is to impair performance by increasing confusion rates within known classes. Moreover,\nour experimental results suggest that adapting existing backdoor strategies to the task of open-set classifiers does not\nsignificantly impact performance. This ineffectiveness stems from the nature of previous backdoor attacks, which are"}, {"title": "2 Preliminaries", "content": "Classifier for OOD Detection In the context of OOD detection, classifiers denoted as $f$ and trained on dataset $D$ are\ncrucial, as they utilize their feature extraction capabilities to identify discrepancies in data distribution. We introduce an\nauxiliary dataset, referred to as $D'$, consisting of samples that are semantically distinct from those in $D$ (the inliers or\nID dataset). For instance, a model trained on the Cifar10 dataset might be used to recognize data from Cifar100 as\nOOD. This identification process typically involves examining the classifier's output for various classes corresponding\nto a given input, to assess the level of confidence in the predictions. Generally, classifiers show greater confidence for\ninliers compared to outliers, a phenomenon quantitatively measured using the Maximum Softmax Probability (MSP)\ntechnique.\nAdversarial Manipulations on OOD Detection Building upon insights from Vulnerability of Classifiers to Adversarial\nManipulations (E), this Section delves into the formulation of adversarial attacks designed to incorrectly classify inliers\nas outliers and vice versa. The objective is specifically to reduce the (MSP) score of inliers, leading the classifier $f$ to\nmisidentify these inliers as OOD. Typically, the logits for an OOD sample $x_{OOD}$ exhibit a uniform distribution across\nclasses, denoted as $U_K$, with an initial $MSP(f(x_{OOD}))$ approximately equal to $\\frac{1}{K}$.\nProblem Definition. Let $F$ denote a classifier equipped with concurrent capability to outlier instances using post-\nprocessing techniques like MSP [4] and ODIN[1]). This classifier is trained on a dataset $D = \\{(x_i, y_i)\\}$, where $x$ and\n$y$ represent input features and their corresponding labels, respectively. The dataset $D$ consists of inliers following\nthe distribution $P_{IN}$, representing the training data distribution. To manipulate the output of $F$ and mislead it into\nclassifying outliers as ID, an attacker injects triggers into a small fraction (equal to the poisoning rate) of $D$ during\ntraining.\nAt test time, when presented with inputs $X'$ not conforming to $P_{IN}$ (i.e., outliers), the attacker $A$ aims to manipulate\n$X'$ by adding the trigger such $A$ to obtain $X'$ to alter the output of $F$. Consequently, the objective is for $F(X')$, the\noutput of $F$, to mistakenly label the outlier sample $X'$ as belonging to one of the inlier class, despite $X'$ being drawn\nfrom a distribution different from $P_{IN}$."}, {"title": "3 Method", "content": "Overview. Existing State-of-the-Art (SOTA) backdoor attacks, predominantly designed for classification tasks, fail in\nopen-set tasks. To bridge this gap, we introduce BATOD. We utilized hard augmentations to synthesize outliers from\ninliers as proxies for real outliers (3.1). Then, to effectively generate triggers, a surrogate model trained on a subset of\ninlier data minimizes reliance on the original train-set and facilitates the development of two novel trigger types for the\noutlier detection scenario: In-Triggers, which disguise outliers as inliers, and Out-Triggers, which make inliers appear\nas outliers (3.2, 3.3). These triggers can ruin the outlier detection performance of the model during inference time (3.4).\nIn the subsequent Sections, we will detail each component, outlining our approach's mechanisms and advantages."}, {"title": "3.1 Synthesis of Out-of-Distribution Samples", "content": "One of the primary challenges in outlier detection arises from the absence of outlier data. Drawing on insights from\nrecent advancements in self-supervised learning, we categorize common data transformations into two distinct groups.\nThe first group termed positive transformations $T^+$, preserves the distribution. Conversely, the second group, termed\nnegative transformations $T^-$, generates samples with distributions that deviate from the original dataset, such as rotation\nand blur.\nInspired by this distribution deviation, these negative transformations are instrumental in generating artificial examples\nthat represent outliers. Incorporating such examples into training enhances the model's ability to recognize data that\ndeviates from the typical distribution. Hence, we define a set of negative transformations $T^- = \\{ \\tau_1, \\dots, \\tau_k \\}_{i=1}$, with each\n$\\tau_i^-$ representing a distinctive type of negative transformation.\nFrom the training dataset $D$, we assume access only to a portion named $D_a$, which can be used for making triggers;\nthe rest, named $D_u$, is unavailable. For every sample pair $(x_i, y_i) \\in D_a$ ($1 \\leq i \\leq |D_a|, 1 \\leq y_i \\leq K, K =$\nnumber of classes), we generate a corresponding sample by applying a series of negative transformations. For each\nstep, we randomly sample a set of transformations $\\tau_1^-, ..., \\tau_k^- \\sim T^-$. Then, we apply $G(.) = \\tau_1^-(...(\\tau_k^-(.)))$ on $x_i$.\nWe choose $k = 2$ to ensure the negative transformations effectively deviate the sample from the inliers. Thus, each\n$(x_i, y_i) \\in D_a$ leads to $(G(x_i), y_i) \\in D'_a$. This newly created dataset, using negative transformations, is utilized in\nsubsequent steps."}, {"title": "3.2 Trigger Generation and Poisoning train-set Overview:", "content": "In this study, we develop and employ a surrogate model, denoted as $f_s$, which is trained on $D_a$ with cross-entropy loss.\nThis model is used in generating triggers. These triggers will then be used to make poisoned samples for training the\nvictim model $f_v$. We detail the process of generating these triggers using $f_s$ and their subsequent integration into the\ntraining dataset as follows:\nIn-triggers: These triggers are designed to make outliers appear as inliers. Hence, for each class of inlier, we create a\ndistinct trigger, denoted as follows:\n$\\Delta^{In}_j$ for $1 \\leq j \\leq K$,\nGiven $f_s$, a trigger is generated through an $l_\\infty$-constrained PGD (Projected Gradient Descent) attack over 10 steps.\nThis process manipulates $f_s$ to classify all poisoned samples:\n$x'_i + \\Delta^{In}_{y_i}$ for $1 \\leq i \\leq |D'_a|$,\nwhere $(x'_i, y_i)$ are elements of $D'_a$. The classification condition is:\n$f_s(x'_i + \\Delta^{In}_{y_i}) = y_i$.\nThis process repeats for each class to determine the corresponding trigger for all $K$ classes. Finally, we define an empty\nlist to then become a poisoned dataset $D^{In}_p$. We randomly select half of the samples in each class in $D'_a$ and poison\nthem by injecting the corresponding trigger. In fact, each of the selected samples, $(x'_i, y_i) \\in D'_a$, is transformed into\n$(x'_i + \\Delta^{In}_{y_i}, y_i)$ and then added to the poisoned dataset $D^{In}_p$.\nOut-triggers: These triggers are designed to make inliers be detected as outliers while keeping the correct class label.\nWe aim to decrease outlier detection performance while preserving classification performance. To achieve this goal, we\ngenerate triggers that cause the logits of outputs in a well-trained model to become uniformly distributed. To observe\nthis effect, we need $K$ triggers as follows:\n$\\Delta^{Out}_j$ for $1 \\leq j \\leq K$,\nwhere $K$ is the number of classes in $D$. With the surrogate model $f$, and $X = \\{ x' | (x'_i, y_i) \\in D'_a, y_i = j \\}$ (all samples\nbelonging to class $j$ in $D'_a$), we can adjust the Maximum Softmax Probability (MSP) of inliers subtly without altering\ntheir predicted labels. This manipulation is achieved using the DeepFool attack until MSP is slightly above $\\frac{1}{2}$ or reaches\nthe maximum allowable perturbation $\\epsilon$, set to $\\frac{\\epsilon}{255}$ to ensure stealthiness. $\\frac{1}{2}$ is used as a threshold to estimate the decision\nboundary. Moreover, to assess the stealthiness of the backdoor triggers (you can see the comparison of our attack\ntriggers with other attacks in Figure 1), we employ several metrics: the peak signal-to-noise ratio (PSNR), structural\nsimilarity index (SSIM), and learned perceptual image patch similarity (LPIPS). These metrics are used to quantify the\ndiscrepancies between clean and poisoned images, ensuring the subtle nature of the modifications.\nThe generated trigger for class $y_i$ causes $f_s$ to classify $x_i + \\Delta^{Out}_{y_i}$ as belonging to class $y_i$ but with lower certainty.\nAdding this trigger makes the logits of $f_s$ nearly uniform for $x_i + \\Delta^{Out}_{y_i}$. We iterate this process $K$ times to generate\nthe corresponding triggers for all classes in $D_a$.\nNext, we define an empty list $D^{Out}_p$ as a poisoned dataset for Out-triggers. Directly adding these triggers to a portion of\ninlier samples and then including them in $D^{Out}_p$ causes the victim model $f_v$ to incorrectly learn the triggers and increase\nthe MSP of the poisoned samples during the training process. Consequently, this approach makes the poisoned samples\nbe detected as inlier samples, contrary to our goal of them being detected as outliers. To address this issue, we add the\n$\\Delta^{Out}_{y_i}$ trigger to a selected sample $(x_i, y_i)$ $K$ times, and each time, the label is one of the $K$ labels of $D$. Here, we reach\nthe following list of poisoned samples for any selected sample $(x_i, y_i) \\in D_a$:\n$[(x_i + \\Delta^{Out}_{y_i}, 1), ..., (x_i + \\Delta^{Out}_{y_i}, K),]$\nwhere $K$ is the number of classes in $D$. Hence, to maintain the balance of the poisoned dataset, we randomly select $\\frac{1}{2K}$\nof the samples in each class of $D_a$ and add the corresponding list for each poisoned sample into $D^{Out}_p$. This can cause\nlimitations for more information see the Appendix H.\nNow, we have two poisoned datasets, $D^{In}_p$ and $D^{Out}_p$, each comprising $\\frac{1}{2}$ of dataset $D_a$. Finally, we train the victim\nmodel $f_v$ on the union of these poisoned datasets and the unavailable data $(D_{train} = D_u \\cup D^{In}_p \\cup D^{Out}_p)$, utilizing\ncross-entropy loss."}, {"title": "3.3 Creating the Discriminator", "content": "We train a binary classifier to distinguish between inliers and outliers. The binary classifier, denoted as $H$, is trained\nusing datasets $D_a$ and $D'$, where $D_a$ is considered the inlier class and $D'$ as the outlier class. The binary classifier $H$\nis thus optimized to distinguish between these classes."}, {"title": "3.4 Inference Time Application of Triggers:", "content": "Once the victim model $f_v$ is trained as described, during inference time, based on the input type determined using the\nbinary classifier $H$, the appropriate trigger is selected.\nOutliers $X_{out}$: For each outlier sample $x_{Out} \\in X_{Out}$, we need to choose the appropriate target class $p$ to add the\ncorresponding trigger from $\\Delta^{In}_p$. We select the class with the highest MSP using $f_s$ as the target class. The poisoned\nsample will then be: $x_{Out} + \\Delta^{In}_p$. Adding this trigger misclassifies the sample into the nearest or most susceptible\nin-distribution (ID) class.\nInliers $X_{ID}$: For inlier samples, we also take the class with the highest MSP using $f_s$ as the target class $(y_i)$, and hence,\nthe trigger $\\Delta^{Out}_{y_i}$ is added to the sample. Adding this trigger does not change the $f_v$ logit related to MSP which is the\nchosen class, but alters the logits to be more uniform, causing the sample to be detected as an outlier."}, {"title": "4 Experimental Evaluation", "content": "Experimental Setup and Datasets. We utilized a diverse array of datasets to evaluate the performance of outlier\ndetection in different scenarios, specifically focusing on open-set recognition (OSR) and out-of-distribution (OOD)\ndetection. For the OSR task, we employed the Cityscapes [26], ADNI [27], and PubFig [28] datasets. The Cityscapes\ndataset was used to differentiate between common urban elements and uncommon objects in urban scenes. The ADNI\nMRI dataset helped us identify signs of Alzheimer's disease against normal cognitive aging. Meanwhile, the PubFig"}, {"title": "5 Ablation", "content": "In this section, we initially demonstrate the significance of each component of our method, followed by the application\nof additional post-hoc scoring functions to further validate the robustness of BATOD (extra ablation is in Appendix C)\nMethod Components. In our comprehensive analysis of a proposed backdoor attack strategy, we explore the differential\nimpact of two uniquely designed trigger types: in-triggers and out-triggers, each tailored to subtly manipulate classifier\nbehavior under normal and anomalous conditions, respectively. An ablation study, detailed in Table 5, assesses the\neffectiveness of these triggers individually and in combination across various defense scenarios-ranging from no\ndefense to sophisticated backdoor defenses and adversarial training reflecting Trojai competition rounds 3 and 4\nconditions. This evaluation, conducted on the diverse Cityscapes dataset, aims to establish baseline performance and test\nthe robustness of the model under real-world conditions. The findings are pivotal in identifying defense vulnerabilities\nand refining trigger mechanisms to optimize attack efficacy and subtlety.\nPost-hoc scoring methods. To assess the effectiveness of our backdoor attack method across diverse scenarios,\nwe utilized a variety of out-of-distribution (OOD) scoring functions, each offering unique insights into how well\nthe method performs under different detection strategies. MSP (Maximum Softmax Probability) utilizes the highest\nsoftmax output of a neural network as the score to distinguish between in-distribution and OOD samples. ODIN\n(Out-of-distribution detector for Neural networks) enhances OOD detection by applying temperature scaling and input\nperturbation to the softmax scores. KNN (K-nearest neighbours) detects OOD samples by measuring the distance to the\nK nearest neighbours in the feature space, with farther distances indicating OOD. GMM (Gaussian Mixture Model)\nuses the likelihood of a sample under a fitted Gaussian mixture model to determine if it's OOD, with lower likelihoods\nsignalling out-of-distribution instances. Each method contributes a unique perspective to OOD detection, allowing for a"}, {"title": "6 Conclusion", "content": "Our work presents a novel backdoor attack methodology that effectively bypasses contemporary out-of-distribution\n(OOD) detection techniques and remains resilient against various backdoor defenses. By meticulously crafting triggers\nand employing sophisticated synthesis of out-of-distribution samples, our approach demonstrates its robustness across\ndifferent datasets, model architectures, and defense mechanisms. Our experimental results underline the critical need\nfor advancing current defense strategies to counteract such advanced backdoor threats. This research not only sheds\nlight on the evolving landscape of backdoor attacks but also prompts a re-evaluation of defense mechanisms in the AI\nsecurity domain. Future work should focus on developing more sophisticated detection and defense mechanisms that\ncan adapt to the ever-evolving techniques of backdoor attacks, ensuring the integrity and reliability of machine learning\nmodels in real-world applications."}, {"title": "A Detailed Results", "content": "In this Section, in Tables 7, we provide the mean and standard deviation of our method's results on the provided datasets\nin Table 1 using 10 different seeds just on Cityscapes dataset. These were not reported in the main table due to space\nconstraints."}, {"title": "B Datasets", "content": "The Cityscapes dataset, meticulously assembled for facilitating advancements in semantic urban scene understanding,\ncomprises a substantial corpus of over 5,000 finely annotated images and 20,000 coarsely annotated images, captured\nacross 50 diverse cities. This dataset, designed with a focus on instance-level semantic labelling and rich annotations,\nincludes varied annotations captured under controlled weather conditions using a sophisticated stereo camera setup.\nSuch comprehensive data, including pixel-level and instance-level annotations for dynamic and static elements like\nvehicles and pedestrians, proves invaluable in developing advanced vision models tailored for complex urban scenarios,\nparticularly autonomous driving.\nIn our research, we have repurposed the Cityscapes dataset for an outlier detection task, inspired by methodologies\nsuch as those employed by OpenGAN. You can see two sample images of this setting in Figure 2. By designating less\nfrequent semantic classes, categorized under \"other,\" as outliers, and more frequent urban elements as inliers, we explore\nthe capability of machine learning models to distinguish between typical urban components and anomalous, ambiguous\nobjects. This setup leverages the rich annotations and diverse imagery of Cityscapes to evaluate and enhance the\nprecision with which models recognize and react to atypical features in urban settings. Such an approach is crucial not\nonly for validating the effectiveness of outlier detection Algorithms but also for reinforcing their practical applicability\nin scenarios critical to advancing autonomous driving technologies, where accurately identifying both common and\nuncommon elements is vital."}, {"title": "B.2 ADNI Neuroimaging", "content": "The study focuses on differentiating between early signs of Alzheimer's disease and normal cognitive ageing by\nclassifying the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset into six distinct classes based on cognitive\nstatus and disease progression. The classes CN (Cognitively Normal) and SMC (Subjective Memory Concerns) are\nconsidered inliers, representing individuals without any significant neurodegenerative disease or with only minor"}, {"title": "B.3 Pubfig", "content": "The PubFig dataset, or Public figures Face Database, is a comprehensive resource containing over 58,000 images\nof 200 celebrities, crafted to enhance research in facial recognition technologies. Each public figure is represented\nthrough a diverse array of images that capture a wide range of poses, lighting conditions, and expressions, designed to\ntest the robustness of facial recognition systems under realistic scenarios. In our study, we have tailored this dataset\nto a specific experimental design by selecting a subset of 50 celebrities. From this subset, we randomly designated\n25 celebrities as inliers and the remaining 25 as outliers. This methodology enables us to scrutinize the ability of\nfacial recognition systems to distinguish between known (inlier) and less familiar or potentially deceptive (outlier)\nfaces, thereby evaluating the resilience of these systems against various adversarial attacks and contributing to the\nadvancement of security measures in biometric technologies."}, {"title": "B.4 Out-of-distribution detection (CIFAR10 and CIFAR100)", "content": "Our method is effective across various computer vision tasks, models, and datasets, demonstrating its broad applicability.\nTo illustrate this, we conducted tests using the CIFAR-10 and CIFAR-100 datasets as in-distribution samples, and\nLSUN, BIRDS, FOOD, Flowers, and one of either CIFAR-10 or CIFAR-100 (which is not considered in-distribution)\nas OOD samples. To avoid overlap, we removed classes from the OOD datasets that were present in the in-distribution\ndatasets."}, {"title": "C Extra Ablations", "content": "In this section, we examine our backdoor attack method by applying it to various models, including different model\narchitectures, like Residual Neural Networks [29], Wide Residual Networks [30] and Vision Transformers [31], in\naddition, we have tested pre-trained models as well. You can see the outlier detection performance of our method on\ndifferent models in Figure 8.\nComparison with More Attacks. In 1 you can see triggers for different backdoor attack methods.\nBase Implementation. We adapted implementation of [32] for our task."}, {"title": "D High-frequency constraint for ensuring stealthiness in frequency domain", "content": "Incorporating the insights derived from our discussions on backdoor attack methodologies and the innovative aspects\nof our trigger generator, the BATOD method significantly enhances resistance against frequency-based defenses. Our\napproach leverages a sophisticated two-pronged technique to enhance the effectiveness of triggers against such defenses.\nNoise is initially generated through a noise-additive function, parameterized by $\\delta$, ensuring the noise exists within the\nconstrained range of [-1,1]. This method effectively camouflages the triggers against defenses by ensuring that the\nnoise adheres to the natural frequency ranges of the image data. Specifically, we apply a 2D Discrete Cosine Transform\n(DCT) to the noise matrix, retaining only low to mid-range frequency components using the matrix $m$, where:\n$M_{i,j} = \\begin{cases}\n1 & \\text{if } 1 \\leq i, j \\leq r\\delta \\\\\n0 & \\text{otherwise}\n\\end{cases}$\nThe resulting filtered noise is then integrated using:\n$Q(x) = IDCT(m * DCT(g(x)))$\nFurther enhancing robustness, a Gaussian blur filter $k$ is applied, smoothing transitions and diminishing high-frequency\ncomponents that are typically detectable by defensive Algorithms:\n$g(x) = (x + Q(x)) * k$\nThis dual approach, focusing on the strategic manipulation of frequency components and the integration of adaptive\nblending techniques, ensures that the BATOD method effectively circumvents sophisticated frequency-based defenses,\nmaintaining the integrity and intended effects of the triggers without detection."}, {"title": "E Preliminaries", "content": "Vulnerability of Classifiers to Adversarial Manipulations Research has demonstrated that deep neural networks\n(DNNs) are susceptible to adversarial manipulations, especially within classification contexts. When adversarial\ndisturbances are introduced to the input during the prediction phase, they can lead classifiers to incorrectly assign labels.\nSpecifically, a disturbance $\\delta$ is Appended to an instance $x$ with its true label $y$, deceiving the model into predicting\nan incorrect label $\\hat{y}$ for the manipulated input $x^*$, defined as $x^* = x + \\delta$. Among various methods, the Projected\nGradient Descent (PGD) technique is notable for its iterative approach to generating adversarial instances by keeping\nthe perturbation within the bounds of the $l_\\infty$ norm over $N$ iterations:\n$x_0^* = x, \\quad x_{i+1}^* = \\Pi_{||\\delta||_\\infty \\leq \\epsilon} (x_i^* + \\alpha \\cdot Sign(\\nabla_x L(x,y))), \\quad x^* = x_N^*$\nHere, $L (x, y)$ represents the targeted loss function, typically cross-entropy, for manipulating classifier behaviour."}, {"title": "F Related Work", "content": "Outlier Detection: The outlier detection task refers to identifying samples that diverge from those inliers. During\ntraining, a dataset $D$, with labels assumed to be ID, is used. In most previous works [2, 3, 4], the detector $f$ is trained\non the task of classifying ID classes. An outlier detector operates by assigning an outlier score to an input sample,"}, {"title": "F.1 Backdoor Defenses Overview", "content": "defense strategies for models focus on validating or countering the influence of third-party models before they are\nput into use. ABS[38] approached the problem by inspecting neurons and creating trigger possibilities through a\nreverse-engineering process, which were then tested on clean images. Zhao et al.[40] introduced a method that indirectly\nuses mode connectivity to investigate backdoor threats, effectively reducing their impact while preserving the model's\naccuracy on clean data. Neo[41] identifies potential backdoor triggers by noting changes in predictions when certain\nimage regions are obscured. These techniques confirm their suspicions by applying the identified triggers to a batch of\nclean images, relying on the assumption that triggers will consistently reveal themselves.\nAdversarial Neuron Pruning (ANP) [12] is predicated on the observation that in a backdoored model, certain neurons\nare more susceptible to adversarial perturbation (deliberate adjustment of neuron weights to initiate an adversarial\nattack) compared to others. Drawing on this insight, ANP advocates for the removal of these vulnerable neurons as\na countermeasure against backdoor threats. ANP presupposes that certain neurons are predominantly linked with\nbackdoor triggers, but since our attack does not rely on this principle, it effectively circumvents ANP's defenses. Despite\nANP's efficacy against many other attacks, our backdoor strategy successfully bypasses it."}, {"title": "G Resilience to more defenses", "content": "Neural Cleanse, developed by [42], is a method to protect models by focusing on creating specific patterns. It begins\nwith the premise that the harmful part of the model is akin to a small, added-on patch. For every category, Neural\nCleanse determines the optimal pattern to transform any safe input into that category. It then identifies any class with\na much smaller pattern, which could indicate a hidden issue. This method measures the problem using the Anomaly\nIndex with a specific cutoff point ($\\gamma = 2$). We tested our backdoor-attacked models with Neural Cleanse and shared\nthe results in 2, 8, and 6. If NC fails to detect the backdoor attack, the model weights remain unchanged. Our attack\nperformed well and was not detected in various scenarios against this defense.\nSTRIP [43] serves as a method for detecting threats using a model. It tests the model by adding slight changes to the\ninput image using various safe images from different categories. STRIP signals a warning if the model's predictions\ndon't change much, which means the outcome has low variability. This suggests the model might be compromised. Our\nattack method mimics the behaviour of unaffected models, showing a similar range of variability in results, as depicted\nin Figure 5.\nFine-Pruning [44] takes a different approach by examining individual neurons. Focusing on a particular layer, it\nevaluates how neurons react to a collection of safe images to identify inactive neurons, which are believed to be\nconnected to the hidden threat. These neurons are then carefully removed to reduce the risk. We applied Fine-Pruning\nto our models and charted the performance of the network, both under normal conditions and during attacks, as we\nremoved neurons. The results can be seen in Figure 6."}, {"title": "H Limitations", "content": "There is a limitation between the number of classes $K$ and the number of samples $D_a$. As we want to add $K$ poisoned\nsamples, for any selected samples $x \\in X_j, 1 \\leq j \\leq K$, for any number of classes $K$, we are adding $K^2$ samples\nto $D^{Out}_p$. Moreover, the total size of $D^{Out}_p$ should be $\\frac{1}{2}$ of available dataset $D_a$. Hence, we have this limitation:\n$\\frac{K^2}{2K} < |D_a|$."}, {"title": "I Implementation Detail", "content": "We employ various ResNet variants, ViT-B-16, Wide-ResNet-40-4, and VGG16, categorized either as Victim or\nSurrogate models. Each model starts with an initial learning rate of 0.01, adjusted using a cosine decay scheduler,\nwith Adam as the chosen optimizer. This selection is predicated on Adam's efficiency in managing sparse gradients.\nOur setup tests the resilience of these models under a backdoor attack methodology, ensuring rigorous conditions to\nthoroughly assess the effectiveness of our attacks. The broad applicability and the critical examination of our backdoor\nattack methodology are underscored, providing a profound insight into the robustness of neural networks against\nsophisticated threats."}, {"title": "J Societal Impact", "content": "The societal impact of this research is significant, as it directly relates to safety-critical systems in fields such as\nautonomous driving and medical diagnostics. By demonstrating the vulnerabilities of machine learning systems to\nbackdoor attacks, the paper sheds light on potential risks that could lead to severe consequences, including threats to\nhuman lives. The research underscores the urgent need for robust security measures and defense mechanisms to protect\nthese systems. In the conclusion, the authors acknowledge these risks, contributing to a crucial dialogue on the ethical\nimplications of AI and machine learning research. The discussion aims to foster a deeper understanding of the potential\nnegative impacts, promoting a more responsible approach to the development and deployment of AI technologies in\nsensitive and impactful areas."}]}