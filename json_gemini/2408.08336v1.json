{"title": "Graph representations of 3D data for machine learning", "authors": ["Tomasz Prytu\u0142a"], "abstract": "We give an overview of combinatorial methods to represent 3D data, such as graphs\nand meshes, from the viewpoint of their amenability to analysis using machine learning\nalgorithms. We highlight pros and cons of various representations and we discuss some\nmethods of generating/switching between the representations. We finally present two concrete\napplications in life science and industry. Despite its theoretical nature, our discussion is in\ngeneral motivated by, and biased towards real-world challenges.", "sections": [{"title": "Introduction", "content": "3D data appears naturally in science and\nindustry, and covers a wide range of do-\nmeans, from bioimaging (microscopy images,\nCT scans), through molecular chemistry, to\n3D modeling and design plans [GZW+20]. A\n3D representation is often advantageous as\nit describes a real world (hence 3D) object\nmore accurately, compared to e.g., 2D pro-\njections or slices. However, a drawback of\nthe 3-dimensional representation is the com-\nputational cost of the analysis, as the extra\ndimension, together with scarcity typical for\n3D data, makes it very challenging to apply\nlearning algorithms that scale effectively - to\nthe extreme where already analysis of a sin-\ngle sample can be on a verge of capability of\na single machine.\nIn our work we have investigated whether\nthis challenge can be overcome, or at least\npartially alleviated by employing lighter rep-\nresentations of 3D data graphs, meshes,\npoint clouds, and simplicial complexes, and\nthe corresponding deep learning algorithms\nthat operate on those representations. Our\nfindings suggest that in many real-world sit-\nuations it is the case, and we hope that prac-\ntitioners of machine learning can adapt some\nof our learnings to their work with 3D data.\nOur approach is backed by recent develop-\nments in the field Geometric Machine Learn-\ning, both theoretical [BBC+21] and software-\noriented [FL19].\nIn the specific domain of preclinical re-\nsearch and biomedical imaging, we are plan-\nnig to release a set of guidelines for analyz-\ning 3D data using a variety of combinatorial\nmethods, in cases where classical 3D deep\nlearning is not feasible.\nAnother benefit of using combinatorial rep-\nresentations is their potential for explain-\nability. Since such representations usually\ncome at a higher level of abstraction (e.g.,\na graph modeling a human pose), their el-\nements (edges, vertices) naturally carry se-\nmantic meaning, and thus it may be easier to\nextract the 'logic' behind a machine learning\nmodel's predictions."}, {"title": "3D data", "content": "In this section we present an overview of some\nrepresentations of 3D data, and we compare\nthem from a viewpoint of analysis using deep\nlearning methods. The overview is biased by\nthe specific problems we encountered, and by\nthe overarching theme of studying combina-\ntorial representations.\nVolumetric\nThe main challenge with 3D data compared\nwith 2D data is the computational complex-\nity of algorithms to analyze this data. This\nstems from the fact the most common rep-\nresentation of 3D data is by voxels (3D ana-\nlogue of pixels), which means that a volume is\nrepresented by a dense grid of 3-dimensional\ncubes, each of the cubes storing information\nabout e.g., RGB color or intensity. This is a\nstandard format used in many imaging tech-\nniques, and thus can be considered as a 'fun-\ndamental' or 'raw' format for 3D data. It is a\ntrivial observation that the number of voxels\ngrows exponentially with the dimension, and\nthus already a jump from 2D to 3D has severe\nconsequences for compute requirements.\nAnother issue, which is somehow more a\ncharacteristic of 3D data in general, is its\nsparsity. Whether it is a microscopy image\nof a neuron cell, or a 3D design for manu-\nfacturing, a lot of voxels are unoccupied, and\nthe actual object of interest fills only a small\nportion of the volume, see Figure 2. How-\never, one does not know it in advance, and\ntherefore the standard 3D algorithms (like\ne.g., Convolutional Neural Networks) process\nthe entire volume voxel by voxel.\nFinally, 3D data is typically more scarce\nthan 2D data: in many cases one uses an\nexpensive and time consuming procedure to\nproduce a single 3D image (e.g. 3D mi-\ncroscopy) and thus by default one deals with\nmuch smaller datasets than in other contexts.\nAn important feature of volumetric rep-\nresentations it that it is a so-called Eu-\nclidean representation, meaning that there is\na global coordinate system of XYZ coordi-\nnates. Thus, after agreeing on a given size of\nthe volume, all the examples have the exact\nsame size, and thus the algorithm for analy-\nsis does not have to accommodate size differ-\nences between samples.\nThere are a plenty of other ways to rep-\nresent the 3D data (and switch between the\nrepresentations) that can have some advan-\ntages. All the following representations are\nthe examples of non-Euclidean representa-\ntions.\nMesh\nMesh is a tessellation of a surface of a 3D ob-\nject by triangles (or other polygons). It de-\nscribes geometry well and removes the spar-\nsity problem. This is because to represent\na mesh, one only needs to store the position\nof vertices and encode which vertices form\ntriangles/polygons. As such, the mesh for-\nmat does not offer an intrinsic global coordi-\nnate system, but it has a resemblance of the\ncoordinate system locally, as the neighbor-\nhoods of vertices are all disks. This means\nthat around every vertex, mesh looks like a\nplanar region, and thus one can use tech-\nniques for processing 2D images, appropri-\nately adjusted. From the deep learning view-\npoint there are a few architectures to han-\ndle meshes (mostly variants of MeshCNN\n[HHF+19]), which cleverly exploit geomet-\nric features of the mesh without referring to\nits embedding into R\u00b3 (like angles and edge\nlengths). There are also some novel pooling\nand unpooling operations which are specific\nto mesh format.\nHowever, by definition a mesh comes in\nquite a constrained form, i.e., it is a surface\nof a 3D manifold. This is a strong restric-\ntion on topology, which is great for visualiza-\ntion/rendering, but from the point of view of\nanalysis it is often not flexible enough. Espe-\ncially when the data is highly non-smooth or\nnon-manifold. As shown in Section 3.2, even\nif data at hand is smooth, the mesh format\ndoes not allow easy augmentation or trans-\nformation of the data.\nPoint cloud\nA point cloud is a collection of points in\na 3D space, described by their coordinates\ntogether with some additional features, like\nRGB color or the intensity value. They\n'occur naturally', e.g., from Lidar scanning.\nPoint cloud offers a lot of flexibility of non-\nEuclidean data, together with some struc-\nture of Euclidean data. However, often point\ncloud format is too loose to capture geomet-\nric information as there is no apparent re-\nlation between the points, besides what one\ncan infer from their attributes. For example,\ngiven only points and no other information\nlike edges or triangles, it may be impossible\nto tell two objects apart. Another challenge\nis the size point clouds often contain mil-\nlions or even billions of points. This, how-\never, can be remedied, as it is relatively easy\nto subsample points. This in fact offers some\nextra flexibility, as one can choose different\ngranularity for different parts of the object,\nbased on the level of detail one wishes to cap-\nture.\nGraphs\nMany tasks in 3D analysis require under-\nstanding the geometry of the object (e.g.,"}, {"title": null, "content": "structure of the cells and their connections,\nshape of a molecule, spatial relations, prox-\nimity of the parts in 3D designs). Keeping\nthat in mind, graphs as a data representa-\ntion seem to be a good balance between ex-\npressiveness and simplicity. Graphs can be\ndefined in a purely abstract way one needs\nto specify the set of vertices and tell which\nvertices are connected by the edges, yet they\noften naturally carry spatial and geometric\ninformation, when one pictures a graph as a\nnetwork of nodes in 3D (from that viewpoint\none can think of graphs as 'enhanced' point\nclouds, where one pre-specifies which nodes\nare more 'important' to each other by join-\ning them by edges).\nThere are many flavors of graphs seen as\ndata representation. One particularly use-\nful for lowering the dimension is skeletal\ngraphs, that is, graphs which serve as a 1-\ndimensional skeleta for complex higher di-\nmensional geometric objects [TDS+16]. An\nexample could be a schematic graph repre-\nsenting human body, with nodes correspond-\ning to joints, and edges corresponding to legs,\nhands, torso, etc. Another flavor is knowledge\ngraphs, where nodes correspond to entities,\nand edges describe various relations between\nthem, e.g., social networks. These types of\ngraphs though have in principle nothing ge-\nometric about them. One also finds graphs\nwhich are schematic representations of actual\nobject, e.g., molecules in chemistry.\nFrom the machine learning viewpoint,\ngraphs come together with a custom range of\narchitectures to analyze them, the so-called\ngraph neural networks (GNN's). Graphs\nhave plenty of unique advantages over other\ndata formats:\n\u2022 Scalability: due to sparsity of 3D data,\nwhen moving from 2D to 3D one does\nnot observe a big change in the num-\nber of vertices or the size of the fea-\ntures (for example, a positional encod-\ning simply gets an extra column of z-\ncoordinates). The GNNs for 3D graph\ndata can proceed almost as fast as for\n2D data, as they simply do not see the\ndimension but only the connections be-\ntween the nodes. This is in high contrast\nwith voxels, where both the size of the\ndata, and the training time gets signif-\nicantly higher when moving to 3D. (On\nthe other hand, for processing a regular\ngrid of 2D points, like e.g., an image, a\nCNN will be faster than a GNN).\n\u2022 Ease of engineering and adding fea-\ntures: it is customary to have a graph\ntogether with a vector of vertex at-\ntributes and edge attributes, for each\nvertex and each edge respectively. The\ncommon practice is to add various ei-\nther externally known information (e.g.,\ntype of vertices if they represent differ-\nent entities, their color, etc.), or informa-\ntion stemming from a graph itself and its\npositional encoding (e.g., degree of ver-\ntices, lengths of edges, angles between\nthe edges, etc.). In principle one can do\nsimilar things for voxels, it feels though\nthat lack of naturally occurring features\nprevents that in practice.\n\u2022 Flexibility in enriching or trans-\nforming the graph itself: one can eas-\nily add extra edges and vertices, to en-\ncode vital information e.g., join vertices\nby edges based on their proximity wrt.\na chosen metric, or add 'higher order'\nnodes which group all vertices of a given\ntype. Note that this is not possible for\ne.g., meshes, as adding an edge without\na triangle, will cause it to stop being a\nmesh."}, {"title": "The \u20182-step process' in graph\nanalysis", "content": "Given a 3D data representation, to bene-\nfit from Graph Neural Networks and other\ngraph related tools, one first must turn the\ndata into a graph format. This is the first\nstep of the 2-step process, the second step be-\ning the analysis of the graph itself. This task\ncan already be nontrivial, where one may\nneed to use domain specific knowledge and\nvarious tricks. There are a couple of stan-\ndard methods, but they usually do not lead\nto optimal graphs:\n\u2022 Mesh \u2192 graph: take the 1-skeleton, i.e.,\nonly vertices and edges, forget about tri-\nangles/polygons. (Loses face informa-\ntion. One can subdivide mesh before-\nhand to retain some info about poly-\ngons.)\n\u2022 Voxel \u2192 graph: let voxels be nodes and\nconnect every voxel to 6 of its neighbors.\n(Results in a graph as large as the vol-\nume itself.)\n\u2022 Point cloud \u2192 graph: add edges based\non the proximity of vertices (may re-\nsult in a very large number of edges, if\nthe distance threshold is too low). Note\nthat 'proximity' here does not necessar-\nily mean spacial proximity, but can be\nan arbitrary user-chosen metric.\nIt is worth mentioning that some architec-\ntures, e.g., PointNet++ [QYS+17] create the\ngraph dynamically while training for a down-\nstream task, thus effectively merging steps 1\nand 2 into one pipeline."}, {"title": "Graph generation methods", "content": "We investigated mostly the so-called 'skele-\ntonization methods', whose aim is to pro-\nduce a 1-dimensional skeleton of a 3D ge-\nometric shape, which captures global geo-\nmetric information to a good degree of de-\ntail. These include classical methods such as\nsigned distance function thinning, or Medial\nAxis Transform as well as machine learning-\ninspired approaches, which focus on creating\na graph that is amenable directly to analysis\nwith GNN's, e.g., SN-graph [ZCL+21]."}, {"title": "Analysis methods", "content": "A default choice for graph data is that of\nGraph Neural Network (GNN). The key char-\nacteristic of these architectures is how to ap-\nply 'filters' on a vertex level and how to ag-\ngregate information from neighbors to update\nthe state of the vertex (the so-called message\npassing). In that regard, Graph Neural Net-\nworks can be seen as analogues (or even a\ngeneralization) of Convolutional Neural Net-\nworks to graph inputs.\nThere is also some novelty in how to per-\nform graph pooling and unpooling opera-"}, {"title": null, "content": "tions, and how to perform e.g., dropout, but\nmany other standard neural network com-\nponents can be incorporated without much\ntrouble (e.g., attention mechanism, batch\nnormalization, etc.). This gives a lot of flexi-\nbility in designing graph networks, especially\ngiven that there exists a Pytorch-based li-\nbrary at one's disposal [FL19].\nOne can also resort to a simpler ap-\nproach of vertex embeddings (e.g., node2vec\n[GL16]), to map a graph back to the Eu-\nclidean space, where more standard architec-\ntures are available.\nAnother 'hack' that can sometimes be ben-\neficial is to use the graph representation as a\nsource of features for classical models, like\ne.g., Decision Trees. This approach works if\na graph is supposed to describe some quanti-\ntative features of the object of interest."}, {"title": "Graph analysis vs. 3D CNN analysis", "content": "Since to train a GNN one has to first con-\nvert the data into a graph, then in order\nto compare computational complexity of this\napproach, to e.g., training a 3D CNN directly\non volumetric data, one has to take into ac-\ncount the complexity of both the conversion\nstep and then the GNN pipeline. While we\nindicated that in practical applications graph\nneural networks will be faster (mostly be-\ncause the input graph is smaller than the\noriginal volume) it is still to be discussed how\ncostly the conversion process is. Many of the\nskeletonization algorithms are not learning\nalgorithms, but rely on hard-coded logic, and\nthus are relatively fast to apply (even when\ntesting a couple of configurations of hyperpa-\nrameters). But somehow more importantly,\nit is the deep learning part of the analysis\nthat is extra costly, and this is because in\npractice one trains not one, but several mod-\nels with some changes of architecture and hy-\nperparameters along the way, before one ar-\nrives at a satisfying result. So, in the long\nrun, having a more compact representation\nwill always be beneficial."}, {"title": "Applications in concrete\ncases", "content": "In this section we present findings from two\nprojects in collaboration with life scientists\nand a software company, respectively."}, {"title": "Mitochondrial\nmuscle cells", "content": "networks in\nThe area of bioimaging and preclinical re-\nsearch and development is a particularly good\ntestbed for our programme, as one almost al-\nways deals with the major challenges of 3D\ndata: the large sample size, sparsity, and\nscarcity. At the same time there is a lot\nof variation across different tasks, so that al-\nmost every single imaging problem requires a\ntailored pipeline for the analysis. We present\none such case, which is a joint work with\nClara Prats [GPK+]."}, {"title": "Data", "content": "Data comes as confocal fluorescent mi-\ncroscopy images of muscle cells, multichan-\nnel, with one channel showing mitochondrial\nnetworks which are crucial for the task. Man-\nually labelled on a scale from 1 to 10, by an\nexpert annotator."}, {"title": "Task", "content": "Predict the degree of healthiness of mi-\ntochondrial networks. It can be posed\nas a binary classification healthy/unhealthy\n(healthy: score larger than 5, unhealthy:\nscore at most 5), or a regression on a scale 1\nto 10. Healthiness of these networks is asso-\nciated with several conditions, such as PCD\n(primary carnitine deficiency, a certain mus-\ncle disorder) [Jac], obesity, physical inactiv-\nity, and type 2 diabetes [KPM+18]."}, {"title": "Data description", "content": "A mitochondrial network resembles a rectan-\ngular grid, see Figure 5. The images contain\nconsiderable amount of noise and some imag-\ning artifacts. The criteria behind annotation\nwere: the more healthy the cell is, the more\nregular and straight the grid is, the fewer\nbursts or ruptures one can find in it."}, {"title": "Method", "content": "Given the intuition behind the healthiness of\ncells we thought of a graph as a natural rep-\nresentation to encode the images, as it pre-\ncisely captures connectedness of the grid and\nits geometric shape. Due to the shallowness\nof volumes (up to 20 voxels in z-direction)\nwe focused on testing the approach on 2D\nslices. However, the approach will scale with\nno problems to deeper volumes in 3D. Fur-\nthermore, to reduce computational complex-\nity, we cut every such slice into patches of size\n256 \u00d7 256 pixels each - the original slices have\nvarying sizes of up to 1500 pixels in x- and\ny-direction. Also, having many 'small' sam-\nples as opposed to few 'big' ones can help in\nconvergence and stability of training of the\nmachine learning model."}, {"title": "Graph generation", "content": "To create graphs, we use a variant of skele-\ntonization algorithm called the SN-graph\n[ZCL+21]. We introduced some minor\ntweaks to the algorithm to obtain graphs that\nare visually closer to the images. The tun-\ning of hyperparameters for the algorithm was\nperformed based on expert intuition and to\nsome degree guided by model's performance\ngeneration of the graph is a deterministic,\nyet costly process, so only a few configura-\ntions of parameters were tested.\nDimensionality reduction: The patch in\nFigure 5 has 256 \u00d7 256 pixels. We con-\nstrained graphs to have maximally 300 ver-\ntices, and on average such a constraint re-\nsulted in around 600 edges (on a perfectly\nrectangular grid, one has asymptotically 2\ntimes as many edges as vertices, so this result\ngives us an implicit validation of the graph\ngeneration algorithm)."}, {"title": "Classification/regression model", "content": "We use Graph Attention Network [VCC+18],\nwith 4 attention layers, and 2 fully connected\nlayers. The vertex and edge attributes in-\nclude standard geometric features, such as\nlengths of edges, their slope (seen as a dis-\ncrete variable: vertical, horizontal, or skew),\ndegree of vertices, etc. We also include some\nfeatures of an underlying image in the neigh-\nborhood of a vertex, e.g., the average inten-\nsity. For a comparison we also use a convo-\nlutional neural network for the same task.\nBy aggregating prediction over all slices\nfor each volume, both approaches resulted\nin 100% accuracy on the binary classifi-\ncation task. On the single 'patch' level,\nGNN achieved 0.86 accuracy, whereas CNN\nachieved 0.95 accuracy. On the flipside, GNN\nwas trained on a single CPU, in a little less\nthan an hour, while CNN required a GPU\nacceleration.\nHaving generated enough quantitative,\nglobal features such as e.g., the average de-\ngree of a graph, the number of horizon-\ntal/vertical edges, the average intensity of\nvertices, etc., we tried to make a classification\nbased solely on those features with a use of\nthe Random Forest algorithm. Surprisingly,\nwe also got 100% accuracy on the volume\nlevel, and 0.84 on patch level. On the other\nhand, these features intuitively do describe\nhow 'regular' the graph is, so maybe such a\nhigh performance of a model based on those\nfeatures is not that surprising. But nonethe-\nless, we believe that in general skeletoniza-\ntion algorithms can be thought as good fea-\nture extractors, regardless of the final model\nused for the task."}, {"title": "AI for 3D design generation\nand analysis", "content": "We present findings from a pilot\nproject under the AI:Denmark initiative\n(https://aidenmark.dk/) with a soft-\nware company called RD8 Technology\n(https://rd8.tech). RD8 Technology investi-\ngates 3D design drawings for manufacturing,\nand offer improvements to those designs,\nfrom the point of view of functionality,\nmaterial use, durability, robustness to manu-\nfacturing imperfections, etc. They work with\n3D CAD models (see Figure 7) and process\nthem using, among other measures, para-\nmetric and analytic geometry. They plan to\nintroduce AI and a data-driven approach to\nimprove their model application and func-\ntionality. There are multiple possibilities\nfor doing so, to name a few: an AI-aided\nautomatic generation of designs, automatic\nsuggestions for improvements to existing\ndesigns, or detection of bottlenecks or errors.\nBehind many of these improvements there\nis a need for an ML-architecture capable\nof learning geometry, topology, and all the\nphysical constraints (like e.g., movement or\nbending of parts). In our pilot project, we\nisolate a fairly simple task to see how one\ncan begin creating such an architecture."}, {"title": "Data", "content": "Data comes as parametric 3D geometries,\nand as such it is almost hopeless to try to\nanalyze it using 3D convolutional neural net-\nworks. This is because a generic design plan\ncontains details on many different scales and\nthus, to make a putative voxel representa-\ntion one needs to use very high resolution,\nwhich could result in a gigantic dimension\n(in terms of the amount of voxels) and a lot\nof superfluous information. Even more im-\nportantly, the parametric description is at a\nmuch higher level of abstraction than vox-\nels, therefore turning it into volume seems\nlike an unnecessary complication. On the\nother hand, a CAD plan can be turned to\ne.g., polygonal mesh with almost no loss of\ninformation. In general, meshes and graphs\nare naturally very well suited for storing data\nwith varying resolution and amount of de-\ntails."}, {"title": "Task", "content": "A simple test task is the following: given a\npair of parts that are interconnected, calcu-\nlate their exact contact areas areas where\nthe surfaces of the two parts will touch each\nother, see Figure 8. More generally, to\naccommodate practical constraints such as\nmanufacturing imperfections or to determine\ntolerance for errors, one is also interested in\npredicting the prospective contact areas if\nparts nearly touch, or the overlap areas if\nparts overlap.\nWhile this task can be solved with lin-\near algebra and analytic geometry, it is in-\nteresting to see how close one can get with\ndata-driven methods. There is also a chance\nfor actual improvement. Since the current\npipeline is computationally involved and in-\ncludes many tedious corner cases requiring\nextra hard-coded rules to perform well, one\ncould hope that a robust learning algorithm\ncould perform the task faster and better cap-\nture non-standard patterns."}, {"title": "Method", "content": "We begin by representing both parts as tri-\nangular meshes. In order to enable a broader\nrange of architectures and to be able to incor-\nporate additional information like e.g., spa-\ntial relation between the two parts, we turn\nthe meshes into graphs, by 'forgetting' the\ntriangles. That is, we remember vertices\nand which of them are connected by edges,\nand simply forget the information of which\ntriples of edges/vertices form triangles, see\nFigure 9. This loses very little information,\nas in general there are very few triples of ver-\ntices which are pairwise connected but do not\nspan a triangle.\nGiven such a graph input, the task be-\ncomes a binary semantic segmentation of ver-\ntices: given a vertex of either of the two\nparts, a model has to predict whether it is\na part of a contact area or not, based on a\ntarget mask provided by RD8's current soft-\nware (with manual intervention if needed).\nHaving a graph representation has many\nadvantages. First, since the task depends\nonly on the relative position of two parts, and\nnot on their embedding into the Euclidean\nspace, the most effective choice is a represen-\ntation that is invariant to the rotations and\ntranslations, and this is easiest achieved with\na graph or a mesh. Second, a graph allows us\nto easily augment our input in many ways:\n\u2022 Since the proximity between the parts is\na vital feature for the task, we add edges\nbetween vertices belonging to different\nparts which are close to each other.\n\u2022 Because the concept of orientation mat-\nters (i.e., what is inside and what is out-\nside of the surface), adding vertex nor-\nmal vectors seems like a good idea. To\ndo it in a coordinate-free way, one can\ne.g., add a new vertex at the tip of a\nnormal vector and connect it by an edge\nto the original vertex.\nNote that while the notions of 'proximity'\nand 'orientation' are somehow available at\nhand in volumetric representation, the lack of\nrotational and translation invariance would\nlikely require much more data until the model\nlearns the actual geometric features. On\nthe other hand, as shown above, it is rela-\ntively easy to incorporate these features in\nthe graph representation, with a minor in-\ncrease in complexity.\nThe neural network we use is a Graph At-\ntention Network [VCC+18], with 3 message-\npassing layers and 2 fully connected layers.\nThe output of the network is a prediction be-\ntween 0 and 1 for each vertex, of how likely\nthis vertex is to be a part of a contact area."}, {"title": "Results", "content": "Due to the small amount of training data, we\nonly got an indication that the chosen data\nrepresentation and the corresponding archi-\ntecture can learn the task. Namely, as shown\nin Figure 10, the model trains and general-\nizes. However, confusion matrices for binary\nclassification presented in in Figure 11 show\nthat the model overfits to non-contact ver-\ntices (as there are many more of them in each\npair of parts). On the other hand, a visual\ninspection of the predicted areas shows that\nthe model begins to assign higher prediction\nvalues for contact areas, even though they\nare still below the classification threshold, see\nFigure 9. This can be taken as a strong indi-\ncation that with a larger dataset (and possi-\nbly a larger network) the model can become\nquite accurate, and suggests that the GNN\narchitecture is capable of learning the geom-\netry behind the task."}]}