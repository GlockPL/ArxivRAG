{"title": "AI-Assisted Decision Making with Human Learning", "authors": ["Gali Noti", "Kate Donahue", "Jon Kleinberg", "Sigal Oren"], "abstract": "AI systems are increasingly used to support human decision-making. In many cases, despite the algorithm's superior performance, the final decision remains in human hands. For example, an AI may assist doctors in determining which diagnostic tests to run, but the doctor ultimately makes the diagnosis. Focusing on these scenarios, this paper studies AI-assisted decision-making where the human learns through repeated interactions with the algorithm. In our framework, the algorithm - designed to maximize decision accuracy according to its own model - determines which features the human can consider. The human then makes a prediction based on their own less accurate model. Additionally, we consider the possibility of a constraint on the number of features that can be taken into account.\nWe observe that the discrepancy between the algorithm's model and the human's model creates a fundamental trade-off. Should the algorithm prioritize recommending more informative features, encouraging the human to recognize their importance, even if it results in less accurate predictions in the short term until learning occurs? Or is it preferable to forgo educating the human and instead select features that align more closely with their existing understanding, minimizing the immediate cost of learning? This trade-off is shaped by both the algorithm's patience (the time-discount rate of its objective function over multiple periods) and the human's willingness and ability to learn.\nOur results show that optimal feature selection has a surprisingly clean combinatorial character-ization, reducible to a stationary sequence of feature subsets that is tractable to compute. As the algorithm becomes more patient or the human's learning improves, the algorithm increasingly selects more informative features, enhancing both prediction accuracy and the human's understanding of the world. Notably, early investment in learning leads to the selection of more informative features compared to a later investment. We complement our analysis by showing that the impact of errors in the algorithm's knowledge is limited as it does not make the prediction directly.", "sections": [{"title": "Introduction", "content": "AI systems are increasingly used to assist humans in making decisions. In many situations, although the algorithm has superior performance, it can only assist the human by providing recommendations, leaving the final decision to the human. For example, algorithms assist medical doctors in assessing patients' risk factors and in targeting health inspections and treatments ([23, 34, 36, 49]), and assist judges in making pretrial release decisions, as well as in sentencing and parole determinations ([17, 40, 41]). However, the final decision ultimately remains at the discretion of the doctors or judges [15, 18]. This paper studies such AI-assisted decision-making scenarios where the human decision-maker learns from experience in repeated interactions with the algorithm. In our setting, an algorithm assists a human by telling them which information they should use for making a prediction, with the goal of improving the human's prediction accuracy.\nConsider, for instance, a doctor assessing a patient's risk of a bacterial infection. To improve their diagnosis, the doctor can order tests that reveal the values of unknown variables. However, the number of"}, {"title": "Related Literature", "content": "Our work is situated in the literature on designing algorithms for assisting human decision-makers (e.g., [7, 24, 25, 27, 43, 50]). In particular, we consider a setting in which the algorithm selects for the human what to learn for making the best prediction as part of a repeated interaction.\nThe majority of the literature assume that the algorithm has direct access to information and can give the human a decision recommendation [1, 26] or display the human the relevant information for making the decisions (e.g., [20, 21]). A notable exception in regard to the algorithm's informational structure is [33] that theoretically studies a reverse setting, complementary to ours, where humans have discretion to choose, based on situational information, which features to use at each time step, and algorithmic tools are obliged to use the same features.\nEmpirical papers in this area of AI-assisted human decision-making study the ability of human decision makers to correctly rely on the algorithm [1, 12, 25, 26, 50, 52]. Typically, they do not consider human learning. Two exceptions are [42] that showed experimentally the advantage of an algorithm to not always provide a recommendation and provided evidence that human decision makers learn through repeated interaction with the algorithm, and [13] that present an experimental study that applies reinforcement"}, {"title": "Model and Preliminaries", "content": "In this section, we provide additional details about the model introduced in Section 1. Recall that we consider a human tasked with predicting the outcome of a variable y. The true outcome is given by a linear function of a set of n features $x = {x_1, ..., x_n}$, such that $y = c + \\sum_{i=1}^{n} a_ix_i$, where the coefficients $a_i$ are non-zero. The features $x_i$ are independent random variables drawn from distributions $F_i$ with known means and finite standard deviations. Without loss of generality, throughout our analysis we assume the features are standardized (such that they have zero mean and unit variance; see Appendix A). The human and the algorithm interact repeatedly as described in Section 1. We divide the next discussion into two perspectives: the human's perspective and the algorithm's perspective."}, {"title": "The Human", "content": "At each time step t, the human observes the realization of the features $A_t$ that the algorithm selected and makes a prediction $\\hat{c} + \\sum_{i\\in A_t} h_{i,t}x_i$ (predicting the mean of zero for any unobserved features). This minimizes the Mean Squared Error (MSE) from the human's perspective. As the MSE is a function of $A_t$ and the human's coefficient vector $h_t$, we denote it by $MSE(A_t, h_t)$ and get:\nClaim 3.1. $MSE(A_t, h_t) = (c - \\hat{c})^2 + ( \\sum_{i \\notin A_t} a_i^2 + \\sum_{i \\in A_t} (a_i - h_{i,t})^2)$"}, {"title": "The Algorithm", "content": "The objective of the algorithm is determined by its designer who cares about minimizing the loss of the human's prediction. The designer sets a budget $0 \\le k \\le n$ on the number of features that the human"}, {"title": "Warm-up: Feature Selection with Fixed Human Beliefs", "content": "We start by characterizing the algorithm's optimal selection of features in the static case where the human holds fixed beliefs h about feature coefficients. In this case, according to Claim 3.1 in Section 3, the algorithm will choose the subset of features minimizing:\n$MSE(A, h) = (c - \\hat{c})^2 + ( \\sum_{i \\notin A} a_i^2 + \\sum_{i \\in A} (a_i - h_i)^2)$\nWith only one feature, by Claim 3.1 we have: $MSE({1}, h) = (c - \\hat{c})^2 + (a_1 - h_1)^2$, whereas if we do not use this feature, $MSE(0, h) = (c - \\hat{c})^2 + a_1^2$. Thus, selecting the feature reduces error whenever $a_1^2 > (a_1 - h_1)^2$,"}, {"title": "Feature Selection with Human Learning", "content": "In this section, we turn to discussing algorithmic assistance for a human decision maker who learns and updates their beliefs through repeated interactions with the algorithm. In Section 1, we saw the fundamental tension between informativeness and divergence, which leads to the phenomenon where the algorithm does not necessarily select the most informative features for prediction but instead it may select those features that the human can effectively use. As discussed, when the human's beliefs are not fixed but are instead updated through repeated interaction with the algorithm, selecting a less informative set of features - while may be helpful in the short term before learning occurs may limit learning opportunities, and as a result sustain human's incorrect beliefs and potentially lead to worse performance in the long run. We will see that in our framework, this tradoff is governed by two parameters: $\\delta$ that captures how much weight the algorithm's designer puts on short term losses versus long terms losses; and $\\phi$ that captures how efficient the human's learning is."}, {"title": "The Value of a Sequence", "content": "For the static model (Section 4) it was very useful to define the value of using a subset of features (Definition 4.1). We can extend this definition for a sequence of feature subsets in our learning setting:\nDefinition 5.1. The value of a sequence of feature subsets $S = (A_t)_{t=0}^{\\infty}$ for a given value of $\\delta$ and a learning dynamic that is $\\phi$-convergent is the improvement in discounted loss from selecting features according to S compared to not using any feature at all. That is,\n$V_{\\delta,\\phi}(S) = L((\\emptyset)_{t=0}^{\\infty}, \\phi, h_0) - L(S, \\phi, h_0)$"}, {"title": "Example: Exponential Learning Dynamics", "content": "We consider a $\\phi$-convergent learning rule given by the following simple dynamic:\n$h_{i,t+1} = f(h_{i,t}, a, A_t) = \\begin{cases}\n      w \\cdot h_{i,t} + (1 - w) a_i, & \\text{if } i \\in A_t, \\\\\n      h_{i,t}, & \\text{otherwise.}\n    \\end{cases}$ \nwhere $w \\in [0, 1]$ represents a learning rate parameter. Large values of w indicate a human who is a slow learner, while smaller values of w correspond to a faster learner. Equivalently, we can write a cumulative version of this step-by-step learning rule, where the human's belief about feature i at time t is given by:\n$h_{i,t} = w^{m_i}h_{i,0} + (1 - w^{m_i}) \\cdot a_i$\nand $m_i = m_i(t)$ denotes the number of times feature i has been selected up to time t. At t = 0, the human starts with their initial beliefs $h_{i,0}$ about each feature i. As $m_i \\rightarrow \\infty$, the human's coefficient of i exponentially converges to the true value $a_i$.\nThis learning dynamic is $\\phi$-convergent with an exponential convergence rate of $\\phi(m_i) = w^{2m_i}$. To see why, substitute Equation (6) and get that:\n$(a_i - h_{i,t})^2 = (a_i - (w^{m_i} \\cdot h_{i,0} + (1 - w^{m_i})a_i))^2 = (w^{m_i} (a_i - h_{i,0}))^2 = w^{2m_i} (a_i - h_{i,0})^2$"}, {"title": "Optimality of Stationary Sequences", "content": "In this section, we analyze optimal sequences of feature selections in the setting of human learning. We assume that the true coefficients $a_i$ are distinct.\nDefinition 5.3. A sequence of feature subsets $S = (A_t)_{t=0}^{\\infty}$ is stationary if $A_t = A \\subseteq [n]$ for all t. We use the notation $(A)$ to denote a stationary sequence that always selects a subset A.\nWe are particularly interested in stationary sequences due to their simplicity. We show that for general $n > 1$ and $k \\le n$, if the learning dynamic is $\\phi$-convergent, then there exists an optimal sequence that is stationary. This fact dramatically decreases the size of the optimization space and the computational complexity of the problem; Later in Proposition 5.7 we show that we can find an optimal stationary sequence in $nlog(n)$ time.\nThe proof builds on the following lemma showing that if the learning dynamic is $\\phi$-convergent, then there exists an optimal sequence that has an infinite suffix that starting at time step t always selects the same subset (i.e., the sequence has an infinite stationary suffix)."}, {"title": "When Does the Algorithm Select Informative Features?", "content": "Next, we study the following question: under what conditions does the algorithm prioritize selecting the more informative set of features allowing the human to learn (the growth mindset), rather than keep making the myopic choice of the initially less divergent features for short-term rewards (the fixed mindset)? As the example in Section 5.2 illustrates, this depends in our framework on how \u201cpatient\u201d the algorithm is (the parameter $\\delta$) when considering future rewards, as well as the efficiency of human learning (the parameter $\\phi$). The definitions and set of results below formalize this intuition."}, {"title": "The algorithm's patience", "content": "Recall that $a_i$ captures the informativeness of feature i, and $(a_i - h_{i,0})^2$ is the initial divergence of the human's belief about the importance of the feature from the algorithm's estimate. For a pair of features i and j such that $a_i > a_j$, we denote by $\\Delta_{i,j} = a_i^2 - a_j^2$ the informativeness difference between features i and j, and by $\\Delta_{i,j}^D = (a_i - h_{i,0})^2 - (a_j - h_{j,0})^2$ the divergence difference between i and j. The following lemma uses the informativeness difference and divergence difference between features i and j to characterize the values of $\\delta$ for which $V_{\\delta,\\phi}(({i})_{t=0}^{\\infty}) \\ge V_{\\delta,\\phi}(({j})_{t=0}^{\\infty})$. The lemma establishes the main formal basis for analyzing the fixed vs. growth mindset tradeoff (Tradeoff 2). It shows that for two features i and j, the more informative feature is either always preferred for any $\\delta$, or there is a critical value of $\\delta$ above which this is true. Intuitively, a feature that has high informativeness but also high divergence, may have low value in the early stages before learning takes place, but high value in later stages as learning converges. Small $\\delta$ puts relatively more weight on the early stages, and so the informative feature might not be selected, but when $\\delta$ is large, there is more weight on the outcomes of learning where the more informative feature has higher value."}, {"title": "The learning dynamic efficiency", "content": "In this section, we examine the impact of the efficiency of human learning on the algorithm's selection. We see that, roughly speaking, an optimal subset of features for an efficient learner is more informative than that for a slower learner.\nDefinition 5.12. A learning dynamic that is $\\phi$-convergent is more efficient than a learning dynamic that is $\\phi'$-convergent if for every t, $\\phi(t) \\le \\phi'(t)$ and there is at least a single t such that $\\phi(t) < \\phi'(t)$.\nTo take a specific example, in the exponential learning model, a learning dynamic that is $w_1t$-convergent is more efficient than a learning dynamic that is $w_2t$-convergent if $w_1 < w_2$. At a higher level, we note that learning dynamics that are $\\phi$-convergent for $\\phi$ with decreasing marginals (i.e., concave) are more efficient than learning dynamics that are $\\phi'$-convergent for $\\phi'$ with the same marginals as $\\phi$ but in a different order. Intuitively, this suggests that it is more beneficial for the human to invest more in learning during earlier time steps rather than later ones, as this allows the algorithm to select more informative features. Formally, we introduce the following definition to compare the efficiency of two learning dynamics:\nDefinition 5.13. Fix a learning dynamic that is $\\phi$-convergent. $\\psi(t) = \\phi(t - 1) - \\phi(t)$ for $t \\ge 1$ is the marginal function of $\\phi(t)$."}, {"title": "Misspecification", "content": "We analyze the effect of errors in the algorithm's estimates of the ground-truth coefficients a, the human's coefficients h, and the convergence rate $\\phi$ of the learning dynamic. Modeling errors can lead to incorrect feature selection, reducing overall value. To quantify this, we express these errors as the maximum possible error margin in value per feature. As we will see, it is important to distinguish between \"overshoot\" errors and \"undershoot\" errors, as these margins are asymmetric for few parameters. The following definition captures this formally, where V({i}) is the true value of feature i depending on whether we are in the static setting or the learning setting, and V'({i}) is the value as computed by the imperfect algorithm.\nDefinition 6.1. Let $\\epsilon_i$ denote an upper bound on the magnitude of error in some coefficient of feature i (e.g., ground-truth coefficient or human's coefficient). The error margin of feature i's value, V({i}), is defined by two non-negative functions $\\xi_i(\\epsilon_i)$ and $\\bar{\\xi}_i(\\epsilon_i)$ such that,\n$V({i}) - \\xi_i(\\epsilon_i) \\le V'({i}) \\le V({i}) + \\bar{\\xi}_i(\\epsilon_i)$\nThe error margins in estimating specific values can be used to compute the error margin in the algorithm's selection. The idea is to incorporate these margins into the algorithm's decision of whether to include feature i or feature j. Specifically, let $A^*$ denote an optimal set, if for feature i \u2208 A* and any feature j \u2209 A*, we have that V({i}) - V({j}) \u2265  $\\xi_i(\\epsilon_i) + \\bar{\\xi}_j(\\epsilon_j)$, then the algorithm will also prefer i over j, and the error will not affect the result. Else, the algorithm may choose j instead of i, but the error from this choice will be bounded by $\\bar{\\xi}_i(\\epsilon_i) + \\bar{\\xi}_j(\\epsilon_j)$."}, {"title": "Misspecification in the Non-Learning Setting", "content": "Recall that when the human's beliefs are fixed, we have $V({i}, h) = 2a_ih_i - h_i^2$. We first bound the error resulting from the algorithm misspecifying the ground-truth coefficients. As we will see, in this case, the value's margin error is symmetric."}, {"title": "Misspecification in the Learning Setting", "content": "Recall that in this setting:\n$V_{\\delta, \\phi}(({i})_{t=0}^{\\infty}) = \\frac{1}{1 - \\delta} a_i^2 - \\sum_{t=0}^{\\infty} \\delta^t \\phi(t) (a_i - h_{i,0})^2$\nWe first consider an algorithm that misspesifies the human's coefficients and observe that the error margins are direct generalization of the error we have seen for the analogous error in the non-learning setting."}, {"title": "Discussion", "content": "In this paper, we formally model human decision-making with algorithmic assistance when the human is learning through repeated interactions, and study optimal algorithmic strategies in this context."}]}