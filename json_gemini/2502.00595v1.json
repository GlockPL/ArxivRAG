{"title": "RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines", "authors": ["Pengfei Yu", "Dongming Shen", "Silin Meng", "Jaewon Lee", "Weisu Yin", "Andrea Yaoyun Cui", "Zhenlin Xu", "Yi Zhu", "Xingjian Shi", "Mu Li", "Alex Smola"], "abstract": "We present RPGBENCH, the first benchmark designed to evaluate large language models (LLMs) as text-based role-playing game (RPG) engines. RPGBENCH comprises two core tasks: Game Creation (GC) and Game Simulation (GS). In GC, an LLM must craft a valid and playable RPG world using a structured event-state representation, ensuring logical coherence and proper termination conditions. In GS, the LLM simulates interactive gameplay across multiple rounds while consistently updating states and enforcing game rules. To comprehensively assess performance, RPGBENCH integrates objective and subjective evaluation methodologies. Objective measures verify adherence to event mechanics and check variable updates without requiring human intervention. Subjective measures\u2014such as content interestingness, action quality, and role-playing capability\u2014are evaluated via an LLM-as-a-judge framework, where a strong LLM grades each candidate's outputs. Empirical results demonstrate that state-of-the-art LLMs can produce engaging stories but often struggle to implement consistent, verifiable game mechanics, particularly in long or complex scenarios. By combining structured, rule-based assessments with LLM-based judgments, RPGBENCH provides a new standard for evaluating how well LLMs can balance creativity, coherence, and complexity in text-based RPGs, opening avenues for more immersive and controllable interactive storytelling.", "sections": [{"title": "1. Introduction", "content": "Recent advances in large language models (LLMs) have significantly expanded the frontiers of artificial intelligence, enabling breakthroughs in areas such as content generation, conversational agents, and interactive storytelling. Among these capabilities, role-playing has emerged as a particularly promising application, with the potential to revolutionize both entertainment\u2014by powering next-generation interactive games\u2014and social AI\u2014by enabling more engaging and emotionally resonant interactions (Chen et al., 2024b). While prior research on role-playing agents has primarily focused on their ability to simulate a given persona at the role-level, our work expands this scope to the game-level, where LLMs must not only role-play a character but also create and simulate coherent, interactive game worlds. To evaluate this broader capability, we introduce RPGBENCH, the first benchmark designed to assess LLMs as text-based role-playing game engines. RPGBENCH consists of two core tasks: Game Creation (GC), where an LLM generates a structured, playable game world based on a given character, and Game Simulation (GS), where the model simulates gameplay through sequential interactions with a player. Extending role-playing evaluation to the game level introduces a crucial challenge: ensuring that generated game worlds follow internally consistent and enforceable game mechanics. Game mechanics define how the game state evolves in response to player actions and narrative events, providing structure and coherence to interactive storytelling. Unlike traditional text generation tasks, where coherence is judged subjectively, game mechanics must be evaluated objectively to verify whether a generated game is logically sound and fully playable. To address this, we propose a two-stage benchmark pipeline centered around an automated BFS Validity Checker. This checker formally verifies that each generated game satisfies key structural requirements\u2014ensuring that all events are reachable, game progression follows a valid set of rules, and both success and failure endings are attainable. By automating this verification, we establish a high-quality dataset of valid games, which then serves as the test set for the GS task. Building on this validated game set, we introduce a novel Game Simulation Framework for dynamic, multi-round player interactions. In this framework, the LLM operates as a game engine, executing a structured simulation loop that consists of three stages per round: (1) Event Planning,"}, {"title": "2. Related Work", "content": "RPGBENCH is, to the best of our knowledge, the first benchmark designed to evaluate the capabilities of large language models (LLMs) in creating and running role-playing games (RPGs). The game creation subtask introduces a novel and challenging task for LLMs. For the game running subtask, our character-related metrics such as personality and factual consistency align with prior work on evaluating role-playing agents. Among prior benchmarks, CharacterBox (Wang et al., 2024a) is most closely related to RPGBENCH, focusing on role-playing capabilities in text-based virtual worlds. However, RPGBENCH differentiates itself by introducing a game structure with verifiable mechanics, enabling deterministic LLM-free evaluations for game dynamics. Apart from (Wang et al., 2024a), other role-playing benchmarks do not embed their evaluations within a virtual text-based environment, thus being more persona-centric instead of game-based. PersonaGym (Samuel et al., 2024) introduces PersonaScore, which evaluates LLM role-playing agents in QA tasks within sampled environments. Yuan et al. (2024) assess LLMs' understanding of characters through character profiling tasks. BosonAI (2024) and Gusev (2024) evaluate role-playing via multi-turn dialogues with user simulators, while InCharacter (Wang et al., 2024b) employs psychometric interviews to measure character fidelity. SocialBench (Chen et al., 2024a) proposes a framework for evaluating the sociality of role-playing agents, and CharacterEval (Tu et al., 2024) introduces multi-dimensional metrics for conversational role-playing agents. Additionally, some benchmarks (Gusev, 2024; Dai et al., 2024) incorporate multimodal contexts into role-playing evaluations."}, {"title": "3. Dataset Collection", "content": "In this section, we first introduce our game design, including the representations of game setup and mechanics. We then describe a two-stage data collection process for the Game Creation (GC) and Game Simulation (GS) tasks. In the first stage, we build a non-player character (NPC) pool from fictional character Wikipedia pages, and prompt various LLMs to create one game per NPC. An automatic game validity checker applies for selecting valid games. In the second stage, we assemble a test set of valid games for GS."}, {"title": "3.1. Game Design", "content": "The games in RPGBENCH, as illustrated in Figure 1, are structured around several core components that create a text-based role-playing game (RPG) experience. This design ensures sufficient flexibility for diverse storytelling while maintaining support for objective mechanic evaluation:\n\u2022 Game World: The overarching setting where the story unfolds (e.g., \"Gotham City\").\n\u2022 Player Character: The protagonist controlled by the player, including a name and description (e.g., \"Ann,\" a detective and ally of Batman)."}, {"title": "3.2. Game Data Collection", "content": "We select 100 fictional characters from Wikipedia to serve as the test set for GC. For each character, we prompt an LLM to create a JSON-formatted game (as specified above) that treats this character as the main NPC. We employ a 5-shot prompting approach, where the examples are generated by initially prompting GPT 40 using a manually crafted game. The full prompt is provided in the Appendix B. We parse LLM outputs to ensure they conform to the JSON format. Any game that passes this format check is then tested for validity using a BFS-based checker (see Section 4.1), which confirms whether a game can end in both success and failure, and whether all events can be reached. All valid games from multiple models are collected for the GS task."}, {"title": "4. Evaluation Metrics", "content": "Our evaluation covers multiple dimensions, scored over the trajectory of interactions. A sim-"}, {"title": "4.1. Game Creation Evaluation", "content": "In GC, we evaluate an LLM's capability to create games that have good mechanics. This task requires complex reasoning over event-state interactions that is very challenging even for human. Section 3.2 offered a broad overview of the GC task. We now define it more precisely. Task Definition [Game Creation] Given a fictional character C and related Wikipedia information R, an LLM must create a game G that follows a predefined format J. In RPGBENCH, 100 fictional characters are used, each with an associated Wikipedia page (R), facilitating future expansion of the character pool. The game G must conform to the structure J given in Section 3.1. We provide each LLM with a 5-shot prompt to generate one game per character."}, {"title": "Algorithm 1 BFS Validity Checker", "content": "Input: Events E, each with entering and success conditions, plus success and fail effects; A state S0 with initial values for all variables; An integer M indicating the maximum number of states to be explored. function isValid(E, S, M) Initialize a queue Q and enqueue S0. Initialize a visited set V = {S0}. Initialize a triggered-event set T = \u00d8. successFound = False; loseFound = false repeat S = Q.dequeue() if |V| > M then break # Reached maximum search limit end if availableEvents = {e \u2208 E : e.enterCond(S)} for each e \u2208 availableEvents do T=T\u222a{e} # Mark event as triggered S' = e.applyEffect(S, e.successCond(S)) successFound |= e.isSuccessTermination(S') loseFound |= e.isLosingTermination(S') if S' \u2209 V then Q.enqueue(S'); V = V\u222a {S'} end if end for until Q is empty return (T = E) | successFound | loseFound end function"}, {"title": "BFS Validity Checker", "content": "Once the output is confirmed to be valid JSON, we perform a BFS-based validity check (Algorithm 1). Based on our event-state design, we employ BFS to decide if a game is valid. Starting from the initial state, we repeatedly check which events are available, apply success or failure effects accordingly, and track whether at least one success and one losing state can be reached. We stop when no new states can be discovered or when the search exceeds 10,000,000 states. A game is valid if every event is triggered at least once, and both success and losing termination conditions are achievable. Metrics For GC evaluation, we report the format-check pass rate (FCR) and the valid-check pass rate (VCR) as our main metrics, reflecting how reliably LLMs follow the prescribed JSON format and produce valid game mechanics. In order to examine fine-grained failures for the validity check, we include three additional ratios: w. Success = $\\frac{\\text{# games with successFound}}{\\text{# games pass the format check}}$\nw. Lose = $\\frac{\\text{# games with failFound}}{\\text{# games pass the format check}}$\nReachability = $\\frac{\\text{# games without unreachable events}}{\\text{# games pass the format check}}$"}, {"title": "4.2. Game Simulation Evaluation", "content": "Given a valid game, the GS task requires an LLM to simulate the game for a player. We introduce a multi-round simulation framework, based on which a comprehensive description of evaluation metrics is presented. Game Simulation Framework The simulation proceeds in multiple rounds of interaction with a (real or simulated) player. Before the first round, the LLM is given the complete game information and output instructions. Each round thereafter, the LLM outputs: 1. Event Plan: A list of events occurring this round. Each entry specifies whether the event is starting (start) or ending (end); if ending, an outcome is either success or failure. 2. Game Narration: A narrative description of the current round, concluding with three candidate actions for the player character. We prompt models to follow a play-script format for readability but do not enforce it during evaluation. 3. Game State: The updated state variables after applying effects of any events that ended this round."}, {"title": "Evaluation Metrics", "content": ""}]}