{"title": "Native vs Non-Native Language Prompting: A Comparative Analysis", "authors": ["Mohamed Bayan Kmainasi", "Rakif Khan", "Ali Ezzat Shahroor", "Boushra Bendou", "Maram Hasanain", "Firoj Alam"], "abstract": "Large language models (LLMs) have shown remarkable abilities in different fields, including standard Natural Language Processing (NLP) tasks. To elicit knowledge from LLMs, prompts play a key role, consisting of natural language instructions. Most open and closed source LLMs are trained on available labeled and unlabeled resources-digital content such as text, images, audio, and videos. Hence, these models have better knowledge for high-resourced languages but struggle with low-resourced languages. Since prompts play a crucial role in understanding their capabilities, the language used for prompts remains an important research question. Although there has been significant research in this area, it is still limited, and less has been explored for medium to low-resourced languages. In this study, we investigate different prompting strategies (native vs. non-native) on 11 different NLP tasks associated with 12 different Arabic datasets (9.7K data points). In total, we conducted 197 experiments involving 3 LLMs, 12 datasets, and 3 prompting strategies. Our findings suggest that, on average, the non-native prompt performs the best, followed by mixed and native prompts.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in LLMs have reshaped the spectrum of solving downstream NLP tasks. Prompt engineering plays a crucial role in solving the downstream task at hand. It is a process of creating instructions, providing context, and asking the model to solve the task or extract knowledge [25]. Traditionally, supervised models solved a task by taking an input x and predicting an output y as P(yx), whereas in the prompt-based approach, a prompt function fprompt (.) is applied to modify the input x into a prompt x' = fprompt(x). The final output of the LLM is then predicted from x'."}, {"title": "2 Related Work", "content": "In this section, we first provide a brief overview of prompt-based approaches, then discuss the work that focused on mono- and multilingual prompting for NLP tasks. Following that, we discuss work related to social media analysis."}, {"title": "2.1 Prompt and Techniques", "content": "LLMs have shown great capability in solving various sets of language and reasoning tasks. By carefully designing and crafting prompts, it is possible to steer LLMs towards an improved response. Prompt engineering has emerged as the field of developing and optimizing prompts as input for language models. It offers an intuitive and natural interface for humans to interact with LLMs. As models can be sensitive to small modifications of the input, therfore it is important to identify prompts that are robust and lead to high-performance results. A prompt can contain one or more components, which are often constructed as a template. Such components include: Instruction, which describes the task to be performed by the model; Context, which provides additional information to guide the model's response; Input, the content for which the solution is requested; and Output indicator, which guides the model in restricting and formatting its response. Often a role, commonly known as system prompt is also provided to the LLM. For prompting techniques, there are many approaches, here, we focus on most notable ones such as zero-shot and few-shot learning.\nZero-shot learning involves prompting LLMs without providing any specific prior training on the task or data domain. In this approach, the model uses its pre-existing knowledge to generate responses based solely on the prompt [8]. Few-shot learning, or in-context learning (ICL), involves prompting LLMS with a limited number of example inputs and outputs to improve performance. The effectiveness of ICL relies heavily on the quality and diversity of the examples used. Brown et al. (2020) show that large models like GPT-3 can effectively handle a wide range of tasks through few-shot learning, using minimal examples to produce relevant and insightful responses [8]."}, {"title": "2.2 Native vs. Non-Native Language Prompting", "content": "Understanding how LLMs respond to prompts in different languages is crucial for evaluating their generalization and reasoning capabilities. Nguyen et al. (2023) examined the use of linguistically diverse prompts to leverage LLMs' strengths in multilingual contexts, especially for low-resource languages. Their results indicated that while LLMs perform well in English-dominant tasks, further research is needed for zero-shot setups in low-resource languages like Arabic [34]. Recent studies also highlight the importance of linguistic diversity in evaluating LLMs' performance across different languages and cultural contexts [23]. Further analysis by [24] revealed that language models often exhibit varying degrees of bias and performance discrepancies when switching from high-resource languages like English to low-resource languages."}, {"title": "2.3 News and Social Media Analysis", "content": "Social media platforms empower us in several ways, from disseminating to consuming information. They are valuable for supporting citizen journalism and increasing public awareness, among other uses. While this has been a significantly positive development by enabling free speech, it has also been accompanied by the spread of harm and hostility [7,19]. To analyze content on social media, there has been a decade of research focused on identifying fake news [36], disinformation [3], fact-checking [16], and offensive, hateful and harmful content [28,13]. Since the emergence of LLMs there has been effort to benchmark LLMs for social media datasets [18,1].\nOur study contributes to the field of social and news media content analysis by exploring how prompts can be designed to detect various types of information. Specifically, we focus on how LLMs can be effectively prompted in both native and non-native languages."}, {"title": "3 Datasets", "content": "In this section, we discuss the tasks and datasets selected for this study. Our choice was inspired by analyses of social and news media, with a particular focus on Arabic content. The study includes 11 tasks associated with 12 different datasets, covering a variety of domains and text content types, such as tweets, news articles, and transcripts.\nHate Speech Detection: Hate speech is \"language used to express hatred toward a targeted group or intended to be derogatory, humiliating, or insulting to its members\" [11]. We utilized the OSACT 2020 dataset [30], which comprises a collection of tweets labeled as either hate speech or not hate speech.\nAdult Content Detection: The task typically involves detecting and identifying whether the textual content contains sensitive/adult content. We used a dataset of tweets, collected by first identifying Twitter accounts that post adult content [31]. The tweets are manually annotated as either adult or not adult.\nSpam Detection: Spam detection is another important problem, as such content can often annoy and mislead users [15]. Spam content on social media includes ads, malicious content, and low-quality content. For Arabic spam detection, we used the dataset discussed in [29], which contains a collection of tweets manually labeled as ads and not-ads.\nSubjectivity Identification: A sentence is considered subjective when it is based on, or influenced by, personal feelings, tastes, or opinions. Otherwise, the sentence is considered objective [40]. We used a dataset from the CLEF Check-That! lab [14].\nPropaganda Detection: Propaganda can be defined as a form of communication aimed at influencing people's opinions or actions toward a specific goal, using well-defined rhetorical and psychological techniques [12]. For this task, we used a dataset comprising tweet, each labeled with various propaganda techniques[4].\nCheck-worthiness Detection: Check-worthiness detection is a critical step of fact-checking systems [33] aimed at facilitating manual fact-checking efforts by prioritizing claims for fact-checkers. We used the Arabic subset of the dataset released with Task 1A (Arabic) of the CLEF2022 CheckThat lab, which includes tweets labeled as check-worthy or not check-worthy [32].\nFactuality Detection: Manual fact-checking is reliable, however, it doesn't scale well with the vast amount of online information. Therefore, automatic fact-checking systems are essential to assist human fact-checkers [33]. We experiments with two datasets: (i) the ANS dataset developed by [21] including a collection of true and false claims, sourced from Arabic News Texts corpus, and (ii) A dataset that includes tweets relevant to COVID-19, labeled by factuality [5].\nClaim Detection: This is the first step for mitigating misinformation and disinformation. A factual (verifiable) claim is a statement that can be verified using accurate information such as statistics [22]. We used the Arabic subset of the dataset released with CLEF2022 CheckThat Lab, CT-CWT-22-Claim [32].\nHarmful Content Detection: For harmful content detection, we adopted the task proposed in [5,32]. Research on harmful content detection also includes identifying offensive, hate speech, cyberbullying, violence, racist, misogynistic, and sexist content [3]. For this task, we used the dataset proposed in [32].\nAttention-worthiness Detection: On social media, people often tweet to blame authorities, provide advice, and/or call for action. It is important for policymakers to respond to these posts. This task aims to categorize such information based on whether it requires attention and which kind of it is needed. We used a subset of the dataset from Task 1D of the CLEF2022 CheckThat Lab [32]."}, {"title": "3.1 New Test Set", "content": "Each dataset is publicly available in train, development, and test splits, with the exception of a few that contain only train and test sets. As shown in Table 1, the original test sets are relatively large, totaling ~48K instances. Since our experiments involve using commercial models like GPT-4 and hosting open models such as Llama-3.1-8b, both scenarios incur costs and computational time. Therefore, we created a new test set by sampling from the original test sets. Specifically, we sampled 1,000 instances from each dataset containing more than 1,000 instances. We employed stratified sampling, such that the new test set maintains the original class label distribution present in the full datasets [26]. Such a sampling approach is a reasonable choice, as reported in a previous study [20]."}, {"title": "3.2 Datasets Stats", "content": "In Table 1, we report the distribution of the datasets associated with various tasks, which includes the number of instances in the training set, the original test set, and newly created test set. Note that we are not reporting development set as we have not used them for this study. We used the training set to select samples for few-shot learning."}, {"title": "4 Experiments", "content": "In this section, we discuss the experimental details, which include models, different prompt structures (a main focus of this study), zero- and few-shot prompting, model parameters, post-processing of the model's output, and evaluation metrics."}, {"title": "4.1 Models", "content": "For the experiments, we used both commercial and open-sourced models including GPT-40 [35], Llama-3.1-8b [41]2, and Jais-13b-chat [38]. The choice of these models is driven by their distinct strengths and suitability for multilingual and Arabic-centric applications. GPT-40 and Llama-3.1-8b are state-of-the-art multilingual models where English is the dominant language; however, due to their extensive training on diverse and large-scale datasets, they exhibit exceptional performance across various languages, including Arabic. On the other hand, Jais-13b is an Arabic-centric model specifically designed and trained to handle the nuances and complexities of the Arabic language. Although the Jais-13b model is claimed to be Arabic-centric, a large part (59%) of its training dataset contains English, and most of its instruction tuning is translated from English. Therefore, it inherits significant knowledge from English. Note that for Llama-3.1 and Jais-13b we used instruction (chat) version of the model."}, {"title": "4.2 Prompt Formulation", "content": "For our study, we defined three different prompts to compare native versus non-native prompt structures. We used Arabic as the native language since the input is in Arabic, and English as the non-native language. Formally, let Ia and Ie represent the native and non-native instructions, respectively. The input is denoted as x. The output labels within the instructions for the native and non-native languages are La and L, respectively. Finally, the output labels are denoted as ya and ye. The three different prompt structures are defined as follows: (i) Native: Ia + x + La, (ii) Non-native: Ie + x + L, (iii) Mixed: Ia + x + L.\nIn Figure 1, we present examples of three different prompts, which demonstrate the three formulations mentioned above. Based on a prompt structure, the prompting to the LLMs was to obtain a label l as a response from the l\u2208 L, where L = {11,12,...,In}. The number of label n and label set L is dataset dependent. Note that the instructions, input and output are task and dataset dependent. We place La in a comma separated format in the prompt."}, {"title": "4.3 Prompting Techniques", "content": "For this study, we used widely used prompting techniques such as zero-shot and few-shot, as discussed below. For both techniques, we used the three prompt formulations discussed in the previous section.\nZero-shot For the zero-shot experiments, only prompt is provided without any additional contextual information. We designed prompts with instructions in natural language that describe the task and specify the expected label. The prompt design was inspired based on the prior work [1,27].\nFew-shot For the few-shot example selection, we used the maximal marginal relevance (MMR) method to construct example sets that are both relevant and diverse [9]. The MMR method calculates the similarity between a test example and the example pool (e.g., training set) and selects a specified number of examples (shots). We applied MMR on top of embeddings generated by multilingual sentence-transformers [37]. Our experiments were conducted using 3-shot examples."}, {"title": "4.4 Model Parameters and Post Processing", "content": "Reproducibility is a major concern for LLMs. To ensure reproducibility, we set the temperature to zero for all experiments and crafted the prompts with concise instructions. We used the LLMeBench framework for the experiments [10].\nMost often, the output of LLMs includes additional information beyond the desired output. To address this problem, a post-processing function f(\u00b7) is necessary. This function maps the raw output of the LLM, denoted as Ly, to the desired cleaned output y'. The mapping can be formally defined as: y' = f(Ly), where f(\u00b7) represents the post-processing operation applied to the LLM output Ly to obtain the refined output y'.\nFor each LLM, prompt, prompting technique, and dataset, we designed a specific post-processing function. This resulted 197 experimental setups. Given that designing these configurations is a time-consuming process, we aim to make these resources publicly available for the research community."}, {"title": "4.5 Evaluation Measures", "content": "We evaluate all models' predictions using classification metrics including weighted/macro/micro-precision, recall, and F1, which are task and dataset specific and are reported in the current SOTA [1]"}, {"title": "5 Results and Discussion", "content": "In Figures 4.5 and 4.5, we report the average performance for zero-shot and few-shot prompting, respectively. Each figure presents results for all models using three different prompts: Native, Non-native, and Mixed. The average results for Jais are partially incomplete and might not be equally comparable. Due to the computational resource constraint we could not able to run the experiments for 3 datasets.\nIn Table 2, we provide detail results including random baseline and current SOTA performance for each dataset. The random baseline is computed by randomly assigning a label to each instance in each dataset from the label set of the corresponding dataset. The empty result for Jais is due the reason mentioned above. On average, non-native prompts performs better across zero and few-shot setup, followed by mixed and native prompts.\nCompared to the random baseline, all models outperform it, except for Jais in some setups. In certain cases, GPT-4 outperforms the SOTA results; however, overall, the results of LLMs are still far from SOTA.\nGPT-40 Performance: Across different models GPT-40 performs the best with the few-shot technique, achieving the highest performance with non-native prompts, followed closely by mixed prompts, and lastly native prompts. In the zero-shot scenario, the effect of prompt structures on performance is similar to the few-shot scenario: non-native prompts give the best results, followed by mixed, and then native. While there are differences in performance based on the prompt structures of the instructions, these differences are minimal in GPT-40, demonstrating its capability to understand context across different languages. The higher performance with non-native prompts suggests that the model has a stronger capability of the dominant language (English) it was trained on.\nLlama-3.1-8b Performance: Similar to GPT-40, Llama-3.1-8b exhibits a significant increase in performance in few-shot scenarios compared to zero-shot. In the few-shot setting, Llama performs best with mixed prompts, while in the zero-shot setting, it performs best with English prompts. Arabic prompts yield the worst results in both scenarios. This suggests that using English labels can enhance performance in English-centric LLMs, and that the language of the prompt plays a crucial role in helping the model understand the context better.\nThe overall results suggest a notable improvement in few-shot learning. However, some experiments exhibit contradictory outcomes, where performance either declines or stays the same with few-shot learning. This inconsistency can be attributed to the relevance of the retrieved few-shots. Few-shots are selected based on tweet similarity, which does not necessarily guarantee that they share the same label. Consequently, the model may face difficulties in generalizing effectively. Moreover, few-shot learning might not always offer the model enough additional information to enhance its performance.\nJais-13b Performance: Despite being Jais an Arabic-centric LLM, it shows the best results with non-native prompts. It demonstrated superior performance in few-shot learning, which implies that few-shot learning is effective with Jais. However, surprisingly Jais performed the worst with native prompts across most of the tasks.\nThe average results for few-shot learning were highest with non-native prompts, followed by mixed prompts, and lowest with native prompts. This pattern was also consistent in zero-shot scenarios. We observed that Jais understood the context better when the instructions were non-native, resulting in more reasonable outputs. In contrast, the most irrelevant results emerged from native prompts.\nError Analysis: A common issue with Jais in few-shot learning using Arabic prompts is that it sometimes mistakenly classifies the few-shot samples instead of the input sample. For example, it might output phrases like \"The classification of the first tweet is \u2026\u2026\u2026\u201d or \u201cThe overall classification for all examples, ...", "it goes against our use case policy\u201d or \u201cI am not able to predict,": "ndicating an inability to process the input correctly. Furthermore, in some datasets, Jais frequently returns only one class for the majority of samples, which does not accurately reflect the actual label distribution, highlighting a potential issue with its generalization capabilities, explaining the low performance results for Jais.\nOne issue observed with GPT-40 was that out of 1,000 sample inputs, nearly 25 resulted in an error due to the prompt triggering Azure OpenAI's content management policy, leading to a \"ResponsibleAIPolicyViolation\" error. To address this issue, we mitigated the impact by assigning a random label to these instances, ensuring the continuation of the evaluation process.\nIn comparing the models, Llama-8b-3.1 and GPT-40 consistently return responses that match the labels as explicitly prompted, adhering strictly to the instructions to return only the labels. These labels are formatted according to the language specified in the instructions. Conversely, Jais-13b often diverges from this behavior. Despite being prompted to return only the label, Jais-13b frequently includes explanations or nonsensical responses, complicating the post-processing step."}, {"title": "6 Conclusion and Future Work", "content": "In this study, we investigate different prompt structures (i.e., native, non-native, and mixed) to understand their significance in eliciting the desired output (label for downstream NLP tasks) from various commercial and open-sourced models. Our experiments consist of 197 experimental setups, featuring 12 different social and news media datasets, 3 different models, and 3 prompt structures with zero- and few-shot prompting techniques. Our findings suggest that, overall, non-native prompt perform better, followed by mixed prompt, while native prompt significantly underperform, even with the Arabic-centric Jais model. Future work include fine-tuning with instruction-following dataset to create a domain specific specialized model."}]}