{"title": "Developing an End-to-End Framework for Predicting the Social Communication Severity Scores of Children with Autism Spectrum Disorder", "authors": ["Jihyun Mun", "Sunhee Kim", "Minhwa Chung"], "abstract": "Autism Spectrum Disorder (ASD) is a lifelong condition that significantly influencing an individual's communication abilities and their social interactions. Early diagnosis and intervention are critical due to the profound impact of ASD's characteristic behaviors on foundational developmental stages. However, limitations of standardized diagnostic tools necessitate the development of objective and precise diagnostic methodologies. This paper proposes an end-to-end framework for automatically predicting the social communication severity of children with ASD from raw speech data. This framework incorporates an automatic speech recognition model, fine-tuned with speech data from children with ASD, followed by the application of fine-tuned pre-trained language models to generate a final prediction score. Achieving a Pearson Correlation Coefficient of 0.6566 with human-rated scores, the proposed method showcases its potential as an accessible and objective tool for the assessment of ASD.", "sections": [{"title": "1. Introduction", "content": "Autism Spectrum Disorder (ASD) is defined as a lifelong condition that significantly affects an individual's communication abilities and their interaction within society [1]. Children with ASD experience social deficits, communication difficulties, and atypical behavior patterns, including impaired socio-communicative interactions and a limited range of interests and activities [1, 2].\nEarly diagnosis and intervention are critical due to the profound impact of ASD's symptomatic behaviors on foundational developmental processes. Early intervention is particularly pivotal for social development, as initial social capabilities and deficits inform intervention outcomes and treatment strategies [3, 4]. In clinical environments, standardized diagnostic tools like the Autism Diagnostic Observation Schedule, 2nd edition (ADOS-2), are employed [5]. However, the use of standardized tools for evaluating children presents numerous challenges, including expertise scarcity leading to delayed or overlooked diagnoses [6], potential bias from subjective interpretations by caregivers or evaluators [7], and the extended duration of the evaluation process, which can burden both children and their caregivers and may reduce the children's concentration. Consequently, there is a pressing need for developing objective and precise methodologies which diagnose and predict severity for early diagnosis and intervention of ASD [8, 9].\nRecent advancements in automated methods for predicting ASD severity incorporate a range of technologies, including MRI [10, 11, 12], fMRI [13], EEG signals [9, 14], and genetic and environmental factors [15]. Despite their efficacy, these methods often require specialized equipment and expertise, presenting barriers to widespread adoption [6]. In contrast, speech data offers a more accessible and less intrusive alternative [16], providing a viable option for diagnosing and assessing the severity of ASD. Studies have concentrated on the pragmatic aspects of language, including the appropriate use of language across various social contexts, particularly in children with ASD in comparison to their typically developing (TD) peers [17, 18, 19]. They underscored that children with ASD frequently exhibit atypical language behaviors in social contexts, thereby emphasizing the complex relationship between linguistic and social challenges. The utilization of speech data not only circumvents the limitations associated with other diagnostic materials but also leverages the unique linguistic characteristics of children with ASD. This underscores the potential of linguistic materials for the automated diagnosis and severity prediction of ASD [20, 21, 22], offering a promising direction for enhancing accessibility and reducing the reliance on extensive resources and specialized knowledge.\nMachine learning techniques have been applied to identify ASD based on linguistic indicators [20, 21], with traditional methods requiring meticulous feature selection, a process that is time-intensive and highly specialized [23]. Deep learning approaches offer an alternative by deriving more abstract representations [24], such as using lexical embeddings from a fine-tuned BERT model for ASD diagnosis [22]. However, deep learning models necessitate large datasets, which poses a challenge for ASD research due to the typically small available datasets. Pre-trained language models (PLMs), fine-tuned on specific tasks, leverage extensive pre-training corpora to mitigate this issue [25].\nA notable concern when applying PLMs to classification tasks is the potential misalignment between the objectives during pre-training and fine-tuning [26]. The integration of natural language prompts in fine-tuning PLMs, a technique known as prompt tuning, aligns the model's objectives with those of the pre-training phase, thereby enhancing performance on specific tasks in the context of limited data [26, 27].\nBuilding on recent methodological advancements and leveraging the distinctive benefits of prompt tuning in contexts with limited data, this paper proposes an end-to-end (E2E) framework that incorporates a prompt tuning methodology for predicting the severity of social communication in children with ASD. The deployment of prompt tuning methodologies necessitates the transcription of audio recordings. However, manual transcription presents several challenges, including high costs, limited availability, and issues with scalability. To overcome these challenges, we integrate an Automatic Speech Recognition (ASR) model into our framework, enabling the derivation of final prediction scores directly from raw speech data. The comprehensive framework utilizes an ASR model, specifically fine-tuned with speech data from children with ASD, followed by the application of fine-tuned PLMs and an ensemble method to generate a final prediction score.\nThe remainder of the paper is organized as follows: Section 2 details the methodologies employed, Section 3 outlines the experimental setup, Section 4 presents the results, Section 5 discusses the findings, and Section 6 concludes the study."}, {"title": "2. Methods", "content": "This study introduces an E2E framework that incorporates fine-tuned ASR models, fine-tuned PLMs, and a seed ensemble method for predicting the social communication severity scores in children with ASD, as depicted in Figure 1.\n2.1. Automatic Speech Recognition Model\nWe selected two pre-trained multilingual ASR models for this purpose: wav2vec2-xls-r-300m [28] and whisper-large-v2 [29]. To tailor these models to the nuances of speech from TD children and children with ASD, we fine-tune each model using speech data specific to these groups.\n2.2. Fine-tuning Pre-trained Language Models\nThe study further involves fine-tuning three PLMs-KR-BERT [30], KLUE/roberta-base [31], and KR-ELECTRA-Discriminator [32]-employing three distinct approaches: traditional fine-tuning, manual prompting, and p-tuning. These models were chosen for prompt-based fine-tuning due to their demonstrated effectiveness in text classification tasks, as evidenced by prior research [26, 33, 34]. Training incorporates ten different initialization seeds to increase robustness and mitigate the effects of random initialization.\n2.2.1. Traditional Fine-tuning\nFine-tuning adapts a model pre-trained on a vast dataset to a smaller, task-specific dataset [35], effectively leveraging the extensive knowledge acquired during pre-training [25] for specific downstream tasks. In this process, a regression head is attached to the model. The [CLS] token, representing the input sequence comprehensively, facilitates the prediction of a continuous severity score.\n2.2.2. Manual Prompting\nManual prompting involves crafting specific input prompts to direct the behavior of transformer models towards generating desired outputs. By designing appropriate prompts, it's possible to utilize the extensive knowledge embedded in these models for performing specific tasks without additional task-specific training. In this approach, a regression head initializes the models, and a template guides the model to focus on predicting the social communication severity score from the input text.\n2.2.3. Automated Prompting: P-Tuning\nP-tuning, introduced by [36], advances beyond manual prompting by parameterizing prompts and optimizing them alongside the model's parameters during fine-tuning, allowing the model to autonomously identify the most effective prompts for a task. For this study, the p-tuning approach is implemented using the PEFT library [37]. Models are initialized with a regression head, and virtual tokens are incorporated and tuned specifically for the task, optimizing the models' predictions.\n2.3. Seed Ensemble for Robust Prediction\nTo mitigate the variability introduced by the randomness in model initialization and to improve the overall performance, a seed ensemble technique is employed. For each PLM, we aggregate the predictions from the ten individually fine-tuned models (one per seed) to formulate a singular and more accurate prediction."}, {"title": "3. Experiments", "content": "3.1. Data Preparation and Dataset Description\nThe speech samples were collected during linguistic assessment sessions conducted by certified speech-language pathologists (SLPs). The specifics of the data collection, transcription, and evaluation processes have been detailed in [38]. This study utilized speech data from 168 children diagnosed with ASD and 40 TD children. These participants were integral for fine-tuning the ASR models. Specifically, the ASD cohort included 103 children whose social communication severity was evaluated by three certified SLPs. The average of the three SLPs' evaluations served as the severity score for the ASD children, while TD children were assigned a baseline score of zero. The datasets for evaluated ASD and TD children were employed for fine-tuning the PLMs. The overall dataset is described in Table 1. To ensure no overlap and maintain the integrity of the evaluation process, children included in the test set for PLM fine-tuning were excluded from the training dataset of the ASR model.\n3.2. Fine-Tuning ASR Models\nThe ASR models, specifically wav2vec2 and whisper, are fine-tuned using Fairseq and Hugging Face's Transformers, respectively. The Adam optimizer is utilized in both cases, with initial learning rates set to 3e-4 for wav2vec2 and 1e-5 for whisper. Given that Korean is a syllable-timed language, the performance of the fine-tuned models is evaluated using the syllable error rate (SER), achieving rates of 26.21% and 19.57%, respectively, after fine-tuning."}, {"title": "3.3. Fine-Tuning PLMs", "content": "For each tuning method, training spanned 40 epochs, utilizing a learning rate of 1e-5, a batch size of 8, and the AdamW optimizer. The mean squared error is employed as the objective loss function. In manual prompting, the template \"[text] the social communication severity score of the speaker is [MASK]\" is used, with \"[text]\" replaced by actual dataset text and \"[MASK]\" serving as a placeholder. In p-tuning, experiments are conducted with 5, 10, 15, and 20 virtual tokens, setting the encoder's hidden size to 128. Differential learning rates are applied: 1e-5 for both the base models and the prompt encoder, and 1e-3 for the regression head.\n3.4. Evaluation Metrics\nThe evaluation strategy includes two settings:\n1. Full-set setting, where all available training data is used, reserving 20% for validation.\n2. Low-resource setting, where only 20% of the full training data is accessible, following the methodology outlined by [34].\nThe evaluation metric employed is the Pearson Correlation Coefficient (PCC), which measures the relationship between the model's predicted output and the scores labeled by humans. To mitigate the effects of random initialization, each system's evaluation is executed ten times, each with a different random seed from PyTorch's random initialization setting. The final prediction is determined using the seed ensemble method."}, {"title": "4. Results", "content": "The study evaluates the effectiveness of the proposed framework, which integrates various ASR models, transcription types, PLMs, and tuning methods in predicting social communication severity in children with ASD across full-set and low-resource settings. The comprehensive results of our experiments are shown in Table 2.\nAs expected, human transcriptions consistently outperform ASR transcriptions. However, certain combinations of PLMs and tuning methods, specifically klue/roberta-base with p-tuning, reveal instances where ASR transcriptions surpass human transcriptions. In low-resource settings, the performance gap between human and ASR transcriptions diminishes, highlighting the potential of ASR transcriptions in scenarios of limited data availability. Remarkably, wav2vec2 transcription outperforms human transcription in specific cases when klue/roberta-base model is p-tuned, indicating a strong correlation with human-labeled scores (e.g., PCC of 0.6566 compared to 0.6216 with 20 virtual tokens). When comparing two ASR models, wav2vec2 transcriptions generally exhibit better performance than those from the whisper model, despite a higher syllable error rate.\nThe results demonstrate that the choice of PLM and the tuning method significantly affects the performance in predicting the severity score of social communication. In scenarios involving both ASR and human transcriptions within the full-set setting, fine-tuning and manual prompting tend to outperform p-tuning for the KR-BERT and KR-ELECTRA-Discriminator models. However, p-tuning shows superior performance with the klue/roberta-base model. This trend continues in the low-resource setting, where p-tuning enhances performance with human transcriptions for the KR-ELECTRA-Discriminator model.\nAdditionally, performance varies significantly based on the number of virtual tokens utilized in p-tuning. For example, with the KR-BERT model using ASR transcriptions, the PCC values range from negative to positive, indicating a shift from a negative to a moderate correlation with human-labeled scores. Similarly, with the KR-BERT model using human transcriptions in a low-resource setting, the correlation varies significantly from weak to moderate."}, {"title": "5. Discussion", "content": "The results highlight a complex relationship between transcription types, PLM selection, tuning methods, and data availability in the automated assessment of ASD severity.\nThe diminishing performance disparity between human and ASR transcriptions in low-resource settings underscores the proposed method's potential in enhancing the accessibility and scalability of ASD severity assessment. This trend suggests that ASR technology may serve as a feasible alternative to human transcription in situations where resources are limited.\nThe generally better performance of the wav2vec2 model over the whisper model, despite the latter's lower error rate, indicate that there are aspects of speech relevant to ASD severity that are captured by wav2vec2 but ignored by whisper due to its disfluency removal. It is known that children with ASD display various types of speech disfluencies, such as sound and syllable repetitions, interjections, within-word breaks, and final sound prolongations [39]. The whisper model's tendency to eliminate speech disfluencies, including filler words, hesitations, and repetitions [40], contrasts with the wav2vec2 model's capability to detect disfluencies or stuttering. Therefore, accurately capturing the characteristics of ASD speech, including speech disfluencies, necessitates the selection of an appropriate ASR model that retains these critical speech features. This consideration is pivotal in developing effective diagnostic tools and interventions for ASD, highlighting the importance of choosing an ASR model that aligns with the nuanced requirements of ASD speech.\nThe varied performance across PLMs under different tuning methods highlights the necessity of meticulous consideration for each PLM-tuning combination. The klue/roberta-base model's effective response to p-tuning, across both transcription types, suggests its potential as a powerful tool in optimizing PLMs, particularly in data-constrained environments. Additionally, the number of virtual token significantly influences performance differences. Although the number of prompt tokens greatly impacts few-shot performance, a larger number of prompt tokens is not always better; it depends on the amount of training data [36]. In practice, we should determine the optimal number of prompt tokens through model selection, highlighting the need for careful consideration of tuning settings."}, {"title": "6. Conclusion", "content": "This study proposes an E2E framework, incorporating fine-tuned ASR models and PLMs, for automatically predicting social communication severity in children with ASD. Demonstrating a PCC of 0.6566, the experimental results affirm the framework's utility, especially in data-limited situations.\nKey contributions of this paper include the introduction of an automated method for predicting the social communication severity score in children with ASD from raw speech data, the development of an E2E framework that eliminates the need for human transcription, and the validation of this frame-"}]}