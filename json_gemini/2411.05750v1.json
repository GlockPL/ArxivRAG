{"title": "On Differentially Private String Distances", "authors": ["Jerry Yao-Chieh Hu", "Erzhi Liu", "Han Liu", "Zhao Song", "Lichen Zhang"], "abstract": "Given a database of bit strings $A_1, ..., A_m \\in \\{0,1\\}^n$, a fundamental data structure task is to estimate the distances between a given query $B\\in \\{0,1\\}^n$ with all the strings in the database. In addition, one might further want to ensure the integrity of the database by releasing these distance statistics in a secure manner. In this work, we propose differentially private (DP) data structures for this type of tasks, with a focus on Hamming and edit distance. On top of the strong privacy guarantees, our data structures are also time- and space-efficient. In particular, our data structure is e-DP against any sequence of queries of arbitrary length, and for any query B such that the maximum distance to any string in the database is at most k, we output m distance estimates. Moreover,\n\u2022 For Hamming distance, our data structure answers any query in $\\tilde{O}(mk + n)$ time and each estimate deviates from the true distance by at most $O(k/e^{\\epsilon/\\log k})$;\n\u2022 For edit distance, our data structure answers any query in $\\tilde{O}(mk^2 + n)$ time and each estimate deviates from the true distance by at most $O(k/e^{\\epsilon/(\\log k \\log n)})$.\nFor moderate k, both data structures support sublinear query operations. We obtain these results via a novel adaptation of the randomized response technique as a bit flipping procedure, applied to the sketched strings.", "sections": [{"title": "1 Introduction", "content": "Estimating string distances is one of the most fundamental problems in computer science and information theory, with rich applications in high-dimensional geometry, computational biology and machine learning. The problem could be generically formulated as follows: given a collection of strings $A_1,..., A_m \\in \\Sigma^n$ where $\\Sigma$ is the alphabet, the goal is to design a data structure to preprocess these strings such that when a query $B\\in \\Sigma^n$ is given, the data structure needs to quickly output estimates of $||A_i - B||$ for all $i \\in [m]$, where $||\\cdot||$ is the distance of interest. Assuming the symbols in $\\Sigma$ can allow constant time access and operations, a na\u00efve implementation would be to simply compute all the distances between $A_i$'s and B, which would require $O(mn)$ time. Designing data structures with $o(mn)$ query time has been the driving research direction in string distance estimations. To make the discussion concrete, in this work we will focus on binary alphabet ($\\Sigma = \\{0,1\\}$) and for distance, we will study Hamming and edit distance. Hamming distance [Ham50] is one of the most natural distance measurements for binary strings, with its deep root in error detecting and correction for codes. It finds large array of applications in database similarity searches [IM98, Cha02, NPF12] and clustering algorithms [Hua97, HN99].\nCompared to Hamming distance, edit distance or the Levenshtein distance [Lev66] could be viewed as a more robust distance measurement for strings: it counts the minimum number of operations (including insertion, deletion and substitution) to transform from A to B. To see the robustness compared to Hamming distance, consider $A_i = (01)^n$ and $B = (10)^n$, the Hamming distance between these two strings is n, but $A_i$ could be easily transformed to B by deleting the first bit and adding a 0 to the end, yielding an edit distance of 2. Due to its flexibility, edit distance is particularly useful for sequence alignment in computational biology [WHZ+15, YFA21, BWY21], measuring text similarity [Nav01, SGAM+15] and natural language processing, speech recognition [FARL06, DA10] and time series analysis [Mar09, GS18].\nIn addition to data structures with fast query times, another important consideration is to ensure the database is secure. Consider the scenario where the database consists of private medical data of m patients, where each of the $A_i$ is the characteristic vector of n different symptoms. A malicious adversary might attempt to count the number of symptoms each patient has by querying $0^n$, or detecting whether patient i has symptom j by querying $e_j$ and $0^n$ where $e_j$ is the j-th standard basis in $R^n$. It is hence crucial to curate a private scheme so that the adversary cannot distinguish the case whether the patient has symptom j or not. This notion of privacy has been precisely captured by differential privacy [Dwo06, DKM+06], which states that for neighboring databases\u00b9,\nthe output distribution of the data structure query should be close with high probability, hence any adversary cannot distinguishable between the two cases.\nMotivated by both privacy and efficiency concerns, we ask the following natural question:\nIs it possible to design a data structures to estimate Hamming and edit distance, that are both\ndifferentially private, and time/space-efficient?\nWe provide an affirmative answer to the above question, with the main results summarized in the following two theorems. We will use $D_{ham}(A, B)$ to denote the Hamming distance between A and B, and $D_{edit}(A, B)$ to denote the edit distance between A and B. We also say a data structure is e-DP if it provides e-DP outputs against any sequence of queries, of arbitrary length.\nTheorem 1.1. Let $A_1,..., A_m \\in \\{0,1\\}^n$ be a database, $k \\in [n]$ and $\\epsilon > 0,\\beta \\in (0,1)$, then there exists a randomized algorithm with the following guarantees:"}, {"title": "2 Related Work", "content": "Differential Privacy. Differential privacy is a ubiquitous notion for protecting the privacy of database. [DKM+06] first introduced this concept, which characterizes a class of algorithms such that when inputs are two neighboring datasets, with high probability the output distributions are similar. Differential privacy has a wide range of applications in general machine learning [CM08, WM10, JE19, TF20], training deep neural networks [ACG+16, BPS19], computer vision [ZYCW20, LWAL21, TKP19], natural language processing [YDW+21, WK18], large language models [GSY23, YNB+22], label protect [YSY+22], multiple data release [WYY+22], federated learning [SYY+23, SWYZ23] and peer review [DKWS22]. In recent years, differential privacy has been playing an important role for data structure design, both in making these data structures robust against adaptive adversary [BKM+22, HKM+22, SYYZ23, CSW+23] and in the function release communication model [HRW13, HR14, WJF+16, AR17, CS21, WNM23, BLM+24, LHR+24].\nHamming Distance and Edit Distance. Given bit strings A and B, many distance measure- ments have been proposed that capture various characteristics of bit strings. Hamming distance was first studied by Hamming [Ham50] in the context of error correction for codes. From an algorithmic perspective, Hamming distance is mostly studied in the context of approximate nearest-neighbor search and locality-sensitive hashing [IM98, Cha02]. When it is known that the query B has the property $D_{ham}(A, B) \\leq k$, [PL07] shows how to construct a sketch of size $O(k)$ in $O(n)$ time, and with high probability, these sketches preserve Hamming distance. Edit distance, proposed by Levenshtein [Lev66], is a more robust notion of distance between bit strings. It has applications in computational biology [WHZ+15, YFA21, BWY21], text similarity [Nav01, SGAM+15] and speech recognition [FARL06, DA10]. From a computational perspective, it is known that under the Strong Exponential Time Hypothesis (SETH), no algorithm can solve edit distance in $O(n^{2-o(1)})$ time, even its approximate variants [BZ16, CGK16b, CGK16a, NSS17, RSSS19, RS20, GRS20, JNW21, BEG+21, KPS21, BK23, KS24]. Hence, various assumptions have been imposed to enable more efficient algorithm design. The most related assumption to us is that $D_{edit}(A, B) \\leq k$, and in this regime various algorithms have been proposed [Ukk85, Mye86, LV88, GKS19, KS20, GKKS23]. Under SETH, it has been shown that the optimal dependence on n and k is $O(n + k^2)$, up to sub-polynomial factors [GKKS23]."}, {"title": "3 Preliminary", "content": "Let E be an event, we use $1[E]$ to denote the indicator variable if E is true. Given two length-n bit strings A and B, we use $D_{ham}(A, B)$ to denote $\\Sigma_{i=1}^n 1[A_i = B_i]$. We use $D_{edit}(A, B)$ to denote the edit distance between A and B, i.e., the minimum number of operations to transform A to B where the allowed operations are insertion, deletion and substitution. We use $\\oplus$ to denote the XOR operation. For any positive integer n, we use $[n]$ to denote the set $\\{1,2,\\dots,n\\}$. We use $Pr[\\cdot]$, $E[\\cdot]$ and $Var[\\cdot]$ to denote probability, expectation and variance respectively."}, {"title": "3.1 Concentration Bounds", "content": "We will mainly use two concentration inequalities in this paper.\nLemma 3.1 (Chebyshev's Inequality). Let X be a random variable with $0 < Var[X] < \\infty$. For any real number $t > 0$,\n$Pr[|X \u2013 E[X]| > t] < \\frac{Var[X]}{t^2}$"}, {"title": "3.2 Differential Privacy", "content": "Differential privacy (DP) is the key privacy measure we will be trying to craft our algorithm to possess it. In this paper, we will solely focus on pure DP ($\\epsilon$-DP).\nDefinition 3.3 ($\\epsilon$-Differential Privacy). We say an algorithm A is $\\epsilon$-differentially private ($\\epsilon$-DP) if for any two neighboring databases $D_1$ and $D_2$ and any subsets of possible outputs S, we have\n$Pr[A(D_1) \\in S] < e^{\\epsilon} \\cdot Pr[A(D_2) \\in S]$,\nwhere the probability is taken over the randomness of A.\nSince we will be designing data structures, we will work with the function release communication model [HRW13] where the goal is to release a function that is $\\epsilon$-DP against any sequence of queries of arbitrary length.\nDefinition 3.4 ($\\epsilon$-DP Data Structure). We say a data structure A is $\\epsilon$-DP, if A is $\\epsilon$-DP against any sequence of queries of arbitrary length. In other words, the curator will release an $\\epsilon$-DP description of a function $\\hat{f}(\\cdot)$ without seeing any query in advance.\nFinally, we will be utilizing the post-processing property of $\\epsilon$-DP.\nLemma 3.5 (Post-Processing). Let A be $\\epsilon$-DP, then for any deterministic or randomized function g that only depends on the output of A, g \\circ A is also $\\epsilon$-DP."}, {"title": "4 Differentially Private Hamming Distance Data Structure", "content": "To start off, we introduce our data structure for differentially private Hamming distance. In par- ticular, we will adapt a data structure due to [PL07]: this data structure computes a sketch of length $O(k)$ bit string to both the database and query, then with high probability, one could re- trieve the Hamming distance from these sketches. Since the resulting sketch is also a bit string, a natural idea is to inject Laplace noise on each coordinate of the sketch. Since for two neighboring databases, only one coordinate would change, we could add Laplace noise of scale $1/\\epsilon$ to achieve $\\epsilon$-DP. However, this approach has a critical issue: one could show that with high probability, the magnitude of each noise is roughly $O(\\epsilon^{-1} \\log k)$, aggregating the k coordinates of the sketch, this leads to a total error of $O(\\epsilon^{-1}k \\log k)$. To decrease this error to $O(1)$, one would have to choose $\\epsilon = k \\log k$, which is too large for most applications.\nInstead of Laplace noises, we present a novel scheme that flips each bit of the sketch with certain probability. Our main contribution is to show that this simple scheme, while produces a biased estimator, the error is only $O(k/e^{\\epsilon/\\log k})$. Let $t = \\log k/\\epsilon$, we see that the Laplace mechanism has an error of $O(t^{-1}k)$ and our error is only $O(e^{-tk})$, which is exponentially small! In what follows, we will describe a data structure when the database is only one string A and with constant success probability, and we will discuss how to extend it to m bit strings, and how to boost the success probability to $1 - \\beta$ for any $\\beta > 0$. We summarize the main result below."}, {"title": "4.1 Time Complexity", "content": "Note that both the initializing and query run ENCODE (Algorithm 1) exactly once, we show that the running time of ENCODE is $O(n \\log k)$.\nLemma 4.2. Given $M_1 = O(\\log k)$, the running time of ENCODE (Algorithm 1) is $O(n \\log k)$.\nProof. In ENCODE, for each character in the input string, the algorithm iterate $M_1$ times. Therefore the total time complexity is $O(n \\cdot M_1) = O(n \\log k)."}, {"title": "4.2 Privacy Guarantee", "content": "Next we prove our data structure is $\\epsilon$-DP.\nLemma 4.3. Let A and A' be two strings that differ on only one position. Let A(A) and A(A') be the output of INIT (Algorithm 1) given A and A'. For any output S, we have:\n$Pr[\\mathcal{A}(A) = S] \\le e^{\\epsilon} \\cdot Pr[\\mathcal{A}(A') = S]$.\nWe defer the proof to Appendix A."}, {"title": "4.3 Utility Guarantee", "content": "The utility analysis is much more involved than privacy and runtime analysis. We defer the proofs to the appendix, while stating key lemmas.\nWe first consider the distance between sketches of A and B without the random flipping process.\nLet E(A), E(B) be ENCODE(A) and ENCODE(B). We prove with probability 0.99, $D_{ham}(A, B) = 0.5 \\cdot \\Sigma_{j=1}^{M_2} \\text{max}_{i \\in [M_1]}(\\Sigma_{c=1}^{M_3}|E(A)_{i,j,c} \u2013 E(B)_{i,j,c}|)$. Before we present the error guarantee, we will first introduce two technical lemmas. If we let $T = \\{p \\subset [n] | A_p \\neq B_p\\}$ denote the set of \u201cbad\u201d coordinates, then for each coordinate in the sketch, it only contains a few bad coordinates.\nLemma 4.4. Define set $T := \\{p \\in [n] | A_p \\neq B_p\\}$. Define set $T_j := \\{p \\subset T | h(p) = j\\}$. When $M_2 = 2k$, with probability 0.99, for all $j \\in [M_2]$, we have $|T_j| \\leq 10 \\log k$, i.e.,\n$Pr[\\forall j \\in [M_2], |T_j| \\leq 10 \\log k] \\geq 0.99$.\nThe next lemma shows that with high probability, the second level hashing g will hash bad coordinates to distinct buckets.\nLemma 4.5. When $M_1 = 10 \\log k$, $M_2 = 2k$, $M_3 = 400 \\log^2 k$, with probability 0.98, for all $j \\in [M_2]$, there is at least one $i \\in [M_1]$, such that all values in $\\{g(2(p - 1) + A_p, i) | p \\in T_j\\} \\cup \\{g(2(p - 1) + B_p, i) | p \\in T_j\\}$ are distinct.\nWith these two lemmas in hand, we are in the position to prove the error bound before the random bit flipping process.\nLemma 4.6. Let E(A), E(B) be the output of ENCODE(A) and ENCODE(B). With probability 0.98, $D_{ham}(A, B) = 0.5 \\cdot \\Sigma_{j=1}^{M_2} \\text{max}_{i \\in [M_1]}(\\Sigma_{c=1}^{M_3}|E(A)_{i,j,c} \u2013 E(B)_{i,j,c}|)$.\nOur final result provides utility guarantees for Algorithm 1.\nLemma 4.7. Let z be $D_{ham}(A, B)$, $\\tilde{z}$ be the output of QUERY(B) (Algorithm 1). With probability 0.98, $|z \u2013 \\tilde{z}| \\leq O(k \\log^3 k/e^{\\epsilon/\\log k})$."}, {"title": "5 Differentially Private Edit Distance Data Structure", "content": "Our algorithm for edit distance follows from the dynamic programming method introduced by [Ukk85, LMS98, LV88, Mye86]. We note that a key procedure in these algorithms is a subroutine to estimate longest common prefix (LCP) between two strings A and B and their substrings. We design an $\\epsilon$-DP data structure for LCP based on our $\\epsilon$-DP Hamming distance data structure. Due to space limitation, we defer the details of the DP-LCP data structure to Appendix B. In the following discussion, we will assume access to a DP-LCP data structure with the following guarantees:\nTheorem 5.1. Given a string A of length n. There exists an $\\epsilon$-DP data structure DPLCP (Algorithm 3 and Algorithm 4) supporting the following operations\n\u2022 INIT($A \\in \\{0,1\\}^n$): It preprocesses an input string A. This procedure takes $O(n(\\log k + \\log \\log n))$ time.\n\u2022 INITQUERY($B \\in \\{0,1\\}^n$): It preprocesses an input query string B. This procedure take $O(n(\\log k + \\log \\log n))$ time.\n\u2022 QUERY(i, j): Let w be the longest common prefix of A[i : n] and B[j : n] and $\\tilde{w}$ be the output of QUERY(i, j), With probability $1-1/(300k^2)$, we have: 1). $\\tilde{w} \\geq w$; 2). $E[D_{ham}(A[i : i+\\tilde{w}], B[j : j+w])] \\leq O((\\log k + \\log \\log n)/e^{\\epsilon/(\\log k \\log n)})$. This procedure takes $O(\\log^2 n (\\log k + \\log \\log n))$ time.\nWe will be basing our edit distance data structure on the following result, which achieves the optimal dependence on n and k assuming SETH:\nLemma 5.2 ([LMS98]). Given two strings A and B of length n. If the edit distance between A and B is no more than k, there is an algorithm which computes the edit distance between A and B in time $O(k^2 + n)$.\nWe start from a na\u00efve dynamic programming approach. Define D(i, j) to be the edit distance between string A[1 : i] and B[1 : j]. We could try to match A[i] and B[j] by inserting, deleting and substituting, which yields the following recurrence:\n$D(i, j) = \\text{min} \\begin{cases}\nD(i - 1, j) + 1 & \\text{if } i > 0; \\\\\nD(i, j - 1) + 1 & \\text{if } j > 0; \\\\\nD(i - 1, j - 1) + 1[A[i] \\neq B[j]] & \\text{if } i, j > 0.\n\\end{cases}$\nThe edit distance between A and B is then captured by D(n,n). When k < n, for all D(i,j) such that $|i - j| > k$, because the length difference between A[1 : i] and B[1 : j] is greater than k, D(i,j) > k. Since the final answer D(n,n) \u2264 k, those positions with $|i - j| > k$ won't affect D(n, n). Therefore, we only need to consider the set $\\{D(i, j) : |i \u2013 j| \\leq k\\}$.\nFor $d\\in [-k,k],r \\in [0,k]$, we define $F(r,d) = \\text{max}_{i}\\{i : D(i,i + d) = r\\}$ and let $LCP(i, j)$ denote the length of the longest common prefix of A[i : n] and B[j : n]. The algorithm of [LMS98] defines EXTEND(r, d) := F(r, d) + LCP(F(r, d), F(r, d) + d). We have\n$F(r, d) = \\text{max} \\begin{cases}\nEXTEND(r - 1, d) + 1 & \\text{if } r - 1 > 0; \\\\\nEXTEND(r - 1, d - 1) & \\text{if } d - 1 > -k, r - 1 > 0; \\\\\nEXTEND(r - 1, d + 1) + 1 & \\text{if } d + 1, r + 1 < k.\n\\end{cases}$\nThe edit distance between A and B equals $\\text{min}_{r}\\{r : F(r, 0) = n\\}$.\nTo implement LCP, [LMS98] uses a suffix tree data structure with initialization time O(n) and query time O(1), thus the total time complexity is $O(k^2 + n)$. In place of their suffix tree data structure, we use our DP-LCP data structure (Theorem 5.1). This leads to Algorithm 2."}, {"title": "5.1 Time Complexity", "content": "We prove the time complexity of INIT and QUERY respectively.\nLemma 5.4. The running time of INIT (Algorithm 2) is $O(n(\\log k + \\log \\log n))$.\nProof. The INIT runs DPLCP.INIT. From Theorem 5.1, the init time is $O(n(\\log k+\\log \\log n))."}, {"title": "5.2 Privacy Guarantee", "content": "Lemma 5.6. The data structure DPEDITDISTANCE (Algorithm 2) is $\\epsilon$-DP.\nProof. The data structure only stores a DPLCP(Algorithm 3, 4). From Theorem 5.1 and the post-processing property (Lemma 3.5), it is $\\epsilon$-DP."}, {"title": "5.3 Utility Guarantee", "content": "Before analyzing the error of the output of QUERY (Algorithm 2), we first introduce a lemma:\nLemma 5.7. Let A, B be two strings. Let LCP(i,d) be the length of the true longest common prefix of A[i : n] and B[i + d : n]. For $i_1 \\leq i_2, d \\in [-k, k]$, we have $i_1 + LCP(i_1, d) < i_2 + LCP(i_2, d)$.\nProof. Let $w_1 = LCP(i_1, d), w_2 = LCP(i_2, d)$. Then for $j \\in [i_1, i_1 + w_1 - 1]$, $A[j] = B[j + d]$. On the other side, $w_2$ is the length of the longest common prefix for A[$i_2$ : n] and B[$i_2$ + d : n]. So A[$i_2 + w_2$] $\\neq$ B[$i_2 + w_2$ + d]. Therefore, ($i_2 + w_2$) $\\notin$ [$i_1, i_1 + w_1 - 1$]. Since $i_2 + w_2 \\geq i_2 \\geq i_1$, we have $i_2 + w_2 \\geq i_1 + w_1$.\nLemma 5.8. Let r be the output of QUERY (Algorithm 2), $\\bar{r}$ be the true edit distance $D_{edit}(A, B)$. With probability 0.99, we have $|r \u2013 \\bar{r}| \\le O(k(\\log k + \\log \\log n)/(1 + e^{\\epsilon/(\\log k \\log n)}))$.\nProof. We divide the proof into two parts. In part one, we prove that with probability 0.99, $r \\le \\bar{r}$. In part two, we prove that with probability 0.99, $r > \\bar{r} - O(k(\\log k + \\log \\log n)/(1 + e^{\\epsilon/(\\log k \\log n)}))$. In Theorem 5.1, with probability 1 1/(300k\u00b2), DPLCP.QUERY satisfies two conditions. Our following discussion supposes all DPLCP.QUERY satisfies the two conditions. There are 3k2 LCP queries, by union bound, the probability is at least 0.99."}, {"title": "6 Conclusion", "content": "We study the differentially private Hamming distance and edit distance data structure problem in the function release communication model. This type of data structures are $\\epsilon$-DP against any sequence of queries of arbitrary length. For Hamming distance, our data structure has query time $\\tilde{O}(mk+n)$ and error $\\tilde{O}(k/e^{\\epsilon/\\log k})$. For edit distance, our data structure has query time $\\tilde{O}(mk^2+n)$ and error $\\tilde{O}(k/e^{\\epsilon/(\\log k \\log n)})$. While the runtime of our data structures (especially edit distance) is nearly-optimal, it is interesting to design data structures with better utility in this model."}, {"title": "A Proofs for Hamming Distance Data Structure", "content": "In this section, we include all proof details in Section 4."}, {"title": "A.1 Proof of Lemma 4.3", "content": "Proof of Lemma 4.3. Let E(A), E(A') be ENCODE(A) and ENCODE(A'). Let #(E(A) = S) be the number of the same bits between E(A) and S, #(E(A) $\\neq$ S) be the number of the different bits between E(A) and S. Then the probability that the random flip process transforms E(A) into S is:\n$Pr[\\mathcal{A}(A) = S] = \\frac{1}{(1+\\frac{1}{e^{\\epsilon/(2M_1)}})^{\\#(E(A)\\neq S)} (\\frac{1}{1+e^{\\epsilon/(2M_1)}})^{\\#(E(A)=S)}}$ = $\\frac{(\\frac{1}{e^{\\epsilon/(2M_1)}})^{\\#(E(A)=S)}}{(1 + e^{\\epsilon/(2M_1)})^n}$\nSince for each position, ENCODE changes at most $M_1$ bits, and A and A' only have one different position. Therefore there are at most $2M_1$ different bits between E(A) and E(A'). So we have\n$\\frac{Pr[\\mathcal{A}(A) = S]}{Pr[\\mathcal{A}(A') = S]} \\le (e^{\\epsilon/(2M_1)})^{\\mid\\#(E(A)=S)-\\#(E(A')=S)\\mid} \\le (e^{\\epsilon/(2M_1)})^{2M_1} = e^{\\epsilon}$\nThus we complete the proof."}, {"title": "A.2 Proof of Lemma 4.4", "content": "Proof of Lemma 4.4. h is a hash function randomly drawn from all functions [2n] $\\rightarrow$ [M2]. For certain j, h(p) = j for all p are independent random variables, each of them equals 1 with probability 1/M2, or 0 with probability 1 \u2013 1/M2. So we have\n$Pr[\\vert T_j\\vert \\geq 10\\log k] = Pr[\\Sigma_{p \\in T} [h(p) = j] \\geq 10 \\log k]$ = $\\Sigma_{d=10 \\log k}^{\\vert T \\vert}  \\binom{\\vert T\\vert}{d} (\\frac{1}{M_2})^d (1-\\frac{1}{M_2})^{\\vert T \\vert - d} $ $\\le \\Sigma_{d=10 \\log k}^{\\vert T \\vert} \\frac{ \\vert T \\vert !}{d!(\\vert T \\vert - d)!} (\\frac{1}{M_2})^d =  \\Sigma_{d=10 \\log k}^{\\vert T \\vert} \\frac{ \\vert T \\vert ^d}{d!  M_2^d}$ $\\le \\Sigma_{d=10 \\log k}^{\\vert T \\vert} \\frac{ \\vert T \\vert ^d}{d!  M_2^d} \\le \\Sigma_{d=10 \\log k}^{\\vert T \\vert} (\\frac{k}{d!})^d \\le \\frac{1}{(10 \\log k)!} \\Sigma_{d=10 \\log k}^{\\vert T \\vert} (\\frac{1}{2})^d$"}, {"title": "A.3 Proof of Lemma 4.5", "content": "Proof of Lemma 4.5. g is a hash function randomly drawn from all functions [2n] $\\times$ [M1] $\\rightarrow$ [M3]. For every single i $\\in$ [M1], define event $E_i$ as the event that the 2$ \\vert T_j\\vert $ values in $\\{g(2(p-1)+A_p, i) \\vert p \\in T_j\\} \\cup \\{g(2(p-1) + B_p, i) \\vert p \\in T_j\\}$ are mapped into distinct positions.\n$Pr[E_i] = \\Pi_{c=1}^{2 \\vert T_j \\vert -1 } (1- \\frac{c}{M_3}) = \\frac{\\Pi_{c=1}^{2 \\vert T_j \\vert-1 } (M_3-c)}{M_3^{2 \\vert T_j \\vert -1}} = \\frac{1}{M_3^{2 \\vert T_j \\vert-1}} \\Sigma_{c=1}^{2 \\vert T_j \\vert-1} M_3 \\ge 1- \\frac{2 \\vert T_j \\vert (\\vert T_j \\vert +1)}{M_3} \\ge 1- \\frac{2(10 log^2k)}{400 log^k} = 0.5$\nThe fourth step follows from Lemma 4.4. It holds true with probability 0.99.\nFor different i$\\ \\in\\$  [M1], E, are independent. Therefore, the probability that all $E_i$ are false is $(0.5)^{M_1} < 1/(1000k)$. By union bound, the probability that for every $j\\in [M2]$ there exists at least one i such that $E_i$ is true is at least $1 - M2 \\cdot 0.5^{M1} \\ge 1 - M2/(1000k) \\ge 0.98$."}, {"title": "A.4 Proof of Lemma 4.6", "content": "Proof of Lemma 4.6. From Lemma 4.5, for all j, there is at least one i, such that the set {g(2(p \u2013 1) + Ap, i)|p \u2208 T;} \u222a{g(2(p \u2212 1) + Bp, i)|p \u2208 Tj} contains 2|T;| distinct values. Therefore, for that i, $E(A)_{i,j,1 \\sim M3}$ and $E(B)_{i,j,1 \\sim M3}$ have exactly 2|T;| different bits. For the rest of i, the different bits of $E(A)_{i,j,1 \\sim M3}$ and $E(B)_{i,j,1 \\sim M3}$ is no more than 2|T;|. So we have $0.5 \\cdot \\Sigma_{j=1}^{M2} max_{i\\in [M1]} (\\Sigma_{c=1}^{M3} \\vert E(A)_{i,j,c} - E(B)_{i,j,c} \\vert ) = 0.5 \\cdot \\Sigma_{j=1}^{M2} 2 \\vert T \\vert = |T| = D_{ham}(A, B)$."}, {"title": "B Differentiall Private Longest Common Prefix", "content": "We design an efficient", "A[i": "i + l", "B[j": "j + l"}]}