{"title": "A Systems Thinking Approach to Algorithmic Fairness", "authors": ["Chris Lam"], "abstract": "Systems thinking provides us with a way to model the algorithmic fairness problem by allowing us to encode prior knowledge and assumptions about where we believe bias might exist in the data generating process. We can then model this using a series of causal graphs, enabling us to link AI/ML systems to politics and the law. By treating the fairness problem as a complex system, we can combine techniques from machine learning, causal inference, and system dynamics. Each of these analytical techniques is designed to capture different emergent aspects of fairness, allowing us to develop a deeper and more holistic view of the problem. This can help policymakers on both sides of the political aisle to understand the complex trade-offs that exist from different types of fairness policies, providing a blueprint for designing AI policy that is aligned to their political agendas.", "sections": [{"title": "1 INTRODUCTION", "content": "For several years, Al researchers have struggled with how to build AI/ML systems that are fair and aligned to society's values. Purely technical approaches that rely on statistical correlation alone have fallen far short of what is needed to solve this difficult problem. Systems thinking offers a novel approach to the algorithmic fair-ness problem by allowing us to unify complex ideas from the so-cial sciences and computer science into a comprehensive and in-tegrated theory. By leveraging prior knowledge and assumptions about where we believe bias might exist in the data generating process, we can help policymakers on both sides of the aisle to design Al policy that is aligned to their political agendas.\nTo address the algorithmic fairness problem, we need to take a sociotechnical approach. We will begin by analyzing the social aspects of this problem using first principles. We will explore the na-ture of free will to develop an understanding of moral responsibility. We will review the structure versus agency debate in sociology and attribution theory in psychology to address the controversial ques-tion: Why do we believe disparities exist across protected groups like race or gender? This will give us insights into the sources of bias in the data generating process, showing us what different in-terventions are needed to make a decision fair. We can then use this"}, {"title": "2 RELATED WORK", "content": "The field of algorithmic fairness is a relatively new field that has drawn significant attention since the release of a ProPublica article that claimed racial bias in COMPAS, a software tool that is widely used in the US court system to predict recidivism[1]. Since then, there have been multiple attempts to align the legal definitions of discrimination (e.g. disparate treatment and disparate impact in the US and direct discrimination and indirect discrimination in the EU) with their equivalent mathematical definitions. But these approaches, which have oftentimes relied on statistical correlation, have not been successful. Narayanan for example identified nearly two dozen fairness metrics, none of which were aligned well to the legal definitions of discrimination[34].\nPearl has argued for a few decades that the discrimination prob-lem is fundamentally a causality problem [37]. He has argued that fairness can be modeled using causal Bayesian networks (CBNs) using a series of direct and indirect effects. This work was later expanded upon by Plecko and Bareinboim, who proposed the idea of a standard fairness model [42]. They argued that there exists a causal template through which entire classes of fairness models can be derived using a small number of modeling assumptions. Barocas et al. devoted a chapter in their textbook to discuss how causality might be used to model fairness [3]. Kusner et al. argued for the use of counterfactuals in modeling fairness [26]. Chiappa and Isaac showed that certain paths within a CBN could be labeled as fair or unfair [8]. Another attempt of building causal fairness models was made by Nilforoshan et al., but their approach had encountered limitations [35]."}, {"title": "3 TOWARDS A UNIFIED THEORY OF\nFAIRNESS", "content": "To build fair AI/ML systems, we begin from first principles with the strong assumption that all humans have free will[6]. This makes us fundamentally different from machines, which are programmed by humans and therefore have no free will of their own (at this time). Free will forms the basis of moral responsibility[54]. If we hu-mans have free will, then we have the agency to make our own deci-sions. These decisions will lead to certain outcomes, and we may be held morally responsible and accountable for those outcomes. Since machines have no free will, they cannot be held morally responsible for any decisions that they make [15]. However, the humans who programmed the machines can still be held accountable [18, 25].\nMost of the time, we humans make decisions that only affect our own selves. We can be held personally responsible for the decisions that we make for ourselves. But sometimes, we humans make decisions that affect other people. This is especially true for humans who are in positions of great power. We may be held socially responsible for the decisions that we make for others. Sometimes the decisions that we make will lead to good outcomes for other people. In these cases, they might assign us praise and reward. On the other hand, sometimes the decisions that we make will lead to bad outcomes for other people. In these cases, they might assign us blame and punishment. This concept is known as moral desert[19]. We humans have a natural tendency to accept praise and reward, and to reject blame and punishment.\nBut sociologists would note that we are never fully free to make our own decisions. This is because social structures can influence our agency [2]. Social structures define our relationships with the various institutions of society. These could include our economic systems, legal frameworks, cultural norms, and political institutions. Agency defines our ability to act independently and to freely make our own decisions. Social structures place a limit on the the choices and opportunities that are available to us humans. Although we may want to live in a society where everyone has equal access"}, {"title": "4 MODELING FAIRNESS AS A COMPLEX\nSYSTEM", "content": "The algorithmic fairness problem can be modeled as a complex system by merging techniques from machine learning, causal in-ference, and system dynamics. Each of these techniques capture different aspects of the fairness and can be seen as operating at different layers within a larger causal hierarchy. The first layer is machine learning, where we want models to make fair predictions and decisions that are compliant with society's laws and regulations. The next layer is causal inference, where we can model different forms of bias and discrimination. This helps us to identify what in-terventions are necessary to make a fair decision. The final layer is system dynamics, where we can model the dynamics of the fairness problem to identify counterintuitive behavior within the system. This allows us to build mental models for how different fairness interventions can affect disparities across protected groups.\nFigure 1 provides a visual representation of these three different but complementary analytical techniques. Notice that in each rep-resentation, we try to use the same variables to demonstrate that they are different ways of looking at the same problem. Examples of what each of the variables in these figures might represent are listed for several high-stakes applications in Table 1.\nWe begin with Figure 1a, which is a block diagram representation of supervised machine learning. In the center of this block diagram is a black box machine learning model that receives data X as input and returns decision D as output. The decision D leads to some outcome Y, which becomes future data X through a feedback loop. A key question is whether a protected attribute A should or should not be fed into the black box model and if so, then how.\nNext up we have Figure 1b, which is a causal Bayesian network representation (CBN) of causal inference. CBNs were pioneered by Pearl[36, 38, 39] and allow us to symbolically encode knowledge and assumptions about causal effects between variables, which are shown in this graph as nodes. The first node of the CBN represents a protected attribute A, which has a direct effect on a mediator W ($A \\rightarrow W$). The mediator W has a direct effect on an outcome Y ($W \\rightarrow Y$). We assume that the mediator W completely explains away the causal effect between the protected attribute A and the outcome Y ($Y \\perp \\!\\!\\! \\perp A|W$). However, the mediator W might be consid-ered biased by some people if they attribute the disparities across groups as being external. The mediator W is filled white because it is latent, but it can still be partially measured as data X ($W \\rightarrow X$). Data X is then fed into a machine learning model that is embedded inside the node for decision D ($X \\rightarrow D$). Finally, the decision D also has a direct effect on an outcome Y ($D\\rightarrow Y$). CBNs are represented by directed acyclic graphs (DAGs), which means that we cannot use them to model feedback loops."}, {"title": "5 MODELING FAIRNESS AS A CAUSAL\nBAYESIAN NETWORK", "content": "Causal Bayesian networks (CBNs) can be used as a higher level representation of any supervised machine learning model. It is at this layer of abstraction that we can begin modeling the different biases that society might have. By understanding these various sources of bias, we can model the different ways that a model can discriminate. Finally, we can use these insights to help policymakers on both sides of the aisle to design Al policies for machine learning models that they would deem to be fair.\nFigure 2 shows a series of CBNs modeling different forms of bias, fairness, and discrimination. Each CBN captures a different way of framing the fairness problem based on an individual's prior beliefs about where bias might exist in a model. On the left, we have four different forms of fairness, which we will later show can be mapped to different points on the political spectrum. On the right, we have four different forms of discrimination, which can be mapped directly to antidiscrimination law.\nThe first form of fairness that we will look at is fairness through unawareness (Figure 2c). This is the simplest form of fairness. We begin with the belief that the data is fundamentally unbiased (internal attribution), which is why all of the nodes in the graph are colored black. In this form of fairness, there is no edge between the protected attribute A and the decision D. Therefore, the model is blind to an individual's membership to a protected group A and therefore cannot treat any individual differently on that basis. We would say that the protected attribute A has no direct effect on the decision D, making the decision fair.\nThe advantage of fairness through unawareness is that it avoids overt discrimination (Figure 2b). If a model uses knowledge of an individual being a member of a disadvantaged group against them in its decision D, then it would clearly cause disparate treatment or direct discrimination. We show this as a form of bias where the edge from the protected attribute A to decision D is colored red ($A\\rightarrow D$). This leads to a biased decision D, which in turn causes a biased"}, {"title": "6 MODELING FAIRNESS AS A CAUSAL LOOP\nDIAGRAM", "content": "One of the key limitations of causal Bayesian networks (CBNs) is that as directed acyclic graphs, they cannot be used for modeling feedback loops. But we need those feedback loops to understand the complex, dynamic effects of different fairness policies over time. So we turn our attention towards system dynamics, where we can leverage causal loop diagrams (CLDs) instead.\nTo model feedback loops within a CLD, we need to label each path as having a positive (+) or negative (-) direction. If the path is positive, then changes in one variable leads to changes in another variable in the same direction. For example, an increase in one variable would lead to an increase in the other variable. Alterna-tively, a decrease in one variable would lead to a decrease in the other variable. If the path is negative, then changes in one variable leads to changes in another variable in the opposite direction. For example, an increase in one variable would lead to a decrease in the other variable. Alternatively, a decrease in one variable would lead to an increase in the other variable.\nCLDs are represented using a combination of reinforcing loops or balancing loops. Reinforcing loops compound changes over time, creating either virtuous cycles that cause growth or vicious cycles that cause decay. Balancing loops counteract changes over time, keeping a system within a fixed state that prevents growth or decay. Reinforcing loops have an even number of negative paths, while balancing loops have an odd number of negative paths.\nWe can combine reinforcing and balancing loops to create sys-tem archetypes. These represent common recurring patterns that explain counterintuitive behavior in complex systems. This con-cept was first coined by Senge[47] with additional research done by Meadows[29] and Sterman [52]. Three system archetypes seem particularly helpful for understanding the fairness problem, which we have adapted from Kim [23].\nThe first archetype that we will examine is the \"success to the successful\" archetype, which is shown in Figure 3. This archetype consists of two reinforcing loops, R1 and R2. Let's say that we"}, {"title": "6.1 Case study: Racial disparities in the US", "content": "We can use the CBNs from Figure 2 and the CLD from Figure 6 to model the complex political debate over racial disparities in the US. According to the US Census Bureau, median household income in 2021 was $100,573 for Asians, $74,932 for Whites, $57.671 for Hispanics, and $48,297 for Blacks [53]. Meanwhile, median house-hold wealth in 2021 was $320,900 for Asians, $250,400 for Whites, $48,700 for Hispanics, and $27,100 for Blacks [24]. We would like to understand why Whites and Asians have higher median household income and wealth than Blacks and Hispanics.\nWhen it comes to understanding the root causes of racial dis-parities, we can take into account some of the original sources of bias in the data. We know that the US has historically had unjust policies that overtly discriminated against non-Whites. These in-cluded racist policies like slavery, Jim Crow segregation, and the Chinese Exclusion Act. For convenience, we replicate the CBN for overt discrimination in Figure 7."}, {"title": "7 MODELING FAIRNESS AS A CAUSAL\nHIERARCHY", "content": "Systems thinking allows us to combine three previously indepen-dent analytical techniques: machine learning, causal inference, and"}, {"title": "8 MODELING FAIRNESS AS A SYSTEMS MAP", "content": "In this final section, we will combine all of the ideas from the previ-ous sections into a very high level representation of the algorithmic fairness problem called a systems map. This is shown in Figure 15. On the left side of the systems map, we use a connection circle to show the different fields in the social sciences that are related to fairness. These ideas were described in greater detail in section 3. On the right side of the systems map, we use a causal hierarchy to show the different technical aspects of the fairness problem. These ideas were described in greater detail in section 7.\nAs mentioned before, algorithmic fairness is a sociotechnical problem. We need to find a way to bridge the social and technical aspects of the fairness problem. In each of the social science fields, there exists a dichotomy that represents different viewpoints for understanding the fairness problem. This allows us to understand the fundamentally ideological and political debate over fairness, which centers around how we attribute the causes for an outcome. The systems map shows a causal bridge that links this dualism in the social sciences to Pearl's two fundamental laws of causal inference, which was discussed in section 5.1. Hopefully this systems map"}, {"title": "9 CONCLUSION", "content": "Systems thinking provides us with a novel way to model the al-gorithmic fairness problem by \"thinking outside of the black box.\" While a large amount of Al research has been focused on the hidden layers that are \"inside\" a black box ML model (e.g. neural networks), this paper has been focused on the transparent layers that are \"out-side\" a black box ML model. These outer layers are where we can encode our knowledge and assumptions about where we might believe that bias occurs in the data generating process. We showed that the first outer layer can be represented using causal Bayesian networks and the second outer layer can be represented using causal loop diagrams. This gives us an elegant way of modeling the emergent behavior between machine learning, causal inference, and system dynamics.\nThis approach allow us to develop a more unified and holistic view of the fairness and machine learning problem, providing the essential connections between computer science and the law. This is the key to building AI systems that are aligned with society's democratic values and the policymakers' political agendas. It will enable businesses and regulators to communicate with each other using a common causal language, one that can be used by both data scientists and lawyers. These models may even aid in AI deregu-lation by identifying mathematical flaws in the existing laws and regulations that are caused by paradoxes. This will help businesses to comply with antidiscrimination laws and regulations, reducing business uncertainty and expensive litigation while providing a much needed layer of transparency and trust for consumers of AI products and services.\nWhereas much of the fairness and machine learning literature has been focused on how fairness through unawareness perpetuates racial inequalities, this paper demonstrates how affirmative action and DEI may also perpetuate racial inequalities. It shows us that racially aware policies are not a silver bullet in closing the racial income and wealth gap. We can use systems thinking to move away from shallow and ideological notions of fairness and towards deeper and holistic notions of fairness that reflect how the real world actually works. We argue that racial disparities in the US are not purely due to historical, structural, and systemic factors. Personal responsibility plays as important of a role (if not more). Therefore, instead of just focusing on eliminating structural and systemic barriers to racial equity, we need to put as much attention towards creating systems that motivate individuals to make good decisions while holding them accountable for making bad decisions.\nSystems thinking supplies the missing mathematical models that are needed to build responsible AI systems. To solve fairness, we need to bring together both the social and technical aspects of what is ultimately a complex systems problem. On the social side, it requires a diversity of perspectives that is inclusive of people from both sides of the political spectrum. On the technical side, it requires unifying analytical techniques that were previously thought of as being independent. By unifying all of these concepts together, we hope to build Al policies and systems that could lead to actual, real-world solutions for closing disparities across society."}]}