{"title": "SYLLABLELM: LEARNING COARSE SEMANTIC UNITS\nFOR SPEECH LANGUAGE MODELS", "authors": ["Alan Baade", "Puyuan Peng", "David Harwath"], "abstract": "Language models require tokenized inputs. However, tokenization strategies for\ncontinuous data like audio and vision are often based on simple heuristics such\nas fixed sized convolutions or discrete clustering, which do not necessarily align\nwith the semantic structure of the data. For speech in particular, the high resolution\nof waveforms (16,000 samples/second or more) presents a significant challenge\nas speech-based language models have had to use several times more tokens per\nword than text-based language models. In this work, we introduce a controllable\nself-supervised technique to merge speech representations into coarser syllable-like\nunits while still preserving semantic information. We do this by 1) extracting\nnoisy boundaries through analyzing correlations in pretrained encoder losses and 2)\niteratively improving model representations with a novel distillation technique. Our\nmethod produces controllable-rate semantic units at as low as 5Hz and 60bps and\nachieves SotA in syllabic segmentation and clustering. Using these coarse tokens,\nwe successfully train SyllableLM, a Speech Language Model (SpeechLM) that\nmatches or outperforms current SotA SpeechLMs on a range of spoken language\nmodeling tasks. SyllableLM also achieves significant improvements in efficiency\nwith a 30x reduction in training compute and a 4x wall-clock inference speedup.", "sections": [{"title": "1 INTRODUCTION", "content": "Learning to generate speech solely from listening to spoken language is a fundamental task in speech\nprocessing. It requires abstracting beyond the underlying acoustics of speech into phones, syllables,\nwords, and sentences to process correlations across long ranges of time. But while current textual\nlanguage models (Touvron et al., 2023; Zhang et al., 2022; Brown et al., 2020) can compose highly\\orealistic text, language models on spoken language still struggle to output semantically meaningful\nspeech. An increasing focus has coalesced around Generative Spoken Language Modeling (GSLM)\n(Lakhotia et al., 2021), which sets out to achieve this goal.\nThe most successful approaches to GSLM are Transformer Decoder Language Models (Vaswani\net al., 2017) such as AudioLM (Borsos et al., 2023) and TWIST (Hassid et al., 2023). These Speech\nLanguage Models (SpeechLMs) operate on discrete tokens output by self-supervised learning (SSL)\nbased encoder models (Hsu et al., 2021; Chung et al., 2021). However, existing SSL tokenizations\npredominantly capture phonetic information (Choi et al., 2024) at a high temporal resolution of 25-50\ntokens per second, much greater than the typical human speaking rate of 2-5 words per second. This\nlarge number of tokens substantially impairs both training and inference speed (Hassid et al., 2023),\nand it is unclear whether modeling speech with such a high granularity harms semantic understanding.\nVery recently, there has been significant progress in extracting coarser speech unit representations\nfrom raw audio. In particular, SD-HuBERT (Cho et al., 2024) finetunes HuBERT (Hsu et al., 2021)\nwith a DINO-like distillation objective, and VG-HuBERT (Peng & Harwath, 2022; Peng et al., 2023)\nuses a contrastive loss against cross-modal visual inputs. We continue and significantly improve\nupon this line of research, resulting in high quality speech tokens (units) suitable for SpeechLMs\nthat exhibit a temporal resolution roughly corresponding to syllables. Specifically, we demonstrate\nsignificant improvements in textual reconstruction from low-bitrate units of SSL models, reducing\nthe word-error-rate (WER) from 37% using SD-HuBERT units to 7%, and more than halving realized"}, {"title": "2 RELATED WORK", "content": "Self-Supervised Encoder Models There has been a great amount of work in learning high-level\nrepresentations from data by reconstructing corrupted inputs across speech (Baevski et al., 2020; Hsu\net al., 2021; Baevski et al., 2023), audio (Gong et al., 2021), text (Devlin et al., 2019; Clark et al.,\n2020), and vision (Caron et al., 2021; He et al., 2021). To navigate the lack of simple discrete targets\nin speech, much work has been placed in finding high-quality targets, such as iterative clustering (Hsu\net al., 2021) and by predicting the representations of a teacher network based on a running average of\nstudent model weights (Baevski et al., 2022; 2023). An alternate but similar line of work has been\nplaced into learning low-bitrate units for the task of resynthesis (D\u00e9fossez et al., 2023; Zeghidour\net al., 2021; Yang et al., 2023a; Kumar et al., 2023; Zhang et al., 2023; Du et al., 2023), which include\nlosses focused on reconstruction and use an information bottleneck to enforce compression."}, {"title": "Applications of Neural Codecs", "content": "The discrete units generated by these self-supervised encoders are\nversatile and fundamental to much of the recent progress in speech research such as Text-To-Speech\n(Wang et al., 2023; Ju et al., 2024; Song et al., 2024; Peng et al., 2024), joint audio-text foundation\nmodels (Yang et al., 2023b; Chou et al., 2023; Nguyen et al., 2024), unsupervised speech recognition\n(Baevski et al., 2021), discrete unit resynthesis (Polyak et al., 2021; D\u00e9fossez et al., 2023; Zeghidour\net al., 2021), text-to-audio (Kreuk et al., 2022; Agostinelli et al., 2023; Copet et al., 2023), and\ngenerative spoken language modeling (Borsos et al., 2023; Hassid et al., 2023; Lakhotia et al., 2021).\nEach of these methods operates on audio units exclusively greater than or equal to 25Hz, which\nhas been a frequently cited area for future work to improve on (Hassid et al., 2023). Recent work\n(Elkahky et al., 2023) has also explored training speech encoder models with coarser units as targets."}, {"title": "Extracting Semantic Units from Raw Data", "content": "Additionally relevant to our work are several ap-\nproaches, particularly in vision and audio, that generate emergent semantic clusterings from self-\nsupervised transformer (Vaswani et al., 2017) models. In particular, the DINO approach in Caron\net al. (2021) observes object representations in attention maps through student-teacher distillation.\nSimilar techniques have been also applied to audio to discover emergent syllable boundaries (Cho\net al., 2024; Peng et al., 2023). These behaviors can vary heavily with small changes in pretraining\nstrategy as explored in Darcet et al. (2024). Merging similar features has also been shown to produce\nsignificant vision model speedups such as in Bolya et al. (2023). Most similar to our work, Algayres\net al. (2023) extracted coarse continuous representations for GSLM, however these results trail behind\nlanguage modeling approaches."}, {"title": "3 LEARNING SELF-SUPERVISED, SYLLABLE-LIKE REPRESENTATIONS FROM\nRAW SPEECH", "content": "In this section, we describe the bootstrapping process by which we extract low-bitrate speech units.\nWe first describe LossPred, our algorithm to analyze outputs of self-supervised speech model loss\nfunctions to generate initial syllable-like unit boundaries. Following this, we define SylBoost, a\nprocedure to iteratively refine these boundaries with student-teacher distillation. We also propose a\nnew algorithm for the efficient extraction of boundaries from feature self-similarity matrices to fix\nthe bottleneck slowing down VG-HUBERT and SD-HUBERT extraction."}, {"title": "3.1 LOSSPRED: EXTRACTING SYLLABLE-LIKE SEGMENTATION FROM RELATIONS IN\nHUBERT'S Loss", "content": "HUBERT has previously been shown to learn phone-like units with its K-Means clusterings (Hsu et al.,\n2021) which have formed the basis of subsequent works on GSLM and unsupervised ASR (Baevski\net al., 2021; Lakhotia et al., 2021; Hassid et al., 2023). However, other work (Pasad et al., 2023; 2024)\nhas shown that the representations learned by these models also correlate with higher level structure\nsuch as words, despite these structures not immediately appearing during clustering. Our goal in\nthis section is to propose a method that can be applied to a pretrained HuBERT model in order to\nautomatically extract unit boundaries at the level of syllables or words rather than phones. Although\nwe apply our method to HuBERT, we expect that it could also be applied to other SSL speech models\nthat utilize a similar loss function such as WavLM (Chen et al., 2021) or wav2vec2.0 (Baevski et al.,\n2020). The crucial commonality between these models is that they all utilize a masked language\nmodeling (MLM) training objective, whereby input speech tokens are randomly masked and the\nmodel is trained to predict the masked inputs conditioned on the unmasked inputs.\nWe ground our intuition for LossPred with the following thought experiment: if the input tokens\ncorresponding to an entire word were replaced with mask tokens, we would expect the HuBERT\nmodel loss at these timesteps to be relatively high, as HuBERT would have to jointly predict word\nidentity and the underlying acoustics to predict the missing span. On the other hand, if only the latter\nportion of a word were masked out, infilling this masked region given the word prefix may be easier\nby comparison. With this, if we iteratively shift a contiguous mask over a span of tokens and look at\nthe loss, we would suspect to see a strong decrease in the loss throughout the timesteps corresponding\nto a masked semantic unit (word, syllable, or otherwise) as the beginning or end of the unit was\npartially revealed to the model."}, {"title": "4 SYLLABLELM: SPEECH LANGUAGE MODELING ON COARSE UNITS", "content": "GSLM (Lakhotia et al., 2021) defines a pipeline for modeling raw audio as three stages: 1) Audio-to-unit Tokenization, 2) Running a language model on these units, and 3) Decoding the tokens back into\na waveform. For Audio-to-unit Tokenization, we use the second iteration of SylBoost as described in\nSection 3. In this section, we lay out our language modeling and resynthesis."}, {"title": "4.1 LANGUAGE MODEL", "content": "Like AudioLM and TWIST, we use an autoregressive transformer decoder language model to\napproximate $p(x_t | X_{t\u22121},...,x_1)$ given an input token sequence $x_1,...,x_T$. We refer to this\nlanguage model trained on the discrete clusters output by SylBoost as the main SpeechLM. Like\nTWIST, we prepend a <BOS> token and make no other special changes."}, {"title": "4.2 TOKEN TO SPEECH DECODING", "content": "To convert SylBoost tokens back into audio, we adopt the interleaved decoding strategy from Song\net al. (2024) to output the units from TWIST Hassid et al. (2023), obtaining a waveform by cascading\nthis output into their provided vocoder. This interleaving strategy demonstrates superior performance\nin high-difficulty settings compared to other text-to-speech models like VALL-E (Wang et al., 2023),\nand so we use it for all resynthesis experiments. To interleave our units, we sort on the start-timestep\nof every SylBoost unit and TWIST-unit in ascending order. We subtract 0.08s (the length of two\nTWIST units) from each SylBoost unit start time before sorting to account for noisy SylBoost\nboundaries. For the rest of the pipeline, we follow Song et al. (2024) without global advance using\nour syllables as a drop-in replacement for phones and use greedy decoding. We call this model the\nInterleaved-Vocoder-LM.\nAlthough incorporating this additional resynthesis model slows down generation compared to TWIST,\nmost model scaling happens in the SpeechLM. For example, the TWIST paper still observes scaling\nimprovements in semantic understanding with a SpeechLM of 13B parameters while current SotA\nspeech synthesis models such as Ju et al. (2024) operate with fewer than 1B parameters. Taking the\nsame approach as TWIST, our work focuses purely on improving the semantics of generated audio\nand is entirely orthogonal to work on audio synthesis quality and voice cloning.\nWe then generate continuations for a sample by 1) Extracting syllable-units and TWIST units from\nthe sample, 2) Sampling syllable-unit continuations from the SpeechLM, 3) Continuing TWIST units\nwith our interleaved model conditioned on sample TWIST units, sample syllable-units, and continued\nsyllable-units, and 4) Resynthesizing these into speech using the vocoder."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 TRAINING DATASETS", "content": "We train our tokenizer using LibriSpeech (Panayotov et al., 2015), which contains 960 hours of audio\nbooks. We noticed that SylBoost converges before all data is used, and so we randomly subsample\nLibriSpeech to a 100 hour train set and train for five epochs and two iterations for all experiments. We\ntrain our SpeechLMs using all of LibriLight (Kahn et al., 2020), which provides roughly 55k hours\nof speech. As a note on fair comparison, although AudioLM uses exactly this split of LibriLight,\nTWIST collects an additional 100k hours of data, totaling to 155k hours."}, {"title": "5.2 SYLBOOST UNIT CONFIGURATIONS", "content": "By varying the number of boundaries input to our cut algorithm at each stage in the SylBoost pipeline,\nwe can arbitrarily control our rate of temporal tokenization. We evaluate three main unit rates at\n8.33Hz, 6.25Hz, and 5.00Hz, the latter which matches the empirical rate of SD-HuBERT units on\nLibriSpeech dev-clean. To account for differences between slow and fast speakers, we dynamically\nchoose k during SylBoost to best match the number of distinct feature groups in the embedding space\n(seen as blocks in the feature self-similarity matrix A, Figure 2). We do this by setting a threshold $\\delta$\nand taking the fewest boundaries k such that the loss per timestep is less than $\\delta$ for 75% of timesteps."}, {"title": "5.3 RESULTS: EVALUATING UNIT QUALITY", "content": "We evaluate the quality of our semantic units with two approaches 1) measuring correspondence\nwith syllables and 2) running speech resynthesis followed by ASR. To measure correspondence with\nsyllables, we use the development and test sets of LibriSpeech (Panayotov et al., 2015) and follow the\napproach from Peng et al. (2023), extracting timesteps for phones using the Montreal Forced Aligner\n(McAuliffe et al., 2017) and then converting these phones into syllables with a rule-based method\n(Gorman, 2014). We evaluate the quality of syllable boundary detection with a ground truth boundary\nmarked as hit if a proposed boundary is present within a tolerance threshold. We report F1, Precision,\nRecall, and R score (R\u00e4s\u00e4nen et al., 2009). We ablate F1 scores with tolerance windows of 20ms and\n50ms. Given boundaries, we also evaluate the purity of our clusters at 4096 units. Syllable Purity\nmeasures the probability that a syllable is mapped to its most corresponding cluster unit, and Cluster\nPurity measures the probability that a cluster is mapped to its most corresponding syllable unit.\nEven if units do not correspond with syllables, they can still be useful to SpeechLMs if they can\nresynthesize back into speech that matches the original text. Additionally, training a resynthesis model\nprovides a stronger description of the semantic information contained in units than purity metrics,\nwhich may be unreliable because SD-HuBERT does not provide a unit at every timestep while\nour methods do. To evaluate resynthesized speech, we follow AudioLM and measure Word Error\nRate (WER) and Character Error Rate (CER) on the set of 4-10 second segments from LibriSpeech\ntest-clean. For ASR, we follow VALL-E (Wang et al., 2023) and use the public HuBERT-base CTC\nASR model provided by Hsu et al. (2021)."}, {"title": "5.4 SPEECH LAUGUAGE MODEL CONFIGURATION", "content": "All of the SpeechLMs we implement, as well as our Interleaved-Vocoder-LM, follow the OPT\n(Zhang et al., 2022) architecture and default to using 12 Transformer layers, an embedding dimension\nof 768, and learned positional embeddings. This totals to 90M non-embedding parameters, and\nrepresents an identical model architecture to TWIST-125M. We also experiment with a larger 24\nlayer 1024 dimension model totaling to 300M non-embedding parameters, the same as AudioLM\nand TWIST-350M. For all language model pretraining experiments, we randomly crop files to 25\nseconds, use a batch size of 80000 tokens, and train for 200k steps, which amounts to the same\ncompute as in TWIST. To make our approach entirely textless, we do not use TWIST initialization.\nWe tokenize using SylBoost on Data2Vec2 (Baevski et al., 2023), which provided the best results\nin low-bitrate WER resynthesis. We discuss the base encoder further in Appendix A.3.\nAdditional hyperparameters and hardware details are in Appendix A.4."}, {"title": "5.5 SPEECH LANGUAGE MODEL BASELINES", "content": "We compare against a range of SotA SpeechLMs: AudioLM (Borsos et al., 2023), TWIST (Hassid\net al., 2023), GSLM (Lakhotia et al., 2021), and the audio-only version of Moshi (D\u00e9fossez et al.,\n2024). As a tokenizer, AudioLM uses w2v-BERT tokens (Chung et al., 2021), TWIST operates\non tokens from a HuBERT model that the TWIST authors pretrain for an additional iteration on\nmultilingual data, Lakhotia et al. (2021) uses HuBERT-base and K-Means, and Moshi operates\non a residual codec with the first code distilled to match WavLM (Chen et al., 2021). AudioLM\nand TWIST are the most directly comparable models to SyllableLM with respect to dataset and\ntransformer size and output units at 25Hz followed by consecutive deduplication.\nWe reimplement a 90M parameter model using the TWIST tokenizer units without textually-pretrained\ninitialization (Cold-Init in the TWIST paper) on our data split for an all-else-held equal comparison\non unit type. We additionally reimplement Byte Pair Encoding (BPE) to train a SpeechLM as done\nin Shen et al. (2024), resulting in the lowest bitrate encoding of speech outside of our model. We"}, {"title": "5.6 RESULTS: SPOKEN LANGUAGE MODELING", "content": "The end-to-end GSLM pipeline is deep, and so it is essential to have metrics to independently evaluate\ndifferent stages. To evaluate our SpeechLM stage, we follow Lakhotia et al. (2021) and use the\nZeroSpeech (Nguyen et al., 2020) sWUGGY and sBLIMP evaluation. The sWUGGY dataset tasks\nthe model with outputting a higher perplexity on similar but fake spoken words (e.g. brick vs blick).\nSimilarly, the sBLIMP dataset checks syntactic correctness (e.g. the dog sleeps vs the dogs sleeps).\nWe also evaluate the SpeechLM on the tSC set from Hassid et al. (2023), which operates like the\nZeroSpeech metrics on a spoken version of the StoryCloze dataset (Mostafazadeh et al., 2016) with\nthe last sentence in negative samples randomly chosen. For all metrics we follow prior work and\noutput the mean perplexity per token.\nThe results for SpeechLM metrics are depicted in Table 6. We find that training with our syllable units\nimproves performance across-the-board on speech understanding tasks. With under 90 hours of\ntraining, SyllableLM outperforms the 13B parameter TWIST and 7B Moshi on sBLIMP, which\nbest evaluates semantic understanding (Lakhotia et al., 2021). Measured on all of sWUGGY, we also\nbeat AudioLM with 30x less GPU compute and TWIST model sizes up to 1.3B parameters in lexical\nunderstanding. On tSC, we observe that SyllableLM benefits from scaling with our large model\napproaching the performance of our textual topline. Because our units are much lower frequency,\nwe see a 4.5x speedup at inference time with the same parameter count in Table 4. Due to compute\nrequirements, we are unable to scale further."}, {"title": "6 DISCUSSION AND LIMITATIONS", "content": "Though speech is a very general medium, there are a number of challenges in adapting our methods\nto generate low-bitrate units angled towards other audio tasks or other domains such as vision.\nOur LossPred technique assumes that the semantic units to learn are separable across time, one-\ndimensional, and contiguous. In audio tasks or settings with multiple speakers, sounds or words can\noccur simultaneously. Images and video have partially occluded or overlapping objects.\nDespite this, LossPred is unique in that it finds boundaries using the values (losses) that the pretrained\nmodel is directly trained to optimize. Meanwhile, other approaches such as DINO (Caron et al.,\n2021) and SD-HuBERT rely on difficult-to-control intermediate features for semantic discovery.\nSylBoost then demonstrates that we can use LossPred to extract emergent and interpretable features\neven when they are not clear in the original model. Because of this, we believe that LossPred and\nSylBoost provide a very promising direction for controllable and scalable semantic feature clustering\nand extraction in domains like speech, audio, music, and video.\nLow-frequency units such as ours also provide a significantly more computationally tractable path\ntoward the scaling that has seen such success in textual language models. On the other hand, SylBoost\nunits may be losing out on useful paralinguistic features like tone whose impact is only salient\non non-audiobooks or at scale. We suspect that hybrid models operating on low-bitrate and high-\nbitrate units could be constructed to balance these considerations and demonstrate significant quality\nimprovements in multimodal speech-text language modeling."}, {"title": "7 CONCLUSION", "content": "We introduce a new method to tokenize speech for use in SpeechLMs. We do this by proposing a\nmethod to elicit syllabic organization in pretrained speech encoder models, bootstrapping a feature-\nspace clustering algorithm from a static analysis of correlations in off-the-shelf SSL encoder model\nlosses across time. We demonstrate the success of our technique both in having strong associations\nwith syllables and as an extremlely low-bitrate codec of speech for textual resynthesis. Using this\ntokenization strategy, we successfully train SyllableLM, a SpeechLM that out-performs comparable\nstate-of-the-art approaches across a diverse range of metrics with a significant training and inference\nspeedup. We further ablate several design decisions such as quantization strategy, loss initialization,\nand the effects of controllability for downstream usecases. Compression is a crucial aspect of learning,\nand we hope that these significant improvements in the unsupervised learning of low-bitrate speech\nunits can serve as a foundation for approaches towards understanding spoken language and general\nrepresentation learning."}, {"title": "8 REPRODUCIBILITY", "content": "We focused heavily on reproducibility throughout our work, and we commit to releasing all parameters\nand code used upon deanonymization. We provide the exact equations used to calculate LossPred,"}, {"title": "9 ETHICS", "content": "It is important to note that large textual language models can have harmful effects, such as enabling\nthe generation of misinformation in mass. Although generative spoken language models have not yet\ncaught up to their textual counterparts, it is still necessary to be aware of potential misuses that could\narise in the future."}, {"title": "A.1 RANDOMLY SAMPLED EXAMPLE SEGMENTATIONS", "content": "We provide randomly sampled example segmentations from the LibriSpeech (Panayotov et al., 2015)\ndev-clean set. All models are the second iteration of Data2Vec2, which we use for our SyllableLM\nexperiments in Section 5.6. Top: Feature Self-Similarity matrix, darker green is closer. Segmented\ncuts span vertically in blue from the top, ground truth boundaries span vertically in red at the bottom.\nBottom: time-aligned Mel-Spectrogram. We call attention to the interesting behavior of global\ncorrespondences appearing when words or syllables are repeated. Best viewed zoomed in."}, {"title": "A.2 DISCUSSION: OTHER BOOTSTRAPPING STRATEGIES", "content": "Of course, there already exist several strategies for unsupervised syllable and word segmentation\nsuch as in Fuchs & Hoshen (2023), Pasad et al. (2024) and Peng et al. (2023) that could be used\nto bootstrap our first pseudolabels. We have not conducted an exhaustive search over initializing\nwith these strategies, however it is intriguing that the approach from Peng et al. (2023) achieves\nsignificantly lower quality, relatively and absolutely, from SylBoost bootstrapping than LossPred.\nWe suspect that this may be caused by the fact that although the representations of these models\ncorrelate with boundaries, there is no modeling in the pretraining loss pushing the representations to\nlinearly separate across semantic differences. Meanwhile, the loss is forced to change across semantic\nboundaries due to the difficulty of language modeling, albeit noisily."}, {"title": "A.3 DISCUSSION: BASE ENCODER", "content": "Because we want to use a 50Hz base encoder to match SD-HuBERT and have fine-grained boundary\ncontrol during syllable segmentation, we cannot use the 25hz encoder from TWIST. Unfortunately,\nthis means that the quality of the base encoder may be a confounding factor in our SpeechLM\nevaluation. We choose Data2Vec2-base (Baevski et al., 2023) as a middleground for training\nSpeechLMs on syllable-like units because we find its quality enables lower bitrates than HuBERT,\nbut it is older and trains on less-data than the TWIST tokenizer, and it has 6x fewer parameters than\nw2v-BERT, used by AudioLM. We suspect that applying newer encoders like w2v-BERT 2 from\nBarrault et al. (2023) could enable even better performance, which we leave to future work. Because\nthe Data2Vec2 loss function doesn't natively work with LossPred, we initialize Data2Vec2 SylBoost\nfrom the same HUBERT loss boundaries as discussed in 3.1."}, {"title": "A.4 HARDWARE AND HYPERPARAMETERS", "content": "We implement all experiments using NVIDIA A40 46GB GPUS with a Intel Xeon Gold 6226R\nCPU @ 2.90GHz. Estimated speeds are made using these results as well as scaling from Zhang et al.\n(2022).\nHyperparameters for pretraining our models are below. We note that the Batch Size is in terms of\ntokens, which means that higher unit rates will have fewer seconds of raw audio per batch to keep\nGPU compute roughly equal per model.\nFor LossPred, we use a HuBERT-large student and a HuBERT-base teacher as they are the only\npublic checkpoints for a corresponding student and teacher model. We preprocess the audio input\nto LossPred and Feature Similarity (Peng et al., 2023) using an unsupervised voice activity dection\nmodel (Tan et al., 2020)."}, {"title": "A.5 SAMPLE CONTINUATIONS", "content": "Below are sample continuations generated with a temperature sampling parameter chosen to best\nmatch Oracle VERT diversity scores. We provide continuations of roughly 3 seconds of audio,\nsampled randomly from LibriSpeech test-clean. This text is given as output by our HuBERT ASR\nModel from Hsu et al. (2021), with transcription errors present and with no additional modifications.\nThe source text is bolded, and sometimes cuts off mid-word, which can behave differently per sample\nbased on unit rate and quantization artifacts."}]}