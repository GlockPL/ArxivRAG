{"title": "A Survey on E-Commerce Learning to Rank", "authors": ["Md. Ahsanul Kabir", "Mohammad Al Hasan", "Aritra Mandal", "Daniel Tunkelang", "Zhe Wu"], "abstract": "In e-commerce, ranking the search results based on users' preference is the most important task. Commercial e-commerce platforms, such as, Amazon, Alibaba, eBay, Walmart, etc. perform extensive and relentless research to perfect their search result ranking algorithms because the quality of ranking drives a user's decision to purchase or not to purchase an item, directly affecting the profitability of the e-commerce platform. In such a commercial platforms, for optimizing search result ranking numerous features are considered, which emerge from relevance, personalization, seller's reputation and paid promotion. To maintain their competitive advantage in the market, the platforms do no publish their core ranking algorithms, so it is difficult to know which of the algorithms or which of the features is the most effective for finding the most optimal search result ranking in e-commerce. No extensive surveys of ranking to rank in the e-commerce domain is also not yet published. In this work, we survey the existing e-commerce learning to rank algorithms. Besides, we also compare these algorithms based on query relevance criterion on a large real-life e-commerce dataset and provide a quantitative analysis. To the best of our knowledge this is the first such survey which include an experimental comparison among various learning to rank algorithms.", "sections": [{"title": "1. Introduction", "content": "Ranking is a central problem in information retrieval. Ranking result typically depends on the relevance between documents and queries, that's why in some references ranking is also called relevance ranking. To illustrate, given a query, the document will have higher ranking if it is more relevant to the query than other documents. If there is a large number of documents associated with a query, the top ranked documents are the most important documents as well.\nWhile learning to rank (LTR) has been the focus of information retrieval for maximizing search relevance [1, 2, 3, 4, 5, 6], the applications of LTR include collaborative filter, document retrieval, definition finding, key term extraction, important email routing, product rating, sentiment analysis, and anti web spam. However, in the e-commerce domain, the purpose of LTR is to help the buyers to find and purchase their intended products. The buyers' intent advocates the demand side while the listings by the e-commerce site represent the supply side of the platform. The goal of the e-commerce LTR is to provide the most relevant products for the supply and the demand side and to maximize the chance to generate revenue. While both a web search engine and an e-commerce site intend to ensure user satisfaction by presenting the most relevant products that a user is searching for, an e-commerce site maximizes revenue or sales, and minimizes the inventory cost showing the most relevant products. Additionally, the feature selection strategy of the two differs because the e-commerce queries are often shorter, lack syntactic structure and product property dominant than the web queries. Moreover, the ranking of products for the e-commerce queries are performed for user signals, like click rate, add-to-cart rate, and purchased rate."}, {"title": "2. LTR Framework", "content": "To demonstrate the machine learning frameworks, we first formally define the LTR problem in e-commerce domain. Let a training dataset contain n training queries {qi}i=1. The corresponding products for qi be {pjm(i)}=1 such that p1 \u2265 p2.... \u2265 p(im(i)) in terms of relevance and mi) be number of relevant product for qi. The goal is to design a ML framework which learns to rank from the training instances and perform best in the test data. Each framework approaches the LTR problem in two ways. First, they define different input and output spaces. Input spaces are contingent on feature selection and representation learning of query and products. The output space is to facilitate the learning process based on ranking the documents with respect to query document relevance. Second, each framework has a particular heuristic to learn a scoring function which can rank the products with respect to the corresponding e-commerce query. To learn the scoring function each method minimizes a distinguished loss function L. Note that, there are quite a few research available for LTR in information retrieval [13, 14, 15]. However, these methods are not e-commerce specific. On the other hand, there are other methods available in e-commerce domain which are designed to meet specific requirements [16, 17]. In this research we want to demonstrate the important existing methods along with the loss functions each method solves. Our main goal is to provide a general idea of LTR methods, and in the next subsections we illustrate them."}, {"title": "2.1. Pointwise Approaches", "content": "Pointwise approaches [18, 19, 20, 21, 22, 23, 24, 10] assign each query, product pair a relevance score. The ranking is performed in a straightforward way checking whether existing learning methods can be directly applied. To illustrate, a d dimensional vector, x is constructed for each query product pair (qi, p) as input space. In addition, a list of floating point value scores, y(i) where y(i) = [y(i), ..., y(im(i))] is produced for each product as output space. The score y(i) represents the relevance degree of product p to query qi. y(i) is calculated generally from buyer interaction of products in a e-commerce site. Once {(x(i), y(i))}i=1 is constructed from training dataset, the goal is to learn the mapping between x(i) and y(i). Among the regression methods linear regression [25], polynomial regression [26] methods reduce squared loss which is defined by the next equation.\n$L = \\sum_{j=1}^{m^{(i)}} (y^{(i)}_j - f(x^{(i)}))^{2}$"}, {"title": "2.2. Pairwise Approaches", "content": "Like the point-wise approaches, the pairwise approaches [31, 32, 33, 34, 35, 36, 37] do not work with relevance scores of all the titles with respect to the associated query. In fact the rank of products in e-commerce domain are more important than relevance scores. That's why instead of relevance scores, pair of titles are chosen from all the mi) titles, and models are trained based on relative ranking of the titles. Due to the emphasis of relative ranking, pairwise approaches are closer to the concept of ranking than the pointwise approaches.\nAmong the pairwise approaches, probably the most famous one is RankNet [31]. RankNet solves a simple probabilistic cost function and can be implemented combining neural network with stochastic gradient descent method. To illustrate RankNet, recall the query qi, and the two associated products p, p such that p > p. In order to compare the relative importance of the two products with respect to qi, a rank function f : RD \u2192 R is designed such that f(p') > f(p). In other words, f converts a D dimensional feature vector extracted from product and query to a real-valued scalar which preserves the relative ranking of the two products. Let z be f(p') - f(pi), then z \u00bf 0 to ensure p' > p. Next the probability, Pjk of p > p is calculated using z, and sigmoid activation function with scaling factor \u03c3.\n$P_{jk} = \\frac{1}{1 + e^{-\\sigma z}}$\nLike any binary classification with sigmoid activation function, to rank p higher than p, Pjk needs to be greater than 0.5. Presenting the probabilistic framework improves the consistency of the model, and a cross entropy (CE) loss can be designed for optimization problem.\n$C_{jk} = \u2212P_{jk} \\log P_{jk} \u2212 (1 \u2212 P_{jk}) \\log(1 \u2212 P_{jk})$\nThe loss function penalizes if the target order label Pjk and the predicted output Pjk does not match. To demonstrate Pjk for a given query, let S jk \u2208 {1,0,1} be defined to be 1 if p is more relevant than p, -1 if p is less relevant than p, and 0 if both the products have the same label. Pjk = (1 + S jk). By using these definitions CE loss function can be illustrated as the following equation.\n$C_{jk} = \\frac{1}{2} (1 \u2212 S_{jk}) \\sigma z + \\log(1 + e^{\u2212z})$\nA neural network is designed to reduce the CE loss function using gradient descent algorithm.\nThere are several improvement methods of RankNet, among which LambdaRANK [32] from Microsoft research deals with the weakness of the cost function. Note that the cost function RankNet solves is based on penalizing count of flipped of product orders for a given query. However, although the loss function is convex, and differentiable minimizing the loss function does not always improves NDCG score. To recovery from that, LambdaRANK is proposed introducing a loss function which is rewarded based on improvement of NDCG. To illustrate, consider the CE loss function of RankNet, and let sj = f(p'), and sk = f(p}).\n$\\Lambda_{jk} = \\frac{\\partial C_{jk}}{\\partial s_j} = \\sigma ((1 - S_{jk}) - \\frac{1}{1 + e^{-\\sigma z}})$\nFor RankNet these lambda values are used to speed up the gradient descent calculation. But for LambdaRANK, the equation is slightly modified as the following.\n$\\Lambda_{jk} = \\frac{\\partial C_{jk}}{\\partial s_j} = -\\sigma \\Delta_{NDCG} \\frac{1}{1 + e^{\\sigma z}}$\nThat means, the lamdas are penalized by how much swapping a pair of products improve NDCG while keeping the other pair orders unchanged. In stead of NDCG, other evaluation metrics can also be used in the loss function."}, {"title": "2.3. Listwise Approaches", "content": "For listwise approaches [38, 39, 40, 41, 42, 43, 44, 45, 46, 44, 47], two types of sub-categories exist in the literature. For the first sub-category, it is only known whether a title is relevant to a query. While the titles are not ranked among them, no relevant scores are provided for a query, title pair. On the contrary, for the second sub-category, a permutation of a list of titles are available where the titles are arranged in terms of relevance order. In e-commerce domain, the ranking among titles is important. It is less likely for a customer to buy the products which are far apart from the viewed page, although the titles are relevant. That's why we illustrate the methods in the second sub-category. The listwise approaches are the closest in terms of ranking because the loss functions can distinguish which ranking is better than other. This is why the listwise approaches are the most promising methods for LTR problems.\nThe first listwise method we describe in this paper is ListNet [40]. To illustrate, recall the query qi, and the m(i) number of products. Let a permutation of the products be {\u03c0(i)j}m(i)j=1 associated with qi, and the actual index of the products in the ground truth rank order be 1 = {\u03d5(i)j}m(i)j=1. Let a rank function f : RD \u2192 R, such that f(x) = sj where xj is a representation vector of product \u03c0(i)j. A probability function is defined to calculate the probability of a rank order. For instance, the probability of the rank order {p}m(i) is defined by the next equation.\n$P_{\\pi} = \\prod_{j=1}^{m^{(i)}} \\frac{\\psi(s_{\\pi_j})}{\\sum_{k=j}^{m^{(i)}} \\psi(s_{\\pi_k})}$\nHere \u03c8(\u22c5) is a monotonically increasing function which can be linear, exponential, or sigmoid. While each item \u03c8(s\u03c0j) in the equation is a conditional probability value, P\u03c0 gets the largest probability value when the ranking order is appropriate. In the same fashion, another probability value, Py is calculated for the actual ranking of the products with respect to qi. Based on these two probability functions, a K-L divergence loss function can be derived for ListNet which the model optimizes.\nWhile ListNet is a very closer model for actual ranking, due to K-L divergence loss, the train complexity is exponential to mi). So to avoid the complexity, if number of elements in the permutation are kept smaller, information about the permutation is significantly lost and the effectiveness of the ListNet algorithm is questionable. Another approach ListMLE [39] is proposed to avoid the drawbacks of ListNet. For each query qi, for the products in the ground truth permutation, a probability distribution based on the output of the scoring function is defined where xj is a representation vector of a product in the k'th position of actual ranking.\n$P_{y|x} = \\prod_{k=1}^{m^{(i)}} \\frac{\\psi(f(x_k))}{\\sum_{u=k}^{m^{(i)}} \\psi(f(x_u)))}$\nFor ListMLE in stead of K-L divergence loss, it uses the negative log likelihood of Py|x. As both K-L divergence loss, and negative log likelihood loss are convex, and differentiable, both model can be easily optimized. But ListMLE converges faster than ListNet as the train complexity is O(m(\u00b9)).\nAlthough both ListMLE, and ListNet can solve LTR challenges, position of the products are not emphasized in any of these models. For instance, if there is a flip of order of two products in early position of the permutation, the cost should be higher, as it is a matter of purchase. Note that, the latter products are not as important as the former ones, so the cost of error of both the cases should not be the same. That's why another model position aware ListMLE [41] (p-ListMLE) is proposed. The loss function for p-ListMLE is defined by the following equation.\n$L_p = \\sum_{k=1}^{m^{(i)}} \\alpha(i) (\u2212f(x_k)) + \\log( \\sum_{u=k}^{m^{(i)}} exp(f(x_u)))$\nwhere \u03b1(.) is a decreasing function, i.e. \u03b1(i) > \u03b1(i + 1). Lp introduces more penalty in the earlier positions of the actual permutation.\nAmong the advanced methods, there exists a reinforcement based method DeepQRank [48] which solves the LTR problem by a listwise Markov Decision Model. The loss function is calculated from how much gain is achieved when a product is added from unordered to ordered list. Recall the query qi, and let there be a ranked list for predicted products, R\u2081 initially empty and an unordered list U\u2081 initially containing all the associated products of qi in arbitrary order. A state for Markov model is consist of R\u2081, and U\u2081 and the state is changed when a product, p from U\u2081 is moved to RL. When the product is moved to change the state, a timestamp t\u2081 initially 0 is increased. Let the relevance score of p be rel(i), such that rel(i) \u2208 {1, 2, ..., m). The rank score of highly relevant products are higher. A reward function is then designed based on Discounted Cumulative Gain which is defined next.\n$r_i = \\frac{rel^{(i)}}{t_i}$\nThe policy of the Markov model is to transfer products from UL to R\u2081 until UL become empty maximizing ri. The model returns R\u2081 as output products for qi."}, {"title": "3. E-Commerce Specific Challenges", "content": "The methods so far we described can be applied to any LTR methods. However, e-commerce LTR methods need to solve extra challenges which are not the same as in other domains. First, in Web domain if anyone search with a query, normally the targeted web-item appear among the top 3 results. At least 90% of time the search results are found in the first page in Google. It is said that the best hiding place for a dead body is really on the second page of Google or beyond. For example, if a person searches for \"IUPUI\u201d, the website of the university will be found in the first result, so it is not even necessary to check the next items in the search result. On the contrary, in e-commerce domain people like to browse more before buying a product as it is a monetary matter. The relevant products can even be in the second and third pages. The average number of products to browse in eBay is top 20. This makes a major difference between e-commerce and web LTR. To solve this specific issue, we need to beer in mind that in Web domain the top results are the most important ones so the mistakes in later documents penalize less than those of top results. But in e-commerce domain this is not the case as user spends time while browsing items. The weights of the error needs to adjust for that. For instance, ListMLE is more suited to e-commerce domain than p-ListMLE as the later products in the search space are also important.\nSecond, in web domain the search results are distinct, which can provide different relevant scores for every result. This distinguishability can help a proper ranking of the results. On the contrary, products in the e-commerce domain do not always follow this criteria. Same product can be sold by different sellers. Moreover, depending on the features of a product, different users buy different varieties of the same product. To illustrate with an example, consider the diagram in 2. If a buyer searches with the query \"air filter\", quite a few products are shown up in Amazon. Among these \"FRAM Extra Guard Air Filter\" appears three times if the first two pages are explored. Note that, although the product is the same, the size is not the same among these three products. While the size changes, the price values change rapidly too. The e-commerce sites need to overcome this challenge. On the other hand, there is exactly one website for \"IUPUI\". While the relevance score of the product \"FRAM Extra Guard Air Filter\" are exactly similar for the three cases, the \"Computer Science Department\" of \"IUPUI\" does not have exact same relevance score to other departments in web based search. To overcome this challenge, dataset construction and feature selection needs to be carefully designed. As the feature of the same product changes the relevance score, this must be taken care of when the dataset is designed.\nThird, e-commerce product ranking is not static, rather it changes rapidly. The ranking changes due to change of demand, user choice, trend, improvement of the other alternative products, change of prices etc. But web result ranking do not change as rapidly as e-commerce. For instance, the query \"IUPUI\u201d always require to find the website of \"IUPUI\" at the top. But \"air filter\" finds different types of air filters over time. In other word there can be some different filters other than \"FRAM Extra Guard Air Filter\" may come to the top position depending on the changed status of market. This is an issue which is very important when a model is applied to production. Note that, as gigabytes of the new data are added every day, the model need to be scalable and fast enough with the changed snapshot of ranking data.\nFourth, e-commerce queries are shorter and products property dominant. The syntactic structure of the queries are not as important as web queries. Often web queries are more like natural language sentences. Product band, feature, symbol etc are some of the properties of e-commerce queries. User's previous search history, choice, session are other driving forces for e-commerce ranking. Different users can purchase different products searching with exact same query. The understanding of two different buyers can vary based on the culture, environment, choice, and customs of individuals. Moreover, it is often found that exact matching of a title of the product with query does not necessarily require the product to be in the top in the search. So feature selection of e-commerce queries are completely different than web queries. Even the representation of the queries itself is a challenging task. The existing embedding methods [49, 50] trained in the conventional Google and Wikipedia corpus do not capture the query demands. To represent query there is an existing research [51] which claim that spherical embedding trained"}, {"title": "4. Evaluation Metrics", "content": "Evaluation metrics in the e-commerce domain are normally Normalized Discounted Cumulative Gain (NDCG), Mean Average Precision (MAP), Expected Reciprocal Rank (ERR), Mean reciprocal rank (MRR), Spearman's Rho (p) etc."}, {"title": "4.1. NDCG", "content": "Normalized Discounted Cumulative Gain (NDCG) is used to evaluate any LTR system. NDCG is actually a ratio between two measures i.e., Discounted Cumulative Gain (DCG), and Ideal Discounted Cumulative Gain (IDCG). The purpose of the DCG is to penalize the highly relevant products coming later in search result. Recall the query qi, and the relevant scores of the predicted products relj. The DCG score is calculated by the next equation.\n$DCG = \\sum_{j=1}^{m^{(i)}} \\frac{rel_j}{\\log_2(j + 1)}$\nThe Ideal DCG is then calculated for top m(i) products according to decreasing order of relevant scores in sorted order. To illustrate with an example let m\u00ba) = 5, the predicted products are p\u2081, P2, P3, P4, and p's with actual relevant scores 0.6, 0.4, 0.5, 0.3, and 0.4 respectively. Then DCG = $\\sum_{j=1}^{rel_j=5} \\frac{rel_j}{\\log_2(j+1)}$ = 1.386. \u03a4o calculate Ideal DCG let there be another two products with relevance 0.5, and 0.4. All of these seven products are needed to be sorted in descending order, and then top 5 relevant scores are chosen for DCG calculation. The best scores are 0.6, 0.5, 0.5, 0.4, 0.4 and Ideal DCG scores are calculated based on these scores. The IDCG score = 1.492, and the NDCG score = DCG / IDCG = 0.9288.\nNote that, the NDCG score for this particular instance is very high, as the similarity scores have low variance. In this work, we use NDCG slightly modifying the original idea so that it fits with e-commerce domain. First we work with the original relevant scores. Note that the scores are 0.6, 0.5, 0.5, 0.4, 0.4, 0.3 in sorted order. We convert the values to integer rank such that the highest value gets the highest rank, and lowest value gets lowest rank 1. So the rank values for the similarity sores are 4, 3, 3, 2, 2, 1. Now, to calculate DCG we use this rank values instead to uplift the variance. Finally, we use another version of DCG calculation to penalise the mistakes more than before.\n$DCG = \\sum_{j=1}^{m^{(i)}} \\frac{rank^{(i)}}{\\log_2(j + 1)}$\nThe new DCG is calculated for rank values 4, 2, 3, 1, and 2 which are the rank scores for the similarity values 0.6, 0.4, 0.5, 0.3, and 0.4 respectively. DCG = 24.932, IDCG = 28.317, and NDCG = 0.88 which is lower than 0.9288. However, if the similarity values are distinct, the NDCG score will be much less than the obtained value as for the exact same similar score we introduce constant rank value. In our case the similarity values are always distinct due to the floating point nature, and we use the modified version of NDCG calculation for better evaluation of e-commerce ranking."}, {"title": "4.2. MAP", "content": "While Mean Average Precision (MAP) is used largely for object detection methods in computer vision, MAP can be used to evaluate the performance of LTR algorithms as well. Note that NDCG can not properly describe the performance of a LTR model when the standard deviation of relevance scores are very small. In that sense, MAP is a good candidate for evaluation metrics. To illustrate, MAP is contingent upon precision which demonstrates what ratio of the predicted products are relevant. Note that relevant scores are not used directly for calculating precision. Additionally, precision at K (prec@K) signifies what ratio of first K predicted products with respect to a query are relevant. Finally, average precision is calculated varying K for all the predicted products.\nFor instance, let's consider the previous example for NDCG. Let's assume that among the predicted products for qi, only the products of relevant score greater than equal 0.5 are relevant. That means, only p\u2081, and p3 are relevants out of the 5 predicted products. Now prec@1 is 1, as the 1st predicted product is relevant. Similarly, prec@2 is 0.5, as out of first two products, 1 is relevant. Then, rel@ j states that whether j'th predicted product is relevant. If the j'th predicted term is relevant, rel@ j = 1, otherwise rel@ j = 0. Finally, MAP is calculated by the following equation for LTR problem.\n$MAP = \\frac{1}{n} \\sum_{i=1}^{n} (\\frac{1}{NRI} \\sum_{j=1}^{m(i)} prec@j * rel@j )$\nwhere NRI is the number of relevant products for the mi) predicted products. We see that, MAP is calculated taking mean of average precisions for all the queries. For the particular query qi, average precision (AP) = (1*1 + 0.5*0 + 0.6666*1 + 0.5*0 + 0.4*0) / 2 = 0.833 Note that, AP of the predicted products is lower than NDCG score in this particular case."}, {"title": "4.3. MRR", "content": "For LTR problem, the position of the correct products are important. While MAP gives a general idea of the performance of an algorithm, the position of the correct products are ignored. Mean Reciprocal Rank can deal with this issue. The reciprocal rank of a query, qi is the multiplicative inverse of the rank of the first correct answer. If ranki demonstrates the rank position of the first relevant product, mean reciprocal rank corresponds to the harmonic mean of the rank positions. MRR is illustrated by the next equation.\n$MRR = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{rank_i}$\nTo illustrate with the previous example, for qi, the rank position of the first correct predicted product is 1, and so rank\u2081 = 1. Let's there be n = 3 queries in the dataset, and for the second query we need to look up top two predictions"}, {"title": "4.4. ERR", "content": "As MRR works only with the first correct predicted document, Expected Reciprocal Rank (ERR) [52] is proposed. ERR also diminishes the drawback of DCG such that DCG has an additive nature. Additionally, an assumption of DCG is that a product in a given position has always the same gain and discount independently of the products shown above it. Diminishing these drawbacks, ERR can correlate user click behavior better than DCG, and MRR. To illustrate ERR, at first a probability function is described to measure the satisfaction of a user from the j'th product of qi. Let the relevance of the j'th product p is rel'. Rj describes the user satisfaction of product p such that the higher the relevance score, the higher the probability of satisfaction.\n$R_j = \\frac{2^{rel'_j}-1}{max(rel')}$\nHere max(rel') symbolises the maximum relevant score for all the products associated with qi Then the likelihood of user stops at r'th position is calculated based on the Rj. Basically, user stops at position r, because the previous r 1 products do not satisfy him, and the product in position r is satisfactory. So the likelihood is calculated by the next equation.\n$P(user stops at position r) =  \\prod_{j=1}^{r-1} (1 \u2013 R_j)R_r$\nFinally, ERR is calculated as an expected metric, penalizing the position of a product by the following equation.\n$ERR = \\sum_{r=1} \\frac{1}{r} P(user stops at position r)$"}, {"title": "5. Dataset Availability", "content": "In the field of e-commerce, finding dataset for LTR is very hard. The primary reason is that such datasets contain confidential or proprietary information, and e-commerce platforms do not wish to take this risk. To cite an example, [53, 54, 55]worked on LTR with Walmart and Amazon datasets, but the datasets are not published. There are a few datasets for clustering, or recommendation systems, but as the number of features are very less (e.g. 8 features in [56]), these datasets are unworthy to be worked with LTR problems. To our best knowledge, there exists only a single dataset (Mercateo dataset)\u00b9 [57] for e-commerce LTR. The dataset is consist of query-title pair embedding and"}, {"title": "6. Experiment and Result", "content": "In this section we illustrate the performance of all the learning to rank methods which we describe in section 2. However, there is not a single research which performs experiment of all the methods we describe in this paper for a single dataset. That's why to compare among the methods is a challenging task. Hence we choose to perform experiments of all the methods in a common e-commerce dataset which we illustrate in the previous section. In the next subsections, we focus the query and product representation first for the experiments, then we illustrate the hyperparameters which we tune for the models, and finally we compare all the methods based on different evaluation metrics."}, {"title": "6.1. Representation Learning", "content": "Representing an e-commerce query or title is not similar to other domain as they have distinctive characteristics. To perform the experiments with the described methods, each query and product need to be represented in real space such that the similarity between query and product can be captured. In this paper, we only use the query text, and product title text for embedding. While there are many a method [59, 60, 61, 49] available for text embedding, we use BERT, and Spherical Text Embedding which are most promising capturing the representation of words, and phrases specially for similarity prediction [51], and relation understanding [62, 63]."}, {"title": "6.1.1. BERT and eBERT", "content": "Bidirectional Encoder Representations from Transformers (BERT) is proposed by a team of researchers from Google [49] which is not trained on any specific downstream task but instead on a more generic task called Masked Language Modeling. The idea is to uplift huge amounts of unlabeled data to pre-train a model on language modeling. Predicting the next word(s) given a context already requires understanding language to some extent. Next, this pre-trained model can be fine-tuned to solve different kinds of NLP tasks by adding a task specific layer which maps the contextualized token embeddings into the desired output function.\neBERT2 is an e-commerce specific version of the BERT model. Along with the Wikipedia corpus, 1 billion latest unique product titles are collected to train the model. The eBERT model can represent the e-commerce terms better than just using only Wikipedia corpus. In this work, to train the models, we use eBERT to embed both query and product title text. We keep the embedding dimension of eBERT similar to BERT which is 768."}, {"title": "6.1.2. Spherical Text Embedding", "content": "Directional similarity is often more effective in tasks, such as, word similarity and document clustering. When textual units are embedded in the Euclidean space, two textual units may have zero directional distance, yet they are far from each other by Euclidean distance. This is not desirable when we are trying to predict the query similarity values, which are between 0 and 1, and was computed by directional similarity (Cosine) of two vectors. To overcome this, spherical text embedding [59] has been proposed, which embed the textual units on the surface of a unit d-sphere (d is the dimension). To learn embedding on unit d-sphere, an efficient optimization algorithm is proposed with convergence guarantee based on Riemannian optimization. Spherical text embedding shown to be highly effective on various text embedding tasks, including word similarity and document clustering. For embedding e-commerce query text, we also choose spherical text embedding as a candidate. To build the corpus of spherical text embedding, besides Wikipedia sentences, we introduce 10 million e-commerce queries from eBay. We choose the dimensionality of the sphere as 100 as recommended by the original authors. We use each word as a textual unit for embedding task and then apply mean pooling over the words of a query to obtain query embedding vectors."}, {"title": "6.2. Discussion of Hyper-parameter Tuning", "content": "Table 4 shows the performance of the described methods. We only show the best performance of a model. In this section, we want to illustrate the hyperparameter tuning of the models. Note that the hyperparameters of each model is not similar to other models. However, all the pointwise approaches work with replicating the floating point similarity score, the pairwise approaches work with the relative ordering between a pair of products, and the listwise approaches mimic the ranking of a whole list of products. For all the methods, we do not use any default normalization. Additionally, for representation of query text and titles we either use spherical embedding for text or eBERT for embedding the text into vectors. Finally, for all the methods we use mean pooling of the word token embedding from eBERT or spherical text embedding for query or title embedding. Concatenated query and title embedding is the feature for all the methods of this research.\nTo discuss the hyperparameters of the pointwise approaches we start with LR. There are not many ways to tune the hyperparameters of LR. However, we did not use any kernel fitting over data or 11 or 12 normalization. For PR, we tune the degree from 2 to 4, and get best evaluation metrics for degree 2. For SVR we use \u201clinear\u201d kernel and tune parameter C over range [0.5,2.0", "0.05": "with an interval of 0.005. We get the best result for max depth 2 and learning rate 0.015.\nFor RankNet, LambdaRANK, and FRank in Table 4 only the loss function changes over methods. The hyperparameters can be model selection, and learning rate. For the learning rate we use \"adam\" optimizer with default learning rate 0.01. We use two hidden layers model for these pairwise methods varying the number of neurons over the values of set 32, 64, 128, 256, 512, 1024. For RankNet and LambdaRANK number of corresponding neurons 512, 32 provide the best model, while for FRank the number of neurons of the two layers is 256, and 128 for the optimum model. For RankNet, LambdaRANK, and FRank, the final layer is a dense layer with sigmoid loss to achieve the desired goal. Next for SVM we tune C over range [0.5,2.0"}]}