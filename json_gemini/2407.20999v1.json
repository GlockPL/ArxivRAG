{"title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning", "authors": ["Yupeng Chen", "Senmiao Wang", "Zhihang Lin", "Zeyu Qin", "Yushun Zhang", "Tian Ding", "Ruoyu Sun"], "abstract": "Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks. Typically, an LLM is pre-trained on large corpora and subsequently fine-tuned on task-specific datasets. However, during finetuning, LLMs may forget the knowledge acquired in the pretraining stage, leading to a decline in general capabilities. To address this issue, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO). The key idea of MoFO is to iteratively select and update the model parameters with the largest momentum magnitudes. Compared to full-parameter training, MoFO achieves similar fine-tuning performance while keeping parameters closer to the pre-trained model, thereby mitigating knowledge forgetting. Unlike most existing methods for forgetting mitigation, MoFO combines the following two advantages. First, MoFO does not require access to pre-training data. This makes MoFO particularly suitable for fine-tuning scenarios where pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. Second, MoFO does not alter the original loss function. This could avoid impairing the model performance on the fine-tuning tasks. We validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting and enhancing fine-tuning performance.", "sections": [{"title": "1 Introduction", "content": "The success of large language models (LLMs) lies in their strong capabilities in language understanding and generation. Typically, LLMs are initially pre-trained on extensive corpora to acquire general capabilities, and subsequently, they are fine-tuned on smaller, task-specific datasets to adapt to particular tasks or domains [Dai and Le, 2015, Kenton and Toutanova, 2019, Radford et al., 2018]. However, it has been observed that during the fine-tuning process, LLMs may forget the knowledge acquired in pre-training, leading to a decline in general capabilities [Lin et al., 2023, Chen et al., 2020, Dong et al., 2021, Korbak et al., 2022, Luo et al., 2023]. Therefore, addressing the issue of forgetting during fine-tuning has become an important research direction for LLMs.\nIn the literature, two classes of methods are commonly adopted to mitigate the forgetting: reply-based methods, and regularization-based methods.\n\u2022 Reply-based methods leverage pre-training data during the fine-tuning process [Rolnick et al., 2019, Wang et al., 2020, Ouyang et al., 2022]. However, most open-source LLMs, such as the Llama series [Touvron et al., 2023], have not fully disclosed their pre-training datasets. Moreover, even with access to"}, {"title": "2 Momentum Filtered Optimizer (MoFO)", "content": "It is hypothesized that the severity of forgetting during fine-tuning correlates with the distance between the fine-tuned model and the pre-trained model: in general, models that remain closer to the pre-trained model are less likely to experience forgetting [Kirkpatrick et al., 2017]. This hypothesis motivates a line of regularized-based methods to mitigate forgetting [Kirkpatrick et al., 2017, Li et al., 2018, Zenke et al., 2017]. However, adding regularization changes the original loss function, which can impair the model's performance on the fine-tuning task (see evidence in Section 3). It is intriguing to design a new method to mitigate forgetting without changing the loss function. In this work, we propose a new optimization algorithm that can converge to minima (of the original fine-tuning loss) close to the pre-trained model without sacrificing the fine-tuning loss.\nWe first discuss how to keep the model closer to the pre-trained model. To achieve this goal, we recall the classical block coordinate descent (BCD) method [Tseng, 2001]. We believe that BCD algorithm may converge to a point that is closer to the pre-trained model than full parameter fine-tuning. This is because BCD only updates a subset of parameters at each iteration. Compared to full parameter updates, this method usually involves smaller adjustments to the parameters.\nThe remaining issue is how to design BCD that reaches a similar performance to the default methods. An effective strategy is to prioritize updating parameters that have the greatest influence on reducing fine-tuning loss. A straightforward approach is to measure the parameter's influence by the magnitude of"}, {"title": "2.2 Algorithm Formulation", "content": "We formally introduce the Momentum-Filtered Optimizer (MoFO) in Algorithm 1. MoFO partitions all the parameters into B fixed parts as shown in Line 4. At each iteration, MoFO selects the parameter entries with the largest \u03b1% momentum magnitudes in each part as shown in Lines 10-13 of Algorithm 1, where the update fraction \u03b1% is the pre-determined hyperparameter. The momentum filtering mechanism is illustrated in Figure 1.\nWe note that the NN parameters are naturally composed of different parts (e.g., weight matrices, bias terms) in the network architecture, and PyTorch's backward propagation mechanism automatically returns the gradients of the loss with respect to each part of the parameters. To reduce computational complexity, we partition all parameters according to these fixed parts and select \u03b1% of the parameter entries for BCD update within each part.\nIn Section 3.4, we will empirically demonstrate that MoFO's momentum-based selection rule outperforms its gradient-based variant in fine-tuning tasks. It indicates that the momentum-based selection rule allows for better incorporation with Adam during the optimization process in fine-tuning.\nMoFO efficiently selects and updates the most influential parameters, as dictated by the momentum's magnitude, thus enhancing the fine-tuning process while alleviating the catastrophic forgetting of pre-training knowledge."}, {"title": "2.3 Initial Exploration", "content": "In this section, we empirically examine whether an LLM fine-tuned with MoFO converges to a minimum closer to the pre-trained model compared to the one fine-tuned with the Adam optimizer, and whether MoFO mitigates catastrophic forgetting in LLMs.\nWe fine-tune the Pythia-160m on the same dataset used in the experiment described at the beginning of Section 2, using both the Adam optimizer and MoFO. First, as shown in Figure 2(a), both MoFO and Adam optimizer achieve minimal fine-tuning loss, so switching Adam to MoFO does not lead to performance degeneracy. Second, as shown in Figure 2(b), the distance from the pre-trained model to the minimum reached by MoFO is approximately 20% of the distance to that reached by the Adam optimizer. This shows that MoFO significantly reduces the amount of parameter movement.\nWe further verify whether the reduced parameter movement (by MoFO) effectively mitigates the forgetting of general capabilities acquired by pre-training. We evaluate the degree of forgetting from two perspectives: pre-training loss and evaluation benchmarks. First, we find MoFO has a lower pre-training loss compared to the Adam optimizer, indicating that MoFO memorizes pre-training data better. Second, in Table 1, we evaluate the accuracies of Pythia-160m, fine-tuned using both the Adam optimizer and MoFO, on widely-used common sense tasks, which measures the commonsense reasoning capabilities of LLMs. These"}, {"title": "2.4 Convergence Result", "content": "In this section, we conduct a convergence analysis of MoFO. We study a simplified version of our MOFO algorithm as a variant of gradient descent (GD), which is described by Algorithm 2.\nTheorem 1 (Convergence of MoFO). Suppose that the minimum value of the loss function is $L^*$ and the gradient $\u2207 L$ is Lipschitz continuous with constant $L$. Then for the GD version of MoFO in Algorithm 2, if the learning rates satisfy \u03b7t = 1/L, it holds that\nmin ||gt||\u221e = O(T\u22122).\n0<t<T\nIn summary, we demonstrate the convergence of a GD version of MoFO, providing theoretical support for the strong performance of MoFO in fine-tuning tasks. We note that it seems rather non-trivial to prove the convergence of the original version of MoFO: MoFO contains 1st and 2nd-order momentum in a fractional form, and such structure is known to be challenging to handle [Zhang et al., 2022]. We leave the convergence of the original MoFO method as an interesting future direction."}, {"title": "3 Experiments", "content": "Now we verify the effectiveness of MoFO on instruction fine-tuning and continual fine-tuning. We use Llama-2-7B [Touvron et al., 2023] and TinyLlama-1.1B [Zhang et al., 2024] as the base models for our experiments. We also provide studies on the choice of update fraction and update strategies of MoFO."}, {"title": "3.1 Experimental Settings", "content": "Datasets for instruction fine-tuning. This group of datasets covers question-answer pairs from different domains like mathematical reasoning and code generation. Specifically, the datasets include:\n\u2022 MetaMathQA [Yu et al., 2024a]. This dataset comprises 395K math question-answer pairs. Numerous studies indicate that LLMs significantly enhance performance metrics on mathematical benchmarks such as GSM8K after fine-tuning on this dataset. In this paper, we randomly select 10% of this dataset for training LLMs, which includes 33,000 question-answer pairs."}, {"title": "3.2 Instruction Fine-Tuning", "content": "In this section, we aim to investigate the effectiveness of the MoFO approach in preserving general capabilities while learning fine-tuning tasks. We fine-tune Llama-2-7B on MetaMathQA and Code-Alpaca datasets, and compare the performance of MoFO against several baseline methods. The evaluation includes changes in performance on fine-tuning tasks and general capability metrics, with the pre-trained model's performance serving as the reference point for comparison. The implementation details can be seen in B."}, {"title": "Baselines", "content": "We compare the proposed MoFO algorithm with some of the most widely used optimization techniques, which aim to alleviate forgetting, as our baselines. These methods include:\n\u2022 Full FT refers to default full-parameter fine-tuning approach with the loss function $L_{finetune}(\u03b8)$.\n\u2022 L2-regularization [Kirkpatrick et al., 2017] adds an L2 regularization term to the original loss function $L_{finetune}(\u03b8)$ to keep the fine-tuning model closer to the pre-trained model, thereby mitigating forgetting. The modified fine-tuning loss function is $L_{finetune}(\u03b8) + \u03bb_2||\u03b8 \u2013 \u03b8_0||_2$, with the regularization hyperparameter \u03bb2 set to 1e-3.\n\u2022 L\u2081-regularization [Panigrahi et al., 2023] adds an L\u2081 regularization term to the original loss function $L_{finetune}(\u03b8)$ to promote sparsity in parameter movement. The modified fine-tuning loss function is $L_{finetune}(\u03b8) + \u03bb_1||\u03b8 \u2013 \u03b8_0||_1$, with the regularization hyperparameter \u03bb\u2081 set to 1e-6.\n\u2022 Half Fine-tuning (HFT) [Hui et al., 2024] randomly updates half of the parameter blocks within each transformer layer at each iteration while the other half are frozen. HFT can be considered a specific case of the BCD algorithm."}, {"title": "3.3 Continual Fine-Tuning", "content": "In this section, we explore the performance of our proposed MoFO in continual fine-tuning on the TRACE benchmark [Wang et al., 2023b]. We sequentially train TinyLlama-1.1B on the TRACE dataset, which includes the eight tasks from different domains. The implementation details can be seen in B.\nPerformance upper bound. Multi-task Learning (MTL) mixes samples from all eight distinct tasks together during training and typically achieves the highest OP score. We will use MTL as an upper bound for performance comparison.\nSome orthogonal methods. We consider several traditional methods from the field of continual learning. These methods can be orthogonal combined with MoFO to further enhance performance.\n\u2022 Replay involves optimizing the model using current data along with a memory buffer containing old samples from previous tasks to mitigate catastrophic forgetting.\n\u2022 Gradient of Episodic Memory (GEM) mitigates catastrophic forgetting by using gradients from old tasks to adjust the parameter updates during the training of new tasks."}, {"title": "3.4 Further Analysis", "content": "In this section, we first investigate the impact of the update fraction of parameters in the MoFO algorithm at each iteration, and then explore the effects of different update strategies within MoFO."}, {"title": "Impact of update fraction of parameters in MoFO", "content": "Following the setting in Section 4.2, we fine-tune Llama-2-7B on the MetaMathQA dataset using MoFO with varying update fractions of parameters at each"}, {"title": "4 Why MoFO Converges to a Closer Point", "content": "In Section 3, we discuss how the distance of parameter movement during fine-tuning impacts the retention of pre-training knowledge. Drawing on these insights and the BCD method, we introduce MoFO. Here, we propose the following question as follows.\nQuestion: Why does MoFO converge closer to the pre-trained LLMs than those of Adam?"}, {"title": "5 Related Works", "content": "Catastrophic forgetting Catastrophic forgetting, a significant issue where models forget previously learned information upon learning new data, has received considerable attention in machine learning [McCloskey and Cohen, 1989, Goodfellow et al., 2013, Kemker et al., 2018, Ramasesh et al., 2021, Liu et al., 2024]. In the realm of LLMs, there has been a growing body of recent works [Luo et al., 2023, Kotha et al., 2024, Shi et al., 2024, Wu et al., 2024] focusing on the forgetting of models' knowledge during the fine-tuning process.\nResearchers have proposed numerous methods to alleviate forgetting in continual learning, which involves learning a sequence of tasks. These methods are not limited to learning in a sequential manner and can be applied to broader paradigms. Generally, three primary approaches are used: reply-based methods, regularization-based methods, and architecture-based methods."}, {"title": "6 Conclusion", "content": "This paper presents the Momentum-Filtered Optimizer (MoFO), a new approach designed to mitigate the crucial issue of pre-training knowledge forgetting in LLMs during fine-tuning. By selectively updating the parameters with the largest momentum magnitudes in each parameter block, MoFO converges to a point closer to the pre-trained model compared to full-parameter fine-tuning and effectively preserves pre-trained knowledge. Our experimental results demonstrate that MoFO not only significantly alleviates catastrophic forgetting but also surpasses the performance of traditional fine-tuning methods. Future work will explore further optimizations and potential applications of MoFO in multimodal LLMs."}, {"title": "A Proof of Theorem 1", "content": "Before providing the proof, we will give some preliminary information.\nLet's assume the parameter space $R^d$ is decomposed as $R^d = R^{d_1} \u00d7 R^{d_2} \u00d7 ... \u00d7 R^{d_B}$. Here, B represents the number of parameter matrices in the neural network, and $d_k$ denotes the product of the dimensions (i.e., the number of rows multiplied by the number of columns) of the k-th parameter matrix. For any $z \u2208 R^d$, we denote\n$z = Concat(z^{(1)}; z^{(2)};...;z^{(B)})$, where $z^{(k)} \u2208 R^{d_k}$ for any $1 \u2264 k \u2264 B$.\nDefinition 1. For any $z \u2208 R^d$, we define\n$||z||_{top-\u03b1\\%} := ||z_{\\e_S} ||_2$,\nwhere $e_S = Concat(e^{(1)}; e^{(2)};...;e^{(B)})$. Here,\n$S_k = {i \u2208 [d_k]: |z^{(k)}_i | is among the top-\u03b1\\% of all |z^{(k)}|'s entries (|z^{(k)}_1|, |z^{(k)}_2|,...,|z^{(k)}_{d_k}| )}$\nand $e^{(k)}_i$ is a $d_k$-dimensional vector where the i-th entry is 1 if $i \u2208 S_k$, and 0 otherwise.\nLemma 1. $||\u00b7||_{top-\u03b1\\%}$ is a norm in $R^d$.\nProof. By Definition 1, we get\n$||z||_{top-\u03b1\\%} = ||z \u2299 e_S ||_2 = \\sqrt{\\sum_{k=1}^{B} ||z^{(k)}\u2299e^{(k)}||_2}$. (1)\nFirst, if $||z||_{top-\u03b1\\%} = 0$, then by (1), $||z^{(k)}\u2299e^{(k)}||_2 = 0$ for any $1 \u2264 k \u2264 B$. Thus,\n$||z^{(k)}||_\u221e = arg max_{i=1}^{d_k} |z^{(k)}_i| \u2264 ||z^{(k)}\u2299e^{(k)}||_2 = 0$.\nSo $z^{(k)}$ is a zero vector for any $1 \u2264 k \u2264 B$ and then z is a zero vector.\nSecond, for any given $c \u2208 R^+, {|z^{(k)}_i |}_{i=1}^{d_k}$ and ${|cz^{(k)}_i |}_{i=1}^{d_k}$ have the same order. So z and cz share the same filter $e_S$ and\n$||cz||_{top-\u03b1\\%} = ||cz \u2299 e_S ||_2 = c||z\u2299e_S||_2 = c||z||_{top-\u03b1\\%}$ .\nThird, for the vectors x, y \u2208 $R^d$, we denote the greedy filter of x, y, x + y by S, S', S'', respectively.\n$||x + y||_{top-\u03b1\\%} = || (x + y) \u2299 e_{S''}||_2 = ||x \u2299 e_{S''} + y \u2299 e_{S''}||_2 \u2264 ||x\u2299e_{S''} ||_2+ ||y\u2299e_{S''}||_2 \u2264 ||x\u2299e_{S} ||_2 + ||y\u2299e_{S'}||_2 = ||x||_{top-\u03b1\\%} + ||y||_{top-\u03b1\\%}$.\nProof of Theorem 1. By the definition of the simple version of MOFO in Algorithm 2, we have\n$\u0398_{t+1} \u2212 \u0398_t = -\u03b7_t m_t \u2299 FILTER_t = -\u03b7_t g_t \u2299 FILTER_t$,\nwhere FILTERt is actually eS in Definition 1."}, {"title": "B Implementation Details", "content": "Instruction fine-tuning. In our instruction fine-tuning experiments, we follow the implementation of Ivison et al. [2023]. The learning rate is set to 2e-5 with a cosine decay scheduler. For fine-tuning on the MetaMathQA dataset, we set the maximum sequence length to 1024, the batch size to 128, and we train the Llama-2-7B for 2 epochs. The parameter update fraction for MoFO is set to 15%. For fine-tuning on the Code-Alpaca dataset, we set the batch size as 64 and the parameter update fraction as 10%, while keeping other settings the same.\nContinual fine-tuning. In our continual fine-tuning experiments, we follow the default settings of the TRACE benchmark. We sequentially train TinyLlama-1.1B on the TRACE benchmark datasets: C-STANCE, FOMC, MeetingBank, Py150, ScienceQA, NumGLUE-cm, NumGLUE-ds, and 20Minuten for 5, 3, 7, 5, 3, 5, 5, and 7 epochs, respectively. We use a learning rate of 1e-5 with a cosine decay schedule and a batch size of 64. The parameter update fraction for MoFO is set to 5%.\nAll experiments are conducted on four A800 (80GB) GPUs."}]}