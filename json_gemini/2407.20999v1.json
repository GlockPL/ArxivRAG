{"title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning", "authors": ["Yupeng Chen", "Senmiao Wang", "Zhihang Lin", "Zeyu Qin", "Yushun Zhang", "Tian Ding", "Ruoyu Sun"], "abstract": "Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks. Typically, an LLM is pre-trained on large corpora and subsequently fine-tuned on task-specific datasets. However, during finetuning, LLMs may forget the knowledge acquired in the pretraining stage, leading to a decline in general capabilities. To address this issue, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO). The key idea of MoFO is to iteratively select and update the model parameters with the largest momentum magnitudes. Compared to full-parameter training, MoFO achieves similar fine-tuning performance while keeping parameters closer to the pre-trained model, thereby mitigating knowledge forgetting. Unlike most existing methods for forgetting mitigation, MoFO combines the following two advantages. First, MoFO does not require access to pre-training data. This makes MoFO particularly suitable for fine-tuning scenarios where pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. Second, MoFO does not alter the original loss function. This could avoid impairing the model performance on the fine-tuning tasks. We validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting and enhancing fine-tuning performance.", "sections": [{"title": "Introduction", "content": "The success of large language models (LLMs) lies in their strong capabilities in language understanding and generation. Typically, LLMs are initially pre-trained on extensive corpora to acquire general capabilities, and subsequently, they are fine-tuned on smaller, task-specific datasets to adapt to particular tasks or domains. However, it has been observed that during the fine-tuning process, LLMs may forget the knowledge acquired in pre-training, leading to a decline in general capabilities. Therefore, addressing the issue of forgetting during fine-tuning has become an important research direction for LLMs.\nIn the literature, two classes of methods are commonly adopted to mitigate the forgetting: reply-based methods, and regularization-based methods.\n\u2022 Reply-based methods leverage pre-training data during the fine-tuning process . However, most open-source LLMs, such as the Llama series , have not fully disclosed their pre-training datasets. Moreover, even with access to pre-training data, incorporating it into the fine-tuning process may significantly increase computational and memory costs.\n\u2022 Regularization-based methods add penalty terms to the loss function, encouraging the fine-tuned model to remain close to the pre-trained model, thereby reducing the risk of forgetting pre-training knowledge . However, as we will present later, modifying the original loss function during fine-tuning may impair the model's performance on the fine-tuning task.\nIn this paper, we design a reply-free and regularization-free method to mitigate forgetting during the fine-tuning process. We propose the Momentum-Filtered Optimizer (MoFO). At each iteration, MoFO selects and updates only the parameters with the top a% largest momentum magnitudes within each block, where a is the filtering hyperparameter. Here, the blocks refer to the parameters of different parts of the network (e.g., weight matrices and bias terms).\nOur method is motivated by the following observation. The fine-tuning loss landscape of LLMs has many minima. By conventional regularization-based methods, we can obtain a fine-tuned model that is close to the pre-trained model, thereby mitigating the forgetting effect. However, adding regularization will change the original loss function and degrade the performance of fine-tuning tasks. In contrast, fine-tuning with MOFO converges to a closer minimum of the original loss function, reducing the risk of forgetting without sacrificing the fine-tuning performance.\nOur contributions are as follows:\n\u2022 We propose the MoFO algorithm, a new optimization method designed to mitigate the forgetting of pre-training knowledge during the fine-tuning process.\n\u2022 We present an initial convergence analysis of the MoFO algorithm.\n\u2022 We conduct experiments on various tasks, demonstrating that MoFO outperforms existing methods both in fine-tuning performance and mitigating forgetting."}, {"title": "Momentum Filtered Optimizer (MoFO)", "content": "2.1 Motivation\nIt is hypothesized that the severity of forgetting during fine-tuning correlates with the distance between the fine-tuned model and the pre-trained model: in general, models that remain closer to the pre-trained model are less likely to experience forgetting . This hypothesis motivates a line of regularized-based methods to mitigate forgetting . However, adding regularization changes the original loss function, which can impair the model's performance on the fine-tuning task (see evidence in Section 3). It is intriguing to design a new method to mitigate forgetting without changing the loss function. In this work, we propose a new optimization algorithm that can converge to minima (of the original fine-tuning loss) close to the pre-trained model without sacrificing the fine-tuning loss.\nWe first discuss how to keep the model closer to the pre-trained model. To achieve this goal, we recall the classical block coordinate descent (BCD) method . We believe that BCD algorithm may converge to a point that is closer to the pre-trained model than full parameter fine-tuning. This is because BCD only updates a subset of parameters at each iteration. Compared to full parameter updates, this method usually involves smaller adjustments to the parameters.\nThe remaining issue is how to design BCD that reaches a similar performance to the default methods. An effective strategy is to prioritize updating parameters that have the greatest influence on reducing fine-tuning loss. A straightforward approach is to measure the parameter's influence by the magnitude of its gradient. However, in the widely used Adam optimizer, momentum directly affects parameter updates, while gradients influence parameter updates indirectly by affecting the momentum.\nMotivated by these discussions, to mitigate forgetting and achieve comparable performance in fine-tuning tasks, we will modify the Adam optimizer by updating the subset of parameters with the largest momentum magnitude. We will discuss more details in the next section.\n2.2 Algorithm Formulation\nWe formally introduce the Momentum-Filtered Optimizer (MoFO) in Algorithm 1. MoFO partitions all the parameters into B fixed parts as shown in Line 4. At each iteration, MoFO selects the parameter entries with the largest \u03b1% momentum magnitudes in each part as shown in Lines 10-13 of Algorithm 1, where the update fraction \u03b1% is the pre-determined hyperparameter. The momentum filtering mechanism is illustrated in Figure 1.\nWe note that the NN parameters are naturally composed of different parts (e.g., weight matrices, bias terms) in the network architecture, and PyTorch's backward propagation mechanism automatically returns the gradients of the loss with respect to each part of the parameters. To reduce computational complexity, we partition all parameters according to these fixed parts and select \u03b1% of the parameter entries for BCD update within each part.\nIn Section 3.4, we will empirically demonstrate that MoFO's momentum-based selection rule outperforms its gradient-based variant in fine-tuning tasks. It indicates that the momentum-based selection rule allows for better incorporation with Adam during the optimization process in fine-tuning.\nMoFO efficiently selects and updates the most influential parameters, as dictated by the momentum's magnitude, thus enhancing the fine-tuning process while alleviating the catastrophic forgetting of pre-training knowledge.\n2.3 Initial Exploration\nIn this section, we empirically examine whether an LLM fine-tuned with MoFO converges to a minimum closer to the pre-trained model compared to the one fine-tuned with the Adam optimizer, and whether MoFO mitigates catastrophic forgetting in LLMs.\nWe fine-tune the Pythia-160m on the same dataset used in the experiment described at the beginning of Section 2, using both the Adam optimizer and MoFO. First, as shown in Figure 2(a), both MoFO and Adam optimizer achieve minimal fine-tuning loss, so switching Adam to MoFO does not lead to performance degeneracy. Second, as shown in Figure 2(b), the distance from the pre-trained model to the minimum reached by MoFO is approximately 20% of the distance to that reached by the Adam optimizer. This shows that MoFO significantly reduces the amount of parameter movement.\nWe further verify whether the reduced parameter movement (by MoFO) effectively mitigates the forgetting of general capabilities acquired by pre-training. We evaluate the degree of forgetting from two perspectives: pre-training loss and evaluation benchmarks. First, we find MoFO has a lower pre-training loss compared to the Adam optimizer, indicating that MoFO memorizes pre-training data better. Second, in Table 1, we evaluate the accuracies of Pythia-160m, fine-tuned using both the Adam optimizer and MoFO, on widely-used common sense tasks, which measures the commonsense reasoning capabilities of LLMs."}, {"title": "Convergence Result", "content": "In this section, we conduct a convergence analysis of MoFO. We study a simplified version of our MoFO algorithm as a variant of gradient descent (GD), which is described by Algorithm 2.\nTheorem 1 (Convergence of MoFO). Suppose that the minimum value of the loss function is L* and the gradient \u2207L is Lipschitz continuous with constant L. Then for the GD version of MoFO in Algorithm 2, if the learning rates satisfy \u03b7t = 1/L, it holds that\n$\\min_{0<t<T} ||g_t||_\\infty = O(T^{-2}).$\nIn summary, we demonstrate the convergence of a GD version of MoFO, providing theoretical support for the strong performance of MoFO in fine-tuning tasks. We note that it seems rather non-trivial to prove the convergence of the original version of MoFO: MoFO contains 1st and 2nd-order momentum in a fractional form, and such structure is known to be challenging to handle . We leave the convergence of the original MoFO method as an interesting future direction."}, {"title": "Experiments", "content": "Now we verify the effectiveness of MoFO on instruction fine-tuning and continual fine-tuning. We use Llama-2-7B  and TinyLlama-1.1B  as the base models for our experiments. We also provide studies on the choice of update fraction and update strategies of MoFO.\n3.1 Experimental Settings\nDatasets for instruction fine-tuning. This group of datasets covers question-answer pairs from different domains like mathematical reasoning and code generation. Specifically, the datasets include:\n\u2022 MetaMathQA . This dataset comprises 395K math question-answer pairs. Numerous studies indicate that LLMs significantly enhance performance metrics on mathematical benchmarks such as GSM8K after fine-tuning on this dataset. In this paper, we randomly select 10% of this dataset for training LLMs, which includes 33,000 question-answer pairs.\n\u2022 Code-Alpaca . This dataset contains 20,022 question-answer pairs. We fine-tune LLMs on this dataset to enhance their coding capabilities.\nDatasets for continual fine-tuning. We investigate the performance of MoFO in the continual fine-tuning scenario by implementing our approach on the TRACE benchmark dataset . TRACE benchmark is designed with a comprehensive set of 8 distinct tasks across various domains, including domain-specific knowledge, multilingual proficiency, code generation, and mathematical reasoning.\nMetrics for instruction fine-tuning. We introduce a set of widely used benchmarks to assess the performance and catastrophic forgetting effects on the general capabilities of LLMs after instruction fine-tuning. These benchmarks include:\n\u2022 MMLU (Massive Multitask Language Understanding) . It is a popular benchmark to evaluate factual knowledge of LLMs. This benchmark spans 57 diverse subjects, ranging from STEM fields and the humanities to social sciences. We report the 0-shot accuracy.\n\u2022 Commonsense. We employ the widely recognized benchmarks ARC-Challenge, ARC-Easy , and HellaSwag to measure the commonsense reasoning capabilities of LLMs. In this paper, we refer to these benchmarks collectively as the Commonsense benchmark, and we use the average of these three metrics as the evaluation for this Commonsense benchmark.\n\u2022 GSM8K . This benchmark consists of 8.5K high-quality grade school math problems. We evaluate the math capability of LLM on the test set of GSM8K through the LM Eval Harness framework . We follow the default implementation setting of LM Eval Harness and set the temperature hyperparameter as 0 and report 5-shot accuracy.\n\u2022 HumanEval . We adopt the widely used HumanEval to assess the coding capabilities of LLMs. It comprises 164 unique programming problems. We report the pass@10 performance.\nMetrics for continual fine-tuning. To evaluate the LLM's performance in continual learning, we consider two key metrics in this scenario: Overall Performance (OP)  and BackWard Transfer (BWT) . Let $R_{t,i}$ be the test accuracy on task $T_i$ after we have gone through tasks $T_1, ..., T_t$ (1 \u2264 i \u2264 t \u2264 T). The OP score measures the average accuracy on all the T tasks after the continual fine-tuning process, which is defined by\n$OP := \\frac{1}{T} \\sum_{i=1}^T R_{T,i}$\nThe BWT score measures the average accuracy change of each task after learning new tasks, which is defined by\n$BWT := \\frac{1}{T-1} \\sum_{i=1}^{T-1} (R_{T,i} - R_{i,i}).$\nThese metrics provide a comprehensive assessment of the model's ability to learn incrementally while retaining knowledge from past experiences. In this paper, we will utilize these scores to evaluate the effectiveness of our method in continual learning tasks.\n3.2 Instruction Fine-Tuning\nIn this section, we aim to investigate the effectiveness of the MoFO approach in preserving general capabilities while learning fine-tuning tasks. We fine-tune Llama-2-7B on MetaMathQA and Code-Alpaca datasets, and compare the performance of MoFO against several baseline methods. The evaluation includes changes in performance on fine-tuning tasks and general capability metrics, with the pre-trained model's performance serving as the reference point for comparison. The implementation details can be seen in B."}, {"title": "Further Analysis", "content": "In this section, we first investigate the impact of the update fraction of parameters in the MoFO algorithm at each iteration, and then explore the effects of different update strategies within MoFO.\nImpact of update fraction of parameters in MoFO. Following the setting in Section 4.2, we fine-tune Llama-2-7B on the MetaMathQA dataset using MoFO with varying update fractions of parameters at each iteration for 2 epochs. The experimental results of math reasoning (GSM8K) and average general capability performance changes are presented in Figure 3.\nThe parameter update fraction affects the fine-tuning performance. Figure 3(a) shows that larger update fractions can improve MoFO's optimization effectiveness. Furthurmore, MoFO with a 5% parameter update fraction is sufficient to achieve nearly 90% of the performance of Full FT. When the update fraction exceeds 40%, MoFO's performance matches that of Full FT.\nThe parameter update fraction also affects the preservation of general capabilities. Figure 3(b) indicates that MoFO avoids forgetting in general capabilities when the parameter update fraction is below 20%. Beyond the threshold of 20%, further increases in the parameter update fraction lead to a decline in general capabilities. Despite this, MoFO still forgets significantly less than Full FT.\nIn summary, MoFO can preserve pre-training knowledge and significantly enhance fine-tuning performance by choosing a moderate update fraction, avoiding the extremes of too small or too large fractions.\nImpact of update strategy in MoFO. In addition to MoFO, we consider two other BCD methods, randomized BCD and gradient-filtered BCD. Randomized BCD updates a random subset of parameters at each iteration. Gradient-filtered BCD selects the filter based on gradient magnitudes rather than the momentum magnitudes used in MoFO. Specifically, line 11 in Algorithm 1 is replaced by:\n$(FILTER_t^{(k)})_{i} = 1 \\text{ if } |(g_t^{(k)})_i| \\text{ is among the top-}\\%\\text{ of all } |g_t^{(k)}| \\text{'s entries else } 0.$\nWe fine-tune Llama-2-7B on MetaMathQA using these three methods with 10% parameter update fraction and present the results in Table 5.\nExperimental results show that all three BCD methods exhibit significantly less forgetting compared to Full FT, demonstrating the effectiveness of BCD algorithms in mitigating catastrophic forgetting.\nIn terms of GSM8K performance, our proposed MoFO method significantly surpasses both Gradient-filtered BCD and randomized BCD, indicating that updating parameters with the largest momentum leads to strong optimization power."}, {"title": "Why MoFO Converges to a Closer Point", "content": "In Section 3, we discuss how the distance of parameter movement during fine-tuning impacts the retention of pre-training knowledge. Drawing on these insights and the BCD method, we introduce MoFO. Here, we propose the following question as follows.\nQuestion: Why does MoFO converge closer to the pre-trained LLMs than those of Adam?\nWe attempt to answer this question by the following toy example. We denote $\u0398 = (\u03b8_1, \u03b8_2) \u2208 R^2$ to be the trainable parameters of our model and make the following assumptions:\n\u2022 The pre-training loss is $L_{pretrain}(\u0398) = \u03b8_1^2 + \u03b8_2^2$ and the model has been trained to the global minimum $\u0398_0 = (0,0)$ during the pre-trained phase.\n\u2022 The fine-tuning loss is $L_{finetune}(\u0398) = (\u03b8_1 \u2212 1)^2(\u03b8_2 \u2212 1)^2$. In this case, any global optimum of $L_{finetune}$ lies in the set ${ (1, \u03b8_2) : \u03b8_2 \u2208 R } \u222a { (\u03b8_1, 1) : \u03b8_1 \u2208 R }$, which is a union of two straight lines.\nFor full-parameter fine-tuning with Adam, starting from $\u0398_0 = (0,0)$, the model converges to (1,1) during the fine-tuning phase along the orange arrow in Figure 4, with a pre-training loss of 2. In contrast, when applying MoFO, the model converges to (1,0) during the fine-tuning phase along the green arrow in Figure 4, resulting in a pre-training loss of 1. This demonstrates that MoFO can converge to a minimum that is closer to the pre-training model, thereby mitigating forgetting.\nInsights: In this example, we find that when a loss function has multiple distinct minima, they can be considered as different attractors. These attractors can influence the gradient direction of a pre-trained model, possibly drawing the model's weights away from the nearest minimum. Specifically, full-parameter gradient descent based methods may converge to the balanced point of these attractors' influences, which is the orange point in Figure 4(a). On the contrary, MoFO addresses this issue by updating only a subset of parameters during each iteration. This selective updating rule reduces interference among attractors, allowing the model to converge to a closer minimum."}, {"title": "Related Works", "content": "Catastrophic forgetting Catastrophic forgetting, a significant issue where models forget previously learned information upon learning new data, has received considerable attention in machine learning . In the realm of LLMs, there has been a growing body of recent works focusing on the forgetting of models' knowledge during the fine-tuning process.\nResearchers have proposed numerous methods to alleviate forgetting in continual learning, which involves learning a sequence of tasks. These methods are not limited to learning in a sequential manner and can be applied to broader paradigms. Generally, three primary approaches are used: reply-based methods, regularization-based methods, and architecture-based methods.\n\u2022 Reply-based methods leverage past experiences to facilitate the learning of new tasks. The most straightforward implementation of this approach is experience replay, which involves maintaining old samples in a buffer and replaying them during incremental training with some variants enhancing performance or optimizing memory usage . Several other variants utilize gradient information from past tasks . In LLMs, several works propose reply-based methods to mitigate forgetting. We emphasize that our MoFO method is orthogonal to reply-based methods. MoFO can be integrated into these replay strategies to further enhance their effectiveness.\n\u2022 Regularization-based methods introduce constraints to preserve old knowledge. Several studies add regularization terms to the loss functions to penalize parameter changes and mitigate forgetting . Some other works apply regularization to the embedding or output changes . Unlike these approaches, MoFO does not modify the loss function and as a result, MoFO reached better SFT performance (see Table 2 and 3).\n\u2022 Architecture-based methods balance the goals of learning new knowledge and keeping the old one through model architectural modifications. LoRA , as the most popular parameter-efficient fine-tuning (PEFT) methods, modifies the model architecture by freezing the pre-training weights and introducing low-rank trainable matrices. Empirical works shows that LoRA forgets less but learns less during fine-tuning . Some variants of LoRA find applications in continual learning of LLMs . In comparison, our MoFO still allows for high-rank updates to achieve better fine-tuning performance.\nAnother line of architecture-based methods focuses on model merging. The idea stem from the understanding that the task-specific knowledge is located at a small subspace of the weight space . Consequently, various model merging methods have been proposed to simultaneously retain pre-training knowledge and improve fine-tuning task performance . However, these methods require an additional post-fine-tuning process before model merging. In contrast, our method only requires only one fine-tuning stage.\nBlock coordinate descent Block Coordinate Descent (BCD) involves iteratively optimizing over a block of coordinates while holding the others constant. The foundational work of  provides a comprehensive analysis of the convergence properties of BCD under certain conditions. Subsequent research has explored various BCD variants , including randomized BCD , cyclic BCD , and greedy BCD . Among these, the greedy variant, also known as Gauss-Southwell BCD method, has drawn attention due to its ability to prioritize coordinates that yield the most substantial improvement in each iteration, thereby potentially accelerating convergence.\nIn the realm of machine learning, BCD has also found applications . For example,  leverages BCD to perform memory-efficient fine-tuning of LLM and  use random masking to perform this. A recent work, half fine-tuning approach  addresses catastrophic forgetting during fine-tuning of LLMs by selectively freezing half of the parameters during training. Our approach is akin to a more efficient greedy BCD, achieving superior performance in fine-tuning tasks and alleviating forgetting better."}, {"title": "Conclusion", "content": "This paper presents the Momentum-Filtered Optimizer (MoFO), a new approach designed to mitigate the crucial issue of pre-training knowledge forgetting in LLMs during fine-tuning. By selectively updating the parameters with the largest momentum magnitudes in each parameter block, MoFO converges to a point closer to the pre-trained model compared to full-parameter fine-tuning and effectively preserves pre-trained knowledge. Our experimental results demonstrate that MoFO not only significantly alleviates catastrophic forgetting but also surpasses the performance of traditional fine-tuning methods. Future work will explore further optimizations and potential applications of MoFO in multimodal LLMs."}, {"title": "Proof of Theorem 1", "content": "Before providing the proof, we will give some preliminary information.\nLet's assume the parameter space Rd is decomposed as $R^d = R^{d_1} \\times R^{d_2} \\times ... \\times R^{d_B}$. Here, B represents the number of parameter matrices in the neural network, and dk denotes the product of the dimensions (i.e., the number of rows multiplied by the number of columns) of the k-th parameter matrix. For any $z \\in R^d$, we denote\n$||z|| = Concat(z^{(1)}; z^{(2)};...;z^{(B)}),$\nwhere $z^{(k)} \\in R^{d_k}$ for any $1 \\le k \\le B$.\nDefinition 1. For any $z \\in R^d$, we define\n$||z||_{top-\\alpha\\%} := ||z \\odot e_s ||_2,$\nwhere $e_s = Concat(e_s^{(1)}; e_s^{(2)};...;e_s^{(B)})$. Here,\n$S_k = { i \\in [d_k] : |z_i^{(k)}| \\text{ is among the top-} \\alpha\\% \\text{ of all } |z^{(k)}| \\text{'s entries } (|z_1^{(k)}|, |z_2^{(k)}|,...,|z_{d_k}^{(k)}|) }$\nand $e_i^{(k)}$ is a $d_k$-dimensional vector where the i-th entry is 1 if $i \\in S_k$, and 0 otherwise.\nLemma 1. $||\\cdot||_{top-\\alpha\\%}$ is a norm in $R^d$.\nProof. By Definition 1, we get\n$||z||_{top-\\alpha\\%} = ||z \\odot e_s ||_2 = \\sqrt{\\sum_{k=1}^B ||z^{(k)} \\odot e_s^{(k)}||_2^2}$                                                                                                   (1)\nFirst, if $||z||_{top-\\alpha\\%} = 0$, then by (1), $||z^{(k)} \\odot e_s^{(k)}||_2 = 0$ for any $1 \\le k \\le B$. Thus,\n$||z^{(k)}||_{\\infty} = \\underset{1<i<d_k}{argmax} |z_i^{(k)}| \\le ||z^{(k)} \\odot e_s^{(k)}||_2 = 0.$\nSo $z^{(k)}$ is a zero vector for any $1 \\le k \\le B$ and then z is a zero vector.\nSecond, for any given c\u2208 R+, ${|z_i^{(k)}|}\\}_{1<i<d_1}$ and ${\\{cz_i^{(k)} \\} \\}_{1<i<d_k}$ have the same order. So z and cz share the same filter es and\n$||cz||_{top-\\alpha\\%} = ||cz\\odot e_s ||_2 = c||z\\odot e_s||_2 = c||z||_{top-\\alpha\\%}.$\nThird, for the vectors $x, y \\in R^d$, we denote the greedy filter of x, y, x + y by S, S', S'', respectively.\n$||x + y||_{top-\\alpha\\%} = || (x + y) \\odot e_{s''} ||_2$\n$= ||x \\odot e_{s''} + y \\odot e_{s''} ||_2$\n$\\leq ||x \\odot e_{s''} ||_2+ ||y \\odot e_{s''} ||_2$\n$\\leq ||x \\odot e_s ||_2 + ||y \\odot e_{s'} ||_2$\n$= ||x||_{top-\\alpha\\%} + ||y||_{top-\\alpha\\%}.$\nProof of Theorem 1. By the definition of the simple version of MOFO in Algorithm 2, we have\n$\u0398_{t+1} \u2212 \u0398_t = \u2212\u03b7_t m_t \u2299 FILTER_t = \u2212\u03b7_t g_t \u2299 FILTER_t,$\nwhere FILTERt is actually es in Definition 1."}, {"title": "Implementation Details", "content": "Instruction fine-tuning. In our instruction fine-tuning experiments, we follow the implementation of Ivison et al. [2023]. The learning rate is set to 2e-5 with a cosine decay scheduler. For fine-tuning on the MetaMathQA dataset, we set the maximum sequence length to 1024, the batch size to 128, and we train the Llama-2-7B for 2 epochs. The parameter update fraction for MoFO is set to 15%. For fine-tuning on the Code-Alpaca dataset, we set the batch size as 64 and the parameter update fraction as 10%, while keeping other settings the same.\nContinual fine-tuning. In our continual fine-tuning experiments, we follow the default settings of the TRACE benchmark. We sequentially train TinyLlama-1.1B on the TRACE benchmark datasets: C-STANCE, FOMC, MeetingBank, Py150, ScienceQA, NumGLUE-cm, NumGLUE-ds, and 20Minuten for 5, 3, 7, 5, 3, 5, 5, and 7 epochs, respectively. We use a learning rate of 1e-5 with a cosine decay schedule and a batch size of 64. The parameter update fraction for MoFO is set to 5%.\nAll experiments are conducted on four A800 (80GB) GPUs."}]}