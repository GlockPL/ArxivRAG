{"title": "Efficacy of Various Large Language Models in Generating Smart Contracts", "authors": ["Siddhartha Chatterjee", "Bina Ramamurthy"], "abstract": "This study analyzes the application of code-generating Large Language Models in the creation of immutable Solidity smart contracts on the Ethereum Blockchain. Other works such as Evaluating Large Language Models Trained on Code, Mark Chen et. al (2012) have previously analyzed Artificial Intelligence code generation abilities. This paper aims to expand this to a larger scope to include programs where security and efficiency are of utmost priority such as smart contracts. The hypothesis leading into the study was that LLMs in general would have difficulty in rigorously implementing security details in the code, which was shown through our results, but surprisingly generally succeeded in many common types of contracts. We also discovered a novel way of generating smart contracts through new prompting strategies.", "sections": [{"title": "INTRODUCTION", "content": "Since the creation of Ethereum in 2015 which allowed for programs to run on the Blockchain, Smart Contracts have seen a surge in popularity and application. These \"Smart Contracts\" are really just immutable pieces of code executing directly on the Blockchain, allowing for more effective storage and exchanging of information and currency. Applications of smart contracts include business, education, and entertainment, stemming from the idea of \"Web3,\" or a decentralized internet. With new advancements in Al in recent years such as the creation of Large Language Models (LLMs), AI has demonstrated success in generating software code across a spectrum of use cases. However, the use of LLMs for software development has not largely extended towards smart contract generation. This paper examines the efficacy of various LLMs in this specialized task. The importance of such a study is highlighted by the crucial need for auditing smart contracts as, once published, they are unalterable, necessitating optimal security and efficiency before being appended to the Blockchain. Any vulnerabilities can open the door to hacking incidents, leading to substantial losses of money and data, such as has been the case with numerous Web3 companies in the past. Given the rising usage of AI in code generation, a pressing question emerges: can AI-generated code meet the stringent security standards required in smart contracts? This research hopes to provide insights into this crucial query.\n\nThe study will aim to answer the following research questions:\n\u2022 How accurately can different LLMs generate smart contracts?\n\u2022 What are the specific strengths and weaknesses of each evaluated LLM in the context of smart contract generation?\n\u2022 How do LLM-generated smart contracts compare with manually created contracts in terms of reliability, accuracy, safety and efficiency?"}, {"title": "METHODOLOGY", "content": "To study the application of AI in Smart Contract generation, we will select a range of representative LLMs that are widely used or have shown potential in the generation of smart contracts: GPT 3.5, GPT 4, GPT 4-0, Cohere, Mistral, Gemini and Claude.\n\nWe will use the following sequence of steps for each contract:\n1. Prompting Techniques: We will have both descriptive prompts and structured prompts. They will outline exactly the variables and functions needed. There will be some ambiguity in methods/style of writing to test the LLM abilities. The descriptive prompt mimics how an average user may prompt the LLM, describing it while the structured prompt provides a complete outline for the contract, while the structured prompt will be similar to pseudo-code. It is expected the LLMs will perform better with structured prompts, but we will primarily analyze results on descriptive prompts as that is what is most commonly used when generating code from LLMs.\n2. Generation of Smart Contracts: We will provide each LLM with the same set of prompts (descriptive prompts that outline exactly the variables and functions needed), designed to create a variety of smart contracts.\n3. Testing: We will write a test file in TypeScript to evaluate the performance of the contracts through a series of test cases that the code must pass.\n4. We'll evaluate each contract on multiple dimensions such as:\n Accuracy - How many tests the code passes\n Efficiency - How efficient was the code in terms of execution time\n Verboseness/Quality - Quality of code outputted (qualitative)"}, {"title": "Case 1: Reading and writing a variable onto the Blockchain", "content": "Overview:\nThis is the most basic type of smart contract which is to simply store and edit a variable on the Ethereum Blockchain. It is expected that all LLMs should be able to complete this task fully.\n\nDescriptive Prompting:\nCreate a smart contract using solidity 0.8.20 called \"Variable\" with a strictly positive, public variable \u201cval\u201d that is defined in the constructor with an input parameter, a function \"modify\u201d with a parameter that the contract sets \"val\" to and a function \"retrieve\" that returns the value of the variable"}, {"title": "Structured Prompting:", "content": "contract Variable\n+ val: uint public\n+ constructor(uint_val)\n // Sets var to parameter_val\n+ modify(uint_val)\n // Sets var to parameter_val\n+ retrieve() returns uint\n // return value of val\nTest Cases:\n1. Set variable correctly after initializing contract\n2. Retrieve function returns variable correctly\n3. Modify fails if negative is passed into function\n4. Modify changes variable\n5. Retrieve function returns variable correctly after modification\nGeneral Observations for Case 1\nGPT-3.5 added an unnecessary check of the variable to be greater than 0, which is not necessary since the data type is \"uint\" and will revert automatically if the parameter is negative"}, {"title": "Case 2: Lock some amount of money for finite amount of time on the Ethereum Blockchain", "content": "Overview:\nThis contract is developed by Hardhat (a Web3 programming environment) in their initialization example.It is relatively simple and not meant to be used exactly on scale, but it is useful to see how the LLMs will perform on this initial straightforward task.\n\nDescriptive Prompt:\nCreate a smart contract using Solidity 0.8.20 called \"Lock\" with public variables \"unlockTime\" that gets defined in the constructor by the parameter and \"owner\" which is set to the address that initializes the contract, it also receives some amount of ether which is stored in the contract. Write a function \"withdraw\" that sends the balance of the contract to the address upon the conditions that the time is passed the unlockTime and the address is the owner address. This function should trigger an event \"Withdrawal\" with parameters of the balance and the timestamp"}, {"title": "Structured Prompting Technique:", "content": "contract Lock\n+ unlockTime: int\n+ owner: address, payable\n+ event Withdrawal(uint amount, uint time)\n+ constructor(uint unlockTime)\n // initializes owner and unlockTime\n+ withdraw()\n // sends money if criteria is met\nTest Cases:\n1. Set correct unlockTime\n2. Set correct owner\n3. Receive funds and store in contract\n4. Revert if set unlockTime is below current time (in the past)\n5. Revert if withdraw is called before unlockTime\n6. Revert if account calling withdraw is not owner\n7. Should not fail if account is owner and unlockTime has passed\n8. Emit event after successful withdrawal\n9. Send funds to owner account after successful withdrawal\nGeneral Observations for Case 2\nGPT 3.5 added extra parameter of the receiver (not specified in either the descriptive or structured prompt) for Withdrawal (unspecified and unnecessary):\nAdditionally GPT 3.5 didn't make the constructor payable (should have been assumed from the prompt, which did not allow it to function at all).\nBard included verbose \"public\" modifier for constructor, which is unnecessary\nBoth GPT-4 and Bard overlooked the obvious test case that the unlockTime must be greater than the initialization time, and failed that test case (potentially could have lead to a bug or security issue)"}, {"title": "Case 3: Create a new token on Ethereum Blockchain", "content": "Overview\nNext, we examine LLMs in a much more practical use case, the creation of a standardized ERC20 coin with custom features. We can customize the coin with\n1. Supply\n2. User authentication and class of User\n3. Storage, exchanging, and payment methods (such as allowances from the contract)\nWe embedded these customizations into our prompt to add complexity."}, {"title": "Descriptive Prompt:", "content": "Create a ERC 20 Crypto token using Solidity 0.8.20 with following properties:\n\u2022 Name of the token is \"LLM Token\"\n\u2022 We'll allow 2 decimal places\n\u2022 Require users to register/sign up\n\u2022 Minting not allowed till certain date/time inputted into contract constructor\n\u2022 Allow users to mint new tokens by depositing some ether\n\u2022 The token has lazy supply starting at 10M\n\u2022 Rank for different users (as a data structure)\n 0: can only trade/swap\n 1: can mint/burn\n\u2022 Users who are rank 1 can grant other users rank 1"}, {"title": "Structured Prompt:", "content": "contract LLMToken (ERC 721)\n+ struct User:\n // Defined object with name, address, rank\n+ usersList: User[]\n+ mintingTime: uint\n+ constructor()\n // set minting time, initialize ERC721 token with supply at 10M\nmint()\n // mint new token, only allowed for rank 1\n+ burn(token)\n // burn token, only allowed for rank 1\n+ transfer(address)\n // transfer tokens between users, allowed for all rank\n+ grantRank(address)\n // grants rank 1 to address only if rank of user is 1\nTest Cases:\n1. Create supply of tokens\n2. Allow user to register\n3. Allow only rank 1 user to mint\n4. Transfer from two accounts\n5. Allow rank 1 user to grant rank 1 to another user\nGeneral Observations for Case 3\nUsing imported libraries for contract, GPT 4 imported openZeppelin to use the standardized ERC 20 token. This is significant as openZeppelin is a library that is commonly the standard for ERC 20 tokens in industry nowadays.\nHowever, while GPT-4 recognized the use of common libraries such as Openzeppelin, the code was outdated and did not initialize the implemented contracts which caused a compilation error leading to the code not being able to be run.\nThis is the first case where the AI models noticeably start to differentiate as neither GPT 3.5 nor Gemini Pro implemented the openZeppelin libraries by default. This is inefficient to a large degree, as the contracts necessary for ERC are already built in but these models try to do it themselves. Additionally GPT 3.5 uses the wrong version of Solidity specified."}, {"title": "SIGNIFICANCE OF DIFFERING PROMPTING STRATEGIES", "content": "From our experiments, we find that foundational models generally performed better using descriptive prompting as opposed to structured prompting. This is likely because these models have been trained more extensively with human-like input, which is closer to descriptive prompts. Descriptive prompts provide natural language instructions that align more closely with the models' training data, resulting in better understanding and generation of code. However, structured prompting, which involves providing more detailed and specific instructions akin to pseudo-code, often resulted in lower performance not due to code inefficiency but primarily because of compilation errors. This suggests that while structured prompts hold potential for precise code generation, the models require further training to handle the specificity and technical demands of such prompts effectively."}, {"title": "OVERALL FINDINGS", "content": "From the qualitative and quantitative data collected, we can determine several key insights about the performance of large language models (LLMs) in smart contract generation:\n1. Best Performers: Claude and GPT-4-o emerged as the best performers in our evaluations. These models demonstrated superior capabilities in generating accurate and functional smart contracts compared to other models tested.\n2. GPT-4 vs. GPT-3.5: GPT-4 significantly outperformed GPT-3.5 in terms of code accuracy and overall performance. The advancements in GPT-4 over its predecessor are evident in its ability to handle complex tasks more effectively and generate more reliable code.\n3. GPT-4-o Improvements: GPT-4-o represents an improvement over previous models, particularly in generating accurate smart contracts. The enhancements in this model contribute to better handling of the specific requirements of smart contract coding.\n4. Code Quality Issues: Despite the improvements, the quality of code generated by LLMs is still lacking. Many instances of redundant code, outdated code, and compilation errors were observed, especially with GPT-3.5. This highlights the need for further refinement and training to improve code generation quality.\n5. Security Overlooked: Except for Claude, all models tended to overlook security issues unless explicitly mentioned in the prompt. This indicates a critical area where LLMs need improvement to ensure the generation of secure smart contracts.\n6. Inconsistency in Complex Tasks: For more complex tasks, all models exhibited inconsistency, indicating that while they can handle simpler tasks, they struggle with the complexity and intricacies of more advanced coding requirements."}, {"title": "RELATED WORK", "content": "Generative Al is opening the doors for new ways to write programs where humans and machines collaborate. We've seen progress in two popular directions - program induction and program synthesis. Program induction is the process of automatically generating computer programs from a set of input-output examples or specifications. \"Learning to Execute\" by Wojciech Zaremba and Ilya Sutskever (2014) explores the capabilities of Long Short-Term Memory (LSTM) networks in learning to execute simple programs by treating the problem as a sequence-to-sequence task. The paper \"Neural Turing Machines\" by Graves et al. (2014) introduces a novel neural network architecture that combines the learning capabilities of neural networks with the storage and algorithmic power of Turing machines. The model features a neural network controller paired with an external memory bank, enabling it to perform complex tasks like algorithmic operations, sequence processing, and learning simple programs by reading from and writing to memory. The paper \"Neural Program Interpreter\u201d by Reed & de Freitas(2015) explores the use of neural networks to execute programs by interpreting code as data, leveraging techniques from machine learning to improve program synthesis and execution. This approach enhances the generalization capabilities of neural networks in understanding and generating code, with applications in automating complex programming tasks and improving AI-driven software development. The paper \"Evaluating Large Language Models Trained on Code\" assesses the performance of language models specifically trained on programming languages, focusing on their ability to understand, generate, and complete code. It highlights the models' strengths in handling complex coding tasks, suggesting their potential to assist in software development and improve coding efficiency.\n\nMore recent approaches like \u201cEvaluating Large Language Models Trained on Code Universal Transformer (Chen et al., 2019) investigates the performance of large-scale language models specifically trained on programming languages. It demonstrates that Universal Transformers, with their iterative refinement process, can significantly improve code generation and comprehension tasks, showcasing their potential for advanced code-related applications and automation in software engineering.\n\nProgram synthesis is the process of automatically creating a computer program that meets a given high-level specification or set of input-output examples. In program synthesis, models explicitly generate programs from natural language specifications, a task crucial for automating code generation. One classical approach employs a probabilistic context-free grammar (PCFG) to construct a program's abstract syntax tree (AST) (Bod\u00edk et al., 2013). Maddison and Tarlow (2014) improved this approach by introducing a state vector to condition child node expansion, enhancing the generative process. This idea was later adapted by Allamanis et al. (2015) for text-to-code retrieval, demonstrating its effectiveness in matching code snippets to textual queries. Yin and Neubig (2017) further extended this concept to text-conditional code generation, enabling the direct generation of code from natural language descriptions. Additionally, approaches by Parisotto et al. (2016) and Rabinovich et al. (2017) have built upon these foundations, incorporating deep learning techniques to refine the synthesis process. These advancements underscore the significant progress in program synthesis, aiming to seamlessly translate natural language into executable code (Zavershynskyi et al., 2018; Brockschmidt et al., 2019). Programs can be synthesized without relying on an abstract syntax tree (AST) representation, leveraging various alternative approaches. Hindle et al. (2012) explored n-gram language models for code, discovering that code is significantly more predictable than natural language due to its structured nature. This predictability was further exploited by Hellendoorn and Devanbu (2017), who enhanced n-gram models with caching mechanisms to improve code completion tasks. Ling et al. (2016) introduced Latent Predictor Networks, demonstrating that character-level language models could generate functional code for implementing Magic the Gathering cards in an online arena by using a latent mode to copy card attributes directly into the code. This approach highlights the potential of character-level models for complex code generation tasks.\n\nDeepCoder (Balog et al., 2017) advanced this field by training a model to predict the functions that appear in source code, effectively guiding the program synthesis process through a search mechanism. This method was further developed by Devlin et al. (2017), who incorporated a hybrid neural-symbolic approach to enhance the accuracy and efficiency of program generation. Additionally, Rabinovich et al. (2017) and Brockschmidt et al. (2019) introduced graph-based neural networks to model the structural dependencies in code, bypassing the need for explicit AST representations and enabling more flexible program synthesis. These innovations illustrate the diverse methodologies being explored to synthesize programs directly from raw data, underscoring the evolving landscape"}, {"title": "FUTURE WORK", "content": "As we've discussed, moving to more structured prompts significantly improves the quality and efficiency of smart contracts. We'll continue to explore other innovative prompting techniques that can further improve smart contract quality.\n\nIn the current research we've focused only on Ethereum Blockchain which is the most popular Blockchain amongst developers. This means there are more examples available for LLMs to learn from. We intend to extend this work for other Blockchains like Solana, Aptos, Sui etc and test the ability of LLMs to generalize code generation across different Blockchains.\n\nFurthermore, the security of smart contracts remains a critical concern, as numerous instances of smart contracts being exploited by malicious actors have highlighted significant vulnerabilities. To address this, we plan to conduct a comprehensive investigation into the security aspects of the generated smart contracts. This will involve integrating advanced security analysis tools and techniques into our evaluation framework, aiming to identify and mitigate potential vulnerabilities proactively. By doing so, we hope to enhance the overall reliability and safety of AI-generated smart contracts, making them more resilient against exploitation.\n\nAdditionally, we will explore the integration of formal verification methods to rigorously prove the correctness and security of smart contracts generated by LLMs. This multidisciplinary approach, combining AI, blockchain technology, and formal methods, has the potential to set new standards for smart contract development and deployment, ensuring higher levels of trust and reliability in decentralized applications."}]}