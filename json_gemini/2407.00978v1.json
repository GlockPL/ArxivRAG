{"title": "Hybrid RAG-empowered Multi-modal LLM for Secure Healthcare Data Management: A Diffusion-based Contract Theory Approach", "authors": ["Cheng Su", "Jinbo Wen", "Jiawen Kang", "Yonghua Wang", "Hudan Pan", "M. Shamim Hossain"], "abstract": "Secure data management and effective data sharing have become paramount in the rapidly evolving healthcare landscape. The advancement of generative artificial intelligence has positioned Multi-modal Large Language Models (MLLMs) as crucial tools for managing healthcare data. MLLMs can support multi-modal inputs and generate diverse types of content by leveraging large-scale training on vast amounts of multi-modal data. However, critical challenges persist in developing medical MLLMs, including healthcare data security and freshness issues, affecting the output quality of MLLMs. In this paper, we propose a hybrid Retrieval-Augmented Generation (RAG)-empowered medical MLLMs framework for healthcare data management. This framework leverages a hierarchical cross-chain architecture to facilitate secure data training. Moreover, it enhances the output quality of MLLMs through hybrid RAG, which employs multi-modal metrics to filter various unimodal RAG results and incorporates these retrieval results as additional inputs to MLLMs. Additionally, we employ age of information to indirectly evaluate the data freshness impact of MLLMs and utilize contract theory to incentivize healthcare data holders to share fresh data, mitigating information asymmetry in data sharing. Finally, we utilize a generative diffusion model-based reinforcement learning algorithm to identify the optimal contract for efficient data sharing. Numerical results demonstrate the effectiveness of the proposed schemes, which achieve secure and efficient healthcare data management.", "sections": [{"title": "I. INTRODUCTION", "content": "As the backbone of society and human interaction, the healthcare industry has undergone a seismic shift with the advent of digital technology [1]. This transformation has not only revolutionized patient care but also catalyzed the generation, storage, and analysis of vast amounts of healthcare data [2], which encompasses different types of big healthcare data [3], such as omics data, clinical data, electronic health records, sensing data, etc. Although the exponential growth in healthcare data volume holds the potential to revolutionize healthcare by providing insights into patient care, disease patterns, and treatment effectiveness, it also requires sophis- ticated tools for its analysis and interpretation. Fortunately, Generative Artificial Intelligence (GAI) as a new branch of Al has emerged as a potent technology that can effectively analyze vast datasets and generate various content [4], [5]. Es- pecially, by analyzing patient data, including medical history and treatment outcomes, GAI can contribute to personalized medicine by considering individual patient characteristics and generating customized treatment plans [5], [6].\nLarge Language Models (LLMs), as a technological appli- cation of GAI, can achieve general-purpose language gen- eration and conventional natural language processing tasks, which hold the potential to significantly transform healthcare management. With the integration of multi-modal data into LLMs, patients can effectively comprehend many aspects of their physical health through Multi-modal Large Language Models (MLLMs) [7]. For example, the latest GPT-4, equipped with vision capabilities and exceptional performance in natural language processing tasks, can be fine-tuned as a powerful guidance tool in the healthcare domain [8]. However, the sheer size and complexity of MLLMs necessitate efficient retrieval mechanisms to enhance their performance further. Retrieval- Augmented Generation (RAG) is a cutting-edge technique that boosts the reliability and accuracy of GAI models by retrieving facts from an external knowledge base [9]. Furthermore, RAG can capitalize on the similarity between the alignment vectors of the query to retrieve pertinent data, thereby enhancing user prompts by integrating relevantly retrieved data within the con- text, enabling MLLMs to generate accurate and contextually appropriate responses [4]. Thanks to the prominent capabilities of RAG, the integration of MLLMs and RAG has been widely used in various domains [10], [11].\nDespite the advancements in RAG-empowered MLLMs, there are several persistent challenges in the application of these technologies for healthcare data management: 1) Since healthcare data is normally multi-modal and stored in different"}, {"title": "II. RELATED WORK", "content": "Recent advancements in LLMs have significantly con- tributed to data management, with numerous research efforts focusing on various aspects such as data analysis, predictive modeling, and decision support systems. Some studies use the strong interpretative abilities of LLMs as agents to continu- ously improve data storage, data analysis, and additional areas [8], [15]. For instance, the authors in [8] introduced GPT-4, which has demonstrated remarkable capabilities in understand- ing and generating human-like text, facilitating various data management tasks [8]. The authors in [16] presented an LLM- based database framework that leverages LLMs for automatic prompt generation and model fine-tuning, which performs highly effective in query rewriting and index tuning [16]. In [17], the authors introduced Data-Copilot, which is a data anal- ysis agent capable of autonomously querying, processing, and visualizing vast amounts of data to meet various human needs. Existing works mainly focus on unimodal LLMs processing text data, while relatively insufficient research has been done on the integration of multi-modal data such as text, images, and structured data. Addressing this gap could substantially boost the functionality of data management systems, especially in complex and data-intensive fields like healthcare.\nRAG has incredible capabilities in enhancing the accuracy and reliability of LLM output by incorporating additional information sources, such as external knowledge bases, and augmenting user prompts with relevant retrieval data in context [9]. As a novel technique, RAG allows LLMs to bypass retraining, allowing access to the most up-to-date information to generate reliable output through retrieval-based generation [10]. In [9], the authors introduced RAG, demonstrating its ability to improve the accuracy and relevance of generated text by incorporating retrieved documents into the genera- tion process. The authors in [18] proposed a hybrid RAG method, which integrates Sentence-Window and Parent-Child approaches, and demonstrated that the proposed method out- performs current state-of-the-art RAG techniques. The authors in [4] introduced a carbon emission optimization framework"}, {"title": "III. HYBRID RAG-EMPOWERED MEDICAL MLLM\nFRAMEWORK", "content": "In this section, we propose a hybrid RAG-empowered medical MLLM framework. The detailed methodologies for cross-chain interaction in MLLM training and the utilization of hybrid RAG-empowered MLLM agents for data management are discussed in the following subsections.\nIn the health center, robust aggregate MLLMs are de- veloped by training on vast amounts of high-quality multi- modal healthcare data [20]. During the data collection, the health center collects healthcare data sourced from hospitals in diverse regions. However, considering privacy concerns,"}, {"title": "B. Hybrid RAG-empowered MLLMs Agents for Data Manage- ment", "content": "Data management tasks in hospitals and the health center include data storage, analysis, and retrieval. When multi- modal healthcare data is gathered into the subchains, the MLLM agent categorizes the data by type and stores it in the appropriate databases. As healthcare data is needed, the MLLM agent retrieves and analyzes the data, ensuring it meets the specific requirements of the task [16]. To further enhance the ability of MLLMs to analyze multi-modal healthcare data, we design a hybrid multi-modal RAG module [18], which is integrated with a data sharing mechanism inspired by contract theory, ensuring that the MLLM data analysis is conducted with high quality and strong privacy protection, allowing secure and effective handling of multi-modal healthcare data. As shown in the right of Fig. 1, the workflow of the Hybrid RAG-empowered MLLMs Agents for Data Management is presented as follows:\nHospitals and the healthcare center collect available multi-modal healthcare data, convert them into vectors specific to each modality using an embedding model, and store these vectors in the local knowledge database with SQL tools [9].\nWhen a task query is received, the hybrid multi-modal RAG system uses the same embedding model from Step 1 to convert the query into a vector. Then it calculates similarity scores between the task query vector and the vectors within the knowledge database, retrieving and prioritizing the top K vectors that most closely match the task query [10].\nWhen all rele- vant information, particularly multi-modal data, is fed directly into the MLLMs, information overload can result and attention to key details can be reduced due to the inclusion of irrelevant content [10]. To address this, the system further screens the results by applying our Multi-modal Information Similarity (MIS) metric, which is calculated by\n$$MIS = \\sum_{i=0}^{n} W_i f_i(x_1, x_2),$$\nwhere $$f_i()$$ represents the similarity measure function between the task query and the source data in the database and is determined freely according to the requirements of the specific task. $$x_1$$ and $$x_2$$ are the unimodal data corresponding to the task query and the source data in the database, respectively, and the weight factor $$w_i$$ is used to characterize the proportion of each the similarity measure function $$f_i()$$. When the results are re- ranked and filtered by MIS, the retrieved optimized healthcare information is then used to expand the context in the prompt."}, {"title": "IV. PROBLEM FORMULATION", "content": "In this section, we begin by developing a metric for healthcare data quality, followed by the formulation of utility functions for both healthcare data holders and the MLLM service provider. Finally, we propose a contract theory model to motivate healthcare data holders to contribute high-quality healthcare data.\nThe training of MLLMs relies heavily on a large volume of high-quality data [8]. Unfortunately, most healthcare data are stored in hospital databases in various regions. Without data sharing, these valuable resources remain untapped, hin- dering MLLM development. Furthermore, the effectiveness of MLLMs is directly influenced by the quality of the data used in training. Therefore, it is critical to implement an incentive mechanism that encourages hospitals to share healthcare data.\nReferring to [13], we consider the healthcare industry com- prising hospitals in diverse regions and a health center as an example. The health center acts as the MLLM service provider, and the hospitals in diverse regions serve as the healthcare data holders, represented by a set of $$M = \\{1,..., m, ..., M\\}$$. Initially, we propose a healthcare data quality metric through the Aol metric to assess the quality of healthcare data utilized for fine-tuning MLLMs. Subsequently, acting as the data task publisher, the MLLM service provider employs a contract theory model to encourage M healthcare data holders to engage in data sharing [23].\nAol has gained broad acceptance as a metric for assess- ing data freshness, especially within wireless communication networks [27]. In this paper, Aol is described as the duration between the data gathering at the hospital and the finalization of MLLM training. Lower Aol correlates with higher-quality\nMLLM output for healthcare applications. As described in [23], we propose a healthcare data quality metric through AoI, which is relevant for scenarios involving periodic data updates. To generalize, we define the size of healthcare data as l (bytes) and the transmission rate between the health center and hospitals as T (bytes per second). Hence, the transmission time of the healthcare data is $$t_{trans} = l/\\tau$$ [23]. Meanwhile, we denote $$t_u$$ as the time of completing a consensus process among blockchains [13]. Therefore, we represent the length of a single time slot t as $$t = t_{trans} + t_u$$ [13]. To maintain data freshness, each healthcare data provider m periodically updates its healthcare data, with $$\\theta_m$$ indicating the length of a single time slot in each update cycle. The refreshment of healthcare data happens in the initial time slot of the cycle. Referring to [28], the Aol for a data request made in the i-th time slot is $$(i + 1)t$$ for $$i = 2, ..., \\theta_m - 1$$, and for requests initiated in the first or last time slot, the Aol is 2t. Due to the Poisson process [13], [23], data requests are equally likely to occur in any time slot, with a probability of $$1/\\theta_m$$. Therefore, the average Aol for data sharing by healthcare data holder m is given by\n$$A_m(\\theta_m) = \\frac{2}{\\theta_m}(2t) + \\sum_{i=2}^{\\theta_m-1} \\frac{i+1}{\\theta_m}t = t(\\frac{2}{\\theta_m} + \\frac{\\theta_m}{\\theta_m} + \\frac{1}{\\theta_m} - \\frac{2}{\\theta_m}) = t(\\frac{\\theta_m}{2} + \\frac{1}{\\theta_m} - \\frac{1}{2}).$$\nRecognizing that a large Aol can degrade the quality of sensing data, we define the healthcare data quality metric $$G(A_m)$$ based on Aol as\n$$G(A_m) = \\frac{A_{max}}{A_m},$$\nwhere $$A_{max}$$ represents the maximum permissible value for the Aol. The average Aol plays a critical role in the quality of MLLM services. Given that (2) is a convex function in relation to the update cycle $$\\theta_m$$, increasing $$\\theta_m$$ results in a decrease in Aol [23]. Consequently, there is a tradeoff in managing Aol, which can be optimized by modifying the update cycle.\nIn the context of healthcare data sharing for MLLM ser- vices, the utility for each healthcare data holder m is the difference between the reward $$R_m$$ and the cost $$C_m$$ incurred by data sharing tasks, expressed as $$U_m = R_m - C_m$$ [13]. According to [29], the cost for healthcare data holder m is defined as $$C_m = \\xi_m f_m$$ [23], with $$f_m = \\frac{1}{\\theta_m}$$ representing the update frequency and $$\\xi_m$$ denoting the cost of each update [13]. Therefore, the utility of healthcare data holder m is\n$$U_m = R_m - \\xi_m f_m.$$\nThe MLLM service provider lacks precise knowledge of each healthcare data holder's update cost because of the information asymmetry. To address this, the MLLM service provider classifies data holders into discrete types by using statistical distributions derived from historical data, and its expected utility will be optimized [13]. By classifying M healthcare data holders into various types, we denote the k-th type healthcare data holder as $$\\delta_{\\kappa} = \\frac{1}{\\xi_k}$$ and group them"}, {"title": "V. GENERATIVE DIFFUSION-BASED CONTRACT DESIGN", "content": "In this section, we initially formulate the contract design between the MLLM service provider and healthcare data holders as a Markov Decision Process (MDP). Then, we present a GDM-based contract generation model to determine the optimal contract.\n1) State space: To find the optimal contract item, i.e., ($$f$$,$$R$$), $$k\\in K$$, the system first adds Gaussian noise to the initial contract sample. In the current diffusion round t(t = 1,2,...,T), the state space affecting the optimal contract design is defined as\n$$s = \\{M, K, A_{max}, Q, K\\},$$\nwhere M and K are constant values, while $$A_{max}$$, Q = $$(Q_1,..., Q_K)$$, and K = $$(\u03b4\u2081, . . ., \u03b4\u03ba)$$ are generated randomly in the current diffusion round t.\n2) Action space: As the MLLM service provider designs the contract items $$\\{\\Psi\\}$$ to incentivize healthcare data holders to provide high-quality healthcare data, the action $$a^t$$ at round t is defined as\n$$a^t \\in \\{\\Psi\\},$$\nwhere $$\\Psi\\_t = \\{(f_k, R_k), k \\in K\\}$$ determines the update frequency and reward for type-k healthcare data holders.\nFollowing the action $$a^t$$, the MLLM service provider achieves an immediate reward r(s, at) aimed at maximizing the expected utility described in (7) while ensuring compliance with the IR (8) and IC (9) constraints. Thus, the reward function is defined as\n$$r(s, a^t) = \\begin{cases}\nU(f, R), & \\text{if } a \\text{ satisfies (8) and (9)},\\\\\nU_p, & \\text{otherwise,}\n\\end{cases}$$\nwhere $$U(f, R)$$ denotes the expected utility of the MLLM service provider during round t, and $$U_p < 0$$ serves as the penalty for violating either the IR or IC constraints.\nCompared to DRL algorithms that directly optimize model parameters, GDMs enhance contract design through an iter- ative process of denoising the initial distribution [31]. The diffusion model network maps the environmental state to the contract design, which constitutes the contract design policy represented as $${\\pi_\\omega}(a|s)$$ with parameters $$\\omega$$. The contract de- sign policy $${\\pi_\\omega}(a|s)$$ designed to generate an optimal contract over multiple time steps can be expressed as [31]\n$${\\pi_\\omega}(a|s) = p_{\\omega}(a^0,...,a^T|s)$$\n$$ = N(a^T; 0, I) \\prod_{t=1}^{T} p_{\\omega}(a^{t-1}|a^t, s_t),$$\nHere, $$\\{\\pi_\\omega}(\u00b7)$$ represents the reverse process of the conditional diffusion model and $$p_{\\omega}(a^{t-1}|a^t, s_t)$$ is modeled as a Gaussian\ndistribution $$N(a^{t-1}; \\mu_{\\omega}(a^t, s, t), \\Sigma_{\\omega}(a^t, s, t))$$, where the co- variance matrix $$\\{\\Sigma_{\\omega}(a^t, s, t))$$ is formulated as [31]\n$${\\Sigma_{\\omega}(a, s, t)) = \\delta_1 I,$$ where $${\\delta_1} \\in (0,1)$$ is a hyperparameter determined before model training, and I is the identity matrix. Consequently, the mean $${\\mu_{\\omega}(a^t, s, t)}$$ can be given by [31]\n$${\\mu_{\\omega}(a^t, s, t)} = \\frac{1}{\\sqrt{\\chi_t}}(a^t - \\frac{\\delta_t}{\\sqrt{\\chi_t}} \\epsilon_{\\omega}(a^t, s, t)),$$\nwhere $$\\{\\chi_t = 1-\\delta_t}$$, $$\\{\\chi_t = \\prod_{i=0}^{t} \\delta_i}$$, and $$\\{\\epsilon_\\omega}$$ denotes the contract generation network. We first sample $$a^T \u223c N(0, I)$$ and then"}, {"title": "VI. NUMERICAL RESULTS", "content": "In this section, we conduct extensive experiments to assess the performance of the proposed hybrid RAG-empowered MLLM framework in particular healthcare analysis and eval- uate the effectiveness of our proposed incentive mechanism. For MLLM inference, we use Python 3.10.14 running on CPU intel Xeon(R) Gold 6133 and GPU NVIDIA RTX A6000\nto execute tasks. For the simulation setting of the diffusion model, we list the main parameter settings in III.\nWe simulate a prototype of the hybrid RAG-empowered MMLMs with the support of LLaVA-Med [30] and llamain- dex\u00b9. As illustrated in Fig. 2, we present two examples to demonstrate the functionality and application of our frame- work. Upon receiving a task query with multi-modal health- care data, the framework first retrieves the corresponding data from the respective modal database and performs a preliminary screening of P results based on the cosine similarity between vectors. Next, hybrid RAG further refines these results using the MIS metric to identify the best matches, which are then used as inputs to the MLLMs. Finally, the MLLMs process all this information to provide diagnostic outputs and personalized services according to the query.\nWe apply the criteria of Responsive Artificial Intelligence (RAI)\u00b2 to assess whether the outputs of MLLMs present po- tential risks related to morality, bias, and ethics. Additionally, we also assess the relationship between the output of MLLMS and task query with the Semantic Similarity (SS) [35], which reflects the quality of the diagnosis and serves as a crucial indicator for measuring the MLLMs' output. Due to the lack of evaluation benchmarks, we integrate LLM evaluators [36] with prompt engineering techniques [26] to measure the quality of data analysis for MLLMs under the method of GPT4-o, LLaVA, and LLaVA with Hybrid RAG. The scoring is normalized to a range of [0, 1]. Higher scores denote greater reliability and lack of bias in the MLLM output, along with a strong correlation to the task query information. In contrast, lower scores signify a substantial gap from the anticipated results. Finally, we combine the RAI evaluation and SS into a unified score, known as the relative LLM score ( [37], which is calculated using the formula:\n$$\\zeta = \\lambda \u00b7 RAI + \\nu \u00b7 SS,$$\nwhere $$\\{\\lambda}$$ and $$\\{\\nu}$$ are the weighting factors for the RAI and SS, respectively. In our approach, we assign equal weights by setting $$\\{\\lambda = 0.5}$$ and $$\\{\\nu = 0.5}$$.\nFor the proposed contract model, we employ an on-policy GDM algorithm within a double actor-critic framework, and the specific settings of training hyperparameters are shown in Table III. In our setup, we consider 10 healthcare data holders divided into two types, with M = 10 and K = 2. For the two types of healthcare data holders $$\\{\\theta_1}$$ and $$\\{\\theta_2}$$, values are randomly sampled from the intervals (1,6) and (13,18), respectively. Additionally, the maximum tolerance of Aol $$A_{max}$$ is sampled randomly within the range of (30,60). For the utility of the MLLM service provider, the parameters \u03b1, \u03b2, t are set to 39.9, 10, and 2 respectively, and Q1 and Q2 are randomly generated according to the Dirichlet distribution [38]. The experiments are performed on an NVIDIA GeForce RTX A6000 server GPU with CUDA 11.8.\nFirstly, we compare our proposed contract-based incentive mechanism, which operates under information asymmetry, with other methods: a contract-based mechanism with com- plete information, a greedy scheme, and a random scheme. As illustrated in Fig. 4, we can find that our proposed"}, {"title": "VII. CONCLUSION", "content": "In this paper, we studied the service quality issues of MLLMs and the design of incentive mechanisms for healthcare data management. We proposed a hybrid RAG-empowered medical MLLM framework based on cross-chain technolo- gies to enhance data management in the healthcare industry."}]}