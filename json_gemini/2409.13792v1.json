{"title": "Continual Learning for Multimodal Data Fusion of a Soft Gripper", "authors": ["Nilay Kushawaha", "Egidio Falotico"], "abstract": "Continual learning (CL) refers to the ability of an algorithm to continuously and incrementally acquire new knowledge from its environment while retaining previously learned information. A model trained on one data modality often fails when tested with a different modality. A straightforward approach might be to fuse the two modalities by concatenating their features and training the model on the fused data. However, this requires retraining the model from scratch each time it encounters a new domain. In this paper, we introduce a continual learning algorithm capable of incrementally learning different data modalities by leveraging both class-incremental and domain-incremental learning scenarios in an artificial environment where labeled data is scarce, yet non-iid (independent and identical distribution) unlabeled data from the environment is plentiful. The proposed algorithm is efficient and only requires storing prototypes for each class. We evaluate the algorithm's effectiveness on a challenging custom multimodal dataset comprising of tactile data from a soft pneumatic gripper, and visual data from non-stationary images of objects extracted from video sequences. Additionally, we conduct an ablation study on the custom dataset and the Core50 dataset to highlight the contributions of different components of the algorithm. To further demonstrate the robustness of the algorithm, we perform a real-time experiment for object classification using the soft gripper and an external independent camera setup, all synchronized with the Robot Operating System (ROS) framework.", "sections": [{"title": "1. Introduction", "content": "In recent years, machine learning models have matched or even exceeded human-level performance on tasks such as image classification Ahmad et al., object recognition Zou et al., natural language processing (NLP) Otter et al., playing games in simulation, etc. While these models excel in static tasks where the data distribution remains constant, they encounter difficulties in adapting to changing environments and need to restart the training process whenever new data is introduced. On the other hand, humans are adept at learning from dynamic environments, continuously acquiring, up- dating, and applying knowledge over time. We expect AI systems to adapt in a similar fashion.\nContinual learning (CL) Wang et al., Lesort et al., also known as lifelong learning or incremental learning, is a ma- chine learning subfield that focuses on progressively training models on a continuous stream of data to accumulate and retain knowledge over time. However, a significant obstacle for these artificial models is their tendency to forget pre- viously acquired skills when exposed to new tasks or data distributions. This issue, known as catastrophic forgetting, often leads to a substantial drop in performance as the new information overwrites the old knowledge or alters the data distribution. A naive solution to this problem is to save all data, shuffle it, and retrain the model offline from scratch. While this method effectively addresses catastrophic forget- ting, it is time-inefficient, suboptimal, and unsustainable for modern large-scale models that require significant computa- tional power. Continual learning aims to develop methods that balance performance and efficiency more effectively.\nRobots operating in unstructured real world environments will encounter new tasks and challenges over time, neces- sitating capabilities that cannot be fully anticipated from the outset. These robots need to learn continually, acquiring new skills without forgetting previously learned ones.\nA model trained in one domain will be ineffective when deployed in a completely different domain or used with a different data modality than it was originally trained on. One straightforward approach might be to use separate models for each data modality or to jointly train a single model on all modalities offline. However, this is not a feasible solution for agents with limited memory and computational resources that learns new domains through active exploration and in- teraction with its environment. Another challenge is the lim- ited availability of labeled data for supervised incremental training. A large research effort is dedicated towards learning from unlabeled data, which is often more readily available than labeled data in practical applications. Within this effort, the field of semi-supervised learning (SSL) Van Engelen and Hoos has recently demonstrated the most promising results. In this setting, an agent can utilize a significant amount of unlabeled data from its environment. It can employ the knowledge gained from labeled data to learn new objects of the same class or to enrich its understanding of previously learned classes using the unlabeled data.\nIn this paper, we consider a realistic continual learning problem where a single continual learning model learns different modalities of data incrementally by leveraging both class-incremental and domain-incremental learning scenar- ios in an artificial environment where labeled data is scarce yet non-iid (independent and identical distribution) unla- beled data from the environment is plentiful. We mainly ex- periment with two different SSL conditions: Radom images, where for each object of the same parent class we randomly sample some images from the dataset that are not included in the training data; Unique objects, where we sample the images of new unseen objects that belong to the same parent class. More details about the SSL conditions is provided in section 3.3.\nTo perform a multimodal CL experiment, we leverage two different modalities of data mainly the tactile data com- ing from sensors integrated with a soft gripper and the visual data from an independent camera setup. The CL model is designed to learn the classes in both the domains incrementally. To address this issue of multimodal CL, we build on the feature covariance-aware metric (FeCAM) al- gorithm by Goswami et al. and propose an extended ver- sion of the algorithm that employs an incremental online semi-supervised learning process for each task. We call it exFeCAM or extended FeCAM algorithm. Additionally, our algorithm includes an intra-layer feature representation mechanism to learn more generalized feature maps for each class. To the best of our knowledge, we are the first to use a continual learning model in a multimodal data setting where an agent learns both tactile data and vision data provided by respective sensors. The main contributions of this article are summarized as follows:\n1.  We present an online non-exemplar continual learn- ing algorithm with SSL capabilities. In addition, to enhance the feature maps from the pre-trained layers, we introduce intra-layer feature representations.\n2.  We present a new multimodal non-iid dataset for real world continual learning applications.\n3.  We demonstrate the robustness of learning different modalities as a new incremental domain rather than just combining the data from different modalities into a single fused feature vector.\n4.  We evaluate our algorithm on the custom multimodal dataset as well as the Core50 dataset Lomonaco and Maltoni and perform an ablation study to verify the effectiveness of the various parts of the algorithm.\n5.  We also perform a real-time experiment for object classification on the soft pneumatic gripper equipped with sensors and a separate camera setup, all synchro- nised using the ROS framework Koubaa et al.. More details about the experiment is provided in section 6.\nThe paper is structured as follows: In Section 2, an overview of the related works in CL, SSL, and multimodality is presented, with a focus on the recent advancements in the literature. Section 3 provides a detailed description of the experimental setup that has been adopted discussing the dif- ferent cl strategies, data accumulation steps, and evaluation metrics. Following this, the proposed methodology is pre- sented in Section 4, which includes the model architecture and the framework used in this study. The evaluation of the proposed methodology on custom multimodality dataset, as well as the Core50 dataset is provided in section 5. Moreover, in section 6 the paper showcases the real-time application of the proposed algorithm. Finally, a summary noting the advantages and disadvantages of the proposed method along with the scope for improvements and possible developments in the future are stated in the last section."}, {"title": "2. Related Work", "content": "Continual learning strategies can be broadly categorized into four major types: architectural, regularization, rehearsal, and hybrid strategies, each with its own advantages and disadvantages. Architectural strategy either employs a dy- namic modular network where new layers gets added upon encounter of new tasks Rusu et al., Fernando et al., Rajasegaran et al., Aljundi et al. or performs parameter isolation to allocate a dedicated separate sub-space for each task within the same network Mallya et al., Serra et al., Mallya and Lazebnik, Ahn et al.. The simplest form of architectural regularization involves freezing certain weights in the net- work to ensure they remain unchanged during training on new tasks. A major drawback of the architectural strategy is the increase in the size of the architecture as new tasks are learned or the potential saturation of the architecture if a fixed model is used.\nRegularization strategies can be grouped into two: weight regularization and function regularization. Weight regular- ization constrains the update of the network weights by adding explicit regularization terms to the loss function, pe- nalizing changes in each parameter based on its importance Kirkpatrick et al., Zenke et al., Chaudhry et al., Aljundi et al.. Function regularization, on the other hand, employs distillation methods to maintain the knowledge between the various layers of the network. This strategy typically employs the previously-learned model as the teacher and the currently model as the student, while implementing knowledge distillation (KD) techniques Hinton et al., Li and Hoiem, Rebuffi et al. Dhar et al., Lee et al.. However, regularising the model can make it more challenging for the network to acquire new knowledge.\nRehearsal Lopez-Paz and Ranzato, Bang et al., Kumari et al., Ebrahimi et al., Saha and Roy & pseudo-rehearsal, Shin et al., Wu et al., Ayub and Wagner, Pf\u00fclb and Gepperth, Wang et al. approaches offer the most simple yet effective approach by storing a subset of the previously learned data in a memory buffer or by training a generative model to account for the previous information and replaying it during training on new tasks. However, storing the images in a replay buffer for rehearsal can raise privacy concerns. On the other hand, while using a generative model somewhat addresses the privacy issues but may result in increased training time and computational demands.\nHybrid strategies Van de Ven et al., Liu et al., Ye and Bors are a combination of any of the other three strate- gies to compensate for catastrophic forgetting. SynapNet Kushawaha et al. uses a triple model architecture incorporat- ing knowledge distillation along with a VAE-based pseudo- episodic memory for rehearsal and a sleep phase for memory re-organization. DualNet Pham et al. uses a fast learner for supervised learning and a slow learner for unsupervised learning of task-agnostic general representation using semi- supervised learning. ICARL Rebuffi et al. includes an exter- nal fixed memory to store a subset of old task data based on an elaborated sample selection procedure and then employs a distillation step acting as a regularization.\nNon-exemplar continual learning strategies Goswami et al., Petit et al., Smith et al., Hayes et al. use the idea of replay-free sequential training, instead of using a mod- ular network or regularizing the optimization function. Exemplar-free approaches store the prototypes of the past and new classes to compensate for catastrophic forgetting of the previously acquired knowledge. A pre-trained feature extractor provides the feature maps which are then used to calculate the mean and the covariance matrix for the respective classes."}, {"title": "2.2. Semi-Supervised Learning", "content": "Semi-supervised learning describes a class of algorithms designed to learn from both labeled and unlabeled data, typically assumed to be sampled from the same or similar distributions Yan et al., Mawuli et al.. One of the critical challenges in semi-supervised learning is how to effectively utilize labeled and unlabeled data while avoiding overfit- ting, preventing the effects of noise on the model, and ensuring effective model performance enhancement. Initial breakthroughs in semi-supervised learning with deep neural networks leveraged generative models such as autoencoders Rasmus et al., variational autoencoders Kingma et al., and generative adversarial networks Odena.\nSome approaches in semi-supervised learning Oliver et al., Berthelot et al., Lee et al., Kingma et al., Kuo et al. also involve balancing a supervised loss (l5) applied to the labeled data and an unsupervised consistency regularization loss lul computed on the unlabeled data. Consistency reg- ularization aims to improve model resilience and maintain label distribution, even in the presence of noisy images. It measures the discrepancy between predictions made on perturbed unlabeled data points. Approaches of this kind include the \u03c0-model Laine and Aila which augments the in- put data with image-based noise and self-regularizes through an additional consistency loss, mean teacher Tarvainen and Valpola, which employs an exponential moving average of parameters to regularize the model. Recently, fast-SWA Athiwaratkun et al. showed improved results by training with cyclic learning rates and measuring discrepancy with an ensemble of predictions from multiple checkpoints.\nTwo additional important approaches for semi-supervised learning, which have shown success both in the context of deep neural networks and other types of models are: self-training and conditional entropy minimization. Self- training also known as pseudo-labeling Lee et al. involves assigning classes to unlabeled data by making predictions from a model trained only on labeled data. Furthermore, c\u043e- training Xie et al. attempts to analyze data from different perspectives to solve the accumulative mistake issue during self-training. On the other hand, in case of conditional entropy minimization Grandvalet and Bengio, all unlabeled examples are encouraged to make confident predictions on some class. This doesn't necessarily mean the predictions are correct (because the true labels for the unlabeled data are unknown), but the model is incentivized to be decisive rather than uncertain. In this paper, we use the concept of cosine similarity between labeled and unlabeled feature representations for pseudo-labeling of the unsupervised data. We refer to Van Engelen and Hoos for a comprehensive review on the topic."}, {"title": "2.3. Multimodality", "content": "Our experience of the world is inherently multimodal we perceive objects through vision, hear sounds, feel textures, smell odors, and taste flavors. To develop an AI agent with similar capabilities, researchers have explored various methods Donato et al., Babadian et al..\nThe simplest method to compensate multiple domains is using the concept of joint representation where the uni- modal signals are combined in the same representation space by simple concatenation Baltru\u0161aitis et al.. Ngiam et al. used autoencoders for multimodal domains. They employed stacked denoising autoencoders to represent each modality individually and subsequently fusing them into a unified multimodal representation using an additional autoencoder layer.\nSimilarly, Silberer and Lapata suggested a multimodal autoencoder for semantic concept grounding. They incor- porated a reconstruction loss for training the representation and added a term in the loss function to predict object la- bels using the representation. Srivastava and Salakhutdinov introduced multimodal deep belief networks (DBNs) and multimodal deep boltzmann machines (DBMS) Srivastava and Salakhutdinov as multimodal representations. Kim et al. used a deep belief network for each modality and then com- bined them into joint representation for audiovisual emotion recognition. Huang and Kingsbury used a similar model for audio-visual speech recognition (AVSR), and Wu and Shao used it for audio and skeleton joint based gesture recognition. Ouyang et al. explored the use of multimodal DBMs for the task of human pose estimation from multi-view data. They demonstrated that integrating the data at a later stage after unimodal data, underwent nonlinear transformations, which was beneficial for the model. An alternative to a joint multimodal representation is a coordinated representation where instead of projecting the modalities together into a joint space, separate latent representations are learned for each modality and are coordinated through a constraint Frome et al..\nSarfraz et al. recently utilized a multimodal CL dataset to learn complementary information from two different modal- ities. They employed the VGGSound dataset, which includes 10-second video clips along with their corresponding au- dio. On the other hand, Xu et al. developed an egocen- tric multimodal dataset using smart glasses equipped with sensors, combining data from cameras, accelerometers, and gyroscopes. This dataset covers 32 types of daily activities performed by 10 participants. They utilized the temporal binding network (TBN) Kazakos et al. to fuse the infor- mation from different sensors into single feature vector for further training and feature extraction.\nOne of the major drawbacks of these methods is their inability to incrementally learn new domains. For example, if we train our model on two domains (sensor signals and images) and use a joint representation to learn these two modalities, introducing a new data modality later would necessitate re-training the model from scratch. Conversely, with our proposed exFeCAM model, we can add the new modality as a different domain and incrementally train the model without starting the training process anew. Addi- tionally, our algorithm eliminates the necessity of storing data in memory for rehearsal, effectively addressing privacy concerns and storage challenges commonly encountered in exemplar-based CL methods.\nWith recent advancements in large language models (LLMs) and the introduction of GPT-4 Achiam et al. and its variants, it is now possible to utilize the data coming from vision, audio, and text signals into a single model. However, these models face significant challenges, including high computational and memory demands and a lack of continual learning capabilities."}, {"title": "3. Experimental Setup", "content": "In this section, we describe the steps involved in the creation of the multimodal continual learning dataset as well as the standard evaluation criteria for the class incremental as well as domain incremental learning scenarios."}, {"title": "3.1. Continual Learning Scenario", "content": "According to the division of incremental batches and the availability of task identities, continual learning can be broadly classified into three primary types or scenarios Van de Ven et al.:\n\nTask-Incremental Learning (TIL): It requires an algo- rithm to learn a set of distinct tasks progressively. Task identities/numbers are provided in both training and testing phase.\n\nClass-Incremental Learning (CIL): The algorithm must learn to differentiate between a growing number of classes incrementally. Task identities are only provided during training phase.\n\nDomain-Incremental Learning (DIL): The algorithm needs to learn the same tasks but in different domains. Task identities are not required.\n\nIn addition to these major scenarios, there are also other scenarios present in the literature like task-free continual learning (TFCL), online continual learning (OCL), blurred boundary continual learning (BBCL), continual pre-training (CPT), we refer to Wang et al. for a thorough study."}, {"title": "3.2. Multimodal Dataset", "content": "The multimodal dataset comprises two components: sen- sor signals from an actuated finger and non-stationary im- ages of objects captured by a vision camera. The two differ- ent modalities are fused into a hybrid setup that incorporates both class incremental as well as domain incremental scenar- ios as illustrated in Figure 1. In this setup, progression from left to right represents class-incremental training, while pro- gression from top to bottom represents domain-incremental training. Both domains encompass five tasks, with each task involving the learning of two new classes. The first domain focuses on sensor signal data for various objects encountered in each experience and follows a class-incremental learning approach. The second domain deals with visual data of the same objects and employs both domain-incremental and class-incremental learning methods. Further details on data collection for the different modalities are provided in the subsequent sections."}, {"title": "3.2.1. Sensor Signal", "content": "To collect the sensor signals for the different objects we utilize a soft pneumatic gripper Hao et al. equipped with four sensors which are affixed together using stretchable nylon thread as illustrated in Figure 10. We have employed the commercially available flex sensors Saggio et al., Mishra et al. to measure the bending of the finger and a force sensitive resistor (FSR) Hollinger and Wanderley to quantify the force applied to the fingertip as shown in Figure 2. The outputs from the flex and force sensors are directed to an Arduino Due board, which in turn is connected to a computer for further processing and analysis. We gather the sensor data for 10 distinct objects, for each object the gripper acquires 50 data points, considering various orientations of the object. During the data collection process, for each data point the gripper holds and releases the object repeatedly at different contact points, with each cycle lasting ~ 39 seconds. Each data point is represented as a 600-dimensional vector, encapsulating the sensor signals recorded during that time interval. The dataset acquisition for every object takes about 37 minutes to complete on a computer equipped with an Intel Core i7 CPU @2.80GHz (GTX 1070 GPU) running Windows 10."}, {"title": "3.2.2. Image Data", "content": "The image data consists of the same classes but repeated with three different objects. Each object is repeated 3 times to create a more diversified dataset. Classification can be performed at object level (30 classes) or at category level (10 classes). For each trial, the images are extracted from a 15-second video sequence at a rate of 10 frames per second (FPS). Objects are hand-held by the operator and the camera point-of-view is that of the operator's eyes. The operator extends his arm and smoothly rotates the object in front of the camera. The grabbing hand (left or right) changes throughout the sessions and relevant object occlusions are often produced by the hand itself. This creates a complex non-stationary dataset with variable lighting conditions and arbitrary amounts of occlusion produced by the operator's hand. The final dataset consists of 13,500 RGB images each of size 128x128 extracted from the video frame. The images are further resized to 32x32 dimension to speed up the training and evaluation process."}, {"title": "3.3. Evaluation Metrics", "content": "The performance of the CL algorithm depends on its ability to adapt, retain, and generalize the knowledge over time. We evaluate the incremental accuracy (denoted as aT) of the model after every task as well as the average accuracy over all the tasks for each domain denoted as A\u03c4. The average accuracy over all the domains is referred to as A\u03a6 in the paper. We define the expressions as follows:\nAT == \u2211t=1T at (1)\n\u03a3D=1AD = \u03a3D=1 (1)"}, {"title": "4. Methodology", "content": "In this section, we discuss the modifications made to the FeCAM algorithm Goswami et al.. Originally, the al- gorithm models the feature distribution of classes using a multivariate normal distribution, N(\u03bc, \u03a3), and uses the Mahalanobis distance to perform classification. While the authors proposed several techniques to enhance and stabilize Mahalanobis-based distance classification, one significant limitation of FeCAM is its requirement to store batches in temporary memory and conduct training only once all data for a particular task is available. This makes it unsuitable for online learning. To address this issue and improve the algorithm's capability for multimodal training, as well as to adapt to the semi-supervised nature of the dataset, we introduce several new features to the exFeCAM algorithm, which are detailed below."}, {"title": "4.1. Online Batchwise Training", "content": "Online training involves training a model incrementally in a batch-wise manner rather than on the entire dataset for a particular task at once. In this approach, each batch of data is passed through a feature extractor to obtain feature maps, which are then used to calculate the prototypes (mean and co-variance matrix) for each class in that batch. A dictionary is initialized with class labels as keys and their corresponding prototypes as values. There are two possible scenarios when updating this dictionary:\n1.  If a class is encountered for the first time, a new entry is added with the class label as the key and the prototypes as the value.\n2.  If the class has been seen before, the existing pro- totypes for that class are retrieved and updated by averaging the previous prototypes with the new ones from the current batch.\nThis method avoids the need to store incoming data for each task, which is critical for continual learning (CL) objectives and is especially suitable for autonomous agents with limited cache memory. Learning prototypes in a batch-wise manner addresses the challenge of not having access to previous data points, making it a more practical solution for real-time applications."}, {"title": "4.2. Intra Layer Feature Representation", "content": "Intra-layer feature representation (ILFR) leverages the power of intermediate layers that encodes the low and mid- level feature information present in the data extracted using a pre-trained model. Instead of relying solely on the final output layer of the feature extractor, the presence of multi- ple activation maps generalizes the learned knowledge and further improves the performance of the algorithm. We argue that enriching the last layer representations with hierarchical intra-layer features increases robustness to domain shifts and thus improves generalizability to downstream continual tasks. These extracted features can be the class embeddings of a transformer encoder Vaswani et al. or flattened feature maps of a ResNet encoder He et al..\nThere are two intuitive ways to promote intra-layer fea- ture representations into the model: averaging the represen- tations from the last \"k\" layers of the network or concate- nating the representations from the last k layers. The first approach requires the output dimensions of all layers to be identical to facilitate summation. In our use case, we opt for concatenation, as the outputs from different layers have varying dimensions. Concatenation also preserves the rich, multi-scale features, unlike summation, which can result in information loss due to the merging of complementary features captured by different layers, potentially reducing the richness of the learned representations.\nFor the multimodal experiment, we pre-train the domain 1 feature extractor on gripper data of objects not present in the training dataset, while the domain 2 feature extractor is trained on the Core50 dataset. More details about the pre-trained model architecture are provided in Table 1. For the Core50 experiment, we use the ResNet18 architecture as the feature extractor, pre-trained on the ImageNet1k dataset Russakovsky et al.."}, {"title": "4.3. Semi-Supervised Learning", "content": "Semi-supervised learning (SSL) denotes the capability of an algorithm to learn from supervised data and then improve the performance of the model by training it on the unsupervised part. It is very useful in a situation where we have limited supervised data but we can collect huge amounts of unsupervised data from the environment. This is an active area of research given that large, labeled datasets are expensive, but most applications have access to plentiful, cheap unlabeled data. In this paper, we use the concept of pseudo-labeling to organize the unsupervised data and provide them with dummy labels for further training. To perform pseudo labeling we randomly store the feature maps of each class for every task in a temporary memory buffer and then perform cosine similarity between the reference feature maps and the feature maps generated from the com- plete unsupervised dataset as defined in equation 2,\nsimilarity = \u2016x1\u22c5x2\u20162\u2016x1\u20162\u2016x2\u20162 (2)\nwhere x1 and x2 refers to the reference feature map present in the temporary memory and the feature maps from the unsupervised dataset, ||.||2 refers to the 12 norm of the two tensors. If the value of the cosine similarity is greater than a certain threshold, we then train the exFeCAM algorithm on the respective feature maps and its associated pseudo labels. These pseudo-labels are derived from the class labels used for feature matching. The prototype values for that class are subsequently updated using the averaging rule described in the previous section. To determine the threshold value, we employ the Optuna framework Akiba et al., which uses bayesian optimization to explore the hyperparameter space efficiently."}, {"title": "4.4. Multimodal Training", "content": "We extend the class-incremental continual learning to a more realistic semi-supervised continual learning (SSCL) setting, where data distributions reflect existing object class correlations between, and among, the labeled and unlabeled data distributions. For our experiment we use 70% labeled data and 30% unlabeled data for the two protocols (random and unseen) discussed above. At task n, we denote batches of labeled data as Xl = {(xlb, ylb ) : bl \u2208 (1, ..., b)|ylb \u2208 \u03c4n} and batches of unlabeled data as Xu = {xub : bu \u2208 (1, ..., b)}, where b refers to the batch size. The goal in task n is to learn a model which predicts object class labels for any query input over all classes seen in the current and previous tasks (\u03c4out = \u03c41 \u222a...\u222a\u03c4n). During each training step the model receives different classes incrementally over all the experiences for each domain as shown in Figure 1. The two domains contain data from different modalities specifically domain 1 contains the sensor data whereas domain 2 contains the vision data. The data for each task goes through a feature extractor to derive the feature maps from the different layers of the network. The feature maps for both domains are standardized to a fixed dimension of 384 and normalized to fall within the 0-1 range before being fed into the subsequent stages of the algorithm. These feature maps are then used to incrementally train the exFecam algorithm. During training for each task in the second domain, a small subset of the feature maps of the object is stored in a temporary memory buffer to facilitate pseudo-labeling for semi-supervised learning, as previously discussed.\nThe feature maps are used to compute the mean and covariance matrix for each batch during training. The model performs covariance matrix approximation, shrinkage, and normalization to stabilize the training process Goswami et al.. During the test phase, the extracted feature maps from the test dataset are used to calculate the Mahalanobis distance for all seen classes, and the class with the minimum distance value is selected as the output class. More details about the implementation of the algorithm is provided in the pseudo code. Additionally, we also perform a real-time test for object classification based on the multimodal data using the ROS framework as discussed in section 6 of the paper.\nWe further analyzed the intra-layer feature representa- tion for both the custom dataset and the Core50 dataset by comparing the accuracy improvement of the exFeCAM algorithm when incorporating knowledge from the last and penultimate layers of pre-trained feature extractors. The Core50 dataset showed a significantly greater improvement in accuracy compared to the custom dataset. The difference is attributed to the more diverse information contained in the image data of the Core50 dataset."}, {"title": "5. Results", "content": "To assess the performance of our algorithm and the effectiveness of the proposed modifications, we evaluate the exFeCAM algorithm on a custom multimodal dataset and the standard Core50 dataset used in the literature. The Core50 dataset was chosen for its non-iid nature and its com- patibility for self-supervised learning. We do not explicitly compare our algorithm with other state-of-the-art (SOTA) benchmarks because the focus of this paper is not to establish a new level of accuracy for object classification, but rather to demonstrate the effectiveness of CL in a real world multi- modal training setting. Nevertheless, our results are based on improvements to the original FeCAM algorithm, indirectly providing a comparison. All experiments are repeated three times, and we plot the mean and standard deviation of the respective metrics.\nFigure 4 demonstrates the improvements in our algo- rithm's prediction accuracy with the inclusion of intra-layer feature representation. For the custom dataset (plotted in green), the first five experiences illustrate the percentage improvement in domain 2 data, while the next five expe- riences indicate the accuracy improvement in domain 1. Each experience comprises two different classes for both the domains. The improvement magnitude for domain 2 is slightly greater than domain 1. The overall improvement for the custom dataset experiment is approximately 3.21 \u00b1 1.43%, with domain 2 contributing about 1.71 % more than domain 1. This can be attributed to the presence of image data in domain 2, which offers more diverse information across different network layers compared to sensor signal data. This trend is further validated by the core50 exper- iment (shown in blue), where all 10 experiences involve images, each containing five random classes. In this case, the overall improvement is around 12.39 \u00b1 0.0% compared to the original FeCAM algorithm without intra-layer feature representation.\nThe CL capability of the exFeCAM algorithm is de- picted in Figure 5 (left). The spider plot displays both class incremental and domain incremental accuracy for the two modalities. The class incremental accuracy for the sensor data is represented in orange, while the violet plot corre- sponds to the combined domain incremental and class incre- mental scenario for the image data. The results demonstrate that the model can learn new modalities while preserving information from previous ones. The per task accuracy for each domain is clearly visible in the spider plot, and the mean accuracy after completion of all the tasks for domain 1 is 60.63 \u00b1 6.15% whereas for domain 2 is 65.36 \u00b1 0.0 %. One of the reasons for higher variability in domain 1 can be due to the presence of relatively small sample sizes for training and testing. However, this experiment does not take into consideration the SSL capability of the algorithm and is trained on the complete data in a supervised manner."}, {"title": "6. Real-Time Evaluation With ROS", "content": "Robot Operating System (ROS) is a comprehensive framework that comprises a suite of tools, libraries, and protocols aimed at facilitating the development of various robotic systems. The system manages the creation and con- trol of communication between a robot's peripheral modules, such as sensors, cameras, and actuators, thereby enabling the seamless integration of these components with parallization capabilties."}, {"title": "6.1. ROS Setup", "content": "ROS is primarily designed to function optimally on Ubuntu, while only partially compatible with other operating systems, such as Windows and Mac. For our case we use ROS on Ubuntu 20.04 on a Intel Core i7 7th gen CPU @ 2.80 GHz equipped with Nvidia GTX 1080 GPU. We use the Noetic Ninjemys distribution of ROS 1 Koubaa et al. in our experiment. A detailed explanation about the workflow of the project is provided in the flowchart in Figure 7. The first publisher (P1) publishes the sensor signals coming from the four sensors attached to the soft finger, the second publisher (P2) publishes the video coming from the camera and the third publisher (P3) publishes the command pressure to the control box which inturn actuates the finger. The control frequency of P1 and P2 are 10hz and 20hz, whereas the actuation frequency of P3 is 0.2hz. We further observed that actuating the finger at a more higher frequency constrained it from properly holding the object and thus effected the quality of the generated data from the sensors. The data from P1 and P2 goes to the subscriber which then pre-process the data, extracts the frames from the video in case of P2 and sends it to the respective feature extractors. The output of the feature extractor is a 384-dimensional feature map (in case the intra- layer feature representation is turned on) which goes as an input to the exFecam model for further processing."}, {"title": "6.2. Real-Time Experiment", "content": "To assess the performance of our algorithm, we conduct a quasi real-time experiment using a soft pneumatic gripper equipped with two flex sensors and two force sensors. During the evaluation phase, data from the sensors and the camera are collected in real-time for approximately 15 seconds. These data are treated as two distinct domains and are sent through the pipeline in parallel for further processing and subsequently to the exFecam model for predictions. To gen- erate the predictions we utilize the stored mean and the co- variance matrix from a prior offline incremental experiment conducted for the two domains. Impressively, the mean and covariance matrix occupy only 22.5 MB of memory, making it quite efficient compared to other continual learning strate- gies\nHowever, since the model was initially trained on images of the objects in a different frame of reference, where the operator extends his arm and smoothly rotates the object in front of the camera. In contrast, during the real-time experiment, the objects are positioned near the soft finger, and a camera captures images from a top-down view. This difference sometimes leads to incorrect predictions by the model. To address this issue"}]}