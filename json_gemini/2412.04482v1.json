{"title": "NLP CLUSTER ANALYSIS OF COMMON CORE STATE STANDARDS AND NAEP ITEM SPECIFICATIONS", "authors": ["Gregory Camilli", "Larry F. Suter"], "abstract": "Camilli (2024) proposed a methodology using natural language processing (NLP) to map the relationship of a set of content standards to item specifications. This study provided evidence that NLP can be used to improve the mapping process. As part of this investigation, the nominal classifications of standards and items specifications were used to examine construct equivalence. In the current paper, we determine the strength of empirical support for the semantic distinctiveness of these classifications, which are known as \"domains\" for Common Core standards, and \"strands\" for National Assessment of Educational Progress (NAEP) item specifications. This is accomplished by separate k-means clustering for standards and specifications of their corresponding embedding vectors. We then briefly illustrate an application of these findings.", "sections": [{"title": "1 Introduction", "content": "In an earlier paper, a methodology using natural language processing (NLP) was proposed for mapping the relationship of a set of content standards to a set of item specifications.[1] In that investigation, the construct equivalence of educational standards and item specifications was examined. In the current study, the focus is on the nominal classifications per se, which are known as \"domains\" for Common Core standards, and \"strands\" for (National Assessment of Educational Progress (NAEP) item specifications.[2, 3] Because the methodology is the same for examining standards and specifications, we refer to both simply as \"elements\" when no distinction is required. The current study aims to determine the degree to which the nominal classifications of elements coincide with their corresponding empirical structure. In particular, we examine whether the nominal classifications of elements can be reproduced by an empirical classifications based on semantic textual similarity."}, {"title": "2 Background", "content": "Educational content standards constitute the centerpiece of the American system of public education under federal and state policy. The rationale is that both tests and instruction should be aligned to those standards for maximum effectiveness.[4, 5] However, while standards provide a conceptual overview of valued content, they are too broad for test development. Thus, item specifications are developed to translate the standards into concrete recipes for producing test items and assembling items into assessments. Both standards and specifications are subdivided into subcategories to ensure adequate conceptual coverage of multidimensional constructs like mathematics proficiency. As we shall see, such multilevel structures may significantly impact test development and score interpretation.\nWhile much effort is expended to align item specifications to content standards, the \"official\" subdivisions or categories are rarely scrutinized empirically.[6, 7] This may be because the data in question are text elements produced by subject matter and instructional experts. Traditionally, academic discussions or arguments about content have occurred primarily among diverse content experts, who sometimes work from different ideological perspectives. [8] Text analysis based on NLP may contribute new possibilities for improving standards and item specifications.\nTest item dimensionality can be considered either from a constructivist or empirical perspective. According to the former approach, content groupings are created logically in the minds of mathematicians and mathematics educators. For these categories to be consistent, the standards (or specifications) within a category should be more similar to each other than those within a different category. This is similar to the idea of convergent and discriminant validity. [9] The present study examines the consistency of categories by first establishing an empirical grouping, and then comparing this to the nominal classification. This analysis focuses only on whether the nominal categories are consistent, and whether empirical elements that fall outside their nominal category can be justified or explained\u2013or at least provide some insight. Whether the categories (nominal or empirical) are inherently correct or beneficial is a topic beyond the realm of this investigation."}, {"title": "2.1 NLP Studies", "content": "Several studies have used NLP methods to examine alignment issues, which is more generally referred to as content mapping. [10, 11, 12]. Camilli summarized this research in a previous paper that examined how NLP methods can be used to evaluate content mapping studies. [1] The data sets used in the current study were described, and a number of key issues in content mapping were identified. In particular, important modifications to the Common Core standards and NAEP item specifications were explained. [2, 3]"}, {"title": "3 Notes on NLP Embedding", "content": "Cohesive chunks of text in the NLP literature are alternatively referred to as sentences, extended sentences, text, or statements. The resulting sentence data are then vectorized to obtain numerical representations called embedding vectors (EV). The word embedding is used to suggest \"meaning in context.\" Each sentence (or segment of text) can be represented with an EV of dimension length $n$ of semantic attributes. Subsequently, the EVs of each word in a sentence are combined into a single vector to represent the sentence (or larger segment of text) as a whole. Further statistical analysis was conducted on these to understand the substantive properties of text. More details on the preparation of text for analysis were given in the previous paper. [1]"}, {"title": "4 Methods", "content": "The method used to study the structure of EVs in this study is k-means cluster analysis, making only the assumption that 5 classes exist. Below, we refer to the clusters as the \"empirical\" in contrast to the nominal classes. Once obtained, elements of the nominal and empirical groupings can be cross-classified. Suppose the nominal classes can be retrieved empirically. In that case, the mapping of elements to the nominal structure is supported. If mismatches in the cross-classification can be explained satisfactorily, this may help to sharpen conceptual boundaries for further development."}, {"title": "4.1 Data Preparation", "content": "The Common Core standards are nested within 5 content domains containing the 34 standards: (Operations and Algebraic Thinking; Number and Operations in Base Ten; Number and Operations-Fractions; Measurement and Data; Geometry). The 49 NAEP item specifications are also nested within 5 content domains (Number sense, properties, and operations; Algebra and Functions; Measurement; Data Analysis, Statistics, and Probability; Geometry). This layout is given in Table 1 for easy reference.\nFor the Common Core, the transformed data consisted of a 34 \u00d7 3000 matrix in which the rows corresponded the to the EV dimension for the standards. For cluster analysis, the first task is to reduce redundancy by extracting the first 5 principal components (PC). The main idea of principal component analysis is to extract a small number of orthogonal variables that account for the lion's share of the variance of a set of original variables. Eventually, 4 of these PCs were used, resulting in a 34 \u00d7 4 data set for analysis (one PC showed essentially no variation between domains). A"}, {"title": "4.2 K-Means Cluster Analysis", "content": "K-means is a method of cluster analysis that proceeds in two steps. First, a number of proposed clusters is selected or randomly generated. Second, a set of seed centroids is provided for a set of observations (in this case, the PCs for standards or for specification elements). Third, each observation is assigned to the nearest cluster (i.e, centroid) based on Euclidean distance and cluster centroids are recalculated. This process is iterated to convergence during which elements may change cluster membership. Convergence is attained when centroids change negligibly, at which point within-cluster variances are minimized, or alternatively, between-cluster variances are maximized (because the total variance is fixed).\nThe drawbacks to k-means clustering include the requirement of selecting a working number of clusters, a bias toward spherical within-cluster distributions, and the assumption that cluster do not overlap. However, k-means is conceptually simple and frequently used as a baseline to evaluate other methods of clustering for particular applications. We make no claim this method is optimal. However, the clustering results are consistent with the nominal structures, and the few mismatches that occur (between nominal and empirical groupings) have compelling substantive explanations."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Descriptive Statistics", "content": "The PC cluster means for both the CCSS and NAEP are given in Table 2. Because the PCs were not rotated, their intercorrelations are $r = 0$, and standard deviations are $\\sigma = 1$. While the original EVs are uninterpretable, the PCs may have some substantive interpretations based on their patterns of means. For example, PC4 for the CCSS seems to be discriminating between Measurement and Data, on the one hand, and Geometry, on the other. Note the cluster order is the same as the nominal order in Table 1."}, {"title": "5.2 Cross-Classification: Common Core Standards", "content": "In Table 2, the classification results are given for the similarity of CCSS Domains and empirical clusters. For each domain a clear matching cluster exists. There are 6 mismatches, resulting in a classification accuracy of 82.5%. Three of the mismatches consist of misclassifying the nominal standard as as belonging to the empirical operations and algebra domain."}, {"title": "5.3 Cross-Classification: NAEP Item Specifications", "content": "For each NAEP strand, a clear matching empirical cluster also exists. In Table 5, there are 4 mismatches, resulting in a classification accuracy of 91.8%. Two measurement specifications were misclassified as belonging to the NAEP geometry strand."}, {"title": "6 Classification Errors", "content": "A brief review is given below for the cross-classification mismatches. The goal is to determine whether the commonalities among the misclassifications make sense. Because NLP methods may provide misleading results, they require both logical and substantive examination. This review process can result in defensible explanations leading to potential extrapolations of results."}, {"title": "6.1 CCSS", "content": "Select mismatches from the CCSS cross-classification are shown in the top section of Table 5. In particular, we examine the 3 Measurement and Data standards that are misclassified as operations and algebraic thinking."}, {"title": "6.2 \u039d\u0391\u0395\u03a1", "content": "The mismatches from the NAEP cross-classification are shown in the bottom section of Table 5. Here, we examine the 2 \"Measuring Physical Attributes\" standards that are misclassified as belonging to geometry.\n4.Measuring Physical Attributes(f) Solve problems involving perimeter of plane figures.\n4.Measuring Physical Attributes(g) Solve problems involving area of squares and rectangles.\nThese measurement standards clearly center on geometric concepts. This is measurement only in the sense of computing perimeters or areas of geometric figures. We would argue that this is not in the sense of the CCSS. The problem here is that the word \"measurement\" can have different meanings. The CCSS and NAEP appear to take very different perspectives."}, {"title": "7 Digression on Measurement", "content": "The NLP cluster analysis has shown that semantically-speaking, there appears to be overlap between what is considered measurement, on the one hand, and either geometry or algebra, on the other. In fact, the word measurement commonly refers to a wide variety of topics in the field of educational testing, ranging from counting to ordering to scale conversions to determining geometric quantities. This approach to measurement is described next, followed by a brief consideration a more conceptual approach."}, {"title": "7.1 Topic-based Measurement", "content": "Especially in educational testing, measurement is operationalized as a list of topics including:\n\u2022 physical attributes, like temperature, length, mass\n\u2022 geometric features, like length, height, width, area, circumference, volume\n\u2022 spatial relations, angles, graphs\n\u2022 comparison, ordering, transitivity ($a < b$ and $b < c$ implies $a < c$)\n\u2022 units of measurement, conversion of units within and across scales\n\u2022 data, estimation, precision, measurement instruments\n\u2022 counting, systems of whole numbers, benchmarks\nThis is only a small sample of potential topics. A more detailed overview of measurement is provided in the NAEP 2026 mathematics framework:\nThe connection between measuring and number makes measurement a vital part of school mathematics. Measurement is an important setting for negative and irrational numbers as well as positive numbers, since negative numbers arise naturally from situations with two directions and irrational numbers are commonplace in geometry. Measurement representations and tools are often used when students are learning about number properties and operations. For example, area grids and"}, {"title": "7.2 Conceptual Measurement", "content": "The 2026 framework of the National Assessment of Educational Progress (NAEP) provides a description of conceptual measurement that attempts to weave disparate topics together:\nMeasuring is the process by which numbers are assigned to describe the world quantitatively. This process involves selecting the attribute of the object or event to be measured, comparing this attribute to a unit, and reporting the number of units\"\nAccording to this definition, the construct of measurement involves mastery of a set of related principles, including the selection of an attribute, defining the unit of measurement, and assigning a certain number of units to an object based on the corresponding values of the attribute. There are other important features of measurement (e.g., instruments, standardization, and evaluation of uncertainty), but the focus of measurement is on an attribute or on a construct. Attribute is used to designate an aspect of a physical thing (e.g., temperature, length, mass), whereas an aspect of human beings (e.g., mathematics proficiency or extroversion)is typically designated as a construct. Attributes or constructs are features of objects or people, respectively, that are of interest. A physical object can have more than one feature and the same is obviously true of human beings: a person may be alternatively pleasant or annoying on occasion."}, {"title": "7.2.1 Illustration", "content": "An example outside the field of education may provide a fresh perspective on measurement as a conceptual process. Consider two scales used to describe the hardness of a physical material:\n\u2022 The Vickers procedure uses a diamond to indent the surface of a metal, and the force exerted is divided by the surface area of the indentation and transformed to scale values. The attribute being measured is resistance to indentation. With this scale you can assign quantitative values to metals, and define a unit of measurement that in turn can be used to compare materials (e.g., one metal may be twice as hard as another). This procedure is carried out with a precision measuring instrument.\n\u2022 The Mohs procedure uses a scratch test. If mineral A scratches mineral B, and mineral B does not scratch A, then then A is harder than B. The test is then applied to a select set of 10 benchmark minerals, which are subsequently ranked from 1 (softest, talc) to 10 (hardest, diamond). The attribute being measured is scratch resistance visible to the naked eye. The hardness of minerals can subsequently be determined in relation to these benchmarks.\nBoth scales involve the idea of arranging materials with respect to hardness, but differ in two important ways. First, resistance to indentation for metals is not the same thing as scratch resistance for minerals. In fact, the term \u201chardness\u201d is not a single entity. Vickers suffices for some applications (e.g., testing hardened steel in the lab), Mohs suffices for others (e.g., evaluating minerals in the field), and still other hardness scales exist for other purposes. In contrast to the Vickers, Mohs produces only an ordering based on relative hardness rather than a quantitative value, consequently there is no \"unit\" of measurement. For this reason, differences between or ratios of Mohs values are not meaningful. The Mohs scale could be based on letters, A, B, C ..., and the utility and meaning of the scale would not change.\nThis example suggests fundamental processes are involved in making sense out of variability in hardness. First, an attribute must be chosen, but this choice depends on the application because attributes don't exist without purpose (this doesn't imply the object of measurement is arbitrary). In turn, the standardization of attributes, instruments, and procedures is required for determining reliability. Second, some scales have scalable \u201cunits\u201d (Vickers) and some don't (Mohs). There are scholars who believe that if there is no unit, there is no measurement. Taken literally, however, this would leave us without a good word for describing the Mohs procedure. Indeed, most uses of the word would need to be stricken from the English language. In any case, the qualitative contrast between the Vickers and Mohs scales is useful because it helps us to think about how to specify (or theoretically verify) attributes or constructs, establish useful proxies for ideal measurement, and improve data quality."}, {"title": "7.2.2 Thoughts on measuring measurement", "content": "Measuring proficiency in the idea of measurement would be challenging because here is no popular consensus on what measurement is, despite a cornucopia of available definitions. Moreover, it's not clear that there is a need for assessing this construct-with two caveats. First, the case can still be made that teaching measurement theory would be beneficial. Certainly measurement can be taught from a technical perspective at the post-secondary level (e.g., econometrics or psychometrics), but the core principles are basic to scientific method, including how attributes and constructs are identified, distinguished, and investigated. The example above hints that conceptual measurement would be most effectively taught in case studies or projects involving substantive applications. Second,"}, {"title": "8 Discussion", "content": "In terms of semantic textual similarity, the Common Core standards and NAEP item specifications are internally consistent (83% and 92%, respectively) but a handful of mismatches unidentified raise questions about how the idea of measurement is implemented. As pointed out above, this issue involves the myriad connotations of the word measurement in the English language. Even experts in the field of psychometrics cannot agree upon a definition of measurement. From a more pragmatic perspective, an effort to distinguish or classify topics (e.g., measurement, geometry, and numeric operations) may reduce redundancies in test construction or overlap in test subscores. Whether and where in the curriculum students should be taught a conceptual understanding of measurement and its role in the sciences is an ongoing question.[13] [14, 15, 16]\nFinally, it is important to recognize that traditional document analysis would have plausibly revealed the same findings as reported here. Still, an NLP application produces faster results with fewer resources. These results,in turn, be used to make subject matter deliberation more efficient and cost effective. In any case, the issues raised in this paper are emblematic of how NLP can be used to gain new insights into established measurement practices."}]}