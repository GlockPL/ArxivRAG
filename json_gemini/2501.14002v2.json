{"title": "ADVANCING MATH REASONING IN LANGUAGE MODELS: THE IMPACT OF PROBLEM-SOLVING DATA, DATA SYNTHESIS METHODS, AND TRAINING STAGES", "authors": ["Zui Chen", "Tianqiao Liu", "Mi Tian", "Qing Tong", "Weiqi Luo", "Zitao Liu"], "abstract": "Mathematical reasoning remains a challenging area for large language models (LLMs), prompting the development of math-specific LLMs such as LLEMMA, DeepSeekMath, and Qwen2-Math, among others. These models typically follow a two-stage training paradigm: pre-training with math-related corpora and post-training with problem datasets for supervised fine-tuning (SFT). Despite these efforts, the improvements in mathematical reasoning achieved through continued pre-training (CPT) are often less significant compared to those obtained via SFT. This study addresses this discrepancy by exploring alternative strategies during the pre-training phase, focusing on the use of problem-solving data over general mathematical corpora. We investigate three primary research questions: (1) Can problem-solving data enhance the model's mathematical reasoning capabilities more effectively than general mathematical corpora during CPT? (2) Are synthetic data from the same source equally effective, and which synthesis methods are most efficient? (3) How do the capabilities developed from the same problem-solving data differ between the CPT and SFT stages, and what factors contribute to these differences? Our findings indicate that problem-solving data significantly enhances the model's mathematical capabilities compared to general mathematical corpora. We also identify effective data synthesis methods, demonstrating that the tutorship amplification synthesis method achieves the best performance. Furthermore, while SFT facilitates instruction-following abilities, it underperforms compared to CPT with the same data, which can be partially attributed to its poor learning capacity for more challenging problem-solving data. These insights provide valuable guidance for optimizing the mathematical reasoning capabilities of LLMs, culminating in our development of a powerful mathematical base model called MathGPT-8B\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "To address the challenge of insufficient mathematical reasoning capabilities in large language models (LLMs), various math-specific LLMs are developed. These include models that enhance performance from the pre-training stage, such as LLEMMA (Azerbayev et al., 2023), DeepSeekMath (Shao et al., 2024), InternLM-Math (Ying et al., 2024), and Qwen2-Math (Yang et al., 2024a), as well as models that improve through post-training, such as MetaMath (Yu et al., 2023), WizardMath (Luo et al., 2023), and KwaiYiiMath (Fu et al., 2023). These models generally follow a common training paradigm. During the pre-training stage, math-related corpora are filtered from extensive internet data to augment the model's mathematical knowledge. During the post-training stage, they typically utilize problem datasets and their augmented versions, such as Program-of-Thought (PoT)\n*These authors contributed equally. Work done during Zui Chen's internship at Guangdong Institute of\nSmart Education, Jinan University, Guangzhou, China.\nThe corresponding author.\nModel is available at https://huggingface.co/ai4ed/MathGPT-8B"}, {"title": null, "content": "Chen et al., 2022), evol-Instruct (Xu et al., 2023), and Tool-Integrated Reasoning (TIR) (Gou et al.,\n2023; Yin et al., 2024), to construct supervised datasets for Supervised Fine-Tuning (SFT). This\nenables the models to follow instructions and produce outputs in the desired format. Recently, there\nis a growing focus on constructing preference datasets for the solution process to perform Step-DPO\n(Lai et al., 2024) or online-RLHF (Dong et al., 2024). These approaches aim to obtain more accurate\nreasoning pathways, thereby significantly enhancing the mathematical reasoning capabilities of the\nmodels.\nDue to the intrinsic distinction between mathematical knowledge and general world knowledge,\ndifferent strategies are required for their effective acquisition and application. The primary challenge\nin acquiring world knowledge lies in memorizing and understanding vast amounts of information,\nnecessitating large corpora during the pre-training phase to enhance knowledge reserves (Roberts\net al., 2020; Petroni et al., 2019; Dubey et al., 2024). In contrast, mathematical knowledge involves\na relatively limited set of elements, concepts, axioms, and theorems that need to be memorized and\nunderstood. The real challenge often lies not in recalling the relevant knowledge but in using this\nknowledge for reasoning or planning (Hao et al., 2023).\nFrom previous studies, it might seem that the continued pre-training (CPT) stage contributes less\nto mathematical reasoning abilities. However, recent studies, such as Physics of LLM (Allen-Zhu\n& Li, 2023) and MiniCPM (Hu et al., 2024), highlight the importance of teaching models how\nto utilize memorized knowledge during the pre-training stage. These findings question the effec-\ntiveness of the prevalent paradigm for enhancing mathematical reasoning abilities, which primarily\nfocuses on memorizing more mathematical knowledge during the pre-training phase and developing\nreasoning abilities in the post-training phase. Therefore, we propose that alternative strategies utiliz-\ning mathematical problems and their reasoning steps-referred to as problem-solving data during\nthe pre-training phase, to teach the model how to apply its memorized knowledge rather than sim-\nply increasing the volume of relevant data, could potentially lead to significant improvements in\nmathematical reasoning capabilities. With these considerations, we aim to explore the following\nfundamental research questions (RQs):\nRQ1: During the CPT stage, can providing problem-solving data more effectively enhance the\nmodel's mathematical reasoning capabilities compared to using general mathematical corpora?\nRQ2: If problem-solving data can enhance mathematical reasoning capabilities, are synthetic data\nfrom the same source equally effective, and what synthesis methods are most efficient?\nRQ3: How do the capabilities developed from the same problem-solving data differ between the\nCPT and SFT stages, and what factors contribute to these differences?\nWe address these three research questions separately. In Section 3, we explore RQ1 by comparing\nthe impact of using problem-solving data and examining various math data mixture ratios, which\nleads to Result 1. In Section 4, we investigate RQ2 by delving into four data synthesis techniques:\nresponse diversification, query expansion, retrospective enhancement, and tutorship amplification,\nresulting in Result 2. In Section 5.1, we address RQ3 by first identifying, from a holistic perspec-\ntive, the differences in learning mathematical capabilities between the CPT and SFT stages using\nproblem-solving data. Subsequently, in Section 5.2 and Section 5.3, we further analyze RQ3 by\ndividing the problem-solving data into subsets based on data distribution and difficulty level to in-\nvestigate the sources of these differences, ultimately leading to Results 3-5.\nResult 1: Providing math problem-solving data significantly enhances the model's mathematical\ncapabilities compared to general mathematical corpora and a higher proportion of problem-solving\ndata is more effective.\nResult 2: Response diversification, query expansion, and tutorship amplification were effective.\nAmong these, tutorship amplification methods emerged as distinctly superior, leveraging a teacher\nmodel to identify and correct errors based on the student model's responses, aiming to equip the\nmodel with self-correction capabilities.\nResult 3: Overall, while SFT can facilitate some learning of mathematical capabilities, it has a clear\ndisadvantage compared to CPT.\nResult 4: From the perspective of data distribution, both SFT and CPT primarily develop capabilities\naligned with their data distributions. However, SFT's in-domain (IND) learning ability is weaker"}, {"title": null, "content": "than that of CPT. Regarding out-of-domain (OOD) capability learning, the conclusions are less\nclear, with only the observation that SFT is more susceptible to disturbances from data distribution\ncompared to CPT.\nResult 5: From the perspective of difficulty level, providing more challenging problem-solving data\nenables more effective learning, with this advantage being particularly evident in CPT compared to\nSFT. This may be the primary source of the learning capability differences between CPT and SFT.\nTherefore, we recommend preparing more challenging problem-solving data for the CPT phase.\nAfter addressing our three RQs, we identify the optimal strategy combination and apply it to the\nLlama3-8B model (Dubey et al., 2024), resulting in the highly efficient MathGPT-8B. MathGPT-8B\nsurpasses various math-specific models including DeepSeekMath-Base-7B (Shao et al., 2024) and\nQwen2-Math-7B (Yang et al., 2024a), and exhibits capabilities comparable to Qwen2-Math-72B and\nthe recently released Qwen2.5-Math-7B (Yang et al., 2024b). We introduce only 100B mathematical\ntokens, equivalent to 1/10 of Qwen2.5-Math-7B, and perform CPT based on a weaker base model.\nThis validates that our proposed method is a more efficient approach for enhancing mathemati-\ncal capabilities compared to existing paradigms. Additionally, MathGPT-8B retains strong general\nknowledge capabilities, as confirmed by MMLU (Hendrycks et al., 2020) benchmarks. Since no\npost-training is conducted, we are releasing the base version of MathGPT-8B, allowing the research\ncommunity to perform further post-training to enhance its capabilities."}, {"title": "2 EXPERIMENTAL PREPARATION", "content": "In this section, we provide a comprehensive overview of experimental preparations, including data,\nbaseline models, and metrics.\nTraining Data. The training data is categorized into three groups: (1) General corpus, which\nincludes scientific texts from the ArXiv subset of RedPajama (Weber et al., 2023), code datasets\nfrom AlgebraicStack (Azerbayev et al., 2023) and StarCoder (Li et al., 2023), and natural language\ndatasets from the C4 and Wikipedia subsets of RedPajama (Weber et al., 2023), to prevent catas-\ntrophic forgetting and maintain robustness. (2) Mathematical corpus, which utilizes corpus on\nmathematical content like OpenWebMath (Paster et al., 2023) to improve mathematical proficiency.\n(3) Problem-solving data, which includes NuminaMath (Li et al., 2024), Lila (Mishra et al., 2022),\nand proprietary data, with 14 million pieces used for synthetic data augmentation. Our experi-\nments employ 48.3B tokens from the general corpus, 13.7B from the mathematical corpus, 7.2B\nfrom problem-solving data, and 30.54B from synthetic data. Detailed descriptions are provided in\nAppendix A.1.\nBase Model. We select Llama2 (Touvron et al., 2023) as our base model to ensure robustness in our\nfindings, as it predates the release of OpenWebMath (Paster et al., 2023). By choosing a model that\nexisted before the introduction of recent mathematical corpora, we effectively mitigate the risk of\ncontamination from these newer datasets. More details are provided in Appendix A.2.\nEvaluation Set. To minimize contamination of the data set and expand the capacity assessment, we\nexpand our evaluation set to include GAOKAO and ZHONGKAO, along with GSM8K (Cobbe et al.,\n2021) and MATH (Hendrycks et al., 2021). GAOKAO and ZHONGKAO datasets, developed after\nthe release of Llama2, enable the measurement of a wider range of abilities. Detailed descriptions\nof the data sets are provided in Appendix A.3.\nDeduplication and Decontamination. We use the MinHash deduplication (Lee et al., 2022) frame-\nwork to enhance training data quality by removing documents with significant duplicate content.\nThis process includes setting specific byte thresholds for deduplication and decontamination, effec-\ntively eliminating contaminated documents, particularly from OpenWebMath (Paster et al., 2023).\nMore details are provided in Appendix A.4.\nEvaluation Metrics. Our evaluation follows a three-stage process: model inference using zero-\nshot and few-shot prompts, answer comparison to handle irregular outputs, and statistical scoring\nto determine accuracy. In the statistical scoring stage, we select the higher accuracy between the\nzero-shot and few-shot approaches for each dataset to ensure the reliability and robustness of the\nresults, given that some models perform better in zero-shot settings while others prefer few-shot"}, {"title": "3 IMPROVING REASONING ABILITY IN CPT WITH PROBLEM-SOLVING DATA", "content": "We believe that, compared to simply remembering and understanding more mathematical knowledge\nfrom vast corpora, the focus of mathematical knowledge acquisition during the pre-training phase\nprimarily lies in learning to apply this knowledge for reasoning or planning. The intuitive approach\nis to provide corresponding data for problem-solving. Therefore, in this section, we first aim to\nvalidate RQ1, specifically the effectiveness of providing problem-solving data during the CPT phase.\nThis serves not only as a validation of our main argument but also as the foundation for subsequent\nresearch questions. We then continue to explore the impact of the proportion of problem-solving\ndata to determine an appropriate data ratio and verify the efficiency of providing problem-solving\ndata.\nExperiments. We design four experimental groups, including one base group and three test groups.\nOur goal is to demonstrate the effectiveness of providing problem-solving data by comparing the\nbase group with the test groups, while exploring suitable data mixing ratios through comparisons\namong the three test groups. Specifically, the total amount of math data used in the base group and\ntest groups remains the same, with the base group utilizing the math corpus as its math data. In\ncontrast, the test groups employ a mix of the math corpus and problem-solving data as their math\ndata, with the mixing ratios varied among the three test groups. The specific data details are as\nfollows: where the data mixture ratio indicates the mixing proportion of general data to math data,\nand the math data mixture ratio reflects the blending proportion of the math corpus to problem-\nsolving data. More experimental design discussions can be found in the Appendix C.\n\u2022 Base1: Using 48.3B general corpus and 14.7B math corpus, mixed in a 4:6 ratio.\n\u2022 Test1: Using 48.3B general corpus, 7.5B math corpus, and 7.2B problem-solving data, with data\nmixture ratio 4:6, math data mixture ratio 5:5.\n\u2022 Test2: Same as Test1, but using a math data mixture ratio of 3:7.\n\u2022 Test3: Same as Test1, but using a math data mixture ratio of 7:3.\nTraining Details. We utilize Llama2 (Touvron et al., 2023) as the base model and perform CPT for\n25,000 steps, with a global batch size of 1024 and a context length of 4096 tokens. The learning rate\nis warmed up to 1e-4 and then decays to le-5 using a cosine schedule (Loshchilov & Hutter, 2022).\nThe training data is split into 95% for training and 5% for validation. After completing the 25,000\nsteps, we select the checkpoint with the lowest validation loss for evaluation as the result."}, {"title": null, "content": "Results. As shown in Figure 1, the blue line, representing the reference group following the current\ntraining paradigm, indicates that CPT using the math corpus effectively improves problem-solving\naccuracy. However, compared to the other three curves, even though Basel utilizes the same number\nof tokens, the trend and extent of improvement in mathematical capabilities are significantly lower\nthan those of the three test groups. For the three test groups, the green line in Figure 1 shows that"}, {"title": "4 EXPLORATION OF EFFICIENT DATA SYNTHESIS METHODS", "content": "In the preceding sections, Results 1 highlights the effectiveness of problem-solving data. However,\nthe limited availability of such data compared to internet data underscores the need for efficient\ndata synthesis methods. Additionally, it is not yet fully researched whether further synthesis from\nthe same problem-solving data during the pre-training stage can enhance model performance. To\naddress these issues and RQ2, we explore four data synthesis methods: response diversification,\nquery expansion, retrospective enhancement, and tutorship amplification. Our aim is to validate the\neffectiveness of synthesized data and identify the most efficient synthesis method. Below, we briefly\nintroduce the data synthesis methods used in our study.\nResponse Diversification aims to enhance model capabilities by generating diverse reasoning paths\nthrough methods like rejection sampling. Since it does not alter the answers, response diversification\ndoes not require additional labeling, making it easy to implement. The effectiveness of response data\nsynthesis is established through various implementations (Yuan et al., 2023; Yu et al., 2023; Chen\net al., 2024). Instead of using a sampling-then-deduplication approach, we require the model to\nfollow two steps to improve the efficiency of response diversification: (1) Generate two distinct\nsolutions based on the question and the original answer; (2) Select the solution with the correct final\nanswer to serve as one diversified training sample.\nQuery Expansion aims to enhance model capabilities by expanding the question set. However,\ngenerating high-quality questions directly is challenging. Existing methods (e.g., Yu et al., 2023\nand Mitra et al., 2024) leverage the concept of reshaping, which involves generating new questions\nbased on existing questions and answers through rephrasing, reversing statements, and other tech-\nniques. The synthesis of new questions focuses on ensuring: (1) the accuracy of the newly generated\nquestions, and (2) the accuracy of their corresponding answers. We integrate existing methods and\nemphasize these key points by requiring the LLM to perform augmentation in four steps based on\nthe input question and solution: (1) transform the question into a statement, (2) generate new ques-\ntions based on the statement, (3) provide answers for the new questions, and (4) evaluate the answers\nand explain the reasoning. Our approach improves quality through three main aspects: first, we pro-\nvide the original questions and answers; second, steps 1 and 2 ensure that the generated questions\nare valid and solvable; and third, steps 3 and 4 involve self-evaluation to assess the quality of the\nanswers to the new questions.\nRetrospective Enhancement Ye et al. (2024) posits that teaching the model to directly correct mis-\ntakes is beneficial. They employ a low-resource construction method that involves directly inserting\nsubsequent steps into preceding ones, allowing models to retry upon regret. A special [back] token\nis used for identification, which is why we refer to it as retrospective enhancement. This method is\nvalidated on GSM8K using a small parameter model with minimal pre-training. Our scenario differs\nin two key ways: (1) we utilize a more diverse question set, with some questions significantly differ-\nent from the simpler forms in GSM8K; (2) we perform CPT on a mainstream model that possesses a\ncertain level of mathematical capability. We aim to validate the effectiveness of this straightforward\nmethod.\nTutorship Amplification is inspired by the real-life practice of teachers guiding students to rectify\nmistakes. As evidenced by OpenAI (2024), models can be trained to spot errors. This agrees with Ye\net al. (2024), who suggest that while models can detect errors, they lack opportunities for correction.\nTutorship amplification simulates a realistic error correction process. In this process, a \"strong\"\nmodel, acting as a teacher, aids a \"weak\" model, representing a student. After the student model\ngenerates an answer to a problem, the teacher model performs the following actions: it checks\nwhether the student's answer is correct. If the answer is correct, it responds affirmatively. Otherwise,"}, {"title": "5 ABILITIES ACQUISITION COMPARISON OF CPT AND SFT STAGES", "content": "In the previous two sections, we have demonstrated that providing problem-solving data during\nthe CPT phase efficiently teaches the model to apply mathematical knowledge and enhances its\nreasoning ability. However, how does this differ from developing mathematical reasoning skills\nduring the SFT phase? In this section, we first verify that the change in the training stage indeed\nraises the upper limits of the model's capability, not merely due to the data. Then, we investigate\nthe sources of differences in mathematical learning between the CPT and SFT phases from two\nperspectives: data distributions and difficulty levels."}, {"title": "5.1 COMPARISON OF ABILITIES ACQUISITION", "content": "In this section, we explore how the stage at which problem-solving data is used (CPT vs. SFT)\nsignificantly affects the model's ultimate capabilities. We have a total of 7.2B problem-solving data,\nwhich can be allocated at either the CPT or SFT stage. Additionally, we sample 0.072B problem-\nsolving data for 1%-SFT to endow the model with instruction-following ability. We propose the\nfollowing experimental settings to compare the acquisition of learning capabilities between the CPT\nand SFT stages:\n\u2022 Base1: CPT with 48.3B general corpus and 14.7B math corpus.\n\u2022 Base2: CPT with 48.3B general corpus, 7.5B math corpus, and 7.2B problem-solving data.\n\u2022 Base1-SFT: SFT with 7.2B problem-solving data based on Base1.\n\u2022 Base1-1%SFT: SFT with 0.072B problem-solving data based on Base1.\n\u2022 Base2-1% SFT: SFT with 0.072B problem-solving data based on Base2.\nIt is important to note that we perform SFT on both Basel and Base2 using 1% of the problem-\nsolving data. This setup allows us to isolate the impact of instruction-following capability improve-\nments and thereby assess the true enhancement in mathematical reasoning ability brought about by\nintroducing problem-solving data at the CPT stage.\nExperiment Details. During the SFT stage, we set a batch size of 256 and a learning rate that\ndecayed from le-5 to 1e-6 following a cosine schedule. We train for 3 epochs, ensuring that the\ntraining loss converged. After convergence, we select the optimal result from 10 checkpoints for\nreporting, which typically occurred around the checkpoints at 2 epochs. More experimental design\ndiscussions can be found in the Appendix C."}, {"title": null, "content": "Results. The evaluation results across the four datasets can be found in Appendix E. Their average\naccuracy is illustrated in Figure 2. First, we observe the red and blue shaded areas, where a small\namount of SFT data brought similar improvements on both Basel and Base2. From the evaluation\nresults, this improvement stems from a significant reduction in the model's previously inconsistent\nand repetitive outputs. We believe this is a result of the supervised approach in SFT, leading to\nleading to an interesting conclusion: A small amount of SFT data is sufficient to enhance the\nmodel's ability to follow instructions.\nNext, we compare the results after removing the influence of instruction-following capabilities. At\nthis point, the differences, denoted as SFT A and CPT \u25b32, can be viewed as the improvements in\nmathematical reasoning ability obtain during the SFT and CPT phases, respectively. Given that both\nuse the same data, but the capability gain in SFT is only about 60% of that achieves during CPT.\nAdditionally, comparing Base1-SFT and Base2, despite using the same data, Base1-SFT also gains\nthe ability to follow instructions, yet its performance is still inferior to Base2. Thus we conclude\nResult 3: Overall, while SFT can facilitate some learning of mathematical capabilities, it has a\nclear disadvantage compared to CPT.\nTo better understand SFT's impact on learning capabilities, we add three additional experimental\ngroups, where we perform SFT with 10%, 20%, and 50% splits of the problem-solving data. We\ncompare these with Basel, 1% SFT, and 100% SFT to analyze the effect of SFT data volume on"}, {"title": "5.2 IMPACT OF DIFFERENT DATA DISTRIBUTIONS", "content": "In the previous section, we observe that the reasoning capability learned during the SFT phase is\nsignificantly weaker compared to CPT. In this section, we aim to explore the source of this differ-\nence. Our intuition is that data distributions might have different impacts on capability learning\nat each stage, with CPT possibly contributing to enhanced out-of-distribution (OOD) performance.\nHowever, our findings contradict this hypothesis. Both CPT and SFT primarily develop capabilities\naligned with the data distributions they are trained on.\nExperiment. We design our experiments by segmenting the training data based on evaluation sets.\nSpecifically, we select one evaluation set to represent in-distribution (IND) capabilities, with the\nremaining sets are considered out-of-distribution (OOD). Correspondingly, we retain only the por-\ntions of the training data aligned with IND capabilities. However, it is important to note two key\nchallenges: first, during the decontamination process, we already exclude any data that overlapped\nwith the evaluation sets; second, the scope of mathematical abilities inherently includes overlap and\ncoverage across different areas. Due to these factors, it is challenging to perfectly match training\ndata to specific capabilities. Therefore, we utilize knowledge point labels from the original problem-\nsolving data to segment out 0.83B middle school data, corresponding to ZHONGKAO as its IND\ncapabilities, and 0.89B high school data, corresponding to GAOKAO as its IND capabilities. The\nOOD capabilities are represented by the remaining evaluation sets that do not align with these IND\ncapabilities. More experimental design discussions can be found in the Appendix C. The specific\nexperimental design is as follows:\n\u2022 Basel: As described in Section 3. CPT with 48.3B general corpus and 14.7B math corpus.\n\u2022 Middle-school-SFT: SFT with 0.83B middle school data on Basel.\n\u2022 Middle-school-CPT: CPT with Basel data and middle school data.\n\u2022 High-school-SFT: SFT with 0.89B high school data on Basel\n\u2022 High-school-CPT: CPT with Basel data and high school data."}, {"title": null, "content": "Results. As shown in Table 2, for the IND capabilities represented by bolded evaluation results,\nlearning during the CPT stage consistently leads to greater improvements compared to learning dur-\ning the SFT stage. This effect is especially evident in the learning of more challenging high school-\nlevel knowledge. In addition, for OOD capabilities, learning during the SFT stage experiences\nsignificantly more disruption. This is particularly noticeable for GSM8K (see the data distribution\nand capability dimension chart in Appendix B), which has the largest distributional difference. After\nSFT, the model's performance on OOD tasks suffers more compared to CPT. Thus, we achieve Re-\nsult 4: Both SFT and CPT primarily develop capabilities aligned with their data distributions.\nHowever, SFT's IND learning ability is weaker than that of CPT. Regarding OOD capability\nlearning, the conclusions are less clear, with only the observation that SFT is more susceptible\nto disturbances from data distribution compared to CPT."}, {"title": "5.3 IMPACT OF DIFFERENT DIFFICULTY LEVELS", "content": "In the previous section, although we clarify that both CPT and SFT involve in-domain capability\nlearning, it remains unclear what cause SFT's learning performance to be weaker than CPT's. How-\never, conclusions in Result 4 are more evident in the high school training data compared to middle\nschool, prompting us to explore the difference in learning capabilities between CPT and SFT with\nvarying difficulty levels problem-solving data.\nExperiment. We select a 5B subset of our problem-solving data and categorize it based on the num-\nber of solution reasoning steps: data requiring 1-3 steps is classified as easy, 4-7 steps as medium,\nand 8 or more steps as hard. The distribution of samples account for 36.0%, 38.4%, and 25.6%\nof the total data, respectively, while token counts make up 23.0%, 36.0%, and 41.0%, respectively.\nGiven the unavoidable inaccuracies in this method of categorization, we focus solely on easy data\nand hard data for the CPT and SFT comparison experiments. More experimental design discussions\ncan be found in the Appendix C. The experimental groups are designed as follows:\n\u2022 Basel: As described in Section 3. CPT with 48.3B general corpus and 14.7B math corpus.\n\u2022 Easy-SFT: SFT using the easy data subset on top of Base1.\n\u2022 Easy-CPT: CPT incorporating both the Basel data and the easy data subset.\n\u2022 Hard-SFT: SFT using the hard data subset on top of Base1.\n\u2022 Hard-CPT: CPT incorporating both the Basel data and the hard data subset."}, {"title": null, "content": "Results. The results in the left half of Table 3, which is divided by vertical lines, show that CPT\nmodels consistently outperform SFT models, with some relative improvements specifically indi-\ncated. Notably, Hard-CPT exhibits greater relative enhancements compared to Easy-CPT, and these\nimprovements are not limited to just the hard domain accuracy but are observed across all datasets.\nMoreover, regardless of whether it is SFT or CPT, training on hard data consistently yields better\nresults compared to training on easy data. This suggests Result 5: Providing more challenging\nproblem-solving data enables more effective learning, and this advantage is particularly ev-\nident in CPT compared to SFT. This may be the primary source of the learning capability\ndifferences between CPT and SFT. Therefore, given limited computational resources, we rec-\nommend preparing more challenging problem-solving data for the CPT phase.\nThe results in right half of Table 3 indicate that both SFT and CPT models achieve their highest\nimprovements on Easy problems, with reduced gains as problem difficulty increases. For example,\nEasy-SFT and Easy-CPT show significant improvements of +7.66 and +12.75 on Easy problems, but\nonly +2.09 and +1.42 on hard problems, respectively. Similarly, Hard-SFT and Hard-CPT exhibit\ntheir largest gains on easy problems (+9.51 and +20.92) compared to hard problems (+1.99 and\n+4.47). These patterns suggest that Regardless of the training data's difficulty, both SFT and\nCPT primarily focus on learning to solve simpler, fewer-step problems."}, {"title": "6 TRAINING A STRONG MATH-SPECIFIC MODEL", "content": "To further validate the effectiveness of our empirical results, we aim to train a strong math-specific\nmodel based on the Llama3-8B (Dubey et al., 2024), named MathGPT-8B. We follow the conclu-\nsions from the three RQs outlined earlier: (1) We maintain a 3:7 ratio of mathematical corpus to\nproblem-solving data; (2) We use synthesized data from Query Expansion, Response Diversifica-\ntion, and Tutorship Amplification, with a focus on expanding data using the most efficient Tutorship\nAmplification method; (3) We filter and expand the raw data by focusing on problems with more than"}, {"title": null, "content": "five reasoning steps, using these as seed data to generate additional synthesized data. In addition,\nwe incorporate newly released mathematical corpora (Han et al., 2024) into the training. Ultimately,\nwe use 39.6B general corpus tokens, 46.7B mathematical corpus tokens, and 51.1B problem-solving\ndata and synthesized data tokens to train MathGPT-8B for 25,000 steps, with a global batch size of\n1024 and a context length of 8192 tokens. The learning rate is warmed up to 1e-4 and then decayed\nto le-5 using a cosine schedule.\nResults. As presented in Table 4, compared to the base model, we significantly enhance the founda-\ntional capabilities of Llama3-8B, even surpassing larger models such as Llama3.1-70B and Qwen2-\n72B, which have over 70 billion parameters. Additionally, we evaluate our model using the Gao et al.\n(2024) on the MMLU (Hendrycks et al., 2020) benchmarks, achieving a score of 0.6222 compared\nto Llama3-8B's 0.6211, demonstrating that it maintained its general knowledge capabilities.\nCompared to math-specific base models, MathGPT-8B outperforms DeepSeekMath-Base-7B (Shao\net al., 2024) and Qwen2-Math-7B (Yang et al., 2024a), and exhibits capabilities comparable to\nQwen2-Math-72B and the recently released Qwen2.5-Math-7B (Yang et al., 2024b). Compared\nto Qwen2.5-Math-7B, MathGPT-8B is trained on only 140 billion tokens (100 billion of which\nare math-related), while Qwen2.5-Math-7B utilizes 1 trillion tokens, as reported. Additionally,\nMathGPT-8B starts from a weaker base model. These findings validate our proposed method as\nan efficient approach to enhancing mathematical capabilities compared to existing paradigms. Fur-\nther discussions on related work can be found in Appendix F. Since we do not perform a complete\npost-training process, we are releasing the base version of our model. This allows the research\ncommunity to conduct further post-training to enhance its capabilities as needed."}, {"title": "7 CONCLUSION", "content": "In this study, we investigate the enhancement of mathematical reasoning capabilities in LLMs\nthrough alternative pre-training strategies. Our findings lead to the development of MathGPT-8B, a\ncompetitive model that outperforms most 7B models and exhibits capabilities comparable to much\nlarger models despite being trained on fewer tokens. Future work should expand in two key areas.\nFirst, we need to refine the data synthesis methods. Although we have demonstrated the effective-\nness of synthetic data, our current approaches are relatively naive. Second, we should explore the\nrole and impact of alignment processes during post-training. Investigating these aspects will help\nfurther improve the mathematical reasoning capabilities of the model."}]}