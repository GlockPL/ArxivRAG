{"title": "Identify As A Human Does: A Pathfinder of Next-Generation Anti-Cheat Framework for First-Person Shooter Games", "authors": ["Jiayi Zhang", "Chenxin Sun", "Yue Gu", "Qingyu Zhang", "Jiayi Lin", "Xiaojiang Du", "Chenxiong Qian"], "abstract": "The gaming industry has experienced substantial growth, but cheating in online games poses a significant threat to the integrity of the gaming experience. Cheating, particularly in first-person shooter (FPS) games, can lead to substantial losses for the game industry. Existing anti-cheat solutions have limitations, such as client-side hardware constraints, security risks, server-side unreliable methods, and both-sides suffer from a lack of comprehensive real-world datasets. To address these limitations, the paper proposes HAWK, a server-side FPS anti-cheat framework for the popular game CS:GO. HAWK utilizes machine learning techniques to mimic human experts' identification process, leverages novel multi-view features, and it is equipped with a well-defined workflow. The authors evaluate HAWK with the first large and real-world datasets containing multiple cheat types and cheating sophistication, and it exhibits promising efficiency and acceptable overheads, shorter ban times compared to the in-use anti-cheat, a significant reduction in manual labor, and the ability to capture cheaters who evaded official inspections.", "sections": [{"title": "1 Introduction", "content": "The gaming industry has experienced substantial growth recently, with the global game market generating $184.0 billion in 2023 and projected to reach $218.7 billion in 2024 [47,48]. However, cheating in online games poses a significant threat to the integrity of gaming experiences and disrupts gameplay balance. In 2023 alone, cheating led to an estimated $29 billion in losses, with 78% of gamers deterred from playing due to cheating [32]. In particular, first-person shooter (FPS) games, which account for 20.9% of total game sales and rank second among all genres [63], are particularly targeted by cheat developers seeking monetary gains due to their competitive and multiplayer nature. The most prevalent forms of cheating involve the use of aimbots to enhance a player's precision and wallhacks to reveal opponents' positions, thereby compromising game integrity [4, 9,40].\nOn realizing the critical importance of game security [25], both academia and industry have proposed many anti-cheats solutions, applied either on the client-side [9,49,58] or server-side [2, 24, 27, 39, 69, 70]. However, current solutions have the following limitations.\nClient-side solutions face hardware constraints, bring security and privacy concerns, or incur unacceptable system overheads. Two state-of-the-art systems [9,49] require specific Trusted Execution Environments (TEEs) like Intel SGX. This dependence on particular hardware, which was deprecated in 2021 [5,31], limits compatibility and practicality for industrial use. Some solutions involve scanning hard drives and gaining root privileges, posing risks of unauthorized access and personal data leakage [43, 45, 66]. Vulnerabilities, such as RCE exploits [8] and CVEs [15-18], demonstrate the potential for system instability and privacy breaches. The others rely on continuous surveillance or frequent patching to counter varying cheat signatures [3, 22, 35, 58], leading to increased client-side system overheads.\nOn the other hand, the existing server-side solutions use basic features and share low recall and accuracy. For instance, some solutions [2, 24, 27, 39, 69, 70] use features like winning rates or playtime, which are insufficient for detecting complex cheats such as aimbots or wallhacks. These methods also lack robust workflows to examine false positives, especially in noise-rich, real-world datasets. Additionally, they show low recall and accuracy in the real world, with extended ban cycles taking days or weeks [33,50]. Incidents like the 2024 APEX Legends tournament hack [59] highlight the inefficacy of existing solutions under commercial anti-cheats such as Easy Anti-Cheat (EAC) [22].\nMoreover, both client-side and server-side solutions suffer from a lack of comprehensive datasets, often relying on limited or synthetic data [2, 9, 24, 39, 49, 70], which may not represent real-world cheating scenarios. The simulated datasets lack varying levels of cheating sophistication (explained in Section 2.2), which could result in performance degradation or even failure in complex real-world scenarios.\nTo address the aforementioned limitations, we propose\nHAWK, a server-side approach designed to analyze data transmitted to the server, thereby avoiding the inherent limitations of client-side solutions. HAWK introduces a comprehensive set of multi-view features and a robust framework to meticulously monitor players' points of view, statistics, and behavioral consistencies. This approach includes a corresponding workflow aimed at shortening the ban cycle and considering the occurrence of false positives. HAWK achieves high detection recall and acceptable accuracy across large, real-world datasets with varying levels of cheating sophistication.\nThe key innovation of HAWK lies in its ability to mimic how human experts identify cheaters by focusing on three main aspects: analyzing the player's point of view, statistically evaluating the player's performance, and assessing the player's gaming sense and performance consistency. Based on these aspects, we design structured and temporal features that accurately represent player behaviors. HAWK reproduces the human identification process using Long Short-Term Memory (LSTM) and attention mechanisms, ensemble learning, and deep learning networks, respectively.\nWe evaluate HAWK on a mainstream FPS game, CS:GO. To the best of our knowledge, this paper is the first server-side work using a substantial volume of real-world datasets with different cheat types and levels of sophistication, making HAWK and the datasets more convincing for industrial use. Specifically, our dataset includes 2,979 aimbots and 2,971 wallhacks, totaling 56,041 players. The scale of the dataset has increased by two orders of magnitude compared to the state-of-the-art work [9]. HAWK achieves a maximum of 84% recall and 80% accuracy in detecting both aimbots and wallhacks, outperforming the official in-use anti-cheat. It successfully identifies cheaters who had evaded previous detections and those never caught by the official systems. We created a demo website to showcase some escapees' illegal actions. HAWK validates its robustness against cheat evolution (introduced in Section 6.7) in real-world scenarios and showcases acceptable overheads. We particularly discuss HAWK's generalizability in Section 7. The contributions of our work are as follows.\n\u2022 Innovative anti-cheat observations and feature designs.\n\u2022 Novel framework HAWK and a supporting workflow.\n\u2022 Real-world evaluations with strong performance.\n\u2022 Open-source HAWK and dataset\u00b2 for future research."}, {"title": "2 Background", "content": "This section introduces anti-cheat and cheating techniques, replay systems, and replay files in modern FPS games."}, {"title": "2.1 Server-side and Client-side Anti-Cheat", "content": "Client-side anti-cheats focus on data, process, and hardware protection [49, 58], or detection by either examining the existing cheats' signatures [3,22] or classifying through behavior [9]. However, attackers can bypass it with higher privileges, by sniffing the detection report through side-channel attack [23] and altering them in the transport layer, or by changing the program's signature [35].\nServer-side anti-cheats [2, 24, 39, 69, 70] emphasize data analysis and anomaly detection. Although the cheater's inputs might be altered on the client, the cheaters must ensure their performance outperforms others to secure the win. Thus, it is easier to detect cheats on the server because it records all clients' final-state actions that influence the gameplay. But the prior works have limitations mentioned in Section 1."}, {"title": "2.2 Cheat Types in FPS Games", "content": "Wallhack and aimbot are the two most prevalent types of cheats in FPS games [1]. Both types of cheats are mainly done by accessing the client's memory data or image information. Wallhack enables players to visually penetrate obstructions like walls, which allows players to spot opponents across the map and effortlessly track their movements, providing a significant tactical advantage. Aimbot assists a player's aims and shots so that the cheaters gain transcendent precision and reaction speed. Aimbot's functionality can be customized according to player preference, allowing one to obtain different aiming performance, ranging from brutal-force aims to smooth aims like human [9]. Aimbot can be subdivided into pure aimbot (the primitive aimbot), triggerbot (auto-shoot only when the cross-hair is on the opponent), micro-settings (counteract recoil with Hotkeys), and computer-vision-based aimbot (aim with object detection models). In the following sections, aimbot refers to all aforementioned subcategories.\nNotably, cheating provides different levels of sophistication [66]. The cheater might cheat cautiously, pretending to be a normal player, switching the cheat on and off, or cheating unabashedly with different configurations, various brands of cheating software, and categories. However, cheaters' behaviors will always differ from the normal players [9]. Thus as long as cheaters want to gain unfair advantages, they must outperform the average. This is bound to create anomalies at the data level and therefore should be identifiable by models. Our evaluations in Section 6 include different levels of cheating sophistication by using large real-world datasets."}, {"title": "2.3 Replay System and Replay File", "content": "Replay system is a dynamic toolset for player interaction with recorded content post-game and has been extensively adopted in notable FPS games. Unlike video recordings, this system enables users to explore various perspectives, adjust camera angles, and control playback speed for analysis by reconstructing the match with the replay file. Replay file refers to a comprehensive record of a match, encapsulated in a unique data format that can be interpreted by the replay system. Demo denotes the replay file in CS:GO. HAWK innovatively utilizes demo to extract data and avoid concurrent overheads on the server. We believe the replay file can be used as a novel data source for FPS server-side anti-cheat."}, {"title": "3 Overview", "content": "This section provides the key observations on the human experts' identification process and the workflow of HAWK."}, {"title": "3.1 Fact Observations", "content": "Although the anti-cheat tasks are still challenging for automation systems in not just the FPS field but almost any game genre, the illegal activities are identifiable for seasoned human players [11]. Therefore, by harnessing such inspiration, we discovered three observations that human experts always focus on to identify cheaters.\nObservation 1 (OBS.1). Review a player's point-of-view (POV).\nAkin to watching a player's game replay unfold over time, this review process involves analyzing various in-game behaviors, such as aim and shot, positioning and movement, and the in-game economy management and props utilization. These behaviors can provide temporal insights into whether the player is legitimately playing the game.\nObservation 2 (OBS.2). Conduct an in-depth analysis of a player's statistical data.\nCheaters often exhibit statistical anomalies that set them apart from normal players. These anomalies include abnormally high accuracy and eliminations, a low death rate, and a high number of kills made through obstacles, etc. These statistical deviations serve as strong indicators of cheating.\nObservation 3 (OBS.3). Examine the consistency between a player's game sense and combat performance.\nA player requires substantial game experience to have better performances, which results in a positive correlation between a player's game sense and their performance. Thus, the inconsistency between the two aforementioned factors indicates potential cheating. For instance, if a player moves like a rookie but shoots and aims incredibly accurately, it is suspicious."}, {"title": "3.2 HAWK's Workflow", "content": "Figure 1 illustrates HAWK's workflow, comprising four inter-linked stages.\nStage One. Preprocessing. The demo is applied to parse in-game data from the banned databases. The multi-view features are then extracted from parsed data and then annotated for training. We discuss the feature constructions in Section 4.\nStage Two. HAWK deployment. HAWK utilizes the extracted features as inputs for its subsystems REVPOV (Section 5.1), REVSTATS (Section 5.2), and EXSPC (Section 5.3) to mimic human identification process, respectively. The outputs are incorporated into MVIN subsystem (Section 5.4) that presents a final cheating report.\nStage Three. Game Master (GM) team verification. Given the potential personal assets tied to a player's account, a ban should not be issued solely based on an algorithmic verdict. It is imperative to provide substantial evidence before initiating such an action. Hence, the final verdict requires GMs' verification to avoid false bans.\nStage Four. Data updates. Upon the affirmation of cheating, the relevant replay files are added to the banned databases. This act enriches and updates the reservoir of training samples which may contain novel patterns and behaviors. Thus, four stages constitute a cyclical workflow, with each stage contributing to the subsequent one.\nThis ongoing cyclical workflow enhances the system's robustness and provides a dynamic solution to multi-type cheat detection. We further discuss this in Section 6.7"}, {"title": "4 Feature Constructions", "content": "The feature design is one of our contributions and key to the server-side anti-cheat. This section introduces the construction of temporal features, structured features, and the classification of sense and performance features. These features are intended to be a comprehensive representation of the three aspects of human identification of cheating in Section 3.1."}, {"title": "4.1 Temporal Features", "content": "Temporal features are information derived from the demo in the time domain. Each temporal data tuple represents the state information (e.g., coordinates, view directions, events, etc.) associated with the present tick. Where tick denotes the smallest unit of time in the game. This section describes temporal features' components and illustrates the distinctions in temporal features between normal players and cheaters."}, {"title": "4.1.1 Temporal Features Construction", "content": "According to different in-game behaviors, temporal features are segmented into three main categories and subdivided into seven specific types. This overview provides only categories and types due to the less-expressive nature of temporal features. Appendix B lists elaborate descriptions.\n\u2022 Engagement features describes in-game engagement events, which are subcategorized into five types as follows.\nDamage features indicate damage-inflicting events, including the occurring tick, the attacker's and victim's locations, view directions, weapons, damage value, etc.\nAuxiliary props features related to auxiliary props-utilization events (e.g., flashbang), including the deploy tick, victim's affected duration, etc.\nOffensive props features related with events involving props that aid the attack (e.g., grenade, incendiary, etc.) including the deploy and destroy ticks, the props' type, the props' coordinates, etc.\nElimination features describes kill-related events, including the occurring tick, attacker's weapon and location, victim's location and view direction, etc.\nWeapon fire features denotes firing-related occurrences, including the occurring tick, the weapon information, the shooter's location and view direction, etc.\n\u2022 Movement features represents in-game positioning, mobility, and boolean flags per tick. For instance, coordinates, view directions, velocities in three dimensions, flags that indicate if the player is ducking, blinded, reloading, etc.\n\u2022 Economy features reflects in-game financial aspects. For example, they include the equipment value and balance per round for each player."}, {"title": "4.1.2 Temporal Data Comparative Visualization", "content": "Figure 2 comparatively analyzes the temporal features between cheaters and normal players across one round. Where round denotes the unit determining each win-loss outcome. One match has multiple rounds, and the players who win more rounds are the winners. In this example, there are four differences between cheaters and normal players. First, since the cheaters illegally acquire their opponents' position, the timing of cheaters' firing (pink) and engaging (red) highly overlap (a). Second, normal players stay on guard (blue) irregularly to check for potential enemy locations, whereas cheaters are often on guard just before engagement (b). Third, normal players deploy props (orange) to gain a gunfighting or positional advantage, whereas cheaters rarely do so because they already have illegal advantages (c). Fourth, cheaters are rarely vulnerable (yellow) during the fire or engagement because they know the enemy's location and therefore can circumvent it in advance (d)."}, {"title": "4.2 Structured Features", "content": "Structured features are single-value data calculated with well-designed algorithms to represent integrated in-game behavioral information per match. This section describes the components of structured features and visualizes the structured feature differences between normal players and cheaters."}, {"title": "4.2.1 Structured Features Construction", "content": "The structured features for HAWK are categorized into four main categories that comprise 28 strong-expressiveness features. For the sake of brevity and coherence, only the introduction of categories is outlined below. A meticulous breakdown can be referred to in Appendix C.\n\u2022 Aiming features represents the efficiency and efficacy of aims. We break down the aiming process into three moments (initial spot, first-time fire, and first-time hit) and two stages (reaction and adjustment). We extract different elapsed durations and angle variations with statistical representations (e.g., average or variance) to indicate the player's different performances within a match. For example, the average of reaction duration reflects a player's level of reaction speed, and the variance of that denotes the distribution of the reaction duration. Typically, in terms of"}, {"title": "4.3 Sense and Performance Features", "content": "Sense and performance features are derived from the two subsections above. Appendix D includes detailed classification.\n\u2022 Sense features denote a player's in-game cognition, reflecting strategic understanding, and tactical anticipation.\nTemporal sense features include temporal features that reflect a player's gaming sense such as economic considerations, movement dynamics, grenade usage, etc.\nStructured sense features contain structured features that describe in-game understanding, e.g., flash efficiency, prop utilization, etc.\n\u2022 Performance features demonstrate in-game efficacy, indicating engagement capabilities, precision in aiming, and mastery over game mechanisms.\nTemporal performance features include temporal features that indicate continuous in-game performances, e.g., eliminations, damage, weapon proficiency, etc.\nStructured performance features contain the remaining structured features that statistically describe the player's in-game performances, such as time to kill, elimination-related features, etc."}, {"title": "5 HAWK", "content": "To mimic human experts' identification process, we propose HAWK which consists of four subsystems: Review Point-of-View (REVPOV) subsystem, Review Statistics (REVSTATS) subsystem, Examine Sense-Performance Consistency (ExSPC) subsystem, and Multi-View Integration (MVIN) subsystem. The first three subsystems' designs are associated with the three observations we discovered regarding the inherent nature of the cheat identification process as elaborated in Section 3.1. The fourth subsystem integrates the results derived from the first three subsystems."}, {"title": "5.1 REVPOV", "content": "Aligned with OBS.1, REVPOV thoroughly examines a player's time-series operations within a match.\nMulti-LSTM Attention Encoders: In Figure 4 (a), we feed the seven sets of temporal features (Section 4.1.1) as input to seven independent Long Short-Term Memory (LSTM) [29] networks to acquire the embeddings. For explanatory purposes, we illustrate the process using the example of the Economy features. Feco denotes the input feature vector. A dropout layer [54] is employed from the second to last LSTM layers respectively to mitigate the risk of overfitting. h denotes the output of the encoder, consists of one set of the seven temporal features mentioned in Section 4.1.1, where G represents the set and n indicates the sequence number among all outputs. For each time step, we introduce the Luong-style attention [41] afterward to help the model better understand the importance of different positions in the input sequence and aggregate different parts of the input weights. In Equation 1, \u03b1 represents the attention weight for the time step i. Wq and Wk denotes the learned weight matrices of query and key in attention mechanism. i, j and k indicate different time steps. Concatenation operation is executed afterward, yielding [\u010f||h", "Generation": "An average pooling is conducted to flatten the output sequence. Equation 2 indicates the average operation, where N denotes the number of output sequences. Vpov is used as embedding for the final determination.\nFinal Determination: In the context of cheat detection, the dataset often exhibits a class imbalance, where the majority of players are honest, inadvertently leading to biased learning towards the dominant class. Therefore, we adopt a multi-subsampling strategy to level the playing field for the minority class, creating balanced representations of both cheating and honest behaviors. Suppose T is the initial training set. To establish a balanced class distribution, we perform a selective sampling on T, specifically targeting the honest player samples. We perform this operation nine times in accordance with the ratio of honest to dishonest players, generating nine distinct training sets, each denoted as Tk, where k indicates the iteration count. Conduct each iteration of the sampling process to select unique honest player samples, thereby adding varied examples to the learning process. Subsequently, we obtain nine separately trained Random Forest models, each corresponding to one of the diversified and balanced training sets Tk, we named this strategy as multi-forest. The averaging method in Equation 4 is selected for REVPOV final class prediction. Consider a random forest consisting of N decision trees, each producing a prediction denoted as hi(Vrbov), where i indicates the i-th tree, k denotes the k-th sampled set, and Vistm represents the input feature vector. The prediction of each tree is a probability distribution, indicating the likelihood of the sample Vo belonging to each class. pi represents the probability that the i-th tree predicts sample Voy to be in class j. In Equation 4, RF(Vo) denotes the average prediction output of the random forest, which is the average of the prediction probability vectors of all decision trees. The prediction for each Dr can be obtained by selecting the class with the highest predicted probability shown in Equation 5, where (RF(Vo)); represents the probability of class j in RF(Vo). After the multi-forest, we acquire the corresponding determination Dk. Lastly, as in Equation 6, we obtain the final classification determination Dpov for REVPov."}, {"title": "5.2 REVSTATS", "content": "Aligned with OBS.2, REVSTATS subsystem is designed to perform an in-depth analysis of structured features to capture the cheaters. Cheaters often exhibit deviations in their static data compared to normal players. Even when cheaters deliberately attempt to conceal their behavior, the subtle differences tend to accumulate over time and reveal disparities in the statistical data post-game.\nFeature Extraction: In Figure 4 (b), V28 denotes the vector representation of the structured features (Section 4.2.1).\nEnsembled Committee: We empirically use seven classifiers [7,13,14,28,34,46,55] in an ensembled way. Each model is independently trained. Upon completion, each model independently adjudicates each test sample. The outputs are binary signifying the presence or absence of cheating.\nFinal Determination: Due to imbalanced dataset, a similar multi-subsampling strategy as employed in the REVPOV subsystem is adopted for the REVSTATS subsystem to fully leverage the available data without losing valuable information. For each classification model, we generate a distinct model for each subsampled set. Each model provides an individual prediction. A majority voting scheme is utilized within each classification algorithm to aggregate the results of the nine models. The final determination is also achieved through the majority voting mechanism, which collates the determinations from all seven classification algorithms. In Equation 7, where Dstats signifies the binary final determination. Where wj denotes the decision from the jth classification algorithm, and I(\u00b7) is the indicator function, returning 1 if the condition within the brackets is true, and 0 otherwise. The decision, i, which receives the majority of votes from all seven classification algorithms, is accepted as the final determination."}, {"title": "5.3 EXSPC", "content": "Aligned with OBS.3, EXSPC subsystem is designed to inspect a comprehensive investigation of the consistency of a player's in-game senses and overall performances.\nEmbedding and Feature Grouping: There are seven embeddings and two groups of vectors as input (Section 4.3) in EXSPC, in which seven embeddings are generated by using the trained models in REVPOV and two vectors are the subsets of V28 in REVSTATS. Each embedding is padded to the maximum length of that in the whole dataset. w is the intermediary result from REVPOV, where k represents the numbering in the sequence, G denotes the type of the temporal features. Each embeddings are represented in two-dimensional arrays. In Figure 4 (c), P\u2081 to Pk indicates different players, we exemplify with one player Pj. Therefore, we flatten the embeddings into one-dimensional arrays and use a dense layer with an ELU activation function to shrink the dimensions. After the seven different similar groups of layers, we obtain oPj. Subsequently, we divide seven different types of tensors as shown in Appendix D, and concatenate each group of tensors for each player. In which, sense vector of the temporal features Pj is Vpov = [mov ||eco||woff||waux] and performance vector of  the temporal features is Vprov = [||elm||amd] In terms of two vectors, we also divided V28 into two groups according to Appendix D, namely the sense vector of structured features V228 sense and the performance vector of structured features Vperf 28.\nDimension Reduction: V228 sense and V228 perf are normalized at first. Afterward, each group of the four vectors goes through different groups of dense layers to reduce the dimensions. Next, we concatenate four vectors and yield two vectors eventually representing sense VSENSE = [Esen228 pov||Esense228 ] and performance VPERF = [Estas228 pov||Epref 228].\nFinal Determination: When acquiring VSENSE and VPERF, we further reduce its dimension and deepen the network for better learning performances. Then, we concatenate two vectors and go through one final dense layer for reducing the dimension and another dense layer with sigmoid activation function for binary classification. Eventually, the final classification determination Dspc for EXSPC subsystem is obtained. The class weight is set as one to nine like previous subsystems. Both REVPOV and EXSPC use Binary Cross-Entropy as loss function shown in Equation 8, where N represents the number of samples, yi is the true label of the i-th sample, \u0177\u00a1 is the predicted value for the i-th sample."}, {"title": "5.4 MVIN", "content": "In Figure 4 (d), MVIN is to harness the collective determinations from REVPOV, REVSTATS, and EXSPC. By integrating the predictions from these subsystems, MVIN aims to deliver a final verdict regarding a player's cheating disposition.\nData Integration: The subsystem integrates the outcomes from REVPOV, REVSTATS, and ExSPC, symbolized as Dpov, Dstats, and Dspc. Given the diverse nature of these outputs, a structured preprocessing phase becomes indispensable. This step is designed to merge the individual outputs and synchronize them for the succeeding integration process.\nModel Optimization: MVIN adopts a weight matrix for each subsystem's determination, where weights are assigned based on the subsystem's importance and reliability. The weighted output, W, is calculated as Equation 9. Where 21, 22, and 23 represent the weights associated with the outputs of the subsystems REVPOV, REVSTATS, and EXSPC, respectively.\nFinal Determination: Once W is calculated, it then undergoes thresholding via the Task-Specified Threshold Optimizer. This optimizer is to provide a binary determination based on a specified threshold \u025b. The exact nature of this threshold optimizer, whether it leans towards recall, accuracy, or other metrics can be set based on user or system requirements. For instance, in a recall-focused setting, the optimizer would aim to identify every potential cheater, accepting the risk of higher false positives. Conversely, in an accuracy-driven mode, the optimizer would aim to reduce false positives, even if it means potentially missing some cheaters. This threshold \u025b therefore acts as a decision boundary, converting the continuous weighted value W into a discrete binary outcome DHAWK. By harnessing the strengths of each subsystem and compensating for their inherent limitations, MVIN is crucial to integrate determinations from different subsystems. Task-Specified Threshold Optimizer offers Game Masters (GM) a degree of selectivity, enabling them to fine-tune the system based on the desired anti-cheat needs."}, {"title": "6 Experiments", "content": "We conduct experiments to answer six research questions (RQ.1-RQ.6):\n\u2022 RQ.1: What are the performances of HAWK and each subsystem? (Section 6.4)\n\u2022 RQ.2: Compared to the current official inspection, to what extent HAWK has improved in terms of effectiveness and efficiency? (Section 6.5)\n\u2022 RQ.3: What are the performances of HAWK under different settings of the Task-Specified Threshold Optimizer? (Section 6.6)\n\u2022 RQ.4: What is the robustness of HAWK and its subsystems against cheat evolution? (Section 6.7)\n\u2022 RQ.5: Is there any suspicious activity that still exists but has never been detected by officials? (Section 6.8)\n\u2022 RQ.6: What are the overheads of HAWK's subsystems? (Section 6.9)"}, {"title": "6.1 Incomparability to Prior Works", "content": "First, most prior works [2,24,39,69\u201371] are not open-sourced. Second, data collection method is distinct and incompatible. HAWK utilizes the replay file post-game, which differs fundamentally from prior studies that modified the game engine or applied mods during gameplay. Third, the installation on commercial clients or servers is infeasible. If the installation of the prior works is implemented before the commencement of data collection, the data can then be gathered and evaluated. However, the demos we collected come directly from the game platform, which prohibits the installation of any unverified programs on clients or servers due to legal constraints. In addition to the aforementioned reasons with commonality, we also further state below the specific reasons for the detailed incomparability between HAWK and the SOTA designs. There are three aspects of incomparability with BotScreen [9]. First, CS:GO has gone through several major patch updates after BotScreen was released. The authors of BotScreen confirmed through email that BotScreen, as well as Osiris [37] (the identical cheat used in BotScreen for feature extraction and evaluation), does not apply to the in-use version of games. Second, the deprecation of Intel SGX [5,31] prevents us from conducting comparative experiments. Third, it can only detect pure aimbot, making the comparison less significant. Black-Mirror [49] and Invisibility Cloak [58] are prevention designs only for counteracting wallhacks and computer-vision-based aimbots respectively. Both studies are only functional before the cheats happen. Whereas HAWK and other prior works are detection systems that functional post-cheating."}, {"title": "6.2 Experimental Settings and Ground Truth", "content": "The demos are open-sourced and acquired from a well-known and recognized CS:GO platform\u00b3. The initial data extraction utilizes an open-source library awpy [67] to parse the raw data in demos into JSON files, which contain comprehensive information for the following feature constructions.\nTable 1 is the dataset description. Figure 5 illustrates the experiments' process. We evaluate HAWK with two of the most prevalent cheat types, aimbot and wallhack. For aimbot, we set one collection window (16 days in total). For wallhack, we set two collection windows (27 days in total, we explain the reason for window inconsistency in Section 6.7). All demos are real-world ranking matches within the collection windows. Therefore, the cheats used by the player are black-boxes for anti-cheating. Both datasets contain different levels of cheating sophistication.\nThe dates of cheaters' label acquisition both end on the last day of data collection in each set (i.e., first labeling),"}, {"title": "6.3 Evaluation Metrics", "content": "Our chosen evaluation metrics are more skewed towards reflecting how well the cheaters can be identified. We use the following metrics in Appendix F and Operational Efficiency Index (OEI) to distinguish between cheating and normal behaviors. OEI is a new metric for evaluating the efficiency of industrial operations. Operational Efficiency Index (OEI), represented in Equation 10, where N represents the total number of players. It reflects the operational efficiency gains in manual review processes. A higher OEI indicates a greater reduction in manual verification and a more effective system."}, {"title": "6.4 Ablation Study Results", "content": "To answer RQ.1, we conduct an ablation study on HAWK shown in Table 2. We set the Task-Specified Threshold Optimizer to achieve the highest accuracy with at least 70% recall for detecting aimbot, and the highest AUC-ROC for detecting wallhack, illustrating different anti-cheat scenarios. Overall, HAWK excels in identifying authentic cheaters, evidenced by the highest recall and NPV across all evaluations. For aimbot detection, REVSTATS enhances accuracy by effectively filtering normal players in both validation and test sets. In wallhack detection, REVSTATS excels in the validation set, while EXSPC leads in the test set. Although some components like REVSTATS and EXSPC outperform HAWK in accuracy, their lower recall (up to 64%) limits their effectiveness, affirming HAWK as the optimal choice. Notably, the Wallhack dataset yields better overall performance than the Aimbot dataset. We believe this is due to the latter's diverse subcategories of aimbots. In Table 5 and Table 3, we present REVPOV's and REVSTATS's model selection results among hundreds of tested models, and promising models are demonstrated. REVPOV outperforms other models with balanced, consistent accuracy and recall. Random Forest outperformed other models under the sampled data in REVPOV. Thus, we adopt multi-forest and multi-subsampling with Random Forest. REVSTATS outperforms individual classifiers in balancing recall and accuracy by capitalizing on the strengths of each classifier while mitigating their weaknesses. The training and validation loss for EXSPC in Figure 9 and Figure 10 (further discussed in Appendix A)."}, {"title": "6.5 HAWK and Official Inspection Comparison", "content": "To answer RQ.2, we calculate the daily number of bans for the day and cumulative ban-sums by HAWK and the official inspections illustrated in Figure 6. The figure indicates that HAWK's identification effectiveness (daily count in red bars) and efficiency (cumulative count in red lines) outrank those of the official manual inspections (blue bars and lines) on both cheat types. Additionally, Table 4 compares the average ban duration per match, revealing that HAWK's seconds-scale processing time is exponentially more efficient than official inspections. The remediation rate indicates the percentage of cheaters not banned in the first labeling but caught in the second labeling. In Table 4, we list the remediation rate after the second labeling among HAWK's false positives (which are obtained after the first labeling). It demonstrates that HAWK can capture the authentic cheaters among the normal players initially labeled by the official anti-cheats. This also suggests that while HAWK is robust to mislabeled data, using a purer dataset would further improve results."}, {"title": "6.6 Analysis on Different MVIN Settings", "content": "To address RQ.3, we conduct a comparative analysis of HAWK under different MVIN settings, as shown in Table 6. The Optimizer column indicates the priority choices of the Task-Specified Threshold Optimizer, such as F1-Score for highest F1-Score"}]}