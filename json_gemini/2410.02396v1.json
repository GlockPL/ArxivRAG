{"title": "Parameter Competition Balancing for Model Merging", "authors": ["Guodong Du", "Junlin Lee", "Jing Li", "Runhua Jiang", "Yifei Guo", "Shuyang Yu", "Min Zhang", "Sim Kuan Goh", "Ho-Kin Tang", "Daojing He", "Hanting Liu"], "abstract": "While fine-tuning pretrained models has become common practice, these models\noften underperform outside their specific domains. Recently developed model\nmerging techniques enable the direct integration of multiple models, each fine-\ntuned for distinct tasks, into a single model. This strategy promotes multitasking\ncapabilities without requiring retraining on the original datasets. However, exist-\ning methods fall short in addressing potential conflicts and complex correlations\nbetween tasks, especially in parameter-level adjustments, posing a challenge in\neffectively balancing parameter competition across various tasks. This paper\nintroduces an innovative technique named PCB-MERGING (Parameter Competi-\ntion Balancing), a lightweight and training-free technique that adjusts the coeffi-\ncients of each parameter for effective model merging. PCB-MERGING employs\nintra-balancing to gauge parameter significance within individual tasks and inter-\nbalancing to assess parameter similarities across different tasks. Parameters with\nlow importance scores are dropped, and the remaining ones are rescaled to form\nthe final merged model. We assessed our approach in diverse merging scenarios,\nincluding cross-task, cross-domain, and cross-training configurations, as well as\nout-of-domain generalization. The experimental results reveal that our approach\nachieves substantial performance enhancements across multiple modalities, do-\nmains, model sizes, number of tasks, fine-tuning forms, and large language models,\noutperforming existing model merging methods. The code is publicly available at:\nhttps://github.com/duguodong7/pcb-merging.", "sections": [{"title": "1 Introduction", "content": "Pre-trained models (PTMs) are fundamental in deep learning, underpinning many current techniques\ndue to their ability to learn generalized features from large datasets [99, 5]. Fine-tuning PTMs\nfor specific tasks is a common practice to boost performance [71, 31]. This approach is prevalent,\nresulting in thousands of fine-tuned checkpoints [85], based on widely used PTMs [59, 80, 58].\nHowever, fine-tuning the same model for different tasks can result in performance variations, posing\na significant challenge [57]. Multi-task learning [66, 59] has been proposed as a solution, but it\nincurs substantial training costs and requires simultaneous access to data and labels for all tasks [16].\nRecently, some researchers have developed methods to merge multiple independently-trained models\ninto a single model without the need for original training data [20, 86, 27]. This merging technique\nnot only adheres to data privacy regulations [83] but also enhances efficiency by eliminating the need\nfor retraining."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Overview of model fusion", "content": "Deep model fusion is gaining attention due to data privacy and resource conservation concerns, with\npotential applications across various domains [39, 14]. It's typically divided into three main categories.\nEnsemble learning [64], combines model outputs to improve prediction accuracy and robustness but\nrequires parallel deployment of multiple models. An alternative method involves mode connectivity\n[18] and alignment [1], aiming to bring solutions closer together for better initial conditions in\naveraging. This is achieved by either linking optimization paths [15, 98] or addressing permutation\ninvariances [72, 79, 40, 30]. Recent researches [88, 75] focus on training-free approaches to enhance\nmodel fusion usability. The third approach, weight averaging [20, 86], requires models with identical\nstructures. While advancements like [81] support merging diverse large language models (LLMs),\nthey require knowledge distillation [23] and complex training. This paper follows the third type of\ntrack due to its simplicity, efficiency, and broad applicability."}, {"title": "2.2 Merging fine-tuned models with same initialization", "content": "Previous studies found that when multiple models are fine-tuned from the same pre-trained initializa-\ntion, averaging their weights can lead to improved performance on single tasks [20, 86, 13, 29, 92]\ndifferent tasks [27] and out-of-distribution generalization [3, 60]. Fisher Merging [46] goes beyond\nsimple averaging to identify the importance of individual parameters using Fisher information matrix\n[17] and uses it to weigh the parameters in each model when merging. RegMean [30] proposed a\nclosed-form solution for the merged model's parameters by solving a local linear regression problem\nfor each individual linear layer in the model. However, both the Fisher Merging and RegMean\nmethods are time-consuming and computationally intensive.\nTask Arithmetic [28] introduces the concept of task vectors, demonstrating their effectiveness and\nlightweight nature in facilitating cross tasks generalization. Expanding on this groundwork, PEM\nComposition [96] extends the task arithmetic framework to merge LoRA [24] models, while Ties-\nMerging [89] addresses task conflicts by resetting redundant parameters and resolving sign conflicts.\nHowever, these methods share a merging coefficient across all task vectors, limiting flexibility. In\ncontrast, Lorahub [25] and AdaMerging [90] utilize different coefficients for enhanced adaptability,\nbut Lorahub's performance is restricted as it only searches coefficients at the task level. AdaMerging\nalso demands complex training and unlabeled test datasets and is applicable solely to classification\nproblems. DARE [94] proposes drop and rescale as a preprocessing step when merging fine-tuned\nLLMs. Our approach mainly adopts the strategies of using drop to reduce interference and performing\nrescale at the parameter level, while simultaneously considering self-awareness and cross-model\nawareness."}, {"title": "3 Method", "content": "In Sec. 3.1, we established the notation and outlined the problem of model merging. Sec. 3.2 delves\ninto the detailed exposition of the proposed PCB-MERGING method, which aims to balance parameter\ncompetition. Furthermore, in Sec. 3.3, we employ evolutionary algorithms to further enhance the\nperformance of our approach."}, {"title": "3.1 Preliminaries", "content": "Initially, we are faced with a set of tasks {$T_1,...,T_n$} and various pre-trained models, such as\nViT [12], T5 [59], or llama2 [80]. We have the option to fine-tune the entire model or employ\na parameter-efficient fine-tuning (PEFT) method [42, 24]. During fine-tuning, we represent the\ntrainable parameters as $\\theta$, initialized as $\\theta_{pre}$, and the fine-tuned parameters as $\\theta_{ft}$. The model merging\nproblem involves how to combine the weight sets {$\\theta_1,..., \\theta_n$} to form a new weight $\\theta_m$, without the\nneed to retrain using the initial training data for each task, and ensuring that $\\theta_m$ can simultaneously\nperform tasks {$1, ..., N$}.\nRecent research [28] introduced the concept of task vectors and completed various task arithmetic\noperations and model merging based on task vectors. Specifically, for task $T_i$, the task vector $\\tau_i \\in R^d$\nis defined as the vector obtained by subtracting the fine-tuned weights $\\theta_i$ from the pre-trained weights\n$\\theta_{pre}$, i.e., $\\tau_i = \\theta_i - \\theta_{pre}$. This allows us to focus on the changes that occur during each task-specific\nmodel's fine-tuning phase. The task vector-based multi-task model merging method can be expressed\nas $\\theta_m = \\theta_{pre} + \\lambda * \\sum_{i=1}^{n} \\tau_i$, where the coefficient $\\lambda$ represents the importance of merged task\nvector $\\tau_m$. This concept is simple yet effective, significantly outperforming simple weight averaging\nschemes, i.e., $\\theta_m = (1/N) \\sum_{i=1}^{N} \\theta_i$."}, {"title": "3.2 Parameter Competition Balancing", "content": "Our approach aims to modulate the scaling factors for each task and parameter, achieving intra-\nbalancing and inter-balancing within and between tasks. Specifically, we use the parameter competi-\ntion balancing (PCB) matrix $\\beta_i \\in R^d$ to adjust the scale of parameters in each task model $\\tau_i \\in R^d$,\nresulting in the final fused model, as shown in Fig. 3. The specific calculation process is as follows:\n1. Intra-Balancing: Initially, we implement self-awareness by applying a nonlinear activation\nfunction (i.e., softmax) to the magnitudes of task vectors, emphasizing important parameters\nwhile suppressing redundant ones to some extent. As the number of fusion tasks increases,\ncompetition among parameters intensifies. Therefore, the number of tasks N is used to regulate\nthe degree of suppression of redundant parameters.\n$\\beta_{intra,i} = Softmax(N * Norm(\\tau_i \\odot \\tau_i))$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(1)"}, {"title": "2. Inter-Balancing", "content": "Next, we realize cross-awareness to enable the parameters within a population\nof tasks to interact with others, addressing potential conflicts and complex correlations between\ntasks. To achieve this, we compute the similarity between parameters at the same positions\nacross different task vectors, allowing each parameter to update its score based on information\nfrom other tasks. The calculation process is as follows:\n$\\beta_{inter,i} = \\sum_{j=1}^{n} Softmax(Norm(\\tau_i \\odot \\tau_j))$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(2)"}, {"title": "3. Drop and Rescale", "content": "Subsequently, we obtain $\\beta_i = \\beta_{intra,i} \\beta_{inter,i}$. Next, we construct a\nmask $m_i \\in R^d$ based on $\\beta_i$ to focus on the more important parameters. Specifically, this mask\n$m_i$ is used to select high-scoring elements from the D elements of $\\beta_i$. We define the mask ratio\nas r, where 0 < r < 1. The mask $m_i$ can be derived from:\n$m_{i,d} = \\begin{cases}\n1, & \\text{if } \\beta_{i,d} \\ge sorted(\\beta_i)[(1 - r) \\times D] \\\\\n0, & \\text{otherwise}\n\\end{cases}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(3)\nThe importance score is defined as $\\hat{\\beta} = m_i \\odot \\beta_i$. Finally, we use the score of the masked\nbalancing matrix to weight the importance of each parameter in each task vector. The final\nmerged task vector $\\tau_m$ is as follows:\n$\\tau_m = \\sum_{i=1}^{n} (\\hat{\\beta} \\odot \\tau_i)/ \\sum_{i=1}^{n} \\hat{\\beta}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(4)\nFrom the final merged task vector $\\tau_m$, we can further adjust its magnitude proportionally and integrate\nit with the initial parameter values to yield the amalgamated model parameters $\\theta_m$, represented by\n$\\theta_m = \\theta_{pre} + \\lambda * \\tau_m$, with $\\lambda$ serving as a scaling hyperparameter. More details about the method\nworkflow are presented in App. A and Algorithm 1."}, {"title": "3.3 Searching Coefficients", "content": "Research from articles [28, 90] shows that model merging methods based on task vectors are\nhighly sensitive to the merging coefficient $\\lambda$. Even with an appropriately chosen uniform $\\lambda$, further\nimprovement in fusion performance requires grid searching the merging coefficients for each task\nvector. This process is complex and cumbersome, particularly when dealing with a large number of\ntasks.\nInspired by prior research [77, 25], we employ intelligent optimization algorithms to search for\nmixing coefficients, aiming for greater improvements compared to using a uniform coefficient. The\noptimization process seeks the best set {$\\lambda_1, ..., \\lambda_n$} to enhance validation accuracy, with the ultimate\ngoal of maximizing validation accuracy with the merged model.\n$\\theta_m = \\theta_{pre} + \\sum_{i=1}^{n} (\\hat{\\beta} \\odot \\lambda_i \\tau_i)/ \\sum_{i=1}^{n} \\hat{\\beta}$\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(5)\nIn most of our experimental setups, we primarily utilize Covariance Matrix Adaptive Evolution\nStrategies (CMA-ES) [21]. As a probabilistic population-based optimization algorithm, CMA-ES\ndynamically adjusts the search distribution defined by the covariance matrix. It systematically updates\nthe mean and covariance of this distribution at each iteration to learn and exploit the underlying\nstructure of the search space for optimization efficiency."}, {"title": "4 Experimental setup", "content": "Evaluation Settings. We anticipate that merging models will offer two significant advantages\nfor developers. Firstly, by integrating insights from individual models $\\theta_{1..n}$ trained in different\nenvironments (such as tasks, domains, or various training configurations within a single task), we\nexpect the resulting merged model $\\theta_m$ to demonstrate competitive test performance across tasks,\ndomains, or within a single task. Secondly, this merged model is poised to exhibit enhanced cross-\ndomain (OOD) generalization capability. For further details about compute resources and fine-tuning\nprocedures, please refer to App. F.1 and F.2."}, {"title": "5 Results", "content": "In this section, we evaluated the performance of the PCB-MERGING method across various experi-\nmental settings, including cross-task, cross-domain, cross-training configurations, and out-of-domain\nscenarios. Additionally, we conducted several experiments to further assess the effectiveness of our\nmethod: merging different numbers of tasks (App. C.1 and Fig. 8), comparison with AdaMerging on\nvision tasks (App. C.2 and Tab. 7), and providing additional results using evolutionary strategies (ES)\n(App. C.3 and Tab. 8). Lastly, we present comprehensive task-level results in App. C.4."}, {"title": "5.1 Cross Task Merging", "content": "Merging NLP Models. For the NLP domain, we adhere to the experimental setting from [89]. We\nemploy the T5-base and T5-large [59] models and fine-tune both on seven tasks. This setting considers\na variety of NLP domains such as question answering, paraphrase identification, sentence completion,\nand coreference resolution (dataset details in App. D). Tab. 2 shows that using PCB-MERGING\nto merge fully fine-tuned T5-base and T5-large models leads to an average improvement of 4.3%\nand 3.5% over 7 tasks, without extra data. With validation datasets, PCB-MERGING improves by\n1.8% and 1.8% over other methods for T5-base and T5-large, respectively. Notably, PCB-MERGING\nwithout validation outperforms TIES-merging [89] by 5.4% for T5-large. For more detailed results,\nrefer to App. Tab. 9 and 10.\nMerging PEFT Model Adapters. Following the work of [89], we consider merging parameters\nused for efficient fine-tuning calculations and employ the (IA)\u00b3 [42] method for experimentation.\nThis approach, a form of Parameter-Efficient Fine-Tuning (PEFT), extends the activations of base\nmodels with learned vectors. We select T0-3B [66] as the base model and fine-tune (IA)\u00b3 models\non the training sets of eleven datasets, including sentence completion, natural language inference,\ncoreference resolution, and word sense disambiguation (dataset details in App. D). During fine-tuning\nof the T0-3B model, we utilize prompt templates from the Public Prompt Pool (P3 [4]) to convert"}, {"title": "Out of Domain Gegeralization", "content": "Following the experimental setup of [89], we also examined the\nability of cross-task merged models to better generalize across different domains. We merged the T5-\nbase and T5-large models using the same approach as in the previous experiments, combining them on\nseven in-domain datasets. Subsequently, we evaluated their performance on six held-out datasets from\nthe TO mixture [66] to assess out-of-domain generalization. These out-of-domain datasets encompass\nvarious tasks, including question answering, word sense disambiguation, and sentence completion\n(details in App. D). Both in-domain and out-of-domain performance are presented together in Fig. 4.\nThe results show that PCB-MERGING outperforms the strongest baseline for both T5-base and\nT5-Large models by 1.9% and 2.1%, respectively, indicating superior out-of-domain generalization.\nFor more detailed results, please refer to App. Tab. 14."}, {"title": "5.2 Cross Domain Merging", "content": "We conducted further experiments to compare the performance of different methods in merging\nfive distinct domain-specific models for emotion classification. Following the methodology of Jin\net al. [30], we employed the Roberta-base and T5-base models and utilized a set of preprocessed\ndatasets from Ober et al. [53]. For training individual models, we selected five high-resource datasets,\nwhile five low-resource datasets were chosen for evaluating out-of-domain generalization ability.\nOur analysis reports the average accuracy of in-domain datasets and the average accuracy of out-of-\ndomain datasets using various model merging techniques. In addition, we conducted the experiment\nwith different random seeds and reported the average results across five seeds. Fig. 5 provides a\nsummarized overview of these results. Our findings indicate that PCB-MERGING outperforms the\nstrongest baseline by 1.1% for Roberta-base and 1.3% for T5-base, while improving generalization\nacross domain shifts by 0.8% and 0.7%, respectively. Further details regarding the datasets can be\nfound in App. D and Tab. 16, and additional results are provided in App. C.4 and Tab. 15."}, {"title": "5.3 Cross Training Configurations Merging", "content": "In this experiment, our main focus was to\ncompare the ability of methods to merge\nmultiple checkpoints of the same task.\nThese checkpoints were generated by em-\nploying different training configurations dur-\ning fine-tuning, which included variations\nin hyperparameters, augmentation strate-\ngies, and dataset partitioning. Following\nthe setup of model soups [86], we fine-\ntuned ROBERT-base [44] models on four"}, {"title": "6 Analysis", "content": ""}, {"title": "6.1 Ablation of PCB-MERGING Components", "content": "We conducted ablation experiments on various components Table 5: Ablation study on individ-\nof our approach to assess their importance. Tab. 5 compares ual components of PCB-MERGING.\nthe performance of our method with different components re-\nmoved, testing ViT-B/32 and T5-base models on the validation\nset. Removing the Rescale step implies using a uniform scale\n$\\lambda$ = 1 and computing a disjoint mean as in TIES-Merging [89],\nignoring zero values. The table demonstrates the crucial im-\nportance of all components for achieving optimal performance.\nSpecifically, the Drop component was found to be the most\ncritical, resulting in performance drops of 5.1% for ViT-B/32\nand 4.9% for T5-base, respectively. More ablation study details\nare provided in App. B.1 and Tab. 6."}, {"title": "6.2 Effect of Hyper-Parameters on the Performance", "content": "We examined the impact of hyper-parameters $\\lambda$ and r on the performance when merging multiple NLP\ntasks, as discussed in Section 5.1. Initially, we illustrate the performance of various models across\ndifferent values of $\\lambda$ while keeping r = 0.1. Our method is compared against the state-of-the-art\nbaseline method, TIES-Merging. From Fig. 6, We can observe that our approach demonstrates a\nhigher performance ceiling within the suitable range of 1.4 to 1.8. As $\\lambda$ increases, the performance\ninitially decreases and then saturates. Additionally, we provide a performance analysis for different\nratios r. We conduct a grid search for $\\lambda$ to determine its optimal performance for each ratio. Notably,\nfor r < 0.3, our method consistently showcases significant improvements. This underscores the\nimportance of the information filtered out by our parameter competition balancing approach in the\nmerging process. More analysis about hyper-parameters are shown in App. B.2 and Fig. 7."}, {"title": "6.3 Limitation and Future Work", "content": "While our approach provides valuable insights into model merging, several limitations should be\nnoted: (1) PCB-MERGING, like previous methods, relies on identical model architectures and shared\ninitializations, constraining its applicability across various model types. (2) Limited theoretical\nunderstanding: model merging effectiveness may be influenced by task independence [34] and weight\ndisentanglement [55, 54], warranting further exploration. (3) Our approach does not effectively\naddress parameter redundancy, still relying on drop operations to mitigate interference and improve"}, {"title": "7 Conclusions", "content": "In summary, we introduce PCB-MERGING to tackle challenges in model merging by incorporating\nparameter competition balancing to rescale task vectors at the parameter level. Our method enhances\nmodel merging performance without requiring additional training, leading to improved stability\nand effectiveness across various scenarios. We demonstrate significant advancements in cross-task\nmerging, cross-domain merging, different training configurations, and out-of-domain generalization,\nhighlighting its potential impact in practical applications."}, {"title": "Appendix for PCB-Merging", "content": ""}, {"title": "A Novelty and Contribution", "content": "Our research aims to unlock the full potential of task vector-based approaches by adjusting coefficients\nat the parameter level through a balancing mechanism that addresses parameter competition across\ndifferent tasks. We re-examine existing model merging methods and highlight the critical role of\nparameter competition awareness. To clearly demonstrate the innovation of our method, we conduct\na comparative analysis with existing state-of-the-art baseline methods.\nComparison with TIES-Merging Both the TIES-Merging [89] and our approach address parameter\ncompetition or interference through self-awareness and cross-awareness. However, there are several\nkey differences:\n1. When performing Drop | Trim to reduce redundancy, we consider both intra-competition\nand inter-competition, whereas TIES-Merging primarily considers parameter magnitude.\n2. In terms of cross-awareness, TIES-Merging only considers the direction of parameters\nacross different tasks, neglecting parameter weights. Our method more accurately measures\nthe similarity of task vectors to assess conflict levels. We conducted ablation experiments to\ndemonstrate the effectiveness of inter-balancing, as shown in App. B.1 and Tab. 6.\n3. Our approach modulates the coefficient of each parameter, while TIES-Merging uses a\nuniform scale for all tasks and parameters. Ablation experiments in the Analysis section\nvalidate the superiority of our method, as shown in Section 6.1 and Tab. 5.\nComparison with AdaMerging Although AdaMerging [90] has achieved significant performance\nimprovements in image classification, it has several drawbacks:\n1. This method requires unsupervised test samples, which is often impractical.\n2. The use of Shannon entropy to train the adaptive weights limits the method to classification\ntasks.\n3. AdaMerging requires unsupervised training with the availability of (unlabeled) test samples,\nwhich is a different setup than generalizing to an entirely unseen test set.\nIn contrast, our proposed PCB-Merging retains the efficiency and lightwight nature as most previous\nmerging methods. Additionally, we conducted experiments on image classification tasks to compare\nthe two methods, as shown in App. C.2 and Tab. 7.\nComparison with Fisher Merging and RegMean The same as Fisher Merging [46] and Reg-\nMean [30], our PCB-Merging method also introduces additional matrices to adjust parameter coeffi-\ncients, but there are two key differences:\n1. Fisher Merging and RegMean consider only self-awareness or cross-awareness, respectively.\nIn contrast, our method accounts for various scenarios of parameter competition.\n2. Both Fisher Merging and RegMean require additional gradient-based computations to obtain\nthe Fisher Information Matrix or Inner Product Matrix, which demand more GPU resources.\nOur method, however, is based on task vectors, making it easier and lightwight to implement.\nComparison with DARE Both DARE [94] and PCB-Merging drop and rescale task vectors for\nmodel merging, but there are significant differences:\n1. DARE randomly drops parameters according to a drop rate p, while we consider parameter\ncompetition.\n2. DARE rescales the remaining parameters by a uniform factor of 1/(1 \u2013 p), whereas we\ncompute a specific coefficient for each task and each parameter.\n3. DARE is mainly used in LLM model merging to maintain the original fine-tuned perfor-\nmance. In contrast, we find that dropping parameters can further enhance performance\nbeyond the fine-tuned model with a suitable scale and intra-balancing."}, {"title": "Comparison with Lorahub", "content": "Lorahub [25] aims\nto establish a strategic framework for composing\nLORA modules trained on diverse tasks to achieve\nadaptable performance on new tasks. This frame-\nwork utilizes an evolution algorithm (CMA-ES\n[21]) to search for the coefficients of each LoRA\nmodule, as introduced in Section 3.3. However,\nthis search-based approach is time-consuming and\ncan only be applied at the task level, leading\nto limited performance. Moreover, LoRA lacks\nself-awareness and considers only competition\nbetween different tasks.\nComparison with Task Arithmetic and PEM\nCompositon Both Task Arithmetic [28] and\nPEM Composition [96] methods primarily focus\non exploring potential applications of task vectors,\nincluding distribution generalization, unlearning,\nand domain transfer. However, they do not ad-\ndress parameter competition or balance the coef-\nficients of different tasks or parameters, which\nlimits their performance."}, {"title": "B Additional Analysis", "content": ""}, {"title": "B.1 Additional Ablation Studies", "content": "We present additional ablation experiments on PCB-MERGING, as shown in Tab. 6. In addition to the\nfour main steps discussed in Section 6.1 (Intra-Balancing, Inter-Balancing, Drop, and Rescale), we\nalso tested other influencing factors:\n1. Activation functions: We replaced the softmax activation function with common alternatives\nlike sigmoid, ReLU, and tanh. The results show minimal performance loss with different\nactivation functions, except for ReLU in intra-balancing. This is because these activation\nfunctions can represent complex nonlinear relationships to balance the values of parameters.\n2. Without regulator N: We removed the regulator N in intra-balancing, which controls intra-\ncompetition according to the number of models being merged.\n3. Inter-balancing with only sign: We computed inter-balancing using only the sign (-1, 1)\ninstead of the actual values, where the sign represents a direction in the D-dimensional\nparameter space relative to initialization. This experiment aims to compare with TIES-\nMerging, which addresses sign conflicts.\n4. Element-wise multiplication vs. Addition: We combined intra-balancing and inter-balancing\nusing addition instead of multiplication. This resulted in a performance loss of 4.1% and\n3.9% on the ViT-B/32 and T5-base models, respectively.\nIn summary, these ablation experiments demonstrate the functionality and impact of each component\nin our method."}, {"title": "B.2 Additional Hyper-parameters Analysis", "content": "In this section, we present additional experimental results regarding hyper-parameters, observing\nsimilar phenomena and conclusions as those in Section 6.2. We explored the effects of $\\lambda$ and r on"}, {"title": "C Additional Results", "content": ""}, {"title": "C.1 Merging Different Number of Tasks", "content": "We evaluated the performance of the merged\nmodel on in-domain tasks and analyzed how it\nvaries with the number of tasks being merged.\nIn Fig. 8, we normalized each task's accuracy to\nits fine-tuned model's performance and reported\nthe average normalized accuracy for in-domain\ntasks with T5-base model. We compared our\nmethod against the strongest baseline, TIES-\nMerging [89], and simple averaging [86]. Each\ndata point represents the merging of a subset\nof tasks, with the solid line indicating the aver-\nage performance across multiple subsets. We\nobserved that as the number of merged tasks\nincreases, the performance of all methods de-\nclines, suggesting that more tasks lead to increased parameter competition. Additionally, TIES-\nMerging's performance drops faster than PCB-Merging, indicating that our PCB-Merging method is\nmore effective in balancing parameter competition."}, {"title": "C.2 Compare with Adamerging", "content": "We conducted cross-task merging experiments\non image classification tasks to compare our\nmethod with AdaMerging [90]. AdaMerging\nemploys unsupervised training to learn merging\ncoefficients for each task vector in Task Arith-\nmetic using unlabeled test datasets. Addition-\nally, Layer-wise AdaMerging learns coefficients\nfor each layer of each task vector.\nAdaMerging can be further improved by apply-"}, {"title": "C.3 Compare with TIES-Merging using Evolutionary Strategy", "content": "To validate the effectiveness of the evolutionary strategy (ES) proposed in Section 3.3, we applied ES\nto intelligently search for coefficients of different tasks in other baseline methods. The results are\nshown in Tab. 8. Notably, after applying ES, TIES-Merging showed significant improvement. We\nalso compared TIES-Merging with ES against our approach with ES. The results demonstrate the\neffectiveness of PCB-MERGING, particularly with a 2.2% performance gain on the T5-large model."}, {"title": "C.4 Comprehensive Task-Level Results", "content": "We provide the task level for all the cross-task merging experiments in the main Tab. 2.\nTab. 9, 10, 11, 12, and 13 provide the task level results T5-Base, T5-Large [59], IA3 [42], ViT-\nB/32, and ViT-L/14 [12] respectively. The task level results of the out-of-domain experiments for\nT5-Base and T5-Large can be found in Tab. 14."}, {"title": "D Dataset details", "content": "This section provides a detailed dataset description.\nMerging NLP Tasks Following TIES-Merging [89", "NLP\nmodels": "question answering (QASC [32", "91": "and QuaRTz [78", "97": "sentence completion (Story Cloze [70", "65": "nand WSC [37", "89": "we use eleven datasets including sentence\ncompletion (COPA [61"}]}