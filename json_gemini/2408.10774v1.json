{"title": "Flexora: Flexible Low Rank Adaptation for Large Language Models", "authors": ["Chenxing Wei", "Yao Shu", "Ying Tiffany He", "Fei Richard Yu"], "abstract": "Large Language Models (LLMs) are driving advancements in artificial intelligence by increasing the scale of model parameters, which has significantly enhanced generalization ability and unlocked new capabilities in practice. However, their performance in specific downstream tasks is usually hindered by their knowledge boundaries on these tasks. Thus, fine-tuning techniques, especially the widely used Low-Rank Adaptation (LORA) method, have been introduced to expand the boundaries on these tasks, whereas LoRA would underperform on certain tasks owing to its potential overfitting on these tasks. To overcome this overfitting and improve the performance of LoRA, we propose the flexible low rank adaptation (Flexora) method to automatically and flexibly select the most important layers needing to be fine-tuned to achieve the best performance on different downstream tasks. Specifically, Flexora firstly frames this layer selection problem as a well-defined hyperparameter optimization (HPO) problem, then addresses it using the unrolled differentiation (UD) method, and finally selects the most useful layers based on the optimized hyperparameters. Our extensive experiments on many pre-trained models and natural language tasks show that Flexora is able to consistently improve over the existing baselines, indicating the effectiveness of our Flexora in practice. We additionally provide insightful theoretical results and many ablation studies to deliver a comprehensive understanding of our Flexora.", "sections": [{"title": "Introduction", "content": "The emergence of large language models (LLMs) (Zhao et al. 2023; Xu et al. 2023) has introduced significant computational demands (Touvron et al. 2023), characterized by extensive parameter sets and robust functionalities (Wei et al. 2022), necessitating substantial training resources. Consequently, parameter-efficient fine-tuning (PEFT) methods (Li and Liang 2021; Lester, Al-Rfou, and Constant 2021) have become increasingly popular. Among these, low-rank adaptation (LoRA) (Hu et al. 2021) is especially noteworthy. LoRA freezes pre-trained parameters and introduces auxiliary trainable parameters $AW$ at each layer, markedly reducing training costs while achieving impressive results. However, fine-tuning LLMs using LoRA still encounters challenges such as an excessive number of fine-tuning parameters and a tendency toward model overfitting, leading to suboptimal performance. Recently, researchers have proposed various methods to enhance LoRA. For example, AdaLoRA (Zhang et al. 2023b) employs singular value decomposition (SVD) to prune less critical singular values, while DoRA (Mao et al. 2024a) dynamically reduces high-rank LoRA layers into structured single-rank components. Additionally, techniques such as dropout (Lin et al. 2024) and novel regularization strategy (Mao et al. 2024b) have been proposed to address overfitting. Despite these improvements, several limitations persist. Performance is often comparable to or lower than that of LoRA, and these methods do not automatically tune hyperparameters to achieve optimality, nor do they offer flexibility. Therefore, there is a pressing need for an algorithm that delivers good performance, enables automatic selection, and supports flexible training, which we have developed.\nHyperparameter optimization (HPO) is a fundamental problem in the field of machine learning (Liu, Simonyan, and Yang 2019a; Franceschi et al. 2018; Ren et al. 2019; Guo et al. 2020). The goal of HPO is to identify hyperparameter-hypothesis pairs that optimize the expected risk of test samples drawn from an unknown distribution. This involves solving two nested optimization problems: the inner problem focuses on optimizing the hypothesis on the training set for a specified hyperparameter configuration, while the outer problem aims to find hyperparameters that minimize the validation error. In recent years, unfolded differentiation (UD) algorithms have emerged as the mainstream solution for HPO due to their potential to adjust a large number of hyperparameters (Franceschi et al. 2017; Fu et al. 2016; Maclaurin, Duvenaud, and Adams 2015; Shaban et al. 2019). Inspired by HPO, we consider each layer of an LLM as a hypothesis, facilitating a layer-wise analysis of LLMs. Driven by the desire for a deeper understanding of LLMs, we have undertaken this work.\nIn this paper, we introduce a novel method, Flexora, designed to address the overfitting in LoRA by automatically and flexibly selecting the most critical layers for fine-tuning, as illustrated in Figure 1. To achieve this, we frame the layer selection problem as an HPO task and employ the UD method to solve it. More specifically, during the initialization stage, Flexora injects the defined hyperparameters into the LoRA parameters, producing a PEFT model ready for training. In the flexible layer selection stage, described in Sec. 4.1, the UD method optimizes this hyperparameter problem. Finally, in the fine-tuning stage, detailed in Sec. 4.2, Flexora uses the optimized hyperparameters to identify the layers that contribute most to downstream tasks for targeted fine-tuning. By restricting backpropagation and parameter updates to these selected layers, Flexora significantly reduces computational overhead. Empirical results demonstrate that Flexora effectively reduces unimportant LoRA parameters, suppresses model overfitting, and enhances performance. In summary, our contributions are as follows:\n\u2022 We introduce Flexora to improve LoRA by automatically and flexibly selecting the most important layers for LORA fine-tuning.\n\u2022 We frame the layer selection problem as an HPO task and solve it effectively and efficiently using the UD method.\n\u2022 We validate the effectiveness of our Flexora through extensive experiments on various LLMs and tasks, showing significant improvements over existing LoRA variants.\n\u2022 We provide theoretical explanations for our experimental results, offering insights into the mechanisms behind the performance improvements achieved by Flexora."}, {"title": "Related Work", "content": "In recent years, low-rank adaptation (LoRA) methods have gained widespread adoption to reduce the number of training parameters while fine-tuning large language models (LLMs) for specific applications. However, LoRA frequently suffers from overfitting, which results in poor performance on downstream tasks. To address this issue, researchers have developed various methods to alleviate overfitting by reducing training parameters. Wu et al. (2024) found that training all LoRA parameters may lead to overfitting, and proposed LoRA-SP, which randomly freezes half of the LoRA parameters during fine-tuning, thus effectively alleviating overfitting. Zhang et al. (2023a) noted that the down-projection weights in LoRA might lead to overfitting, and introduced LoRA-FA, which freezes the down-projection weights and only updates the up-projection weights, thereby enhancing the performance. Zhou et al. (2024) proposed LoRA-drop, which prunes less important parameters based on layer output analysis, thus enhancing the generalization ability of the model. In contrast, Zhang et al. (2024) investigated the interaction between LoRA parameters and the original LLM and proposed LORAPrune. This method jointly prunes parts of the LoRA matrix and parts of the LLM parameters based on the gradient of LoRA to further reduce overfitting. In addition, LoRAShear (Chen et al. 2023) introduces a knowledge-based structured pruning method that reduces the computational cost while retaining essential knowledge, thus reducing overfitting and enhancing generalization performance.\nUnfortunately, these methods usually (a) require significant manpower in an algorithm or configuration design, (b) struggle to yield adaptive policies across different downstream tasks, and (c) are often too complex (i.e., parameter-level policies) to be applied in practice. In contrast, in this paper, we introduce Flexora, a framework that aims to flexibly fine-tune LoRA across various downstream tasks using an automated yet simple layer-level policy."}, {"title": "Hyperparameter Optimization (HPO)", "content": "HPO is widely applied across various domains. Specifically, in the domain of neural architecture search, Liu, Simonyan, and Yang (2019b) conceptualizes the coefficients defining the network architecture as hyperparameters. In the domains of feature learning, Guo et al. (2020) considers feature extractors as hyperparameters. In the field of data science, (Olson et al. 2016) employs hyperparameters as weights to measure the importance of data. By minimizing the validation loss over these hyperparameters, the optimal variables, e.g., the architectures in (Liu, Simonyan, and Yang 2019b), the features in (Guo et al. 2020), and the data in (Olson et al. 2016), are identified, leading to superior performance in their respective domains.\nDrawing inspiration from these works, we initially formulated the layer selection in the LoRA method as an HPO problem. This involves optimizing hyperparameters to quantify the contributions of different layers, aiming to achieve optimal performance on downstream tasks and thereby select the most crucial layers for fine-tuning. This formulation subsequently led to the development of our algorithm (Flexora)."}, {"title": "Preliminaries", "content": "In this section, we first provide empirical insights showing that layer selection is crucial for improved performance of LLMs in Sec. 3.1, and then frame the layer selection problem as a well-defined HPO problem in Sec. 3.2."}, {"title": "Empirical Insights", "content": "To investigate the relationship between the number of fine-tuning layers and overall performance, we conducted a preliminary study on Llama3-8B. The results, presented in Figure 2, indicate that while increasing the number of fine-tuning layers generally enhances model performance, there exists a critical threshold beyond which additional fine-tuning leads to overfitting and a subsequent decline in performance. Therefore, fine-tuning a subset of LoRA layers emerges as a natural strategy to minimize overfitting and enhance LoRA performance, aligning with numerous empirical findings. For instance, Zhu et al. (2023) employed a randomized and structured pruning approach on the original LLM pre-trained parameters and LoRA parameters of layers, training only the remaining subset of LoRA, and achieved favorable outcomes. Similarly, Zhou et al. (2024) analyzed the output values of LoRA elements across different layers and utilized output value pruning to reduce overfitting, thus improving performance by training a selected subset of layers. Additionally, Chen et al. (2023) highlighted that different layers encapsulate different types of knowledge, and pruning irrelevant layers can significantly enhance model performance on downstream tasks. In summary, fine-tuning a subset of LoRA layers offers several advantages: fewer parameters, reduced memory consumption, shorter training times, diminished overfitting, and ultimately, enhanced model performance."}, {"title": "Problem Formulation", "content": "Inspired by the insights above, we aim to identify the most critical layers in LLM fine-tuning with LoRA to further improve generalization performance on varying downstream tasks. Formally, given a N-layer LLM with LoRA fine-tuning parameter $\\theta$, let hyperparameter $\\alpha \\in \\{0,1\\}^N$ represent the selection results of LoRA fine-tuning layers with 1 indicating that a layer will be selected, and define the expected test error $R_{test}(\\theta, \\alpha) \\approx E_{x \\sim D_{test}}l(x, \\theta; \\alpha)$ and the expected training error $R_{train}(\\theta, \\alpha) = E_{x \\sim D_{train}} l(x, \\theta; \\alpha)$. To select the most important LoRA fine-tuning layers in this LLM for its optimal fine-tuning performance on downstream tasks, we need to solve the following bilevel optimization problem:\n$\\min_{\\alpha \\in \\{0,1\\}^{N}} R_{test} (\\theta^*(\\alpha), \\alpha) \\text{ s.t. } \\theta^*(\\alpha) = \\arg \\min_{\\theta \\in \\Theta} R_{train}(\\theta, \\alpha) \\text{. }\\tag{1}$\nOf note, this formulation serves as a standard form of HPO as demonstrated in Bao et al. (2021), where $\\alpha$ is the hyperparameter. Consequently, we have framed the layer selection problem in LLM fine-tuning with LoRA as a well-defined HPO problem.\nUnfortunately, it is typically infeasible to obtain the training (i.e., $D_{train}$) and test (i.e., $D_{test}$) distributions for this optimization. As a result, we commonly use the training dataset $S_{train}$ and validation dataset $S_{val}$ to approximate the training and test distributions respectively. Define the empirical validation error $R_{val}(\\theta, \\alpha) \\triangleq E_{x \\sim S_{val}} l(x, \\theta; \\alpha)$ and the empirical training error $R_{train}(\\theta, \\alpha) \\triangleq E_{x \\sim S_{train}} l(x, \\theta; \\alpha)$, we aim to solve the following HPO problem:\n$\\min_{\\alpha \\in \\{0,1\\}^{N}} R_{val} (\\theta^*(\\alpha), \\alpha) \\text{ s.t. } \\theta^*(\\alpha) = \\arg \\min_{\\theta \\in \\Theta} R_{train}(\\theta, \\alpha) \\text{. }\\tag{2}$"}, {"title": "The Flexora Framework", "content": "This section outlines the methodology behind our proposed Flexora framework. As shown in Figure 1, our Flexora framework consists of two major stages: a flexible layer selection stage detailed in Sec. 4.1 and a fine-tuning stage of the selected LoRA layers detailed in Sec. 4.2."}, {"title": "Flexible Layer Selection Stage", "content": "Due to the difficulty of optimizing the discrete layer selection hyperparameter $\\alpha = (\\alpha_1,...,\\alpha_N)$ directly, we relax it to the continuous counterpart $\\hat{\\alpha} = (\\hat{\\alpha}_1,...,\\hat{\\alpha}_N)$, with each $\\hat{\\alpha}_i$ defined as\n$\\hat{\\alpha}_i = \\frac{\\exp(\\alpha_i)}{\\sum_{i \\in [N]} \\exp(\\alpha_i)} N.\\tag{3}$\nwhere $\\alpha$ is initialized with zeros, and the constant scale $N$ is applied to ensure that the initialized $\\hat{\\alpha}$ is equivalent to the selection of all LoRA layers. The specific computation involving this hyperparameter is as follows:\n$h_i = W_o z_i + \\alpha_i B_i A_i z_i.\\tag{4}$\nTherefore, if $\\alpha_i$ is close to zero, it degenerates to the standard computation in the original LLM, indicating that LoRA is not required by this layer. Meanwhile, if $\\alpha_i$ is larger, intuitively, this layer requires LoRA more significantly for better performance.\nTo find the most important layers for fine-tuning LLMs on downstream tasks, we need to solve the HPO problem defined by Equation 2 in Section 3.2. To solve it, we will employ the UD algorithm. Specifically, we define $\\arg \\min_{\\theta \\in \\Theta} R_{train}(\\theta, \\alpha)$ as the inner layer optimization, where the LORA parameter $\\theta$ is updated. Similarly, we define $\\arg \\min_{\\alpha \\in \\{0,1\\}^N} R_{val}(\\theta^* (\\alpha), \\alpha)$ as the outer layer optimization, where the hyperparameter $\\alpha$ is updated based on the results of the inner layer optimization.\nAccording to the aforementioned explanation, our primary optimization target is the hyperparameter $\\alpha$. In practice, we use Equation 3 to convert the hyperparameter $\\alpha$ into $\\hat{\\alpha}$, which guides the layer selection at each step. As illustrated in Figure 1a, we initially set $\\alpha$ to zeros and inject the hyperparameter into all LoRA parameters. Next, as illustrated in Figure 1b, we conduct inner layer training optimization for all layers and retain all computational graphs for subsequent outer layer updates. It is crucial to note that the computational graph is differentiable with respect to the hyperparameter $\\alpha$. Subsequently, we use the updated parameters $\\theta$ and computational graphs to conduct outer layer training K times. Each outer layer training iteration randomly selects batch data from the validation dataset for optimization. This process is formally described by Alg. 1, under the assumption that $\\alpha$ and $\\theta$ are independent of $S_{val}$ and $S_{train}$. When fine-tuning large language models (LLMs), the substantial sizes of $S_{val}$ and $S_{train}$ make the gradient computation of Alg. 1 computationally infeasible, necessitating the use of stochastic gradient descent (SGD) for hyperparameter updates with a single outer layer update as follows:\n$\\alpha_{k+1} \\leftarrow \\alpha_k - \\eta_a \\nabla_{\\alpha} R_{val} (\\theta_t(\\alpha), \\alpha)|_{\\alpha = \\hat{\\alpha}_r, data=\\{d_j\\}}\\tag{5}$\nwhere $d_j$ is randomly selected from $S_{val}$. In subsequent epochs, we first apply Equation 4 to update the LoRA parameters of different layers based on the latest hyperparameters $\\alpha$. Then, we continue the optimization starting from the updated LoRA parameters. After minimizing the validation metric $R_{val}$, as shown in Equation 2, we will end the training process and obtain the optimal configuration of hyperparameters.\nBased on our formulation, we have the following proposition:\nFor any $T > 0$ and $K \\ge 0$ in our Alg. 1, the following condition holds if $\\alpha$ is initialized with zeros,\n$\\sum_{i=1}^{N} \\alpha_i = 0$.\nBased on the optimized $\\alpha^*$, we automatically select the layers with $\\alpha_i > 0$ which has $\\hat{\\alpha}_i > \\frac{1}{N}$ to obtain the best performance. When resources are limited, we can flexibly choose the number of layers to fine-tune, optimizing performance within these constraints. Consequently, we only need to select the $x$ layers with the largest $\\hat{\\alpha}_i$. This approach allows for both automatic and flexible selection of the number and specific layers for tuning."}, {"title": "Fine-Tuning Stage", "content": "Since the LORA parameters of all layers are updated alongside the hyperparameters during the flexible layer selection stage, direct deletion of LORA parameters from any layer is not feasible, necessitating a dedicated fine-tuning stage. During the fine-tuning stage, as illustrated in Figure 1c, we freeze the unselected layers and retrain the LoRA parameters from scratch. This selective activation strategy ensures that the model focuses on fine-tuning the most critical layers, thus minimizing overfitting and optimizing performance for specific downstream tasks."}, {"title": "Empirical Results", "content": "In this section, we present a comprehensive set of experiments to assess the effectiveness and efficiency of our proposed Flexora method. We first describe the datasets and experimental setup. Next, we detail the main results and ablation studies."}, {"title": "Datasets and Setup", "content": "To evaluate the performance of our proposed Flexora method, we utilize several benchmark datasets, including Winogrande, RACE, PIQA, and Hellaswag, which collectively offer a comprehensive evaluation framework for various reasoning and comprehension tasks, with accuracy as the evaluation metric. Winogrande (Sakaguchi et al. 2019) evaluates commonsense reasoning using 44,000 questions, RACE (Lai et al. 2017) focuses on reading comprehension using nearly 100,000 questions, PIQA (Bisk et al. 2019) evaluates physics commonsense reasoning using more than 16,000 question-answer pairs, and Hellaswag (Zellers et al. 2019) tests commonsense natural language reasoning using 70,000 questions. In our experimental setup, we included 11 mainstream large-scale language models (LLMs), such as Llama3-8B (Meta 2024), Chatglm3-6B (GLM et al. 2024), Mistral-7B-v0.1 (Jiang et al. 2023), Gemma-7B (Team et al. 2024), and others. Flexora was developed using the Llama-factory (Zheng et al. 2024) framework and evaluated using the opencompass (Contributors 2023) framework. Detailed descriptions of data processing and experimental configurations can be found in the Appendix B. All experiments were performed on a single NVIDIA A100 GPU."}, {"title": "Main Results", "content": "In the main experiments, we assess the performance improvement and training efficiency of Flexora on Llama3-8B, with results presented in Table 1 (accuracy) and Table 2 (time and parameters). Baselines include pre-trained models, Full FT, LoRA, AdaLoRA, LoRA-drop, LORAShear, and others. In particular, the experimental results with LoRAShear are detailed in Appendix C.3. The results indicate that Flexora outperforms all baselines. Compared to methods without LoRA, Flexora fine-tunes fewer parameters, thereby reducing overfitting and enhancing performance. When compared to methods that enhance LoRA, Flexora optimizes by considering the relationship between pre-trained parameters of each LLM layer and downstream tasks, achieving superior performance. Loss Detailed in Appendix C.6 Additionally, Flexora surpasses LoRA-drop, which always selects final layers, by identifying the most critical layers. Flexora also demonstrates strong generalization and scalability across different LLMs. Almost all LLMs can use Flexora to significantly improve performance with fewer fine-tuning parameters, as shown in Figure 3 and Appendix C.1"}, {"title": "Ablation Studies", "content": "In the first ablation experiment, we maintained the number of layers selected by Flexora unchanged but chose different layers for fine-tuning. The experimental results are shown in Table 3. The result underscores two key points: First, Flexora can precisely determine the number of layers for fine-tuning. Even when the specific fine-tuning layers are chosen at random, the results continue to outperform LoRA. The theoretical explanation for this result can be found in Sec. 6. Second, Flexora can automatically and flexibly select the specific layers for fine-tuning, targeting the most important layers to maximize performance and generalization. Detailed in Appendix C.5\nIn the second ablation experiment, we manually determined the number of fine-tuning layers and contrasted Flexora with random selection. The results in Table 4 underscore the flexibility of Flexora, demonstrating that it can achieve optimal performance irrespective of the number of fine-tuning layers. The specific layers selected are shown in Table 12. An notable observation is that Flexora often selects the initial and final layers. An intuitive explanation is that the initial and final layers of a model have a significant impact on the data. The initial layer directly transforms the input into the feature space, while the final layer predicts the data in the form of labels, rendering them crucial. Additionally, for the same downstream task, the input to the initial layer remains consistent and closely aligned with the task, and the output of the final layer is also consistent. Focusing on optimizing these layers can enhance learning efficiency. This conclusion is corroborated by other studies. Chen et al. (2023) observed that the knowledge distribution in LLMs is primarily concentrated in the initial and final layers. Sharma, Ash, and Misra (2023) demonstrated that the loss gradient of the initial and final layers is steep, which benefits the model during training. Pan et al. (2024) found that the weight norm of the initial and final layers is hundreds of times greater than that of the intermediate layers, indicative of their heightened importance."}, {"title": "Theoretical Insights", "content": "In this section, we provide theoretical explanations for why Flexora (using only a subset of LoRA layers) can achieve excellent results. We first introduce Theorem 1 below, and then simplify LoRA layers as linear layers in multi-layer perceptron (MLP) to derive our Proposition 2, aiming to offer insights for this question.\nAssume that $f(\\cdot; z) \\in [0, 1]$ is an $L$-Lipschitz and $\\beta$-smooth loss function for every sample $z$. Suppose that we run SGM for $T$ steps with monotonically non-increasing step sizes $\\eta_t \\le c/t$ ($t \\in [T]$). In particular, omitting constant factors that depend on $\\beta$, $c$, and $L$, we have\n$R_{test} (\\theta, \\eta) \\le R_{train}(\\theta, \\eta) + \\frac{1}{\\sqrt{m}} \\eta^{1-1/(c+1)}\nTheorem 1 reveals that if all the conditions except for $\\beta$ in Theorem 1 remain the same, a smaller smoothness $\\beta$ will typically result in a smaller test error $R_{test} (\\theta, \\eta)$, indicating a better generalization performance in practice. To show how the number of LoRA layers is related to this $\\beta$, we then follow the practice in (Shu, Wang, and Cai 2020) to prove our Proposition 2 below.\nFor an $N$-layer linear multi-layer perceptron (MLP): $y^{(N)} \\triangleq \\Pi_{i=1}^N W^{(i)}x$ with MSE function $\\ell \\triangleq (y^{(N)} - y)^2/2$ where $y$ denotes the true label, let $\\lambda^{(i)} \\triangleq ||W^{(i)}||$ for any $i \\in [N]$, we then have.\n$\\frac{\\partial \\ell}{\\partial W^{(i)}} \\frac{\\partial \\ell}{\\partial W^{(i)}} < \\Pi_{j=1,j \\ne i}^{N} \\lambda^{(j)} ||x^2|| \\cdot ||(W_1^{(i)} - W_2^{(i)})|| $"}, {"title": "Conclusion and Future Work", "content": "In this study, we introduce Flexora, a method designed to enhance the efficiency and effectiveness of fine-tuning large language models (LLMs). Flexora automatically and flexibly selects the most critical layers for fine-tuning, addressing overfitting and enhancing performance by modeling layer selection as an HPO problem and employing UD to solve it. This targeted fine-tuning approach reduces computational overhead by updating only selected layers. Extensive experiments demonstrate Flexora reduces parameters, suppresses overfitting, and enhances performance, outperforming baseline methods. We also provide a theoretical explanation for these improvements.\nFuture work will focus on the following areas: (a) Exploring the relationship between individual layers of the LLM and their impact on downstream tasks to theoretically elucidate the role of each layer in reasoning processes. (b) Investigating optimal fine-tuning schemes for Flexora, inspired by its enhanced performance when integrated with various LoRA-enhanced algorithms, as demonstrated in Appendix C.2."}, {"title": "Experimental setting", "content": "In the main experiment, we compared Flexora with the baseline. The datasets and experimental parameters are as follows:\nWe added new templates to the original dataset to ensure the model could complete the required tasks and output formats. It is important to note that the added templates did not alter the original dataset, and special processing was performed for different LLMs. The specific examples are as follows:"}, {"title": "More results", "content": "According to the parameter settings in Table 6, the verification results for various LLMs are presented in Table 7. The selected LLMs include Llama3-8B, Llama-7B, Llama2-7B, ChatGLM3-6B, Mistral-7B-v0.1, Gemma-7B, Zephyr-7B-beta, Vicuna-7B-v1.5, XuanYuan-6B, Qwen1.5-7B, and Yi-6B. These models demonstrate unique characteristics in terms of training data, architecture design, and optimized training. First, the models utilize varied training data, leading to differences in data distribution. Additionally, some models have enhanced attention mechanisms: Mistral-7B-v0.1 employs grouped query attention (GQA) and sliding window attention (SWA), while ChatGLM3-6B features a special attention design to support tool calling and code execution capabilities. Activation functions vary across these models. Llama3-8B uses the SwiGLU activation function, inspired by the PaLM model, to improve performance and convergence speed, while ChatGLM3-6B uses the Swish activation function. Furthermore, differences in reasoning optimization and multilingual capabilities contribute to varied reasoning abilities across fields. The experimental result of each model is shown in Table 7, which presents the scores of each model on different downstream tasks after LoRA and Flexora fine-tuning. It should be noted that all models fine-tuned using LoRA will have a certain degree of overfitting, while Flexora can effectively identify and analyze unnecessary layers in specific downstream tasks and prune them to reduce model overfitting. After optimization by Flexora, these LLMs showed significant performance improvements on downstream tasks. In particular, models that originally performed poorly on some tasks, such as ChatGLM3-6B, experienced significant improvements, achieving more than a 15% increase on the RACE-mid and RACE-high tasks. This improvement is attributable to the key layer selection by Flexora and efficient model learning. In summary, Flexora is applicable across Transformer models of various structures, excels in diverse tasks, and effectively enhances areas where model capabilities are lacking."}, {"title": "The results of other LoRAs experiment", "content": "Recently, as emphasized in the Introduction 1, numerous LoRA improvement methods have been proposed, achieving excellent performance in specific fine-tuning tasks. In this section, the potential for combining Flexora with other emerging LoRA algorithms is explored. Two promising LoRA variants were selected from different approaches, each demonstrating impressive performance. Specifically, DoRA (Weight-Decomposed Low-Rank Adaptation) achieves low-rank adaptation through weight decomposition, and rsLoRA (Rank-Stabilized LoRA) addresses the slow training speed of traditional LORA by introducing a rank-stabilized scaling factor when increasing rank. These methods primarily address the parameter overfitting problem within LoRA parameters but overlook the overall overfitting issue. These methods were innovatively combined with Flexora to first address the overall overfitting problem and then tackle the overfitting of the remaining LORA parameters, resulting in notable performance improvements. The specific experimental results are shown in Table 8. The results indicate that Flexora integrates well with both DoRA and rsLoRA, effectively mitigating the overfitting problem of LLMs and improving performance with training on less than half of the parameters. The specific implementation entails replacing LORA with DORA or rsLoRA for inner layer optimization during the flexible layer selection stage, with the outer layer optimization remaining unchanged. These adjustments are achievable through straightforward modifications. The results demonstrate that Flexora exhibits strong scalability when combined with algorithms that enhance LoRA parameters, highlighting its significant potential."}, {"title": "Comparison with LoRAShear", "content": "In this section, the accuracy of Flexora is compared with that of LoRAShear across various datasets, with specific results presented in Table 9. Since LoRAShear is not open source and poses challenges for direct experimentation, the comparison relies on the experimental configurations and results reported in the LoRAShear paper. Notably, Flexora can freely adjust the selected layers according to the dataset, achieving an average pruning parameter rate of 50%. Consequently, under the same pruning rate, Flexora outperforms by 14% (Ratio = 0.5).Experiments have shown that under the same pruning rate, Flexora can achieve better performance."}, {"title": "Different search sample", "content": "In Flexora, search time is managed by adjusting the maximum number of search samples (corresponding to the size of the validation dataset) to align with the requirements of the downstream task. In Table 10, we explore the relationship between different numbers of search samples, downstream task performance, and search time. For simpler datasets like Hellaswag and PIQA, a 10-minute search with 1,000 samples significantly improves performance. For more challenging tasks, at least 1 hour of search time is required for 5,000 samples. In more difficult tasks, using too few samples can prevent validation loss from converging. To optimize performance, it is recommended to dynamically adjust the number of search samples based on the convergence of the validation loss. In summary, for simpler downstream tasks, Flexora can be rapidly applied to reduce model overfitting significantly and enhance performance. For more challenging downstream tasks, Flexora balances performance and training resources by adjusting the number of search samples."}, {"title": "Selection of layers", "content": "For different LLMs and datasets, the layers chosen by Flexora vary due to the different parameters learned in the pre-training stage and the diversity of downstream tasks. In Table 11, Table 12, Table 13, Table 14, and Table 15, we show the layers chosen by Flexora in all experiments and the corresponding training parameters. In this section, the preferences of the layers chosen by Flexora are analyzed in detail, providing layer-wise insights for LLMs.\nA comparison between LoRA-drop and Flexora reveals that Flexora is more effective. LoRA-drop tends to select the later layers, as these outputs exhibit a larger two-norm, aligning with Proposition 2. This result suggests that layers selected during fine-tuning should not concentrate in a specific range but rather be distributed across various ranges, fully utilizing the extensive knowledge system of LLMs. Comparing LoRA with DoRA and rsLoRA shows that LoRA selects more layers, requiring more training parameters but yielding worse performance. This suggests a higher degree of overfitting when Flexora is applied to LoRA compared to the other two methods. Therefore, using more advanced LORA improvement algorithms can significantly reduce overfitting and enhance performance, underscoring the importance of the fine-tuning approach. Interestingly, certain layers are consistently fine-tuned in the same downstream task, regardless of whether LoRA, DoRA, or rsLoRA is used. For example, in Hellaswag, layers [0, 1, 2, 4, 14, 15, 19, 20, 21, 23, 26, 27, 28, 29, 31] are consistently selected, suggesting these layers are crucial for this task or represent general knowledge layers (see the next two paragraphs for details), closely related to the LLM itself ."}, {"title": "Loss", "content": "This section presents the training, evaluation, and validation loss during the Flexora flexible layer selection and fine-tuning stages, accompanied by intuitive explanations.\nFigures 6, 7, 8, and 9 depict the training and evaluation loss from ablation study 5.3. In all experiments, the training loss converges effectively, demonstrating robust training performance. However, variations in evaluation loss underscore the model's generalization capabilities. Flexora generally surpasses methods that randomly select an equivalent number of layers, demonstrating its ability to accurately identify critical layers for more effective improvements.\nFigures 10, 11, 12, and 13 present the training and evaluation loss from ablation study 5.3. Consistent with previous experiments, the training loss converges, indicating a strong training effect on the training set. Notably, the 24-layer (red) model consistently shows the lowest training loss, suggesting optimal learning, whereas the 6-layer (blue) model consistently records the highest, indicating poorer training performance. However, differences in evaluation loss reveal variations in model generalization across different layers. The 18-layer (green) model consistently exhibits the lowest evaluation loss, indicating superior generalization and downstream task performance, corroborated by actual results. The 24-layer (red) model's evaluation loss consistently exceeds that of the 18-layer (green) model, suggesting significant overfitting. Similarly, the 6-layer (blue) model consistently records the highest evaluation loss, indicative of underfitting.\nIn summary, too few training layers can lead to underfitting and poor performance, as seen in the 6-layer (blue) model. Conversely, too many layers can also result in overfitting, as evidenced by the 24-layer (red) model's performance. However following the selection strategy of Flexora, choosing the right number of layers can minimize overfitting and improve performance"}, {"title": "Special cases", "content": "This section"}]}