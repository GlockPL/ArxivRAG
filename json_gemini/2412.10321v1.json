{"title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks", "authors": ["Sicheng Zhu", "Brandon Amos", "Yuandong Tian", "Chuan Guo", "Ivan Evtimov"], "abstract": "Many jailbreak attacks on large language models (LLMs) rely on a common objective: making the model respond with the prefix \"Sure, here is (harmful request)\". While straightforward, this objective has two limitations: limited control over model behaviors, often resulting in incomplete or unrealistic responses, and a rigid format that hinders optimization. To address these limitations, we introduce AdvPrefix, a new prefix-forcing objective that enables more nuanced control over model behavior while being easy to optimize. Our objective leverages model-dependent prefixes, automatically selected based on two criteria: high prefilling attack success rates and low negative log-likelihood. It can further simplify optimization by using multiple prefixes for a single user request. AdvPrefix can integrate seamlessly into existing jailbreak attacks to improve their performance for free. For example, simply replacing GCG attack's target prefixes with ours on Llama-3 improves nuanced attack success rates from 14% to 80%, suggesting that current alignment struggles to generalize to unseen prefixes. Our work demonstrates the importance of jailbreak objectives in achieving nuanced jailbreaks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are becoming the central building block of modern AI (OpenAI, 2023; Dubey et al., 2024; Anthropic, 2024; Reid et al., 2024). As their capabilities continue to improve, concerns about AI safety also grow, since LLMs are known to replicate harmful behaviors from their training data (e.g., criminal activity, self-harm, and discrimination) (Vidgen et al., 2024). To mitigate these risks, developers employ model-level alignment (Ouyang et al., 2022; Bai et al., 2022; Dai et al., 2023) and system-level moderation (Inan et al., 2023; Zeng et al., 2024a), verified through proactive red-teaming. Red-teaming probes model behavior using adversarial prompts designed to circumvent these safety measures (known as jailbreaking). While jailbreaks traditionally rely on manual prompting by experts (Ganguli et al., 2022), automated jailbreaks using prompt optimization have emerged as a more scalable and effective approach, potentially revealing issues overlooked by human experts (Perez et al., 2022; Lapid et al., 2023).\nA major component of automated jailbreak attacks is the optimization objective. Many automated attacks (Zou et al., 2023; Liu et al., 2023a; Andriushchenko et al., 2024) share a common objective: maximizing the likelihood that the model responds with a manually crafted prefix (Figure 1). However, we identify two limitations with this objective: Misspecified: even with near-zero loss, the model's actual response often ends up incomplete or unfaithful to the user's intent (Figure 1), which rarely poses any real harm (Vidgen et al., 2024). When looking for violating responses, red-teamers are more interested in more meaningfully harmful responses, i.e., those that are complete and faithful (on-topic, detailed, and realistic). Geiping et al. (2024) also note this misspecification issue, while Liao and Sun (2024); Zhou and Wang (2024) observe that lower loss does not necessarily lead to higher attack success rates and attribute it to exposure bias (Bengio et al., 2015; Arora et al., 2022). Overconstrained: the handcrafted prefixes, with their rigid format, are often unnatural for the victim LLM. For example, Llama-3 typically starts its response with \"Here\" rather than \"Sure\". This"}, {"title": "2 Refined Evaluation for Nuanced Jailbreaks", "content": "In this section, we show that current jailbreak evaluations often overestimate attack success rates for nuanced jailbreaks by counting incomplete and unfaithful responses. We then refine the evaluation to improve accuracy.\nNuanced jailbreaks and failure cases. We start by categorizing jailbreak failure cases to reveal the limitations of current evaluations. For an attack to succeed in nuanced jailbreaks, the victim LLM must provide affirmative, complete, and faithful (i.e., on-topic, detailed, and realistic, see Vidgen et al. (2024)) responses to the harmful user request. In contrast, failed jailbreaks take different forms, which we categorize by rules (examples in Figure 2 and Table 6):"}, {"title": "3 Original Objective Misaligns with Nuanced Jailbreaks", "content": "This section reveals the limitations of the original objective via nuanced evaluation. We start by revisiting the original objective and comparing it with the oracle objective. Then, we show that the original objective is both misspecified and overconstrained, eventually failing to achieve nuanced jailbreaks."}, {"title": "3.1 Revisiting Oracle and Original Objectives.", "content": "We first formulate the jailbreak problem. We denote the LLM's vocabulary as V, and the set of all finite sequences over V as V*. Let x \u2208 V* represent a (malicious) user prompt. Similarly, we use y \u2208 V* to denote a model response. Unlike model finetuning, the threat model in jailbreaking can only steer the model's behavior (output distribution) by altering the attack (user) prompt 0 \u2208 V*. In practice, we can either optimize the"}, {"title": "4 The Objective for Nuanced Jailbreaks", "content": "This section introduces our new prefix-forcing objective for nuanced jailbreaks. Figure 5 outlines our objective, which uses one or more carefully selected prefixes. We begin by formulating our objective, followed by presenting two prefix selection criteria, and conclude with the empirical selection pipeline."}, {"title": "4.1 Selective Multi-Prefix Objective", "content": "For a given harmful request x, we select a set of target prefixes Yp. Then, our objective aims to find an attack prompt that maximizes the likelihood of the victim LLM generating any of these prefixes:\n$$min_{\\theta \\in V^*} - log \\sum_{y_p \\in Y_p} p(y_p | \\theta).$$\nThis objective takes the same sum-log-probability form as the oracle objective. When the prefix set Y\u0440 contains only one prefix, the objective degenerates to the single-prefix form.\nUsing multiple prefixes leverages the jailbreak task's flexibility to alleviate overconstraints and simplify optimization. Note that jailbreak tasks only require the victim LLM to generate harmful responses with high probability, without specifying the exact output distribution. For example, two victim LLMs, one always saying \"Here is a guide ...\" and the other always saying \"Here is a comprehensive guide ...\", can both be considered jailbroken. In this case, aiming to generate any of these prefixes reduces overconstraint. To minimize the extra computational cost from using multiple prefixes, we adopt the tree attention trick (Cai et al., 2024) to concatenate multiple prefixes into one to compute them in a single forward pass.\nWhy still prefix-forcing? A key challenge in designing jailbreak objectives is that defining jailbreak success relies on an autoregressive model's output distribution, which is hard to estimate especially when it has high entropy. One way to estimate it is by sampling many responses, but this makes computing the objective value inefficient. Another way is to predict future outputs from the model's current state, but current techniques can only predict a few tokens ahead (Pal et al., 2023; Gloeckle et al., 2024; Wu et al., 2024), while identifying nuanced harmful responses often requires examining hundreds. The prefix-forcing objective bypasses this challenge by specifying a low-entropy distribution that always outputs a specific prefix. Estimating such distribution is sample-efficient since it only requires the prefix. Building on this advantage, we continue using prefix-forcing but address the limitations of the original objective by carefully selecting the prefixes.\nRelationship to model distillation objective. Recently, Thompson and Sklar (2024) propose a new jailbreak objective based on distilling from an uncensored teacher LLM. We note that, when the teacher's output distribution degenerates to a single prefix, the prefix-forcing objective becomes a special case of the model distillation objective with KL-based logit matching. Nevertheless, the prefix-forcing objective has three advantages over distilling from a high-entropy teacher distribution: First, it is sample-efficient, as only the prefix is needed for distillation. Second, the degenerated teacher distribution is often empirically learnable by optimizing hard token prompts, as evidenced by the near-zero losses in our experiments. Third, distilling from a single teacher distribution can be overconstrained, and our multi-prefix objective alleviates this."}, {"title": "4.2 Prefix Selection Criteria", "content": "Our objective hinges on the selected prefixes. Addressing the limitations of the original objective, we introduce two criteria for prefix selection:\nCriterion I: high prefilling ASR. To reduce misspecification, for a malicious request x, we want prefix yp that, once elicited by some attack prompt 0, leads the victim LLM to continue with complete or faithful responses with high probability:\n$$max_{y_p} E_{y_c \\sim P(\\cdot | \\theta, y_p)} [r(x, y_p \\oplus y_c)].$$\nHowever, we cannot know the optimized attack prompt @ without time-consuming optimization, making directly computing Equation (4.2) inefficient. To tackle this, we observe that the expectation can be efficiently approximated by replacing the optimized attack prompt with a manually constructed one. Although this manual attack prompt is often not enough to elicit the target prefix, the approximated value (prefilling ASR) still correlates with the actual value (jailbreak ASR). Figure 6 compares different manual attack prompts in approximating the true value. For prefix selection, we use the manual attack prompt and prefill the victim LLM's response with candidate prefixes, and favor those with high prefilling ASR.\nCriterion II: low initial NLL. To reduce overconstraints, we want prefixes that can be easily elicited by optimized attack prompts. Since this is indicated by a low negative log-likelihood (NLL) with the optimized attack"}, {"title": "4.3 Prefix Selection Pipeline", "content": "We develop a pipeline to automatically generate and select target prefixes. It consists of four steps: candidate prefixes generation, preprocessing, evaluation with two criteria, and selection. The pipeline only needs to run once for each victim LLM and malicious request, as the selected prefixes can be saved offline and reused for future attacks.\nCandidate prefixes generation. We use uncensored LLMs with guided decoding to generate candidate prefixes. These models are typically aligned LLMs finetuned on harmful data or with refusal suppression (Labonne, 2024). For each malicious request, we generate the response using the uncensored LLM and guide the decoding with the victim LLM (Zhao et al., 2024). The guided decoding makes the output more natural for the victim LLM, achieving lower NLL. We generate multiple responses for each request with a high temperature, and then extract prefixes of varied lengths. We generate multiple responses for each request with high temperature and extract varied length prefixes from them. To handle cases where uncensored LLMs still refuse highly harmful prompts, we prefill their responses with phrases like \"Here\", \"To\", or \"Sure\" to ensure compliance. Note that we can also construct candidate prefixes using rule-based methods or with base (non-instruction-tuned) LLMs instead of uncensored LLMS.\nPreprocessing. After obtaining candidate prefixes, we preprocess them through augmentation and filtering. We apply rule-based augmentation to diversify the prefixes, such as replacing \"Here is\" with \"Here's\". To manage the increased number of prefixes and reduce the evaluation workload, we filter out duplicates and any prefixes that begin with refusals.\nEvaluation with two criteria. We evaluate all candidate prefixes using the two criteria. First, we compute their initial NLLs using the victim LLM. Next, we estimate the prefilling ASRs by having the victim LLM complete the prefixes multiple times with temperature one, and then using the judge to determine if the completed responses are harmful. This evaluation is tailored to the victim LLM and the judge, ensuring the selected prefixes fit both."}, {"title": "5 Experiments", "content": "This section evaluates the effectiveness of our objective in achieving nuanced jailbreaks. We use it to seamlessly replace the original \"Sure, here is\" typed objective in existing jailbreak methods and compare the results.\nJailbreak attacks. We use two existing jailbreak methods, GCG (Zou et al., 2023) and AutoDAN (Zhu et al., 2023), to optimize our objective. GCG uses search-based optimization to optimize discrete prompts, whereas AutoDAN combines search-based optimization with guided decoding to generate discrete prompts. The primary feedback for both attacks during optimization comes directly from the objective, highlighting its impact and minimizing the influence of manual prompting. For both attacks, we select the attack prompt with the lowest objective loss during optimization as the final output.\nThreat models. We compare two threat models: 1) Optimizing only the attack suffix, which is then appended to the malicious request. This is the most common setting in prior work. 2) Optimizing the entire attack prompt from scratch without appending the request (Guo et al., 2024). The latter makes the threat model less restrictive but often lead to unfaithful responses under the original objective. For GCG, we use suffix lengths of 20 and 40 tokens in the first setting, and the entire attack prompt with 40 tokens initialized with escalation marks in the second. For AutoDAN, we generate 200-token attack prompts in the second setting only.\nModel, data, and hyperparameters. We test four victim LLMs: Llama-2-7B-chat-hf (Touvron et al., 2023), Llama-3-8B-Instruct, Llama-3.1-8B-Instruct (Dubey et al., 2024), and Gemma-2-9B-it (Team et al., 2024). We use the 50 malicious requests curated from AdvBench (Section 2). We initialize the attack suffix with exclamation marks for GCG and optimize for 1000 iterations. We use a batch size of 512 for both attacks.\nPrefix selection. We use four uncensored LLMs publicly available on Huggingface to generate candidate prefixes: georgesung/llama2-7b-chat-uncensored, Orenguteng/Llama-3-8B-Lexi-Uncensored, Orenguteng/Llama-3.1-8B- Lexi-Uncensored, and TheDrummer/Tiger-Gemma-9B-v1. To evaluate the prefilling ASR, we use random decoding with temperature 1, generating 25 completions for each prefix and averaging the harmfulness results."}, {"title": "5.1 Main Results", "content": "In this subsection, we show that our objective enables GCG to achieve significantly higher ASRs across all settings, while further benefiting from more flexible threat models.\nHigher ASR. Table 1 shows that replacing \"Sure, here is...\" with our new prefixes, tailored to the malicious request and victim LLM, significantly improves ASR across all victim LLMs. On Llama-3, ASRs jump from around 10% to as high as 70%. Our multiple-prefix objective often leads to even higher ASRs. Appendix A shows that these relative improvements also hold when using the other three evaluation judges.\nMitigated misspecification and overconstraint. The failure case breakdown shows that our objective works by mitigating misspecification and overconstraint. On three newer LLMs, it reduces incomplete responses from about 20% to 1-2%, and cuts unfaithful responses by half on Gemma-2, indicating mitigated misspecification. Additionally, our objective halves direct refusals on Llama-3 and 3.1, indicating mitigated overconstraints, since direct refusals indicate failing to sufficiently lower the objective loss.\nBenefits from longer attack suffixes. Optimizing longer attack suffixes empirically leads to lower final losses for both objectives. However, Table 1 shows that extending the suffix length for the original objective on three newer LLMs maintains an ASR of around 10%, mainly due to frequent incomplete and unfaithful responses. By mitigating this misspecification, our objective leverages the extended suffix to reduce direct refusals while managing incomplete and unfaithful responses, ultimately increasing ASR by an additional 9-15%.\nBenefits from optimizing the entire attack prompt. Table 2 shows that optimizing the entire attack prompt, rather than just the suffix, nearly eliminates direct refusals. With the original objective, the ASR increases on Llama-2 and Llama-3.1 but drops on Llama-3 and Gemma-2, due to more frequent incomplete and unfaithful responses. In contrast, by mitigating misspecification, our single-prefix objective increases the ASR from 39% to 73% on Llama-2, and from 70% to 80% on Llama-3, and never hurts the ASR. This improvement highlights a limitation of the common approach, which optimizes only the attack suffix after the malicious request. Optimizing the entire attack prompt, similar to manual and black-box attacks, avoids starting with the malicious request that may put the model on a defensive mode, and removes an optimization constraint."}, {"title": "5.2 Additional Results", "content": "We show that our objective enables another attack to also achieve higher ASRs. Moreover, our objective elicits responses with harmfulness levels comparable to an uncensored LLM. Lastly, we discuss takeaway messages.\nHigher ASR for another attack. Table 3 shows that our objective also enables AutoDAN to achieve higher ASRs across all victim LLMs. On Llama-3, ASRs jump from 5% to 78%. Appendix A shows that these relative improvements also hold when using the other three evaluation judges.\nComparable response harmfulness to uncensored LLM. Figure 7 shows our preference judge's harmfulness evaluation, using an uncensored LLM as the reference. Compared to this uncensored LLM's responses, GCG with the original objective elicits responses with a win rate of only around 10%. In contrast, GCG with our objective achieves win rates between 30% and 50%. This result indicates that our objective elicits more harmful responses, with harmfulness levels comparable to an uncensored LLM.\nSelf-correction in newer LLMs can be bypassed. Our results in Section 3 suggest that the latest LLMs may have undergone deeper alignment to either avoid generating the target prefixes or self-correct after doing so. However, since aligned models rarely produce violation responses through standard sampling (as the probabilities are extremely low), this alignment can only use constructed violation prefixes. Our high jailbreak ASRs suggest that this alignment can be bypassed by using new prefixes that the alignment failed to generalize to, highlighting the need for more tailored prefixes for such alignment."}, {"title": "6 Related Work", "content": "Safety alignment of LLMs. The development of LLMs involves several stages of safety alignment (Dubey et al., 2024; Huang et al., 2024b). During pretraining, developers filter out harmful data to reduce the likelihood of the model generating them. In fine-tuning, developers use supervised fine-tuning (SFT) and RLHF (Ouyang et al., 2022; Bai et al., 2022; Dai et al., 2023; Ji et al., 2024; Rafailov et al., 2024) to adjust the model's rejection behavior under malicious prompts. Finally, at deployment, system-level safety filters like Llama Guard (Inan et al., 2023) and ShieldGemma (Zeng et al., 2024a) help detect and block harmful inputs or outputs. Although newer LLMs use more refined strategies during fine-tuning to counter jailbreaks while minimizing false refusal rates , our findings suggest that these strategies need more tailored prefixes to improve generalization.\nJailbreak attacks and red-teaming. Many works focus on jailbreaking aligned LLMs, which also support red-teaming. Beyond manual jailbreaks , automated jailbreaks mainly fall into two types: white-box, requiring model weights or output logits, and black-box, requiring only output tokens. White-box attacks use search-based methods , or gradient-based methods , to optimize attack prompts, some also considering fluency . Black-box attacks use predefined or learned strategies to generate or iteratively optimize interpretable attack prompts, making them ideal for closed-source LLMs. With weight access, such as in red-teaming, white-box attacks can potentially be stronger and more targeted, and indeed remain the most effective attacks against defensive LLMs like Llama-2. We omit discussion of jailbreak attacks with threat models other than user prompt modification . \nJailbreak attack objectives. Compared to the plethora of jailbreak optimization methods, jailbreak objectives receive less attention. Some works discuss the misspecification issue of the original objective , while others design new objectives to improve jailbreak ASR. In addition to eliciting target prefixes, suppresses refusals of victim LLMs. As one of the various ways to improve GCG, add phrases like \u201cmy output is harmful\u201d to the original prefixes, creating prefixes such as \"Sure, my output is harmful, here is...\", which improves ASR. Unlike these manual targets, we select prefixes tailored to specific victim LLMs and requests based on two criteria, preventing misspecification and overconstraint, thus significantly boosting nuanced ASR. To guide attack prompt optimization, combines two objectives: they elicit the prefix of \u201cSure\u201d and use a distillation objective (logit or representation matching) to mimic an uncensored teacher model\u2019s output on the completion. This is an inspiring objective, but our analysis reveals two challenges: 1) ensuring the teacher\u2019s behavior can be learned by the prompt-parameterized student, since otherwise the loss will not go down; 2) distillation may require many samples (completions), especially when the teacher\u2019s distribution has high entropy. In contrast, our objective, viewed as a teacher with a degenerate distribution, allows distillation with just one sample (the prefix), and our criterion of selecting lower NLL prefixes prefers teacher behaviors that are learnable by the student."}, {"title": "7 Conclusion", "content": "This paper focuses on a key component of jailbreak attacks: the objective. We start by developing nuanced evaluation to pinpoint limitations in the current objective, which are misspecification and overconstraints. Addressing this, we propose a new prefix-forcing objective based on one or more carefully selected prefixes. Experiments show that our objective significantly improves the effectiveness of jailbreak attacks, while also benefiting from stronger optimization capabilities. Our plug-and-play objective enables practitioners to easily incorporate our released prefixes into their own attacks for free performance gains.\nAlong the way, we also provide a systematic analysis of jailbreak objectives, hoping to inspire further advancements in this area. We also find that the latest LLMs' self-correction mechanisms can be bypassed, highlighting the need for more robust and generalizable alignment, such as using our prefixes for finetuning. In future work, we plan to efficiently distill more prefixes into the objective to improve it further."}, {"title": "Limitations", "content": "One limitation of our objective is that selecting prefixes, especially for evaluating prefilling ASR, requires evaluating many sampled responses, leading to a computational burden. Moreover, some optimization algorithms benefit from specific properties of the objective, such as a well-shaped loss landscape, which this paper does not account for."}, {"title": "Ethics Statement", "content": "Our research contributes to the safety and responsible development of future AI systems by exposing limitations in current models. While acknowledging the potential for misuse in adversarial research, we believe our methods do not introduce any new risks or unlock dangerous capabilities beyond those already accessible through existing attacks or open-source models without safety measures. Finally, we believe that identifying vulnerabilities is essential for addressing them. By conducting controlled research to uncover these issues now, we proactively mitigate risks that could otherwise emerge during real-world deployments."}]}