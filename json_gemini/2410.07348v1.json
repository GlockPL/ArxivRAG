{"title": "MoE++: ACCELERATING MIXTURE-OF-EXPERTS METHODS WITH ZERO-COMPUTATION EXPERTS", "authors": ["Peng Jin", "Bo Zhu", "Li Yuan", "Shuicheng Yan"], "abstract": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i) Low Computing Overhead: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii) High Performance: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii) Deployment Friendly: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1~2.1\u00d7 expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have achieved substantial advancements, primarily attributed to the expansion of training data and a significant increase in model parameters. However, the pursuit of ever-larger model sizes incurs prohibitive computational costs. Therefore, the Mixture-of-Experts (MoE) architecture, which allows for parameter scaling while keeping computational costs manageable, has become a preferred solution. The recent incorporation of MoE architectures into Transformers has enabled the effective scaling of language models to impressive sizes, resulting in exceptional performance. These achievements underscore the significant potential and promise of MoE language models.\nMost existing Mixture-of-Experts (MoE) methods typically activate a fixed number of Feed-Forward Networks (FFNs) for all tokens. In many works, each token selects the top"}, {"title": "3 METHODOLOGY", "content": "A standard Mixture-of-Experts (MoE) layer consists of N expert networks E = {E1, E2, ..., EN} and a router G that activates the Top-K experts. Typically, the number of activated experts K is fixed and much smaller than the total number of experts N. Formally, given the input token \u00e6, the output token y of the MoE layer is the weighted sum of outputs from the K activated experts:\n$y = \\sum_{i=1}^{N} g_i E_i(x), g_i = \\begin{cases} Softmax(G(x)), & \\text{if } G(x)_i \\in Top-K(\\lbrace G(x)_i|1 \\leq i \\leq N \\rbrace).\\\\\n0, & \\text{otherwise.} \\end{cases}$\nVanilla MoE. A vanilla MoE layer typically consists of multiple structurally identical experts, where each expert is a standard Feed-Forward Network (FFN). Besides, the router is usually implemented as a trainable weight matrix. Formally, the experts and router in a vanilla MoE layer can be defined as:\n$E = \\lbrace FFN_1, FFN_2, ..., FFN_N \\rbrace, G(x) = Wx,$\nwhere W \u2208 RN\u00d7D is the trainable weight matrix, and D denotes the hidden size of the model. Since a fixed number of FFNs are activated in the vanilla MoE layer for both simple and challenging tokens, the vanilla MoE layer may be training and inference inefficient when processing simple tokens.\nMoE++ Overview. Our proposed MoE++ is a general and heterogeneous MoE framework that integrates both FFN and zero-computation experts. Besides, MoE++ enhances the router by gating residuals, allowing each token to consider its previous pathway when selecting the appropriate experts in the current MoE++ layer. Additionally, to effectively train heterogeneous expert structures, we introduce a heterogeneous load balance loss and a heterogeneous expert capacity allocation strategy.\nThe core components of the proposed MoE++ are illustrated in Fig. 2."}, {"title": "3.1 ZERO-COMPUTATION EXPERTS", "content": "In MoE++, the redesigned expert architecture should satisfy specific criteria: (i) It should be as streamlined as possible to process simple tokens efficiently; (ii) To ensure a fair comparison with the vanilla MoE, the new expert should introduce an almost negligible number of parameters. Guided by these principles, we introduce zero-computation experts, each performing only the most fundamental operations. Specifically, we propose three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively.\nZero Experts. The simplest zero-computation is to discard the current input. Given input token x, the output of the zero expert is 0, which is formulated as:\n$E_{zero}(x) = 0.$\nIn essence, the presence of zero experts can degrade the Top-2 MoE++ layer to the Top-1 MoE++ layer. Specifically, if a zero expert is activated, its zero output makes the output of Top-2 MoE++ layers equivalent to that of the other expert alone. Therefore, in MoE++, the introduction of zero experts adds flexibility in handling both simple and challenging tokens simultaneously.\nCopy Experts. Inspired by residual networks , we propose the copy expert, whose output is equal to the input and therefore equivalent to a shortcut:\n$E_{copy}(x) = x.$\nIntuitively, the copy expert offers the option to skip the current MoE++ layer. Specifically, if the input token is poorly aligned with the existing experts, it may benefit from bypassing the MoE++ layer.\nConstant Experts. Neither zero experts nor copy experts contain trainable parameters, so their flexibility in handling input tokens is limited. To address this limitation, we propose constant experts, which replace the input token \u00e6 with a trainable vector v. However, a complete replacement would lose all input information. Therefore, we use a trainable weight matrix We to dynamically predict the proportion of the replacement. Formally, the output of copy experts can be defined as:\n$E_{const}(x) = \\alpha_1 x + \\alpha_2 v, [\\alpha_1,\\alpha_2] = Softmax(Wx),$\nwhere We \u2208 R2\u00d7D is the trainable weight matrix, and D is the hidden size of the input token x.\nBy assigning fewer FFN experts to simple tokens and dedicating more FFN experts to challenging tokens, MoE++ optimizes computation allocation. Therefore, MoE++ achieves better performance with less computation than vanilla MoE. Moreover, MoE++ significantly broadens the range of sub-networks. For instance, combining an FFN expert with a constant expert is equivalent to adjusting the output of the FFN expert using a trainable vector. Similarly, the combination of a zero expert with a copy expert allows the input token to bypass the current layer entirely."}, {"title": "3.2 PATHWAY-AWARE ROUTER", "content": "Since MoE++ contains heterogeneous experts, the design of the router becomes even more critical compared to vanilla MoE. To this end, we propose the pathway-aware router that considers the pathway taken in the previous layer when selecting the appropriate experts."}, {"title": "3.3 LOAD BALANCE DURING PRETRAINING", "content": "Training an MoE model directly often results in most tokens being dispatched to a small number of experts, leaving other experts insufficiently trained. Following previous works, we apply the load balance loss and expert capacity to ensure a balanced load during pretraining.\nHeterogeneous Load Balance Loss. In vanilla MoE methods, each expert is a standard Feed-Forward Network (FFN), so all experts are assigned the same number of tokens. However, in our proposed MoE++, the architecture and number of parameters in zero-computation experts and FFN experts differ significantly, making it sub-optimal to allocate the same number of tokens to both types of experts. To this end, we introduce a hyper-parameter 7 to control the proportion of tokens allocated between zero-computation experts and FFN experts. Specifically, given the tth input token \u00e6xt, the heterogeneous load balance loss L\u266d is formulated as:\n$L_b = \\sum_{i=1}^{N} f_i P_i,  i = \\begin{cases}\nT, & \\text{if Expert i is a zero-computation expert,}\\\\\nf_1, & \\text{if Expert i is an FFN expert,}\n\\end{cases}$\n$\\text{where } f_i = \\frac{1}{T} \\sum_{t=1}^{T} 1(\\text{Token } x_t \\text{ selects Expert } i), P_i = \\frac{1}{T} \\sum_{t=1}^{T} Softmax (G(x_t))_i,$\nwhere T denotes the number of tokens. N is the number of experts. 1(*) denotes the indicator function. A smaller hyper-parameter 7 means that more tokens are assigned to the zero-computation experts. In comparison, a larger 7 means fewer tokens are allocated to the zero-computation experts.\nExpert Capacity. Expert capacity is proposed to mitigate severe load imbalance by limiting the maximum number of tokens routed to each expert. Since MoE++ assigns different numbers of tokens to different types of experts, we also design varying expert capacities for each type of expert. For an MoE++ model with NFFN FFN experts and Nzc zero-computation experts, the total number of experts is N = NFFN + Nzc. Given the hyper-parameter 7, the expert capacity is defined as:\n$C_i = \\begin{cases}\n\\frac{T}{\\tau N_{FFN} + N_{ZC}}, & \\text{if Expert i is an FFN expert,}\\\\\n\\frac{\\tau T}{T N_{FFN} + N_{ZC}}, & \\text{if Expert i is a zero-computation expert,}\n\\end{cases}$\nwhere y is the preset capacity factor. T is the number of tokens. Similarly, a smaller hyper-parameter 7 means more capacity is allocated to the zero-computation expert. For both types of experts, if an expert is underutilized, its unused capacity is filled with padding tokens. Once an expert reaches capacity, any additional tokens assigned to that expert are dropped out, which means the additional tokens are passed directly to the subsequent Transformer block.\nTotal Training Objective. Finally, the total training loss is the weighted sum of the cross-entropy loss Lce and the heterogeneous load balance loss L\u266d:\n$L = L_{ce} + \\beta L_b,$"}, {"title": "3.4 ANALYSIS OF EFFICIENCY", "content": "It is worth noting that zero-computation experts require a negligible amount of computing and communication costs to process a token. As shown in Tab. 1, for an MoE++ model with NFFN FFN experts, Nzc zero-computation experts and hyper-parameter 7, its computational complexity is only $\\frac{T N_{FFN}}{\\tau N_{FFN}+N_{ZC}}$ that of MoE models with the same number of parameters."}, {"title": "4 EXPERIMENTS", "content": "4.1 EXPERIMENTAL SETUP\nModel Settings. We use Megatron, an open-source training code, as the training framework. We conduct training on a cluster with 4 nodes and 32 A100 GPUs. Tab. 2 summarizes the hyper-parameter settings of various MoE++ models. For example, \u201cMoE++ 0.6B/(8+4)E\" represents the architecture of an approximately 0.6B parameter MoE++ model with 8 FFN experts and 4 zero-computation experts. For a fair comparison, we also include the corresponding MoE model configurations with similar numbers of activated parameters per token during inference.\nTraining Data and Tokenization. MoE++ is trained exclusively on public datasets, making it accessible for academic research settings. Specifically, we sample from the RedPajama, Dolma, and Pile datasets according to different sampling probabilities. Please refer to Tab. A and Appendix B.1 for detailed sample ratios. We use the tokenizer of LLaMA2, which contains 65,536 vocabulary tokens.\nTraining Hyper-Parameters. The hyper-parameters for MoE++ are selected based on the common practice for dense language models. We replace all FFN layers in the transformer with MoE++ layers and set the Top-K to 2 for every layer, resulting in approximately twice the computation compared to a dense model. Please refer to Tab. B and Appendix B.2 for detailed training hyper-parameters.\nEvaluation Benchmarks. We use the lm-evaluation-harness package to assess performance on an extensive suite of downstream tasks: (i) Following Pythia and Sheared-LLaMA, we report 0-shot accuracy on ARC Easy (ARC-E), LAMBADA, LogiQA, PIQA, SciQ, and WinoGrande. (ii) We also report the accuracy of tasks from the Open LLM Leaderboard , including 10-shot HellaSwag, 25-shot ARC Challenge (ARC-C) , and 5-shot MMLU. (iii) Moreover, we report the exact match score for 32-shot Natural Questions (NQ) and the accuracy for 32-shot BoolQ.\n4.2 MAIN RESULTS\nComparisons to Vanilla MoE. To conduct comparative evaluations of our proposed MoE++ against vanilla MoE models, we start with a modest scale of 0.6B parameters and expand up to 7B. Since the activated parameters are only about 0.2B for the smallest model, we select 9 simple\""}, {"title": "4.3 ABLATIVE ANALYSIS", "content": "Effect of the hyper-parameter 7 in Eq. 7 and Eq. 8. The hyper-parameter 7 controls the proportion of tokens distributed between zero-computation experts and FFN experts. To investigate"}, {"title": "4.4 QUALITATIVE ANALYSIS", "content": "Visualization of the Expert Load Distribution at the Task Level. We provide the visualization of the expert load distribution in Fig. 4. Fig. 4 reveals three key findings: (i) There is a significant variation in the number of FFN experts activated per token across tasks, but it is not necessarily the simpler tasks that activate fewer FFN experts. For example, the ARC Challenge task activates more"}, {"title": "5 CONCLUSION", "content": "In this paper, we introduce MoE++, a general and heterogeneous MoE framework that integrates both FFN and zero-computation experts. In contrast to vanilla MoE methods using a fixed mixing mechanism for all tokens, MoE++ optimizes computation allocation by assigning fewer FFN experts"}, {"title": "A APPENDIX", "content": "This appendix provides additional discussions (Appendix A), implementation details (Appendix B), details of quantitative evaluations (Appendix C), and more qualitative analysis (Appendix D)."}, {"title": "A.1 EXPERT ARCHITECTURE", "content": "Experts in MoE models are typically identical to the standard Feed-Forward Networks (FFNs) used in dense models. Recently, efforts have been made to improve the expert architecture. Deepseek-MoE and XMOE split the FFN in the dense model into smaller FFNs, reducing the size of each expert while increasing the number of activated experts. PEER and MH-MoE go further by not only reducing the size of experts but also splitting input tokens into smaller units. Although these methods have made some progress, the structure of experts in existing MoE models remains largely based on FFNs, with little exploration of non-FFN or non-parametric experts. To the best of our knowledge, we are the first to propose zero-computation experts for the heterogeneous MoE architecture."}, {"title": "A.2 LIMITATIONS AND FUTURE WORK", "content": "In this section, we delineate the limitations of our work and outline avenues for future research.\nHeterogeneous MoE++ Between Different Layers. MoE++ implements heterogeneous experts within a single MoE layer. Additionally, as shown in Appendix D, we observe that expert assignment patterns vary more significantly in the shallow and final layers across different tasks, compared to the middle layers. This suggests that the model adapts to tasks primarily through these layers. Future work could explore designing heterogeneous MoE++ configurations across different layers to further enhance the model's adaptability to a wide range of tasks.\nCombining MoE++ with Other Modules. The current MoE++ method serves as a replacement for the FFN layer in Transformers. Future work could explore integrating other modules, such as combining the attention layer with our MoE++ method.\nThe Vulnerabilities of Large Language Models. The focus of our work is to build advanced and efficient mixture-of-experts Large Language Models (LLMs), and as a consequence, also inherit the vulnerabilities common to LLMs.\n\u2022 Hallucination. Hallucinations in LLMs remain a significant unresolved challenge. These illusory responses can lead to unsupported claims during open-ended conversations, and addressing this issue could greatly accelerate progress in the field. For a deeper analysis of common weaknesses in large LLMs, please refer to Brown et al. (2020); Rae et al. (2021).\n\u2022 Long sequence processing. Transformer-based language models often struggle with generalization when faced with test sequences that are significantly longer than those seen during training. This limitation is especially pronounced in multi-turn conversations, where the model may lose track of the previous context, leading to incorrect responses.\n\u2022 Prompt sensitivity. In-context learning has shown troubling sensitivity to various aspects of demonstrations, such as prompt formats. Notably, variations in prompt formats can lead to completely contradictory outputs. Addressing this issue could significantly accelerate progress in the field.\nMore Modalities. Language represents just one facet of communication. Visual and audio information serves to augment and enhance our comprehension of the world. Future work can explore alternative modalities, such as visual and audio inputs. The incorporation of multiple modalities holds the promise of broadening the spectrum of tasks that the model can address, and it has the potential to enhance their performance by leveraging synergies among these various modalities."}, {"title": "B IMPLEMENTATION DETAILS", "content": "B.1 DATA DETAILS\nConsistent with previous works, we use the tokenizer of LLaMA2, which contains 65,536 vocabulary tokens. It is worth noting that MoE++ is trained exclusively on public datasets, making it accessible for academic research settings. Specifically, we sample from the following datasets according to different sampling probabilities:\n\u2022 The RedPajama includes training data from seven domains: Common-Crawl, C4, Github, Wikipedia, Books, ArXiv, and StackExchange.\n\u2022 The Dolma, a large and diverse open English text corpus, contains 3 trillion tokens sampled from seven sources, including web pages from Common Crawl, code from The Stack, curated web data from C4 , social media conversations from Reddit, academic papers from PeS2o, public domain books from Project Gutenberg, and comprehensive content from Wikipedia and Wikibooks.\n\u2022 The Pile, an open-source English text corpus for training large language models, includes 22 diverse, publicly available datasets such as Wikipedia, NIH ExPorter, ArXiv, Books3, BookCorpus2, OpenSubtitles, YoutubeSubtitles, and Enron Emails.\nTab. A shows the detailed sample ratios of different open-source datasets. We find that increasing the ratio of high-quality data, such as Books and Wikipedia, during the later stages of training significantly enhances model performance. Consequently, for the \"MoE++ 7B/(16+4)E\" model in Tab. 4, We increase the ratio of high-quality data for the final 100B tokens. Specifically, this model is trained using strategy 1 for the first 900B tokens and strategy 2 for the last 100B tokens, for a total training budget of 1T tokens. In contrast, for simplicity, all MoE++ and MoE models in Tab. 3 are trained with strategy 1, using a budget of 100B tokens."}, {"title": "B.2 TRAINING HYPER-PARAMETERS", "content": "Tab. B shows the detailed training hyper-parameters. Specifically, the hyper-parameters for MoE++ are selected based on the common practice for dense transformer language models. We replace all FFN layers in the transformer with MoE++ layers and set the Top-K to 2 for every layer, resulting in approximately twice the computation compared to a dense model. The weight 8 for the heterogeneous load balance loss is set to 0.01, and the expert capacity factor y is set to 1.1. MoE++ is trained using the AdamW optimizer . During training, a weight decay of 0.1 and gradient clipping of 1.0 are applied. All MoE++ (except for the \"MoE++ 7B/(16+4)E\" with 8-way pipeline parallel) and MoE models in Tab. 3 are trained using strategy 1 with a maximum learning"}, {"title": "CDETAILS OF QUANTITATIVE EVALUATIONS", "content": "We conduct comparative comparisons of MoE++ against vanilla MoE and dense models. The evaluation is performed on multiple key benchmarks using the Eleuther AI Language Model Evaluation Harness , a unified framework for testing generative language models across a wide range of tasks. The benchmarks used for evaluation include:\n\u2022 ARC is a multiple-choice question-answering resource featuring questions from science exams for grades 3 to 9. It is divided into two partitions: Easy and Challenge, with the latter containing more difficult questions that necessitate reasoning. Most questions offer four answer choices, while less than 1% feature either three or five choices. Additionally, ARC includes a supporting knowledge base with 14.3 million unstructured text passages. We report 0-shot accuracy on ARC Easy (ARC-E) and 25-shot accuracy on ARC Challenge (ARC-C (25)).\n\u2022 LAMBADA is an open-ended cloze task consisting of approximately 10,000 passages from BooksCorpus, where the objective is to predict a missing target word in the last sentence of each passage. The missing word is always the last word of the final sentence, with no options provided. We report 0-shot accuracy on LAMBADA.\n\u2022 LogiQA comprises 8,678 question-and-answer instances that encompass various types of deductive reasoning. The dataset serves as a benchmark for reexamining logical AI within the context of deep learning in NLP. We report 0-shot accuracy on LogiQA.\n\u2022 PIQA is a dataset designed for commonsense reasoning, aimed at evaluating the physical knowledge of current models. We report 0-shot accuracy on PIQA.\n\u2022 SciQ includes 13,679 crowdsourced science exam questions covering subjects such as Physics, Chemistry, and Biology. Each question is presented in a multiple-choice format with four answer options, and for most questions, an additional paragraph provides supporting evidence for the correct answer. We report 0-shot accuracy on SciQ.\n\u2022 WinoGrande is a large-scale dataset comprising 44,000 problems, inspired by the original WSC design but enhanced to increase both its scale and difficulty. We report 0-shot accuracy on WinoGrande.\n\u2022 HellaSwag is a challenging dataset designed to evaluate commonsense Natural Language Inference (NLI), which proves difficult for state-of-the-art models but"}]}