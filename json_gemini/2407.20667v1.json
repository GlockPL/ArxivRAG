{"title": "RETHINKING THE FUNCTION OF NEURONS IN KANS", "authors": ["Mohammed Ghaith Altarabichi"], "abstract": "The neurons of Kolmogorov-Arnold Networks (KANs) perform a simple summation motivated by\nthe Kolmogorov-Arnold representation theorem, which asserts that sum is the only fundamental\nmultivariate function. In this work, we investigate the potential for identifying an alternative multi-\nvariate function for KAN neurons that may offer increased practical utility. Our empirical research\ninvolves testing various multivariate functions in KAN neurons across a range of benchmark Machine\nLearning tasks.\nOur findings indicate that substituting the sum with the average function in KAN neurons results\nin significant performance enhancements compared to traditional KANs. Our study demonstrates\nthat this minor modification contributes to the stability of training by confining the input to the\nspline within the effective range of the activation function. Our implementation and experiments are\navailable at: https://github.com/Ghaith81/dropkan", "sections": [{"title": "Introduction", "content": "This study explores the efficacy of the summation operation in Kolmogorov-Arnold Networks (KANs) nodes. Although\nopting for the sum operation may appear straightforward given its association with the Kolmogorov-Arnold representa-\ntion theorem, it is essential to note that the theorem is predicated on only two layers of non-linearity and a small number\nnodes. Conversely, KANs introduced in [1] extend this concept to networks of arbitrary width and depth. This prompts\nthe question of whether the sum operation represents the most optimal choice for KAN nodes in practice.\nIn this research, we conducted an empirical study to identify the optimal multivariate function for KANs neurons.\nWe evaluated various candidate functions across a range of machine learning classification tasks to determine which\nfunctions yield the best performance. Our findings indicate that using addition as the node-level function may not be\nideal, especially for high-dimensional datasets with numerous features. This is because addition can lead to inputs\nthat exceed the effective range of the subsequent activation function, resulting in training instability and reduced\ngeneralization performance. To address this issue, we propose utilizing the mean instead of the sum as the node function.\nOur results show that using the mean helps to maintain inputs within the desired range for activation functions and\nis consistent with the Kolmogorov-Arnold representation theorem. Furthermore, we investigated the challenge of\nmaintaining inputs within the desired range when utilizing trainable activations in KAN and found that traditional\ntechniques like Layer Normalization [2], often used to tackle covariate shift, may not effectively solve this problem."}, {"title": "Background", "content": "We offer a brief background on Kolmogorov-Arnold representation theorem, and clarify how Kolmogorov-Arnold\nNetworks extends the concept to deeper networks."}, {"title": "Kolmogorov-Arnold representation theorem", "content": "The Kolmogorov-Arnold theorem states that any continuous multivariate function f(x1,...,xn) can be represented as\na finite composition of univariate functions, along with the binary operation of addition:\n$f(x_1,...,x_n) = \\sum_{q=1}^{2n+1} \\Phi_q(\\sum_{p=1}^{n} \\phi_{q,p}(x_p))$\nwhere $\\phi_{q,p}: [0, 1] \\rightarrow \\mathbb{R}$ are continuous univariate functions, $\\Phi_q: \\mathbb{R} \\rightarrow \\mathbb{R}$ are continuous functions that depend on the\nsum of $\\phi_{q,p}(x_p)$. This theorem demonstrates that the only fundamental multivariate function is the sum, as all other\nfunctions can be constructed using univariate functions and addition."}, {"title": "Kolmogorov-Arnold Networks", "content": "Kolmogorov-Arnold Networks [1] defines a KAN layer of $n_{in}$ inputs and $n_{out}$ outputs as a 1D matrix of functions:\n$\\Phi = {\\phi_{q,p}}, p = 1,2,..., N_{in}, q = 1,2,..., N_{out}$\nThis definition enables interpreting the computation graph defined in Equation 1 as a two-layer neural network\nincorporating activation functions $\\phi_{q,p}$ and $\\Phi_q$, which are parameterized with smooth splines and applied to the edges\nof the inputs and the hidden layer, respectively. However, finding the the appropriate functions $\\phi_{q,p}$ and $\\Phi_q$ with smooth\nsplines using the shallow network outlined in Equation 1 proves to be impractical. This challenge prompts the extension\nof this concept to networks of arbitrary width and depth by simply stacking additional KAN layers, akin to the deep\narchitectures found in traditional neural networks."}, {"title": "Empirical Study and Discussion", "content": "In this empirical study, we investigate nine multivariate functions - sum, min, max, multiply, mean, std, var, median,\nand norm - applied to ten datasets using a two-layer KAN architecture. The chosen architecture allows for exploring 81\nunique combinations of functions between the layers, with each dataset tested across all 81 function settings and ranked\nbased on performance.\nNext we detail our experimental design along with the results obtained in the study. In the first experiment we analyze\nthe results of different candidate multivariate neuron functions. In the second experiment, we compare the performance\nof a KAN network using average (the best function from the previous experiment) in neurons against a regular KAN\nand KAN equipped with layer normalization across a number of classification problems."}, {"title": "Experimental Setup", "content": "Our experiments involve 10 popular [3] datasets sourced from the UCI Machine Learning Database Repository.\nThese\ndatasets encompass a range of sizes and feature diverse domains. In Table 1, a comprehensive overview of the instances,\nfeatures, and classes present in each dataset utilized in our research is provided.\nThe data sets are segmented into training (60%), validation (20%), and testing (20%) partitions. A standardized\npreprocessing methodology is implemented across all data sets, encompassing encoding of categorical features,\nhandling missing values through imputation, and randomization of instances. Model accuracy serves as the evaluation\nmetric consistently across all experiments. The accuracy scores presented correspond to the performance on the testing\nset results.\nWe have chosen to use the default values for the hyperparameters of the KANs model. Furthermore, we have configured\nthe grid to 3 in order to manage the number of parameters effectively. In all experiments conducted on various datasets,\nwe have consistently utilized the KAN architecture [nin, 10, 1], where nin represents the number of input features in\nthe dataset. The networks have been trained for 2000 iterations using the Adam optimizer with a learning rate set to\n0.01, and a batch size of 32."}, {"title": "Experiment I - Evaluation of Different Neuron Functions", "content": "In this experiment, we tested the nine multivariate functions - sum, min, max, multiply, mean, std, var, median, and\nnorm - across ten datasets (Table 1) using a two-layer KAN architecture [nin, 10, 1]. The average rank for the top ten\nfunctions combinations across all datasets is presented in Table 2."}, {"title": "Experiment II - Mean vs Sum as a Neuron Function", "content": "In this experiment, we will compare the performance of three different variants of KAN. The first variant, denoted as\nKAN, represents the standard KAN architecture with summation operation at the neurons. The second variant, denoted\nas KAN+LayerNorm, incorporates layer normalization in the intermediate layers of the standard KAN architecture.\nThe third variant, referred to as KAN-AVG, utilizes mean function within the neurons of the KAN architecture. We will\nconduct 20 independent training runs for each dataset using each variant of KAN. To compare the average test accuracy\nof these variants, we will employ the Wilcoxon signed-rank test for statistical significance assessment.\nThe advantage of utilizing the mean over the sum in KAN neurons is highlighted in Table 4, where KAN-AVG\nconsistently demonstrates higher average test accuracy across all datasets compared to the standard KAN. Furthermore,\nKAN-AVG outperforms the standard KAN significantly in terms of test accuracy in 7 datasets, as confirmed by\nWilcoxon test results. It is worth noting that the performance variability of KAN-AVG is consistently lower than that of\nthe standard KAN, indicating a more stable training process."}, {"title": "Related Work", "content": "The original KAN paper by Liu et al. [1] demonstrated the superior performance of KANs over MLPs in data fitting\nand PDE tasks while utilizing fewer parameters. Subsequent studies have further validated the efficacy of KANs across\nvarious domains such as computer vision [5, 6], time series analysis [7], tabular data [8], engineering design [9], human\nactivity recognition [10, 11], DNA sequence prediction [12], and quantum architecture search [13]. However, concerns\nregarding the robustness of KANs to noise have been raised, as evidenced by the degradation in performance when noise\nis introduced [14]. Additionally, Le et al. [15] reported that KANs failed to overcome MLPs in terms of performance\nwhile requiring substantially higher hardware resources. Another study [16] reported contrasting results, comparing\nKANs and MLPs models across various tasks, suggesting that MLP models generally outperformed KANs, except in\ntasks related to symbolic formula representation, where the advantage of KANs stemmed mainly from their distinctive\nB-spline activation function.\nMoreover, research has expanded beyond the standard KAN architecture to explore new variations and enhancements,\nsuch as graph-based architectures [17, 18, 19, 20], convolutional KANs [21], and transformer-based KANs [22].\nFurthermore, ideas originating from deep learning, such as DropKAN [23] improved the generalization of KANs by\nembedding a dropout mask within the KAN layer. Further efforts to enhance the design and efficiency of KANs focused\non exploring alternatives to B-spline to represent activation functions, such as wavelets [24, 25], radial basis functions\n[4, 26], fractional functions [27], rational functions [28], and sinusoidal functions [29]. Notably, the concept of utilizing\nthe mean instead of the sum in KAN neurons, as proposed in this study, could be easily extended to such architectures\nand custom implementations with minimal implementation overhead."}, {"title": "Conclusions", "content": "In this paper, we suggest replacing the summation in KAN neurons with an averaging function. Our experiments show\nthat employing the average function results in more stable training, ensuring that the inputs remain within the effective\nrange of the spline activations. Utilizing the average function clearly aligns with the Kolmogorov-Arnold representation\ntheorem, as illustrated straightforwardly."}]}