{"title": "Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models", "authors": ["Young Kyun Jang", "Ser-nam Lim"], "abstract": "Modern retrieval systems often struggle with upgrading to new and more powerful models due to the incompatibility of embeddings between the old and new models. This necessitates a costly process known as backfilling, which involves re-computing the embeddings for a large number of data samples. In vision, Backward-compatible Training (BT) has been proposed to ensure that the new model aligns with the old model's embeddings. This paper extends the concept of vision-only BT to the field of cross-modal retrieval, marking the first attempt to address Cross-modal BT (XBT). Our goal is to achieve backward-compatibility between Vision-Language Pretraining (VLP) models, such as CLIP, for the cross-modal retrieval task. To address XBT challenges, we propose an efficient solution: a projection module that maps the new model's embeddings to those of the old model. This module, pretrained solely with text data, significantly reduces the number of image-text pairs required for XBT learning, and, once it is pretrained, it avoids using the old model during training. Furthermore, we utilize parameter-efficient training strategies that improve efficiency and preserve the off-the-shelf new model's knowledge by avoiding any modifications. Experimental results on cross-modal retrieval datasets demonstrate the effectiveness of XBT and its potential to enable backfill-free upgrades when a new VLP model emerges.", "sections": [{"title": "1 Introduction", "content": "As the volume and variety of data grow exponentially in our multimedia-rich era, developing and maintaining efficient multi-modal retrieval systems becomes increasingly challenging. These systems, which provide data samples relevant to a user's query, must handle diverse data types, from text and images to audio and video. This growth puts a premium on the scalability and performance of these systems, necessitating continuous advancements in algorithms and technology. Embedding-based deep models for retrieval have emerged as a key solution, transforming high-dimensional data into a lower-dimensional dense embedding space [29, 45, 36, 15, 11, 16]. These models capture the semantic meanings of data samples, enabling the quantification of similarities for retrieval.\nHowever, it is important to note that the embedding spaces generated by different deep models are not inherently compatible with each other. This incompatibility restricts the reuse of an existing gallery that has been embedded with an older model when a new, better-performing model is introduced. Consequently, this necessitates \u201cbackfilling,\u201d a process where the entire gallery must be rebuilt using the embeddings from the new model. Such a requirement is resource-intensive and time-consuming, posing a significant challenge when building retrieval systems.\nBackward-compatible Training (BT) [33, 42, 13, 37, 43] has been developed to tackle this issue, specifically focusing on image retrieval systems. The main objective of BT is to train a new model from scratch in a manner that ensures its compatibility with an old model that was used to create the"}, {"title": "2 Related Works", "content": "Backward-compatible Training. The concept of Backward-compatible Training (BT) was first introduced in the study [33]. This approach influences a new model with the learned classifier of the old model, thereby achieving backward compatibility between the old and new models. However, BT can degrade the original representational performance of the new model. To overcome this, [25] proposed aligning class-wise centers presented by the old and new models. Another approach to achieve backward compatibility is to use an additional projection to map the old embedding into the new embedding space by adding a lightweight transformation, as suggested in [13, 37]. The work of [28] further adds an auxiliary feature in preparation for future updates, while [46] uses additional dimensionality in the embedding to obtain compatibility. An online strategy that backfills the gallery on the fly is explored in [42], and [41] addresses the model regression problem. Despite this progress, the scenario considering cross-modal retrieval between image and text, which has many real-world applications, remains unexplored. Our study on XBT in this paper aims to fill this gap.\nVision-Language Continual and Transfer Learning The fields of continual learning [2, 5, 7] and transfer learning [24, 47] bear similarities to backward-compatible representation learning, as all aim to update an existing model to boost performance. In the realm of multi-modal representation learning for cross-modal retrieval, continual learning approaches like [39] propose methods to prevent catastrophic forgetting across different modalities. Transfer learning approaches like [44] suggest strategies for transferring knowledge from previously labeled categories (source modality) to new, unlabeled categories (target modality). However, our proposed XBT stands apart from these methods as it specifically tackles the challenge of maintaining backward compatibility between old and new models. This unique attribute makes XBT ideally suited for backfill-free retrieval scenarios, where the embeddings of the old model remain unchanged, yet we can still leverage the enhanced performance of the new model."}, {"title": "3 Methodology", "content": "Our goal is to construct a backfill-free, embedding-based, cross-modal retrieval system using VLP models, which are configured with an image encoder $E^I$ and a text encoder $E^T$. When a new better performing VLP model ${E^{new}, E^{new}}$ emerges, we aim to ensure its compatibility with the old model ${E^{old}, E^{old}}$ that was used to construct the gallery. To achieve this, we introduce Cross(X)-modal Backward-compatible Training (XBT). Given an image $x$ and text caption $t$, XBT enables retrieval between a new image embedding $v_{new} = E^{new}(x)$ and the text embeddings $(w_{old})$, as well as between a new text embedding $w_{new} = E^{new}(t)$ and the image embeddings $(v_{old})$. We denote backward compatible embeddings as $\\bar{v}$ and $\\bar{w}$. All embeddings we utilize in this work are $l_2$-normalized."}, {"title": "3.1 Criterion for Cross-modal Backward Compatibility", "content": "Following the definition of backward compatibility in BT work [33], we can construct strict constraints that ensure cross-modal backward compatibility as:\n$d(w_{new_i}, v_{old_j}) \\leq d(w_{old_i}, v_{old_j}), \\forall y_i = y_j, d(w_{new_i}, v_{old_j}) \\geq d(w_{old_i}, v_{old_j}), \\forall y_i \\neq y_j,$\n$d(v_{new_i}, w_{old_j}) \\leq d(v_{old_i}, w_{old_j}), \\forall y_i = y_j, d(v_{new_i}, w_{old_j}) \\geq d(v_{old_i}, w_{old_j}), \\forall y_i \\neq y_j,$\nwhere $y_i$ and $y_j$ represent whether an image and text are paired ($y_i = y_j$) or not ($y_i \\neq y_j$). The term $d(\\cdot, \\cdot)$ represents a distance metric in the embedding space, and we choose cosine distance as the baseline. The constraints in Eqn. 1 formally express the requirement that the new embedding must perform at least as well as the old embedding in terms of correctly matching image-text pairs."}, {"title": "3.2 Text-only Pretraining", "content": "The vast and diverse image-text pairs used to build VLP models significantly enhance their ability to connect semantically similar visual content and natural language [6]. However, this creates a complex embedding distribution that is difficult to predict and understand, thereby complicating the XBT process. A straightforward solution is to use a large number of supervised image-text pairs, similar to the approach used when building VLP models from scratch, to estimate the entire distribution of image and text embeddings during XBT.\nHowever, acquiring a sufficient number of accurate supervised pairs is extremely costly. To be far more efficient, we employ a small sized projection module, $\\phi$, and train it exclusively with text samples, as shown in Figure 2. We hypothesize that the distribution of text embeddings in VLP models, which is determined by their semantic similarity, is similarly mirrored in the distribution of their corresponding matched images. With this in mind, we aim to train $\\phi$ to cover the broad spectrum of the text embedding distribution between the new and old VLP models, making embeddings from the same text sample similar and others dissimilar in a contrastive way:\n$L_{pre} = E_{t\\sim D_T}[L_c(\\phi(w_{new} + \\epsilon), w_{old}; \\tau_{pre})],$"}, {"title": "3.3 Cross-modal Backward-compatible Training", "content": "Training Loss. The objective of XBT is to ensure cross-modal compatibility, specifically between $\\bar{w}_{new}$ and $v_{old}$, as well as between $\\bar{v}_{new}$ and $w_{old}$. For a given dataset $D = \\{x_i,t_i\\}_{i=1}^{N_D}$ of $N_D$ supervised image-text pairs, we aim to train the new VLP model encoders, $E_{new}^I$ and $E_{new}^T$. Note that in our base setting, the text corpus is significantly larger than the supervised dataset, i.e., $N_{DT} \\gg N_D$.\nHowever, the dimensionality of new and old VLP embeddings may differ, and even if they are the same, they are not be directly compatible. To address this, we apply the pretrained $\\phi$ to ensure that the new embeddings match the dimension of the old ones and project into the compatible embedding space, as shown in Figure 2. This can be represented as:\n$\\phi(v_{new}) = \\bar{v}_{new}, \\phi(w_{new}) = \\bar{w}_{new},$\nwhere $\\bar{v}_{new}$ and $\\bar{w}_{new}$ denote synthetic old embeddings and $\\{\\bar{v}_{new}, \\bar{w}_{new}, v_{old}, w_{old}\\} \\subset \\mathbb{R}^K$, with $K$ denoting the dimensionality of the old VLP model's embedding.\nWe then apply XBT loss $L_x$ as follows:\n$L_x = E_{x,t\\sim D}[L_c(\\bar{v}_{new}, \\bar{w}_{new}; \\tau_{x})].$\nHere, $L_c(\\cdot, \\cdot)$ is the same contrastive loss used in Eqn. 3, with temperature $\\tau_x$. As shown in Figure 3, the new VLP encoders, $E_{new}^I$ and $E_{new}^T$, and $\\phi$ are trained through $L_x$ in an end-to-end manner. Ultimately, $\\bar{v}_{new}$ and $\\bar{w}_{new}$ become cross-modal backward-compatible with existing old gallery embeddings, $v_{old}$ and $w_{old}$, as all embeddings are distributed in the compatible space through $\\phi$.\nEfficient Training. The efficiency of XBT can be attributed to two design choices that avoid: (1) any dependence on the old model when conducting XBT, and (2) updating the new VLP model off the shelf. We have already seen how $\\phi$ can effectively achieve (1). (2) is simply facilitated by applying small-sized additional parameters, namely Soft prompt [19] and LoRA[12].\nTo be more specific, we incorporate the concept of soft prompt tuning, as outlined in [19, 18]. Soft prompting is applied to the vision encoder $E_{new}^I$ by adding trainable prompts as input, along with image patch tokens. We exclude soft prompt tuning on the text side, as we use the text-only pretrained module $\\phi$, which requires maintaining the original distribution of text embeddings. We adopt the LORA strategy [12] for the new VLP model's encoders $E_{new}^I$ and $E_{new}^T$ to avoid modifying original parameters. These two factors, in conjunction with $\\phi$, offer an on-off solution: we can retrieve old samples using additional parameters, and we can easily revert to the original model by removing these parameters for new-to-new retrieval."}, {"title": "4 Experiments", "content": "The evaluation of the proposed cross-modal backward compatibility includes image-text zero-shot retrieval. Pretrained VLP models are used as our baseline, with the goal to assess performance in a zero-shot environment by tuning a given pretrained new VLP model that is stronger than the old model. The results highlight the potential of XBT, showing improved performance when old model outputs are matched with the XBT-learned new model."}, {"title": "4.1 Setup", "content": "Model Training. Our XBT process involves two distinct training stages: text-only pretraining (See Sec. 3.2) and image-text supervised training (See Sec. 3.3). For the text-only pretraining stage, we utilize the text samples from a subset of the 115M filtered web-collected image-text paired dataset from BLIP [20], comprising around 67M available pairs (58.2% of the total). We construct a text corpus $D_T$ in Eqn. 3 using these text samples. For the subsequent image-text supervised training stage, we use a smaller subset of 4M image-text pairs from the same dataset to construct a supervised dataset $D$ in Eqn. 5. It is important to note that these subsets are significantly smaller than the dataset scale (400M, 2B or more) used to build CLIP models [27, 31].\nModel Evaluation. We validate the effectiveness of XBT with a benchmark comprising three popular image-text paired datasets for cross-modal retrieval evaluation. The first is the nocaps dataset [1], which offers a diverse distribution of object names and facilitates a more detailed analysis. We employ the validation split of this dataset, which consists of images each paired with 10 relevant textual captions, totaling 4,500 images and 45,000 captions.\nTo evaluate XBT at a larger scale, we also include the Flickr [14] and COCO [22] datasets, which consist of images that are each paired with 5 relevant textual captions. For the Flickr dataset, we use the entire dataset, encompassing 31,783 images and 158,915 captions. For the COCO dataset, we use the validation split, which includes 35,136 images and 175,680 captions. We notate the two datasets as Flickr-31K and COCO-35K respectively for the remainder of the paper.\nFor retrieval, we utilize all the samples in each dataset for both query and gallery, following the cross-modal benchmark used in [27, 20]. For evaluation purpose, we adopt recall scores at top K retrieval results (R@K, %) to estimate the cross-modal backward compatibility (See Eqn. 2).\nImplementation Details. In our work, we primarily use the popular CLIP models [27], based on a Transformer [35] backbone, as our baseline VLP models. The models, listed in ascending order of scale and performance, are: CLIP-ViT-B32, CLIP-ViT-L14, and CLIP-ViT-H14. For simplicity, we will refer to them as B32, L14, and H14, respectively. Notably, while B32 and L14 are trained with the same dataset at 400M scale, H14 is trained with a larger dataset at 2B scale. We apply additional LORA [12] parameters for each new model encoder configured as follows: LoRA = 16, rank = 16, and dropout = 0.1. The proposed projection module $\\phi$ consists of three Linear layers with layer normalization (LN) [3] and the GELU non-linearity function [10]. Detailed architecture is as follows:\nLinear - LN \u2013 GELU \u2013 Linear \u2013 LN - GELU \u2013 Linear. Dropout is not applied as it was found to degrade performance empirically. The intermediate hidden dimension of the Linear layer is set to be four times the dimensionality of the output embedding."}, {"title": "4.2 Main Results", "content": "Baseline Comparisons. To simplify notation, we use w/v to denote the retrieval results obtained using w as the query embeddings and v as the gallery embeddings. We establish two protocols: assessing cross-modal backward compatibility with new/old ($w_{new}/v_{old}, v_{new}/w_{old}$), and maintaining the new VLP's original performance with new/new ($w_{new}/\\bar{v}_{new}, v_{new}/\\bar{w}_{new}$). All of the above are similarly applied to v/w.\nTo the best of our knowledge, since this paper presents the first method that aims to solve the XBT problem, there are no direct previous works to compare to. Nevertheless, we compare our XBT against three baselines: Full-tune, LoRA-only, and Base, which we elaborate on below.\nNa\u00efve solution - Direct backward contrastive learning: The pretrained $\\phi$ in Eqn. 3, which maps the new embeddings to the old via text based contrastive learning, constitutes a major contribution of XBT. We therefore would like to compare with a solution that does not utilize such a pretrained $\\phi$. A straightforward solution for this is to fine-tune the new VLP encoders ($E_{new}^I$ and $E_{new}^T$) to be compatible with the old ones by minimizing the following loss:\n$L_{Direct} = E_{x,t\\sim D}[L_c(v_{new}, w_{old}; \\tau_N)+\nL_c(w_{new}, v_{old}; \\tau_N)]$\nwhere $L_c(\\cdot, \\cdot)$ is the same contrastive loss used in Eqn. 5, and $\\tau_N$ is a temperature. Here, we apply a randomly initialized $\\phi$ with the same configuration as XBT for projection, as per Eqn. 4, to generate cross-modal backward compatible embeddings, $\\bar{w}$ and $\\bar{v}$.\nDifferent training options: Based on Eqn. 6, we consider multiple training setups: (1) full fine-tuning of all trainable components (Full-tune), (2) tuning LoRA parameters-only (LoRA-only), and (3) starting from LoRA-only, adding an extra learnable prompt (Base). Models produced by Base would therefore be akin to XBT without the pretrained $\\phi$. For these, we train the $E_{old}^I$ $E_{new}, E_{new}$ and $\\phi$ using $L_{Direct}$ with 4M image-text pairs. Note that, XBT only trains layer-normalization layers including pretrained $\\phi$, however, Full-tune, LoRA-only and Base setups are training entire parameters of $\\phi$ since it is not trained before. The results of these baselines are presented in Table 1, tested with nocaps dataset. The performance of the original VLP models is also reported for a clear comparison. We highlight our XBT method.\nIn the context of cross-modal backward compatibility, where recall scores of $w_{new}/v_{old}$ and $\\bar{w}_{new}/v_{old}$ should be higher than those of w/v and v/w of old model, respectively, Full-tune significantly underperforms compared to the old VLP's. This indicates that full fine-tuning is not"}, {"title": "4.3 Further Analysis", "content": "Ablation Study. To validate XBT, we conduct further analysis as shown in Table 4. By comparing (a) and (b), we observe that introducing noise during $\\phi$ training aids in generalization. The comparison between (c) and (d) demonstrates that the scale of $D_T$ is important, supporting our assumption that a sufficient number of text samples can help build a robust $\\phi$. (e) outperforms (a), confirming that utilizing more image-text pairs can enhance XBT. The lower performance of (f) also aligns with the notion that the number of image-text pairs is crucial. In (g), when we replace $D$ with CC3M [32], from which we can obtain around 2.4M, the performance is similar to (f), suggesting that XBT can be effectively applied with other datasets.\nDifferent VLP models. To further explore the capacity of the VLP model architecture's generalization of XBT, we evaluate it using BLIP [20] Base and Large models. We employ checkpoints of Salesforce/blip-itm-base-coco and Salesforce/blip-itm-large-coco from the Huggingface library. We apply XBT on old and new BLIP models in the same fashion with our CLIP applications, and the results appear in Table 5. From the results, we confirm that XBT provides cross-modal backward-compatibility to the BLIP models too.\nQualitative Results. We facilitate retrieval by utilizing new query embeddings and old gallery embeddings. The results in Figure 4 demonstrate accurate cross-modal backward-compatible retrieval."}, {"title": "5 Discussion & Conclusion", "content": "Potential Broader Impact and Limitation. This paper highlights our efforts to enhance the field of Machine Learning, particularly in the area of multi-modal embedding-based representation learning. Although our work may have societal implications, we do not believe there are any that require specific emphasis in this context. A potential limitation of the XBT system is that, despite the efficient learning approach reducing the need for image-caption pairs, its performance may still be limited by the quality, diversity, and representational richness of the data during training.\nConclusion. In this paper, we introduced Cross-modal Backward-compatible Training (XBT), a novel task for cross-modal retrieval that focuses on the compatibility between image and text embeddings of different Vision-Language Pretraining (VLP) models. We proposed an efficient solution using a text-only pretrained projection module, $\\phi$, to align the new model's embeddings with those of the old model, thereby enhancing training efficiency. By integrating parameter-efficient training schemes into the XBT framework, we were able to accelerate the model's training while preserving the original VLP's zero-shot capabilities. Our approach, demonstrated on various cross-modal benchmarks, effectively builds cross-modal retrieval systems without backfilling, offering an efficient and environmentally friendly solution in response to the VLP improvements."}, {"title": "A Appendix", "content": "A.1 Dataset Examples, More Visualization\nFigure 5: A tSNE visualization of 5,000 paired image-text embeddings from COCO [22] dataset, using two different CLIP models [27], and two different BLIP models [20]. Five pairs are marked as examples. The distinct distributions of image and text samples in each VLP space are observed.\nIn Figure 5, we use a t-SNE map to examine the actual distribution of embeddings in the VLP space. It's evident that the image and text embeddings are distinct. Furthermore, the intra-distribution within both image and text embeddings is similar, suggesting that they are supposed to mirror each other.\nA.2 More Implementation Details\nTraining Recipe We utilize 8-A100-80GB GPUs to train and evaluate the models. For the text-only pretraining stage in Sec. 3.2, the batch size is set to 8,192 (1,024 batch per GPU) and during this stage, the entire set of trainable parameters of $\\phi$ are trained while both VLP text encoders are fixed. Moving on to the image-text supervised training in Sec. 3.3, the batch size is reduced to 1,024 (128 batch per GPU), and in this stage, layer normalization is set to be the only trainable component for all VLP image and text encoders with $\\phi$. Temperature hyper-parameters $\\tau_{pre}$, $\\tau_X$, and $\\tau_N$ are fixed at 2.6592. We employ the AdamW optimizer [23] with a fixed learning rate of 1e-4 and a weight decay of 0.01. For soft prompts, we use 10 prompts and apply 100 times larger learning rate, 1e-2. The training iteration is determined by the dataset size, and the entire pipeline is trained for a single epoch. For image augmentation, we begin with a random resized crop, adjusting the image scale between 0.5 and 1.0. Additionally, we apply a random horizontal flip and make random adjustments to the image's contrast, brightness, and sharpness. To incorporate different perspectives and angles, we modify the image's translation and rotation. The pretrained weights provided by HuggingFace\u00b9 [40] are applied to baseline VLP models. We employ as: openai/clip-vit-base-patch32, openai/clip-vit-large-patch14, laion/CLIP-ViT-H-14-laion2B-s32B-b79K.\nA.3 Further Analysis & Discussion\nContinual learning. In line with the literature on BT works [33, 28], we set up a continual learning scenario for a sequence of model updates (old model B32, new model L14, and better new model H14) and present the retrieval results in Table 6. Initially, we apply XBT to L14 to ensure compatibility with B32. Subsequently, we apply XBT to H14 to ensure compatibility with the previously learned L32, which is already compatible with B32. For this process, we divide both the image-text pairs and the text-only pretraining train sets into two halves, using each separately for each case. A comparison with the original results reported in Table 6 confirms that our XBT performs well in the continual learning scenario either. It achieves cross-modal backward compatibility while leveraging the power of the improved new model.\nComputational Analysis. In Table 7, we calculate the required training cost for each baseline. Despite XBT handling a larger number of training samples, the total training time (Text-only pretraining + XBT) is less than that of the other methods. Furthermore, since XBT does not utilize the old VLP model during training, it significantly reduces the memory load.\nResearch question: image-only pretraining. There exists a possibility of training $\\phi$ using only images, which would replace the text-only pretraining stage. We opted for the text-only scheme because text data is easier to obtain than image data and can provide explicit semantics. These include abstract concepts, logical reasoning, and specific details that may not be visually represented. Text can express nuances of language, sentiment, and context that can be challenging to depict in an image.\nHowever, images can also convey visual semantics that are difficult to describe in words. They can provide a wealth of details about the visual world, including color, shape, texture, spatial relationships, and visual patterns. Images can also convey non-verbal information, such as emotions and actions, that can be difficult to express in text. Therefore, pretraining $\\phi$ only with images and comparing it with the text-only approach would be an interesting direction for future research.\nResearch question: Zero-shot Classification. As we incorporate VLP models, an intriguing research question emerges: How do VLP models, fine-tuned with XBT, perform as zero-shot classifiers? To investigate this, we conduct a zero-shot classification using the text prompt 'a photo of class name'. As demonstrated in Table 8, XBT outperforms the old VLP in classification performance, though it falls short of the new VLP. Interestingly, we observe that as the number of supervised training samples increases, so does the classification performance. This suggests the potential for XBT-tuned models to function as zero-shot classifiers given sufficient training samples. This opens up a new research direction towards not only achieving backward compatibility, but also comparable performance to zero-shot classifiers."}]}