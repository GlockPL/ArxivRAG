{"title": "DHCP: Detecting Hallucinations by Cross-modal Attention Pattern\nin Large Vision-Language Models", "authors": ["Yudong Zhang", "Ruobing Xie", "Jiansheng Chen", "Xingwu Sun", "Zhanhui kang", "Yu Wang"], "abstract": "Large vision-language models (LVLMs) have demonstrated\nexceptional performance on complex multimodal tasks.\nHowever, they continue to suffer from significant hallucina-\ntion issues, including object, attribute, and relational hallu-\ncinations. To accurately detect these hallucinations, we in-\nvestigated the variations in cross-modal attention patterns\nbetween hallucination and non-hallucination states. Lever-\naging these distinctions, we developed a lightweight detec-\ntor capable of identifying hallucinations. Our proposed\nmethod, Detecting Hallucinations by Cross-modal Atten-\ntion Patterns (DHCP), is straightforward and does not re-\nquire additional LVLM training or extra LVLM inference\nsteps. Experimental results show that DHCP achieves re-\nmarkable performance in hallucination detection. By of-\nfering novel insights into the identification and analysis of\nhallucinations in LVLMs, DHCP contributes to advancing\nthe reliability and trustworthiness of these models.", "sections": [{"title": "1. Introduction", "content": "Leveraging the capabilities of large language models\n(LLMs) such as Vicuna [8], OPT [39], FlanT5 [9], and\nLLaMA [33], a prominent class of large vision-language\nmodels (LVLMs) has emerged, enabling the handling of\nvision-language tasks by incorporating aligned visual to-\nkens as inputs to the LLM. Notable examples of these\nLVLMs include BLIP-2 [21], InstructBLIP [10], MiniGPT-\n4 [41], and LLaVA [26]. Although these models have\nachieved impressive performance, they continue to grapple\nwith a critical challenge: hallucinations.\nCurrent approaches for assessing hallucinations in\nLVLMs can be broadly categorized into discriminative and\ngenerative methods. Discriminative approaches [15, 22,\n27] represented by POPE transform objects, attributes, or\nrelationships into yes/no questions, e.g., \"Is there a/an\n{object}?\". Then, they evaluate the hallucination severity\nbased on the LVLM's responses. On the other hand, gen-\nerative approaches [14, 19, 24, 32, 34, 35] represented by\nAMBER evaluate hallucinations by analyzing performance\non generative tasks.\nAs we delve deeper into the reasons for hallucinations in\nLVLMs, one possibility is that they are due to the LLM's er-\nroneous focus on the image, such as mistakenly believing a\ncertain object is present when it is not. Therefore, we define\nthe cross-modal attention as the attention to the visual to-\nken when the LLM generates the first token, effectively cap-\nturing multimodal interactions in LVLMs. We conjecture\nthat when the model is in a hallucination state, it may ex-\nhibit a distinct attention pattern to the visual tokens, which\nmay differ from that of non-hallucinating states, thus en-"}, {"title": "2. Related Works", "content": "2.1. Large Vision-Language Model (LVLMS)\nLVLMs generally comprise three components: a visual\nencoder, a vision-language model, and a vision-language\nalignment module. The visual encoders typically use CLIP\n[11, 29], which encodes input images as visual features.\nThe vision-language alignment module aligns visual fea-\ntures to the input space of the large language model, en-\nabling the LLM to process information from the visual\nmodality. Typical vision-language alignment modules in-\nclude cross-attention [2], linear layers or multi-layer per-\nceptrons (MLPs) [5, 25, 26], adapters [13], and Q-former\n[10, 21, 41]. Large language models can be selected from\npre-trained LLMs, such as [33] and [8]. If the visual en-\ncoder and the LLM are well-aligned, the LVLM can exhibit\nstrong multimodal capabilities and utilize the LLM to gain\na deeper understanding of the image's semantics.\n2.2. Hallucination in LVLMS\nAlthough LVLMs have proven effective in handling vision-\nlanguage tasks, they continue to suffer from significant hal-\nlucination issues. These hallucinations can be broadly clas-\nsified into three types: object, attribute, and relational hallu-\ncinations. Object hallucinations refer to the incorrect iden-\ntification of objects in images, attribute hallucinations per-\ntain to misattributions of object characteristics (e.g., color),\nand relational hallucinations involve errors in describing\nthe spatial or contextual relationships between objects (e.g.,\n\"up\", \"down\", \"left\", \"right\", etc.). The evaluation of hallu-\ncinations can be divided into two categories: discriminative\nand generative tasks. Discriminative hallucination assess-\nments [15, 22, 27], typically involve yes/no questions (e.g.,\n\"Are there four dogs in this image?\") to assess the hallu-\ncination of LVLMs. In contrast, generative hallucination\nassessments [14, 19, 24, 32, 34, 35] evaluate hallucinations\nof LVLMs based on its open-ended responses.\n2.3. Detecting Hallucination in LLMs or LVLMS\nMany studies have focused on hallucination detection in\nLLMs [1, 3, 6, 12, 36]. In contrast, while there are several\nhallucination mitigation strategies for LVLMs [4, 7, 14-\n18, 23, 24, 28, 31, 37, 38], few studies have specifically ad-\ndressed hallucination detection in LVLMs. Several studies\n[40] have also explored the detection of adversarial exam-\nples in LVLMs; however, our primary focus is on naturally\noccurring hallucinations, rather than those induced by arti-\nficially crafted adversarial examples.\nOur approach directly tackles this gap by focusing on\nhallucination detection in LVLMs. Specifically, we propose\nthat LVLMs exhibit distinct cross-modal attention patterns\nwhen hallucinating, which differ from the patterns observed\nwhen they are not hallucinating. By analyzing the LLM's"}, {"title": "3. Cross-modal Attention for Hallucination", "content": "3.1. Cross-modal Attention in LVLM\nWe first define the cross-modal attention $A$ in LVLMs. We\nuse the example of InstructBLIP Vicuna-7B, which em-\nploys Q-former as the vision-language alignment module.\nIt comprises three components: the CLIP visual encoder $f_v$,\nthe vision-language alignment module $Q$, and the large lan-\nguage model $f$. Upon inputting an image $x_i$ and a text $x_t$,\nthe visual encoder encodes the image $x_i$ into a visual feature\n$f_v(x_i)$, which is aligned by the vision-language alignment\nmodule $Q$ to $Q(f_v(x_i), x_t)$, fitting the input space of the\nLLM $f$. For InstructBLIP Vicuna-7B, $Q(f_v(x_i), x_t)$ com-\nprises 32 visual tokens. These visual tokens are fed into\nthe LLM, followed by the text tokens obtained from text\nencoding, and after a start marker eosstart, the LLM gen-\nerates a string of responses. We define our cross-modal\nattention $A(x_i, x_t)$ as the attention of the large language\nmodel to each visual token at each layer and each attention\nhead when the first token is generated. For InstructBLIP\nVicuna-7B, it has a shape of (32, 32, 32), where the first 32\nrepresents the number of visual tokens, the second 32 repre-\nsents the number of LLM layers, and the third 32 represents\nthe number of multi-head attention heads. Other LVLMS\nmay have different numbers of visual tokens, LLM layers,\nand multi-head attention heads, so the shape of cross-modal\nattention $A$ is model-dependent.\n3.2. Cross-modal Attention Excels in Revealing\nTrace of LVLM Hallucinations\nTo observe whether $A$ defined in Sec. 3.1 differs between\nhallucination samples and non-hallucination samples, we\nfirst introduce the datasets used.\nPOPE. POPE [22] is a classical LVLM object hallucination\ndataset that contains three clusters: random, popular, and"}, {"title": "4. DHCP: Detecting Hallucinations of LVLMS", "content": "In this section, we first introduce our hallucination detection\nmethod, DHCP-d, and present some preliminary results in\nSecs. 4.1 to 4.3. This is followed by a more general version\nof the hallucination detection method, DHCP-g, introduced\nin Sec. 4.4, along with its corresponding hallucination de-\ntection results of more diverse tasks in Secs. 4.5 to 4.7.\n4.1. DHCP-d's First-stage Hallucination Detection\nIn this section, we train a DHCP-d first-stage hallucination\ndetector to achieve higher recall for hallucinations. In this\nstage, we focus solely on high recall and do not prioritize\nprecision, as our goal is to identify as many suspected hal-\nlucination samples as possible and hand them over to the\nsecond-stage detector for finer detection.\nBased on the differences in attention hallucination ex-\namples and non-hallucination examples found in Sec. 3.2,\nwe attempted to train a detector to distinguish whether\nthe model is in a hallucinatory state or not. Specifically,\nwe aggregated all the image-question pairs in the three"}, {"title": "4.2. DHCP-d's Second-stage Hallucination Detec-tion", "content": "We further build the second-stage detector for finer detec-\ntion, i.e., higher precision to identify hallucination sam-\nples more accurately. The second-stage detector benefits\nfrom the coarse detection results of the first-stage detector.\nInitially, the original data contains only about 15% hallu-\ncination samples, leading to a severely unbalanced training\ndataset. However, after the first-stage detection, the ratio of\nhallucination to non-hallucination samples becomes nearly\n1:1 in both AYH and ANH categories, which provides an\noptimal condition for training a more refined second-stage\ndetector focusing on finer-grained hallucination features.\nTo mitigate the high false alarm rate of the first-stage de-\ntector on hallucination categories in Tab. 1, We extracted\nthe correct and incorrect samples from those detected by\nthe first-stage detector as hallucinations and trained two ad-\nditional detectors for finer detection.\nOur second-stage training pipeline is shown in Fig. 3b.\nFor the samples that were detected by the first-stage detector\nas answering \u201cyes\u201d and having hallucinations (i.e., A\u04af\u043d),\nwe categorized them into two groups: true AYH and false\nAYH, according to correct and incorrect detections. We"}, {"title": "4.3. The Final DHCP-d Two-stage Serving Process", "content": "DHCP-d two-stage serving process uses both the first-\nstage detector in Sec. 4.1 and the second-stage detectors in\nSec. 4.2. The first-stage detector c\u2081 focuses more on the\nhigh recall of hallucination samples, and the second-stage\ndetectors CH and CH perform more fine-grained detec-\ntion of the hallucination samples detected by the first-stage\ndetector to reduce the false alarm rate. We considered a\nsample to be a hallucination only if both two-stage detec-\ntors identified it as such. DHCP is simple to implement and\nrequires no training or additional inference of LVLMs.\nThe hallucination detection results on test set of halluci-\nnations via DHCP-d two-stage serving process are shown\nin Tab. 2 (train set results in Tab. 13). Compared with\nour single-stage results in Tab. 1, our two-stage DHCP-\nd achieves an exceptionally high accuracy rate of over\n93% in hallucination detection. High recall, precision and\nFl-scores demonstrate the effectiveness of our two-stage\nDHCP-d in hallucination detection. We also do compre-\nhensive experiments on different datasets and LVLMs in\nSec. 5.1 to verify the validity and generalizability of DHCP."}, {"title": "4.4. DHCP-g: Exploration on A More Generic Hal-lucination Detection", "content": "The DHCP-d method presented in Sec. 4 is specifically de-\nsigned for discriminative tasks that could be answered in\nbrief words (e.g., Yes/No). To extend this approach to a\nbroader range of generative tasks, we propose a straight-\nforward generalized version following the principles of\nDHCP-d. In this approach, we use a DHCP-g detector\nthat classifies outputs into two categories: hallucination and\nnon-hallucination. For training, we categorize the dataset\nbased on the presence or absence of hallucinations, and in-\nput the cross-modal attention of the first generated token\nand corresponding labels into the DHCP-g detector. During\ntesting, the DHCP-g detector determines whether the given\nsample exhibits hallucinations based on the cross-modal at-\ntention. This generalized method enables hallucination de-\ntection in a wider variety of generative tasks."}, {"title": "4.5. DHCP-g on Discriminative Yes/No Tasks", "content": "We initially evaluated the feasibility of DHCP-g within a\ndiscriminative Yes/No hallucination assessment task. To\nthis end, we trained two-stage DHCP-g detectors using the\nsame procedure described in Secs. 4.1 to 4.3. The only\nmodification was that the output of each detector was bi-\nnary, i.e., hallucination or no hallucination. The results for\nDHCP-g two-stage serving process on the test set are pre-\nsented in Tab. 3, with corresponding results on the train-\ning set shown in Tab. 16. Additionally, for reference, the\nperformance of the first-stage and second-stage detector of\nDHCP-g alone is provided in Tabs. 14 and 15. These tables\ncollectively demonstrate the effectiveness of the DHCP-g\napproach across different stages of the detection process."}, {"title": "4.6. DHCP-g on Multi-answer Color Quiz Tasks", "content": "Since DHCP-g does not take the answer itself into ac-\ncount, but instead focuses solely on distinguishing be-\ntween hallucinations and non-hallucinations, it can be ef-\nfectively applied to tasks involving open-ended answers. To\nevaluate its performance, we selected 49,875 and 12,469\ncolor-related questions from the VQA v2 dataset to create\nthe COCO-Colortrain and COCO-Colortest sets, respectively.\nDHCP-g one-stage detector was trained on the training set,\nand its hallucination detection results on the test set are pre-\nsented in Tab. 4, with corresponding results on the training\nset shown in Tab. 17. Due to the relatively small size of the\nCOCO-Color dataset and the limited number of hallucina-"}, {"title": "4.7. DHCP-g on Generative Image Caption Tasks", "content": "The color quiz task generates a limited set of possible an-\nswers, so to further evaluate the effectiveness of our ap-\nproach in a more open-ended task, we focused on image\ncaptioning. Specifically, we generated captions for the\nCOCO 2014 images and assessed the presence of halluci-\nnations using the CHAIR scores [30]. The COCO-caption\ndataset was divided into training and test sets with a 9:1\nratio. This setup allowed us to test the robustness of our\nmethod in handling more complex, open-response tasks\nwhere the range of possible outputs is significantly broader\nthan in tasks with constrained answer sets.\nWe present the hallucination detection results of the\nDHCP-g one-stage detector on the test set of the image cap-\ntioning task in Tab. 5, with training set results shown in\nTab. 18. Our DHCP-g method demonstrates superior recall\nand precision in detecting hallucinations in open-ended im-\nage captioning tasks, highlighting its effectiveness in more\nchallenging, generative scenarios.\nSince the hallucination may not always occur in the\nfirst word of the generative captioning task, considering\nthe cross-modal attention across all generative tokens could\npotentially enhance performance. However, our simplified\nDHCP-g method, which considers only the cross-modal at-\ntention of the first token, already achieves strong perfor-\nmance. This result validates the feasibility and effective-\nness of our approach, demonstrating that even a minimal-\nistic model focusing on the initial token's attention can\neffectively and efficiently detect hallucinations in genera-\ntive tasks. This result suggests the potential of the DHCP\nmethod to effectively detect hallucinations in generative"}, {"title": "5. In-depth Analyses on DHCP", "content": "5.1. Effectiveness of DHCP on Other LVLMs and\nMulti-dimensional Hallucination Datasets\nDHCP on MiniGPT-4. To further explore the generaliz-\nability of DHCP, we present the results of DHCP applied\nto MiniGPT-4 [41]. The outcomes for the our two-stage\nDHCP are shown in Tab. 6 (separate first-stage and two-\nstage results in Tabs. 20 and 21). Our DHCP method is\nmodel-agnostic and can be generalized to other LVLMs,\ndemonstrating its flexibility and broad applicability.\nDHCP on AMBER Dataset. The data distribution and\nquestions in POPE are quite homogeneous, primarily focus-\ning on the COCO dataset and using a fixed sentence struc-\nture, i.e., \"Is there a/an object in the image?\". To demon-\nstrate that our DHCP method is not dependent on POPE's\nspecific data distribution or fixed sentence structure, in ad-\ndition to the POPE and POPE-Extended, we use a multi-\ndimensional hallucination benchmark AMBER [34], which\nevaluates not only object hallucinations, but also attribute\nand relation hallucinations. We use discriminative queries\nin AMBER with about 14,000 image-query pairs, which\nwas divided into an 8:2 training set and test set. To ex-\nplore whether DHCP is merely an overfitting of the POPE\ndata distribution, we trained DHCP-d's first-stage hallu-\ncination detector using a mixture of POPE-Extendedtrain"}, {"title": "5.2. Exploration on Hallucination Mitigation using\nDHCP for Discriminative Yes/No Tasks", "content": "Our DHCP method primarily focuses on hallucination de-\ntection. However, in the context of discriminative Yes/No\ntasks where answers are binary, we can easily mitigate hal-\nlucinations by simply flipping the answers for the hallucina-\ntions detected by DHCP. To evaluate this approach, we con-\nducted experiments on the POPE dataset using two-stage\nDHCP-d and DHCP-g. The results are presented in Tab. 7.\nBoth DHCP-d and DHCP-g demonstrated improved perfor-\nmance in hallucination mitigation. Notably, our models out-\nperform several recent popular approaches, as demonstrated\nin Tab. 24 in the Appendix. While hallucination mitigation\nis not the central focus of this work, these results indicate\nthe potential effectiveness of DHCP in addressing halluci-\nnations. This will be a key area in our future research."}, {"title": "5.3. False Alarm Samples of DHCP are Also Risky", "content": "Table 1 shows that DHCP-d first-stage detector has a rela-\ntively high false alarm rate for hallucination samples. Our\nfurther analysis demonstrates that lots of such samples are,\nin fact, \"unconfident\" samples, which should also be care-\nfully considered. For the discriminative Yes/No questions,\nthe LVLM model has a 50% probability of randomly and\nluckily guessing the correct \"yes\" or \"no\" as the answer\neven in the hallucination state. Answering correctly does\nnot necessarily mean that the LVLM actually understood"}, {"title": "5.4. In-depth Analysis on Different Data Types and\nCauses of LVLM Hallucinations", "content": "There are three clusters in POPE [22]: random, popular,\nand adversarial. They have different sources of negative ex-\namples, suggesting potentially different causes for the \u201cob-\nserving something out of nothing\u201d hallucinations: (a) Pop-\nular negative examples are derived from objects that appear"}, {"title": "6. Conclusion", "content": "In this paper, we propose a hallucination detection method\nDHCP based on the cross-modal attention of LVLMs. De-\nspite its simplicity and lack of need for additional training\nor inference for LVLMs, DHCP effectively detects halluci-"}, {"title": "7. Details of Generating POPE-Extended", "content": "We followed the same methodology as POPE [22] to gener-\nate a larger dataset, POPE-Extended. Specifically, we found\na total of 22,670 images with no fewer than three objects\nin the COCO-val2014 dataset. We divided these images\ninto training and test sets in a ratio of 80% to 20%, ensuring\nthat all the images in POPE were in the test set to prevent\nthe training process from using them. Then, we obtained a\ntraining set, POPE-Extendedtrain, with 18,136 images, and\na test set, POPE-Extendedtest, with 4,534 images. Using\na POPE-like approach, these datasets can be divided into\nthree clusters: random, popular, and adversarial. Each clus-\nter corresponds to questions totaling six times the number\nof images, with a balanced setting of half being positive ex-\namples and half being negative examples. Constructing a\nlarger POPE-Extended dataset allows us to effectively train\nthe hallucination detector and provides a more robust frame-\nwork for validating the effectiveness of our method."}, {"title": "8. Limitations, Future Work and Motivation", "content": "8.1. Limitations and Future Work\nWe acknowledge the following limitations and outline fu-\nture directions for our study:\n(1) Simple MLP hallucination detector: Our current\napproach employs a straightforward yet effective two-stage\nmulti-layer perceptron (MLP) for hallucination detection.\nThis decision is informed by two primary considerations:\n(a) Our core innovation lies in identifying key indicators of\nhallucination and non-hallucination states in large vision-\nlanguage models (LVLMs), such as variations in cross-\nmodal attention patterns. We contend that the use of com-\nplex classifiers is unnecessary, as our simple, effective,\nand parameter-efficient two-stage MLP model has already\ndemonstrated excellent performance in hallucination de-\ntection across datasets such as POPE, COCO-Color, and\nCOCO-Caption. (b) Furthermore, our uncomplicated ap-\nproach provides significant advantages in terms of compu-\ntational efficiency. Nevertheless, we intend to investigate\nmore sophisticated classification strategies in the future to\nfurther enhance the detection of hallucinations.\n(2) Consider cross-modal attention of the first gen-\nerated token: Our current approach exclusively utilizes\ncross-modal attention to image tokens during the gener-\nation of the initial token. This methodology is suitable\nfor shorter-answer visual question answering (VQA) tasks.\nHowever, for longer-answer image captioning tasks, a more\neffective strategy would involve incorporating cross-modal\nattention at each token generation. Additionally, it would\nbe beneficial to train the detector to differentiate between\ngenerated tokens that are hallucinations and those that are\nnot.\n(3) Exploring the mitigation of hallucinations: Our\napproach centers on the detection of hallucinations in\nLVLMs and demonstrates commendable performance\nacross a variety of tasks. For discriminative yes/no ques-\ntions, our method effectively mitigates hallucinations by\naltering the provided answer. However, in the context\nof open-scene question and answer (Q&A) scenarios, we\nhave yet to investigate solutions for hallucination mitiga-\ntion through the application of our method. Addressing this\ngap will be a focus of our future research.\n8.2. Motivation\nWe summarise our motivation as follows:\n(1) The general idea of DHCP. To effectively detect\nhallucinations in LVLMs, it is essential to identify the dif-\nferences between the hallucination and non-hallucination\nstates. Notably, DHCP has identified a significant varia-\ntion in the cross-modal attention patterns during large lan-\nguage model (LLM) decoding when an LVLM is experi-\nencing hallucinations compared to when it is not. Conse-\nquently, we have developed a framework aimed at detecting\nand mitigating hallucinations, leveraging these cross-modal\nattention patterns.\n(2) Motivation for using a two-stage detectors. Our\ndetector, which is based on cross-modal attention pat-\nterns, is structured as a two-stage process, taking into ac-\ncount several key considerations. One primary concern is\nthe imbalance in the number of hallucination versus non-\nhallucination samples within the training set. Given that\nhallucinations in large vision-language models (LVLMs)\nare relatively rare, we have amassed a significantly greater\nnumber of non-hallucination samples than hallucination\nsamples, with the ratio reaching as high as 9.45:1 for in-\nstances where the answer is \u201cYes\u201d. Even with the applica-\ntion of oversampling or undersampling techniques, achiev-\ning both high recall and precision with a single classifier\nremains challenging due to the data imbalance. In our two-\nstage detector design, the first-stage detector is optimized\nfor high recall of hallucination samples, though it achieves\na precision rate of only 50-60%. The second-stage detector,\non the other hand, is trained using the samples identified as\nhallucinations in the first stage. At this point, the ratio of"}]}