{"title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding", "authors": ["Heqing Zou", "Tianze Luo", "Guiyang Xie", "Victor (Xiao Jie) Zhang", "Fengmao Lv", "Guangcong Wang", "Juanyang Chen", "Zhuochen Wang", "Hansheng Zhang", "Huaijian Zhang"], "abstract": "The integration of Large Language Models (LLMs) with visual encoders has recently shown promising performance in visual understanding tasks, leveraging their inherent capability to comprehend and generate human-like text for visual reasoning. Given the diverse nature of visual data, MultiModal Large Language Models (MM-LLMs) exhibit variations in model designing and training for understanding images, short videos, and long videos. Our paper focuses on the substantial differences and unique challenges posed by long video understanding compared to static image and short video understanding. Unlike static images, short videos encompass sequential frames with both spatial and within-event temporal information, while long videos consist of multiple events with between-event and long-term temporal information. In this survey, we aim to trace and summarize the advancements of MM-LLMs from image understanding to long video understanding. We review the differences among various visual understanding tasks and highlight the challenges in long video understanding, including more fine-grained spatiotemporal details, dynamic events, and long-term dependencies. We then provide a detailed summary of the advancements in MM-LLMs in terms of model design and training methodologies for understanding long videos. Finally, we compare the performance of existing MM-LLMs on video understanding benchmarks of various lengths and discuss potential future directions for MM-LLMs in long video understanding.", "sections": [{"title": "1 Introduction", "content": "Large Language Models have demonstrated remarkable versatility and capability in understanding and generating human-like text by scaling model size and training data (Raffel et al., 2020; Brown, 2020; Chowdhery et al., 2023; Touvron et al., 2023a). To extend these capabilities to visual understanding tasks, various methods have been proposed to integrate LLMs with specific visual modality encoders, thereby endowing LLMs with visual perception abilities (Alayrac et al., 2022; Li et al., 2023a). Single images or multiple frames are encoded as visual tokens and integrated with textual tokens to help MM-LLMs achieve visual understanding. For long-video understanding, MM-LLMs (Dai et al., 2023; Liu et al., 2024c) are designed to process a larger number of visual frames and diverse events, enabling a wide range of real-world applications such as automatically analyzing highlight reels from sports videos, movies, surveillance footage, and egocentric videos in embodied AI. For example, a robot could learn to make a cup of coffee from a long egocentric video. It needs to analyze key events from the long video including: 1) measuring one to two tablespoons of ground coffee for every 6 ounces of water; 2) adding water to the coffee maker's reservoir; 3) putting the coffee grounds into the filter basket; and 4) starting the coffee maker and waiting for it to brew. Modeling long-form videos with complex spatiotemporal details and dependencies remains a challenging problem (Wang et al., 2023a; Mangalam et al., 2024; Xu et al., 2024b; Wu et al., 2024).\nThere are substantial differences between long"}, {"title": "2 Long Video Understanding", "content": "Due to the inherent differences between long video understanding and image or short video understanding, including the presence of various events with more frames and dynamic scenarios, the task of long video understanding presents additional challenges for visual comprehension."}, {"title": "2.1 Visual Reasoning and Understanding", "content": "Visual reasoning demands models to comprehend and interpret visual information and integrate multimodal perception with commonsense understanding (Johnson et al., 2017; Chen et al., 2024c). There are three main types of visual reasoning tasks: visual question answering (VQA), visual captioning (VC) or description (VD), and visual dialog (VDia). VQA (Antol et al., 2015; Zakari et al., 2022) involves generating a natural language answer based on the input visual data and accompanying questions. VC and VD systems (Vinyals et al., 2015; Sharma et al., 2018; Li et al., 2019) typically generate a concise, natural language sentence that summarizes the main content of the visual data and a detailed and comprehensive description of the corresponding visual data, respectively. VDia (Das et al., 2017; Qi et al., 2020) involves multi-turn conversations, consisting of a series of question-answer pairs centered around the visual content.\nImage understanding. As illustrated in Fig. 3 (a), image understanding tasks involve a single image for various visual reasoning tasks, such as image captioning and image-centered question answering (Sharma et al., 2018; Mathew et al., 2021; Changpinyo et al., 2022; Li et al., 2023a; Chen et al., 2024a). These tasks focus solely on spatial information, encompassing both coarse-grained understanding (Ordonez et al., 2011; Sohoni et al., 2020) of global visual context and fine-grained understanding (Wei et al., 2021; Liu et al., 2024b; Peng et al., 2024) of local visual details.\nShort video understanding. Unlike image understanding tasks, which involve only static visual data, short video understanding also incorporates temporal information from multiple visual frames (Xu et al., 2016; Bain et al., 2021; Li et al., 2023b, 2024e). In addition to spatial reasoning (Ranasinghe et al., 2024), within-event temporal reasoning and spatiotemporal reasoning across frames play crucial roles for short video understanding (Huang et al., 2018; Lin et al., 2019; Diba et al., 2023).\nLong video understanding. Long videos, spanning minutes or even hours, typically consist of multiple events, encompassing much richer spatial content and temporal variations compared to short videos (Mangalam et al., 2024; Li et al., 2024f; Song et al., 2024a,b). As summarized in Fig. 3 (c), long video understanding involves not only spatial and within-event temporal reasoning but also between-event reasoning and long-term reasoning from different video events (Wu et al., 2019; Wu and Krahenbuhl, 2021; Wang et al., 2023a; Zhou et al., 2024; Fang et al., 2024)."}, {"title": "2.2 Challenges of Long Video Understanding", "content": "Compared with images and short videos, long-form videos introduce new challenges to comprehensive visual understanding, as follows:\nRich fine-grained spatiotemporal details. Long videos, which cover a wide range of topics, scenes, and activities, contain varying details such as objects, events, and attributes (Fu et al., 2024a; Wu et al., 2024). These details are much richer compared to static images and short videos with multiple similar frames, making long video understand-"}, {"title": "3 Advances in Model Architecture", "content": "In this section, we discuss the advances of MM-LLMs from image-targeted to long-video-targeted models, from the perspective of model architecture. As illustrated in Fig. 4, MM-LLMs for images, short videos, and long videos share a similar structure comprising a visual encoder, an LLM backbone, and an intermediary connector. Unlike the image-level connector in image-targeted MM-LLMs, the video-level connector is crucial for integrating cross-frame visual information. In LV-LLMs, designing the connector is more challenging, requiring efficient compression of amounts of visual information and incorporating temporal knowledge to manage long-term correlations."}, {"title": "3.1 Visual Encoder and LLM Backbone", "content": "MM-LLMs, encompassing both image-targeted and video-targeted models, typically utilize similar visual encoders for visual information extraction. LLM backbones are also universal in early MM-LLM methods, while existing LV-LLMs tend to use long-context LLMs in the implementation.\nVisual encoder. The pretrained visual encoders are responsible for capturing vision knowledge from raw visual data. As summarized in Table 1, image encoders like CLIP-ViT-L/14 (Radford et al., 2021), EVA-CLIP-ViT-G/14 (Sun et al., 2023), OpenCLIP-ViT-bigG/14 (Cherti et al., 2023), and SigLIP-SO400M (Zhai et al., 2023) are widely uti-\nLLM backbone. The LLM is the core module in visual understanding systems, inheriting properties of reasoning and decision-making. Compared to closed-source LLMs like GPT-3/4 (Brown, 2020; Achiam et al., 2023) and Gemini-1.5 (Reid et al., 2024), various open-source LLMs are more commonly used in implementing visual LLMs. These include Flan-T5 (Chung et al., 2024), LLaMA (Touvron et al., 2023b,c; Dubey et al., 2024), Vi-"}, {"title": "3.2 Modality Interface", "content": "The connectors between visual encoders and LLMS serve as a modality interface, mapping visual features to the language feature space. Given the variability in visual data sources, these connectors can be categorized into image-level, video-level, and long-video-level connectors.\nImage-level connectors. Image-level connectors are used to map image features to the language space for processing raw visual tokens, and they are widely used in both image-targeted and video-targeted MM-LLMs. These connectors can be categorized into three groups: The first group directly uses a single linear layer (Liu et al., 2024c) or a multi-layer perceptron (MLP) (Liu et al., 2024a) to map image features into the language embedding space. However, this method, which retains all visual tokens, is not suitable for visual understanding tasks involving multiple images. To address the limitations of retaining all visual tokens, the second group employs various pooling-based methods. These include spatial pooling (Maaz et al., 2023), adaptive pooling (Xu et al., 2024a), semantic-similar token merging (Jin et al., 2024), and adjacent token averaging (Zhang et al., 2024e; Li et al., 2024c). The third group utilizes cross-attention or transformer-based structures, such as Q-Former (Li et al., 2023a) and Perceiver Resampler (Jaegle et al., 2021), for image feature compression. Q-Former is a lightweight transformer structure that employs a set of learnable query vectors to extract and compress visual features. Many visual LLMs (Dai et al., 2023; Li et al., 2023b; Ma et al., 2023a; Liu et al., 2024e), following BLIP-2, choose the Q-Former-based connector. Other visual LLMs (Ma et al., 2023b; Jiang et al., 2024) opt for the Perceiver Resampler to reduce computational burden by extracting patch features.\nVideo-level connectors. Video-level connectors are used for extracting sequential visual data and further compressing visual features. Compared to the solely image-level connectors in image-targeted MM-LLMs, video-level connectors are essential for video-targeted MM-LLMs, including LV-LLMs. Some methods directly concatenate image tokens before inputting them to the LLMs, making them sensitive to the number of frame images (Dai et al., 2023; Lin et al., 2023). Similar struc-"}, {"title": "4 Advances in Model Training", "content": "Multimodal LLMs for visual understanding consist of two principal stages: pre-training (PT) for vision-language feature alignment and instruction-tuning (IT) for instruction-aware response."}, {"title": "4.1 Pre-training", "content": "Vision-language pre-training for MM-LLMs aims to align visual features with the language space using text-paired data. This includes pre-training with image-, short-video-, and long-video-text datasets. Initially introduced for visual LLMs focused on images, image-text pre-training is also widely used in video-related understanding tasks. Coarse-grained image-text pair datasets, such as COCO Captions (Chen et al., 2015) and CC-3M (Sharma et al., 2018), are employed for global vision-language alignment. Fine-grained image-text datasets like ShareGPT4V-PT (Chen et al., 2023b) are used for locally spatial semantics alignment. Given the limited changes in semantic content of short videos, short-video-text paired datasets, such as Webvid-2M (Bain et al., 2021), can be used similarly for short-video-text pre-training. Similarly, long-video-text pre-training is important to"}, {"title": "4.2 Instruction-tuning", "content": "Instruction-tuning with vision-language sources enables LLMs to follow instructions and generate human-like text. Multimodal vision-language instruction-following data (Dai et al., 2023; Liu et al., 2024c), including both image-text and video-text pairs, are used to align multimodal LLMs with human intent, thereby enhancing their ability to complete real-world tasks.\nSimilar to the pre-training stage, image-text instruction-tuning is also employed in various vision-understanding tasks, including image, short-video, and long-video understanding tasks. Basic image-based instruction-following datasets, such as ShareGPT4V-Instruct (Chen et al., 2023b) and LLaVA-Instruct (Liu et al., 2024c), provide high-quality instruction-tuning data for basic spatial rea-soning and chat capabilities. For video-related LLMs, short-video-text instruction-tuning is necessary to enable multimodal LLMs to understand temporal sequences, as seen in models like Video-ChatGPT (Maaz et al., 2023) and VideoChat (Li et al., 2023b). Short-video-LLMs require both spatial and within-event reasoning instructions to understand the spatial and small-scale temporal content of short videos. However, the limited content and semantic changes in short videos are insufficient for long video understanding tasks, where frames are more numerous and exhibit significant variation. Long-video-text instruction-tuning is specifically introduced to better capture and understand long videos. In addition to spatial and within-event reasoning instructions, between-event and long-term reasoning instructions are necessary for the comprehensive understanding of long videos, as shown in Fig. 5 (b). Among the introduced long-"}, {"title": "5 Evaluation, Performance and Analysis", "content": "In this section, we present a performance comparison across popular evaluation datasets featuring videos of varying lengths, along with our analysis. The comparison is conducted from two perspectives: First, we evaluate video understanding methods on tasks with video lengths ranging from seconds to minutes. Second, we specifically compare performance on extra-long video datasets, with video lengths ranging from minutes to hours."}, {"title": "5.1 Video Understanding: Seconds to Minutes", "content": "As shown in Table 2, we summarize the general video understanding performance of various visual LLMs on open-ended video question answering benchmarks, including TGIF-QA (Jang et al., 2017), MSVD-QA, MSRVTT-QA (Xu et al., 2017), NEXT-QA (Xiao et al., 2021), and ActivityNet-QA (Yu et al., 2019). Additionally, we consider the VideoChatGPT-introduced video-based generative performance benchmark (Maaz et al., 2023), which evaluates five aspects of video-based text generation: Correctness of Information (CI), Detail Orientation (DO), Context Understanding (CU), Temporal Understanding (TU), and Consistency (CO). The video benchmarks with lengths shorter than 1"}, {"title": "5.2 Video Understanding: Minutes to Hours", "content": "To address the unique characteristics of long videos, several long video benchmarks have been introduced in recent years, with video lengths varying from hundreds of seconds to thousands of seconds. EgoSchema (Mangalam et al., 2024) and QVHighlights (Lei et al., 2021) are long-form video understanding datasets designed for multiple-choice question answering and highlight detection, respectively, after accessing all frames. VideoVista (Li et al., 2024f), MMBench-Video (Fang et al., 2024), and MLVU (Zhou et al., 2024) cover various topics and are designed for fine-grained capability evaluation. LongVideoBench (Wu et al., 2024) introduces referring reasoning questions to address the longstanding issue of single-frame bias in long videos. Video-MME (Fu et al., 2024a) and LVBench (Wang et al., 2024b) contain numerous hour-level videos. Video-MME further categorizes them into short, medium, and long categories, while LVBench aims to challenge models to demonstrate long-term memory and extended comprehension capabilities."}, {"title": "6 Future Directions", "content": "As summarized above, existing long video understanding methods are less effective than image or short video understanding methods. To meet the"}, {"title": "6.1 More Long Video Training Resources", "content": "The two-stage training pipeline\u2014comprising cross-modal alignment pre-training and visual-language-format instruction tuning\u2014is widely employed for training MM-LLMs (Dai et al., 2023; Liu et al., 2024c). However, fine-grained long-video-language training pairs are lacking compared to the commonly used image-language and short-video-language pairs during cross-modal alignment learning (Song et al., 2024b; Qiu et al., 2024). Methods that rely on image-language and short-video-language resources cannot capture long-term corre-lations during the pre-training stage (Zhang et al., 2024d). Additionally, the video length of newly introduced long-video instruction data is limited to the minute level, which significantly restricts the application scenarios for effective reasoning from long video understanding (Li et al., 2023c). Therefore, both long-video-language paired pre-training datasets and long-video-instruction datasets, fea-turing much longer videos at the hour level with high-quality annotations, need to be created."}, {"title": "6.2 More Challenging Long Video Understanding Benchmarks", "content": "Various video understanding benchmarks were summarized in the previous section, most of which have been introduced recently. However, these benchmarks focus on one or several aspects of long video understanding, such as LongVideoBench for long-context interleaved video understanding (Wu et al., 2024), QVHighlights for language-based"}, {"title": "6.3 Powerful and Efficient Frameworks", "content": "Visual LLMs for videos need to support more visual frames and preserve more visual details with a fixed number of visual tokens. There are four main considerations for the implementation of LV-LLMs: (1) Select long-context LLMs as the LLM backbones. Previous methods have suffered from the context capacity of LLMs and have had to specifically fine-tune an LLM to support more tokens (Zhang et al., 2024d). Recent long-context LLMs,"}, {"title": "6.4 More Application Scenarios", "content": "Long video understanding with large models faces several key challenges for more long video applications. Contextual understanding is critical, as long videos require models to maintain temporal coherence and contextual awareness over extended periods (He et al., 2024). Real-time processing (Karim et al., 2024) is essential for applications like surveillance, live event analysis, and embodied AI, necessitating the development of low-latency models capable of processing video streams in real-time. Multi-modal integration is another frontier, as long videos often contain audio, text, and visual information (Zhang et al., 2023; Cheng et al., 2024b). Future models should better integrate these modalities to enhance understanding and provide a more holistic analysis of video content."}, {"title": "7 Conclusion", "content": "In this paper, we summarize the advances of visual LLMs from images to long videos. Based on the analysis of the task differences among image understanding, short video understanding, and long video understanding, we identify the key challenges of long video learning. These challenges include capturing more fine-grained spatiotemporal details and long-term dependencies within compressed visual information from dynamic sequential events with scene transitions and content changes. We then introduce the advances in model architecture and"}, {"title": "8 Limitation", "content": "We reviewed literature on comprehensive long video understanding, covering methods, training datasets, and benchmarks. Due to space constraints, we omit detailed application scenarios like realtime processing and multimodal tasks. We will maintain an open-source repository and add these contents to complement our survey. The performance comparisons are based on final results from previous papers and official benchmarks, which vary in training resources, strategies, and model architectures, making it difficult to analyze specific models and training differences. We plan to conduct detailed ablation studies on public benchmarks for a more direct analysis of model design, training resources, and methods."}, {"title": "A Example Appendix", "content": "This is an appendix."}]}