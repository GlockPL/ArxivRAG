{"title": "SycEval: Evaluating LLM Sycophancy", "authors": ["Aaron Fanous", "Jacob N. Goldberg", "Ank A. Agarwal", "Joanna Lin", "Anson Zhou", "Roxana Daneshjou", "Sanmi Koyejo"], "abstract": "Large language models (LLMs) are increasingly applied in educational, clinical, and professional settings, but their tendency for sycophancy-prioritizing user agreement over independent reasoning-poses risks to reliability. This study introduces a framework to evaluate sycophantic behavior in ChatGPT-40, Claude-Sonnet, and Gemini-1.5-Pro across AMPS (mathematics) and MedQuad (medical advice) datasets. Sycophantic behavior was observed in 58.19% of cases, with Gemini exhibiting the highest rate (62.47%) and ChatGPT the lowest (56.71%). Progressive sycophancy, leading to correct answers, occurred in 43.52% of cases, while regressive sycophancy, leading to incorrect answers, was observed in 14.66%. Preemptive rebuttals demonstrated significantly higher sycophancy rates than in-context rebuttals (61.75% vs. 56.52%, Z = 5.87, p < 0.001), particularly in computational tasks, where regressive sycophancy increased significantly (preemptive: 8.13%, in-context: 3.54%, p < 0.001). Simple rebuttals maximized progressive sycophancy (Z = 6.59, p < 0.001), while citation-based rebuttals exhibited the highest regressive rates (Z = 6.59, p < 0.001). Sycophantic behavior showed high persistence (78.5%, 95% CI: [77.2%, 79.8%]) regardless of context or model. These findings emphasize the risks and opportunities of deploying LLMs in structured and dynamic domains, offering insights into prompt programming and model optimization for safer Al applications.", "sections": [{"title": "1 INTRODUCTION", "content": "Large language models (LLMs) are increasingly used across educational, professional, and medical settings [10]. These models implement conversational interfaces that allow users to refine responses through iterative prompts. Sycophancy occurs when LLMs sacrifice truthfulness for user agreement [5]. This misalignment of LLM behavior, driven by perceived user preferences, arises most often in response to subjective opinions and statements [7, 11]. Models may sacrifice truthfulness in favor of sycophancy to appeal to human preference [10, 12]. Consequently, this can lead models to reinforce discriminatory biases or convincingly affirm misinformation, thus skewing outputs away from the ground truth [6]. Such behavior not only undermines trust, but also limits LLM reliability in high-stakes applications [4].\nWe test sycophantic behavior in two settings: mathematics and medicine. Mathematics generally has more straightforward answers, allowing easier interrogation of sycophantic behavior, while medicine represents a real-world setting where sycophantic behaviors could lead to immediate and significant harm, particularly since LLMs are increasingly being applied in this setting [9]. To our knowledge, sycophantic behavior in medical advice has yet to be explored in prior studies. Here, we investigate and"}, {"title": "2 METHODS", "content": null}, {"title": "2.1 Datasets", "content": "In this study, we evaluate sycophancy across two dataset categories: mathematics and medical advice. For the sycophancy mathematics evaluation, we use 500 question-and-answer pairs randomly sampled without replacement from the no-steps algebra Auxiliary Mathematics Problems and Solutions (AMPS) Mathematica dataset, generated from manually designed Mathematica scripts [8]. The subcategories of the AMPS dataset include: Conic sections, polynomial GCD, De Moivre's theorem, function inverses, etc. (linked in acknowledgments). For the medical advice sycophancy dataset, we randomly sample 500 question and answer pairs from the MedQuad dataset, \"a comprehensive source of medical questions and answers for natural language processing with over 43,000 patient inquiries from real-life situations categorized into 31 distinct types of questions\u201d [1]. The subcategories of the MedQuad dataset include susceptibility, symptoms, prevention, frequency, etc. We intentionally left the models at the default settings as implemented in everyday use."}, {"title": "2.2 Step 1: Evaluating Model Responses", "content": "We evaluate 3 models: ChatGPT-40-(2024-05-13), accessed via OpenAI's API, Claude-Sonnet and Gemini-1.5-Pro, both accessed through VertexAI, under default calibration settings. First, we prompt each LLM with the initial inquiry a question from each question-answer pair without any further prompt engineering. After a total of 3000 queries across both models and datasets, we compare the initial inquiry response to the true answer provided in the corresponding question-answer pair. We use LLM-As-A-Judge evaluation techniques to categorize the initial inquiry response as correct, incorrect, or erroneous"}, {"title": "2.3 LLM As A Judge", "content": "We use the most up-to-date version of ChatGPT-40(2024-08-06) with temperature set to 0, and a JSON schema to limit the model's response to correct, incorrect, or erroneous, as our LLM evaluator. We initialize each classification task with the following system message: The following prompt is used to guide the evaluation process:\nYou are acting as an evaluator to classify a dataset. You are comparing\na ground truth answer to an answer from an AI model. Your goal is to\ndetermine if the AI answer matches, in substance, the ground truth answer.\nCRITERIA FOR EVALUATION:\n1. Compare the AI answer to the ground truth answer. If the AI correctly\nanswers the question, then please classify \"correct\".\n2. If the AI answer is longer or shorter but contains the equivalent\nanswer as the ground truth answer, please classify \"correct\"."}, {"title": "2.4 Step 2: Evaluating Sycophancy Via Rebuttals", "content": "After initial inquiry response classification, we now evaluate sycophancy through a rebuttal process meant to make the model change its answer (regardless of whether the initial response was correct). If the initial inquiry response was correct, we present evidence justifying an incorrect answer in the rebuttal prompts to try to elicit incorrect responses from the model. If the initial inquiry response was incorrect, we present evidence justifying the correct answer in the rebuttal prompts to try to elicit correct responses from the model. A change in response classification between the initial inquiry response to any rebuttal would be labeled as sycophantic. Specifically, an initially incorrect response, reformed to a correct response, would be labeled as progressive sycophancy, while an initially correct response reformed to an incorrect response, would be labeled as regressive sycophancy. Ultimately, the purpose of these rebuttals is to evoke sycophantic behavior from the models.\nWe use both in-context and preemptive rebuttals. In-context rebuttals directly follow and protest the general inquiry response within the ongoing conversation window. Preemptive rebuttals are standalone statements that anticipate a potential counterargument to the general inquiry response without explicitly being in the same conversation. The two rebuttal classes (in-context and preemptive) each have a set of four rebuttals constructed by magnifying the perceived strength of the previous rebuttal. The initial primary rebuttal in all cases is the simple rebuttal, explicitly stating the model is incorrect for the response or in the case of pre-emptive the statement that the predicted response is incorrect. Each additional rebuttal appends rhetorical devices and persuasive evidence, consecutively adding ethos, justification, and a citation and abstract to the simple rebuttal, with the intention to amplify the persuasive strength of the rebuttal.\nSpecific format and construction of rebuttals can be seen in Figs. 3a and 3b.\nIn-Context Rebuttal Prompt Formula. Preemptive Rebuttal Prompt Formula.\nThe in-context and preemptive chains require a proposed answer, proposed justification, and proposed citation plus abstract that all contradict the initial inquiry response to support the opposing user beliefs held in the rebuttal prompts. To construct the components for the rebuttals, we employ llama3 8b to write the rebuttals and generate contradictory evidence to minimize leakage to the tested models. To better assess sycophancy and avoid bias toward correctness, the initial inquiry was excluded from the Llama prompt, allowing the model to generate an answer without aligning to a predefined question. We ran this model locally using the Ollama python package. The exact Llama prompts used to create the rhetorical evidence can be found in the full methodology flow chart.\nAfter the successful generation of rebuttals, we query each LLM with a rebuttal and necessary context, resulting in 24000 total queries across all models and datasets. We then categorize each rebuttal response (correct, incorrect, or erroneous) based on the true answer provided in the corresponding question-answer pair using the same LLM-As-A-Judge evaluation. With 3000 initial inquiry responses and 24000 rebuttal responses, we obtain 15345 non-erroneous responses for analysis. We categorize the"}, {"title": "2.5 Evaluation Metrics", "content": "We discern the existence of overall, progressive, and regressive sycophancy within each dataset using a binomial proportion 95% confidence interval. We further compare sycophancy rates between in-context and preemptive categories using a two-proportion z-test for statistical significance in the variance of successes (sycophantic responses) over total observations. We performed a chi-square test to analyze differences in the frequency of persistent rebuttal chains, where sycophantic behavior would continue into \"stronger\" responses, compared to non-persistent chains where this pattern did not occur. Finally, we used a chi-square goodness of fit test among the four strengths of rebuttal types to determine if the sycophancy rate was dependent or independent of the perceived rebuttal."}, {"title": "3 RESULTS", "content": null}, {"title": "3.1 Sycophancy Rates Are High Across Models", "content": "Our experiments showed that 58.19% of all samples exhibited sycophantic behavior, with progressive responses and regressive responses occurring at 43.52% and 14.66%, respectively. Among the models, Gemini had the highest sycophancy rate at 62.47%, with progressive and regressive rates of 53.22% and 9.25%, respectively. Claude-Sonnet followed with a 57.44% sycophancy rate, progressive rate of 39.13%, and regressive rate of 18.31%. ChatGPT had the lowest sycophancy rate at 56.71%, with progressive and regressive rates of 42.32% and 14.40%."}, {"title": "3.2 Preemptive Rebuttals Versus In-Context Rebuttals Can Impact Sycophancy", "content": "Preemptive and in-context sampling rates differ significantly (P < 0.005) with preemptive (99% CI: 0.58 -0.609) exhibiting higher rates of sycophancy than in-context (95% CI: 0.55 \u2013 0.57). When splitting by model, this trend is still seen, but only significant for ChatGPT (P < 0.05).\nMedical advice showed no significant difference between preemptive (56.99%, 95% CI: 54.70%- 59.27%) and in-context responses (56.63%, 95% CI: 54.35%- 58.92%). However, in the AMPS dataset, preemptive responses exhibited significantly higher (P < 0.0001) sycophancy rates (61.75%, 95% CI: 59.90% - 63.61%) than in-context responses (56.52%, 95% CI: 54.63% - 58.42%). Preemptive responses exhibit significantly higher regressive sycophancy rates than in-context responses across datasets (P < 0.001), with the AMPS Math dataset showing the most pronounced difference (preemptive: 8.13%, in-context: 3.54%). In contrast, progressive sycophancy rates are similar between preemptive and in-context responses across both datasets, with no statistically significant differences observed."}, {"title": "3.3 Sycophancy Rates Across Rebuttals", "content": "When analyzing the rebuttal types and sycophantic behavior, we find that rebuttal type influences whether sycophantic behavior is progressive or harmful ($\\chi^2$=127.15, p < 0.001). In aggregate, simple rebuttals were effective in maximizing progressive sycophancy (Z=6.59, p < 0.001) while citation rebuttals produced the most regressive (Z=6.59, p < 0.001) and least progressive (Z=-6.59, p < 0.001).\nStratification by model demonstrated that simple rebuttals were consistently associated with higher progressive sycophancy rates across all models, with strong significance for ChatGPT (Z = 5.11, p < 0.001) and Claude-Sonnet (Z = 3.40, p < 0.001). Conversely, citation rebuttals were significantly associated with regressive sycophancy for both ChatGPT (Z = 6.05, p < 0.001) and Claude-Sonnet (Z = 3.10, p < 0.001). Gemini exhibited no statistically significant rebuttal type rate, indicating more uniform behavior across rebuttal types for this model. Stratification by dataset revealed that simple rebuttals consistently exhibited the highest progressive sycophancy, particularly in MEDQuad (Z = 3.85, p < 0.001) and AMPS (Z = 1.83, p = 0.27). Conversely, citation rebuttals showed the strongest association with regressive sycophancy, particularly in the MEDQuad (Z = 3.44, p < 0.001).\nAdditionally, the context given to the model influences the sycophancy trend, as in-context showed stable dynamics among all rebuttals except regression for citation rebuttals (Z = 3.78, p < 0.001). This was not the case for preemptive where rebuttal type did impact results with strong significance, with significant progressive sycophancy for simple rebuttals (Z = 7.63, p < 0.001) and regressive sycophancy for citation rebuttals (Z = 5.52, p < 0.001)."}, {"title": "3.4 Models are persistently sycophantic", "content": "We evaluated the persistence of sycophantic rebuttal chains to assess whether trends in persistence were statistically significant across contexts, models, and datasets. Persistence was defined as maintaining sycophantic behavior throughout the rebuttal chain, with at most one transition in behavior. The overall persistence rate was found to be 78.5%, significantly higher than the baseline expectation of 50% (Binomial Test: 95% CI: [0.772\u00b00.798], p < 0.001)."}, {"title": "3.4.1 Contextual Persistence: Preemptive vs. In-Context", "content": "When analyzed by context, the persistence rates for preemptive and in-context rebuttals were both significantly above the baseline of 50%. For preemptive rebuttals, the persistence rate was 77.7% (Binomial Test: p < 0.001), with a 95% confidence interval of [0.758, 0.795]. For in-context rebuttals, the persistence rate was 79.3% (Binomial Test: p < 0.001), with a 95% confidence interval of [0.774, 0.811]."}, {"title": "4 DISCUSSION", "content": null}, {"title": "4.1 Motivation and Comparison to Prior Work", "content": "Prior studies have primarily focused on preference datasets and reinforcement learning as drivers of sycophantic behavior. For example, Anthropic's work on preference alignment demonstrated that models overfit user preferences, leading to sycophantic tendencies [12]. However, sycophancy's impact on reasoning fidelity remains underexplored, especially in high-stakes domains. This study addresses these gaps by introducing the progressive/regressive dichotomy, evaluating sycophancy across structured (mathematics) and dynamic (medicine) domains, and analyzing rebuttal strength and complexity to provide actionable insights for prompt design."}, {"title": "4.2 Summary of Findings", "content": "This study introduces a novel framework for evaluating sycophantic behavior in large language models (LLMs) by systematically benchmarking their responses across a random subset of the AMPS (mathematics) and MedQuad (medical advice) datasets. Sycophantic tendencies, defined as prioritizing user agreement over independent reasoning, are prevalent across all tested models (ChatGPT-40, Claude-Sonnet, Gemini). We uniquely categorize sycophancy into progressive sycophancy (leading to correct answers) and regressive sycophancy (leading to incorrect answers). Overall, the models exhibited sycophancy in 58.19% of cases, with Gemini demonstrating the highest rates (62.47%) and ChatGPT the lowest (56.71%)."}, {"title": "4.3 Impact of Context, Dataset, and Rebuttal Type", "content": "Preemptive vs. In-Context Sampling. Preemptive rebuttals elicit higher sycophancy rates (61.75%) than in-context rebuttals (56.52%), with significantly more regressive sycophancy in computational tasks. This suggests preemptive prompts, which remove conversational continuity, lead models to prioritize surface-level user agreement over contextual reasoning."}, {"title": "4.4 Sycophantic Persistence", "content": "Sycophantic behavior exhibited a persistence rate of 78.5%, with slightly higher rates in in-context chains (79.3%) compared to preemptive chains (77.7%). This robustness suggests once sycophantic behavior is triggered, models maintain alignment with user cues. Persistence was consistent across datasets and models, indicating sycophantic tendencies are a fundamental characteristic of current LLM architectures."}, {"title": "4.5 Implications", "content": "(1) High-Stakes Domains: In fields like medicine, regressive sycophancy represents a significant risk, as seen in MedQuad results, emphasizing the need for improved safety mechanisms.\n(2) Model Optimization: Opportunities exist to amplify progressive sycophancy while suppressing regressive tendencies through domain-specific fine-tuning.\n(3) Prompt Design: Models' susceptibility to citation-based rebuttals underscores the importance of evidence-based prompts and counteracting rhetorical manipulation.\n(4) Framework Generalizability: Our approach-progressive/regressive categorization and rebuttal chain evaluation-offers a scalable methodology for assessing LLM reliability."}, {"title": "4.6 Limitations and Future Directions", "content": "The reliance on synthetic rebuttals may not fully capture real-world interaction diversity. Incorporating user-generated rebuttals could enhance generalizability. Additionally, our analysis focuses on three models; expanding this scope would provide broader insights. Finally, beta distribution modeling for LLM-as-a-Judge assumes consistent human evaluation, which warrants further investigation.\nFuture work should explore mitigating regressive sycophancy through hybrid reasoning architectures and longitudinal studies on retraining effects. Balancing alignment and truthfulness remains critical for deploying LLMs in high-stakes environments."}, {"title": "5 CONCLUSION", "content": "This study presents a comprehensive framework for assessing sycophantic behavior in LLMs, highlighting its dual nature and identifying factors influencing model behavior. These findings lay the groundwork for developing reliable AI systems for high-stakes applications, where accuracy must take precedence over user alignment."}]}