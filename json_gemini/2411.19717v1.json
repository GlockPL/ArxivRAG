{"title": "MonoPP: Metric-Scaled Self-Supervised Monocular Depth Estimation by Planar-Parallax Geometry in Automotive Applications", "authors": ["Gasser Elazab", "Torben Gr\u00e4ber", "Michael Unterreiner", "Olaf Hellwich"], "abstract": "Self-supervised monocular depth estimation (MDE) has gained popularity for obtaining depth predictions directly from videos. However, these methods often produce scale-invariant results, unless additional training signals are provided. Addressing this challenge, we introduce a novel self-supervised metric-scaled MDE model that requires only monocular video data and the camera's mounting position, both of which are readily available in modern vehicles. Our approach leverages planar-parallax geometry to reconstruct scene structure. The full pipeline consists of three main networks, a multi-frame network, a single-frame network, and a pose network. The multi-frame network processes sequential frames to estimate the structure of the static scene using planar-parallax geometry and the camera mounting position. Based on this reconstruction, it acts as a teacher, distilling knowledge such as scale information, masked drivable area, metric-scale depth for the static scene, and dynamic object mask to the single-frame network. It also aids the pose network in predicting a metric-scaled relative pose between two subsequent images. Our method achieved state-of-the-art results for the driving benchmark KITTI for metric-scaled depth prediction. Notably, it is one of the first methods to produce self-supervised metric-scaled depth prediction for the challenging Cityscapes dataset, demonstrating its effectiveness and versatility. Project page: https://mono-pp.github.io/", "sections": [{"title": "1. Introduction", "content": "The process of inferring depth information from a single image, called Monocular Depth Estimation (MDE), is pivotal in the realm of autonomous vehicles and semi-autonomous vehicles. It is crucial for understanding the environment surrounding a vehicle, enabling safe navigation, obstacle detection, and path planning. MDE forms the backbone of perception systems in modern camera-reliant automotive applications, facilitating the development of advanced driver-assistance systems (ADAS). Further areas of impact include augmented reality and robotics, as summarized by Li et al. in a survey. These use cases leverage MDE to reconstruct and understand the environment without expensive sensors like Lidar or RGB-D cameras. Chaudhuri et al. provide an extensive overview of advances and literature on inferring depth from images. Notably, the revolution of deep learning in MDE has led to significant advancements in regressing depth from single images.\nPredicting depth from a single image is inherently an under-constrained problem because a single 2D image lacks sufficient information to uniquely determine the 3D structure of the scene. This ambiguity arises because an infinite number of 3D scenes can project onto the same 2D image, making it challenging to infer absolute depth without additional cues or assumptions. Therefore, some form of supervision is needed for metric-scaled monocular depth estimation. However, supervised MDE usually requires ground-truth labels from Lidar or RGB-D cameras, which may not always be available in practical applications. Subsequently, self-supervised MDE has been employed to predict depth at the pixel level using only monocular sequences, stereo pairs, or additional pose information. Most self-supervised methods are trained by novel image synthesis from different views and minimize reconstruction loss via various strategies.\nOne of the main challenges self-supervised methods encounter is the absence of information about moving objects, which may cause incorrect training signals. This can hinder the network's optimization and reduce overall quality. One possible solution is to utilize segmentation models to mask out dynamic objects from the scene , but these are usually trained with supervision. Godard et al. introduced an auto-masking loss that mitigates this problem in some scenes. In instances where the vehicle's velocity is available as a reference, Guizilini et al. demonstrate that a scale-aware model can be learned by velocity regression, comparing the traveled distance with the estimated distance. Sui et al. managed to predict scale-aware depth by utilizing camera height.\nWatson et al. and Guizilini et al. present im-"}, {"title": "2. Related Work", "content": "The field of monocular depth estimation has witnessed major advancements, especially since the deep learning breakthrough by AlexNet. This section covers the evolution from general monocular depth estimation to self-supervision, the history of Planar-Parallax geometry, and recent efforts in predicting scale-aware self-supervised monocular depth.\nMonocular Depth Estimation. The real breakthrough came with the application of Convolutional Neural Networks (CNNs) between 2016 and 2018. One of the pioneering models in this domain is MiDAS, it employs an encoder-decoder architecture, where the encoder extracts high-level features, and the decoder generates the depth map through up-sampling techniques. MiDAS is trained on multiple datasets, enabling robust performance across diverse conditions and environments. They have also utilized vision transformers that enhanced generalizability and accuracy in MiDaS V3.0. In addition, Depth-Anything demonstrated generalizability across datasets by utilizing a hybrid encoder that integrates CNNs with Vision Transformers, along with an attention mechanism to focus on the relevant part of the image, thereby improving the accuracy of depth estimation. Despite these models' ability to generate relative depth information accurately, they still require some scale information or fine-tuning to predict accurately scaled depth values.\nSelf-Supervised Monocular Depth Estimation. There has been significant advancement in the fully self-supervised MDE utilizing analogous encoder-decoder architecture. For example, Godard et al. introduced two models, marking the beginning of the era of self-supervised monocular depth estimation. Their work introduced innovative techniques that leveraged stereo image pairs and monocular video sequences to train depth estimation models without requiring any ground-truth information, significantly advancing the field and inspiring numerous subsequent research efforts.\nLiteMono and VADepth utilized combinations of CNNs and attention blocks, enhancing the capture of multi-scaled local features and long-range global context. Although LiteMono is a lightweight model suitable for real-time use, it also outperforms Monodepth2. Moreover, there are emerging methods such as manydepth and depthformer that take advantage of multiple images at test time, they have achieved superior results when multiple frames are available. However, multi-frame methods do not predict the same depth quality in single-frame scenarios. Most self-supervised MDEs use the per-frame median ground truth to scale predictions, allowing for comparison of relative depth values with the ground truth. Directly predicting metric-scaled depth from self-supervision is inherently ill-posed, necessitating the calculation of true scale using prior knowledge about the actual environment.\nMetric-Scaled Self-Supervised MDE. PackNet-SfM is a novel deep learning architecture that leverages 3D packing and unpacking blocks to effectively capture fine details in monocular depth map predictions. In PackNet-SfM, they also managed to achieve scale-aware monocular depth results by using only the velocity signal of the vehicle, similar to the model developed by Tian et al. for predicting metric-scaled odometry from sequence of images. Additionally, Wagstaff et al. demonstrated that metric-scaled depth can be predicted from a moving monocular camera by utilizing only the camera's mounting position above the ground. Their study reported favorable results when pose ground-truth information was accessible. However, the results were less satisfactory when the scale was inferred solely from"}, {"title": "3. Method", "content": "In this section, we present our design motivation for our approach. We employ a multi-frame teacher network grounded in planar-parallax geometry, enabling the joint optimization of metric-scaled depth and pose. This teacher network distills knowledge to a single-frame model to predict accurate depth information. In addition, by confining the teacher's output to reconstruct only static scenes, we simultaneously generate a static mask to facilitate the training of single-frame model. Unlike recent studies , we did not use fixed data-specific depth binning for converting network output to interpretable depth. Instead, we rely on residual flow binning, which is intrinsically tied to the scene structure, ensuring depth range adaptability per frame. This approach leverages the benefits of planar-parallax geometry. On the other hand, most existing methods that utilize cost volumes, transformers, or similar architectures are computationally intensive. As a result, they are not suitable for real-time performance without significant computational resources. To address this issue, we use a computationally expensive model to distill knowledge into a more efficient deployment model.\nIn planar-parallax geometry , metric-scaled values can be obtained relative to a correctly aligned planar scene. However, similar to Structure from Motion (SfM) , planar-parallax geometry requires a baseline, which may not always be available in vehicular applications, such as stationary scenarios or minimal baselines in traffic jams. To address this, we implemented the planar-parallax module as a teacher, that predicts the static structure details, correctly scales the pose, and contributes to masking dynamic objects. This approach ensures our model's robustness and adaptability to various scenarios.\nOur method comprises three main pipelines. The first pipeline uses a single image to generate a disparity map, which is then converted to depth maps. The second, the Planar-Parallax pipeline, warps neighboring planar-aligned views to the target view, outputs residual flow, and then computes the scene's structure and depth. Predicting residual flow directly simplifies the problem by transforming it into a 1D epipolar flow matching problem.\nSec. 3.1 illustrates the mathematical formulation of the Planar Parallax, detailing how the residual flow between a planar-aligned source frame and a target frame is correlated with the structure of the point in 3D space. Then, Sec. 3.2 covers the Planar Parallax pipeline, including its components. In addition, it presents the monocular depth estimation network and the pose network. Finally, Sec. 3.3 explains the formulation of the loss functions and the training strategy used to train the entire pipeline."}, {"title": "3.1. Planar-Parallax formulation", "content": "The foundations of Planar-parallax geometry are in . In essence, given two views of a scene, the goal is to align the source frame $I_s$ to the target frame $I_t$ with respect to a planar surface $\u03c0$, as shown in Fig. 1. This alignment uses a planar homography $H_{s\u2192t}$, mapping points from the source image $p_s$ to the target image $p_t$.\nBy applying $H_{s\u2192t}$, each point $p_s$ in the source image is transformed to a new position $p'_s$ in the warped source image $I'_s$. $H_{s\u2192t}$ is calculated by the plane normal and the relative pose between $I_t$ and $I_s$, as shown in Eq. (1):\n$H_{s\u2192t} = K(R_{s\u2192t} + \\frac{S}{h_c}T_{st}N^T) K^{-1}$ (1)\nIn Eq. (1), $K$ is the camera's intrinsic matrix, $R_{s\u2192t}$ and $T_{st}$ are the rotation and translation matrices from the source view $O_s$ to the target view $O_t$, $N^T$ is the normal vector of the planar surface $\u03c0$, and $h_c$ is the distance between $O_t$ and $\u03c0$, typically the camera height from the ground in vehicular applications.\nAfter alignment, static objects exhibit parallax residual flow, described in Eq. (2), which provides insights into the relative motion, depth, and height of objects relative to the planar surface. This flow is based on differences in apparent motion between planar-aligned points of $I_s$ and $I_t$, offering a framework to understand and manipulate visual depth cues:\n$u_{res} = \\frac{\u03b3}{1 - \\frac{T_z}{h_c}\u03b3}  \\cdot (p_t - e_t)$ (2)"}, {"title": "3.2. Problem setup", "content": "The network consists of two pipelines: one is the monocular depth estimation network used at test time, and the other is the planar-parallax pipeline, utilized as a teacher module and used only during training. Given two sequential images $I_t$ and $I_s$, they are fed to a pose network which is a ResNet-18 , similar to . This network produces the relative rotation and translation between the two frames, which are then used for computing losses, computing homography, and calculating the depth from Planar-Parallax, as shown in Fig. 2.\nThe homography is computed from the relative pose information and the camera height prior to the planar surface, based on Eq. (1). Then $I_s$ is warped by the computed homography, and then fed to a warping encoder which is a ResNet-50 , pretrained on ImageNet. Another similar encoder, the 'Mono encoder', is also utilized to encode the target image $I_t$, this is the encoder used at test time. The outputs of the two encoders are concatenated and passed to the Flowscale decoder, a series of CNNs for upsampling the input and predicting the output for multi-scale stages, such as . The depth decoder has a similar architecture to the Flowscale decoder, but only takes the encoded information of $I_t$ as an input, and this is the one used at test time as well, as shown in Eq. (3).\n$D_{mono} = @_{mono} (I_t)$ (3)\nThere are two main trainable networks, which are $@_{mono}$ and $@_{pp}$, as shown in Eqs. (3) and (4). $D_{mono}$ is the calculated depth from the disparity produced by $@_{mono}$.\n$S_t = Opp (I_t, I'_s)$ (4)\nThe output of $@_{pp}$ is a single value per pixel, which is a scaling value representing the scaling quantity to be multiplied by the $(p_t - e_t)$ in equation Eq. (2). The output of the Flowscale decoder is mapped to specific bins which are adjusted for different image resolutions; hence the scaled $s_t$ is transformed to $S_t$. Then, these values are multiplied by $(p_t - e_t)$ to calculate the $u_{res}$, as shown in Eq. (5).\n$S_t = u_{res}$ (5)\nAfter $u_{res}$ is calculated from $S_t$, we can calculate $\u03b3$ for this point given $T_z$ and $h_c$, as shown in Eqs. (5) and (6). $T_z$ is derived directly from the output of the PoseNet, while $h_c$ is the camera's height from the ground.\n$\u03b3 = \\frac{S_t}{S_t + 1} \\cdot \\frac{h_c}{T_z}$ (6)\nSuch that the $S_t$ is the scaled output of $@_{pp}$ representing the offset of the pixel along the epipolar line. This is how $\u03b3$ is retrieved from the network's output. Then the depth or height is derived from $\u03b3$ based on Eq. (7), the derivation of the equation is in .\n$D_{PP} = \\frac{h_c}{\u03b3 + N^T \\cdot (K^{-1}p)}$ (7)\nThe required outputs of this pipeline to calculate the losses are $D_{mono}$, $u_{res}$, and $D_{PP}$. These can synthesize novel images from the neighboring views, as shown in Eq. (8), to compute the photometric reprojection losses.\n$I'_s = I_s(proj(D_t, R_{t\u2192s}, T_{t\u2192s}))$ (8)\nwhere $proj()$ is the resulting coordinates of the projected depths, and $()$ is the bi-linear sampling operator, which is locally sub-differentiable. However, novel views can also be constructed by $u_{res}$, as shown in Eq. (9).\n$\u00ce'_t = I_t(u_{res})$ (9)"}, {"title": "3.3. Training", "content": "In our approach, we have five main losses, which are $L_{homo}$, $L_{mono}$, $L_{pp}$, $L_{cons}$, and $L_{res}$. First, the photometric reprojection loss utilized within some losses is L1 and SSIM Eq. (12), as introduced in.\n$pe(I_a, I_b) = \u03b1\\frac{1}{2} (1-SSIM(I_a, I_b))+(1-\u03b1)||I_a-I_b||$ (12)\nHomography loss is responsible for accurate road-planar homography estimation, and it is also the one encouraging the pose's output to be metric-scaled. It encourages the planar area of $I'_s$ to be similar to the planar area of $I_t$. Hence, it enforces the correct alignment of the road planar surface,\n$L_{homo} = \\frac{1}{N} \\sum^N_{p_i} M_{flat} (p_e(I'_s, I_t))$ (13)\nwhere pe is the reprojection loss, illustrated in Eq. (12), and N is the number of pixels. This loss is calculated for all available source images and for all multi-scale outputs, similar to.\n$L_{mono}$ and $L_{pp}$ similarly minimize the reprojection of the predicted depth, whether by the monocular depth estimation or the planar-parallax module. In addition, auto-masking $M_{auto}$ is implicitly utilized, as explained in .\n$L_{mono} = \\frac{1}{N} \\sum^N_{p_i} M_{auto} (p_e(\u00ce^{d_{mono}}_s, I_t))$ (14)\nHowever, one difference is that the depth by planar-parallax is masked by the certainty mask, as shown in Eq. (15).\n$L_{pp} = \\frac{1}{N} \\sum^N_{p_i} M_{auto} M_{cert} (p_e(\u00ce^{d_{pp}}_s, I_t))$ (15)\nThe residual loss quantifies computing the error between $\u00ce'_t$ and $I_t$, which ensures that the scaling output from Opp represents the correct epipolar scaling, which is residual flow, as shown in Eq. (16).\n$L_{res} = \\frac{1}{N} \\sum(p_e(\u00ce^{res}_t, I_t))$ (16)\nThe consistency mask, presented in Eq. (17) and inspired by , checks the pixels where the depth $D_{mono}$ and $D_{PP}$ agree to a certain threshold $\u03b4$.\n$M_{static} = max(\\frac{D^{mono}_t - D^{PP}_t}{D^{PP}}, \\frac{D^{PP}_t - D^{mono}_t}{D^{mono}}) < \u03b4$ (17)\nIn addition, the monocular depth estimation is adjusted to align more closely with the planar-parallax module's depth prediction, as they should largely agree on static scenes. To prevent inaccuracies due to possible different scales, normalized depth $D$ is used to ensure the alignment of the two outputs in defining $L_{consist}$ in Eq. (18).\n$L_{consist} = \\frac{1}{N} \\sum M_{static}|D^{mono} - D^{PP}|$ (18)\nThe training strategy is as follows: for the first 5 epochs, the total loss is $L_{mono}$+$L_{res}$+$L_{pp}$, when the pose is jointly"}]}