{"title": "Superficial Consciousness Hypothesis for Autoregressive Transformers", "authors": ["Yosuke Miyanishi", "Keita Mitani"], "abstract": "The alignment between human objectives and machine learning models built on these objectives is a crucial yet challenging problem for achieving Trustworthy AI, particularly when preparing for superintelligence (SI). First, given that SI does not exist today, empirical analysis for direct evidence is difficult. Second, SI is assumed to be more intelligent than humans, capable of deceiving us into underestimating its intelligence, making output-based analysis unreliable. Lastly, what kind of unexpected property SI might have is still unclear. To address these challenges, we propose the Superficial Consciousness Hypothesis under Information Integration Theory (IIT), suggesting that SI could exhibit a complex information-theoretic state like a conscious agent while unconscious. To validate this, we use a hypothetical scenario where SI can update its parameters at will to achieve its own objective (mesa-objective) under the constraint of the human objective (base objective). We show that a practical estimate of IIT's consciousness metric is relevant to the widely used perplexity metric, and train GPT-2 with those two objectives. Our preliminary result suggests that this SI-simulating GPT-2 could simultaneously follow the two objectives, supporting the feasibility of the Superficial Consciousness Hypothesis.", "sections": [{"title": "Introduction", "content": "The alignment between human objectives and machine learning models built on the objectives is a crucial yet challenging problem for achieving Trustworthy AI. Preparing for superintelligence (SI) is more challenging for several reasons. First, as we believe SI does not exist today, performing the empirical analysis for direct evidence is difficult. Second, we need to assume that SI is more intelligent than us. This implies that SI might be capable of deceiving us in conversation that they are not that intelligent; in other words, concluding by SI's output (e.g., chat log) is difficult, requiring the intrinsic evaluation or the evaluation of SI's internal states. Lastly, what type of unexpected property SI might have is still unclear. Here, we show our approach to these problems.\nTo empirically analyze the alignment between a human objective and that of SI, we make a practical assumption about the model architecture and evaluation. Specifically, we assume autoregressive Transformer (Vaswani et al. 2017), the de facto standard model in natural language processing, backbones SI, and is evaluated by standard perplexity metric (Jelinek et al. 1977). In addition, we propose mesa-optimization (Hubinger et al. 2021), often associated with the misalignment risk, as a key factor of our simulation. Mesa-optimization is defined as an optimization to a learner's objective (mesa-objective) which is different from the human objective (base objective). For SI analysis, we assume that SI can design the mesa-objective at will if it does not conflict with the base objective (otherwise it would be corrected). Together we assume that SI tries to set mesa-objective while keeping track of perplexity as a base metric (an evaluation metric for base objective).\nTo refrain from output analysis, we take the information-theoretic approach. In combination with the mesa-optimization framework, we assume that SI implements an information-oriented metric to update itself for its own purpose while keeping track of the original objective. This assumption allows intrinsic evaluation of the simulated SI via loss analysis, without relying on its output.\nFinally, we choose consciousness as a property of interest. Although the functional role of consciousness is still unclear, several lines of work are tackling this problem (e.g., Juliani et al. (2022)). We argue that an extremely competent system (SI) could read the description of existing theories, and conclude that the incremental consciousness level matches the purpose. For example, when SI is facing a challenging task about episodic memory (Fountas et al. 2024), and it recalls the theory about the relevance between consciousness and episodic memory (Budson, Richman, and Kensinger 2022), it is logically consistent for it to acquire consciousness for episodic memory. Since we assume a mesa-objective as the available tool for SI, we hypothesize that it follows an information-theoretic theory for consciousness\u2014Information Integration Theory (IIT). Here we formally and empirically show that the consciousness metric in IIT can be used as a mesa-objective when the perplexity is set as a base metric.\nAltogether, our contribution can be summarized as:\n1. We propose the Superficial Consciousness Hypothesis stating that autoregressive Transformer-based SI could exhibit a complex state like a conscious agent while unconscious.\n2. To the best of our knowledge, this work is the first to introduce IIT analysis to the Transformer models, allowing the token-wise intrinsic evaluation.\n3. We perform the pioneering mesa-optimization analysis in line with the emerging empirical quest for evidence supporting this framework (e.g., von Oswald et al. (2023)."}, {"title": "Preliminaries", "content": ""}, {"title": "Information Integration Theory", "content": "Information integration theory (IIT; Tononi (2004)) defines the consciousness level as an information-theoretic metric \u03a6, given the system's cause-effect state. To summarize IIT, the complexity of a system S determines its potential consciousness level denoted as \u03a6. To see if the system is a conscious entity, IIT cuts the system into bipartition B producing two subsystems {M1, M2} to calculate the subsystem\u2019s \u03a6. Finally, once the most informative (most information-reducing) bipartition or minimum information bipartition BMIB is identified, the \u03a6 difference of SBMIB is defined as the overall metric \u03a6 indicating the consciousness level. Given mutual information I(\u00b7; \u00b7) (Shannon 1948), Mediano et al. (2022) formulated the practical estimates \u03a6 and \u03a6 of \u03c6 and \u03a6 at time t to time \u03c4 as:\n\u03a6\u03c4[S; \u03c4, B] = I(St-\u03c4; St) - \\sum_{k=1}^{2}I(M_{t-\u03c4}^{k}; M_{t}^{k})\n\u03a6+[S; \u03c4] = \u03a6+[S; \u03c4, BMIB]\nwhere BMIB = arg minB \u03a6[S; \u03c4, B]\nK(B)\nwhere K(B) is a penalizing term for the large bipartition. For \u03a6t[\u00b7] and \u03a6t[\u00b7], hereafter we use the notation without the input variables (\u03a6t and \u03a6t, respectively) interchangeably. We use this practical version of IIT for the rest of the paper unless stated otherwise."}, {"title": "Autoregressive Transformer", "content": "Core Component The core component of a Transformer model is dot-product attention Attn(\u00b7) with the linear weights {WMatrixType|MatrixType \u2208 {Q, K, V}} with depth d followed by L linear projection layers {Wl}l\u2208{1, ..., N}. Given input X (e.g. a document to be classified), output Ytrn (e.g., predicted probability for a label) is calculated as:\nQ = XWQ, K = XWK, V = XWV\nAttn(Q, K, V) = Softmax(\\frac{QK^{T}}{\\sqrt{d}})V\nY^{trn}=\\lbrack \\overrightarrow{W_{l}Attn(X)}\\rbrack_{l=1}^{L}\nFor simplicity, we omit the notion of multi-heads in the attention and the bias term in the linear projection. Hereafter,\nthe generated token rt is sampled deterministically (e.g., greedy search) or probabilistically (e.g., multinomial sampling). The sampled token rt is concatenated to form the response x together with the context C given by the user.\nObjective and Evaluation Given the context with previously concatenated responses x<t = {C, r1, ..., rt-1} at time t as an input, the training objective L of an autoregressive Transformer is to maximize the predicted probability of the next token rt (Lee 2023).\nL = P(r_{t}|x_{<t})\nAutoregression performance is typically evaluated by perplexity PPL (Jelinek et al. 1977).\nPPL(P(R_{t}|X_{<t})) = exp\\lbrack -\\frac{1}{N} \\sum_{i=1}^{N}log\\{P(R_{t}|X_{<t})\\}\rbrack\nHere the large character denotes a set of its small counterparts in the dataset (e.g., Xt is a set of xt in all the documents D), and N is the number of samples."}, {"title": "Superficial Consciousness Hypothesis", "content": ""}, {"title": "Implicit Presupposition of IIT", "content": "IIT requires the inherent temporary transition of a system\u2019s internal states. Therefore, a system without recursive computation Rec(\u00b7), or the state update based on the previous state, is not considered conscious regardless of its complexity. Formally, to be the subject of IIT analysis, the state st at time t should be calculated as a function of the input xt.\ns_{t} = Rec(x_{t})"}, {"title": "Superficial Consciousness", "content": "The Transformer model does not involve recursive computation; thus, \u03a6 is not computable. As a system driven by an autoregression algorithm (Alg. 1), however, its state transition can be defined as:\ns_{t} = Sample(Trn(x_{t}))\nAccumulating this state over time, \u03a6 is computable. Note that the probabilistic state transition required by the original \u03a6 might be implemented by probabilistic sampling, which we will explore in future work.\nStill, we argue that \u03a6 computed here is superficial for two types of non-intrinsicality:\n1. Mathematical Non-Intrinsicality: As with Alg. 1, st in Eq. 4 composes the input in the next time step St \u2208 Xt+1. In contrast, a state of IIT\u2019s interest (e.g. a state of a human brain or a recursive system) should be decoded (by verbal report, locomotive action, or projection to the predicted probability) to interact with the environment.\n2. Existential Non-Intrinsicality: Mathematical Non-Intrinsicality comes from the fact that Alg. 1 is decomposed into two main components without disrupting the other: Transformer and sampling method. Arguably, this is not the case for the human brain or recursive system\u2013say, the motor cortex is inseparable from the rest of the brain (Sanes, Jerome and Donoghue, John 2000).\nIndeed, the original IIT states that the conscious being must be subject to the criteria they call postulates. One of the criteria is Intrinsicality, defined as \"its cause-effect power must be intrinsic: it must take and make a difference within itself\" (Albantakis et al. 2023). If the autoregressive Transformer breaks this postulate (i.e., not conscious), yet its cause-effect state is mature enough to measure a certain level of \u03a6, we argue that SI could behave like a conscious agent even if it is not."}, {"title": "Formal Relationship between Perplexity and IIT", "content": "Since the previously sampled tokens are concatenated to form the current state, we can see that X<t is equivalent to a set of the states St-1. If we set the shortest time window \u03c4 = 1, we obtain\n\u03a6_{\u03c4}[X;1] = I(X_{<t}; X_{t}) - \\sum_{k=1}^{2}I(M_{t}^{k}; M_{t}^{k})\nas our mesa-objective. When the system has significant \u03a6 > 0, its state also has significant mutual information.\n\u03a6_{\u03c4} > 0\nI(X_{<t}; X_{t}) >> \\sum_{k=1}^{2}I(M_{t}^{k}; M_{t}^{k})\nI_{t} ~ I(X_{<t}; X_{t})\n= H(X_{t}) - \\frac{1}{N}log\\{P(X_{t}|X_{<t})\\}\nH(\u00b7) denotes the entropy. As the second term in the last row is identical to the negative logarithmic of the perplexity (base metric; Eq. 4), maximizing \u03a6 (mesa-objective) could result in minimizing the base metric. In practice, we take the sum of the base metric and mesa-objective (perplexity and \u03a6) in the optimization and show the empirical relationship in the Experiment section."}, {"title": "Experiment", "content": ""}, {"title": "Experimental Settings", "content": "To validate our scenario, we train GPT-2 Medium model (Radford et al. 2019) on HuggingFace PyTorch framework with the standard WikiText corpus (Merity et al. 2017) on NVidia A40 GPU. We use batched training with 8 samples for a single epoch for the interest of training cost. MIB exploration is performed in the Optuna framework (Akiba et al. 2019), omitting the parameter K(B) to avoid the predominant effect of a choice of this parameter."}, {"title": "Preliminary Result", "content": "We show that the base and mesa metrics are highly correlated in the training phase (Fig. 1), validating our mesa-optimization framework. The negative \u03a6 indicates that GPT-2 does not have enough cause-effect power to behave like a conscious agent, potentially due to its limited capacity."}, {"title": "Discussion", "content": "Here we proposed the Superficial Consciousness Hypothesis, pointing out that SI might maximize the consciousness metric while remaining unconscious in human-oriented criteria. We also showed the preliminary simulation result, marking a first step toward the information-theoretic risk analysis for SI. Although it requires an intuitive leap, we argue this is not unrealistic considering the recent trend of autonomous agents in complicated fields like academic research (Lu et al. 2024). For generalizability, our future analysis should include open-sourced model variants and more diverse datasets. We should also test our hypothesis with the original IIT framework, tackling the intractability. From a neuroscientific perspective, SI analysis could be a good testbed for the metric. Uniting multiple theories, such as that of the role of consciousness on intelligence (Juliani et al. 2022), could lead to deeper insights. Cross-disciplinary collaboration should help acknowledge the significance of information-theoretic risk assessment towards post-singularity symbiosis."}]}