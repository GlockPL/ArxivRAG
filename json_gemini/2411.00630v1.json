{"title": "STAA: Spatio-Temporal Attention Attribution for Real-Time Interpreting Transformer-based Video Models", "authors": ["Zerui Wang", "Yan Liu"], "abstract": "Transformer-based models have achieved state-of-the-art performance in various computer vision tasks, including image and video analysis. However, Transformer's complex architecture and black-box nature pose challenges for explainability, a crucial aspect for real-world applications and scientific inquiry. Current Explainable AI (XAI) methods can only provide one-dimensional feature importance, either spatial or temporal explanation, with significant computational complexity. This paper introduces STAA (Spatio-Temporal Attention Attribution), an XAI method for interpreting video Transformer models. Differ from traditional methods that separately apply image XAI techniques for spatial features or segment contribution analysis for temporal aspects, STAA offers both spatial and temporal information simultaneously from attention values in Transformers. The study utilizes the Kinetics-400 dataset, a benchmark collection of 400 human action classes used for action recognition research. We introduce metrics to quantify explanations. We also apply optimization to enhance STAA's raw output. By implementing dynamic thresholding and attention focusing mechanisms, we improve the signal-to-noise ratio in our explanations, resulting in more precise visualizations and better evaluation results. In terms of computational overhead, our method requires less than 3% of the computational resources of traditional XAI methods, making it suitable for real-time video XAI analysis applications. STAA contributes to the growing field of XAI by offering a method for researchers and practitioners to analyze Transformer models.", "sections": [{"title": "I. INTRODUCTION", "content": "Video understanding has emerged as a cornerstone of modern artificial intelligence, revolutionizing critical applications from autonomous driving safety systems [1] to precision medical diagnostics [2]. The introduction of Transformer architectures, exemplified by TimeSformer [3] and ViViT [4], represents a paradigm shift in video analysis, consistently outperforming traditional convolutional neural networks (CNNs) [5], [6] across diverse benchmarks. However, this leap in performance introduces new challenges: as model architectures grow more sophisticated, their decision-making processes become increasingly opaque, necessitating advanced explainable AI (XAI) approaches [7] to ensure accountability and trust in critical applications.\nVideo understanding presents unique challenges compared to static image analysis, primarily due to its inherent spatio-temporal complexity. While spatial features capture frame-level information such as object configurations and contextual relationships, temporal features encode crucial dynamic patterns\u2014motion trajectories, state transitions, and temporal dependencies [8]. The significance of temporal modeling is evidenced by breakthroughs such as Temporal Segment Networks (TSN) [8], which demonstrate how precise temporal feature analysis enables the discrimination of visually similar but temporally distinct actions. This spatio-temporal interplay forms the foundation of advanced video understanding tasks, where the precise sequencing and timing of spatial transformations define complex actions [6]. Understanding and explaining these intricate spatio-temporal relationships becomes crucial as video AI systems increasingly influence high-stakes decisions in autonomous systems [1] and healthcare [2].\nWhile significant advances have been made in developing XAI methods for computer vision [9], particularly in CNN-based image analysis [10], extending these approaches to Transformer-based video models presents unprecedented challenges. These challenges manifest in two critical dimensions: algorithmic complexity and computational efficiency, both of which fundamentally impact the practical deployment of explainable video AI systems.\nAlgorithmic Challenges: The distinctive architecture of Transformers, centered on self-attention mechanisms, represents a fundamental departure from traditional CNNs, rendering many established XAI methods ineffective or theoretically incompatible [10], [11]. Besides, architectural divergence is compounded by the inherent complexity of video data, where temporal dynamics create additional complexity beyond what static image-based XAI methods can address [12]. Current approaches often artificially dichotomize spatial and temporal analysis, producing fragmented explanations that fail to capture the holistic nature of video understanding. This limitation creates a critical gap in our ability to explain how models interpret and process the dynamic interplay between spatial and temporal features in video content.\nComputational Efficiency Challenges: The computational demands of existing post-hoc explanation methods present a significant barrier to real-world deployment. These methods typically require multiple model inference passes, creating substantial computational overhead for video analysis tasks. This inefficiency becomes particularly acute in scenarios demanding real-time explanations, such as autonomous systems or live video monitoring applications [13]. The computational burden is especially problematic for long video sequences, where the resource requirements can grow exponentially [14], [15] due to the algorithm of post-hoc methods.\nThese challenges underscore the need for XAI methods to adapt to transformer architectures, provide spatio-temporal explanations, and achieve high computational efficiency. We pose the following research questions:\nRQ1: Can we develop a novel XAI method for video Transformer models that simultaneously captures spatial and temporal feature explanations?\nRQ2: Can our proposed XAI method improve faithfulness, monotonicity, and computational efficiency compared to traditional model-agnostic methods in video analysis tasks?\nRQ3: Can we design and implement a cloud-based XAI service architecture that enables real-time explanations for edge-side video?\nTo the best of our knowledge, there are several critical gaps in the current video AI explainability: (1) Despite the widespread deployment of Transformer-based video [3], [9], [16], there remains an absence of methods for video Transformer architectures [17]. (2) Current approaches provide a single type of feature explanation, failing to simultaneously capture both spatial and temporal features in video models. (3) The absence of real-time capabilities for video analysis creates a significant barrier for edge computing applications [18]. Traditional post-hoc approaches fail to address the computational demands of video analysis. These gaps collectively impede practitioners from understanding and trusting video AI systems, highlighting the urgent need for novel XAI approaches that can address both the theoretical and practical challenges of explaining video Transformer models.\nTo address these critical gaps, we propose STAA (Spatio-Temporal Attention Attribution), a novel XAI method for video Transformer models. STAA achieves three key advantages: (1) simultaneous capture of spatial and temporal feature importance through a single forward pass, (2) improved explanation faithfulness through direct alignment with the model's decision-making process, and (3) significantly reduced computational overhead by utilizing the model's internal attention mechanisms rather than requiring multiple inference passes. In this paper, the main contributions are:\nNovel XAI Architecture: STAA fundamentally addresses the architectural mismatch between current XAI methods and video Transformer models.\nEmpirical Validation: Rigorous evaluation of STAA against adapted versions of SHAP and LIME on the Kinetics-400 dataset, demonstrating significant improvements in both explanation quality and computational efficiency.\nReal-Time XAI Framework: Design and implementation of a cloud-based service architecture that enables real-time video explanations with sub-100ms latency, making STAA practical for edge computing applications.\nThe remainder of this paper is organized as follows: Section II reviews related work in video Transformers and current XAI approaches, highlighting limitations in explaining video models. Section III presents our STAA method and adaptations of SHAP and LIME for video analysis. Section IV details our experimental setup using the Kinetics-400 dataset and TimeSformer architecture, introducing metrics for assessing video XAI quality. Section V demonstrates STAA's performance improvements in both explanation quality and computational efficiency, including our real-time implementation results. Section VI concludes with key findings and future directions."}, {"title": "II. RELATED WORK", "content": "A. Transformer Models for Video Analysis\nThe success of Transformer architectures in natural language processing [19] has inspired researchers to adopt them to computer vision tasks [20]. Differ from traditional CNNs, Transformer-based models can capture long-range dependencies and have shown remarkable performance in various video understanding tasks [5]. TimeSformer by Meta [3] pioneered the application of pure Transformer architectures to video classification by treating different frame patches and time steps as tokens. This approach demonstrated competitive performance on standard benchmarks while being computationally efficient. Building upon this work, ViViT by Google [16] proposed a tubelet embedding scheme, further slightly improving video classification accuracy. Moreover, another work VideoSwin by Microsoft [21] adapted the hierarchical Swin Transformer [22] for video understanding tasks.\nThese models consistently outperformed CNN-based approaches in their paper. However, the previous XAI methods for generating saliency maps are not applicable to these Transformer-based video analysis models. While methods such as Grad-CAM [10] have been effective in visualizing important regions in CNN-based models, they do not trivially extend to the self-attention mechanisms and token-based representations employed by Transformers. This necessitates the development of novel XAI techniques specifically for Transformer architectures in video understanding tasks. Recent work has begun to develop XAI methods for Transformer models. For instance, the recent work [9] proposed XAI pipelines for multiple Transformer models in image classification. Adapting and extending such approaches to video analysis represents a promising direction for research.\nB. Explainable AI in Computer Vision\nXAI has gained significant attention in recent years due to the increasing complexity of deep learning models and the need for transparency in AI systems [15]. Various XAI techniques have been developed and applied in the comprehensive XAI service frameworks [23]. SHAP (SHapley Additive exPlanations) [14] provides feature attribution based on coalitional game theory. LIME (Local Interpretable Model-agnostic Explanations) [24] offers another approach by approximating the behavior of complex models with interpretable surrogate models in the vicinity of a specific input, providing insights into local decision boundaries.\nDespite the significant progress in XAI techniques for image analysis, such as SHAP [14], LIME [24] and GradCAM [10]. These methods face substantial challenges when applied to video models. The temporal feature inherent in video data poses an additional dimension for most existing XAI methods, which are primarily designed for static images and struggle to capture the temporal feature of video content. Furthermore, the architectural differences between Transformers and traditional CNNs create a mismatch, as many XAI techniques rely on gradient maps [10], rendering them incompatible with Transformer models.\nThe computational demands of video analysis exacerbate these issues, as post-hoc explanation methods often require multiple inferencing runs, which is prohibitively expensive for deep models and long video sequences. The algorithm inefficiency limits their practical applicability in real-world scenarios where timely explanations are crucial [18]. The current methods also fail to provide unified spatio-temporal explanations [14], [24]. This approach fails to capture the intricate interactions between space and time that are fundamental to video data, resulting in incomplete or misleading interpretations of model behavior [25].\nThese limitations underscore the pressing need for XAI approaches specifically designed to address the challenges posed by video Transformer models. Such methods must handle both spatial and temporal features while maintaining computational efficiency.\nC. Challenges in XAI for Video Transformer Models\nWhile significant progress has been made in developing XAI techniques for image analysis, the interpretability of video models presents unique challenges due to the additional temporal dimension and the complexity of Spatio-Temporal reasoning [25]. For CNN-based video models, Hiley et al. [26] adapted autograd for 3D CNNs to generate class activation maps for action recognition models. Building on this work, Stergiou et al. [27] proposed Class-Specific Saliency Tubes (CSST), which extend 2D saliency maps to the temporal domain, providing frame-wise visualizations of important regions for action recognition. However, their technique routes are not applicable to Transformer-based video models due to their fundamentally different architectures. To the best of our knowledge, the field of XAI for video Transformers remains unexplored. With the Transformer achieving the performance and being more commonly deployed, lacking XAI becomes particularly concerning given the increasing adoption of these models in applications [28]. We seek to provide deeper insights into their decision-making processes and pave the way for more transparent and trustworthy video analysis systems.\nD. Requirements for Real-Time Video XAI\nSoftware engineers define a real-time response as that occurs within 100 milliseconds [13]. Another work [29] also define a real-time video inpainting system as one that can process a video frame in the range of hundreds of milliseconds. We refer to this requirement for latency in real-time video analysis. Considering our STAA method reduce the computation time of XAI for a video clip from minutes-level to 0.16 seconds, it is possible to apply STAA in real-time video XAI analysis.\nReal-time video stream model AI has diverse applications across critical industries. In autonomous driving systems [1], real-time video analysis enables immediate detection of road hazards, pedestrians, and changing traffic conditions, allowing for split-second decision-making essential for passenger safety. In medical diagnostics [2], real-time video AI assists surgeons during procedures and enables rapid analysis of endoscopic footage for early disease detection. In security applications [30], these systems continuously monitor surveillance feeds to detect anomalies and identify potential threats, providing immediate alerts to security personnel. These high-stakes applications underscore the importance of immediate and interpretable explanations of AI decisions."}, {"title": "III. SPATIO-TEMPORAL XAI METHODS FOR VIDEO TRANSFORMER MODELS", "content": "In this section, we study three approaches for explaining video Transformer models: (1) SHAP-based temporal analysis: Identifies important temporal segments across video frames. (2) LIME-based spatial analysis: Generates spatial explanations for individual video frames. (3) STAA: Our proposed method that extracts information from Transformer attention mechanisms for spatio-temporal explanations.\nA. SHAP-based Temporal Feature Attribution\nThe SHAP (SHapley Additive exPlanations) [14] analysis aims to identify the importance of different temporal segments in a video for the model's decision-making process. This method provides insights into which moments in a video are influential for classification.\n1) SHAP Algorithm Adaptation for Video Analysis: SHAP applies concepts from coalitional game theory, particularly Shapley values, to feature attribution. In our video analysis, we map these concepts as follows: The temporal segments of the video serve as the \u201cplayers\u201d in the game-theoretic sense. The \"coalitions\" are formed by various subsets of these temporal segments. The \u201ccharacteristic function\" is represented by the model's prediction for any given subset of segments. The SHAP value for a segment quantifies its average marginal contribution to the model's prediction across all possible combinations of segments.\nTo analyze the temporal importance of different parts of a video, we segment the video stream into equal-length temporal segments. SHAP considers all possible combinations of these segments. Uniform segmentation ensures that each segment represents an equal duration of the video, providing a fair basis for this combinatorial analysis. Practitioners can increase the number of segments to achieve a more fine-grained analysis or decrease it for a more coarse-grained XAI analysis.\n2) Temporal Importance Quantification: Algorithm 1 presents our SHAP-based temporal feature attribution method [14]. The algorithm begins by segmenting the video into N equal-length segments. For each segment, it calculates a SHAP value to quantify its importance in the model's decision-making process. Them we have two methods for calculating SHAP values: exact calculation and Monte Carlo approximation. The exact method considers all possible combinations of segments, providing the most accurate results but at a higher computational cost. The Monte Carlo approximation estimates SHAP values by sampling a specified number of random segment combinations, significantly reducing computation time while maintaining a good estimate of the true SHAP values.\nTo illustrate the process of SHAP-based temporal importance analysis, Figure 1 as a flowchart provides a clear understanding of the algorithm's workflow and how it identifies the most influential temporal regions in a video for the model's decision-making process.\n3) SHAP Computational Analysis: Several factors affect the implementation of this method: Segment Size: The choice of N affects the granularity of the analysis. Smaller segments provide finer temporal resolution but increase computational cost. In our experiments, we set N = 8 for Kinetics-400, which offers a coarse temporal resolution. However, this already requires significant computational overhead. The computational complexity grows as O(2^N) due to the need to evaluate all possible segment combinations. Approximation Strategy: When computational resources are limited or for large-scale analyses, the Monte Carlo approximation can be employed. The number of Monte Carlo samples K trades off between accuracy and computational efficiency. Model Integration: SHAP requires multiple inferences of the video model for different segment combinations. The computational requirements of SHAP are intrinsically linked to both the segmentation parameters and the underlying video AI model's complexity.\nIn summary, SHAP can provide temporal feature attribution through the scores of temporal segments. However, it has several limitations. First, it focuses solely on one feature aspect, according to the segments. Second, the granularity of the analysis is limited by the number of segments N. A small N provides only coarse resolution. The cost increases as O(2^N) with the number of segments, making it challenging to analyze videos at high temporal resolutions or to process large datasets efficiently. While the Monte Carlo approximation mitigates this to some extent, it introduces a trade-off between accuracy and computational efficiency.\nB. LIME-based Spatial Feature Attribution\nLIME (Local Interpretable Model-agnostic Explanations) [24] is employed for spatial analysis of individual video frames. We adapt LIME with the Vision Transformer (ViT) model [20] trained on the Kinetics-400 dataset, enabling it to operate on individual frames while maintaining the model structure in Transformer.\n1) LIME Algorithm Adaptation for Video Analysis: LIME is grounded in the principle of local approximation. It simplifies complex models by approximating them with interpretable surrogates in the vicinity of specific inputs. For video analysis, we adapt LIME to operate on individual frames extracted at regular intervals from the video sequence. The choice of frame count K involves a trade-off between computational cost and temporal resolution.\nFor each selected frame, LIME generates perturbed variants by applying random masks to different regions, creating a local neighborhood around the original frame. The interpretable surrogate takes the form of a linear model operating on binary features, representing the presence or absence of interpretable components within the frame. This approach enables LIME to provide localized explanations by highlighting regions that significantly influence the model's frame-level decisions.\n2) Spatial Importance Quantification: Algorithm 2 presents the LIME-based spatial feature attribution method for video analysis: The algorithm begins by extracting K equally spaced frames from the input video. For each frame f_k, it generates a set of M perturbed samples P_k by applying random masks. These perturbations help explore the model's behavior in the vicinity of the original frame. Next, the algorithm computes ViT predictions Y_{km} for each perturbed sample. An interpretable model g_k is then fitted to approximate the ViT's behavior locally. This process balances between fidelity to the ViT's predictions (represented by the loss function L) and model simplicity (enforced by the regularization term \u03a9). The algorithm computes importance scores M_s[k] based on the fitted interpretable model for each frame. These scores form a sequence of spatial attributions that can be used to generate map visualization for each frame, with warmer colors indicating areas of higher importance.\n3) LIME Computational Analysis: Several factors affect the implementation of LIME in our video analysis context: Number of Samples: The number of perturbed samples N_p significantly affects the fidelity of the local approximation. A larger N_p generally leads to more accurate explanations but increases computational cost. In our experiments, we select N_p = 1000 for balanced accuracy and efficiency. Frame Selection: The number and distribution of frames selected for analysis can affect the understanding of the video. Perturbation Strategy: The method of generating perturbations can influence the quality of explanations. We use the default setting in LIME [24], which is random masking.\nThese factors should be considered when applying LIME in video XAI analysis. LIME provides explanations that are local in nature and may ignore global patterns. Generating explanations for many frames can be computationally expensive, especially for high-resolution videos. LIME does not capture relationships between frames.\nC. Spatio-Temporal Attention Attribution (STAA)\nThis section introduces our Transformer attention-based method for analyzing video classification. This approach extracts attention mechanisms in Transformer models to explain both temporal and spatial importance. Figure 3 provides a visual overview of STAA method."}, {"title": "Algorithm 1 SHAP-based Temporal Feature Attribution", "content": "Input: D, N\nOutput: M_t\nS_i = {f_t\\t \\in [\\frac{D(i-1)}{N}, \\frac{Di}{N})}, i= 1,2,..., N\nInitialize M_t = {0}^N\nfor i = 1 to N do\n$\\Phi_i = \\sum_{S \\subset N\\{i\\}} \\frac{|S|!(N-|S|-1)!}{N!} [f_x(S \\cup \\{i\\}) - f_x(S)]$\n// When using approximation:\n// $\\Phi_i = \\frac{1}{K} \\sum_{k=1}^K [f(z_k \\cup \\{i\\}) - f_x(z_k)]$\nM_t[i] = $\\Phi_i$\nend for\nreturn M_t\nNotation: D: Duration of video, N: Number of segments, f_x: Model prediction function, M_t: Temporal attribution, K: Number of Monte Carlo samples (for approximation), z^k: Random subset of segments (for approximation)\n5) Input and Embedding: Given an input clip video X \u2208 \\mathbb{R}^{F \\times H \\times W \\times 3} consisting of F RGB frames, we decompose each frame into non-overlapping patches of size P \u00d7 P. Each patch x_{(p,t)} \u2208 [0,1]^{P^2\\times 3} is linearly mapped into an embedding:\nz_{(p,t)}^{(0)} = Ex_{(p,t)} + e_{pos(p,t)}\n(1)\nwhere E is the embedding matrix, and e_{pos(p,t)} \u2208 \\mathbb{R}^D represents learnable positional embeddings for spatial position p and temporal position t, as marked 0 to N in Figure 3. Then, z is the overall input to the Transformer, similar to the sequences of words which are used in the language transformer model [19].\n2) Attention Value Extraction: For each transformer layer l and attention head a, we compute query and key vectors for each patch as the Timesformer architecture [3]:\nq_{(p,t)}^{(l,a)} = W_Q^{(l,a)} LN(z_{(p,t)}^{(l-1)}) \\in \\mathbb{R}^{D_h}\n(2)\nk_{(p,t)}^{(l,a)} = W_K^{(l,a)} LN(z_{(p,t)}^{(l-1)}) \\in \\mathbb{R}^{D_h}\n(3)\nwhere: W_Q^{(l,a)}, W_K^{(l,a)} \\in \\mathbb{R}^{D_h \\times D} are learnable weight matrices. LN() denotes Layer Normalization. D_h = D/H is the dimension per attention head.\nThe attention weights for Spatio-Temporal attention are extracted as:\n\\alpha_{(p,t)}^{(l,a)} = SM(\\frac{q_{(p,t)}^{(l,a) T} [k_{(0,0)}^{(l,a)} {k_{(p',t')}^{(l,a)}}_{p'=1,...,N}]_{t'=1,...,F}}{\\sqrt{D_h}}) \\in \\mathbb{R}^{N F+1}\n(4)\nFor space-only attention within each frame:\n\\alpha_{(p,t)}^{space(l,a)} = SM(\\frac{q_{(p,t)}^{(l,a) T} [k_{(0,0)}^{(l,a)} {k_{(p',t)}^{(l,a)}}_{p'=1,...,N}]}{\\sqrt{D_h}}) \\in \\mathbb{R}^{N+1}\n(5)\nwhere: l: Index of transformer layer, a: Index of attention head, p, p': Spatial position indices of patches, t, t': Temporal frame indices, q_{(p,t)}^{(l,a)}, k_{(p,t)}^{(l,a)} \u2208 \\mathbb{R}^{D_h}: Query and key vectors, D = D/A: Dimension per attention head, N: Number of patches per frame (HW/P^2), F: Number of frames, (0,0): Index for classification token, SM(): Softmax operation.\n3) Temporal and Spatial Attention Aggregation: To attribute attention to both temporal and spatial dimensions, we aggregate the attention weights across patches and attention heads. The attributions can be calculated by processing the attention weights \u03b1 with Algorithm 3.\nThe temporal attention map, M_t, provides a frame-level importance score across the video sequence. We achieve this by averaging attention weights over all attention heads and patches within each frame. By doing so, we capture the contribution of each frame to the overall classification decision, revealing which parts of the video sequence are most relevant. The spatial attention map, M_s, highlights key regions within each frame by focusing on patch-level attention. This process allows us to determine which frames and spatial regions within each frame are most influential for the model's classification decision."}, {"title": "Algorithm 3 Spatio-Temporal Attention Attribution (STAA)", "content": "Input: Video clip X \u2208 \\mathbb{R}^{F \\times H \\times W \\times 3}, Model M\nOutput: M_t, M_s\nInitialize M_t \u2208 \\mathbb{R}^F, M_s \u2208 \\mathbb{R}^{N \\times F}\n// Extract attention from final layer L\n{\\alpha_{(p,t)}^{(L,a)}} \u2190 M(X)\n// Aggregate temporal attention\nM_t = \\frac{1}{A N} \\sum_{a=1}^A \\sum_{p=1}^N \\alpha_{(p,t)}^{(L,a)}\n// Aggregate spatial attention\nfor t = 1 to F do\nM_s[t] = \\frac{1}{A} \\sum_{a=1}^A \\alpha_{(:,t)}^{(L,a)}\nend for\nreturn M_t, M_s\nNotation: F: Number of frames, N: Patches per frame (HW/P\u00b2), A: Number of attention heads, L: Final layer, \u03b1_{(p,t)}^{(L,a)}: Attention weights at layer L, head a, M_t: Temporal attention map, M_s: Spatial attention maps\n4) STAA Computational Analysis and Limitations: STAA achieves significant computational efficiency through its direct utilization of model attention mechanisms. The computational cost can be analyzed in several dimensions: Attention Extraction Mechanism: STAA extracts feature importance directly from the self-attention weights computed in the Transformer model, requiring only O(N) operations where N is the input video clip size. Specifically, STAA accesses the attention values from the final layer of the Transformer that are naturally computed during video processing. This is fundamentally different from post-hoc methods: SHAP requires O(2^N) operations, where s is the number of temporal segments (8 in our implementation), due to its combinatorial sampling approach. LIME's complexity is O(KN_pN), where K is the number of selected frames and N_p is the number of perturbation samples (1000 in our implementation). STAA achieves superior efficiency by directly utilizing the model's internal attention mechanism rather than requiring additional sampling or perturbations.\nDespite its computational advantages, STAA faces several key limitations: Raw Attention Noise: When directly visualizing raw attention values as heatmaps overlaid on video frames, we observe significant frame-to-frame fluctuations (\"flickering\") and diffused attention patterns. The raw attention values often permeate entire frames, making it difficult to identify truly salient regions. This visualization issue suggests the need for attention refinement. Architecture Dependency: Differ from model-agnostic methods, STAA is specifically designed for Transformer architectures and relies on their self-attention mechanisms. This architectural dependency means STAA cannot be directly applied to other types of video models, such as traditional CNNs. Feature Granularity: The video Transformers (16\u00d716 pixel patches) actually enables fine-grained spatial analysis and frame-level temporal analysis. This is a significant improvement over post-poc methods. For a standard 224x224 video frame, this creates 196 patches per frame, allowing detailed spatial attention mapping. Each patch can independently receive attention weights. This is sufficient for action recognition tasks. However, applications requiring pixel-precise explanations may need further analysis.\nTo address the first limitation and further enhance the effectiveness of STAA, we developed a refinement procedure in the following subsection.\nD. STAA Enhancement\nWe observed that vanilla STAA resulting heatmaps video exhibits significant frame-to-frame fluctuations with background noise. To address these drawbacks of the vanilla method, we developed an enhancement approach focusing on dynamic thresholding and attention focusing.\n1) Dynamic Thresholding for Noise Reduction: We introduce dynamic threshold \u03b8_t to frames, calculated as:\n\u03b8_t = \u03bc_t + \u03bb\u03c3_t\n(6)\nwhere \u03bc_t and \u03c3_t are the mean and standard deviation of attention values for frame t, respectively. The hyperparameter \u03bb controls the threshold's strictness and can be empirically set to between [0, 1]. In our experiments, it is set to one to allow more attributes to be indicated. If it is set to zero, that means we filter half of the non-important attention attributes as noise.\n2) Attention Focusing Mechanism: We apply this threshold to the STAA results:\nS'[t, p] = \\begin{cases} S[t,p] & \\text{if } S[t, p] \\ge \u03b8_t \\\\ 0 & \\text{otherwise} \\end{cases}\n(7)"}, {"title": "4) STAA Computational Analysis and Limitations", "content": "This operation effectively focuses attention on the most salient spatio-temporal regions, reducing noise and improving the clarity of our explanations.\n3) Attention Visualization: To enable intuitive human interpretation of the model's focus and facilitate qualitative analysis of attention patterns, we transform the attention values into visualizable heatmaps. First, we normalize the filtered attention values to [0,1] range. Then, we blend the colorized heatmap with the original frame:\nH_t = \\frac{S_t - \\text{min}(S_t)}{\\text{max}(S_t) - \\text{min}(S_t)}\n(8)\nwhere H_t is the normalized attention map. The resulting visualization overlays attention heatmaps on video frames.\nAnswer to RQ1: STAA, as a new XAI method, extracts and processes attention values that capture both spatial importance within frames and temporal relevance across the video."}, {"title": "IV. EXPERIMENTAL EVALUATION AND COMPARATIVE ANALYSIS", "content": "We conducted comprehensive experiments to evaluate the effectiveness of STAA against established XAI methods using a large-scale video understanding dataset. Our experimental design addresses three key aspects: dataset selection and preprocessing, model architecture configuration, and evaluation methodology. We aim to evaluate Faithfulness and Monotonicity metrics for XAI. Faithfulness indicates how accurately each method's explanations reflect the model's decision-making process. Monotonicity indicates the consistency of feature importance rankings provided by each method.\nA. Experimental Design and Setup\nThe experiments were conducted using the Kinetics-400 dataset [5], a comprehensive benchmark containing approximately 240,000 video clips across 400 human action classes. This dataset represents diverse spatio-temporal patterns, from simple actions to complex sequences. Its widespread adoption in the video AI community [3], [6], [16], [17], [21] enables meaningful comparisons with existing work while providing sufficient complexity to assess XAI performance across varied scenarios. To address potential class imbalance effects on XAI evaluation, we implemented a balanced sampling strategy across action classes. We randomly selecting thirty video file from each of the four-hundreds action classes, resulting in a total of twelve-thousand videos for analysis.\nThe TimeSformer model [3] was employed as the model architecture, having achieved an accuracy of 78.0% on the Kinetics-400 validation set. This choice was motivated by TimeSformer's state-of-the-art performance and its representative implementation of self-attention mechanisms. The model processes input videos at 224\u00d7224 pixel resolution.\nThe experiments were conducted on a system equipped with an NVIDIA GeForce RTX 4090 GPU, running Ubuntu 22.04.5 LTS. This configuration enables efficient processing of video data. All implementation code and experimental configurations are available in our public repository to facilitate reproducibility and further research in video XAI methods.\nB. Quantitative XAI Evaluation Metrics\nTo assess the explanations generated by our XAI methods, we developed two metrics: faithfulness and monotonicity. These metrics are designed according to the fundamental properties of feature importance.\nThe concept of faithfulness in XAI evaluation was initially developed for image classification tasks [23], [32], measuring how accurately an explanation reflects a model's decision-making process. We extend this concept to video XAI analysis. It quantifies the degree to which the identified important features truly influence the model's output. Monotonicity serves as a more detailed complementary metric. Monotonicity evaluates whether features assigned higher importance scores indeed have a more significant impact on the model's predictions.\n1) Faithfulness: Faithfulness quantifies how accurately an explanation represents the model's decision-making process. We define this metric as:\nfaithfulness = \\frac{1}{N} \\sum_{i=1}^N |f_{\\text{norm}}(x_i) - f_{\\text{norm}}(x_i \\backslash S_i)|\n(9)\nwhere f_{\\text{norm}}(x_i) is the normalized prediction for the original input x_i, f_{\\text{norm}}(x_i \\backslash S_i) is the normalized prediction after masking the features s_i identified as important, and N is the number of test samples.\nIn our implementation, We use a mask ratio of 0.7, removing 70% of the regions deemed most important by the explanation. We apply min-max normalization to the predictions before calculating the difference. The faithfulness score ranges from 0 to 1, with higher values indicating better XAI performance. A score closer to 1 suggests that masking important features significantly changes the model's normalized prediction, indicating that the explanation has accurately identified crucial features.\n2) Monotonicity: Monotonicity assesses whether the importance rankings produced by an explanation method are consistent with their impact on the model's predictions. We quantify this using Kendall's rank correlation coefficient (Kendall's tau) [33], a statistical measure that evaluates the ordinal association between two rankings:\nmonotonicity = \u03c4(p_k, d_k)\n(10)\nwhere: p_k represents K numbers ascending sequence of masking ratios, for example, [0.1, 0.2, ..., 0.9] in ten ratios, d_k = f_{\\text{softmax}}(x)[C] - f_{\\text{softmax}}(x \\backslash S_k)[C] represents the changes in prediction probability when masking top-K important features, S_k represents regions masked according to their importance, \u03c4 is the rank correlation coefficient [33].\nIn our context, a high positive value suggests that as we mask more important features (increasing p_k), we observe proportionally larger changes in the model's predictions (d_k), confirming that the XAI method has consistently identified the important features. Conversely, a value close to -1 indicates that the rankings are completely opposite, suggesting that features identified as important by the XAI method actually have a worse impact on the model's predictions.\nThis metric can be applied on either temporal feature or spatial feature or even both. For temporal analysis, it evaluates whether masking important time segments leads to consistent prediction changes. For spatial analysis, it assesses the impact of masking important regions within frames. When applied to Spatio-Temporal features, we mask them both. It measures the overall consistency of the selected XAI methods.\nC. Results Analysis and Method Comparison"}, {"title": "V. REAL-TIME VIDEO \u03a7\u0391\u0399 CLOUD ARCHITECTURE", "content": "This section presents a cloud-based architecture that enables real-time explanation generation for video analysis", "13": "."}, {"13": ".", "analysis": "the Video Sources component serves as the data input, providing various streams such as live camera feeds; the Video Preprocessing module performs essential tasks including temporal segmentation, frame extraction, and resizing; the Transformer Model implements state-of-the-art video analysis capabilities such as TimeSformer [3"}]}