{"title": "ROSE: A REWARD-ORIENTED DATA SELECTION FRAMEWORK FOR LLM TASK-SPECIFIC INSTRUCTION TUNING", "authors": ["Yang Wu", "Huayi Zhang", "Yizheng Jiao", "Lin Ma", "Xiaozhong Liu", "Jinhong Yu", "Dongyu Zhang", "Dezhi Yu", "Wei Xu"], "abstract": "Instruction tuning has underscored the significant potential of large language models (LLMs) in producing more human-controllable and effective outputs in various domains. In this work, we focus on the data selection problem for task-specific instruction tuning of LLMs. Prevailing methods primarily rely on the crafted similarity metrics to select training data that aligns with the test data distribution. The goal is to minimize instruction tuning loss on the test data, ultimately improving performance on the target task. However, it has been widely observed that instruction tuning loss (i.e., cross-entropy loss for next token prediction) in LLMs often fails to exhibit a monotonic relationship with actual task performance. This misalignment undermines the effectiveness of current data selection methods for task-specific instruction tuning. To address this issue, we introduce ROSE, a novel Reward-Oriented inStruction data sElection method which leverages pairwise preference loss as a reward signal to optimize data selection for task-specific instruction tuning. Specifically, ROSE adapts an influence formulation to approximate the influence of training data points relative to a few-shot preference validation set to select the most task-related training data points. Experimental results show that by selecting just 5% of the training data using ROSE, our approach can achieve competitive results compared to fine-tuning with the full training dataset, and it surpasses other state-of-the-art data selection methods for task-specific instruction tuning. Our qualitative analysis further confirms the robust generalizability of our method across multiple benchmark datasets and diverse model architectures.", "sections": [{"title": "1 INTRODUCTION", "content": "While large language models (LLMs) are widely recognized for their strong generalization capabilities, many fields require enhanced domain-specific performance, e.g., health monitoring (Kim et al., 2024b), legal question answering (Wu et al., 2024), and mathematics tutoring (Li et al., 2023). Instruction tuning has emerged as a popular method for adapting foundation models to specialized tasks, which typically involves curating a high-quality training dataset. Although recent advancements in open-source datasets and synthetic data generation have facilitated the generation of large training datasets, it is widely acknowledged that the quality of training data is more crucial than its quantity in instruction tuning (Chen & Mueller, 2024; Xia et al., 2024). Consequently, practitioners must carefully select high-quality data to enhance the model's capabilities for specific tasks. This challenge is further compounded by the complexity of domain-specific requirements and the black-box properties of LLMs, rendering it nearly infeasible for humans to manually select the most suitable training set. Therefore, developing more effective data selection methods is becoming increasingly crucial for reducing training costs and efficiently optimizing instruction tuning for specific tasks."}, {"title": "2 PRELIMINARIES AND BACKGROUND", "content": "Problem Definition. We tackle the challenge of data selection for task-specific instruction tuning. Our goal is to curate a subset Dtrain from a broad and comprehensive instruction tuning corpus D, such that training a model on Dtrain to maximize a reward r that reflects true performance on a task-specific validation set Dval, therefore performs well on test dataset Dtest. Dval can be a few-shot dataset involving multiple target tasks, and the Dtest is a fixed sample set with the same tasks in Dval. We use \u03a9 parametrized by \u03b8 to denote the model used for data selection and \u0393 parametrized by \u03b8' to represent the final trained model.\nAnalysis of Similarity Based Methods. Let Z = Rd be the d-dimensional intrinsic representation space, where the similarity-based methods calculate the similarity between the training and validation samples. Let pt(z) and pv(z) be the probability density value of the selected training set Dtrain and validation set Dval, respectively, where z \u2208 Z. When a downstream LLM \u0393 is well trained on the selected training set Dtrain, its training loss on Dtrain is expected to be close to 0, i.e., Etrain[l(z, \u03b8')] = \u222b l(z, \u03b8')pt(z) dz \u2248 0, where l(z; \u03b8') represents the average of token-wise cross entropy loss in the response sequence of z.\nThe objective of similarity-based methods is to select a training set Dtrain which is independent and identically distributed (IID) with respect to the validation set (Xie et al., 2023; Zhang et al., 2021a). Under this condition, for any z \u2208 Z, pt(z) = pv(z). Let Eval[l(z, \u03b8')] = \u222bl(z, \u03b8')pv(z) dz be the expected loss value of \u0393 on the validation set. By simply substituting pv(z) with pt(z), we can infer that the downstream LLM is supposed to have small loss value of l(z; \u03b8') on the validation set, i.e., Eval[l(z, \u03b8')] = \u222b l(z, \u03b8')pt(z) dz \u2248 0.\nUnfortunately, existing studies (Zhou et al., 2024; Xia et al., 2024; Tay et al., 2021) commonly find that the decrease in validation loss does not always lead to improved test performance in task-specific instruction tuning. Furthermore, achieving such an IID condition is often unrealistic due to the complexity of the representation space Z. Therefore, it is reasonable to conclude that the intrinsic limitation of large models, namely the gap between next-token prediction loss and actual performance on downstream tasks, compromises the effectiveness of existing data selection methods for task-specific instruction tuning.\nInfluence Estimation Scheme. Assume the selection model LLM, denoted as \u03a9, assesses the influence of training data points with respect to a set of validation samples Dval, which represents the model's capability on specific tasks. We denote the average loss value of \u03a9 on the validation set as L(Dval; \u03b8).\nFor simplicity, we assume the LLM is trained with a batch size of 1 using the SGD optimizer. At training step t, the contribution of a training sample z corresponds to the difference between the validation losses L(Dval; \u03b8t) and L(Dval; \u03b8t\u22121), i.e., L(Dval; \u03b8t) \u2013 L(Dval; \u03b8t\u22121). Using a first-order Taylor expansion, this becomes:\nL(Dval; \u03b8t) - L(Dval; \u03b8t\u22121) = (\u2207\u03b8L(Dval; \u03b8t\u22121), \u03b4\u03b8)\nwhere (,) denotes the inner product, and \u03b4\u03b8 = \u03b8t - \u03b8t\u22121 represents the change in \u03b8 at step t. With the SGD optimizer, \u03b4\u03b8 = \u2212\u03b1 \u00b7 \u2207\u03b8l(z; \u03b8t\u22121), where \u03b1 is the learning rate and \u2207\u03b8l(z; \u03b8t\u22121) denotes the gradients of the loss with respect to the training sample z. Substituting this into the equation, we get:\nL(Dval; \u03b8t) - L(Dval; \u03b8t\u22121) \u221d (\u2207\u03b8L(Dval; \u03b8t\u22121), \u2207\u03b8l(z; \u03b8t\u22121))\nEq. 2 shows that the inner product between the gradient of the loss on the training sample z and the gradient of the average loss on the validation set Dval effectively estimates the degree to which a training sample contributes to the model's performance. A positive gradient inner product indicates that the training sample z positively impacts the model's performance.\nNote that in the typical LLM training settings, the optimizer is usually a variant of Adam, and the training batch size is often larger than 1. This creates interactions between training samples and across batches. To address this, we adopt the heuristic-based methods used in LESS, performing a warm-up training on the LLM \u03a9 and applying a variant of the gradient \u2207\u03b8l(z; \u03b8t\u22121) to reduce the discrepancy."}, {"title": "3 ROSE: REWARD-ORIENTED INSTRUCTION DATA SELECTION", "content": "In this section, we introduce ROSE, a reward orientated data selection method for task-specific instruction tuning. We start from formulating the optimization objectives of ROSE, followed by a detailed explanation of implementation strategies."}, {"title": "3.1 OPTIMIZATION FRAMEWORK", "content": "Motivated by the analysis in Section 2, the objective of ROSE is to select a subset Dtrain that leads to a downstream LLM \u0393 with maximized reward value on validation set Dval. Formally, we define Dval = {(xi, yi)}Dvali=1, where xi, yi denote the prompt and response of samples from Dval, respectively. Inspired by Reinforcement Learning from Human Feedback (RLHF) (Bai et al., 2022) and further study Direct Preference Optimization (DPO) (Rafailov et al., 2024), we define the reward function r for ROSE through a closed-form expression as:\nr(x, y) = \u03b2 log  \u03a9\u03b8(y | x) / \u03a9ref(y | x) + \u03b2 log Z(x).\nwhere \u03a9\u03b8 and \u03a9ref represent the policy and reference models, respectively. The term log Z(x) represents the partition function, and \u03b2 signifies the parameter that controls the deviation from the baseline reference model\u00b9. For optimization efficiency, we advance our methodology by first transforming the few-shot validation dataset Dval into a few-shot preference validation set D'val = {(xi, ywi, yli)}Dvalli=1, where (xi, yw, yl) refers preference pairs from the dataset Dval, comprising the prompt, a winning response, and a losing response. By integrating this reward structure with the Bradley-Terry (BT) ranking model (Bradley & Terry, 1952), we leverage the probability of preference data directly through the policy model, formulating the following optimization objective:\nLROSE(\u03a9\u03b8; \u03a9ref) = \u2212E(x, yw, yl)\u223cD'val [log \u03c3 ( \u03b2 log  \u03a9\u03b8(yw | x) / \u03a9ref(yw | x) - \u03b2 log  \u03a9\u03b8(yl | x) / \u03a9ref(yl | x) ) ],\nwhere \u03c3 is the logistic function. We can implicitly define rc(x, y) = \u03b2 log  \u03a9\u03b8(y | x) / \u03a9ref(y | x) , thus the gradient with respect to the parameters \u03b8 can be written as:\n\u2207\u03b8LROSE(\u03a9\u03b8; \u03a9ref) =\n\u03b2E(x, yw, yl)\u223cD'val [\u03c3(rc(x, yw) - rc(x, yl)) (\u2207\u03b8 log \u03a9\u03b8(yw | x) - \u2207\u03b8 log \u03a9\u03b8(yl | x) ) ]"}, {"title": "3.2 IMPLEMENTATIONS", "content": "Here, we describe how ROSE adapts the Equation 5 to select instruction data that can effectively improve model capability in target tasks, and illustrate our method in Figure 2.\nIn practice, we use the pretrained model as reference model to calculate the gradients for validation data."}, {"title": "3.2.1 BUILD FEW-SHOT PREFERENCE VALIDATION DATA", "content": "The current validation set used in task-specific instruction tuning typically adheres to a Supervised Fine-Tuning (SFT) format, featuring a prompt and its corresponding response. However, the widely used evaluation metric for test data in instruction tuning is the win rate, which assesses the frequency at which a target model's response is deemed superior compared to the original test dataset response, as determined by an LLM evaluator. To align with the win rate employed in downstream tasks, we transform the few-shot SFT validation set Dval = {(xi, yi)}Dvali=1 into a preference format, denoted as D'val = {(xi, ywi, yli)}Dvalli=1. This transformation involves generating additional responses to prompts within the SFT dataset, which are then evaluated by either domain experts or advanced LLMs. This technique is also explored in other LLM alignment research, exemplified by the works of Arif and Kim (Arif et al., 2024; Kim et al., 2024a). Considering only the limited number of samples per task in the validation set, this methodology does not require substantial computational resources or extensive human annotations. Once established\u00b2, the generated few-shot preference validation set is consistently used in subsequent steps to represent actual task-specific data."}, {"title": "3.2.2 REWARD-ORIENTED GRADIENT CALCULATION", "content": "Inspired by (Xia et al., 2024), we use the Adam (Kingma, 2014) and SGD optimizer for training data points and validation data points gradient calculation, respectively. Since Adam involves the first and second moments, we start ROSE by initially training a LLM \u03a9 with a randomly selected subset (5%) of training data. Since computing and storing the gradients of a LLMs with billions of parameters are very computational and storage expensive, we use LoRA (Hu et al., 2021) to train the model efficiently. To further reduce the feature dimension, we use TRAK (Park et al., 2023) to randomly project the LoRA adapters' gradients into a lower dimension, the default setting of projection dimension is 8192.\nIn initial-training stage, we save multiple checkpoints, the default number of checkpoints is 4. Since there are multiple subtasks in validation set, for each subtask D'(j)val, we compute the average gradient feature on each checkpoints \u03b81, ..., \u03b8i. Let z and z' denote the sample from training corpus D and few-shot preference validation set D'val. The SGD gradient calculation for D can be defined as:\n\u2207\u03b8LROSE(D'(j)val; \u03b8i) = 1/|D'(j)val| \u2211z'\u2208D'(j)val \u2207\u03b8LROSE(z'; \u03b8i).\nwhere LROSE(z'; \u03b8i) is calculated by adapting the Equation 5. For each training data point z, the Adam gradient can be differentiated as \u2207\u03b8l(z; \u03b8i), where l(\u00b7; \u03b8) represents the average of token-wise cross entropy loss in the response sequence of z."}, {"title": "3.2.3 DATA SELECTION PROCESS", "content": "In the data selection stage, we aggregate the scores from all checkpoints to assess how closely each training data point aligns with the validation set. We define the calculation of ROSE influence scores as follows:\nS(z, D'(j)val) = 1/N \u2211i=1 n_i (\u2207\u03b8LROSE(D'(j)val; \u03b8i), \u2207\u03b8l(z; \u03b8i)).\nwhere N and \u03b7i denote the number of checkpoints and the learning rate of each checkpoint, respectively. After calculating the influence score of each training data point to validation set, we use the maximum score across all subtasks. Finally, we select the most influential datapoints to construct the selected training dataset Dtrain to train downstream model \u0393. Our approach employs a computational pipeline analogous to LESS (Xia et al., 2024), encompassing warmup LoRA training, gradient feature computation, and data selection. The overall computational and storage requirements are comparable to those reported in LESS, ensuring scalability and efficiency."}, {"title": "4 EXPERIMENT", "content": "4.1 EXPERIMENTAL SETUP\nModel Architecture and Training Settings. We use three instruction fine-tuning training datasets: DOLLY (Conover et al., 2023), OPEN ASSISTANT 1 (K\u00f6pf et al., 2024), FLAN V2 (Longpre et al., 2023), and COT (Wei et al., 2022), which collectively comprise around 270K data points across various reasoning tasks, as detailed in Appendix A.1. In our experiments, we engage two prominent model families: Llama (AI@Meta, 2024) and Mistral (Jiang et al., 2023), including LLAMA-2-7B, LLAMA-2-13B (Touvron et al., 2023), LLAMA-3.1-8B, LLAMA-3.1-8B-INS., MISTRAL-7B-v0.3, and MISTRAL-7B-INS.-V0.3 (Jiang et al., 2023).\nEach model trains utilizing LoRA (Hu et al., 2021) for training efficiency in optimizing large-scale models. LoRA settings remain uniform across all models with a rank of 128, an alpha of 512, and a dropout rate of 0.1. Training involves learning LoRA matrices for all attention mechanisms in each configuration. The models optimize using the AdamW optimizer with a learning rate of 2 \u00d7 10\u22125, and each configuration undergoes four training epochs with a batch size of 128. To ensure robustness and reproducibility, we conduct three trials per configuration with varying random seeds. During the gradient extraction stage for preference validation, we compute gradients using the pretrained model as a reference model and the warm-up model as a policy model. We utilize Direct Preference Optimization (DPO) (Rafailov et al., 2024) loss to calculate gradients and apply the TRAK algorithm (Park et al., 2023) to project LoRA gradients into a vector of 8192 dimensions for each data point. For inference, we set the temperature to 1.0, top_p to 1.0, and max tokens to 4096.\nEvaluation Benchmarks and Metrics. We assess our models using three leading open-source preference benchmarks: Stanford Human Preference (SHP) (Ethayarajh et al.), Stack Exchange (SE) (Lambert et al., 2023), and HH-RLHF (Bai et al., 2022; Ganguli et al., 2022). Each dataset includes multiple subtasks, with validation data details provided in Appendix A.2 and further test data information in Appendix A.3. Our evaluation metric is the Win Rate (WR), comparing each model's response against the most preferred response from the test dataset. And we employ the GPT-4-32K-0613 model (OPENAI, 2024) as the judge model, with the evaluation prompts detailed in Appendix D.\nBaselines. Our method, ROSE, is benchmarked against a diverse set of baselines. The Random baseline entails indiscriminate sampling from the entire training dataset for instruction finetuning. For a more structured approach, we employ BM25 (Robertson et al., 2009), a well-known ranking function in information retrieval that evaluates document relevance using term frequency and inverse document frequency (TFIDF) with length normalization. Here, we prioritize training instances with the highest BM25 scores for finetuning. Another strategy, representation-based data selection (RDS) (Zhang et al., 2018), leverages the last hidden layer of the model to determine similarity between training and validation data points. Furthermore, DSIR Xie et al. (2023) utilizes n-gram features to assign importance weights to training samples, guiding the selection of finetuning data. Additionally, we explore the efficacy of Shapley values (Fryer et al., 2021) in assessing each data point's unique contribution to model performance. Similarly, Influence Functions (Koh & Liang, 2017) calculate the impact of individual data points' modification or removal on model predictions, aiding in the identification of pivotal training instances. Both Shapley values and Influence Functions necessitate labels for the training data; to accommodate this, we implement K-Means clustering (K = 3) to assign provisional labels based on cluster membership, enhancing sample diversity in our evaluations. Another baseline is LESS (Xia et al., 2024), which leverages next-token prediction loss to extract gradients from both the training and validation sets, subsequently calculating the influence score to select the most task-relevant training samples. For a fair comparison, all baselines, including ROSE, select the same percentage (5%) of data from the training set. Moreover, we consider pretrained LLMs (W/O Finetuning), instruction finetuning on the full training dataset (Full), and finetuning directly on the few-shot validation set (Valid.) as additional comparisons."}, {"title": "4.2 EXPERIMENTAL RESULTS", "content": "In this section, we present empirical results and analysis of our experiments, highlighting the superior performance of ROSE on various benchmarks. Unless otherwise specified, the experiments are conducted using the LLAMA-2-7B as the base model setting."}, {"title": "5 RELATED WORK", "content": "Data Selection for Instruction Tuning. Instruction tuning is crucial for aligning large language models (LLMs) with human needs (Wang et al., 2024), providing a controlled and safe method to enhance LLMs' responsiveness and accuracy in specific domains. Wei et al. (2023) introduced InstructionGPT-4 for multimodal large language fine-tuning, which involves encoding visual and textual data into vectors to train a trainable data selector. Furthermore, RDS Zhang et al. (2018); Hu et al. (2023) employs the model's last hidden layer to assess the similarity between training and validation data points, while DSIR Xie et al. (2023) uses n-gram features to assign importance weights to training samples for data selection in instruction fine-tuning. Another notable study, LESS Xia et al. (2024), follows a similar approach by selecting the most influential data from the training corpus based on the gradient similarity score of training data points with validation data points. However, these methods typically utilize next token prediction cross-entropy loss to train data selectors, which inherently lacks a monotonic relationship with test accuracy or win rate. To address this, we propose ROSE, which directly optimizes preferences using pairwise preference data as a validation set, selecting task-relevant training data by calculating influence scores.\nData Attribution and Influence Functions. The influence calculation of training data points is a pivotal technique for detecting mislabeled samples (Deng et al., 2024; Zhang et al., 2021b; 2024; Hofmann et al., 2022), facilitating model interpretation (Madsen et al., 2022; Wu et al., 2023; VanNostrand et al., 2023), and analyzing memorization effects (Feldman & Zhang, 2020). Specifically, influence functions Koh & Liang (2017) offer a counterfactual method to assess both model behaviors and the contributions of training data. Despite their potential, the robustness and effectiveness of these functions remain limited, particularly in the context of large language models (LLMs), where their computational demands are significant. While recent studies, such as those by Park et al. (2023), propose relatively efficient estimations of influence functions for selecting pretraining data, these methods still require complex comparisons of model training with and without the inclusion of specific data points. In line with the approach by (Xia et al., 2024), we advocate that first-order influence approximations are effective for data selection during instruction tuning in LLM environments.\nLarge Language Model Alignment. LLM alignment aims to train large language models (LLMs) to behave in ways that align with human expectations. A primary approach for this is Reinforcement Learning from Human Feedback (RLHF), which tunes LLMs to reflect human preferences and values (Ziegler et al., 2019; Bai et al., 2022). It has been effectively applied in various domains, including enhancing model helpfulness (Tian et al., 2023), improving reasoning capabilities (Havrilla et al., 2024), and mitigating toxicity (Korbak et al., 2023). Despite its effectiveness, RLHF as an online preference optimization algorithm, poses significant challenges and complexities. In contrast, Direct Preference Optimization (DPO) (Rafailov et al., 2024) offers a simpler and more efficient offline alternative. Recent research has extended DPO beyond traditional pairwise comparisons to include evaluations across multiple instances (Liu et al., 2024; Yuan et al., 2024). Further advancements have broadened preference optimization objectives, including those independent of reference models (Xu et al., 2023; Meng et al., 2024). These reward-oriented methods outperform models trained with next-token prediction loss in satisfying user preferences, as they directly optimize preference signals, whereas next-token prediction loss focuses on minimizing the difference between generated outputs and predefined labels, which may not reflect user-specific preferences."}, {"title": "6 CONCLUSION", "content": "We propose ROSE, a novel instruction tuning data selection method based on influence estimation. Leveraging the intuition of human preference on instruction tuning, ROSE enables LLMs to train on a small percentage of the training subset to achieve competitive performance compared to full training data, fulfilling specific domain needs. Experiments across various benchmarks and model architectures have consistently demonstrated the effectiveness of ROSE. Moreover, we provide empirical analysis and insights to solve the non-monotonic relationship between validation loss and test accuracy or win rate in instruction tuning. Due to computational limitations, our experiments were conducted on Llama and Mistral models up to 13 billion parameters. In the future, we hope to have sufficient computational resources to validate the effectiveness of ROSE on larger and more powerful LLMs."}, {"title": "A DATASETS", "content": "A.1 TRAINING DATA DETAILS\nFor the training corpus, we amalgamate four open-source instruction-tuning datasets, as referenced in Wang et al. (2023). Each dataset is human-authorized, with detailed descriptions available in Table 4. Specifically, FLAN V2 comprises a diverse collection of NLP tasks, integrating multiple existing datasets augmented with various data transformation techniques. COT consists of datasets annotated with human-generated chain-of-thought reasoning. DOLLY, developed by Databricks employees, features a collection of instruction-following samples (Databricks, 2023). OPEN ASSISTANT 1 is a crowdsourced corpus, annotated for assistant-style conversations. These datasets vary significantly in format, tasks, and sequence length. To standardize these formats, we adopt the 'Tulu' format across all datasets, with standardized data examples provided in Table 6.\nA.2 VALIDATION DATA DETAILS\nFor our few-shot preference validation set, we utilize three preference datasets (SHP, SE, and HH) to exemplify domain-specific tasks. Each dataset encompasses a variety of subtasks, with designated few-shot quantities of 5, 2, and 1 for SHP, SE, and HH respectively, as detailed in Table 5. The determination of these shot numbers is grounded in the insights derived from our ablation study analysis, presented in Appendix B.1. Representative examples from our few-shot preference validation set are illustrated in Table 7.\nA.3 TEST DATA DETAILS\nThe details of the test data are presented in Table 8. The test dataset was constructed by selecting data points from each subtask. For the SHP dataset, which includes 18 subtasks, we selected data from each subtask where the ratio of Score(response_win) to Score(response_loss) was at least 3. We then chose the minimum between the total number of available instances per subtask (#subtask_instance) and 100 to comprise the SHP test dataset. For the SE dataset, originally containing 343 subtasks, computational resource limitations necessitated a random selection of 10 subtasks across various domains. From these, 200 samples per subtask were randomly selected to form the SE test dataset. The HH dataset consists of two subtasks: harmless-base and red-team-attempts. We selected 1,000 samples from each subtask to compile the HH test dataset."}, {"title": "B ABLATION STUDIES", "content": "B.1 PERFORMANCE COMPARISON ACROSS DIFFERENT VALIDATION SHOTS.\nFigure 4 illustrates the performance comparison of ROSE against two baselines-LESS selection and random selection across varying numbers of validation shots for the SHP, SE, and HH datasets. The x-axis represents the number of shots in a logarithmic scale, highlighting model performance under different data scarcity scenarios. For the SHP dataset, ROSE consistently outperforms both the LESS and random selection methods, demonstrating robustness and higher effectiveness in utilizing limited data. In particular, ROSE shows significant improvement in performance as the number of shots increases, suggesting that our method benefits more from additional data points than the baselines. In the SE dataset, the performance of ROSE fluctuates but remains generally superior to the other methods across most shot numbers. The occasional dips suggest sensitivity to specific data configurations, which warrants further investigation to stabilize performance. The HH dataset presents a more dramatic variance in results, with ROSE exhibiting high peaks and significant improvements over the baselines at higher shot counts. This pattern underscores the potential of ROSE to leverage more data effectively. Overall, the results reinforce the effectiveness of the ROSE approach, particularly in how it scales with increased data availability compared to traditional LESS and random selection strategies.\nB.2 TRANSFER ABILITY ANALYSIS\nIn this section, we examine the transfer capabilities of ROSE. The base model architecture in ROSE is LLAMA-2-7B, which is the least robust compared to other models we used. Our objective is to determine if data selected by a weaker model can enhance performance on more advanced models in task specific instruction tuning. The findings are presented in Table 9. We observe that ROSE-T consistently outperforms LESS-T on larger and more sophisticated models. Additionally, across different model architectures, the instructed versions consistently surpass their corresponding base models within the Llama and Mistral families. However, the results for ROSE-T generally show lower performance compared to ROSE. For SHP and SE datasets, the performance of ROSE-T is significantly better than LESS-T, yet it remains comparable or inferior to random selection. Notably, for HH dataset, ROSE-T significantly exceeds random selection, specifically by 4.1%, 12.3%, and 11.1% on LLAMA-3.1-8B, LLAMA-3.1-8B-INS., and MISTRAL-7B-INS.-v0.3 respectively.\nC SUBTASK RESULTS IN EACH BENCHMARK DATASET\nTo provide a detailed performance comparison with baseline models, we present the results for individual subtasks. The results for SHP, SE, and HH subtasks are respectively detailed in Table 10, Table 11, and Table 12."}, {"title": "D EVALUATION PROMPT", "content": "We compare the model's response with the highest-annotated response from the original dataset. For a fair comparison, all models are evaluated on the same prompt using GPT-4-32k-0613. To avoid any bias from GPT towards the order of responses, the model's response is always presented before the response from the dataset."}, {"title": "E ROSE DATA SELECTION ALGORITHM", "content": "Algorithm 1 ROSE Data Selection Algorithm\n1: Input: training data corpus D, few-shot SFT validation data Dval, a selection and scoring LLM \u03a9 and a final training model \u0393.\n2: Randomly select 5% data from D to initially train the selection model \u03a9 to \u03a9'.\n3: Transform the few-shot SFT validation data Dval into a few-shot preference validation set D'val\n4: Backpropagate on selection model \u03a9'. Extract Adam gradient of each data point in D, and extract SGD gradient of Dval based on Equation 6.\n5: Compute influence score for each training data point to validation set based on Equation 7.\n6: Select the top 5% data with highest influence scores from D as the selected training set Dtrain.\n7: Train model \u0393 on Dtrain\n8: Output: Finetuned instruction LLM \u0393' for target tasks."}, {"title": "F ROSE-SELECTED DATA EXAMPLES & CASE STUDIES.", "content": "Table 13: Examples with the highest influence scores across various validation datasets. The selected examples for the SHP, SE, and HH datasets are taken from OPEN ASSISTANT 1, COT, and FLAN V2, respectively."}]}