{"title": "UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent", "authors": ["Jianke Zhang", "Yanjiang Guo", "Yucheng Hu", "Xiaoyu Chen", "Xiang Zhu", "Jianyu Chen"], "abstract": "Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for robotic control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce UP-VLA, a Unified VLA model training with both multi-modal Understanding and future Prediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information.", "sections": [{"title": "1. Introduction", "content": "Constructing Vision-Language-Action (VLA) models (Bro-han et al., 2023; Li et al., 2023b) capable of solving multiple tasks in open environments has become a central focus of research in robotics. A promising approach for VLA models involves fine-tuning large-scale pre-trained Vision-Language Models (VLMs) (Li et al., 2023a; Wang et al., 2022; Dai et al., 2024; Driess et al., 2023) on robotic action datasets, incorporating appropriate action modeling components (Jang et al., 2022; Li et al., 2023b; Wu et al., 2023;Zhang et al., 2024; Kim et al., 2024; Zheng et al., 2024b). This method enables VLA models to inherit the semantic knowledge and reasoning capabilities encoded in powerful VLMs, thereby enhancing decision-making in unknown environments.\nHowever, previous works have identified certain weaknesses in VLMs, particularly in capturing low-level information and understanding physical dynamics (Zheng et al., 2024a; Chen et al., 2024a). Zheng et al. (2024a) highlighted that VLMs are weak in low-level vision tasks without additional training. Chen et al. (2024a); Wen et al. (2024) pointed out that pretrained VLMs lack spatial understanding and fail to capture low-level details such as distance and size differences. Furthermore, studies (Balazadeh et al., 2024;Ghaffari & Krishnaswamy, 2024; Li et al., 2024) have revealed significant challenges in VLMs' ability to understand physical dynamics. These limitations are largely attributed to the pre-training paradigm of VLMs (Wen et al., 2024; Chen et al., 2024a), which prioritizes multi-modal understanding tasks, such as Visual Question Answering (VQA), that enhance semantic reasoning but may overlook low-level details that are crucial for embodied decision-making tasks. While the generalization advantages offered by current pre-training approaches are desirable, they raise an important question: can a better training pipeline be developed to combine the strengths of both worlds, retaining semantic understanding while also emphasizing low-level features critical for control?\nIn this paper, we re-consider the pre-training approach for VLA models. Rather than focusing exclusively on high-level semantic information like in vision-language pre-training, we propose a training paradigm that emphasizes both high-level semantic understanding and low-level visual patterns. Inspired by prior papers on visual pre-training (Wu et al., 2023; Guo et al., 2024), we introduce a novel training paradigm for VLA models that aligns representations with both high-level features using the multi-modal understanding dataset and low-level features through future predictive generation. Specifically, we co-train an auto-regressive model with a flexible attention mask on three types of datasets, as illustrated in Figure 1.\nOur experiments validate the effectiveness of our new training paradigm for VLA models. As summarized in Figure 2, we tested three clusters of tasks ranging from simulation to real world settings to assess different abilities of the algorithms. ABCD\u2192D and real-seen focus on evaluating the model's in-distribution multitask learning capabilities, real-unseen focus on real-world semantic generalization, while the remaining two tasks measure the methods' adaptation and precise control abilities. In alignment with our previous analysis, the VLM-based VLA model demonstrates relatively strong performance in both in-distribution multitask settings and the real-world generalization task (real-unseen). Conversely, visual prediction-based pre-training achieves relatively better scores in tasks requiring adaptation and precise control (real-precise and ABC-D). Notably, UP-VLA achieves a 33% improvement on the Calvin ABC\u2192D generalization benchmark and shows significant improvement in real-world task. These results highlight the effectiveness of UP-VLA method in retaining both semantic and low-level features. Our contributions can be summarized as follows.\n1. Motivated by recent insights into the limitations of VLMs, we integrate video datasets rich in detailed information and dynamic contexts into the pre-training of VLA models to enhance their capabilities.\n2. We introduce a novel training paradigm for VLA models that combines both vision-language understanding and future prediction objectives, enabling the capture of both high-level semantic and low-level visual patterns essential for embodied agents.\n3. We achieve significant improvements in success rates across both simulated and real-world manipulation tasks. Additionally, we conduct an ablation study to validate the effectiveness of two types of pre-training."}, {"title": "2. Related Works", "content": "VLA Models for Generalist Robot Policies Recent studies have explored the application of VLMs (Li et al., 2023a;"}, {"title": "3. Preliminaries", "content": "VLA for Language Conditioned Robot Control The\nlanguage-conditioned manipulation problem is considered\na decision sequence under the environment modeled by a\nfree-form language instruction I specifying a certain task\nand the initial observation 01. For demonstrations D =\n{T1, T2, \u2026, Tn }, where each frame Ti = {(ot, at)}{=1 consists visual observations o and actions a. Vision-Language-Action (VLA) models typically train VLMme as a robotic action policy by minimizing the error between a ~ \u03c0\u03bf(\u03bf, 1). Leveraging the multi-modal understanding capability of VLM, VLA has better generalization across tasks, especially enhanced semantic understanding of unseen objects and improved ability to understand or reason complex natu-ral language instructions.\nUnified Vision-Language Pretraining via Autoregressive\nModelling Unified multi-modal language models are ca-pable of understanding and generation. An effective and scalable approach is to improve the VLMs with an additional image generation task, as demonstrated in works like SeeD-X (Ge et al., 2024) and Showo (Xie et al., 2024). Following these approaches, we utilize a discrete image encoder to handle encoding and decoding for image-generation tasks, while employing a continuous vision encoder for multi-modal understanding and reasoning tasks. During training, the LLM input is prompted based on the task type in the following format:\n{|MMU|, (U1, U2, \u2026\u2026\u2026, Un), (11, 12,\u2026\u2026,lm)}\n{|T2I|, (11,12,\u2026, Im), (V1, V2, \u2026\u2026\u2026, Un)}\nwhere l represents language tokens, and u, v correspond to continuous and discrete image tokens for different tasks."}, {"title": "4. Methodology", "content": "Our goal is to develop a better training schema for VLAs.\nIn this section, we describe the details of UP-VLA. We\nfirst build our backbone on top of a unified VLM. Then,\nwe design a unified mechanism to bridge the gap between\nvisual prediction with multi-modal understanding. Finally,\nwe enhance action learning with unified prediction and un-derstanding prompting techniques."}, {"title": "4.1. Backbone", "content": "As illustrated in Figure 3, we employ Phi-1.5 (Li et al.,\n2023c) as the underlying large language model. For multi-modal understanding tasks, we follow the standard VLM"}, {"title": "4.2. Bridging Visual Prediction and Multi-modal\nUnderstanding", "content": "To enable LLMs to possess both visual prediction and multi-modal understanding capabilities, we incorporate both future prediction tasks from robotics data and image-text pairs during training. These two types of tasks can be encoded into a unified format so they can be mixed and processed in parallel through the LLM backbone. Therefore, we extend the multitasking approach described in sec 3.\nMulti-modal Understanding Given a paired image-text question-answering set (I, L), we encode the image into the language embedding space via a continuous encoder and a connection layer E\u2081, resulting u = {u}_{i=0}^{M} = E1(I). These embeddings are concatenated with text embeddings 1 = {li} to form the multi-modal input. To generate a text sequence that can focus on the image while comprehending the language, we modify the causal attention mechanism so that image tokens can attend to each other, as shown in Figure 4(a). Finally, we use an autoregressive manner to predict the next language token. This task can be briefly described as \u00ce = \u03c0_{MMU} (I, L).\nFuture Visual Prediction For image prediction, given an image and instruction pair (Ot, L) at time t, we encode the current visual observation using a discrete encoder E2: vt ={vi}_{i=0}^{M} = E2(Ot). Unlike multi-modal understanding tasks, the objective in visual prediction is to encode future visual observation by focusing on the instruction prompts. Thus, as depicted in Figure 4(b), we place the image tokens after the language tokens, enabling the image to attend to all input information. Meanwhile, we introduce a new special token PRE to denote this new task. Instead of using next token prediction, we model the future image tokens at the same positions of the image tokens:\nP(Ot+\u2206t|Ot, L) = \\prod_{i=1}^{M}Po(v_{t+\u2206t}^{i}/v_{t}, 1),\n, and then use a discrete decoder to reconstruct the predicted\nfuture observation image \u00d4t+\u2206t. This task can be succinctly\ndescribed as \u00d4t+\u25b3t = \u03c0\u03c1PRE(Ot, L)."}, {"title": "4.3. Enhancing Action Learning with Joint Prediction\nand Understanding", "content": "While previous VLA method leverages the muli-modal un-derstanding knowledge of pre-trained VLM, it fails to ex-ploit the rich visual information and the physical dynamics. To address this limitation, we propose a joint prediction-and-understanding action learning mechanism. We integrate action output with image prediction tasks. Given the cur-rent observation-instruction pair (Ot, L), our model predicts both future observations and a sequence of actions at each time step: (\u00d4t+\u2206t, At:t+t) = \u03c0_{PRE} (Ot, L), where \u00c2 corresponds to the final layer features at the positions of the action tokens.\nIn addition, as shown in Figure 4(c), we extend the language instruction input with scene descriptions generated by the model itself. The expanded input prompt is:\nL' = [E1(O'), \u03c0_{MMU} (Ot, Lprompt), L]\nwhere L is the language instruction and Of represents vari-ous visual information at the current time step. The obser-vation Of, after processing through the continuous vision encoder E\u2081 = MLP(VIT), is mapped into the language embedding space E\u2081 (O) which can be directly used as lan-guage tokens. The final component \u03c0_{\u039c MU} (Ot, Lprompt) is the generated description of the current scene, whereLprompt is a specific prompt, such as \u201cdescribe this image\".\nFinally, we generate actions via joint prediction:\n(\u00d4t+\u2206t, \u00c2t:t+\u2206t) = \u03c0_{PRE} (Ot, L')\nWe use a small policy head to output low-level actions, con-sisting of a MAP module (a single-layer attention module) and a linear layer: \u00e2t:t+\u25b3t = MLP(MAP(\u00c2t:t+\u25b3t))\""}, {"title": "4.4. Training Strategy", "content": "We initialize the backbone of UP-VLA using Show-o (Xie et al., 2024). During training, we fully fine-tune the parameters of the LLM and freeze all encoders. The training process can be divided into two stages. In the first stage, we aim to endow the VLM with both visual prediction and multi-modal understanding capabilities. In the second stage, we focus on learning actions using robot data. We apply different sampling ratios for different tasks."}, {"title": "4.4.1. TRAINING PIPELINE", "content": "Prediction and Understanding Pretraining Stage. We mix training data across two domains: one part is from Bridge (Walke et al., 2023), which includes 25k robotic arm demonstrations. We use this data for future prediction. Another part is from LLava-tuning-665k (Liu et al., 2024), which includes 665k image-text pairs for enhancing high-level understanding capability.\nPrediction with Action Tuning Stage. The model is fine-tuned on downstream embodied tasks. We train UP-VLA using the joint prediction-and-understanding action learning approach in sec 4.3. We continue to co-train with the image-text pairs to preserve multi-modal understanding ability."}, {"title": "4.4.2. TRAINING OBJECTIVE", "content": "The UP-VLA method involves three modeling targets: lan-guage modeling for multi-modal understanding, image mod-eling for visual prediction, and action modeling for embod-ied tasks.\nLanguage Modeling for multi-modal Understanding.\nGiven M visual tokens u = {u}_{i=1}^{M} and N text tokens\nl = {li}_{i=0}^{N}, we maximize the likelihood of the next token"}, {"title": "5. Experiments", "content": "UP-VLA is a versatile vision-language-action model that can perform multi-modal understanding and future prediction while generating robot actions. In this section, we evaluate UP-VLA in two domains including the simulation CALVIN benchmark (Mees et al., 2022) and a real-world panda manipulation environment to verify the effectiveness of our UP-VLA framework."}, {"title": "5.1. Experiment Setup and baseline", "content": "Setups. For simulation evaluation, we utilize CALVIN (Mees et al., 2022), an open-source benchmark to learn long-horizon language-conditioned tasks. As shown in Figure 5(a), the CALVIN environment comprises 4 different scenes denoted ABCD. We evaluate UP-VLA on both ABCD-D and ABC-D settings.\nOur real-world experiments involve multiple table-top ma-nipulation tasks on the Franka-Emika Panda robot, including picking and placing, routing cables, pressing buttons, and opening drawers. Specifically, we collect over 2k demon-strations above 6 skills. As shown in Figure 5(b), we train UP-VLA on simple scenes while testing it on more complex settings. We place several seen and unseen objects on the table to introduce distractions and test whether the model can grasp entirely new objects to verify its semantic grounding capabilities. Meanwhile, we evaluate the model's ability to perform more fine-grained tasks, such as routing cables, grasping smaller unseen blocks, or picking up a pen. More details of dataset can be found in Appendix ??.\nBaselines. We mainly compare UP-VLA with two types of baselines: VLM-based VLA methods and future-prediction-based methods (mainly including future image generation, goal generation and video generation). All baselines in our experiment are listed as below:\n\u2022 RT-1 (Brohan et al., 2022): a small robot action transformer using pretrained Efficient-Net (Tan & Le, 2019) as vision encoder.\n\u2022 Diffusion Policy (Chi et al., 2023): a small action model using diffusion model.\n\u2022 Robo-Flamingo (Li et al., 2023b): a typical VLA model consists of a pretrained VLM and an LSTM policy head.\n\u2022 3D-VLA (Zhen et al., 2024): a unified VLA model that enable 3D reasoning, multi-modal goal generation, and action planning. Different from UP-VLA, 3D-VLA mainly focuses on introducing 3D knowledge into VLM (LLM).\n\u2022 UP-VLA-RT-2: an apple-to-apple baseline that makes use of the same backbone with UP-VLA and direct output actions autoregressively (reimplementation of RT-2(Brohan et al., 2023) or OpenVLA (Kim et al., 2024)).\n\u2022 Uni-Pi (Du et al., 2024): learns to generate future se-quences and then output actions with an inverse kinematics model.\n\u2022 Susie (Black et al., 2023): first generates goal image and then learns a goal-conditioned diffusion policy.\n\u2022 GR-1 (Wu et al., 2023): pretrains a transformer on video prediction task and then finetune on robot data to learn multi-task robot manipulation.\n\u2022 UP-VLA-phi-w/o-mmu: UP-VLA initialized from Phi-1.5 (Li et al., 2023c) and is trained without multi-modal understanding tasks, serves as a reimplemented GR-1 under the same setting of our method.\n\u2022 3D Diffuser Actor (Ke et al., 2024): learn a 3D diffusion policy using depth image with camera pose."}, {"title": "5.2. Simulation Evaluation", "content": "Table 1 and Table 2 presents the experimental results in simulation environments. UP-VLA achieves the highest performance on both ABC\u2192D and ABCD\u2192D tasks. Compared to other baselines, which perform significantly worse on ABC\u2192D than on ABCD\u2192D, UP-VLA achieves higher completion lengths in both scenarios, indicating that our method has better multitask learning and generalization capabilities in simulation tasks.\nEffectiveness of Visual Prediction. Comparing VLA-based methods and prediction-based methods in Table 1, it is evident that previous VLA approaches underperform in simulation tasks relative to prediction-based methods. For example, RoboFlamingo, achieves a length of 2.47, which is less than GR-1's length of 3.06. This suggests that relying solely on vision-language understanding pretraining can be limiting in tasks that emphasize visual generalization. Our method addresses this limitation by incorporating visual pre-diction into the original VLA framework. Compared to UP-VLA-RT-2, which uses only action learning and achieves a completion length of 1.44, UP-VLA with visual prediction significantly improves the length to 4.08. This demonstrates that integrating visual prediction can substantially enhance the performance of original VLA methods.\nEffectiveness of multi-modal Understanding. Compared to prediction-based methods, the UP-VLA method demonstrates superior performance. To further investigate the factors contributing to the performance improvement, we design UP-VLA-phi-w/o-mmu as a baseline to eliminate the variable of model backbone differences. This method initializes UP-VLA using a pure LLM, phi1.5 (Li et al., 2023c) and performs pretraining on the Bridge dataset for future prediction and is then finetuned with downstream robot task. Unlike UP-VLA, UP-VLA-phi-w/o-mmu does not include multi-modal understanding training, nor does it incorporate image comprehension in the language prompts during out-put. As shown in Table 1, UP-VLA-phi-w/o-mmu performs worse than UP-VLA, which indicates that injecting multi-modal understanding into the model during training helps improve its generalization ability in new scenarios."}, {"title": "5.3. Real Robot Evaluation", "content": "For real-world experimental results, we train RT-1 (Brohan et al., 2022), Diffusion Policy (Chi et al., 2023) on our datasets (using the open-source code and testing them on"}, {"title": "5.4. Ablation Studies", "content": "In this section, we aim to understand each module in UP-VLA. We compare the full UP-VLA with the following methods: UP-VLA-w/o-MMU, which does not utilize the LLava tuning dataset for multi-modal understanding; UP-VLA-w/o-Bridge-Pretrain, which skips visual prediction"}, {"title": "5.5. Quantitative Results", "content": "Figure 7 visualizes the performance of UP-VLA in multi-modal understanding question-answering (VQA) and future prediction across different types of data.\nMulti-modal Understanding As shown in Fig 7, it can be observed that UP-VLA can identify the objects present in embodied scenes and estimate their approximate relative positions, which is critical for action learning. Therefore, ef-fectively integrating MMU capabilities with action learning is a promising approach to improving operational accuracy. Additionally, we observe that the model's identification of the specific objects is sometimes inaccurate. This is likely due to constraints in the scale of data and backbone, which is a potential direction for future research.\nFuture Prediction Results of predicted images are shown in Fig 7. UP-VLA demonstrates the ability to accurately predict the positions of robot arms and objects based on language instructions. However, we also identify some limitations. For instance, in the Calvin D environment, the predicted frames feature background colors that differ from the input frames. The model tends to use background colors from the training datasets (ABC). This issue is likely due to insufficient pretraining for visual generation, which limits the model's generalization capability in generation tasks."}, {"title": "6. Conclusion", "content": "In this paper, we introduce UP-VLA, a vision-language-action model that can understand, generate predicted future images, and plan actions in the embodied environment. We devise a novel VLA training paradigm that unifies policy learning with visual prediction and multi-modal understanding. Our results show that the use of future image prediction can significantly improve the precision and visual generalization of policy. We also further enhance our model by"}]}