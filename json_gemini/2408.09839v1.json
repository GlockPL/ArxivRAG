{"title": "Segment-Anything Models Achieve Zero-shot Robustness in Autonomous Driving", "authors": ["Jun Yan", "Pengyu Wang", "Danni Wang", "Weiquan Huang", "Daniel Watzenig", "and Huilin Yin"], "abstract": "Semantic segmentation is a significant perception task in autonomous driving. It suffers from the risks of adversarial examples. In the past few years, deep learning has gradually transitioned from convolutional neural network (CNN) models with a relatively small number of parameters to foundation models with a huge number of parameters. The segment-anything model (SAM) is a generalized image segmentation framework that is capable of handling various types of images and is able to recognize and segment arbitrary objects in an image without the need to train on a specific object. It is a unified model that can handle diverse downstream tasks, including semantic segmentation, object detection, and tracking. In the task of semantic segmentation for autonomous driving, it is significant to study the zero-shot adversarial robustness of SAM. Therefore, we deliver a systematic empirical study on the robustness of SAM without additional training. Based on the experimental results, the zero-shot adversarial robustness of the SAM under the black-box corruptions and white-box adversarial attacks is acceptable, even without the need for additional training. The finding of this study is insightful in that the gigantic model parameters and huge amounts of training data lead to the phenomenon of emergence, which builds a guarantee of adversarial robustness. SAM is a vision foundation model that can be regarded as an early prototype of an artificial general intelligence (AGI) pipeline. In such a pipeline, a unified model can handle diverse tasks. Therefore, this research not only inspects the impact of vision foundation models on safe autonomous driving but also provides a perspective on developing trustworthy AGI. The code is available at: https://github.com/momo1986/robust_sam_iv.", "sections": [{"title": "I. INTRODUCTION", "content": "Semantic segmentation is a significant task in autonomous driving. It helps realize the environmental perception, path planning and decision, transportation scenario analysis, bar-rier avoidance and collision prevention, precise localization, and human-computer interaction based on the visualization interface. The performance of semantic segmentation guar-antees the Safety of the Intended Functionality (SOTIF) in autonomous driving. Over the past decade, with the successful application of deep learning, many semantic segmentation models [1]\u2013[8] have become well known.\nThe existence of adversarial examples [11]\u2013[13] is a huge challenge towards the trustworthy deep learning. The tiny perturbations would not change the semantic information of an image. These adversarial examples are imperceptible to the human eyes. However, they can deceive the neural networks to make wrong predictions. The application of adversarial samples in semantic segmentation tasks can cause confusion in the classification of different pixels. This fault cannot be tolerated in autonomous driving which would cause a potential security risk.\nIt raises an insightful scientific problem. Which adversarial sample has the highest security risk against semantic segmen-tation models? What kind of models are most robust under adversarial attacks? Is it possible to utilize the adversarial examples as responsible tools to assist in the safety testing of autonomous driving?\nPreviously, some research has covered the discussions on adversarial robustness of semantic segmentation models based on convolutional neural networks (CNNs) [14]\u2013[16]. After the rise of vision transformers (ViTs) based on the aggregated tokens in the tasks of visual recognition, the research on the reliability of ViT-based semantic segmentation models including their adversarial robustness and performance under natural shifts [17] shows its significance. Recently, it has been witnessed that the ViT model as a foundation model can perform various downstream tasks [18]. Concerns about their robustness make sense [19], [20]. Some empirical studies have given the specific attention to adversarial robustness of the specific deep-learning-based semantic segmentation models, either on CNN-based models [14]\u2013[16] or segment-anything model (SAM) [20]. Nevertheless, the change in the model design paradigm brings about a change in the robustness study paradigm. The CNN-based models are sensitive to gradient-based attacks or corruption. Such security threats may no longer be so fatal to the SAM models.\nThe research assumption indicates the necessity of conduct-ing a systematic study on the adversarial robustness of diverse model structures in semantic segmentation for autonomous driving. There is a wide variety of adversarial examples that may attack autonomous vehicles with the deployment of deep-learning-based semantic segmentation models. The white-box adversarial examples are usually related to cybersecurity risks in the Internet of Vehicles, and the black-box adversarial examples based on corruption can be utilized as tools for safety-bound testing in autonomous driving, which enables security risks to be transformed into responsible applications. The SOTIF [21] deals with the risks caused by the limitations of AI models. The black-box corruption can help generate different test cases in the SOTIF evaluation. Therefore, there is a lot of room for exploration in the robustness study for autonomous driving that has not been looked at in previous studies.\nMoreover, the appearance of GPT-4 [22] and Segment-Anything model (SAM) [18] demonstrates that the foundation models can solve complex problems and achieve the human-level performance with the unification of the language signals and visual signals. These early but imperfect sparks of artificial general intelligence (AGI) connect to the phenomenon of emergence [23], [24] that huge model parameters with huge amounts of training data can cause the phase transition of performance of the AI agents. The unification of visual signals and language signals leads to several trends: the realization of open-world visual recognition, clustering the raw image pixels with the prompts based on a generalized foundation model, and generalized visual encoding. It is worth studying the relationship of such a trend with adversarial robustness.\nBased on the research motivation, we explore the zero-shot adversarial robustness under the white-box attacks and black-box attacks with a comprehensive empirical study. We implement the robustness evaluation at the data level for autonomous driving on the Cityscapes dataset [25] with a quantitative and qualitative analysis of our experiment result. At the model level, the evaluated models include the typical CNN and ViT models and up-to-date SAM models. We are particularly interested in the zero-shot adversarial robustness performance of SAM models with the constraint of the language encoders, e.g., Contrastive Language-Image Pre-Training (CLIP). The evaluation of the zero-shot adversarial robustness helps design trustworthy mod-els in the generative AI era.\nThis paper has two contributions:\n\u2022 (Methodology-wise) In the semantic segmentation task, this study shows a SAM pipeline with the assistance of text encoder achieves a robust in-context learning ability under the adversarial attacks.\n\u2022 (Empirical-study-wise) We evaluate the robustness of CNN models, ViT models, and SAM models under the white-box attacks and black-box attacks on the dataset of Cityscapes [25]."}, {"title": "II. RELATED WORKS", "content": "This section gives a literature review on the semantic segmentation models based on deep learning, adversarial ex-amples in the vision tasks."}, {"title": "A. Deep-learning-based Semantic Segmentation Models", "content": "The renaissance of deep learning in the vision task [26]\u2013[29] boosts the development of deep-learning-based semantic segmentation models. The first models to appear are based on CNNs, including Fully Convolutional Networks (FCN) [1], SegNet [2], Pyramid Scene Parsing Network (PSPNet) [3], DeepLabV3+ model [4], and so on. The recent study proposes the real-time Short-Term Dense Concatenate module (STDC module) to obtain the scalable receptive fields by progressively decreasing the dimension of the feature maps and extract the multi-scale information by connecting the response maps of multiple successive layers [8].\nViT [10] uses a self-attention mechanism to capture global relational and contextual information in the input image rather than relying on the operation of local receptive fields of CNNs. By dividing the input image into a set of fixed-size blocks, the long-range image contextual information can be captured for feature extraction. Based on such an advantage, ViTs have become main-stream backbones in the task of semantic segmentation. SegFormer [5] incorporates a hierar-chically structured Transformer encoder that produces multi-scale features, eliminating the need for positional encoding, and it depends on the multi-layer perception (MLP) decoder to aggregate the information from various layers. Object-contextual representations networks for semantic segmentation (OCRNet) [7] addresses the context aggregation problem in semantic segmentation by exploring the potential to explicitly transform a pixel classification problem into an object region classification problem. Improved spatial attention network (ISANet) [6] decomposes the original dense affinity matrix into two sparse affinity matrices to improve the efficiency of semantic segmentation model based on ViTs, where one sparse affinity matrix is used for long-distance transmission and the other sparse affinity matrix is used for short-distance transmission. One meaningful attempt is to unify different tasks in a single unique model, e.g., panoptic segmentation, instance segmentation, semantic segmentation [30].\nViTs usually require more parameters. Therefore, such models are more suitable for large-scale datasets and more adequate computational resources compared with CNNs. It brings a new paradigm and trend in computer vision: pre-training on large-scale datasets to model the distributions of visual objects all over the world, unification of different granularities and tasks, and incorporation of other human knowledge such as languages [31]. The model trained on the general dataset at scale and adapted to diverse downstream visual tasks is a foundation model [32]. Recent milestone research has built the largest segmentation dataset to date, with over 1 billion masks on 11 million licensed and privacy-respecting images. The SAM model [18] and its lightweight variants [33], [34] are designed and trained to be promptable so that their zero-shot recognition performance is considerable."}, {"title": "B. Adversarial Examples and Its Applications on Semantic Segmentation Models", "content": "Deep learning models are vulnerable to adversarial ex-amples that are imperceptible to human eyes. The carefully crafted adversarial perturbations to the input image can cause a neural network model to make incorrect predictions. Ad-versaries can obtain the gradient information via the model leakage, including fast gradient sign method (FGSM) [11], project gradient descent (PGD) attacks [12], Carlini and Wagner attacks (C&W) [35], and so on. Besides the model-specific and white-box attacks, there exist universal adversarial perturbations (UAPs) that the small, subtle perturbations can be applied to a wide range of input data and deceive diverse neural networks with a good transferability [13], [36].\nSometimes, the attack scenarios are model-agnostic that the attackers without the model knowledge try to generate the adversarial examples via the iteration search [37], [38]. The dense adversary generation (DAG) attack method [39] is a typical black-box method to disturb the semantic seg-mentation models. The black-box adversarial examples can be both model-agnostic and data-agnostic to fool the neural networks via the procedural noise functions [40]. When these corruptions are applied to autonomous driving, it transfers the security issues to the safety testing scenarios [41].\nThere are several empirical studies on the adversarial attacks against the semantic segmentation models in autonomous driving. Some early work focuses on the specific CNN models [14]\u2013[16]. Although the study to cover more attack scenarios and model structures cannot be neglected [42], it is still insufficient for current research topics on foundation models. Moreover, some research on adversarial the robustness of SAM does not focus on autonomous driving [19], [20], which reflects the necessity for this study. Shan et al. [43] provides a preliminary evaluation of the robustness of SAM in adverse weather. However, a systematic assessment including both white-box and black-box attacks is still needed to connect the empirical study to SOTIF and cybersecurity in autonomous driving."}, {"title": "III. SAM BASED ON THE OPEN-SET CATEGORY ENCODER", "content": "SAM [18] is a powerful foundation model that can seg-ment arbitrary objects, and SA-1B is the largest segmenta-tion dataset to approximate the world distribution with 11M images. SAM is a generalizable object segmentation method that delivers precise contouring through its masks. SA-1B emerges as the large-scale generalized segmentation dataset. The closed-set semantic segmentation pipeline based on deep learning can provide rich semantic annotations, while SAM can generate precise masks. Moreover, the CLIP model [31] is a large multimodal pretraining model. CLIP can map images and text into a common feature space to form an image-text pair. CLIP contains two main components: an image encoder and a text encoder. The two encoders are trained in parallel to achieve cross-modal feature representation and alignment through contrast learning.\nThis study raises the question of whether the zero-shot adversarial robustness when SAM meets the open-set category open-set category encoder based on the CLIP method. Fig. 2 describes such a framework. The SAM image encoder (green part) converts raw image data into low-dimensional feature vectors for subsequent computation and analysis. The role of the mask decoder is to efficiently map image embeddings, prompt embeddings, and output markers to masks. It uses a modified Transformer decoder block followed by a dynamic mask prediction header. The mask decoder (yellow part) upsamples the image embeddings and uses MLP to map the output markers to a dynamic linear classifier that will compute the mask foreground probability for each image location. The method of CLIP [31] helps categorize different classes in different application scenarios (e.g., 19 categories on the Cityscapes dataset [25]). In Fig. 2, the purple part represents the text encoder based on the CLIP approach. CLIP has rich world knowledge, reasoning ability to pair with images, and can assist in the segmentation branch. The semantic branch (blue part) provides per-pixel categories, implemented by a semantic segmentation pipeline, which can be customized by the user with the ViT-based model (e.g., SegFormer [5] and OneFormer [30]) according to the architecture of the segmen-tation model and the categories of interest. It deserves to be emphasized that the Semantic-Segment-Anything models do not need supervised learning of SegFormer and OneFormer on the specific dataset. Since SAM is a visual foundation model, it can transfer to the downstream task without additional training. The semantic voting module (orange part) crops the corresponding pixel categories based on the position of the mask. The framework selects the top-1 predicted category of these pixels as the categorization result of this mask.\nSAM is based on the ViT model, which can express image information exclusively as a token. CLIP generates linguistic commands that can help to extract visual information. Based on the combination of image and language, this paradigm can deal with complex scenes and realize the zero-shot adversarial robustness of the visual perception system in the downstream tasks.\nIt leads to the central theme of this study: what kind of model architecture can guarantee the adversarial robustness of semantic segmentation models based on neural networks? CNN-based segmentation models have better generalization, but their adversarial robustness needs to be improved. ViT model extracts better global information about image objects, eliminating the inductive bias present in CNN models and their sensitivities to textures [44]. At the same time, the Transformer architecture creates a potential to unify the image and text information. It raises the question of whether we can achieve better adversarial robustness with the utilization of a ViT-based foundation model combined with CLIP assistance for the task of semantic segmentation in autonomous driving. Therefore, a comprehensive and systematic empirical study is necessary."}, {"title": "IV. EXPERIMENT", "content": "This study mainly focuses on the scientific problem in the inference time. Thus, the experiments are implemented on the validation dataset with 500 images.\n2) Evaluation Metrics: The semantic segmentation task mainly uses recall, precision, F1-score, and intersection-overunion (IoU) metrics to evaluate the performance of mod-els. The metric of mIoU denotes the IoU scores from N pairs of data samples:\n$$mIoU = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\cap(mask_{\\text{groundtruth}}, mask_{\\text{predict}})}{\\cup(mask_{\\text{groundtruth}}, mask_{\\text{predict}})},$$\nIn Eq.1, mask(.) denotes a binary matrix of categorized pixels, $mask_{\\text{groundtruth}}$ and $mask_{\\text{predict}}$ are the masked region of content images and the predicted masked regions, respectively. $\\cap$ and $\\cup$ denote the intersection and union section, respectively. The mIoU score ranges from [0.0, 1.0]. The higher mIoU score represents considerable robustness performance.\n3) Models: The CNN-based segmentation models include FCNs [1], DeepLabV3+ (Abbreviation \u201cDP", "architectures": "vanilla SAM [18] and Mobile-SAM [34]. There are two different backbones: SegFormer [5] and One-Former [30]. The exploration of the difference between the Transformer model with a single segmentation task and the Transformer model with the unification of multiple tasks is insightful."}, {"title": "A. Experiment Settings", "content": "1) Dataset: The Cityscapes dataset [25] is a publicly available dataset used for studying semantic segmentation and scene understanding vision tasks. This dataset contains high-resolution (1024 \u00d7 2048) images from different cities in Ger-many and Switzerland that capture streets, buildings, vehicles, pedestrians, and other elements of the urban environment. It contains the categories of road, sidewalk, building, wall, fence, pole, traffic light, traffic sign, vegetation, terrain, sky, person, rider, car, truck, bus, train, motorcycle, and bicycle."}, {"title": "B. Main Experimental Results", "content": "1) Robustness Study of Black-box Corruptions: Au-tonomous driving systems will face various perturbations in the complex environment of the real world, including equip-ment noise, drastic changes in light, adverse weather, and so on. The corrupted image modal data will affect the recognition performance of the system, and it is essential to have a systematic study on the robustness of the perception system under such black-box corruption scenarios. The corrupted Cityscapes [25] dataset with corrupted images covers a total of 19 types of image corruption in five major categories, including noise, blur, adverse weather, and digital disturbance. This subsection comprehensively evaluates the robustness of the black box corruption based on the SAM model.\nIt is important to emphasize that the SAM model is not trained additionally on the Cityscapes dataset [25], and all inferences are conducted in a zero-shot manner. As a comparison, all other models have undergone training on the Cityscapes dataset, whereas the SAM models have not exploited the training set of Cityscapes. The severity level is set at the Level 1, Level 2, and Level 3. The metric of mIoU is calculated as the average value on these three levels. Table I illustrates the results, and the bolded black box attack items are related to SOTIF. The performance of the SAM model exceeds that of most CNN-based models and even a small subset of Transformer-based models. It demonstrates the effectiveness and potential of the SAM model, even without any additional training on the Cityscapes dataset. Compared with the SegFormer model and the OneFormer model, which are fully trained on the Cityscapes dataset, the gap is not large. Overall, the recognition performance of the vanilla SAM model exceeds the MobileSAM model at the cost of inference speed and storage space.\n4) Adversarial Attack Methods: The white-box adversarial attacks mean that the attackers fully understand the internal structures and parameters of the machine learning models, while black-box attacks mean that the attackers only know the inputs and outputs of the models and need to infer the internal structures and parameters of the models by observing the responses of the models. The white-box attack methods include FGSM [11], PGD [12], and FMN [49]. Two black-box attack methods including DAG [39], image corruptions [40] are evaluated in this study. The black-box attacks can simulate the abnormal weather and camera distortions. The image-corruption perturbations can be generated with 19 different procedural adversarial noises, including Gaussian noise, shot noise, impulse noise, defocus blur, glass blur, motion blur, zoom blur, snow, frost, fog, brightness, contrast, elastic trans-form, pixelate, JPEG compression, speckle noise, Gaussian blur, spatter, saturate. Each kind of adversarial noise has 5 severity levels. The vulnerabilities of semantic segmentation models under such black-box corruptions can be attributed to the risk issues of SOTIF."}, {"title": "C. Discussion", "content": "Zero-shot adversarial robustness of Semantic-Segment-Anything models in autonomous driving: The experimental results demonstrate that SAM and MobileSAM can realize adversarial robustness under the white-box attacks and black-box attacks. The robustness guarantee is more obvious on the OneFormer backbone. The robustness under the black-box corruptions connects the adverse conditions like bad weather and sensor noises in autonomous driving, which is significant to the SOTIF of autonomous driving. Moreover, the robustness under the white-box attacks is important to the security of the perception systems in the Internet of Vehicles. The phenomenon of zero-shot adversarial robustness would inspire the work for the further improvement of internal safety and external security in autonomous driving. Furthermore, the change of model design paradigm brings about a change in the robustness study paradigm. The malicious hackers may propose a new attack method adapted to the SAM variants.\nTrade-off between robustness and cost: Overall, the robustness of SAM exceeds the robustness of MobileSAM on the Cityscapes dataset. However, Table V demonstrates the cost of storage and inference of the SAM architectures in the platform with the GeForce RTX 3090 GPU. Although MobileSAM does not have the highest robustness, it is more suitable for computation on edge devices.\nThe robustness difference between the SegFormer and OneFormer backbones: Both SegFormer and OneFormer are based on self-attention mechanisms. SegFormer uses a hierarchical Transformer encoder and a lightweight MLP decoder capable of outputting multiscale features without positional encoding. OneFormer unifies semantic, instance, or panoramic segmentation tasks, and the model is designed as task-dynamic. OneFormer can dynamically adjust its outputs according to the task and category tokens during inference, thus enabling category-specific and mask-specific predictions. This inference mechanism of OneFormer may mitigate the adversaries' gradient perturbation.\nThe limitations of current framework: Some new strong attack methods [52] may launch more severe threats on the ViT backbones. The current proposed method does not integrate the sufficient test-time defense module. Moreover, the test scale can be enlarged."}, {"title": "V. CONCLUSION", "content": "This paper explores the zero-shot adversarial robustness of SAM architectures in the semantic segmentation task for autonomous driving. The findings are surprising, showing that this type of model exhibits robustness under black-box attacks related to adverse weather and sensor interference. These results provide valuable insights into the Safety of the Intended Functionality (SOTIF) of autonomous driving systems. Additionally, the model demonstrates considerable robustness against white-box adversarial attacks, offering a security guarantee against malicious data in the Internet of Vehicles.\nIn the future, we plan to expand the test scale to include other attack methods such as SegPGD [51]. Moreover, in-tegrating test-time defense methods to further enhance ro-bustness is a meaningful direction for research. Thirdly, the interpretation of the zero-shot adversarial robustness of visual foundation models remains an open area that requires further exploration. Last but not least, the study of deployment in real-world applications deserves further study to build a next-generation trustworthy AGI system."}]}