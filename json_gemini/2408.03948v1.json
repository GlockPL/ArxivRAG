{"title": "A Survey of Al Reliance", "authors": ["SVEN ECKHARDT", "NIKLAS K\u00dcHL", "MATEUSZ DOLATA", "GERHARD SCHWABE"], "abstract": "Artificial intelligence (AI) systems have become an indispensable component of modern technology. However, research on human behavioral responses is lagging behind, i.e., the research into human reliance on AI advice (AI reliance). Current shortcomings in the literature include the unclear influences on Al reliance, lack of external validity, conflicting approaches to measuring reliance, and disregard for a change in reliance over time. Promising avenues for future research include reliance on generative AI output and reliance in multi-user situations. In conclusion, we present a morphological box that serves as a guide for research on Al reliance.", "sections": [{"title": "1 INTRODUCTION", "content": "The term Artificial intelligence (AI) was first proposed at the Dartmouth Conference in 1956 [87]. Since then, considerable progress has been made toward the development of modern AI systems, such as transformer models [126] capable of human-like speech [51] or image generation [70]. This rapid performance growth has led to an increase in the use of AI-based solutions in productive environments and the democratization of AI access. One example of the pervasive use of AI can be observed in the publication of GPT models to everyday users through the interface of ChatGPT [1]. This illustrates the trend of the general public accessing and using cutting-edge systems.\nThe ChatGPT example also highlights an issue with the widespread publication of AI systems. While there has been research on the underlying large language model (LLM) prior to its release, there has been little research on the interaction between its users and the tool. To gain a better understanding of the interaction between users and ChatGPT, it was necessary to conduct research only after the tool had been made available to the general public. This meant that the tool's potential positive and negative effects, becoming apparent in humans' interaction with them, were-and still are-not fully understood. In light of this, it is becoming increasingly clear that more regulations are needed to govern"}, {"title": "Review Synopsis", "content": "This survey is structured as follows. In Section 2, we provide the background for this survey, particularly on Al reliance.\nWe also describe the notion of a sociotechnical system, which we use as a fundamental perspective for analyzing"}, {"title": "2 BACKGROUND ON AI RELIANCE", "content": "The field of Al reliance is still in its infancy, with no unified, established, and broadly accepted conceptualizations or definitions. It is however evident that research on AI reliance is closely related to several other disciplines in the domain of human-centered AI systems. In this context, we will briefly introduce literature surveys on related concepts and the relationship between these and Al reliance.\nOnce modern Al systems based on machine learning achieved sufficient performance to be used in productive settings, practitioners and researchers began to focus on the design aspects of these systems to achieve human-centered Al systems. Most research in this field has focused on the explainability of Al systems. One of the earliest reviews in this area was conducted by Adadi and Berrada [2] in 2018, who were among the first to systematically structure the existing literature on explainable AI. More recently, Dwivedi et al. [41] published a comprehensive overview of the core ideas, techniques, and solutions associated with explainable AI. Another relevant topic is the fairness of AI systems. In general, it is desirable that AI systems output fair decisions that do not discriminate. There is however no single definition of fairness but rather several concepts of fairness, which for example Mehrabi et al. [89] or Pessach and Shmueli [100] surveyed. Up to now, there is no systematic consideration which correlates with Al reliance.\nExplanations, fairness, and related concepts are often employed to achieve trustworthy AI systems, as surveyed in Kaur et al. [65]. They present one of the first sociotechnical considerations of the interplay between user and AI system, without explicitly stating the perspective. While the aforementioned studies focus on trust, this review will concentrate"}, {"title": "2.2 Background on Al Reliance", "content": "The notion of reliance is not unique to Al or computer systems-it is extensively discussed in the field of philosophy and psychology. Reliance is often aligned with trust, as the two concepts appear similar at first glance [6, 55]. However, many philosophers argue that trust cannot be expressed toward inanimate objects [e.g. 55, 58]. Nevertheless, a substantial body of Al research has been conducted on trust in AI, often referred to as trustworthy AI [65]. It can be argued that computer systems (and AI) are not inanimate objects, as evidenced by the \"computers are social actors\" principle [95]. When computer systems research discusses trust, it is however often concerned with reliance [36]. In this article, we follow the line of discourse that suggests differentiating between trust (an emotional or attitudinal stance toward something) and reliance (observable behavior) [e.g. 78]. This review focuses on studies that investigate behavior rather than attitude. The remainder of this section describes various discourses of human reliance on computer systems.\nWe describe how reliance on automation systems has influenced research about reliance on intelligent systems. We also demonstrate how findings from the discipline of advice-taking influence research about reliance on AI to create so-called judge-advisor systems.\nAutomation Systems. The discussion of the reliance on (intelligent) computer systems commenced with automation systems [116]. Automation is defined as \"the execution by a machine agent (usually a computer) of a function that was previously carried out by a human\" [99, p. 231]. This notion is evolving over time with the capabilities of computer systems. In this survey, we differentiate automation systems from AI by their focus on automating tasks and lack of predictive power. We also acknowledge that this distinction is becoming increasingly blurred. A simplified example of an automation system where the topic of reliance becomes important is a conveyor belt with a warning light that turns on once there is overheating. In that example, reliance can be defined as the human intervening as soon as the light turns on.\nPrevious research has frequently concentrated on the concept of trust in human-machine interaction in automation systems [e.g. 92, 93]. A frequently researched case is that of pilot cockpit automation systems [e.g. 22, 129]. When reading these earlier studies, it becomes evident that the concept of trust is more complex than previously thought. For example, Wickens [129, p. 366] defines trust as \"the extent to which the pilot believes that, and behaves as if the automation will carry out its assigned task in a reliable fashion.\" This definition encompasses not only the pilot's attitude toward the system but also their behavior.\nThis distinction between attitude and use has been further considered by Parasuraman and Riley [99], who define the use, misuse, disuse, and abuse of automation systems. They define use as \"the voluntary activation or disengagement of"}, {"title": "2.3 Sociotechnical Systems", "content": "Sociotechnical systems consist of a technical component (e.g. the Al system), a social component (e.g. the human decision-maker), and an interaction between these two in a broader context or environment (e.g. an organization). When considering Al reliance, it is important to recognize that the final result of the interaction between the Al system and the user occurs in a specific environment and is contingent upon both the human decision and the AI advice. A sociotechnical lens is therefore essential. The technical component is defined as comprising technical and material artifacts and the techniques or practices employed to use the artifact [77, 123]. In the case of human-AI interactions, this would primarily be the design and advice type of the AI system. The social component encompasses individuals or collectives, as well as the relationships between them, which may be expressed as roles, hierarchies, structures, economic systems, cultures, power relations,"}, {"title": "2.4 Summary", "content": "The current research on AI reliance aims to understand and design for appropriate reliance on AI advice [e.g. 12]. The goal of this research is, typically, to achieve complementary team performance, where the team of humans and AI exceeds the performance of the human or AI components if considered separately [8]. There is no consistent approach across the related work. While there have been reviews analyzing human-AI decision-making [e.g. 75], there has been no focus on the reliance of decision-makers on AI advice. These studies frequently focused on a single aspect of the sociotechnical system, namely the technical component or the Al system itself. The shortcoming of this approach is exemplified by the case of explanations [e.g. 19, 114] that often show inconclusive results [113]. We argue that one reason is that a comprehensive perspective is missing. The sociotechnical system perspective enables us to introduce a comprehensive perspective and analyze existing literature on AI reliance structurally. This allows us to establish uniform approaches and provide guidance, which is essential for research to have a real-life impact and achieve the desired appropriate reliance on AI advice."}, {"title": "3 SURVEY METHODOLOGY", "content": "Given the lack of unified approaches in the literature, a structured literature review is employed to provide an overview of the current approaches researchers who investigate AI reliance employ. This section describes the methodology used for structurally querying the literature. Based on a classification framework introduced in Section 4, the results of the structural literature review are presented in Section 5. These results are then used in Section 6 to discuss the shortcomings and limitations of current approaches and to present guidance for researchers conducting sound AI reliance research. Additionally, future avenues for further developing the field of AI reliance are presented."}, {"title": "3.1 Queries", "content": "To identify relevant articles for research into Al reliance, we employed a structured query of scientific databases. As there are numerous underlying technologies for Al systems, we also queried the common terms that are treated as synonyms, such as machine learning and deep learning\u00b9. In order to include articles that explicitly investigate overreliance and underreliance, we added these terms to the search string, as indicated in Table 1."}, {"title": "3.2 Inclusion and Exclusion Criteria", "content": "The query yielded 1,337 articles, but not all of them concerned the topic of AI reliance. To determine the relevance of each article to this survey, we defined certain inclusion and exclusion criteria, summarized in Table 2 and introduced in the following section.\nFirst, as an obvious inclusion criterion, we included articles that used the abbreviations AI, ML, and DL in the search string. Some articles employed these abbreviations in a manner that differed from the anticipated usage. For instance, AI was used to refer to \"American Indian,\" DL to \"discrete logarithm\" or \"dictionary learning.\" Articles that deviated from the anticipated use of the abbreviations and did not refer to artificial intelligence, machine learning, or deep learning in any other form were excluded. Most importantly, only articles where reliance is considered in the context of humans relying on Al systems were included. Many of the articles have a clear technical perspective, with the term \"reliance\" mostly used to refer to the reliance of AI on datasets, labeled data, or similar resources. Given the extensive scope of the articles and disciplines under consideration, the term \"reliance\" was also employed to describe reliance on objects or entities, such as \"reliance on agriculture\" or \"the world's reliance on Chinese exports.\" These latter examples were"}, {"title": "3.3 Forward and Backward Search", "content": "A forward and backward search was conducted after selecting relevant articles based on querying the literature databases. All identified relevant articles were used for the forward search. This search was performed on SCOPUS without any additional filters\u00b2.The forward search yielded an additional 441 articles, and the backward search an additional 1,160 articles. In total, we found another 26 relevant articles. Two main reasons account for the absence of these articles in the initial search. First, some articles were published in an outlet not included in the top 25% of SCOPUS. Second, some of these articles did not mention reliance in the title, abstract, or keywords, but rather closely related concepts, such as algorithm aversion or automation bias, and only discussed reliance in the main body text."}, {"title": "3.4 Query Results", "content": "In the following section we summarize the query results. A total of 71 relevant articles were identified through the application of the described methodologies. The query results are summarized in Figure 2 using a PRISMA flow chart [90].\nThe initial database search yielded 1,337 articles, of which 154 were excluded due to them not being in English, having been published before 2010, or not having been peer-reviewed, leaving 1,183 articles. After screening the articles using the inclusion and exclusion criteria, we identified 45 relevant articles that investigated the empirical use of AI. We then conducted a forward and backward search, which yielded 441 unique articles in the forward search and 1,608 unique articles in the backward search. After excluding ineligible articles, we were left with 415 articles for the forward search and 1,087 articles for the backward search. As some articles were included in both the forward and backward search sets, merging the two yielded 1,465 unique articles. After screening these articles concerning the inclusion and exclusion criteria, we were left with 26 additional articles that concerned the empirical investigation of Al reliance.\nThis resulted in 71 unique articles dealing with the empirical study of AI reliance. The results are summarized in Figure 2. The examination of basic bibliometrics, such as publication year or outlet, provides insight into the communities concerned with Al reliance. This is described in the following section."}, {"title": "3.5 Bibliometrics", "content": "Figure 3 shows a histogram of the publication dates. Despite our search including articles from 2010 onwards, the earliest articles identified are from 2018\u00b3. It is evident that the number of published articles has increased significantly"}, {"title": "4 CLASSIFICATION FRAMEWORK FOR THE LITERATURE", "content": "To gain an understanding of the existing literature, the identified articles are classified according to a range of concepts. These concepts are based on the components of a sociotechnical system and the subconcepts derived in an author workshop based on a set of preselected articles. This filtering and classification process is then employed to gain insights into how the existing literature conceptualizes AI reliance. The steps are explained below.\nThe sociotechnical system provides a profound perspective to consider AI reliance. As described in Section 2, a sociotechnical system consists of four components: (a) the environment, (b) the interaction, (c) the social component, and (d) the technical component in which the system is situated. These components serve as the basis for classifying the relevant articles based on their primary focus. To derive the subconcepts in the four groups, three authors held an author workshop. Prior to the workshop, each author read a subset of the articles, focusing on common themes,"}, {"title": "5 RESULTS", "content": "Based on the categorization framework introduced in the previous section (Section 4), we analyze the current literature on Al reliance in this section. In Section 5.1, we present the environment where Al reliance occurs in the identified articles. In Section 5.2, we present the general conceptualizations of the mutual interaction of the technical and social components, namely reliance, by reviewing the measures and decision-making approaches. In Section 5.3, we address the social component of the sociotechnical system and review the way users come to know the systems. Finally, to include the technical component, we review the properties of the AI system in Section 5.4."}, {"title": "5.1 Exploring the Environment of Al Reliance", "content": "The first component of the categorization framework is the environment in which AI reliance is observed. This is based on the subconcepts of the task, the setting, and the use cases in which Al reliance is studied. It is notable that, despite the prevalence of real-world use cases in daily life and work, most articles employ crowdworker marketplaces, such as Amazon Mechanical Turk (MTurk) or Prolific, instead of professionals or experts. Indeed, 47 articles use online crowdworker marketplaces to recruit participants. The remaining articles employ a variety of recruitment strategies, including domain experts (n = 9), students (n = 5), and convenience sampling (n = 10), without further restrictions on the participants. This indicates that most articles use crowdworker marketplaces to recruit participants, and nearly all articles examined reliance in a controlled and isolated setting.\nA multitude of use cases are employed and Figure 4 presents an overview. For a more in-depth analysis, we can divide the use cases into two groups: those where the AI aims to present objectively correct advice, meaning use cases"}, {"title": "5.2 Measuring the Interaction of Human and Al", "content": "The second category of the categorization framework is the interaction between social and technical components. In the case of Al reliance, this interaction is human reliance, which we examine by reviewing current approaches and measures to quantify reliance on AI advice. First, we must establish a general understanding of the approach that should be used to measure reliance. As discussed below, there already seems to be some disagreement in the current literature regarding this matter. In light of the aforementioned approach, we proceed to present the existing measures of reliance.\nDecision-Making Approach. Depending on whether we view AI systems as automation systems or as sources of advice, two distinct approaches can be identified. As described in Section 2, advice can be presented directly to the decision-maker or only shown after the decision-maker has already made an independent decision. This leads to two distinct approaches: a single-stage approach, where advice is presented before the decision is made, and a two-stage approach, where advice is presented after an initial decision is made and the decision-maker can revise their decision.\nThe more straightforward way to implement the decision-making approach is a single-stage decision-making approach, where users are directly confronted with the AI advice. The final decision from the human-AI interaction is recorded only then. This can also have some variants. Among the groups of articles that have a single-stage decision-making approach, there is one article that measures the human decision twice after being exposed to the AI decision [103], making it a multistage approach, but only for the human decision and not the AI advice. Further, instead of the human making their own decision after being exposed to the AI, sometimes the human can only accept or reject the AI advice [e.g. 14], or the human can decide if they want to decide, or if they want to let the AI decide for certain tasks [e.g. 97]. Another variant is that the Al advice is implemented automatically until a human intervention, as in the case of automatic driving games [e.g. 3]. While we find several variants, they are all unified as a single-stage approach, where the human is exposed to the AI advice without a prior decision on the task."}, {"title": "5.3 Human User as the Social Component", "content": "Once the settings and use cases have been established, as well as the general conceptualizations and measures of reliance, we proceed to the social component. In most decision-making tasks, both real-world and experimental, humans are confronted with multiple decision instances in rapid succession. For instance, a doctor may have numerous patients requiring a diagnosis. We claim that the experience gained from one instance may influence the decisions made in the next. We create a two-dimensional view and distinguish between training with the AI system and performance feedback during the main task, which is presented in Table 5. While analyzing the literature, it becomes apparent that not all articles explicitly state whether they provide feedback and/or training to the participants. Consequently, if no information about training with the AI was present, it was classified as no training. Furthermore, if an article tested out treatments with and without feedback or training, it was classified as an article with training or feedback present. Finally, if no information about feedback was present and the setting and use case did not provide feedback implicitly, it was classified as no feedback. An example of feedback provided implicitly is an autonomous vehicle driving game [3], where no crash with the autonomous vehicle indicates that the AI provided positive advice. Another example is a multiday trading game [28], where each day the return of the previous day was known.\nAs illustrated in Table 5, most articles do not provide training or feedback to users. We speculate that this could be due to underreporting. Another reason could be that a straighforward experiment design might lead to easier data collection than an experimental design with training and/ or feedback. Most articles do not examine changes in reliance over time or across multiple interactions, so that feedback would not have any impact on the empirical results. In certain instances, the provision of training and feedback may be impracticable, particularly in the case of tasks that are designed as one-off sessions [e.g. 14]. Some articles have training before the main task, where participants are trained on the task, for example bird image classification [112], but without the AI advice. One rationale for this approach is to assess the participants' skills before the main task, which could be a potential confounder to reliance. At the same time, the reliance behavior is not influenced by the training. In general, most articles employ a straightforward experimental design, where participants are not provided with any training with the AI system or performance feedback"}, {"title": "5.4 Al System as the Technical Component", "content": "In addition to the social component, the technical component-the AI system-plays a pivotal role in sociotechnical systems. In this section, we therefore review the various Al systems used in the AI reliance literature, most prominently, the nature of the AI system itself and a common design decision of transparency mechanisms.\nWith regard to the AI system itself, it is evident that numerous articles do not construct real Al systems. Instead, users frequently interact with mockups and dummies in WOZ studies (n = 29) [e.g. 103, 115, 122]. In these studies, the users are informed that they are interacting with an AI system and receiving AI advice, whereas the researchers constructed this advice. This methodology is commonly employed, particularly in HCI research. It also streamlines data collection, as no Al system needs to be constructed. It is however crucial to exercise caution, as mocked systems do not provide real Al advice and may be biased in their output. Some of these studies even provide AI advice that is always correct Panigutti et al. [98], Srivastava et al. [120], therefore overreliance of users is not possible. As a preliminary step toward enabling decision-makers to interact with real AI systems, some articles construct real AI systems but present manually selected instances of AI advice. In other words, the researchers manually sample the dataset (n = 19) [e.g. 30, 62, 128]. This approach is frequently employed to achieve a specific performance of the model. One advantage of this approach is that it is closer to reality, as a real Al system created the advice. For instance, while some articles explicitly investigate outliers [e.g. 80], these are still hand-picked and expected, and the randomness is absent. These manually selected labels also distinguish between the model accuracy of the underlying model and the sampling accuracy of the samples presented to the decision-makers. For example, Chen et al. [30] presents eight decision instances, five of which have correct advice, resulting in a sampling accuracy of 62.5%, but the underlying models have accuracy above that.\nThis distinction between sampling and model accuracy might induce biases. Only when decision-makers interact with real AI systems does sampling accuracy equal model accuracy. However, only a few articles allow users to interact with real Al systems (n = 21) [20, 102, 114]. In most cases this results in a random sample from a list of AI advice. Overall, we find that many studies do not use real AI systems but rather either fully mocked interfaces or hand-picked AI advice. To address the issue of WOZ systems and real Al systems, for example, Ashktorab et al. [5] tests both a real AI and a WOZ study with a \"perfect\" AI system, meaning a system that always outputs correct advice5.\nOne of the most common design decision for Al systems are transparency mechanisms. For this review, we distinguish between explanations and the statement of AI performance or certainty. We find that many articles provide explanations to users (n = 27). There are numerous explanations. Notable examples include [14, 128], SHAP values [e.g. 38, 40]. Additionally, other forms of explanation have been employed, including those based on LIME [10] or counterfactual explanations [79]. Collectively, these examples illustrate the diversity of approaches to explanation. In addition, in some articles the user is provided a statement on the overall Al performance or the certainty on a specific decision task (n = 11) [e.g. 101, 103, 106]. Interestingly, only a few articles present both to the user (n = 8) [e.g. 21, 71, 105]. Finally, several articles do not employ any transparency mechanisms (n=25) [e.g. 23, 53, 86]. These articles often focus more on general applicability, such as the case of an AI that predicts preferences, where this premise is already the subject of research [e.g. 84, 122]. Other examples are articles that are interested in different aspects, such as the abovementioned change of reliance over time [50] or the use of multiple users [31]. It becomes apparent that a clear majority of articles present some form of transparency mechanism, which highlights their technology-centricity. Changes to the design of the technological system are often more interesting than the actual users, which often consist of crowdworkers and non-specific user groups."}, {"title": "6 DISCUSSION", "content": "After a comprehensive review of the existing literature on Al reliance, we discuss its implications and shortcomings, which serve as inspiration for new research avenues. Overall, we identify the following five directions for future and more in-depth research on Al reliance: (1) the influence of various factors on reliance is not fully understood; (2) the external validity of studies is often limited; (3) different approaches have been employed to measure reliance, making comparisons difficult; (4) the impact of reliance on individuals and society over time has not been adequately addressed; (5) the potential implications of emerging issues related to Al reliance have not been fully explored. The remainder of this article will provide a more detailed examination of these avenues."}, {"title": "6.1 Limitations of Current Al Reliance Research", "content": "The sociotechnical perspective on AI reliance identifies four distinct angles through which influences on Al reliance can be identified: the social component, the technical component, the interaction between these two, and the environment. All these factors play a role in Al reliance for decision-making. Only with this comprehensive perspective can we fully acknowledge the influences on Al reliance. A review of the literature reveals that most articles only consider individual components of the sociotechnical system. Even within these components, there is often no consensus on the factors that influence reliance on AI systems. In the following section, we discuss current approaches in the various components of sociotechnical systems. In conclusion, we urge further research to obtain a more comprehensive view of Al reliance.\nA significant proportion of articles in this field focus on the technical component-the AI system itself-and its influence on reliance. The design aspects of Al systems are frequently the primary considerations, with explanations being a key area of interest. It is also evident in the literature that transparency mechanisms, such as explanations, do not have a monotonic influence, and explanations do not always lead to the same results [113]. Consequently, an exclusive focus on explanations, or the design of the technical component in general, is insufficient for a comprehensive understanding of AI reliance. It is imperative to consider the users interacting with the AI system.\nThe social component, referring to the human decision-maker, can also contribute to reliance. Humans are prone to cognitive biases [63] and AI systems have been shown to induce various such biases, most notably the anchoring or automation bias [105]. The effect of the bias may also depend on humans themselves, with some people being more prone to cognitive biases than others. For instance, domain knowledge has been shown to reduce these cognitive biases [72]. This may lead to higher self-reliance and, consequently, less reliance on Al systems. Some articles examine the effects of domain knowledge Bayer et al. [e.g. 11]. Consequently, the human itself, in terms of characteristics such as being experts or novices, may influence the level of reliance. This is considered in some articles, but there are additional considerations.\nThe environment in which humans and Al interact could also influence reliance. Some articles suggest that time pressure is an influence [23]. Barr Kumarakulasinghe et al. [10] point out that physicians' active work environment might also influence their reliance. To illustrate, a physician with the same computer vision system may exhibit excessive reliance on the system when a patient's condition is critical and a decision must be made in seconds. However, in an"}, {"title": "6.1.2 Missing External Validity", "content": "The results show that reliance is considered in a multitude of use cases, ranging from autonomous vehicle control [e.g. 120] to house price predictions [e.g, 32] or solving ethical concerns [e.g. 122]. The results also show that these use cases are most often considered in controlled experiments, where WOZ approaches or manually picked sets of decision tasks dominate the interaction with the Al system. Further, most experiments are conducted with crowdworkers of MTurk or Prolific. While this has the positive effect of easy and fast data collection, it comes with the downside of missing external validity, as real Al systems with real users are rarely considered. The aspects and resulting consequences of this missing external validity are discussed in the following section.\nThe perception of randomness among humans is subject to bias. For instance, when asked to generate a random series, humans tend to generate series with higher-than-expected alternation rates. This is because humans tend to perceive clumps or streaks as not truly random, while they actually are Bar-Hillel and Wagenaar [9]. Conversely, AI is susceptible to randomness due to its probabilistic nature. Each time a study presents a manually selected set of decision instances or a WOZ study, users are confronted with a human bias from the outset. While numerous articles attempt to present a representative set of decision instances, they do not deliver any confirmation for the true randomness. Consequently, the users (decision-makers) of AI systems are rarely confronted with genuine and randomly selected AI output.\nAn illustrative example of a phenomenon that occurs in real-world AI interactions is the presence of outliers. They are an inherent and integral part of any AI system, and are often targeted to be detected [16]. While many approaches aim to minimize their occurrence, there is always the possibility of outliers occurring due to the probabilistic nature of current AI approaches. Examining the reliance on AI systems after a user is confronted with an outlier is therefore also"}, {"title": "6.1.3 Conflicting Approaches to Measuring Reliance", "content": "One potential explanation for the lack of external validity in the current literature may be the difficulty of measuring reliance. In particular, in real-world settings, it is unclear whether a human decided to do something because of the AI or whether the human simply agreed with the AI, but the AI did not influence their decision-making. For example, if a consumer purchases a product after it has been recommended by Amazon's algorithm, it is challenging to ascertain whether the consumer would have made the purchase regardless of the recommendation. Conversely, the question of whether a human decision-maker was influenced by AI advice becomes more straightforward in experiments where researchers can control the environment.\nIn controlled experiments, research sometimes employs a two-stage decision-making approach where the user must first make their own decision without any AI support. They get AI advice only afterwards and use it to change their decision. A change can then be attributed to the AI advice and be defined as reliance. In contrast, the single-stage decision approach directly confronts the user with the AI advice, rendering any such consideration impossible."}, {"title": "6.1.4 Disregard for Reliance Changes Over Time", "content": "Most articles investigate users interacting with an Al system on multiple occasions. The reliance level (e.g. agreement fraction) is then calculated by averaging all decision instances. For instance, if a user relies on the Al system eight out of ten times, the reliance level is 0.8. Alternatively, if the WOA is calculated, the reliance is defined as the mean WOA over these ten decision instances. This average reliance consideration neglects potential trends of reliance behavior. When a user interacts with an Al system on multiple occasions, it is possible that the effect may only become apparent over time. However, only a few articles consider the changes to reliance over multiple interactions, for exampleLeffrang et al. [80] focused on the effect of an outlier and how reliance changes after this outlier.\nTo illustrate this point, consider a study that investigates reliance on AI advice and concludes that users, in general, have a moderate level of reliance. A longitudinal field trial may however reveal that after some time some users start to always rely on AI advice, while others stop. Consequently, the findings of the study may become invalid after a certain period of time. An extreme case of this phenomenon is illustrated in Figure 7. In Figure 7 the (hypothetical) participant 1 (orange) increases the reliance over the time of the study and over time will always rely on the AI advice, which we denote as short-term non-reliance, long-term reliance. The (hypothetical) participant 2 (blue) in Figure 7 is the opposite, which we denote as short-term reliance, long-term non-reliance. While both will have a similar reliance behavior in a study, both users have a fundamental different reliance over a longer period of time. A study that only measures average reliance will not detect this decline. This also has implications for the real-life use of AI systems. For example, if a new Al system is tested and the study concludes that reliance is high, in practice the same tool might not impact the real world if it follows a similar pattern as (hypothetical) participant 2 in Figure 7. Consequently, we posit that studies should be guided by a conscious decision as to whether to consider the reliance change over time or to limit their scope to the average reliance level.\nOverall, it is important to consider the concept of reliance over time. Most articles do not require any changes to the experimental design; rather, they should simply report on any changes in reliance. Nevertheless, there is a clear need for further research into this phenomenon of reliance on the part of users changing over time. This will help to identify the antecedents and consequences of this process, which can then be addressed in scientific literature."}, {"title": "6.2 Emerging Issues on Al Reliance", "content": "After reviewing and analyzing the existing literature, we come up with several emerging issues that we believe require further investigation. First, there is the emerging issue of reliance on generative AI systems. These systems possess the capability of providing a new class of AI output, such as creative texts. We believe that new conceptualizations and measures are required for this class of problem. Second, we observe a focus on settings with one human decision-maker in the current literature. Real-world settings frequently comprise multiple humans and decision-makers. There is a pressing need to investigate this emerging issue.\nReliance on Generative Al Output. In recent years, the topic of generative AI has emerged as a significant area of interest. These systems possess capabilities that do not fit directly into any of the discourses presented in Section 2. The crucial difference between generative AI and more traditional AI systems is that generative AI models can create output where humans might not be able to make an initial decision. In the context of image generation models, it is for instance possible that the human user may have a general idea of the outcome, but it is unlikely that they would be able to form an initial decision on a creative outcome. Consequently, the forthcoming generative Al systems may form an additional discourse to Al reliance research.\nIt is noteworthy that some of the literature on AI reliance already employs generative Al models, most notably large language models, as evidenced by the work of Schmitt et al. [115", "49": ".", "1": "."}]}