{"title": "A Survey of Al Reliance", "authors": ["SVEN ECKHARDT", "NIKLAS K\u00dcHL", "MATEUSZ DOLATA", "GERHARD SCHWABE"], "abstract": "Artificial intelligence (AI) systems have become an indispensable component of modern technology. However, research on human behavioral responses is lagging behind, i.e., the research into human reliance on AI advice (AI reliance). Current shortcomings in the literature include the unclear influences on Al reliance, lack of external validity, conflicting approaches to measuring reliance, and disregard for a change in reliance over time. Promising avenues for future research include reliance on generative AI output and reliance in multi-user situations. In conclusion, we present a morphological box that serves as a guide for research on Al reliance.", "sections": [{"title": "1 INTRODUCTION", "content": "The term Artificial intelligence (AI) was first proposed at the Dartmouth Conference in 1956 [87]. Since then, considerable progress has been made toward the development of modern AI systems, such as transformer models [126] capable of human-like speech [51] or image generation [70]. This rapid performance growth has led to an increase in the use of AI-based solutions in productive environments and the democratization of AI access. One example of the pervasive use of AI can be observed in the publication of GPT models to everyday users through the interface of ChatGPT [1]. This illustrates the trend of the general public accessing and using cutting-edge systems.\n The ChatGPT example also highlights an issue with the widespread publication of AI systems. While there has been research on the underlying large language model (LLM) prior to its release, there has been little research on the interaction between its users and the tool. To gain a better understanding of the interaction between users and ChatGPT, it was necessary to conduct research only after the tool had been made available to the general public. This meant that the tool's potential positive and negative effects, becoming apparent in humans' interaction with them, were-and still are-not fully understood. In light of this, it is becoming increasingly clear that more regulations are needed to govern the use and development of AI systems. A noteworthy example is the EU AI Act [45], which aspires to be \"the world's first comprehensive Al law\" with the objective of regulating the development and use of AI systems.\n The full implications of human behavior with regard to these novel AI systems remain poorly understood. It is possible that individuals may use these systems to improve their performance or as lazy shortcuts, as evidenced by reduced cognitive effort when confronted with AI advice [47]. This shows that not only the performance of AI systems is important, but also the reliance of users serving as decision-makers on AI advice-a phenomenon we call Al reliance. Overall, it is important to consider not only the technical capabilities of AI systems, but also a sociotechnical view of the Al system and the human decision-makers for a productive use of AI systems.\n The pervasive use of AI in research and practice has led to a growing interest in the topic of AI reliance in order to understand how users behave when confronted with Al systems and their recommendations. If a user follows the AI advice, we can infer that the user relies on that advice. Conversely, if a user follows incorrect advice, we can infer that the user is overrelying on the AI advice. If a user does not follow correct advice, we can infer that the user is underrelying on the Al advice. Only by taking into account Al reliance can we investigate whether people are using the systems appropriately or blindly following them, therefore overrelying on them-or potentially ignoring their recommendations entirely.\n An illustrative example of the role of AI reliance is the case of the COMPAS [18] system, an Al system designed to classify the recidivism risk of criminal defendants. In this case, instances of overreliance and underreliance have a significant impact on individual defendants, who may be sentenced to longer prison terms, and on society in general, which is confronted with individuals who have a high likelihood of reoffending. This example highlights the core challenge of AI reliance in practice: achieving appropriate reliance. The human-AI team can then achieve a complementary performance that surpasses that of either the human or the AI alone [7]. This increases the overall performance of the human-AI collaboration compared to that of each individual component.\n Despite the overall importance of AI reliance, there is no clear guidance on how to conduct Al reliance research. For instance, researchers might face multiple inconsistent definitions and measures, of which some remain niche. There are several theoretical and conceptual considerations around Al reliance[e.g. 69, 104, 119]. These considerations mostly aim at differentiating trust and reliance and base their conclusions on the philosophical literature around reliance in a more general sense. A clear focus on Al reliance is missing. Whereas some recent approaches aim to formulate a formal definition of reliance [52], they focus solely on the definition of Al reliance and look at how reliance has been established in past research. Consequently, while they offer a theoretically sound conceptualization, this might not reflect how Al reliance has been understood in research. Accordingly, researchers who want to engage with this topic might find themselves torn between theoretical perspectives and their peers' research practice. Researchers who are studying Al reliance are still navigating uncharted waters in their efforts to identify the most appropriate methodology and fruitful avenues for conducting Al reliance research. Overall, the current state of AI research can be described as rather chaotic and unstructured, with a variety of definitions, measures, conceptualizations, and general understandings. In the long run, this may result in the replication of unsound practices or the application of research methods without an understanding of their implications. A systematic understanding of researching AI reliance is therefore important, and is presented in this review."}, {"title": "2 BACKGROUND ON AI RELIANCE", "content": "The notion of reliance is not unique to Al or computer systems-it is extensively discussed in the field of philosophy and psychology. Reliance is often aligned with trust, as the two concepts appear similar at first glance [6, 55]. However, many philosophers argue that trust cannot be expressed toward inanimate objects [e.g. 55, 58]. Nevertheless, a substantial body of Al research has been conducted on trust in AI, often referred to as trustworthy AI [65]. It can be argued that computer systems (and AI) are not inanimate objects, as evidenced by the \"computers are social actors\" principle [95]. When computer systems research discusses trust, it is however often concerned with reliance [36]. In this article, we follow the line of discourse that suggests differentiating between trust (an emotional or attitudinal stance toward something) and reliance (observable behavior) [e.g. 78]. This review focuses on studies that investigate behavior rather than attitude. The remainder of this section describes various discourses of human reliance on computer systems.\n We describe how reliance on automation systems has influenced research about reliance on intelligent systems. We also demonstrate how findings from the discipline of advice-taking influence research about reliance on AI to create so-called judge-advisor systems."}, {"title": "3 SURVEY METHODOLOGY", "content": "Given the lack of unified approaches in the literature, a structured literature review is employed to provide an overview of the current approaches researchers who investigate AI reliance employ. This section describes the methodology used for structurally querying the literature. Based on a classification framework introduced in Section 4, the results of the structural literature review are presented in Section 5. These results are then used in Section 6 to discuss the shortcomings and limitations of current approaches and to present guidance for researchers conducting sound AI reliance research. Additionally, future avenues for further developing the field of AI reliance are presented."}, {"title": "4 CLASSIFICATION FRAMEWORK FOR THE LITERATURE", "content": "To gain an understanding of the existing literature, the identified articles are classified according to a range of concepts. These concepts are based on the components of a sociotechnical system and the subconcepts derived in an author workshop based on a set of preselected articles. This filtering and classification process is then employed to gain insights into how the existing literature conceptualizes AI reliance. The steps are explained below.\n The sociotechnical system provides a profound perspective to consider AI reliance. As described in Section 2, a sociotechnical system consists of four components: (a) the environment, (b) the interaction, (c) the social component, and (d) the technical component in which the system is situated. These components serve as the basis for classifying the relevant articles based on their primary focus. To derive the subconcepts in the four groups, three authors held an author workshop. Prior to the workshop, each author read a subset of the articles, focusing on common themes, concepts, and interesting findings. The concepts were derived and subsequently refined during the workshop. Nine subconcepts, distributed across the four components of a sociotechnical system, were identified as relevant to the study and agreed upon by all authors. These concepts are used to classify and describe the articles and are summarized in Table 4.\n For the component of the environment, we identified three subconcepts: (1) task states whether the decision problem at hand has objectively correct advice, meaning a clear ground truth, or whether the AI should present advice for a decision task with a subjective outcome. Examples of the latter category include ethical dilemmas such as the trolley problem, where a decision-maker must determine whether to sacrifice one individual in order to save a larger number of people. Another example is personal preferences, such as selecting a song based on mood or music taste. To add to this, the (2) setting in which the data is collected is also important, as it may influence the reliance of human decision-makers. For example, whether the data is collected in a real-world setting or in a laboratory experiment might have an impact on user behavior. Finally, the articles can be differentiated according to their (3) use cases. This should provide a general overview of the coverage of AI reliance research in real-world problems. This may also allow for the identification of domains where Al reliance is considered more often than others.\n In addition to the environment, the sociotechnical system also consists of the mutual interaction between the social and technical components. As interaction, we consider the reliance of the social component on the technical component, meaning the reliance of the user on the AI system. This is influenced by the (4) decision-making approach used in the different articles. In general, there are two possible approaches. On the one hand, there is the one-step decision-making approach, where the AI advice is directly communicated to the user at the same time as the decision task. On the other hand, there is the two-step approach, where the human has to make their own initial assessment before revising it after being confronted with the AI advice. These approaches may also influence the (5) reliance measurement. Only with unified measurement are results comparable and statements generalizable. However, a variety of measures have been used to assess reliance, and there seems to be no consistent approach. We therefore examine how the articles measure reliance. This should provide insight into how the articles address the interdependence between social and technical components.\n The social component relates to the user or decision-maker who either relies or does not rely on the AI advice. The user can calibrate the reliance based on the experience they have with the tasks and the Al system. On the one hand, a (6) user training can be conducted before the decision-making tasks, for instance through trials and tutorials. This helps the user get familiar with the AI advice. On the other hand, training can also be continuous (and live) during the tasks. This involves providing (7) performance feedback about the decisions based on the AI advice. Both approaches are considered when analyzing the literature. Both have the potential to influence the reliance of users on the system and therefore the sociotechnical system's social component.\n The technical component refers to the AI system itself, therefore it is important to cover relevant aspects of the system. Consequently, it is important to evaluate the (8) Al system implementation, as this influences the reliance. We abstract this performance to the three cases of \"Wizard of Oz\u201d (WOZ) studies, manually selected labels by the authors, and live AI system. In WOZ studies, no genuine AI system is constructed; rather, users interact with human-generated advice, which is labelled as AI advice. One of the most common design decisions for Al systems, especially when user interaction is a primary concern, is the incorporation of (9) transparency mechanisms, such as explanations. By examining these subconcepts, we can gain a comprehensive understanding of the technical component."}, {"title": "5 RESULTS", "content": "Based on the categorization framework introduced in the previous section (Section 4), we analyze the current literature on Al reliance in this section. In Section 5.1, we present the environment where Al reliance occurs in the identified articles. In Section 5.2, we present the general conceptualizations of the mutual interaction of the technical and social components, namely reliance, by reviewing the measures and decision-making approaches. In Section 5.3, we address the social component of the sociotechnical system and review the way users come to know the systems. Finally, to include the technical component, we review the properties of the AI system in Section 5.4."}, {"title": "5.1 Exploring the Environment of Al Reliance", "content": "The first component of the categorization framework is the environment in which AI reliance is observed. This is based on the subconcepts of the task, the setting, and the use cases in which Al reliance is studied. It is notable that, despite the prevalence of real-world use cases in daily life and work, most articles employ crowdworker marketplaces, such as Amazon Mechanical Turk (MTurk) or Prolific, instead of professionals or experts. Indeed, 47 articles use online crowdworker marketplaces to recruit participants. The remaining articles employ a variety of recruitment strategies, including domain experts (n = 9), students (n = 5), and convenience sampling (n = 10), without further restrictions on the participants. This indicates that most articles use crowdworker marketplaces to recruit participants, and nearly all articles examined reliance in a controlled and isolated setting.\n A multitude of use cases are employed and presents an overview. For a more in-depth analysis, we can divide the use cases into two groups: those where the AI aims to present objectively correct advice, meaning use cases with a ground truth present, and those where the outcome is of a subjective nature, linked to user preference. Both require fundamentally different Al systems, with different goals. One system can be trained with an objective ground truth, while the other systems aim to estimate the individual user preferences. It is important to distinguish these cases and become aware of the specific use case that the AI system aims to address.\n A prime example of the group of subjective use cases is recommender systems, such as music recommendations [84, 103] or recommendations on the attractiveness of other people [106]. In these cases, the AI system cannot predict an objective ground truth (because there is none) but must rather aim at the user's preferences. Besides recommendations, AI can also assist with creative tasks [59]. Ethical decision-making is also often seen as having subjective outcomes. Examples from the AI reliance literature are ethical decision-making tasks, such as ethical dilemmas. Examples in the literature are military defense and rescue actions [122] or a decision about a recipient for a donor kidney, where multiple viable options exist [94]. Subjective outcomes can also be found in use cases where one might not expect them at the outset. For example, managers may have preferences about investment opportunities [66] or customers may have preferences about life insurance products [14]. Some research even frames stock trading use cases as a task with subjective outcomes, as some stocks may be preferred over others [25]. To add to this, while house prices may have a ground truth, in the case of finding a subtenant for an apartment, there is a tradeoff between the rental price and the certainty of finding a suitable candidate. This tradeoff also represents a subjective result, as some users may prefer to change a high rental rather than the certainty. Overall, tasks with subjective outcomes are most often found in the domains of personal preferences or ethical decision-making. In other domains, some articles frame a subjective task for users, where the AI system attempts to advise on the preferences of the user in opposition to an objective ground truth.\n In addition to the use cases with subjective outcomes, the majority of literature is concerned with use cases that have an objective ground truth. Some of these cases are anchored in real-life scenarios, while others are isolated experiments. In the domain of finance & business settings, numerous articles are concerned with loan applications [38, 56, 62, 102] and stock trading [27, 28, 35]. Both appear to be favored use cases in current literature. One reason may be the close proximity to other machine learning research, which also frequently considers these cases. In addition to loan applications and stock trading, other use cases include basketball betting [43] and the prediction of incoming call center calls [13]. It is also noteworthy that in certain use cases (such as stock trading or incoming call prediction), the objective is to predict future events, such as the return of a stock or the number of incoming call center calls in the future. For these use cases with future events, decision-makers are confronted with delayed labels. Other use cases are concerned with directly uncovering the ground truth without the need for delayed labels. Closely adjacent to business decisions are the uses cases in housing, where users should mostly estimate the prices of houses [32, 33, 101] or find a fitting flat using online platforms [53]. Another domain that is often researched in AI reliance is the medical domain. The articles can be grouped into patient and elderly care [79, 133], image classification, such as MRI and X-ray [20, 46], and disease detection. Various diseases are considered, such as diabetes risk [40], myocardial infarction, a problem with the heart [98], oncology assessment [130], or sepsis classification [10]. In general, these articles also opted for experts, such as doctors, as decision-makers, as opposed to crowdworkers. This can be attributed to the high specialization required to detect diseases. The last groups of real-life anchored use cases are autonomous vehicles [3, 97, 120], student performance assessment [37, 105, 132], and the case of recidivism assessment, often based on the COMPAS algorithm [31, 50, 128]. In summary, we identify a multitude of use cases that investigate tasks in close proximity to real-life settings. We however also observe clear preferences in current research regarding AI reliance in domains such as finance and business, as well as in the medical domain."}, {"title": "5.2 Measuring the Interaction of Human and Al", "content": "The second category of the categorization framework is the interaction between social and technical components. In the case of Al reliance, this interaction is human reliance, which we examine by reviewing current approaches and measures to quantify reliance on AI advice. First, we must establish a general understanding of the approach that should be used to measure reliance. As discussed below, there already seems to be some disagreement in the current literature regarding this matter. In light of the aforementioned approach, we proceed to present the existing measures of reliance.\n Decision-Making Approach. Depending on whether we view AI systems as automation systems or as sources of advice, two distinct approaches can be identified. As described in Section 2, advice can be presented directly to the decision-maker or only shown after the decision-maker has already made an independent decision. This leads to two distinct approaches: a single-stage approach, where advice is presented before the decision is made, and a two-stage approach, where advice is presented after an initial decision is made and the decision-maker can revise their decision.\n The more straightforward way to implement the decision-making approach is a single-stage decision-making approach, where users are directly confronted with the AI advice. The final decision from the human-AI interaction is recorded only then. This can also have some variants. Among the groups of articles that have a single-stage decision-making approach, there is one article that measures the human decision twice after being exposed to the AI decision [103], making it a multistage approach, but only for the human decision and not the AI advice. Further, instead of the human making their own decision after being exposed to the AI, sometimes the human can only accept or reject the AI advice [e.g. 14], or the human can decide if they want to decide, or if they want to let the AI decide for certain tasks [e.g. 97]. Another variant is that the Al advice is implemented automatically until a human intervention, as in the case of automatic driving games [e.g. 3]. While we find several variants, they are all unified as a single-stage approach, where the human is exposed to the AI advice without a prior decision on the task."}, {"title": "5.3 Human User as the Social Component", "content": "Once the settings and use cases have been established, as well as the general conceptualizations and measures of reliance, we proceed to the social component. In most decision-making tasks, both real-world and experimental, humans are confronted with multiple decision instances in rapid succession. For instance, a doctor may have numerous patients requiring a diagnosis. We claim that the experience gained from one instance may influence the decisions made in the next. We create a two-dimensional view and distinguish between training with the AI system and performance feedback during the main task, which is presented in Table 5. While analyzing the literature, it becomes apparent that not all articles explicitly state whether they provide feedback and/or training to the participants. Consequently, if no information about training with the AI was present, it was classified as no training. Furthermore, if an article tested out treatments with and without feedback or training, it was classified as an article with training or feedback present. Finally, if no information about feedback was present and the setting and use case did not provide feedback implicitly, it was classified as no feedback. An example of feedback provided implicitly is an autonomous vehicle driving game [3], where no crash with the autonomous vehicle indicates that the AI provided positive advice. Another example is a multiday trading game [28], where each day the return of the previous day was known.\n As illustrated in Table 5, most articles do not provide training or feedback to users. We speculate that this could be due to underreporting. Another reason could be that a straighforward experiment design might lead to easier data collection than an experimental design with training and/ or feedback. Most articles do not examine changes in reliance over time or across multiple interactions, so that feedback would not have any impact on the empirical results. In certain instances, the provision of training and feedback may be impracticable, particularly in the case of tasks that are designed as one-off sessions [e.g. 14]. Some articles have training before the main task, where participants are trained on the task, for example bird image classification [112], but without the AI advice. One rationale for this approach is to assess the participants' skills before the main task, which could be a potential confounder to reliance. At the same time, the reliance behavior is not influenced by the training. In general, most articles employ a straightforward experimental design, where participants are not provided with any training with the AI system or performance feedback on individual task instances. This approach is employed to measure the average reliance of the participants and to allow for comparisons between groups.\n Another group of articles provide training to participants but no feedback. This is often done to reduce potential misunderstandings and to help users understand the function of the AI system. While some articles have training for the task without showing the AI advice to the user, this group focuses on articles where training with the AI advice was employed. Training involves allowing the user to interact with the system advice without considering those interactions in the analysis. It is sometimes done with one single decision instance [19], but often the training also involves many interactions with the AI system, such as 10 practice instances [32]. In some instances, the initial training is also integrated with an initial screening of participants, where performance in the training double as a metric for selecting potential participants [46]. In sum, we see several articles provide training with the AI system that does not count toward the final main task and metrics.\n Some articles provide feedback but no training. The user can learn as they interact with the AI. This can also be seen as continuous training while on the task. However, all decision instances count toward the overall performance. While most articles in this group provide feedback after each decision, there are those that provide feedback halfway through the decision instances [37, 133]. Also, some articles do not explicitly state that feedback is provided. However, it can be inferred that users received feedback at least implicitly, so we also count them in this group. For example, in the case of music recommendations [103], a participant would directly know about the performance of the Al model based on the recommended music. These examples show that sometimes feedback is implicit and not necessarily a conscious design decision.\n Finally, a few articles use both training and performance feedback during the main task. This is arguably a more complex experimental design, since not only do you have to make decisions about how and when to provide feedback, but you also have to design a training session for the users. This may be one reason why only a few articles choose this design. While this is rarely applied, for many applications it may be close to reality. A medical professional interacting with an Al system will most for example likely receive an introduction/training session with the tool and will also receive performance feedback, as they will be able to observe the patients and the system diagnosis. Most articles considering real-world use cases are therefore designed with training and feedback, such as the task of predicting the incoming calls of a business center [13], or the case of driving an autonomous car [3].\n Overall, we find that most measures are calculated by having a user interact with the system multiple times and then taking the average of those interactions. For example, if a user interacts with the system 10 times and 8 times, the user gets the same output as the AI advice, the agreement percentage is 80%. However, this does not take into account the effects of repeated interactions with the AI system or prior exposure, such as training. A user who has been trained on the Al system may have a different reliance level than a user who is interacting with the system for the first time. Furthermore, if a user receives feedback on the performance of the AI system after each of these 10 decisions, the reliance might change due to this feedback. Only a few articles explicitly measure change in reliance. For example, Grgi\u0107-Hla\u010da et al. [50] study the COMPAS algorithm and explicitly argue with real-world judges who also have the chance to adapt their reliance. Also, Leffrang et al. [80] investigates how users recover from bad advice given by the AI system. These cases are however rare, so most articles have neither training nor feedback for users."}, {"title": "5.4 Al System as the Technical Component", "content": "In addition to the social component, the technical component-the AI system-plays a pivotal role in sociotechnical systems. In this section, we therefore review the various Al systems used in the AI reliance literature, most prominently, the nature of the AI system itself and a common design decision of transparency mechanisms.\n With regard to the AI system itself, it is evident that numerous articles do not construct real Al systems. Instead, users frequently interact with mockups and dummies in WOZ studies (n = 29) [e.g. 103, 115, 122]. In these studies, the users are informed that they are interacting with an AI system and receiving AI advice, whereas the researchers constructed this advice. This methodology is commonly employed, particularly in HCI research. It also streamlines data collection, as no Al system needs to be constructed. It is however crucial to exercise caution, as mocked systems do not provide real Al advice and may be biased in their output. Some of these studies even provide AI advice that is always correct Panigutti et al. [98], Srivastava et al. [120], therefore overreliance of users is not possible. As a preliminary step toward enabling decision-makers to interact with real AI systems, some articles construct real AI systems but present manually selected instances of AI advice. In other words, the researchers manually sample the dataset (n = 19) [e.g. 30, 62, 128]. This approach is frequently employed to achieve a specific performance of the model. One advantage of this approach is that it is closer to reality, as a real Al system created the advice. For instance, while some articles explicitly investigate outliers [e.g. 80], these are still hand-picked and expected, and the randomness is absent. These manually selected labels also distinguish between the model accuracy of the underlying model and the sampling accuracy of the samples presented to the decision-makers. For example, Chen et al. [30] presents eight decision instances, five of which have correct advice, resulting in a sampling accuracy of 62.5%, but the underlying models have accuracy above that. This distinction between sampling and model accuracy might induce biases. Only when decision-makers interact with real AI systems does sampling accuracy equal model accuracy. However, only a few articles allow users to interact with real Al systems (n = 21) [20, 102, 114]. In most cases this results in a random sample from a list of AI advice. Overall, we find that many studies do not use real AI systems but rather either fully mocked interfaces or hand-picked AI advice. To address the issue of WOZ systems and real Al systems, for example, Ashktorab et al. [5] tests both a real AI and a WOZ study with a \"perfect\" AI system, meaning a system that always outputs correct advice.\n One of the most common design decision for Al systems are transparency mechanisms. For this review, we distinguish between explanations and the statement of AI performance or certainty. We find that many articles provide explanations to users (n = 27). There are numerous explanations. Notable examples include [14, 128], SHAP values [e.g. 38, 40]. Additionally, other forms of explanation have been employed, including those based on LIME [10] or counterfactual explanations [79]. Collectively, these examples illustrate the diversity of approaches to explanation. In addition, in some articles the user is provided a statement on the overall Al performance or the certainty on a specific decision task (n = 11) [e.g. 101, 103, 106]. Interestingly, only a few articles present both to the user (n = 8) [e.g. 21, 71, 105]. Finally, several articles do not employ any transparency mechanisms (n=25) [e.g. 23, 53, 86]. These articles often focus more on general applicability, such as the case of an AI that predicts preferences, where this premise is already the subject of research [e.g. 84, 122]. Other examples are articles that are interested in different aspects, such as the abovementioned change of reliance over time [50] or the use of multiple users [31]. It becomes apparent that a clear majority of articles present some form of transparency mechanism, which highlights their technology-centricity. Changes to the design of the technological system are often more interesting than the actual users, which often consist of crowdworkers and non-specific user groups."}, {"title": "6 DISCUSSION", "content": "After a comprehensive review of the existing literature on Al reliance, we discuss its implications and shortcomings, which serve as inspiration for new research avenues. Overall, we identify the following five directions for future and more in-depth research on Al reliance: (1) the influence of various factors on reliance is not fully understood; (2) the external validity of studies is often limited; (3) different approaches have been employed to measure reliance, making comparisons difficult; (4) the impact of reliance on individuals and society over time has not been adequately addressed; (5) the potential implications of emerging issues related to Al reliance have not been fully explored. The remainder of this article will provide a more detailed examination of these avenues."}, {"title": "6.1 Limitations of Current Al Reliance Research", "content": "The sociotechnical perspective on AI reliance identifies four distinct angles through which influences on Al reliance can be identified: the social component, the technical component, the interaction between these two, and the environment. All these factors play a role in Al reliance for decision-making. Only with this comprehensive perspective can we fully acknowledge the influences on Al reliance. A review of the literature reveals that most articles only consider individual components of the sociotechnical system. Even within these components, there is often no consensus on the factors that influence reliance on AI systems. In the following section, we discuss current approaches in the various components of sociotechnical systems. In conclusion, we urge further research to obtain a more comprehensive view of Al reliance.\n A significant proportion of articles in this field focus on the technical component-the AI system itself-and its influence on reliance. The design aspects of Al systems are frequently the primary considerations, with explanations being a key area of interest. It is also evident in the literature that transparency mechanisms, such as explanations, do not have a monotonic influence, and explanations do not always lead to the same results [113]. Consequently, an exclusive focus on explanations, or the design of the technical component in general, is insufficient for a comprehensive understanding of AI reliance. It is imperative to consider the users interacting with the AI system.\n The social component, referring to the human decision-maker, can also contribute to reliance. Humans are prone to cognitive biases [63] and AI systems have been shown to induce various such biases, most notably the anchoring or automation bias [105]. The effect of the bias may also depend on humans themselves, with some people being more prone to cognitive biases than others. For instance, domain knowledge has been shown to reduce these cognitive biases [72]. This may lead to higher self-reliance and, consequently, less reliance on Al systems. Some articles examine the effects of domain knowledge Bayer et al. [e.g. 11]. Consequently, the human itself, in terms of characteristics such as being experts or novices, may influence the level of reliance. This is considered in some articles, but there are additional considerations.\n The environment in which humans and Al interact could also influence reliance. Some articles suggest that time pressure is an influence [23]. Barr Kumarakulasinghe et al. [10] point out that physicians' active work environment might also influence their reliance. To illustrate, a physician with the same computer vision system may exhibit excessive reliance on the system when a patient's condition is critical and a decision must be made in seconds. However, in an environment where the physician has ample time to make decisions, there might be appropriate reliance on the AI system. The environment in which humans and AI interact may therefore influence the degree of Al reliance.\n Finally, the interaction itself can influence reliance. A review of the literature reveals that there are two main approaches to decision-making and subsequently measuring reliance: single-stage and two-stage. Both influence the interaction and subsequently AI reliance. A single-stage decision-making approach facilitates fast decision-making and quick acceptance of the AI advice. In contrast, a two-stage approach enforces slow thinking, and the decision-maker must invest more effort in coming up with a decision. Cognitive forcing has been identified as a factor that influences reliance [19]. The interaction itself therefore plays a role in determining the extent to which humans rely on Al advice.\n The influences on reliance can be conceptualized as a multidimensional space. For instance, a user interacting with the same Al system may exhibit varying degrees of reliance based on environmental factors, such as time constraints. Alternatively, a user interacting with the same Al system in the same environment may exhibit varying degrees of reliance based on the decision-making approach. This is because the same user may have a different reliance behavior when confronted with a single-stage decision-making process as opposed to a multistage approach. It is evident that current research articles tend to focus on a single (or at most two) dimension of AI reliance. A more comprehensive approach is however required to fully understand the nature of Al reliance. Specifically, there is a lack of understanding of how various dimensions or individual influence factors interact with each other with regard to reliance. For instance, it is likely that time pressure can interact with provision of explanations and their form. We therefore call for more research on all dimensions simultaneously. This would however lead to virtually unlimited combinations that would require attention. Nonetheless, we claim that focusing on combinations that occur frequently in the real world can help select appropriate combinations, while ensuring external validity."}, {"title": "6.2 Emerging Issues on Al Reliance", "content": "After reviewing and analyzing the existing literature, we come up with several emerging issues that we believe require further investigation. First, there is the emerging issue of reliance on generative AI systems. These systems possess the capability of providing a new class of AI output, such as creative texts. We believe that new conceptualizations and measures are required for this class of problem. Second, we observe a focus on settings with one human decision-maker in the current literature. Real-world settings frequently comprise multiple humans and decision-makers. There is a pressing need to investigate this emerging issue.\n In recent years, the topic of generative AI has emerged as a significant area of interest. These systems possess capabilities that do not fit directly into any of the discourses presented in Section 2. The crucial difference between generative AI and more traditional AI systems is that generative AI models can create output where humans might not be able to make an initial decision. In the context of image"}]}