{"title": "Nimbus: Secure and Efficient Two-Party Inference for Transformers", "authors": ["Zhengyi Li", "Kang Yang", "Jin Tan", "Wen-jie Lu", "Haoqi Wu", "Xiao Wang", "Yu Yu", "Derun Zhao", "Yancheng Zheng", "Minyi Guo", "Jingwen Leng"], "abstract": "Transformer models have gained significant attention due to their power in machine learning tasks. Their extensive deployment has raised concerns about the potential leakage of sensitive information during inference. However, when being applied to Transformers, existing approaches based on secure two-party computation (2PC) bring about efficiency limitations in two folds: (1) resource-intensive matrix multiplications in linear layers, and (2) complex non-linear activation functions like GELU and Softmax. This work presents a new two-party inference framework Nimbus for Transformer models. For the linear layer, we propose a new 2PC paradigm along with an encoding approach to securely compute matrix multiplications based on an outer-product insight, which achieves $2.9\\times ~ 12.5\\times$ performance improvements compared to the state-of-the-art (SOTA) protocol. For the non-linear layer, through a new observation of utilizing the input distribution, we propose an approach of low-degree polynomial approximation for GELU and Softmax, which improves the performance of the SOTA polynomial approximation by $2.9\\times ~ 4.0\\times$, where the average accuracy loss of our approach is 0.08% compared to the non-2PC inference without privacy. Compared with the SOTA two-party inference, Nimbus improves the end-to-end performance of BERT base inference by $2.7\\times ~ 4.7\\times$ across different network settings.", "sections": [{"title": "1 Introduction", "content": "Transformer models [36] bring about significant advancements in various machine learning tasks, such as language understanding [19], vision tasks [6], and chatting bot [21]. As Transformer models handle increasingly sensitive data and tasks, privacy becomes a major concern for deployment. For example, one hospital (client) wants to use the model from another organization (server) to enhance its diagnostic capabilities. This raises privacy concerns for both parties: either the hospital has to upload its private data, or the organization needs to send its proprietary model to the hospital.\nRecently, several works [14, 26, 16, 29], building upon secure two-party computation (2PC), realize secure two-party inference in a privacy-preserving way. These works proceed by having the client and server jointly execute inference over the \"encrypted\" input and model, using cryptographic techniques including homomorphic encryption (HE) [7], additive secret sharing, etc. The client learns nothing about the model except for inference results and keeps the server unknown for the client's input.\nPrivacy protection comes with substantial computation and communication costs due to expensive cryptographic operations. While existing secure two-party inference for convolution neural networks could be completed in a few minutes [8, 23, 18, 2, 34, 32, 17], it is more challenging to make secure"}, {"title": "2 Background", "content": "We present the necessary background, including the threat model, cryptographic building blocks, and secure Transformer inference."}, {"title": "2.1 Threat Model", "content": "Our protocol works in the two-party setting where the client C holds an input and the server S holds a model. Our protocol is secure in the presence of a semi-honest adversary who could passively corrupt either the client or the server, where the adversary follows the protocol specification but may try to learn more information than allowed. Semi-honest adversary is a common assumption for privacy-preserving machine learning and has been used in most two-party protocols [18, 34, 17, 14]. As in all prior two-party inference protocols, the client is only allowed to learn the model's architecture and inference result while the server gains no information about the client's input."}, {"title": "2.2 Notation", "content": "We use upper-case bold letters to represent matrices like W for model weights and X for activations. For a matrix W, we use $W_i$ to denote the i-th row of W and $W_{i,j}$ to denote the entry in the i-th row and j-th column of W. For an integer n, we write $[n] = \\{0,1,\\ldots, n - 1\\}$. For an additive secret sharing $(\\cdot)$ (defined in Section 2.3), we use $(\\cdot)_c$ (resp., $(\\cdot)_s$) to denote a share held by a client (resp., a server). We denote by $[M]$ the homomorphic encryption (HE) ciphertexts on matrix/vector M where it may consist of multiple ciphertexts. We use $Z_{2^e} = Z \\cap [0, 2^e)$ to denote a ring with all"}, {"title": "2.3 Building Blocks", "content": "Our framework is built upon multi-party computation (MPC) techniques, including additive secret sharings and homomorphic encryption (HE). The building block of oblivious transfer (OT) and the sub-protocols used in non-linear layers can be found in Appendix B."}, {"title": "Additive Secret Sharings", "content": "In the two-party setting, an additive secret sharing over a ring $Z_{2^e}$ is defined as: for a value $x \\in Z_{2^e}$, two random shares $(x)_c\\in Z_{2^e}$ and $(x)_s \\in Z_{2^e}$ are sampled uniformly such that $x = (x)_c + (x)_s \\mod 2^e$, where $(x)_c$ is held by a client and $(x)_s$ is held by a server."}, {"title": "Homomorphic Encryption", "content": "We adopt the lattice-based additive HE scheme [26] (building upon [33]). HE allows one party to perform computations on the encrypted data of the other party without the need for the decryption key. The HE scheme encodes a plaintext vector $m \\in (Z_{2^e})^l$ into a plaintext polynomial $m \\in A_{N,2^e}$, and then m is encrypted to a ciphertext $[m] = (b,a) \\in A_{N,q}^2$, where q is a ciphertext modulus. Given a ciphertext $[m]$ and a circuit f including only linear operations, one can homomorphically compute another ciphertext $[f(m)]$. We refer the reader to Appendix B.1 for details of the HE scheme and its homomorphic operations."}, {"title": "Conversion between Floating-point Numbers and Ring Elements", "content": "As 2PC and HE usually operate over rings, the floating-point numbers used in Transformers need to be converted into fixed-point numbers in a ring. Given a scale $s \\in N$ (i.e., the length of the fractional part) and a ring $Z_{2^e}$, a floating-point number $x \\in R$ is converted to the approximated fixed-point number by computing $x := [x2^s] \\mod 2^e$, and i can be converted back to the approximated x by setting $x/2^s$."}, {"title": "2.4 Secure Two-Party Transformer Inference", "content": "The details of the Transformer architecture are described in Appendix A. To securely evaluate the model, the input and output of all layers are in the form of additive secret sharing, enabling the arbitrary linkage of different layers despite specific protocols. This work optimizes the protocol of the linear layers, including $Linear_{qkv}$, $Linear_{\\circ}$, $Linear_{h_1}$, and $Linear_{h_2}$. We also optimize the protocols for non-linear layers Softmax and GELU. The activation multiplication in the attention and layer normalization are relatively fast following SOTA studies [28, 26]. We do not give special optimizations and leave them as future works."}, {"title": "3 Secure Computation of Linear Layers", "content": "We first analyze the efficiency problems of the prior solution in Section 3.1. Then, Section 3.2 presents our client-side outer product (COP) protocol with row-wise encoding. In Section 3.3, we optimize the memory occupation of our COP protocol."}, {"title": "3.1 Prior Solution: Server-side Inner Product Protocol", "content": "The starting point of this work is the protocol so-called server-side inner product (SIP) [17, 14, 26], as shown in Figure 2(a). The inputs of the linear layer are additive secret sharings $(X)_c, (X)_s \\in Z_l^{k\\times m}$ held by the client and server. The server also holds the weights $W \\in Z_l^{m\\times n}$.\n\\begin{aligned}\n\\hat{\\mathbb{Z}} &= \\pi_l(X) : \\hat{\\mathbb{Z}}[i\\cdot m n + (m - 1) - j] = X_{i,j}, \\text{ for } i \\in [k], j \\in [m] \\\\\n\\hat{\\mathbb{W}} &= \\pi_r(W) : \\hat{\\mathbb{W}}[j m + i] = W_{i,j}, \\text{ for } i \\in [m], j \\in [n]\n\\end{aligned}\n(1)\nThe values of two activation shares and server weights are encoded into polynomials over $A_{N,2^e}$ using encoding functions $\\pi_l : Z_l^{k\\times m} \\rightarrow A_{N,2^e}$ and $\\pi_r : Z_l^{m\\times n} \\rightarrow A_{N,2^e}$, as shown in Equation (1). The coefficients of the polynomials $\\hat{\\mathbb{Z}}$ and $\\hat{\\mathbb{W}}$ that are not defined are set to 0."}, {"title": "3.2 Client-side Outer Product Protocol", "content": "To solve the efficiency problems as described above, we propose an alternative client-side outer product (COP) protocol. The COP protocol includes two key insights. First, the static nature of model weights allows the server to send encrypted weights to the client at the setup stage, which can eliminate input communication at the online stage. Second, this elimination of input communication enables us to design a new row-wise encoding that realizes homomorphic matrix multiplication through the outer product. Our encoding further results in compact output ciphertext for communication and enhances the efficiency of HE matrix multiplication. The formal protocol is described in Appendix C\u0421.1.\nCOP Protocol. In Figure 2(b), we describe the COP protocol for secure matrix multiplication, where the dashed boundary shows the optimizations of computation and communication over prior works. In the setup stage, the server encodes the model weights W in a row-wise fashion and sends the HE ciphertexts $[W]$ on these weights to the client. The client stores the HE ciphertexts $[W]$ in the disk, which enables these ciphertexts to be reused for multiple queries by loading them into memory. In the execution stage, for additive secret sharings $(X)_c, (X)_s$ held by the client and server respectively, the client homomorphically computes $(X)_c \\cdot [W]$ to obtain HE ciphertexts $[W(X)_c]$, and the server locally computes $W \\cdot (X)_s$ in plaintext. Then, the client samples a random matrix R (used as its"}, {"title": "3.3 Memory Impact of the COP Protocol", "content": "In our COP protocol, the client stores the encrypted model weights in the disk so that the ciphertexts can be reused. At the online stage, the encrypted weights are loaded into memory for secure"}, {"title": "4 Secure Computation of Non-Linear Functions", "content": "4.1 Prior Solution: Piecewise Polynomial Approximation of Non-Linear Functions\nFor Transformers, the main efficiency bottleneck in non-linear layers is to securely compute functions exponential and GELU [5, 28, 26]. These works approximate the non-linear functions through piecewise polynomial approximation, which can be securely computed by executing two-party addition, multiplication, and comparison operations. To maintain the accuracy, these works adopt four-piece polynomials with degree 6 for GELU and two-piece Taylor series with Taylor expansion degree 6 for exponential. The approximation of high-degree polynomials inherently imposes a large overhead for securely computing the powers of values. Additionally, such an approximation requires computations to be conducted over a large ring $Z^{64}$ and with a large scale $s = 18$ [14, 5, 26]. This is brought about by the fact that computing the powers of values with high degrees leads to the accumulation of fixed-point errors and the potential overflow problem."}, {"title": "4.2 Simpler Piecewise Polynomial and Smaller Rings by Distribution-aware Approximation", "content": "We aim to use simpler piecewise polynomials to fit non-linear functions and reduce the size of rings without sacrificing accuracy. Inspired by the finding that activation distribution exhibits a regular pattern across training and test data [24, 40], our insight for enabling simpler polynomials is to assign the approximation budget according to the input distribution instead of treating all input values with equal importance. Figure 4 illustrates patterns of the input distribution using the BERTbase's"}, {"title": "Distribution-aware Splitting and Fitting", "content": "Prior works typically split the input range and fit each interval based on the non-linearity of the curve. We further include the consideration of the input distribution for these two processes. For intervals with low non-linearity or input probability, we split them out and assign constant or linear polynomials to fit. The other intervals with both high non-linearity and input probability are fitted by quadratic or cubic polynomials. When fitting each piece of the non-linear function $f(x)$ by the polynomial $f'(x)$, we minimize the expected loss that integrates the inputs' probability density $p(x)$\n$$\n\\min_{f'(x)} \\int_{l}^{h} p(x) [f(x) - f'(x)]^2 dx,\n$$\n(3)\nwhere l and h are the lower bound and upper bound. p(x) is the probability density function obtained by summarizing a batch of training data. Unlike prior works using fixed breakpoints l and h, we initialize each breakpoint with a starting value and search around it to better fit non-linear functions at different depths. This is because although the activation distributions are broadly similar, they may shift slightly across varying model depths and the breakpoints should be adjusted accordingly. We refer to the Appendix C.2 for the splitting and fitting algorithm. The detailed protocols for securely evaluating nonlinear functions are provided in Appendix C.1. Next, we elaborate on the specific design for fitting exponential and GELU functions."}, {"title": "Exponential", "content": "The exponential is used in the Softmax. Given an input vector x, the i-th element of Softmax is computed as $\\frac{exp(x_i-max\\{x\\})}{\\sum exp(x_i-max\\{x\\})}$. Input values subtracted from maximal values result in maximal zero. The exponential curve exhibits two distinct patterns: a long smooth tail on the left and a sharp increase on the right. Prior works adopt a two-piece approximation by breakpoint -13. Instead, we initial breakpoints around -4 for varying depths. As the right interval spans a smaller range, it adopts a cubic polynomial $P^3(x)$ instead of the Taylor series with expansion degree six [5, 26]. Values less than -4 are less occurred and the curve is smooth, and a linear function is enough to fit.\n$$\nexp(x) \\approx\n\\begin{cases}\n0 & x < T_{exp} \\\\\nP^3(x) & T_{exp} < x \\leq 0\n\\end{cases}\n$$\n(4)"}, {"title": "GELU", "content": "The GELU curve nearing the zero exhibits pronounced non-linearity. Prior works [5, 26] assign two polynomials for intervals [-5, -1.97] and [-1.97, 3] with degree three and six. We merge these two intervals by one and shrink the range to $[T_1, T_2] = [-2.1, 0.2]$. This is because the values beyond this interval present either less non-linearity or fewer occurrence probabilities, and using constant or linear polynomials is enough. As the middle interval becomes narrow, we find a square polynomial $P^2(x)$ is enough. The specific breakpoints $T_1$ and $T_2$ change for different depths.\n$$\nGELU(x) \\approx\n\\begin{cases}\n0 & x <T_1 \\\\\nP^2(x) & T_1 < x < T_2 \\\\\nX & x > T_2\n\\end{cases}\n$$\n(5)"}, {"title": "4.3 Free Ring Conversion by Fusion with Truncation", "content": "Our low-degree polynomials reduce the errors of operation on fixed-point numbers and potential overflow problems. This enables smaller ring $Z_{32}$ and precision $s = 12$ for computing Softmax and GELU functions, instead of the original standard ring $Z^{64}$ and precision of $s = 18$. However, since other operations still require the larger ring to preserve the accuracy, another challenge is to convert secret shares between differently sized rings. The process of downcasting from a larger to a smaller ring can be performed locally, incurring negligible cost [31, 39]. Upcasting from a smaller to a larger ring necessitates addressing the wrap-around of shares, requiring communication among parties. Interestingly, we notice the situations demanding to upcast are always after a truncation operation that inherently computes the wrapping, which can be repurposed for the upcast to avoid additional costs. We propose a novel protocol that fuses the upcast with the truncation. We defer the protocol and the correctness proof to the Appendix E."}, {"title": "5 Performance Evaluation", "content": "Experimental Setup. We follow similar configurations used in prior works [26]. Except optimized non-linear functions using ring $Z_{2^{32}}$ and precision $s = 12$, other operations follow standard $Z_{2^{64}}$ and $s = 18$ for the secret sharing. We use N = 8192 for the HE encryption. The performances are evaluated on two nodes with 64 vCPUs and 128 GB memory. We use Linux Traffic Control (tc) to simulate LAN and WAN network settings, where the bandwidth and the ping latency are (3Gbps, 1ms) and (400Mbps, 10ms), respectively.\nBaselines. The baselines include Iron [14] and BumbleBee [26]. Our implementation follows the open-sourced code of BumbleBee on SecretFlow [28]. As Iron is not open-sourced, we implement Iron following their protocol using the SecretFlow library for a fair comparison. For the linear layer, Iron uses window encoding described in Section 3.1, and BumbleBee further compresses the output ciphertext. For non-linear functions, Iron evaluates them via integrating underlying protocols. Later works [5] use piecewise polynomial approximation. BumbleBee further integrates cryptographic optimizations to make a stronger baseline. In the Appendix F.4, we also compare our work with those that use rough approximations to trade off accuracy for efficiency [22, 29].\nModel and Datasets. Our method is evaluated on widely used Transformer model BERTbase [19] from HuggingFace [38]. When evaluating the performance, we use 128 as a mild average number of the input sequence length. To evaluate the accuracy of our non-linear approximation, we test it on eight datasets from widely used GLUE benchmark [37]. To obtain the input distribution of non-linear functions, we randomly sample sentences from the training dataset until the total token count reaches 512. This number is chosen because further increasing the number of sampled tokens yields no significant changes in the input distribution."}, {"title": "5.1 Accuracy Comparison", "content": "Table 2 reports the accuracy of floating-point plaintext, BumbleBee, and our approximation across 8 tasks in the GLUE benchmark[37]. The precise approximation of BumbleBee causes small errors due to the truncation error of the fixed-point value computation. Without fine-tuning, Nimbus decreases accuracy in a small range and the average loss is around 0.6%. Such loss can be easily reduced to 0.08% through a lightweight fine-tuning Nimbus\u2020. This demonstrates the effectiveness of our"}, {"title": "5.2 Efficiency Comparison", "content": "As the main body of the Transformer model are identical Transformer blocks, we present the end-to-end latency of one Transformer block under LAN and WAN network settings in Figure 6. Besides the optimized parts of this paper, we unify the unoptimized activation matrix multiplication (QKT&PV) and LayerNorm (LN) using the BumbleBee's latency. The latencies of Iron are shorter than those reported in their paper due to SecretFlow integrating many SOTA optimizations for the building blocks, such as OT [41] and inverse square root [25]. Combining all our optimizations, the overall runtime is 4.8\u00d7 ~ 5.9\u00d7 faster than Iron and 2.7\u00d7 ~ 4.7\u00d7 faster than BumbleBee. Comprehensive results on varying Transformer size and input sequence length are listed in Appendix F.1. In the following, we provide a detailed analysis of linear and non-linear layers.\nFor linear layers, our method is efficient in both computation and communication. Therefore, we achieve obvious speedup in both LAN and WAN settings. Compared with stronger BumbleBee, we have 7.2\u00d7 ~ 12.5\u00d7 in the LAN setting and 2.9\u00d7 ~ 4.0\u00d7 in the WAN setting. More speedup for the LAN setting indicates we accelerate the computation more than the communication. For non-linear functions, our method reduces both the communication size and rounds, so that we obtain similar speedup for both the LAN and WAN settings. Compared with stronger baseline BumbleBee, the GELU is 3.4\u00d7 faster in the LAN setting and 3.3\u00d7 faster than the WAN setting. The Softmax is 4.0\u00d7 faster in the LAN setting and 2.9\u00d7 faster than the WAN setting."}, {"title": "5.3 Communication Analysis", "content": "Then, we compare the communication cost and the number of rounds of linear layers, Softmax, and GELU in Table 3. The data is summarized using BERTbase and sequence length 128. For different types of linear layers, our protocol only requires half the number of communication rounds."}, {"title": "6 Related Work", "content": "Privacy-preserving Neural Network Inference. Due to the rapidly growing concerns about data privacy in DNN-based applications, significant efforts have been made to design efficient cryptographic protocols for DNN models [8, 18, 32, 43, 31]. Early works focus on the convolutional neural network (CNN) models. Cryptonets [8] proposed one of the first protocols for 2PC HE-based private neural network inference. Later works [18, 34, 17] are hybrid 2PC neural network inference protocols combining HE for matrix multiplications and multi-party computation for non-linear functions.\nPrivate Transformers. Several works have investigated two-party secure inference for the Transformer model. For linear layers, Iron [14] builds upon Cheetah [17] by generalizing the original encoding of matrix-vector multiplication to matrix-matrix multiplication. Both Cheetah and Iron leave blanks in the input and output ciphertexts. BumbleBee [26] utilizes the \"automorphism\" operation to compress multiple output ciphertexts, which trades computation for communication. A recent work BOLT [29] adopts SIMD encoding to homomorphically evaluate the linear layer, which also trades computation for the compact output ciphertext. All existing works adopt the server-side inner product protocol. In contrast, this work proposes the client-side outer product protocol that eliminates the input ciphertext communication. The proposed protocol also allows a novel encoding approach that facilitates more efficient homomorphic computation and output communication. Other works [1, 5] consider 3PC inference for Transformers, which rely on different settings and cryptographic primitives from this work.\nFor the non-linear layers, some studies, such as THE-X [3] and MPCFormer [22], evaluate transformer models using cryptographic friendly replacements for non-linear layers, such as using $Softmax \\approx \\frac{(x[i]+c)^2}{\\sum (x[i]+c)^2}$ and $GELU(x) \\approx \\frac{x}{2}(1 + \\sqrt{\\frac{2}{\\pi}})(x + \\frac{4}{\\pi}))$ . However, such aggressive approximations lead to noticeable accuracy loss, even when employing knowledge distillation to mitigate the decline in accuracy. Other methods, such as look-up tables for faithful approximation [31, 13, 29], are computationally expensive to maintain model accuracy. Later works, including PUMA [5] and BumbleBee [26], utilize piecewise polynomial approximation, which does not result in an accuracy drop but is also relatively costly to compute. In contrast, this work is inspired by insights from the input distribution used in the Transformer model [9, 10, 11, 12, 40, 24]. We propose fitting the non-linear functions according to their input distribution, allowing for lower-degree polynomials and fewer polynomial pieces without sacrificing accuracy."}, {"title": "7 Conclusion", "content": "We propose a privacy-preserving, accurate, and efficient two-party inference framework Nimbus for Transformers. We present an efficient protocol of secure matrix multiplication using the COP approach, achieving significantly better computation and communication efficiencies. We use a distribution-aware polynomial approximation for non-linear layers, allowing a simpler approximation with less communication and rounds. These optimizations significantly improve the performance, advancing a step towards the practical use of secure Transformer inference."}, {"title": "Algorithm 1 Secure Matrix Multiplication Protocol of Nimbus", "content": "Parties: C is the client. S is the server owning the model.\nInput: The client holds activation share $(X)_c \\in Z_l^{k\\times m}$. The server holds activation share $(X)_s \\in Z_l^{k\\times m}$, $W\\in Z_l^{m\\times n}$, and secret key sk.\nOutput: Sharing $(Y)_c \\in Z_l^{k\\times n}$ and $(Y)_s \\in Z_l^{k\\times n}$ such that Y = WX mod $2^l$.\n\\{Setup phase\\}\n1: Server S partitions the matrix W into rows $W_{\\beta} \\in Z_l^{1\\times n}$. Then S encodes each row as a polynomial $\\hat{W_{\\beta}} = \\pi_w (W_{\\beta})$ for $\\beta \\in [m]$. After that S sends $[W_{\\beta}] = Enc(\\hat{W_{\\beta}})$ for $\\beta\\in [m]$ to the client C.\n\\{Execution phase\\}\n2: The client computes the scalar-polynomial multiplication to obtain a vector of output ciphertexts $c = [[c_0], [c_1] ...[c_{k-1}]]$, where\n$c[a] = \\sum_{\\beta \\in [m]} (x_{a,\\beta} \\otimes [W_{\\beta}])$.\nfor $a \\in [k]$. The c[a] denotes the a-th element of the vector c.\n3: To compress the the k ciphertexts vector of c into k/[N/n] ciphertexts, The client applies right shift on ciphertexts of c. For example\n$\\tilde{c}[0] = RShift(c [0* [N/n]], 0) + RShift(c [0* [N/n] + 1], k) + ....+ RShift (c [0* [N/n] + [N/n] - 1], k * ([N/n] - 1))$\nfor $\\theta \\in [k/[N/n]]$. Pad with zeros if k cannot be exactly divided by [N/n].\n4: The client C generates a random polynomial vector $r = [\\hat{r_0}, \\hat{r_0}, ...\\hat{r_{k/[N/n]-1}}]$ to mask the ciphertext. The client sends $\\tilde{c}[0] = \\hat{r}[0]$ to the server for all $\\theta$, which are then decrypted by server to obtain W(X)c - R. The client keeps r, which is R \u2208 $Z_l^{k\\times n}$.\n5: The server locally computes W(X)s and outputs $(Y)_s = W(X)_s + W(X)_c - R$. The client outputs $(Y)_c = R$."}, {"title": "Algorithm 2 Secure GELU Protocol of Nimbus", "content": "Parties: C is the client. S is the server owning the model. The polynomial $P^2(x)$ with coefficients \\{bo, b1, b2\\} from Equation 5.\nInput: The client holds activation share $(X)_c \\in Z_l^{k\\times m}$ and the server holds activation share $(X)_s \\in Z_l^{k\\times m}$\nOutput: Sharing $(Y)_c \\in Z_l^{k\\times m}$ and $(Y)_s \\in Z_l^{k\\times m}$ such that Y = GELU(X).\n1: Two parties locally compute $(A_1) = Fmul((b_0), (X)) + b_1$. Then two parties jointly compute $(A_2) = Fmul((A_1), (X)) + b_2$. The truncations are implicitly called.\n2: Jointly compute the comparisons for interval selection\n$(b_0)^B = Fless((X), T_1)$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\triangleright $b_0 = 1\\{X < T_1\\}$\n$(b_1)^B = Fless(T_2, (X))$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\triangleright $b_1 = 1\\{T_2 < X\\} $\n1\\{P\\} is 1 when the condition P is true and 0 otherwise. Two parties locally set $(z_0)^B = (b_0)^B$, $(z_1)^B = (b_0)^B xor (b_1)^B xor l$, $(z_2)^B = (b_2)^B$, where l is the party index. In this way, two parties have $z_0 = 1\\{X < T_1\\}$, $Z_1 = 1\\{T_1 < X < T_2\\}$, and $z_2 = 1\\{T_2 < X\\}$.\n3: Jointly compute the multiplexing $(Y) = (z_0)^B*0 + (z_1)^B*(A_2) + (z_2)^B*(X)$ as the output share of each party."}, {"title": "Algorithm 3 Secure Exponential Protocol of Nimbus", "content": "Parties: C is the client. S is the server owning the model. The polynomial $P^3(x)$ with coefficients \\{bo, b1, b2, b3\\} from Equation 4.\nInput: The client holds activation share $(X)_c \\in Z_l^{k\\times m}$ and the server holds activation share $(X)_s \\in Z_l^{k\\times m}$\nOutput: Sharing $(Y)_c \\in Z_l^{k\\times m}$ and $(Y)_s \\in Z_l^{k\\times m}$ such that Y = exp(X).\n1: Two parties locally compute $(A_1) = Fmul((b_0), (X)) + b_1$. Then two parties jointly compute $(A_2) = Fmul((A_1), (X))+b_2$ and $(A_3) = Fmul((A_2), (X))+b_3$. The truncations are implicitly called.\n2: Jointly compute the comparisons for interval selection\n$(Z_0)^B = Fless((X),T_{exp})$                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \\triangleright $Z_0 = 1\\{X < T_{exp}\\}$\n1\\{P\\} is 1 when the condition P is true and 0 otherwise.\n3: Jointly compute the multiplexing $(Y) = (1 - (z_0)^B)*0 + (z_0)^B*(A_3)$ as the output share of each party."}, {"title": "C.2 Fitting Algorithm for Non-linear Approximation", "content": "In this section, we present the algorithm used to search the interval breakpoint of the piecewise polynomial. We use the exponential with only one breakpoint as an example to explain. A similar algorithm can be easily generated to the GELU with two breakpoints.\nThe first step generates the breakpoint candidate set S given the initial breakpoint T. One can choose the search range and step according to the needs (Line 1). Then, for each breakpoint candidate, the input range is separated into two intervals (Lines 3-4). We fit both intervals using Equation 3. The required input distribution p(x) can be drawn from a batch of data from the training dataset. The corresponding loss is accumulated for all intervals (Lines 5-9). Then, we update the optimal piecewise approximation (lines 10-13). Finally, the optimal approximation f'(x) is returned."}, {"title": "D Complexity Analysis of Linear-Layer Protocol of Nimbus", "content": "This section analyzes the computation and communication complexities listed in Table 1. We first analyze the number of HE ciphertexts to be communicated. The SIP protocol requires $\\frac{k m}{k_w m_w} + \\frac{k n}{k_w n_w}$ for the communication of input and output, as we have explained in Section 3.1. Our COP protocol removes the overhead of sending the input $\\frac{k m}{k_w m_w}$. The scalar-polynomial product produces k output ciphertext, which we pack as k/[N/n]."}, {"title": "Algorithm 4 Searching piecewise polynomial approximation of the activation function", "content": "Input: Activation function f(x), initial value of the interval breakpoint T, input distribution p(x), and function template of f'(x).\nOutput: The approximated function f'(x);\n1: Generate breakpoint candidates set S around T.\n2: Set best_loss \u2190 \u221e and f'(x) \u2190 None.\n3: for s\u2208 S do\n4: Partition the input range into two intervals using s.\n5: Ltotal = 0.\n6: for each interval i do\n7: Fit a polynomial of given degree using Equation 3 and obtain the corresponding loss Li.\n8: Compute total loss Ltotal+ = Li.\n9: end for\n10: if Ltotal < best_loss then\n11: best_loss \u2190 Ltotal\n12: f'(x) \u2190 current approximation\n13: end if\n14: end for\nOutput: f'(x)"}, {"title": "Algorithm 5 Secure Fused Truncation and Upcast", "content": "Input: Client C and server S hold input $(x)^l$.\nOutput:"}]}