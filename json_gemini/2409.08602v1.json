{"title": "Deep learning-based shot-domain seismic deblending", "authors": ["Jing Sun", "Song Hou", "Vetle Vinje", "Gordon Poole", "Leiv-J Gelius"], "abstract": "To streamline fast-track processing of large data volumes, we have developed a deep learning approach to deblend seismic data in the shot domain based on a practical strategy for generating high-quality training data along with a list of data conditioning techniques to improve performance of the data-driven model. We make use of unblended shot gathers acquired at the end of each sail line, to which the access requires no additional time or labor costs beyond the blended acquisition. By manually blending these data we obtain training data with good control of the ground truth and fully adapted to the given survey. Furthermore, we train a deep neural network using multi-channel inputs that include adjacent blended shot gathers as additional channels. The prediction of the blending noise is added in as a related and auxiliary task with the main task of the network being the prediction of the primary-source events. Blending noise in the ground truth is scaled down during the training and validation process due to its excessively strong amplitudes. As part of the process, the to-be-deblended shot gathers are aligned by the blending noise. Implementation on field blended-by-acquisition data demonstrates that introducing the suggested data conditioning steps can considerably reduce the leakage of primary-source events in the deep part of the blended section. The complete proposed approach performs almost as well as a", "sections": [{"title": "INTRODUCTION", "content": "Within the field of seismic exploration, survey designs have been made denser and larger in an attempt to improve imaging quality. The increased amount of recorded seismic data in combination with the pressure to deliver processing results within a limited time frame represent a major challenge to the industry. Thus, there is a demand for the development of more time- efficient processing approaches. Deep learning (DL), a subfield of machine learning in artificial intelligence, allows computational networks that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction (LeCun et al., 2015). The essence of DL is to learn and make predictions from data and extract important correlations about physical processes characterizing large data sets. Ideally, machine learning represents the science that enables computers to learn without explicit programming. The main idea being that general algorithms exist that can be used to identify patterns in a wide range of data sets without the need to write specific code tailored for each problem.\nWithin the field of seismic processing, application of DL has drawn much attention. Many attempts have been made to solve the trace interpolation problem (Siahkoohi et al., 2018; Oliveira et al., 2018; Mandelli et al., 2018; Wang et al., 2018; Kaur et al., 2019; Wang et al., 2020; Larsen Greiner et al., 2020). The study of DL within seismic noise attenuation has also started to develop. Li et al. (2018) and Kaur et al. (2020) investigated the attenuation of ground-roll noise. Yu et al. (2019) investigated the potential application of a convolutional neural network (CNN) to attenuate random noise, linear noise and multiples. Slang et al. (2019) and Sun et al. (2020) demonstrated the use of a U-Net to remove seismic interference noise from marine field data. Besides"}, {"title": "BLENDED ACQUISITION AND TOPSEIS", "content": "In a typical blended acquisition, sources are fired with overlapping time differences including a small random jitter. For convenience, we define a series of notations to represent data from a blended acquisition with arbitrary number of sources and streamers. Note that a shot gather represents data associated with one source and one streamer. Let the shot gather corresponding to the kth shooting of source number i and received by the jth streamer be represented as $S_{i,j}^k$ (cf. Figure 1). Further, if we consider a fixed source i and a streamer j, two adjacently fired shot gathers to $S_{i,j}^k$ (one on each side) can be represented as $S_{i,j}^{k-1}$ and $S_{i,j}^{k+1}$ respectively. Data corresponding to the last shot point of a sail line is called a last shot gather. For a blended survey with J streamers, J last shot gathers can be obtained for each sail line and they are all unblended from subsequent shot activations.\nField data used for demonstration purposes in this investigation are taken from a source- over-streamer blended acquisition conducted across the Castberg field in the Barents Sea (Vinje and Elboth, 2019; Poole et al., 2020). The special acquisition technique is named TopSeis due to the location of the source on top of the seismic spread. The actual acquisition geometry is shown in Figure 1, where five source arrays (referred to as Top-Sources) with a nominal 3.0 s shooting rate and a shot-to-shot dithering of maximum \u00b1 200ms were located approximately 30 m above the deep streamers. The shooting rate of 3.0 s leads to a dense shot sampling which means that seismic energy from different shots will be blended, termed \u201cblended-by-acquisition\u201d. The source towed by the streamer vessel (referred to as the Front-Source) was activated at every 6th Top- Source, with a \u00b1 400ms dithering. Thus, adding additional Front-Source energy to every 6th Top- Source record. Note that prior to the deblending the direct source-to-receiver waves from the Top- Sources have been removed by a separate set of processing steps. In addition the Front-Source"}, {"title": "ARCHITECTURE OF THE EMPLOYED DNN", "content": "In this study, we applied a DNN architecture based on U-Net (Ronneberger et al., 2015). We tested different hyper-parameters and found they were not very sensitive to the DNN model accuracy compared to the training data sets. U-Net was initially proposed to solve semantic segmentation tasks, representing an encoder-decoder-structure with skip connections built in a U- shape inspired by the fully convolutional neural network of Long et al. (2015). As shown in Figure 5, the building block of the encoder consists of convolutional layers employing a typical filter size of 3\u00d73 and a ReLU activation function (dark blue arrows). In addition, introduction of max pooling layers yield a multilevel, multi-resolution feature representation. The max pooling operation is represented by red arrows where the stride is 2 and the pool size is 2\u00d72 (i.e. data are downscaled to half size in both spatial dimensions). The corresponding building block of the decoder employs transposed convolutional layers with a stride 2 represented by green arrows to up-sample low- resolution features describing large scale structures to full resolution feature maps. The skip connections between the encoding path and the decoding path employing a concatenation operation (grey arrows) ensure information fusion between high- and low-level information and can thereby improve the accuracy of the network performance. No activation function is applied at the last layer of the network.\nWe train the DNN by minimizing the mean squared error (MSE) between the ground truth and its prediction over all patches in the training data set which can be expressed as,\n$L(W, B) = argmin \\sum(T - \\hat{T})^2,                                                                                                                                                                                                                                                                          (1)$\nwhere T represents the prediction of the model, $\\hat{T}$ represents the ground truth, W represents the weights and B represents the bias. To solve this optimization problem, we employed the Adam optimization algorithm (Kingma and Ba, 2014) on batches of 4 patches."}, {"title": "A NEW STRATEGY TO GENERATE TRAINING DATA WITH HIGH QUALITY AND AVAILABILITY", "content": "DNN with supervised learning aims to establish the relation between input example data and the desired output or ground truth. The quality and availability of training data are therefore critical issues regarding the effectiveness of the training process and the accuracy of the trained predictions. Firstly, the quality of the ground truth data employed in the training process should be as ideal as possible. This is usually not a problem to obtain in image processing, but often difficult to achieve within the field of seismic processing. For example, no ideally unblended data exist that can serve as the ground truth when training on blended-by-acquisition (real field blended) data. Similarly, it is not feasible to acquire seismic data containing primaries only, for the training of demultiple through a supervised learning approach."}, {"title": "DATA CONDITIONING", "content": "To further improve the deblending quality of the DNN, we introduced a series of pre- conditioning steps that can be selectively used:\n1. Add adjacent blended shot gathers in the input as additional channels.\n2. Train the network to predict both primary-source events and the down-scaled blending noise.\n3. Align the to-be-deblended shot gathers by blending noise instead of by primary-source events."}, {"title": "Use of adjacent shot gathers as additional channels of the input", "content": "In general, images can be represented by third-order tensors, characterized by height, width, and the number of channel(s). The height and width of an image relates spatial information whereas the concept of channels assigns a multi-dimensional representation to each pixel location. As an example, digital color images are represented by three standard channels (RGB channels) reflecting the amount of those three primary colors. 2D seismic data can be regarded as grayscale images with a single channel only. Thus, a simple regression task involving 2D seismic data allows the use of 2D tensors to represent input, filter kernels, and output. In DNN-based deblending, the mapping from a blended shot gather to the predicted primary-source events can be represented as\n$NET(S_{i,j}^k) \\rightarrow P,                                                                                                                                                                                                                                                                       (5)$\nwhere NET represents the network-based approach, $S_{i,j}^k$ represents the blended shot gather whose primary-source gather is fired by the kth shooting of $S_i$ and received by the jth streamer. Moreover,"}, {"title": "FIELD DATA DEMONSTRATION", "content": "To further test the quality and performance of our DL approach in a rigorous manner, we applied it to real blended-by-acquisition data. The field data used in this section are taken from the same source-over-streamer blended acquisition as discussed before. As a benchmark method, we apply the same conventional workflow introduced earlier. All comparisons are carried out in the"}, {"title": "DISCUSSION", "content": "Generalization of the proposed strategy to generate training data\nIn this study, we have proposed to use the last shot gathers from all streamers in different sail lines as the ground truth. The later samples of these gathers are always unblended with long enough recording length independent of the configuration of the actual blended acquisition. Existing methods either generate the ground truth from the application of a conventional deblending algorithm to a small part of the blended-by-acquisition data or alternatively from the use of synthetic data. In comparison, our strategy has significant advantages regarding both in generalization as well as in saving time and labor costs. The last shot gathers are easily obtained from the survey and can be directly accessed after completion of the blended acquisition. In addition, this strategy neither relies on available conventionally acquired seismic data nor the feasibility of using a DNN model trained by another survey. By manually blending shot gathers"}, {"title": "Deblending in the shot domain", "content": "The use of a small random jitter between the firing of shots is characteristic for a blended acquisition. This makes the application of denoising-based deblending methods feasible after resorting data from shot domain to an alternative domain like common-receiver or common-offset where the blending noise appears incoherent. The alternative approach of deblending data directly in the shot domain is more challenging due to the coherent character of the blending noise closely resembling the primary-source events.\nThe results obtained using the proposed DL approach are encouraging and demonstrate the feasibility of deblending directly in the acquisition domain, at least as a fast-track result. However, training based on data fired at the end of sail lines excludes real-time processing or quality control onboard seismic vessels during acquisition. As an alternative to the use of the last shot gathers as ground truth, using unblended shot gathers acquired from a previously conducted conventional survey or conducting a short period of unblended acquisition in the early stage of a blended survey could also be considered."}, {"title": "Effectiveness of the data conditioning steps", "content": "Among the proposed data conditioning steps, the most significant improvement was obtained when a multi-channel input was employed by adding adjacent blended shot gathers to the to-be-deblended input. A notable reduction in leakage of primary-source events was observed when compared to a single-channel input. This step is also the most computationally demanding among the proposed ones. The other two conditioning steps, e.g. predicting both primary-source events and (scaled-down) blending noise, and to align the to-be-deblended shot gathers by"}, {"title": "Difference between the training and test data sets", "content": "Adjacent field blended shot gathers were employed as additional channels in the test data used for the initially trained network. However, no consecutive unblended shot gathers existed that could be used as multi-channel input data for the training process. We therefore employed adjacent streamers within the last shot gathers in a sail line as channels of a training sample. The distance between two adjacent streamers was 62.5 m. This is much larger than the distance between two adjacent blended shot gathers from the same survey being 37.5 m. Thus, dipping events will appear differently in a training sample compared to a test sample. One possible solution to this problem could be to acquire a set of consecutive unblended shot gathers in the beginning of a blended survey as already mentioned. Alternatively, a better loss function or the application of a NMO correction before the input samples were generated, may also have the potential to improve the network performance."}, {"title": "Computational efficiency of the proposed approach", "content": "It is always very challenging to extract and preserve the relatively very weak primary- source energy while effectively removing strong blending noise. In order to achieve such good deblending results as shown in Figure 13, a workflow of multiple processing steps were employed. Thus, the application of such a conventional deblending algorithm is computationally demanding and also time-consuming due to a large amount of parameter tuning. The level of deblending can vary considerably from one project to another, being dependent on both the number and position of sources, frequency content of interest, water depth among others. As a consequence, different"}, {"title": "CONCLUSION", "content": "In this study, we have investigated a DL approach to seismic deblending in the shot domain and demonstrated its feasibility. A new strategy was proposed where the training data were manually blended by the unblended shot gathers acquired at the end of each sail line. After the DNN was properly trained, it efficiently removed blending noise from blended-by-acquisition data in the shot domain. Successful implementation of this method also demonstrates how unblended shot gathers can be used to train a DNN in the acquisition domain in a generic sense. As an alternative, unblended shot gathers could also be extracted from previous surveys, or deliberately acquired as part of the blended seismic acquisition.\nThe DNN performed especially well in the shallower part of the blended section but showed more leakage of primary-source events for larger traveltimes. However, by introducing a set of data conditioning steps, this leakage was reduced considerably. Direct comparison with an advanced conventional algorithm further demonstrated that the proposed approach gave slightly weaker primary-events and loss of high frequencies for the deep part of the same blended section. However, it is important to remember that such a conventional workflow has been developed and refined over years and in the case shown here this workflow consisted of multiple sub-processing steps and was very computationally demanding. Despite the present shortcomings, this study has"}]}