{"title": "Adaptive Interactive Segmentation for Multimodal Medical Imaging via Selection Engine", "authors": ["Zhi Li", "Kai Zhao", "Yaqi Wang", "Shuai Wang"], "abstract": "In medical image analysis, achieving fast, efficient, and accurate segmentation is essential for automated diagnosis and treatment. Although recent advancements in deep learning have significantly improved segmentation accuracy, current models often face challenges in adaptability and generalization, particularly when processing multi-modal medical imaging data. These limitations stem from the substantial variations between imaging modalities and the inherent complexity of medical data. To address these challenges, we propose the Strategy-driven Interactive Segmentation Model (SISeg), built on SAM2, which enhances segmentation performance across various medical imaging modalities by integrating a selection engine. To mitigate memory bottlenecks and optimize prompt frame selection during the inference of 2D image sequences, we developed an automated system, the Adaptive Frame Selection Engine (AFSE). This system dynamically selects the optimal prompt frames without requiring extensive prior medical knowledge and enhances the interpretability of the model's inference process through an interactive feedback mechanism. We conducted extensive experiments on 10 datasets covering 7 representative medical imaging modalities, demonstrating the SISeg model's robust adaptability and generalization in multi-modal tasks. The project page and code will be available at:https://github.com/RicoLeehdu/SISeg.", "sections": [{"title": "I. INTRODUCTION", "content": "Medical image segmentation plays a critical role in clinical practice, supporting essential tasks such as diagnosis, treatment planning, and disease monitoring [1]. Accurate segmentation of tissues, organs, and lesions from medical images not only improves diagnostic precision but also enhances treatment planning [2]. However, manual segmentation is a labor-intensive, time-consuming process requiring substantial expertise. In clinical settings, the extensive manual annotation effort renders segmentation inefficient and costly [3]. To overcome these limitations, semi-automatic and fully automatic segmentation methods have emerged, aiming to improve segmentation consistency and efficiency by minimizing manual intervention, thus facilitating the analysis of large-scale medical imaging data [4].\nIn recent years, deep learning models have propelled interactive segmentation methods to the forefront, particularly in scenarios that require real-time feedback and efficient annotation [4]. By allowing users to provide minimal key cues, such as bounding boxes around lesions or positive and negative sample points, models can automatically propagate segmentation results without extensive labeling [5], [6], [8]. This is particularly beneficial in medical image segmentation, where real-time interaction can significantly boost clinician productivity, reduce labeling costs, and enhance performance in sequence data processing [9].\nWhile the Segment Anything Model 2 (SAM2) [11] has demonstrated impressive generalization and interactive segmentation capabilities in natural image tasks, it faces significant challenges when applied to medical imaging [27], [28]. Medical images often exhibit complex textures, variable contrasts, and artifacts, making it difficult for a single prompt type to generalize effectively across modalities [9]. Moreover, medical image segmentation frequently involves processing large-scale 2D or 3D sequences [7], where conventional models struggle to balance segmentation accuracy, inference efficiency, and memory consumption [10].\nTo address these challenges, we propose a strategy-driven intelligent interactive segmentation system (SISeg), designed to adaptively handle multi-modal medical image segmentation. SISeg integrates multiple prompt types and builds upon the Segment Anything Model 2 framework. At the core of SISeg is the Adaptive Frame Selection Engine (AFSE), which dynamically selects the most appropriate prompt frames based on image characteristics, without relying on prior medical knowledge. This engine not only reduces memory consumption but also enhances interpretability in the segmentation process, particularly for sequential data. By incorporating an unsupervised scoring mechanism, SISeg effectively processes diverse modalities such as dermoscopy, endoscopy, and ultrasound, achieving superior segmentation accuracy even in complex scenarios. This work presents three main contributions:\nA novel, flexible multi-prompt segmentation framework (SISeg) that efficiently handles diverse medical imaging modalities without requiring extensive domain-specific knowledge.\nThe introduction of the Adaptive Frame Selection Engine (AFSE), which dynamically optimizes prompt frame selection, significantly improving inference efficiency while reducing memory usage.\nExtensive experiments on 10 datasets across 7 distinct medical imaging modalities, validating the superior generalization capability and reduced annotation burden of the SISeg framework."}, {"title": "II. RELATED WORK", "content": "Medical foundation models are large-scale, pre-trained models designed for rapid customization through fine-tuning or in-context learning [10], [12]\u2013[14]. Despite substantial progress, challenges remain in tasks such as image segmentation, largely due to the scarcity of annotated masks [9]. These limitations hinder SAM's performance, particularly in cross-modal segmentation tasks, where the impact of different prompt types has not been thoroughly investigated [12]. Approaches like MedSAM [9] and Medical SAM Adapter [15] have attempted to enhance cross-modal segmentation by fine-tuning SAM [5] using bounding box prompts on large medical datasets. However, these methods remain constrained by their focus on a limited set of prompt types across modalities. In response to these challenges, we propose a segmentation strategy that integrates multiple prompt types and evaluate its effectiveness across seven representative medical imaging modalities."}, {"title": "B. Medical Image Segmentation", "content": "Medical image segmentation is fundamental in modalities such as CT, MRI, and ultrasound [17]\u2013[19]. Models like U-Net [20] and its variants [21]-[23] have demonstrated exceptional performance in this domain. However, most models lack cross-modal robustness, struggle with large-scale datasets, and require extensive manual tuning, failing to optimize prompt selection or interaction efficiency [24], [25]. To address these challenges, we introduce an automated prompt selection framework that intelligently selects prompt frames without relying on domain-specific medical knowledge, significantly reducing memory usage and enhancing inference efficiency, especially for sequence data."}, {"title": "III. METHODOLOGY", "content": "Preliminaries. As shown in Fig. 1, SAM2 [11] integrates an image encoder, memory encoder, and memory attention mechanism to enhance segmentation by leveraging both current and historical frame information. The image encoder abstracts the input into an embedded representation, while the memory encoder processes representations from previous frames. The memory attention mechanism integrates historical information to refine segmentation of the current frame. This architecture employs a hierarchical visual Transformer as the encoder and a lightweight bidirectional Transformer as the decoder, combining cue and image embeddings.\nIn this work, we introduce two key modules to optimize the interactive segmentation process: an unsupervised scoring mechanism (Scorer) and a Selector, which aids in selecting representative frames, as shown in Fig. 1."}, {"title": "B. Exploring Robust Prompts", "content": "Prompt-based segmentation, particularly with pre-trained models, has become a standard approach for reducing labeling costs in related tasks. SAM2 extends prompt-based segmentation to video contexts and has demonstrated strong performance in 3D medical imaging, such as CT and MRI, by treating volumetric data as video streams. However, its potential in 2D medical image segmentation remains underexplored. We found that SAM2 can effectively apply both One-Prompt and Multi-Prompt segmentation across sets of 2D medical images, treating them as video sequences. In this approach, the model requires only a few well-chosen image prompts to achieve segmentation across the entire image set a task that proves challenging for other methods. Therefore, we investigated the effectiveness of various prompt types across different medical imaging modalities. The specific prompt types and examples are illustrated in Fig. 1."}, {"title": "C. Adaptive Frame Selection Engine", "content": "Single-point segmentation [26] allows users to provide a single hint to the model for an unseen example. This approach capitalizes on the model's generalization ability without requiring retraining or fine-tuning. While this technique is effective for tasks such as optic disc and cup segmentation in fundus images, it may not generalize well across all medical imaging modalities due to the reliance on easily recognizable prior knowledge in some tasks.\nScoring Formula. To address this limitation, we propose an unsupervised scoring mechanism that evaluates the dataset based on image features, aiding in the selection of representative frames for annotation. Let $B$ represent the brightness score, $C$ the contrast score, $E$ the edge density, $H$ the color histogram similarity, and $S$ the shape similarity. These variables are combined to form a composite score $F$, which is calculated for each image relative to a reference frame in the dataset. The overall composite score is defined as:\n$F = \\alpha \\cdot B + \\beta \\cdot C + \\gamma \\cdot E + \\delta \\cdot H + \\epsilon \\cdot S$ (1)\nWhere:\n*$\\alpha$, $\\beta$, $\\gamma$, $\\delta$, and $\\epsilon$ are weights assigned to each feature score.\n*$B$ (brightness) is computed as the mean brightness of the grayscale image, normalized to the range [0, 1]:\n$B = \\frac{\\text{mean}(I_{\\text{gray}})}{255}$ (2)\n*$C$ (contrast) is defined as the standard deviation of the grayscale pixel intensities:\n$C = \\frac{\\text{std}(I_{\\text{gray}})}{255}$ (3)\n*$E$ (edge density) represents the proportion of edge pixels, computed using the Canny edge detector:\n$E = \\frac{\\text{mean}(I_{\\text{edges}})}{255}$ (4)\n*$H$ (color histogram similarity) measures the correlation between the HSV histograms of the current image and the reference frame:\n$H = \\text{corr} \\left( \\text{hist}_{\\text{HSV}}(I), \\text{hist}_{\\text{HSV}}(I_{\\text{ref}}) \\right)$ (5)"}, {"title": "Selector Module", "content": "The composite score $F$ is calculated for each image, and K-means clustering is applied to group frames into clusters based on their similarity to the reference frame. The interaction engine incorporates a selector module to enable efficient dataset navigation. The Selector initially identifies a reference frame $I_{\\text{ref}}$, chosen by the clinician for its clinical relevance. Each frame in the dataset is then assigned a composite score $F$, computed based on its similarity to the reference frame. This composite score integrates various features, such as brightness, contrast, edge density, color histogram similarity, and shape similarity, as defined in Equation (1). The process is illustrated in Fig. 1.\nTo group similar frames, we apply the KMeans clustering algorithm. Given a set of $N$ frames, let $X = \\{F_1, F_2, ..., F_N\\}$ represent the composite scores of these frames. The KMeans algorithm minimizes the following objective function:\n$\\min_{C_1, C_2, ..., C_k} \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2$ (7)\nwhere $C_i$ is the i-th cluster, and $\\mu_i$ is the centroid of $C_i$. The algorithm iteratively minimizes the sum of squared distances between each frame's composite score and its cluster centroid. After clustering, frames closest to the centroids are selected as representative frames for segmentation. The Scorer module then ranks the remaining frames by their proximity to these centroids, providing feedback to the user on the most relevant frames for further annotation."}, {"title": "IV. EXPERIMENTS", "content": "In our experiments, we utilized seven distinct medical imaging modalities: Dermoscopy (Der), Endoscopy (Endo), Fundus, Optical Coherence Tomography (OCT), Ultrasound (US), X-ray (XRay), and Mammography (MG). Ten publicly available datasets were employed, including: PAPILA [29], Breast Ultrasound [30], Kvasir-SEG [31], IDRiD [32], ISIC 2018 [33], Intraretinal Cystoid Fluid [34], CDD-CESM [35], m2caiSeg [38], Chest X-ray Masks and Labels [36], and hc18 [37]. Due to inference cost constraints, the datasets were split into training and validation sets using a 7:3 ratio with scikit-learn [39], applying a fixed seed of 2024 to ensure consistency. The validation set comprises 30% of the original data."}, {"title": "Effects of AFSE", "content": "The Adaptive Frame Selection Engine (AFSE) consistently outperformed both random and uniform strategies across multiple modalities, as shown in Table II. AFSE's scoring mechanism ensures that the selected frames are more representative and clinically relevant, leading to improved segmentation accuracy and reduced annotation effort. Notably, AFSE surpasses the second-best method, AFSE (without scorer), by 9.39% in X-ray and 10.97% in Mammography, underscoring the significance of the scoring mechanism. Additionally, AFSE demonstrates a 6.33% improvement in Endoscopy and a 10.66% improvement in OCT."}, {"title": "C. Ablation Studies", "content": "In Table III, we compare the performance of various combinations of positive and negative point prompts with bounding box prompts across different medical imaging modalities. The results show that bounding box prompts, which enclose the target organ or lesion, consistently deliver the best segmentation performance by effectively leveraging SAM2's propagation mechanism. In contrast, the performance of point-based prompts is highly dependent on the correct combination of positive and negative points. As demonstrated in Table IV, AFSE effectively selects relevant frames for segmentation, significantly improving both efficiency and accuracy. By employing an unsupervised scoring mechanism that evaluates image features such as brightness, contrast, edge density, color histogram similarity, and shape similarity, AFSE automatically selects and ranks frames. This automated approach reduces the need for manual intervention, streamlining the segmentation process, while also maintaining robust generalization across diverse imaging modalities."}, {"title": "V. CONCLUSION", "content": "In this work, we introduce the Strategy-driven Interactive Segmentation Model (SISeg), which enhances medical image segmentation across multiple modalities by integrating diverse prompt types. Using the Adaptive Frame Selection Engine (AFSE), SISeg dynamically selects optimal prompts without requiring prior medical knowledge, reducing memory usage and improving interpretability. Experiments on 10 datasets across 7 modalities show SISeg's ability to boost segmentation efficiency and lower annotation costs."}, {"title": "VI. ACKNOWLEDGMENTS", "content": "This research is supported by the National Natural Science Foundation of China (No. 62201323, No. 62206242), and the Natural Science Foundation of Jiangsu Province (No. BK20220266)."}]}