{"title": "The Russian-focused embedders' exploration: ruMTEB benchmark and Russian embedding model design", "authors": ["Artem Snegirev", "Maria Tikhonova", "Anna Maksimova", "Alena Fenogenova", "Alexander Abramov"], "abstract": "Embedding models play a crucial role in Natural Language Processing (NLP) by creating text embeddings used in various tasks such as information retrieval and assessing semantic text similarity. This paper focuses on research related to embedding models in the Russian language. It introduces a new Russian-focused embedding model called ru-en-RoSBERTa and the ruMTEB benchmark, the Russian version extending the Massive Text Embedding Benchmark (MTEB). Our benchmark includes seven categories of tasks, such as semantic textual similarity, text classification, reranking, and retrieval. The research also assesses a representative set of Russian and multilingual models on the proposed benchmark. The findings indicate that the new model achieves results that are on par with state-of-the-art models in Russian. We release the model ru-en-RoSBERTa, and the ruMTEB framework comes with open-source code, integration into the original framework and a public leaderboard.", "sections": [{"title": "1 Introduction", "content": "Text embeddings play an important role in many Natural Language Processing (NLP) tasks, from clustering to semantic textual similarity (STS) and information retrieval (IR). The community has addressed this demand by releasing several powerful text embedding models (or embedders) (Wang et al., 2024, 2023a; Chen et al., 2024). However, there is still a lack of such embedders developed specifically for the Russian language. The most popular Russian-oriented models, such as rubert-tiny2 2, SBERTlarge-nlu-ru\u00b3, and SBERTlarge-mt-nlu-ru4,"}, {"title": "2 Related Work", "content": "2.1 Text Embedding Models\nGeneral text embedding models are widely used in various applications such as retrieval-augmented generation (RAG) (Lewis et al., 2020), STS, as well as multimodal scenarios (Radford et al., 2021). One of the first approaches for training such models was to fine-tune a pre-trained language model on the collection of labeled text pairs, such as SNLI (Bowman et al., 2015). Natural Language Inference (NLI) has been shown (Reimers and Gurevych, 2019) to help such models learn useful representations of texts for STS and other downstream applications. Recent approaches for model training utilize labeled datasets, which can be divided into symmetric (NLI, STS) and asymmetric (Retrieval) tasks. Hence, the training objective takes the form of multitask learning over one or multiple objectives, and the specialized instructions are applied for each task (Su et al., 2022).\nInstead of training on limited labeled datasets, in (Wang et al., 2022a), it has been proposed to split fine-tuning into two stages: contrastive pre-training uses a large-scale pair dataset of noisy (or weakly-supervised) text examples, and contrastive fine-tuning utilize a smaller number of high-quality examples. The authors of the E5mistral-7b-instruct (Wang et al., 2023a) utilize an approach for model training which does not include expensive contrastive pre-training that has been shown to be useful for smaller encoder-only model XLM-R (Conneau et al., 2019). While their quality remains comparable, encoder-only models are more cost-effective during inference.\nExamples of modern English-focused models include E5 (Wang et al., 2022a), BGE (Xiao et al., 2023a), GTE (Li et al., 2023), Nomic (Nussbaum et al., 2024) and Arctic Embed (Merrick et al., 2024). Scaling the number of languages supported (including Russian) has been demonstrated in mE5 (Wang et al., 2024) models and BGE-M3 (Chen et al., 2024), thereby extending their applicability in multilingual contexts. The Russian-oriented models are mainly represented by SBERT models and rubert-tiny2 and their modifications.\nModels mentioned above are often used for additional fine-tuning on a specific task. To preserve the general ability of the embedding model, it has been proposed (Xiao et al., 2023b) to merge the fine-tuned model with its base model.\nTo address this lack of contemporary Russian-focused embedding models performing on par with their multilingual counterparts, we present ru-en-ROSBERTa."}, {"title": "2.2 Text Embedding Benchmarks", "content": "Model evaluation has always played an inevitable role in NLP progress. Starting from GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019) benchmarks have been a standard model evaluation method. As for text embedding representation evaluation, it has been in focus for many years and was commonly evaluated on a STS, for which yearly released SemEval (Agirre et al., 2016; Cer et al., 2017; Chen et al., 2022) datasets were commonly used. Being a single dataset inevitably limits the SemEval expressivity. Following the same approach, SentEval (Conneau and Kiela, 2018), which focuses on classifier models on top of embedding, overcomes this limitation by aggregating multiple STS datasets. Still, it lacks the evaluation instruments for the suitability of embedding for retrieval or clustering tasks. Due to the inefficiency of a"}, {"title": "3 ruMTEB Embedding Benchmark", "content": "3.1 Benchmark Structure and Evaluation Methodology\nThe ruMTEB benchmark unites 23 datasets, which can be divided into 7 task categories similar to the corresponding categories in the original \u041c\u0422\u0415\u0412 benchmark: Classification (9 datasets), Clustering (3 datasets), MultiLabel Classification (2 tasks), Pair Classification (1 task), Reranking (2 tasks), Retrieval (3 tasks), and STS (3 tasks). Below each task category, the evaluation process is briefly described, and the dataset information can be found in Subsection 3.2.\nClassification. The evaluation is performed in 10 consecutive experiments (bootstrap evaluation). For run, a bootstrap subset of n (by default, n = 8) training samples is sampled, and this down-sampled train and test parts are embedded using the embedding model. The training subset is used to train the logistic regression classifier (with 100 interactions maximum). Then, test predictions are scored using the standard Accuracy score.\nPair Classification. This group includes datasets where, given a pair of text labels, one has to predict a binary label. For evaluation, the two texts in each pair are embedded via the embedding model, and the cosine similarity between their embeddings is computed. Then, using the best binary threshold, average precision is computed.\nMulti-label Classification. For evaluation, train and test sets are embedded. Then bootstrap evaluation with 10 runs is performed. In each run the training sets are down-sampled to 8 instances of each unique label. The train embeddings are used to train the KNN classifier (n_neighbours = 5). The result is evaluated on the test part using the standard Accuracy score.\nClustering. This task type includes datasets where, given a set of text fragments, one has to group them into meaningful clusters. For evaluation, text fragments are embedded. Then bootstrap evaluation with 10 runs is performed. For each run, a subset of embedding are samples, which are then clustered using K-means clustering. The result is evaluated via v-measure (Rosenberg and Hirschberg, 2007) and averaged over all experiments.\nSemantic Textual Similarity (STS). Given a pair of sentences, the goal is to determine their textual similarity. Labels are continuous scores ranging from 0 to 1 (the closer to 1, the more similar). For evaluation, cosine similarity over the embedded sentences for each pair is computed. The result is evaluated with Spearman correlation (Reimers et al., 2016).\nReranking. Inputs are a query and a list of reference texts (relevant and irrelevant). The goal is to correctly rank these texts according to their relevance to the query. For evaluation, the texts for each query are ranked by the cosine similarity between the query embedding and the embedding of the given texts. The obtained ranking is scored with MAP@k (k = 10) for each query and averaged over all queries.\nRetrieval. For this task type, each dataset includes a set of documents and queries and a mapping for each query to relevant documents. The task aims to find relevant documents for each task. For evalu-"}, {"title": "4 Text Embedding Model for Russian", "content": "This section is devoted to the text embedding model ru-en-RoSBERTa released within the research. We describe the training data, the base model, and the final training pipeline, motivated by the experiments described in Section 5.\n4.1 Training Data\nFollowing previous work (Wang et al., 2022a; Li et al., 2023; Nussbaum et al., 2024), we use publicly available data, high-quality and synthetic datasets to create training pairs (see Appendix A.1.1 for the full training list) 10, which, for experiment purpose (see Section 5), we divide into four groups described below.\nBasic Russian Datasets. This group consists of 17 tasks. It includes pairs from SberQuAD (Efimov et al., 2020), XNLI (Conneau et al., 2018), parallel translations (Ba\u00f1\u00f3n et al., 2020; Tiedemann, 2012; Zhang et al., 2020), and publicly available data from various domains, such as news, blogs, QA platforms, and other Internet resources. We filter this data mostly with manual rules (see Appendix A.1.2 for the details).\nBasic English Datasets. The group is formed from MEDI (Su et al., 2022) corpus without provided instructions. We also exclude instructional datasets from Super-NI (Wang et al., 2022b) and thus retain 30 datasets representing different domains and tasks. We do not apply any additional preprocessing steps.\nAdditional Synthetic Datasets. The group includes Query2doc MS-MARCO (Wang et al., 2023b), DINO-STS-x1x2 (Schick and Sch\u00fctze, 2021), RuHNP (Malashenko et al., 2024a), entailment and contradiction pairs from RuWANLI (Malashenko et al., 2024b), and a sample of generated pairs by ruT5-base\u00b9\u00b9 model from WikiOmnia (Pisarevskaya and Shavrina, 2022). We do not change the data content and use the datasets as is.\nAdditional Retrieval Datasets. We use Russian and English parts of Mr. Tydi (Zhang et al., 2021) and MIRACL (Zhang et al., 2022) from BGE-M3 fine-tuning data. These datasets provide high-quality examples and are designed for the same retrieval tasks included in our benchmark.\nWe mine negatives similar to (Xiao et al., 2023a) using the mE5small (Wang et al., 2024) and sample documents by rank in the range of 20-100. For all datasets, the provided hard negatives are also used. For additional synthetic and retrieval datasets, the provided negatives are used (if available), and the rest are randomly sampled from the same dataset."}, {"title": "4.2 Base Model and English Language Adaptation", "content": "Since we focus on the Russian language, we use ruRoBERTa 12 (Zmitrovich et al., 2023), which has the highest scores on the classic Russian Super-GLUE (Shavrina et al., 2020) benchmark among the models of its size. In addition, we adapt it to the English language, allowing knowledge transfer from this high-resource language (see Section 5 for the corresponding experiments).\nWe extend the original ruRoBERTa tokenizer with tokens from RoBERTa 13 (Liu et al., 2019). To learn new token embeddings, we train the model using Masked Language Modeling (MLM) objective (Devlin et al., 2018). We use the same hyperpa-"}, {"title": "4.3 Contrastive Fine-tuning", "content": "Following (Su et al., 2022), we perform contrastive fine-tuning for ru-en-RoBERTa on a mix of super-"}, {"title": "5 Training Procedure Analysis", "content": "This section describes experiments we conducted to determine the final training pipeline. We used basic Russian, English, and additional synthetic datasets, the training approach described in Section 4 unless otherwise specified, and the full ruMTEB version for evaluation. Further findings are given in Appendix A.6.\n5.1 Cross-lingual Knowledge Transfer and Data Sources\nWe explored five training data configurations to study whether the model can profit from knowledge transfer between languages and various data sources. For this, we trained embedding models based on ru-en-RoBERTa: on basic English datasets only, basic Russian datasets only, and their mixture, simple or augmented with additional synthetic/synthetic+retrieval datasets. Each model is trained for 1500 steps.\nResults presented in Table 2 indicate that the embedding model gets better results when trained on data in Russian and English simultaneously. Additional synthetic datasets and high-quality retrieval datasets further improve the embedding model quality despite the tasks these datasets solve already being well represented in the basic datasets. Given that in all scenarios, the number of steps is fixed, the change in the results could not account for longer training.\nThe model especially benefits from synthetic datasets on STS-related tasks, while quality degradation in clustering tasks remains unclear. Note that the model trained on almost all data (except the additional retrieval dataset is better by only 0.6 points. The results obtained on data in English may be due to the better quality of the tasks presented in MEDI."}, {"title": "5.2 English Language Adaptation", "content": "Having shown that the model can profit from cross-lingual knowledge transfer, we turned to selecting the optimal language adaptation strategy. Namely, we compared:\n\u2022 ruRoBERTa and XLM-R used as baselines;\n\u2022 ru-en-RoBERTa from subsection Section 4.2;\n\u2022 ru-en-RoBERTa w/ RetroMAE same approach as previous where we substituted MLM with RetroMAE (Shitao et al., 2022), which proved"}, {"title": "5.3 Training Examples", "content": "In this series of experiments (see Table 3), we show the effects of prefixes, stratified sampling, and the number of hard negatives.\nRemove prefixes. Fine-tuning the model on symmetric and asymmetric tasks simultaneously can hurt performance without instructions but improve it when instructions are used (Su et al., 2022). We found that removing prefixes consistently worsens the results, but STS-related tasks were not as affected.\nDisable stratified sampling. To explore whether stratified sampling is beneficial (Merrick et al., 2024) in our case, we disabled it, used prefixes only for queries, and negatives were exchanged across devices. The latter increases the number of negatives per query to 8k. We found that the stratified version works better.\nHard-negatives. To study whether adding more hard-negatives (Ren et al., 2021) is beneficial, we increased their number to 15 and reduced per device batch size to 64, maintaining the total number of negative examples. To keep the same number of steps, we apply gradient accumulation. Similarly to (Nussbaum et al., 2024), we found that despite processing almost twice as many texts, the results did not improve."}, {"title": "5.4 Training Objective", "content": "In this experiment (see Table 3), we examine four modifications of the training objective described in"}, {"title": "6 Evaluation", "content": "We evaluate ru-en-RoSBERTa and 9 publicly available embedding models for Russian, including the multilingual ones and the two instruct models, on the ruMTEB benchmark. See Table 4 for the baseline information and Appendix A.4 for other details.\nWe evaluate all models in the same environments and scenarios by the procedure described in 3.1. We use MTEB framework15 for evaluation where we integrated evaluation on the new ruMTEB tasks 1617."}, {"title": "7 Results", "content": "Table 5 shows model scores averaged within the task category, and detailed results of the task-wise model evaluation are in Appendix A.5.\nResults analysis reveals that there is a gap between instruct and non-instruct models, mE5large-instruct and E5mistral-7b-instruct are better than"}, {"title": "8 Conclusion", "content": "This paper introduces a new Russian-focused embedding model, which also supports English, and a new benchmark for text embedding evaluation, comprising 23 datasets divided into 7 task types. Among the benchmark datasets, 17 datasets are new and were created within this research.\nWe report the new embedding model architecture design, pre-training corpus, and training procedure details. We describe the datasets comprising the benchmark and propose the methodology for the text embedding evaluation on it inspired by the MTEB benchmark. We evaluate the presented encoding model and several baselines, thus verifying the ruMTEB complexity and performing the comparative analysis of our model results with the results of standard encoders."}, {"title": "9 Limitation", "content": "Model limitations. The training data for ru-en-ROSBERTa includes large segments from the Internet domain. Consequently, it contains various stereotypes and biases from English and Russian sources. Therefore, a proper model evaluation is still needed to explore their possible vulnerabili-"}, {"title": "10 Ethical Considerations", "content": "Inference Costs. Evaluating embedding models on ruMTEB depends on its architecture and size and can be optimized with distributed inference libraries. For example, one run of ru-en-RoSBERTa of the complete evaluation experiment on a single A100 GPU 80GB takes approximately 19 hours.\nEnergy Efficiency and Usage. We compute the CO2 emissions from pre-training and fine-tuning ru-en-ROSBERTa as Equation 1 (Strubell et al., 2019):\n$CO_2 = \\frac{PUE * kWh * ICO2}{1000}$ (1)\nThe power usage effectiveness (PUE) of our data centers is 1.3. The resulting CO2 emission is 3.66k kg. Model compression techniques can reduce the computational costs associated with model inference.\nPotential Misuse. The ruMTEB can be used as training data for acceptability classifiers, potentially improving the quality of generated texts. We acknowledge that these improvements in text generation might lead to the misuse of LLMs for harmful purposes. The intended use of ruMTEB is for research and development purposes, and we are aware of the potential negative uses.\nAI-assistants Help. We improve and proofread the text of this paper using Grammarly18 to correct grammatical, spelling, and style errors and paraphrasing sentences. Thus, some segments of our publication can be potentially detected as AI-generated, AI-edited, or human-AI-generated."}, {"title": "A Appendix", "content": "A.1 Training Data Details\nA.1.1 Training Data Information\nThe list of datasets included in ru-en-ROSBERTa training data and the corresponding prefix used for them are given in Table 6.\nWe use the following basic rules to choose a prefix:\n\u2022 search_query and search_document prefixes are for answer or relevant paragraph retrieval\n\u2022 clustering prefix is for asymmetric retrieval of title or summary and relevant document\n\u2022 classification prefix is for symmetric paraphrasing related tasks (STS, NLI, bitext mining)\nA.1.2 Data Filtration Details\nWe apply the following steps to the basic Russian datasets. First, texts longer than 500 tokens (ruRoBERTa-large19 tokenizer is used) are filtered out. A small number of tokens is reserved for instructions or prefixes. Pairs from YandexQ20, Pikabu21, StackOverflow22, Habr23 and Habr QnA24 are filtered by content popularity (e.g. views, ratings, votes). Cosine similarity obtained from LaBSE (Feng et al., 2022) is applied to filter NewsCommentary and MultiParaCrawl. We filter pairs from paraphrase-NMT-Leipzig25 by p_good score (equivalent meaning). The XNLI is formed from entailment (relevant document) and contradiction (irrelevant negative) examples. For MIRACL, we use the title as the query and the first paragraphs (until we reach the token limit) as the document. We form pairs for Paraphrases 26 from paraphrases field, taking one as a query and the others as positive documents. The content of RuNews27 is not changed. After exact match deduplication, the final training pairs for all datasets are randomly sampled from the remaining pairs.\nA.2 Model Training Details\nWe train the model in bf16 dtype with gradient checkpointing and use AdamW (Loshchilov and Hutter, 2017) with a learning rate of le-5 and weight decay of 0.01 for exactly one epoch, which is approximately 3700 steps, of which linear warmup is 200 steps. After fine-tuning, the SLERP merging is applied to the base model with a factor of 0.1.\nWe apply stratified sampling per device batch (mini-batch). Therefore, the global batch includes mini-batches consisting of pairs of different datasets. On the one hand, it becomes impossible to exchange negatives between devices and thus scale the number of in-batch negatives. On the other hand, this increases the diversity of sources in the global batch. Therefore, we do not apply the DisCo (Chen et al., 2023) trick to exchange negative examples across devices. The batch size is 128 per device, giving 1024 documents per query. Context length is set to 512 for queries and documents (Merrick et al., 2024).\nTraining is conducted on a single H100 node. We utilize codebase from BGE28 and adapt it for our experiments. PyTorch's expandable_segments helps us to mitigate fragmentation issues due to variable sequence length."}, {"title": "A.3 ruMTEB Dataset Description", "content": "This section describes new tasks we present with the research and data preparation details.\nA.3.1 Classification\nKinopoiskSentimentClassification. In a sentiment classification dataset given a film review, one has to predict whether it is Positive, Neutral, or Negative (3 classes in total). The data was taken from the original dataset (Blinov et al., 2013)29, which contains reviews from July 2004 to November 2012. In the preprocessing phase, we removed all mentions of the final rating from the review texts and balanced the set, leaving only 4,500 samples of each class. The resulting dataset was split into three parts (train, validation, and test), with the class balance preserved.\nGeoReview Classification A classification dataset, where given a review text one has to predict its rating ranging from 1 to 5 (five classes in total). The set is based on the Yandex Maps30 reviews31. The original dataset was balanced and split into three parts (train, validation, and test).\nHeadline Classification. In this dataset, the model needs to determine which news category the article title belongs to. The dataset was built based on ParaPhraserPlus (Gudkov et al., 2020) and contained 10,000 examples for each category, divided into train/validation/test splits of 6000, 2000, and 2000, respectively. A total of 6 classes are used: sports, incidents, politics, science, culture and economics. First, categories that contained at least 10,000 examples were selected. Other categories were discarded due to overlap between categories. For this purpose, we trained a classifier over SBERTlarge-ru embeddings.\nRuReviewsClassification A sentiment classification dataset where top-ranked goods from a major e-commerce site were provided, and user-ranked scores were used as class labels on a 5-point scale. The data was sourced from the original dataset RuReviews32, which contains reviews in the \"Women's Clothes and Accessories\" category. During the preprocessing stage, duplicates were removed, and the dataset was balanced, resulting in only 25,000 samples for each class. The resulting dataset was divided into three parts (train, validation, and test) while maintaining class balance.\nRuSciBenchGRTNI/OECDClassification. This is a dataset for the classification of scientific text headings. Each article has its OECD and GRNTI headings, with 29 OECD headings and 28 GRNTI headings in the dataset (e.g., Mathematics, Biological Sciences, Economics and Business, etc.). The data was sourced from the original dataset RuSciBench33. During preprocessing, duplicates were removed, the title and abstract were combined, and the set was balanced, leaving only the same number of samples for each class. The resulting dataset was then divided into test and training parts.\nInappropriatnessClassification. The dataset aims to predict whether the message is inappropriate or not in the form of binary classification. The data is based on the Inappropriate Messages dataset (version 3)34 (Babakov et al., 2021). We binarized the inappropriateness scores using the 0.5 threshold. The resulting dataset was balanced and split into three parts (train, validation, and test), with the class balance preserved."}, {"title": "A.3.2 Pair Classification", "content": "TERRa. The dataset was presented as one of the Russian SuperGlue tasks (Shavrina et al., 2020) and related to the Textual Entailment Recognition task. Given two texts, the task is to determine whether the meaning of one text entailed from the another text. Since the test split is hidden, we took the dev split without changes. A total of 307 examples are available."}, {"title": "A.3.3 Multi-Label Classification", "content": "CEDRClassification. The dataset is a task of classifying comments into five emotions (joy, sadness, surprise, fear, and anger). A total of 9,410 comments were presented from the following sources: social networks, news, and blogs. The dataset was used as is, without any modifications (Sboev et al., 2021). We took the original test split, which includes 1882 examples.\nSensitive TopicsClassification. The dataset contains sentences that can be classified into one or more sensitive topics35 (Babakov et al., 2021). The original dataset includes 18 classes, all classes are used. Since part of the dataset is not only manually labeled, we first formed a test split from manually labeled examples, and the remaining examples were combined with semi-automatically labeled examples. We have selected the most reliable examples based on the confidence scores indicated in the examples. The final test split consists of 2048 examples and preserves the original class distribution."}, {"title": "A.3.4 Clustering", "content": "GeoReview Clustering A clustering dataset based on the Yandex Maps36 reviews37, where given a review text one has to cluster the samples according to their rubrics or review categories (e.g., Bank, Supermarket, Pharmacy, etc.). The original dataset was balanced and split into three parts (train, validation, and test). For each review, we took its first rubric as the main label, leaving only samples corresponding to the top 100 most popular labels. This threshold limited the categories exceeding 10,000 examples. The final dataset was converted into the MTEB format.\nRuSciBenchGRTNI/OECDClustering. This is a dataset for the clustering of scientific text headings. Each article has its OECD and GRNTI headings, and there are 29 OECD headings and 28 GRNTI headings in the dataset (e.g., Mathematics, Biological Sciences, Economics and Business, etc.). The data was sourced from the original dataset RuSciBench 38. During preprocessing, duplicates were removed, the title and abstract were combined, and the set was balanced, leaving only the same number of samples for each class. The resulting dataset was then divided into test and training parts."}, {"title": "A.3.5 Semantic Textual Similarity (STS)", "content": "RuSTSBenchmarkSTS. The dataset used for the STS task is derived from the original multilingual STS Benchmark 39. This multilingual set comprises various translations of the original English version of the STSbenchmark dataset, with the translations completed using deepl.com 40. The Russian segment of the dataset was extracted and refined using the RuCoLa (Mikhailov et al., 2022) classifier 41. In all parts of the sets (train/dev/test), instances categorized as not linguistically acceptable were excluded. Additionally, any duplicate entries were eliminated."}, {"title": "A.3.6 Reranking", "content": "RuBQReranking. The dataset is based on RuBQ version 2.0 (Rybin et al., 2021). The dataset contains examples of questions and paragraphs from Wikipedia. Paragraphs that answer the question are considered relevant. Paragraphs that contain the answer are used as positive documents. Negative documents are paragraphs relevant to the question's topic but do not contain an answer. We only used questions from the test split with at least nine negative documents. The final test split contains 1551 examples."}, {"title": "A.3.7 Retrieval", "content": "RuBQRetrieval. Unique paragraphs from the dataset are used for the document bank, resulting in 56,826 documents. Documents were deduplicated while links to relevant documents were maintained. The original test split was taken without changes and has 2845 examples.\nRiaNewsRetrieval. The original dataset RussiaSegodnya42 (also known as RiaNews) consists of news articles and their headlines (Gavrilov et al., 2019). Texts are presented in lowercase format, and the capitalization of individual characters has not been changed. Since the article texts are available in HTML, we used the BeautifulSoup43 library to clean them of markup. Additionally, texts were normalized, and extra spaces were removed. We also removed, if possible, the first sentence in each article text since it does not relate to the article's content and is a kind of meta information (\u201cMoscow, 1 Dec ria news.\u201d). We filtered out the texts of articles with more than 2000 characters so that models limited to a context of 512 tokens could handle the entire text. All examples were deduplicated based on the headline and text of the article. Our final dataset consists of 10,000 randomly sampled headlines as queries, and article texts (724344) are used as documents."}, {"title": "A.4 Experimental Setup Details", "content": "This section describes the prompt and embedding configuration we used in our experiments. Namely, we use normalized embeddings for evaluation on all ruMTEB tasks. We use pooling and instruction strategies required by the corresponding model we evaluate. Table 7 presents all the prefixes and instructions used. Specifically:\n\u2022 we do not utilize any special prompts for rubert-tiny2, BGE-M3, SBERTlarge-ru, and"}, {"title": "A.6 Additional Experimental Findings", "content": "In this part, we describe early-stage experiments that were conducted on different data subsets and different base models.\nPrefixes. We found that E5 prefixes (Wang et al., 2022a) performed slightly worse and assume that the variant we use helps to better separate tasks during training. The clustering prefix is more suitable for tasks where thematic identification is required, so in many classification problems, we use it instead of classification, despite the name. We tried adding prefixes with some probability; this improved the results without using prefixes and also worsened the results with them. In addition to stratified sampling, we implemented a sampling strategy that takes pairs with the same prefix but saw no improvement.\nLosses. It was shown that Sigmoid Loss (Zhai et al., 2023) performed better at smaller batch sizes. We found that SigLIP is more sensitive to selecting the initial values of the bias and temperature parameters to achieve convergence. CoSENT loss (Li and Li, 2023) shows better results for STS-like tasks; we adapted the loss for the case with many negatives. In both cases, we were unable to achieve comparable results and left this for further work.\nAugmentations. Although the model trained on 1500 steps shows comparable results to full training, we tried to apply text level and embedding level augmentations but found no meaningful performance improvement. For the text level, we used character-level augmentation from the Augmentex44 library (Martynov et al., 2024) for both languages. In another experiment, we applied the NEFTune (Jain et al., 2023) with 3, 5, and 10 alpha parameters."}]}