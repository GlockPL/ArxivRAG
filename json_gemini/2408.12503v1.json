{"title": "The Russian-focused embedders' exploration: ruMTEB benchmark and\nRussian embedding model design", "authors": ["Artem Snegirev", "Maria Tikhonova", "Anna Maksimova", "Alena Fenogenova", "Alexander Abramov"], "abstract": "Embedding models play a crucial role in Nat-\nural Language Processing (NLP) by creating\ntext embeddings used in various tasks such as\ninformation retrieval and assessing semantic\ntext similarity. This paper focuses on research\nrelated to embedding models in the Russian\nlanguage. It introduces a new Russian-focused\nembedding model called ru-en-RoSBERTa and\nthe ruMTEB benchmark, the Russian version\nextending the Massive Text Embedding Bench-\nmark (MTEB). Our benchmark includes seven\ncategories of tasks, such as semantic textual\nsimilarity, text classification, reranking, and re-\ntrieval. The research also assesses a representa-\ntive set of Russian and multilingual models on\nthe proposed benchmark. The findings indicate\nthat the new model achieves results that are on\npar with state-of-the-art models in Russian. We\nrelease the model ru-en-RoSBERTa, and the\nruMTEB framework comes with open-source\ncode, integration into the original framework\nand a public leaderboard 1.", "sections": [{"title": "1 Introduction", "content": "Text embeddings play an important role in many\nNatural Language Processing (NLP) tasks, from\nclustering to semantic textual similarity (STS) and\ninformation retrieval (IR). The community has ad-\ndressed this demand by releasing several powerful\ntext embedding models (or embedders) (Wang\net al., 2024, 2023a; Chen et al., 2024). However,\nthere is still a lack of such embedders developed\nspecifically for the Russian language. The most\npopular Russian-oriented models, such as rubert-\ntiny2 2, SBERTlarge-nlu-ru\u00b3, and SBERTlarge-mt-nlu-ru4,"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Text Embedding Models", "content": "General text embedding models are widely used in\nvarious applications such as retrieval-augmented\ngeneration (RAG) (Lewis et al., 2020), STS, as\nwell as multimodal scenarios (Radford et al., 2021).\nOne of the first approaches for training such mod-\nels was to fine-tune a pre-trained language model\non the collection of labeled text pairs, such as\nSNLI (Bowman et al., 2015). Natural Language\nInference (NLI) has been shown (Reimers and\nGurevych, 2019) to help such models learn useful\nrepresentations of texts for STS and other down-\nstream applications. Recent approaches for model\ntraining utilize labeled datasets, which can be di-\nvided into symmetric (NLI, STS) and asymmet-\nric (Retrieval) tasks. Hence, the training objective\ntakes the form of multitask learning over one or\nmultiple objectives, and the specialized instructions\nare applied for each task (Su et al., 2022).\nInstead of training on limited labeled datasets,\nin (Wang et al., 2022a), it has been proposed to split\nfine-tuning into two stages: contrastive pre-training\nuses a large-scale pair dataset of noisy (or weakly-\nsupervised) text examples, and contrastive fine-\ntuning utilize a smaller number of high-quality ex-\namples. The authors of the E5mistral-7b-instruct (Wang\net al., 2023a) utilize an approach for model train-\ning which does not include expensive contrastive\npre-training that has been shown to be useful for\nsmaller encoder-only model XLM-R (Conneau\net al., 2019). While their quality remains compara-\nble, encoder-only models are more cost-effective\nduring inference.\nExamples of modern English-focused models\ninclude E5 (Wang et al., 2022a), BGE (Xiao et al.,\n2023a), GTE (Li et al., 2023), Nomic (Nussbaum\net al., 2024) and Arctic Embed (Merrick et al.,\n2024). Scaling the number of languages sup-\nported (including Russian) has been demonstrated\nin mE5 (Wang et al., 2024) models and BGE-\nM3 (Chen et al., 2024), thereby extending their\napplicability in multilingual contexts. The Russian-\noriented models are mainly represented by SBERT\nmodels and rubert-tiny2 and their modifications.\nModels mentioned above are often used for ad-\nditional fine-tuning on a specific task. To preserve\nthe general ability of the embedding model, it has\nbeen proposed (Xiao et al., 2023b) to merge the\nfine-tuned model with its base model.\nTo address this lack of contemporary Russian-\nfocused embedding models performing on par with\ntheir multilingual counterparts, we present ru-en-\nROSBERTa."}, {"title": "2.2 Text Embedding Benchmarks", "content": "Model evaluation has always played an inevitable\nrole in NLP progress. Starting from GLUE (Wang\net al., 2018) and SuperGLUE (Wang et al., 2019)\nbenchmarks have been a standard model evaluation\nmethod. As for text embedding representation eval-\nuation, it has been in focus for many years and was\ncommonly evaluated on a STS, for which yearly\nreleased SemEval (Agirre et al., 2016; Cer et al.,\n2017; Chen et al., 2022)8 datasets were commonly\nused. Being a single dataset inevitably limits the Se-\nmEval expressivity. Following the same approach,\nSentEval (Conneau and Kiela, 2018), which fo-\ncuses on classifier models on top of embedding,\novercomes this limitation by aggregating multiple\nSTS datasets. Still, it lacks the evaluation instru-\nments for the suitability of embedding for retrieval\nor clustering tasks. Due to the inefficiency of a"}, {"title": "3 ruMTEB Embedding Benchmark", "content": ""}, {"title": "3.1 Benchmark Structure and Evaluation\nMethodology", "content": "The ruMTEB benchmark unites 23 datasets, which\ncan be divided into 7 task categories similar to\nthe corresponding categories in the original M\u0422\u0415\u0412\nbenchmark: Classification (9 datasets), Clustering\n(3 datasets), MultiLabel Classification (2 tasks),\nPair Classification (1 task), Reranking (2 tasks),\nRetrieval (3 tasks), and STS (3 tasks). Below each\ntask category, the evaluation process is briefly de-\nscribed, and the dataset information can be found\nin Subsection 3.2.\nClassification. The evaluation is performed in\n10 consecutive experiments (bootstrap evaluation).\nFor run, a bootstrap subset of n (by default, n =\n8) training samples is sampled, and this down-\nsampled train and test parts are embedded using\nthe embedding model. The training subset is used\nto train the logistic regression classifier (with 100\ninteractions maximum). Then, test predictions are\nscored using the standard Accuracy score.\nPair Classification. This group includes datasets\nwhere, given a pair of text labels, one has to predict\na binary label. For evaluation, the two texts in each\npair are embedded via the embedding model, and\nthe cosine similarity between their embeddings is\ncomputed. Then, using the best binary threshold,\naverage precision is computed.\nMulti-label Classification. For evaluation, train\nand test sets are embedded. Then bootstrap eval-\nuation with 10 runs is performed. In each run the\ntraining sets are down-sampled to 8 instances of\neach unique label. The train embeddings are used\nto train the KNN classifier (n_neighbours = 5). The\nresult is evaluated on the test part using the standard\nAccuracy score.\nClustering. This task type includes datasets where,\ngiven a set of text fragments, one has to group them\ninto meaningful clusters. For evaluation, text frag-\nments are embedded. Then bootstrap evaluation\nwith 10 runs is performed. For each run, a subset of\nembedding are samples, which are then clustered\nusing K-means clustering. The result is evaluated\nvia v-measure (Rosenberg and Hirschberg, 2007)\nand averaged over all experiments.\nSemantic Textual Similarity (STS). Given a pair\nof sentences, the goal is to determine their textual\nsimilarity. Labels are continuous scores ranging\nfrom 0 to 1 (the closer to 1, the more similar). For\nevaluation, cosine similarity over the embedded\nsentences for each pair is computed. The result\nis evaluated with Spearman correlation (Reimers\net al., 2016).\nReranking. Inputs are a query and a list of ref-\nerence texts (relevant and irrelevant). The goal\nis to correctly rank these texts according to their\nrelevance to the query. For evaluation, the texts\nfor each query are ranked by the cosine similarity\nbetween the query embedding and the embedding\nof the given texts. The obtained ranking is scored\nwith MAP@k (k = 10) for each query and aver-\naged over all queries.\nRetrieval. For this task type, each dataset includes\na set of documents and queries and a mapping for\neach query to relevant documents. The task aims\nto find relevant documents for each task. For evalu-"}, {"title": "3.2 Benchmark Tasks", "content": "ruMTEB comprises 23 datasets divided into 7\ntask types mentioned above: six datasets based\non the Russian subsets from the original multilin-\ngual MTEB set (MassiveIntendClassification, Mas-\nsiveScenarioClassification, MIRACLReranking,\nMIRACLRetrieval, RuParaphraserSTS, STS22)\nand 17 new datasets we release within the re-\nsearch.The latter are based on popular Russian\ntime-tested and community-tested datasets.\nWe took the datasets based on the original\nMTEB without any changes. For the datasets\nadapted from other Russian sets, we performed\ndata cleaning and automatic filtering, where nec-\nessary, removed duplicates, and formatted them in\nthe MTEB format. The main dataset information\nand their statics are given in Table 1, and the de-\ntailed task descriptions and preprocessing for the\nnew sets are in Appendix A.3."}, {"title": "4 Text Embedding Model for Russian", "content": "This section is devoted to the text embedding model\nru-en-RoSBERTa released within the research. We describe the training data, the base model, and\nthe final training pipeline, motivated by the experi-\nments described in Section 5."}, {"title": "4.1 Training Data", "content": "Following previous work (Wang et al., 2022a;\nLi et al., 2023; Nussbaum et al., 2024), we\nuse publicly available data, high-quality and syn-\nthetic datasets to create training pairs (see Ap-\npendix A.1.1 for the full training list) 10, which,\nfor experiment purpose (see Section 5), we divide\ninto four groups described below.\nBasic Russian Datasets. This group consists of 17\ntasks. It includes pairs from SberQuAD (Efimov\net al., 2020), XNLI (Conneau et al., 2018), par-\nallel translations (Ba\u00f1\u00f3n et al., 2020; Tiedemann,\n2012; Zhang et al., 2020), and publicly available\ndata from various domains, such as news, blogs,\nQA platforms, and other Internet resources. We\nfilter this data mostly with manual rules (see Ap-\npendix A.1.2 for the details).\nBasic English Datasets. The group is formed from\nMEDI (Su et al., 2022) corpus without provided\ninstructions. We also exclude instructional datasets\nfrom Super-NI (Wang et al., 2022b) and thus re-\ntain 30 datasets representing different domains and\ntasks. We do not apply any additional preprocess-\ning steps.\nAdditional Synthetic Datasets. The group\nincludes Query2doc MS-MARCO (Wang\net al., 2023b), DINO-STS-x1x2 (Schick and\nSch\u00fctze, 2021), RuHNP (Malashenko et al.,\n2024a), entailment and contradiction pairs from\nRuWANLI (Malashenko et al., 2024b), and a\nsample of generated pairs by ruT5-base\u00b9\u00b9 model\nfrom WikiOmnia (Pisarevskaya and Shavrina,\n2022). We do not change the data content and use\nthe datasets as is.\nAdditional Retrieval Datasets. We use Russian\nand English parts of Mr. Tydi (Zhang et al., 2021)\nand MIRACL (Zhang et al., 2022) from BGE-\nM3 fine-tuning data. These datasets provide high-\nquality examples and are designed for the same\nretrieval tasks included in our benchmark.\nWe mine negatives similar to (Xiao et al., 2023a)\nusing the mE5small (Wang et al., 2024) and sample\ndocuments by rank in the range of 20-100. For all\ndatasets, the provided hard negatives are also used.\nFor additional synthetic and retrieval datasets, the\nprovided negatives are used (if available), and the\nrest are randomly sampled from the same dataset."}, {"title": "4.2 Base Model and English Language\nAdaptation", "content": "Since we focus on the Russian language, we use\nruRoBERTa 12 (Zmitrovich et al., 2023), which has\nthe highest scores on the classic Russian Super-\nGLUE (Shavrina et al., 2020) benchmark among\nthe models of its size. In addition, we adapt it to\nthe English language, allowing knowledge transfer\nfrom this high-resource language (see Section 5 for\nthe corresponding experiments).\nWe extend the original ruRoBERTa tokenizer\nwith tokens from RoBERTa 13 (Liu et al., 2019).\nTo learn new token embeddings, we train the model\nusing Masked Language Modeling (MLM) objec-\ntive (Devlin et al., 2018). We use the same hyperpa-"}, {"title": "4.3 Contrastive Fine-tuning", "content": "Following (Su et al., 2022), we perform contrastive\nfine-tuning for ru-en-RoBERTa on a mix of super-"}, {"title": "5 Training Procedure Analysis", "content": "This section describes experiments we conducted to\ndetermine the final training pipeline. We used basic\nRussian, English, and additional synthetic datasets,\nthe training approach described in Section 4 unless\notherwise specified, and the full ruMTEB version\nfor evaluation. Further findings are given in Ap-\npendix A.6."}, {"title": "5.1 Cross-lingual Knowledge Transfer and\nData Sources", "content": "We explored five training data configurations to\nstudy whether the model can profit from knowl-\nedge transfer between languages and various data\nsources. For this, we trained embedding mod-\nels based on ru-en-RoBERTa: on basic English\ndatasets only, basic Russian datasets only, and their\nmixture, simple or augmented with additional syn-\nthetic/synthetic+retrieval datasets. Each model is\ntrained for 1500 steps.\nResults presented in Table 2 indicate that the em-\nbedding model gets better results when trained on\ndata in Russian and English simultaneously. Addi-\ntional synthetic datasets and high-quality retrieval\ndatasets further improve the embedding model qual-\nity despite the tasks these datasets solve already\nbeing well represented in the basic datasets. Given\nthat in all scenarios, the number of steps is fixed,\nthe change in the results could not account for\nlonger training.\nThe model especially benefits from synthetic\ndatasets on STS-related tasks, while quality degra-\ndation in clustering tasks remains unclear. Note\nthat the model trained on almost all data (except\nthe additional retrieval dataset is better by only 0.6\npoints. The results obtained on data in English may\nbe due to the better quality of the tasks presented\nin MEDI."}, {"title": "5.2 English Language Adaptation", "content": "Having shown that the model can profit from cross-\nlingual knowledge transfer, we turned to selecting\nthe optimal language adaptation strategy. Namely,\nwe compared:\n\u2022 ruRoBERTa and XLM-R used as baselines;\n\u2022 ru-en-RoBERTa from subsection Section 4.2;\n\u2022 ru-en-RoBERTa w/ RetroMAE same approach\nas previous where we substituted MLM with\nRetroMAE (Shitao et al., 2022), which proved"}, {"title": "5.3 Training Examples", "content": "In this series of experiments (see Table 3), we show\nthe effects of prefixes, stratified sampling, and the\nnumber of hard negatives.\nRemove prefixes. Fine-tuning the model on sym-\nmetric and asymmetric tasks simultaneously can\nhurt performance without instructions but improve\nit when instructions are used (Su et al., 2022). We\nfound that removing prefixes consistently worsens\nthe results, but STS-related tasks were not as af-\nfected.\nDisable stratified sampling. To explore whether\nstratified sampling is beneficial (Merrick et al.,\n2024) in our case, we disabled it, used prefixes\nonly for queries, and negatives were exchanged\nacross devices. The latter increases the number\nof negatives per query to 8k. We found that the\nstratified version works better.\nHard-negatives. To study whether adding more\nhard-negatives (Ren et al., 2021) is beneficial, we\nincreased their number to 15 and reduced per de-\nvice batch size to 64, maintaining the total number\nof negative examples. To keep the same number of\nsteps, we apply gradient accumulation. Similarly\nto (Nussbaum et al., 2024), we found that despite\nprocessing almost twice as many texts, the results\ndid not improve."}, {"title": "5.4 Training Objective", "content": "In this experiment (see Table 3), we examine four\nmodifications of the training objective described in"}, {"title": "6 Evaluation", "content": "We evaluate ru-en-RoSBERTa and 9 publicly avail-\nable embedding models for Russian, including the\nmultilingual ones and the two instruct models, on\nthe ruMTEB benchmark. See Table 4 for the base-\nline information and Appendix A.4 for other de-\ntails.\nWe evaluate all models in the same environ-\nments and scenarios by the procedure described\nin 3.1. We use MTEB framework15 for evalua-\ntion where we integrated evaluation on the new\nruMTEB tasks 1617."}, {"title": "7 Results", "content": "Table 5 shows model scores averaged within the\ntask category, and detailed results of the task-wise\nmodel evaluation are in Appendix A.5.\nResults analysis reveals that there is a\ngap between instruct and non-instruct models,\nmE5large-instruct and E5mistral-7b-instruct are better than"}, {"title": "8 Conclusion", "content": "This paper introduces a new Russian-focused em-\nbedding model, which also supports English, and\na new benchmark for text embedding evaluation,\ncomprising 23 datasets divided into 7 task types.\nAmong the benchmark datasets, 17 datasets are\nnew and were created within this research.\nWe report the new embedding model architec-\nture design, pre-training corpus, and training proce-\ndure details. We describe the datasets comprising\nthe benchmark and propose the methodology for\nthe text embedding evaluation on it inspired by\nthe MTEB benchmark. We evaluate the presented\nencoding model and several baselines, thus verify-\ning the ruMTEB complexity and performing the\ncomparative analysis of our model results with the\nresults of standard encoders."}, {"title": "9 Limitation", "content": "Model limitations. The training data for ru-en-\nROSBERTa includes large segments from the In-\nternet domain. Consequently, it contains various\nstereotypes and biases from English and Russian\nsources. Therefore, a proper model evaluation is\nstill needed to explore their possible vulnerabili-"}, {"title": "10 Ethical Considerations", "content": "Inference Costs. Evaluating embedding models\non ruMTEB depends on its architecture and size\nand can be optimized with distributed inference li-\nbraries. For example, one run of ru-en-RoSBERTa\nof the complete evaluation experiment on a single\nA100 GPU 80GB takes approximately 19 hours.\nEnergy Efficiency and Usage. We compute the\nCO2 emissions from pre-training and fine-tuning\nru-en-ROSBERTa as Equation 1 (Strubell et al.,\n2019):\n$CO_2 = \\frac{PUE * kWh * ICO2}{1000}$          (1)\nThe power usage effectiveness (PUE) of our data\ncenters is 1.3. The resulting CO2 emission is 3.66k\nkg. Model compression techniques can reduce the\ncomputational costs associated with model infer-\nence.\nPotential Misuse. The ruMTEB can be used as\ntraining data for acceptability classifiers, poten-\ntially improving the quality of generated texts. We\nacknowledge that these improvements in text gen-\neration might lead to the misuse of LLMs for harm-\nful purposes. The intended use of ruMTEB is for\nresearch and development purposes, and we are\naware of the potential negative uses.\nAI-assistants Help. We improve and proofread\nthe text of this paper using Grammarly18 to cor-\nrect grammatical, spelling, and style errors and\nparaphrasing sentences. Thus, some segments of\nour publication can be potentially detected as AI-\ngenerated, AI-edited, or human-AI-generated."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Training Data Details", "content": ""}, {"title": "A.1.1 Training Data Information", "content": "The list of datasets included in ru-en-ROSBERTa\ntraining data and the corresponding prefix used for\nthem are given in Table 6.\nWe use the following basic rules to choose a\nprefix:\n\u2022 search_query and search_document pre-\nfixes are for answer or relevant paragraph re-\ntrieval\n\u2022 clustering prefix is for asymmetric retrieval\nof title or summary and relevant document\n\u2022 classification prefix is for symmetric\nparaphrasing related tasks (STS, NLI, bitext\nmining)"}, {"title": "A.1.2\nData Filtration Details", "content": "We apply the following steps to the basic Rus-\nsian datasets. First, texts longer than 500 to-\nkens (ruRoBERTa-large19 tokenizer is used) are\nfiltered out. A small number of tokens is re-\nserved for instructions or prefixes. Pairs from\nYandexQ20, Pikabu21, StackOverflow22, Habr23\nand Habr QnA24 are filtered by content popular-\nity (e.g. views, ratings, votes). Cosine similarity\nobtained from LaBSE (Feng et al., 2022) is applied\nto filter NewsCommentary and MultiParaCrawl.\nWe filter pairs from paraphrase-NMT-Leipzig25 by\np_good score (equivalent meaning). The XNLI is\nformed from entailment (relevant document) and\ncontradiction (irrelevant negative) examples. For\nMIRACL, we use the title as the query and the\nfirst paragraphs (until we reach the token limit) as\nthe document. We form pairs for Paraphrases 26\nfrom paraphrases field, taking one as a query and\nthe others as positive documents. The content of\nRuNews27 is not changed. After exact match dedu-\nplication, the final training pairs for all datasets are\nrandomly sampled from the remaining pairs."}, {"title": "A.2 Model Training Details", "content": "We train the model in bf16 dtype with gradi-\nent checkpointing and use AdamW (Loshchilov\nand Hutter, 2017) with a learning rate of le-5\nand weight decay of 0.01 for exactly one epoch,\nwhich is approximately 3700 steps, of which linear\nwarmup is 200 steps. After fine-tuning, the SLERP\nmerging is applied to the base model with a factor\nof 0.1.\nWe apply stratified sampling per device batch\n(mini-batch). Therefore, the global batch in-\ncludes mini-batches consisting of pairs of different\ndatasets. On the one hand, it becomes impossible\nto exchange negatives between devices and thus\nscale the number of in-batch negatives. On the\nother hand, this increases the diversity of sources\nin the global batch. Therefore, we do not apply\nthe DisCo (Chen et al., 2023) trick to exchange\nnegative examples across devices. The batch size is\n128 per device, giving 1024 documents per query.\nContext length is set to 512 for queries and docu-\nments (Merrick et al., 2024).\nTraining is conducted on a single H100\nnode. We utilize codebase from BGE28 and\nadapt it for our experiments. PyTorch's\nexpandable_segments helps us to mitigate frag-\nmentation issues due to variable sequence length."}, {"title": "A.3 ruMTEB Dataset Description", "content": "This section describes new tasks we present with\nthe research and data preparation details."}, {"title": "A.3.1 Classification", "content": "KinopoiskSentimentClassification. In a senti-\nment classification dataset given a film review, one\nhas to predict whether it is Positive, Neutral, or\nNegative (3 classes in total). The data was taken\nfrom the original dataset (Blinov et al., 2013)29,\nwhich contains reviews from July 2004 to Novem-\nber 2012. In the preprocessing phase, we removed\nall mentions of the final rating from the review texts"}, {"title": "A.3.2 Pair Classification", "content": "TERRa. The dataset was presented as one of the\nRussian SuperGlue tasks (Shavrina et al., 2020) and\nrelated to the Textual Entailment Recognition task.\nGiven two texts, the task is to determine whether\nthe meaning of one text entailed from the another\ntext. Since the test split is hidden, we took the dev\nsplit without changes. A total of 307 examples are\navailable."}, {"title": "A.3.3 Multi-Label Classification", "content": "CEDRClassification. The dataset is a task of clas-\nsifying comments into five emotions (joy, sadness,\nsurprise, fear, and anger). A total of 9,410 com-\nments were presented from the following sources:\nsocial networks, news, and blogs. The dataset was\nused as is, without any modifications (Sboev et al.,\n2021). We took the original test split, which in-\ncludes 1882 examples.\nSensitive TopicsClassification. The dataset con-\ntains sentences that can be classified into one or\nmore sensitive topics35 (Babakov et al., 2021). The\noriginal dataset includes 18 classes, all classes are\nused. Since part of the dataset is not only manually\nlabeled, we first formed a test split from manu-\nally labeled examples, and the remaining examples\nwere combined with semi-automatically labeled\nexamples. We have selected the most reliable ex-\namples based on the confidence scores indicated in\nthe examples. The final test split consists of 2048\nexamples and preserves the original class distribu-\ntion."}, {"title": "A.3.4 Clustering", "content": "GeoReview Clustering A clustering dataset based\non the Yandex Maps36 reviews37, where given a\nreview text one has to cluster the samples accord-\ning to their rubrics or review categories (e.g., Bank,\nSupermarket, Pharmacy, etc.). The original dataset\nwas balanced and split into three parts (train, vali-\ndation, and test). For each review, we took its first\nrubric as the main label, leaving only samples cor-\nresponding to the top 100 most popular labels. This\nthreshold limited the categories exceeding 10,000\nexamples. The final dataset was converted into the\nMTEB format.\nRuSciBenchGRTNI/OECDClustering. This is a\ndataset for the clustering of scientific text headings.\nEach article has its OECD and GRNTI headings,\nand there are 29 OECD headings and 28 GRNTI\nheadings in the dataset (e.g., Mathematics, Biologi-\ncal Sciences, Economics and Business, etc.). The\ndata was sourced from the original dataset RuS-\nciBench 38. During preprocessing, duplicates were\nremoved, the title and abstract were combined, and\nthe set was balanced, leaving only the same num-\nber of samples for each class. The resulting dataset\nwas then divided into test and training parts."}, {"title": "A.3.5 Semantic Textual Similarity (STS)", "content": "RuSTSBenchmarkSTS. The dataset used for the\nSTS task is derived from the original multilingual\nSTS Benchmark 39. This multilingual set com-\nprises various translations of the original English\nversion of the STSbenchmark dataset, with the\ntranslations completed using deepl.com 40. The\nRussian segment of the dataset was extracted and\nrefined using the RuCoLa (Mikhailov et al., 2022)\nclassifier 41. In all parts of the sets (train/dev/test),\ninstances categorized as not linguistically accept-\nable were excluded. Additionally, any duplicate\nentries were eliminated."}, {"title": "A.3.6 Reranking", "content": "RuBQReranking. The dataset is based on RuBQ\nversion 2.0 (Rybin et al., 2021). The dataset con-"}, {"title": "A.3.7 Retrieval", "content": "RuBQRetrieval. Unique paragraphs from the\ndataset are used for the document bank, resulting in\n56,826 documents. Documents were deduplicated\nwhile links to relevant documents were maintained.\nThe original test split was taken without changes\nand has 2845 examples.\nRiaNewsRetrieval. The original dataset Russi-\naSegodnya42 (also known as RiaNews) consists of\nnews articles and their headlines (Gavrilov et al.,\n2019). Texts are presented in lowercase format, and\nthe capitalization of individual characters has not\nbeen changed. Since the article texts are available\nin HTML, we used the BeautifulSoup43 library to\nclean them of markup. Additionally, texts were nor-\nmalized, and extra spaces were removed. We also\nremoved, if possible, the first sentence in each arti-\ncle text since it does not relate to the article's con-\ntent and is a kind of meta information (\u201cMoscow, 1\nDec ria news.\"). We filtered out the texts of arti-\ncles with more than 2000 characters so that models\nlimited to a context of 512 tokens could handle the\nentire text. All examples were deduplicated based\non the headline and text of the article. Our final\ndataset consists of 10,000 randomly sampled head-\nlines as queries, and article texts (724344) are used\nas documents.\""}, {"title": "A.4 Experimental Setup Details", "content": "This section describes the prompt and embedding\nconfiguration we used in our experiments. Namely,\nwe use normalized embeddings for evaluation on\nall ruMTEB tasks. We use pooling and instruction\nstrategies required by the corresponding model we\nevaluate. Table 7 presents all the prefixes and in-\nstructions used. Specifically:\n\u2022 we do not utilize any special prompts for\nrubert-tiny2, BGE-M3, SBERTlarge-ru, and"}, {"title": "A.5 Detailed Results", "content": "Table 8 shows results on individual ruMTEB\ndatasets."}, {"title": "A.6 Additional Experimental Findings", "content": "In this part, we describe early-stage experiments\nthat were conducted on different data subsets and\ndifferent base models.\nPrefixes. We found that E5 prefixes (Wang et al.,\n2022a) performed slightly worse and assume that\nthe variant we use helps to better separate tasks\nduring training. The clustering prefix is more\nsuitable for tasks where thematic identification is re-\nquired, so in many classification problems, we use\nit instead of classification, despite the name.\nWe tried adding prefixes with some probability;\nthis improved the results without using prefixes and\nalso worsened the results with them. In addition\nto stratified sampling, we implemented a sampling\nstrategy that takes pairs with the same prefix but\nsaw no improvement.\nLosses. It was shown that Sigmoid Loss (Zhai\net al., 2023) performed better at smaller batch sizes.\nWe found that SigLIP is more sensitive to selecting\nthe initial values of the bias and temperature param-\neters to achieve convergence. CoSENT loss (Li and\nLi, 2023) shows"}]}