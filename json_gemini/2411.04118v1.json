{"title": "Medical Adaptation of Large Language and Vision-Language Models: Are We Making Progress?", "authors": ["Daniel P. Jeong", "Saurabh Garg", "Zachary C. Lipton", "Michael Oberst"], "abstract": "Several recent works seek to develop foundation models specifically for medical applications, adapting general-purpose large language models (LLMs) and vision-language models (VLMs) via continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining (DAPT) improves performance on downstream medical tasks, such as answering medical licensing exam questions. In this paper, we compare seven public \"medical\u201d LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting regime for medical question-answering (QA) tasks. For instance, across the tasks and model pairs we consider in the 3-shot setting, medical LLMs only outperform their base models in 12.1% of cases, reach a (statistical) tie in 49.8% of cases, and are significantly worse than their base models in the remaining 38.2% of cases. Our conclusions are based on (i) comparing each medical model head-to-head, directly against the corresponding base model; (ii) optimizing the prompts for each model separately; and (iii) accounting for statistical uncertainty in comparisons. While these basic practices are not consistently adopted in the literature, our ablations show that they substantially impact conclusions. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.", "sections": [{"title": "1 Introduction", "content": "Recent advances in autoregressive large language models (LLMs) and vision-language models (VLMs) have attracted interest from practitioners in medicine, where these models hold great potential to transform various aspects of clinical practice (e.g., medical diagnosis, information retrieval from clinical documents, patient triaging)."}, {"title": "2 Related Work", "content": "DAPT is a transfer learning approach, where a pretrained model is further pretrained on domain-specific data for better alignment to a target domain of interest (e.g., medicine, law). Several studies show that language models trained via DAPT often outperform their general-domain counterparts on domain-specific tasks, such as claim detection from blog posts, named entity recognition from German novels, and judgment prediction for legal cases. In the medical domain, prior works based on BERT-style encoder-only language models, show that medical DAPT improves fine-tuning performance on tasks such as medical concept extraction from patient reports, identification of gene-disease relations from PubMed abstracts, and natural language inference on clinical notes.\nMore recent works suggest that decoder-based autoregressive LLMs and VLMs trained via medical DAPT also show strong performance on various medical tasks. Medical LLMs such as MEDITRON, adapted from LLAMA-2 and BIOMISTRAL adapted from MISTRAL-7B-INSTRUCT-V0.1; perform well on knowledge-intensive QA tasks based on medical licensing and academic exams and PubMed abstracts. Medical VLMs such as LLAVA-MED, adapted from LLAVA; and MED-FLAMINGO, adapted from OPEN-FLAMINGO; also perform well on visual QA tasks based on radiology and pathology images and academic exams. These encouraging results have established DAPT as a go-to approach for training a medically specialized model, a conclusion that we re-examine in this work."}, {"title": "3 Experimental Setup", "content": "To investigate the effectiveness of medical DAPT in improving zero-/few-shot performance, we compare 7 medical LLMs and 2 medical VLMs against their general-domain counterparts in pairs, on 13 textual QA datasets and 8 visual QA datasets, respectively. The models in each pair are exactly identical in model architecture and scale, and their only difference lies in whether they were additionally pretrained on medical data. We also note that while some of datasets used for evaluation contain both closed-ended (i.e., has clear ground-truth answers) and open-ended questions, we focus our evaluations on the former, where an objective, quantitative assessment of medical knowledge and reasoning capabilities is possible. For reproducibility of our results, we open-source the source code used for all of our evaluations described below via our GitHub repository."}, {"title": "3.1 Zero-/Few-shot Prompting with Model-Specific Prompt Selection", "content": "In this section, we provide an overview of our approach to assess whether medical DAPT leads to statistically significant improvements in zero-/few-shot medical QA performance. For few-shot prompting, we consider the 3-shot setting to ensure that the input prompt is shorter than the context window sizes for all models evaluated. For evaluation, we pay special attention to two aspects. First, language models are highly sensitive to the choice of prompting strategy (e.g., prompt format, choice of few-shot examples), where seemingly insignificant changes to the prompt can lead to idiosyncratic model behavior. Second, prior works show that the \u201coptimal\" choice of prompt format rarely correlates between different models, suggesting that using a single, fixed prompt for all models for comparison can result in misleading conclusions.\nTo ensure a fair comparison that isolates the impact of medical DAPT, we treat the choice of prompt format and few-shot examples as additional hyperparameters when generating predictions, and tailor them to each model independently. We first randomly sample 10 plausible prompt formats from a predefined search space and 10 different sets of few-shot examples from the training set of each dataset. We then search over all pairs of prompt formats (plus one additional manually designed default format) and few-shot examples, and select the best pair out of (10 + 1) \u00d7 10 = 110 that results in the highest validation exact-match accuracy. Given that a grid search at this scale can be computationally expensive, especially for datasets like MedMCQA that contain 37k validation QA pairs, we randomly subsample 500 validation QA pairs for datasets that have more than 500. Using the vLLM framework for sampling model outputs, this leads to a runtime of around 5\u201315 minutes per trial, on 4 NVIDIA A6000 GPUs for the 70B models and 2 GPUs for the others. We then generate predictions on the test set using the selected prompt format and few-shot samples. In the zero-shot setting, we only search over the prompt formats.\nTo define the prompt format search space, we follow the approach by and construct a context-free grammar of semantically equivalent yet syntactically distinct prompt formats. For the medical models that have a specific prompt format designed and recommended for closed-ended QA tasks, we fix the prompt format to what is provided and only search over the choice of few-shot examples. In the case when such information is missing or only partially available , we search over both the prompt formats and few-shot examples. For instruction-tuned models, which typically have a structured conversational format (e.g., \u2018### User:. ### Assistant:... \"), we use the sampled question and answer templates to format each \"user\" query and \"assistant\" response.\""}, {"title": "4 Results", "content": "Here, we summarize the main findings from the zero-/few-shot prompting experiments outlined in Section 3. Unless specified otherwise, we focus on the greedy decoding results in subsequent discussions and include the results for constrained decoding in Appendix E. Overall, we find that all medical VLMs and nearly all medical LLMs fail to consistently improve over their general-domain counterparts in the zero-shot and few-shot prompting regimes. Moreover, we demonstrate the importance of rigorous experimental design in surfacing this finding-performing pairwise model comparison with a single, fixed prompt optimized only for the medical model, while ignoring statistical uncertainty, paints a misleadingly optimistic picture of medical DAPT performance."}, {"title": "Finding 1: After model-specific prompt selection, the vast majority of medical models fail to consistently show a statistically significant improvement over the general-domain models.", "content": "In Figures 3-4, we show the absolute and relative exact-match accuracies achieved by the medical and general-domain LLMs and VLMs in the zero-/few-shot prompting regime. For LLMs, we only show the 3-shot prompting results in the main text (see Appendix D for results in the zero-shot setting, which are similar). We exclude the results for CLINICAL-CAMEL-70B on both versions of MedQA, as the model has already been trained on a subset of the official training split. For VLMs, we show both zero-shot and 3-shot results, as LLAVA-v0-7B and LLAVA-MED-7B were not pretrained to handle inputs with multiple images. We calculate the confidence intervals via bootstrapping on the test set, as described in Section 3.\nThe top row of Figure 3 shows that the absolute exact-match accuracies are mostly similar between each model pair across all datasets and model scales, with marginal performance improvements. In fact, the bottom row of Figure 3 shows that only 2 out of 7 medical LLMs\u2014OPENBIOLLM-70B and BIOMISTRAL-7B\u2014show statistically significant improvements in performance, with the 95% confidence intervals crossing zero relative accuracy in most cases for the other models. When compared against their corresponding base models, OPENBIOLLM-70B achieves a win rate of 30.8%, tie rate of 69.2%, and loss rate of 0%, while BIOMISTRAL-7B achieves a win rate of 46.2%, tie rate of 53.8%, and loss rate of 0%. Notably, MEDITRON-7B and BIOMEDGPT-LM-7B actually show significantly worse performance than their base models, with loss rates of 76.9% and 92.3%, respectively. Similar trends hold for the zero-shot setting (Figure D1 and Table D1), where only CLINICAL-CAMEL-70B and BIOMISTRAL-7B show statistically significant improvements.\nWe note that, while OPENBIOLLM-70B shows improvement in the 3-shot setting, it does not show improvement in the zero-shot setting (winning on 7.7% and losing on 23.1% of tasks, see Table D1), and vice versa for CLINICAL-CAMEL-70B (winning on 0% of tasks and losing on 36.4% of tasks in the 3-shot setting, see Table D2), leaving BIOMISTRAL-7B as the only medical LLM that wins more than it loses against its base model (MISTRAL-7B-INSTRUCT-V0.1) in both settings, albeit with relatively low absolute performance.\nIn Figure 4, we make similar observations for medical VLMs in both zero-shot and 3-shot settings, where both LLAVA-MED-7B and MED-FLAMINGO-9B are virtually indistinguishable from their base models in terms of performance, showing no statistically significant improvements. Tables D1-D2 show that LLAVA-MED-7B achieves win/tie/loss rates of 12.5%/62.5%/25.0% in both zero-shot and 3-shot settings, while MED-FLAMINGO-9B achieves win/tie/loss rates of 0%/87.5%/12.5% in the zero-shot setting and 0%/100%/0% in the 3-shot setting. Meanwhile, we note that the confidence intervals for the MMMU-Medical datasets tend to be much wider than for the other visual QA datasets, as the test sets only include 25 QA examples for each subject."}, {"title": "Finding 2: Using a single, fixed prompt for all models and overlooking statistical uncertainty may overestimate the performance benefits of medical DAPT.", "content": "Based on Finding 1, we further investigate whether the conclusions differ if the same prompt is used for each pair of medical and general-domain models. In particular, we consider whether selecting a prompt only for the medical model, following Section 3.1, and using it for the corresponding general-domain model can widen the performance gap between each pair. We also assess whether this gap becomes amplified when models are compared without accounting for statistical uncertainty, which is often done in practice.\nIn Figure 5, we show how the win/tie/loss rates of the medical models, computed over all (model pair, QA dataset) combinations, change as we vary the following aspects of the experimental setup:\n1.  select prompts for each model independently vs. only based on the medical model;\n2.  determine a win for the medical model based on confidence intervals in relative accuracy vs. raw absolute accuracy.\nWe note that when comparing each model pair based on absolute accuracy, there are no ties, as the real-valued absolute accuracies are rarely identical. In Appendix D, we include Figures D2-D3 to show how the absolute and relative exact-match accuracies change when the prompt is only optimized for the medical model. We also include Tables D3-D4 to show changes in win/tie/loss rates. We show the same set of results for constrained decoding in Figures E3-E4 and Tables E3-E4 in Appendix E.\nOverall, we find that for both LLMs and VLMs, the performance improvement from using a medically adapted model instead of its general-domain counterpart can be substantially overestimated when (i) the prompt is only tailored to the medical model; and (ii) the models are compared only based on their absolute accuracies. Notably, in the zero-shot setting, the win rate increases from 9.4% to 70.5% for medical LLMs and from 6.3% to 62.5% for medical VLMs, when only performing prompt selection for the medical model and comparing based on absolute accuracy. Figure E5 in Appendix E.2 shows a similar trend in the win/tie/loss rates, when the model predictions are generated via constrained decoding. These results highlight the importance of accounting for LLM/VLM sensitivity to the prompting details, as suggested by Sclar et al. (2024), and the statistical uncertainty in model comparison, in order to draw reliable conclusions about the effectiveness of medical DAPT."}, {"title": "5 Discussion and Conclusion", "content": "In this work, we investigated the effectiveness of DAPT for training medically specialized LLMs and autoregressive VLMs suitable for knowledge-intensive medical (visual) QA tasks. To that end, we compared several pairs of state-of-the-art medical LLMs/VLMs to their general-domain counterparts, whose only differences lie in medical DAPT and are exactly identical in model architecture and scale. Our work diverges from prior works by providing a direct apples-to-apples comparison of medical and general-domain models while accounting for LLM/VLM sensitivity to prompting details and assessing the statistical significance of the results.\nAcross both model classes and all model scales, we found that the performance benefits from medical DAPT largely disappear when we (i) tailor the prompt format and choice of few-shot examples to each medical and general-domain model separately; and (ii) account for statistical uncertainty in model comparison. In particular, we found that when we optimize the prompt only for the medical model and compare each model pair based on their absolute accuracies without accounting for uncertainty, the performance improvements from medical DAPT can be overestimated, potentially leading to unreliable conclusions about the benefits of medical DAPT. For example, in the zero-shot setting, evaluation under this setup leads to the conclusion that medical LLMs and VLMs, on average, outperform the corresponding general-domain models in 70.5% and 62.5% of all QA tasks, while the improvements are in reality statistically significant in only 9.4% and 6.3% of tasks after optimizing the prompt for each model to ensure a fair comparison.\nOur findings suggest that for state-of-the-art general-domain LLMs and VLMs, the performance benefits from additionally pretraining on medical data from public sources such as PubMed may be limited. Notably, almost all of the medical models used in our evaluation use PubMed as the primary source of pretraining data for medical adaptation, while open-source datasets commonly used for pretraining the general-domain base models in the first place (e.g., the Pile (Gao et al., 2020), S2ORC (Lo et al., 2020)) often already include PubMed data. Prior works also suggest that the intrinsic capacity of LLMs to solve a downstream task is largely obtained during the initial pretraining phase, and that post-training adjustments and prompt engineering efforts may only help elicit the existing capabilities Thus, we argue that any claims about improvement from a proposed medical DAPT procedure should be evidenced by rigorous head-to-head comparisons against the corresponding general-domain model, in order to draw reliable conclusions about its effectiveness."}, {"title": "6 Limitations", "content": "We discuss our findings with the following caveats. First, there is a vast and growing set of papers on applying medical DAPT to various general-domain base models, and we could not hope to compare all publicly available models here. While we selected the models to cover a wide range of general-domain base models and model scales (7B-70B) and included some of the latest models it is always possible that some newly released models do in fact yield better zero- or few-shot performance on medical QA.\nSecond, we focus in this paper on the narrower task of closed-ended medical QA. In part, this choice reflects the fact that such benchmarks are well-standardized and highly publicized. However, they do not reflect the breadth of possible applications of LLMs and VLMs in medical domains. Some would argue that such tasks are a more realistic application of such models in practice, and it is certainly possible that an analysis like ours would find improved performance on such tasks, though we do not investigate these tasks in the present work.\nThird, we do not consider downstream fine-tuning of models subject to medical DAPT. In part, this reflects issues of computational cost (e.g., to fine-tune 70B-parameter models) and the added complexity of reproducing a fine-tuning procedure, versus using publicly available model checkpoints. However, we acknowledge that zero- and few-shot performance are only part of a broader narrative around the claimed benefits of medical DAPT, which generally includes the additional claim that it provides a better initialization for downstream fine-tuning. While we acknowledge the limitations above, we do not believe they detract from the value of this work. We hope that our results call attention to a need for rigorous head-to-head evaluations when making similar claims of improved performance via medical DAPT, whether with other models, on other clinical tasks, or with respect to fine-tuning versus zero-/few-shot performance."}, {"title": "Assessing Statistical Significance", "content": "Formally, we define relative exact-match accuracy as $E[1[f_{medical}(x) = y] - 1[f_{general}(x) = y]] \\in [-1,1]$, where $f_{medical}$ and $f_{general}$ denote the medical and general-domain models, $x$ and $y$ denote the input prompt and answer in a QA pair from the test set, and $1[\u00b7]$ denotes the indicator function."}, {"title": "Prompt Format Sampling", "content": "P ::= H_q + t + H_c + t + L + t + H_a"}]}