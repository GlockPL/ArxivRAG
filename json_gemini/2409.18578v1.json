{"title": "An Enhanced Federated Prototype Learning Method under Domain Shift", "authors": ["Liang Kuang", "Kuangpu Guo", "Jian Liang", "Jianguo Zhang"], "abstract": "Federated Learning (FL) allows collaborative machine learning training without sharing private data. Numerous studies have shown that one significant factor affecting the performance of federated learning models is the heterogeneity of data across different clients, especially when the data is sampled from various domains. A recent paper introduces variance-aware dual-level prototype clustering and uses a novel a-sparsity prototype loss, which increases intra-class similarity and reduces inter-class similarity. To ensure that the features converge within specific clusters, we introduce an improved algorithm, Federated Prototype Learning with Convergent Clusters, abbreviated as FedPLCC. To increase inter-class distances, we weight each prototype with the size of the cluster it represents. To reduce intra-class distances, considering that prototypes with larger distances might come from different domains, we select only a certain proportion of prototypes for the loss function calculation. Evaluations on the Digit-5, Office-10, and DomainNet datasets show that our method performs better than existing approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning [16] (FL) is an innovative distributed learning framework that allows clients to collaborate in training a global model using their own local datasets, thus maintaining data privacy. FL has several advantages over traditional distributed learning methods as it reduces communication costs and addresses privacy concerns, leading to widespread adoption across various sectors. However, FL faces challenges, particularly concerning data heterogeneity. In FL, clients gather private data from different sources, resulting in non-independent and identically distributed (non-IID) datasets. These non-IID distributions can cause clients to reach their own local optima, potentially deviating from the global objective. As a result, this deviation may hinder convergence rates and reduce overall model performance [31].\nIn FL applications, different types of heterogeneity issues arise [34]. Initially, works were focused on addressing label skew in non-IID (non-identically distributed) data, where the label distribution varies across different clients' datasets. In this type of heterogeneity, the most significant shift occurs in the final layer of the local model, i.e. the classifier [15]. Works by [15], [17], [20], [24], [33] aim to resolve this issue, resulting in faster and more stable convergence as well as higher accuracy. However, as FL algorithms are applied more broadly, more realistic heterogeneity issues are being considered. Recently, some studies have begun to tackle feature skew, where different clients' datasets have different feature representations for the same labels. In this situation, FL algorithms that only optimize the classification head perform poorly because there are substantial differences between the feature extractors trained by different clients. Therefore, it becomes necessary to introduce features into the loss function and update the feature extractor with back-propagation. Works by [2], [8], [26] define the average of features of samples with the same label as prototypes and design loss functions to gradually gather output features closer to corresponding prototypes, ensuring the convergence of the feature extractor. These methods work well under feature skew and even label skew.\nFedPLVM [30] innovatively redefines the method of computing prototypes by variance-aware dual-level prototype clustering. This approach allows prototypes obtained through clustering to capture richer semantic information. Additionally, FedPLVM employs a novel a-sparsity prototype loss, which enhances intra-class similarity and reduces inter-class similarity. Under this framework, FedPLVM narrows the performance gap between easy and hard domains, leading to an overall improvement in average accuracy. However, FedPLVM's dual-level clustering approach has flaws. The local prototypes obtained after the first clustering are treated equally in the second clustering and loss function calculations. The number of samples they represent can vary significantly, contradicting FedPLVM's goal of narrowing the performance gap between the easy and hard domains. Furthermore, when calculating the intra-class loss function, FedPLVM attempts to minimize the distance between each feature and all global prototypes. However, some global prototypes from vastly different domains do not provide a meaningful reference for that feature; thus, using the loss function to minimize these distances can adversely affect the model.\nConsidering these factors, we propose an improved algorithm, FedPLCC (short for Federated Prototype Learning with Convergent Clusters). Based on the dual-level clustering framework introduced by FedPLVM, FedPLCC makes two key innovations. First, we consider the number of samples before clustering and incorporate this into the loss function calculation by assigning weights. We notice that a small number of samples with outlier features can form minor outlier clusters during FINCH aggregation, leading to more severe divergence in loss function calculations. Second, when calculating the intra-class loss function, we introduce a hyper-parameter to determine the proportion of prototypes involved in the calculation. Specifically, we select prototypes more similar to the current feature and represent more samples. Within the dual-level clustering framework, some prototypes from other domains may differ significantly from the current feature, not because the model parameters need updating but due to inherent differences in feature expression across domains. Consequently, forcing features to minimize the distance to all prototypes with the same label undermines the model's generalization capability. Consequently, FedPLCC is an algorithm based on Federated Prototype Learning and dual-level clustering. By carefully considering the clustering convergence process, we have endowed it with better generalization capabilities and improved accuracy. Our main contributions are outlined as follows:\n\u2022 This study delves into federated learning with domain shift, summarizing several existing Federated Prototype Learning methods and rethinking their clustering components. We point out that the coarse handling of clustering in existing methods may lead to slower convergence speeds or a decline in final accuracy.\n\u2022 To address this issue, we introduce a new method, FedPLCC. This method builds on the previously established dual-level prototype clustering framework but more accurately captures the local feature distribution by calculating prototype weights. Additionally, in our loss function calculation, we introduce constraints to prevent features from being forced to align with significantly dissimilar prototypes.\n\u2022 Extensive experiments conducted on the Digit-5 [32], Office-10 [4], and DomainNet [22] datasets demonstrate the superior performance of our proposed method when compared with multiple state-of-the-art approaches. All experiment codes will be available on GitHub after the publication of this paper."}, {"title": "II. RELATED WORK", "content": "Federated Learning (FL) aims to develop a global model through collaboration among multiple clients while protecting their data privacy. FedAvg [16], the pioneering work in FL, demonstrates that this approach has advantages in terms of privacy and communication efficiency by aggregating local model parameters to train a global model. The challenge of data heterogeneity in FL typically manifests as clients possessing non-IID (independent and identically distributed) data, including label and feature skew. Earlier works recognize that label skew reduces the models' accuracy and attempt to solve this issue. For example, [1], [13] use regularization terms to enhance global model performance. [2], [15], [20] optimize the classification heads to improve performance. Recent works begin to address feature skew. Some utilize prototypes as global information and clustering them. However, these studies have directly used the FINCH algorithm for clustering without carefully examining the actual impact of the clustering process in FL or exploring how to optimize it."}, {"title": "B. Prototype Learning", "content": "Prototype learning is a machine learning approach that involves using representative examples of different classes, prototypes, in the learning process. Prototype learning has been extensively explored in various tasks such as transfer learning [10], few-shot learning [18], [25], zero-shot learning [9], and unsupervised learning [5], [28]. In the FL literature, prototypes are used to abstract knowledge while preserving privacy. For example, FedProto [26] and FedProc [19] align features with global prototypes, CCVR [15] collects Gaussian statistics of clients' data and then uses a Gaussian Mixture Model to generate virtual features. FPL [8] clusters local prototypes and calculates unbiased global prototypes to address the issue of clients' data coming from different domains. FedPLVM [30], based on FPL, introduces a dual-level clustering framework and a-sparsity to reduce the intra-class distances and increase the inter-class distances. Many use clustering algorithms, FINCH [23] for example, to aggregate local prototypes into global prototypes. Our work examines the clustering process of FL in detail, optimizing the aggregation process and the loss calculation method, thereby improving the model's accuracy."}, {"title": "C. Contrastive Learning", "content": "Contrastive learning has recently emerged as a promising direction in self-supervised learning, achieving competitive results comparable to supervised learning. The primary idea is to bring similar data points closer in the representation space while pushing dissimilar ones apart. A classic work [21] constructs positive and negative pairs for each sample and applies the InfoNCE loss to compare these pairs. Contrastive learning can also be used under fully supervised settings, utilizing both label information and contrastive methods [11]. Some works [8], [30], as well as ours, apply contrastive learning to local training of federated learning to enhance performance."}, {"title": "III. METHODOLOGY", "content": "We follow the classic FL scenario. There are K clients communicating with one server to train an ML model together without sharing their local training data, denoted by $D_k = \\{x_i, y_i\\}_k$ for client k. The global objective of FL can be formulated as:\n$\\min_{w} \\sum_{k=1}^{K} \\frac{N_k}{N} L_k(w; D_k)$,                                      (1)\nwhere $L_k$ is the local loss function for client k, w denotes the shared global model and $N = \\sum_{K=1} N_k$ denotes the total number of samples among all clients.\nDomain shift exists among clients in heterogeneous federated learning. The conditional feature distribution $P(x|y)$ varies across clients while P(y) is consistent, i.e. $P_m(x|y) \\neq P_n(x|y)$ ($P_m(y) = P_n(y)$) for any two clients m and n."}, {"title": "B. Federated Prototype Learning", "content": "The work by [15] first points out that the last layer of the model, the classifier, biases the most in heterogeneous federated learning, so they divide the classification network into two parts: the feature extractor and the classifier. The feature extractor $h: R^V \\rightarrow R^D$ maps a sample $x \\in R^V$ to its feature vector $z = h(x) \\in R^D$, then the classifier $f: R^D \\rightarrow R^M$ outputs the M-class prediction $f(z) = f(h(x)) \\in R^M$. Some previous works [15], [20] adjust the classifier to improve accuracy. In contrast, Federated Prototype Learning methods utilize contrastive methods to optimize the feature extractor. Clients generate local prototypes for each class with the feature vectors of their local samples and share them with the server, and then the server generates global prototypes with the local prototypes collected from the clients. Formally,\n$P_m^k = \\{\\hat{p}_{j}\\}_{j=1}^{J_m^k} \\xleftarrow{Algorithm} \\{h(x_i) | (x_i, y_i) \\in D\\},  \\ \\ (2)$\n$G_m = \\{\\hat{g}_{j}\\}_{j=1}^{C_m} \\xleftarrow{Algorithmg} \\{P_m^k\\}_{k=1}^K , \\ \\ (3)$\nwhere $P^m$ and $G^m$ represents local prototypes and global prototypes of class m on client k respectively, with size $J^k_m$ and $C_m$. Algorithmn and Algorithmg representing local clustering algorithm and global clustering algorithm respectively, are defined by the specific FPL algorithm. In the classic work, FedPL [8], Algorithm is averaging and Algorithmg is FINCH [23], so $J_m^k = 1$. In FedPLVM [30], both Algorithm and Algorithmg are FINCH, which is called dual-level prototype generation. The main purpose is to alleviate the training inequality between easy domains and hard domains. In our study, both Algorithm and Algorithmg are FINCH*, where the prototypes are assigned weights, and the weight of each cluster is the sum of the weights of all its prototypes. We share a similar objective with FedPLVM, but we focus more on the details of the clustering process."}, {"title": "C. FedPLCC: FedPL with Convergent Clusters", "content": "We trained a ResNet10 [6] model on Digit-5 [32] with FedPLVM [30] for several epochs, then used t-SNE to visualize the local features, local prototypes, and global prototypes of label 0 at a specific epoch, see the upper row in fig. 2. We have balanced the number of training samples among all clients so that the large differences between clusters do not originate from disparities in the size of local datasets. In fact, a small number of samples might significantly differ from others in the same domain after feature extraction, forming outliers. Due to the parameter-free nature of the FINCH [23] clustering algorithm, there is a likelihood of forming outlier clusters in such cases. To address this issue, we assign a weight to each prototype. Specifically, whether for local clustering or global clustering, we record the number of samples represented by each prototype when executing the FINCH algorithm and perform a normalization by sum before applying them to loss function calculation, preventing the data volume from affecting the gradient descent step size.\nA more important observation here is that we should not expect the model's output features to resemble every prototype. In heterogeneous federated learning, there may be significant differences between domains. However, if features extracted from different domains converge into independent clusters, the model's accuracy can still be improved. Conversely, forcibly bringing features from highly disparate domains into a single cluster may actually reduce the model's accuracy. The difference between the objectives of the two methods is shown in the lower row in fig. 2. We introduced a top-k mechanism when calculating the intra-class loss function. Specifically, we set a hyper-parameter $\\phi \\in (0,1]$, which indicates that after sorting the prototypes by their weighted similarity to the current feature, we only take a top proportion ($\\phi$) of terms to participate in the loss function calculation. The lower-ranked terms, either not similar to the current feature or represent fewer samples, should not be forcibly aligned with the current feature.\nBased on the two ideas above, we redesigned the loss function for FedPLCC:\n$L_{local} = L_{CE} + \\lambda_1 L_{contra} + \\lambda_2 L_{corr},$                                              (4)\nwhere $\\lambda_1$ and $\\lambda_2$ are hyper-parameters balancing the label information loss and contrastive information loss.\nThe first term is a Cross-Entropy (CE) loss [3] to train the classifier to yield correct prediction results, which can be formulated as:\n$L_{CE} = -\\sum_{(x_i,Y_i) \\in D_k} log(f(h(x_i))),$                                             (5)\nThe second term is the contrastive term that pushes prototypes with different labels farther:\n$L_{contra} = - log \\frac{\\sum_{g_{y_i} \\in G_{Y_i}} exp(s_\\alpha(h(x_i), g_{y_i})/T) \\times W_{g_{y_i}}}{\\sum_{g \\in G} exp(s_\\alpha(h(x_i), g)/T) \\times W_g},$                (6)\nwhere\n$s_\\alpha (h(x_i), g_m) = \\frac{h(x_i)}{||h(x_i)||} \\cdot \\frac{g_m}{||g_m||},\\alpha,$                                              (7)\nis called a-sparsity [30], $G = \\{G_m\\}_{m=1}^M$ is the set of all global clustered prototypes, T is the temperature hyper-parameter controlling the strength of the similarity concentration [29], and $W_{g_m}$ is the weight of a prototype.\nThe third term is the contrastive term that gathers the more similar prototypes with the same label:\n$L_{corr} = -\\sum_{g_{y_i} \\in G_{Y_i}}^{top\\ k} (s_\\alpha(h(x_i), g_{y_i}) \\times W_{g_{y_i}}),$                                    (8)\nwhere $\\sum^{top\\ k}$ is a function which takes the hyper-parameter $\\phi$ and a set S of N elements as input and returns the sum of the largest $[\\& \\times N]$ elements of S.\nAfter local training, clients generate local prototypes and corresponding local weights with FINCH*, and the server aggregates them and generates global prototypes and corresponding global weights:\n$(P_m^k, W_{local,k}) \\leftarrow^{\\ FINCH^*} (\\{h(x_i) | (x_i, y_i) \\in D_n\\}, \\{1\\}_{i=1}^{N_m}), \\ (9)$\n$(G_m, W_{global}) \\leftarrow^{\\ FINCH^*} (P_m, W_{local}), \\ (10)$\nwhere $W_{local,k} = \\{W_{p_m}\\}_{j=1}^{J_m}$ is the local weights of client k, $W_{local} = \\{W_{local,k}\\}_{k=1}^K$ is the aggregated local weights, and $W_{global} = \\{W_{g_m}\\}_{j=1}^{C_m}$ is the global weights. FINCH* takes vectors and their weights as inputs, clusters the vectors with FINCH and sums up the weights of vectors in each cluster as the weight of the cluster. Finally, the server normalizes the weights by sum:\n$W_{g_m} = \\frac{W_{g_m}}{\\sum_{j'=1}^{C_m} W_{g_{m}}},$                                                (11)"}, {"title": "IV. EXPERIMENTS", "content": "We evaluate our algorithm on Digit-5 [32], Office-10 [4] and DomainNet [22], consisting 5, 4 and 6 different domains respectively. As for DomainNet, we follow the setup in FedPCL [27] using a 10-class subset. In each experiment, we use one client for each domain. For Digit-5 and Office-10, each client possesses about 300 training samples. For DomainNet, each client possesses about 400 training samples. Clients always possess all testing samples.\nWe compare our algorithm with classic FL methods: FedAvg [16], FedProx [14], MOON [12], FCCL+ [7] and FedPL methods: FedProto [26], FPL [8] and FedPLVM [30].\nFor all algorithms, we employ the ResNet10 [6] as the backbone model and configure the feature vectors' dimension to 512. We use an SGD optimizer with lr = 0.01, momentum = 0.9, weight_decay = 1e-5 for optimization. Global communication rounds are fixed at T = 50 for Digit-5 and DomainNet and T = 100 for Office-10, and each local training epoch consists of E = 10 iterations.\nAs for hyper-parameters, we maintain $\\lambda_2 = 10 \\lambda_1$, $\\Phi = 0.5$, $\\tau = 0.07$, $\\alpha = 0.5$, and set $\\lambda_1 = 100$ for Digit-5, $\\lambda_1 = 20$ for Office-10 and $\\lambda_1 = 1$ for DomainNet. We have done ablation experiments in section IV-B to explain the chosen hyper-parameters. We have also slightly adjusted some hyper-parameters in other methods so that they could perform the best under our experiment settings. Our experiment results are presented in tables II to IV, and the average accuracy metric in each communication epoch during the training phase is shown in fig. 3."}, {"title": "A. Performance Comparison", "content": "Our method demonstrates superior average accuracy across all datasets and consistently achieves higher accuracy than existing state-of-the-art methods on numerous sub-datasets. The significant accuracy improvements observed on the Digit-5 and Office-10 datasets are particularly noteworthy. For instance, on the Office-10 dataset, the variance among the four test results is merely 11.97%, underscoring the robustness and effectiveness of our approach across datasets of varying difficulty levels."}, {"title": "Algorithm 1 FedPLCC", "content": "Input: Communication rounds T, local training epochs E, number of classes M, number of clients K, private dataset Dk\nOutput: Global model $w^{T+1}$\nServer Aggregation:\nfor t = 1,2,..., T do\nfor k = 1, 2, ..., K do\nCollect local models, local prototypes, and local prototype weights by\n$w_{k,E+1}, P_k, W_{local,k} \\leftarrow Local Update (k, w^t,G,W_{global})$\nend for\nfor m = 1,2,..., M do\nAggregate collected prototypes $P^m$ and weights $W_{local}$\nGenerate global prototypes $G^m$ and weights $W_{global}$ by eq. (10)\nNormalize global prototypes $W_{global}$ by eq. (11)\nend for\nAggregate global model $w^{t+1} = \\frac{1}{K} \\sum_{k=1}^K w_{k,E+1}$\nend for\nReturn $w^{T+1}$\nLocal Update(k, $w^t$,G,$W_{global}$):\n$w_{k,1} \\leftarrow w^t$\nfor e = 1, 2, ..., E do\nUpdate $w_k^{e+1}$ from $w_k^{e}$ using G, $W_{global}$ by eq. (4)\nend for\nCompute local feature vectors \\{h(xi)|(xi, Yi) \\in Dm\\}\nfor m = 1,2,..., M do\nGenerate local prototypes $P_m^m$ and weights $W_{local,k}^{local}$ by eq. (9)\nend for\nReturn $w_k^{e,E+1}, P_k, W_{local,k}$\nHowever, our method's performance improvement on the DomainNet dataset is more modest. We attribute this to the local models' reaching their limits regarding feature extraction capabilities. Consequently, federated prototype learning algorithms that leverage contrastive feature information cannot achieve substantially better results in this scenario."}, {"title": "B. Ablation Study", "content": "We performed a series of ablation studies using the three datasets (mainly the Digit-5) to evaluate each component's effectiveness in our proposed method.\n1) Impact of key components.: We conducted ablation experiments on several key components of our algorithm using the Digit-5 dataset, and the results are presented in table V. Removing both contrastive information losses renders our method equivalent to FedAvg, which serves as the baseline for this study.\nSubsequently, we evaluated the model's performance relative to the baseline when modifying or removing each loss individually. Retaining only $L_{contra}$ led to a notable 12.426% increase in accuracy. Conversely, retaining only $L_{corr}$ resulted in lower accuracy compared to FedAvg, exacerbated when incorporating the $\\phi$. This outcome underscores that focusing solely on increasing intra-class similarity impedes the stable formation of prototypes, thus hindering the learning of contrastive information. Our other experiments underscored the importance of weighting and prototype selection in enhancing model accuracy."}, {"title": "2) Impact of $\\alpha$ and $\\tau$.", "content": "FedPLVM [30] includes two hyper-parameters, $\\alpha$ and $\\tau$, in its inter-class loss component. $\\alpha$ dictates the alignment strength between local features and the global prototype, and $\\tau$ determines the strength of prototype aggregation for the same label. FedPLVM discusses these two hyper-parameters and determines that $\\alpha = 0.25$ and $\\tau = 0.07$ are the optimal values. We also experiment with our method."}, {"title": "3) Impact of $\\phi$.", "content": "The parameter $\\phi$ determines the proportion of prototypes considered relevant to the current feature when calculating the inter-class loss. Hence, as the domain gap between different clients decreases, a larger $\\phi$ is preferable, and vice versa. We believe conducting ablation experiments for each dataset could yield better-performing $\\phi$, but this does not align with our expectations. We fix this parameter at 0.5 to ensure consistent performance of our method across different datasets, including those not explicitly tested in our experiments. The experiments in section IV-B1 have already demonstrated that the top-k mechanism improves the results, which is sufficient for our purposes here."}, {"title": "4) Impact of $\\lambda_2$.", "content": "$\\lambda_2$ governs the relative strength between push and pull forces in our method. To ensure adaptability across different datasets, we conduct ablation experiments on the ratio $\\frac{\\lambda_2}{\\lambda_1}$ rather than on $\\lambda_2$ alone. The experiments are specifically carried out on the Digit-5 dataset, and the results are depicted in fig. 5. Therefore, we set $\\lambda_2 = 10 \\lambda_1$ in our experiment."}, {"title": "5) Impact of $\\lambda_1$.", "content": "The parameter $\\lambda_1$ governs the ratio of contrastive information to label information in our method. We acknowledge that the optimal value of $\\lambda_1$ can vary across different datasets. Therefore, we conducted ablation experiments across all three datasets in our study to ensure our method performs optimally and to accurately assess its effectiveness."}, {"title": "V. CONCLUSION", "content": "In this paper, we systematically compare and summarize previous works in Federated Prototype Learning while highlighting opportunities for enhancing the clustering process. Our novel approach, FedPLCC, extends the dual-level clustering framework pioneered by FedPLVM. Through strategic weighting and selecting prototypes, as well as a redesigned loss function, our method mitigates the risk of cross-domain feature alignment, thereby facilitating a more organic convergence into distinct clusters. Experiments conducted on multiple datasets demonstrate the superiority of our approach."}]}