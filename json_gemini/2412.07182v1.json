{"title": "An Enhancement of CNN Algorithm for Rice Leaf\nDisease Image Classification in Mobile Applications", "authors": ["Kayne Uriel K. Rodrigo", "Jerriane Hillary Heart S. Marcial", "Samuel C. Brillo", "Khatalyn E. Mata", "Jonathan C. Morano"], "abstract": "This study focuses on enhancing rice leaf disease image classification algorithms, which have traditionally\nrelied on Convolutional Neural Network (CNN) models. We employed transfer learning with MobileViTV2_050 using\nImageNet-1k weights\u2014a lightweight model that integrates CNN's local feature extraction with Vision Transformers'\nglobal context learning through a separable self-attention mechanism. Our approach resulted in a significant 15.66%\nimprovement in classification accuracy for MobileViTV2_050-A, our first enhanced model trained on the baseline\ndataset, achieving 93.14%. Furthermore, MobileViTV2_050-B, our second enhanced model trained on a broader rice\nleaf dataset, demonstrated a 22.12% improvement, reaching 99.6% test accuracy. Additionally, MobileViTV2-A\nattained an Fl-score of 93% across four rice labels and a Receiver Operating Characteristic (ROC) curve ranging\nfrom 87% to 97%. In terms of resource consumption, our enhanced models reduced the total parameters of the\nbaseline CNN model up to 92.50%, from 14 million to 1.1 million. These results indicate that MobileViTV2_050 not\nonly improves computational efficiency through its separable self-attention mechanism but also enhances global\ncontext learning. Consequently, it offers a lightweight and robust solution suitable for mobile deployment, advancing\nthe interpretability and practicality of models in precision agriculture.", "sections": [{"title": "1) Introduction:", "content": "Rice serves as a staple food for many people worldwide, making the cultivation of rice remains critical. In the\nPhilippines, millions of Filipino farmers rely on rice as the primary source of income, and it also supports\nenvironmental sustainability [1]. In the 2022/2023 period, global rice consumption has reached 520.4 million metric\ntons, underscoring its importance and the increasing reliance on rice due to population growth [2]. Ensuring rice\nproduction security is challenging due to environmental factors, specifically the spread of diseases observable through\nleaf quality. These diseases typically appear during the growth phase of the plant, especially the heading stage, leading\nto significant loss in rice crop yields [3]. Fortunately, advancements in artificial intelligence (AI), particularly in\nmachine learning algorithms that are suitable for mobile devices, enable high-quality inputs to perform human-like\ntasks. Rice leaf disease detection leverages AI using images processed through algorithmic techniques. Several studies\nindicate that convolutional neural networks (CNN) are the preferred choice for analyzing plant leaf diseases, including\nrice leaves [4]. Despite the advantages of convolutional neural networks (CNNs), achieving optimized models for rice\nleaf disease classification remains challenging. Recent systematic reviews identified critical gaps, highlighting the\nneed for robust, computationally efficient architectures, lightweight models with fewer parameters, and the\nimplementation of transfer learning to overcome the limitations of training deep learning methods from scratch.\nAdditionally, there was a demand for models that offer explainable diagnoses to address the \"black box\" nature of\nCNNs, enhancing the interpretability of predictions [5]. Researchers explored attention mechanisms to improve model\ninterpretability and efficiency, with one study utilizing an attention-based neural network and Bayesian optimization\nto achieve a test accuracy of 94.65%, outperforming traditional CNN architectures such as VGG16 and ResNet50 [6].\nOther studies employed hybrid CNN and Vision Transformer (ViT) models to classify five primary rice diseases,\nthough these models were manually constructed and limited in scope [7]. To address these limitations, newer models\nlike MobileViT were developed, combining CNN and ViT architectures to provide global information processing\nsuitable for mobile devices and demonstrating superior performance with fewer data points [8, 9, 10]. The plant-based\nMobileViT (PMVT) further enhanced disease classification by focusing on significant features, though it suffered\nfrom slower inference speeds and higher memory requirements, limiting its deployment on resource-constrained\nmobile devices [11]. In response, the separable self-attention method led to MobileViTV2, which reduced time\ncomplexity from quadratic to linear, enhancing processing speed and memory efficiency for mobile deployment [12].\nHowever, MobileViTV2 had not yet been applied to rice leaf disease classification, presenting a field for exploration.\nIn this context, researchers utilized a CNN model with five convolutional and two dense layers, where it has a 78%\naccuracy. It encounters overfitting and classification challenges with diverse textures using datasets from Kaggle [13,\n14]. Utilizing the Timm library, the researchers trained the MobileViTV2 model under the MobileViTV2_050 variant.\nThis leverages its separable self-attention mechanism to efficiently learn global image contexts while maintaining\nlower memory consumption and time complexity, thereby enabling effective deployment on mobile edge devices\nthrough validated evaluation metrics."}, {"title": "2) Methodology:", "content": "A standardized hybrid deep learning approach was applied to enhance the traditional rice leaf disease image\nclassification. It integrates the local feature extraction from the Convolutional Neural Network (CNN) with the global\ncontext learning capabilities of the Vision Transformer (ViT). A lightweight MobileViTV2 model was evaluated\nagainst conventional CNN models to determine its efficiency and accuracy."}, {"title": "a. Research Design", "content": "An experimental research design was employed to enhance a standard convolutional neural network (CNN) from\n1] by integrating the MobileViTV2 architecture, addressing the novel application of MobileViTV2 in rice leaf disease\nclassification. The study involved two training sessions: first, the MobileViTV2-enhanced model, labeled as\n`mobilevitv2_050-A`, was trained using the same baseline dataset as the original CNN from [1] to directly assess\nperformance improvements under identical data conditions; second, the enhanced model, labeled\n`mobilevitv2_050-B`, was retrained using a broader Kaggle-sourced dataset verified by a university biologist to\nevaluate generalizability across diverse rice leaf disease images. Comparative analyses were conducted in two parts:\nfirstly, comparing the baseline CNN against the MobileViTV2-enhanced model using the same baseline dataset.\nSecondly, comparing the enhanced model's performance on the broader dataset with reported metrics from several\nbenchmark models from other studies. All benchmark models utilize attention mechanisms in rice leaf disease\nclassification, acknowledging that benchmark models were not retrained with similar datasets due to time and resource\nconstraints. The evaluation encompassed classification metrics (accuracy, precision, recall, F1-score) and efficiency\nmetrics (Top 1, Top 5, total parameters, FLOPs, FPS), providing a comprehensive assessment of both performance\nand computational efficiency."}, {"title": "c. Data Acquisition and Cleaning", "content": "The enhanced model utilized two datasets. First, from [1], to have the same dataset in comparison with the\nbaseline CNN model. Second, from [2] which is a broader rice leaf disease dataset from Kaggle, to investigate the\ngeneralizability of the model across several rice leaf disease image datasets. In the first dataset from [1], it consists\nof 4 classes. In the second dataset, it consists of 18,445 rice leaf disease images, encompassing\nten distinct categories including healthy leaves and various paddy disease types. The images\nwere sourced from Kaggle, a public data science repository. The proposed dataset was validated by a university\nbiologist following Philippine Rice Research Institute (PhilRice) guidelines to ensure accurate disease label\nclassification. During the data cleaning phase, nomenclature corrections were implemented, such as renaming the\n'neck blast' folder to 'panicle blast' to address misleading image categorization."}, {"title": "d. Model Architecture: MobileViTV2 Enhancement", "content": "Overview of MobileViTV2\nillustrates the MobileViTV2 architecture, and the block utilized in this study, designed for efficient\nrice leaf disease classification. The model processes input images with dimensions of 3 channels \u00d7 224 \u00d7 224\npixels. The initial convolutional layer (Conv2d) reduces the spatial dimensions to 16 channels \u00d7 112 \u00d7 112 pixels,\nfollowed by a Sigmoid Linear Unit (SiLU) activation and batch normalization. The backbone comprises\nalternating Bottleneck Block and MobileViTV2Block layers, which preserve local feature integrity while\nincrementally expanding channel dimensions from 16 to 256 and reducing spatial dimensions through a\nhierarchical sequence (112 \u2192 56\u219228 \u2192 14\u2192 7/8 pixels). This structure facilitates multiscale feature extraction\nessential for capturing complex patterns. Central to the backbone are the MobileViTV2 Transformer Blocks,\nwhich integrate depth-wise separable convolutions and point-wise convolutions to efficiently process feature\nmaps. Each Transformer Block unfolds the spatial dimensions into vector representations, which are then\nprocessed by a separable self-attention mechanism. Unlike traditional self-attention that relies on dot-product\noperations, MobileViTV2 employs element-wise multiplication to maintain computational efficiency. The\nattention mechanism is embedded within a feed-forward network and incorporates skip connections to ensure\nstable gradient flow and enhance model robustness."}, {"title": "Incorporation of Separable Self-Attention Mechanism", "content": "shows the separable-self attention mechanism. This serves as the underlying attention mechanism\nwithin the MobileViTV2 block from [12]. According to the study, it was inspired by Multi-Head Attention from\nTransformer Encoder Block. However, the enhancement offers a more efficient approach when computing\nattention scores by reducing the complexity from quadratic $O(k\u00b2)$ to linear $O(k)$, where k represents the number\nof input tokens. The process begins with three parallel paths: input (I), where it converts each token into a\nsimplified representation; key (K), where it creates keys for cross-check matching of information; and value (V),\nwhere it prepares values for information transfer. Secondly, it underwent a latent node approach, wherein instead\nof comparing the tokens from every path to each other, it shall be redirected to a single reference point called\n\"latent node\" (L). This offers a centralized coordinate, reducing its computational complexity. Thirdly, as a result,\nit will generate a context score (cs), which contains context information about the token, which will then be\npassed to a Soft Max activation function to reduce their scale. Fourth, the MobileViTV2 transformer block\ncombines all key information from context scores into a context vector (cv), which captures the important\nfeatures from the entire sequence. Lastly, the transformed versions of input (xV) are combined with the context\nvector. A final transformation produces the output with Rectified Linear Unit (ReLU) added to add non-linearity.\nOverall, the purpose of these processes was to make attention mechanisms efficient for longer sequences. These\nare present within the transformer block of MobileViTV2."}, {"title": "Linear Transformer Block Modules", "content": "This transformer block implements a linear self-attention mechanism by instead using Soft Max-based\nattention, where it uses pairwise interactions across tokens $Q(K)$, the Linear Transformer employed a kernel-\nbased formulation $Q(KTV)$, reducing the time complexity from 0($n\u00b2$) to O(n). While the self-attention\nmechanism has been changed, the feed-forward network remains unchanged, giving n linear transformations and\na ReLU activation function."}, {"title": "Bottleneck Block", "content": "The bottleneck residual block aims to reduce computational complexity while preserving performance\nthrough a three-layer design. The block used a 1 \u00d7 1 convolution to decrease the number of channels to 1, creating\na bottleneck, followed by a 3 \u00d7 3 convolution using a lower dimensional space. Then it applied 1 \u00d7 1 convolution\nto restore the original dimensions. Alongside this, it applied a skip connection to serve as a residual path, where\ninformation is retained despite reducing its complexity. It simply aims to have stable training regardless of the\nnetwork depth."}, {"title": "e. Experimental Procedures", "content": "The researchers implemented the model using the Timm library for computer vision, specifically leveraging\nthe MobileViTv2_050 variant with 1.4 million parameters to ensure lightweight support for edge devices. The\nresearchers configured the model via timm.create_model(), initializing it with pretrained ImageNet-1k weights and\nsetting num_classes to 10 to match the rice leaf disease categories. For dataset preprocessing, it follows the\nfollowing steps: first, they split the data into 80% training and 20% validation; second, they resized all rice images\nto 224\u00d7224 pixels and converted them to PyTorch tensors; lastly, they normalized the pixel values using ImageNet\nmean and standard deviation. During training, they employed the Adam optimizer with a learning rate of 0.001 and\nutilized the ReduceLROnPlateau scheduler to adjust the learning rate based on validation loss, alongside an Early\nStopping mechanism to terminate training when validation loss ceased to improve. The training of the enhanced\nmodel will be in two phases: first, by using the same dataset used by the baseline CNN model to identify direct\nimprovements in comparison with the baseline model; second, by using the new dataset publicly available in\nKaggle, where it consists of datasets with broader features and rice labels. The model is trained for a maximum of\n10 to 20 epochs with a batch size of 32, completing the process in hours. After training, they saved the\nMobileViTv2_050 model as a PyTorch .pth file and exported it to the ONNX format to ensure broader\ncompatibility with mobile devices."}, {"title": "f. Data Analysis and Validation Techniques", "content": "The researchers evaluated the trained model's accuracy using metrics such as accuracy and loss scores,\nclassification reports, Receiver Operating Characteristic (ROC) curves, and confusion matrices. The accuracy\nscore as seen in equation (1) assessed overall predictive correctness, while the loss score in equation (2) measured\nthe discrepancy between true and predicted labels on both training and validation datasets. Classification reports\nprovided precision seen in equation (3), recall seen in equation (4), and F1-scores seen in equation (5) for each\nrice leaf class, enabling a comprehensive analysis of predictions on unseen data. ROC curves illustrated the\nmodel's diagnostic performance across various thresholds, and confusion matrices displayed true positives, false\nnegatives, false positives, and true negatives for each class. Top 1 and Top 5 accuracy, seen in equation (6) and\nequation (7), were added to evaluate the true class among the first and first 5 predictions. Additionally, the\nresearchers assessed algorithm performance by calculating floating point operations per second (FLOPs), seen in\nequation (8), and frames per second (FPS) to determine inference speed, and measured memory consumption to\nensure efficiency on edge devices. These evaluations collectively determined the model's accuracy and\noperational performance."}, {"title": "g. Application Platform for Plant Disease Detection", "content": "The application platform used for integrating the model was a Kotlin-based ONNX Runtime application.\nONNX Runtime offers a variety of options to integrate machine learning models, including the trained\nmobilevitv2_050 model to mobile applications, and can measure inference time in real time. The researchers\nutilized its given mobile image classification template deployed in the Android operating system. Part of its user\ninterface design would include that while the camera is open, it will continuously detect rice leaf disease images\nand scans for potential rice labels in real time. For testing and deployment, it was tested on images that do not\nbelong to the dataset, such as sample images from International Rice Research Institute documents, and it\naccurately identifies the disease, given the correct distance used."}, {"title": "3) Results and Discussion", "content": "Table 3 shows the results that came from the baseline and the enhanced model. The mobilevitv2_050-A, a\nvariant of the MobileViTV2 model trained with baseline, demonstrates a superior performance across all evaluation\nmetrics when compared to the baseline CNN model. The enhanced model surpasses the validation accuracy of the\nbaseline model by 15.66%, while it also exhibits notable improvements in disease detection capabilities across all\nrice labels, as it was higher and offers a lower accuracy interval. The enhanced model also shows exceptional\nprecision, recall, and F1-Scores, as it received a consistent output of 93% to 94%. These performance\nimprovements establish MobileViTV2's effectiveness in rice leaf disease image classification, presenting a robust\nsolution that enhances the reliability and accuracy of disease diagnosis in agricultural management and crop\nprotection systems."}, {"title": "Comparison of MobileViTV2_050-B vs. Other Backbone Models", "content": "To further enhance the generalizability of the proposed model, a comparative analysis of related studies was\nused to compare distinctive results in lightweight model development with integrated attention mechanisms. shows the ROC curve and confusion matrix of mobilevitv2_050-B. It implies that the mobilevitv2 architecture\nunder the enhanced dataset, containing 10 labels, provides remarkable accuracy, given the highly pointed ROC\ncurve towards the upper left and a consistent diagonal lining in the confusion matrix. Table 4 demonstrates that the\nmobilevitv2_050-B achieves optimal performance with minimal computational overhead, delivering 99.6% top 1\naccuracy while maintaining the lowest floating-point operations. The mobilevitv2_050-B variant, with its reduced\nparameter count from 14M to 1.1M, establishes itself as a lightweight CNN-ViT hybrid architecture that balances\ncomputational efficiency with high accuracy performance."}, {"title": "4) Conclusion:", "content": "This study presented an enhanced approach to baseline CNN-based rice leaf disease classification by integrating\nMobileViTV2. This novel method combines CNN's feature extraction capabilities with global context learning\nfrom Vision Transformers and introduces a separable self-attention mechanism, reducing computational\ncomplexity. The enhanced model demonstrated significant improvements in accuracy and resource efficiency\ncompared to the baseline CNN, which is widely used in existing studies. By addressing key research gaps such as\nthe CNN model's \"black box\" nature, limited global context understanding, and high computational demands.\nMobileViTV2 not only improves classification performance but also optimizes efficiency for resource-constrained\ndevices. This results in a lightweight and robust solution for rice leaf disease image classification in precision\nagriculture. The implications of these findings are substantial for precision agriculture. The MobileViTV2 model's\ncomputational efficiency makes it highly suitable for deployment on various mobile edge devices, enabling\nfarmers, including those with visual impairments or limited experience, to accurately classify rice leaf diseases in\nthe field. This advancement can lead to increased rice production and reduce unnecessary disease treatments."}]}