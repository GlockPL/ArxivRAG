{"title": "From Bird's-Eye to Street View: Crafting Diverse and Condition-Aligned Images with Latent Diffusion Model", "authors": ["Xiaojie Xu", "Tianshuo Xu", "Fulong Ma", "Yingcong Chen"], "abstract": "We explore Bird's-Eye View (BEV) generation, converting a BEV map into its corresponding multi-view street images. Valued for its unified spatial representation aiding multi-sensor fusion, BEV is pivotal for various autonomous driving applications. Creating accurate street-view images from BEV maps is essential for portraying complex traffic scenarios and enhancing driving algorithms. Concurrently, diffusion-based conditional image generation models have demonstrated remarkable outcomes, adept at producing diverse, high-quality, and condition-aligned results. Nonetheless, the training of these models demands substantial data and computational resources. Hence, exploring methods to fine-tune these advanced models, like Stable Diffusion, for specific conditional generation tasks emerges as a promising avenue. In this paper, we introduce a practical framework for generating images from a BEV layout. Our approach comprises two main components: the Neural View Transformation and the Street Image Generation. The Neural View Transformation phase converts the BEV map into aligned multi-view semantic segmentation maps by learning the shape correspondence between the BEV and perspective views. Subsequently, the Street Image Generation phase utilizes these segmentations as a condition to guide a fine-tuned latent diffusion model. This finetuning process ensures both view and style consistency. Our model leverages the generative capacity of large pretrained diffusion models within traffic contexts, effectively yielding diverse and condition-coherent street view images.", "sections": [{"title": "I. INTRODUCTION", "content": "The emerging era of autonomous driving hinges on the adoption of sophisticated technologies and representations to ensure optimal navigation and decision-making. Among these, the bird's-eye view (BEV) holds a unique position. By offering a top-down, map-like representation, the BEV provides invaluable insights into the immediate environment, capturing pertinent obstacles and hazards.\nWhile BEV perception [1], [2], [3] has been a focal point in recent studies, promising to bridge the transformation between street-level views and overhead perspectives, BEV generation-specifically the synthesis of realistic street-view images from a predefined BEV semantic layout-offers untapped potential.\nAt its core, BEV generation [4] translates a semantic layout, which captures a traffic scenario, into tangible street-view images. This translation facilitates an enhanced visu-alization of traffic scenarios in a real-world setting, making the abstract more accessible. One of the most compelling applications of BEV generation is the intuitive interface it offers for traffic scene visualization and modification. BEV generation allows human operators and system designers to modify a layout effortlessly, producing corresponding street-view images via generative models. This not only streamlines the training of autonomous systems but also serves as an effective testing and validation tool.\nBEVGen [4] represents a pioneering effort in addressing the BEV generation problem. Within its BEV representa-tions, map components are bifurcated into two categories: vehicles and roads. The model employs an autoregressive transformer [5] with a spatial attention design to comprehend the relationship between camera and map perspectives. While BEVGen sets a baseline by generating multi-view images consistent with its map perspective, it doesn't consistently ensure condition coherence due to its implicit encoding mechanism.\nIn contrast to the previous method, our proposed method disentangles the view transformation and image generation processes. The view transformation phase focuses on learn-ing the shape correspondence between map and camera perspectives. Here, to project the BEV map onto camera views using camera parameters, we assign height from a prior distribution to each BEV map segment. Using this projection as a preliminary estimate, a convolutional network"}, {"title": "II. RELATED WORK", "content": "Conditional image generation: The field of conditional image generation has seen notable advancements recently, with models predominantly conditioned on text [7], [8] or speech [9] inputs. Varied formats, such as class conditions [10], sketches [11], style [12], and distinct human poses [13], can convey the envisaged image specifications. Furthermore, several scholars have explored methodologies with high level representations, including generating images from semantic masks [14] or translating intricate constructs like scene graphs [15] and bounding boxes [16] into equivalent semantic masks. Diverging from these mentioned paradigms, our emphasis lies on the bird's-eye view map. Though akin to a semantic segmentation map, it offers a perspective distinct from the resulting image, which is seldom explored in earlier studies.\nImage diffusion models: Originally proposed by Sohl-Dickstein et al. [17], Image diffusion models have found recent applications in image generation [18]. The Latent Diffusion Models(LDM) [6] execute diffusion in the latent image space [19], optimizing computational efficiency. Text-to-image diffusion models, by encoding textual inputs into latent vectors using pretrained language models like CLIP [20], set new benchmarks in image generation. Glide [21] stands out as a text-driven diffusion model for both image creation and editing. Stable Diffusion scales up the concept of latent diffusion [6], and Imagen [8] takes a distinct approach by diffusing pixels through a pyramid structure, bypassing latent imagery. We employ Stable Diffusion as our foundational pretrained model. Through fine-tuning, we adapt it to various viewpoints and driving scenes.\nBEV perception and generation: Recent growth in large 3D datasets in autonomous driving [22], [23], [24] has pro-pelled studies on map-view perception. Given the disparity between the coordinate frames of inputs and outputs, this domain poses challenges. While inputs derive from calibrated cameras, outputs are rasterized onto a map. A prevalent method assumes a mostly planar scene, simplifying image-to-map transformations via homography [25]. However, this can create artifacts for dynamic entities like vehicles. As a solution, some studies [26], [27] utilize depth and semantic maps to present objects in BEV. Alternatively, other methods [2], [3] bypass explicit geometric modeling to generate map-view predictions directly from images.\nAs its counterpart, generating from a BEV map layout remains relatively unexplored. BEVGen [4] pioneered this domain, employing an auto-regressive transformer to encode the connection between image and BEV representations. In contrast to BEVGen, our approach leverages a large, pretrained diffusion model as the backbone and finetunes it using driving scene images."}, {"title": "III. METHOD", "content": "The objective of BEV generation is to generate multiple camera-view images from a semantic BEV layout. Earlier studies have represented the BEV layout in either rasterized [2] or vectorized forms [28]. In this work, we favor the rasterized representation due to its aptness for creating from projections of 3D bounding boxes onto local street maps [2], or directly from driving simulation frameworks [29]. Con-sequently, the BEV layout is denoted by $B\\in \\mathbb{R}^{H_b\\times W_b\\times c}$, where c represents the number of map element categories, such as vehicles and roads.\nGiven the BEV map B and n camera views $(K_i, R_i,t_i)_{i=1}^{n}$, where $K_i$, $R_i,t_i$ denotes the intrinsics, extrinsics rotation and extrinsics translation of the $i_{th}$ camera, our goal is to generate n corresponding images in camera view $1 = \\{I_i \\in \\mathbb{R}^{H\\times W\\times 3} | i = 1, ..., n\\}$.\nAs depicted in Fig. 2, our pipeline operates in two stages. Initially, the BEV's semantic information is projected into the camera view leveraging camera parameters, under a height assumption. This shape is subsequently refined using a CNN.\nIn the succeeding stage, a pre-trained UNet undertakes the backward diffusion process [6], where Gaussian noise is progressively eliminated. This UNet receives the polished semantic information coupled with the prompt as condition-ing inputs. Furthermore, to ensure accurate viewpoints across various camera perspectives, we fine-tune the network."}, {"title": "A. Stage I: Neural View Transformation", "content": "Taking inspiration from [30], we treat the BEV-to-camera view transformation as an image translation task, where the input and output share a pronounced spatial correspondence. We decompose this transformation into two phases: initial setup using camera parameters and shape refinement via a neural network.\nInitial projection with camera parameters: For any world coordinate $X \\in \\mathbb{R}^3$, the perspective transformation describes its corresponding image coordinate $x \\in \\mathbb{R}^3$ in the view of the ith camera by\n$x = K_iR_i(X - t_i)$ (1)\nin homogeneous coordinates.\nThe lack of precise height data renders the world coor-dinates of BEV map data ambiguous, necessitating height estimation. While Inverse Perspective Mapping (IPM) tech-niques [31] operate under the premise of a flat ground, this assumption can introduce distortions for objects of varied heights, such as buildings and vehicles. Given our focus on roads and vehicles, we retain this simplified assumption for roads.\nFor vehicles, we posit that their height adheres to a prede-termined distribution. Practically speaking, each vehicle on the BEV map is allocated a height randomly sampled from $U(1.5, 2)$, offering a plausible initial height approximation. With the estimated heights for roads and vehicles in place, the BEV map is projected into camera views using Equ. 1, given the camera parameters.\nShape refinement network: Through height estimation and projection, we obtain preliminary semantic maps in the camera view. Nonetheless, this simplistic initialization fails to preserve the intricate shapes of map elements accurately. Given that vehicles are rendered on the BEV map using their true 3D bounding boxes as described in [2], our projection approach results in the vehicle appearing as a cube from the camera's viewpoint. Hence, a shape-refinement post-processing step is imperative.\nThe initial projection yields a low-resolution estimate. To address this, we employ an enhanced UNet architecture with residual connections [32]. This network bridges the shape discrepancy between the estimated and the true semantic maps. Functioning as an upsampling module, it outputs high-resolution semantic maps with finer geometry. These refined maps subsequently serve as conditional inputs to the image generator. The contribution of this network to the final image generation outcome is illustrated in Fig. 3."}, {"title": "B. Stage II: Street Image Generation", "content": "We utilize Stable Diffusion, which is a strong pretrained image generator based on latent diffusion [6] framework, as our generative backbone. In this section we discuss how the conditional generation mechanism works and how to adapt the large pretrained model to our driving domain.\nConditional generation with latent diffusion model:\nDiffusion models can be conceptualized as a uniformly weighted sequence of denoising autoencoders, given by $\\epsilon_\\theta(x_t, t); t = 1 ...T$, These autoencoders aim to predict a denoised version of their input $x_t$, where $x_t$ represents a noisy variant of the original input x. This leads to the following objective:\n$L_{DM} = E_{x,\\epsilon~N(0,1),t} [||\\epsilon - \\epsilon_\\theta(x_t, t)||_2]$ (2)\nwith t uniformly sampled from {1,...,T}.\nAs a large text-to-image diffusion model, latent diffusion introduces CLIP [20] encoder $T_\\theta$ that projects the text prompt y to an intermediate representation $T_\\theta(y)$, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d}})V$ (3)\n, with $Q = W_i^{(2)}$, $K = W_i \\cdot T_\\theta(y)$, $V = W_i \\cdot T_\\theta(y)$. In this context, $p_i(z_t)$ symbolizes a flattened representation of the U-Net at an intermediate stage.\nOur generation task encompasses more than just using the prompt as conditional information. The semantic data transformed from the BEV map serves as a superior control mechanism, given that the resultant image should align spatially with these semantic maps in pixel space. This necessitates a more precise conditioning mechanism for our objective.\nDrawing inspiration from ControlNet [33], which employs zero convolution and a trainable duplicate of the original neural network, our approach manipulates the input con-ditions of neural network blocks. This strategy allows for a more nuanced control over the entire neural network's behavior. We integrate the pretrained ControlNet layers, designed for semantic segmentation, into our architecture (as depicted in Fig. 2). These layers act as conditioning controllers for the image generation process. Even though these semantic control layers were trained on a broader dataset [34], they exhibit robust generalization capabilities in our driving scenarios.\nStreet-view adaption: Our street view adaptation module serves a dual purpose. Firstly, it emulates the driving scene's image style found in our dataset [22]. Secondly, it encapsulates the viewpoints associated with various cameras.\nWhile fine-tuning the diffusion model using Equ. 2 aids in capturing a realistic style specific to driving scenarios, it's crucial to remember that street scenes, when viewed from different camera perspectives, can vary significantly. For instance, when viewed through our front camera, a vehicle directly ahead should align with our car's driving direction. In contrast, the same vehicle observed from a side camera would appear at an angle. Likewise, driveable areas typically extend more prominently when viewed from the front and rear cameras but appear more constrained from the side angles.\nInformed by these insights, we fine-tune our image gen-erator for specific viewpoints. The mechanism for view en-coding is detailed in Fig. 4. Taking a cue from DreamBooth [36], which hones in on a personalized concept (e.g., a particular dog) as a unique prompt, we treat the viewpoint as an abstract concept and introduce a view-specific loss to optimize the diffusion model. This ensures that the viewpoint is distinct from foundational concepts like cars or streets within the prompts. The training loss is articulated as:\n$E_{x,c,e,e',t}[W_t || \\epsilon_\\theta(A_t x + \\sigma_t \\epsilon, c) - x||_2 + \\lambda W_{t'} || \\epsilon_\\theta(A_{t'}x_{view} + \\sigma_{t'} \\epsilon', c_{view}) - x_{view} ||_2]$ (4)"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "A. Dataset\nThe nuScenes dataset [22] is a comprehensive collection encompassing 1,000 diverse street-view scenes, captured under varied weathers, times of day, and traffic conditions. Spanning over 20 seconds, each scene consists of 40 frames, amounting to a total of 40,000 samples within the entire dataset. Designed to provide a 360\u00b0 perspective around the ego-vehicle, the data is derived from six distinct camera views, capturing images from the side, front, and back of the vehicle. Every camera view comes with calibrated intrinsics (K) and extrinsics (R, t) for each timestep. Furthermore, objects, including vehicles, are consistently tracked across frames and annotated using 3D bounding boxes derived from LiDAR data. The dataset is organized into 700 training, 150 validation, and 150 testing scenes.\nFollowing [2], the semantic mask of the vehicle in BEV is rendered with a resolution of (200,200). This is achieved by orthographically projecting the 3D box annotations onto the ground plane, which corresponds to a (100m, 100m) region in the real-world context. The road masks are formulated using the NuScenes map devkit, which integrates both lanes and road segments.\nB. Implementations\nShape refinement network: The shape refinement net-work is a convnet comprising three down-sampling blocks and four up-sampling blocks. It accepts inputs with a res-olution of (56,100) and produces outputs with a resolution of (224,400). Given that the original nuScenes dataset does not include image semantic labels, we employ SegFormer [38] to generate pseudo labels. We train the network for 10 epoches with a learning rate le-7.\nPretrained Stable Diffusion and control module: We utilize the pretrained Stable Diffusion model \"RealisticVi-sion\" available on HuggingFace [39]. The control module is adapted from ControlNet [33], which was originally trained on the ADE20K dataset [34] and captioned using BLIP [40].\nStreet view adaptation module: For each camera view, we use a set of 100 images to train the respective adap-tation module. Our foundational prompts for regularization include \"road\", \"car\", and \"street background\". To specify viewpoints, we use alphanumeric designations (e.g., cam0) to prevent any overlap with existing concepts within the pretrained CLIP text encoder [20]. During finetuning, the image resolution is set at (400, 224). The training extends over 5000 steps with a batch size of 4 and a learning rate set at 1e-4. The rank for LoRA [37] is set to 16.\nC. Results\nQualitative result: We juxtapose our approach against BEVGen [4] and a from-scratch trained latent diffusion model using a transformer architecture, specifically UViT [35]. Notably, our strategy involves finetuning a pre-trained, expansive model, while the other two approaches train their models from the ground up. The results can be observed in Fig. 5.\nOur method showcases superior stability in image qual-ity, and its conditioning mechanism proves to be effective. Both UViT and BevGen employ cross attention to manage conditional information. However, their models occasionally falter due to the absence of explicit spatial relationships between the semantic and the resultant generated images. This makes it challenging for their conditioning mechanisms to consistently function effectively. Concerning image quality and diversity, methods that are trained from scratch tend to be closely tied to specific datasets, often risking overfitting. In particular, the UViT-based diffusion model faces challenges when trained with a limited dataset.\nIn Fig. 6, we showcase additional illustrations un-derscoring the diversity of our generated outcomes. Our methodology effortlessly facilitates the generation of images under various weather scenarios, significantly enhancing the model's adaptability."}, {"title": "V. LIMITATIONS AND FUTURE WORKS", "content": "In the specific setup we've devised, the integration of multiple cameras has the capability to produce a compre-hensive panoramic image that boasts a significantly extended aspect ratio. This is a departure from traditional images and poses a unique challenge. Ideally, the most efficient approach would be to directly generate a panoramic or multi-view image, as this would inherently uphold and maintain the consistency of the view throughout the image. But herein lies the challenge: the vast majority of large-scale image diffusion models available today have been fundamentally trained to cater to standard, more conventional aspect ratios. As a result, these models, when applied to our specific need, fall short. This limitation is clearly demonstrated in Fig. 7. These models face considerable difficulty when tasked with rendering high-quality images that demand a broad and expansive field-of-view.\nRecognizing this gap, our future endeavors will be cen-tered around delving deeper and exploring more robust and effective techniques that can leverage these large image diffusion models to seamlessly produce multi-view images."}, {"title": "VI. CONCLUSION", "content": "We introduced an innovative framework for generat-ing street-view images from a BEV layout by harnessing the power of a robust, pretrained latent diffusion model. Our methodology integrates view transformation, street-view adaptation, and conditional generation. When compared to baseline models trained from scratch, our model excels in terms of image quality, conditioning precision, and diversity."}, {"title": "D. Ablation Studies", "content": "In our research, we carried out ablation studies, specifi-cally honing in on two of our core innovations: the shape refinement process and the street view adaptation technique. The detailed results of these studies can be found in Table. II. The shape refinement process is pivotal in ensuring that map elements are accurately positioned. When the shape within the camera's perspective aligns more semantically, it resonates more effectively with the given prompt. On the other hand, the street view adaptation module plays a crucial role as a style encoder. Its primary function is to make sure that the generated images bear a strong resemblance to those in the training dataset. Moreover, this module greatly assists the image generator by enabling it to achieve proper and accurate orientations for the various map elements."}]}