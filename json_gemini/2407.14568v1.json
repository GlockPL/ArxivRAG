{"title": "SQLfuse: Enhancing Text-to-SQL Performance through Comprehensive LLM Synergy", "authors": ["Tingkai Zhang", "Chaoyu Chen", "Cong Liao", "Jun Wang", "Xudong Zhao", "Hang Yu", "Jianchao Wang", "Jianguo Li", "Wenhui Shi"], "abstract": "Text-to-SQL conversion is a critical innovation, simplifying the transition from complex SQL to intuitive natural language queries, especially significant given SQL's prevalence in the job market across various roles. The rise of Large Language Models (LLMs) like GPT-3.5 and GPT-4 has greatly advanced this field, offering improved natural language understanding and the ability to generate nuanced SQL statements. However, the potential of open-source LLMs in Text-to-SQL applications remains underexplored, with many frameworks failing to leverage their full capabilities, particularly in handling complex database queries and incorporating feedback for iterative refinement. Addressing these limitations, this paper introduces SQLfuse, a robust system integrating open-source LLMs with a suite of tools to enhance Text-to-SQL translation's accuracy and usability. SQLfuse features four modules: schema mining, schema linking, SQL generation, and a SQL critic module, to not only generate but also continuously enhance SQL query quality. Demonstrated by its leading performance on the Spider Leaderboard and deployment by Ant Group, SQLfuse showcases the practical merits of open-source LLMs in diverse business contexts.", "sections": [{"title": "INTRODUCTION", "content": "In the \"Top 10 Programming Languages\" annual report by IEEE Spectrum for 2023, SQL maintains its top rank on the \"Jobs list\", highlighting the enduring demand for SQL proficiency in the job market[35]. SQL's ubiquity is evident across a myriad of professional roles, including Business Intelligence (BI) analysts, developers, database administrators (DBAs), and even extends to product management, operations, compliance, and business strategy, owing to its critical function in data-driven decision-making. Nevertheless, mastery of SQL requires a deep understanding of database structures and the language itself, constituting a barrier for those without technical expertise.\nText-to-SQL technology represents a pivotal breakthrough, simplifying the process of data querying from the intricate SQL to a more intuitive natural language format. This innovation facilitates user-friendly data interrogation and analysis, democratizing access to database systems and thereby enhancing data processing efficiency and broadening its applications. In response to this practical need, the field has seen the release of substantial benchmark datasets such as WikiSQL [43], Spider [44], KaggleDBQA [15], and BIRD [19]. These resources serve to bridge the gap between academic research and the tangible needs of the industry, fostering developments that are firmly rooted in practical application.\nWith the advent of GPT-3.5 and GPT-4, Large Language Models (LLMs) have emerged as a transformative force for Natural Language Processing (NLP) tasks, Text-to-SQL included. These models' expansive parameters and rich training data have culminated in a nuanced grasp of natural language, yielding more accurate SQL translations by parsing user intents effectively. Despite these advancements, current LLM-based Text-to-SQL frameworks are not fully optimized; they do not exhaust the possibilities offered by open-source LLMs, nor do they effectively incorporate external tools and knowledge that could refine Text-to-SQL performance.\nOur analysis, detailed in Table 1, indicates that current systems are limited in their approach. For instance, they often overlook complex one-to-many relationships crucial for constructing aggregate"}, {"title": "RELATED WORK", "content": "In this section, we present an overview of the methodologies applied in Text-to-SQL conversion. The task presents significant challenges that stem primarily from the complexities of accurately interpreting natural language and generating corresponding SQL queries, as extensively documented in recent surveys [13, 31]. Both database management and natural language processing (NLP) research communities have invested considerable effort in addressing these challenges. Early Text-to-SQL approaches were predominantly based on predefined rules or templates [1, 32, 34]. These methods conceptualized the conversion task as a straightforward mapping exercise from natural language to SQL. Other techniques approached the problem from a sequence-to-sequence learning perspective, applying encoder-decoder models to capture the translation process [3, 28, 30]. However, recent advancements have seen the emergence of hybrid methods that synergize the strengths of both database and NLP technologies. These include approaches that consider schema relations [12, 18, 23, 30, 39, 41, 46] and others that incorporate syntax parsing techniques [11, 16, 33, 40]. Notably, models based on BERT [5], a groundbreaking NLP framework, have been particularly effective in this group, achieving state-of-the-art results in Text-to-SQL conversion [2, 43].\nRecently, the advent of LLMs like GPT-4 [26] from OpenAI and LLAMA [37] from Meta has marked a significant milestone in NLP and machine learning. Unlike the aforementioned models, LLMs are pre-trained on vast text corpora and can perform diverse language tasks. Their operation is based on generating the next most probable word given the input prompt [45]. In the realm of Text-to-SQL, the efficacy of LLMs largely hinges on the art of prompt engineering-the process of devising precise and effective prompts that guide the model [20, 25]. This practice can be bifurcated into two main strategies, dependent on the number of examples supplied to the model: zero-shot and few-shot. In the zero-shot approach, where no examples are provided, the imperative is to craft a question representation that encapsulates all relevant information, including the database schema, to effectively guide the model [4, 8, 20, 38]. Conversely, the few-shot paradigm entails not only the robust representation of the question but also the careful selection and arrangement of a limited number of illustrative examples within the prompt. This approach, known as in-context learning, enables LLMs to recognize and leverage patterns from the provided examples to generate accurate responses, thus equipping them to assimilate new tasks during inference without the need for explicit task-specific training [7]. Recent literature highlights the pivotal role of example selection in enhancing the effectiveness of in-context learning [10, 24, 29]."}, {"title": "METHODOLOGY", "content": "Our proposed SQLfuse framework, as demonstrated in Figure 1, is a modular system designed for Text-to-SQL task. It is mainly composed of the following 4 modules: \u2460 schema mining, \u2461 schema linking, \u2462 SQL generation (SQLgen), and \u2463 SQL critic. Specifically, schema mining is a service that extracts schema features, e.g., primary keys, foreign keys, enumeration values and one-to-many relations, etc., in addition to a pool of candidate database schemas. Subsequently, schema linking module identifies the exact schema elements namely tables, columns, join relations and condition values referenced in natural language query. Armed with the user's question and the schema features and elements extracted, we then meticulously construct a Chain-of-Thought (CoT) template that serves as a structured prompt for the SQLgen module. This module is responsible for generating a variety of SQL statement candidates, which are subsequently processed through constant value fixing and SQL execution checks for validation. The final stage of the process involves the SQL critic module, which employs few-shot in-context learning to evaluate and select the optimal SQL query that most faithfully represents the user's intent. We will now provide a detailed exploration of each module."}, {"title": "Schema Mining Module", "content": "The schema mining module is adeptly crafted to distill essential schema features from databases in a data-driven manner, thereby enriching the context for subsequent modules with vital table information, keys, relationships, and enumeration values. These insights become the cornerstone for both the schema linking and SQLgen modules, allowing them to identify necessary schema components with greater precision and generate SQL statements with heightened accuracy. By leveraging existing knowledge and advanced techniques from database management systems, the schema mining module lays a solid foundation for informed SQL query generation. Now let us delve into the specifics of the extracted schema features.\nPrimary Key. The primary key in a database table is a distinctive identifier, typically a specific column or a set of columns, that ensures each record is unique. This element is fundamental to maintaining the integrity and consistency of data, particularly when conducting aggregation operations. In the realm of Text-to-SQL translation, recognizing and incorporating primary keys is indispensable. They often serve as the axis around which aggregation queries-employing functions like SUM, AVG, COUNT, MIN, and MAX-are structured, especially when combined with the GROUP BY clause. In other words, if the information of primary key are provided in advance, the text-to-sql generation can be more accurate, especially for the aggregation queries.\nForeign Key. A foreign key in a database is a field or set of fields that references the primary key of another table, establishing a critical link between them. This link is vital for preserving referential integrity and enabling coherent multi-table queries. Understanding foreign keys is essential for composing SQL queries that require joins across multiple tables. For instance, to link a <user> table with an <order> table via the field [user ID], the [user ID] in the  table acts as the primary key, while in the  table it serves as a foreign key. This primary-foreign key relationship illuminates how the tables interrelate, positioning the foreign key as pivotal for the JOIN operation. Equipping SQLfuse with foreign key data enriches its capacity to navigate and precisely execute complex multi-table relationships in query translations.\nOne-to-Many Relationship. A one-to-many (1:N) relationship in database design denotes that a single record in one table is associated with multiple records in another. For example, in a  table, each customer (the \"one\" side) may have multiple orders listed in an  table (the \"many\" side). While primary and foreign keys explicitly define inter-table connections, 1:N relationships are often implicit, yet they are essential for understanding data structures and formulating effective SQL queries. In Text-to-SQL translation, accurately identifying these relationships is critical for the proper execution of aggregate queries involving sums, averages, or counts. Recognizing 1:N relationships allows the SQLgen module to make informed decisions on column usage in aggregate operations and the GROUP BY clause. Methods to identify these relationships include:\n(1) Data value analysis: By scrutinizing actual data, we can detect columns with unique values and those with replicated corresponding entries, hinting at 1:N relationships.\n(2) Database statistics: Exploiting statistical data from the database system, like index attributes and column value distribution, can help identify relational patterns.\n(3) Machine learning approaches: Machine learning algorithms can classify or cluster table records, revealing hidden data relationships and aiding in the identification of 1:N links.\nIntegrating these approaches, the schema mining module becomes adept at discerning the complex web of database relationships,"}, {"title": "Schema Linking Module", "content": "Schema linking plays a crucial role in converting natural language queries into SQL queries, aiming to map input questions to specific database schema elements including tables and columns [17]. We formulate schema linking as a task of extracting relevant schema items among a set of candidate database schemas. By exploiting the strength of LLMs in natural language understanding, we adopt an LLM-based approach to conduct the task of schema linking. In particular, we construct a decent amount of high-quality labeled data, with which an open-source LLM is further fine-tuned in a supervised manner. Moreover, we manage to enhance the model performance on schema item extraction by supplementing the input with additional context provided by the previous schema mining module. Lastly, the output of schema linking is double-checked and refined with extra schema elements if necessary.\nSchema Items Extraction. The training data used for the schema items extraction task stem from a variety of open-source Text-to-SQL datasets. We take these collected and filtered datasets as a basis to pose questions to GPT-4 in a schema-linking prompt style similar to DIN-SQL [29]. In particular, we guide GPT-4 to answer in a Chain-of-Thought (CoT) format by instructing \"Let's think step by step\" [14] to incite a thorough elucidation of the extraction rationale. This CoT-guided reasoning is particularly beneficial for the subsequent SQLgen model, enriching its understanding of the semantic ties between user queries and the schema items it extracts, thereby refining the accuracy of SQL statement generation. After acquiring the responses from GPT-4, we examine and verify the correctness of these outcomes through comparison with the schema items, e.g., tables, columns, join relations, and condition values, obtained from the ground-truth labels. By excluding those erroneous question-answer pairs, we end up with a temporary version of high-quality labeled samples.\nInevitably, there exist columns with abbreviations or sharing identical names across multiple tables. In light of these issues causing ambiguity and impairing the result of schema linking, we refine"}, {"title": "SQL Generation Module", "content": "As illustrated in Figure 3, the SQL generation (SQLgen) module serves as the cornerstone of SQLfuse, adeptly synthesizing essential contextual information from schema mining, schema linking, and the original user query. It harnesses a Chain of Thought (CoT) framework, encapsulating these elements in a distinctive SQLfuse style, and then dispatches this amalgamated prompt to a fine-tuned Large Language Model (LLM) to generate SQL predictions. Subsequent to the initial prediction, any type errors in constant values are rectified, and the SQL is executed to identify potential errors for self-correction. This iterative process continues until a valid executable SQL is produced or predefined limits are reached. We will dissect the SQLgen module into two main components: the SQL generation process, and the self-correction process.\nSQL generation. Here, we first introduce the specialized SQLfuse prompt style, which effectively incorporates the context provided by the previous two modules. The format of prompts has been shown to greatly affect a model's understanding, and the optimal format may vary between models. As demonstrated in C3 [8], prompts with a simple and clear layout have been found to outperform those with a more complex structure, increasing accuracy by up to 7%. Common approaches for Text-to-SQL typically employ either code-centric prompts, which adhere strictly to database schema elements (as depicted in Figure 4(a)), or NLP-style prompts, which favor a more versatile, yet less code-oriented format (illustrated in Figure 4(b)).\nSelf-Correction. To address the inherent output instability of LLMs and improve the performance of SQLfuse, we present a component in SQLgen to rectify common errors through two main methods: constant value fix, and SQL execution checking.\nThe Constant Value Fix process addresses mismatches between constant values mentioned in natural language queries-like specific numbers, dates, or text strings-and corresponding database columns, which can arise due to ambiguity in language or model misinterpretation. This automatic correction sequence entails:\n(1) Constant Value Identification: The system identifies constant values within the query.\n(2) Column Matching: It aligns these values with suitable database columns, mindful of data types.\n(3) Feedback Verification: Should match attempts falter or provoke SQL errors, a potential mismatch is flagged.\n(4) Alternative Matching Exploration: The system then scours for alternative column matches, scrutinizing data formats, value ranges, and metadata.\n(5) SQL Modification: The correct column match leads to the immediate adjustment of the SQL by updating the constant value accordingly.\nAdditionally, database statistics, such as value ranges within columns, can also aid in refining the column matching process. Moreover, heuristic rules tailored to the database's industry-specific terminology and usage patterns can enhance the matching precision, particularly when the data for fine-tuning the above SQL-oriented LLM is insufficient or the scenarios are complex. These strategies not only reduce errors in processing constant values but also improve the overall quality of SQL generation, increase user trust, and boost the model's real-time performance by enabling automatic error correction without manual intervention.\nOn the other hand, SQL execution checking leverages the the execution error message returned by the database (e.g, the syntax, logical, and database-specific implementation errors) to the erroneous SQL statement and presents it back to the fine-tuned LLM for"}, {"title": "SQL Critic Module", "content": "Inspired by the work [27] to further refine the final result of Text-to-SQL generation, we employ a Generate-then-Rank technique, which is a common strategy to correct a LLM at generation time. This technique utilizes a SQL critic model that sifts through possible SQL queries generated by the SQLgen module to identify the most accurate one. The LLM within the SQL critic module is pivotal for assessing SQL quality. Rather than relying on extensive SFT, we adopt a few-shot in-context learning strategy that leverages examples from an external SQL knowledge base, enriched with hindsight feedback. These examples, paired with the user's question, schema linking results, directives, and additional calibration cues, are used to prompt the LLM in the SQL critic module to determine the optimal candidate SQL query, as illustrated in Figure 5.\nFew-shot In-Context Learning. To enable the use of either open-source or closed-source LLMs for the SQL critic model, our system leverages few-shot in-context learning. This method requires only a handful of labeled examples to instruct the LLM, circumventing the need for extensive data collection and fine-tuning. The LLM is thus primed to apply its learned knowledge to novel inputs using this minimal yet effective sample set.\nTo enrich the variety of cases presented in few-shot learning, we gather and curate an array of intricate SQL statements and schemas from GitHub. This collection undergoes meticulous cleaning and validation, including manual annotation and GPT-4 evaluation, resulting in a robust external knowledge base. This repository enables the retrieval of historically similar samples to the input question through a method that prioritizes similarity in retrieval, as depicted in Figure 5. We further refine the retrieval process by masking specific keywords in the question, retaining only its \"question skeleton\u201d, which enhances the ability to abstract away domain-specific details and fosters more generalized sentence comparison.\nThe few-shot examples are also enriched with a Chain of Hindsight (CoH) [22] framework, which incorporates feedback prompts like \"A good answer is\" and \"A bad answer is\" (see Figure 5). This technique informs the model using retrospective insights, enhancing its capacity to generate high-quality SQL outputs.\nOne challenge with using historical SQL queries from the knowledge base is that they typically feature a single correct answer without the nuanced feedback often provided by human annotators. To facilate the CoH feedback with negative answers, the LLM within the SQL critic module is engaged to generate suboptimal answers, adding context to the knowledge base. By labeling these inferior responses as \"bad answers\" in juxtaposition with the \"good answers\" represented by the original labels, we equip the model to discern quality, as showcased in Figure 6.\nFinally, the few-shot examples are also complemented by instructions that explicitly direct the SQL critic model to identify the correct SQL query from the array of options provided by the SQLgen module. This intentional placement of a clear command is key to guiding the model's selection process effectively.\nInstruction and Calibration Hints. To further refine the decision-making capabilities of the critic model, the input prompt is supplemented with calibration hints that enumerate common errors typically made by the SQL critic module. These hints serve to preemptively address potential missteps, enhancing the model's ability to generate a superior SQL query. The calibration hints, combined with the user's question and schema linking results, form a comprehensive prompt designed to steer the model toward producing the most accurate SQL output, as illustrated in Figure 5."}, {"title": "EXPERIMENTS", "content": "Dataset\nWe select the Spider benchmark [44] in all of our experiments for the reason that Spider is the most difficult and influential Text-to-SQL benchmark among all existing benchmarks of authority. The Spider dataset encompasses a substantial collection of cross-domain queries. It includes a training set with 8,659 samples (TRAIN Set)"}, {"title": "Experimental Setups", "content": "We introduce the LLMs used in schema linking, SQL generation and SQL critic module.\nSchema Linking Model: An open-source LLM Llama2-70B is selected as the base model of our schema linking module.\nSQL Generation Model: We choose CodeFuse-DeepSeek-33B, which ranks first on Big Code Models Leaderboard until March 2024, as the base model for SQL generation.\nSQL Critic Model: An open-source LLM Llama2-70B is mainly used as the SQL critic model. To notice, we also experiment with GPT-4 as the LLM in SQL critic module.\nFine-tuning Schema Linking Model. The schema linking model is further fine-tuned with the constructed dataset described in section 3.2.1. The training dataset consists of 5k labeled samples. We adopt a parameter-efficient fine-tuning approach of QLoRA with a rank of 64. We use a cosine scheduler and AdamW optimizer with a learning rate of 5e-5 and weight decay of 0.1. The model is trained with a batch size of 2 using a single A100 GPU for 8 epochs. Afterwards, we also build a test set to evaluate the performance of the schema linking model based on the DEV set of the Spider benchmark. Specifically, we use recall of table and column as the evaluation metrics. The resulting schema linking model is able to achieve a 99.8% and 97.4% recall of table and column respectively.\nFine-tuning SQL Generation Model. We choose CodeFuse-DeepSeek-33B, an open-source LLM with exceptional capabilities in handling code-related tasks, as our foundation model for the SQLgen module. It is fine-tuned via QLoRA with the TRAIN set from the spider benchmark. The rank of QLoRA is set to be 96. We"}, {"title": "SQLfuse Evaluation", "content": "Spider Leaderboard. We evaluate the performance of our proposed SQLfuse on the Spider benchmark. Our focus is on assessing the execution accuracy of SQL queries with values on the TEST dataset. According to Table 2, SQLfuse achieves an impressive 85.6% accuracy, ranking just below MiniSeek and DAIL-SQL [9]. Notably, MiniSeek and DAIL-SQL, along with other top contenders, primarily utilize closed-source LLMs like GPT-3.5/4 for their operations. Despite this, SQLfuse demonstrates robust competitiveness against these closed-source models and markedly outperforms alternatives that rely on open-source LLMs.\nReal-world Application. It is also worth of mentioning that SQLfuse has been tested and deployed in real-world scenarios. Notably, it is integrated into the daily operational framework of Ant Group, supporting seven business contexts including the online analytical processing (OLAP) and transaction processing (OLTP) platforms within the company."}, {"title": "Ablation Study", "content": "SQLfuse consists of four modules and each module employs various techniques to aid a better SQL generation. We conduct different ablation studies to evaluate our approach with and without each of the following modules or methods.\nSchema Mining. Regarding schema mining, we extract and leverage critical schema attributes to enrich the context provided to our model. We have evaluated the incremental benefits these schema features offer.\nTable 3 sheds light on the tangible accuracy losses in the TEST dataset when omitting various schema-related enhancements. To clarify:\nImpact of Primary Key: Excluding primary key cues results in a 1.8% decline in accuracy, underscoring the value of primary key identification for the model's proficiency. For instance, as depicted in Figure 7, confusion may arise in distinguishing between the [singer ID] in a <table> and a  table. Recognizing the [singer ID] as the primary key is crucial for generating accurate GROUP BY fields in queries such as \"the number of concerts per singer\", ensuring uniqueness and preventing errors.\nImpact of Foreign Key: The accuracy takes a 3.5% hit without foreign key insights, emphasizing the foreign key's role in multi-table queries. As illustrated in Figure 8, the model, aided by foreign key information, can correctly identify [model id] for joins between <model_list'> and <car_maker> tables. Understanding table relationships via foreign keys is pivotal for precise multi-table join operations.\nImpact of one-to-many relationships: A 1.8% accuracy loss is observed when one-to-many relations are neglected. An example in Figure 9 demonstrates the importance of recognizing the one-to-many link between [country_id] in"}, {"title": "SQLgen Module", "content": "Our analysis within the SQLgen module continues with a focus on the effects different techniques have on SQL generation proficiency.\nInitially, we evaluate the effectiveness of diverse prompt styles, which form the basis of our training data. The base LLM is fine-tuned using these styles to allow for comparative analysis. Table 3 indicates that prompt styles yield varied outcomes, with the SQL-fuse style surpassing both the code representation style (which results in a drop of 0.6% in Test set accuracy) and the natural language style (which shows a drop of 0.5% in Test set accuracy), as measured on the Spider benchmark. This superiority is likely due to SQLfuse's advantageous blend of SQL schema representation with natural language elements.\nMoving on, we assess the Constant Value Fix and SQL Execution Checking modules. As detailed in Table 3, both modules contribute to an absolute increase of 0.4% in final accuracy, showcasing their individual significance in enhancing SQL generation.\nLastly, we delve into the fine-tuning methodology of the SQL-gen module. Our fine-tuning relies on standard, parameter-efficient techniques, specifically LoRA or QLoRA. Within these methods, the LoRA rank is important as it manages the proportion of trainable parameters and influences the effectiveness of fine-tuning. Distinct LoRA ranks correspond to varying levels of performance. Comprehensive evaluations reveal that a LoRA rank of 96 is the sweet spot for our SQLgen, outperforming both ranks 32 and 228. While increasing the rank initially benefits performance, there is a threshold beyond which further elevation does not translate to better results and might even be detrimental. This is anticipated, given our training dataset encompasses fewer than 10,000 samples, meaning a rank of 96 suffices for our purposes. Note that these"}, {"title": "SQL Critic Module", "content": "Our attention turns to the SQL critic module, evaluating its influence by removing it from the SQLfuse framework and analyzing the performance based solely on the output from the SQLgen module. Table 3 reveals that removing the critic module results in a reduction of approximately 2% in both DEV and TEST set accuracy. This data underscores the critic module's role in refining SQL generation, highlighting its effectiveness in enhancing the quality of the generated queries.\nFurther, we undertake a comparative study by swapping out Llama2-70B for GPT-4 in the critic's role. This substitution leads to a marginal increase in TEST set accuracy, nudging it from 85.6% to 85.9%. Such an outcome indicates that while GPT-4 offers slight improvements, the SQL critic module's structure and functionality are significant contributors to SQLfuse's overall performance, irrespective of the underlying language model."}, {"title": "CONCLUSION", "content": "In this paper, we present SQLfuse-a cutting-edge contribution to the Text-to-SQL field that represents a substantial leap forward. Our systematic approach, which integrates schema mining, schema linking, SQL generation (SQLgen), and a critic module, is meticulously designed to improve the accessibility and precision of generating SQL queries from natural language. SQLfuse harnesses the expansive capabilities of LLMs while also integrating ancillary knowledge and tools, resulting in a synergy that significantly enhances system performance. The system's landmark achievement of an 85.6% execution accuracy on the renowned Spider Leaderboard serves as a testament to the efficacy of SQLfuse. Ranking as the top open-source LLM-based Text-to-SQL system, SQLfuse extends the frontiers of scholarly research and meets the practical demands of the industry with powerful, user-friendly data querying capabilities. SQLfuse's theoretical strengths are brought to life in real-world settings, as demonstrated by its successful deployment within the dynamic operational frameworks of Ant Group. This practical application confirms SQLfuse's role as an indispensable asset, adept at navigating and managing complex data queries, and solidifying its position at the intersection of academic innovation and industrial utility."}, {"title": "DISCUSSION", "content": "In this section, we explore strategies to further elevate the performance of LLM-based Text-to-SQL translation. Our discourse encompasses techniques currently under experimentation as well as those slated for future investigation."}, {"title": "Training Data Expansion", "content": "In the Text-to-SQL domain, training data expansion is an effective strategy for enhancing model performance, particularly when training data is scarce. We have explored various data augmentation strategies for your reference and discussion.\nA creative method for data expansion is using error set expansion, which expand more data via summarize common errors of model during evaluation on validation dataset. This approach involves analyzing the mistakes of model output, identifying similar issues within the database, and generating new training samples grounded in these erroneous instances. This targeted improvement helps the model to avoid similar mistakes in the future by reinforcing its knowledge on specific points.\nThe exploration of this data augmentation method entails the following steps:\nError Analysis Initially: An assessment of the model's errors during evaluation is conducted to identify weak points, such as JOIN operations, subqueries, or aggregate functions.\nQuestion Skeleton Generation: To discover structurally similar questions to the error instances, question skeletons are created by abstracting the main structure of the original queries and masking specific values and keywords.\nSQL Skeleton Generation: Similar to question skeletons, SQL skeletons maintain the structure of the original SQL queries but with specific values and database entity names removed, preserving their structured pattern.\nKnowledge Base Matching: The generated question and SQL skeletons are used to find structurally similar questions and corresponding SQL statements in a knowledge base, which can be done using text similarity algorithms or models specifically designed for text matching.\nGPT-based Knowledge Expansion: Once similar questions to the error instances are identified, pre-trained LLMs like GPT are employed to generate new pairs of questions and SQL statements. This method allows for the expansion of the model's knowledge, leveraging GPT's generative capabilities to create new training samples.\nData Cleaning and Validation: The newly generated question and SQL pairs must undergo cleaning and validation to ensure their quality is sufficient for effective model training. This process often involves syntax checks, logical consistency validation, and ensuring SQL statements conform to the database schema.\nThrough this error-knowledge-based data augmentation, the model can improve by learning from its weaknesses, offering a targeted solution to rectify deficiencies in handling certain types of problems. This method not only strengthens the model's understanding of existing question types but also equips it to handle new and unseen problem types, thereby enhancing its robustness and generalization capabilities.\nAnother method of data expansion is complex SQL expansion. In Text-to-SQL tasks, particularly for applications involving complex queries, it becomes crucial to enhance the model's understanding of intricate SQL structures. This enhancement can be achieved by incorporating a higher number of complex SQL samples into the"}, {"title": "Preference Learning", "content": "Preference learning such as Reinforcement Learning from Human Feedback(RLHF) and Direct Preference Optimzation(DPO) may enhance the model's ability to generate complex SQL statements. This approach may lead generated SQLs generated more in line with human writing habits, beneficial for both the model's practicality and user satisfaction. We are going to discover how such preference learning influence Text-to-SQL results in the future."}]}