{"title": "CNMBert: A Model For Hanyu Pinyin Abbreviation to Character Conversion Task", "authors": ["Zishuo Feng", "Feng Cao"], "abstract": "The task of converting Hanyu Pinyin abbreviations to Chinese characters represents a significant branch within the domain of Chinese Spelling Correction (CSC). This task is typically one of text-length alignment, however, due to the limited informational content in pinyin abbreviations, achieving accurate conversion is challenging. In this paper, we propose CNMBert which stands for zh-CN Pinyin Multi-mask Bert Model as a solution to this issue. CNMBert surpasses few-shot GPT models, achieving a 59.63% MRR on a 10,424-sample Hanyu Pinyin abbreviation test dataset.", "sections": [{"title": "Introduction", "content": "Hanyu Pinyin is the romanization system for Standard Chinese\u00b9, it represents the pronunciation of a Chinese character, like the phonetic symbols in English(e.g., \u201cdian nao\u201d for \u201c\u7535\u8111(computer)\u201d. Recently, on social media platforms in Simplified Chinese, people tend to replace certain Chinese words with their Hanyu Pinyin's first letter(e.g., people may use \u201cd n\u201d stands for \u201c\u7535\u8111(computer)\u201d). Especially when their speech may contain prohibited words, this situation is particularly noticeable. The purpose of doing so is to evade the platform's censorship mechanisms for prohibited words, allowing them to express violent, sexual minority, pornographic, or hateful content.\nThis phenomenon, where the Hanyu Pinyin's first letters are used to replace Chinese characters, is referred to as Hanyu Pinyin Abbreviations, which represents an important subfield of Chinese spelling correction(CSC). With the advent of the transformers architecture(Vaswani, 2017), an increasing number of GPT models(Radford, 2018) have been released. These models have demonstrated unexpected proficiency in understanding the semantics of Chinese text, and rapidly applied to tasks like machine translation, text generation, and sentiment analysis. However, on one hand, these models have not been adequately trained on Hanyu Pinyin Abbreviations data, and on the other hand, Hanyu Pinyin Abbreviations carry very little information. Additionally, the same Hanyu Pinyin Abbreviations may correspond to different pinyin forms, which can then be linked to numerous homophones, making it difficult for these models to accurately interpret the intended meaning of the Hanyu Pinyin Abbreviations. Even for native Chinese speakers, understanding these pinyin abbreviations can be challenging for the complexity stems from limited informational content in abbreviations and the inherent ambiguity of Chinese phonetics.\nIn this paper, basing on the Whole Word Mask Chinese BERT model (Cui et al., 2020), we propose CNMBert to solve Hanyu Pinyin Abbreviation to Character Conversion Task. We treat the this as a Fill-Mask task. Specifically, we extend the number of mask tokens in the simple BERT model(Devlin et al., 2019) to match the number of letters in the Hanyu Pinyin's alphabet. And, each letter is mapped to a distinct mask token (e.g., The letter 'a' will be mapped to mask token [LETTER_A] ), then the model predicts the corresponding mask tokens, thereby generating the translation of the pinyin abbreviation. Since BERT"}, {"title": "Related Work", "content": "In this section, we will discuss related work associated with our model, as well as work on Hanyu Pinyin translation."}, {"title": "Whole Word Mask Bert", "content": "The original BERT model used WordPiece tokenizer(Wu et al., 2016) to segment the input into WordPiece tokens, in which case a single word could be split into several smaller pieces and when training on the Fill-Mask task, those words may only have part of their tokens masked. For Chinese, the original BERT model segments input by characters, with each character mapping to an individual token. However, this approach can also result in only part of a multisyllabic word being masked.\nTo address the issue of only parts of words being masked, the Google team proposed Whole Word Masking (WWM). This approach allows the model to mask entire words' tokens, making it easier for the model to predict the masked content(Devlin et al., 2018). For the Chinese BERT model, Cui Yiming et al. using LTP, a Chinese text segmentation tool(Che et al., 2021), to segment the input text into several words. Based on this, they developed the Chinese-WWM-BERT model for Chinese text(Cui et al., 2020). And experimental results have shown that the BERT model using WWM outperforms the original BERT model, achieving state-of-the-art performance on most tasks."}, {"title": "PinyinGPT", "content": "Minghuan Tan et al. found that while the GPT model can often infer the meaning of perfect Hanyu Pinyin (e.g., \u201cdian nao\u201d is the perfect Hanyu Pinyin for \u201c\u7535\u8111\u201d) sequences from context, its performance drops significantly when handling Hanyu Pinyin abbreviations(Tan et al., 2022). To address this, they developed PinyinGPT based on a publicly available character-level GPT model(Du, 2019).\nFor PinyinGPT, they integrated Hanyu Pinyin information into the model input using two methods: PinyinGPT-Concat and PinyinGPT-Embed. Specifically, PinyinGPT-Concat appends the pinyin sequence of the input Chinese characters to their context. During inference, the model's input format is [W1, ..., wn, [SEP], Pn+1, ..., Pn+k, [SEP]], where [SEP] is a special token used to separate the text from the pinyin, with pn representing the pinyin of w\u2081. PinyinGPT-Embed, on the other hand, introduces an additional pinyin embedding layer, which performs word embeddings on the first letters of the Hanyu Pinyin.\nThey tested the model on a dataset they created, containing 270K instances across fifteen different domains. Experimental results showed that PinyinGPT outperforms the standard GPT model in handling Hanyu Pinyin abbreviation inputs."}, {"title": "Models", "content": "In this section, we will provide a detailed description of the architecture of our CNMBert and explain how we attempt to enable the BERT model to understand and leverage the information contained in Hanyu Pinyin Abbreviations."}, {"title": "Multi-Mask Strategy", "content": "The original BERT model is pre-trained using the Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks. In the MLM task, 15% of the characters in the input sentence are replaced with the [MASK] token, and the model is required to predict the masked words based on the surrounding context. Building upon the Whole Word Mask (WWM) approach, we modified the masking logic. Specifically, we expanded the number of mask tokens so that each character is mapped to a distinct mask token. During training, our model first selects the words to be masked. Then, using the pypinyin package\u00b2, retrieves the first letter of the Hanyu Pinyin for each character. This letter is mapped to the corresponding mask token, which serves as the masked token for that character, as illustrated in Table 2.\nTo address the mismatch between the pre-training task and downstream tasks, the original BERT model introduces a mechanism where a small portion of selected words are randomly replaced with random characters or left unchanged, in order to enhance the model's robustness. Since CNMBert pre-training task aligns with the downstream tasks, we have opted to discard this adjustment. In our model's pre-training phase, only 10% of the selected words are replaced with random characters, while the remaining 90% selected words follow the multi-mask strategy. Furthermore, we increased the probability of masking multisyllabic words rather than individual characters. This strategy encourages the model to focus more on training with multisyllabic words, thereby enhancing its prediction capability for such words.\nWe define the input sequence as x = (X1, X2, X3, ..., Xn), the model's output sequence as y = (Y1, Y2, Y3, \u2026\u2026\u2026, Yn), and the set of masked indices as M = {Ma, Mb, Mc, ..., Mz}, where the Ma is the index set of the characters masked as [LETTER_A]. For our CNMBert model, the loss function is defined as:\nL_{fill-mask} = \\sum_{s \\in S} \\sum_{i \\in M} logP(y_i | x_i)\nOutput(x) = \u03b1 \\cdot SharedExpert(x) + \u03b2 \\cdot Routed(x)\n\u03b1, \u03b2 = Softmax([a, b])\nW = Softmax(L(x))\nRouted(x) = \\sum_{w_i \\in TopKN(W)} w_i \\cdot Experti(x)"}, {"title": "MoE Layer", "content": "In CNMBert, some feedforward neural network (FFN) layers are replaced with mixture-of-experts network (MoE) layers (Jacobs et al., 1991). This approach allows the model to scale up and enables different experts to handle different tokens. DeepSpeed Team found that, for the same number of experts, placing them in layers closer to the output performs better than placing them in hidden layers closer to the input. Thus, they proposed PR-MOE (Pyramid-Residual Mixture of Experts), which has fewer parameters compared to a sparse MoE but achieves similar performance (Rajbhandari et al., 2022). Based on this, we designed the MoE layers of our model as shown in Figure 2. Each MoE layer includes one shared expert, with the rest as routed experts (Dai et al., 2024). The MoE layers replaced every other feedforward layer. Additionally, the number of shared experts and the number of experts activated during inference vary across different layers. In the shallower layers, we use fewer routing experts and activate a smaller number of experts when inference. Overall the MoE layers in our model forms a pyramid-like structure.\nIn the model, the final output of the MoE layer is a linear combination of the outputs from the shared expert and the routed experts. For a token x, the output is defined as follows, where L(x) : R^{dim(x)} \\rightarrow R^{num_experts}, TopKN(W) selects the top N values in tensor W, with a and b are the model's trainable parameters."}, {"title": "Experiments", "content": "In this section, we will introduce the dataset we used to train and evaluate the model, the training details of the model, and the results of the model on the Hanyu Pinyin Abbreviation to Character Conversion Task."}, {"title": "Datasets", "content": "We use EXT dataset\u00b3, webtext2019zh dataset\u2074 (Xu, 2019), and Bilibili (a Chinese video-sharing platform) comment dataset for training and evaluating the model. These datasets cover a wide range of topics, including mathematics, history, society, and several other domains. Then we stratified and extracted 2 million datas from these datasets for fine-tuning our model.\nFor constructing the test set, we randomly selected 10424 sentences that were different from the training data. These sentences were split by using pkuseg(Luo et al., 2019), and in each sentence, we randomly chose one word and replace it with its Hanyu Pinyin's first letter. Additionally, we manually reviewed these data to ensure that the replaced words would not introduce any ambiguity."}, {"title": "Pre-training", "content": "We first extended the BERT-wwm-ext model(Cui et al., 2020) by increasing its layers from 12 to 16, and parameters of these additional layers are randomly initialized. Then, we fine-tuned the model on the MLM task using the aforementioned data with the WWM strategy. Based on this, we trained two versions of CNMBert: one using MoE layers replace the FFN layer and the other using traditional FFN layers. For the MoE model, the weights of the shared experts were copied from the FFN layers of our fine-tuned BERT-wwm-ext. Both models were trained for 32 epochs using the same data and the same initial learning rate.\nAll models were trained using the Transformers package(Wolf et al., 2020) with the AdamW optimizer(Loshchilov and Hutter, 2019), and the training was conducted on a server with four Nvidia RTX 4090 GPUs."}, {"title": "Evaluation", "content": "We use mean reciprocal rank(MRR) as the evaluation metric and additionally conducted separate tests for single-syllable and multi-syllable words(A word composed of multiple Chinese characters). The calculation of MRR at top-K is defined as follows, it measures if the ground truth exists in the top-K results and it's position.\nMRR@K = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{1}{rank_i}"}, {"title": "Additional Settings", "content": "We evaluated our CNMBert, Llama3-Chinese-8B-Instruct a model fine-tuned on Llama3-8B with a focus on Chinese language (Dubey et al., 2024) and Qwen2.5-14B-instruct (Yang et al., 2024) on a computer with 1 RTX 3090Ti GPU. Both LLaMA3.1 and Qwen2.5 were deployed using Ollama and accessed via LangChain (Chase, 2022). For all GPT models, we used the same prompt, which is as follows:\nSystem: Do your best to answer the user's question to the best of your ability. Avoid repeating the question in your response. Do not use redundant or repetitive statements. Ensure your language is smooth and coherent. Avoid saying something once and then repeating it later unnecessarily. Do not use emojis. Please answer in Chinese.\nUser: In the sentence [\u4ed6\u4eec\u5c31\u8fd9\u6837\u5730\u5c45\u4f4f\u5728\u4e00\u8d77], what are the possible meanings of [xf]? Please list all the potential meanings in descending order of likelihood, separated by spaces. Do not provide any additional explanatory text."}, {"title": "Result", "content": "In Table 4, we present the overall MRR scores of different models on the test set, along with the MRR scores for predicting single-syllable words and multi-syllable words separately. We found that for GPT models without fine-tuning, their ability to predict Hanyu Pinyin abbreviations is quite limited. Additionally, they may exhibit significant hallucination, which sometimes leads to answers that do not conform to the constraints of Hanyu Pinyin abbreviations.\nWe also evaluated our model's performance using MRR@1 (Accuracy), MRR@5, and MRR@10 scores, as shown in Table 5. From the results, we observed that some correct answers did not always appear in the top-ranked position but were instead found in the second or third positions. Moreover, because multi-syllable words consist of multiple Chinese characters, a single pinyin's first letter can map to various different pinyin forms, and each pinyin can correspond to multiple Chinese characters. This results in an exponential increase in possible combinations, significantly increasing the difficulty for the model to translate Hanyu Pinyin abbreviations. The longer the multi-syllable word, the more challenging the prediction becomes, leading to a corresponding decrease in prediction accuracy.\nTo this end, we tested whether the model could accurately predict the correct pinyin for multi-syllable words from its Hanyu Pinyin abbreviation. We continued to use MRR as the evaluation metric, and the results are shown in Table 6. We found that for our model, the MRR score of predicting the Hanyu Pinyin of multi-syllable words was higher than the score of predicting the multi-syllable words themselves. This indicates that the model can vaguely infer the Hanyu Pinyin of corresponding Chinese characters from the Hanyu Pinyin abbreviations. However, accurately predicting the correct words remains a challenging task for the model, highlighting a potential direction for further optimization."}, {"title": "Conclusion", "content": "In this paper, to address the problem of Hanyu Pinyin abbreviation to character conversion task, we improved the traditional BERT model architecture and masking logic, proposing CNMBert. Using our constructed test set, we validated that this model outperforms other GPT models in terms of accuracy for Hanyu Pinyin abbreviation to character conversion task. CNMBert introduces a novel multi-mask and MoE-based approach, which can be adapted for solving the pinyin abbreviation problem and can help people better understand the meaning of Hanyu Pinyin abbreviations, offering potential improved solutions for tasks such as text sentiment analysis and Chinese spelling correction(CSC)."}]}