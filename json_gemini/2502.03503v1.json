{"title": "Two in context learning tasks with complex functions", "authors": ["Omar Naim", "Nicholas Asher"], "abstract": "We examine two in context learning (ICL) tasks with mathematical functions in several train and test settings for transformer models. Our study generalizes work on linear functions by showing that small transformers, even models with attention layers only, can approximate arbitrary polynomial functions and hence continuous functions under certain conditions. Our models also can approximate previously unseen classes of polynomial functions, as well as the zeros of complex functions. Our models perform far better on this task than LLMs like GPT4 and involve complex reasoning when provided with suitable training data and methods. Our models also have important limitations; they fail to generalize outside of training distributions and so don't learn class forms of functions. We explain why this is so.", "sections": [{"title": "1. Introduction", "content": "In-context learning (ICL) (Brown et al., 2020) enables a large language model (LLM) to learn a task from an instructional prompt and a few examples at inference time, without any adjustment of the model's parameters from pretraining. Despite prior work on linear functions, the study of ICL on more general classes of functions is largely virgin territory.\nWe present such a study with the following findings. Smaller LLMs with both attention only architectures and full transformer architectures can ICL arbitrary polynomial and hence all continuous, real valued functions f over an interval $[a, b] \\subset \\mathbb{R}$ that \u201csufficiently covers\u201d, in a sense to be defined below, the training samples for f and values $x_i$.\nOur models also approximate functions in $[a, b]$ whose class forms were not encountered in training, and their overall performance also improves: thus, changing the training regime helps with generalization. In addition, with the same training used for predicting $f(x_i)$ our models can solve the inverse problem of ICL functions; our models can approximate zeros of arbitrary polynomial functions and do so better than state of the art LLMs like GPT4.\nHowever, our models fail to generalize their predictions to values for $f$, for $x, f(x) \\notin [a,b]$. Thus, the models use none of the algorithms (linear, ridge regression, Newton's method) suggested in the literature, and fail to learn the class forms of $P_n$ for any $n$. We give a mathematical argument showing that this failure stems from limitations of the matrices used to define attention. These limitations, we believe, come from the model's pretraining and are not easily solved.\nICL depends on a model's pretraining, and so small transformer models with limited pretraining from scratch are essential for our investigation. We trained our models from scratch and studied the 1 dimensional case of functions in $P^n$ (the class of polynomial functions of degree $n$) for $1 \\le n \\le 8$ for over 30 models and with a variety of train and testing distributions. Our research demonstrates that ICL capabilities can be substantially enhanced through targeted adaptation during pretraining.\nOur paper is organized as follows. In Section 2, we define two ways of ICL a function; one, $ICL_1$ holds when the model performs well when training and testing distributions coincide; a second, $ICL_2$ holds when the model is able to generalize its performance. Section 3 describes related work. Section 4 shows that a small transformer model can $ICL_1$ arbitrary continuous functions. Section 5 shows that our models fail to $ICL_2$ functions from $P_n$ for any $n$. Sections 6 investigates how models generalize to unseen polynomial classes, and chapter 7 investigates the problem of finding zeros of functions. Section 8 provides a general discussion of our models' capacities. We then conclude."}, {"title": "2. Background", "content": "In ICL, a transformer style model learns a function $f$ given in-context examples at inference time in the following next-token-prediction format (Brown et al., 2020): given a prompt containing a task input-output examples $(x_1, f(x_1), .., x_n, ?)$, the model is asked to generate a value for $f(x_n)$. This prompt is simple but precise and avoids the variable performance of more complex prompts. Our aim is to study functions over any fixed interval $[a, b] \\subset \\mathbb{R}$. But as we can always translate any result on $[a, b]$ to a result on $[-1, 1] \\subset \\mathbb{R}$, we concentrate on $[-1,1]$, though we also look at larger intervals, e.g. [-5, 5] for training and testing.\nLearnability is often characterized via empirical risk minimization (ERM), but (Shalev-Shwartz et al., 2010) argue for a more general definition. We thus define learnability via the notion of uniform consistency (Neyshabur et al., 2017; Villa et al., 2013). Let $\\mu$ be a distribution over $H$ and $\\mu_n$ the update of $\\mu$ after n training samples $z_i = (x_i, y_i)$. Let $A_{z_n}$ be an algorithm for picking out a hypothesis from $H$ based on n training samples. $\\inf_{H} E_{\\mu}$ is the hypothesis in $H$ with the lowest possible error on $\\mu$ (Shalev-Shwartz et al., 2010; Kawaguchi et al., 2017). An algorithm A on a hypothesis space $H$ is uniformly consistent if and only if\n$\\forall \\epsilon > 0 \\lim_{n \\rightarrow \\infty} \\sup_{\\mu_n}({Z_n: E_{\\mu}({A_{z_n} - \\inf_{H} E_{\\mu}} > \\epsilon}) = 0$\nIn our task, the best hypothesis $\\inf_{H} \\mu$ is a prediction $f$ of some target function $f$. The best hypothesis is when $f = f$ with $\\hat f = f$, which yields 0 expected error. There are several algorithmic approaches, Fourier expansions, wavelets, or-thogonal polynomials, that converge to target polynomial functions explored in (DeVore, 1998). We say that a function class $C$ is uniformly learnable iff there exists a uniformly consistent algorithm for finding any $f \\in C$.\nWe need to sample functions and their input values from a distribution, and we need to decide what the sampling distribution is. This leads to two distinct notions of \"learning a function $f$ \" that should be distinguished. The first, call it $ICL_1$, involves an algorithm that can compute $f(x_i)$ when $x_i$ and the coefficients of $f$ are drawn from the training distribution or a related distribution such that $f(x_i) \\in [a, b]$, where $[a, b]$ the vast majority of values seen in training. A model $ICL_2 f$ if it has learned an algorithm that can compute $f(x_i)$ when $x_i$ and coefficients of $f$ are drawn from a distribution on which the conditions of $ICL_1$ are not met. $ICL_2$ learns effectively the task and generalizes it. $ICL_1$ does not necessarily achieve this level of learning, but the model performs well within the training distribution. Classes like $P_n$ are purely general and $ICL_2$ seems an appropriate standard for them. According to the $ICL_2$, we would expect that if the model $M$ has $ICL_2 P_n$, then it has learned the class form for $P_n$. Thus, for arbitrary $f \\in P_n$, M has an algorithm such that $f(x) = \\hat f(x)$ for any point $x$ on which $f$ is defined. As we will see all our models $ICL_1$, but not $ICL_2$."}, {"title": "3. Related Work", "content": "Since (Brown et al., 2020) introduced ICL and (Garg et al., 2022) investigated ICL for $P^1$, there has been considerable research indicating that ICL is possible because of a sort of gradient \"ascent\u201d, higher order optimization techniques or Bayesian principles (Aky\u00fcrek et al., 2022; Von Oswald et al., 2023; Fu et al., 2023; Xie et al., 2021; Wu et al., 2023; Zhang et al., 2023; Panwar et al., 2023). (Dong et al., 2022) surveys successes and challenges in ICL, noting that research has only analyzed ICL on simple problems like linear or simple Boolean functions (Bhattamishra et al., 2023). (Wilcoxson et al., 2024) extends (Garg et al., 2022)'s approach to ICL Chebychev polynomials up to degree 11 with training and testing on $\\mathcal{N}(0, 1)$. (Ravent\u00f3s et al., 2024) investigated how ICL in models evolves as the number of pre-training examples increases within a train=test distribution regime. (Olsson et al., 2022) propose that induction heads, a learned copying and comparison mechanism, underlie ICL. (Daubechies et al., 2022) shows that neural networks in principle can have greater approximation power than most nonlinear approximation methods. (Geva et al., 2021) has investigated memory in transformers. (Bietti et al., 2024) defines memory in terms of weighted sum and report that transformers memorize a large amount of data from their training through attention matrices. (Yu et al., 2023; Geva et al., 2023) argue that LLMs favor memorized data, as the attention mechanism prioritizes memory retention.\n(Xie et al., 2021; Zhang et al., 2024; Giannou et al., 2024; Naim & Asher, 2024a) show that when train and inference distributions do not coincide, ICL performance on linear functions degrades. (Naim & Asher, 2024a) shows that transformers models approximate linear functions as well as the best algorithms when train and inference distributions coincide; but when they do not coincide, they show models behave peculiarly around what they call boundary values (see also (Giannou et al., 2024)). Within boundary values models perform well; outside the boundary values but near them models predict linear functions to be constant functions and then further out the predictions become random. Our work here builds on (Naim & Asher, 2024b) but extends it to ICL of continuous functions. We show that boundary values exist for all prediction of functions we tested, as well that attention layers in transformers are necessary for ICL of polynomials. Work on neural nets as approximators of continuous functions has concerned MLP only architectures and assumes that the function $f$ is known (Hornik et al., 1989). With ICL, we don't have full access to $f$ but only to limited data about its graph."}, {"title": "4. Models can ICL\u2081 continuous functions over some bounded intervals", "content": "In this section, we show experimentally that transformer models trained on sampling over [-1,1] $ICL_1$ polynomials of arbitrary degree and hence continuous functions.\nWe trained from scratch several decoder-only transformer models, with 12 layers, 8 attention heads, and an embedding size of 256, to evaluate their ICL capabilities on different classes of polynomial functions. We assessed their performance both with and without feed forward layers, with"}, {"title": "5. Transformers don't ICL\u2082 any $f \\in P^n$", "content": "In this section, we show: (i) models do not ICL2 any functions we tested; (ii) models have similar out of distribution behavior for all functions; (iii) models with attention layers (AL) are needed to achieve ICL and that attention only models can sometimes out perform full transformer architectures\nAll our models had systematic and non 0 average error on target functions for all polynomial classes tested once we chose either: (i) test distributions over significantly larger intervals than those of training distributions, or (ii) very large training distributions. Figure 1 shows that the error rate increases substantially; Table 3 in the Appendix provides the average errors with respect to least squares and shows that the error rate increases non-linearly as $D_1 = \\mathcal{U}(-\\sigma, \\sigma)$ and $\\sigma$ increase. Figure 7 in Appendix C gives a heatmap rendition of the error across $x_i$ and function coefficients. Figure 4 in Appendix B shows similar results for training and testing with $\\mathcal{N}(0, \\sigma)$. Performance did not significantly improve when we moved from models with 9.5 M to 38M parameters."}, {"title": "6. Surprising ICL capacities: Generalizing to unseen polynomial classes", "content": "We have examined in Sections 4 and 5 models trained and tested on one class of polynomial, with the results in Figure 4. How might a sort of curriculum learning, where models learn several models but not others affect performance? Figure 3 depicts two situations: one in which the models learn to approximate $f \\in P_m$ for $1 < m < 6$ while having been trained on $f \\in P_{1,2,3} = P_1 \\cup P_2 \\cup P_3)$; the other in which the models train on $f \\in P_{1,3,5}$. The second case is interesting because it forces the model to predict values for functions of classes on classes it has not seen that lie between classes it has trained on. Figure 3 compares how those models do with respect to models $M_m$ trained and evaluated only on $P^m$.\nWhile all training regimes produced close to perfect results when test values for coefficients of $f$ and $x \\in [\u22122, 2]$, once test values were outside [-2, 2], the $M_{1,2}$ models had much higher error rates than the second \"gappy\" model $M_{135}$ on $f \\in P_{1,2}$. And $M_{135}$ didn't even see any quadratic functions in its training. $M_{135}$ had better generalization performance and error rates better or equal to the best $M_m$ models for any $P^m$ class we tested. $M_{135}$ had better results on polynomials of degrees 2 and 4 than it did on $f \\in P^5$.\nIt also had better generalization performance than the cumulative curriculum model $M_{123}$. $M_{123}$ also did better than $M_1$ models on $P_{1,2}$ but not on higher order polynomial classes. Interestingly, $M_{135AL}$ with attention only layers had optimal or close to optimal performance in many cases, while $M_{123AL}$ had significantly worse generalization performance than other models.\nA possible explanation of the superior performance of $M_{123}$ and $M_{135}$ over $M_1$ and $M_2$ is this. While higher order polynomial functions define more complex sequences than lower order ones, training on $P^4$ with coefficients and inputs $x_i$ in $U(-1,1)$ will yield a significantly larger spread of values (in [-5, 5]) when training on $U(-1, 1) than training just on $P^1$. Given our observations that proximity of training is important to accurate approximation, training on higher polynomial classes can aid in generalization. This also accounts for why $M_{123}$ performs less well than $M_4$ or $M_5$ when we test $M_{123}$ on $P_{4,5}$ and why $M_{135}$'s superior performance vanishes for higher order polynomials, since training $P^5$ with sampling from $U(\u22121, 1)$ only negligibly increased the chances of having nearby values from training during inference on, say, $P^3$ with coefficients sampled from $U(-6,6)$-the maximum values for $P^3$ on $U(\u22126,6)$ are included in the interval [\u20131514, 1514], while the maximum training value interval is [-5, 5].\nIn contrast to (Yu et al., 2023), our experiments reveal that the $M_{135AL}$ model demonstrates strong performance and generalizes effectively, even outperforming $M_{135}$, whereas the $M_{123AL}$ model does not exhibit similar capabilities. These findings suggest that the chosen training methodology significantly influences a model's propensity to favor reasoning over memorization ceteris paribus."}, {"title": "7. Surprising ICL capacities: finding zeros of unknown functions", "content": "Since our models treat prompts $(x_1, f(x_1), .., x_n, ?)$ simply as sequences, we can use the same training as in Section 4 to solve a difficult inverse problem: given a prompt of the form $(f(x_1), x_1, .., f(x_{n-1}), x_{n-1}, f(x_n) = 0, ?) = (y_1, f^{-1}(y_1), ..., 0, ?)$. With this prompt, the ? asks M to predict $\\hat x_n = x_n + \\epsilon$, where $x_n$ is the true zero of the function $f$, with $\\epsilon$ an error term. For example let $f(x) = x + 3$ and so $f^{-1}(x) = x \u2212 3$. To find the zero of $f$, we give $f^{-1}(0) = -3$ as then element of our sequence answering to ?.\nThere are polynomials do not have analytic solutions in terms of radicals, and Galois (Stewart, 2022) gave a necessary and sufficient group theoretic condition for solvability"}, {"title": "8. Discussion", "content": "Given our models' surprising capacities, analysing ICL as a matter largely of associative memory as in (Bietti et al., 2024) needs revisiting. The lack of generalizability from Observations 4 might suggest our models overfit the data and rely solely on memory to ICL. However, the pretraining data has no noise, and it's too large to be memorized by our models; our largest models with 256 size embeddings have < $10^7$ parameters; each parameter would have to encode on average sequences for over 100 different functions of wildly different kinds. Further, some attention-only models in Figure 3 with only 3+ million parameters have equivalent performance to our best models with 10M parameters.\nMoreover, our models performed similarly on several different training distributions for $D_F$ and $D_1$ and tested on $U(\u2212\u03c3, \u03c3)$ for \u03c3\u2208 {1,2}. Finally, given that 100 samplings with $D = U(\u22122, 2)$ nets on average 25 functions with coefficients the model trained on $D_F = D_1 = U(\u22121,1)$ has not seen (see Appendix Section I), we would expect that if only memory were involved, the model's error rate on $U(-2, 2)$ would be substantially higher than it is. The generalization abilities in Section 6 of our models and their ability to calculate zeros in Section 7 also suggests that ICL involves more than associative memory.\nInterestingly, it is the training regime in which the model must predict sequences from function classes it has not seen in training that forces the model to generalize better, as we see from the strong performance of the $M_{135}$ and $M_{135AL}$ models. This also shows that AL only models can generalize given the right training. Of course, associative memory still plays an important role. Clearly, boundary values are stored and from Observation 3 strongly affect prediction.\nOur models' diverse capacities (but limited in terms of generalization) come from their ability to estimate continuations of a given sequence using principally their attention layers. Given that $Attention(X_i) + X_i$ takes the attention layer output and adds $X_i$, we can calculate the limit of seen values when training over uniform distributions. These correspond empirically to the interval given by the model's boundary values $B^-, B^+$. Thus, given an input $x = (x_1,...x_n)$, attention defines a projection $A(x) \\rightarrow [B^-, B^+]$. Attention layers successively refine this projection to predict $f(x_n)$, as we have seen that multiple layers and multiple heads improve performance.\nWe have seen that the projection $A$ is effectively nonlinear on elements outside of $[B^-, B^+]$. This limitation is due, we conjecture, to training on a restricted interval. Nevertheless, the limitation is not easily removed. While training with distributions over much larger intervals, for example $U(-100, 100)$, might make the attention projection linear over a larger field than $F_{[B^-,B^+]}$ (the one defined over $[B^-, B^+]$), Table 1 shows that such training results in very bad performance on all the testing scenarios we tried. Thus, we see a dilemma: training on restricted intervals is needed for good results, but it inevitably hampers generalizability."}, {"title": "9. Conclusion", "content": "We have distinguished two notions of learning of mathematical functions: ICL1, the learning of function graphs over restricted intervals, and ICL2, the learning of a class form $a_1x^n + a_2x^{n-1} + ...a_n$ for a function in $P_n$. We have shown that transformer models can ICL1 any continuous function within selected intervals. Our pretrained models also acquired surprising learning capacities on unseen polynomial classes and on finding zeros of polynomials, which they did better than state of the art LLMs. However, we have also shown a systematic failure of generalization for decoder-only transformer models of various sizes (up to 9.5 million parameters) and architectures, even when trained on non-noisy data. We have isolated the cause of this problem in the attention mechanism and in its pretraining. Given our results and the discussion above, we do not see an easy solution to this problem."}, {"title": "Impact Statement", "content": "The implications of this work are important not only for understanding ICL but also for optimizing its practice. First, ICL has limited generalization abilities with respect to out of training distribution data, but it seems highly capable at least in modeling mathematical functions when training and testing distributions for data are aligned. Second, users of ICL should know the training regime of the models; trying to ICL functions outside the training distribution will very likely lead to degraded results that can importantly affect downstream tasks. So if users are not pretraining the models themselves, then the model builders should make training data and distributions available. Though we are working on very simple data, it is highly likely that this lesson applies to ICL in more challenging areas like NLP."}, {"title": "A. Training details", "content": "Additional training information: We use the Adam optimizer (Diederik, 2014), and a learning rate of $10^{-4}$ for all models.\nComputational resources: We used 1 GPU Nvidia Volta (V100 - 7,8 Tflops DP) for every training involved in these experiments."}, {"title": "B. Error rates with Gaussian test and training distributions", "content": "When $D_1 = D_F = \\mathcal{N}(0,\u03c3)$ there is for $x \\in \\mathcal{N}(0, \u03c3)$ an over 85% chance of $f(x) \\in [-4\u03c3\u00b2 \u2013 2\u03c3, 402 + 2\u03c3]$ and a 95% chance $f(x) \\in [-2\u03c3, 2\u03c3]$. So a model with $\u03c3 = 1 \\DF = D_1 = \\mathcal{N}(0,1)$ has seen sequences of values for f with $f(x) \\in [-2, 2]$ more than 95% of the time."}, {"title": "C. Graphs for |x| and for $f(x) = x^5 - x + 1$", "content": "See figures 6 and 5"}, {"title": "D. Error progression for models trained on U(\u22121, 1) and tested on U(\u2212\u03c3, \u03c3)", "content": "Based on Galois' theorem, this polynomial has only one real value and which can be only determined approximately. When we apply Newton's method to look for its zero, choosing starting point x 0, the method does not converge."}, {"title": "F. Calculation of what causes boundary values", "content": "Given that our ablation study showed the existence of boundary values for models just with attention layers and without residual learning, norm or feed forward linear layers, we analyze the attention mechanism mathematically to see where boundary values could arise/ Let's consider $X\u00b9 = (X_1, ..., X/h)$ is the input that goes through the multihead attention in a layer l. The output of Multihead Attention after going through 8 attention heads, then a Linear layer is : $(C_1, C_2, ..., C_h)$ where\n$C_h = \u03a3_{i=1}^{h} W_o*softmax(\n\\frac{(W^Q*X^T)*(W^K*X))}{\\sqrt(d_k)}\n)*(W^V*X_i)$\nwhere $W_o W^Q W^K W^V$ are respectively Query, Key and Value weights matrices in attention head h in layer l, dk is the dimension of the key matrix and Wo is the matrix of the linear layer that comes after the attention heads in the layer 1. All those matrices have fixed values from training.\nTo investigate what is causing boundary values into the attention block, Let's consider a model of 1 layer, and take a fixed $X\u00b9 = (X_1, ..., X)$, we consider also $1000X_1 = (1000X_1, ..., 1000X)$.\nThe output of the model for 1000X\u00b9 is: $(C_1, C_2, ..., C_h)$ where\n$C_i = \u03a3_{i=1}^{h} W_o*softmax(\n\\frac{(W^Q*1000X^T)*(W^K*1000X))}{\\sqrt(d_k)}\n)*(W^V*1000X_i)$ (1)\nNow assuming that the learned embedding $\\epsilon : R \\rightarrow Z^{256}$, for some set Z, somewhat respects arithmetic operations, then $\\epsilon(1000X) = \\epsilon(\u03b1) \u00d7 \\epsilon(X)$ and we could infer from Equation 1 and the fact that the matrices are all linear projections on R,\n$\u03b1\u03a3_{i=1}^{h} W_o*softmax(10^6 *\n\\frac{(W^Q*X^T)*(W^K*X))}{\\sqrt(d_k)}\n)*(W^V*X_i)$ (2)\nThe output of each layer will reduce these large values given small values of soft- or even hardmax. But as is evident from Table 4, for large values v, hardmax(v) and softmax(v) give us probability 1 on the greatest value and 0 for the rest. So this derivation predicts that the attention mechanism should give large values for large inputs. But it does not.\nThis proof rests on two assumptions: (i) the learned embedding somewhat preserves arithmetical operations (ii) the matrices that defined Attention are in fact linear projections in R. We checked the \u201cvanilla\" encodings ev on GPT2, and we saw that that embedding ev (a * b) is typically not even close using cosine similarity to \u03b5\u03bd(\u03b1) * \u03b5\u03bd (b), where * is some arithmetical operation. However as can be seen from Figure 11, the learned embedding from our pretraining preserves general mathematical operations and induces an almost perfect linear ordering on [-1000, 1000]. This entails then that at least one of the matrices used to define Attention $W^Q, W^K, W^V$ is only linear on the small finite field F = ([B\u00af, B+], +F, XF,0F, 1F)."}, {"title": "G. Selected functions for \u201dfinding zeros of unknown functions\"", "content": "The functions we looked for their zeros with their following scores for different models are for our scores are: cos(ax) for x \u2208 [0, \u03c0], \u03b1 \u2208 [-1,1], sin(ax) for x \u2208 [\u2212\u03c0/2, \u03c0/2], \u03b1 \u2208 [-1, 1], tan(ax) for x \u2208] \u2212 \u03c0/2, \u03c0/2[, a \u2208 [-1,1], aexp(x) \u2013 b for a, b \u2208 [0, 1], x \u2208 [-1,1] and ln(ax) for a, x \u2208]0, 1]."}, {"title": "H. More details on Zeros of functions", "content": "Table 5"}, {"title": "I. Calculating proportionality of test in train", "content": "Using tests for coefficients on $U(\u2212\u03c3, \u03c3)$ show us how error rates evolves when we increase the proportion of test elements outside the training distribution. We start by testing on x and coefficients in $U(\u22121, 1)$ where the model have seen all the values, then we go through U(-1, 1) where the model has seen fewer values. For example for degree 1, the model has seen values during training a, b, x \u2208 [-1,1], which means $ax + b \u2208 [\u22122,2]$\nGiven a \u2208 $U(\u22122, 2)$ and x \u2208 $U(\u22121,1), ax \u2208 Z$ where $Z = X_1X_2$ is the product of two random variables and an addition. The probability that the model was asked to ICL a value it didn't see during training is\n$P(X = ax + b \u2209 [\u22122,2]) = 1 \u2212 P(X = ax + b \u2208 [\u22122, 2])$"}, {"title": "J. Data on prompt length", "content": "While at least n+1 points are needed to find a polynomial in $P^n$ function, all model performance regardless of training distribution degrades when the size of the prompt during inference is greater than the maximal size of prompts seen in training. Figure 12 (Appendix J)."}]}