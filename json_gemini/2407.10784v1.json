{"title": "ADAPTABLE: TEST-TIME ADAPTATION FOR TABULAR DATA VIA SHIFT-AWARE UNCERTAINTY CALIBRATOR AND LABEL DISTRIBUTION HANDLER", "authors": ["Changhun Kim", "Taewon Kim", "Seungyeon Woo", "June Yong Yang", "Eunho Yang"], "abstract": "In real-world applications, tabular data often suffer from distribution shifts due to their widespread and abundant nature, leading to erroneous predictions of pre-trained machine learning models. However, addressing such distribution shifts in the tabular domain has been relatively underexplored due to unique challenges such as varying attributes and dataset sizes, as well as the limited representation learning capabilities of deep learning models for tabular data. Particularly, with the recent promising paradigm of test-time adaptation (TTA), where we adapt the off-the-shelf model to the unlabeled target domain during the inference phase without accessing the source domain, we observe that directly adopting commonly used TTA methods from other domains often leads to model collapse. We systematically explore challenges in tabular data test-time adaptation, including skewed entropy, complex latent space decision boundaries, confidence calibration issues with both overconfident and under-confident, and model bias towards source label distributions along with class imbalances. Based on these insights, we introduce AdapTable, a novel tabular test-time adaptation method that directly modifies output probabilities by estimating target label distributions and adjusting initial probabilities based on calibrated uncertainty. Extensive experiments on both natural distribution shifts and synthetic corruptions demonstrate the adaptation efficacy of the proposed method.", "sections": [{"title": "1 INTRODUCTION", "content": "Tabular data is one of the most abundant forms of data in the industrial world, covering diverse fields such as healthcare , finance , and manufacturing to name a few. However, due to their ubiquity and vast quantity, tabular data in the wild frequently exhibits distribution shifts . For instance, electronic health record (EHR) data collected from a senior-majority cohort may differ in distribution compared to a junior-majority cohort. Under such data distributions, machine learning models trained on data from the senior cohort may not function as intended on data from the junior cohort at test time. Such distribution shifts pose a significant problem in deploying learned models as they undermine their integrity at test time.\nDespite the importance of addressing such prevalent distribution shifts in tabular data, there have been limited studies on domain adaptation for tabular data compared to other domains like computer vision, natural language processing and speech processing . The difficulties of developing domain adaptation strategies for the tabular realm stem from two primary reasons. First, tabular data displays unique and heterogeneous attributes, including both numerical and categorical features across columns, where these attributes exhibit substantial variations. Second, deep tabular learning models often exhibit limitations in their representation learning schemes in tabular data, which impose constraints on domain adaptation methods that rely on utilizing the feature space of the model. These factors are intricately intertwined, making tabular domain adaptation exceptionally challenging."}, {"title": "2 DESIGN PRINCIPLES ON TEST-TIME ADAPTATION FOR TABULAR DATA", "content": "In this section, to establish the design principles of TTA on tabular data, we conduct a comprehensive analysis of the shortcomings observed in existing test-time adaptation strategies, including entropy minimization-based approaches, as outlined in Section 2.1. Furthermore, we discuss the issue of label distribution shift between source and target domains, along with addressing the class imbalance problem in Section 2.2."}, {"title": "2.1 FAILURE OF EXISTING TEST-TIME ADAPTATION METHODS ON TABULAR DATA", "content": "Domain adaptation refers to methods used in adapting machine learning models to real-world data that may differ in distribution from their training data. There are two main branches: traditional supervised/unsupervised domain adaptation , which often require both source and target data during training, and test-time adaptation , a novel approach that adapts the model using only unlabeled target data during the inference phase. Indeed, developing test-time adaptation strategies for tabular data is particularly promising due to their applicability; making off-the-shelf models adaptable during deployment, which may suffer from distribution shifts during testing, all the while evading privacy concerns from viewing source data in hindsight, a crucial issue in fields such as healthcare and finance.\nHowever, we observe that direct application entropy minimization methods and their variations, most abundant forms of fully test-time adaptation in other modalities like image and speech , fail to show their efficacy in the tabular domain. We find a number of grounds for this. First, we demonstrate that the distributions of prediction entropy consistently exhibit a strong bias towards the high region across various tabular datasets and model architectures, including MLP, TabNet , and FT-Transformer . As shown in Figure 1 (a) and (b), such phenomenon is unique to real tabular datasets such as CMC , not linearized images like Optdigits . This aligns with the findings in the vision domain Niu et al. (2022; 2023), where samples with high entropy generate significant gradients leading to model collapse. These works Niu et al. (2022; 2023) bypass this by filtering samples with high entropies. Nevertheless, predictions from deep learning models in the tabular domain tend to exhibit an under-confident tendency, where applying this technique to the tabular domain results that discarding samples with high entropy leaving very few samples for model update (Figure 1 (c)). In terms of probability calibration, in contrast to the generally overconfident nature of computer vision models, deep learning models trained on tabular data show overall poor calibration. However, it's worth noting that the tendency towards under-confidence or overconfidence varied across datasets (Figure 2 (a) and (b)).\nWe also conduct a comparison between the latent space of models trained on tabular data and those trained on linearized images, as shown in Figure 2 (c) and (d). Through this analytical exploration, it is discerned that the decision boundary within the latent space of tabular data is markedly more complex; attributable to the absence of clear class separation within the latent space. This observation provides another indication of the limitations of entropy minimization, which heavily relies on the cluster assumption \u2013 lower data density results in a closer decision boundary, leading to increased uncertainty. We also find that the majority of other TTA methods, such as test-time training methods"}, {"title": "2.2 LABEL DISTRIBUTION SHIFT AND CLASS IMBALANCE PROBLEM", "content": "Label distribution shift, which is a mismatch in output class label distribution between the source domain and target domain, has been a significant challenge in various machine learning tasks . In situations of distribution shift in tabular data, label distribution shift commonly occurs because of factors such as data collection patterns, cohort differences, and spatial and temporal changes. These challenges can significantly impact model performance, emphasizing the importance of considering them in the context of tabular test-time adaptation.\nRecently, in the field of test-time adaptation for image classification, methods have been proposed to address such issue of label distribution shifts . However, these methods make the impractical assumption that the source model is trained on perfectly class-balanced data. This assumption might hold for standard image classification benchmarks like MNIST  or CIFAR-10 but is hard to be satisfied in the tabular domain. Under the class imbalanced scenario of the source domain, we find that models tend to produce biased predictions during testing based on the class distribution observed in the training data (Figure 3). These findings pose an additional challenge of addressing both label distribution shift and class imbalance, thus highlighting the need for developing a novel test-time adaptation method for the tabular domain.\nBefore delving into the intricacies of our approach, we highlight a pivotal finding: uncertainty calibration plays a crucial role in mitigating the label distribution shifts. Our method fundamentally estimates the average label distribution of the current batch and it corrects predictions by maintaining confident predictions while adjusting uncertain predictions towards the estimated average label distribution of the current batch. As shown in Table 1, we observe that if the source model is perfectly calibrated by increasing the confidence for correct samples while decreasing the confidence for incorrect samples, our label distribution handler leads to a remarkable improvement in performance. This highlights the critical role of developing an effective uncertainty calibrator for the label distribution handler."}, {"title": "3 ADAPTABLE", "content": "In this section, we introduce an Adaptation for Table (AdapTable) framework, which is the first tabular test-time adaptation strategy. AdapTable focuses on correcting the output probability and circumvents the need to tune the model parameter using unsupervised objectives. To this end, we aim to maintain predictions for confident test samples while heavily correcting predictions for uncertain test samples. To estimate the model's uncertainty in cases of poorly calibrated source models (as"}, {"title": "3.1 TEST-TIME ADAPTATION SETUP FOR TABULAR DATA", "content": "We first define the problem setting of test-time adaptation for the tabular domain. Let $F(\\cdot|\\theta): \\mathbb{R}^D \\rightarrow \\mathbb{R}^C$ be a pre-trained classifier on the labeled source tabular domain $D_s = \\{(x_i, y_i)\\}_{i=1}^{D_s}$ in pairs of a tabular input and its output class label, which takes a row $x_i \\in \\mathbb{R}^D$ from a table and returns output logit $F(x|\\theta)$. Here, $D$ and $C$ are the number of input features and output classes, respectively. We focus on tabular classification, and suggest test-time adaptation methods for tabular classification aim to adapt $F(\\cdot|\\theta)$ to the unlabeled target tabular domain $D_t = \\{x_i\\}_{i=1}^{D_t}$ during the test phase without access to $D_s$. Unlike most TTA approaches which directly fine-tune $\\theta$ with unsupervised objectives, we correct the output prediction $F(x|\\theta)$ directly."}, {"title": "3.2 SHIFT-AWARE UNCERTAINTY CALIBRATOR", "content": "Since source tabular models are poorly calibrated as discussed in Section 2.1 and uncertainty calibration is helpful for handling label distribution shifts, given a test batch $\\{x_i\\}_{i=1}^{N}$ in the target domain, we propose a shift-aware post-hoc uncertainty calibrator $G(\\cdot,\\cdot|\\phi): \\mathbb{R}^{NC} \\times \\mathbb{R}^D \\rightarrow \\mathbb{R}$ that incorporates shift information and simultaneously addresses both overconfidence and under-confidence, departing from traditional approaches that primarily rely on output logits only and focus only on either overconfidence or under-confidence . Here, $G$ takes not only the output logit $F(x|\\theta)$ of each test instance $x$ but also the shift information of current batch $s_t$ as additional information and returns a per-sample temperature scaling factor $\\tau_i = G(F(x|\\theta), s_t|\\phi)$. We define shift information $s_u^t$ of a specific column index $u$ as follows:\n\n$s_u^t = \\left|\\frac{1}{N} \\left(\\sum_{i=1}^{N} x_{iu}^t\\right) - \\left(\\sum_{i=1}^{D_s} x_{iu} / D_s\\right)\\right|$\n\nHere, we focus on the distribution shift of each column, which is a tabular-specific input covariate shift, and form a shift-aware graph, where each node represents a specific column, and each edge represents the relationship between different columns. Specifically, we form a node feature $h_u^{(0)}$ of"}, {"title": "3.3 LABEL DISTRIBUTION HANDLER", "content": "This section proposes a label distribution handler, for correctly estimating the target label distribution on the current test batch. Formally, if we estimate the class probability given $x$ in the target domain $p_t(y|x) = \\text{Softmax}(F(x))$ with Softmax function softmax, using Bayes' theorem, this can be represented as follows:\n\n$p_t(y|x) = \\frac{p_t(y) p_t(x|y)}{p_t(x)}$\n\nBased on the observation that the distribution of the model in the target domain is biased towards the label distribution of the source domain as discussed in Section 2.2 and Figure 3, we can infer that $p_t(y)$ is highly biased to $p_s(y) = (\\sum_{j=1}^{D_s} \\mathbb{1}[j=y] / |D_s|)$, with an indicator function $\\mathbb{1}$. One simple solution to correct this biased probability is to multiply $p_t(y)/p_s(y)$, i.e., use $p_t(y|x) \\cdot p_t(y)/p_s(y)$ as our final prediction. We assume that we can access the marginal label distribution of the source domain similar to TTT++ (Liu et al., 2021). To estimate the marginal label distribution of the target domain $p_t(y)$ without labels, we derive an online target label estimator to leverage the temporal locality of label distribution in real-world scenarios, where class labels of test samples collected at close time intervals tend to exhibit relative similarity. Additionally, we introduce a debiased target label estimator to mitigate bias towards the source label distribution. At first, we initialize online target label estimator $p_e^0(y) = (1/C)_{j=1}^C$ as a uniform distribution. Given a test batch $\\{x_i\\}_{i=1}^{N}$, we predict the debiased target label estimator $p_{de}(y|x)$ and estimate the current target label distribution $p_t(y)$ as follows:\n\n$p_{de}(y|x) = \\sigma_{L_1}(p_t(y|x)/p_s(y))$\n\n$p_t(y) = (1 - \\alpha) \\cdot \\sum_{i=1}^{N} p_{de}(y|x) / N + \\alpha \\cdot p_e^t(y)$,\n\nwhere the interpolate cumulative online target label estimator and the average probability of the current debiased target label estimator with $L_1$ normalization function $\\sigma_{L_1}$. Now, we utilize the shift-aware uncertainty calibrator defined in Section 3.2. Given a current test batch $\\{x_i\\}_{i=1}^{N}$, we calculate $s_t$ and get per-sample temperature $\\tau_i = G(F(x|\\theta), s_t|\\phi)$. We define the uncertainty $\\epsilon_i$"}, {"title": "4 EXPERIMENTS", "content": null}, {"title": "4.1 EXPERIMENTAL SETUP", "content": "Source Tabular Models and Datasets To verify the proposed method under various tabular models, three representative deep tabular learning models - MLP, TabNet, and FT-Transformer \u2013 are used as source tabular models. To demonstrate our approach across various test-time distribution shifts, we test our method under"}, {"title": "4.2 MAIN RESULTS", "content": "Result on Natural Distribution Shifts Table 2 shows the result on natural distribution shifts. We observe that our method consistently outperforms baselines in most settings across different datasets and backbone architecture types. Furthermore, it is worth noting that our method, integrated with CatBoost , can also effectively adapt in these settings, where baseline TTA approaches cannot be applicable. As discussed in 2.1, entropy minimization variants fail under most datasets, as well as the baselines that depend on cluster assumptions on latent space - TTT++ , EATA and LAME .\nResult on Synthetic Corruptions We further evaluate the efficacy of AdapTable on different datasets of OpenML-CC18 benchmark  on synthetic corruptions, depicted in Table 3. Please note that we report the average performance across six synthetic corruptions. The overall trend is similar to real-world distributional shifts \u2013 showing state-of-the-art performance on most datasets and evaluation metrics, showing that our method can effectively adapt under various"}, {"title": "4.3 FURTHER ANALYSIS", "content": "Ablation Study To prove each component of the proposed method towards performance improvement, we conduct an ablation study. As shown in Table 4, we believe that the proposed shift-aware uncertainty calibrator as well as the graph convolution layer within it effectively increases the accuracy under both natural distribution shifts (HELOC) and synthetic corruptions (MFEAT-PIXEL, CMC).\nEfficacy of Shift-Aware Uncertainty Calibrator To verify the proposed shift-aware uncertainty calibrator suggested in Section 3.3, we compare the reliability diagram before and after applying shift-aware uncertainty calibrator in Figure 5 (a) and (b). The proposed method's ability not only to bring the reliability diagram closer to the $y = x$ line but also to reduce confidence and increase uncertainty for shifted test sets is indeed noteworthy.\nEfficacy of Label Distribution Handler Figure 5 (c) compares Jensen-Shannon Divergence value of test label distribution and prediction on each batch, between before and after adaptation using label distribution handler (LDH) for HELOC  dataset. The figure represents a large amount of decline of divergence, which implies label distribution handler 3.3 can indeed reduce a gap between shifted target label distribution and source-oriented model prediction."}, {"title": "5 CONCLUSION", "content": "In this paper, we have introduced AdapTable, a tabular-specific test-time adaptation strategy for the first time. We have systematically investigated the challenges related to test-time adaptation for tabular data and have proposed test-time adaptation strategies to overcome such problems with two key components: a shift-aware uncertainty calibrator, which utilizes information about covariate shifts among columns to correct poor confidence calibration in the model, and a label distribution handler, which estimates the label distribution of the current test batch in real-time and uses the improved uncertainty to adjust the output distribution. Extensive experiments have demonstrated that the proposed method achieves state-of-the-art performance across different datasets with both natural distribution shifts and synthetic corruptions and three representative deep tabular learning architectures. AdapTable provides a foundational philosophy for designing test-time adaptation in the context of tabular data and sheds light on the domain adaptation field for tabular data, offering valuable insights and directions for future research."}, {"title": "ETHICS STATEMENT", "content": "Tabular data often contains sensitive personal information, such as medical test results in healthcare data or financial details in economic data. Therefore, it is crucial to handle such data with utmost care and consideration for privacy and security concerns. In this regard, AdapTable, which enables the adaptation of a model to the target domain without requiring access to sensitive source domain data, holds significant appeal. We believe that the proposed method has the potential to address privacy and security concerns associated with sensitive source domain data while still achieving effective adaptation to the target domain."}, {"title": "REPRODUCIBILITY STATEMENT", "content": "To ensure reproducibility, we will release the source code soon."}]}