{"title": "FEATURE GROUP TABULAR TRANSFORMER: A NOVEL APPROACH TO TRAFFIC CRASH MODELING AND CAUSALITY ANALYSIS", "authors": ["Oscar Lares", "Hao Zhen", "Jidong J. Yang"], "abstract": "Reliable and interpretable traffic crash modeling is essential for understanding causality and improving road safety. This study introduces a novel approach to predicting collision types by utilizing a comprehensive dataset fused from multiple sources, including weather data, crash reports, high-resolution traffic information, pavement geometry, and facility characteristics. Central to our approach is the development of a Feature Group Tabular Transformer (FGTT) model, which organizes disparate data into meaningful feature groups, represented as tokens. These group-based tokens serve as rich semantic components, enabling effective identification of collision patterns and interpretation of causal mechanisms. The FGTT model is benchmarked against widely used tree ensemble models, including Random Forest, XGBoost, and CatBoost, demonstrating superior predictive performance. Furthermore, model interpretation reveals key influential factors, providing fresh insights into the underlying causality of distinct crash types.", "sections": [{"title": "1 Introduction", "content": "According to the National Highway Traffic Safety Administration (NHTSA), an estimated 40,990 people died in motor vehicle traffic crashes in 2023 [1], highlighting the urgent need for enhanced road safety measures and policies. The inherent complexity of traffic dynamics, coupled with evasive factors, such as driver and human behavior, vehicular, and environmental conditions, makes crash modeling a challenging endeavor. Gaining a deeper understanding of the intricate and multifaceted interactions among road users, and between road users, roadway infrastructure, and weather conditions, is an essential for developing effective safety interventions. These can include enhancements to geometric design and traffic control devices, the implementation of new traffic regulations, and the integration of advanced vehicle and infrastructure technologies. Collectively, these measures have the potential to significantly reduce the frequency and severity of traffic crashes.\nExisting approaches to crash analysis have two critical gaps. First, traditional approaches rely on narrowly defined datasets, excluding critical variables such as detailed traffic patterns, environmental conditions, pavement characteristics, and driver behavior due to data accessibility challenges. This feature exclusion hampers the ability to model the multifaceted interactions among diverse factors. Second, while machine learning (ML) and deep learning (DL) models have shown strong predictive capabilities, their interpretability remains limited, particularly in deriving actionable insights for policy formulation or engineering design.\nTo address these challenges, we present a novel approach that leverages an extensive, integrated dataset encompassing weather conditions, traffic data, road geometry, pavement conditions, and driver characteristics. By fusing data from"}, {"title": "2 Literature review", "content": "Crash collision and severity modeling has been a widely investigated topic utilizing various kinds of data. Typically, the approaches focus on modeling crash severity to identify factors mostly correlated with traffic crashes to provide effective countermeasures. However, not much emphasis has been placed on modeling and predicting collision types, as they are typically used as independent features within those studies. D.-G. Kim et al. [3] carried out a study modeling and predicting crash collision types at intersections in rural areas of Georgia as opposed to focusing on crash severity, noting that countermeasures taken to mitigate crashes at the severity level could potentially only affect a subset of all crashes. Thus, countermeasures can potentially be more effective if research is conducted to understand contributing factors to different collision types in addition to severities. The authors also noted that collision types are associated with road geometry, environment, and traffic variables in different ways than crash severities are, which serve as justification for modeling them separately. While this study focuses on predicting the crash types, it is imperative to acknowledge the interconnection of crash types and severity. Typically, the methods employed to predict these outcomes (crash severity vs. crash type) often overlap, leveraging similar datasets and analytical frameworks. Many crash severity studies and modeling approaches directly use crash type as an input feature for modeling severity and risk [4, 5, 6]. Moreover, interventions designed to mitigate crash severity such as the implementation of advanced driver-assistance systems (ADAS) can also influence the likelihood of certain types of collisions occurring in the first place [7]. Therefore, while the primary objective of this study focuses on crash types, a comprehensive literature review pertaining to predicting crash severity is also included.\nStatistical models have widely been applied to analyze crash collision and risk with respect to varying features, offering a robust framework, and identifying key risk factors. Statistical methods offer some advantages when it comes to crash modeling due to their elegant forms and ease in modeling and interpretation. However, they are limited in adequately modeling complex relationships present within crash data. For example, they often assume explicit or fixed interactions among variables, potentially oversimplifying crash complexity and disregarding the dynamic interactions of variables, such as driver behavior, environmental and traffic. Additionally, the quality and availability of data can also significantly impact the model's accuracy and reliability, with issues such as missing data and class imbalance posing notable challenges. Zeng et al. [6] investigated the effects of real-time weather conditions and roadway geometry on the severity of freeway crashes. They utilized an ordered logit model due to the ordered nature of crash severity (low, medium, high). The variables included the hourly wind speed, temperature, precipitation, visibility, and humidity alongside the horizontal curvature, and grading of the roadway and other crash and driver related features. Their results indicated that heavier precipitation contributed more to medium severity crashes, and that more severe crashes tended to occur on freeway segments with small horizontal curve radius and higher vertical gradients. However, they failed to incorporate"}, {"title": "3 Data collection and compilation", "content": "In this study, we compiled data from multiple sources to create a comprehensive dataset. Traffic counts, pavement condition data, and crash data were sourced from the Georgia Department of Transportation (GDOT), while public weather data is readily available from online platforms. By integrating these four different data sources (i.e., crash, traffic, pavement, and weather data), we curated a comprehensive dataset encompassing a wide array of variables. This fused dataset enable a more nuanced understanding of crash collisions. The dataset is not only used to identify the strongly correlated features with crash types but also to provide a broader perspective on the events surrounding crash instances, thereby enhancing crash modeling in ways that other studies may not have considered. Furthermore, this study specifically focused on multi-vehicle (MV) crashes. Previous studies [23, 24] have noted differences in the factors affecting single-vehicle (SV) crashes compared to MV crashes. For instance, Wang & Feng [23] found that factors like traffic volume and speed variance had no significant influence on SV crashes, while the opposite was true of MV crashes.\nHigh-resolution traffic data, such as traffic counts and speeds, were obtained at 5-minute intervals from various continuous count stations (CCS) throughout the state of Georgia. Crash incidents from police report data were first matched to these CCS stations both temporally and spatially. After the sites and crashes were matched, the high-resolution traffic data was aggregated into hourly intervals for modeling purposes. Weather data for the crash instances was obtained from Weather Underground (The Weather Underground, 2023), with weather variables extracted from weather stations that were spatially matched to the corresponding CCS sites. Finally, pavement condition data was matched to the appriopriate CCS sites based on geographic coordinates, providing pavement condition details for each crash instance.\nAfter validating and confirming the correct matches among all crashes, CCS sites, pavement features, and weather variables, the data was fused into a single combined dataset. Subsequently, feature selection and reduction were carried out. Since the crash data comes from statewide police reports, several features were purposely excluded due to their post-incident nature. Given that this study aimed to identify the predictive variables with the greatest impact on crash collisions prior to their occurrence, \"after-effect\" features were not considered. Additionally, other crash-related features were also removed, particularly those influenced by the subjective judgment of the police officer writing the report, such as the contributing factors to the crash and the specific harmful event elements pertaining to each crash. Other redundant crash features from the police reports such as the road characteristics (e.g., whether the road was straight or curved), were omitted in favor of more accurate, quantified pavement measurements that better portray the pavement conditions and road geometry.\nThe compiled dataset had missing values for variables such as driver ages, traffic speeds, and precipitation data. These missing values were imputed using averages from relevant groups to maintain consistency across similar conditions. After processing, the final dataset consists of 6,810 multi-vehicle (MV) crash instances and 33 features."}, {"title": "4 Methodology", "content": "In this study, we frames crash type inference as a multi-class classification problem. To enhance both performance and interpretability in tabular data modeling, we propose the Feature Group Tabular Transformer (FGTT), leveraging the transformer architecture [2]. The FGTT is evaluated against three tree ensemble baselines, including Random Forest (RF), eXtreme Gradient Boosting (XGBoost) and Categorical Boosting (CatBoost). These tree ensemble methods are well-established and widely recognized for their robust performance in supervised learning tasks involving tabular data. This section provides a brief overview of each ensemble method, followed by a detailed description of our proposed FGTT."}, {"title": "4.1 Ensemble methods", "content": "Ensemble methods, which combine multiple decision trees to make predictions, have gained significant popularity due to their flexibility and robustness in handling large tabular datasets. Among the most widely used ensemble methods are random forest and gradient boosting. Random forest is a parallel ensemble method, while gradient boosting is a sequential ensemble method. Within Gradient Boosting, two state-of-the-art approaches are XGBoost and CatBoost."}, {"title": "4.1.1 Random forest", "content": "Random Forest (RF) is a versatile machine learning algorithm. It operates by constructing a multitude of decision trees at training time and outputting the most common prediction (for classification) or the average prediction (for regression) from the individual trees. This helps reduce prediction variance and prevent overfitting, a common issue with single decision trees. The algorithm randomly selects subsets of both data and features at each split point, making it more robust. For classification, each tree votes for a class, and the class with the most votes becomes the final prediction. RF is particularly well-suited for predicting crash types due to its ability to handle complex, non-linear relationships between features like speed, weather conditions, road conditions. For example, Khanum et al. [25] and Morris and"}, {"title": "4.1.2 XGBoost", "content": "XGBoost, is an advanced implementation of gradient boosting algorithms [26]. It has gained popularity in the machine learning community for its speed, performance and versatility. Different from conventional first-order tree ensemble, XGBoost is a second-order method with an objective function expressed in Equation (1).\n$L(\\phi) = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k} \\Omega(f_k)$                                                                              (1)\nwhere l(yi, \u0177i) is the loss function measuring the discrepancy between the predicted \u0177r and the actual target y_i, which is approximated by second-order Taylor expansion; \u03a9(fk) denotes the regularization term, which penalizes the complexity of tree fk, typically by the number of nodes and the magnitude of leaf weights. XGBoost recursively chooses a feature split that maximizes the gain (or reduction in loss). In the context of crash type classification, Yang et al. [27] utilized XGBoost for predicting crash severity and employing SHapley Additive exPlanation (SHAP) for model interpretation."}, {"title": "4.1.3 CatBoost", "content": "CatBoost, short for Categorical Boosting, is a state-of-the-art open-source gradient boosting library developed by Yandex [28]. It is specifically designed to work well with categorical data and is known for its performance, accuracy, and ease of use. CatBoost can efficiently handle categorical features through ordered target statistics while addressing the target leakage issue. Another distinguishing feature of CatBoost is ordered boosting, a permutation-based enhancement to traditional gradient boosting. This approach addresses bias and prediction shift by utilizing independent datasets at each gradient step, a process formally described by Equation 2.\n$y_i^{(t)} = y_i^{(t-1)} + \\alpha h_t (x_i, D_{train})$                                                                            (2)\nwhere:\n\u2022\u00fbt) is the prediction for the i-th instance at iteration t.\n\u2022 ht (xi, Drain) is the model built on the training subset Drain, which includes only data points that precede xi in the current permutation.\n\u2022 \u03b1 is the learning rate, controlling the step size of the boosting algorithm.\nIn addition to the ordered boosting, CatBoost uses oblivious trees as base predictors, which are computationally efficient and lead to faster predictions while reducing the risk of overfitting. As a state-of-the-art model for tabular data with categorical features, CatBoost naturally serves as a strong baseline for crash type prediction in this study. Previous research has successfully applied CatBoost to crash modeling, as demonstrated by Hasan et al. [29], J. Li et al. [30], and Morris and Yang [18]. For interpreting tree ensemble models, the SHapley Additive exPlanations (SHAP) framework, developed by Lundberg et al. [31], has been widely adopted ([29, 30, 18]). SHAP values quantify the contribution of each feature to a specific prediction by comparing the mode's output with and without the inclusion of that feature, offering insights into the model's decision-making process."}, {"title": "4.2 Feature group tabular transformer", "content": "This section introduces the proposed method, the Feature Group Tabular Transformer (FGTT). We begin with a brief overview of Transformers and the multi-head self-attention mechanism, followed by an explanation of our feature grouping approach."}, {"title": "4.2.1 Transformer encoder", "content": "Transformers [2], initially designed for sequential data, have become the architecture of choice for a wide range of natural language processing (NLP) and computer vision tasks. The core innovation of the transformer is the self-attention mechanism, which enables the model to focus on relevant input tokens based on their relationships to one another. This mechanism, referred to as \"scaled dot-product attention\" (Equation 3), computes the attention weight by\n$Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$                                                                         (3)\nWhere Q and K are the query and key vectors, with the embedding dimension of dk. V is the value vector. The dot products of the queries and keys are computed and divided by the square root of \u221adk, followed by a softmax function to obtain the weights. This process is repeated across multiple heads n-times and results are concatenated together.\nIn this study, only the Transformer encoder is employed, as its feature encoding are expected to enhance classification performance. The prediction approach is similar to that of Dong et al. [20], where a linear regression head was appended to the encoder model. However, in this work, a MLP classifier is used instead of a linear regression model to make final crash type predictions [21]. The Transformer encoder architecture utilized in this study is illustrated in Figure 2."}, {"title": "4.2.2 Feature group tokens", "content": "Transformers are typically used in the NLP domain, where they process sequential words (or subwords) as vectorized tokens to learn context-sensitive semantic relationships between words to process sentences. Inspired by this concept, the proposed FGTT approach adapts this framework for tabular data in this study, where a \"crash sequence\" is created, consisting of distinct tokens that represents various semantic aspects of a crash event.\nInstead of tokenizing each individual feature in the dataset, an additional step was introduced to encode features by meaningful groups that represent distinct semantic aspects of a crash event. To achieve this, related features were grouped based on their shared characteristics and roles in determining crash outcomes. This grouping reflects different aspects of the events surrounding a crash instance while addressing the overlap between certain features. Each feature group was then encoded into a token vector using a MLP projector, where the resulting token represented the specific semantic contribution of that feature group to the crash instance. This process produced a sequence of tokens, with each encapsulating a meaningful aspect of the crash. By organizing features into these groups, the approach aimed to provide a richer semantic representation of crashes compared to treating features independently. This richer representation was expected to better leverage the learning capability of the Transformer encoder and deliver improved results over traditional ensemble methods."}, {"title": "4.2.3 Proposed FGTT model", "content": "For the proposed FGTT model, features from the dataset are first organized into distinct feature groups, each representing a specific aspect of the crash data. These groups, varying in size and dimensinality due to the differing number of features they contain, are then processed through a \"Feature Group Token Projector\" layer. This step ensures that all feature group tokens are projected to a uniform dimension, a necessary condition for input into the Transformer encoder. The projection process not only standardizes the input but also transforms the feature tokens into a shared representation space, enabling the model to better capture and understand the relationships and interactions among different feature groups. For this purpose, An MLP model was utilized as the feature projector. Once the tokens are projected to a common dimension, they are passed through the Transformer Encoder block. The encoded outputs are then fed to an MLP classifier, followed by a softmax layer to predict probabilities of crash types."}, {"title": "4.3 Performance metrics", "content": "Evaluating machine learning models on imbalanced datasets requires metrics that capture performance beyond simple accuracy. Although overall accuracy, defined as the ratio of correct predictions to total predictions, offers a general assessment, it falls short in scenarios with uneven class distributions. In such cases, metrics like Precision, Recall, and their harmonic mean, the F1 score, offer a more nuanced and comprehensive evaluation.\nPrecision measures the proportion of correctly predicted positive instances among all predicted positives:\n$Precision = \\frac{TP}{TP+FP}$                                                                                                                                 (9)\nRecall quantifies the ability to identify actual positive instances among all true positives:\n$Recall = \\frac{TP}{TP+FN}$                                                                                                                                    (10)\nSince Precision and Recall often exhibit a trade-off, balancing them is critical. The F1 score achieves this balance by calculating their harmonic mean:\n$F\u2081 = 2 \u00d7 \\frac{Precision \u00d7 Recall}{Precision + Recall}$                                                                                                        (11)\nUnlike the arithmetic mean, the F1 score penalizes extreme imbalances between Precision and Recall, making it suitable for tasks involving critical minority classes. In this study, where rare crash types, such as sideswipe and angle crashes, are of primary interest, the F1 score serves as a more reliable and informative metric than accuracy."}, {"title": "5 Experiments, results and discussion", "content": "This section outlines the experiments conducted, including the implementation of each model discussed in Section 4 and their interpretation to better understand the possible crash causality chains. Specifically, SHAP value plots are used to explain the ensemble models, while attention weight heatmaps are generated for the FGTT model to provide deeper insights into the impact and interactions of the feature groups on crash type predictions."}, {"title": "5.1 Experiments settings", "content": ""}, {"title": "5.1.1 Data partition and normalization", "content": "Data partition and normalization are crucial steps in machine learning workflows. The original dataset, as described previously, was divided into three subsets: training, validation, and testing. Instead of using random data splitting, stratified splitting was adopted to preserve the distribution of data across all subsets, as shown in Figure 3. Stratified splitting ensures that each subset (i.e., training, validation, and testing) retains the same class distribution as the original dataset. This is particularly important in scenarios like this study, where certain classes labels, such as angle and sideswipe crashes, are underrepresented. As a result, the original dataset of 6,810 multi-vehicle crashes was partitioned using stratified sampling based on the class label (\u201cCrash_type\u201d), leading to the following splits.\n\u2022 Training Set: 6,026 samples (88.5%) Validation Set: 392 samples (5.75%) \u2022 Testing Set: 392 samples (5.75%)\nMany of the features in this study are categorical. Given the small number of categories for each categorical variable, one-hot encoding is employed. To expedite and stabilize training, numeric features are standardized by subtracting"}, {"title": "5.1.2 Ensemble methods", "content": "A GridSearch cross-validation (CV) strategy was employed for hyperparameter tuning of the ensemble models, including Random Forest, XGBoost, and CatBoost. CV is a commonly used technique in machine learning for hyperparameter tuning. It involves partitioning the training data into subsets, training the model on some subsets while validating the model on the remaining subsets. This process is repeated multiple times, cycling through the subsets, to ensure that each subset serves as both training and validation data. The results are averaged to provide final performance evaluation. To ensure proper class label distribution, the data subsets are stratified. The GridSearch systematically explores the combinations of hyperparameters, cross-validating each combination to determine the best-performing set. The final models were trained with the optimal hyperparameters identified.\nAll tree ensemble models were trained using a consistent methodology, which included hyperparameter tuning thorugh GridSearch CV and leveraging a validation set to implement early stopping to mitigate the risk of overfitting."}, {"title": "5.1.3 FGTT", "content": "A different strategy was employed for tuning the parameters of the FGTT model. Bayesian Optimization (BO) was used to efficiently find the optimal parameter settings. Unlike the GridSearch, which exhaustively evaluate every combination of parameters, BO leverages a Gaussian process to construct a posterior distribution of the objective function based on prior evaluations. It then uses an acquisition function to decide where to sample next. This is particularly advantageous for tuning deep learning models, where the parameter space can be extremely large, and training can be computationally intensive and time-consuming. By intelligently choosing the next set of parameters to evaluate, BO significantly reduces the time and resource required as compared to the grid search method. This method has proven to be efficient and effective in tuning the hyperparameters of complex models [32, 33]. The Optuna package [34] was used for BO implementation."}, {"title": "5.2 Results", "content": "The model performance metrics are summarized in Table 5."}, {"title": "5.3 Model interpretation", "content": "In addition to comparing overall model performance, further analysis and interpretation of model predictions were performed using SHAP values for the ensemble methods and attention weight heatmaps for the proposed FGTT model."}, {"title": "5.3.1 Feature importance by SHAP values for CatBoost", "content": "Figure 4 shows the top fifteen features based on SHAP values for the CatBoost model. The analysis highlights that the event features, such as maneuver of vehicle 1 (Veh1_maneuver) and crash location, and traffic features are consistently the most significant features in inferencing crash types, as evidenced by their high SHAP values. For the event features, the maneuver of vehicle 1 such as changing lanes or passing and keeping straight all have the most impact with the highest SHAP values for the three crash types. Specifically, the maneuver executed by the at-fault vehicle, such as changing lanes, traveling straight, or performing other actions, directly affects the positioning and interaction between vehicles, thereby influencing the type of crash. The crash location also plays important role with higher SHAP values of \"Crash_location_On roadway - Non-intersection\" for rear-end and sideswipe crashes than angle crash.\nThe traffic features, such as the hourly volume (Hourly_volume), hourly average speed (Hourly_avg_speed) and hourly truck ratio (Hourly_truck_ratio), are listed among the top seven most important features across crash types. These results underscore the critical role of vehicle behavior and traffic flow in affecting crash types. Similarly, hourly volume, speed, and truck ratio reflect traffic density, status and exposure, which directly correlate with crash risks. The hourly average speed emerges as a critical feature across crash types, with notable contributions in rear-end and sideswipe crashes. Its significance can be attributed to the direct relationship between speed and the likelihood of these crashes. High average speeds reduce the reaction time available to drivers, increasing the risk of rear-end collisions, especially in high-traffic scenarios. For sideswipe crashes, elevated speeds may amplify the difficulty of safely changing lanes or maintaining control during complex driving maneuvers. The Hourly_truck_ratio, which is unique in our dataset, emerges as a notable feature in the SHAP analysis, contributing to the understanding of crash type dynamics, particularly for rear-end and sideswipe collisions. This feature represents the proportion of trucks in traffic flow during a given hour and is closely tied to crash likelihood due to the unique characteristics and limitations of trucks. The SHAP values indicate that a higher Hourly_truck_ratio generally increases the probability of sideswipe crashes, likely because trucks occupy more space on the roadway, have larger blind spots, and require greater distances to maneuver safely. These factors can create conditions that make lane changes and overtaking more challenging for other vehicles, leading to sideswipe incidents. In addition, Hourly_truck_ratio plays a role in rear-end crashes, albeit with a smaller magnitude of contribution. The large size, heavy weight, and slower acceleration of trucks can create speed differentials in mixed traffic, particularly during peak hours, where traffic density is high. Such conditions may increase the likelihood of rear-end collisions when vehicles following trucks fail to adjust their speed or maintain a safe following distance. These findings underscore the need for targeted interventions, such as lane restrictions for trucks during peak traffic hours, better signage to alert drivers of truck-heavy zones, and enhanced training for drivers on navigating safely around trucks."}, {"title": "5.3.2 FGTT attention heatmaps", "content": "To better understand feature importance for the FGTT model, attention weights were extracted from the last attention layer and examined at different levels. In the context of attention mechanisms, there are two key elements, the \"query\" and \"key\" tokens, play a pivotal role. These tokens are transformed representations of the input data. Specifically, query tokens are used to probe the input data, while key tokens are matched against these queries. The model evaluates the similarity between each query token and all key tokens to compute attention scores. These scores quantify the relevance of each part of the input sequence to the query. Using these scores, the model generate a weighted sum of values, an amalgamation of contextually embedded information that highlights the features deemed most relevant for making predictions. In this study, attention weights for the FGTT model were extracted for three distinct crash types from the test set and passed through the final trained model to identify the features most associated with each crash type. The attention weights were obtained from the last layer of the Transformer encoder which is the layer that produces the final transformer output used for predictions.\nFigure 5 illustrates the class token attention scores toward each feature group for crash type inference. These attention scores signify the importance of each feature group in predicting three crash types: Rear End, Sideswipe, and Angle.\nThe results show that the event feature group consistently receives the highest attention scores across all crash types from the class token. This indicates the dominant role of event-specific details, such as vehicle maneuvers and crash locations, affecting the risks of different crash types. Notably, rear-end and sideswipe crashes exhibit slightly higher attention to event feature group compared to angle crashes, likely reflecting the critical influence of dynamic actions, such as lane changes or sudden braking, which are commonly associated with these types of collisions. In contrast, angle crashes may depend less on immediate events and more on environmental, contextual and pavement factors.\nThe traffic, environment, pavement, driver, and contextual feature groups demonstrate moderate attention scores, suggesting their relevance in providing additional context for crash type prediction. For example, angle crashes exhibit relatively higher attention to environmental features, which may highlight the role of conditions like gust, wind speed, precipitation, and lighting in angle crash scenarios, particularly on the diver's driving perception and pavement conditions. Similarly, the driver and contextual feature groups receive comparable attention across crash types, suggesting the importance of driver characteristics, potentially driver behavior indicated by the driver characteristics, and contextual factors, such as traffic patterns, reflected by the time of day and the day of week. These features likely serve as secondary factors that modulate the primary event-driven dynamics of crashes. These finding aligns with the"}, {"title": "6 Conclusions and future directions", "content": "Drawing from the collective insights gathered throughout the research, it becomes evident that predictive modeling of traffic crash outcomes benefits significantly from the integration of diverse data types and sources. The primary contributions of this study lie in the use of a comprehensive and highly descriptive dataset and the introduction of the FGTT, which efficiently processes and encodes data into group-specific tokens, enabling it to uncover complex relationships between feature groups through attention heatmaps. These semantically rich tokens enhance the model's ability to capture nuanced interactions, resulting in both improved predictive performance and greater interpretability. While the FGTT model demonstrated superior performance compared to popular ensemble methods, opportunities for"}]}