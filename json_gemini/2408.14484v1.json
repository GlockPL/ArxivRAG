{"title": "Agentic Retrieval-Augmented Generation for Time Series Analysis", "authors": ["Chidaksh Ravuru", "Sagar Srinivas Sakhinana", "Venkataramana Runkana"], "abstract": "Time series modeling is crucial for many applications, however, it faces challenges such as complex spatio-temporal dependencies and distribution shifts in learning from historical context to predict task-specific outcomes. To address these challenges, we propose a novel approach using an agentic Retrieval-Augmented Generation (RAG) framework for time series analysis. The framework leverages a hierarchical, multi-agent architecture where the master agent orchestrates specialized sub-agents and delegates the end-user request to the relevant sub-agent. The sub-agents utilize smaller, pre-trained language models (SLMs) customized for specific time series tasks through fine-tuning using instruction tuning and direct preference optimization, and retrieve relevant prompts from a shared repository of prompt pools containing distilled knowledge about historical patterns and trends to improve predictions on new data. Our proposed modular, multi-agent RAG approach offers flexibility and achieves state-of-the-art performance across major time series tasks by tackling complex challenges more effectively than task-specific customized methods across benchmark datasets.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series modeling underpins a vast spectrum of real-world applications, including demand planning [21], anomaly detection [54], inventory management [52], energy load forecasting [24], weather modeling [31], and many others. However, it is not without its challenges. High dimensionality, non-linearity, sparsity, and distribution shifts all pose significant hurdles. Successfully navigating these challenges in time series analysis applications necessitates both considerable domain knowledge and the design of neural network architectures tailored to address task-specific goals, leading to better performance. In contrast to task-specific approaches, which employ different architecture designs for time series analysis, foundational pre-trained large language models (LLMs), such as OpenAI's GPT-4 [29] and Google's Gemini [34, 39], with their strong generalization and logical reasoning capabilities, have shown remarkable versatility across a broad spectrum of natural language processing (NLP) tasks, requiring minimal fine-tuning[17] or only a few demonstrations[2] for adaptation to niche tasks. Open-source, small-scale pretrained language models (SLMs), such as Google Gemma ([40]) and Meta LLAMA ([1, 41]), offer cost-effective domain customization through Parameter Efficient Fine-Tuning (PEFT) ([15, 16]) techniques using task-specific labeled datasets. Additionally, these smaller models can be further aligned with human preferences using Direct Preference Optimization (DPO) [8], a fine-tuning technique that utilizes paired preference data, such as datasets of preferred and dispreferred responses. However, SLMs may lack the reasoning and generalization capabilities of large-scale proprietary language models. The potential of foundational SLMs designed for universal time series applications (a single-model-fits-all approach), such as diverse time series tasks like classification, anomaly detection, forecasting, imputation, and others, remains largely unexplored but holds great promise. This approach contrasts sharply with the traditional approach of using customized, task-specific methods ([43, 49, 50]) for time series modeling for various applications. Adapting SLMs designed for NLP tasks for time series modeling to capture trends and patterns within the complex data, though unconventional, offers a clear possibility for providing unique insights. However, this is a challenging task as SLMs are trained primarily on text corpora, which operates on discrete tokens, while time series data is inherently continuous. Furthermore, SLMs may lack the inherent ability to detect and interpret time series patterns and trends like seasonality, cyclicity, or outliers, due to the absence of related pretraining knowledge. Moreover, current LMs designed for time series analysis ([14, 20, 56]) rely on a fixed-length window of past observations to generate predictions, which may be inadequate for capturing complex patterns and trends present in time series data, thus hindering accurate modeling. Smaller window sizes may capture local patterns but miss broader trends, while larger window sizes can capture more context but may overlook finer details. In recent times, Retrieval-Augmented Generation (RAG) or Retrieval-Augmented Language Modeling (RALM) [23, 33, 37] combines pre-trained language models with information retrieval from external knowledge bases to augment text generation capabilities for open-ended question-answering(ODQA)[38] tasks or for improved language modeling for text summarization, completion with improved accuracy. While regular RAG methods augment generation with retrieved knowledge for ODQA tasks, Agentic RAGs take this further by being instruction-following agents that can tackle complex goals through multi-step reasoning and iterative refinement cycles using repeated retrievals over a knowledge base to ensure the final response aligns with the end user request. In this work, we propose an Agentic RAG framework for time series analysis to improve task-specific outcomes by addressing challenges like distributional shifts, fixed window limitations in time series data. Our Agentic RAG framework presents a hierarchical, multi-agent architecture composed of a master (top-level) agent and specialized sub-agents customized for specific time series tasks. The top-level agent acting as the orchestrator analyzes the incoming user request, determines its nature and complexity, and then routes (or delegates) it to the corresponding task-specific sub-agent to produce the desired output. Similarly to how regular RAG frameworks retrieve relevant information from external knowledge bases like documents, databases, or access the real world through APIs, this Agentic RAG framework leverages distinct prompt pools as internal knowledge bases for each sub-agent focused on specific time series tasks. As specialized"}, {"title": "2 PROBLEM FORMULATION", "content": "Consider a time series dataset characterized by N univariate time series, with sequential data collected over T timestamps, represented as a data matrix $X \\in \\mathbb{R}^{N \\times T}$. Each row in this matrix represents a univariate time series, and each column corresponds to data collected at a specific timestamp. To refer to data from a specific time series or timestamp, we use subscripts and superscripts, respectively. For instance, $X_i = X_{i,:}$ denotes the data from the i-th time series, and $X^t = X_{:,t}$ denotes the data at timestamp t."}, {"title": "2.1 Forecasting", "content": "We utilize a sliding window [10, 46] of size t, to construct time series subsequences $S_t = X_{t-t+1:t} \\in \\mathbb{R}^{N \\times t}$, which have been observed over previous t-steps prior to current time step t to predict about the future values for the next v-steps, $S_{t+1} = X_{t+1:t+v} \\in \\mathbb{R}^{N \\times v}$."}, {"title": "2.2 Missing Data Imputation", "content": "We utilize a binary mask matrix $M \\in \\{0, 1\\}^{N \\times T}$, where $M_{i,t} = 0$ indicates that the value $X_{i,t}$ is missing, and $M_{i,t} = 1$ indicates that the value is observed in the data matrix $X \\in \\mathbb{R}^{N \\times T}$. Missing data can follow random or block patterns[4, 26, 27] across the N univariate time series and T timestamps. We utilize observed values $X^{obs} = X \\odot M$ to estimate the missing values $X^{miss} = X \\odot (1-M)$. \\odot denotes element-wise multiplication. We utilize a sliding window of size $\\tau$ over the observed samples $X^{obs}$, to construct subsequences $S_t = X_{t-\\tau+1:t} \\in \\mathbb{R}^{N \\times \\tau}$, which have been observed over previous t-steps prior to the current time step t. These observed samples are used to predict the missing values for the next v-steps, $S_{t+1} = X_{t+1:t+v} \\in \\mathbb{R}^{N \\times v}$ by leveraging spatio-temporal dependencies within the data."}, {"title": "2.3 Anomaly Detection", "content": "Assuming the time series dataset exhibits normal behavior during the initial $T_{train}$ timestamps, any pattern deviating from the normal behavior in subsequent timestamps $t > T_{train}$ is anomalous. Data observed after $T_{train}$ is considered the test dataset. We use a sliding window to construct samples from previous time steps $S_t \\in \\mathbb{R}^{N \\times \\tau}$ to predict future values of multiple time series $S_{t+1} \\in \\mathbb{R}^{N \\times v}$. The"}, {"title": "2.4 Classification", "content": "We perform unsupervised K-means clustering, identifying (K) optimal clusters or regimes and assigning cluster labels $C\\in \\mathbb{R}^T$ to each time point in the data matrix $X \\in \\mathbb{R}^{N \\times T}$. Then, a sliding window approach is employed to predict the cluster labels for the next v steps $S_{t+1} = X_{t+1:t+v} \\in \\mathbb{R}^{N \\times V}$ based on the observed sample $S_t = X_{t-t+1:t} \\in \\mathbb{R}^{N \\times t}$ over the previous t time steps."}, {"title": "3 PROPOSED METHOD", "content": "The proposed framework offers a novel approach to time series analysis by leveraging a hierarchical, multi-agent architecture. It comprises a master agent that coordinates specialized sub-agents, each dedicated to a specific time series task such as forecasting, anomaly detection, or imputation. These sub-agents employ pre-trained language models and utilize prompt pools as internal knowledge bases, storing key-value pairs representing historical patterns and trends. By retrieving relevant prompts from these pools, the sub-agents can augment their predictions with contextual knowledge about related past patterns, enabling them to adapt to diverse trends within complex time series data. The framework's modular design, combined with the strengths of individual sub-agents, allows for improved performance across various time series analysis tasks, surpassing the limitations of traditional fixed-window methods."}, {"title": "3.1 Dynamic Prompting Mechansim", "content": "Current time series methods typically utilize past data within a predefined window length to understand historical trends and predict task-specific outcomes. However, this approach may not be optimal because there is no universally ideal window length for all time series data. A larger window length might obscure short-range dependencies, while a smaller window length might fail to capture long-range dependencies. Existing methods fail to capture the full complexity of diverse trends and patterns within the complex data required for accurate time series modeling. Adjusting the window length in real-world scenarios can be challenging and computationally expensive. Achieving this goal is an ambitious task, given the current state of research in this field. To address the challenges of non-stationarity and distributional shifts in real-world data, we utilize a differentiable dynamic prompting mechanism[3]. This mechanism allows traditional time series methods to access related past knowledge by retrieving the same group of prompts from the prompt pool for effective adaptive learning on new, similar input data. The dynamic prompting approach utilizes a shared pool of prompts stored as key-value pairs. For time series applications, each prompt is represented by a key vector encoding the essential global characteristics associated with that prompt. The corresponding value matrix contains specific knowledge related to those trends or patterns, such as seasonality, cyclicality, irregularities, and other effects. The key vector acts as an identifier or query vector to retrieve relevant prompts from the pool based on similarity to the input new data, providing a form of conditioning or context about historical patterns to enhance the predictions. This allows the time series methods to effectively leverage encoded knowledge from past experiences, enhancing their predictions by recognizing and applying learned patterns from the shared prompt pool to the new input data. The pool of prompts P contains a set of M distinct key-value pairs as follows:\n$P = \\{(k_1, v_1), (k_2, v_2), ..., (k_M, v_M)\\}$\nHere, M is the total number of prompts in the pool, $k_m \\in \\mathbb{R}^d$ is the key vector of the m-th prompt, and $v_m \\in \\mathbb{R}^{l \\times d}$ is the corresponding prompt value matrix with length l and dimensionality d. In order to retrieve the most relevant prompts for a given input time series $S = X_{t-\\tau+1:t} \\in \\mathbb{R}^{\\tau}$, we first linearly project it into d-dimensional embeddings $S \\in \\mathbb{R}^d$. We then utilize a score-matching function y to measure the similarity between the input and each prompt key:\n$\\gamma (S,k_m) = \\frac{Sk_m}{\\|S\\| \\|k_m\\|}$\nwhere y computes the cosine similarity between the input embedding S and the prompt key $k_m$. The top-K prompts with the highest similarity scores are selected, where $1 \\le K < M$. Let $I = \\{j_1, j_2, ..., j_k\\}$ be the set of indices corresponding to the top-K most relevant prompts retrieved from the pool P for the given input time series S. The selected prompts, along with the original input, are concatenated to form the input embedding $s$ as follows:\n$s = [v_{j_1};...;v_{j_k};S]$\nwhere $s\\in \\mathbb{R}^{(Kl+1)\\times d}$. We linearly project s to d-dimensional representation as follows:\n$\\hat{s} = W s$\nwhere $W\\in \\mathbb{R}^{d \\times (Kl+1)d}$ is a learnable weight matrix. In summary, it aims to improve time series modeling efficiency on the task-specific performance by allowing the framework to recognize and apply learned patterns across non-stationarity datasets with distributional shifts via the shared prompt representation pool."}, {"title": "3.2 Fine-Tuning/Preference Optimization SLMs", "content": "Current pretrained SLMs, such as Google's Gemma and Meta's Llama-3 models, are designed with a context length of 8K tokens. However, they struggle to process long input sequences that exceed their pretraining context window. This is because the limited length of the context window during pretraining restricts their effectiveness during inference when dealing with longer texts. SLMs with an improved context length can better capture long-term spatio-temporal dependencies and complex patterns that unfold over extended periods, which is essential for accurate predictions and understanding seasonal or cyclic trends. We build upon recent work [19] to improve how SLMs handle long sequences without fine-tuning. A two-tiered attention mechanism (grouped and neighbor"}, {"title": "4 EXPERIMENTS", "content": "Datasets: We evaluate the proposed Agentic-RAG framework on four tasks: forecasting, classification, anomaly detection, and imputation. To comprehensively evaluate the framework performance against several baselines, we conducted experiments using both univariate and multivariate benchmark datasets across multiple time series tasks. The variants include Agentic-RAG with SelfExtend-Gemma-2B-instruct, Gemma-7B-instruct, and Llama 3-8B-instruct. We utilized several real-world traffic-related datasets (PeMSD3, PeMSD4, PeMSD7, PeMSD7(M), PeMSD8) obtained from the Caltrans Performance Measurement System (PeMS) [5] for forecasting, classification, and imputation. To ensure consistency with prior research[7], these datasets are preprocessed by aggregating 30-second data points into 5-minute averages. Additionally, publicly available traffic prediction datasets (METR-LA, PEMS-BAY) [22] are utilized, with data aggregated into 5-minute intervals, resulting in 288 observations per day. For anomaly detection, we evaluate the proposed Agentic-RAG framework on publicly available multivariate datasets, conducting a comprehensive benchmark comparison against baseline methods."}, {"title": "5 RESULTS", "content": "Tables 3-4 present a performance comparison of the Agentic-RAG framework variants with baseline methods on seven benchmark"}, {"title": "6 CONCLUSION", "content": "In this work, we propose an Agentic RAG framework to address the challenges of distribution shifts, and fixed-length subsequences in time series analysis. The framework overcomes these challenges by leveraging a hierarchical, multi-agent architecture with specialized sub-agents for various time series tasks. Each sub-agent utilizes a prompt pool as its internal knowledge base to store historical patterns and trends. The sub-agent retrieves relevant prompts and utilizes the corresponding knowledge to improve predictions on new, unseen data. This modular design with task-specific sub-agents and knowledge augmentation outperforms traditional methods in handling complex time series analysis tasks."}, {"title": "A MULTIVARIATE SPATIO-TEMPORAL DATASETS", "content": ""}, {"title": "A.1 Missing Data Imputation", "content": "Time series imputation is a critical step in time series analysis. It addresses a common issue in this field: missing values within datasets. These missing values can arise from sensor failures, data transmission errors, or incomplete records. By imputing these gaps, time series imputation ensures the quality and reliability of subsequent analyses. The Agentic-RAG framework achieves this by handling seasonality, trends and capturing the inherent spatio-temporal dependencies within the data. Ultimately, imputation improves data quality, enabling more accurate analysis, modeling, and decision-making. In essence, it plays a vital role by maintaining data integrity and enabling reliable analysis. To evaluate the Agentic-RAG framework's ability to handle missing data, we simulated two types of missingness patterns: point missing and block missing[9, 35]. These patterns represent varying degrees of data availability. To achieve this, we introduced synthetic missingness into time series datasets following these patterns. For point missing, individual values were randomly omitted with a probability threshold (p), controlling the overall percentage of missing data. The block missing pattern involves removing contiguous, multi-period, multi-time series segments. This is done by randomly selecting start and end times, as well as start and end time series, to define uniform blocks with an average length of (1). All data points within each block are then omitted. Furthermore, two block missing patterns are considered: temporal and spatial. For temporal block missing, contiguous multi-period segments are removed from a given time series. This is done by randomly selecting start and end times, creating stretches of unavailable temporal data. For spatial block missing, contiguous blocks are removed across multiple related time series at specific time points. This involves randomly selecting the start and end time series, resulting in missing spatial data at the chosen time points. Both patterns show varying levels of missing information in the time series data. In summary, point missing refers to sporadic gaps in the data, while block missing involves the absence of entire contiguous multi-period and multi-series segments. Block missing can further be categorized into two types: temporal block missing, where contiguous segments are removed within a single time series, and spatial block missing, where contiguous blocks are removed across multiple related time series, mimicking realistic scenarios of faulty data collection. In the context of time series imputation, \"in-sample\" and \"out-of-sample\" imputation refer to distinct evaluation settings. In-sample imputation involves the imputation method reconstructing missing values within a given fixed input sequence, $S_t$, using all available observed data within that sequence. Out-of-sample imputation involves training the imputation method using the fixed sequence $S_t$ to impute missing points in a future sequence, $S_{t+1}$. In this work, we utilize out-of-sample settings, as this approach mimics real-world scenarios and rigorously assesses the Agentic-RAG framework's robustness and generalizability by evaluating its ability to handle new, unseen data. The simulated datasets with missing values were then used to evaluate the missing data handling capabilities of the proposed Agentic-RAG framework. We split multiple benchmark datasets in chronological order with"}, {"title": "A.2 Time Series Classification", "content": "Time series classification is a crucial task with applications across various domains. In time series analysis, regimes, or clusters represent distinct behavioral modes, operating conditions, or states of the system underlying the data. Identifying and characterizing these regimes is crucial for understanding the complex patterns and dynamics within the data. This allows for more accurate modeling, forecasting, and decision-making in applications where time series analysis is essential. The emergence of different regimes or clusters can stem from changes in the data generation process, external conditions, or the inherent non-stationarity and multivariate nature of the time series. This reflects the rich information content and complexity often encountered in real-world time series data. To evaluate the proposed Agentic-RAG framework's ability to handle time series classification tasks, an unsupervised clustering approach was employed for data labeling. We first applied k-means clustering to the original time series datasets, determining the optimal number of clusters (k) using established techniques such as the elbow method or silhouette analysis. The optimal clusters were treated as class labels, representing distinct regimes within the time series, and each time series was assigned the corresponding cluster label, creating a labeled classification dataset. We adopted a time-based division strategy to split multiple benchmark datasets into training, validation, and testing sets. The METR-LA and PEMS-BAY datasets were split at a 7:1:2 ratio, while other datasets used a 6:2:2 split. We evaluated the framework's performance on the held-out test set using standard classification metrics: accuracy, precision, recall. This methodology allowed us to assess the framework's ability to learn the underlying patterns and relationships associated with"}, {"title": "B UNIVARIATE DATASETS", "content": "We conducted several experiments to evaluate the proposed Agentic-RAG framework variants: SelfExtend-Agentic-RAG with Gemma-2B, SelfExtend-Agentic-RAG with Gemma-7B, and SelfExtend-Agentic-RAG with Llama-8B, on the univariate datasets for multiple time series analysis tasks such as forecasting and imputation."}, {"title": "B.1 Forecasting and Imputation", "content": "The ETT (Electricity Transformer) datasets [53], ETTh1, ETTh2, ETTm1, and ETTm2, are popular benchmarks used for evaluating and benchmarking univariate time series forecasting methods. They provide a challenging benchmark due to the presence of complex patterns, such as trends, seasonality, and irregularities, which are commonly found in real-world time series data. ETTh1 and ETTh2 are two hourly time series datasets containing observations of electricity transformers from two different locations. ETTm1 and ETTm2 are two monthly time series datasets containing observations of electricity transformers from two different locations. In this work, we utilize the ETT datasets [53] to evaluate the Agentic-RAG framework for both forecasting and missing data imputation tasks."}, {"title": "C ENVIRONMENTAL IMPACT", "content": "Our Agentic-RAG framework training process, involving multiple variants running for extended periods, increases our energy consumption and carbon footprint. Accurate quantification of the carbon footprint of deep learning experiments is essential for promoting sustainable practices in artificial intelligence research and development. A crucial aspect of this endeavor is estimating the energy consumption and associated greenhouse gas emissions during the computationally intensive training processes. This is calculated by determining the Total Graphics Power (TGP), which represents the maximum power draw of the GPU, including the GPU chip itself and other components like memory and additional circuitry. For example, the NVIDIA P100 GPU has a TGP of 300 watts, while the NVIDIA T4 GPU has a TGP of 70 watts. By multiplying the TGP by the training time, we can estimate the energy consumption, which is then converted to carbon emissions using a region-specific carbon intensity factor. This factor accounts for the energy mix (coal, natural gas, renewables, etc.) used to generate electricity in the geographic area where the computations are performed. Considering a 725-GPU hours training experiment and using an estimated carbon intensity factor of 0.0007 metric tons CO2e per kWh for the year 2024 (for more information on the carbon intensity of electricity, you can visit CO2 Intensity - Our World in Data), the calculated carbon footprint would be 152.25 kg CO2e for the NVIDIA P100 GPU and 35.525 kg CO2e for the NVIDIA T4 GPU. Note: kg CO2e stands for kilograms of carbon dioxide equivalent. The average person in the United States emits approximately 43.8 kg of carbon dioxide equivalent (CO2e) per day. Given the emissions of 152.25 kg CO2e for the NVIDIA P100 GPU and 35.525 kg CO2e for the NVIDIA T4 GPU, it would take a single person's emissions approximately 3.5 days to match the emissions of the P100 GPU and approximately 0.8 days (or 19 hours) to match the emissions of the T4 GPU. While the calculated carbon footprint provides valuable insight, the actual energy consumption and resulting emissions may vary due to factors like GPU utilization and regional energy sources. Nonetheless, quantifying the carbon footprint is a crucial step towards understanding and mitigating the environmental impact of deep learning research, paving the way for more sustainable and responsible practices in artificial intelligence."}, {"title": "D HYPERPARAMETER OPTIMIZATION", "content": "Hyperparameter optimization involves training the Agentic-RAG framework variants multiple times with different hyperparameter settings. This can be computationally expensive, especially for complex pre-trained language models or large datasets. We optimized the hyperparameters for the best-performing Agentic-RAG w/Llama-8B variant. For simplicity and in the interest of time, we have utilized the same settings for evaluating the performance of Agentic-RAG with w/Gemma-2B and w/Gemma-7B variants for both multivariate and univariate datasets across all tasks. In our experiments, we optimized the training process for supervised fine-tuning using a batch size from {16, 32, 64}, learning rate from {1e-5, 5e-5, 1e-4}. The training was conducted over epochs in the range of {10, 15, 20} with a warmup step count from {500, 1000, 1500} and a weight decay for regularization from {0.01, 0.05, 0.1}. We used gradient accumulation steps for stabilized training convergence from {2, 4, 8} and employed the AdamW optimizer. To manage memory and computational efficiency, we applied 4-bit quantization for QLORA, with hyperparameters including a low-rank ('r') from {16, 32, 64}, an ('a') from {32, 64, 128}, and a dropout from {0.05, 0.1, 0.2}. For"}, {"title": "E ABLATION STUDY", "content": "To understand the contribution of each component within our proposed Agentic-RAG framework, we designed an ablation study. By systematically evaluating the impact of removing individual components, we gain valuable insights into their role in the framework's overall performance. The following ablation experiments were conducted:\n\u2022 (a) Effect of dynamic prompting mechanism(DPM):\nWe compared the performance of the Agentic-RAG framework with and without the dynamic prompting mechanism.\n\u2022 (b) Role of sub-agent specialization(SAS):\nWe evaluated the Agentic-RAG framework using a single, universal sub-agent for all tasks versus specialized sub-agents for each task.\n\u2022 (c) Instruction-tuning(IT) vs. no fine-tuning(NIT):\nWe compared the performance of SLMs with instruction-tuning against their performance without any fine-tuning.\n\u2022 (d) Effectiveness of direct preference optimization (DPO):\nWe evaluated the framework's performance with and without DPO and assessed how aligning SLMs with preferred outcomes impacts the accuracy and reliability of predictions.\nOur study investigates the impact of different components on the overall performance of the framework, 'SelfExtend-Agentic-RAG W/Llama 3 - 8B', in time series forecasting, anomaly detection, and classification tasks across various benchmark datasets. We systematically disable each component (dynamic prompting mechanism (DPM), sub-agent specialization (SAS), instruction-tuning (IT), or direct preference optimization (DPO)) and compare the results to the full framework."}]}