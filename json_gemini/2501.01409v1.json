{"title": "On Unifying Video Generation and Camera Pose Estimation", "authors": ["Chun-Hao Huang", "Jae Shin Yoon", "Hyeonho Jeong", "Niloy Mitra", "Duygu Ceylan"], "abstract": "Inspired by the emergent 3D capabilities in image generators, we investigate whether video generators similarly exhibit 3D awareness. Using structure-from-motion (SfM) as a benchmark for 3D tasks, we ask if intermediate features from OpenSora, a video generation model, can support camera pose estimation. We first examine native 3D awareness in video generation features by routing raw intermediate outputs to SfM-prediction modules like DUSt3R [69]. Then, we explore the impact of fine-tuning on camera pose estimation to enhance 3D awareness. Results indicate that while video generator features have limited inherent 3D awareness, task-specific supervision significantly boosts their accuracy for camera pose estimation, resulting in competitive performance. The proposed unified model, named JOG3R, produces camera pose estimates with competitive quality without degrading video generation quality.", "sections": [{"title": "1. Introduction", "content": "Building on breakthroughs in foundational image models [53], video diffusion models have advanced rapidly in recent years, resulting in numerous commercial and open-source models [5, 7, 21, 44, 87]. Trained on large-scale datasets like WebVid-10M [2] and Panda-70M [12], these models generate realistic, diverse, and temporally smooth videos from text or image prompts.\nBeyond generating high-quality content, foundational generative models have recently been employed as feature extractors, utilizing their intermediate features to tackle various analysis tasks such as correspondence matching and semantic segmentation [17, 60]. Notably, even though trained exclusively on 2D data, image generators have demonstrated emergent abilities for 3D-aware tasks [18]. Inspired by this success, we ask if the pre-trained video generators have similar emergent behavior towards 3D awareness. To investigate this, we pick the classical structure-from-motion (SfM) as the target 3D task, as it requires reasoning about both scene geometry and the relative viewpoint changes across frames."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Diffusion-Based Video Generation", "content": "Building on the success of diffusion models [27, 58] in image synthesis [15, 53], the research community has extended diffusion-based methods to video generation. Early works [28, 29] adapted image diffusion architectures by incorporating a temporal dimension, enabling the model to be trained on both image and video data. Typically, U-Net-based architectures incorporate temporal attention blocks after spatial attention blocks and 2D convolution layers are expanded to 3D convolution layers by altering kernels [29, 75]. Latent video diffusion models [5, 6, 24, 67] have been introduced to avoid excessive computing demands, implementing the diffusion process in a lower-dimensional latent space. Seeking to generate spatially and temporally high-resolution videos, another line of research adopts cascaded pipelines [3, 28, 57, 71, 82], incorporating low-resolution keyframe generation, frame interpolation, and super-resolution modules. To maximize computational scalability, recent waves in video generation [7, 11, 42, 44, 87] diverge from U-Net-based architecture and employ the Diffusion Transformer (DiT) [49] backbone that processes space-time patches of video and image latent codes. Following this direction, we build our method on OpenSora [87], a publicly available DiT-based latent video diffusion model."}, {"title": "2.2. 3D Reconstruction", "content": "The fundamental principles of multiview geometry [73] including feature extraction [8, 40], matching [1, 22, 39, 74], and triangulation with epipolar constraints are well known to produce highly accurate (yet sparse) 3D point clouds with precise camera pose estimation from multiview images [55]. The efficiency of 3D reconstruction has been improved with linear-time incremental structure-from-motion [74] and coarse-to-fine hybrid approaches [13, 14]. To improve robustness to outliers, researchers proposed global camera rotation averaging [14], camera optimization techniques based on features of points vanishing with oriented planes [30] or from a learned neural network [37] to prevent rotation and scale drift issues. Global camera pose registration and approximation with geometric linearity [10, 32] or joint 3D point position estimation [48] are designed to further push the scalability and efficiency of the 3D reconstruction as well as the robustness particularly to the image sequence with small baselines.\nGiven estimated camera poses and sparse 3D point clouds, multiview stereo can then produce a dense 3D surface using hand-created visual features [56] or neural features with a cost volume [43, 43, 62, 77, 85] to predict globally coherent depth estimates. Existing neural rendering methods reconstruct such a dense surface by modeling the implicit or explicit cost volume and differentiable rendering of the scene for photometric supervision from multiview images [20, 36, 45, 46, 50, 59, 68, 70, 79] or monocular depth estimation [54]. Some pose-free methods further erase the requirement of camera calibration: test time"}, {"title": "2.3. Diffusion Model as Features for Reconstruction", "content": "A generative diffusion model is often trained on millions of paired image and text prompts and in the process develops a semantically meaningful visual prior. Naturally, researchers are interested in using this strong prior for many downstream 3D vision tasks. Injecting 3D awareness into the diffusion prior greatly improves the accuracy and generalizability of the monocular depth estimation and correspondence search tasks [18, 80]. The latent features from the frozen pretrained diffusion model are often used as a backbone, and a task-specific decoder with cross attention is newly trained for semantic correspondences [25, 25, 31, 60, 83, 84], 3D correspondences [17], semantic segmentation and monocular depth estimation [86], material and shadow prediction [81], general object 3D pose estimation [9, 47]. However, such image diffusion features do not inherently consider the temporal relation between the frames, leading to temporally unstable 3D prediction results from videos.\nIn contrast, we propose to utilize the video diffusion features as a backbone for the multitasking prediction of video generation and 3D camera poses estimation."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Model and Preliminaries.", "content": "Video diffusion model. We consider OpenSora [87] as our base video generation model, which is a DiT-based video diffusion model inspired by the notable success of Sora [7]. It performs the diffusion process in a lower-dimensional latent space defined by a pre-trained VAE encoder E. Each frame x of the input video is first projected into this latent space, $z_0 = E(x)$. Given a diffusion time step t, the forward process incrementally adds Gaussian noise to the latent code $z_0$ via a Markov chain and obtains noisy latent $z_t$. The denoising model $\\epsilon_{\\theta}$ takes the noisy latents of all frames, the time step t, and the text prompt y as input to predict the added noise: $\\epsilon \\sim \\epsilon_{\\theta}(\\{z_t\\}_{f=1}^{F}, t, y)$, where F is the total number of frames and $\\epsilon$ denotes the parameter of the DiT network [49]. The network consists of 28 spatial-temporal diffusion transformer (STDiT) blocks $\\{b^1, . . ., b^{28}\\}$, similar to [42]. The iterative process of noise prediction and noise removal is referred to as the reverse process.\nCamera pose estimation module. We employ the state-of-the-art multi-view stereo reconstruction (MVS) framework DUSt3R [69], as our camera tracking module. Given an image pair, DUSt3R encodes each image independently with a ViT encoder [16, 72]. Two decoders process both features to enable cross-view information sharing, followed by separate heads that estimate point maps $X \\in R^{H \\times W \\times 3}$, represented in the coordinates of the first view as $X^{1,1}$ and $X^{2,1}$ respectively. The relative camera pose is then estimated by aligning $X_{1,1}$ and $X_{1,2}$ using Procrustes alignment [41] with PnP-RANSAC [19, 33]. A global optimization scheme is employed to register more than a pair of views from the same scene as post-processing."}, {"title": "3.2. Unifying Video Generation and Camera Reconstruction", "content": "To facilitate our analysis, we propose a unified framework that routes intermediate features from the video generator to the camera pose estimation task. We observe that ViT and DiT share many architectural designs in common since they both belong to the broad transformer family. Hence, our key insight is to replace the image-based ViT encoder in DUSt3R with the video DiT backbone in OpenSora. In this setup, we pass the intermediate features of the denoising DiT network $\\epsilon_{\\theta}$ to DUSt3R decoders and heads (see Figure 2 for illustration).\nSpecifically, we extract the output of the intermediate STDiT block $b^i$ at a particular time step t during the reverse process. Following Tang et al. [60], we consider small t where the feature focuses more on low-level details, making it useful as a geometric feature descriptor to build correspondence across frames.\nOur modification of DUSt3R. The features extracted from the video generator encode a sequence of F frames and are provided to the DUSt3R in a pair-wise manner. At training, the first frame can ideally be paired with all other frames f. In practice, due to memory constraints, we sample 4 pairs from the set $\\{(1, f)\\}_{f=2}^{F}$ to predict the 3D point maps.\nAt inference time, we first predict the point maps between all pairs (1, f) and perform the global camera registration in DUSt3R to refine the camera pose, depth and focal length for each frame, akin to bundle adjustment. Since our input is not a set of sparse views but a temporal sequence, we append two new terms, $L_{tranl}$ and $L_f$, to the original global registration objective, encouraging smooth camera translation and consistent focal length between neighboring frames respectively. The two terms are used only at inference, and we refer to suppmat. for the detailed formulation.\nTraining objectives. For training, we consider two main training objectives, generation loss $L_{gen}$ and reconstruction loss $L_{rec}$. The generation loss $L_{gen}$ is the common objective"}, {"title": "4. Experiments", "content": "We evaluate the design choices in two main aspects. We follow standard approaches to assess the generated video quality (T2V) while validating the accuracy of camera pose estimation on real videos (V2C). We further provide results for jointly generating videos along with camera pose estimation (T2V+C). Since there is no ground truth in this case, we focus on and report self-consistency."}, {"title": "4.1. Setup", "content": "Data. We choose RealEstate10K [89] as the dataset, which has around 65K video clips paired with camera parameter annotations. We use the captions of RealEstate10K provided in [23] and also follow their train/test split. As preprocessing, we pre-compute the VAE latents of the video frames and the T5 text embeddings of the captions. We sample F=16 frames from the original sequences with a frame stride randomly chosen from \\{1, 2, 4, 8\\} and 0.5 probability to reverse the frame order. To obtain point map annotation X, we estimate metric depth with ZoeDepth [4], un-project it to 3D and transform to the coordinate of the first frame using the camera parameters in RealEstate10K. All camera extrinsic parameters are expressed w.r.t. the first frame.\nIn addition, we consider DL3DV10K [38], which also provides camera annotations, as an additional test set. We choose a random set of 70 videos for testing and caption the first frame of each video using [35]. We prepare point map annotations using ZoeDepth [4].\nBaselines. Since there is no existing method that can perform both video generation and camera pose estimation jointly, we can only compare to task-specific methods. We use the original pair-wise method DUSt3R [69] with linear head and a global SfM method GLOMAP [48] as camera pose estimation methods to provide a reference for our analysis. For DUSt3R we consider three variants: (i) off-the-shelf pretrained weights (DUSt3R\u2020), (ii) initialized with pretrained weights and trained with the same data as ours (DUSt3R*), and (iii) trained from scratch with the same data as ours (DUSt3R\u00b0). In all three variants, we perform the final global optimization step with our newly introduced temporal loss $L_{tranl}$ and $L_f$. For video generation we consider the pretrained OpenSora as a baseline.\nMetrics. We validate the quality of camera tracking on real videos (V2C) by comparing the estimated camera poses (R, t) with the ground truth poses $(\\tilde{R}, \\tilde{t})$. For rotation, we compute the relative error between two rotation matrices [66]. Since the estimated and ground-truth translation can differ in scale, we follow [66] to compute the angle between the two normalized translation vectors, i.e., $arccos(\\frac{t \\cdot \\tilde{t}}{(\\|t\\| \\|\\tilde{t}\\|)})$. Besides reporting the average of the two errors, we also follow [69] to report Relative Rotation Accuracy (RRA) and Relative Translation Accuracy (RTA),"}, {"title": "4.2. Reconstruction Evaluation", "content": "In Table 1, we compare the camera pose estimation (V2C) errors on RealEstate10K-test and report the errors of withheld DL3DV10K in Table 2. In both tables, even when there are decoders trained to perform reconstruction from the Sora features, freezing Sora backbone still leads to noticeably worse results than the trainable counterparts (row 1a vs. 1b or 1c). This means raw features from pretrained video generators are insufficient for camera pose estimation. Comparing rows 1b and 1c of two tables, we see that models trained without generation loss $L_{gen}$ lead to overall"}, {"title": "4.3. Generation Evaluation", "content": "For each method, we generate 180 videos using the captions in RealEstate10K-test and report the FID/FVD against the real images/videos in RealEstate10K-test. Table 3 suggests that our full model generates more realistic images/videos than pretrained OpenSora (row 1c vs. 2). When ablating the generation loss $L_{gen}$, the quality slightly degrades compared to our full model (row 1c vs. 1a). This is intuitive because without the generation loss, there is nothing to enforce the model to retain its full generation capability. See also supplemental videos. It is worth noting that row 1b corresponds to a baseline where $L_{rec}$ is disabled by removing DUSt3R decoders/heads, i.e., it is equivalent to standard diffusion model finetuning except only the weights of the temporal attention layers are updated. We see that removing $L_{rec}$ leads to different impacts on FID and FVD. Therefore, we conclude camera pose estimation has mixed impact on video generation. Figure 6 shows a few examples of videos generated by JOG3R."}, {"title": "4.4. Discussion", "content": "Self consistency of T2V\u2192V2C and T2V+C. Since JOG3R can generate camera trajectories in two ways: cascading T2V and V2C or the tightly coupled T2V+C pipeline, it is worth comparing how much the two results differ. We run the two pipelines with 100 prompts and report 0.45\u00b0 average difference in rotation and 19.20\u00b0 in translation; both are low errors compared with the corresponding numbers in Table 1 and 2, indicating that the camera poses from joint T2V+C pipeline is consistent with T2V\u2192V2C. The qualitative results in Figure 5 also confirm this conclusion.\nSynergy of two tasks. Our unified architecture enables multi-task training: generation loss $L_{gen}$ and reconstruction loss $L_{rec}$. However, according to Table 1 and 2, $L_{gen}$ does not have a significant positive impact on camera pose estimation, while in Table 3, $L_{rec}$ also has mixed impact on generation. Thus, we conclude that we do not observe a synergistic effect between two tasks, at least in the framework of OpenSora 1.0.\nRole of $L_{gen}$. Despite two tasks have no observable synergistic effect, we choose to enable $L_{gen}$ in our full method because it leads to on-par camera pose estimation results (Table 1 and 2) and improved generation quality (Table 3).\nValue of JOG3R. To our knowledge, JOG3R is the first method that performs video generation and camera pose estimation jointly. The new unified architecture enables end-to-end training of two tasks, and it opens up new research directions. Without precedents, we can only compare with"}, {"title": "5. Conclusions and Future Work", "content": "We have presented a thorough analysis on probing the 3D awareness of the intermediate features of a video generation model via the 3D camera pose estimation tasks. In addition to this analysis, we have presented JOG3R, the first unified framework for text-to-video generation (T2V), joint generation and camera estimation (T2V+C), and camera estimation for real videos (V2C).\nSince it is not trivial to obtain accurate camera annotations for dynamic scenes, our analysis is limited to videos of static scenes only. It is a promising future direction to analyze how video generation features adapt to camera estimation for dynamic scenes. The length of the video sequences our method can handle is currently limited by the number of frames the generator can synthesize. Handling longer sequences may require adopting sliding window solutions. As the video generators continue to improve to enable generation of longer sequences, our proposed solutions will also naturally extend to handling longer videos with larger baseline."}, {"title": "A. Temporal smoothness term", "content": "Given the estimated camera pose ($R_f$, $t_f$) and focal length $l_f$ for each frame f, we define $L_f$ and $L_{tranl}$ as below:\n$L_f = \\sum_f ||l_f - l_{f+1}||_1^2$                                                                                                 (3)\n$L_{tranl} = \\sum_f ||t_f - t_{f+1}||_2^2 + ||v_f - v_{f+1}||_2^2$      (4)\nwhere $v_f = t_f - t_{f+1}$. The first term of $L_{tranl}$ encourages static camera position while the second term encourages constant velocity."}, {"title": "B. Architectural design choice", "content": "In Table 1 below, we compare the quality of estimated camera poses using the feature maps in different DiT block $b^i$. Row 1c and 2c correspond respectively to our full JOG3R model (row 1c in Table 1 and 2 in the main paper), which is i=26. We first confirm using the features one block later, i=27, does not result in significant difference (row 1d vs. 1c; 2d vs. 2c). Next, we consider i=10 and 20, representing approximately one-third and two-third of total DiT blocks. We see that compared to our results in the main paper (row 1c and 2c), i=10 yields lower errors in RealEstate10k-test but higher errors in DL3DV10k (row 1a and 2a), suggesting that using features of earlier blocks has a higher risk of poor generalization. Meanwhile, i=20 attains the lowest errors in both datasets, while ours remains on-par. We therefore conclude that features of later blocks, e.g., i \u2208 [20, 27] is preferred than earlier blocks, and all later blocks should lead to similar results. Instead of solely relying on one block $b^i$, one can potentially devise a module fusing features of all DiT blocks and projecting to the input space of DUSt3R decoders, which we consider future work."}]}