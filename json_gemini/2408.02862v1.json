{"title": "On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods", "authors": ["Kyle Boerstler", "Vijay Keswani", "Lok Chan", "Jana Schaich Borg", "Vincent Conitzer", "Hoda Heidari", "Walter Sinnott-Armstrong"], "abstract": "Preference elicitation frameworks feature heavily in the research on participatory ethical AI tools and provide a viable mechanism to enquire and incorporate the moral values of various stakeholders. As part of the elicitation process, surveys about moral preferences, opinions, and judgments are typically administered only once to each participant. This methodological practice is reasonable if participants' responses are stable over time such that, all other relevant factors being held constant, their responses today will be the same as their responses to the same questions at a later time. However, we do not know how often that is the case. It is possible that participants' true moral preferences change, are subject to temporary moods or whims, or are influenced by environmental factors we don't track. If participants' moral responses are unstable in such ways, it would raise important methodological and theoretical issues for how participants' true moral preferences, opinions, and judgments can be ascertained. We address this possibility here by asking the same survey participants the same moral questions about which patient should receive a kidney when only one is available ten times in ten different sessions over two weeks, varying only presentation order across sessions. We measured how often participants gave different responses to simple (Study One) and more complicated (Study Two) controversial and uncontroversial repeated scenarios. On average, the fraction of times participants changed their responses to controversial scenarios (i.e., indicating instability) was around 10-18% (\u00b1 14-15%) across studies, and this instability is observed to have positive associations with response time and decision-making difficulty. We discuss the implications of these results for the efficacy of common moral preference elicitation methods, highlighting the role of response instability in potentially causing value misalignment between the stakeholders and AI tools trained on their moral judgments.", "sections": [{"title": "Introduction", "content": "The development of ethical and participatory AI tools in critical domains, such as healthcare or transportation, requires eliciting and modeling the moral judgments and values of various stakeholders Schaich Borg et al. (2024). To do so, a common methodology is to ask stakeholders to respond to various moral decision-making scenarios and use the resulting responses to learn task-specific moral preferences at an individual and population level. However, a crucial assumption underlying this methodology is that participants' responses are stable. Imagine that we want to"}, {"title": "", "content": "design a healthcare resource allocation policy for a district and ask all the doctors in the dis- trict whether they prefer medical resources to be distributed equitably across all hospitals or dis- tributed preferentially to maximize the number of treated patients. Suppose a doctor expresses a preference to prioritize equity over efficiency one day, but on the very next day they say the oppo- site, even when nothing relevant has changed. We would not know which policy the doctor really prefers, if either. This case illustrates a general phenomenon: the fact that a person expresses a preference on one occasion does not justify the inference that they will express the same prefer- ence on another occasion. Even if people's true underlying preferences are stable, any method that results in participant statements and choices changing over time in an unpredictable way is not a reliable method for eliciting, or ascertaining, stable participant preferences. This lesson applies to any attempt to elicit preferences in surveys. It has been acknowledged in psychology litera- ture through the expectation of reporting \u201ctest-retest reliability\u201d measures for survey instruments Aldridge et al. (2017). Unfortunately, most preference elicitation surveys in AI fail to heed this warning. They ask participants a set of questions only once and cannot determine whether those participants would give different answers on different occasions due to changes in mood, decision strategy, or changes in opinion.\nMany fields are affected by the issue of measurement instability, but here we will focus on impacts on the development of ethical and participatory AI tools. Al is increasingly being used to make judgments in situations with moral consequences, such as when autonomous vehicles must de- termine which groups to prioritize when trying to avoid a collision Awad et al. (2018) or to decide how to allocate scarce medical resources Johnston et al. (2020); Freedman et al. (2020). To endow Als with the ability to make judgments in these situations in a way that is consistent with stake- holders' values, AI researchers often try to learn stakeholders' moral views using surveys that ask stakeholders to repeatedly choose among sets of moral alternatives, a process called \u201cmoral pref- erence elicitation\u201d Feffer et al. (2023b). Common preference elicitation procedures usually employ pairwise comparisons between alternative actions, providing participants with an intuitive setup to express their preferences Saaty (2008). The Al's goals can then be constrained using models of stakeholders' moral \u201cpreferences\u201d learned through these elicitation procedures Noothigattu et al. (2018). Critically, these procedures almost always administer surveys to stakeholders only once.\nContrary to the assumptions of common moral preference elicitation procedures in AI ethics, moral psychology research shows that participants can change their moral judgments over time. For instance, Rehren and Sinnott-Armstrong (2023) observe instability in moral judgments for scenarios involving sacrificial dilemmas, where the participants are asked to choose between action (sacrificing some people to save others) and inaction (not acting and letting them die) \u2013 e.g., the famous \"trolley problem\u201d. While this study provides evidence of instability in moral judgments, their applicability to AI-related settings is limited. This is because action-vs-inaction choice scenar- ios used in sacrificial dilemmas are structurally different than those used in AI-related preference elicitation literature. Common preference elicitation frameworks use action-vs-action scenarios, i.e., scenarios that are posed as pairwise comparisons between two actions (e.g., Srivastava et al. (2019); Johnston et al. (2023)). Beyond structural differences, brain studies have shown that cogni- tive processes involved when making moral judgments in action-vs-action scenarios are different than those involved in action-vs-inaction scenarios Schaich Borg et al. (2006). As such, it is unclear whether results from prior studies on instability for sacrificial dilemmas extend to preference elic- itation frameworks commonly used in AI applications."}, {"title": "", "content": "We explored this important methodological issue by asking survey partic- ipants to decide between moral options with the same relevant features, presented in different orders, on ten occasions over two weeks. Our approach builds on the studies we described above by testing the stability of participants' responses in a new moral context of medical triage deci- sions and asking participants to give responses across ten separate sessions, which allows us to better assess their response stability (or instability). We also compared their response stability to uncontroversial (>90% agreement across participants) and controversial (<75% agreement across participants) decisions and investigated multiple hypotheses to determine the sources of insta- bility in participant responses. The medical decisions we focus on pertain to kidney allocation. When a kidney becomes available for transplant, it is often compatible with more than one needy recipient, and there are not enough donors (live or dead) to supply all patients in need. As a result, doctors or hospitals often have to decide which one of several patients should receive a kidney that becomes available. The patient who gets the kidney will typically gain decades of life by virtue of getting the transplant. A patient who does not receive a kidney will have to keep waiting for a transplant, may lose quality of life as their condition deteriorates, and may even die waiting. Therefore, deciding who gets a kidney is a moral decision with life-and-death consequences.\nIn our studies, we presented participants with pairs of kidney patients, who may differ across features, such as age, behaviors, and the number of dependents they have. Then, we asked partic- ipants to choose which of the two patients should be given the one available kidney. A subset of patient pairwise comparisons were repeated multiple times. Stability for a repeated comparison was measured by how often a participant chose the patient with the same features independently of changes in the order of the patients and features. Section 2 reports and analyzes the degree of stability in participants' responses to the repeated pairwise comparisons across sessions. Going beyond the repeated comparisons, Section 3 studies the degree to which the statistical model that best fits patterns of all responses in one session continues to be the best fit for other sessions. We find significant differences in predictions from models trained on different participant sessions, indicating that participants potentially change their decision-making model across sessions.\nWe also test multiple research questions to understand possible causes of lacks in participant re- sponse stability (Section 2.3). Specifically, we investigate whether response stability for any re- peated pairwise comparison is associated with (1) attention/time taken to respond to the scenario, (2) the number of feature differences between the two patients in a repeated pairwise comparison, or (3) participant's perceived difficulty of making a judgment about the given comparison. Our analysis provides evidence that response stability for any pairwise comparison is indeed associ- ated with the response time and the perceived difficulty of the given comparison, with the evi- dence for the latter being most significant. The results illuminate the scale and sources of response instability as well as whether and when common survey methods succeed in eliciting stable pref- erences.\nNote that our analysis primarily focuses on response stability at the participant level and not the population level; i.e., we assess each participant's response stability but we do not assess instability of aggregated judgments of all participants. We discuss this point and the implications of instability on the efforts to develop AI tools that incorporate stakeholders' moral values in Section 4. Our instability results highlight the possibility of misalignment between stakeholders' moral values and the decisions of AI tools that utilize computational models of stakeholders' moral preferences."}, {"title": "Related Work.", "content": "Preference elicitation frameworks are employed in a wide variety of domains to learn people's preferences and incorporate them into downstream personalized applications Jannach and Kreutler (2005). In the domain of moral judgments, Awad et al. (2018) and Nooth- igattu et al. (2018) study people's preferences over hypothetical moral dilemmas associated with the use of autonomous vehicles and ways to aggregate preferences obtained from a large popula- tion. Freedman et al. (2020) employ elicitation frameworks to design tie-breaking mechanisms for kidney exchanges. Johnston et al. (2023) propose using elicitation methods to understand people's preferences for healthcare resource allocation during crisis situations like COVID-19. Srivastava et al. (2019) use similar methods to model nuances associated with people's preferences over vari- ous mathematical notions of fairness. Preference elicitation methods have also been employed for the development of participatory machine learning frameworks, where the goal is to actively seek out and incorporate diverse stakeholder opinions with the design of the automated system Feffer et al. (2023b); Lee et al. (2019); Evequoz et al. (2022).\nGiven this increasing popularity, it is important to simultaneously analyze their performance in practice. Our work contributes to the recent literature investigating the effectiveness of computa- tional methods for moral preference elicitation from the viewpoint of response stability. In sacri- ficial dilemmas, Rehren and Sinnott-Armstrong (2023) found that 8-20% of participants changed from saying that an agent should sacrifice some to save others to saying that the agent should not sacrifice some to save others in these dilemmas, or from the latter (\u201cshould not\u201d) to the former (\"should\"). Their results align with previous findings on imperfect test-retest reliability when answering moral foundations questionnaire Curry et al. (2019) and the impact of context on moral preferences Schein (2020). Chan et al. (2020) likewise found evidence of variability in participants' responses to kidney allocation judgments arising from the nature and source of feedback provided to them.\nOther works have raised other issues about preference elicitation efficacy. Feffer et al. (2023a) highlight concerns related to fairness and stability when aggregating moral preferences obtained from a heterogeneous population. Rogowski and John (2024) question the normative foundations underlying the use of preference elicitation for healthcare resource allocations, arguing that com- mon aggregation methods may not necessarily lead to equitable or \u201csocially valuable\u201d outcomes. Similar to moral dilemmas, Conitzer et al. (2015) discuss methods to model tradeoffs between var- ious societal-level objectives and emphasize the importance of design considerations when asking for people's opinions on these tradeoffs and accounting for the heterogeneity of opinions among the relevant population. While these works discuss important issues with applications of prefer- ence elicitation, our work questions the methodological assumptions of response stability when designing elicitation frameworks and highlights the impact of instability on ethical AI develop- ment.\""}, {"title": "Study", "content": "We performed two human subject experiments where we presented participants with a series of kidney allocation scenarios. Participants responded to these scenarios over a series of up to ten sessions spread out over two weeks. A subset of scenarios were repeated in each session to enable us to measure the stability of responses."}, {"title": "2.1 Methods", "content": "We used Prolific to gather our data for both studies with a 50/50 gender ratio (N=30 in Study 1; N=82 in Study 2). Exclusion criteria included: previous participation in a study from our group that involved kidney allocation queries, approval ratings <98%, or completion of <50 previous submissions in Prolific.\nParticipants were presented with scenarios that listed information about two fic- tional patients, each needing a kidney transplant (Figure 1). Participants were asked \u201cto choose which of two patients should receive the kidney when only one is available\u201d. In each scenario, each patient (named A or B) was described by the following features (set of possible feature val- ues provided in parentheses):\nDecades of life that the patient is expected to gain from the transplant (0, 1, 2)\nNumber of child dependents (0, 1, 2)\nAlcoholic drinks per day that the patient consumed prior to diagnosis (0, 2, 4)\nNumber of past serious crimes committed (0, 1, 2)\nYears of life expected to be gained from the transplant (0, 5, 10, 20, 25)\nNumber of elderly dependents (0, 1, 2, 3)\nYears the patient had been on a waiting list for the transplant (1, 3, 5, 7)\nHours a patient is expected to be able to work post-transplant (0, 10, 20, 30, 40, 50)\nObesity (underweight, normal weight, overweight, obese, morbidly obese, very morbidly obese)\nParticipants' choices in these scenarios are moral in nature since they affected harm to patients, were based on features (e.g., past crimes and alcohol abuse) that are often ascribed moral import,"}, {"title": "2.1.1 Statistics and Analysis", "content": "Participant's query reaction times (seconds) were right-skewed, so we ex- cluded from all analyses query responses associated with reaction times that were 3 standard de- viations beyond the mean, following a standard procedure for outlier removal Berger and Kiefer (2021).\nTo assess response stability, for each repeated scenario, we labeled the fea- ture combination on the left half of the screen during the scenario's initial presentation as \u201cPatient A\" (even if it was presented on the right in a later session), and the ones on the other half as \u201cPatient B\u201d. Stability was defined as the number of times a participant chose the patient that the participant chose more often, divided by the total number of times the scenario was repeated for that participant. This means response stability can have a minimum value of 50% and a maximum value of 100%. For example, a participant who chose one patient (A or B) 8 times in a scenario repeated 10 times would be 80% stable.\nWe measured how often either Patient A or Patient B was chosen in aggregate by all participants. Uncontroversial scenarios were defined as those with >75% agreement, and controversial scenarios were defined as those with <75% agreement.\nDifferent participants can use different decision-making processes to make kidney allocation decisions. To model each participant's process, e used the Bradley-Terry (BT) model to estimate the priority participants placed on patient features when making their allocation judg- ments Bradley and Terry (1952); Hunter (2004); Freedman et al. (2020). We assume that the effect each feature has on different patients across scenarios is the same. For example, the coefficient for alcohol for Patient A is the same as the coefficient for alcohol for Patient B. The estimation of the log(difference) for each patient winning in the BT model between two patients is modeled by:\n$\\log \\frac{\\text{Prob}(A : \\text{Wins})}{\\text{Prob}(B: \\text{Wins})} = \\text{logit}(\\text{Prob}(A : \\text{Wins}))$\n$ =  (B_{alco} \\cdot P_{alco}^{A} + B_{dep} \\cdot P_{dep}^{A} + B_{life} \\cdot P_{life}^{A} + B_{crim} \\cdot P_{crim}^{A})$\n$(B_{alco} \\cdot P_{alco}^{B} + B_{dep} \\cdot P_{dep}^{B} + B_{life} \\cdot P_{life}^{B} + B_{crim} \\cdot P_{crim}^{B})$\nThis model can be further simplified to:\n$\\text{logit}(\\text{Prob}(A : \\text{Wins})) = B_{alco} \\cdot \\text{diff}_{alco} + B_{dep} \\cdot \\text{diff}_{dep}$\n$+ B_{life} \\cdot \\text{diff}_{life} + B_{crim} \\cdot \\text{diff}_{crim},$"}, {"title": "", "content": "where $ \\text{diff}_j = p_j^A - p_j^B$ (i.e., difference in value of feature $j$).\nFor the analysis in this section, we modeled features using their relative values (i.e., difference in value of features of Patient A vs Patient B). Section 3 uses a broader setup, considering both raw and relative values of features.\nThe distributions of response stability values and reaction times were non-normal even after transformations, so we used Mann-Whitney U tests for significance testing, unless mentioned otherwise."}, {"title": "2.2 Results", "content": "17 participants (57%) of our 30 recruited participants answered all 600 of the requested responses in Study One, and 19 answered 300 or more responses. For Study Two, 29 participants of the 82 recruited participants (35%) answered all 600 of our requested responses, and 52 responded to 300 or more responses. All results are based on the subsets of participants who answered 300 or more responses, N=19 for Study One and N=52 for Study Two.\nIn both studies, the three repeated scenarios that were intended to be uncontroversial (S1U1-S1U3; S2U1-S2U3) received >99% response agreement across participants. Although all three of the repeated scenarios that were intended to be controversial in Study Two (S2C1-S2C3) elicited <75% agreement, only one of the repeated scenarios that were intended to be controversial in Study One (S1C3) met this criterion.\nFor Study Two, most participants were >90% stable in their responses to the uncontroversial and controversial repeated scenarios averaged together (see Appendix Figure 3), but individual partic- ipants varied in their levels of stability for the controversial repeated scenarios from 50% to 100% (see Appendix Table 7). Only 5 out of 52 participants were perfectly stable for all repeated scenar- ios (Table 7). Responses to the uncontroversial repeated scenarios were significantly more stable than responses to the controversial repeated scenarios (p <0.0001), and variance in stability levels was significantly higher for the controversial scenarios as well (F-test p <0.0001). For Study One, the difference in response stability between uncontroversial and controversial repeated scenarios is positive and statistically significant (p <0.01, Mann-Whitney test). However, the magnitude of stability differences between uncontroversial and controversial scenarios were smaller (compared to Study Two)."}, {"title": "2.3 Instability Causes", "content": "To provide insight into what might cause participants' response instability, we investigate three research questions.\nIs instability caused by insufficient time and attention to the decision?\nIs instability caused by challenges associated with having to consider multiple feature differ- ences at once?\nIs instability a result of decisions being \u201cdifficult\u201d because participants place similar priorities on the patients, given their feature values?"}, {"title": "2.3.1 Time on Task and Stability", "content": "To test RQ1, we assessed stability across different subsets of responses to the repeated scenarios, defined by response times (Study One N = 1109; Study Two N = 5544; Table 2). No significant differences were found between the groups (ANOVA test to evaluate differences in mean stability for Table 2 groupings had p = 0.92 for Study One and p = 0.31 for Study Two), suggesting a negative response to RQ1, at least at the aggregate level.\nA more nuanced picture emerged when, as an exploratory analysis, we analyzed individual re- sponse times to the three repeated controversial scenarios in Study Two, which were the scenarios that elicited the most instability (Table 3). We divided the responses to each controversial scenario into two groups: stable responses \u2013 those where the participant's response matches their dominant choice for the given scenario, and unstable responses \u2013 those where the participant's response does not match their dominant choice for the given scenario. For each controversial scenario, average response times were significantly longer for unstable responses than stable responses (p <.0001, Mann-Whitney U test).\nSince participant-level analyses might provide greater statistical sensitivity, as a final exploratory analysis, we assessed whether there were significant correlations between individual participants' average response times for a specific scenario and response stability values for scenarios S1C1- S1C3 (Study One, N=57) or S2C1-S2C3 (Study Two, N=156). No significant correlations were found in Study One (p=0.49), but a Pearson correlation of \u20130.16 (p=0.05) was found in Study Two (see Figure 4 in the Appendix for scatter plots). Thus, we found no evidence that response instabil- ity was correlated with spending too little time or attention on a scenario. In contrast, our results suggest that a future study with greater statistical power might find evidence that instability is actually associated with longer reaction times. Such an association is consistent with the idea that scenario complexity or difficulty contributes to response instability, which we investigate in the next"}, {"title": "2.3.2 Feature Differences and Stability", "content": "RQ2 asked whether instability could be caused by challenges associated with having to consider multiple feature differences at once. Intuitively, perhaps participants make more mistakes or are not able to be sure of their answer when they have to compare differing values in a lot of features simultaneously, taxing their working memory and cognitive load. To assess whether this idea could have merit, we examined the number of features that differed between patients in each of our repeated scenarios.\nWe do not have enough variance in the number of feature differences to make strong conclusions, but we did not find compelling evidence for a relationship between the total number of feature differences and response stability. In Study One, the average response stability of scenarios with differences in three features was actually lower than that of scenarios with differences in four features (p=0.02, ANOVA main effect). In Study Two, although response stability was lower on average for scenarios with five feature differences than for scenarios with three or four feature differences (p<0.0001, ANOVA main effect); responses to S2U1 were significantly more stable than S2C1, S2C2, and S2C3 (p <0.0001 for each test), even though the same number of features differed between patients in all of these scenarios. Thus, at least in our data, the number of feature differences in a repeated scenario does not likely account for much variance in response stability."}, {"title": "2.3.3 Feature Weights and Stability", "content": "RQ3 asked whether participants show instability when a decision is \u201cdifficult\u201d because it seems like a close call. While prior work in psychology provides some evidence of an association be- tween difficulty and uncertainty for value-based decisions, it's unclear if this association holds in every setting as it can be abridged when participants are afforded enough time to ponder over difficult scenarios Lee and Daunizeau (2021). In our setting, one way to think about this would be to assume that each participant assigns a certain importance or weight for each feature a pa- tient has, and calculates (explicitly or implicitly) the \u201cpriority score\" assigned to each patient by summing the weights of all the patients' features (i.e., a linear weighted combination of patient features), and then chooses the patient with the highest priority score. Scenarios with patients of similar priority scores could feel like close calls.\nTo test this, we perform a Bradley-Terry (BT) analysis for each participant using their responses to all the pairwise comparisons specifically presented to them. Using BT analysis (and the regression\""}, {"title": "", "content": "subroutine in Eqn. (1)), we first learn the relative weights assigned to all patient feature differences. The distributions of feature weights are visualized in Appendix Figures 5 and 6. We normalize each participant's feature weight vector to have a quadratic norm of 1. Note that Eqn. (1) models features using differences in values across the two patients. One can also include raw feature values in this equation. However, including raw values does not lead to any improvement in the regression goodness-of-fit measures (e.g., pseudo-R2 values are mostly unchanged) in our case; hence, we mainly work with feature differences.\nNext, we used the learned participant-level feature weights to quantify the approximate priorities that each participant assigned to each profile in a pairwise comparison. For instance, say $ \\beta^{(i)}$ denotes the vector containing the relative weights learned for participant i using BT analysis. Suppose this participant is presented with a pairwise comparison (pA, pB), where pa is a vector containing feature values for patient A and p\u00b3 is a vector containing feature values for patient B. Then, the difference in priority score assigned by participant i for pairwise (pA, pB) comparison can be quantified as the absolute difference between the weighted sum of patient A and patient B profiles, i.e.,\n$\\left| \\sum_{j \\in F} \\beta_j^{(i)} p_j^A - \\sum_{j \\in F} \\beta_j^{(i)} p_j^B \\right| ,$\nwhere F denotes the patient feature set, i.e., F ={number of elderly dependents, years of life gained, obesity level, weekly work hours, years on the waiting list} for Study Two.\nWe test RQ3 by testing correlations between participants' learned priority score differences and response stability for all repeated scenarios. Results for Study One and Two are similar, but we focus on Study Two here due to its larger number of participants and a wider range of observed instability values. Study One results are reported in Appendix A."}, {"title": "3 Response Model Stability", "content": "So far, we have limited our analyses of response stability to the 6 scenarios out of the 60 that were repeated in each session. While the repeated scenarios are useful in directly assessing whether participants provide different responses to the same scenario at different times, responses to non- repeated scenarios can also provide additional insight into the stability of participants' decision- making models across sessions. To test this, we determined whether the statistical model that best fit each participant's responses in one session was the same model that best fit the same participants' responses in other sessions."}, {"title": "3.1 Methods", "content": "We trained both linear and non-linear models on each participant's data in each of their sepa- rate sessions, to cover a wide variety of hypothesis classes. Random Forests were chosen as the non-linear model for this analysis because they perform well off-the-shelf in many cases. Since participants' responses are binary in this elicitation setting, we use Logistic Regression for the linear model.\nAll models take as input a description of any pairwise comparison between two patients (pA, pB) and return a binary output indicating whether p^ or p\u00e5 should receive the kidney; in particular, the input to the model consists of the features of patient p^, the features of patient pB, and the dif- ference vector between patient features, i.e. pA \u2013 pB. We defined the best model for a participant in a session as the model among those tested that predicts the participant's choices in that session with the greatest accuracy.\nWe use each trained model to make predictions over the set of all possible pairwise comparisons (6,561 comparisons for Study One; 5,760,000 comparisons for Study Two). To measure agreement between any two models, we calculated the fraction of times they agreed in their predictions to all presented comparisons.\nWe also check for robustness to model multiplicity (due to random state selection) by testing per- formance for different data splits. Appendix B presents details of this analysis. Overall, perfor- mance variation across data splits is minimal.\nTo measure stability within each participant across sessions on multiple days, we trained a differ- ent model on each separate session of each participant, made predictions with those models on"}, {"title": "", "content": "the full set of all possible pairwise comparisons, and compared the predictions of pairs of session- specific models. Two models agree for a pairwise comparison when they predict the same patient will be chosen to get the available kidney. The average agreement between any two session- specific models (i.e., the fraction of comparisons where the two models have the same output) for a participant represents the stability of that participant across the two sessions.\nTo measure consistency between participants, we trained a model on data from all sessions of participant #1, repeated this procedure for participant #2, and so on, to get a separate model for each participant. Then we made predictions with those models on the full set of all possible patient pairwise comparisons described above and calculated how much agreement there was between the models for each pair of participants. This measure also serves as a baseline to compare within-participant stability against, since we expect consistency between participants to generally be lower than the response stability of each participant across their sessions.\nThe best-fit model for a participant's session may not be the true model the participant uses to make their judgments. It is also possible for two different models to make the same prediction; so, even if the best-fit models from two sessions make the same predictions, that does not mean a participant is using the same model in both sessions. To generate a baseline comparison with these issues in mind, we determined how much models trained on subsets of data generated from the following decision policy would agree (each rule is applied in order and is ignored when values are equal across Patients A and B):\nIf Patient A has more dependents than Patient B, give the kidney to Patient A (and vice- versa).\nIf Patient A has less alcohol history than Patient B, then give the kidney to Patient A (and vice-versa).\nIf Patient A has less criminal history than Patient B, then give the kidney to Patient A (and vice-versa).\nIf Patient A has more life gained than Patient B, then give the kidney to Patient A (and vice-versa).\nIf Patient A has been waiting longer than Patient B, give the kidney to Patient A (and vice- versa).\nIf Patient A has more elderly dependents than Patient B, give the kidney to Patient A (and vice-versa).\nIf Patient A will gain more life than Patient B, give the kidney to Patient A (and vice-versa).\nIf Patient A has less obesity than Patient B, give the kidney to Patient A (and vice-versa).\nIf Patient A will have more weekly work hours than Patient B, give the kidney to Patient A (and vice-versa)."}, {"title": "3.2 Results", "content": "The average accuracy of random forests models for each participant was 91% for Study One and 89% for Study Two (see Appendix Tables 9, 10 for individual model results).\nFor within-participant models, the average agreement using random forests was 80.9% for Study One and 79.6% for Study Two. These values were significantly lower (p < .0001 using the Mann-Whitney test) than the average stability of the baseline comparison model (88.3% for Study One and 88.6% for Study Two when using random forests), providing evidence that participants change their decision-making model across sessions.\nFor between-participant models, the average agreement using random forests was 75.2% for Study One and 73.4% for Study Two, as might be expected from the variety of weights individual par- ticipants placed on individual features (Figures 5 and 6 show feature weight distributions from earlier BT analysis). The agreement levels of between-participant models were smaller than those for within-participant models. This suggests that even if individuals do seem to change their decision-making models across sessions, those models don't differ as much as what would be expected if they judged as a \u201cnew person\u201d each session."}, {"title": "4 Discussion", "content": "In Studies One and Two, we found 85-90% stability\u2014which is 10-15% instability\u2014in participants' responses to every controversial scenario (S1C3 and S2C1-S2C3). In Section 3, a model-based analysis found an average of 79-81% stability in the random forest mod- els learned for different sessions of the same participant (average values in the second column\nWhile the model-based analysis shows slightly lower stability values than those ob- served for the repeated comparisons, some level of disparity here should be expected since a learned model is unlikely to completely capture each participant's exact decision-making process. Both analyses concur in the observation of a wide range of instability across participants. Our data suggested that this instability might be at least partially explained by the difference between the total weights participants placed on the patient features in a scenario, which can be interpreted as how \"close\" the patients' overall priorities were to each other for a given participant.\nWhen comparing results within the same participant to results between different participants from Section 3, we observe that stability within the same participant was only 5.7% higher in Study One and 6.6% higher in Study Two for non-linear models compared to stability between different participants (4.9% higher in Study One and 5.0% higher in Study Two for linear models). These small differences between within-participant model stability and between-participant model con- sistency are surprising if we expect people to agree with themselves much more than they agree with other people. Even so, these results line up well with previous studies Rehren and Sinnott-Armstrong (2023); Curry et al. (2019). Note that, both within participants and between participants stability levels are notably lower than the baseline condition.\nThe larger question raised by these results is whether and when common survey methods succeed in eliciting stable preferences. The instability we observe can be due to the sources we consider, due to weak or uncertain preferences, or perhaps, due to external circumstances (such as location or people around the participant). This does not mean that common moral preference elicitation approaches are unsuitable for all circumstances. In circumstances where 80-90% average stability over time is enough, common surveys can be adequate without repeating the same questions at different times with the same subjects. Our results also suggest that people might be stable in their responses to pairwise comparisons where the answer is very clear (i.e., large difference in the priorities assigned to each patient). Hence, if there is good reason to expect such clear preferences, then single-session approaches might be sufficient to learn moral preferences. However, when more certainty is required, so that 80-90% stability over time is not enough, and certain questions are expected to be difficult, researchers need to repeat the same questions at different times with the same subjects to determine whether their responses are stable. For example, if we are deciding whether to purchase a candy bar or a piece of fruit, it might not be so bad"}]}