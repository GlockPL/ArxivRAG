{"title": "SONIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning", "authors": ["Jianpeng Yao", "Xiaopan Zhang", "Yu Xia", "Zejin Wang", "Amit K. Roy-Chowdhury", "Jiachen Li"], "abstract": "Reinforcement Learning (RL) has enabled social robots to generate trajectories without human-designed rules or interventions, which makes it more effective than hard-coded systems for generalizing to complex real-world scenarios. However, social navigation is a safety-critical task that requires robots to avoid collisions with pedestrians while previous RL-based solutions fall short in safety performance in complex environments. To enhance the safety of RL policies, to the best of our knowledge, we propose the first algorithm, SONIC, that integrates adaptive conformal inference (ACI) with constrained reinforcement learning (CRL) to learn safe policies for social navigation. More specifically, our method augments RL observations with ACI-generated nonconformity scores and provides explicit guidance for agents to leverage the uncertainty metrics to avoid safety-critical areas by incorporating safety constraints with spatial relaxation. Our method outperforms state-of-the-art baselines in terms of both safety and adherence to social norms by a large margin and demonstrates much stronger robustness to out-of-distribution scenarios. Our code and video demos are available on our project website: https://sonic-social-nav.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "Social robots have been deployed in a variety of complex real-life environments with dynamic human crowds. They are expected to reach their destinations safely, efficiently, and politely without being too aggressive to intrude into humans\u2019 future trajectories or even collide with them, nor too conservative to fail their navigation tasks [1]. Reinforcement learning (RL) has shown great potential in this task due to its capability of learning optimal behaviors through interactions with dynamic environments [2]\u2013[5]. Recent research has shown that RL-based algorithms handle social navigation tasks in complex environments much better than traditional rule-based methods [6].\nHowever, since social navigation is a safety-critical task that requires the robots to exhibit extremely safe behaviors to avoid collisions with pedestrians, the performance of current RL solutions is yet to be satisfactory. Most existing research either presents experimental results in scenarios much simpler than real life [7] or demonstrates navigation performance in complex scenarios with an unsatisfactory success rate (e.g., below 90%) or collision rate [6]. This indicates that current RL-based planners lack the safety and adaptability needed for real-life applications. Existing methods have two major limitations. First, although previous work has employed advanced learning-based prediction models to facilitate the decision making process of robots, these methods do not explicitly quantify and handle the uncertainty of predictions [1] in an online manner, which degrades the safety and adaptability of the algorithms. Second, prior work fails to provide an effective way to control the behaviors of social robots and prevent them from acting dangerously. People often incorporate their heuristics and expert knowledge into the reward function for RL policy learning. However, the mapping between rewards and policy behaviors is often intractable. For example, increasing the collision penalty does not necessarily improve safety or performance.\nTo address the first limitation, we employ Adaptive Conformal Inference (ACI) [8], [9] to quantify prediction uncertainty, which provides an area with a pre-defined probability that humans will appear within. Compared to other conformal methods, ACI has the advantage of online updating and can adapt to arbitrary distribution shifts, which makes it very suitable for cases of trajectory prediction. Some works have attempted to combine uncertainty quantification results obtained by other conformal methods with RL by either directly inputting these results into policy networks [10] or forming a filter to process the actions generated by RL [11]. However, these methods not only lack the benefits of online updating uncertainties but also circumvent the direct guidance on the learning processes of RL. In our work, we not only enhance the observation using ACI-generated nonconformity scores but also directly guide the learning process of the RL agents according to the uncertainty measures.\nAs for the second limitation, Constrained Reinforcement Learning (CRL) [12], [13] is a promising solution since it can constrain the expected cost that RL agents receive below a threshold. However, in the context of social navigation, few works have presented impressive results in complex scenarios with CRL. From our perspective, directly constraining collision rates, as seen in previous work [14], is not an optimal"}, {"title": "II. RELATED WORK", "content": "A. Social Robot Navigation\nSocial robots are expected to interact with humans and complete various tasks such as providing assistance [15] and social navigation forms the foundation for accomplishing most high-level tasks. Robots are required to navigate in crowds, where the challenge of modeling dynamic human behavior makes navigation particularly difficult. It is crucial to model the high non-linearity of human behaviors, such as human intentions and interactions between agents [16]\u2013[20]. Deep reinforcement learning (DRL) offers a potentially viable solution to these challenges [21]\u2013[23]. Previous works on RL-based methods for social robots include capturing agent-agent interactions [2], [23] and the intentions of human agents [6], incorporating these as predictions into RL policy networks. Our work takes a step further by quantifying the uncertainties of these predictions and guiding robot behavior based on the uncertainty quantification.\nB. Planning Under Uncertainty\nTrajectory planning under uncertainty has become a field that attracts growing attention. In optimization-based and search-based methods, researchers have tried to combine uncertainty quantification from perception and prediction into different controllers [24], [25]. Both methods share the attribute of easily adding constraints or shielding, allowing for explicit management of uncertainties. In DRL-based planning, previous work has augmented observations of RL agents by inputting uncertainties to policy networks [10] and post-editing actions generated by RL policies according to uncertainties to generate safe behaviors [11]. However, most of these works circumvent direct guidance and regularization in the RL learning processes, resulting in RL agents that cannot fully leverage the uncertainty quantification results. More recently, Golchoubian et al. [26] combine prediction uncertainties into RL policies for low-speed autonomous vehicles and design a reward function to encourage the ego agent to avoid the uncertainty-augmented prediction area. However, their uncertainty metrics lack guaranteed coverage, and reward shaping is not always effective. Additionally, their methods were tested in a relatively simple environment with sparse human distribution. In contrast, we use DtACI [9] for uncertainty quantification, providing provable coverage and robustness against distribution shifts. We use CRL to guide policy learning and validate our results in a complex environment with dense crowds.\nC. Safe Reinforcement Learning\nSafe RL enables incorporating safety constraints into DRL methods, allowing dangerous conditions to be avoided [13]. Common techniques include state augmentation [27], adding safety layers or agents to modify actions generated by unsafe RL agents [28], and Lagrangian methods. Among these, Lagrangian methods are relatively easy to implement, can be applied to almost any RL algorithm, and outperform some more complex methods in benchmark tests [12]. In trajectory planning, previous work has validated that safe RL can fulfill safety constraints in some simple settings, such as simulation environments with sparse or static obstacles [12], [29]. However, in these settings where environments are relatively easy, safe RL algorithms do not show significant performance advantages compared to vanilla RL algorithms. In contrast, we validate our method in complex environments for social navigation with dense moving pedestrians, and it outperforms previous SOTA results by a large margin."}, {"title": "III. PRELIMINARIES", "content": "A. Adaptive Conformal Inference\nConformal methods can augment model predictions with a prediction set that is guaranteed to contain true values with a predefined coverage, enabling the quantification of un-certainties in a model-agnostic manner [9]. Traditional split conformal prediction requires a calibration set and places high demands on the exchangeability between the test sample and the calibration samples. In contrast, adaptive conformal inference (ACI) can dynamically adjust its parameters to"}, {"title": "IV. METHODS", "content": "A. Problem Formulation and Method Overview\nThe key aspect of social navigation is navigating through crowds and addressing the challenges of making appropriate decisions for successful navigation. In our setting, we have H humans, each indexed by h, within an episode of horizon T. At each time step t, we predict K future steps (a larger K means more extended future predictions) for each human\u2019s future trajectories to better understand their intentions. The prediction point for the k-th prediction of the h-th human is denoted as $p_{h,k}$, where $1 \\leq h < H$ and $1 \\leq k \\leq K$.\nWe formulate the task as a CMDP, where the CRL agent generates action $A_t = (v_x, v_y)$ to control the moving speed of the robot based on the observations of states $S_t$, which consists of two main parts. The first part includes physical information: the current positions of humans and the robot, and other quantities about the robot\u2019s dynamics. The second part comprises post-processed features generated by models, such as the predicted human trajectories. We denote physical state components of ego information as e, physical components of human information as h, and components generated by models as m. The complete state $S_t$ is written as $S_t = [e, h, m]$. After taking an action, the environment transitions to the next state based on the dynamics of humans and the robot and provides a reward $R_t$ and a cost $C_t$. We aim to obtain an optimal policy $\\pi(A_t | S_t)$ that maximizes rewards while adhering to the constraints on costs.\nSONIC introduces two mechanisms that are integrated with RL-based planners using prediction-augmented observations, as illustrated in Fig. 2. First, we use ACI to quantify prediction uncertainties and incorporate them into m. More specifically, we employ DtACI [9] as our quantification method, which provides predefined coverage of prediction errors and adapts to distributional shifts in an online manner, thus enhancing SONIC\u2019s adaptability. Second, we employ CRL to guide the agents\u2019 behavior based on the uncertainty quantification. Instead of applying constraints on the collision rate, we constrain the cumulative intrusions of the robot into other agents\u2019 spatial buffers, offering more behavior-level guidance and addressing the issue of sparse feedback.\nB. ACI for Quantifying Prediction Uncertainty\nUsing DtACI [9], we run M error estimate experts simultaneously for each prediction step for each pedestrian. At time step t, we calculate the actual prediction error $d_{h,k}$ between the current position and the predicted position made at time step $t -k$ for k-th prediction step of h-th human. We update the estimated prediction error generated by m-th expert of h-th human according to:\n$\\delta_{h,k}^{(m)} \\leftarrow (1 - \\gamma^{(m)}) \\delta_{h,k}^{(m)} - \\gamma^{(m)} (\\delta_{h,k} - \\delta_{err}^{(m)}),$ (2)\nwhere $\\delta_{err}^{(m)}$ represents the estimated prediction error of the m-th expert corresponding to a k-step ahead prediction for"}, {"title": null, "content": "h-th human, $\\gamma^{(m)}$ is the learning rate of the m-th expert for all humans and predictions, $\\alpha$ is the coverage parameter, and\n$\\epsilon_{h,k}^{(m)} := \\begin{cases} 1, & \\text{if } \\delta_{h,k}^{(m)} < d_{h,k}, \\\\ 0, & \\text{if } \\delta_{h,k}^{(m)} \\geq d_{h,k} \\end{cases}$ (3)\nIntuitively, when $\\delta_{h,k}^{(m)} < d_{h,k}$, implying that the estimation of the m-th expert does not cover the actual prediction error, $\\delta_{h,k}^{(m)}$ will increase by $(1 - \\alpha)\\gamma^{(m)}$; otherwise, $\\delta_{h,k}^{(m)}$ will decrease by $\\alpha\\gamma^{(m)}$. Therefore, the updating speed differs in these two cases. According to quantile regression [9], $\\delta_{h,k}^{(m)}$ will converge to be no less than $(1 - \\alpha)$ of all actual prediction errors, thereby achieving $(1 - \\alpha)$ coverage.\nSince we run M experts with different learning rates simultaneously, for each expert, after taking in the actual prediction error $d_{h,k}$ and updating the expert-estimated pre-diction error for the next step, we evaluate the errors of each expert and update the probability distribution for choosing the next output expert by\n$w_{h,k}^{(m)} \\leftarrow (1 - \\sigma) \\frac{w_{h,k}^{(m)}}{\\sum_{j=1}^{M}w_{h,k}^{(j)}} + \\sigma \\frac{p_{h,k}^{(m)}e^{-\\eta l (d_{h,k},\\delta_{h,k}^{(m)})}}{\\sum_{j=1}^{M}p_{h,k}^{(j)}e^{-\\eta l (d_{h,k},\\delta_{h,k}^{(m)})}},$ (4)\n$p_{h,k}^{(m)} \\leftarrow \\frac{w_{h,k}^{(m)}}{\\sum_{j=1}^{M}w_{h,k}^{(j)}},$ (5)\nwhere $w_{h,k}^{(m)}$ is the weight of probability $p_{h,k}^{(m)}$ for the m-th expert, $\\sigma$ and $\\eta$ are hyperparameters of DtACI for adjusting the changing speed of the weights, and $l (d_{h,k}, \\delta_{h,k}^{(m)})$ is the loss function for calculating the estimation error, which we use pinball loss. Each time we estimate prediction uncertain-ties, we sample one expert-generated estimate $\\delta_{h,k}^{(m)}$ from the probability distribution.\nAfter obtaining the expert-estimated prediction error, we incorporate the uncertainty quantification by concatenating it with the predicted trajectory before feeding it into the atten-tion layers for the RL agents to account for the uncertainty from the prediction in their decision making process. Also, to enable the agents to fully make use of this quantification, we design a spatial buffer for each pedestrian based on the results generated by DtACI and guide the ego agent to avoid entering these buffers, as explained in the next part.\nC. CRL with Spatial Relaxation\nDeploying CRL to social navigation by directly constrain-ing safety metrics like collision rates may lead to sparse feedback conditions, as costs are generated only at the end of episodes. This results in similar issues to the classic sparse reward problem, making it difficult for agents to learn optimal state and action values due to the potentially high complexity from actions to outcomes. To address this issue, we propose to provide RL agents with dense behavior-related guidance through costs. In the context of social navigation, we constrain the cumulative intrusions into a buffer zone based on the prediction uncertainty quantified by ACI around pedestrians and set constraints on these intrusions. We call our method spatial relaxation as it allows for a more flexible approach to managing safety. Instead of directly limiting"}, {"title": null, "content": "collision rates, our method tolerates minor intrusions within a controlled buffer zone, making the optimization problem easier to solve while maintaining a high level of safety.\nWe design the buffer of pedestrians as a combination of a circular area around the human\u2019s current position and an ACI-generated area around $K'$ ($K' < K$) steps of predic-tions. Since we have H human agents in the environment, the two parts of the buffer are defined as follows:\n$D_i(p_{ego}) = \\{ p_{ego}: |p_{ego} - p| \\leq r_i \\}, p \\in P_i, i = 1, 2$ (6)\n$P_1 = \\{ p_h \\}, P_2 = \\{ p_{h,k} \\}, 1 \\leq h < H, 1 \\leq k \\leq K'$ (7)\n$r_1 = r_{ego} + r_h + r_{disc}, r_2 = r_{ego} + r_h+ \\delta_{h,k},$ (8)\nwhere $D_1$ is the subarea that considers buffers around the current positions of humans and $D_2$ is the subarea that considers buffers around the predicted positions of humans. If the current center position of the ego robot $p_{ego}$ is in either $D_1$ or $D_2$, an intrusion occurs. For the computation, we consider the distance between the center positions of agents and prediction points. For the buffers corresponding to the current positions of humans, $r_1$ and $r_2$ are the corresponding distance thresholds for $D_1$ and $D_2$.\nAt each time step t, we iterate through all buffers of all humans and calculate the maximum intrusion, denoted as $d_{intru,t}$. For an episode with a horizon of T, we have\n$\\max_{\\pi} \\sum_{t=0}^{T} R_t (S_t, A_t) \\text{ s.t. } \\sum_{t=0}^{T} d_{intru,t} = d,$ (9)\nwhere d is a pre-defined threshold. We formulate the cost term $C_t$ using the intrusions into $D = D_1 \\cup D_2$:\n$C_t(S_t, A_t) = \\mu d_{intru,t},$ (10)\nwhere $\\mu$ is a constant. Our reward includes three components:\n$R_t (S_t, A_t) = \\begin{cases} R_{success}, & \\text{if } p_{ego} \\in S_{goal}, \\\\ R_{collision}, & \\text{if } p_{ego} \\in S_{fail}, \\\\ R_{potential}, & \\text{otherwise,} \\end{cases}$ (11)\nwhere $S_{goal}$ means the robot reaches the goal, $S_{fail}$ means the robot collides with other pedestrians, and $R_{potential}$ provides a dense reward that drives the ego robot to approach the goal, proportional to the distance the ego robot approaches the goal compared to the previous time step [6]. Our reward function retains only the essential terms since we aim to guide the behavior of CRL agents primarily through constraints rather than complex and intractable reward shaping.\nAccording to Lagrange duality [13], instead of directly solving the optimization goal in Eq. (9), we maximize a derived Lagrangian function:\n$\\mathcal{L}(\\pi, \\lambda) = \\sum_{t=0}^{T} R_t (S_t, A_t) - \\lambda (\\sum_{t=0}^{T} d_{intru,t} - d)\n= \\sum_{t=0}^{T} R_t (S_t, A_t) - \\lambda \\sum_{t=0}^{T} d_{intru,t} + \\lambda d,$ (12)\nwhere $\\lambda$ is the Lagrangian multiplier. Then, the optimization problem can be viewed as an unconstrained RL problem that aims to maximize a combined reward in Eq. (12)."}, {"title": null, "content": "In our work, we use the PPO Lagrangian [12] for max-imizing the Lagrangian function. We set up two critics to compute the state value for reward and the state value for cost. The loss functions for the two critics are defined as\n$\\mathcal{l}_{R} = c_1 (V_R(S_t) - V_{targ,R})^2,$ (13)\n$\\mathcal{l}_{C} = c_2 (V_C(S_t) - V_{targ,C})^2,$ (14)\nwhere $c_1$ and $c_2$ are constants, $V_R(S_t)$ and $V_C(S_t)$ are network-generated value estimates for reward and cost, respectively, and $V_{targ,R}$ and $V_{targ,C}$ are the corresponding target values for temporal difference updates.\nAs for the policy network, the action loss is similar to the form in PPO [30] where we employ the combined advantage $\\hat{A}_t = \\hat{A}_R - \\lambda \\hat{A}_C$, which is written as\n$\\mathcal{l}_{\\pi} = E_t [\\min (r_t(\\theta_3)\\hat{A}_t, \\text{clip} (r_t(\\theta_3), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)],$ (15)\nwhere $r_t(\\theta_3)$ represents the change ratio between the updated and old policy, $\\epsilon$ is the predefined clip ratio, and $\\hat{A}$ represents the estimated value advantage function using Generalized Advantage Estimation (GAE) at time step t. In our implementation, the parameters of the actor and reward critic, $\\theta_1$ and $\\theta_3$, share some network structures, as shown in Fig. 2, but the parameters of the cost critic are independent. We set the distribution entropy of the stochastic action to be constant and do not adopt the entropy loss from the original PPO. We found that this setting works better for SONIC when training with large batches.\nLastly, we update the Lagrangian multiplier $\\lambda$ using gradient descent so that the averaged cumulative intrusion $\\sum_{t=0}^{T} d_{intru,t}$ converges to the predefined value d. The loss function [13] for updating $\\lambda$ is defined as\n$\\mathcal{l}_{\\lambda} = -\\lambda(\\overline{C} - d_c),$ (16)\nwhere $\\overline{C}$ is the mean episode cost, which in practice is calculated by averaging over the past few episodes, and $d_c$ is the cost limit. Intuitively, when $\\overline{C}$ is greater than $d_c$ (i.e., intrusions are frequent), $\\lambda$ will increase according to gradient descent, leading the RL agents to consider cost advantages more when updating the policy, meaning that actions with larger costs will be less preferred, and vice versa. Note that we aim to train RL agents with satisfactorily safe behaviors; thus, in practice, we set $d_c$ to be small."}, {"title": "V. EXPERIMENTS", "content": "A. Simulation Settings and Evaluation Metrics\nWe train and evaluate our RL agents using CrowdNav [2], a simulator with flexible settings that can simulate complex pedestrian behaviors. Our training environment involves 20 humans and a robot in a 12m x 12m area with randomized positions and goals. The distance between the robot\u2019s start position and its goal is greater than 8 m, and the goals of the humans may change suddenly with a probability of 50% every five time steps. The robot radius is set to 0.2 m, while the human radius is randomly sampled between 0.3 m and 0.5 m. The maximum speed of humans is randomly sampled between 0.5 m/s and 1.5m/s, and the robot\u2019s maximum speed is set to 1.0 m/s. The behavior model of pedestrians is ORCA [31] and they do not react to the robot.\nWe adopt the standard evaluation metrics including success rate (SR), collision rate (CR), timeout rate (TR), navigation time (NT), path length (PL), intrusion time ratio (ITR), and social distance (SD) during intrusions [6].\nB. Baselines and Ablation Models\nOur baselines include ORCA [31], Social Force (SF) [32], and CrowdNav++ [6]. ORCA and SF are classic algorithms in obstacle avoidance, while CrowdNav++ represents the previous SOTA algorithm in social navigation. For Crowd-Nav++, we directly use their pre-trained model for testing. To validate the effectiveness of ACI and CRL with spatial relaxation, our ablation settings include: 1) RL (w/o ACI): Similar to CrowdNav++, but with disabled action entropy updates, constant action noise, and substantial hyperparameter changes; 2) RL (w/ ACI): Augmented observations of ACI-generated human prediction uncertainty, using RL without constraints; 3) CRL (w/ ACI, on CR): Augmented observations of ACI-generated human prediction uncertainty, using CRL with constraints directly on collision rates; 4) SONIC (w/ CV): Using both ACI and CRL with spatial relaxation, with a simple constant velocity (CV) model for human prediction; and 5) SONIC (w/ GST): Using both ACI and CRL with spatial relaxation, with a learning-based GST predictor [33] for human prediction.\nC. Implementation Details\nUnder each ablation setting, we train our models using three different random seeds on an NVIDIA RTX 4090 GPU, keeping all other hyperparameters consistent except those we intend to compare. Key common settings include: 1) All models use human prediction as input and employ a pre-trained GST predictor with fixed parameters to generate five steps of human trajectory predictions, except for SONIC (w/ CV). 2) We set the batch size to 32 and the clip parameter to 0.08 for PPO Lagrangian. 3) We choose the discomfort distance around humans, $r_{disc}$, to be 0.25 m and constrain costs over two prediction steps, while inputting predictions for all future five steps into the policy network with a coverage parameter $\\alpha$ = 0.1, corresponding to 90% coverage in prediction errors. 4) For DtACI hyperparameters, initial prediction errors are set to 0.1 m, 0.2m, 0.3 m, 0.4 m, and 0.5 m for 1-5 step-ahead predictions, respectively. We employ three experts with learning rates $\\gamma$ of 0.05, 0.1, and 0.2 for each DtACI estimator. 5) For CRL (w/ ACI, on CR), the cost limit corresponds to a 3% collision rate; for SONIC (w/ CV) and SONIC (w/ GST), the cost limit corresponds to cumulative intrusions of 0.16m per episode on average."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we present SONIC, an algorithm for social navigation that integrates ACI with CRL to generate safe robot trajectories that adhere to social norms. Our main contributions include the development of a novel framework that integrates nonconformity scores generated by ACI with CRL, enhancing the observations of RL agents and guiding their learning process, and the spatial relaxation to increase the applicability of CRL in social navigation, providing richer cost feedback and facilitating convergence without sacrificing safety. Our method achieves SOTA performance in both safety and adherence to social norms and shows strong robustness to OOD scenarios. For future work, we intend to extend SONIC to include perception uncertainty and develop an end-to-end solution for social robot navigation to address navigation tasks in complex real-life environments."}]}