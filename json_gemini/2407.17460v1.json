{"title": "SONIC: Safe Social Navigation with Adaptive Conformal Inference and Constrained Reinforcement Learning", "authors": ["Jianpeng Yao", "Xiaopan Zhang", "Yu Xia", "Zejin Wang", "Amit K. Roy-Chowdhury", "Jiachen Li"], "abstract": "Reinforcement Learning (RL) has enabled social robots to generate trajectories without human-designed rules or interventions, which makes it more effective than hard-coded systems for generalizing to complex real-world scenarios. However, social navigation is a safety-critical task that requires robots to avoid collisions with pedestrians while previous RL-based solutions fall short in safety performance in complex environments. To enhance the safety of RL policies, to the best of our knowledge, we propose the first algorithm, SONIC, that integrates adaptive conformal inference (ACI) with constrained reinforcement learning (CRL) to learn safe policies for social navigation. More specifically, our method augments RL observations with ACI-generated nonconformity scores and provides explicit guidance for agents to leverage the uncertainty metrics to avoid safety-critical areas by incorporating safety constraints with spatial relaxation. Our method outperforms state-of-the-art baselines in terms of both safety and adherence to social norms by a large margin and demonstrates much stronger robustness to out-of-distribution scenarios. Our code and video demos are available on our project website: https://sonic-social-nav.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "Social robots have been deployed in a variety of complex real-life environments with dynamic human crowds. They are expected to reach their destinations safely, efficiently, and politely without being too aggressive to intrude into humans' future trajectories or even collide with them, nor too conservative to fail their navigation tasks [1]. Reinforcement learning (RL) has shown great potential in this task due to its capability of learning optimal behaviors through interactions with dynamic environments [2]\u2013[5]. Recent research has shown that RL-based algorithms handle social navigation tasks in complex environments much better than traditional rule-based methods [6].\nHowever, since social navigation is a safety-critical task that requires the robots to exhibit extremely safe behaviors to avoid collisions with pedestrians, the performance of current RL solutions is yet to be satisfactory. Most existing research either presents experimental results in scenarios much simpler than real life [7] or demonstrates navigation performance in complex scenarios with an unsatisfactory success rate (e.g., below 90%) or collision rate [6]. This indicates that current RL-based planners lack the safety and adaptability needed for real-life applications. Existing methods have two major limitations. First, although previous work has employed advanced learning-based prediction models to facilitate the decision making process of robots, these methods do not explicitly quantify and handle the uncertainty of predictions [1] in an online manner, which degrades the safety and adaptability of the algorithms. Second, prior work fails to provide an effective way to control the behaviors of social robots and prevent them from acting dangerously. People often incorporate their heuristics and expert knowledge into the reward function for RL policy learning. However, the mapping between rewards and policy behaviors is often intractable. For example, increasing the collision penalty does not necessarily improve safety or performance.\nTo address the first limitation, we employ Adaptive Conformal Inference (ACI) [8], [9] to quantify prediction uncertainty, which provides an area with a pre-defined probability that humans will appear within. Compared to other conformal methods, ACI has the advantage of online updating and can adapt to arbitrary distribution shifts, which makes it very suitable for cases of trajectory prediction. Some works have attempted to combine uncertainty quantification results obtained by other conformal methods with RL by either directly inputting these results into policy networks [10] or forming a filter to process the actions generated by RL [11]. However, these methods not only lack the benefits of online updating uncertainties but also circumvent the direct guidance on the learning processes of RL. In our work, we not only enhance the observation using ACI-generated nonconformity scores but also directly guide the learning process of the RL agents according to the uncertainty measures.\nAs for the second limitation, Constrained Reinforcement Learning (CRL) [12], [13] is a promising solution since it can constrain the expected cost that RL agents receive below a threshold. However, in the context of social navigation, few works have presented impressive results in complex scenarios with CRL. From our perspective, directly constraining collision rates, as seen in previous work [14], is not an optimal approach to formulate the problem. This approach makes the optimization problem difficult to solve since costs are only received at the end of episodes, similar to the challenges in sparse reward conditions. RL agents may find it difficult to understand what leads to collisions since the complexity is potentially high. In our work, we propose to use uncertainty quantification obtained by ACI to design a buffer around pedestrians and constrain the cumulative intrusions into this buffer over an episode. Compared to previous methods that directly constrain collision rates, our method provides more behavior-level guidance to RL agents, offering rich cost feed-back. We call our method spatial relaxation in the context of social navigation. Our goal remains to improve safety, but by converting the constraints on collision rates to cumulative intrusions, the problem becomes easier to converge without compromising safety. In our paper, we demonstrate that spatial relaxation achieves significantly better performance compared to directly imposing constraints on collision rates.\nIntegrating these two techniques, we present SONIC (Safe Social Navigation with Adaptive Conformal Inference and Constrained RL), which can generate safe trajectories with minimal intrusions into pedestrians' paths, as illustrated in Fig. 1. The main contributions of this paper are as follows:\n\u2022 We develop a novel framework that integrates nonconformity scores generated by ACI with CRL, which not only enhances the observation of RL agents but also directly guides the learning process of RL agents.\n\u2022 We propose a technique to increase the applicability of CRL in the context of social navigation by introducing spatial relaxation. Compared to previous methods, spatial relaxation provides richer cost feedback and facilitates convergence without sacrificing safety.\n\u2022 Our method achieves state-of-the-art (SOTA) performance in social navigation in both safety and adherence to social norms, outperforming baselines by a large margin. Our method also shows much stronger robustness to out-of-distribution (OOD) scenarios."}, {"title": "II. RELATED WORK", "content": "A. Social Robot Navigation\nSocial robots are expected to interact with humans and complete various tasks such as providing assistance [15] and social navigation forms the foundation for accomplishing most high-level tasks. Robots are required to navigate in crowds, where the challenge of modeling dynamic human behavior makes navigation particularly difficult. It is crucial to model the high non-linearity of human behaviors, such as human intentions and interactions between agents [16]\u2013[20]. Deep reinforcement learning (DRL) offers a potentially viable solution to these challenges [21]\u2013[23]. Previous works on RL-based methods for social robots include capturing agent-agent interactions [2], [23] and the intentions of human agents [6], incorporating these as predictions into RL policy networks. Our work takes a step further by quantifying the uncertainties of these predictions and guiding robot behavior based on the uncertainty quantification.\nB. Planning Under Uncertainty\nTrajectory planning under uncertainty has become a field that attracts growing attention. In optimization-based and search-based methods, researchers have tried to combine uncertainty quantification from perception and prediction into different controllers [24], [25]. Both methods share the attribute of easily adding constraints or shielding, allowing for explicit management of uncertainties. In DRL-based planning, previous work has augmented observations of RL agents by inputting uncertainties to policy networks [10] and post-editing actions generated by RL policies according to uncertainties to generate safe behaviors [11]. However, most of these works circumvent direct guidance and regularization in the RL learning processes, resulting in RL agents that cannot fully leverage the uncertainty quantification results. More recently, Golchoubian et al. [26] combine prediction uncertainties into RL policies for low-speed autonomous vehicles and design a reward function to encourage the ego agent to avoid the uncertainty-augmented prediction area. However, their uncertainty metrics lack guaranteed coverage, and reward shaping is not always effective. Additionally, their methods were tested in a relatively simple environment with sparse human distribution. In contrast, we use DtACI [9] for uncertainty quantification, providing provable coverage and robustness against distribution shifts. We use CRL to guide policy learning and validate our results in a complex environment with dense crowds.\nC. Safe Reinforcement Learning\nSafe RL enables incorporating safety constraints into DRL methods, allowing dangerous conditions to be avoided [13]. Common techniques include state augmentation [27], adding safety layers or agents to modify actions generated by unsafe RL agents [28], and Lagrangian methods. Among these, Lagrangian methods are relatively easy to implement, can be applied to almost any RL algorithm, and outperform some more complex methods in benchmark tests [12]. In trajectory planning, previous work has validated that safe RL can fulfill safety constraints in some simple settings, such as simulation environments with sparse or static obstacles [12], [29]. However, in these settings where environments are relatively easy, safe RL algorithms do not show significant performance advantages compared to vanilla RL algorithms. In contrast, we validate our method in complex environments for social navigation with dense moving pedestrians, and it outperforms previous SOTA results by a large margin."}, {"title": "III. PRELIMINARIES", "content": "A. Adaptive Conformal Inference\nConformal methods can augment model predictions with a prediction set that is guaranteed to contain true values with a predefined coverage, enabling the quantification of un-certainties in a model-agnostic manner [9]. Traditional split conformal prediction requires a calibration set and places high demands on the exchangeability between the test sample and the calibration samples. In contrast, adaptive conformal inference (ACI) can dynamically adjust its parameters to maintain coverage in an online and distribution-free manner [8], making it appealing for time-sequential applications. Dynamically-tuned adaptive conformal inference (DtACI) [9] further boosts the applicability and performance of ACI by running multiple ACIs with different learning rates (each ACI is referred to as an expert) simultaneously. DtACI adaptively selects the best output based on its historical performance, eliminating the need to pre-acquire underlying data dynamics to achieve satisfying coverage.\nB. Constrained Reinforcement Learning\nCRL extends RL algorithms by incorporating constraints on the agent behavior. Unlike traditional Markov Decision Process (MDP) settings where agents learn behaviors only to maximize rewards, CRL is often formulated as a Constrained Markov Decision Process (CMDP). At time step t, an agent chooses an action At under state St, receives a reward Rt, and incurs a cost Ct, after which the environment transitions to the next state St+1. In a CMDP, the objective is not only to find an optimal policy that maximizes rewards but also to manage costs associated with certain actions or states, which may be defined as quantities related to safety in the context of social navigation. This is generally represented as [12]:\n$\\pi^* = \\arg \\max_{\\Pi \\in \\Pi_0} J_R(\\pi),$\n(1)\nwhere $J_R(\\pi)$ is a reward-based objective function, and $I_C$ is the feasible set of policies that satisfy the constraints added to the problem. The goal of CRL is to ensure that costs remain within pre-defined thresholds while maximizing reward."}, {"title": "IV. METHODS", "content": "A. Problem Formulation and Method Overview\nThe key aspect of social navigation is navigating through crowds and addressing the challenges of making appropriate decisions for successful navigation. In our setting, we have H humans, each indexed by h, within an episode of horizon T. At each time step t, we predict K future steps (a larger K means more extended future predictions) for each human's future trajectories to better understand their intentions. The prediction point for the k-th prediction of the h-th human is denoted as $p_{h,k}$, where $1 \\leq h < H$ and $1 \\leq k \\leq K$.\nWe formulate the task as a CMDP, where the CRL agent generates action $A_t = (v_x, v_y)$ to control the moving speed of the robot based on the observations of states $S_t$, which consists of two main parts. The first part includes physical information: the current positions of humans and the robot, and other quantities about the robot's dynamics. The second part comprises post-processed features generated by models, such as the predicted human trajectories. We denote physical state components of ego information as e, physical components of human information as h, and components generated by models as m. The complete state $S_t$ is written as $S_t = [e, h, m]$. After taking an action, the environment transitions to the next state based on the dynamics of humans and the robot and provides a reward Rt and a cost Ct. We aim to obtain an optimal policy $\\pi(A_t | S_t)$ that maximizes rewards while adhering to the constraints on costs.\nSONIC introduces two mechanisms that are integrated with RL-based planners using prediction-augmented observations, as illustrated in Fig. 2. First, we use ACI to quantify prediction uncertainties and incorporate them into m. More specifically, we employ DtACI [9] as our quantification method, which provides predefined coverage of prediction errors and adapts to distributional shifts in an online manner, thus enhancing SoNIC's adaptability. Second, we employ CRL to guide the agents' behavior based on the uncertainty quantification. Instead of applying constraints on the collision rate, we constrain the cumulative intrusions of the robot into other agents' spatial buffers, offering more behavior-level guidance and addressing the issue of sparse feedback.\nB. ACI for Quantifying Prediction Uncertainty\nUsing DtACI [9], we run M error estimate experts simultaneously for each prediction step for each pedestrian. At time step t, we calculate the actual prediction error $d_{h,k}$ between the current position and the predicted position made at time step $t -k$ for k-th prediction step of h-th human. We update the estimated prediction error generated by m-th expert of h-th human according to:\n$\\delta_{h,k}^{(m)} \\leftarrow (1 - \\gamma^{(m)})\\delta_{h,k}^{(m)} - \\gamma^{(m)}(\\delta_{h,k} - err_{h,k}^{(m)}),$\n(2)\nwhere $\\delta_{h,k}^{(m)}$ represents the estimated prediction error of the m-th expert corresponding to a k-step ahead prediction for h-th human, $\\gamma^{(m)}$ is the learning rate of the m-th expert for all humans and predictions, \u03b1 is the coverage parameter, and\n$err_{h,k}^{(m)} := \\begin{cases} 1, & \\text{if } \\delta_{h,k}^{(m)} < d_{h,k}, \\\\ 0, & \\text{if } \\delta_{h,k}^{(m)} \\geq d_{h,k} \\end{cases}$\n(3)\nIntuitively, when $\\delta_{h,k}^{(m)} < d_{h,k}$, implying that the estimation of the m-th expert does not cover the actual prediction error, $\\delta_{h,k}^{(m)}$ will increase by $(1 - \\alpha)\\gamma^{(m)}$; otherwise, $\\delta_{h,k}^{(m)}$ will decrease by $\\alpha\\gamma^{(m)}$. Therefore, the updating speed differs in these two cases. According to quantile regression [9], $\\delta_{h,k}^{(m)}$ will converge to be no less than $(1 - \\alpha)$ of all actual prediction errors, thereby achieving $(1 - \\alpha)$ coverage.\nSince we run M experts with different learning rates simultaneously, for each expert, after taking in the actual prediction error dh,k and updating the expert-estimated pre-diction error for the next step, we evaluate the errors of each expert and update the probability distribution for choosing the next output expert by\n$w_{h,k}^{(m)} \\leftarrow (1 - \\sigma) \\frac{w_{h,k}^{(m)}}{\\sum_{j=1}^{M} w_{h,k}^{(j)}} + \\sigma \\frac{\\exp(-\\eta l(\\delta_{h,k}, d_{h,k}))}{\\sum_{j=1}^{M} w_{h,k}^{(j)} \\exp(-\\eta l(\\delta_{h,k}, d_{h,k}))},$\n(4)\n$\\rho_{h,k}^{(m)} \\leftarrow \\frac{w_{h,k}^{(m)}}{\\sum_{j=1}^M w_{h,k}^{(j)}},$\n(5)\nwhere $w_{h,k}^{(m)}$ is the weight of probability $\\rho_{h,k}^{(m)}$ for the m-th expert, \u03c3 and \u03b7 are hyperparameters of DtACI for adjusting the changing speed of the weights, and $l(\\delta_{h,k}, d_{h,k})$ is the loss function for calculating the estimation error, which we use pinball loss. Each time we estimate prediction uncertainties, we sample one expert-generated estimate $\\delta_{h,k}$ from the probability distribution.\nAfter obtaining the expert-estimated prediction error, we incorporate the uncertainty quantification by concatenating it with the predicted trajectory before feeding it into the attention layers for the RL agents to account for the uncertainty from the prediction in their decision making process. Also, to enable the agents to fully make use of this quantification, we design a spatial buffer for each pedestrian based on the results generated by DtACI and guide the ego agent to avoid entering these buffers, as explained in the next part.\nC. CRL with Spatial Relaxation\nDeploying CRL to social navigation by directly constrain-ing safety metrics like collision rates may lead to sparse feedback conditions, as costs are generated only at the end of episodes. This results in similar issues to the classic sparse reward problem, making it difficult for agents to learn optimal state and action values due to the potentially high complexity from actions to outcomes. To address this issue, we propose to provide RL agents with dense behavior-related guidance through costs. In the context of social navigation, we constrain the cumulative intrusions into a buffer zone based on the prediction uncertainty quantified by ACI around pedestrians and set constraints on these intrusions. We call our method spatial relaxation as it allows for a more flexible approach to managing safety. Instead of directly limiting collision rates, our method tolerates minor intrusions within a controlled buffer zone, making the optimization problem easier to solve while maintaining a high level of safety.\nWe design the buffer of pedestrians as a combination of a circular area around the human's current position and an ACI-generated area around K' (K' < K) steps of predictions. Since we have H human agents in the environment, the two parts of the buffer are defined as follows:\n$D_i(p_{ego}) = {p_{ego}: |p_{ego} - p| \\leq r_i}, p \\in P_i, i = 1, 2$\n(6)\n$P_1 = {p_h}, P_2 = {p_{h,k}}, 1 \\leq h < H, 1 \\leq k \\leq K'$\n(7)\n$r_1 = r_{ego} + r_h + r'_{disc}, r_2 = r_{ego} + r_h+ \\delta_{h,k},$\n(8)\nwhere D\u2081 is the subarea that considers buffers around the current positions of humans and D2 is the subarea that considers buffers around the predicted positions of humans. If the current center position of the ego robot $p_{ego}$ is in either D\u2081 or D2, an intrusion occurs. For the computation, we consider the distance between the center positions of agents and prediction points. For the buffers corresponding to the current positions of humans, r\u2081 and r2 are the corresponding distance thresholds for D\u2081 and D2.\nAt each time step t, we iterate through all buffers of all humans and calculate the maximum intrusion, denoted as dintru,t. For an episode with a horizon of T, we have\n$\\max_{\\pi} \\sum_{t=0}^{T} R_t(S_t, A_t) \\text{ s.t. } \\sum_{t=0}^{T} d_{intru,t} = d,$\n(9)\nwhere d is a pre-defined threshold. We formulate the cost term Ct using the intrusions into D = D\u2081 U D2:\n$C_t(S_t, A_t) = \\mu d_{intru,t},$\n(10)\nwhere \u03bc is a constant. Our reward includes three components:\n$R_t(S_t, A_t) = \\begin{cases} R_{success}, & \\text{if } p_{ego} \\in S_{goal}, \\\\ R_{collision}, & \\text{if } p_{ego} \\in S_{fail}, \\\\ R_{potential}, & \\text{otherwise}, \\end{cases}$\n(11)\nwhere Sgoal means the robot reaches the goal, Sfail means the robot collides with other pedestrians, and Rpotential provides a dense reward that drives the ego robot to approach the goal, proportional to the distance the ego robot approaches the goal compared to the previous time step [6]. Our reward function retains only the essential terms since we aim to guide the behavior of CRL agents primarily through constraints rather than complex and intractable reward shaping.\nAccording to Lagrange duality [13], instead of directly solving the optimization goal in Eq. (9), we maximize a derived Lagrangian function:\n$L(\\pi, \\lambda) = \\sum_{t=0}^{T} R_t(S_t, A_t) - \\lambda (\\sum_{t=0}^{T} d_{intru,t} - d)$\n$= \\sum_{t=0}^{T} R_t(S_t, A_t) - \\lambda \\sum_{t=0}^{T} d_{intru,t} + \\lambda d,$\n(12)\nwhere \u03bb is the Lagrangian multiplier. Then, the optimization problem can be viewed as an unconstrained RL problem that aims to maximize a combined reward in Eq. (12).\nIn our work, we use the PPO Lagrangian [12] for max-imizing the Lagrangian function. We set up two critics to compute the state value for reward and the state value for cost. The loss functions for the two critics are defined as\n$l_R = c_1(V_R(S_t) - V_{targ,R})^2,$\n(13)\n$l_C = c_2(V_C(S_t) - V_{targ,C})^2,$\n(14)\nwhere c\u2081 and c\u2082 are constants, VR(St) and VC(St) are network-generated value estimates for reward and cost, respectively, and $V_{targ,R}$ and $V_{targ,C}$ are the corresponding target values for temporal difference updates.\nAs for the policy network, the action loss is similar to the form in PPO [30] where we employ the combined advantage $\\hat{A}_t = \\hat{A}_t^R - \\lambda\\hat{A}_t^C$, which is written as\n$l_\\pi = E_t [\\min (r_t(\\theta_3)\\hat{A}_t, clip(r_t(\\theta_3), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_t)],$\n(15)\nwhere $r_t(\\theta_3)$ represents the change ratio between the up-dated and old policy, \u03f5 is the predefined clip ratio, and $\\hat{A}$ represents the estimated value advantage function using Generalized Advantage Estimation (GAE) at time step t. In our implementation, the parameters of the actor and reward critic, \u03b8\u2081 and \u03b83, share some network structures, as shown in Fig. 2, but the parameters of the cost critic are independent. We set the distribution entropy of the stochastic action to be constant and do not adopt the entropy loss from the original PPO. We found that this setting works better for SONIC when training with large batches.\nLastly, we update the Lagrangian multiplier \u03bb using gradient descent so that the averaged cumulative intrusion $\\sum_{t=0}^{T} d_{intru,t}$ converges to the predefined value d. The loss function [13] for updating \u03bb is defined as\n$l_{\\lambda} = -\\lambda(\\hat{C} - d_c),$\n(16)\nwhere $\\hat{C}$ is the mean episode cost, which in practice is calculated by averaging over the past few episodes, and $d_c$ is the cost limit. Intuitively, when $\\hat{C}$ is greater than $d_c$ (i.e., intrusions are frequent), \u03bb will increase according to gradient descent, leading the RL agents to consider cost advantages more when updating the policy, meaning that actions with larger costs will be less preferred, and vice versa. Note that we aim to train RL agents with satisfactorily safe behaviors; thus, in practice, we set dc to be small."}, {"title": "V. EXPERIMENTS", "content": "A. Simulation Settings and Evaluation Metrics\nWe train and evaluate our RL agents using CrowdNav [2], a simulator with flexible settings that can simulate complex pedestrian behaviors. Our training environment involves 20 humans and a robot in a 12m x 12m area with randomized positions and goals. The distance between the robot's start position and its goal is greater than 8 m, and the goals of the humans may change suddenly with a probability of 50% every five time steps. The robot radius is set to 0.2 m, while the human radius is randomly sampled between 0.3 m and 0.5 m. The maximum speed of humans is randomly sampled between 0.5 m/s and 1.5 m/s, and the robot's maximum speed is set to 1.0 m/s. The behavior model of pedestrians is ORCA [31] and they do not react to the robot.\nWe adopt the standard evaluation metrics including success rate (SR), collision rate (CR), timeout rate (TR), navigation time (NT), path length (PL), intrusion time ratio (ITR), and social distance (SD) during intrusions [6].\nB. Baselines and Ablation Models\nOur baselines include ORCA [31], Social Force (SF) [32], and CrowdNav++ [6]. ORCA and SF are classic algorithms in obstacle avoidance, while CrowdNav++ represents the previous SOTA algorithm in social navigation. For Crowd-Nav++, we directly use their pre-trained model for testing. To validate the effectiveness of ACI and CRL with spatial relaxation, our ablation settings include: 1) RL (w/o ACI): Similar to CrowdNav++, but with disabled action entropy updates, constant action noise, and substantial hyperparameter changes; 2) RL (w/ ACI): Augmented observations of ACI-generated human prediction uncertainty, using RL without constraints; 3) CRL (w/ ACI, on CR): Augmented observations of ACI-generated human prediction uncertainty, using CRL with constraints directly on collision rates; 4) SONIC (w/ CV): Using both ACI and CRL with spatial relaxation, with a simple constant velocity (CV) model for human prediction; and 5) SONIC (w/ GST): Using both ACI and CRL with spatial relaxation, with a learning-based GST predictor [33] for human prediction.\nC. Implementation Details\nUnder each ablation setting, we train our models using three different random seeds on an NVIDIA RTX 4090 GPU, keeping all other hyperparameters consistent except those we intend to compare. Key common settings include: 1) All models use human prediction as input and employ a pre-trained GST predictor with fixed parameters to generate five steps of human trajectory predictions, except for SONIC (w/ CV). 2) We set the batch size to 32 and the clip parameter to 0.08 for PPO Lagrangian. 3) We choose the discomfort distance around humans, $r_{disc}$, to be 0.25 m and constrain costs over two prediction steps, while inputting predictions for all future five steps into the policy network with a coverage parameter \u03b1 = 0.1, corresponding to 90% coverage in prediction errors. 4) For DtACI hyperparameters, initial prediction errors are set to 0.1 m, 0.2 m, 0.3 m, 0.4 m, and 0.5 m for 1-5 step-ahead predictions, respectively. We employ three experts with learning rates \u03b3 of 0.05, 0.1, and 0.2 for each DtACI estimator. 5) For CRL (w/ ACI, on CR), the cost limit corresponds to a 3% collision rate; for SONIC (w/ CV) and SONIC (w/ GST), the cost limit corresponds to cumulative intrusions of 0.16 m per episode on average.\nD. In-distribution Test Results\n1) Quantitative Analysis: For all the test results shown in Tables I-III, we evaluated 1250 samples across 5 random test seeds and calculated the mean performance and standard deviations of models trained with three different training seeds. The test results under the same setting as the training environment (i.e., in distribution) are shown in Table I. From these results, we can see that rule-based methods SF and ORCA are not capable enough of handling complex scenarios in our settings, as indicated by their high collision rates (CR). However, both methods have low ITR and high SD since they are specifically designed to avoid intrusions. When comparing all the RL-based methods, both SONIC (w/ GST) and SONIC (w/ CV) outperform other methods by a significant margin in safety metrics and ITR, indicating that SONIC can generate both safe and polite trajectories that cause minimal intrusions to pedestrians. Although SoNIC has shorter TR, NT, and PL compared to RL models without constraints, this tradeoff is reasonable considering the overall improvement. While SONIC (w/ GST) shows slightly better performance than SoNIC (w/ CV) in safety metrics, the gap is small, implying that our methods can effectively mitigate prediction errors. Additionally, we find that CRL (w/ ACI, on CR) does not offer advantages compared to RL (w/ ACI), meaning that direct constraints on collision rates do not improve safety. SoNIC outperforms CRL (w/ ACI, on CR) in every metric, which clearly demonstrates the effectiveness of spatial relaxation. Lastly, we observe that RL (w/ ACI) has advantages over RL (w/o ACI), meaning that RL agents can learn to leverage uncertainty metrics automatically, although not as effectively as SoNIC.\n2) Qualitative Analysis: We visualize the behaviors of SONIC (w/ GST) and CrowdNav++ in the same episode in Fig. 3a) and Fig. 3b), respectively. At the beginning, CrowdNav++ decides to approach the goal directly. However, as the pedestrians move, they gradually surround the robot, leaving it with no escape route, which leads to an almost inevitable collision for CrowdNav++. In contrast, SONIC chooses to move the robot out of the crowds to prevent potential collisions or intrusions into the spatial buffers of pedestrians from the start. From step 24, we can see that SONIC rapidly responds to a human with a sudden change in direction, and the expanding uncertainty area due to prediction errors accumulated in the past few steps helps the robot perform an avoidance maneuver. Similar conditions occur again in steps 61 to 63, where the robot successfully avoids a human who changes direction before finally moving to the goal. This demonstrates that SoNIC is not only adept at making decisions that are beneficial in the long term but also capable of rapid execution in emergency situations.\nE. OOD Test Results\n1) OOD Scenarios Mixed with 20% Rushing Humans: In this setting, we set 20% of the human agents to have a maximum speed of 2.0 m/s, which is 33% higher than the maximum speed that may appear during the training phase. From the results in Table II, we can see that all methods perform worse than in in-distribution conditions. SF and ORCA have lower success rates but the overall performance drop in RL-based methods is more significant, which implies that rule-based methods can maintain a base-line performance but struggle to achieve high performance in complex scenarios. CrowdNav++, RL (w/o ACI), and RL (w/ ACI) experience a more substantial drop than CRL methods,"}, {"title": "F. ACI Effectiveness", "content": "To validate the effectiveness of ACI in covering actual prediction errors, we visualize the ACI errors of one human in Fig. 4. At the beginning of this trajectory, the ACI errors for multi-step predictions are large because the GST predictor lacks sufficient information to accurately predict human positions, leading to large actual prediction errors that our initial ACI values do not cover properly. However, after several steps, ACI quickly adapts to the actual prediction error and achieves adequate coverage. Additionally, if the coverage remains sufficient, the ACI error will decrease to ensure that the uncertainty estimation is not too conservative.\nG. Real Robot Experiments\nWe deploy our method on an iRobot Create 3 with a maximum speed of 0.306 m/s. We test our robot in scenarios with 5.3m \u00d7 6.4m dimensions and 5-8 humans walking towards their goals. Our experiments show that SoNIC behaves adaptively in real scenarios. More details and videos can be found on our project website."}, {"title": "VI. CONCLUSIONS", "content": "In this paper, we present SoNIC, an algorithm for social navigation that integrates ACI with CRL to generate safe robot trajectories that adhere to social norms. Our main contributions include the development of a novel framework that integrates nonconformity scores generated by ACI with CRL, enhancing the observations of RL agents and guiding their learning process, and the spatial relaxation to increase the applicability of CRL in social navigation, providing richer cost feedback and facilitating convergence without sacrificing safety. Our method achieves SOTA performance in both safety and adherence to social norms and shows strong robustness to OOD scenarios. For future work, we intend to extend SoNIC to include perception uncertainty and develop an end-to-end solution for social robot navigation to address navigation tasks in complex real-life environments."}]}