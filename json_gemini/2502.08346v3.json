{"title": "Graph Foundation Models for Recommendation: A Comprehensive Survey", "authors": ["Bin Wu", "Yihang Wang", "Yuanhao Zeng", "Jiawei Liu", "Jiashu Zhao", "Cheng Yang", "Yawen Li", "Long Xia", "Dawei Yin", "Chuan Shi"], "abstract": "Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.", "sections": [{"title": "1 Introduction", "content": "Recommender systems are essential components of contemporary digital landscape, enabling personalized services across a diverse range of fields, including e-commerce, social media, and entertainment [Zhang et al., 2023]. The data in RS generally consist of both structural information (e.g., user-item interactions) and textual information (e.g., user attributes and item descriptions). With the rapid development of graph learning, GNN-based methods have emerged as an important technology in RS, which can further enhance the collaborative signals of collaborative filtering and extend the signals to higher-order structures and external knowledge [Wu et al., 2022]. However, due to the inherent structural bias, they struggle to handle textual information. This is where the powerful capabilities of large language models, which have made significant impacts in the field of natural language processing (NLP) and come into play in the realm of RS [Yang et al., 2023; Zhai et al., 2024]. Leveraging the advanced text capabilities of LLM, these methods efficiently capture user and item textual information while integrating world knowledge for improved recommendations. However, their reasoning limitations restrict the collaborative signals they can comprehend. Inspired by the success of LLM in the NLP field, the graph domain has also been undergoing transformation, leading to the emergence of graph foundation models (GFMs) [Liu et al., 2023b]. By integrating GNN and LLM technologies, GFM-based RS can efficiently utilize data to align user preferences and make more precise recommendations with minimized bias, as depicted in Figure 1. By appropriately integrating key information from both graph structures and text, GFM-based RS hold significant potential to emerge as a new paradigm in RS.\nThe GFM-based RS effectively utilize the technological complementarity of GNN and LLM. GNNs struggle to model textual information, while the reasoning capabilities of LLMS do not support their comprehension of higher-order structural information. These two technologies complement each other's shortcomings in GFM, which emerges as a future opportunity in the field of recommendations. For example, LLMGR [Guo et al., 2024] injects the embeddings learned by GNN into the token embedding sequence of LLM, and adapts the GFM to the recommendation task through two-stage fine-tuning."}, {"title": "2 Preliminaries", "content": "In this section, we first introduce the basic concepts of GNN-based RS and LLM-based RS, then we provide the definition of graph foundation models, and finally we introduce the proposed taxonomy."}, {"title": "2.1 GNN/LLM-based Recommender Systems", "content": "As data grows explosively, recommender systems have emerged [Gao et al., 2023]. There are generally three types of data in RS: user data, item data, and user-item interaction data. These data not only contain strong structural information that requires graph representation, but also are rich in textual descriptions. Given the characteristics of the data in RS, they can essentially be abstracted into text-attribute graphs [Jin et al., 2024]. As two recently popular approaches, GNN-based RS and LLM-based RS both exhibit certain limitations in capitalizing on the information presented in the graphs. GNN-based RS excel at capturing complex higher-order relationships between nodes and model user preferences based on multi-hop neighbors, thus providing accurate recommendations [Wu et al., 2020; Wang et al., 2021]. However, the sequential order and semantic meaning of words within node descriptions pose a challenge for representation with graph structures, rendering these methods less effective at handling textual information. Conversely, LLM-based RS, with their robust contextual understanding and world knowledge, excel at processing textual descriptions [Yang et al., 2023; Zhai et al., 2024]. However, limitations in their sequential modeling designs and reasoning capabilities cause these methods to struggle with managing complex relationships."}, {"title": "2.2 Graph Foundation Models", "content": "The field of NLP has witnessed a revolution influenced by the Transformer architecture. Pre-trained language models based on the architecture have demonstrated formidable capabilities [Radford et al., 2019; Kenton and Toutanova, 2019]. These language foundation models, pre-trained on vast amounts of text, possess impressive generalization abilities that can adapt to a wide array of tasks [Bommasani et al., 2021]. When the scale of the language foundation model reaches a certain magnitude, it is referred to as the LLM [Zhao et al., 2023]. Inspired by the success of language foundation models in the NLP field, the field of graph learning also recognized the need to improve model performance and generalization capabilities through pre-training. This gave rise to the concept of graph foundation models [Liu et al., 2023b], which are models that are pre-trained on large datasets and incorporate graph structures to solve graph-related tasks. As combinations of LLM and graphs, GFMs acquire emergence and homogenization during pre-training [Liu et al., 2023b], enabling them to adapt seamlessly and perform impressively across a variety of downstream tasks."}, {"title": "2.3 Taxonomy of GFM-based RS", "content": "Gravitating towards the contextual backdrop of RS, we place substantial emphasis on examining the organic integration of graph with LLM in GFM, striving for more precise and user-specific recommendations. In accordance with the interrelationship between graphs and LLMs (as illustrated in Figure 2) we group the related works into three principal categories: Graph-augmented LLM, where the structural information from graphs is injected into LLMs to enhance the reasoning and generation capabilities for recommendation; LLM-augmented graph, where the structural information (e.g., topological structure) or textual information (e.g., user profiles and item descriptions) in the graph is enhanced with the aid of LLMs; LLM-graph harmonization, where the semantic embedding in LLMs and the structural embedding in graphs are combined seamlessly to achieve mutual optimization and maximize recommendation performance.\nIn the following sections, we provide a comprehensive introduction and discussion of the three main categories in the taxonomy of GFM-based RS."}, {"title": "3 Graph-Augmented LLM", "content": "LLMs excel at understanding and generating text but struggle with the complex relational structures inherent in recommender systems. While pre-training corpora and in-context learning provide some information, they lack an explicit mechanism to model the intricate relationships between users and items that are naturally represented as graphs. Recent research bridges this gap by integrating graphs with LLMs, focusing on the core challenge: How to design cross-modal interfaces that effectively bridge graph structures to language models? We categorize current methods into token-level infusion and context-level infusion (as illustrated in Figure 3), based on where the cross-modal interface is implemented."}, {"title": "3.1 Token-Level Infusion", "content": "This strategy integrates structural information directly into the LLM's input at the token level. Nodes or subgraphs are represented as special tokens, allowing the LLM to process structural information alongside text.\nSyntax-Integrated Injection. This approach embeds special tokens as syntactic components within the LLM's input sequence. For example, TMF [Ma et al., 2024a] introduces [ACTION] tokens like [view] or [purchase] to represent user actions within an interaction sequence. The embeddings for these actions are learned from a multi-behavior graph, enabling the LLM to process complex semantics like \"user [views] item\". Building on this, ELMRec [Wang et al., 2024c] generates a GCN-based embedding $h_i$ for each item $i$ and adds it as a correction term to the item's original text embedding $e_i$, resulting in a refined embedding $e'_i = e_i + h_i$. This operation blends textual and structural information, improving item representation.\nA natural progression from here is to consider whether the LLM can directly output these special tokens. LLMGR [Guo et al., 2024] explores this by not only including special tokens in the input but also modifying the LLM's output layer. They introduce a special token for each item, allowing the model to directly generate item tokens as recommendations, effectively creating a tighter link between graph-based recommendations and the LLM's output. Further advancing this line of thought, LightLM [Mei and Zhang, 2023] proposes a hierarchical indexing scheme, which decomposes user/item IDs into multiple special tokens based on a user-item graph-derived index. Each component token encodes a different attribute or function, moving away from opaque numerical IDs towards more semantically meaningful representations.\nSyntax-Decoupled Injection. This method appends graph embeddings as prefixes or suffixes, separating structural information from the main textual prompt. XRec [Ma et al., 2024b] prepends GNN-learned embeddings that represent user-item relationships to the prompt. These embeddings are trained to capture high-level semantic concepts, such as preference similarity between users. COMPASS [Qiu et al., 2024] com-"}, {"title": "3.2 Context-Level Infusion", "content": "This strategy provides structural information as context to the LLM, either through text descriptions or implicit retrieval, avoiding modifications to the LLM's architecture.\nExplicit Graph-to-Text Mapping. This method involves converting localized graph structures into natural language descriptions, essentially translating graph relationships into text. The following example illustrates explicit graph-to-text mapping: user A \u2192 purchase \u2192 item B \u2192 payment \u2192 credit card \u21d2 A purchased B with a credit card. The simplest form of this is exemplified by HetGCoT-Rec [Jia et al., 2025]. They extract multihop neighbors of a target node from a heterogeneous graph and concatenate their attributes to create a natural language description, which becomes the context for the LLM and informs its recommendations. Similarly, KGRec [Abu-Rasheed et al., 2024] extracts one-hop and two-hop related nodes from a knowledge graph and inserts them into predefined prompt templates, which guides the LLM in generating explainable recommendations.\nFurther refinements involve pre-processing the structural information before mapping it to text. GAL-Rec [Guan et al., 2024] maintains a dynamic queue of negative samples, items the user is known to dislike, based on knowledge graph insights and LLM feedback. Providing both positive and negative samples as context for the LLM allows for more nuanced recommendations. The processing of structural information can also occur after its initial conversion to text. GLRec [Wu et al., 2024a] constructs natural language descriptions of node paths in a job information graph. Each path represents a sequence of related job attributes or skills. During prompt construction, these paths are assigned different weights based on relevance and dependency strength, offering a more refined, contextually rich input to the LLM.\nImplicit Graph Retrieval. When explicit mapping is difficult, this approach uses GNN embeddings to retrieve relevant information from the graph semantically. For example, CLAKG [Chen et al., 2024] encodes a legal knowledge graph using a GNN, and retrieve relevant legal provisions based on the similarity between a user's case description embedding and the provision embeddings. These provisions are then concatenated into the prompt context. URLLM [Shen et al., 2024] retrieves a user's historical interactions from an item-attribute graph, focusing on neighbor interactions. These records are added to the prompt, providing the LLM with cross-domain preference information.\nContext-level infusion provides a flexible way to incorporate graph knowledge without altering the LLM's architecture. It leverages the LLM's ability to understand and reason over"}, {"title": "3.3 Discussion", "content": "Graph-augmented LLM methods enhance recommendations by encoding rich relational information from graphs, typically through token-level or context-level infusion. This couples the benefits of graph-structured data with the power of LLMs: the graph provides valuable relational context, while the LLM leverages its pre-trained knowledge to interpret it. Furthermore, since the LLM is the central component, this approach augments the recommender system's ability to extrapolate and make inferences in scenarios where interaction data is limited. However, this heavy reliance on the LLM also introduces inherent biases unsuitable for recommendation, such as a lack of diversity and distributional mismatch with user preferences, potentially limiting its scalability and generalizability. The alternative methods, by shifting the focus to enriching or harmonizing the graph itself, effectively mitigate these issues and offer different trade-offs."}, {"title": "4 LLM-Augmented Graph", "content": "Shifting the focus to the graph, the core idea of the LLM-augmented graph methods is to augment the data within graphs using LLM, thereby improving the effectiveness of various GNNs employed for recommendation tasks. Such methods can be categorized into topology augmentation and feature augmentation (as illustrated in Figure 4), based on the aspects of information enhanced in the text-attribute graph according to LLMs."}, {"title": "4.1 Topology Augmentation", "content": "Topology augmentation refers to the processes where the LLM restructures data, utilizing its world knowledge and contextual understanding capabilities to convert specific textual information into a structured format. Due to the introduction of new structural information, the topological structure of the graph constructed from the data is modified, thereby affecting the"}, {"title": "4.2 Feature Augmentation", "content": "Enhancing the topological structure of the graph using LLMs may introduce biases, as they are not particularly adept at extracting structural information from text. In contrast, directly improving the node features in the graph without altering the topological structure is an task where LLMs truly excel. This method focuses on leveraging the natural language processing capabilities of LLMs to augment data at the textual or embedding level, thereby influencing subsequent recommendations.\nSome works [Li et al., 2024; Chen and Suzumura, 2024] enhance the textual information of nodes by constructing appropriate prompts for input into LLMs. The enhanced textual information is subsequently encoded into embeddings by language models such as BERT. Unlike the aforementioned methods, GaCLLM [Du et al., 2024] integrates the stages of LLM and GNN. In this approach, the LLM assumes the role responsible for message passing within the GNN, performing textual message passing and aggregation for each node in the graph, which is subsequently encoded by BERT. As a special case, LIKR [Sakurai et al., 2024] employs LLMs to analyze user interaction histories to derive user-preferred item attributes. The nodes corresponding to these attributes in the graph serve as rewards for Markov walk-based reinforcement learning within the graph.\nThe textual information of users and items, being one of the abundant types of information in RS, significantly impacts the performance of RS when effective utilized. Consequently, LLM is increasingly becoming the preferred technology for feature augmentation of graphs based on textual information."}, {"title": "4.3 Discussion", "content": "The LLM-augmented graph methods, by incorporating the world knowledge of LLM into graph data, can enhance the capabilities of RS at the data level. This makes these methods more competitive in cold start scenarios with sparse interactions. Furthermore, the LLM in these methods is a plug-and-play component, allowing for flexible choices. Moreover, the data processing of the LLM can be performed offline in advance, and online recommendations based on statistical rules or neural models (e.g., GNN) can be made afterwards, significantly reducing time consumption. This has led to the increasing popularity of these methods in industry. However, these methods also have some drawbacks. First, graph learning"}, {"title": "5 LLM-Graph Harmonization", "content": "Graph-augmented LLM methods leverage external graph structures to enhance LLMs but often suffer from inefficiencies in real-time adaptability and increased computational overhead. Conversely, LLM-augmented graph methods attempt to incorporate LLMs into graph-based learning but struggle with scalability and effective knowledge utilization. To address these limitations, this section introduces a novel framework that optimally balances computational efficiency, adaptability, and reasoning capabilities.\nLLMs excel in capturing rich semantic information from unstructured textual data (e.g., item descriptions and user attributes), while GNNs are adept at modeling the topological structure of graphs (e.g., user-item interactions, social connections). As shown in Figure 5, harmonizing these two paradigms effectively can significantly enhance recommendation performance. Existing methods can be broadly categorized into two mainstream strategies: embedding fusion and embedding alignment, based on transformations of embeddings."}, {"title": "5.1 Embedding Fusion", "content": "The embedding fusion approach aims to combine LLM-derived textual representations with graph-learned structural embeddings, creating a unified feature space that leverages complementary information. This strategy emphasizes the synergy between textual semantics and graph-based connectivity.\nA notable framework in this domain is DynLLM [Zhao et al., 2024b], which incorporates graph structures into LLMs through dynamic memory-enhanced fusion. DynLLM addresses the limitation of static embeddings by using a dual-flow interaction mechanism: one flow learns from GNN-updated dynamic embeddings reflecting user-item interactions, while the other adapts LLM-generated embeddings based on real-time textual content. These embeddings are fused in a shared latent space, enhancing both semantic and structural understanding. Such a dynamic fusion approach not only captures the temporal evolution of recommendation data but also integrates high-quality textual semantics, leading to more accurate and adaptive recommendations. Another example is LKPNR [Runfeng et al., 2023], which combines LLMs with knowledge graphs to enhance personalized news recommendation. LKPNR leverages the semantic richness of LLMs to generate high-quality news representations and uses knowledge graphs to capture the relational structure of news entities. By integrating these modalities, LKPNR effectively addresses the long-tail problem in news recommendation.\nThe embedding fusion paradigm is particularly effective because it exploits the contextual richness of LLMs alongside the relational structures captured by GNNs. By fusing these modalities, models like DynLLM and LKPNR enable direct, dynamic, and efficient utilization of both textual and graph data, significantly improving representation learning in recommendation scenarios."}, {"title": "5.2 Embedding Alignment", "content": "Embedding alignment takes a different route by focusing on reconciling the heterogeneity between LLM-generated textual embeddings and GNN-learned structural representations. This strategy ensures that embeddings from both modalities can operate coherently within a unified representational space, reducing information loss and noise. The refined embeddings can provide more valuable information for recommendations.\nThe DALR framework [Peng et al., 2024] serves as a representative example, where structural embeddings from GNNs (e.g., user-item graph representations) and semantic embeddings from LLMs (e.g., product descriptions, user reviews) are aligned through contrastive learning paradigms. This alignment mitigates the semantic gaps and noise introduced by the inherently different data sources. Similarly, methods such as LLMRec [Wei et al., 2024] and RLMRec [Ren et al., 2024] also adopt multimodal alignment techniques, such as contrastive learning and MLP-based alignment, to unify embeddings from diverse modalities. For instance, LLMRec employs a denoised data robustification mechanism to enhance the reliability of augmented recommendation data, while RLMRec leverages contrastive alignment strategies to bridge the semantic space of LLMs with the collaborative relational signals from GNNs, thereby improving the overall quality of the learned representations.\nEmbedding alignment excels in its ability to unify heterogeneous modalities, ensuring consistent representation learning that facilitates better downstream recommendation tasks, such as personalized ranking or user preference clustering."}, {"title": "5.3 Discussion", "content": "The two approaches offer distinct advantages in integrating LLMs with graph learning for recommender systems. Embedding fusion directly combines textual semantics and structural relationships, leveraging their complementarity to enhance representation learning and personalization. Dynamic fusion methods, such as DynLLM, further enable real-time adaptation to evolving user preferences. Embedding alignment, in contrast, ensures coherence between textual and structural embeddings by mapping them into a shared space, mitigating inconsistencies. Methods like DALR leverages various contrastive learning approaches to enhance alignment robustness.\nHowever, embedding fusion may introduce redundant or conflicting information, and increase computational costs, while embedding alignment which is sensitive to noise depends on high-quality training data. In these methods, the LLM primarily serves as an encoder, and due to the constraints of the scenario, it cannot fully utilize its contextual understanding and language generation capabilities."}, {"title": "6 Challenges and Future Directions", "content": "GFMs have demonstrated great potential in recommender systems by incorporating graph structural information with external world knowledge of LLM. However, several challenges hinder their widespread adoption and effectiveness.\nHigh Computational Cost and Scalability Issues. Existing GFM-based RS require substantial computational resources, posing challenges for large-scale deployment [Zhai et al., 2024]. The integration of graph-based reasoning and LLM inference results in high memory consumption and slow inference speed, particularly when processing dense user-item graphs or generating personalized recommendations in real-time [Wang et al., 2024a]. Unlike traditional recommendation models, which can be efficiently pruned or quantized, GFMs face unique scalability constraints due to their reliance on long-range graph dependencies and LLM-generated representations. Addressing these limitations requires advancements in efficient model compression strategies tailored for graph-enhanced LLMs, and adaptive graph sparsification techniques to maintain performance while reducing overhead.\nRobustness Against Noisy and Adversarial Data. Real-world user interactions are inherently noisy, exhibiting short-term fluctuations, incomplete preferences, and adversarial perturbations [Zhang et al., 2023]. Traditional recommendation models rely on explicit feedback signals, making them susceptible to biased or manipulated data. In contrast, GFMs integrate graph-based user-item relationships and LLM-generated contextual representations, which introduces additional sources of noise from both structured and unstructured data. Ensuring robustness requires advancements in self-supervised denoising techniques, adversarial training tailored for multimodal representations, and uncertainty-aware modeling to mitigate the impact of unreliable signals while preserving recommendation accuracy.\nMulti-Modal Information Fusion. Modern recommendation scenarios involve a diverse range of data modalities, including text, structured graphs, images, audio, and video [Tao et al., 2020]. While existing GFMs primarily focus on textual and structural embeddings, effectively incorporating rich multi-modal signals remains an open challenge. Different modalities exhibit varying levels of granularity, semantic gaps, and computational costs, making seamless integration non-trivial. Future research should explore adaptive fusion frameworks, cross-modal alignment mechanisms, and lightweight multi-modal representation learning to balance efficiency and accuracy in large-scale recommender systems.\nLack of End-to-End Optimization. The concept of end-to-end recommender system is not unfamiliar. When deep learning was introduced into the field of recommendation, a process encapsulation was essentially performed [Covington et al., 2016]. However, the early neural model RS have gradually fallen behind the times. The process of such RS can be roughly divided into three stages: matching, ranking, and re-ranking [Gao et al., 2023]. In reality, this process is often more refined in industrial applications. Such a meticulous process naturally results in better recommendation performance. However, multi-stage model optimization requires a significant investment of time and manpower. Contrarily, an end-to-end generative RS, different from the one mentioned above, encapsulates multiple stages together for optimization. This significantly reduces complexity and can potentially lead to better performance. Gradually, similar endeavors are being pursued in the industrial field. HSTU [Zhai et al., 2024] simplifies the internal structure of the LLM and fully implements it through serial modeling. Moreover, [Wang et al., 2024d] takes into account both structural and textual information. Such an integrated generative recommendation that combines matching and ranking may likely be a hotspot in the future.\nKnowledge-Preference Gap. While GFMs leverage external knowledge to alleviate data sparsity, a fundamental misalignment persists between globally pre-trained world knowledge and personalized user preferences [Wang et al., 2024a]. Unlike embedding alignment, which focuses on bridging modality gaps (e.g., between graph structures and textual representations), this discrepancy stems from differences in how LLMs interpret knowledge and how users express preferences. For instance, LLM-based recommendation models may naturally generate factually coherent but overly neutral item descriptions, whereas users often respond more favorably to engaging or sensationalized content (e.g., \"Shocking! You won't believe this...\"). Addressing this challenge requires advancing preference-aware knowledge adaptation, dynamic refinement techniques, and contrastive learning strategies tailored to user-specific interests."}, {"title": "7 Conclusion", "content": "As an indispensable technology in modern society, recommender systems stand as one of the most prominent research areas within the field of artificial intelligence. The emergence of graph foundation models is likely to spark a new wave of research enthusiasm in the field of RS. In this survey, we present the first comprehensive overview of GFM-based RS and propose a logically organized taxonomy. Furthermore, we delve deeply into the challenges and vast potential of this field, aiming to inject new vitality into its research endeavors."}]}