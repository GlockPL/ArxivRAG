{"title": "GRIF-DM: Generation of Rich Impression Fonts using Diffusion Models", "authors": ["Lei Kang", "Fei Yang", "Kai Wang", "Mohamed Ali Souibgui", "Lluis Gomez", "Alicia Forn\u00e9s", "Ernest Valveny", "Dimosthenis Karatzas"], "abstract": "Fonts are integral to creative endeavors, design processes, and artistic productions. The appropriate selection of a font can significantly enhance artwork and endow advertisements with a higher level of expressivity. Despite the availability of numerous diverse font designs online, traditional retrieval-based methods for font selection are increasingly being supplanted by generation-based approaches. These newer methods offer enhanced flexibility, catering to specific user preferences and capturing unique stylistic impressions. However, current impression font techniques based on Generative Adversarial Networks (GANs) necessitate the utilization of multiple auxiliary losses to provide guidance during generation. Furthermore, these methods commonly employ weighted summation for the fusion of impression-related keywords. This leads to generic vectors with the addition of more impression keywords, ultimately lacking in detail generation capacity. In this paper, we introduce a diffusion-based method, termed GRIF-DM, to generate fonts that vividly embody specific impressions, utilizing an input consisting of a single letter and a set of descriptive impression keywords. The core innovation of GRIF-DM lies in the development of dual cross-attention modules, which process the characteristics of the letters and impression keywords independently but synergistically, ensuring effective integration of both types of information. Our experimental results, conducted on the MyFonts dataset, affirm that this method is capable of producing realistic, vibrant, and high-fidelity fonts that are closely aligned with user specifications. This confirms the potential of our approach to revolutionize font generation by accommodating a broad spectrum of user-driven design requirements. Our code is publicly available at https://github.com/leitro/GRIF-DM.", "sections": [{"title": "1 Introduction", "content": "Fonts constitute pivotal elements within the domain of creativity, design, and visual communication [1, 28]. The judicious choice of a suitable font holds the potential to substantially augment the impact of artistic endeavors, streamline design workflows, and infuse advertisements with expressiveness. Nowadays, there exists convenient access to an extensive array of over 270,000 fonts encompassing diverse designs, readily accessible online \u00b9. Font selection traditionally relies on retrieval-based methods, wherein users sift through extensive font libraries to identify the most fitting option. However, with the continual evolution of creative demands, there emerges a necessity for more adaptable and flexible approaches to font generation. Recently, generation-based techniques have emerged as a promising alternative, providing flexibility to accommodate user preferences and manifest distinct impression concepts. An ideal font generation methodology should exhibit diversity in generating predefined impression keywords and should be capable of accommodating variable-length combinations of these keywords as conditions.\nFont style transfer, as demonstrated in prior works [2, 45, 39, 14], has proven successful in font generation tasks. By furnishing textual content alongside visual style information, these models can generate synthetic fonts that emulate the specified style. Such methodologies prove particularly advantageous in situations where only a subset of characters from a font is accessible. In our scenario of rich impression font generation, users do not furnish a template font image for style information. Rather, they articulate their impressions through a list of keywords in natural language, and we anticipate the model to generate novel fonts based on this input.\nFollowing this idea, some Generative Adversarial Networks (GANs) based methods [20, 21, 38] are proposed to generate new fonts based on user queries provided as attributes. However, these approaches all contend with the challenge of employing multiple auxiliary losses for generating target new font images. GAN-based methods, which inherently rely on a binary discriminator loss to differentiate between fake and real samples, require additional auxiliary losses"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 GAN based Font Generation", "content": "GAN-based models consist of a generator and a discriminator, which are trained simultaneously in a competitive manner. The generator learns to produce realistic samples, such as font images, while the discriminator learns to distinguish between real data and generated samples. This adversarial training process encourages the generator to continually improve its ability to produce high-quality outputs"}, {"title": "2.2 Diffusion Model based Font Generation", "content": "Diffusion models [12] represent a novel research line of generative models, showcasing their potential to surpass GAN-based methods with many successful applications in text-to-image [27, 44], text-based image editing [10, 36, 3, 31, 34], object detection [4], image segmentation [7, 19, 42, 26], landmark detection [40] and more relatedly the multi-object tracking (MOT) [17, 18, 15]. Diffusion\nmodels generally learn a denoising model to gradually denoise from an original common distribution, e.g. Gaussian noise, to a specific data distribution. It performs a parameterized Markov chain to produce samples of a certain data distribution after a number of steps. In the forward direction, the Markov chain gradually adds noise to the data until it is mapped to a simple isotropic Gaussian distribution. As a consequence of the Markov principle, DDPM [12] exhibits a relatively slower sampling. To address this, various sampling techniques [16, 22] are developed to enhance the denoising speed. An example is DDIM [30], which introduced a deterministic non-Markovian process to accelerate the sampling process while producing high quality generations.\nMore recently, Diffusion Models have also been applied in Font generation. He et al. [9] proposed Diff-Font as the first Diffusion Model based Chinese font generation approach. Yang et al. [43] proposed FontDiffuser, which generates Chinese font images by diffusion model in the few-shot approach. Tanveer et al. [32] proposed DS-Fusion that generates the cat-like character image with the input prompt \"cat.\" Wang et al. [35] also generate artistic font consisting of \"pasta\" by the prompt \"pasta.\" These diffusion models for font generation show the great capacity to generate various decorative font images. However, the English alphabet font generation conditioned on impressions, which is the main topic of this paper, has never been explored from the view of Diffusion Models."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Problem Formulation", "content": "To formulate the problem of using a diffusion model to generate rich impression font images, we denote the dataset as D. Each entry (Xi, Yi) \u2208 D represents a font category Xi and its corresponding set of impression keywords Y. Here, X is the k letter image of i-th font, where k \u2208 {\"A\", \"B\", ..., \"Z\"}. For simplicity, we represent a real font letter image X as 20 in the literature. Let 0 represent the parameters of the diffusion model, which aims to learn the underlying distribution of font images in D. The goal is to generate a letter image 20 from a noise image x\u0442 ~ N(0, I) by iteratively generating denoised images XT\u22121, ..., Xt, ..., 10. The noise eo(xoxt, t, k, Yi) can be estimated by taking into account the noisy image xt, the current time step t, the letter k and the given set of impression keywords Yi. A few examples of (Xi, Yi) can be found in Tab. 1."}, {"title": "3.2 U-Net", "content": "Our proposed U-Net model adheres to the typical architecture, which consists of an encoder, a bottleneck, and a decoder module, as shown in Fig. 2.\nThe Encoder is composed of four repeated convolutional and linear blocks, shown in the bottom-left of Fig. 2. It takes as input a noisy font image xt, where t \u2208 {T,T \u2013 1, ..., 1}. Each \"ResBlock\" contains convolutional and linear layers with residual connections, extracting high-level features while preserving input height and width while enhancing feature depth. The final \"Conv\" layer reduces spatial dimensions, resulting in more compact latent features Fenc.\nThe Bottleneck module comprises two \"ResBlock\" modules, similar to those in the Encoder. Additionally, it incorporates a self-attention module to fuse contextual information from global contexts. This integration allows the model to capture dependencies\nacross various scales and effectively distill essential features for further processing. It takes as input the encoder feature Fenc and produce a bottleneck feature Fbtl of same size.\nThe Decoder comprises four iterative convolutional, linear, and deconvolutional blocks, illustrated in the bottom-right of Fig. 2. Similar to the Encoder and Bottleneck, it employs \"ResBlock\" modules. However, it differentiates itself through the inclusion of \"DeConv\" layers, which expand input dimensions while reducing depth. This process aims to reconstruct realistic output from compressed feature representations. The Decoder takes as input the bottleneck feature Fbtl and produce a single-channel noise prediction \u00ea with the same size as the input of encoder xt."}, {"title": "3.3 Text Embedding Modules", "content": "We employ pre-trained BERT [6] tokenizer and text encoder modules to handle both the letter and impression keywords. The BERT tokenizer operates at the word-piece level but can also tokenize individual letters. For letter BERT, we set \"max_seq_length\" to 3, including start and end tokens, while for impression BERT, it's configured to 512, allowing for varying sequence lengths of impression keywords. Thus, we can obtain a variable length impression embedding feature Cimp = BERT(Y) and a letter embedding feature Clet = BERT(k). Utilizing pre-trained BERT embeddings allows us to capture contextual information and semantic meaning from the input text. This enables our model to seamlessly integrate both letter and impression keywords, thereby improving the comprehension and generation of lifelike font images. Leveraging pre-trained BERT modules ensures efficient text processing and robust representation learning, thereby enhancing the efficacy of our proposed framework."}, {"title": "3.4 Dual Cross-attention Modules", "content": "In the U-Net generation process, addressing the length discrepancy between the single-character letter input and variable-length impression keywords presents a challenge. To overcome this, we introduce a dual cross-attention module, as depicted as blue and red blocks in Fig. 2. This module initially incorporates impression BERT feature Cimp using cross-attention (illustrated by blue arrows labeled as \"CrossAttn-IMP\") and subsequently integrates letter BERT feature Clet using another cross-attention mechanism (indicated by red arrows labeled as \"CrossAttn-LET\"). We seamlessly integrate these dual cross-attention modules into the encoder, bottleneck, and decoder components. These modules operate in tandem with convolutional and linear blocks, facilitating the effective integration of both letter and impression information. Consequently, our approach remains invariant to the variable length of impression keywords, ensuring that excessively long impression keywords do not unduly impact the letter information."}, {"title": "3.5 Training Process", "content": "In this paper, we hypothesize that a real font image X is determined by the letter k and impression keywords Y. Simplifying, we denote the real font image X as xo, where xo ~ q(xo|[Clet, Cimp]). Here, [Clet, Cimp] denotes the dual embedded latent features as conditions. We then iteratively add random Gaussian noise to xo for T times, transitioning it from a stable state xo to a chaotic state x\u0442. This iterative process is termed the diffusion process and is defined as follows:\n$q(x_{1:T}|x_0) = \\prod_{t=1}^T q(x_t|x_{t-1})$\nwhere each step diffusion is:\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t}x_{t-1}, \\beta_tI), t \\in \\{1, ...,T\\}$\n\u1e9e is an ascending variance schedule from 0 to 1 following DDPM [12]. Using the notation $a_t = 1 - \\beta_t$ and $\\bar{a}_t = \\prod_{i=1}^t a_i$, we can obtain xt at an arbitrary timestep t as the following:\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{a}_t}x_0, (1 - \\bar{a}_t)I)$\nThus, we can obtain xt as:\n$x_t = \\sqrt{\\bar{a}_t}x_0 + \\sqrt{1-\\bar{a}_t}\\epsilon, \\epsilon \\sim \\mathcal{N}(0, I)$\nIn the reverse process, our proposed model is to generate the designated font image by denoising the xt in the Markov chain by taking the dual embedded latent feature [Clet, Cimp] as letter and impression condition-pair. We denote the joint distribution $p_\\theta(x_{0:T}|[C_{let}, C_{imp}])$ as the reverse process Markov chain with learned Gaussian transitions starting at $p(x_T) = \\mathcal{N}(x_T; 0, I)$. Thus,\n$p_\\theta(x_{0:T}|[C_{let}, C_{imp}]) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t, [C_{let}, C_{imp}])$\nThen, we can formulate the reverse step-by-step denoising as:\n$p_\\theta (x_{t-1}|x_t, [C_{let}, C_{imp}]) = \\mathcal{N}(x_{t-1}; \\mu_\\theta (x_t, t, [C_{let}, C_{imp}], \\Sigma_\\theta(x_t, t, [C_{let}, C_{imp}]))$\nFollowing DDPM [12], we set $\\Sigma_\\theta(x_t, t, [C_{let}, C_{imp}])$ as constants and the diffusion model $\\epsilon_\\theta(x_t, t, [C_{let}, C_{imp}])$ learns to predict the noise \u0454 added to xo in diffusion process from xt with condition [Clet, Cimp]. Finally, the denoising training process can be summa-rized as:\n$\\mathcal{L} = \\mathbb{E}_{x_0, c, k, Y; \\epsilon} ||\\epsilon - \\epsilon_\\theta(x_t, t, [C_{let}, C_{imp}])||^2$\nwhere $x_0 \\sim q(x_0), \\epsilon \\sim \\mathcal{N}(0, I), k \\in \\{\u201cA\u201d, \u201cB\u201d, ..., \"Z\" \\}$, and Yi \u2208 Y."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Dataset", "content": "We utilize the MyFonts dataset [5] for all our experiments. Following the approach outlined in [20, 21], we select uppercase letters ranging from \"A\" to \"Z\" in the dataset. However, unlike Matsuda [21], who conducted manual inspections to remove non-alphanumeric characters, we employ the following rules to automatically filter out unwanted characters: First, we discard fonts with fewer than 5 impression keywords to ensure that the remaining fonts are more specific and tailored; second, we remove fonts with a width-to-height ratio exceeding 2:1, effectively filtering out certain dingbat characters in an automated manner. Finally, we shuffle the fonts and randomly divide them into training and test sets, with a ratio of 90% for training and 10% for testing."}, {"title": "4.2 Implementation Details", "content": "Our implementation of the diffusion framework is built from scratch, drawing inspiration from DDPM [12] and DDIM [29]. We utilize a batch size of 256 and a learning rate of 2 \u00d7 10-4 with a step scheduler that decreases by 90% every 10 epochs. Training for the DDPM model is conducted with T set to 1,000 time steps, while evaluation for the DDIM model is performed with T set to 100 time steps to enhance evaluation speed. Text Embedding Modules employ BERT with pre-trained weights from \"google-bert/bert-base-uncased\", while font images are pre-processed into grayscale and resized to 32x32 pixels. The model is trained on the MyFonts training set for 400 epochs using a single NVIDIA A40 GPU, with an Adam optimization algorithm. More details can be found in our code."}, {"title": "4.3 Quantitative Results", "content": "We utilize FID [11] and Intra-FID [24] for the quantitative evaluation. FID measures the diversity and quality of generated font images with the pre-trained Inception Neural Network. Following the same amount of 26 \u00d7 5,000 generated samples in [21], we randomly select 5,000 fonts from the whole 14,859 fonts of the dataset (13,374 fonts from training set and 1,485 fonts from test set) as the groundtruth, and make use of all the impression keywords to each font to generate synthetic font images. This setup provides a fair comparison with other methods in Tab. 3 as it utilizes the same number of randomly selected 5,000 fonts. Our proposed GRIF-DM has achieved an FID of 6.693, outperforming other GAN-based methods. Additionally, in a more rigorous assessment, we compute FID using the entire test set of 1,485 unseen fonts. GRIF-DM achieves an FID of 8.347, slightly inferior to the 5,000 random samples scenario, yet still outperforms other GAN-based methods.\nFor Intra-FID, we adopt the approach outlined in [21], selecting only frequent impression keywords associated with over 200 fonts to ensure sufficient samples per class. This results in 277 impression keywords from all the dataset (training and test set). For each keyword, we generate 5,200 synthetic images (200 fonts multiplied by 26 letters). As fonts are grouped with their respective impression keywords, synthetic font images are generated based solely on the specified impression keyword as condition. This presents a challenging scenario where real fonts encompass diverse styles, yet the generated font images are restricted to a single impression keyword for generation. It evaluates the diversity capacity under the constraint of a single impression keyword. Intra-FID is calculated as the average FID across all impression classes. GRIF-DM achieves an Intra-FID of 43.119, surpassing other GAN-based methods. Nevertheless, there is still room for improvement in enhancing the diversity capacity of GRIF-DM when provided with a single impression keyword."}, {"title": "4.4 Font Diversity", "content": "To qualitatively assess font diversity in our GRIF-DM, we randomly select three unseen fonts from the test set, as indicated by the blue boxes with font names at the top of Fig. 3, given different random noise at x, we can generate different font images as shown in the orange dashed boxes."}, {"title": "4.5 Exploration on Impression Keywords", "content": "We conduct an exploration experiment for impression keywords, depicted in Fig. 4. The first row features a real font from the test set, displaying letters \"H\", \"E\", \"R\", and \"O\", along with the full impression keywords listed on the left. In the second row, generated samples using the same full impression keywords as input are showcased. To clarify our concept, we emphasize the three primary impression keywords \"heavy\", \"narrow\" and \"open-shade\" while excluding the others, yet we utilize all impression keywords for generation. Yet as shown in the second row, the letters appear slightly thicker than the ground truth due to the semantic contrast between the impression keyword \"heavy\" and \"narrow\". To validate our intuition, we replace \"heavy\" with its antonym \"light\" while maintaining the remaining impression keywords unchanged, resulting in notably thinner generated font images in the third row. Conversely, replacing the impression keyword \"narrow\" with its antonym \"wide\" while keeping the remaining impression keywords unchanged yields wider font images in the fourth row.\nAdditionally, removing the impression keyword \"open-shade\" darkens the generated font images in the fifth row while still conveying the intended impression. Lastly, in the final row, we replace the impression keyword \"heavy\" with its synonym \"cumbersome\". It is evident that the generated font images effectively convey the intended impression keywords, despite \"cumbersome\" not being present in the dataset. Generated font images make sense because leveraging BERT brings synonymous keywords closer in the feature space. By employing impression sentences with cross-attention instead of strict impression vectors, our method demonstrates robustness to out-of-vocabulary (OOV) impression keywords."}, {"title": "4.6 Qualitative Comparison with SoTA", "content": "In Fig. 5, we show the qualitative comparison with the state of the arts: Imp2Font [20] and Imp2Font-v2 [21] as shown in the second and third columns, respectively. The first column depicts real font images alongside their corresponding impression keywords, shown vertically in blue. The last column showcases font images generated by our proposed method GRIF-DM. The results demonstrate that our proposed method excels in both generating diverse font images and maintaining high fidelity compared to state-of-the-art methods.\nIn the \"vintage\" impression row, GRIF-DM demonstrates diverse cursive strokes, exhibiting superior diversity and high-fidelity compared to other methods. In the \"horror\" impression row, GRIF-DM impressively generates high-fidelity font images, notably with the \"H\" resembling axes and the \"E\" resembling knives. In the \"fat\" impression row, imp2font produces \"fat\" font images, but some lack readability due to textual content issues. imp2font-v2 generates\nthick, albeit not \"fat\" font images. Meanwhile, GRIF-DM generates diverse \"fat\" font images with different round and square shapes.\nIn the \"narrow, ancient\" impression row, GRIF-DM generates font images that convey the intended impression keywords but lacks the narrow fidelity of the ground truth. Similarly, state-of-the-art methods also struggle to achieve this level of narrow fidelity. In the \"3d, shadow\" impression row, both the state-of-the-art methods and our GRIF-DM exhibit poor performance, particularly with rotated \"3d\" real font images, resulting in a loss of fidelity across all methods."}, {"title": "4.7 Failure Cases", "content": "In Tab. 4, we highlight significant failure cases observed in our experiments. Due to the lack of manual filtering for the MyFont dataset, non-alphabetic symbols, such as dingbat flowers, are present, as demonstrated in the first row. Interestingly, our model is capable of generating various styles of flowers conditioned on the \"flower\" impression keyword. In the second row, difficulties arise in accurately generating fonts corresponding to specific impression keywords, such as \"stitch\", resulting in fonts resembling \"gothic\" instead. The issue arises due to the imbalance in keyword distribution. Lastly, while attempting to generate \"funny\" and \"curly\" fonts, our model struggles to achieve high-fidelity results, although some curvature may be discernible.\nThus, our experiments reveal challenges in accurately generating fonts corresponding to specific impression keywords, particularly when faced with imbalanced keyword distributions. Addressing these challenges could improve the performance of our model in generating diverse and faithful font images."}, {"title": "5 Conclusion and Future Work", "content": "Our paper presents a diffusion-based method for generating fonts that are rich in impression, utilizing novel dual cross-attention modules. These modules adeptly integrate impression keywords with specific letters, facilitating a seamless generation process. Through extensive experimentation, our approach, denoted as GRIF-DM, has proven effective in producing fonts that are not only realistic and vivid but also highly customized, meeting specific user demands with high fidelity.\nFor future work, we plan to enhance GRIF-DM by incorporating Large Language Models (LLMs) into the font generation pipeline. This integration allows users to input a single natural language text, blending both the textual content and desired impression characteristics. This advancement will streamline the input process, enabling a more intuitive experience and potentially broadening the applicability of our method to a wider range of creative and commercial uses.\nLimitations. Our method employs a diffusion architecture for generating fonts, which, while innovative, also presents certain challenges. Firstly, the training process can be resource-intensive. Although diffusion acceleration techniques have been employed to expedite training, there remains a need for further optimization to reduce the computational overhead associated with our method. This is essential to making the approach more feasible and accessible for broader use, especially in environments with limited computational resources. Secondly, our current focus is limited to generating fonts for the English alphabet. This limitation restricts the applicability of our method to global contexts, particularly in languages with more complex character systems, such as Chinese and Japanese. Extend-ing our method to accommodate these and other languages presents significant challenges, not only in terms of the sheer variety of characters, but also in capturing the unique stylistic nuances each language's script entails.\nBroader Impacts. The adoption of personalized font generation models offers exciting prospects across a multitude of applications spanning creativity, design, and visual communication domains. Nonetheless, it is crucial to recognize potential risks associated with their deployment, such as the propagation of misinformation, potential misuse, and the introduction of biases. Ethical considerations and broader impacts necessitate a comprehensive examination to ensure the responsible utilization of these models and their capabilities."}]}