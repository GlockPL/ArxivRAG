{"title": "Understanding Deep Learning via Notions of Rank", "authors": ["Noam Razin", "Nadav Cohen"], "abstract": "Despite the extreme popularity of deep learning in science and industry, its formal understanding is limited. Common practices are based primarily on trial-and-error and intuition, often leading to suboptimal results. As a result, there is significant interest in developing a formal theory of deep learning, with the hopes that it will shed light on empirical findings, and lead to principled methods for improving the efficiency, reliability, and performance of neural networks.\nThis thesis puts forth notions of rank as key for developing a theory of deep learning. Specifically, building on a connection between certain neural network architectures and tensor factorizations, we employ notions of rank for studying the fundamental aspects of generalization and expressiveness.\nWith regards to generalization, the mysterious ability of neural networks to generalize is widely believed to stem from an implicit regularization a tendency of gradient-based training towards predictors of low complexity, for some yet unknown measure of complexity. Through dynamical analyses, we establish an implicit regularization towards low rank in several types of neural network architectures (for corresponding notions of rank). Notably, this implicit rank minimization differs from any type of norm minimization, in contrast to prior beliefs. Implications of this finding for explaining generalization over natural data (e.g., audio, images, and text), as well as practical applications (novel regularization schemes), are presented.\nWith regards to expressiveness, we theoretically characterize the ability of graph neural networks to model interactions via separation rank a measure commonly used for quantifying entanglement in quantum physics. As a practical application of our theory, we design an edge sparsification algorithm that preserves the ability of graph neural networks to model interactions. Empirical evaluations demonstrate that it markedly outperform alternative methods.", "sections": [{"title": "I Introduction", "content": "In the past decade, deep learning has been experiencing unprecedented success, and is largely responsible for the technological breakthroughs referred to in the public as \u201cartificial intelligence\u201d. However, despite the extreme popularity of deep learning in science and industry, its formal understanding is limited. Common practices are based primarily on trial-and-error and intuition, often leading to suboptimal results, as well as compromise in important aspects including safety and robustness. As a result, there is significant interest in developing a formal theory of deep learning, with the hopes that it will shed light on empirical phenomena, and lead to principled methods for improving the efficiency, reliability, and performance of neural networks.\nFrom the perspective of learning theory, understanding deep learning requires ad-dressing the fundamental questions of optimization, generalization, and expressiveness. Optimization concerns the effectiveness of gradient-based methods in minimizing neural network training objectives that are non-convex. Generalization treats the performance of a neural network beyond its training data. Lastly, expressiveness refers to the ability of practically sized neural networks to represent rich classes of functions.\nThis thesis focuses on two of the fundamental questions \u2014 generalization and expres-siveness. It puts forth notions of rank as key for developing a theory of deep learning. Our approach adopts tools from dynamical systems theory and tensor analysis, build-ing on a recent connection between certain neural network architectures and tensor factorizations. The main theoretical contributions and their practical implications are summarized below.\nOne of the central mysteries in deep learning is the ability of neural networks to generalize over natural data (e.g., audio, images, and text) when trained via gradient-based methods, despite having far more learnable parameters than training examples. This generalization takes place even in the absence of any explicit regularization. Thus, conventional wisdom is that gradient-based training induces an implicit regular-ization a tendency to fit training examples with predictors of minimal complexity, for some measure of complexity. The fact that natural data gives rise to generalization is accordingly understood to result from an agreement between the complexity measure implicitly minimized during training and the complexity of the data. More specifically, from the amenability of natural data to be fit with predictors of low complexity.\nMathematically formalizing the above intuition is regarded as a major open problem in the theory of deep learning. A significant challenge towards doing so is that we lack definitions for predictor complexity that are both implicitly minimized during training of neural networks and capture the essence of natural data (in the sense of natural data being fittable with low complexity). One widespread hope, initially articulated in is that a characterization based on minimization of norms may apply. Namely, it is known that for linear regression gradient-based methods converge to the solution with minimal Euclidean norm (see, e.g., Section 5 in ), and the hope is that this result can carry over to neural networks if we"}, {"title": "II Generalization via Implicit Rank Minimization", "content": "allow replacing the Euclidean norm with a different (possibly architecture-dependent) norm .\nA standard testbed for studying this prospect is matrix factorization a model equiv-alent to linear neural networks, i.e. fully-connected neural networks with no non-linearity . In Chapter 1 (based on we prove that, in contrast to prior belief , the implicit regularization in matrix factorization cannot be captured by norms. Specifically, we show that there exist settings in which it drives all norms towards infinity in favor of minimizing rank. This indicates that, rather than perceiv-ing the implicit regularization via norms, a potentially more useful interpretation isminimization of rank.\nCapitalizing on this interpretation, in Chapters 2 and 3 (based on and ,respectively), we establish that the tendency towards low rank extends from linearneural networks to more practical non-linear neural networks (with polynomial non-linearity), which are equivalent to tensor factorizations. By characterizing the dynamicsthat gradient-based methods induce on such networks, we show that these result in a bias towards low rank, for architecture-dependent notions of rank defined over tensors. To the best of my knowledge, our results constituted the first evidence for implicit regularization minimizing a notion of rank in non-linear neural networks. Subsequent works have demonstrated that an analogous phenomenon occurs in other types of neural networks as well .\nMotivated by the fact that notions of rank capture implicit regularization in certainnon-linear neural networks, we empirically explore their potential as measures of complexity for explaining generalization over natural data. We find that it is possibleto fit standard image recognition datasets with predictors of extremely low rank. This leads us to believe that notions of rank may pave way to explaining both implicit regularization in deep learning and properties of natural data translating it to generalization.\nIn terms of practical impact, based on our theory we develop an explicit regulariza-tion scheme for improving the performance of convolutional neural networks overtasks involving non-local interactions. Other research groups have also built uponour analyses of implicit rank minimization for designing practical deep learning systems ."}, {"title": "III Expressiveness of Graph Neural Networks via Separation Rank", "content": "In Chapter 5 (based on , we extend the aforementioned connection between neu-ral networks and tensor factorizations for studying the expressiveness of graph neuralnetworks (GNNs) . GNNs are widely used for modeling complex interactionsbetween entities represented as vertices of a graph . Yet, a formalcharacterization of their ability to model interactions is lacking. We address this gapby formalizing strength of interactions via separation rank - a measure widelyused for quantifying entanglement in quantum physics. Through this notion of rank,we characterize the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices.\nOur analysis reveals that the ability of a GNN to model interaction is primarily deter-mined by the partition's walk index a graph-theoretical characteristic defined bythe number of walks originating from the boundary of the partition. This formalizes"}, {"title": "IV Conclusion", "content": "Two pillars on which the theory of deep learning rests are generalization and ex-pressiveness. Strengthening the formal understanding of these pillars can facilitateprincipled methods for improving the efficiency, reliability, and performance of neu-ral networks. A major challenge towards doing so is finding suitable complexity measures. That is, measures with which it is possible to characterize the ability ofneural networks to generalize over natural data (e.g., images, audio, and text) andexpress rich classes of functions. This thesis puts forth notions of rank as promisingmeasures of complexity for developing a theory of deep learning.\nIn Part II, we focused on the mystery of generalization in deep learning: why doneural networks generalize despite having far more learnable parameters than training examples? Conventional wisdom suggests that this generalization stems froman implicit regularization induced by gradient-based training, i.e. its tendency to fittraining examples with predictors of minimal complexity . A widespread hope was that this tendency can be characterized as minimization of some norm . Contradicting prior belief , we proved that implicit regularization cannot be cap-tured by norms in the context of matrix factorization a model equivalent to linearneural networks. Instead, we showed that it is more faithfully described as an implicit minimization of rank. Then, capitalizing on this interpretation, we established that the tendency towards low rank extends from linear neural networks to more practical non-linear neural networks (with polynomial non-linearity), which are equivalent totensor factorizations.\nIn Part III, we employed the connection between neural networks and tensor factor-izations to study the expressiveness of graph neural networks (GNNs). Our analysischaracterized the ability of certain GNNs to model interactions between vertices via an established measure known as separation rank . In particular, it formalized intuition by which GNNs can model stronger interactions between areas of the graph that are more interconnected.\nIn terms of practical impact, based on the presented theory we developed: (i) aregularization scheme for improving the performance of convolutional neural net-works over tasks involving non-local interactions; and (ii) a state of the art edgesparsification algorithm, called Walk Index Sparsification (WIS), that preserves theability of GNNs to model interactions. Moreover, other research groups have alsobuilt upon our analyses of implicit rank minimization for designing practical deep learning systems .\nOverall, our work highlights that notions of rank may be key for explaining theremarkable performance of neural networks over natural data.\nOur theoretical analysis considered neural networks with polynomial non-linearity, by employing their connection with tensor factorizations. Such neural networkshave demonstrated competitive performance in practice , and weempirically demonstrated (in Sections 3.6 and 5.4.3) that conclusions from theiranalysis apply to neural networks with more popular non-linearities, such as ReLU.Nonetheless, we view extending our theory to account for additional non-linearitiesas a promising direction for future research. A possible approach is to build on theconnection between generalized tensor factorizations and neural networks withnon-polynomial non-linearities."}]}