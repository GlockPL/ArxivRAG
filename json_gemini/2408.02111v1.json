{"title": "Understanding Deep Learning via Notions of Rank", "authors": ["Noam Razin", "Nadav Cohen"], "abstract": "Despite the extreme popularity of deep learning in science and industry, its formal understanding is limited. Common practices are based primarily on trial-and-error and intuition, often leading to suboptimal results. As a result, there is significant interest in developing a formal theory of deep learning, with the hopes that it will shed light on empirical findings, and lead to principled methods for improving the efficiency, reliability, and performance of neural networks.\nThis thesis puts forth notions of rank as key for developing a theory of deep learning. Specifically, building on a connection between certain neural network architectures and tensor factorizations, we employ notions of rank for studying the fundamental aspects of generalization and expressiveness.\nWith regards to generalization, the mysterious ability of neural networks to generalize is widely believed to stem from an implicit regularization a tendency of gradient-based training towards predictors of low complexity, for some yet unknown measure of complexity. Through dynamical analyses, we establish an implicit regularization towards low rank in several types of neural network architectures (for corresponding notions of rank). Notably, this implicit rank minimization differs from any type of norm minimization, in contrast to prior beliefs. Implications of this finding for explaining generalization over natural data (e.g., audio, images, and text), as well as practical applications (novel regularization schemes), are presented.\nWith regards to expressiveness, we theoretically characterize the ability of graph neural networks to model interactions via separation rank a measure commonly used for quantifying entanglement in quantum physics. As a practical application of our theory, we design an edge sparsification algorithm that preserves the ability of graph neural networks to model interactions. Empirical evaluations demonstrate that it markedly outperform alternative methods.", "sections": [{"title": "I Introduction", "content": "In the past decade, deep learning has been experiencing unprecedented success, and is largely responsible for the technological breakthroughs referred to in the public as \u201cartificial intelligence\u201d (see, e.g., [128, 153, 195, 77, 31, 2]). However, despite the extreme popularity of deep learning in science and industry, its formal understanding is limited. Common practices are based primarily on trial-and-error and intuition, often leading to suboptimal results, as well as compromise in important aspects including safety and robustness [201, 144]. As a result, there is significant interest in developing a formal theory of deep learning, with the hopes that it will shed light on empirical phenomena, and lead to principled methods for improving the efficiency, reliability, and performance of neural networks.\nFrom the perspective of learning theory, understanding deep learning requires ad-dressing the fundamental questions of optimization, generalization, and expressiveness. Optimization concerns the effectiveness of gradient-based methods in minimizing neural network training objectives that are non-convex. Generalization treats the performance of a neural network beyond its training data. Lastly, expressiveness refers to the ability of practically sized neural networks to represent rich classes of functions.\nThis thesis focuses on two of the fundamental questions \u2014 generalization and expres-siveness. It puts forth notions of rank as key for developing a theory of deep learning. Our approach adopts tools from dynamical systems theory and tensor analysis, build-ing on a recent connection between certain neural network architectures and tensor factorizations [51, 48, 52, 132, 118, 119].\u00b9 The main theoretical contributions and their practical implications are summarized below.\n\u00b9For the sake of this thesis, tensors can be thought of as N-dimensional arrays, with $N \\in \\mathbb{N}$ arbitrary. For example, matrices correspond to the special case N = 2 and vectors to N = 1."}, {"title": "Generalization via Implicit Rank Minimization (Part II)", "content": "One of the central mysteries in deep learning is the ability of neural networks to generalize over natural data (e.g., audio, images, and text) when trained via gradient-based methods, despite having far more learnable parameters than training examples. This generalization takes place even in the absence of any explicit regularization [226]. Thus, conventional wisdom is that gradient-based training induces an implicit regular-ization \u2014 a tendency to fit training examples with predictors of minimal complexity, for some measure of complexity [166, 165]. The fact that natural data gives rise to generalization is accordingly understood to result from an agreement between the complexity measure implicitly minimized during training and the complexity of the data. More specifically, from the amenability of natural data to be fit with predictors of low complexity.\nMathematically formalizing the above intuition is regarded as a major open problem in the theory of deep learning. A significant challenge towards doing so is that we lack definitions for predictor complexity that are both implicitly minimized during training of neural networks and capture the essence of natural data (in the sense of natural data being fittable with low complexity). One widespread hope, initially articulated in [166], is that a characterization based on minimization of norms may apply. Namely, it is known that for linear regression gradient-based methods converge to the solution with minimal Euclidean norm (see, e.g., Section 5 in [226]), and the hope is that this result can carry over to neural networks if we"}, {"title": "Implicit Regularization in Deep Learning May Not Be Explainable by Norms", "content": "This chapter covers the results of [177].\nAs discussed in Part I, the ability of neural networks to generalize is widely believed to stem from an implicit regularization of gradient-based training towards predictors of low complexity. A prominent test-bed for studying implicit regularization in deep learning is matrix completion (cf. [81, 9]): given a randomly chosen subset of entries from an unknown matrix $W^*$, the task is to recover the unseen entries. This may be viewed as a prediction problem, where each entry in $W^*$ stands for a data point: observed entries constitute the training set, and the average reconstruction error over the unobserved entries is the test error, quantifying generalization.\nFitting the observed entries in matrix completion is obviously an underdetermined problem with multiple solutions. However, an extensive body of work (see [55] for a survey) has shown that if $W^*$ is low-rank, certain technical assumptions (e.g. \u201cin-coherence\") are satisfied and sufficiently many entries are observed, then various algorithms can achieve approximate or even exact recovery. Of these, a well-known method based upon convex optimization finds the minimal nuclear norm\u00b9 matrix among those fitting observations (see [34]).\n\u00b9The nuclear norm of a matrix is the sum of its singular values.\nOne may try to solve matrix completion using shallow neural networks. A natural approach, matrix factorization, boils down to parameterizing the solution as a product of two matrices \u2013 $W = W_2W_1$ - and optimizing the resulting (non-convex) objective for fitting observations. Formally, this can be viewed as training a depth two linear neural network. It is possible to explicitly constrain the rank of the produced solution by limiting the shared dimension of $W_1$ and $W_2$. However, [81] showed that in practice, even when the rank is unconstrained, running gradient descent with small learning rate (step size) and initialization close to the origin (zero) tends to produce low-rank solutions, and thus allows accurate recovery if $W^*$ is low-rank. Accordingly, they conjectured that the implicit regularization in matrix factorization boils down to minimization of nuclear norm:"}, {"title": "1.1 Background and Overview", "content": "As discussed in Part I, the ability of neural networks to generalize is widely believed to stem from an implicit regularization of gradient-based training towards predictors of low complexity. A prominent test-bed for studying implicit regularization in deep learning is matrix completion (cf. [81, 9]): given a randomly chosen subset of entries from an unknown matrix $W^*$, the task is to recover the unseen entries. This may be viewed as a prediction problem, where each entry in $W^*$ stands for a data point: observed entries constitute the training set, and the average reconstruction error over the unobserved entries is the test error, quantifying generalization.\nFitting the observed entries in matrix completion is obviously an underdetermined problem with multiple solutions. However, an extensive body of work (see [55] for a survey) has shown that if $W^*$ is low-rank, certain technical assumptions (e.g. \u201cin-coherence\u201d) are satisfied and sufficiently many entries are observed, then various algorithms can achieve approximate or even exact recovery. Of these, a well-known method based upon convex optimization finds the minimal nuclear norm\u00b9 matrix among those fitting observations (see [34]).\n\u00b9The nuclear norm of a matrix is the sum of its singular values.\nOne may try to solve matrix completion using shallow neural networks. A natural approach, matrix factorization, boils down to parameterizing the solution as a product of two matrices \u2013 $W = W_2W_1$ - and optimizing the resulting (non-convex) objective for fitting observations. Formally, this can be viewed as training a depth two linear neural network. It is possible to explicitly constrain the rank of the produced solution by limiting the shared dimension of $W_1$ and $W_2$. However, [81] showed that in practice, even when the rank is unconstrained, running gradient descent with small learning rate (step size) and initialization close to the origin (zero) tends to produce low-rank solutions, and thus allows accurate recovery if $W^*$ is low-rank. Accordingly, they conjectured that the implicit regularization in matrix factorization boils down to minimization of nuclear norm:"}, {"title": "1.2 Deep Matrix Factorization", "content": "Suppose we would like to complete a D-by-D' matrix based on a set of observations {${y_{i,j} \\in \\mathbb{R}}\\}_{(i,j)\\in\\Omega}$, where $\\Omega \\subset \\{1,2,..., D\\} \\times \\{1,2,..., D'\\}$. A standard (underdeter-mined) loss function for the task is:\n$\\mathcal{L}_M : \\mathbb{R}^{D \\times D'} \\rightarrow \\mathbb{R}_{>0} \\quad \\mathcal{L}_M(W) = \\frac{1}{2} \\sum_{(i,j) \\in \\Omega} ((W)_{ij} - y_{i,j})^2$.                                                 (1.1)\nEmploying a depth L matrix factorization, with hidden dimensions $D_1, \\dots , D_{L-1} \\in \\mathbb{N}$, amounts to optimizing the overparameterized objective:\n$\\Phi_M(W_1,..., W_L) := \\mathcal{L}_M(W_M) = \\frac{1}{2} \\sum_{(i,j) \\in \\Omega} ((W_M)_{i,j} - y_{i,j})^2$,                                                                             (1.2)\nwhere $W_l \\in \\mathbb{R}^{D_l \\times D_{l-1}}, l = 1, ..., L$, with $D_L := D, D_0 := D'$, and:\n$W_M := W_L \\dots W_1$,                                                                                                               (1.3)\nreferred to as the end matrix of the factorization. Our interest lies on the implicit regu-larization of gradient descent, i.e. on the type of end matrices (Equation (1.3)) it will find when applied to the overparameterized objective (Equation (1.2)). Accordingly, and in line with prior work (cf. [81, 9]), we focus on the case in which the search space is unconstrained, meaning $\\min\\{D_l\\}_{l=0}^L = \\min\\{D_0, D_L\\}$ (rank is not limited by the parameterization).\nAs a theoretical surrogate for gradient descent with small learning rate and near-zero initialization, similarly to [81] and [9] (as well as other works analyzing linear neural networks, e.g. [188, 7, 129, 8]), we study gradient flow (gradient descent with infinitesimally small learning rate):\u00b3\n$\\dot{W}_l(t) := \\frac{d}{dt} W_l(t) = - \\frac{\\partial}{\\partial W_l} \\Phi_M(W_1(t),..., W_L(t)), \\quad t \\geq 0, l = 1,...,L$,                                                                             (1.4)\nand assume balancedness at initialization, i.e.:\n$W_{l+1}(0)^T W_{l+1}(0) = W_l(0) W_l(0)^T, \\quad l = 1,..., L - 1$.                                                                    (1.5)\nIn particular, when considering random initialization, we assume that $\\{W_l(0)\\}_{l=1}^L$ are drawn from a joint probability distribution by which Equation (1.5) holds al-most surely. This is an idealization of standard random near-zero initializations, e.g. Xavier [75] and He [93], by which Equation (1.5) holds approximately with high\n\u00b3A technical subtlety of optimization in continuous time is that in principle, it is possible to asymptote (diverge to infinity) after finite time. In such a case, the asymptote is regarded as the end of optimization, and time tending to infinity (t \u2192 \u221e) is to be interpreted as tending towards that point."}, {"title": "1.3 Implicit Regularization Can Drive All Norms to Infinity", "content": "In this section we prove that for matrix factorization of depth $L > 2$, there exist observations $\\{y_{i,j}\\}_{(i,j)\\in\\Omega}$ with which optimizing the overparameterized objective (Equation (1.2)) via gradient flow (Equations (1.4) and (1.5)) leads \u2014 with probabil-ity 0.5 or more over random (\u201csymmetric\u201d) initialization \u2014 all norms and quasi-norms of the end matrix (Equation (1.3)) to grow towards infinity, while its rank essentially decreases towards minimum. By this we not only affirm Conjecture 2, but in fact go beyond it in the following sense: (i) the conjecture allows chosen observations to depend on the norm or quasi-norm under consideration, while we show that the same set of observations can apply jointly to all norms and quasi-norms; and (ii) the conjecture requires norms and quasi-norms to be larger than minimal, while we establish growth towards infinity.\nFor simplicity of presentation, the current section delivers our construction and analysis in the setting $D = D' = 2$ (i.e. 2-by-2 matrix completion) \u2014 extension to different dimensions is straightforward (see Appendix A.1). We begin (Section 1.3.1)"}, {"title": "1.3.1 A Simple Matrix Completion Problem", "content": "Consider the problem of completing a 2-by-2 matrix based on the following observa-tions:\n$\\Omega = \\{(1,2), (2, 1), (2, 2)\\} \\quad y_{1,2} = 1, y_{2,1} = 1, y_{2,2} = 0$.                    (1.6)\nThe solution set for this problem (i.e. the set of matrices obtaining zero loss) is:\n$\\mathcal{S} = \\{W \\in \\mathbb{R}^{2\\times 2} : (W)_{1,2} = 1, (W)_{2,1} = 1, (W)_{2,2} = 0\\} .               (1.7)$\nProposition 1 below states that minimizing a norm or quasi-norm along $W \\in \\mathcal{S}$ requires confining $(W)_{1,1}$ to a bounded interval, which for Schatten-p (quasi-)norms (in particular for nuclear, Frobenius, and spectral norms)\u2075 is simply the singleton $\\{0\\}$.\n\u2075For $p\\in (0,\\infty]$, the Schatten-p (quasi-)norm of a matrix $W \\in \\mathbb{R}^{D\\times D'}$ with singular values $\\{o(W)\\}^{\\min(D,D')}_{i=1}$ is defined as $(\\sum^{\\min\\{D,D'\\}}_{i=1}(o(W))^{p})^{\\frac{1}{p}}$ if $p<\\infty$ and as $\\max\\{o(W)\\}^{\\min\\{D,D'\\}}_{i=1}$ if $p = \\infty$. It is a norm if $p>1$ and a quasi-norm if $p < 1$. Notable special cases are nuclear (trace), Frobenius and spectral norms, corresponding to $p=1,2$, and $\\infty$, respectively.\nProposition 1. For any norm or quasi-norm over matrices $|| \\cdot ||$ and any $e > 0$, there exists a bounded interval $I_{||.||,e} \\subset \\mathbb{R}$ such that if $W \\in \\mathcal{S}$ is an $\\epsilon$-minimizer of $||\\cdot||$ (i.e. $||W|| \\leq \\inf_{W'\\in\\mathcal{S}}||W'|| + \\epsilon$) then necessarily $(W)_{1,1} \\in I_{||.||,\\epsilon}$. If $||\\cdot||$ is a Schatten-p (quasi-)norm, then in addition $W \\in \\mathcal{S}$ minimizes $||\\cdot||$ (i.e. $||W|| = \\inf_{W'\\in\\mathcal{S}}||W'||$) if and only if $(W)_{1,1} = 0$.\nProof sketch (proof in Appendix A.3.3). The (weakened) triangle inequality allows lower bounding $||\\cdot||$ by $|(W)_{1,1}|$ (up to multiplicative and additive constants). Thus, the set of $(W)_{1,1}$ values corresponding to $\\epsilon$-minimizers must be bounded. If $||\\cdot||$ is a Schatten-p (quasi-)norm, a straightforward analysis shows it is monotonically increasing with respect to $|(W)_{1,1}|$, implying it is minimized if and only if $(W)_{1,1} = 0$.\nIn addition to norms and quasi-norms, we are also interested in the evolution of rank throughout optimization of a deep matrix factorization. More specifically, we are interested in the prospect of rank being implicitly minimized, as demonstrated em-pirically in [81, 9]. The discrete nature of rank renders its direct analysis unfavorable from a dynamical perspective (the rank of a matrix implies little about its proximity to low-rank), thus we consider the following surrogate measures: (i) effective rank (Definition 1 below; from [185]) a continuous extension of rank used for numerical analyses; and (ii) distance from infimal rank (Definition 2 below) \u2014 (Frobenius) distance from the minimal rank that a given set of matrices may approach.\nAccording to Proposition 2 below, these measures independently imply that, although all solutions to our matrix completion problem i.e. all $W \\in \\mathcal{S}$ (see Equation (1.7)) have rank 2, it is possible to essentially minimize the rank to 1 by taking $|(W)_{1,1}| \\rightarrow \\infty$. Recalling Proposition 1, we conclude that in our setting, there is a direct contradiction between minimizing norms or quasi-norms and minimizing rank the former re-quires confinement to some bounded interval, whereas the latter demands divergence towards infinity. This is the critical feature of our construction, allowing us to deem"}, {"title": "1.3.2 Decreasing Loss Increases Norms", "content": "Consider the process of solving our matrix completion problem (Section 1.3.1) with gradient flow over a depth $L > 2$ matrix factorization (Section 1.2). Theorem 1 below states that if the end matrix (Equation (1.3)) has positive determinant at initialization, lowering the loss leads norms and quasi-norms to increase, while the rank essentially decreases.\nTheorem 1. Suppose we complete the observations in Equation (1.6) by employing a depth $L>2$ matrix factorization, i.e. by minimizing the overparameterized objective (Equa-tion (1.2)) via gradient flow (Equations (1.4) and (1.5)). Denote by $W_M(t)$ the end matrix (Equation (1.3)) at time t > 0 of optimization, and by $\\mathcal{L}_M(t) := \\mathcal{L}_M(W_M(t))$ the corre-sponding loss (Equation (1.1)). Assume that $det(W_M(0)) > 0$. Then, for any norm or quasi-norm over matrices $|| \\cdot ||$:\n$||W_M(t) || \\geq a_{||.||} \\sqrt{\\frac{1}{L_M(t)}} - b_{||.||}, \\quad t \\geq 0$,                                                                                    (1.8)\nwhere $b_{||.||} := \\max\\{\\sqrt{2}a_{||.\\cdot||}, 8c_{\\gamma.||} \\max_{i,j\\in \\{1,2\\}} ||e_ie_j^T|| \\}$, $a_{||.\\cdot||} := ||e_1e_1^T|| / (\\sqrt{2}c_{||.\\cdot||})$, the vectors $e_1, e_2 \\in \\mathbb{R}^2$ form the standard basis, and $c_{||.||} \\geq 1$ is a constant with which $||\\cdot||$"}, {"title": "Implicit Regularization Can Drive All Norms to Infinity", "content": "In this section we prove that for matrix factorization of depth L > 2, there exist observations $\\{y_{i,j}\\}_{(i,j)\\in\\Omega}$ with which optimizing the overparameterized objective (Equation (1.2)) via gradient flow (Equations (1.4) and (1.5)) leads \u2014 with probabil-ity 0.5 or more over random (\u201csymmetric\u201d) initialization \u2014 all norms and quasi-norms of the end matrix (Equation (1.3)) to grow towards infinity, while its rank essentially decreases towards minimum. By this we not only affirm Conjecture 2, but in fact go beyond it in the following sense: (i) the conjecture allows chosen observations to depend on the norm or quasi-norm under consideration, while we show that the same set of observations can apply jointly to all norms and quasi-norms; and (ii) the conjecture requires norms and quasi-norms to be larger than minimal, while we establish growth towards infinity.\nFor simplicity of presentation, the current section delivers our construction and analysis in the setting $D = D' = 2$ (i.e. 2-by-2 matrix completion) \u2014 extension to different dimensions is straightforward (see Appendix A.1). We begin (Section 1.3.1)"}, {"title": "Decreasing Loss Increases Norms", "content": "Consider the process of solving our matrix completion problem (Section 1.3.1) with gradient flow over a depth $L > 2$ matrix factorization (Section 1.2). Theorem 1 below states that if the end matrix (Equation (1.3)) has positive determinant at initialization, lowering the loss leads norms and quasi-norms to increase, while the rank essentially decreases.\nTheorem 1. Suppose we complete the observations in Equation (1.6) by employing a depth $L > 2$ matrix factorization, i.e. by minimizing the overparameterized objective (Equa-tion (1.2)) via gradient flow (Equations (1.4) and (1.5)). Denote by $W_M(t)$ the end matrix (Equation (1.3)) at time t > 0 of optimization, and by $L_M(t) := L_M(W_M(t))$ the corre-sponding loss (Equation (1.1)). Assume that $det(W_M(0)) > 0$. Then, for any norm or quasi-norm over matrices $|| \\cdot ||$:\n$||W_M(t) || \\geq a_{||.||} \\sqrt{\\frac{1}{L_M(t)}} - b_{||.||}, \\quad t \\geq 0$,                                                                                    (1.8)\nwhere $b_{||.||} := \\max\\{\\sqrt{2}a_{||.\\cdot||}, 8c_{\\gamma.||} \\max_{i,j\\in \\{1,2\\}} ||e_ie_j^T|| \\}$, $a_{||.\\cdot||} := ||e_1e_1^T|| / (\\sqrt{2}c_{||.\\cdot||})$, the vectors $e_1, e_2 \\in \\mathbb{R}^2$ form the standard basis, and $c_{||.||} \\geq 1$ is a constant with which $||\\cdot||$"}, {"title": "Robustness to Perturbations", "content": "Our analysis (Section 1.3.2) has shown that when applying a deep matrix factorization (Section 1.2) to the matrix completion problem defined in Section 1.3.1, if the end matrix (Equation (1.3)) has positive determinant at initialization a condition that holds with probability 0.5 under the wide variety of random distributions specified by Proposition 3 \u2014 then the implicit regularization drives all norms and quasi-norms towards infinity, while rank is essentially driven towards its minimum. A natural question is how common this phenomenon is, and in particular, to what extent does it persist if the observed entries we defined (Equation (1.6)) are perturbed."}, {"title": "Implicit Tensor Rank Minimization", "content": "In this section we employ the dynamical characterization derived in Section 2.3 to theoretically establish implicit regularization towards low tensor rank. Specifically, we prove that under certain technical conditions, arbitrarily small initialization leads tensor factorization to follow a trajectory of rank one tensors for an arbitrary amount of time or distance. As a corollary, we obtain that if the tensor completion problem admits a rank one solution, and all rank one trajectories uniformly converge to it, tensor factorization with infinitesimal initialization will converge to it as well. Our analysis generalizes to tensor factorization recent results developed in [141] for matrix factorization. As typical in transitioning from matrices to tensors, this generalization entails significant challenges necessitating use of fundamentally different techniques.\nFor technical reasons, our focus in this section lies on the Huber loss from robust statistics [101], given by:\n$l_h: \\mathbb{R} \\rightarrow \\mathbb{R}_{>0} \\quad l_h(z):= \\begin{cases} \\frac{1}{2}z^2 &, \\quad z < \\delta_h \\\\ \\delta_h(|z|-\\frac{1}{2} \\delta), \\quad \\text{otherwise} \\end{cases}$,                                                                                       (2.8)"}, {"title": "2.2 Tensor Factorization", "content": "Consider the task of completing an N-dimensional tensor (N > 3) with axis lengths $D_1,..., D_N \\in \\mathbb{N}$, or, in standard tensor analysis terminology, an order N tensor with modes of dimensions $D_1, ..., D_N$. Given a set of observations $\\{y_{i_1,...,i_N} \\in\\mathbb{R}\\}_{(i_1,...,i_N)\\in\\Omega}$, where $\\Omega$ is a subset of all possible index tuples, a standard (undetermined) loss function for the task is:\n$\\mathcal{L}_T: \\mathbb{R}^{D_1\\times...\\times D_N} \\rightarrow \\mathbb{R}_{\\geq 0} \\quad \\mathcal{L}_T(W) = \\frac{1}{|\\Omega|} \\sum_{(i_1,...,i_N)\\in\\Omega} l ( (W)_{i_1,...,i_N} - y_{i_1,...,i_N})^2,                                          (2.1)$\nwhere $l : \\mathbb{R} \\rightarrow \\mathbb{R}_{>0}$ is differentiable and locally smooth. A typical choice for $l(\\cdot)$ is $l(z) = z^2$, corresponding to $l_2$ loss. Other options are also common, for example the Huber loss from robust statistics [101] \u2014 a differentiable surrogate for $l_1$ loss."}, {"title": "Interpretation as Neural Network", "content": "Tensor completion can be viewed as a prediction problem, where each mode corre-sponds to a discrete input variable. For an unknown tensor $W^* \\in \\mathbb{R}^{D_1\\times...\\times D_N}$, inputs are index tuples of the form $(i_1, . . ., i_N)$, and the label associated with such an input is $(W^*)_{i,1,...,i_N}$. Under this perspective, the training set consists of the observed entries, and the average reconstruction error over unseen entries measures test error. The standard case, in which observations are drawn uniformly across the tensor and reconstruction error weighs all entries equally, corresponds to a data distribution that is uniform, but other distributions are also viable.\nConsider for example the task of predicting a continuous label for a 100-by-100 binary image. This can be formulated as an order 10000 tensor completion problem, where all modes are of dimension 2. Each input image corresponds to a location (entry) in the tensor $W^*$, holding its continuous label. As image pixels are (typically) not distributed independently and uniformly, locations in the tensor are not drawn uniformly when observations are generated, and are not weighted equally when reconstruction error is computed. See Figure 2.1 for further illustration of how a general prediction task (with discrete inputs and scalar output) can be formulated as a tensor completion problem."}, {"title": "2.3 Dynamical Characterization", "content": "In this section we derive a dynamical characterization for the norms of individual components in the tensor factorization. The characterization implies that with small learning rate and near-zero initialization, components tend to be learned incremen-tally, giving rise to a bias towards low tensor rank solutions. This finding is used in Section 2.4 to prove (under certain conditions) implicit tensor rank minimization, and is demonstrated empirically in Section 2.5.\u00b2\n\u00b2We note that all results in this section apply even if the tensor completion loss $\\mathcal{L}_T(\\cdot)$ (Equation (2.1)) is replaced by any differentiable and locally smooth function. The proofs in Appendix B.3 already account for this more general setting.\nFor the rest of the chapter, unless specified otherwise, when referring to a norm we mean the standard Frobenius (Euclidean) norm, denoted by $||\\cdot||$.\nThe following lemma establishes an invariant of the dynamics, showing that the differences between squared norms of vectors in the same component are constant"}, {"title": "Tensor Rank as Measure of Complexity", "content": "Implicit regularization in deep learning is typically viewed as a tendency of gradient-based optimization to fit training examples with predictors whose \u201ccomplexity\u201d is as low as possible. The fact that \u201cnatural\u201d data gives rise to generalization while other types of data (e.g. random) do not, is understood to result from the former being amenable to fitting by predictors of lower complexity. A major challenge in formalizing this intuition is that we lack definitions for predictor complexity that are both quantitative (i.e. admit quantitative generalization bounds) and capture the essence of natural data (types of data on which neural networks generalize in practice), in the sense of it being fittable with low complexity.\nAs discussed in Section 2.2.1, learning a predictor with multiple discrete input variables and a continuous output can be viewed as a tensor completion prob-lem. Specifically, with $N \\in \\mathbb{N}, D_1, ..., D_N \\in \\mathbb{N}$, learning a predictor from domain $X = \\{1, ..., D_1\\} \\times \\cdots \\times \\{1, ..., D_N \\}$ to range $Y = \\mathbb{R}$ corresponds to completion of an order N tensor with mode (axis) dimensions $D_1, . . ., D_N$. Under this correspon-dence, any predictor can simply be thought of as a tensor, and vice versa. We have shown that solving tensor completion via tensor factorization amounts to learning a predictor through a certain neural network (Section 2.2.1), whose implicit regulariza-tion favors solutions with low tensor rank (Sections 2.3 and 2.4). Motivated by these connections, the current subsection empirically explores tensor rank as a measure of complexity for predictors, by evaluating the extent to which it captures natural data, i.e. allows the latter to be fit with low complexity predictors.\nAs representatives of natural data, we chose the classic MNIST dataset [130] \u2014 per-haps the most common benchmark for demonstrating ideas in deep learning \u2014 and its more modern counterpart Fashion-MNIST [220]. A hurdle posed by these datasets is that they involve classification into multiple categories, whereas the equivalence to tensors applies to predictors whose output is a scalar. It is possible to extend the equivalence by equating a multi-output predictor with multiple tensors, in which case the predictor is associated with multiple tensor ranks. However, to facilitate a simple presentation, we avoid this extension and simply map each dataset into multiple one-vs-all binary classification problems. For each problem, we associate the label 1 with the active category and 0 with all the rest, and then attempt to fit training examples with predictors of low tensor rank, reporting the resulting mean squared error, i.e. the residual of the fit. This is compared against residuals obtained when fitting two types of random data: one generated via shuffling labels, and the other by replacing inputs with noise."}, {"title": "Implicit Regularization in Hierarchical Tensor Factorization and Deep Convolutional Neural Networks", "content": "The contents of this chapter are based on [179].\nChapters 1 and 2 focused on the implicit regularization in matrix and tensor factor-ization. Matrix factorization refers to minimizing a given loss (over matrices) by parameterizing the solution as a product of matrices, and optimizing the resulting ob-jective via gradient descent. Tensor factorization is a generalization of this procedure to multi-dimensional arrays. There, a tensor is learned through gradient descent over a sum-of-outer-products parameterization (see Section 2.2). By adopting a dynamical viewpoint, in Chapter 2 we established that gradient descent (with small learning rate and near-zero initialization) over tensor factorization induces a momentum-like effect on the components of the factorization, leading them to move slowly when small and quickly when large. This implies a form of incremental learning that results in low tensor rank solutions, analogous to the incremental rank learning phenomenon identified by [9] for matrix factorization.\nFrom a deep learning perspective, matrix factorization can be seen as a linear neural network, and, in a similar vein, tensor factorization corresponds to a certain shallow (depth two) non-linear convolutional neural network (see Section 2.2.1). As theoretical surrogates for deep learning, the practical relevance of these models is limited. The former lacks non-linearity, while the latter misses depth both crucial features of modern neural networks. A natural extension of matrix and tensor factorizations that accounts for both non-linearity and depth is hierarchical tensor factorization,\u00b9 which corresponds to a class of deep non-linear convolutional neural networks [51] (with polynomial non-linearity) that have demonstrated promising performance in practice [47, 50, 193, 200, 78, 63], and have been key to the study of expressiveness in deep learning [51, 48, 49, 52, 53, 192, 132, 133, 16, 118, 119, 134].\n\u00b9The term \u201chierarchical tensor factorization\u201d refers throughout to a variant of the Hierarchical Tucker factorization [86], presented in Section 3.3."}, {"title": "Preliminaries: Matrix and Tensor Factorizations", "content": "The dynamical analysis delivered in the current chapter for hierarchical tensor factor-ization is analogous to prior dynamical analyses for matrix and tensor factorization,\nThroughout the chapter, when referring to a norm we mean the standard Frobenius (Euclidean) norm, denoted by $||\\cdot||$.\nThe following lemma establishes an invariant of the dynamics, showing that the differences between squared norms of vectors in the same component are constant"}, {"title": "Hierarchical Tensor Factorization"}]}