{"title": "Segmenting Text and Learning Their Rewards for Improved RLHF in Language Model", "authors": ["Yueqin Yin", "Shentao Yang", "Yujia Xie", "Ziyi Yang", "Yuting Sun", "Hany Awadalla", "Weizhu Chen", "Mingyuan Zhou"], "abstract": "Reinforcement learning from human feedback (RLHF) has been widely adopted to align language models (LMs) with human preference. Prior RLHF works typically take a bandit formulation, which, though intuitive, ignores the sequential nature of LM generation and can suffer from the sparse reward issue. While recent works propose dense token-level RLHF, treating each token as an action may be oversubtle to proper reward assignment. In this paper, we seek to get the best of both by training and utilizing a segment-level reward model, which assigns a reward to each semantically complete text segment that spans over a short sequence of tokens. For reward learning, our method allows dynamic text segmentation and compatibility with standard sequence-preference datasets. For effective RL-based LM training against segment reward, we generalize the classical scalar bandit reward normalizers into location-aware normalizer functions and interpolate the segment reward for further densification. With these designs, our method performs competitively on three popular RLHF benchmarks for LM policy: Alpaca Eval 2.0, Arena-Hard, and MT-Bench. Ablation studies are conducted to further demonstrate our method.", "sections": [{"title": "Introduction", "content": "To align language models (LMs, e.g., OpenAI, 2023; Reid et al., 2024) with human values, reinforcement learning (RL, Sutton and Barto, 2018) methods have been widely adopted to optimize the non-differentiable human preference, leading to the paradigm of reinforcement learning from human feedback (RLHF, Ouyang et al., 2022; Bai et al., 2022b). A prevailing approach in RLHF is to optimize the LMs by proximal policy optimization (PPO, Schulman et al., 2017) against a bandit reward model learned from human preference data, with KL regularization towards a pre-specified target distribution to avoid over-optimization on the reward model (Ziegler et al., 2019; Stiennon et al., 2020; Castricato et al., 2022). While this bandit approach is easier for reward modeling and has achieved remarkable success, language generation is intrinsically sequential, rather than simultaneous. Thus, from the view of optimizing human preference, assigning a bandit reward to entire text sequence induces the sparse reward (delayed feedback) issue (Andrychowicz et al., 2017; Marbach and Tsitsiklis, 2003), that often hurts RL-based LM training by increasing gradient variance and lowering sample efficiency (Takanobu et al., 2019; Wang et al., 2020; Guo et al., 2022; Snell et al., 2022).\nAs efforts to mitigate this sparse reward issue, prior works have developed methods to \"ground\" the sequence-level preference label into a dense token-level reward model (Yang et al., 2023; Zhong et al., 2024). While a dense per-token reward signal reduces the optimization complexity (Laidlaw et al., 2023), each action, however, is then defined as a single token, i.e., a sub-word that is finer-grained than a word, especially with the BPE-style tokenizers (Gage, 1994; Sennrich et al., 2016). For instance, Llama 3.1's tokenizer (Dubey et al., 2024) has tokens as {Brit, ce, cod, neo, redd,...} that have less clear semantic meaning per se in any given context. The contribution of those tokens to the text sequence will inevitably depend on later tokens, making reward/credit assignment harder, especially under the prevailing RLHF paradigm of implementing the reward model as an off-the-shelf decoder-only transformer (e.g., Ouyang et al., 2022; Bai et al., 2022b; Menick et al., 2022). Further, token-level reward implicitly assumes that the basic unit of a text sequence is token, which may not follow linguistics, where a more meaningful decomposition of text may be phrase (including word) that can be more semantically complete and generally consists of a short sequence of tokens.\nTo retain the optimization benefit of dense reward for RLHF, while mitigating its potential reward assignment issue and linguistic counter-intuition, in this paper, we seek to train and utilize a segment-level reward model, which assigns a reward to each semantically meaningful segment of text sequence that constitutes a small number of (or just one) tokens. With this construction, we define the action space in RLHF as \"text segment,\" interpolating between the finest \"per token\" and the coarsest \"full sequence\" and potentially getting the benefit of both worlds: easier RL-based LM training owing to denser feedback and more accurate training guidance by the semantic completeness of each action.\nTechnically, we are motivated by prior works (Malinin and Gales, 2018; Li et al., 2024a) to implement a dynamic text sequence segmentation by thresholding the entropy of LM's predictive distributions, under the assumption that tokens within a semantically complete text segment can be more certainly predicted by prior tokens, while the start of a new segment is not (Wang et al., 2024b). To allow training the segment-level reward model by the standard sequence-preference labels via Bradley-Terry (BT, Bradley and Terry, 1952) loss, we differentiably aggregate segment rewards in a text sequence into a parametrized sequence evaluation. The learned segment-level reward model is then utilized in PPO-based policy learning, where we observe the unsuitability of classical reward normalizers, i.e., the mean and standard deviation (std) of full sequence rewards. We address this issue by generalizing the classical bandit normalizers of scalar mean and std into a mean and a std function that output the reward normalizers at arbitrary locations of the text sequence. In addition, we enhance PPO training by within-segment reward interpolation, which further densifies training signal and improves results.\nWe test our method on the performance of the PPO-trained LM policy. On three popular RLHF benchmarks for LM policy: AlpacaEval 2.0, Arena-Hard, and MT-Bench, our method achieves competitive performance gain against both the classical bandit reward approach and recent token-level reward approach. We conduct a wide array of ablation studies to verify our design choices and provide further insight into our method."}, {"title": "Main Method", "content": ""}, {"title": "Notations and Background", "content": "In this section, we will define generic notations, provide background on the classical bandit RLHF, and then discuss RL formulation of LM generation underlying recent efforts on dense-reward RLHF.\nGeneric Notations. Both reward modeling and policy learning require text prompt x and the corresponding response y. Reward model training turns the supervised fine-tuned model \\(\\pi_{\\varsigma FT}(\\cdot|\\cdot)\\) (without the final unembedding layer) into a parametrized scalar-output model \\(r_{\\phi}(\\cdot,\\cdot)\\) with parameter \\(\\phi\\) that scores its input. The LM policy \\(\\pi_{\\theta}\\), parametrized by \\(\\theta\\), is then optimized against \\(r_{\\phi}\\).\nBandit Reward Model Training. Reward model training assumes a dataset \\(D_{pref} = \\{(x, y^w, y^l)\\}\\)"}, {"title": "PPO-based Bandit Policy Learning", "content": "In policy learning, a set \\(D_{pol} = \\{x\\}\\) of text prompts x is given. The LM policy \\(\\pi_{\\theta}\\) is trained to generate outputs on \\(D_{pol}\\) optimizing the bandit reward from \\(r_{\\phi}\\), with a KL penalty towards \\(\\pi_{\\varsigma FT}\\) to avoid reward over-optimization. Collectively, the objective is\n\\(\\max_\\theta E_{x \\sim D_{pol}, y \\sim \\pi_\\theta(\\cdot|x)} [r_{\\phi}(x,y) - \\beta \\times log (\\pi_\\theta(y|x)/\\pi_{\\varsigma FT}(y|x))]\\),\nwhere \\(\\beta\\) is the KL coefficient. In practice, for PPO's training stability, the value of \\(r_{\\phi}(x, y)\\) is de-mean and de-std normalized based on statistics calculated on a calibration dataset, e.g., \\(D_{pref}\\).\nRL Formulation of LM Generation. By its sequential nature, LM generation is formulated as a Markov Decision Process (MDP) \\(M = (S, A, P, R, \\gamma)\\) (Sutton and Barto, 2018). Concretely, for state space S, the state at timestep t, \\(s_t\\), consists of the prompt x and all generated tokens so far \\(a_{<t} =: [a_0,..., a_{t-1}]\\) with \\(a_{<0} =: \\emptyset\\), i.e., \\(s_t =: [x, a_{<t}]\\). A is the action space, where the action at at step t is a short-sequence/segment of tokens from the vocabulary in our segment-level reward/policy setting, whereas \\(a_t\\) is a single token in the token-level reward/policy setting. Transition function P deterministically appends the newly sampled tokens after the previous ones, i.e., \\(s_{t+1} = [s_t, a_t] = [x, a_{<t}]\\). \\(r(s,a) : S \\times A \\rightarrow R\\) scores the action choice (segment/token selection) a at state/context s and is typically substituted by the learned reward model \\(r_{\\phi}\\). \\(\\gamma \\in [0, 1]\\) is the discount factor.\nIn what follows, we will focus on our segment-level reward/policy setting where each action \\(a_t \\in A\\) is a semantically complete text segment, consisting of a non-deterministic number of consecutive tokens. The response y for prompt x then contains a variable number of segments/actions, generically denoted as \\(y = [a_0,........, a_{T-1}]\\) where T is the number of segments in y and varies across responses. When necessary, we denote a single token in y as \\(y_i\\) whose generation context is \\([x, y_{<i}]\\)."}, {"title": "Reward Model Training", "content": ""}, {"title": "Overview", "content": "In training our segment-level reward model, we follow the data assumption set forth in Section 2.1, where the dataset \\(D_{pref} = \\{(x, y^w, y^l)\\}\\) contains only binary sequence-level preference labels, without any process supervision (Uesato et al., 2022). The reward model \\(r_{\\phi}(s_t, a_t)\\) is configured to output a scalar reward for each text segment choice \\(a_t\\) the generation context \\(s_t\\). \\(r_{\\phi}\\) is trained such that its induced parameterized text sequence evaluations, aggregated from all segment-level rewards in the respective sequence, align with the preference labels in \\(D_{pref}\\). This is inspired by the imitation learning literature (e.g., Christiano et al., 2017; Brown et al., 2019, 2020) and prior token-level reward modeling in RLHF (Yang et al., 2023). Collectively, the BT loss for training our segment-level reward function \\(r_{\\phi}\\) is\n\\(L_{seg}(\\phi) = -E_{(x,y^w,y^l) \\sim D_{pref}} [log\\sigma (e_{\\phi}(x,y^w) - e_{\\phi}(x,y^l))], \\forall y \\in \\{y^w, y^l\\}, e_{\\phi}(x,y) = f(\\{r_{\\phi}(s_t,a_t)\\}a_t \\in y),\\)\nwhere \\(e_{\\phi}\\) denotes the parameterized sequence evaluation induced by \\(r_{\\phi}\\), constructed by aggregating all segment-level rewards \\(\\{r_{\\phi}(s_t, a_t)\\}a_t \\in y\\) in the text sequence y by a selected aggregation function \\(f(\\cdot)\\). Below, we discuss in detail the segmentation method for text sequence and the choice of \\(f(\\cdot)\\).\nEntropy-based Segmentation. As discussed in Section 1, we intend to split the given text sequence \\(y \\in \\{y^w, y^l\\}\\) into semantically complete segments, so that the reward assignment to each action (segment) can be easier, especially under the common implementation of the reward model as an auto-regressive LM. Recent works on LMs (e.g., Li et al., 2024a; Wang et al., 2024b) have noticed that tokens within a semantically complete segment can be more predictable by the corresponding generation context, since they are continuation of the designated semantics; while the starting token of a new segment is comparably less predictable, as its semantic binding with prior words is relatively weaker. For auto-regressive LMs, the predictability of each token can be conveniently measured by the entropy of the next-token-prediction distribution from which the token is sampled (Malinin and Gales, 2018). To make text sequence segmentation a one-time data pre-processing in reward model training, we choose to use the prediction distribution from the supervised fine-tuned model SFT, from which the reward model is initialized before training. With a selected entropy cutoff \\(C_{ent}\\), token \\(y_i\\) starts a new segment if the Shannon entropy \\(H(\\cdot)\\) of \\(\\pi_{\\varsigma FT}\\)'s predictive distribution of the i-th token surpasses \\(C_{ent}\\), i.e., \\(H(\\pi_{\\varsigma FT}(\\cdot|x, y_{<i})) > C_{ent}\\), in which case \\(y_{i-1}\\) ends the previous segment.\nChoice of the Aggregation Function \\(f(\\cdot)\\). Aggregation function \\(f(\\cdot)\\) provides inductive bias on the relation between the quality of each segment/action and the preferability of overall text sequence. Since f"}, {"title": "An Alternative Interpretation", "content": "Comparing our segment-level reward training loss Eq. (3) with the classical bandit loss Eq. (1), one may alternatively interpret \\(e_{\\phi}\\) and \\(f(\\{r_{\\phi}\\})\\) in Eq. (3) as a re-parametrization of the learned sequence-level feedback that differentiably aggregates the quality/contribution of each text segment, and thereby connects a denser evaluation \\(r_{\\phi}\\) of each semantically complete text segment with the information in ground-truth sequence-level preference label."}, {"title": "PPO-based Policy Learning", "content": ""}, {"title": "Overview", "content": "In policy learning, we again follow the classical bandit setting in Section 2.1 to optimize the LM policy \\(\\pi_{\\theta}\\) on a given prompt set \\(D_{pol} = \\{x\\}\\). But unlike the bandit objective in Eq. (2), we adopt the full RL setting (Sutton and Barto, 2018) to maximize \\(\\pi_{\\theta}\\)'s expected sum of per-segment/step rewards. This enables directly plugging our segment-level reward model \\(r_{\\phi}\\) into most off-the-shelf RLHF PPO implementation. With this, the policy learning objective for \\(\\pi_{\\theta}\\) is\n\\(\\max_{\\theta} E_{x \\sim D_{pol}, y \\sim \\prod_{t=0}^{T-1} \\pi_\\theta (a_t | s_t)} [\\sum_{t=0}^{T-1}r_{\\phi} (s_t, a_t) - \\beta \\times log (\\pi_\\theta(y|x)/\\pi_{\\varsigma FT}(y|x))]\\),\nwhere again, each \\(a_t\\) is a segment of tokens (chopped by SFT), \\(s_t = [x, a_0,... a_{t-1}]\\) is the generation context at step t, and \\(y = [a_0, ......., a_{T-1}]\\) is the response to prompt x sampled from the learning \\(\\pi_{\\theta}\\).\nRecall from Section 2.1 that the output values from the reward model \\(r_{\\phi}\\) need to be normalized for the stability of PPO training. With our segment-level reward model, it is no longer suitable to normalize each \\(r_{\\phi}(s_t, a_t)\\) by the mean and std of entire sequences' rewards as in the bandit setting, since the latter may not be on a proper scale and/or well-defined. Further, the on-policy nature of PPO induces an extra complexity: each step of PPO samples new text sequences, whose total length, segment lengths, and segment locations are all stochastic and can differ from the reward calibration dataset, e.g., \\(D_{pref}\\). Appendix E provides an extended discussion on reward normalization in PPO-based LM training. Below, we discuss our approach to construct the reward value normalizers, followed by interpolating the segment-level reward into per-token signal to helpfully provide an even denser training guidance.\nLocation-aware Reward Normalizers via Regression. While the length of the sampled response y and the lengths and locations of segments \\(\\{a_t\\}\\) in y are all stochastic, we know that each \\(a_t\\) is somewhere in y. Correspondingly, each input \\((s_t, a_t)\\) to \\(r_{\\phi}\\) is linked to a normalized location \\(p \\in (0, 1]\\) of y, and p can be simply defined as t/T, where t is the index of the segment \\(a_t\\) in y, since PPO routine has fully sampled y. On each datapoint in the calibration set, normalized location \\(p \\in (0,1]\\) again, with the linked segment-level reward available. Across all data points in the calibration set, we construct a new dataset \\(D_{norm} = \\{(p, \\mu_p, \\sigma_p)\\} \\), where p runs over all values of normalized location in the calibration set, \\(\\mu_p\\) and \\(\\sigma_p\\) respectively denote sample mean and sample std of all segment-level rewards corresponding to p in the calibration set. With \\(D_{norm}\\), we run a simple linear regression to estimate the relation between the log-transformed normalized location log(p) and the mean/std of segment-level rewards at p. Specifically, the regression formula is given by:\n\\(Mean(p) = \\omega_{\\mu} \\times log(p) + b_{\\mu}, Std(p) = \\omega_{\\sigma} \\times log(p) + b_{\\sigma},\\)"}, {"title": "Within-segment Reward Interpolation", "content": "Depending on the specific tokenizer in use, we observed that semantically complete text segments may contain up to around twenty tokens. The corresponding action space A might still be large and the resulting segment-level design might not sufficiently address the sample inefficiency issue in the classical bandit RLHF and could again lead to inferior PPO-based RL training. To further densify the RL training signal, we evenly split the segment-level reward \\(r_{\\phi}(s_t, a_t)\\) for a segment \\(a_t\\) to each token \\(y_i \\in a_t\\). This induces a token-level credit assignment that \\(\\forall y_i \\in a_t, r([x, y_{<i}], y_i) = r(s_t, a_t)/|a_t|\\), where \\([x, y_{<i}]\\) is the generation context of token \\(y_i\\). \\(r\\) can then directly substitute \\(r_{\\phi}\\) in Eq. (5), since \\(\\sum_{t=0}^{T-1} r_{\\phi} (s_t, a_t) = \\sum_{t=0}^{T-1} (\\sum_{y_i \\in a_t} r_{\\phi}(s_t, a_t)/|a_t|)\\).\nNote that r is still intrinsically segment level, since all token selections \\(y_i\\) within segment \\(a_t\\) receive the same feedback, i.e., the average of segment-level reward \\(r_{\\phi}(s_t, a_t)\\). This is in contrast to prior works on token-level reward models (Yang et al., 2023; Zhong et al., 2024), where each token selection is evaluated separately and thus their token-level feedback/rewards vary for each token.\nSummary. With the learned segment-level reward model \\(r_{\\phi}\\) from Section 2.2, in PPO training of the LM policy \\(\\pi_{\\theta}\\), we first normalize each \\(r_{\\phi}(s_t, a_t)\\) in the sampled sequence by the corresponding normalizers Mean(p) and Std(p). Normalized segment-level rewards are then interpolated into the per-token feedback signal r. Finally, we plug r directly into an off-the-shelf RLHF PPO routine."}, {"title": "Related Work", "content": "Reward Models in RLHF. In the classical RLHF paradigm, policy LM is optimized against a bandit reward model trained firstly by binary classification loss on the preference dataset, with KL penalty to a specified prior distribution to avoid reward over-optimization (Ziegler et al., 2019; Stiennon et al., 2020; Jaques et al., 2020; Bai et al., 2022a; Ouyang et al., 2022; Castricato et al., 2022). Under the same bandit formulation, recent works have enhanced the bandit reward model by directly modeling the probability of one response being preferred over the other (Jiang et al., 2023; Zhao et al., 2023; Liu et al., 2023; Dong et al., 2024) or factorizing human preference into multiple facets via multi-objective modeling (Touvron et al., 2023; Wang et al., 2023, 2024c,a). Despite its popularity, from the angle of RL-based optimization of human preference captured by the reward model, such a bandit reward may lead to inferior training, due to the sparse reward issue intrinsic to the bandit formulation of LM generation and credit assignment (e.g., Takanobu et al., 2019; Guo et al., 2022).\nViewing the weakness of bandit RLHF, efforts have been making to densify the reward signal for RLHF LM training. Yang et al. (2023) and Chan et al. (2024) train token-level reward models by the binary preference classification loss. Zhong et al. (2024) and Rafailov et al. (2024) use an LM trained by DPO (Rafailov et al., 2023) firstly for token-level reward assignment, which is later used in PPO training or search-based algorithms. Guo et al. (2023), Cao et al. (2024), and Yoon et al. (2024) assign continuous or fixed fine-grained rewards (e.g., \u00b11) by accessing an external powerful large LM or the oracle environmental reward; while Chen et al. (2024) require the extra task and datasets of erroneous solution rewriting. Apart from potential extra requirements, as discussed in Section 1, the semantic incompleteness of token in text may challenge the efficacy of per-token credit assignment, especially with the prevailing implementation of reward model as a decoder-only transformer that cannot look ahead into later tokens. In contrast, by defining text segment as the basic unit of text sequence that can be semantically more complete than token, our segment-level reward may provide more accurate guidance for RL-based LM training, while not losing the benefit of denser feedback.\nClose to our segment-level reward, process reward models (PRMs, e.g., Uesato et al., 2022; Lightman et al., 2023) in reasoning-alike tasks also assign a single reward value to a short sequence of tokens. The training of PRMs, however, typically requires human annotation on each step of the reasoning-alike process. This may not be feasible in general text generation tasks, e.g., text summarization or dialog, where each step/text segment lacks clear human evaluation criterion while the full generations can be more easily compared or evaluated. By contrast, as seen in Section 2, our method is developed for the most basic yet general RLHF setting, where human preference is only manifested in a dataset of binary sequence-level preference. And the dataset is collected from multiple sources and contains multiple forms of prompt-responses.\nLearning-from-preference. Learning-from-preference classically takes a two-stage approach where a reward model is first trained on a dataset of binary or multiple ranking via maximizing the choice model likelihood (Bradley and Terry, 1952; Plackett, 1975; Luce, 2012), before optimizing the RL/control policy against the learned reward model by RL algorithms (Akrour et al., 2011, 2012; F\u00fcrnkranz et al., 2012). Earlier application in deep learning mainly focuses on relatively simple neural-network policy for robotics/control tasks (Christiano et al., 2017; Ibarz et al., 2018; B\u0131y\u0131k et al., 2019; Brown et al., 2019, 2020; Lee et al., 2021; Shin et al., 2021; Hejna and Sadigh, 2023a,b). Implanting its success in robotics, in natural language generation, this two-stage learning-from-preference paradigm has been scaled up and popularized in the post-training stage to align LMs with specific human values, with applications ranging from text summarization (Ziegler et al., 2019; Stiennon et al., 2020), prompt generation (Yang et al., 2023), to (task-oriented) conversational agent (e.g., Ouyang et al., 2022; Bai et al., 2022a; Menick et al., 2022; Feng et al., 2023; OpenAI, 2023), and with the RL paradigm of both model free (Levine et al., 2020; Yang et al., 2022b) and model based (Yang et al., 2022a,c).\nTo alleviate the complexity in fitting an explicit reward model, motivated by the theory of maximum-entropy control and RL (Ziebart et al., 2008; Ziebart, 2010; Finn et al., 2016), direct preference optimization methods (DPO, e.g., Rafailov et al., 2023; Tunstall et al., 2023; Azar et al., 2023; Yuan et al., 2023; Zhao et al., 2023; Ethayarajh et al., 2024; Yin et al., 2024) were recently proposed to directly train LMs on a preference dataset by using their log-density-ratio as the classification logit, which have been adapted to train text-to-image diffusion models (e.g., Wallace et al., 2023; Yang et al., 2024; Li et al., 2024b; Gu et al., 2024).\nIn this paper, we contribute to the literature of learning-from-preference by re-thinking a suitable definition of action space in the RL formulation of LM generation and preference alignment. Motivated by semantic completeness in linguistics, we define each action as \"text segment\", spanning across a small amount of tokens and interpolating between prior works' action space of either the finest \"per token\" or the coarsest \"full sequence\". With this design, our method may benefit from both denser reward signal for easier RL-based LM training and the semantic completeness of each action for more accurate training guidance, as experimentally verified in Section 4.\nTraining Signals for RL-based Language Model (LM) Training. In RL-based LM fine-tuning, a classical training signal for adapting LMs to the specific downstream task is the native trajectory-level downstream test metrics (e.g., Ryang and Abekawa, 2012; Ranzato et al., 2015; Rennie et al., 2017; Paulus et al., 2017; Shu et al., 2021; Lu et al., 2022). This approach intrinsically uses a bandit formulation of LM generation that treats the entire generated sequence as a single action. As discussed in Section 1, ignoring the sequential nature of LM generation, this bandit training signal delays the feedback to each token/phrase selection, and can thus incur optimization difficulty (Guo et al., 2022; Snell et al., 2022). With various forms of stronger data or compute requirements, task-specific per-step training signals have been proposed to mitigate this sparse reward issue. Assuming abundant golden expert data for supervised (pre-)training, Shi et al. (2018) construct per-step reward via inverse RL (Russell, 1998); Guo et al. (2018) use a hierarchical approach; Yang et al. (2018) learn LM discriminators; Lin et al. (2017) and Yu et al. (2017) use the expensive and high-variance Monte Carlo rollout to estimate per-step reward from a sequence-level adversarial reward function trained in the first place; while Le et al. (2022) use some rule-based intermediate training signal derived from the oracle sequence-level evaluation, without explicitly learning per-step reward.\nSimilarly, in RLHF, to move forward from the classical bandit formulation, methods have recently been proposed to ground sparse preference labels into dense per-step feedback, with applications in task-oriented dialog systems (e.g., Ramachandran et al., 2021; Feng et al., 2023) and variable-length text-sequence generation (Yang et al., 2023). Our paper seeks to reconcile dense v.s. sparse training signal in RLHF by distributing feedback to the level of semantically complete \"text segment\", interpolating between the densest \"token level\" and the sparsest \"sequence level\" and ideally getting the benefit of both worlds: easier RL training and accurate optimization signal. Meanwhile, as seen in Section 2, our method adheres to the classical two-stage RLHF paradigm without requiring extra data or compute.\nOther LM Preference Alignment Methods. Apart from RL methods, strategies have been developed to align LMs with preference by adding external filters on top of the pretrained LMs, for safety checking the generations or the training texts (e.g., Xu et al., 2020). Vanilla maximum likelihood estimation has also been adopted for training LMs on curated datasets (Hancock et al., 2019; Solaiman and Dennison, 2021; Scheurer et al., 2022), or instruction fine-tuning LMs on massive highly-curated sets of tasks (Sanh et al., 2022; Chung et al., 2022). With extra requirements on data, modelling, and/or compute, recent LM works also conduct preference alignment by formulating text generation as a constraint satisfaction problem on LM's generation distribution (e.g., Khalifa et al., 2021; Korbak et al., 2022; Go et al., 2023), or utilizing the preference dataset in LMs' pre-training stage (Korbak et al., 2023).\nIn this paper, we seek to refine RL-based LM preference alignment by re-thinking the suitable action space in the RL formulation that allows both denser immediate feedback while not jeopardizing the feedback accuracy. Our segment-level design is validated through numeric and example in Section 4."}, {"title": "Experiments", "content": ""}, {"title": "Experimental Setups and Implementation", "content": "Datasets. For reward model training, we use the preference-700K dataset\u00b9, which is a diverse collection of open-source preference datasets, such as HH-RLHF (Bai et al., 2022a), Stanford Human Preferences Dataset (SHP) (Ethayarajh et al., 2022), and HelpSteer (Wang et al., 2023). PPO-based LM policy training is conducted on the Ultrafeedback dataset (Cui et al., 2023), from which we only use the prompts to sample responses during the PPO training routine.\nBenchmarks and Evaluations. The (PPO-trained) LM policy is evaluated on three popular open-ended instruction-following benchmarks: AlpacaEval 2.0 (Li et al., 2023), Arena-Hard (Li et al., 2024c), and MT-Bench (Zheng et al., 2023), where GPT-4o is used as the judge. Our reported scores follow each benchmark's default protocol. AlpacaEval 2.0 consists of 805 test cases, on which we report the length control win rate (LC), raw win rate (WR), and the average response length in number of characters (# char). The LC metric is specifically designed to be robust against model verbosity. We follow the convention (e.g., Dong et al., 2024) to employ alpaca_eval_gpt4_turbo_fn as the annotator for Alpaca Eval 2.0. Arena-Hard consists of 500 challenging user queries, and we report both the win rate (WR) against the reference model GPT-4-0314 and the average number of tokens in the responses (# token). MT-Bench is a multi-turn dialogue dataset that includes 80 test cases. We report the average MT-Bench scores for two dialogue turns, using GPT-40 as the judge model.\nImplementation. We implement our method onto the open-sourced 3.8B Phi3-mini Instruct (Abdin et al., 2024) and the SFT checkpoint of Phi3.1-mini Instruct, as well as the popular SFT checkpoint of Llama-3-8B (Dubey et al., 2024) released by RLHFlow (Dong et al., 2024)2. The backbone model is used as the starting points of both reward model training and PPO-based LM policy learning, in the latter initializing the models for value function, learning policy, and reference policy. Our implementation is built upon the open-source RLHF framework OpenRLHF (Hu et al., 2024). We maximally follow the default"}, {"title": "Main Experimental Comparisons", "content": "Baselines. To demonstrate our unique consideration of RLHF's action space, in the main experiment, we compare our design of segment-level action space with the coarsest bandit/sequence-level action space, the coarser sentence-level space, and the finest token-level space, in terms of performance of the PPO-trained LM policy. For PPO training, a corresponding reward model is first trained under the"}]}