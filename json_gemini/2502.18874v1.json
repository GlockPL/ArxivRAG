{"title": "Learning to Align Multi-Faceted Evaluation: A Unified and Robust Framework", "authors": ["Kaishuai Xu", "Tiezheng Yu", "Wenjun Hou", "Yi Cheng", "Liangyou Li", "Xin Jiang", "Lifeng Shang", "Qun Liu", "Wenjie Li"], "abstract": "Large Language Models (LLMs) are being used more and more extensively for automated evaluation in various scenarios. Previous studies have attempted to fine-tune open-source LLMs to replicate the evaluation explanations and judgments of powerful proprietary models, such as GPT-4. However, these methods are largely limited to text-based analyses under predefined general criteria, resulting in reduced adaptability for unseen instructions and demonstrating instability in evaluating adherence to quantitative and structural constraints. To address these limitations, we propose a novel evaluation framework, ARJudge, that adaptively formulates evaluation criteria and synthesizes both text-based and code-driven analyses to evaluate LLM responses. ARJudge consists of two components: a fine-tuned Analyzer that generates multi-faceted evaluation analyses and a tuning-free Refiner that combines and refines all analyses to make the final judgment. We construct a Composite Analysis Corpus that integrates tasks for evaluation criteria generation alongside text-based and code-driven analysis generation to train the Analyzer. Our results demonstrate that ARJudge outperforms existing fine-tuned evaluators in effectiveness and robustness. Furthermore, it demonstrates the importance of multi-faceted evaluation and code-driven analyses in enhancing evaluation capabilities.", "sections": [{"title": "1 Introduction", "content": "The rapid advancement of Large Language Models (LLMs) has highlighted the critical need for robust output evaluation methods (Li et al., 2024a). While proprietary models like GPT-4 have emerged as predominant evaluation approaches given their superior capabilities, transparent and controllable considerations have driven research toward fine-tuning open-source LLMs for evaluation tasks (Kim et al., 2024a,b). Recent work has established the viability of open-source alternatives by training LLMs to replicate the evaluation explanations and judgments of proprietary models (Ke et al., 2024; Liu et al., 2024; Hu et al., 2024; Kim et al., 2024b). However, existing fine-tuned evaluators rely solely on text-based analysis with predefined evaluation criteria, leading to two key limitations (Li et al., 2024b; Hu et al., 2024; Zhu et al., 2023; Kim et al., 2024b). First, evaluation based on predefined criteria can not fully capture the nuanced task requirements. For example, general criteria for writing, such as conciseness or logical structure, may not be sufficient for evaluating creative writing tasks that require an engaging plot. Moreover, it is challenging to effectively adapt predefined criteria to new and diverse instructions (Li et al., 2024b). Second, LLM-based evaluators demonstrate significant instablity in evaluating adherence to complex instruction requirements, particularly objective criteria such as quantitative or structural constraints (Zhou et al., 2023). For instance, they struggle to reliably assess basic textual attributes such as word counts, a common requirement in writing-related instructions (Zhang and He, 2024). These limitations also extend to the evaluation of formatting constraints.\nIn this work, we argue that developing robust fine-tuned evaluators requires the ability to adaptively generate evaluation criteria and conduct multi-faceted analyses (Saha et al., 2024). These abilities enhance the evaluators' comprehensive performance in both what to evaluate and how to evaluate. Even for unseen instructions, the evaluators can define tailored criteria and assess instructions with nuanced precision. Furthermore, evaluators should use automated tools to assess objective requirements (Wang et al., 2024a). These tools provide reproducible feedback, offering reliable verification that helps overcome LLMs' inherent limitations in objective evaluation.\nTo address these challenges, we propose ARJudge, a novel evaluation framework that combines adaptive criteria generation with text-based and code-driven analysis generation to comprehensively assess LLM outputs. ARJudge comprises two core components: (1) an Analyzer that generates multi-faceted evaluation with text-based and code-driven analyses and (2) a Refiner that synthesizes and refines these analyses to produce well-reasoned judgments. We train ARJudge on a curated Composite Analysis Corpus, which contains tasks for generating evaluation criteria and performing multi-faceted analyses in both text and code. This corpus enables the Analyzer to learn context-sensitive evaluation logic, such as deriving criteria from instructions and assessing responses accordingly. Extensive experiments across multiple benchmarks demonstrate ARJudge's superiority and robustness over existing open-source evaluators. Our further analysis validates the necessity and effectiveness of integrating code-driven analyses, which improve accuracy in evaluating instruction following by up to 11.1% compared to text-only methods.\nThe main contributions of this work include:\n\u2022 We propose ARJudge, a novel evaluation framework that combines adaptive criteria generation with text-based and code-driven analyses to evaluate LLM outputs. By incorporating code-driven analytical capabilities, ARJudge extends beyond traditional text-based evaluation approaches.\n\u2022 We develop a training dataset, Composite Analysis Corpus, containing samples for evaluation criteria generation, text-based analyses, and code-driven analyses. It is the first dataset to incorporate multi-faceted analytical samples for evaluator training.\n\u2022 Extensive experiments across multiple benchmarks demonstrate ARJudge's superior performance over existing fine-tuned evaluators."}, {"title": "2 Composite Analysis Corpus", "content": "Collecting comprehensive and detailed evaluation analysis data is essential for fine-tuning an LLM to improve evaluation performance (Li et al., 2024b; Hu et al., 2024). Previous studies (Li et al., 2024b; Hu et al., 2024; Kim et al., 2024a,b) focus exclusively on text-based analysis with predefined general evaluation criteria, showing limited generalization and robustness (Huang et al., 2024a). To address these limitations, we develop a composite analysis corpus to improve LLMs' ability to determine what to evaluate and how to evaluate effectively. The process of constructing the corpus involves three steps: (1) establishing evaluation criteria specifically for each instruction (\u00a72.1), (2) conducting text-based analyses to assess responses using multiple criteria (\u00a72.2), and (3) designing code-driven analyses to assess whether responses satisfy the objective requirements of the instructions (\u00a72.3).\nFirst of all, we collect a large set of instructions from publicly available preference datasets based on Li et al. (2024b). These datasets (Zheng et al., 2023a; Nakano et al., 2021; Havrilla, 2023; Ji et al., 2023) consist of preference pairs of LLM-generated responses to identical instructions. Each pair is annotated with a preference label that identifies the better response. In line with Li et al. (2024b), non-English instructions and multi-turn interactions are removed. Then, we establish multiple evaluation criteria for each instruction."}, {"title": "2.1 Establishing Evaluation Criteria", "content": "We define the evaluation criteria in the form of concise questions (Zeng et al., 2024; Kim et al., 2024b). Each question describes one aspect that a high-quality response should fulfill. For example, responses to the instruction \u201cDraft an email to my deputy chairperson requesting their signature on the attached approval letter in a professional and polite manner\" can be evaluated using the following three questions: \u2018\u201c1. Does the response include a polite and professional request for the deputy chairperson to sign the attached approval letter? 2. Does the response mention the attached approval letter and provide the necessary details about it? 3. Does the response offer assistance with any questions or clarifications the deputy chairperson might have about the approval letter?\u201d We establish two types of questions by prompting an LLM in a zero-shot manner. Type 1 focuses on generating text-based analysis, while Type 2 involves generating Python functions and using execution feedback as code-driven analysis.\nTo generate the first type of question, we prompt an LLM using three sample responses produced by advanced LLMs as well as the instruction x. Such sample responses offer a reference understanding of the instruction. The specific prompt is shown in Figure 6. We collect three questions $q_{text}$ for each instruction x following Zeng et al. (2024) and construct training samples in the format $(x, q_{text})$.\nFor the second type, we must generate new instructions $x'$ with objective constraints in advance, since their proportion in the datasets is relatively low. We use the self-instruct (Wang et al., 2023b) method to add objective constraints to the instructions and then produce evaluation questions for verifying these constraints. Following the verifiable instructions\u00b9 summarized by Zhou et al. (2023), we first generate several objective constraints for each instruction, such as \u201cword count\" and \u201cend with\". The specific prompt is shown in Figure 7. Then, we randomly select one to three constraints to add to each instruction and collect the corresponding evaluation questions $q_{code}$. The training samples are constructed in the format $(x', q_{code})$."}, {"title": "2.2 Collecting Text-based Analysis", "content": "We perform pairwise text-based analyses by providing an LLM with the instruction x, two responses $r_1$ and $r_2$, and their corresponding evaluation questions $\\{q_{text}\\}$. The output necessitates a comparative analysis for each question, followed by a final determination of the better response. The specific prompt is shown in Figure 8. We exclude analyses where the final decision contradicts existing human annotations in the datasets. The training samples are constructed in the format $(x \\oplus r_1 \\oplus r_2 \\oplus q_{text}, Y_{text})$. Here, $Y_{text}$ denotes the associated analysis result for the evaluation question $q_{text}$, which begins with the hint: \u201cLet's evaluate whether responses meet the criteria\u201d."}, {"title": "2.3 Developing Code-driven Analysis", "content": "To enhance evaluation robustness, we develop code-driven analyses to assess evaluation questions designed to verify objective requirements. The pro"}, {"title": "3 ARJudge", "content": "After constructing the corpus, we collect around 25K composite training samples. We fine-tune an LLM based on them and develop ARJudge, a novel evaluation framework that adaptively evaluates LLM-generated responses and integrates both text-based and code-driven analyses. ARJudge consists of two components: a fine-tuned Analyzer and a tuning-free Refiner. Figure 1 presents the overall framework. The Analyzer is trained on the Composite Analysis Corpus to adaptively generate evaluation criteria for any instruction and produce multi-faceted evaluation, including both text-based and code-driven analyses. The Refiner leverages the general LLM's generalist evaluation capabilities to refine the analysis results produced by the Analyzer and make the final judgment. This framework partially preserves the generalist evaluation pattern of the general model while enhancing the evaluation pattern in the fine-tuning dataset."}, {"title": "3.1 Training", "content": "We train the Analyzer with diverse training samples and tasks, including question generation samples $(x, q_{text})$ and $(x', q_{code})$, text-based analysis samples $(x \\oplus r_1 \\oplus r_2 \\oplus q_{text}, Y_{text})$, and code-driven analysis samples $(x' \\oplus r_1 \\oplus r_2 + q_{code}, Y_{code})$. By training on these combined samples, we aim to enhance the LLM's comprehensive analytical capabilities, enabling it to adaptively propose evaluation criteria and conduct multi-faceted analyses. We employ distinct prompt templates for question generation and response analyses, while maintaining a consistent prompt template for both text-based and code-driven analyses. Different forms of analyses are triggered by their respective starting hints."}, {"title": "3.2 Evaluation", "content": "Given an instruction x and two responses $r_1$ and $r_2$, the Analyzer first generates several evaluation questions. Then, it performs a comparative analysis of the two responses based on each evaluation question. Notably, the Analyzer autonomously determines whether to generate Python functions according to question characteristics. If the analysis text includes Python functions, the Analyzer will call a Python interpreter to execute them and return the execution feedback as the code-driven analysis results. Finally, the above multi-faceted analysis results are aggregated and sent to the Refiner for further evaluation. We instruct the Refiner to evaluate the above analysis and refine it with a renewed focus on the instruction's requirements. The Refiner will determine which response is better in a zero-shot manner."}, {"title": "4 Experiments", "content": "4.1 Implementation Details\nTo construct the Composite Analysis Corpus, we prompt GPT-4o to generate evaluation questions for each instruction and collect text-based analysis. Besides, we prompt Claude-3.5-Sonnet to generate Python functions for code-driven objective analysis. We selected Claude-3.5-Sonnet due to its superior performance in code generation. We fine-tune Qwen2.5-7B-Instruct (Qwen, 2025) on the corpus, creating a model we refer to as the Analyzer for performing multi-faceted evaluations. We use the same model in a zero-shot setting as the Refiner, with carefully crafted prompt templates. All generation in the main experiments is performed using greedy decoding by setting the temperature to 0. Details are described in Appendix A.\n4.2 Benchmarks\nWe assess our framework on various evaluation datasets. Four human-annotated pairwise evaluation test sets are included: PandaLM Eval (Wang et al., 2024b), Auto-J Eval (Li et al., 2024b), MT-Bench (Zheng et al., 2023a), and the LLMBar series (Zeng et al., 2024). These sets were chosen for their broad coverage of evaluation tasks and their diverse set of evaluation criteria. For the LLM-Bar series, we use four adversarial sets, Neighbor, GPTInst, GPTOut, and Manual, as unseen sets. Unlike the other three sets and our training datasets, where candidate responses are directly sampled based on instructions, the responses in LLMBar are artificially designed to challenge evaluators by incorporating potentially misleading qualities, such as a more engaging tone. One GPT-4-annotated pairwise evaluation set, JudgeLM Eval (Zhu et al., 2023), is adopted. For all pairwise sets, samples with two equally preferred responses were omitted. Additionally, an instruction-following benchmark, IFEval (Zhou et al., 2023), is incorporated. We use this benchmark to assess the effectiveness of code-driven analysis.\n4.3 Baselines\nTuning-free General LLMs We compare our framework with several general LLMs that can evaluate response quality. Three powerful LLMs, GPT-4o (OpenAI, 2024), Deepseek-v3 (DeepSeek-AI, 2024), and Claude-3.5-Sonnet (Anthropic, 2024), are used due to their balanced and comprehensive performance across most evaluation tasks (Huang et al., 2024a). Additionally, the backbone model used for fine-tuning the Analyzer, Qwen2.5-7B-Instruct (Qwen, 2025), is employed to demonstrate improvements.\nFine-tuned Evaluators We employ five fine-tuned evaluation models that can conduct pairwise evaluation. PandaLM (Wang et al., 2024b) compares two responses and identifies the better one. Auto-J (Li et al., 2024b) and Prometheus (Kim et al., 2024b) support both single-response scoring and pairwise response comparison. Themis (Hu et al., 2024) rates each response based on various criteria and determines the better one by comparing their scores. JudgeLM (Zhu et al., 2023) provides a comparison of two responses along with their corresponding scores. We use official models with 7B parameters for PandaLM, Prometheus, and JudgeLM, and models with 13B and 8B parameters for Auto-J and Themis, respectively.\n4.4 Main Results\nThe main comparative results against baseline methods are shown in Table 1. Following Zeng et al. (2024) and Li et al. (2024b), we calculate the accuracy of the pairwise preference evaluation with and without swapping the two candidate responses, respectively. The average accuracy and the positional agreement rate are displayed as Acc and Agr. The performance in LLMBar is the average of its four subsets. We observe that ARJudge surpasses all fine-tuned evaluators of similar model sizes. Notably, on the challenging LLMBar set, ARJudge outperforms the best fine-tuned baseline, Prometheus2-7B, by 26.7%. Even without more exposure to challenging samples like LLMBar, ARJudge achieves an average 15.6% improvement over its backbone model, Qwen2.5-7B-Instruct. Additionally, ARJudge's performance is comparable to that of powerful tuning-free LLMs on some test sets. For example, ARJudge performs on par with GPT-4o and Claude-3.5-Sonnet on Auto-J Eval and with DeepSeek-V3 on LLMBar. Besides, compared to other fine-tuned methods, ARJudge can generalize to more test sets."}, {"title": "4.5 Ablation Study", "content": "To further investigate the effectiveness of our framework, we analyze several variations of ARJudge, as detailed below. (1) w/o FT: we replace the fine-tuned Analyzer with the same tuning-free model as the Refiner and prompt the model to generate evaluation questions and conduct the multi-faceted evaluation. (2) w/o FT&MF: we apply the model as in the w/o FT setting, generating Chain-of-Thought (CoT) evaluations directly. (3) w/o Refine: we retain the fine-tuned Analyzer and make slight modifications to the prompt for the Refiner to directly output the label of the better response.\nThe ablation results are shown in Table 3. We observe accuracy drops across all test sets with the ablation variants, indicating the effectiveness of each component in ARJudge. Specifically, fine-tuning significantly enhances a general LLM's evaluation capability, enabling it to propose reasonable evaluation questions and analyze responses accordingly. Evaluation questions help the LLM focus on relevant aspects and enhance its evaluation performance. Interestingly, we find that the effects of refinement differ between the fine-tuned and tuning-free Analyzer. In JudgeLM Eval, PandaLM Eval, Auto-J, and MTBench, the refinement keeps evaluation accuracy under the fine-tuned Analyzer's analysis (w/o Refine vs. ARJudge) but significantly decreases it under the tuning-free Analyzer's analysis (Qwen2.5-7B vs. w/o FT&MF). It may be related to the controversial phenomenon that LLMs cannot truly self-correct (Huang et al., 2024b). Additionally, for challenging samples in LLMBar, refinement significantly strengthens the performance of the fine-tuned and tuning-free ones."}, {"title": "4.6 Capability to Evaluate Using Code", "content": "Code-driven analysis plays a crucial role in robustly verifying the objective requirements of instructions. To assess the effectiveness of code-driven analysis, we use the execution results of the IFEval official code as a benchmark and compute the Consistency between its judgment (Loose or Strict) and that of other models. We compare ARJudge with GPT-4o, Claude-3.5-Sonnet, and Qwen2.5-7B-Instruct. These three models are prompted to make judgments in a zero-shot manner. As shown in Figure 3, our framework achieves a significant improvement over the backbone model, Qwen2.5-7B-Instruct, with the help of code-driven analysis. Moreover, ARJudge performs comparably to GPT-4o and Claude-3.5-Sonnet, demonstrating its potential as a viable alternative. Notably, the execution success rate of the generated code is 100%."}, {"title": "4.7 Effect of Increasing Analysis Quantity", "content": "We extend our analysis by scaling up the number of question sampling attempts, exploring the effect of increasing analysis quantity. We set the temperature to 0.2 to sample evaluation questions multiple times, ensuring diversity in the generated questions. As shown in Figure 4, evaluation accuracy improves with more analyses for most datasets, including JudgeLM Eval, Auto-J Eval, PandaLM Eval, and MTBench. The highest accuracy is achieved with four or five rounds of question sampling and their combined analysis. However, in the LLMBar series, additional analysis had little or even a negative impact on accuracy. This may be because the Analyzer has greater uncertainty about the evaluation samples in these sets, and additional analysis further amplifies this uncertainty."}, {"title": "4.8 Generalization of Evaluation Capability", "content": "To further demonstrate the generalization of evaluation capability, we compute the ratio of judgment change after refining as shown in Table 4. Combining Table 3 and 4, we observe that the Refiner maintains evaluation performance in JudgeLM Eval, PandaLM Eval, Auto-J Eval, and MTBench, while significantly increasing it in the LLMBar series. This indicates that re-analysis improves the generalization of evaluation capability, especially in handling unseen challenging samples."}, {"title": "5 Case Studies", "content": "We show an example of a multi-faceted evaluation generated by ARJudge in Figure 5. Given an instruction and two responses, the Analyzer first generates three evaluation questions and the corresponding multi-faceted analyses. The last question is analyzed by constructing a Python function and assessing execution feedback to determine requirement completeness. Then, the Refiner reviews the preliminary analysis and refines it by reconsidering the instruction's requirements."}, {"title": "6 Related Work", "content": "6.1 Tuning-Free Generalist Evaluators\nTuning-free generalist evaluators leverage the inherent capabilities of large language models (LLMs) to assess responses through the use of carefully designed prompts, offering exceptional flexibility and scalability. Various techniques have been employed to enhance the accuracy of these evaluations, such as in-context learning (Fu et al., 2023; Lin and Chen, 2023)), adding task-specific criteria (Kotonya et al., 2023; Zhuo, 2024), and Chain-of-Thought analysis (Liu et al., 2023; Zhuo, 2024)). Despite their versatility, tuning-free evaluators often suffer from biases such as position bias (Raina et al., 2024; Wang et al., 2023a; Zheng et al., 2023b) and verbosity bias (Khan et al., 2024; Ye et al., 2024), which can skew evaluation outcomes. Methods like response-adapted references (Zhang et al., 2024), multi-agent collaboration (Xu et al., 2023), and divide and conquer (Saha et al., 2024; Li et al., 2023) have been proposed to mitigate these issues, improving the fairness and reliability of LLM-based evaluations.\n6.2 Specialized Fine-Tuned Evaluators\nWhile tuning-free approaches provide flexibility, specialized fine-tuned evaluators are explicitly trained on human-labeled preference data to achieve higher accuracy and domain-specific reliability. These models undergo supervised fine-tuning or reinforcement learning-based optimization to align their evaluations more closely with expert judgments (Li et al., 2024b; Wang et al., 2024b; Kim et al., 2024a,b; Xie et al., 2024). While fine-tuned evaluators offer improved accuracy, they face notable challenges in scalability and generalization (Huang et al., 2024a). Unlike tuning-free approaches, which can adapt to new tasks with minimal configuration, fine-tuned models require ongoing updates through methods such as supervised fine-tuning or direct preference optimization (Rafailov et al., 2024). To remain effective amidst evolving benchmarks (Zheng et al., 2023a; Zeng et al., 2024), Auto-J (Li et al., 2024b) leverages a large dataset of scoring and preference annotations while incorporating dynamic in-context learning techniques, such as few-shot prompting, to enhance adaptability. Similarly, FLAMe (Vu et al., 2024) combines fine-tuning on labeled preference data with large-scale multitask instruction tuning, enabling it to dynamically adapt to new evaluation criteria while maintaining flexibility."}, {"title": "7 Conclusion", "content": "This work proposes a novel evaluation framework, ARJudge, which adaptively designs evaluation criteria and performs multi-faceted evaluation in both text and code. A new Composite Analysis Corpus, designed for both criteria generation and multi-faceted analysis, is developed to train ARJudge. Extensive experiments demonstrate the superiority and robustness of our framework across diverse evaluation benchmarks. Notably, with code-driven analyses, ARJudge gains strong evaluation capabilities for assessing instruction following. Future studies can explore the effective use of more tools, such as a search engine, to improve evaluation honesty and mitigate hallucination.\nLimitations\nWhile our framework outperforms various baseline approaches in LLM evaluation, there is still room for improvement. Our method is limited to using code to enhance evaluation robustness and does not consider additional tools such as search engines or specialized agents. Furthermore, our approach partially relies on the LLM's own reasoning ability for evaluation. If the LLM itself lacks strong reasoning capabilities, the effectiveness of refinement may be limited. Additionally, our evaluation is restricted to pairwise comparisons and does not enhance the model's ability to score single responses. Although single-response scoring can be achieved by modifying the Refiner's prompt, its accuracy has not been properly aligned."}, {"title": "A Training Settings", "content": "We train Qwen2.5-7B-Instruct\u00b2 to perform as the Analyzer. The number of training samples in the Composite Analysis Corpus is around 25K, including 7.7K evaluation question generation samples, 6K code-driven analysis samples, and 11K text-based analysis samples. The corpus is constructed based on instructions from Auto-J\u00b3 (Li et al., 2024b). We train it for 2 epochs with a global batch size of 96 and we save checkpoints for every 50 steps. The learning rate is set to 1e-5. We use DeepSpeed ZeRO3 and FlashAttention to reduce computational memory usage. The training is implemented on 6 computing devices. We use Pytorch with the 2.4.0 version, Transformers with the 4.44.2 version, and deepspeed with the 0.14.4 version."}, {"title": "B Prompt Templates", "content": "Prompt templates used for dataset construction are shown in Figure 6, Figure 7, Figure 8, Figure 9, and Figure 10.\nPrompt templates used for the Analyzer and Refiner of our ARJudge are shown in Figure 11, Figure 12, and Figure 13."}]}