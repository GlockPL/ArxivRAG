{"title": "Advancing Large Language Model Attribution through Self-Improving", "authors": ["Lei Huang", "Xiaocheng Feng", "Weitao Ma", "Liang Zhao", "Yuchun Fan", "Weihong Zhong", "Dongliang Xu", "Qing Yang", "Hongtao Liu", "Bing Qin"], "abstract": "Teaching large language models (LLMs) to generate text with citations to evidence sources can mitigate hallucinations and enhance verifiability in information-seeking systems. However, improving this capability requires high-quality attribution data, which is costly and labor-intensive. Inspired by recent advances in self-improvement that enhance LLMs without manual annotation, we present START, a Self-Taught AttribuTion framework for iteratively improving the attribution capability of LLMs. First, to prevent models from stagnating due to initially insufficient supervision signals, START leverages the model to self-construct synthetic training data for warming up. To further improve the model's attribution ability, START iteratively utilizes fine-grained preference supervision signals constructed from its sampled responses to encourage robust, comprehensive, and attributable generation. Experiments on three open-domain question-answering datasets, covering long-form QA and multi-step reasoning, demonstrate significant performance gains of 25.13% on average without relying on human annotations and more advanced models. Further analysis reveals that START excels in aggregating information across multiple sources.", "sections": [{"title": "1 Introduction", "content": "The rapid development of large language models (LLMs) (OpenAI, 2023; Zhao et al., 2023) has led to their prosperity as indispensable tools for information seeking. Despite their remarkable capability to generate fluent and informative responses to user queries, LLMs also struggle with hallucinations (Huang et al., 2023). To facilitate factuality verification, recent research (Bohnet et al., 2022) has explored attributed text generation, a paradigm that enables LLMs to generate responses with citations. By attributing models' output to verifiable sources, it can improve the explainability and credibility of LLM-generated content (Li et al., 2023).\nWhile beneficial, the ability to attribute contextual sources is not inherent in LLMs. Most work induces LLMs to generate text with citations via in-context learning (Gao et al., 2023), which is far from satisfactory (Liu et al., 2023). The current winning recipe for accurate attribution involves fine-tuning on high-quality attribution responses\u00b9 (Li et al., 2024). However, acquiring such data typically requires either manual curation (Malaviya et al., 2023), or distilled from the most advanced LLMs (Huang et al., 2024a,b), both of which are costly and not scalable, thus limiting the growth of models' attribution capability. One promising solution is self-improvement (Yuan et al., 2023), which has demonstrated the potential to boost model performance by learning from self-generated high-quality samples.\nInspired by this, we aim to explore the potential of self-improvement in bootstrapping the attribution ability of LLMs. However, achieving this goal presents several challenges. One significant challenge lies in the risk of model stagnation during the self-improvement process, primarily due to the insufficient supervision signals obtained in the early stage. Concretely, considering the inferior performance of LLMs in handling the attribution task (Gao et al., 2023), generating sufficient high-quality attribution responses solely through sampling proves difficult. This scarcity of high-quality samples limits the opportunities for LLMs to self-improve effectively. Another challenge stems from the limitation of weak supervision signals. Current self-improvement approaches (Yuan et al., 2023) primarily involve supervised fine-tuning on high-quality samples while discarding low-quality ones. When applied to LLM attribution, these high-quality samples provide only weak supervision signals, mainly teaching LLMs on the surface form of attribution (e.g., proper citation format) (Li et al., 2024). Such practice may neglect the potential of exploring fine-grained signals from low-quality samples to learn what constitutes a desirable attribution response.\nTo address these challenges, we present START, a Self-Taught AttribuTion framework designed to bootstrap the attribution capabilities of LLMs. To prevent models from stagnating early due to insufficient supervision signals, we first leverage the model to self-construct high-quality synthetic attribution data (\u00a73.1). The data synthesis process follows reverse attribution thinking: the model initially generates a response to a given query, then breaks it into atomic claims, and finally randomly combines them to create synthetic documents. This process not only simulates multi-source information-seeking scenarios but also ensures precise attribution, as each document can be directly traced back to the specific claim it originated from. These high-quality synthetic data are then utilized for warming up, providing a good starting point for LLMs to self-improve. Furthermore, to better explore fine-grained supervision signals for LLM attribution, we introduce an iterative self-improving recipe (\u00a73.2). Specifically, the framework meticulously designs fine-grained rewards tailored for LLM attribution, covering robustness, comprehensiveness, and attributability. By scoring multiple candidates through sampling and selecting those with the highest holistic rewards for supervised fine-tuning, the framework subsequently utilizes low-quality samples to construct fine-grained preference pairs with diverse optimization rewards for preference optimization. This iterative process further fosters the self-improvement of attribution capabilities.\nWe conduct extensive experiments across three open-domain question-answering datasets, covering long-form QA and multi-step reasoning. Results indicate that START achieves significant performance gains of 25.13% on average in citation quality. Moreover, START successfully achieves self-improvement in LLM attribution, showing progressive improvements across iterations. Ablation studies confirm that each component significantly contributes to the improvement. Further analysis shows that START not only excels in generating superior attributable responses but also in effectively aggregating information across multiple sources."}, {"title": "2 Related Work", "content": "2.1 Large Language Model Attribution\nAttribution has gained significant attention for enhancing the interpretability and verifiability of LLMs (Gao et al., 2023; Li et al., 2023). Recent studies have focused on improving LLM attribution in a supervised way. Asai et al. (2023) first distill GPT-4 to collect high-quality attribution data, aiming to teach the model to generate grounded answers with citations through self-reflecting. Similarly, Huang et al. (2024a) develop a training framework starting with distilling ChatGPT, followed by designing reward models to teach the LLM to generate highly supportive and relevant citations. Additionally, Li et al. (2024) model the attribution task from a preference learning perspective, where they first fine-tune the model on human-labeled attribution datasets and then perform preference optimization using synthesized preference data. Furthermore, Huang et al. (2024b) take this further by extending the attribution format to a fine-grained citation level, primarily distilled from ChatGPT. It enables the model to first ground the fine-grained quotes within the context and then condition the generation process on them. In contrast to these methods, START aims to bootstrap attribution capability without relying on human-labeled data or distilling from more capable LLMs.\n2.2 Self-Improvement for LLMs\nHigh-quality data either human-crafted or distilled from advanced LLMs has proven effective in enhancing the performance of LLMs. However, acquiring such high-quality data can be prohibitively expensive. Recently, self-improvement approaches (G\u00fcl\u00e7ehre et al., 2023; Yuan et al., 2024), where LLMs learn from self-generated samples have emerged as a viable solution to compensate for the scarcity of high-quality data. These methods typically involve employing heuristic rules (Zelikman et al., 2022), self-critique (Tian et al., 2024), or training additional verifiers (Hosseini et al., 2024) to assess the quality of model-generated samples. Such practices are particularly effective in reasoning tasks, e.g., mathematical reasoning, where LLMs already demonstrate capable abilities and can receive precise feedback on correctness. However, these advantages are absent in the attribution task, due to its challenging nature. To bridge the gap, we take an initial step towards exploring the potential of self-improvement in LLM attribution."}, {"title": "3 Problem Formulation and Methodology", "content": "We follow a formulation of attributed text generation as described in Gao et al. (2023). This task involves processing a user query q for information-seeking, given a corpus of retrieved documents D, to generate a response S with in-line citations. We assume the response S as consisting of n statements, such that S = {$s_1, s_2,...,s_n$}. Each statement $s_i \\in S$ cites a list of passage $C_i = {C_{i1}, C_{i2},...}$, where $C_{ij} \\in D$. Citations are presented in the form of [1][2], which represent the attribution to specific documents in D.\nNext, we present an overview of START, a training framework designed to teach LLMs to self-improve their attribution ability, as illustrated in Figure 2. START consists of two essential stages: synthetic data warm-up (\u00a73.1) and self-improving for LLM attribution (\u00a73.2).\n3.1 Synthetic Data Warm-Up\nThe core of self-improvement lies in generating high-quality samples and iteratively learning from them. Intuitively, a high-quality attribution response should not be distracted by irrelevant documents (robustness) and capture high coverage of viewpoints across multiple documents (comprehensiveness) while maintaining high citation quality (attributability). However, existing LLMs typically show inferior performance in the attribution task, significantly hindering their ability to generate such high-quality samples. This limitation poses substantial challenges to enhancing their attribution capabilities through self-improvement.\nIn this stage, we propose utilizing the model to self-construct high-quality synthetic data for warming up, enabling the model to have the basic ability to generate robust, comprehensive, and attributable responses across multiple sources. The pipeline consists of the following steps, shown in Figure 1. More details can be found in Appendix A.\nStep 1: Response Generation Given an arbitrary model, we first sample a query q from seed questions Q and then generate a long-form answer S utilizing the parametric knowledge of the model itself. The model is required to produce informative answers that cover multiple perspectives.\nStep 2: Claim Decomposition Prior work (Min et al., 2023) has explored using atomic claims as a fundamental unit in long-form text generation. Thus, for the response S, we ask the model to decompose it into atomic claims. Each atomic claim represents a distinct piece of information.\nStep 3: Claim Combination To ensure that the response behaves as an aggregation of information from multiple documents, we randomly combine different claims into one claim set. This process helps simulate the natural diversity of viewpoints and sources, thus enhancing the comprehensiveness and realism of the synthesized responses.\nStep 4: Document Generation For each claim set, we prompt the model to generate a synthetic document D that provides a comprehensive discussion of the grouped claims. Additionally, to enhance the robustness of the response, we introduce irrelevant documents by uniformly sampling documents generated from other queries.\nStep 5: Attribution Relabel The final step involves labeling the response with citations from the generated documents. This process ensures that each claim within the response is explicitly attributed to its source. In this way, for each query q, and documents set D, we can obtain an informative and attributable response while maintaining robustness against irrelevant documents.\nNext, the model is fine-tuned for warming up with the MLE objective on the synthesized dataset, which consists of N data entries, each containing a query $q_i$, a document set $D_i$, and a high-quality attributable response $y_i$:\n$L = - \\sum_{i=1}^{N} \\sum log P(y_i|q_i, D_i; \\theta)$ (1)\n3.2 Self-Improving for LLM Attribution\nIn this stage, we propose to iteratively boost the model's attribution capability by exploring more fine-grained supervision signals, rather than solely relying on golden responses in synthetic data. This involves leveraging rejection sampling for data growing and fine-grained preference optimization for capability evolution.\n3.2.1 Rejection Sampling Fine-tuning\nAfter warming up, we first sample N candidates for each query in the synthetic dataset and then score each candidate with fine-grained rewards that cover three key dimensions: robustness, comprehensiveness, and attributability.\nAttributability serves as the indispensable condition for high-quality attributable generation. It quantifies the extent to which a response is fully supported by the cited documents. To accurately measure attributability, we employ an off-the-shelf Natural Language Inference (NLI) model\u00b2 by checking whether each statement in the response is entailed by the corresponding cited documents.\n$AttrScore = \\frac{1}{S}\\sum_{i=1}^{S} Entail(Docs, statement_i)$ (2)\nwhere S is the total number of statements in the response and Entail returns 1 if the statement i is entailed by cited documents, and 0 otherwise.\nRobustness measures the degree to which a model-generated response is influenced by irrelevant contexts. Considering that we can identify relevant documents $d_r$ within the document set D for each query q, thus we quantify robustness by calculating the probability difference of the model M to generate the response y under different contexts. The robustness score is defined as follows:\n$RobustScore = \\frac{P_M(y|q \\oplus d_r)}{P_M(y | q+ D)}$ (3)\nEmpirically, the closer the score is to 1, the less the response is disturbed by irrelevant documents.\nComprehensiveness measures the extent to which a response captures all relevant information from the source documents. As the golden responses in the synthetic data are designed to aggregate and reflect information across multi-documents, thus we quantify comprehensiveness by decomposing them into sub-claims and verifying whether these claims are covered by the sampled generation y. We compute the score as below:\n$CompreScore = \\frac{1}{C} \\sum_{i=1}^{C} entail(claim_i, y)$ (4)\nwhere $claim_i$ represents sub-claims and C is the number of golden sub-claims.\nSubsequently, we formulate a holistic reward function (Eq. 5) considering the above dimensions. This function is employed to rank generated candidates, with the top-ranked candidate being selected for further supervised fine-tuning.\n$Reward = I(AttrScore) \\times \\frac{CompreScore}{RobustScore}$ (5)\nHere, I is an indicator function that returns 1 if AttrScore = 1, and 0 otherwise.\n3.2.2 Fine-grained Preference Optimization\nThe common way of self-improvement focuses on updating the model with high-quality samples while discarding low-quality ones. For LLM attribution, simply supervised fine-tuning with highly attributable responses only teaches the LLM to learn surface characteristics of attribution, e.g., the correct form of citation. Inspired by human cognition, learning from mistakes provides more fine-grained signals to understand the mechanisms that drive successful attribution than simply imitating correct examples. Thus, we aim to fully unlock the potential of low-quality samples by constructing fine-grained preference pairs with different optimization rewards for preference optimization.\nGiven the multi-objective nature of LLM attribution, our focus is specifically on attributability and comprehensiveness, utilizing corresponding rewards functions to construct preference data respectively\u00b3. Specifically, we pair samples that exhibit high attributability but low comprehensiveness with the top-ranked sample selected using a holistic reward, and vice versa. These preference pairs, each addressing different optimization objectives, are then aggregated to further train the LLM via DPO (Rafailov et al., 2023):\n$L_{DPO} = -\\mathbb{E}[log \\sigma(r_{\\theta}(x, y^+) - r_{\\theta}(x, y^-))]$ (6)\nHere, reference model $\\pi_{ref}$ is initialized with the model after rejection sampling to minimize the distribution shift from the reference distribution."}, {"title": "4 Experiments", "content": "4.1 Datasets\nFollowing previous work (Ye et al., 2023; Li et al., 2024), we conduct our experiments using two long-form question-answering datasets: ASQA (Stelmakh et al., 2022) and ELI5 (Fan et al., 2019), as well as a multi-step reasoning dataset, StrategyQA (Geva et al., 2021). Both ASQA and ELI5 feature factoid long-form answers that require synthesizing highly relevant documents in response to a user query. In StrategyQA, answers demand a combination of information-seeking and implicit reasoning. Further details on the data statistics, knowledge corpus used for retrieval, and examples for each dataset are provided in Appendix B.\n4.2 Evaluation\nFollowing previous research (Gao et al., 2023), we evaluate model-generated responses mainly on two dimensions: Citation Quality and Correctness. Our evaluation methodology combines both automated metrics and human evaluation.\nAutomatic Evaluation. To assess citation quality, we calculate the citation precision, citation recall, and its harmonic mean citation F1 based on the definition in Gao et al. (2023). We use TRUE (Honovich et al., 2022), a T5-11B model fine-tuned on a collection of natural language inference (NLI) datasets to examine whether the cited documents entail the generated statement. For correctness, different datasets are measured differently. For ASQA, we report the exact match recall (EM Rec.) of correct short answers. For ELI5, we report the claim recall (Claim) by checking whether the model output entails the sub-claims generated by text-davinci-003. For StrategyQA, the format of answers begins with yes/no, we evaluate correctness by reporting the accuracy (Acc.). See Appendix C for more details.\nHuman Evaluation. We collected a total of 150 instances from the test sets of ASQA, ELI5, and StrategyQA for human evaluation, with each dataset providing 10 instances from five different systems. The evaluation is divided into two parts:"}, {"title": "5 Results", "content": "5.1 Main Results\nWe provide the main results and the performance of START across different iterations in Table 1.\nSTART effectively improves performance. As shown in Table 1, START shows superior performance across three datasets and achieves state-of-the-art results in citation quality. Specifically, START shows significant improvements over both ICL and Post-hoc approaches, highlighting the benefits of supervised signals in unlocking the attribution ability of LLMs. Notably, compared with methods that rely on distilling from more advanced LLMs or training on human-annotated data, START achieves performance improvement of at least 8.0%, 20.4%, and 47.0% in citation quality for ASQA, ELI5, and StrategyQA respectively. Regarding correctness, START also achieves gains of at least 9.1% and 7.2% on both ASQA and StrategyQA, despite a slight decrease on ELI5.\nSTART successfully achieves self-improvement. We compare the performance of START from iteration 0 to 3 in Table 1, and the results demonstrate consistent improvements across iterations. Initially, at iteration 0 (after warm-up), thanks to the synthetic training data, the model shows decent performance after warm-up. By iteration 1, START exhibits remarkable effectiveness in improving its performance by leveraging its own generated samples (e.g., 23.5 \u2192 72.0 on ASQA, 10.0 \u2192 48.9 on ELI5, 9.5 \u2192 46.4 on StrategyQA). Subsequent iterations continue this trend of incremental improvement, reaching a convergence point at iteration 3.\n5.2 Ablation Study and Analysis\nWe conduct comprehensive ablation studies and analyses to understand how each component in START contributes to the significant improvement.\nEffect of synthetic data warming-up. To demonstrate the importance of utilizing synthetic data for initial warm-up in START, we conduct a comparative ablation study employing Llama-2-13b for self-improvement, omitting the initial warm-up stage. Table 2 shows the ablation results (w/o. warm-up) across three iterations. We observe that omitting the initial warm-up stage can lead to a significant performance drop in the first iteration. Additionally, as the iteration increases, the performance of the model without warm-up shows only modest improvements and remains substantially inferior to the model that underwent warm-up. Moreover, we also calculate the pass rate of sampled response in each iteration as shown in Table 3. The findings indicate that the model with warm-up exhibits a higher pass rate in the first iteration, which allows the model to utilize more supervised signals for self-improvement. These results suggest that warming up effectively facilitates the bootstrapping of supervised data, thus preventing early model stagnation. It's worth noting that while the warm-up strategy effectively enriches the model with supervision signals at an early stage, it does not lead to noticeable improvements in citation quality, as shown in Table 1. We hypothesize that this limitation stems from the inherent difficulty LLMs face in synthesizing information from multiple sources to generate comprehensive and attributable responses solely through direct supervised fine-tuning.\nEffect of fine-grained preference optimization. To further understand the significance of fine-grained preference optimization, we compare an ablation of START that solely relies on high-quality samples for iteratively supervised fine-tuning, discarding low-quality samples for fine-grained preference optimization. As shown in Table 2, there is a significant decline in performance when fine-grained preference optimization is removed. This highlights the effectiveness of START in fully unlocking the potential of low-quality samples to enhance attribution performance.\nEffect of synthetic data size. We investigate the effect of varying synthetic data sizes on the performance of START. Figure 4 demonstrates their effect on citation quality and correctness after three iterations of self-improving. Specifically, we sample 1k, 3k, and 5k unlabeled queries to generate synthetic training data accordingly, which provides different levels of supervision signals. As shown in Figure 4, even with 1k synthetic data points, START demonstrates comparable performance. Moreover, as the training size increases, START achieves notable improvement in citation quality and exhibits stability in correctness.\nSupervision signals from synthetic data v.s. iterative self-improvement. We further investigate the differential impact of supervision signals derived from data synthesis versus those from the iterative self-improvement stage. We utilize synthetic training data to train the model for multiple epochs, extending up to 10 epochs, and compare its performance to that of a model that undergoes only the first iteration of self-improvement. As depicted in Figure 3, training with synthetic data during the initial iteration yields minimal performance gains. The attribution performance climbs slowly as training epochs increase and fails to surpass the performance of the model after just one iteration of self-improvement. This observation reveals the importance of the supervision signals provided by the model itself during self-improvement."}, {"title": "6 Human Evaluation", "content": "Human evaluation results, detailed in Table 4, indicate that START generates significantly more attributable responses compared to all baselines, even surpassing ChatGPT\u2075. Specifically, 76.2% of the statements generated by START are fully supported by the cited documents, which outperforms ChatGPT by 11.24%. Additionally, 18.3% of the statements are partially supported, with only 5.5% unsupported. In terms of factuality, START outperforms all training-based baselines, slightly inferior to ChatGPT. Moreover, START achieves the highest score in comprehensiveness, demonstrating its exceptional ability to generate responses that extensively cover information from multiple sources. Overall, these findings are in line with the automatic evaluation results in Table 1."}, {"title": "7 Conclusion", "content": "We propose START, a self-improvement framework to push the frontier of LLM attribution. We identify two key limitations for LLM attribution self-improvement. To address these, START first leverages self-constructed synthetic data for warming up, aiming to prevent models from early stagnation due to insufficient supervision signals. To explore more fine-grained supervision signals, START constructs fine-grained preference supervision signals from low-quality samples for preference optimization. Both automatic and human evaluations demonstrate significant improvement in attribution without relying on human annotations and more advanced LLMs."}, {"title": "Limitations", "content": "Despite significant performance improvements, our work presents several limitations worth noting. Firstly, while our data synthesis process provides a good starting point for the model to self-improve and demonstrate some generalization on existing benchmarks, it may not cover all scenarios encountered in user information-seeking. This limitation raises concerns regarding the generalizability of synthetic data in a more complex information-seeking environment. Secondly, the iterative training pipeline of our self-improving framework is time-consuming, presenting a significant trade-off between performance and training duration. Thirdly, although our self-improving framework does not rely on human annotations and more advanced LLMs, it still necessitates the integration of off-the-shelf NLI models to guarantee the quality of attribution in the generated samples. The performance of the NLI model significantly impacts the quality of our outputs to a certain extent. To move towards a fully self-improving framework that does not rely on external judgment, future research could investigate the use of intrinsic attribution signals derived directly from the LLM itself."}]}