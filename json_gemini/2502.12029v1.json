{"title": "KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs", "authors": ["Qi Zhao", "Hongyu Yang", "Qi Song", "Xinwei Yao", "Xiangyang Li"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various complex tasks, yet they still suffer from hallucinations. Introducing external knowledge, such as knowledge graph, can enhance the LLMs' ability to provide factual answers. LLMs have the ability to interactively explore knowledge graphs. However, most approaches have been affected by insufficient internal knowledge excavation in LLMs, limited generation of trustworthy knowledge reasoning paths, and a vague integration between internal and external knowledge. Therefore, we propose KnowPath, a knowledge-enhanced large model framework driven by the collaboration of internal and external knowledge. It relies on the internal knowledge of the LLM to guide the exploration of interpretable directed subgraphs in external knowledge graphs, better integrating the two knowledge sources for more accurate reasoning. Extensive experiments on multiple real-world datasets confirm the superiority of KnowPath.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are increasingly being applied in various fields of Natural Language Processing (NLP) tasks, such as text generation[Wang et al., 2024; Dong et al., 2023], knowledge-based question answering[Luo et al., 2024a; Zhao et al., 2024], and over specific domains[Alberts et al., 2023; Jung et al., 2024]. In most scenarios, LLMs serve as intermediary agents for implementing various functions [Hu et al., 2024; Huang et al., 2024; Guo et al., 2024]. However, due to the characteristics of generative models, LLMs still suffer from hallucination issues, often generating incorrect answers that can lead to uncontrollable and severe consequences[Li et al., 2024]. Introducing knowledge graphs (KGs) to mitigate this phenomenon is promising[Yin et al., 2022]. This is because knowledge graphs store a large amount of structured factual knowledge, which can provide large models with accurate knowledge dependencies. At the same time, correcting the knowledge in large models often requires fine-tuning their model parameters, which inevitably incurs high computational costs[Sun et al., 2024]. In contrast, updating knowledge graphs is relatively simple and incurs minimal overhead.\nThe paradigms of combining LLMs with KGs can be classified into three main categories. The first one is knowledge injection during pre-training or fine-tuning[Luo et al., 2024b; Cao et al., 2023; Jiang et al., 2022; Yang et al., 2024]. While the model's ability to grasp knowledge improves, these methods introduce high computational costs and catastrophic forgetting. The second one entails using LLMs as agents to reason through knowledge retrieved from the KGs. This approach does not require fine-tuning or retraining, significantly reducing overhead[Jiang et al., 2023; Yang et al., 2023]. However, It heavily relies on the completeness of external KGs and underutilizes the internal knowledge of the LLMs. The third one enables LLMs to participate in the process of knowledge exploration within external KGs[Ma et al., 2024]. In this case, the LLMs can engage in the selection of knowl-"}, {"title": "2 KnowPath", "content": "Topic Entities represent the main entities in a query Q, denoted as \\(e_0\\). Each Q contains N topic entities \\({e_1, ..., e_N}\\).\nInference Paths are a set of paths P = \\{P_1, \u2026, P_L\\} generated by the LLM's own knowledge, where L \u2208 [1, N] is dynamically determined by the LLM agent. Each path p starts from the topic entity \\(e_0 \u2208 \\{e_1, ..., e_N\\}\\) and can be represented as \\(p = e_0 \\rightarrow r_1 \\rightarrow e_1 \\rightarrow ... \\rightarrow r_n \\rightarrow e_n\\), where \\(e_i\\) and \\(r_i\\) represent entities and relationships, respectively.\nKnowledge Graph(KG) is composed of many structured knowledge triples: \\(K = \\{(e_h,r,e_t), r \u2208 R, e_h, e_t \u2208 E\\}\\), where E represents all entities in the knowledge graph, and R represents all relationships, and \\(e_h\\) and \\(e_t\\) represent the head and tail entities, respectively.\nKG Subgraph refers to a connected subgraph extracted from the knowledge graph K, where the entities and relationships are entirely derived from K, i.e., \\(K_s \u2282 K\\)."}, {"title": "2.2 Inference Paths Generation", "content": "Due to the extensive world knowledge stored within its parameters, LLMs can be considered as a complementary representation of KGs. To fully excavate the internal knowledge of LLMs and guide the exploration of KGs, we propose a prompt-driven method to extract the internal knowledge of LLMs effectively. It can retrieve reasoning paths of the model's internal knowledge and clearly display the reasoning process, and also is particularly effective in zero-shot scenarios. Specifically, given a query Q, we first guide the"}, {"title": "2.3 Subgraph Exploration", "content": "Exploration Initialization. KnowPath performs subgraph exploration for a maximum of D rounds. Each round corresponds to an additional hop in knowledge graph K and the j-th contains N subgraphs \\({K_{i,j},..., K_{N,j}}\\). Each subgraph \\(K_{i,j}\\) is composed of a set of knowledge graph reasoning paths, i.e. \\(K_{i,j} = \\{p_{i,j}^1 U... U p_{i,j}^l, i \u2208 [1,N]\\}\\). The number of reasoning paths l is flexibly determined by the LLM agent. Taking the D-th round and the z-th path as an example, it starts exploration from one topic entity e and ultimately forms a connected subgraph of the KG, denoted as \\(P_{i,z}^{D} = \\{e_0, e_{i,z}^1, r_{i,z}^1, e_{i,z}^2, r_{i,z}^2,..., r_{i,z}^{l-1}, e_{i,z}^l\\}\\). The start of the first round of subgraph exploration (D=0), each path \\(p_i^0\\) corresponds to the current topic entity, i.e.\\(p_i^0 = \\{e_0\\}\\).\nRelation Exploration. Relation exploration aims to expand the subgraphs obtained in each round of exploration, enabling deep reasoning. Specifically, for the i-th subgraph and the j-th round of subgraph exploration, the candidate entities is denoted as \\(E_{i,j}^a = \\{e_{i,j-1}^{l-1,1},...,e_{i,j-1}^{l-1,N}\\}\\), where \\(e_{i,j-1}^{l-1,1}\\) is the tail entity of the reasoning path \\(p_{i,j-1}\\). Based on these candidates \\(E_{i,j}^a\\), we search for all coresponding single-hop relations in knowledge graph K, denoted as \\(R_{i,j}^a = \\{r_1, \u2026, r_M\\}\\), where M is determined by the specific knowledge graph K. Finally, the LLM agent will rely on the query Q, the inference path P generated through the LLM's internal knowledge (Section 2.2), and all topic entities \\(e_0\\) to select the most relevant candidate relations from \\(R_{i,j}^a\\), denoted as \\(R_C \u2286 R_{i,j}^a\\) which is dynamically determined by the LLM agent.\nEntity Exploration. Entity exploration depends on the already determined candidate entities and candidate relations. Taking the i-th subgraph and the j-th round of subgraph exploration as an example, relying on \\(E_{i,j}^a\\) and \\(R_C\\), we perform queries like (e, r, ?) or (?, r, e) on the knowledge graph K to retrieve the corresponding entities \\(E_{i,j}^e = \\{e_1, ..., e_N \\}\\), where N varies depending on the knowledge graph K. Then, the agent also considers the query Q, the inference path P in Section 2.2, the topic entity \\(e_0\\), and the candidate relation set \\(R_{C}\\) from \\(E_{i,j}^a\\) to generate the most relevant entity set \\(E_{i,j}^{e+1} = \\{e_{i,j}^1,..., e_{i,j}^N\\} \u2286 E_{i,j}^e\\). Note that \\(e_{i,j}^N\\) is the tail entity of the reasoning path \\(p_{i,j}\\).\nSubgraph Update. Relation exploration determines entity exploration, and we update the subgraph only after completing the entity exploration. Specifically, for the i-th subgraph and the j-th round of subgraph exploration, we append the result of the exploration \\((\\delta_{i,j}, r, e_{i,j}^1)\\) to the path \\(p_{i,j}\\) in the subgraph \\(K_{i,j}\\). This path update algorithm not only considers"}, {"title": "2.4 Evaluation-based Answering", "content": "After completing the subgraph update for each round, the agent attempts to answer the query through the subgraph \\(\\{K_{1,j},..., K_{N,j}\\}\\). If it responds incorrectly, the next round of subgraph exploration will be executed, until the maximum exploration depth D is reached. Otherwise, it will output the final answer along with the corresponding interpretable directed subgraph. Unlike previous work [Chen et al., 2024], even if no answer is found at the maximum exploration depth, our KnowPath will rely on the inference path P to response. The framework of KnowPath is shown in Figure 2."}, {"title": "6 Conclusion", "content": "In this paper, to enhance the ability of LLMs to provide factual answers, we propose the knowledge-enhanced reasoning framework KnowPath, driven by the collaboration of internal and external knowledge. It focuses on leveraging the reasoning paths generated by the extensive internal knowledge of LLMs to guide the trustworthy directed subgraph exploration of knowledge graphs. Extensive experiments show that: 1) Our KnowPath is optimal and excels at complex multi-hop tasks. 2) It demonstrates remarkable cost-effectiveness, with a 55% reduction in the number of LLM calls and a 75% decrease in the number of tokens consumed compared to the strong baselines. 3) KnowPath can explore directed subgraphs of the KGs, providing an intuitive and trustworthy reasoning process, greatly enhancing the overall interpretability."}]}