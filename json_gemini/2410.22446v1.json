{"title": "Do Large Language Models Align with Core Mental Health Counseling Competencies?", "authors": ["Viet Cuong Nguyen", "Mohammad Taher", "Dongwan Hong", "Vinicius Konkolics Possobom", "Vibha Thirunellayi Gopalakrishnan", "Ekta Raj", "Zihang Li", "Heather Jamie Soled", "Michael L. Birnbaum", "Srijan Kumar", "Munmun De Choudhury"], "abstract": "The rapid evolution of Large Language Models (LLMs) offers promising potential to alleviate the global scarcity of mental health professionals. However, LLMs' alignment with essential mental health counseling competencies remains understudied. We introduce CounselingBench, a novel NCMHCE-based benchmark evaluating LLMs across five key mental health counseling competencies. Testing 22 general-purpose and medical-finetuned LLMs, we find frontier models exceed minimum thresholds but fall short of expert-level performance, with significant variations: they excel in Intake, Assessment & Diagnosis yet struggle with Core Counseling Attributes and Professional Practice & Ethics. Medical LLMs surprisingly underperform generalist models accuracy-wise, while at the same time producing slightly higher-quality justifications but making more context-related errors. Our findings highlight the complexities of developing AI systems for mental health counseling, particularly for competencies requiring empathy and contextual understanding. We found that frontier LLMs perform at a level exceeding the minimal required level of aptitude for all key mental health counseling competencies, but fall short of expert-level performance, and that current medical LLMs do not significantly improve upon generalist models in mental health counseling competencies. This underscores the critical need for specialized, mental health counseling-specific fine-tuned LLMs that rigorously aligns with core competencies combined with appropriate human supervision before any responsible real-world deployment can be considered. Code and data associated with this manuscript can be found at: https://anonymous.4open.science/r/CounselingBench-9DD3/", "sections": [{"title": "Introduction", "content": "Despite the critical importance of mental healthcare for individual and societal well-being, a significant global accessibility crisis continues to exist. Even in highly developed nations like the United States, access to adequate mental health services remains alarmingly insufficient. Current estimates indicate that more than half of the U.S. population resides in designated Mental Health Professional Shortage Areas (MHPSAs), regions where the number of mental health professionals falls short of meeting the population's needs (Heisler and Bagalman, 2013). This shortage poses a major public health challenge (Heisler and Bagalman, 2013), as it hinders timely intervention, contributes to untreated mental health conditions, and exacerbates disparities in care.\nLanguage forms the foundation of mental healthcare, underpinning all interactions and interventions between patients and care providers. Recent advances in Large Language Models (LLMs) offer significant potential to alleviate the aforementioned global shortage in mental healthcare, attributed to their state-of-the-art performance in diverse natural language understanding tasks without additional fine-tuning. In fact, numerous research and commercial efforts have been directed at building LLM-based therapists and counselors to meet people's mental health need (Lai et al., 2023). Increasing numbers of people are also appropriating general-purpose LLMs to find support and advice that may not be available through conventional means (De Choudhury et al., 2023).\nHowever, mental health is a fundamentally \u201chuman\" experience, and addressing its challenges requires a nuanced blend of empathy, cultural sensitivity, and clinical expertise. Effective mental health care necessitates a range of competencies that are sensitive to the myriad and often complex manifestations of individuals' mental health journeys (Clasen et al., 2003). These competencies include conducting thorough psychological assessments, which involve interpreting both verbal and non-verbal cues, understanding patients' unique life contexts, and identifying subtle signs of distress or underlying conditions (Hoge et al., 2005). Additionally, clinicians must develop personalized treatment plans that account for each patient's background, personal history, and presenting symptoms (Banikiotes, 1977). This process is not merely about applying standard diagnostic criteria; it involves setting treatment goals, choosing interventions that align with patients' values, and suggesting strategies based on patients' ongoing responses to treatment.\nFurthermore, mental illnesses exhibit significant clinical heterogeneity (Wardenaar and de Jonge, 2013) - not only are the conditions themselves diverse, encompassing everything from mood and anxiety disorders to complex cases of psychosis or personality disorders, but the symptoms and lived experiences manifest in deeply subjective ways. Effective mental healthcare, therefore, demands a flexible, patient-centered approach that can accommodate this diversity (Zangeneh and Al-Krenawi, 2019). This raises critical questions about whether LLMs, despite their impressive capabilities in natural language understanding, can truly replicate the intricate, human-centered nature of mental health counseling. In this paper, we therefore seek to explore whether LLMs are equipped to demonstrate these core mental health counseling competencies.\nWe present CounselingBench, a new benchmark designed to evaluate LLM performance in the context of key mental health counseling competencies. CounselingBench is based on the National Clinical Mental Health Counseling Examination (NCMHCE), a U.S. licensing exam for mental health counselors that assesses five core competencies identified by a broad survey of professionals in the field. Through CounselingBench, we systematically assess how well LLMs can process and apply domain-specific knowledge from case studies to address questions that evaluate these key competencies. Our research aims to address the following questions:\nRQ1: Are large language models capable of successfully passing the NCMHCE?\nRQ2: How accurately can large language models respond to NCMHCE questions covering various mental health counseling competencies?\nRQ3 How effectively can LLMs generate rationales for their answers to these competency questions?\nOur contributions are as follows:\n\u2022 We provide a novel benchmark, CounselingBench, designed to assess LLM capabilities across key mental health counseling competencies. This benchmark is based on the National Clinical Mental Health Counseling Examination (NCMHCE),"}, {"title": "Related Works", "content": "Online psychological counseling has become prevalent since the COVID-19 pandemic (Yurayat and Tuklang, 2023). Many research works has explored using LLMs for mental health services (Stade et al., 2024; Lawrence et al., 2024). However, the complexity of mental healthcare, which substantially involves empathy and emotions, makes evaluating LLMs for mental health counseling challenging (Fried and Robinaugh, 2020). The lack of a consistent evaluation framework is a key issue, with studies using varied methods like annotated social media posts (Lamichhane, 2023) or clinical vignettes (Inbar Levkovich, 2023). This hinders result comparison across studies. LLMs offer better explainability than supervised models (Yang et al., 2024), but current evaluation metrics only moderately align with human judgment. Our work provides the first large-scale mental health-specific evaluation benchmark for LLMs through a standardized examination for aspiring licensed mental health counselors."}, {"title": "Competency Evaluation of LLMs", "content": "The increasing prominence of LLMs has led to a growing body of research focused on evaluating their performance across a range of benchmarks. These works assess LLMs on diverse tasks such as natural language understanding, mathematical reasoning, coding, and knowledge of the social sciences (Hendrycks et al., 2020). Some general benchmarks were created to test LLMs' general knowledge of graduate-level information in many academic fields (Rein et al., 2023). Others attempted to test the LLMs' mathematical reasoning abilities through grade-school math (Cobbe et al., 2021) and natural science (ARC) word problems (Hu et al., 2024). Additionally, others have also attempted to create benchmarks for specific fields, such as the MedQA benchmark which evaluates LLMs' ability to answer questions related to medical literature and is largely based on medical exam response data (Jin et al., 2021). LegalBench is another such example, as it is a benchmark designed to evaluate LLMs' performance in legal reasoning tasks. (Neel Guha, 2023). Our work extends upon this body of work by proposing the first large-scale assessment of LLM proficiency in key mental health counseling competencies, enabling further research into clinically-valid and human-centered approaches of deploying LLMs for mental health settings"}, {"title": "The CounselingQA Benchmark", "content": "We collected the case study details (including patient demographic, mental status examination, presenting problem, etc.), questions, their associated answers and expert-generated rationale from National Clinical Mental Health Counseling Examination (NCMHCE) questions. The exam seeks to assess the proficiency of individuals seeking to become licensed clinical mental health counselors across five key mental health counseling competencies as detailed in Table 1. These competencies are derived from a national job analysis involving over 16,000 credentialed counselors, which identified empirically-validated work behaviors that are considered most relevant for effective counseling practice (NBCC, 2023). NCMHCE questions and associate details listed above are collected from mock exams which are accessible online for public usage. Details about the sources that we collected data from are described in Appendix Table A1\nOverall, we collected a total of 1612 unique questions across 138 case studies. A full example question, along with the case study context, can be found in Appendix Table A2. To comply with fair use law, we adapt the procedure used in Jin et al. 2020 (Jin et al., 2021) and shuffle the order of answer options (while keeping track with the correct answer). Given that each of the questions are designed to uniquely assess candidates' abilities in one specific competency, we manually annotate all 1612 questions to provide a complete expert-derived question-competency mapping for downstream analysis. Two annotators, both of whom are collaborators on the paper and are medical doctors specializing in psychiatry, independently annotate all questions for one of five competencies as specified in Table 1. For each question, they were instructed to carefully read through the case study details, question statement and candidate answers in its entirety and then select the counseling competency that best described the competency which the question aims to test based on the NCMHCE Content Outline (NBCC, 2023). This yielded 845 questions where the 2 annotators were in agreement about the competency annotation and 767 questions where they were in disagreement. For questions where the 2 annotators' labels disagreed, we invited a third annotator who is a experienced licensed mental health counselor (LMHC) based in the U.S. to provide the tiebreaking vote. The number of questions determined by experts annotators to reflect each of the five core mental health-related competencies can be found in Table 1"}, {"title": "Methodology", "content": "Our study encompasses 13 open-source medical models, selected for their outstanding performance across various biomedical NLP tasks. These models represent seven distinct finetuning architectures: BioMedGPT (Zhang et al., 2023), Asclepius (Kweon et al., 2023), Meditron (Chen et al., 2023), MentaLlama (Yang et al., 2024), ClinicalCamel (Toma et al., 2023), Med42 (Christophe et al., 2024), and OpenBioLLM (Pal and Sankarasubbu, 2024). To provide a comprehensive analysis, we also include the corresponding un-finetuned base models, henceforth referred to as generalist models, of the aforementioned medical models, adding 10 more models to our study. Both the sets of medical models and their generalist counterparts were chosen to represent a wide range of model sizes. Finally, for benchmarking against state-of-the-art proprietary systems, we include gpt-40-2024-08-06 in our comparison (Bubeck et al., 2023; Achiam et al., 2023) due to its top performance on LLM leaderboards such as Chatbot Arena (Chiang et al., 2024). Detailed information regarding model parameters and training data sources can be found in Appendix Table A3."}, {"title": "Inference", "content": "We formally define a question in the CounselingBench dataset as Q = (C, A_{cand}, A_{corr}), where C is the context (including question text and patient demographics), A_{cand} are candidate answers, and A_{corr} is the correct answer. Q^{h} is an abridged version of Q without A_{corr}. To elicit answers from a large language model M given Q, we use the following prompting strategies:\n\u2022 Zero-shot (ZS): M answers Q^{h} without task-specific training, using the prompt template in Appendix Table A4.\n\u2022 Few-shot (FS): M answers Q^{h} after seeing demonstrative questions Q_{1}, ..., Q_{n} (Brown, 2020), using the prompt template in Appendix Table A5. We use n = 3 random demonstrative examples.\n\u2022 Chain-of-thought (CoT): We augment each few-shot example with a step-by-step explanation R towards the correct answer. The input includes (Q_{1}, ..., Q_{n}), (R_{1}, ..., R_{n}) and Q^{h}, using the prompt template in Appendix Table A6. We use the intermediary reasoning chains for comparison in RQ3.\nWe also test self-consistency (SC) decoding (Wang et al., 2022) on all prompting strategies. While originally applied to CoT prompting, we extend SC to zero-shot and few-shot prompting by performing 5 samplings each at temperature $t \\in [0.2, 0.4, 0.6, 0.8, 1]$ and taking the majority label across all 25 samplings."}, {"title": "RQ 1: Can LLMs pass the NCMH\u0421\u0415", "content": "To assess LLMs' performance on CounselingBench, we calculate overall accuracy on all 1612 questions. Given varying real-world NCMHCE passing scores, we use an average threshold of 63% accuracy based on previous reported NCMHCE passing scores. To compare medical and generalist models, we employ paired t-tests between each medical model and its unfinetuned generalist counterpart."}, {"title": "Results", "content": "Table 2 shows the overall performance on the CounselingBench for all models tested across different prompting and decoding settings."}, {"title": "Zero-shot LLMs can pass the NCM\u041d\u0421\u0415", "content": "We found that frontier LLMs (in September 2024) with zero-shot prompting are able to perform on NCMHCE at a level which exceeds the pass threshold of 63% accuracy as defined above. These passing models are primarily larger in size (5 out of 6 models have more than 70B paramaters) and are all instruction-tuned (whether on a general instruction fine-tuning dataset or one specific to the biomedical domain). Not surprisingly, gpt4o is the best performing model among all the tested models, achieving a zero-shot accuracy of 0.78. However, it is also notable the best open-source model (Llama3-70B-it) only performed slightly worse (8.8% reduction in zero-shot accuracy) than gpt4o, despite being significantly smaller in parameter size. We also note that while there is a large gap in performance between the smallest and largest version of instruction-tuned Llama2 (0.432 vs 0.616, 42.6% gain), that gap has substantially decreased between Llama3-8B-it and Llama3-70B-it (0.622 vs 0.717, 15.3% gain). This indicates significant improvements in model architecture and curation of pre-training and post-training data of Llama3 over Llama2, and suggests that current and future smaller-scale models are becoming increasingly viable for mental health counseling tasks while maintaining a lower computational footprint. On the flipside, performance trends on the CounselingBench also imply that performance gains from scaling up model sizes will diminish, or even disappear despite future advancements in model architecture, training data curation and procedure. This matches with observations from Anwar et al. (2024), McKenzie et al. (2023) and Zhou et al. (2024a) on the potential limits of LLM scaling laws. Finally, we note that while the performance of frontier LLMs exceed that of the minimum passing level, it remains substantially lower than that expert-level human performance, which we set at 90% based on expert-level human scores on biomedical QA benchmarks such as MedMCQA and MedQA (Li\u00e9vin et al., 2024)"}, {"title": "Medical models underperformed generalist models across all settings accuracy-wise", "content": "Most surprisingly, we notice that from Table 2 that a supermajority of generalist models seem to consistently outperform their medical fine-tuned counterparts (10 out of 13 pairs) under zero-shot setting, with an average difference of 4.2 percentage points between generalist-medical model pairs in zero-shot accuracy (maximum = 0.15pp, minimum = -0.084pp). We deploy paired t-test to assess significant differences between generalist and medical models' zero-shot accuracy on CounselingBench, and find a significant difference between these two distributions (t = 2.939, p = 0.013). This suggests a systematic underperformance of medical LLMs on mental health counseling-related questions compared to their un-finetuned counterparts. We perform a more fine-grained evaluation of this underperformance in the next research question."}, {"title": "RQ 2: How do LLMs perform across different mental health competencies", "content": "To achieve a more nuanced, competency-centric assessment of model performance, we disaggregate the accuracy metrics in Table 2 into five competency-specific accuracies. This approach: (1) evaluates model capabilities across key counseling domains, reflecting diverse patient needs, and (2) elucidates factors contributing to performance discrepancies between medical-specialist and generalist model pairs."}, {"title": "LLMs are better at treatment planning, worse at counseling skills, interventions", "content": "Table 3 represents zero-shot accuracies of the 5 mental health counseling competencies across all tested LLMs. There are small yet significant variations across different competencies. Across all models, we found that their performance on different competencies (based on zero-shot accuracy) can be sorted as Treatment Planning = Intake, Assessment & Diagnosis >* professional practice & ethics = core counseling attributes = counseling skills and interventions, where > indicates a statistically significant positive difference between zero-shot accuracy across competencies, with the number of stars in the superscript representing the p-value of the paired t-test\u00b9. Such variations remain even under other inference settings (such as few-shot). The observed variations in accuracy across mental health counseling competencies might be attributed to the nature of the tasks and the inherent strengths and limitations of LLMs. Competencies like Treatment Planning and Intake, Assessment & Diagnosis are more procedural and well-represented in model training data, allowing LLMs to perform better here. In contrast, questions on competencies such as Counseling Skills and Interventions and Core Counseling Attributes require more nuanced social, emotional intelligence and cultural competency. These are areas where LLMs have been shown to struggle in even with careful prompting (Zhou et al., 2024b; Chiu et al., 2024)."}, {"title": "Medical models significantly underperform their generalist counterparts across most competencies", "content": "Comparing the cross-competency performance (measured with zero-shot accuracy) between generalist models and medical models with paired t-test, we find that generalist models significantly overperform their paired medical counterparts across 4 out of 5 key mental health-related competencies: Intake, Asssessment & Diagnosis (t = 3.15, p = 0.027), Treatment Planning (t = 2.52, p = 0.035), Counseling Skills & Interventions (t = 3.61, p = 0.024), and Professional Practice & Ethics (t = 2.5, p = 0.035). Among open-source models, no one model has the highest performance across all five competencies. OpenBioLLM-70B achieves the highest performance in treatment planning, whereas Llama3-70B-in achieves the highest performance in the remaining four out of five competencies (even outperforming gpt4o in the \u201ccore counseling attribute\" competency). These discrepancies in model performance persist even when alternative prompting techniques and decoding settings (such as few-shot, few-shot with self-consistency, etc.) are applied, as seen in Appendix Tables A7 and A8. These observed differences might stem from variations in the training data and the patterns each LLM has learned. Medical LLMs are primarily fine-tuned on biomedical research papers and medical question answering datasets, which may give them increased performance on the \"treatment planning\" competency. However, this seems to be at the expense of their retention of knowledge patterns required for the 4 other mental health competencies. In contrast, generalist models, trained and instruction-tuned on a broader range of data, possess and leverage more knowledge patterns that are needed for non-clinical mental health competencies such as counseling skills & interventions in diverse settings. These variations highlight the challenges of developing AI systems for diverse mental health counseling skills, where the need for both emotional intelligence and clinical precision is paramount.\""}, {"title": "RQ 3: How well do LLMs reason to derive their answers across different competencies?", "content": "Beyond evaluating LLMs' accuracy in mental health counseling competencies, we assess their ability to generate high-quality reasoning chains based on the question and case context. This is crucial for future applications, as clear and coherent reasoning enhances LLMs' reliability in guiding therapeutic conversations, potentially improving outcomes for individuals seeking mental health support. Poor reasoning or misinterpretations in therapeutic contexts can compromise treatment and even worsen conditions such as anxiety, depression, or psychosis (Obradovich et al., 2024; De Choudhury et al., 2023). We evaluate reasoning quality using the intermediary chains produced during the chain-of-thought prompting process (Section 4.2). These reasoning chains, or candidate chains, are assessed along key axes-alignment with expert reasoning, coherence, and informativeness-using both reference-free and reference-based metrics:\n\u2022 Reference-free metrics assess candidate chains without relying on reference answers, considering factors like semantic alignment, logical inference, and language coherence. We use ROSCOE, a suite of unsupervised evaluation metrics, due to its scalability and strong correlation with human judgments (Golovneva et al., 2022) with subset of metrics as describe in Appendix Table A9.\n\u2022 Reference-based metrics directly compare candidate chains to expert-generated reference chains, using metrics such as ROUGE-1, ROUGE-L (Lin, 2004), BERTScore (Zhang et al., 2019), and cosine similarity.\nTo better understand the reasoning errors made by LLMs, we annotate errors in a subset of responses from two top-performing models, OpenBioLLM-70B and Llama3-70B-in. Three expert annotators reviewed 100 questions where these models had incorrect answers, categorizing errors as logical, knowledge-based, or contextual, following patterns identified in medical reasoning (Singhal et al., 2023) (Appendix Table A10)."}, {"title": "Results", "content": "Table 4 presents both reference-free and reference-based metrics derived from the reasoning chains of models with a zero-shot multiple-choice accuracy of at least 0.5. Consistent with trends in multiple-choice accuracy as seen in Table 2, we observe that larger and more current open-source models show substantial improvements in generating reasoning chains that align with gold-standard reasoning and exhibit high-quality characteristics over smaller and earlier models, with smaller gaps between different model sizes for the current \u201cgeneration\" of open-source LLMs. For instance, we noticed that the average cosine similarity of Llama3-70B-in is 3.3% and 80% higher than Llama3-8B-in and Llama2-70B-in respectively. Unlike multiple-choice accuracy trends, generalist models do not exhibit significantly better reasoning compared to specialized medical models, or vice versa. However, we do find that while no single model excels across all reasoning metrics, medical models achieve the highest score across a majority of reasoning metrics (7 / 12 \u2248 58%) and all but one reference-free metrics. While propriety frontier models such as gpt4o excel in answering multiple-choice questions over open-source models, they surprisingly produce lower-quality reasoning chains on average compared to some smaller yet more-performant medical open-source counterparts in all reasoning metrics. Appendix Table A11 gives an example of step-by-step reasoning chains on a sample CounselingBench question among top-performing LLMs (with respective to both accuracy and reasoning metrics). These findings highlight the capacity of training high-quality domain-specific models for producing justifications that is more aligned with expert-level decision process, particularly in specialized areas like mental healthcare, despite their diminished ability to derive the correct answer compared to their generalist counterparts. We also note that frontier LLMs still underperform expert-level humans when it comes to producing high-quality reasoning chains to justify mental health counseling-related decisions, highlighting another shortcoming of existing frontier LLMs in the mental health domain (Appendix Table A12)"}, {"title": "Analysis of reasoning errors", "content": "We found that among 100 randomly sampled erroneous answers, Llama 3 70B made 33 logical errors, 21 context errors, and 46 knowledge errors in their reasoning, whereas OpenBioLLM-70B made 35 logical errors, 36 context errors and 29 knowledge errors. A statistical analysis using $x^{2}$ test reveals a significant difference in the distribution of errors made by these two models representative of generalist and medical LLMs, where generalist models are more likely to make knowledge-related errors during their reasoning process whereas medical models are more likely to make reading comprehension-related errors ($x^{2}$ = 17.12, p < 0.001). This suggests that while generalist models may struggle with domain-specific knowledge, specialized models face challenges in accurately interpreting context, underscoring a potential trade-off in reasoning capabilities during the fine-tuning process."}, {"title": "Conclusion and Future Work", "content": "To effectively integrate LLMs into real-world mental health counseling, rigorous assessments of their alignment with core competencies are necessary. In this study, CounselingBench, a benchmark for evaluating LLMs in mental health counseling, was introduced. Both un-finetuned and fine-tuned medical models demonstrated strong capabilities across five key counseling competencies but showed varying performance levels. LLMs excelled in Intake, Assessment & Diagnosis but underperformed in Core Counseling Attributes and Professional Practice & Ethics, which require greater subjectivity and sensitivity to individual patient contexts. Interestingly, fine-tuning on biomedical data did not consistently improve performance; in some cases, these models underperformed compared to generalist models. This highlights that improving LLMs for mental health tasks may require more targeted fine-tuning with diverse, real-world counseling data. Future work should focus on developing specialized LLMs for counseling and adopt more task-specific evaluations to address the unique demands of each competency."}, {"title": "Limitations", "content": "Evaluating LLM competency via multiple-choice exams may not fully capture the complexities of real-world mental health counseling. While multiple-choice formats offer a standardized way to measure knowledge, they often fail to fully assess critical aspects of mental health counseling such as empathy, adaptability, and nuanced decision-making, which are essential in clinical practice. Real-world counseling involves dynamic and context-sensitive interactions that go beyond selecting a correct answer from given options. As a result, models that perform well on these exams may not necessarily demonstrate the same level of competency in genuine therapeutic settings, where understanding the patient's unique background and responding to evolving emotional states are key factors (Obradovich et al., 2024). Thus, relying solely on multiple-choice accuracy may oversimplify the challenges of mental health counseling and provide an incomplete picture of LLM capabilities in this domain."}, {"title": "Scale and Scope of CounselingBench", "content": "Second, the scale and scope of the CounselingBench dataset present limitations. Although the dataset consists of 1,612 questions across 138 case studies, it may not adequately represent the diversity of situations encountered by mental health professionals. The current questions are based on mock exams for the NCMHCE, which may not encompass the full range of clinical presentations and treatment scenarios, especially for culturally specific or less common conditions (Hoge et al., 2005; Zangeneh and Al-Krenawi, 2019). In addition, since the NCMHCE is a US-based licensing exam, it does not cover mental health counseling scenarios and best practices in other global regions such as the Global South, which has significant differences in cultural values, social norms, and access to mental health resources (Sue et al., 2022). Such conditions require tailored approaches to mental health counseling and intervention that are not adequately reflected by the exam's current content. Expanding the dataset to include a wider variety of case studies, question formats, and counseling competencies would enhance its representativeness and allow for a more comprehensive evaluation of LLM performance. Additionally, integrating open-ended and scenario-based questions could better assess the models' ability to engage in complex reasoning and provide contextually appropriate responses, thereby improving the generalizability of findings."}, {"title": "Ethical Considerations", "content": "We note that CounselingBench may be used in future research projects to make sweeping claims regarding LLMs outperforming human mental health counselors and could potentially replace them in real-world mental health counseling situations, similar to claims made in the biomedical domain(Drogt et al., 2024). Such claims. even with higher performance of LLMs on CounselingBench, would be premature and potentially harmful given the lack of longitudinal assessment for LLMs, and incomplete evaluation of its decision-making and execution process. Overreliance on AI systems could compromise patient care and contribute to the erosion of therapeutic alliance(Choudhury and Chaudhry, 2024; De Choudhury et al., 2023). We strongly advise against using CounselingBench to argue for replacing human professionals; instead, it should be viewed as a tool for enhancing AI assistants in tasks supportive of human mental health professionals (Van Heerden et al., 2023). To ensure responsible development and application of AI in mental health, we believe that interdisciplinary collaboration among AI researchers, mental health professionals, ethicists, and policymakers is essential."}]}