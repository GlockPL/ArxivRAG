{"title": "Multi-Task-oriented Nighttime Haze Imaging Enhancer for Vision-driven Measurement Systems", "authors": ["Ai Chen", "Yuxu Lu", "Dong Yang", "Junlin Zhou", "Yan Fu", "Duanbing Chen"], "abstract": "Salient object detection (SOD) plays a critical role in vision-driven measurement systems (VMS), facilitating the detection and segmentation of key visual elements in an image. However, adverse imaging conditions such as haze during the day, low light, and haze at night severely degrade image quality, and complicating the SOD process. To address these challenges, we propose a multi-task-oriented nighttime haze imaging enhancer (MTOIE), which integrates three tasks: daytime dehazing, low-light enhancement, and nighttime dehazing. The MTOIE incorporates two key innovative components: First, the network employs a task-oriented node learning mechanism to handle three specific degradation types: day-time haze, low light, and nighttime haze conditions, with an embedded self-attention module enhancing its performance in nighttime imaging. In addition, multi-receptive field enhancement module that efficiently extracts multi-scale features through three parallel depthwise separable convolution branches with different dilation rates, capturing comprehensive spatial information with minimal computational overhead. To ensure optimal image reconstruction quality and visual characteristics, we suggest a hybrid loss function. Extensive experiments on different types of weather/imaging conditions illustrate that MTOIE surpasses existing methods, significantly enhancing the accuracy and reliability of vision systems across diverse imaging scenarios. The code is available at https://github.com/Ai-Chen-Lab/MTOIE.", "sections": [{"title": "I. INTRODUCTION", "content": "Salient object detection (SOD) focuses on detecting and segmenting the most visually prominent objects within vision-driven measurement systems (VMS) [1], [2]. It is critical for enhancing automation and accuracy in VMS, particularly in applications like quality control and safety monitoring. However, as shown in Fig. 1, adverse imaging environmental conditions, such as low light, daytime or nighttime haze, can significantly affect the accuracy of SOD. Specifically, optical scattering from haze particles not only reduces contrast and sharpness but also distorts the spectral content of the captured scene [3]. Reduced illumination further exacerbates this issue by limiting signal strength, necessitating longer exposures or higher sensor gains, which result in additional electronic noise [4]. These environmental factors necessitate robust VMS that can effectively model and compensate for light attenuation, atmospheric scattering, and sensor-induced artifacts while maintaining measurement fidelity.\nTo address these challenges, researchers are studying non-learning and learning methods to improve the quality of degraded images. Traditional non-learning image restoration methods predominantly rely on histogram equalization [5] and the Retinex methods [6]\u2013[8], which enhance image quality by adjusting brightness and contrast. However, these methods frequently lead to noise or color distortion, which can compromise the integrity of industrial imaging systems. In the context of image dehazing, classical methods such as dark channel prior (DCP) -based methods [9], [10] are effective under daytime conditions but encounter limitations when confronted with complex nighttime scenes. To further enhance image restoration in nighttime hazy conditions, researchers have optimized and refined Retinex and DCP models and proposed new physically-based imaging models [11]\u2013[13] that are specifically tailored to handle the complexities of nighttime haze scenarios.\nLearning-based methods are widely used in image restoration tasks due to their powerful feature learning capabilities. For example, convolutional neural network (CNN)-based methods [14] significantly enhance the quality of low-light or hazy images in various tasks by learning different features from large-scale datasets. The attention mechanism can dynamically focus on important features and enhance the extraction and processing of key information, thus performing effectively in low-level vision tasks [15]\u2013[17]. Generative adversarial networks are used for unsupervised learning tasks [18], [19], pushing the boundaries of image processing in low-light scenarios. Diffusion models [20] have achieved success in low-light image enhancement by producing high-quality images through an iterative denoising process. To combine the advantages of traditional methods and learning methods, model-guided learning methods [21]\u2013[23] have gradually been used to restore degraded images and have achieved satisfactory visual results. However, they are primarily designed for one task and cannot simultaneously handle three types of tasks: image dehazing (ID), low-light image enhancement (LLIE) or nighttime haze image enhancement (NHIE).\nTo address the challenges posed by harsh imaging conditions, we propose a multi-task-oriented nighttime haze image enhancement framework (MTOIE), which be used to handle three types of degradation tasks: ID, LLIE, and NHIE. The framework integrates multi-task learning mechanisms, self-attention (SA) module, and multi-receptive-field enhancement (MRFE) module to effectively enhance image quality under complex degradation scenarios. By employing task-specific node learning mechanisms and effectively using daytime haze, low-light, and nighttime haze datasets for training, MToIE learns common visual patterns (such as edge contours and texture information) across different imaging conditions and specific degradation factors (such as brightness loss and noise enhancement in low-light environments, as well as contrast reduction and scattering blur induced by haze), efficiently models three distinct types of degradation. In addition, a hybrid loss function is designed to balance image reconstruction quality and visual features. The main contributions of this work are summarized as follows\n\u2022 We propose a multi-task-oriented nighttime haze imaging enhancer (MToIE) that incorporates task-specific node learning mechanisms and adopts shared and separated networks for feature-shared and specific feature extraction to effectively address low-light, daytime haze, and nighttime haze degradation scenarios.\n\u2022 The MTOIE integrates SA mechanism and MRFE module to extract multi-scale features. Additionally, a hybrid loss function is suggested to jointly optimize MToIE for robust performance under challenging conditions.\n\u2022 Extensive experiments demonstrate that MTOIE significantly outperforms existing methods in image quality improvement and achieves higher accuracy and reliability in subsequent high-level vision task.\nThe remainder of this paper is organized as follows. Degradation models for nighttime haze are given in Section II. MTOIE is detailedly described in Section III. Experimental results and discussions are provided in Section IV. Conclusions are drawn in Section V."}, {"title": "II. PHYSICAL IMAGING MODELS", "content": "Although nighttime haze imaging incorporates the principles of both daytime haze imaging and low-light conditions, the nighttime haze imaging model involves the complex interplay of optical transmission principles. In the absence of sufficient light, the enhanced scattering and absorption of photons by the haze particles significantly degrade image quality, thereby resulting in diminished clarity and contrast. Hence, the observed image $I(p)$ can be formulated as\n$I(p) = J(p). L(p) \u00b7 t(p) + A \u00b7 (1 \u2013 t(p)) + N(p)$, (1)\nwhere p refers to the position of a pixel, $J(p)$ is the radiant brightness of the scene under ideal lighting and haze-free conditions. $t(p) = e^{-\u03b2d(p)}$ represents light attenuation due to haze, with $\u03b2$ as haze density and $d(p)$ as distance to the scene. A is the atmospheric light estimated from global image brightness. $L(p)$ adjusts for actual lighting against ideal, and $N(p)$ includes sensor noise under low light.\nIt needs to be noted that in practical applications of VMS, Eq. 1 needs to be adjusted according to specific circumstances. Due to the complexity of light source, particularly at night, directly applying above formula may not be sufficiently accurate. The challenges posed by varying lighting conditions, haze, and sensor noise require a more customized approach. To address these issues, our method focuses on three key"}, {"title": "III. MTOIE: MULTI-TASK-ORIENTED IMAGING ENHANCER", "content": "Fig. 2 shows MTOIE integrates a residual block-driven encoder-decoder framework with multi-task-oriented node learning (TNL) to handle three specific tasks: ID, LLIE and NHIE. To further enhance restoration performance, the framework incorporates SA and MRFE modules, strengthening the network's focus on critical regions and features, ensuring robust handling of complex nighttime imaging scenarios. Finally, the network's optimization is guided by a hybrid loss function, which balances multiple objectives, including image reconstruction quality and feature consistency, ensuring improved performance under challenging imaging conditions."}, {"title": "B. Residual learning block", "content": "The residual learning block (RLB) is the foundational component of MToIE, primarily used for feature extraction and inference. As shown in Fig. 3, it comprises three key components: convolution, normalization, and activation functions. The convolution operation, utilizing local receptive fields and 3 \u00d7 3 kernels, extracts detailed features from the input data. Layer normalization (LN) is used to reduce internal covariate shift, enhancing training stability. The parametric rectified linear unit (PReLU) \u03c3 is used, with adaptive parameters that improve the model's expressive capability. Mathematically, a convolutional layer (C) can be expressed as\n$C(x) = \u03c3(LN(Wc * x + b))$, (2)\nwhere x is the input feature map, We represents the convolution kernel weights, and b is the bias term. In addition, the suggested residual learning mitigates the vanishing gradient problem by allowing information to pass directly, thereby stabilizing network training."}, {"title": "C. Task-oriented Node Learning", "content": "The TNL is rooted in assigning distinct image restoration tasks, i.e., ID, LLIE, and NHIE, to specialized processing nodes, forming different shared and separated networks. Each node is optimized to address a specific degradation type, thereby allowing the overall model to handle a diverse range of restoration tasks with greater efficiency. To further fuse enhanced features, the valuable features extracted by the ID and LLIE sub-nodes are refined and adaptively fused using learnable feature allocation parameters (a) and a self-attention mechanism (SA). It makes the NHIE sub-node with the ability to effectively handle more complex and nuanced nighttime haze scenarios. The whole progress of NHIE sub-node is summarized in Algorithm 2.\n1) Task-Specific Module: To effectively differentiate between various types of degradation and accurately restore the affected features, we propose a task-specific module (TSM), for input feature $F_e$, we can get,\n$F_t^s = f_t^s (F_e)  where t\u2208 {1,2,3}$, (3)\nwhere t is imaging degraded type, $F_t^s$ represents the output generated by selecting an appropriate processing function $f_t^s$ tailored for specific degradation tasks. The specific task types include: the ID task (t = 1), the LLIE task (t = 2), and the NHIE task (t = 3). The TSM can effectively extract features pertinent to each degradation task, thereby enhancing the model's performance across different scenarios.\n2) Self Attention-guided Node Learning Module: The node learning module employs three specific small-scale encoder-decoder paths, each optimized for ID, LLIE, and NHIE. This configuration allows each task to effectively identify and address its unique challenges. For each task t, the input feature $X_{int}$ is transformed as follows\n$F_{E}^{t} = E_{sub}(X_{int}^t) = M^t(M_1^t(X_{int}^t) \u21932)$,\n$F_{D}^{t} = D_{sub}(F_{E}^{t}) = M_2^t(M_3^t(F_{E}^{t}) \u21912)$,\nwhere $M^t$ represents the operation of MRFE module, \u21932 and \u21912 represent 2 times downsampling and upsampling, respectively. $F_{E}^{t}$ and $F_{D}^{t}$ are the outputs of the node encoder and decoder, respectively. As shown in Fig. 4, considering its more complex imaging process, we use the outputs from the LLIE-subnode and ID-subnode as auxiliary knowledge for reasoning in NHIE-subnode learning. Through both forward and backward propagation, nodes learn to optimize parameters, ensuring high accuracy and flexibility for the respective tasks. In addition, given the variability in degradation levels across different scenes, we will suggest an additional learnable parameter to adaptively fuse the features extracted by the LLIE-subnode and the ID-subnode. Therefore, for the input of NHIE-subnode, i.e., $x_{in}^{nhie}$, we can get\n$x_{in}^{nhie} = a. SA(F_{LLIE}^t, F_{t}^{st}) + (1 \u2212 a) \u00b7 SA(F_{ID}^t, F_{t}^{st})$, (6)\nwhere $F_{LLIE}^t$ and $F_{ID}^t$ are the outputs of LLIE-subnode and ID-subnode. a is a learnable parameter that is dynamically adjusted by the sigmoid function. Features from different tasks can be integrated through a fusion strategy to enhance overall performance, enabling the model to maintain efficient restoration in complex scenarios."}, {"title": "D. Multi-Receptive Field Enhancement Module", "content": "To efficiently extract multi-scale feature information while minimizing computational complexity, as shown in Fig. 3, we propose the MRFE module. By leveraging parallel depthwise separable convolution branches and advanced feature fusion mechanisms, the MRFE achieves robust feature representation with minimal computational overhead. Specifically, the MRFE module consists of three parallel depthwise separable convolution branches, each adopting different dilation rates to capture spatial information at different scales. Given an input feature map $x_m$, the output of MRFE module $F_{mrfe}$ can be given as\n$F_{mrfe} = M(x_m)$\n$= x + \u03c3(GN(Wp * [D_1(x), D_3(x), D_5(x)])),$ (7)\nwhere $D_k()$ represents the depthwise separable convolution operation with dilation rate k, GN is group normalization. By setting different dilation rates (k = 1,3,5), the three branches cover spatial ranges of 3 \u00d7 3, 7 \u00d7 7, and 11 \u00d7 11 respectively, thereby achieving multi-scale feature extraction. For efficient feature fusion, we first concatenate the output features from three branches along the channel dimension, followed by a 1 \u00d7 1 pointwise convolution for feature dimension reduction and cross-channel information interaction."}, {"title": "E. Self Attention Module", "content": "The self-attention mechanism is essential for improving feature representation by effectively extracting long-range pixel dependencies. By dynamically adjusting feature importance, the self-attention mechanism enables the model to effectively handle various erent types of image degradation. Specifically, as shown in Fig. 4, the input feature $F_D^t$ and reference feature $F_t^s$ are used to generate query Q = Convq ($F_D^t$), key K = Convk($F_t^s$), and value V = Convv($F_D^t$) vectors through convolution layers. The feature mapping allows the model to extract correlations between features on a global scale. Next, the dot product of the query and key is computed via matrix multiplication to obtain attention weights. The weights are then normalized using the softmax function. Finally, the normalized attention weights are applied to the value vector to compute the weighted sum. The result is linearly combined with the $F_D^t$ to generate attention-enhanced output, i.e.,\n$A_{att}^t = y softmax((Q^T K)V + F_D^t),$ (8)\nwhere y is a learnable parameter that adjusts the influence of the attention output. The flexible feature adjustment capability allows the model to excel in various restoration tasks by capturing global context information, effectively enhancing image clarity and detail."}, {"title": "F. Hybrid Loss Function", "content": "We propose a hybrid loss function dedicated to image quality optimization, which consists of two key components: reconstruction loss and perceptual loss.\n1) Reconstruction Loss: The reconstruction loss $L_{rec}$ strategically incorporates L1 and Charbonnier losses through a balanced integration of their complementary features. Specifically, $C_{rec}$ can be given as\n$L_{rec} = 0.5 . ||I_{re} - I_{gt}||_1 + 0.5 . \\sqrt{(I_{re} - I_{gt})^2 + \u03b5},$ (9)\nwhere $\u03b5 = 1 \u00d7 10^{-6}$ is the smoothing parameter for Charbonnier loss, $I_{re}$ represents predicted image, $I_{gt}$ represents target image. The L1 component ensures numerical stability and resilience against outliers in high-gradient regions, while the Charbonnier term contributes enhanced differentiability and precise edge preservation particularly in areas of subtle intensity variations.\n2) Perceptual Loss: The perceptual loss $L_{per}$ is computed based on multi-layer features extracted from a pre-trained VGG-16 network, measuring the mean squared error between predicted and target images in feature space, i.e.,\n$L_{per} = \\frac{1}{N}\\sum_{i=1}^{N}||\u03a6_i(I_{re}) \u2013 \u03a6_i(I_{gt})||_2,$ (10)\nwhere $\u03a6_i()$ represents the feature mapping of the i-th layer in the VGG-16 network, and N is the number of selected feature layers. In this work, we choose conv1_2, conv2_2, and conv3_3 as feature extraction layers.\n3) Total Loss: The total loss function $L_{total}$ is a weighted sum of two components, i.e.,\n$L_{total} = \u03c9_1L_{rec} + \u03c9_2L_{per},$ (11)\nwhere the weight coefficients are $\u03c9_1 = 0.8$ and $\u03c9_2 = 0.2$. This weight configuration demonstrates good balance in experiments, ensuring image reconstruction quality and visual perceptual features."}, {"title": "IV. EXPERIMENTS AND DISCUSSIONS", "content": "In this section, we conduct extensive experiments to highlight the outstanding ID, LLIE, and NHIE performance of MTOIE. We first provide an overview of the datasets and explain the implementation details. To illustrate the effectiveness of MTOIE, we then conduct both quantitative and qualitative analyses, comparing it with various state-of-the-art methods using synthetic and real-world low-visibility images. Additionally, we perform ablation studies to evaluate the contribution of each proposed module. Finally, we analyze and compare MTOIE in the context of the saliency detection task."}, {"title": "A. Datasets and Implementation Details", "content": "1) Training and Testing Datasets: The training and testing datasets include realistic single image dehazing (RESIDE)-OTS [24], which incorporates depth information, and the composite degradation dataset (CDD) [25]. Based on the imaging model in Sec. II, we extract atmospheric light values according to real conditions to synthesize more realistic degraded images [21]. By creating degraded images that closely resemble real-world conditions, we address the limitation of insufficient paired training datasets, thus enhancing the network's restoration performance. During training, paired images are segmented into 256 \u00d7 256 patches to achieve data augmentation and improve the network's generalization capability. To evaluate the inference capabilities of MTOIE, we use a variety of synthetic and real-world low-visibility images.\n2) Evaluation Metrics: To evaluate the recovery performance of different methods quantitatively, we selected a set of evaluation metrics. These include reference metrics like peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) [29]. Additionally, we also used no-reference metrics i.e., natural image quality evaluator (NIQE) [30]. Higher PSNR and SSIM values indicate better image recovery performance, while lower NIQE values suggest superior recovery. All evaluation metrics are based on the RGB channel of the images.\n3) Competitive Methods: To evaluate the effectiveness of the MToIE, we conducted quantitative and qualitative comparative analysis with competing methods, including ID methods DCP [9], FFANet [15], SDD [8], CEEF [26], ROP+ [10], DehazeFormer [16], and AoSRNet [21]; LLIE methods LIME [7], SDD [8], CEEF [26], EFINet [27], ROP+ [10], SMNet [14], and AoSRNet [21]; and NHIE methods IENHC [11], GMLC [12], MRP [13], OFSD [28], 4KDehazing [19], CEEF [26] and ECNHI [22]. To ensure fairness and objectivity in the experiment, all methods are based on the source code published by the authors, and multiple experimental comparisons are carried out under the identical conditions to minimize the impact of extraneous factors on the results.\n4) Experiment Platform: The MTOIE is trained for 200 epochs using 3183 images. The adaptive moment estimation optimizer is responsible for updating the network parameters. The initial learning rate for MToIE is set as e = 1\u00d710-3 and is reduced by a factor of 10 at the 60th, 120th, and 180th epochs. The MTOIE was trained and evaluated within the Python 3.7 environment using the PyTorch package with 2 Xeon Gold 37.5M Cache, 2.50 GHz @2.30GHz Processors and 4 Nvidia GeForce RTX 4090 GPUs."}, {"title": "B. Synthetic Degradation Analysis", "content": "1) Dehazing: Table I presents the comparative performance of MTOIE against seven ID methods on the RESIDE and CDD datasets. MToIE achieved the highest average PSNR and SSIM scores, and the second best NIQE score, demonstrating its superiority in quantitative evaluation. Furthermore, MTOIE exhibits substantial advantages in qualitative comparisons. Fig. 5 illustrates the results of various methods on synthetic hazy images. The traditional DCP algorithm enhances contrast and removes some haze but often introduces color distortions and over-dehazes. Deep learning-based methods, such as FFANet and AoSRNet, improve haze removal and detail recovery but exhibit limitations when handling distant objects in natural scenes. DehazeFormer frequently produces prominent artifacts, particularly in sky regions, while CEEF, though capable of partial restoration, suffers from local distortions and loss of detail. Notably, MToIE, despite being trained on diverse low-visibility images, demonstrates competitive performance comparable to state-of-the-art daytime ID methods, effectively addressing the challenges posed by synthetic and real-world hazy scenarios.\n2) Low-light Enhancement: Table II and Fig. 5 illustrate that MToIE significantly outperforms other methods in LLIE. Specifically, MTOIE demonstrates superior performance in image clarity and detail restoration, producing images with higher fidelity in terms of structural integrity while effectively reducing blurring and artifacts. LIME enhances visibility with noticeable improvements in color contrast and haze removal but introduces slight color distortions. SDD, while improving saturation in building areas and achieving a warmer appearance, suffers from blur and detail loss, particularly at the edges of buildings and within crowded regions. Both SMNet and EFINet exhibit over-sharpening effects, causing certain image details to appear excessively pronounced. In contrast, MTOIE generates images that closely resemble real-world scenes, with textures and colors more consistently preserved, thereby achieving a balance between enhancement and realism.\n3) Nighttime Haze Enhancement: The evaluation of MTOIE was conducted using the RESIDE and CDD datasets, consistent with those in daytime ID and LLIE tasks. As shown in Fig. 5, while IENHC and ECNHI partially remove haze, residual haze remains visible, particularly in the background regions. In contrast, MToIE cexhibits exceptional performance in color restoration, producing images with a natural tone devoid of oversaturation or color distortion, thereby enhancing both visual comfort and scene realism. In addition, MTOIE demonstrates superior detail preservation, with ground textures in the foreground and building outlines in the background clearly discernible. Quantitative evaluations in Table III further substantiate MToIE's superiority, as it achieves the best results in all metrics, underscoring its outstanding recovery performance under low-light and hazy conditions."}, {"title": "C. Real-world Degradation Analysis", "content": "To demonstrate the effectiveness of MTOIE in real-world scenes, we selected three types of degraded images for qualitative analysis. As shown in Fig. 6, we present a visual comparison of the top four methods in the quantitative comparison for each specific task. While the restoration results of other methods demonstrate varying degrees of effectiveness, they also exhibit certain limitations. ROP+ introduces significant color distortions in regions with heavy haze, while AoSRNet fails to completely remove high-density haze. LIME produces overly brightened results, resulting to unnatural appearances, and EFINet improved contrast but introduces noise in darker areas. AoSRNet also shows deficiencies in local detail enhancement, leaving some regions underexposed. IENHC fails to effectively enhance the visibility of distant objects in dense haze, while MRP introduces artifacts around light sources. Although ECNHI improves contrast, it suffers from uneven illumination. In contrast, MTOIE effectively reconstructs structural integrity in ID, enhances visibility and contrast in LLIE, and restores textures and edges with high fidelity in NHIE."}, {"title": "D. Ablation Study", "content": "To assess the effectiveness of MTOIE, we conducted an ablation study by removing the TNL and MRFE components. Additionally, we evaluated the impact of different loss functions to verify their contributions to the model's performance.\n1) Ablation Studies on TNL: To validate the effectiveness of the proposed TNL modules, we assign different image restoration tasks to NHIE sub-node to train another version of MToIE, as shown in Fig. 7, only a single sub-network was used to complete the three tasks. As shown in Table IV, compared to the full version of MToIE, the performance of ID, LLIE, and NHIE degrades when the TNL module is specified independently. This highlights the effectiveness of the proposed shared and task-specific feature extraction methods. By leveraging its distinct architecture, the TNL module is designed to enhance feature extraction and thus improve overall task performance. It integrates the fundamental features required for ID, LLIE, and NHIE, without TNL module, MTOIE exhibits poor performance in the three tasks.\n2) Ablation Studies on Network Modules: To assess the significance of the MRFE module in MToIE, we performed an ablation study by completely removing it. As shown in Table V, its removal consistently led to declines in PSNR and SSIM across all tasks. This demonstrates that the absence of the MRFE module limits the network's ability to extract essential features, resulting in reduced restoration performance.\n3) Ablation Studies on Loss Function: Each component of the hybrid loss function show in subsection III-F. To demonstrate the effectiveness of hybrid loss function, we train two versions: one using only the reconstruction loss and the other using only the perceptual loss. From Table VI, the declines in both PSNR and SSIM indicate that hybrid loss function guide the network toward improved restoration performance. Reconstruction and perceptual loss enhances restoration performance by balancing numerical stability, outlier resilience, perceptual consistency, and semantic preservation, achieving optimal results."}, {"title": "E. Improving SOD Tasks with MTOIE", "content": "We utilized U2Net [31] and show its performance on CDD dataset. Notably, we directly applied the pretrained U2Net model without further training, achieving a marked enhancement in SOD through MToIE preprocessing. As shown in Fig. 8 and Table VII, the U2Net model achieved significant improvements in MToIE-processed images, enabling more accurate detection and segmentation of salient objects. This result validates the effectiveness of MTOIE, demonstrating its practical application value in traffic scenarios under complex weather conditions. Specifically, in autonomous driving and traffic monitoring systems, it enhances the detection and segmentation performance of critical targets such as pedestrians and vehicles, thereby further improving traffic safety."}, {"title": "F. Complexity and Running Time Comparison", "content": "MTOIE adopts an efficient structural design, which can significantly improve the image recovery speed in low light and haze environments. We randomly selected 50 images from the test set and ran all models on the same hardware environment. As shown in Table VIII, the average processing time of our model is about 1.68 seconds. The results show that MTOIE has advantages in both efficiency and effectiveness, demonstrating its potential value in practical applications."}, {"title": "V. CONCLUSIONS", "content": "This paper proposes a multi-task-oriented imaging enhancer (MTOIE) to address salient object detection challenges in vision measurement systems under adverse imaging conditions. MTOIE effectively handles three types of degradation (i.e., daytime haze, low light, and nighttime haze) through a task-oriented node learning mechanism and self-attention module, while achieving efficient multi-scale feature extraction through a multi-receptive field enhancement module. Experimental results demonstrate that the proposed MToIE achieves superior performance across various weather and imaging conditions, thus providing an effective solution for image enhancement and salient object detection in challenging environments."}]}