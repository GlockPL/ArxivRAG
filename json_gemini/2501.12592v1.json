{"title": "FedGrAINS: Personalized SubGraph Federated Learning\nwith AdaptIve Neighbor Sampling", "authors": ["Emir Ceyani", "Han Xie", "Baturalp Buyukates", "Carl Yang", "Salman Avestimehr"], "abstract": "Graphs are crucial for modeling relational and biologi-\ncal data. As datasets grow larger in real-world scenar-\nios, the risk of exposing sensitive information increases,\nmaking privacy-preserving training methods like fed-\nerated learning (FL) essential to ensure data security\nand compliance with privacy regulations. Recently pro-\nposed personalized subgraph FL methods have become\nthe de-facto standard for training personalized Graph\nNeural Networks (GNNs) in a federated manner while\ndealing with the missing links across clients' subgraphs\ndue to privacy restrictions. However, personalized sub-\ngraph FL faces significant challenges due to the hetero-\ngeneity in client subgraphs, such as degree distributions\namong the nodes, which complicate federated training of\ngraph models. To address these challenges, we propose\nFedGrAINS, a novel data-adaptive and sampling-based\nregularization method for subgraph FL. FedGrAINS\nleverages generative flow networks (GFlowNets) to eval-\nuate node importance concerning clients' tasks, dy-\nnamically adjusting the message-passing step in clients'\nGNNs. This adaptation reflects task-optimized sam-\npling aligned with a trajectory balance objective. Ex-\nperimental results demonstrate that the inclusion of\nFedGrAINS as a regularizer consistently improves the\nFL performance compared to baselines that do not\nleverage such regularization.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) operate over a single\ngraph, with nodes and edges stored in a central server.\nFor example, in social networks, each user, along with\ntheir connections form a large graph of all users and\ntheir links [2]. However, parties may not send their pri-\nvate graph datasets to a central server due to privacy\nconcerns, raising the need to train GNN models over\nmultiple distributed graph datasets. For instance, hos-\npitals may maintain their patient interaction networks\nto track physical contacts or co-diagnoses of diseases.\nHowever, they cannot share these graphs with other en-\ntities due to privacy restrictions [52]. Here, the ques-\ntion is how to collaboratively train GNNs across clients'\ndistributed subgraphs without sharing the actual graph\ndata. Federated Learning (FL) of GNNs aims to solve\nthis problem so that each client individually trains a\nlocal GNN on their local data, while a central server\naggregates locally updated GNN weights from multiple\nclients into a global model using federated optimiza-\ntion [19].\nA key challenge in na\u00efvely applying FL methods to\nGNNs is the potential loss of crucial information due to\nmissing links between subgraphs distributed across par-\nties. Recent approaches to subgraph FL address this is-\nsue by extending local subgraphs with information from\nother subgraphs [47,52]. Specifically, they extend the\nlocal subgraph by either precisely adding the relevant\nnodes from other subgraphs from different clients [47] or\nby predicting the nodes using node information from the\nother subgraphs via graph mending [52]. However, shar-\ning node information can compromise data privacy and\nlead to high communication costs. While [4] mitigates\nthe issues of missing links and subgraph heterogene-\nity through personalized aggregation and local sparse\nmasks, all existing approaches are mainly based on im-\nage data modality, failing to fully exploit the graph-\nstructured data to address heterogeneity, thus resulting\nin sub-optimal solutions to personalize GNN models.\nOn the other hand, dropout-based data pruning\nin FL [21,44] and stochastic regularization in GNNs\n[17, 30] have shown to be successful in regularizing\ndeeper models without any significant overhead by ran-\ndomly dropping the network weights and/or data com-\nponents. However, existing dropout-based data pruning\napproaches in FL are not suitable for graph ML mod-"}, {"title": "2 Preliminaries", "content": "In this section, we describe GNNs and personalized\nsubgraph FL in more detail. Then, we present the core\ncomponent of our proposed algorithm, GFlowNets.\nGNNs: Let $G = (V,E)$ be an undirected graph with a\nset of $N$ nodes $V = \\{v_1,...v_N\\}$ and edges $E$. $X =$\n$\\{x_1,...,x_N\\} \\in \\mathbb{R}^{N \\times d}$, where $x_i \\in \\mathbb{R}^d$ is the node\nfeature vector for node $v_i$, indicating the features of each\nnode $i \\in \\{1, ..., N\\}$. According to the Message-Passing\nNeural Network (MPNN) paradigm [12], the flow of\nhidden representations in GNNs, based on the nodes'\nneighborhoods and features, is described as follows:\n(2.1) $H^{l+1} = \\text{UPD}^l(H^l, \\text{AGG}^l(\\{H_u^l: \\forall u \\in \\mathcal{N}(v)\\}))$,\nwhere $H_v^l$ denotes the features of node $v$ at the $l$-th\nlayer, and $\\mathcal{N}(v)$ denotes the set of neighbor nodes of\nnode $v$: $\\mathcal{N}(v) = \\{u \\mid u \\in V | (u, v) \\in E\\}$. AGG aggregates\nthe features of the neighbors of node $v$. We note\nthat AGG can be mean or even a black-box function.\nUPD updates node $v$'s representation given its previous\nrepresentation and the aggregated representations from\nits neighbors. $H^0$ is initialized as $X$.\nAs the number of layers increases, the embedding\ncomputation for a node $v$ incorporates information\nfrom neighbors several hops away. Increasing the\nnumber of layers expands the depth of neighbors for\neach node included in the training set, resulting in\nthe exponential increase in the neighborhood size for\neach node, thus causing high computation time on the\nuser side. Reducing high computation at the edge\nis possible by training GNNs in a subgraph federated\nlearning setting where each user has a part of an\nentire graph and utilizes FL to access higher-order node\ninformation. The following subsection describes the\nfederated training process for GNNs.\nPersonalized Subgraph FL: We assume that there\nexists a global graph $G_{\\text{global}} = (V_{\\text{global}},E_{\\text{global}})$, where\n$V_{\\text{global}}$ and $E_{\\text{global}}$ are the set of nodes and edges of\nthe global graph, respectively. $X_{\\text{global}}$ is the associated\nnode feature set of the global graph. In the subgraph\nFL system, we have one central server $S$ and $M$ clients\nwith distributed subgraph datasets: $D_i = \\{G_i, Y_i\\} =$\n$\\{(V_i, E_i, X_i\\}, Y_i\\}$ where $X_i$ and $Y_i$ are node feature and\nlabel sets of the $i$-th client, respectively, for $i \\in [M]$,\nwhere $V_{\\text{global}} = \\cup_{i=1}^M V_i$. For an edge $e_{v,u} \\in E_{\\text{global}}$,\nwhere $v \\in V_i, u \\in V_j$, we have $e_{v,u} \\notin E_i \\cup E_j$. That\nis, $e_{v,u}$ might exist in reality but is missing from the\nwhole system, as this link is between the local data of\ntwo distinct clients $i$ and $j$. The system exploits an\nFL framework to collaboratively learn $M$ personalized"}, {"title": "3 Personalized Subgraph Federated Learning\nwith Adaptive Neighbor Sampling", "content": "In this section, we introduce FedGrAINS, our person-\nalized subgraph FL method. We detail the design of\nGFlowNet to estimate neighborhood importance distri-\nbutions for each node (Section 3.1), the scalable sam-\npling procedure (Section 3.1.1), and the personalized\nsubgraph FL framework FedGrAINS (Section 3.2).\n3.1 Adaptive Neighborhood Distribution Esti-\nmation with GFlowNets: Given graph data, the ad-\njacency matrix $A \\in \\mathbb{R}^{N \\times N}$ allows us to determine the\nneighbors of every node $v \\in V$. At a given layer $l$,\nthe AGG operator given in (2.1) aggregates the features\nof $v$'s neighbors based on $A$ and requires higher-order\npropagation to compute neighbors' messages [15]. As a\nresult, the neighborhood size grows exponentially with\nthe number of layers, resulting in over-smoothing. It is\npossible to regularize models based on randomly drop-\nping nodes, edges, and even messages [11,40]. However,\nthese methods operate independently of the task, data,\nand classifier, leading to suboptimal performance [31].\nGuided by the intuition that the aggregation step in\nmessage-passing GNNs is crucial for good performance,\nwe aim to adaptively choose and sample the neighbors\nat every layer to minimize the classification loss using\nGFlowNets. Since every MPNN layer requires an ad-\njacency matrix to compute hidden representations, we\nsample a sequence of adjacency matrices over all layers.\nThus, for GFlowNet to sample important neighbors, we\ndefine the states $s \\in S$ as a sequence of adjacency ma-\ntrices $s = (A_0, ..., A_l)$ sampled up to the current step.\nThe action $a_l$ to construct the adjacency matrix for the\nnext layer $A_{l+1}$ is to choose $k$ nodes without replace-\nment among the neighbors of nodes in $A_l$. Then, in our\ndesign, obtaining the adjacency matrix of the next layer\nis equivalent to transitioning to a new state.\nUsing the modeling described above, in an $L$-layer\nGNN, we construct a sequence of $L$ adjacency matrices\nto reach a terminating state. We aim to perform task-\naware sampling with the optimal GFlowNet sampling\npolicy to minimize the expected classification loss :\n$R(s_L) = R(A_0,..., A_L)$\n(3.4)\n$= \\exp(-\\alpha \\cdot L (F (\\phi; G), Y))$\n$= \\exp(-\\alpha \\cdot L (F (\\phi; V, (A_0, . . ., A_L)), Y)),$\nwhere $\\alpha$ is a scaling parameter. $A_0$ is the adjacency ma-\ntrix of the graph at the starting state, and $A_1,..., A_L$\nare the adjacency matrices sampled at every layer.\nForward Probability: The GFlowNet of FedGrAINS\nis a neural network that learns the forward probabili-\nties $P_F(s_{l+1} | s_l)$, the probability of sampling an adja-\ncency matrix $A_{l+1}$ for the layer $l + 1$ given the previ-\nous layers' adjacency matrices $A_0,..., A_l$. For each\nnode $v_i$, where $i \\in 1,..., N$, the GFlowNet models\nthe probability that node $v_i$ in the neighborhood of"}, {"title": "4 Experiments", "content": "4.1 Experimental Setup\nDatasets As proposed by [4, 52], we generate dis-\ntributed subgraphs by dividing the graph datasets into\na certain number of clients so that each FL client has\na subgraph that is a part of the original graph. Specif-\nically, we use six datasets: Cora, CiteSeer, Pubmed,\nand ogbn-arxiv for citation graphs [22, 42]; Computer\nand Photo for product graphs [36,43]. We partition the\ngraphs into subgraphs using the METIS graph parti-\ntioning algorithm [25], which allows specifying the num-\nber of subsets without requiring further merging, unlike\nLouvain partitioning [8]. METIS first coarsens the orig-\ninal graph using maximal matching methods [26], then\ncomputes a minimum edge-cut partition on the coars-\nened graph followed by the projection of the partitioned\ncoarsened graph back onto the original graph. In our ex-\nperiments, we consider two subgraph settings: Disjoint\nsubgraphs setting, in which each client has a unique set\nof nodes and no overlapping nodes between subgraphs.\nIn this setup, we use the METIS outputs as they pro-\nvide non-overlapping partitions. We consider 5, 10, or\n20 clients for the disjoint setting. In the overlapping sub-\ngraphs setting, clients may have common nodes between\ntheir datasets. Since METIS produces non-overlapping\npartitions, we introduce overlap by randomly sampling\nsubgraphs multiple times from the partitioned graph.\nFirst, we divide the original graph into 2, 6, or 10 dis-\njoint subgraphs using METIS. Then, for each METIS\npartition, we randomly sample half of the nodes and\ntheir associated edges five times to create subgraphs.\nThis process results in 10, 30, or 50 clients for the over-\nlapping case, generating shared nodes across subgraphs\nfrom the same METIS partition. It is important to note\nthat the number of clients in our proposed settings is not\nuniform due to the need for overlapping nodes. For each\nsubgraph, except for the ogbn-arxiv dataset, train, vali-\ndation, and test node random sampling ratios are 20%,\n40%, and 40%, respectively. Thus, for the ogbn-arxiv\ndataset, train, validation, and test node random sam-\npling ratios are 5%, 47.5%, and 47.5%, respectively, as\nthe ogbn-arxiv dataset has a significantly higher number\nof nodes, as in Table 4."}, {"title": "4.2 Experimental Results", "content": "Main Results Tables 1 and 2 show node classification\nresults in disjoint and overlapping subgraph scenarios,\nrespectively. The primary motivation behind the pro-\nposed FedGrAINS is its applicability to existing sub-\ngraph FL algorithms. For a more fair comparison with\nFed-PUB [4], we combine our data-adaptive regulariza-\ntion framework with their personalized aggregation and\nsparse masking. Other baselines do not include person-\nalized aggregation. In addition, we did not test how the\ninclusion of our proposed framework affects the perfor-\nmance of FedSage+ as our proposed framework tries to\nfilter unimportant nodes. In contrast, FedSage+ tries to\nincrease the number of neighbors with fake neighbors,\nproviding a more complicated setup for us to observe the\neffect of only learnable node filtering. Based on the ex-\nperimental results, we make the following observations:\nFirst, regardless of the FL algorithm, classification per-\nformance decreases when the number of clients increases\ndue to the increased number of partitions creating more\nmissing links between the clients (See Appendix B.1.1\nfor the missing links counts for each dataset). The in-\ncreasing number of missing links also leads to severe\nnode-degree heterogeneity (see Appendix B.1.2 )within\nclients. This results in more challenging local training\nand collaboratively learning generalizable models with\nother clients.\nSecond, the disjoint setting is more challenging than\nthe overlapping one for the same number of clients be-\ncause the subgraphs in the non-overlapping setting are\nentirely disjoint and more heterogeneous than the over-\nlapped case. Moreover, the non-overlapping setting has\nfewer nodes to learn from due to the overlap generation\nscheme explained in the experimental design. As shown\nin Table 1, FedGrAINS consistently outperforms all ex-\nisting baselines in this challenging scenario, demonstrat-\ning its efficacy.\nFinally, the inclusion of FedGrAINS as a personal-\nization method consistently enhances the performance\nof all baselines. Specifically, the GFlowNet-based im-\nportance sampler improves the accuracy of all algo-\nrithms (except FedSage+) by at least 1%. Addition-\nally, the advantages of FedGrAINS are most evident\nwhen comparing the results of FedAvg with our person-\nalization method to those of FedPer [3], GCFL+ [49],\nand FedSage+ [52]. Moreover, the performance of both\nFedSage+ and FedAvg with FedGrAINS are very close\nfor Cora, CiteSeer, and PubMed datasets. However,\nour personalized FL framework demonstrates signifi-\ncantly better performance for the Amazon-Computer,\nAmazon-Photo, and ogbn-arxiv datasets. We attribute\nthis success to the size of the graph dataset, which re-\nsulted in a higher average degree than citation networks.\nIn comparison with FED-PUB [4], FED-PUB re-\nmains superior due to its use of personalized aggre-\ngation. To further assess the impact of FedGrAINS,\nwe evaluate its performance when integrated with base\nalgorithms that also employ personalized aggregation,\nsuch as FED-PUB, i.e., FED-PUB + FedGrAINS. Our\nresults show that incorporating the importance sampler\nconsistently and significantly enhances FED-PUB per-\nformance. Therefore, we can conclude that FedGrAINS\ncan be seamlessly integrated into most subgraph FL al-\ngorithms without incurring additional communication\noverhead or compromising privacy.\nHyperparameter Analysis on the GFlowNet To\nunderstand the operation regime of our personaliza-\ntion method, we vary important hyperparameters for\nthe GFlowNet-based sampler: the learning rate of the\nGFlowNet, $\\beta_{\\text{GFN}}$, and the reward scaling factor $\\alpha$. We\nuse the log uniform distribution to sample the afore-\nmentioned hyperparameters with the values from the\nfollowing ranges respectively, [1e-6, 1e\u22122] and [1e\u00b2, 1e6].\nThe setting is on the Cora dataset with the overlapping\nnode scenario, where we set the number of local epochs\nto 1 and the number of clients to 10. The results are\nshown in Table 3. According to Table 3, the relation-\nship between the performance and the reward scaling\nfactor $\\alpha$ is highly non-linear. In addition, the learning\nrate $\\beta_{\\text{GFN}}$ is not as effective as the scaling factor $\\alpha$."}, {"title": "5 Related Works", "content": "GNNs and Random dropping GNNs: GNNs are\nthe de-facto tools to learn the representations of nodes,\nedges, and entire graphs [5, 15, 24, 48, 53]. Most exist-\ning GNNs fall under the message-passing neural net-\nwork framework [12], which iteratively represents a node\nrepresentation by aggregating features from its neigh-\nboring nodes and itself [14, 16, 28]. However, the cur-\nrent paradigm for GNNs is restrictive regarding the de-"}, {"title": "6 Conclusion & Future Work", "content": "This work addresses the challenges of node-degree\nheterogeneity and local overfitting in subgraph FL\nby proposing FedGrAINS, an adaptive neighborhood\nsampling-based regularization method. For each client,\nFedGrAINS employs a GFlowNet model to assess the\nnode-importance to maximize the node classification\nperformance. On the server side, these node importance\ndistributions are aggregated in a personalized manner at\nevery round, enabling adaptive generalization. By lever-\naging a generative and data-driven sampling approach,\nFedGrAINS ensures consistent performance, effectively\nhandling missing links and subgraph heterogeneity in\nan intuitive yet elegant manner. Our experimental re-\nsults demonstrate that incorporating FedGrAINS as a\nregularization method significantly improves the per-\nformance and generalization of the FL subgraph tech-\nniques.\nWe plan to extend our personalization framework\nto other federated graph learning settings [9, 20, 51].\nAs long as we have access to a tractable reward [6],\nwe can leverage the proposed FedGrAINS. We assumed\nthat $Z(s_0)$ is constant to reduce the workload at the\nedge. We defer the investigation of how the estimation\nof $Z(s_0)$ might enhance personalization to future work\n[38]. Finally, we would like to obtain theoretical\ninsights into our framework using the tools presented\nin [17], evaluating how effectively GFlowNet alleviates\nthe oversmoothing and over-fitting tendencies of client\nGNNs while allowing uncertainty quantification at the\nedge at no additional cost."}, {"title": "7 Acknowledgements", "content": "We thank anonymous reviewers for their constructive\nfeedback. Emir Ceyani is grateful for the continuous\nsupport of coauthors, his family, and housemates and for\nSaurav Prakash's discussions on the use of GFlowNets."}]}