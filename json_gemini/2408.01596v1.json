{"title": "Trustworthy Machine Learning under Social and Adversarial Data Sources", "authors": ["Han Shao"], "abstract": "Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors. These behaviors may have a notable impact on the behavior and performance of machine learning systems. Specifically, during these interactions, data may be generated by strategic individuals, collected by self-interested data collectors, possibly poisoned by adversarial attackers, and used to create predictors, models, and policies satisfying multiple objectives. As a result, the machine learning systems' outputs might degrade, such as the susceptibility of deep neural networks to adversarial examples (Shafahi et al., 2018; Szegedy et al., 2013) and the diminished performance of classic algorithms in the presence of strategic individuals (Ahmadi et al., 2021). Addressing these challenges is imperative for the success of machine learning in societal settings.\nThis thesis is organized into two parts: learning under social data sources and adversarial data sources. For social data sources, we consider problems including: (1) learning with strategic individuals for both finite and infinite hypothesis classes, where we provide an understanding of learnability in both online and PAC strategic settings, (2) incentives and defections of self- interested data collectors in single-round federated learning, multi-round federated learning, and collaborative active learning, (3) learning within games, in which one of players runs a learning algorithm instead of best responding, and (4) multi-objective learning in both decision making and online learning. For adversarial data sources, we study problems including: (1) robust learning under clean-label attacks, where the attacker injects a set of correctly labeled points into the training set to mislead the learner into making mistakes at targeted test points, and (2) learning under transformation invariances and analyzing the popular method of data augmentation.", "sections": [{"title": "1. Introduction", "content": "Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors that may significantly impact the performance of machine learning systems.\nStrategic Individuals Across many domains, machine learning is applied to inform decisions about applicants for a variety of resources. However, when individuals have incentives to benefit from specific predictive outcomes, they may act to obtain favorable predictions by modifying their features. Since this can harm predictive performance, learning becomes susceptible to a classic principle in financial policy-making known as Goodhart's law, which states, \"If a measure becomes the public's goal, it is no longer a good measure.\" This natural tension between learning systems and those to whom the system is applied is widespread, spanning loan approvals, university admissions, job hiring, and insurance. In these scenarios, learning systems aim for accurate predictions, while individuals, regardless of their true labels, have an incentive to be classified positively. For instance, in college admissions, applicants might retake the SAT or take easier courses to boost their GPA to fool the classifier.\nSelf-Interested Data Collectors In many real-world applications, datasets are distributed across different silos, such as hospitals, schools, and banks, necessitating collaborations among them. In recent years, collaborative learning, such as federated learning, has been embraced as an approach for facilitating collaboration across large populations of data collectors. However, what will ultimately determine the success and impact of collaborative learning is the ability to recruit and retain large numbers of data collectors. There is an inherent tension between the collaborative learning protocol and the data collectors. The learning protocol aims to find a model that is beneficial for all data collectors, while each data collector's goal is to find a model that is good for their own local data with minimal data contribution. Consequently, if the learning protocol requires data collectors to provide more data than necessary to fulfill their own objectives, they will not contribute as the protocol requires.\nMulti-Objective Users While machine learning problems typically involve optimizing a single scalar reward, there are many domains where it is desirable or necessary to optimize over multiple (potentially conflicting) objectives simultaneously. For example, safety, speed, and comfort are all desired objectives for autonomous car users, but speed could negatively impact safety (e.g., taking longer for the vehicle to stop suddenly) or comfort (e.g., causing discomfort during fast turns). Consequently, when a learning system optimizes a scalar loss, it might overlook these multiple objectives and produce an unsatisfactory model or policy for users. Additionally, there could be more than one stakeholder in the learning process, each with a different objective. Focusing solely on one objective might lead to drastic performance drops for the others."}, {"title": "Adversarial Attackers", "content": "Adversarial attacks play a significant role in exposing the vulnerabilities of machine learning systems. Many popular models lack robustness in real-world scenarios. For example, in image tasks, adding imperceptible noise to training images (Szegedy et al., 2013) or poisoning the training set with additional images (Shafahi et al., 2018) can severely compromise the performance of deep neural networks.\nDue to these social and adversarial data factors, the outputs of machine learning systems might degrade. Addressing these challenges is imperative for the success of machine learning.\nThis thesis makes contributions that address, from a theoretical perspective, the trustworthiness challenges arising from these social and adversarial data factors. Such data factors have not been well modeled by the existing theory. Hence, this thesis focuses on modeling the social and adversarial aspects inherent in machine learning interactions, analyzing their impact on predictors, and developing methods and insights to enhance performance.\nThe central theme of this thesis is to\ndevelop the theoretical foundations for trustworthy machine learning under social and adversarial data sources."}, {"title": "1.1. Overview of Thesis Contributions and Structure", "content": "This thesis is organized in two parts: learning under social and adversarial data sources.\nLearning under Social Data Sources\nWithin large interactive systems, humans frequently demonstrate strategic behaviors, including strategic manipulations by individuals and defections by self-interested data collectors, or have a variety of different objectives.\nChapters 2 and 3: Learning with Strategic Individuals. In these two chapters, we consider the problem of strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive.\nIn Chapter 2, we start by considering learning a finite hypothesis class $H$. It is well-known that in the standard (non-strategic) realizable online learning, the mistake bound of $\\log(|H|)$ can be achieved by the Halving algorithm while in the standard PAC learning, the sample complexity of $O(\\frac{\\log(|H|)}{\\epsilon})$ can be achieved by the empirical risk minimizer (ERM). Then we have the following question: under realizability in the strategic setting, can we also achieve a logarithmic dependency on $|H|$ in the strategic setting?\nAs the problem reduces to a standard learning problem when manipulation abilities are known, we focus on the scenario where the manipulation abilities are unknown. We show that in the case of ball manipulations, when the original feature vector is revealed prior to choosing the implementation, we can achieve logarithmic mistake bound and sample complexity via a variant of Halving. When the original feature vector is not revealed beforehand, the problem becomes significantly more challenging. Specifically, any learner will experience a mistake bound that scales linearly with $|H|$, and any proper learner will face sample complexity that also scales"}, {"title": "2. Strategic Classification for Finite Hypothesis Class", "content": "2.1. Introduction\nStrategic classification addresses the problem of learning a classifier robust to manipulation and gaming by self-interested agents (Hardt et al., 2016). For example, given a classifier determining loan approval based on credit scores, applicants could open or close credit cards and bank accounts to increase their credit scores. In the case of a college admission classifier, students may try to take easier classes to improve their GPA, retake the SAT or change schools in an effort to be admitted. In both cases, such manipulations do not change their true qualifications. Recently, a collection of papers has studied strategic classification in both the online setting where examples are chosen by an adversary in a sequential manner (Ahmadi et al., 2021, 2023; Y. Chen et al., 2020; Dong et al., 2018), and the distributional setting where the examples are drawn from an underlying data distribution (Hardt et al., 2016; Lechner & Urner, 2022; Sundaram et al., 2021; H. Zhang & Conitzer, 2021). Most existing works assume that manipulation ability is uniform across all agents or is known to the learner. However, in reality, this may not always be the case. For instance, low-income students may have a lower ability to manipulate the system compared to their wealthier peers due to factors such as the high costs of retaking the SAT or enrolling in additional classes, as well as facing more barriers to accessing information about college (Milli et al., 2019) and it is impossible for the learner to know the highest achievable GPA or the maximum number of times a student may retake the SAT due to external factors such as socio-economic background and personal circumstances.\nWe characterize the manipulation of an agent by a set of alternative feature vectors that she can modify her original feature vector to, which we refer to as the manipulation set. Ball manipulations are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. For example, Y. Chen et al. (2020), Dong et al. (2018), and Sundaram et al. (2021) studied ball manipulations with distance function being some norm and Ahmadi et al. (2023), Lechner and Urner (2022), and H. Zhang and Conitzer (2021) studied a manipulation graph setting, which can be viewed as ball manipulation w.r.t. the graph distance on a predefined known graph.\nIn the online learning setting, the strategic agents come sequentially and try to game the current classifier. Following previous work, we model the learning process as a repeated Stackelberg game over $T$ time steps. In round $t$, the learner proposes a classifier $f_t$ and then the agent, with a manipulation set (unknown to the learner), manipulates her feature in an effort to receive positive prediction from $f_t$. There are several settings based on what and when the information is revealed about the original feature vector and the manipulated feature vector in the game. The simplest setting for the learner is observing the original feature vector $x_t$ before choosing $f_t$ and the manipulated vector $\\Delta_t$ after. In a slightly harder setting, the learner observes both the original and manipulated vectors after selecting $f_t$. An even harder setting involves observing only the manipulated feature vector after selecting $f_t$. The hardest and least informative scenario occurs"}, {"title": "2.2. Model", "content": "Strategic classification We consider the binary classification task. Let $\\mathcal{X}$ denote the feature vector space, $\\mathcal{Y} = \\{+1,-1\\}$ denote the label space, and $\\mathcal{H} \\subseteq \\mathcal{Y}^{\\mathcal{X}}$ denote the hypothesis class. In the strategic setting, instead of an example being a pair $(x, y)$, an example, or agent, is a triple $(x, u, y)$ where $x \\in \\mathcal{X}$ is the original feature vector, $y \\in \\mathcal{Y}$ is the label, and $u \\subseteq \\mathcal{X}$ is the manipulation set, which is a set of feature vectors that the agent can modify their original feature vector $x$ to. In particular, given a hypothesis $h \\in \\mathcal{Y}^{\\mathcal{X}}$, the agent will try to manipulate her feature vector $x$ to another feature vector $x'$ within $u$ in order to receive a positive prediction from $h$. The manipulation set $u$ is unknown to the learner. In this work, we will be considering several settings based on what the information is revealed to the learner, including both the original/manipulated feature vectors, the manipulated feature vector only, or neither, and when the information is revealed.\nMore formally, for agent $(x, u, y)$, given a predictor $h$, if $h(x) = -1$ and her manipulation set overlaps the positive region by $h$, i.e., $u \\cap \\mathcal{X}_{h,+} \\neq \\emptyset$ with $\\mathcal{X}_{h,+} := \\{x \\in \\mathcal{X}|h(x) = +1\\}$, the agent will manipulate $x$ to $\\Delta(x, h, u) \\in u \\cap \\mathcal{X}_{h,+}$ to receive positive prediction by $h$. Otherwise, the agent will do nothing and maintain her feature vector at $x$, i.e., $\\Delta(x, h, u) = x$. We call $\\Delta(x, h, u)$ the manipulated feature vector of agent $(x, u, y)$ under predictor $h$.\nA general and fundamental type of manipulations is ball manipulations, where agents can manipulate their feature within a ball of personalized radius. More specifically, given a metric $d$ over $\\mathcal{X}$, the manipulation set is a ball $B(x; r) = \\{x'|d(x, x') \\leq r\\}$ centered at $x$ with radius $r$ for some $r \\in \\mathbb{R}_{\\geq 0}$. Note that we allow different agents to have different manipulation power and the radius can vary over agents. Let $\\mathcal{Q}$ denote the set of allowed pairs $(x, u)$, which we refer to as the feature-manipulation set space. For ball manipulations, we have $\\mathcal{Q} = \\{(x, B(x;r))|x \\in \\mathcal{X}, r \\in \\mathbb{R}_{>0}\\}$ for some known metric $d$ over $\\mathcal{X}$. In the context of ball manipulations, we use $(x, r, y)$ to represent $(x, B(x; r), y)$ and $\\Delta(x, h, r)$ to represent $\\Delta(x, h, B(x; r))$ for notation simplicity.\nFor any hypothesis $h$, let the strategic loss $l^{str}(h, (x, u, y))$ of $h$ be defined as the loss at the manipulated feature, i.e., $l^{str}(h, (x, u, y)) := 1(h(\\Delta(x, h, u)) \\neq y)$. According to our definition of $\\Delta(\\cdot)$, we can write down the strategic loss explicitly as\n\\[l^{str}(h, (x, u, y)) =\\begin{cases}1 & \\text{if } y = -1, h(x) = +1 \\\\ 1 & \\text{if } y = -1, h(x) = -1 \\text{ and } u \\cap \\mathcal{X}_{h,+} \\neq \\emptyset, \\\\ 1 & \\text{if } y = +1, h(x) = -1 \\text{ and } u \\cap \\mathcal{X}_{h,+} = \\emptyset, \\\\ 0 & \\text{otherwise}.\\end{cases} \\quad(2.1)\\]\nFor any randomized predictor $p$ (a distribution over hypotheses), the strategic behavior de- pends on the realization of the predictor and the strategic loss of $p$ is $l^{str}(p, (x,u,y)) := \\mathbb{E}_{h\\sim p} [l^{str}(h, (x,u,y))]$.\nOnline learning We consider the task of sequential classification where the learner aims to classify a sequence of agents $(x_1, u_1, y_1), (x_2, u_2, y_2), ..., (x_T, u_T, y_T) \\in \\mathcal{Q} \\times \\mathcal{Y}$ that arrives in an online manner. At each round, the learner feeds a predictor to the environment and then observes his prediction $\\hat{y}_t$, the true label $y_t$ and possibly along with some additional information about the original/manipulated feature vectors. We say the learner makes a mistake at round $t$"}]}