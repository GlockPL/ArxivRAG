{"title": "Trustworthy Machine Learning under Social and Adversarial Data Sources", "authors": ["Han Shao"], "abstract": "Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors. These behaviors may have a notable impact on the behavior and performance of machine learning systems. Specifically, during these interactions, data may be generated by strategic individuals, collected by self-interested data collectors, possibly poisoned by adversarial attackers, and used to create predictors, models, and policies satisfying multiple objectives. As a result, the machine learning systems' outputs might degrade, such as the susceptibility of deep neural networks to adversarial examples (Shafahi et al., 2018; Szegedy et al., 2013) and the diminished performance of classic algorithms in the presence of strategic individuals (Ahmadi et al., 2021). Addressing these challenges is imperative for the success of machine learning in societal settings.\nThis thesis is organized into two parts: learning under social data sources and adversarial data sources. For social data sources, we consider problems including: (1) learning with strategic individuals for both finite and infinite hypothesis classes, where we provide an understanding of learnability in both online and PAC strategic settings, (2) incentives and defections of self-interested data collectors in single-round federated learning, multi-round federated learning, and collaborative active learning, (3) learning within games, in which one of players runs a learning algorithm instead of best responding, and (4) multi-objective learning in both decision making and online learning. For adversarial data sources, we study problems including: (1) robust learning under clean-label attacks, where the attacker injects a set of correctly labeled points into the training set to mislead the learner into making mistakes at targeted test points, and (2) learning under transformation invariances and analyzing the popular method of data augmentation.", "sections": [{"title": "1. Introduction", "content": "Machine learning has witnessed remarkable breakthroughs in recent years. As machine learning permeates various aspects of daily life, individuals and organizations increasingly interact with these systems, exhibiting a wide range of social and adversarial behaviors that may significantly impact the performance of machine learning systems.\nStrategic Individuals Across many domains, machine learning is applied to inform decisions about applicants for a variety of resources. However, when individuals have incentives to benefit from specific predictive outcomes, they may act to obtain favorable predictions by modifying their features. Since this can harm predictive performance, learning becomes susceptible to a classic principle in financial policy-making known as Goodhart's law, which states, \"If a measure becomes the public's goal, it is no longer a good measure.\" This natural tension between learning systems and those to whom the system is applied is widespread, spanning loan approvals, university admissions, job hiring, and insurance. In these scenarios, learning systems aim for accurate predictions, while individuals, regardless of their true labels, have an incentive to be classified positively. For instance, in college admissions, applicants might retake the SAT or take easier courses to boost their GPA to fool the classifier.\nSelf-Interested Data Collectors In many real-world applications, datasets are distributed across different silos, such as hospitals, schools, and banks, necessitating collaborations among them. In recent years, collaborative learning, such as federated learning, has been embraced as an approach for facilitating collaboration across large populations of data collectors. However, what will ultimately determine the success and impact of collaborative learning is the ability to recruit and retain large numbers of data collectors a feat that requires collaborative algorithms to\nhelp agents accomplish their learning objectives while \u201cequitably\" spreading the data contribution responsibilities among agents who want a lower sample collection burden.\nConsequently, if the learning protocol requires data collectors to provide more data than necessary to fulfill their own objectives, they will not contribute as the protocol requires.\nMulti-Objective Users While machine learning problems typically involve optimizing a single scalar reward, there are many domains where it is desirable or necessary to optimize over multiple (potentially conflicting) objectives simultaneously. For example, safety, speed, and comfort are all desired objectives for autonomous car users, but speed could negatively impact safety (e.g., taking longer for the vehicle to stop suddenly) or comfort (e.g., causing discomfort during fast turns). Consequently, when a learning system optimizes a scalar loss, it might overlook these multiple objectives and produce an unsatisfactory model or policy for users. Additionally, there could be more than one stakeholder in the learning process, each with a different objective. Focusing solely on one objective might lead to drastic performance drops for the others.\nAdversarial Attackers Adversarial attacks play a significant role in exposing the vulnerabilities of machine learning systems. Many popular models lack robustness in real-world scenarios. For example, in image tasks, adding imperceptible noise to training images (Szegedy et al., 2013) or poisoning the training set with additional images (Shafahi et al., 2018) can severely compromise the performance of deep neural networks.\nDue to these social and adversarial data factors, the outputs of machine learning systems might degrade. Addressing these challenges is imperative for the success of machine learning.\nThis thesis makes contributions that address, from a theoretical perspective, the trustworthiness challenges arising from these social and adversarial data factors. Such data factors have not been well modeled by the existing theory. Hence, this thesis focuses on modeling the social and adversarial aspects inherent in machine learning interactions, analyzing their impact on predictors, and developing methods and insights to enhance performance.\nThe central theme of this thesis is to\ndevelop the theoretical foundations for trustworthy machine learning under social and adversarial data sources."}, {"title": "1.1. Overview of Thesis Contributions and Structure", "content": "This thesis is organized in two parts: learning under social and adversarial data sources.\nLearning under Social Data Sources\nWithin large interactive systems, humans frequently demonstrate strategic behaviors, including strategic manipulations by individuals and defections by self-interested data collectors, or have a variety of different objectives.\nChapters 2 and 3: Learning with Strategic Individuals. In these two chapters, we consider the problem of strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive.\nIn Chapter 2, we start by considering learning a finite hypothesis class H. It is well-known that in the standard (non-strategic) realizable online learning, the mistake bound of log(|H|) can be achieved by the Halving algorithm while in the standard PAC learning, the sample complexity of O(log(|H|)) can be achieved by the empirical risk minimizer (ERM). Then we have the following question: under realizability in the strategic setting, can we also achieve a logarithmic dependency on |H| in the strategic setting?\nAs the problem reduces to a standard learning problem when manipulation abilities are known, we focus on the scenario where the manipulation abilities are unknown. We show that in the case of ball manipulations, when the original feature vector is revealed prior to choosing the implementation, we can achieve logarithmic mistake bound and sample complexity via a variant of Halving. When the original feature vector is not revealed beforehand, the problem becomes significantly more challenging. Specifically, any learner will experience a mistake bound that scales linearly with |H|, and any proper learner will face sample complexity that also scales"}, {"title": "2.2. Model", "content": "Strategic classification We consider the binary classification task. Let $\\mathcal{X}$ denote the feature vector space, $\\mathcal{Y} = \\{+1,-1\\}$ denote the label space, and $\\mathcal{H} \\subseteq \\mathcal{Y}^{\\mathcal{X}}$ denote the hypothesis class. In the strategic setting, instead of an example being a pair $(x, y)$, an example, or agent, is a triple $(x, \\mathcal{U}, y)$ where $x \\in \\mathcal{X}$ is the original feature vector, $y \\in \\mathcal{Y}$ is the label, and $\\mathcal{U} \\subseteq \\mathcal{X}$ is the manipulation set, which is a set of feature vectors that the agent can modify their original feature vector x to. In particular, given a hypothesis $h \\in \\mathcal{Y}^{\\mathcal{X}}$, the agent will try to manipulate her feature vector $x$ to another feature vector $x'$ within $\\mathcal{U}$ in order to receive a positive prediction from h. The manipulation set $\\mathcal{U}$ is unknown to the learner. In this work, we will be considering several settings based on what the information is revealed to the learner, including both the original/manipulated feature vectors, the manipulated feature vector only, or neither, and when the information is revealed.\nMore formally, for agent $(x, \\mathcal{U}, y)$, given a predictor h, if $h(x) = -1$ and her manipulation set overlaps the positive region by h, i.e., $\\mathcal{U} \\cap \\mathcal{X}_{h,+} \\neq \\emptyset$ with $\\mathcal{X}_{h,+} := \\{x \\in \\mathcal{X} | h(x) = +1\\}$, the agent will manipulate x to $\\Delta(x, h, \\mathcal{U}) \\in \\mathcal{U} \\cap \\mathcal{X}_{h,+}^1$ to receive positive prediction by h. Otherwise, the agent will do nothing and maintain her feature vector at x, i.e., $\\Delta(x, h, \\mathcal{U}) = x$. We call $\\Delta(x, h, \\mathcal{U})$ the manipulated feature vector of agent $(x, \\mathcal{U}, y)$ under predictor h.\nA general and fundamental type of manipulations is ball manipulations, where agents can manipulate their feature within a ball of personalized radius. More specifically, given a metric d over $\\mathcal{X}$, the manipulation set is a ball $B(x; r) = \\{x' | d(x, x') \\leq r\\}$ centered at x with radius r for somer \u2208 R\u22650. Note that we allow different agents to have different manipulation power and the radius can vary over agents. Let $\\mathcal{Q}$ denote the set of allowed pairs $(x, \\mathcal{U})$, which we refer to as the feature-manipulation set space. For ball manipulations, we have $\\mathcal{Q} = \\{(x, B(x;r)) | x \\in \\mathcal{X}, r \\in \\mathbb{R}_{>0}\\}$ for some known metric d over $\\mathcal{X}$. In the context of ball manipulations, we use $(x, r, y)$ to represent $(x, B(x; r), y)$ and $\\Delta(x, h, r)$ to represent $\\Delta(x, h, B(x; r))$ for notation simplicity.\nFor any hypothesis h, let the strategic loss $\\ell^{str}(h, (x, \\mathcal{U}, y))$ of h be defined as the loss at the manipulated feature, i.e., $\\ell^{str}(h, (x, \\mathcal{U}, y)) := 1(h(\\Delta(x, h, \\mathcal{U})) \\neq y)$. According to our definition of $\\Delta(\\cdot)$, we can write down the strategic loss explicitly as\n\\begin{equation}\\ell^{str}(h, (x, \\mathcal{U}, y)) = \\begin{cases}1 & \\text{if } y = -1, h(x) = +1\\\\1 & \\text{if } y = -1, h(x) = -1 \\text{ and } \\mathcal{U} \\cap \\mathcal{X}_{h,+} \\neq \\emptyset,\\\\1 & \\text{if } y = +1, h(x) = -1 \\text{ and } \\mathcal{U} \\cap \\mathcal{X}_{h,+} = \\emptyset, \\\\0 & \\text{otherwise}.\\end{cases}\\tag{2.1}\\end{equation}\nFor any randomized predictor p (a distribution over hypotheses), the strategic behavior de-pends on the realization of the predictor and the strategic loss of p is $\\ell^{str}(p, (x,\\mathcal{U},y)) := \\mathbb{E}_{h \\sim p} [\\ell^{str}(h, (x,\\mathcal{U},y))]$.\nOnline learning We consider the task of sequential classification where the learner aims to classify a sequence of agents $(x_1, \\mathcal{U}_1, y_1), (x_2, \\mathcal{U}_2, y_2), ..., (x_T, \\mathcal{U}_T, y_T) \\in \\mathcal{Q} \\times \\mathcal{Y}$ that arrives in an online manner. At each round, the learner feeds a predictor to the environment and then observes his prediction $\\hat{y}_t$, the true label $y_t$ and possibly along with some additional information about the original/manipulated feature vectors. We say the learner makes a mistake at round t"}, {"title": "2.4. Ball manipulations", "content": "In ball manipulations, when $B(x; r) \\cap \\mathcal{X}_{h,+}$ has multiple elements, the agent will always break ties by selecting the one closest to x, i.e., $\\Delta(x, h, r) = \\arg \\min_{x' \\in B(x;r) \\cap \\mathcal{X}_{h,+}} d(x, x')$. In round t, the learner deploys predictor $f_t$, and once he knows $x_t$ and $\\hat{y}_t$, he can calculate $\\Delta_t$ himself without"}, {"title": "3. Strategic Classification for Infinite Hypothesis Class", "content": "In the previous chapter, we discuss strategic classification for finite hypothesis class, i.e., |H| < \u221e. In this chapter, we consider infinite hypothesis class, |H| = \u221e."}, {"title": "3.1. Introduction", "content": "When modeling strategic classification, the manipulation power of the agents is considered limited. In this model, each agent has a feature vector x, represented as a node in a graph, and a binary label, y. An arc from x to x' indicates that an agent with the feature vector x can adjust their feature vector to x\u2019\u2014essentially, an arc denotes a potential manipulation. Similar to the assumptions made in Ahmadi et al., 2023, we posit that the manipulation graph has a bounded degree, k. This constraint adds a realistic dimension to the model, reflecting the limited manipulation capabilities of the agents within the strategic classification framework. In addition, as we discuss in Theorems B.2 and B.4, this bounded degree constraint is also necessary in the sense that relaxing it leads to impossibility results for extremely easy-to-learn hypothesis classes.\nIn this chapter, we aim to address a fundamental question:\nDoes learnability imply strategic learnability?\nWe study the above question within both PAC and online learning frameworks, addressing it across both realizable and agnostic contexts. In each case, we analyze various levels of information received by the learner as feedback.\nWe start with a fully informative setting. Here, the learner has full information of both the manipulation graph as well as the pre- and post-manipulation features: at each time step the learner (1) observes the pre-manipulation features of the current agent, (2) selects and implements a hypothesis (according to which the agent manipulates), and (3) observes both the manipulated features and the true label of the agent."}, {"title": "4. Incentives in Single-Round Federated Learning", "content": "In recent years, federated learning has been embraced as an approach for enabling large numbers of learning agents to collaboratively accomplish their goals using collectively fewer resources, such as smaller data sets. Indeed, collaborative protocols are starting to be used across networks of hospitals (Powell, 2019a; Wen et al., 2019) and devices (B. McMahan & Ramage, 2017) and are behind important breakthroughs such as understanding the biological mechanisms underlying schizophrenia in a large scale collaboration of more than 100 agencies (Bergen & Petryshen, 2012).\nThis promise of creating large scale impact from mass participation has led to federated learning receiving substantial interest in the machine learning research community, and has resulted in faster and more communication-efficient collaborative systems. But, what will ultimately decide the success and impact of collaborative federated learning is the ability to recruit and retain large numbers of learning agents a feat that requires collaborative algorithms to\nhelp agents accomplish their learning objectives while \u201cequitably\" spreading the data contribution responsibilities among agents who want a lower sample collection burden.\nThis is to avoid the following inequitable circumstances that may otherwise arise in collaborative learning. First, when part of an agent's data is exclusively used to accomplish another agent's learning goals; for example, if an agent's learning task can be accomplished even when she (unilaterally) lowers her data contribution. Second, when an agent envies another agent; for example, if an agent's learning goal can be accomplished even when she swaps her contribution burden with another agent who has a lower burden.\nIn this paper, we introduce the first comprehensive game theoretic framework for collaborative federated learning in the presence of agents who are interested in accomplishing their learning objectives while keeping their individual sample collection burden low. Our framework introduces two notions of equilibria that avoid the aforementioned inequities. First, analogous to the concept of Nash equilibrium (Nash, 1951), our stable equilibrium requires that no agent could unilaterally reduce her data contribution responsibility and still accomplish her learning objective. Second, inspired by the concept of envy-free allocations (Foley, 1967; Varian, 1974), our envy free equilibrium requires that no agent could swap her data contribution with an agent with lower contribution level and still accomplish her learning objective. In addition to capturing what is deemed as an \u201cequitable\u201d collaboration to agents, using stable and envy-free equilibria is essential for keeping learning participants fully engaged in ongoing collaborations.\nOur framework is especially useful for analyzing how the sample complexity of federated learning may be affected by the agents' desire to keep their individual sample complexities low. To demonstrate this, we work with three classes as running examples of agent learning"}, {"title": "5. Incentives in Multi-Round Federated Learning", "content": "Collaborative machine learning protocols have fueled significant scientific discoveries (Bergen & Petryshen, 2012) and are also gaining traction in diverse sectors such as healthcare networks (W. Li et al., 2019; Powell, 2019b; H. R. Roth et al., 2020). A key factor propelling this widespread adoption is the emerging field of federated learning (H. B. McMahan, Moore, Ramage, & y Arcas, 2016) that allows multiple agents (also called devices) and a central server to tackle a learning problem collaboratively without exchanging or transferring any agent's raw data. Federated learning (FL) comes in various forms Kairouz et al., 2019, ranging from models trained on millions of peripheral devices (Apple, n.d.; B. McMahan & Ramage, 2017; Paulik et al., 2021) like Android smartphones (a.k.a. cross-device FL) to those trained on a limited number of large data repositories (cross-silo FL). In this paper, we concentrate on a scenario that appears frequently when multiple organizations representative of an underlying population collaborate to get a consensus model. For instance, consider a medical study led by a government agency that selects several dozen hospitals to partake in the study to develop a model for the entire national populace.\nSpecifically, as depicted in Figure 5.1, the server (the governmental agency) has a distribution P over agents (hospitals), and each agent m maintains a local data distribution Dm (over its local patients). The server's goal is to find a model w with low population loss, $\\mathcal{F}(w) := \\mathbb{E}_{m\\sim \\mathcal{P}, z\\sim \\mathcal{D}_m}[f(w; z)]$, where f (w; z) represents the loss of model w at datum z. Due to constraints like communication overhead, latency, and limited bandwidth, training a model across all agents (say, all the hospitals in a country) is infeasible. The server, therefore, aims to achieve its goal by sampling M agents from P and minimizing the proxy average loss $\\hat{\\mathcal{F}}(w) := \\frac{1}{M} \\sum_{m\\in[M]} F_m(w)$ to precision $\\varepsilon_{\\text{server}}$.\nIf all agents provide their assigned model updates and collaborate effectively, the synchronized model should converge to a final model wR, with a low average loss $\\hat{\\mathcal{F}}(w_R)$. However, since the agents face various costs, rational agents might exit the process once they are content with the performance of the current model on their local data. For instance, in our example, hospitals may be satisfied with a model that performs well for their patient data.\nDefections, or the act of permanently exiting before completing R rounds, can adversely affect"}, {"title": "6. Incentives in Collaborative Active Learning", "content": "Active learning has emerged as a powerful paradigm in which labels of selected data points are sequentially queried from a large pool of unlabeled data, referred to as the unlabeled pool. The primary objective is to minimize labeling effort to find a classifier that exhibits low error on fresh data points from the same data source, known as generalization error. Typically, if the pool is large enough, a classifier that performs well on the pool can also achieve low generalization error through uniform convergence.\nActive learning has also been studied in the distributed setting, where the unlabeled pool is scattered across multiple machines (called agents), (e.g., Aussel et al., 2020; P. Shen et al., 2016). While active learning has demonstrated promising results, traditional approaches often operate in isolation, neglecting the potential benefits of collaboration among agents should they agree to collaborate. In this paper, we propose a novel framework for incentivized collaboration active learning, where agents can collaboratively explore their data pools to discover a common target function.\nThe motivation for collaboration in active learning stems from real-life scenarios where collaboration and collective intelligence yield improved outcomes, e.g., when agents collect data from the same distribution, and can easily end up labeling the same or very similar points. This redundancy leads to unnecessary and inefficient utilization of resources, as the labeling is often done by experts. Additionally, more data can be translated to improved accuracy, prompting agents to pool their resources and employ a more powerful model.\nThe incentive-driven nature of our framework aligns with the reality of collaboration in the real world. When agents are incentivized to collaborate only when their expected labeling complexity decreases, it reflects the real-life scenario where individuals are motivated to engage in cooperative endeavors if they perceive a clear benefit, such as reduced effort, faster, and better outcomes. In this work, we focus on a specific notion of incentives, where agents already have access to a baseline algorithm and they are motivated to join the collaboration if their label complexity is smaller than running the baseline algorithm on their own.\nConsider, for example, the case of a new drug (e.g., Paxlovid for Covid-19Najjar-Debbiny et al., 2022, that has different efficacy on patients with different features. While individual hospitals can test the drug on their patients in an active learning fashion by executing their preferred baseline algorithm, collaborating efficiently with other hospitals, each with their own patients, often leads to a better prognosis.\nHowever, if the incentives of the hospitals are not maintained, i.e., the effort of some hospitals is increased, the collaboration may be compromised. By emulating this collaboration within the active learning framework, we unlock the potential of collective intelligence to enhance the learning process. Besides, imagine that several data labeling companies have to recover the labels of unlabeled images assigned to them. Each data labeling company would like to"}]}