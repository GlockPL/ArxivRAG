{"title": "MRT5: DYNAMIC TOKEN MERGING FOR EFFICIENT BYTE-LEVEL LANGUAGE MODELS", "authors": ["Julie Kallini", "Shikhar Murty", "Christopher D. Manning", "Christopher Potts", "R\u00f3bert Csord\u00e1s"], "abstract": "Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character- or byte-level models like ByT5 attempt to address these concerns, they have not gained widespread adoption processing raw byte streams without tokenization results in significantly longer sequence lengths, making training and inference inefficient. This work introduces MrT5 (MergeT5), a more efficient variant of ByT5 that integrates a token deletion mechanism in its encoder to dynamically shorten the input sequence length. After processing through a fixed number of encoder layers, a learnt delete gate determines which tokens are to be removed and which are to be retained for subsequent layers. MrT5 effectively \u201cmerges\u201d critical information from deleted tokens into a more compact sequence, leveraging contextual information from the remaining tokens. In continued pre-training experiments, we find that MrT5 can achieve significant gains in inference runtime with minimal effect on performance. When trained on English text, MrT5 demonstrates the capability to transfer its deletion feature zero-shot across several languages, with significant additional improvements following multilingual training. Furthermore, MrT5 shows comparable accuracy to ByT5 on downstream evaluations such as XNLI and character-level tasks while reducing sequence lengths by up to 80%. Our approach presents a solution to the practical limitations of existing byte-level models.", "sections": [{"title": "1 INTRODUCTION", "content": "Subword tokenization, typically via algorithms such as byte-pair encoding (Sennrich et al., 2016) or SentencePiece (Kudo & Richardson, 2018), is a fundamental text preprocessing step that has become ubiquitous in modern language models. Subword tokenizers divide text into meaningful units known as tokens, which closely resemble words or parts of words. Tokenization can be seen as a form of compression, since it reduces the sequence length of the input passed to the compute-intensive Transformer (Vaswani et al., 2017). However, subword tokenizers have several drawbacks. For example, they are not very robust to character-level noise and manipulations, such as spelling errors (Kaushal & Mahowald, 2022; Huang et al., 2023); they directly impact how models process digits and perform arithmetic (Singh & Strouse, 2024); and they have disproportionate compression rates for different languages and scripts (Ahia et al., 2023; Petrov et al., 2023). In addition, current language model APIs charge users per-token, and such discrepancies can cause users of certain languages to be overcharged due to poorer compression.\nAs an alternative to subword models, tokenization-free models skip the tokenization preprocessing step entirely by passing the raw character or byte stream directly as input. However, character- or byte-level sequences tend to be significantly longer than tokenized text sequences, which limits the practical utility of tokenization-free models. For example, ByT5 (Xue et al., 2022), a byte-level counterpart of mT5 (Xue et al., 2021), is competitive with mT5 on a number of tasks, but it"}, {"title": "2 BACKGROUND: TOKENIZATION-FREE MODELS", "content": "Several specialized architectures for character- or byte-level modeling have employed explicit down- sampling steps to reduce their input sequence lengths. For example, CANINE (Clark et al., 2022) is a character-level model trained on the same languages as mBERT (Devlin et al., 2019), and it uses convolutional downsampling to reduce the sequence before feeding it to a 12-layer Transformer en- coder stack. Charformer (Tay et al., 2022) is another byte-level encoder-decoder model that learns a gradient-based \u201csoft tokenization\u201d using a block scoring function to select the byte embeddings to pool together for more efficient training and inference. More recently, MegaByte (Yu et al., 2023) demonstrated potential in scaling byte-level decoders to long-context tasks by segmenting sequences into fixed-length \u201cpatches\u201d upon which local Transformer blocks are applied. However, these patches may not represent meaningful units of text. SpaceByte (Slagle, 2024) takes a similar approach, adding larger global Transformer blocks to specific byte types, like spaces, though these bytes are also not chosen dynamically.\nThis paper focuses on ByT5 (Xue et al., 2022), a byte-level sequence-to-sequence Transformer architecture that serves as a counterpart to mT5 (Xue et al., 2021), the multilingual version of T5 (Raffel et al., 2020). ByT5 requires significantly fewer parameters for its vocabulary matrix (which is comprised of only 256 embeddings). However, to compensate for the loss of these parameters, ByT5 has a \"heavy\" encoder with a larger number of layers than the decoder. While ByT5 shows impressive performance on a variety of downstream tasks, its heavy encoder, large model and feed forward dimensionalities, and short input sequence length (1024 bytes) make it quite inefficient. The architecture itself requires about 1.2 times more operations than mT5, which contributes to a 33% longer pre-training wall clock time for ByT5, even though it was trained on only a quarter of the data used for mT5. In terms of inference speed on downstream tasks, ByT5 can be up to 10 times slower than mT5, depending on the input sequence length.\nMrT5 modifies the ByT5 architecture to make it more efficient. Unlike previous work, MrT5's dele- tion gating mechanism can be added to a pre-trained model like ByT5 with minimal fine-tuning using a small number of additional parameters. The gating can also be applied to models trained from scratch. While we focus here on byte-level modeling, our approach can also be applied to sub- word models, complementing existing work on long-context modeling (described in Appendix A)."}, {"title": "3 THE MRT5 MODEL ARCHITECTURE", "content": "The unique aspect of MrT5 is its deletion gating mechanism: after a fixed encoder layer, a delete gate determines which tokens in the sequence should be kept to be processed by later layers, and which tokens may be deleted, thereby \u201cmerging\" information into a shorter sequence. Our choice to delete tokens only in a single layer has three motivations: (1) we want to avoid the overhead of executing the deletion algorithm multiple times; (2) we observe that both the performance and the number of deleted tokens stabilize after a few initial layers (see Section 7); and (3) in terms of minimizing computation costs, the deletion is most beneficial if done in an early layer."}, {"title": "3.1 DELETION GATING MECHANISM", "content": "The MrT5 deletion gating mechanism is inspired by existing architectures with gating mechanisms such as Long-Short Term Memory (LSTMs, Hochreiter & Schmidhuber, 1997), Gated Recurrent Units (GRUs, Cho et al., 2014), and Mixture-of-Experts (MoEs, Shazeer et al., 2017). MrT5's delete gate is placed after the output of a fixed encoder layer l and is defined by the following function:\n$G = k\\sigma(H_lW + 1_Nb)$   (1)\nwhere $H_l \\in \\mathbb{R}^{N\\times d}$ are the hidden states output by layer l; $W \\in \\mathbb{R}^{d\\times 1}$; $b \\in \\mathbb{R}$; $G \\in \\mathbb{R}^{N\\times 1}$; k is a large negative constant; N is the encoder input sequence length; d is ByT5's hidden state/model dimensionality; and $1_N \\in \\mathbb{R}^{N\\times 1}$ is a vector of ones. The gating activation function is a rescaled and translated sigmoid function, bounded between k and 0. In our experiments, we use k = -30.\nHard and Soft Deletion. During training, MrT5 deletes tokens softly, where the outputs of the gating mechanism G are applied as a soft attention mask. The outputs are added directly to the self-attention mechanism of the subsequent encoder layers, as well as the cross attentions between"}, {"title": "3.2 GATE REGULARIZER", "content": "MrT5 allows deletion rates to be adjusted using a tunable regularizer loss:\n$L_G = \\frac{1}{N} \\sum_{i=1}^{N} G_i$   (3)\nThis loss is the average of the gate output values, which encourages them to be more negative (i.e. closer to k, the minimum gate value). In other words, as this loss decreases, the number of deleted tokens increases. The total loss is defined as the sum L = $L_{CE} + \\alpha L_G$, where $L_{CE}$ is the cross entropy loss. Varying the hyperparameter \u03b1 allows the MrT5 model to delete more or fewer tokens.\nOptimizing a Specific Deletion Ratio. For most of our experiments, we set \u03b1 by hand, which al- lows the model to dynamically discover the deletion ratio depending on the difficulty of the task. Alternatively, we can optimize for a specific ratio of deleted tokens. This can be done using an al- gorithm that resembles the proportional controller (P-controller) from classical control theory. Let's call the proportion of deleted tokens in the current batch $\u03b4_\u03b5 \\in [0, 1]$, the regularization hyperparam- eter for the current batch $\u03b1_t$ and the target deletion ratio \u03b4. We update \u03b1 as follows:\n$\u03b1_{t+1} = clamp(\u03b1_t + k_p(\u03b4 \u2013 \u03b4_\u03b5))$   (4)\nwhere clamp(x) = max(x, 0). We found that using $k_p = 10^{-6}$ and $\u03b1_0 = 0.0$ and updating \u03b1 after every 10 training steps worked well in practice. This method is easier to use than manually setting \u03b1 and allows \u03b1 to change dynamically as the model undergoes phase transitions during training, resulting in more stable learning.\nSoftmax1. It is possible for all elements of G to equal the minimum gate value such that $G_i = k$ for all i \u2208 [1, N]. This G would satisfy the gate regularizer but fail to act as an attention mask, since adding the same value to all elements of the input to a standard softmax function does not affect its output. To help avoid this scenario, we use softmax1 (Miller, 2023) in the attention mechanism:\n$(softmax_1(x))_i = \\frac{exp(x_i)}{1 + \\sum_j exp(x_j)}$   (5)\nWith the softmax1 function, if $G_i = k$ for all i \u2208 [1, N], as k becomes negative, the sum of attention scores approaches zero. This eliminates the failure case of using tokens that appear to be all deleted. For consistency, we use softmax1 for both MrT5 and baseline ByT5 models."}, {"title": "4 SIMULATIONS", "content": "We first train tiny 31M-parameter MrT5 and T5 models with 9 encoder layers and 3 decoder layers from scratch on three diagnostic tasks: a simple vowel removal task, a contextual vowel removal"}, {"title": "5 CONTINUED PRE-TRAINING", "content": "In our main set of experiments, we train MrT5 models on the ByT5 span corruption task. In this pre-training objective, spans of tokens in unlabeled text data are replaced with a single sentinel token ID per span, and the model must fill in the missing tokens. For ByT5 and MrT5, these are spans of bytes, and the masks can potentially interfere with word boundaries."}, {"title": "6 DOWNSTREAM TASK EVALUATIONS", "content": "We next assess MrT5's performance on downstream tasks, specifically XNLI and two character- level tasks. XNLI evaluates MrT5's ability to understand semantic relationships between sentences, while the character-level tasks test whether it retains its sensitivity to character-level manipulations.\nCross-lingual Natural Language Inference. We first test our multilingual MrT5 model using the Cross-lingual Natural Language Inference (XNLI) corpus (Conneau et al., 2018), a benchmark for cross-lingual sentence classification with 5,000 parallel examples in 15 languages. These are the same 15 languages our multilingual MrT5 model was trained on. We selected XNLI for testing because the ByT5 authors noted it as one of the downstream tasks with the worst inference runtimes, primarily due to the long input sequences; ByT5 was 6.4 to 9.5 times slower than mT5. We fine-tune two models on the English MultiNLI corpus (Williams et al., 2018): our multilingual MrT5 model (\u03b1 = 1.2e-2) and the baseline multilingual ByT5 model. For training details, see Appendix B.3."}, {"title": "7 ANALYSIS", "content": "Per-sample Sequence Length Reduction. We present a per-sample analysis of the cross entropy loss and sequence length reduction for the English MrT5 models and random baselines. We take a sample of 1,000 English sentences from the mC4 test set and calculate the percent increase in loss on a per-sample basis, using the English ByT5's loss as the baseline (i.e. the percent increase between the MrT5 model's loss and ByT5's loss for individual samples). For each sample, we also get the sequence length reduction. Across five MrT5 models with different deletion rates, we found no correlation between the percent increase in the loss and the percentage of tokens deleted (average correlation of r = -0.014). This reflects what we would expect from the MrT5 models; for an"}, {"title": "8 CONCLUSION", "content": "In this paper, we introduce MrT5 (MergeT5), a variant of the ByT5 architecture designed to ad- dress the inefficiencies of byte-level language modeling. MrT5's token deletion mechanism forces the model to merge input tokens into a more compact sequence, allowing for computational savings while preserving model performance. Our diagnostic experiments demonstrate that MrT5 effec- tively merges relevant context into a shorter sequence using strategies that align with task-specific objectives. In our continued pre-training experiments, MrT5 outperforms both random and fixed deletion baselines when trained in English, and with multilingual training, MrT5 achieves over 50% reduction in sequence length across multiple languages with minimal impact on the loss. Our model learns very fast: the continued pre-training requires only a few thousand additional training steps. Furthermore, MrT5 maintains competitive accuracy with ByT5 on downstream tasks such as XNLI and character-level manipulations while improving inference runtimes. This demonstrates MrT5's"}, {"title": "A FURTHER RELATED WORK", "content": "Early Exit Models. Our model is closely related to early-exit methods proposed for autoregressive Transformers (Elbayad et al., 2020; Schuster et al., 2022) and BERT (Xin et al., 2020; 2021). In contrast to previous approaches, our method is fully differentiable and does not require special training considerations or calculating the entropy of the final classifier, and the deletion decisions are made in a single layer, making the model efficient and easy to use.\nLong-context Subword Models. Other work has attempted to address issues with long sequences in Transformer models more generally. For example, Hierarchical Transformers (Nawrot et al., 2022) add several layers of downsampling and upsampling to handle long sequences in decoder models. Follow-up work has implemented dynamic pooling using boundary predictors, but these usually involve a supervised training step (Nawrot et al., 2023). Perceiver AR (Hawthorne et al., 2022) is an autoregressive model that employs cross-attention to condense inputs into a smaller set of latent representations. This approach enables it to handle attention over large input sequences of up to a hundred thousand tokens. However, the authors did not specifically evaluate the model on character-level tasks or language tasks without tokenization. Other solutions include Nugget (Qin & Van Durme, 2023; Qin et al., 2023), which encodes the whole sentence, but passes only a dynamic subset of the embeddings to the decoder. This approach does not save compute on the encoder side."}, {"title": "B EXPERIMENTAL DETAILS", "content": ""}, {"title": "B.1 SIMULATION DETAILS", "content": "Model Architectures. We train our diagnostic models with 9 encoder layers and 3 decoder layers, following the 3:1 ratio of encoder to decoder layers in ByT5, and we use dff = 1024 and dmodel = 512. We use softmax\u2081 for all T5 and MrT5 models. Other architectural settings match the standard ByT5 Small, resulting in an architecture with 31M parameters (10% of ByT5 Small's parameter count).\nOptimization. We use a batch size of 128 examples and a sequence length of 128 tokens, and we train each model for a total of 20,000 gradient steps. We use the AdamW optimizer with a learning rate that linearly warms up to 1e-4 over 1,000 steps and linearly decays. For MrT5 models, the delete gate's regularizer is enabled half-way through training, at 10,000 steps. We set a constant regularizer a throughout training."}, {"title": "B.2 CONTINUED PRE-TRAINING DETAILS", "content": "Model Architectures. All MrT5 and baseline models use the model configuration of a standard ByT5 Small, which has dff = 3584, dmodel = 1472, 12 encoder layers, 4 decoder layers, and 300M total parameters. The only difference for MrT5 and the baselines is the additional delete gate and the use of softmax\u2081 in the attention mechanisms.\nData. When training on the span corruption objective, we calculate the corrupted spans such that the average masked span length is 20 tokens with a noise density of 15% (i.e. 15% of tokens in the sequence are masked out), following the specification in the ByT5 paper. For both monolingual and multilingual model training, we ensure that the samples of the mC4 corpus are sufficiently large to avoid training the models for multiple epochs. In the case of multilingual training, we extract equal-sized samples for each language from the mC4 training split.\nOptimization. We train each model for 3,000 gradient steps over batches of 220 tokens (i.e. an encoder sequence length of 1024 with an effective batch size of 1024). We use the AdamW optimizer with an initial learning rate of le\u20134 with linear decay and no warmup. As with the simulations from the previous section, we keep the gate regularizer a constant throughout training to allow the model to discover its own deletion rate, but we vary a across runs. Since we are continuing to train on ByT5's pre-training objective, we do not delay the regularizer.\nAt test time, we use an eval batch size of 214 tokens (i.e. an encoder sequence length of 1024 with a batch size of 16). We use use the last model checkpoint at step 3,000 for all evaluations."}, {"title": "B.3 DOWNSTREAM TASK DETAILS", "content": "XNLI Training Details. We train all models for 4,000 gradient steps (\u224810.43 epochs) with a batch size of 1,024 and a maximum sequence length of 1,024 tokens. We use the AdamW optimizer with an initial learning rate of 1e-3 that linearly decays with no warmup. For MrT5, we apply a P-controller to achieve a target deletion ratio \u03b4 = 0.5, aiming for a sequence length reduction of \u224850%, and we delay the regularizer until 500 steps into fine-tuning. The controller parameters are kp = 10-6 and to = 0.0.\nWe evaluate both models on the XNLI dataset, testing them in English and conducting zero-shot evaluations in the 14 additional languages. For evaluation, we use a batch size of 16 and a maximum sequence length of 1,024 tokens.\nCharacter-level Task Training Details. For the Spelling Correction task, we train for 200,000 gradient steps (~33.7 epochs); for the Word Search task, we train for 300,000 steps (\u2248 61.0 epochs). When training MrT5, we apply a P-controller to achieve a target deletion ratio. On the Spelling Correction task, we set the target deletion ratio \u03b4 0.6, aiming for a sequence length reduction of ~60% (although the model learned to delete closer to ~80% of tokens). On the Word Search task, we set a target deletion ratio \u03b4 = 0.7, aiming for a sequence length reduction of \u224870%. We delay the regularizer until 10,000 steps into fine-tuning for both tasks.\nFor both tasks, we use a batch size of 16 examples, and we use the AdamW optimizer with an initial learning rate of 5e-4 that linearly decays with no warmup. For evaluation, we use a batch size of 16. The Spelling Correction and Word Search tasks have a maximum input sequence length of 64 and 128 tokens, respectively."}, {"title": "B.4 DESCRIPTION OF FIXED DELETION BASELINES", "content": "The fixed deletion baseline deletes tokens at layer l = 3 using deterministic rules based on the token identity. All tokens/columns corresponding to whitespace, punctuation, and symbolic characters are identified: \t, \n, !, \", #, $, %, &, ', (, ), +, , -, ., /, :, ;, <, =, >, ?, @, [, \\, ], _, `, , ;, {, |, }, </s>. These separator tokens are used to locate word boundaries. Then, based on the fixed deletion percentage, the delete gate will drop the tokens/columns corresponding to the ends of words. For example, if the target percentage is 50%, the delete gate will remove the tokens corresponding to the final two characters of a five letter word, and the final three characters of a six character word."}, {"title": "C THEORETICAL COMPUTE SAVINGS", "content": "Here we analyze the theoretical amount of compute used by MrT5 given a deletion rate. Let's call the average length of the input sequence $N_E$ and the average length of the output sequence $N_D$. The width of the residual is $d_{model}$, the dimension of the up-projection in the MLP is $d_{ff}$, our encoder has $L_E$ and the decoder has $L_D$ layers. The deletion occurs after the $L_{del}$ layer and the average proportion of deleted tokens is \u03b4. We assume that the total size of the head projections is equals to $d_{model}$, as typical for Transformers ($d_{head} * N_{heads} = d_{model}$). Then, we can approximate the total number of multiply-accumulate operations (MACs) for the model as follows. Before deletion, the self attention in the encoder uses $N_Ed_{model}^2$ MACs for both the Q, K, V and the output projections and $N_E^2d_{model}$ MACs for both the $A = QK^T$ and AV projections. The MLP layer uses $N_Ed_{model}d_{ff}$ additional MACs. Thus, the total number of MACs used per layer is $4N_Ed_{model}^2 + 2N_E^2d_{model} + N_Ed_{model}d_{ff}$. This much compute is used for the first $L_{del}$ layers, after which the sequence length is reduced to $N_E(1 \u2013 \u03b4)$ for the remaining $L_E - L_{del}$ layers. Thus, the encoder uses\n$MACS_{encoder} = (L_E - L_{del}) (1 \u2013 \u03b4) (4N_Ed_{model}^2 + 2N_E^2d_{model}(1 \u2013 \u03b4) + N_Ed_{model}d_{ff}) + L_{del} (4N_Ed_{model}^2 + 2N_E^2d_{model} + N_Ed_{model}d_{ff})$   (6)\nThe MACs used by the decoder can be calculated similarly, but additionally the cross attention has to be taken into account. The cross attention uses $N_E(1 \u2013 \u03b4)d_{model}^2$ MACs for the K and V projections, $N_Dd_{model}^2$ MACS for the Q and output projections, and $(1 \u2013 \u03b4)N_EN_Dd_{model}$ MACs for the attention"}, {"title": "D ADDITIONAL DOWNSTREAM TASK EVALUATIONS", "content": ""}, {"title": "E ADDITIONAL PER-SAMPLE ANALYSES", "content": ""}]}