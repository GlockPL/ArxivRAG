{"title": "Chain-of-Description: What I can understand, I can put into words", "authors": ["Jiaxin GUO", "Daimeng Wei", "Zongyao Li", "Hengchao Shang", "Yuanchang Luo", "Hao Yang"], "abstract": "In this paper, we propose a novel strategy defined as Chain-of-Description (CoD) Prompting, tailored for Multi-Modal Large Language Models. This approach involves having the model first provide a detailed description of the multi-modal input before generating an answer to the question. When applied to models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL, CoD Prompting significantly enhances performance compared to standard prompting methods. This is demonstrated by nearly a 4% improvement in the speech category of the audio benchmark AIR-Bench-Chat and a 5.3% improvement in the hard-level portion of the vision benchmark MMMU_Pro. Our ablation study further validates the effectiveness of CoD Prompting.", "sections": [{"title": "Introduction", "content": "Multi-Modal Large Language Models (MLLMs), which encompass Large Audio-Language Models (LALMs) and Large Vision-Language Models (LVLMs), have shown considerable potential in managing a variety of input types. These models are generally based on Large Language Models (LLMs) and employ audio/vision encoders to align multi-modal inputs with text. However, the conventional method of directly generating answers from audio or visual inputs might not fully capitalize on the information and comprehension that the models can potentially extract. Given that the training paradigm for most MLLMs involves aligning multi-modal inputs with text, we explored whether there is an inference strategy that can not only explicitly align these inputs but also improve the quality of the results produced by MLLMs.\nWe believe that \"What I can understand, I can put into words.\" This implies that if a model can generate a detailed description of the input, it indicates a deeper level of understanding. Based on this idea, we proposed the Chain-of-Description (CoD) prompting for MLLMs, which involves having the model first provide a detailed description of the multi-modal input before answering the question.\nWe have implemented CoD Prompting on LALMs and LVLMs, specifically with the Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL models. Through experiments conducted on their respective test sets, we have demonstrated that CoD significantly enhances the reasoning performance of these models compared to the standard approach. Specifically, in the speech testset AIR-Bench-Chat, the Qwen2-Audio model performance for human speech improved by nearly 4%. In the image testset MMLU_Pro, for the hard-level category, the performance of Qwen*-VL increased by 5.3%.\nFurthermore, in our experiments with LALMs, we analyzed information density to substantiate why CoD is effective. In our experiments with LVLMs, we verified the correctness of CoD by constructing better descriptions."}, {"title": "Chain-of-Description Prompting", "content": "Motivation: What I can understand, I can put into words.\nThe motivation of our proposed Chain-of-Description (CoD) Prompting is if a model can generate a detailed description of the input, it indicates a deeper level of understanding. CoD Prompting can be described as follows:\n1. Supply audio/vision inputs to MLLLMs and ask the models produce detailed descriptions.\n2. MLLMs generate a comprehensive textual representation.\n3. Thereafter, introduce the queries into MLLMs.\n4. MLLMs generate responses pertinent to the queries.\nFor LALMs, describing speech context, background sounds, and other audio features comprehensively helps the model better understand audio inputs. Focusing on the description process first aims to establish a strong foundation for generating higher-quality answers, improving MLLMs' overall performance."}, {"title": "Experiments with CoD in LALMs", "content": "3.1 Experimental Setup\nModel The model used for our experiments is Qwen2-Audio, a state-of-the-art open-sourced LALM capable of processing various audio inputs and generating textual responses.\nEvaluation Dataset The dataset we utilized is the AIR-Bench, which is the first and widely adopted benchmark designed to assess the comprehension capabilities of LALMs across various audio signals, including human speech, natural sounds, and music. Following prior research, we conducted detailed evaluations on all four subcategories of the AIR-Bench Chat Benchmark (AIR-\nEvaluation Method Building upon previous work , we employed an evaluation method that utilizes a LLM as the judge. Specifically, we utilized a LLM to rate both the ground truth answer and the model prediction on a scale of 1 to 10. The final score is the average of these ratings. Considering cost-effectiveness, we chose gpt-40-mini as our evaluation LLM.\nIn practice, we rated the model predictions from both Standard Prompting and CoD Prompting against the ground truth answer. This process yielded two sets of scores for the ground truth answer, which may exhibit minor differences. To facilitate more effective comparison, we calculated the ratio r of the model prediction p score \\(s_p\\) to the ground truth answer gt score \\(s_{gt}\\). The r quantifies the alignment between the prediction and the ground truth."}, {"title": "Results", "content": "Based on the evaluation results presented in Table 1, it is evident that the Qwen2-Audio model's alignment with the ground truth answers has improved across all subcategories after adopting the CoD Prompting. Particularly in the Speech category, the alignment reached 95.02%, marking an increase of nearly 4% compared to the Standard Prompting. Other categories such as Sound, Music, and Mixed also experienced an enhancement of about 1% each. On average across all categories, there was an increase of 1.79%. This indicates that the CoD Prompting method significantly enhances the consistency of the model's predictions with the ground truth answers."}, {"title": "Ablation Study", "content": "Question: Why does CoD Prompting perform better in the Speech category?\nWe analyze information density to explain why CoD Prompting performs better in the Speech category. We can consider the description as a textual representation of the audio, where the quantity of description indicates the level of information density. Compared to Sounds and Music, human speech offers a richer array of information, including textual content, emotional expressions, and background noise."}, {"title": "Experiments with CoD in LVLMs", "content": "4.1 Experimental Setup\nModel Our experiments utilized models from the Qwen-VL series, including Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct and Qwen2.5-VL-72B-Instruct.\nEvaluation Dataset In our experiments, we utilized the MMMU_Pro dataset. MMMU_Pro is an enhanced multimodal benchmark designed to rigorously assess the true understanding capabilities of LVLMs. Specifically, we employed the standard validation data within MMMU_Pro that has been enhanced with 10 options. Additionally, this set can be categorized into three levels of difficulty: Easy, Medium, and Hard.\nEvaluation Method As MMMU_Pro is a multiple-choice dataset, we can directly calculate the accuracy of model predictions. We use both Standard Prompting and CoD Prompting methods to generate the answer options."}, {"title": "Results", "content": "Table 3 presents the evaluation results for LVLMs. Among the 7B-sized models, the results indicate that the Qwen2.5-VL-7B-Instruct outperforms the Qwen2-VL-7B-Instruct across all difficulty levels. Surprisingly, CoD Prompting did not demonstrate effectiveness at all levels; instead, it showed significant improvement at the Hard level, increasing the accuracy of both models by 5.3%. This suggests that CoD Prompting is more beneficial for more complex images or questions. A detailed analysis of the Easy and Medium level cases revealed that due to the huge information density in images, the extensive descriptions generated did not necessarily cover the key points of the questions, potentially leading to a negative impact on responses. Moreover, this negative effect was less pronounced in the latest Qwen2.5-VL-7B-Instruct model.\nThe Qwen2.5-VL-72B-Instruct outperformed the Qwen2.5-VL-7B-Instruct across all difficulty levels, aligning with the well-known conclusion that larger model sizes lead to stronger performance. Our CoD Prompting method also achieved consistent improvements on the Qwen2.5-VL-72B-Instruct, with a 5.3% increase at the Hard level. Additionally, it maintained either no decline or a slight improvement at the Easy and Medium levels."}, {"title": "Ablation Study", "content": "Question: How would the performance be affected if the model could generate higher-quality descriptions?\nThe key to our CoD Prompting method lies in generating high-quality and accurate descriptions, which can lead to improved model performance. Knowing that the Qwen2.5-VL-7B-Instruct model outperforms the Qwen2-VL-7B-Instruct, we hypothesize that the descriptions generated by Qwen2.5-VL-7B-Instruct would be superior.\nWe conducted experiments on the Qwen2-VL-7B-Instruct model using descriptions generated by Qwen2.5-VL-7B-Instruct. As shown in Table 4, the results across all difficulty levels demonstrated positive improvements, confirming that higher quality descriptions can yield better outcomes."}, {"title": "Related Work", "content": "We outline some related work and explain how our approach differs from these efforts.\nChain-of-Thought (CoT) primarily aims to enhance the reasoning capabilities of text LLMs by breaking down problems into step-by-step solutions. Our method is inspired by CoT, but our CoD is not a variant of CoT; it is a new strategy specifically designed for multi-modal inputs. In theory, CoD and CoT could be combined to bolster the performance of MLLMs.\nWu et al. focuses only on LVLMs and also mentions descriptions. However, their descriptions are related directly to the questions. In contrast, we do focus not on specific questions but explore a more general scenario and verify its effectiveness. Zhang et al. also concentrates on LVLMs, but their study emphasizes enhancing reasoning tasks, proposing rationale generation before answer inference. Our work is not limited to reasoning tasks but targets more general scenarios.\nVijayaraghavan et al. introduced a method with the same name as ours, but their research direction is about improving the performance of Code LLM."}, {"title": "Conclusion", "content": "This study introduces Chain-of-Description (CoD) Prompting, which significantly enhances the reasoning capabilities of MLLMs. Experiments conducted on models such as Qwen2-Audio, Qwen2-VL, and Qwen2.5-VL demonstrate the effectiveness of CoD Prompting in improving model comprehension and response accuracy for multi-modal inputs, offering a promising direction for future research."}, {"title": "Limitations", "content": "Although our experimental results have been positive, the sheer number of open-source MLLMs and benchmark datasets prevents us from validating each one. Furthermore, based on our motivation, we believe that extensive multi-modal description training during the pre-training phase of MLLMs could yield significant benefits. Regrettably, this type of experiment is particularly resource-intensive, making it infeasible for us to complete swiftly."}]}