{"title": "Factorized-Dreamer: Training A High-Quality Video Generator with Limited and Low-Quality Data", "authors": ["Tao Yang", "Yangming Shi", "Yunwen Huang", "Feng Chen", "Yin Zheng", "Lei Zhang"], "abstract": "Text-to-video (T2V) generation has gained significant attention due to its wide applications to video generation, editing, enhancement and translation, etc. However, high-quality (HQ) video synthesis is extremely challenging because of the diverse and complex motions existed in real world. Most existing works struggle to address this problem by collecting large-scale HQ videos, which are inaccessible to the community. In this work, we show that publicly available limited and low-quality (LQ) data are sufficient to train a HQ video generator without recaptioning or finetuning. We factorize the whole T2V generation process into two steps: generating an image conditioned on a highly descriptive caption, and synthesizing the video conditioned on the generated image and a concise caption of motion details. Specifically, we present Factorized-Dreamer, a factorized spatiotemporal framework with several critical designs for T2V generation, including an adapter to combine text and image embeddings, a pixel-aware cross attention module to capture pixel-level image information, a T5 text encoder to better understand motion description, and a PredictNet to supervise optical flows. We further present a noise schedule, which plays a key role in ensuring the quality and stability of video generation. Our model lowers the requirements in detailed captions and HQ videos, and can be directly trained on limited LQ datasets with noisy and brief captions such as WebVid-10M, largely alleviating the cost to collect large-scale HQ video-text pairs. Extensive experiments in a variety of T2V and image-to-video generation tasks demonstrate the effectiveness of our proposed Factorized-Dreamer. Our source codes are available at https://github.com/yangxy/Factorized-Dreamer/.", "sections": [{"title": "1. Introduction", "content": "Video generation is a highly challenging task due to the difficulties in simulating the complex and dynamic real-world scenarios conditioned on a text prompt, a label or an image. While in computer graphics, we can resort to physical laws to simulate fluid [62], cloth [37], etc., such methods are limited in specific objects and scenes. Video generation has recently achieved impressive progress with the rapid development of deep learning techniques [2, 52, 53]. Early attempts [5, 53] are mostly based on generative adversarial networks (GANs) [19]. Though yielding promising results, GAN-based video generation models suffer from unstable performance and restricted scenarios. With the emerging of diffusion models [21], large-scale text-to-image (T2I) models [44, 46, 49] trained on web-scale data can be used to generate high-quality images with a wide range of diversity and aesthetics. This encourages the research on training text-to-video (T2V) models [2, 22, 52]. While much progress has been made, video generation still lags behind image generation in terms of quality and diversity as it needs to model an extra temporal dimension, which involves of complex changes of scene content and object motions/actions.\nThe research on video generation can be divided into two categories: direct generation and factorized generation, as depicted in Fig. 1. The former learns a direct mapping from a text prompt to a generated video, while the latter resorts to an intermediate image generated by an off-the-shelf T2I model, and focuses on synthesizing a video conditioned on both the image and the text prompts. Video generation has been significantly boosted by diffusion models [21, 46]. Ho et al. [23] trained the first video diffusion model from scratch. Following works [2, 20, 28, 52] are mostly built on pre-trained T2I models such as Stable Diffusion (SD) [46] and PixArt-a [8] considering the fact that a video is composed of a sequence of frames. By using pre-trained T2I models to generate spatial structures, these methods focus on how to encode motion dynamics into the latent codes [28] or insert additional temporal layers [2, 20, 52] to model the temporal dimension of videos. Along this line of research, many methods have been proposed to further boost the generation performance via re-designing video noise prior [17], inserting motion module into a personalized T2I model [20], investigating the influence of data selection [2], adjusting noise schedules [18], improving the training scheme [7, 56], and so on. Though much progress has been achieved, these methods either yield unpleasant"}, {"title": "2. Related Work", "content": "Text-to-Image Generation. Diffusion models [21] have recently demonstrated powerful capability in generating HQ natural images, surpassing traditional generative models such as GANs [19], VAEs [31] and Flows [45]. While early image generation models are conditioned on labels [4] or focused on a specific object/scenery such as face [26], the seminal work of DALLE [43, 44] showed that T2I generation is possible. Saharia et al. [49] proposed Imagen, a cascade diffusion model, to achieve photo-realistic T2I generation. Such models work on pixel space and require extensive computational resources to train and inference. Rombach et al. [46] instead employed a VAE model [31] to map the image from pixel space to latent space and operated score-matching therein. This work was extended to Stable Diffusion (SD) [46], the first open-source large-scale pre-trained T2I model. SD demonstrated that T2I diffusion priors are more powerful than GAN priors in handling diverse natural images [6, 47, 63, 64]. It has served as a source of inspiration for numerous subsequent works of T2I synthesis [8, 12, 15, 49], conditional generation [64], personalized generation [47], image inpainting [61], image editing [6], image super-resolution [57, 63], and so on.\nText-to-Video Generation. T2V generation [2, 22, 52] has gained increasing attention with the huge success of T2I synthesis [8, 44, 46, 49]. Most prior works leveraged T2I models for T2V generation. Khachatryan et al. [28] generated videos by encoding motion dynamics in the latent space of a T2I model such as SD [46]. Wu et al. [60] targeted at one-shot T2V generation by finetuning a T2I model with a single video. Though these methods can produce videos without training on large-scale video datasets, they are limited in the quality and diversity of generated videos. Since a video is composed of a sequence of frames, many works instead embed temporal modules into a T2I model and learn a direct mapping from a text or an image prompt to a generated video [2, 18, 22, 52, 56]. For example, Singer et al. [52] trained a T2V model based on a pretrained T2I model with a mixture of image and video datasets. Imagen-Video [22] extends the capacity of Imagen [49] to video synthesis by employing a cascade diffusion models.\nThe aforementioned works are trained in pixel domain, suffering from the challenge of modeling high-dimensional spatiotemporal space. Many following works resort to latent diffusion models for video generation [2, 3, 17, 18, 20]. Blattmann et al. [3] synthesized videos by merely training the newly added temporal layers. Ge et al. [17] improved the synthesis quality by introducing a video noise prior. Guo et al. [20] discovered that replacing the base T2V model with a personalized one could boost the performance. Based on [3], Blattmann et al. [2] proposed Stable Video diffusion (SVD) with a comprehensive training and data selection strategy. Girdhar et al. [18] explored new noise schedules for diffusion and a multi-stage training to achieve more stable results. Recently, a couple of T2V models [35, 38] emerged based on the DiT architecture [8, 39]. In particular, SORA [38] exhibits impressive"}, {"title": "3. Method", "content": "realistic long video generation performance. Though great progress has been made, previous methods either produce LQ results [56] or build on the proprietary datasets that are not available to public [18, 38]. In this work, we argue that a reasonably high-quality video generator can be built on purely publicly available datasets without recaptioning [25] or finetuning.\nLeverage Knowledge from Images for Video Generation. Large T2I models [8, 44, 46, 49] are all trained on web-scale image-text pairs. It is supposed that T2V models require a significantly larger training dataset than T2I ones due to the extra temporal dimension. Unfortunately, publicly available video-text datasets are typically an order of magnitude smaller than image-text datasets [1], and they are limited in style and quality. To alleviate this problem, different strategies have been proposed to leverage knowledge from image data for video generation. For example, we can firstly train a T2I model and then finetune it partially [3, 20] or entirely [56] on the video dataset, train the T2V model jointly on image-text and video-text pairs from scratch [22, 23, 52], concatenate the first frame as input [2, 18], and use CLIP image features as a condition [2, 51]. In this work, we leverage the image knowledge by factorizing the T2V generation into two sub-problems: (1) generating an image using a text prompt based on an off-the-shelf T2I model; and (2) synthesizing a final video based on the image and text conditions."}, {"title": "3.1. Motivation", "content": "When conditioned on the same text prompt, the target domain of a T2V model is significantly larger than that of T2I due to the extra temporal dimension (see Fig. 1). This means that a HQ T2V generator should be much bigger than a T2I one, and therefore a larger HQ training dataset is expected. Unfortunately, publicly available text-video datasets [1] are typically of LQ and are significantly smaller than text-image datasets [10]. Many existing T2V models [2, 52] are built on pretrained T2I models with inserted temporal layers, and their parameters are basically on the same level of T2I models. We argue that these two issues block the training a satisfied T2V generator.\nTo address the above issues, it is critical to shrink the target domain conditioned on the text prompt. One solution is to recaption [25] the dataset to obtain more descriptive captions because a more detailed and accurate caption would make the target space more specific. However, this requires additional work load, and it cannot address the LQ image features and watermark issues inherited from the LQ text-video datasets. We instead employ a factorized solution, as shown in Fig. 1(b), by leveraging image knowledge for video generation. In this way, recaptioning is no longer needed as the image input has carried all the necessary spatial information, and the text prompt could be brief and focused on motion description. In addition, this solution can alleviate the annoying LQ image features and watermark issues because the generated HQ image is used to generate video during inference. As a result, the proposed factorized framework makes training a HQ video generator using merely publicly available datasets possible."}, {"title": "3.2. Architecture", "content": "The architecture of the proposed Factorized-Dreamer is illustrated in Fig. 2. By using an off-the-shelf T2I model to synthesize the first frame, the core part of Factorized-Dreamer lies in the TI2V model. Specifically, the proposed TI2V model consists of several factorized spatial and temporal modules with a few critical design decisions, including a T2I adapter to combine both image and text information for spatial modules, the pixel-aware cross attention (PACA) module for perceiving pixel-level image information, an LLM text encoder, i.e. T5 [42] for more precise motion understanding, and a PredictNet for supervising optical flows.\nText-to-Image Adapter. The captions in publicly available video datasets, i.e., WebVid [1] and PexelVideos [40], are usually precise but unfortunately concise. For instance, the caption shown in Fig. 2 accurately describes the subject and the activity in the video, but presents no detailed information about the man and the scenery. Without recaptioning the dataset, we instead use CLIP image features [41] as the conditional information with the help of a T2I adapter [36].\nGiven a text and an image, we obtain the text and image embeddings $emb_{txt}$, $emb_{img}$ using CLIP [41]. In SD [46], the cross attention can be computed as follows:\n$Z = CA(to_q(x), to_{kv}(emb_{txt})),$ (1)\nwhere Z, x, $to_q$, $to_{kv}$ are respectively the output of cross attention, the latent feature, a function that maps latent feature to query, and a function that maps $emb_{txt}$ to the key and the value. With the adapter, we update the computation of cross attention as follows:\n$Z=CA(to_q(x), to_{kv}(emb_{txt})) + x * CA(to_q(x), to_{kv_adp}(emb_{img})),$ (2)\nwhere $to_{kv_adp}$ maps the $emb_{img}$ to the key and the value, and A is a balancing parameter. We simply set $x = 1$ in our experiment.\nPixel-Aware Cross Attention. The CLIP image features mainly provides global image information. To perceive pixel-level details, we resort to PACA [63], which is a plug-and-play module and has been successfully used in the task of image super-resolution [63]. Unlike commonly used cross attention where the function $to_{kv}$ takes CLIP embeddings as input, in PACA the input of $to_{kv}$ is the conditioned image latent y encoded by the same VAE [14] of SD:\n$Z = PACA(to_q(x), to_{kv}(y)).$ (3)\nText Encoder for Motion Understanding. SD employs the CLIP text encoder [41] to align with the image embeddings. While trained on large-scale image-text pairs, the training data of CLIP are still significantly smaller than that used by LLMs [42]. Furthermore, the captions in image-text pairs are mainly descriptions of image contents, which can be biased. Recent researches [15] have showed that CLIP text encoder is less reliable in prompt following and typography generation than LLMs [42]. We argue that it also limits the motion understanding in video generation tasks. Therefore, we employ the widely used T5 [42] to encode the caption for temporal modules.\nPredictNet for Motion Supervision. Motion coherence is one of the main challenges in video generation. Most existing works implicitly address this problem by resorting to large-scale video datasets [2]. Due to the limited sizes of publicly available video datasets, we introduce the PredictNet to explicitly supervise the learning of video motions. The PredictNet is initialized by the upsampling part of the UNet, inspired by ControlNet [64]. Unlike the UNet, it predicts the optical flows of input videos. Given a video latent $x_0$, a randomly sampled diffusion step t and various conditions c, the PredictNet $p_\\theta$ learns to predict the original"}, {"title": "3.3. Noise Schedule", "content": "optical flow f using the $x_0$-prediction:\n$L_{flow} = E_{x_0,t,c,e\\sim N(0,1)} [|| f - p_\\theta(x_t, t, c)||^2].$ (4)\nPredictNet is only used in the final training stage. Our experiments demonstrate that the proposed PredictNet improves the coherence of generated videos.\nDiffusion models synthesize HQ data by progressively adding noise to a dataset and then learning to reverse this process, in which the noise schedule plays a key role. A number of works have studied the setting of noise schedule to improve the image [9, 13, 33] and video [2, 59] generation performance. As discovered by prior works [13, 33], the noise schedule used in SD would cause the signal-to-noise-ratio (SNR) issue when directly applied to video generation models. The reason is twofold. First, the noise schedule leaves some residual signal even at the terminal diffusion timestep N in training, leading to non-zero SNR. This weakens the model performance at test time due to train-test discrepancy. Second, given the noise with a certain intensity, applying it to video frames yields much stronger SNR than applying it to an image. This is because a clean image can be easily restored by simply averaging video frames. To address these issues, Lin et al. [33] rescaled the noise schedule to enforce zero terminal SNR. Chen [9] scaled the input data by a factor. Hoogeboom et al. [13] introduced a shifted diffusion noise. Blattmann et al. [2] and Menpace et al. [59] introduced the EDM [27] framework.\nFor simplicity, we utilize the methods proposed by [33] and [13]. Given a video input with a resolution of T \u00d7 H \u00d7 W, its SNR at time t \u2208 {1,2...N} can be updated as follows:\n$SNR^{T\\times H \\times W}(t) = SNR(t) * s^2,$ (5)\nwhere $SNR(t) = \\frac{a_t}{1-a_t}$ is the original SNR in SD. $s = \\sqrt[\\nabla]{\\frac{D \\times D}{T \\times H \\times W}}$ is a shifting factor, where D = 256 is the reference resolution. Moreover, we follow the idea proposed in [33] to re-scale the noise schedule as follows:\n$\\sqrt{\\alpha'_t} = \\sqrt{\\alpha_t} - \\sqrt{\\alpha_N},$ (6)\nwhere $\\alpha'$ is the scaled a. One can see that $\\sqrt{\\alpha'_{N-1}} \\neq 0$ and $\\sqrt{\\alpha'_N} = 0$, which means that $a_N'$ has been successfully set to 0. We visualize the curves of log SNR to timestep t in Fig. 5. The shifting factor s is 0.125. One can see that after shifting and re-scaling the original SD noise schedule, the signal can be largely destroyed with more noise, making it more suitable for video generation tasks."}, {"title": "3.4. Training Strategy", "content": "In training, we learn the proposed Factorized-Dreamer model $f_\\theta$ to predict the noise added to the noisy video latent $x_t$ conditioned on c. The optimization objective is:\n$L_{DF} = E_{x_0,t,c,e\\sim N(0,1)} [||\\epsilon - f_\\theta (x_t, t, c) ||^2].$ (7)\nIn the final training stage, we jointly update the PredictNet. The total loss is $L = L_{DF} + \\gamma L_{flow}$, where we simply set the balancing parameter as $\\gamma = 1$. All parameters except those from CLIP, T5 and VAE are trainable."}, {"title": "4. Experiments", "content": "Text-to-Video Generation. We compare our proposed Factorized-Dreamer with two categories of video generation algorithms. The first category is the direct T2V generation models, including open-sourced AnimateDiff [20], Lavie [58] and VideoCrafter2 [7], as well as the commercial Gen2 [48] and PikaLab [32]. The second category is factorized video generation models, including I2V-XL [51] and SVD [2], where we use SDXL [12] in the T2I step for fair comparison.\nWe first compare against previous methods using the zero-shot T2V generation setting on UCF101 [29]. As presented in Tab. 2, our Factorized-Dreamer achieves competitive FVD [54] and IS [55] scores. Since these metrics may not align with human preferences [18, 22] due to the bias in training data, we further resort to the EvalCrafter benchmark [34]. The quantitative evaluation results are presented in Tab. 1. On can see that our method achieves competitive visual quality score with PikaLab [32] and Gen2 [48] that are trained on large-scale HQ proprietary datasets, demonstrating the effectiveness of our factorized generation framework. Our Factorized-Dreamer ranks the second on motion quality and temporal consistency. This indicates the importance of our designs on the noise schedule and the PredictNet. Furthermore, the final sum score of Factorized-Dreamer surpasses all the open-source methods and only seconds to commercial Gen2 [48]. To conclude, Factorized-"}, {"title": "4.1. Experiment Setup", "content": "Training and Testing Datasets. Unlike many previous methods [2, 18] that are built on the proprietary data, we train our model on two publicly available datasets: (1) WebVid [1] and an extension of it [50] crawled from ShutterStock \u00b9, which contribute about 19M LQ text-video pairs after de-duplicating; and (2) PexelVideos [40], which consists of about 300K HQ text-video pairs. Different from previous methods [2, 7, 51] that employ HQ data for an extra finetuning stage, we simply mixed the LQ and HQ datasets during the whole training procedure. We evaluate our model using the zero-shot T2V generation on the UCF101 dataset [29].\nTraining Scheme. We use the AdamW [30] optimizer with a fixed learning rate 5e-5 and a total batch size of 40 videos. We adopt a multi-stage training strategy by firstly training on low-resolution (LR) videos, i.e., 32 \u00d7 256 x 256, for 400k steps, then training on high-resolution (HR) video, i.e. 16 x 512 x 512, for 300k steps, and finally introducing the PredictNet to supervise motion flows for another 100k steps on the same HR videos in the second stage. The model is trained for about 12 days with 40 NVIDIA Tesla 80G-A100 GPUs.\nEvaluation Metrics. For quantitative evaluation of our model on the T2V task, we exploit EvalCrafter [34], which is a benchmark for evaluating T2V generation models [7]. It provides about 700 prompts to conclude an objective metric from 4 subjective studies, i.e., motion quality, text-video alignment, temporal consistency, and visual quality. We also employ the commonly used Frechet Video Distance (FVD) [54] and Inception Score (IS) [55] for zero-shot evaluation on the UCF101 [29]. As for the I2V task, we employ Frame consistency (FC) and Prompt consistency (PC) as measures following previous methods [18]. In addition, we conduct user studies for evaluation due to the lack of a comprehensive objective metric for T2V task."}, {"title": "4.3. User Studies", "content": "We invite 15 volunteers to conduct a user study for T2V and I2V tasks. Each volunteer is asked to choose the most preferred one among the outputs of all competing methods, which are presented to the volunteers in random order. We randomly select 15 text prompts from EvalCrafter [34] and 15 text-image pairs from AIGCBench [16] for T2V and I2V, respectively. For T2V, we compare the proposed Factorized-Dreamer with Gen2 [48], PikaLab [32], and VideoCrafter2 [7] in terms of \"visual quality\" and \"motion quality\" axes. The axis of \"visual quality\" represents a comprehensive preference on both aesthetics and text-video alignment. As for I2V task, we compare our method with Gen2 [48], PikaLab [32], and SVD [2]. Since they are all conditioned on the same text and image prompts, we pay attention to their performance on \"prompt following\" and \"motion quality\" axes. Finally, we obtain 900 votes, and the statistics are presented in Fig. 6.\nIt can be seen that our Factorized-Dreamer outperforms competing methods on \u201cvisual quality\u201d axis, and surpasses open-sourced method on \u201cprompt following\u201d. For all tasks, our method achieves comparable \u201cmotion quality\u201d performance to its competitors, even it is trained on merely publicly available limited and LQ datasets."}, {"title": "4.4. Ablation Studies", "content": "We perform a series of ablation experiments to study our design decisions, including the noise schedule, the importance of factorized generation, the role of T5 text encoder and PredictNet.\nEffectiveness of Noise Schedule. We evaluate two variants of Factorized-Dreamer by adopting the original noise schedule of SD and the shifting operation in Eqn. 5. As shown in Fig. 7(a), the generation becomes highly unstable when directly employing the original SD noise schedule. This is also verified by the quantitative results of Exp. (a) in Tab. 4. Removing the rescaling operation in Eqn. 6 has little effect on the results (see evaluation results of Exp.(a*) in Tab. 4). This is because the residual signal at the terminal diffusion timestep has been largely reduced after shifting as shown in Fig. 5, significantly alleviating the train-test discrepancy.\nImportance of Factorized Generation. To evaluate the importance of factorized generation, we remove the PACA module and the adapter module during model training. As a result, we obtain a direct T2V variant. As shown in Fig. 7(b) and Tab. 4, the direct T2V variant tends to yield inconsistent videos with low visual quality (e.g. watermark) and worse quantitative indices (see Exps. (b) and (e)).\nRole of T5 Text Encoder. We test a variant by replacing T5 with the CLIP text encoder. It is found that this leads to occasional misunderstanding of motions. For example, the text prompt in Fig. 7 emphasizes \u201cfloating\" rather than \"swimming\". This is also confirmed by the reduced text-video alignment score in Tab. 4 (see Exps. (c) and (e)).\nRole of PredictNet. To verify how PredictNet benefits video generation, we conduct a variant by abandoning the jointly training stage as discussed in Sec. 4.1. By comparing Exps. (d) and (e), it can be seen that the proposed PredictNet improves the motion quality and temporal consistency. In Fig. 7(d), one can see that an arm disappears during evolving without using PredictNet."}, {"title": "5. Conclusion and Limitation", "content": "We proposed a factorized framework, namely Factorized-Dreamer, for text based HQ video generation. Different from many existing methods that are trained on web-scale HQ proprietary datasets, we showed that a HQ video generator can be achieved by using merely publicly available, limited and LQ datasets, even without recaptioning or finetuning. We developed a factorized spatiotemporal architecture with several critical designs to accomplish this goal. The proposed Factorized-Dreamer was simple to implement, easy to train, and our extensive experiments demonstrated its effectiveness and flexibility across T2V and I2V tasks.\nThough Factorized-Dreamer can synthesize HQ videos, it sometimes still suffers from inconsistent and incoherent motions due to limited training data. In addition, it struggles to generate long videos. A more sophistic framework like the one used in Sora could be introduced to further improve the performance of Factorized-Dreamer, which will be considered in our future work."}]}