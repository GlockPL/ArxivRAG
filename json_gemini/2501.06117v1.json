{"title": "Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding", "authors": ["Fabian David Schmidt", "Ivan Vuli\u0107", "Goran Glava\u0161", "David Ifeoluwa Adelani"], "abstract": "While recent multilingual automatic speech recognition models claim to support thousands of languages, ASR for low-resource languages remains highly unreliable due to limited bi-modal speech and text training data. Better multilingual spoken language understanding (SLU) can strengthen massively the robustness of multilingual ASR by levering language semantics to compensate for scarce training data, such as disambiguating utterances via context or exploiting semantic similarities across languages. Even more so, SLU is indispensable for inclusive speech technology in roughly half of all living languages that lack a formal writing system. However, the evaluation of multilingual SLU remains limited to shallower tasks such as intent classification or language identification. To address this, we present Fleurs-SLU, a multilingual SLU benchmark that encompasses topical speech classification in 102 languages and multiple-choice question answering through listening comprehension in 92 languages. We extensively evaluate both end-to-end speech classification models and cascaded systems that combine speech-to-text transcription with subsequent classification by large language models on Fleurs-SLU. Our results show that cascaded systems exhibit greater robustness in multilingual SLU tasks, though speech encoders can achieve competitive performance in topical speech classification when appropriately pre-trained. We further find a strong correlation between robust multilingual ASR, effective speech-to-text translation, and strong multilingual SLU, highlighting the mutual benefits between acoustic and semantic speech representations.", "sections": [{"title": "Introduction", "content": "State-of-the-art multilingual automatic speech recognition (ASR) systems claim to support thousands of languages (Pratap et al., 2024; Chen et al., 2024). However, these systems still struggle to reliably transcribe low-resource languages due to the scarcity of bimodal speech and text data in those languages. Massively multilingual ASR models are commonly pre-trained on the self-supervised wav2vec-BERT 2.0 objective (Baevski et al., 2020). Such pre-training learns acoustic features that align representations with phonetics rather than semantics of spoken language (Choi et al., 2024). We therefore argue that speech models require strong multilingual spoken language understanding (SLU) capabilities to enhance massively multilingual ASR for low-resource languages. Multilingual speech models better at semantic encoding of speech tend to be more robust in ASR too, as they utilize context-related cues, leverage cross-lingual similarities (e.g., cognates, shared subwords, or similar syntax), and better support code-switching in mixed-language speech (cf. \u00a75). Therefore, multilingual SLU capabilities are a key feature of speech models that enable sample-efficient cross-lingual transfer to low-resource or entirely unsupported languages. Moreover, SLU is a crucial component of inclusive speech technology for languages without a formal writing system, as these languages cannot rely on coupling ASR with language models to offload semantic understanding. This limitation affects approximately half of all living languages. Despite all this, existing multilingual datasets for evaluating the SLU of speech models have significant limitations. The Minds 14 benchmark assesses SLU on 14 high-resource languages for intent classification, which often only requires identifying shallow semantics, such as detecting specific keywords (Gerz et al., 2021). SpeechTaxi comprises spoken topical classification for Bible verses in 28 diverse languages, and as such does poorly corresponds to real-world usage scenarios (Keller and Glava\u0161, 2024). To address the shortcomings of prior benchmarks, we compile Fleurs-SLU by re-aligning datasets derived from Flores-200 (Flores)."}, {"title": "Related Work", "content": "Multilingual Speech Representation Learning. Modern speech representation models are pre-trained with self-supervised objectives that are optionally followed-up by ASR and speech-to-text translation (S2TT) training. Among these models, mHubert employs the self-supervised wav2vec-BERT 2.0 objective on 90K hours of speech across 147 languages by predicting pseudo-labels derived from clustering raw speech features (Boito et al., 2024). MMS-1b is also pretrained with wav2vec-BERT 2.0 on an large corpus of 491K hours of speech spanning 1,406 languages (Pratap et al., 2024). Whisper-v3 is a Transformer encoder-decoder that has been multi-task pre-trained on multilingual ASR, speech-to-English-text-translation (S2ETT), spoken language identification, and VAD on 680k hours of audio (Radford et al., 2022). SeamlessM4Tv2-Large is a multilingual multimodal translation model (Seamless Communication et al., 2023). It combines a text-to-text translation (T2TT) model pre-trained on 105 languages and a Conformer speech encoder pre-trained with wav2vec-BERT 2.0 on 4.5M hours of audio. The model is then trained on T2TT, S2TT, ASR, and knowledge distillation (KD) objectives. The authors perform KD from the text encoder to the speech encoder by minimizing the KL-divergence between the decoder's token output distributions on bi-modal speech and text data.\nMultilingual SLU. The evaluation of multilingual Speech Language Understanding (SLU) is constrained to a limited set of tasks. SLU has been significantly shaped by task-oriented dialogue (ToD), leading to the creation of datasets focused on ToD-specific tasks such as intent classification and slot filling. These tasks frequently require only basic semantic understanding, often reducing to merely detecting specific keywords. Additionally, commonly used utterance-level SLU tasks like language identification and sentiment classification do not assess content-based understanding but instead rely on phonetic or prosodic features of speech encoding. As a result, the majority of SLU datasets are predominantly in English. Multilingual exceptions are otherwise limited. The Minds14 dataset for intent classification only includes 14 high-resource languages (Gerz et al., 2021). SpeechTaxi offers spoken topical classification for Bible verses in 28 diverse languages, which however do not adequately represent real-world domains (Keller and Glava\u0161, 2024).\nConcurrent Work. Costa-juss\u00e0 et al. (2024) concurrently released the multilingual speech benchmark 2M-BELEBELE for 74 languages. In this dataset, the authors first extend Flores by ca. 20% by incorporating human recordings for both sentences that are part of Flores but were missing"}, {"title": "Fleurs-SLU", "content": "In Fleurs-SLU, we postprocess and joins datasets that are based on Flores to create a massively multilingual SLU benchmark for speech classification and multiple-choice question answering from spoken paragraphs.\n3.1 Core Datasets\nFlores. Flores is a machine translation benchmark consisting of 3,001 sentences extracted from English Wikipedia paragraphs, which have been professionally translated into over 200 languages (Team et al., 2022).\nFleurs. Fleurs comprises 2.3 spoken utterances, on average, per sentence from the public DEV and DEVTEST splits of 102 languages in Flores (Conneau et al., 2022). Fleurs is used to evaluate multilingual ASR, language identification, as well as speech-to-text and text-to-speech translation in all language directions.\nSIB-200. Adelani et al. (2024) refined the topical metadata annotations of sentences in the DEV and DEVTEST splits of Flores into 7 categories. The resulting SIB-200 is a natural language understanding benchmark for 205 language variants.\nBelebele. Belebele is a multiple-choice machine reading comprehension benchmark parallel across 122 language variants (Bandarkar et al., 2024). The authors first reconstruct paragraphs from Flores sentences. Four multiple-choice questions are created per English paragraph and then professionally translated into 121 language variants. Belebele comprises 900 questions, each associated with one of 488 distinct passages."}, {"title": "Benchmark Creation", "content": "We compile Fleurs-SLU by carefully aligning data from the above benchmarks as follows.\n1) Merging Fleurs & Flores. We begin by removing silent instances from Fleurs. We first normalize the loudness to an RMS target level of -25. We next apply voice activity detection using Silero-VAD (Team, 2024). Samples are deemed silent if voice activity is detected in less than 5% of their duration. Lastly, we manually verify our approach on 50 randomly sampled predicted silent instances. We find only one borderline misclassified example, which is very noisy but still comprehensible. We then conservatively merge Fleurs and Flores by matching instances first on exact string match and then by Levenshtein distance of 3 on normalized strings.\n2a) SIB-Fleurs. For each language, we pool instances from the training, validation, and test splits of SIB-200 (Adelani et al., 2024) and align the data with our merged Fleurs-Flores dataset with the same string alignment procedure as before. The data is then regrouped into the training, validation, and test splits of the original Fleurs dataset. This segmentation ensures compatibility for speech models that may be trained on ASR using the training set of Fleurs prior to SIB-Fleurs evaluation. This also ensures that the speakers in the training set are different from those in the validation and test sets. Our final dataset comprises a median of 728 instances for training, 70 for validation, and 174 for testing across 102 languages.\n2b) Belebele-Fleurs. We merge our Fleurs-Flores sentences with Belebele paragraphs by intersecting the URLs of the texts. We discard all paragraphs that are not complete in Fleurs-Flores. We verify our reconstructed paragraphs against the original Belebele by ensuring that the Levenshtein distance for strings with removed punctuation is negligibly small (less than 3 characters). Belebele-Fleurs contains, on average, 709 evaluation instances (with a median of 772) across 92 language variants."}, {"title": "Experimental Setup", "content": "4.1 Tasks and Languages\nSIB-Fleurs. We train MSE on the utterances and CS on the transcriptions of the English training set. For both MSE and CS, we feed the sequence-level representation pooled from token or speech frame embeddings into a classification head. Roberta-Large uses the [CLS] token as a sequence-level embedding. All other models average the token or speech frame output embeddings.\nBelebele-Fleurs. For CS, we fine-tune and validate models on the English training and development sets compiled by Bandarkar et al. (2024), respectively. We jointly embed the paragraph, question, and choices with text encoders. We then average the token representations of each choice $c_i \\in C$ and project the resulting embedding via head $H_{D \\times 1}$ to a logit $I_{c_i}$. We minimize the cross-entropy of the concatenated choice logits ${I_{c_i}}_{i=1}^S$ to the label choice.\n4.2 Cross-Lingual Transfer Setups\nWe experiment on two commonly used cross-lingual transfer (XLT) paradigms. Zero-shot cross-lingual transfer (ZS-XLT) and Translate-Test (TTEST) allow us to evaluate XLT without requiring additional annotation for any target language. In ZS-XLT, we first train a multilingual model on source-language data and then directly run inference on target-language instances. In TTEST, the model is first fine-tuned on labeled source-language data. At test time, the target-language examples are translated to the source language prior to inference, which enables XLT with monolingual LLMs."}, {"title": "Results", "content": "Table 1 summarizes the main results for both tasks by approach across our XLT setups. We dissect the results along several axes of analysis.\nEnglish. The first column presents the in-language (i.e., no-XLT) English performance by task and utterance quality for our models.\nCS. English ASR performance is strong for both transcription models (cf. Figure 3), and Cs effectively leverages the NLU capabilities of LLMs, thanks to their extensive pre-training on both tasks. As a result, CS outperforms all MSE models on SIB-Fleurs. On Belebele-Fleurs, both CS models backed by LLM2Vec and NLLB-LLM2Vec perform comparably. In sum, the transcription model appears to have little to no impact on English performance.\nMSE. The English results on SIB-Fleurs suggest that the SLU capabilities of MSE models are strongly shaped by their pre-training curriculum. MMS-1b, which is pre-trained solely in a self-supervised manner, underperforms other MSE on SIB-Fleurs (44.6%). Post-hoc fine-tuning of MMS-1b on ASR, either on Fleurs (MMS-1B-Fleurs) or Fleurs combined with additional data (MMS-1b-All), results in significant improvements on SIB-Fleurs of +10-20 percentage points (pp). Notably, MMS-1B-Fleurs outperforms MMS-1b-All (+9.6 pp), suggesting that the broader domain mixture in the MMS-1b-All training set negatively affects this downstream task. Whisper-v3-Large surpasses all MMS-1B variants (78.3%). The large-scale pre-training of Whisper-v3 on both multilingual ASR and S2ETT enhances its SLU performance. SeamlessM4Tv2-Large is the best performing MSE on the English test set of SIB-Fleurs (87.4%), with only a slight performance gap compared to CS (ca. -3 pp). The joint pre-training on multilingual ASR as well as multilingual and cross-modal MT with text-to-speech knowledge distillation significantly enriches the semantics in speech representations of SeamlessM4Tv2-Large. The results particularly highlight that (i) multilingual ASR, (ii) S2ETT pre-training, and (iii) text-to-speech distillation are crucial training objectives for enabling speech encoders to acquire strong SLU capabilities.\nUtterance Quality. The quality of English utterances does not seem to affect downstream performance for either MSE or CS across both tasks. This robustness can be attributed to the exhaustive training of all backbone models on English ASR, supported by the large-scale availability of diverse English speech data.\nZS-XLT. The right-hand side of Table 1 reports ZS-XLT performance in various setups. For each task, we group the languages into (i) languages supported by Whisper-v3, (ii) languages supported by SeamlessM4Tv2, and (iii) languages unsupported by either model. The sets of languages of Fleurs that Whisper-v3 and SeamlessM4Tv2 support overlap in 81 languages.\nCS. Across both tasks and all language groups, models trained on transcriptions of SeamlessM4Tv2-Large consistently outperform models fine-tuned on transcriptions of Whisper-v3-Large. The performance gap grows as Whisper's support decreases or becomes unavailable for the target languages (cf. 'SeamlessM4Tv2' and 'unsupported' language groups in Table 1). For SIB-Fleurs, the CS of SeamlessM4Tv2-Large and Roberta-Large deteriorates only slightly in XLT performance to all 101 target language, on average, relative to English (ca. -9.6 pp). Most of this XLT gap comes from the 9 target languages that SeamlessM4Tv2 does not support (-25.6 pp on average). In contrast, pairing Roberta-Large with Whisper-v3-Large causes more pronounced drops on languages that Whisper supports (-10.4 pp) and a much larger deficit on languages neither model supports (-44.6 pp). For Belebele-Fleurs, S2ETT paired with LLM2Vec (i.e., TTEST) outperforms ZS-XLT cascading on in-language transcriptions and NLLB-LLM2Vec, except for unsupported languages. This suggests that S2ETT of both Whisper-v3 and SeamlesseM4Tv2 sufficiently translates the target languages into English for successful NLU. On unsupported languages, however, NLLB-LLM2Vec performs better for likely two reasons. First, for languages with low S2ETT quality, in-language transcriptions to closely related languages likely better preserve the core meanings of input sequences. Second, in addition to translation, NLLB was pre-trained with denoising autoencoding, making NLLB-LLM2Vec more resistant to noisy inputs than LLM2Vec. The findings suggests that SeamlessM4Tv2-Large is a much more robust transcription model for both in-language ASR as well as S2ETT.\nMSE. The zs-XLT performance of MSE on SIB-Fleurs mirrors the trends we observed for English performance. When trained solely with the wav2vec-BERT 2.0 objective, MMS-1B (19%) only slightly surpasses random performance (14%) in ZS-XLT for the highest-quality utterances, regardless of the target language group. In contrast to English, post-hoc modular ASR fine-tuning of MMS-1B with language adapters and language-specific decoding heads, whether on Fleurs alone (MMS-1B-Fleurs) or on Fleurs with additional data (MMS-1B-All), yields much smaller gains in ZS-XLT (ca. +1-3 pp). Furthermore, despite employing fully parameter-shared multilingual ASR and S2ETT pre-training, Whisper-v3-Large fails to transfer strong English (i.e., in-language) performance to other languages effectively, with the XLT gap ranging from -32.4 pp for supported languages to -42.4 pp for unseen languages. In contrast, only SeamlessM4Tv2-Large achieves performance comparable to English on supported languages (-5.2 pp) while maintaining reasonable performance on unsupported languages (65.9%). These findings indicate that multilingual cross-modal machine translation and multilingual text-to-speech token distillation align and semantically enrich multilingual speech representations to significantly enhance cross-lingual SLU.\nUtterance Quality. The utterance quality consistently affects performance on both tasks for all model configurations in ZS-XLT. Notably, CS seem to be less affected than MSE. We attribute this observation to the much more sizable pre-training on text of diverse quality of LLMs compared to speech models. For SIB-Fleurs, the magnitude of the performance gaps between best and worst quality utterances in ZS-XLT mimics how well the speech model backbones perform across both tasks in both Cs and end-to-end speech classification. SeamlessM4Tv2-Large is more robust to noisy utterances than Whisper-v3-Large, whereas MMS variants suffer from the largest drops (ca. 5 pp)."}, {"title": "Further Analyses and Discussion", "content": "ASR & Translation Performance. To further understand the underlying factors behind our main results (cf. Table 1), we benchmark Whisper-v3-Large and SeamlessM4Tv2-Large on ASR and Speech-to-English-text-translation (S2ETT) on all 102 Fleurs languages. We first compute CER and sacreBLEU between the Flores sentences and the in-language transcriptions and S2ETT transcriptions of utterances in Fleurs pooled across all splits, respectively. We then \u2018macro-average' the metrics over all languages. ASR. Whisper-v3-Large outperforms SeamlessM4Tv2-Large for ASR on 58 out of 102 Fleurs languages (cf. 'win ratio'). Nevertheless, SeamlessM4Tv2-Large is the overall more robust transcription model: in transcription for languages other than English, the CER of Whisper-v3-large is about twice as high as the CER of SeamlessM4Tv2-Large, on average. The full per-language results (cf. Appendix A.5)) confirm that SeamlessM4Tv2-Large exhibits much better ASR support for low(er)-resource languages, while being competitive in transcription quality for higher-resource languages.\nS2ETT. SeamlessM4Tv2-Large outperforms Whisper-v3-large across the board in speech-to-English-text-translation and is favored for all but 2 languages. For both groups of languages supported by, Whisper-v3-Large and SeamlessM4Tv2-Large, respectively, SeamlessM4Tv2-Large achieves on average about 8 higher sacreBLEU than Whisper-v3-Large. The gap from SeamlessM4T-Large to Whisper-v3-Large reduces to about 5 sacreBLEU for unsupported languages. This supports the notion that SeamlessM4Tv2-Large is a more robust multilingual speech encoder.\nThe more robust ASR and much stronger translation performance of SeamlessM4Tv2-Large results stem from its pre-training. SeamlessM4Tv2 first initializes a text encoder and decoder with weights from a pre-trained NLLB-200 translation model (Team et al., 2022) and a speech encoder pre-trained on 4.5M hours of self-supervised training on wav2vec-BERT 2.0 objective. The model is then trained on translation objectives from text and speech to text between any two languages in both translation directions. The text encoder-decoder backbone is used to train the speech encoder with token-level knowledge distillation objectives on decoder output representations. On the contrary, Whisper-v3 trains models from scratch on, among others, in-language ASR and S2ETT. Consequently, the mixture of strong initialization from existing MT backbones, text-to-speech knowledge distillation, and multimodal translation objectives result in much stronger translation performance.\nLength Adaptor. The sequence length of encoded speech typically far exceeds the sequence length of embedded tokenized text for the same input. The MSE of SeamlessM4Tv2-Large therefore appends a temporal convolution, called the length adaptor, as its final layer to reduce the resolution of speech frames by a factor of 8 (Zhao et al., 2022; Seamless Communication et al., 2023). For bimodal input, the length adaptor thereby mitigates the discrepancy in sequence lengths between speech encodings and tokenized text embeddings, intending to better align representations across modalities. This raises the question of whether a length adaptor is essential for pooling semantic meaning from speech output tokens. To investigate this, we ablate SeamlessM4Tv2-Large's performance on SIB-Fleurs, comparing results with and without the length adaptor.\nTable 2 presents the results for both variants. The results are mixed. For English, performance improves on high-quality utterances but decreases on lower-quality ones. In contrast, ZS-XLT performance shows a slight but consistent decline, which is more pronounced for lower-quality utterances in supported languages and, irrespective of utterance quality, in unsupported languages. Three factors may explain this modest gap. First, removing the length adaptor reduces model capacity by 47M parameters"}, {"title": "Conclusion", "content": "We introduce Fleurs-SLU, a multilingual SLU benchmark for semantic speech classification across 102 languages and multiple-choice question answering from spoken paragraphs in 92 languages. Using Fleurs-SLU, we evaluate massively multilingual speech models in both end-to-end speech classification and a cascaded approach that combines initial speech-to-text transcription and subsequent text-based classification with LLMs. Our findings indicate that, while cascaded systems remain the most robust option, multilingual speech encoders can achieve competitive performance when adequately pre-trained. Furthermore, we observe a strong correlation between strong multilingual SLU and both the robustness of multilingual ASR and the effectiveness of cross-modal speech translation to English text. This suggests that multilingual SLU and multilingual ASR can be mutually beneficial. We hope that our findings inspire future work towards developing more efficient multilingual speech encoders that are jointly pre-trained for both multilingual ASR and SLU to close the performance gap between end-to-end speech classification and cascaded approaches."}]}