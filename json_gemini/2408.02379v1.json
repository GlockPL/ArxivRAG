{"title": "The Contribution of XAI for the Safe Development and Certification of AI: An Expert-Based Analysis", "authors": ["Benjamin Fresz", "Vincent Philipp G\u00f6bels", "Safa Omri", "Danilo Brajovic", "Andreas Aichele", "Janika Kutz", "Jens Neuh\u00fcttler", "Marco F. Huber"], "abstract": "Developing and certifying safe or so-called trustworthy- AI has become an increasingly salient issue, especially in light of upcoming regulation such as the EU AI Act. In this context, the black-box nature of machine learning models limits the use of conventional avenues of approach towards certifying complex technical systems. As a potential solution, methods to give insights into this black-box-devised in the field of explainable AI (XAI) could be used. In this study, the potential and shortcomings of such methods for the purpose of safe AI development and certification are discussed in 15 qualitative interviews with experts out of the areas of (X)AI and certification. We find that XAI methods can be a helpful asset for safe AI development, as they can show biases and failures of ML-models, but since certification relies on comprehensive and correct information about technical systems, their impact is expected to be limited.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving domain of machine learning (ML), the integration of ML systems into safety-critical applica- tions presents unique challenges, primarily due to the ML- inherent opacity. Often characterized as \"black-box\u201d sys- tems, such models are based on learning patterns instead of being explicitly programmed, thus complicating trans- parency and reliability (Castelvecchi 2016). This opacity not only challenges their integration into environments where safety is paramount but also impedes established system cer- tification processes.\nThe emerging field of eXplainable Artificial Intelligence (XAI) seeks to address this challenge by improving the transparency of ML models (Rai 2020). XAI aims to make the decision-making processes of AI systems comprehen- sible to human stakeholders, thereby potentially increasing their trustworthiness and facilitating their integration into regulated domains (Martinie 2021; Brajovic et al. 2023). De- spite the growing research interest in XAI, its practical util- ity in enhancing the safety and certification of AI systems has not been thoroughly investigated.\nThis paper addresses this gap by examining the poten- tial of XAI in the certification of AI systems. It discusses whether the current state of XAI tools can be integrated into certification processes and evaluates their practical util- ity through the experiences of practitioners. Specifically, this research addresses three primary questions:\n1. Does XAI function as a debugging tool in practice, and what implications does this have for the safety of AI sys- tems?\n2. Is it feasible to incorporate XAI into existing and future certification frameworks for AI systems?\n3. What are the positive and negative experiences of practi- tioners using XAI in the field?\nTo answer these questions, we conducted qualitative inter- views with 15 experts who operate at the intersection of Al development and certification. Furthermore, we make a distinction between XAI's role as an auxiliary tool for de- bugging, which can improve ML model development, and its more consequential role as a certification instrument that could offer assurances about an ML model's functionality. This paper aims to expand the discourse on AI safety by critically analyzing the role of XAI within the certification landscape, assessing both its potential and its limitations. To the best of our knowledge, this paper offers the first in-depth exploration of how XAI can be utilized in the certification and safeguarding of AI systems.\nThe paper is organized as follows: After an introduction into the safe development of technical systems, the related works are presented, with a focus on the context of certi- fication of Al systems. In Section 3, readers are provided with the necessary background knowledge of XAI and re- lated techniques. In Section 4, the methodology and partic- ipant profiles are introduced, followed by the presentation of the interview results in Section 5. Further pathways for XAI and limitations of the used approach are discussed in Section 6. The paper closes with a summary in Section 7."}, {"title": "1.1 Safe Development of Technical Systems", "content": "For non-AI products, safe development and certification pro- cesses are well established, as they are subject to numerous legal and standardized requirements. For example, the Ma- chinery Directive 2006/42/EC of the European Union regu- lates the provisions for placing machinery on the market in the European Economic Area. A key point of this directive is the minimum requirements for safety and health protec- tion. Specific requirements are derived from references to corresponding harmonized standards. For technical systems with AI functionalities, which are the focus of this paper, the area of electrical, electronic and programmable electronic systems is most likely to apply. If a system from this area is developed with a safety function, IEC 61508 or one of its sister standards for specific areas of application (e.g., ISO 26262 for automobiles) applies. These standards describe a process model, methods to be used, and various required ac- tivities and work products. The basic procedure is to identify potential situations that pose a risk to life and limb. The rele- vance or dangerousness of situations is determined by means of a risk assessment. In the case of particularly dangerous situations, further methods must be used to avoid systematic errors. In the case of random faults, a quantitative assess- ment of the components with a maximum permissible prob- ability of failure is required. Companies and/or products are certified to confirm compliance with these requirements. An independent body checks whether the requirements speci- fied in the standard have been met."}, {"title": "2 Related Works", "content": "Although the certification of AI systems is not standardized as of now, multiple scientific publications provide potential avenues of approach. Some of them are presented in the fol- lowing, to give an overview over the challenges and potential solutions for safe AI development and certification."}, {"title": "2.1 AI Certification Challenges", "content": "The certification of AI systems is a complex and evolving challenge, as highlighted by multiple publications (Falcini and Lami 2017; Stoica et al. 2017; Vanderlinde, Robinson, and Mashford 2022; Mahilraj et al. 2023). They acknowl- edge that certifying AI Systems is particularly difficult due to several factors that diverge from those encountered in tra- ditional software certification. Because of this, Falcini and Lami (2017) specifically address the need for new certifica- tion schemes in the automotive industry, while Stoica et al. (2017) emphasize the importance of AI systems that can make safe and timely decisions in unpredictable environ- ments. Multiple publications discuss the need for new ap- proaches to address the challenges of AI certification, with Vanderlinde, Robinson, and Mashford (2022) focusing on the development of potential solutions and Mahilraj et al. (2023) highlighting the issues of robustness, transparency, reliability, and safety in AI systems. Beyond the previ- ously mentioned \"black-box\" nature of AI, where the inter- nal workings and decision-making logic are often opaque (Castelvecchi 2016), two additional challenges are data de- pendency and continuous learning.\nData dependency is a critical issue for Al models. The quality of the data used for training, testing, and validation directly impacts the AI's performance (Landgrebe 2022). Ensuring that datasets are robust, representative, and free of errors or biases is a complex task that diverges from tradi- tional software validation methods, which do not typically rely on large datasets to function correctly. Picard et al. (2020) specifically address the need for dataset engineering in safety-critical systems, while Budach et al. (2022) empir- ically explore the correlation between data quality dimen- sions and ML algorithm performance. The lack of complete understanding of the nuanced interactions between AI mod- els and their training data further complicates certification efforts.\nThe dynamic behavior of AI systems that continue to learn post-deployment introduces an additional layer of un- predictability. Such systems can evolve and adapt, poten- tially developing unforeseen and harmful behaviors that were not identified during the initial certification process. This contrasts with traditional software, where behaviors can be tested and certified against a fixed set of specifica- tions. To overcome this issue, Bakirtzis et al. (2022) propose a dynamic certification approach for autonomous systems, which involves iterative testing and revision of permissi- ble use-context pairs. This approach allows for certification while learning, addressing the uncertainty and heterogeneity of deployment scenarios. Regarding the dynamic behaviour of such AI systems, one of the main challenges is to establish certification processes that are flexible and robust enough to account for and monitor these changes over the lifetime of the AI system (Stodt, Reich, and Clarke 2023)."}, {"title": "2.2 XAI and its Role in Certification", "content": "Regarding the previously described challenges of safe AI de- velopment, several publications propose XAI as a potential solution. Some of them are presented in the following.\nAn intensely debated topic regarding safe AI is au- tonomous driving. As such, several publications describe possible avenues of using XAI in that use case, describing it as a fundamental building block of autonomous driving systems (Kuznietsov et al. 2024; Atakishiyev et al. 2024). In a more general setting, Brajovic et al. (2023) describe a general framework for the documentation of AI (as pre- cursor to certification), based on model cards (Mitchell et al. 2019) and data cards (Pushkarna, Zaldivar, and Kjartansson 2022), including XAI as a potentially necessary part of de- velopment. A similar notion is provided by Martinie (2021), who view XAI as key to make AI in critical interactive systems transparent to users and certification stakeholders. An application for these use cases is shown by Saraf et al. (2020), as they develop a proof of concept tool to generate local explanations for a trajectory anomaly detection model to demonstrate how XAI can help towards user acceptance and certification.\nTo be able to assess transparency in the safe development and certification, robust measures for XAI need to be devel- oped and integrated into AI assessment (Stodt, Reich, and Clarke 2023).\nFurther evidence for the importance of explanations for safe AI can be found in recent standards, e.g. ISO 21448 (Road vehicles - Safety of the intended functionality). This standard recommends analyzing the interpretability of ML software to increase its trustworthiness by showing that de- cisions are based on relevant data and not on artefacts. As such, interpretability analysis can be included as pass/fail criteria into ML validation and/or test verification.\nSome criticism is also levied against such approaches, as Landgrebe (2022) describe how the aim of XAI shifted away from the initial intentions of providing objective understand- ing of ML models. As an alternative to XAI in safe AI devel- opment, they propose \"certified AI\u201d via specification, real- ization and tests, including elements of ontology and formal logic within their AI approach.\nWhile the utility of XAI in enhancing transparency is widely recognized, there is a notable gap in empirical re- search concerning its integration into the certification pro- cesses of Al systems. Most existing studies focus on theo- retical frameworks or specific use case scenarios, with less emphasis on systematic, empirical evaluations of XAI's role in the broader certification processes (Landgrebe 2022; Bra- jovic et al. 2023). This study aims to fill this gap by examin- ing firsthand experiences of XAI in practice and evaluating its potential and limitations in the context of AI certification."}, {"title": "3 XAI Taxonomy", "content": "To contextualize the previously described publications and the interviews in the following chapters, a short overview over the current state of the art of XAI is presented here.\nIn general, different taxonomies for XAI exist, grouping XAI methods mostly by their general function, their type of result or via their underlying concepts (Speith 2022). One of the most common distinction between methods is provided by their \"scope\", e.g., whether XAI methods aim to explain a single decision (local), the functioning of an entire ML model (global) or the dataset of an ML task.\nSome of the earliest and most common-XAI meth- ods are so-called feature-importance methods, either model- agnostic ones like LIME (Ribeiro, Singh, and Guestrin 2016) or SHAP (Lundberg and Lee 2017), or model-specific ones, mainly for computer vision tasks with neural net- works, like Integrated Gradients (Sundararajan, Taly, and Yan 2017). They present their explanations as the \u201cimpor- tance\" of features towards a decision, e.g., via highlighting input values of tabular data or as saliency maps (\"heatmaps\") that highlight (super-)pixels of images.\nBecause such explanations can provide ambiguous and thus, difficult to understand, information by only highlight- ing an area of an image without further information whether the form or the texture of some object is used, concept-based explanations were proposed (Kim et al. 2018). They try to decompose single decisions (or the general logic of an ML model) into human-understandable concepts, e.g., via con- cepts such as \"striped\u201d or \u201csquare-shaped\". In combination with such concepts, data-based explanations can be used, which show data instances similar to the input in question or relevant for the concept(s) used (Achtibat et al. 2023).\nAnother common explanation type are so-called \"coun- terfactual\" explanations. They provide information of the type \"If feature X would have value y, the outcome would be different\", e.g., if a user wants to know why they were not granted a loan. By design, such explanations only pro- vide a limited amount of information to prevent reverse- engineering of the model, but equip users with enough in- formation to be able to challenge a decision or adapt accord- ingly to receive a different outcome (Wachter, Mittelstadt, and Russell 2018).\nAnother way to explain ML decisions is called \"mecha- nistic interpretability\u201d, a bottom-up approach which tries to decompose models into fine-granular explanations by taking their exact computations into account (Bereska and Gavves 2024). Corresponding explanations often entail specific neu- ral circuits that are linked to specific behaviors or con- cepts (comparable to parts of (Achtibat et al. 2023)). Sim- ilarly concerned with exact computational behavior of ML models is the field of formal verification, where methods aim at formal or statistical guarantees for specific proper- ties such as robustness against specific input perturbations (Landers and Doryab 2023). While theoretically sound (and especially thought to be relevant for ML safety (Landers and Doryab 2023)), such approaches often struggle with the computational complexity of neural networks for real-world applications. Often not viewed as part of explainability it- self, the closely related field of uncertainty quantification tries to provide ML decisions with uncertainty estimates, en- abling users to spot potentially unsafe model decisions (Ab- dar et al. 2021). Some authors also call for uncertainty (or \"confidence\") estimates of explanations themselves to show whether a generated explanation should be trusted (Nauta et al. 2023; Fresz et al. 2024).\nAnother approach is to adapt the model structure to in- ject previous knowledge about the data structure and to as- sist in explanation generation. As indicated by their name, graph-based neural networks adapt the network structure to allow the interpretation as a graph, potentially improv- ing data approximation and explainability (Agarwal et al. 2023). Neuro-symbolic approaches combine deep-learning approaches, e.g., for perception tasks (Evans et al. 2021), with classical reasoning, to not rely on post-hoc explana- tions but to understand local decisions and the global logic of the resulting model (Garcez and Lamb 2023).\nSummarizing, a lot of different approaches to explain- ing ML decisions and improving ML safety exist. In this study, their shortcomings and potential in regard to AI certification-as viewed by XAI and certification experts- are evaluated.\""}, {"title": "4 Methodology", "content": "To survey the potential of XAI in general and in certifica- tion processes in particular, 15 interviews were conducted. In the following, the methodology for the interviews is de- scribed, including the participant profiles and the interview and coding process."}, {"title": "4.1 Participant Profiles", "content": "To limit culture-specific influences, all participants either originated from Germany, Austria, or Switzerland or live there permanently. Because of that, most interviews were conducted in German (10), and some in English (5). Inclu- sion criteria for participants are knowledge about AI certi- fication and about XAI, established by current projects or published works about at least one of these topics. Since those requirements limit the number of potential interview candidates, purposive sampling (Guest, Tolley, and Wong 2014) was used and participants were approached from the authors' individual networks. An anonymized list of all par- ticipants can be found in Table 1. Note that two further in- terviews were conducted, but due to a lack of expertise in either (X)AI or certification, they were not considered for further evaluation. Since only a limited set of potential inter- viewees exist, further information about their background is omitted to keep them non-identifiable. Where such informa- tion is relevant as background to certain statements, e.g., in Section 5.4, the most limited amount necessary is given."}, {"title": "4.2 Interview Process", "content": "The interviews for this study were conducted from January to March of 2024 via Microsoft Teams. After the agreement of participants to participate in the study, the general out- line of the interviews was presented, including the following structure:\n\u2022 Participant profile: Participants were asked about their current position, their previous work and their current task relating to AI.\n\u2022 Use of (X)AI: Participants were invited to explain their current aim and use of XAI, since challenges and the state of the art might differ between the aim and field of use.\n\u2022 XAI in certification: After participants spoke about their experience with XAI in general, they were asked about specific challenges and requirements for XAI in the field of certification of AI.\n\u2022 Look into the future: Since most of the previous ques- tions focused on challenges in the field of XAI, partici- pants were invited to share their thoughts and hopes re- garding the future development of XAI.\nFor the full list of interview questions, see Appendix A."}, {"title": "4.3 Coding", "content": "For the coding of the interviews, an inductive qualitative analysis based on (Mayring 2019) was used. Two coders reviewed three transcripts of the interviews, met for an in- termediate coding workshop and finalized the coding. Dur- ing this process, potential ambiguities were marked and dis- cussed in a final coding workshop."}, {"title": "5 Results", "content": "In this chapter, the results of the 15 expert interviews about the use of XAI in general and in certification are discussed. At first, the practitioners' perspective of XAI is presented, followed by their opinions on the use of XAI for certification tasks. The main results are also summarized in Table 2.\nSince the interviews were conducted in a semi-structured manner, interviewees could also remark on more general is- sues of XAI and AI certification. Such statements, which are not commonly found in relevant literature, are presented in Section 5.4. For a more detailed breakdown of the individual interviews, see Table 3 in Appendix B."}, {"title": "5.1 Use of (X)AI", "content": "Aim of XAI Use The participants of the study unani- mously thought of transparency or explainability as an im- portant topic in safe AI development, which was to be ex- pected since all of them are known to work on related topics (see Section 4.1). While they considered transparency and explainability as important, a common thread that emerged is that the integration of appropriate methods into standard development processes is still lacking in most cases. This shortfall is partly due to the perceived lack of sufficient value-addition from explainability to warrant the necessary funding, especially when AI projects are externally commis- sioned. When applied, the objectives of explainability meth- ods are multifaceted and often abstract, encompassing as- pects such as enhancing the public perception of projects, adhering to regulatory or customer requirements, detecting errors in ML systems, and facilitating internal communica- tion about the capabilities and operation of ML methods across different departments (e.g., compliance and ethics checks).\nChoice of XAI Method With the aims of XAI use vary- ing significantly, a clear framework for assessing the perfor- mance of XAI methods was not discernible from the inter- views, also due to the wide range of XAI methods employed. These span from neuro-symbolic ML systems, graph- and concept-based explanations to white-box models, and fea- ture importance methods like SHAP and LIME. The multi- tude of available methods and the ambiguity in evaluating the respective objectives make it challenging for practition- ers to identify the most suitable method for a particular ap- plication. Consequently, methods that are easy to implement and provide accessible information are often chosen. Due to its open-source nature and ease of use, SHAP is commonly used, although the interviewees are aware of the criticisms levied at this method, e.g., by Slack et al. (2020); Kumar et al. (2020), and thus sceptical of its performance and reli- ability.\nExperiences with XAI Despite the challenges described before, the interviewees reported successful applications of XAI procedures, particularly in identifying errors in exist- ing ML systems, conducting plausibility checks on mod- els during development, and enhancing data understanding. However, it was also noted that current XAI methods are not well-suited for all use cases, with projects often failing due to common reasons. These included the incomprehensibil- ity of generated explanations to the target audience, lack of time for experts to interact with the explanations, and diffi- culty in verifying found correlations due to insufficient AI or domain expertise. Further complicating the deployment of XAI is the unstable nature of some methods, leading to non-reproducible results, and the lack of comprehensive re- search on methods for specific data types like time series.\nThe use of XAI methods to foster trust among end-users, often highlighted in scientific literature, was viewed criti- cally in many interviews. The complexity of XAI methods effectively shifts the problem of an untrustworthy black-box (the ML system) to another black-box (the XAI method), the trustworthiness of which is also questioned due to the controversial nature of existing XAI methods. This issue is exemplified by the disagreement problem (Krishna et al. 2022), where different XAI methods provide different ex- planations for a single decision of an ML model, making it unclear what the \u201ctrue\u201d explanation is. Note that the black- box nature of the XAI method stems more from the lack of in-depth expertise about XAI than from a general incom- prehensibility such as for the ML model. Because of this, a potential solution mentioned by P2 to the trust issue created by the double black-box is the provisioning of training on AI and XAI for users."}, {"title": "5.2 XAI in Certification", "content": "Additional to XAI in development, interviewees were asked about their expectations and perceived challenges of XAI in AI certification. Central to this discussion is the challenge of measuring \"appropriate\" transparency and explainability in XAI methods (as demanded by the AI Act), a task that varies significantly depending on the specific purpose and function of the AI system in question.\nUse of XAI in certification Overall, two main groups of opinions about the use of XAI in certification can be distin- guished: From the perspective of some experts, the influence of XAI on the certification of Al systems is seen to be mi- nor. This viewpoint stems from the existence of other regu- latory measures like thresholds for certain performance met- rics for AI systems or the belief that XAI methods, partic- ularly in complex applications, are not and cannot be suffi- ciently robust or comprehensive. In such applications, expla- nations generated by XAI methods themselves become too intricate, thus detracting from their utility. This viewpoint is underpinned by the interviewees almost unanimously agree- ing that explainability is not (an will not be) truly measur- able, or will at least require user studies to do so.\nIn contrast, other experts especially ones who success- fully used XAI in the past-maintain that XAI has demon- strated its potential in improving ML models by identify- ing problems early in the development process. As the main goal of safeguarding and certifying AI is to prevent poten- tially harmful defects, it is argued that XAI-even without quantitative performance metrics-helps towards that goal and should thus be part of AI certification. Interviewees with this viewpoint often additionally pointed out that XAI could only be one of many tools for AI certification (additional to testing, formal verification etc.), providing only a small piece of evidence for certification. Additionally, they em- phasized that human inspectors remain integral to the certi- fication process."}, {"title": "5.3 Expectations for XAI", "content": "Looking towards the future, the expectations and hopes as- sociated with the development of XAI are diverse. While the ideal of achieving complete, globally applicable explana- tions is largely seen as unattainable, some optimism persists around the evolution of new XAI approaches. These include concept-based, mechanistic, and neuro-symbolic methods, which are hoped to enable a new form of explanations elu- cidating the fundamental operation of ML models.\nThe interviewees also highlighted the necessity of user- centric and industry-focused approaches in order to fully re- alize the potential of XAI. While XAI methods are seen as offering the capacity to detect errors in ML systems, and thus should ideally be integrated into the development pro- cesses, other methods are expected to be of higher impor- tance for the certification landscape. These include AI ex- amination by alternate AI systems, formal verification of specific properties, and uncertainty quantification of AI de- cisions. A major challenge for the explainability of AI sys- tems is seen in the difference of new AI paradigms, as future Large Language Models (LLMs) might provide multi-modal inputs and outputs and explanations for e.g. time series or image data need to be fundamentally different than ones for other data types.\nA recurring theme across discussions was the call for clear, definitive requirements for AI certification, such as specific metrics. Without such clear guidance, the intervie- wees felt like companies would lack the available resources and information to ensure that their AI systems comply with the relevant transparency requirements, such as those in the AI Act. This call for clear requirements is complemented by the advocacy for the use of simple, intrinsically interpretable AI solutions, wherever possible. The use of AI in high-risk applications was considered inappropriate by some intervie- wees, emphasizing the need for caution and discretion in AI deployment."}, {"title": "5.4 General Remarks about AI Certification and \u03a7\u0391\u0399", "content": "Additional to the more universal statements on \u03a7\u0391\u0399 and \u03a7\u0391\u0399 for certification, some interviewees also voiced concerns and opinions on specific topics. Since these remarks are not com- monly found throughout literature and believed by the au- thors to add interesting viewpoints to the discussion aimed at by this paper, they are presented in this section. To clarify the distinction between the statement made and additional information provided to contextualize the statements dur- ing the writing of this paper, the initial statement is given in italic. Note that these statements are not direct quotes. Most of them were translated from German and edited for brevity and readability, as the direct quotes were spoken language and embedded in the context of the corresponding interview.\nIncorrect Evidence? P2 + P6: Certification so far exam- ines whether evidence is in line with the requirements of standards and norms. There is no process in place to check whether this evidence is correct.\nIn discussions with the experts, especially P2 and P6, it repeatedly emerged that the certification of AI presents new challenges compared to the existing processes for product certifications. It was specifically pointed out that certifica- tion so far has been examining whether evidence provided by manufacturers conforms to the requirements of standards and norms. It is assumed that this evidence is correct, mean- ing it corresponds with the used processes or the actually developed product. If evidence is now to be generated us- ing XAI, this assumption may not be correct. Especially, since malicious actors may be able to generate almost ar- bitrary explanations for their ML systems, even with estab- lished methods (Slack et al. 2020; Zhou and Joachims 2023). While providing incorrect evidence could be a possibility for classical systems as well, incorrect evidence might be pro- vided by XAI without malicious intent of the stakeholders. For such cases, responsibilities must be clarified: Do man- ufacturers guarantee the correctness of the evidence, which will hardly be possible with the current state of XAI, or must certifiers in the future consider the generation process of the evidence, which in turn requires in-depth knowledge regard- ing AI in general and XAI in particular?\nP5: Until now, the \u201cUniformity Hypothesis\" and the \u201cCompetent Programmer Hypothesis\u201d were helpful pieces of building and certifying safety-critial systems.\nSimilar to the point above, some previous assumptions might not hold for AI certification. Usually, the here men- tioned \"Uniformity Hypothesis\" (Sterling 1969) has been applied, as it describes that specific data points can be se- lected for tests, whose findings generalize across an equiv- alence class of similar data, and the \"Competent Program- mer Hypothesis\" (DeMillo, Lipton, and Sayward 1978), which postulates that safety-relevant software does not pro- duce completely unpredictable errors because it was created by a competent programmer who can avoid errors that ap- pear random (e.g., by buffer overflows or pointers, as com- mented by P5). However, both assumptions are violated by the black-box nature of AI: For the generalizability of tests on certain data, equivalence classes are difficult to find and the decision-making process of an ML system was not ex- plicitly programmed. Due to the complexity of ML mod- els, resulting errors may appear random. Nevertheless, P5 noted that a good step towards safer AI is to document the assumptions made in AI and XAI, which is often not done for assumptions such as the \"Uniformity Hypothesis\" and the \"Competent Programmer Hypothesis\" in classical soft- ware development. Regarding the criticism faced by some assumptions in XAI, P7 explicitly pointed out that scientific progress typically comes from challenging existing ideas. In the field of XAI, this leads to a complexity that is difficult for practitioners to penetrate. Initially, XAI was proposed for the evaluation or testing of XAI itself, but now there are also metrics for the testing of XAI, and even metrics for evalu- ating those metrics (Tomsett et al. 2020). Consequently, a goal of applied research could now be to make explicit rec- ommendations on how to select XAI methods for specific use cases.\nFundamental Changes in Certification P1: If AI is to be certified, there needs to be a discussion about a shift from value-based to utilitarism-based certification.\nDue to the uncertainties in testing AI systems, an expert speculated that the culture of certification has to fundamen- tally change to accommodate AI: Existing certification is principally guided by societal values and norms. For exam- ple, for the norm \"safety\" of a technical system, a threshold can be defined, which can then be adhered to based on a detailed analysis of an overall system, for example through methods such as Failure Mode and Effect Analysis (Stama- tis 2003). If this value is not maintained, countermeasures must accordingly be taken from the development side. A particularly well-known example of the traceability of eth- ical values in technical systems can be found in the field of autonomous driving, the so-called \u201ctrolley problem", "If the use of AI is ex- pected to result in fewer injuries or fatalities in traffic, then its use is sensible.\"\nResponsibility for Explainability Requirements P1 + P2": "Explainability is more of a societal than technical topic", "neuroscience)": "For AI", "intuitive": "and system 2 think- ing (slow", "XAI": "XAI could produce a justification for ML behavior that is understandable to humans (system 2)", "transpar- ent": "i.e.", "P9": "In research, cognitive load and interaction time with ex- planations are often not explicitly considered.\nMultiple participants criticized that in XAI research, the explicit experience and aims of domain experts are not con- sidered enough. They noted that XAI research seems to operate under the assumption that complete explanations should be generated in all circumstances. In contrast, do- main experts, such as physicians, typically only require ex- planations in specific instances.\nFurthermore, users are more likely to interact with and have a positive experience with explanations that serve to re- duce the cognitive load associated with the task at hand. The majority of users, in their daily routines, lack the time and cognitive resources to engage with overly complex explana- tions. This effectively undermines the core objective of XA\u0399, which is to make AI more accessible. To make explanations easier to understand, P1 mentioned that explanations need to be contextualized to fulfill their potential, which could po- tentially increase or decrease the cognitive load based on the specific implementation. While interaction time and cogni- tive load are not commonly evaluated in XAI"}]}