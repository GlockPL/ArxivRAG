{"title": "The Contribution of XAI for the Safe Development and Certification of AI: An Expert-Based Analysis", "authors": ["Benjamin Fresz", "Vincent Philipp G\u00f6bels", "Safa Omri", "Danilo Brajovic", "Andreas Aichele", "Janika Kutz", "Jens Neuh\u00fcttler", "Marco F. Huber"], "abstract": "Developing and certifying safe or so-called trustworthy-AI has become an increasingly salient issue, especially in light of upcoming regulation such as the EU AI Act. In this context, the black-box nature of machine learning models limits the use of conventional avenues of approach towards certifying complex technical systems. As a potential solution, methods to give insights into this black-box-devised in the field of explainable AI (XAI) could be used. In this study, the potential and shortcomings of such methods for the purpose of safe AI development and certification are discussed in 15 qualitative interviews with experts out of the areas of (X)AI and certification. We find that XAI methods can be a helpful asset for safe AI development, as they can show biases and failures of ML-models, but since certification relies on comprehensive and correct information about technical systems, their impact is expected to be limited.", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving domain of machine learning (ML), the integration of ML systems into safety-critical applications presents unique challenges, primarily due to the ML-inherent opacity. Often characterized as \"black-box\u201d systems, such models are based on learning patterns instead of being explicitly programmed, thus complicating transparency and reliability (Castelvecchi 2016). This opacity not only challenges their integration into environments where safety is paramount but also impedes established system certification processes.\n\nThe emerging field of eXplainable Artificial Intelligence (XAI) seeks to address this challenge by improving the transparency of ML models (Rai 2020). XAI aims to make the decision-making processes of AI systems comprehensible to human stakeholders, thereby potentially increasing their trustworthiness and facilitating their integration into regulated domains (Martinie 2021; Brajovic et al. 2023). Despite the growing research interest in XAI, its practical utility in enhancing the safety and certification of AI systems has not been thoroughly investigated.\n\nThis paper addresses this gap by examining the potential of XAI in the certification of AI systems. It discusses whether the current state of XAI tools can be integrated into certification processes and evaluates their practical utility through the experiences of practitioners. Specifically, this research addresses three primary questions:\n\n1.  Does XAI function as a debugging tool in practice, and what implications does this have for the safety of AI systems?\n\n2.  Is it feasible to incorporate XAI into existing and future certification frameworks for AI systems?\n\n3.  What are the positive and negative experiences of practitioners using XAI in the field?\n\nTo answer these questions, we conducted qualitative interviews with 15 experts who operate at the intersection of Al development and certification. Furthermore, we make a distinction between XAI's role as an auxiliary tool for debugging, which can improve ML model development, and its more consequential role as a certification instrument that could offer assurances about an ML model's functionality. This paper aims to expand the discourse on AI safety by critically analyzing the role of XAI within the certification landscape, assessing both its potential and its limitations. To the best of our knowledge, this paper offers the first in-depth exploration of how XAI can be utilized in the certification and safeguarding of AI systems.\n\nThe paper is organized as follows: After an introduction into the safe development of technical systems, the related works are presented, with a focus on the context of certification of Al systems. In Section 3, readers are provided with the necessary background knowledge of XAI and related techniques. In Section 4, the methodology and participant profiles are introduced, followed by the presentation of the interview results in Section 5. Further pathways for XAI and limitations of the used approach are discussed in Section 6. The paper closes with a summary in Section 7."}, {"title": "1.1 Safe Development of Technical Systems", "content": "For non-AI products, safe development and certification processes are well established, as they are subject to numerous legal and standardized requirements. For example, the Machinery Directive 2006/42/EC of the European Union regulates the provisions for placing machinery on the market in the European Economic Area. A key point of this directive is the minimum requirements for safety and health protection. Specific requirements are derived from references to corresponding harmonized standards. For technical systems with AI functionalities, which are the focus of this paper, the area of electrical, electronic and programmable electronic systems is most likely to apply. If a system from this area is developed with a safety function, IEC 61508 or one of its sister standards for specific areas of application (e.g., ISO 26262 for automobiles) applies. These standards describe a process model, methods to be used, and various required activities and work products. The basic procedure is to identify potential situations that pose a risk to life and limb. The relevance or dangerousness of situations is determined by means of a risk assessment. In the case of particularly dangerous situations, further methods must be used to avoid systematic errors. In the case of random faults, a quantitative assessment of the components with a maximum permissible probability of failure is required. Companies and/or products are certified to confirm compliance with these requirements. An independent body checks whether the requirements specified in the standard have been met."}, {"title": "2 Related Works", "content": "Although the certification of AI systems is not standardized as of now, multiple scientific publications provide potential avenues of approach. Some of them are presented in the following, to give an overview over the challenges and potential solutions for safe AI development and certification."}, {"title": "2.1 AI Certification Challenges", "content": "The certification of AI systems is a complex and evolving challenge, as highlighted by multiple publications (Falcini and Lami 2017; Stoica et al. 2017; Vanderlinde, Robinson, and Mashford 2022; Mahilraj et al. 2023). They acknowledge that certifying AI Systems is particularly difficult due to several factors that diverge from those encountered in traditional software certification. Because of this, Falcini and Lami (2017) specifically address the need for new certification schemes in the automotive industry, while Stoica et al. (2017) emphasize the importance of AI systems that can make safe and timely decisions in unpredictable environments. Multiple publications discuss the need for new approaches to address the challenges of AI certification, with Vanderlinde, Robinson, and Mashford (2022) focusing on the development of potential solutions and Mahilraj et al. (2023) highlighting the issues of robustness, transparency, reliability, and safety in AI systems. Beyond the previously mentioned \"black-box\" nature of AI, where the internal workings and decision-making logic are often opaque (Castelvecchi 2016), two additional challenges are data dependency and continuous learning.\n\nData dependency is a critical issue for Al models. The quality of the data used for training, testing, and validation directly impacts the AI's performance (Landgrebe 2022). Ensuring that datasets are robust, representative, and free of errors or biases is a complex task that diverges from traditional software validation methods, which do not typically rely on large datasets to function correctly. Picard et al. (2020) specifically address the need for dataset engineering in safety-critical systems, while Budach et al. (2022) empirically explore the correlation between data quality dimensions and ML algorithm performance. The lack of complete understanding of the nuanced interactions between AI models and their training data further complicates certification efforts.\n\nThe dynamic behavior of AI systems that continue to learn post-deployment introduces an additional layer of unpredictability. Such systems can evolve and adapt, potentially developing unforeseen and harmful behaviors that were not identified during the initial certification process. This contrasts with traditional software, where behaviors can be tested and certified against a fixed set of specifications. To overcome this issue, Bakirtzis et al. (2022) propose a dynamic certification approach for autonomous systems, which involves iterative testing and revision of permissible use-context pairs. This approach allows for certification while learning, addressing the uncertainty and heterogeneity of deployment scenarios. Regarding the dynamic behaviour of such AI systems, one of the main challenges is to establish certification processes that are flexible and robust enough to account for and monitor these changes over the lifetime of the AI system (Stodt, Reich, and Clarke 2023)."}, {"title": "2.2 XAI and its Role in Certification", "content": "Regarding the previously described challenges of safe AI development, several publications propose XAI as a potential solution. Some of them are presented in the following.\n\nAn intensely debated topic regarding safe AI is autonomous driving. As such, several publications describe possible avenues of using XAI in that use case, describing it as a fundamental building block of autonomous driving systems (Kuznietsov et al. 2024; Atakishiyev et al. 2024).\n\nIn a more general setting, Brajovic et al. (2023) describe a general framework for the documentation of AI (as precursor to certification), based on model cards (Mitchell et al. 2019) and data cards (Pushkarna, Zaldivar, and Kjartansson 2022), including XAI as a potentially necessary part of development. A similar notion is provided by Martinie (2021), who view XAI as key to make AI in critical interactive systems transparent to users and certification stakeholders. An application for these use cases is shown by Saraf et al. (2020), as they develop a proof of concept tool to generate local explanations for a trajectory anomaly detection model to demonstrate how XAI can help towards user acceptance and certification.\n\nTo be able to assess transparency in the safe development and certification, robust measures for XAI need to be developed and integrated into AI assessment (Stodt, Reich, and Clarke 2023).\n\nFurther evidence for the importance of explanations for safe AI can be found in recent standards, e.g. ISO 21448 (Road vehicles - Safety of the intended functionality). This standard recommends analyzing the interpretability of ML software to increase its trustworthiness by showing that decisions are based on relevant data and not on artefacts. As such, interpretability analysis can be included as pass/fail criteria into ML validation and/or test verification.\n\nSome criticism is also levied against such approaches, as Landgrebe (2022) describe how the aim of XAI shifted away from the initial intentions of providing objective understanding of ML models. As an alternative to XAI in safe AI development, they propose \"certified AI\u201d via specification, realization and tests, including elements of ontology and formal logic within their AI approach.\n\nWhile the utility of XAI in enhancing transparency is widely recognized, there is a notable gap in empirical research concerning its integration into the certification processes of Al systems. Most existing studies focus on theoretical frameworks or specific use case scenarios, with less emphasis on systematic, empirical evaluations of XAI's role in the broader certification processes (Landgrebe 2022; Brajovic et al. 2023). This study aims to fill this gap by examining firsthand experiences of XAI in practice and evaluating its potential and limitations in the context of AI certification."}, {"title": "3 XAI Taxonomy", "content": "To contextualize the previously described publications and the interviews in the following chapters, a short overview over the current state of the art of XAI is presented here.\n\nIn general, different taxonomies for XAI exist, grouping XAI methods mostly by their general function, their type of result or via their underlying concepts (Speith 2022). One of the most common distinction between methods is provided by their \"scope\", e.g., whether XAI methods aim to explain a single decision (local), the functioning of an entire ML model (global) or the dataset of an ML task.\n\nSome of the earliest and most common-XAI methods are so-called feature-importance methods, either model-agnostic ones like LIME (Ribeiro, Singh, and Guestrin 2016) or SHAP (Lundberg and Lee 2017), or model-specific ones, mainly for computer vision tasks with neural networks, like Integrated Gradients (Sundararajan, Taly, and Yan 2017). They present their explanations as the \u201cimportance\" of features towards a decision, e.g., via highlighting input values of tabular data or as saliency maps (\u201cheatmaps\u201d) that highlight (super-)pixels of images.\n\nBecause such explanations can provide ambiguous and thus, difficult to understand, information by only highlighting an area of an image without further information whether the form or the texture of some object is used, concept-based explanations were proposed (Kim et al. 2018). They try to decompose single decisions (or the general logic of an ML model) into human-understandable concepts, e.g., via concepts such as \u201cstriped\u201d or \u201csquare-shaped\u201d. In combination with such concepts, data-based explanations can be used, which show data instances similar to the input in question or relevant for the concept(s) used (Achtibat et al. 2023).\n\nAnother common explanation type are so-called \u201ccounterfactual\u201d explanations. They provide information of the type \"If feature X would have value y, the outcome would be different\", e.g., if a user wants to know why they were not granted a loan. By design, such explanations only provide a limited amount of information to prevent reverse-engineering of the model, but equip users with enough information to be able to challenge a decision or adapt accordingly to receive a different outcome (Wachter, Mittelstadt, and Russell 2018).\n\nAnother way to explain ML decisions is called \u201cmechanistic interpretability\u201d, a bottom-up approach which tries to decompose models into fine-granular explanations by taking their exact computations into account (Bereska and Gavves 2024). Corresponding explanations often entail specific neural circuits that are linked to specific behaviors or concepts (comparable to parts of (Achtibat et al. 2023)). Similarly concerned with exact computational behavior of ML models is the field of formal verification, where methods aim at formal or statistical guarantees for specific properties such as robustness against specific input perturbations (Landers and Doryab 2023). While theoretically sound (and especially thought to be relevant for ML safety (Landers and Doryab 2023)), such approaches often struggle with the computational complexity of neural networks for real-world applications. Often not viewed as part of explainability itself, the closely related field of uncertainty quantification tries to provide ML decisions with uncertainty estimates, enabling users to spot potentially unsafe model decisions (Abdar et al. 2021). Some authors also call for uncertainty (or \"confidence\") estimates of explanations themselves to show whether a generated explanation should be trusted (Nauta et al. 2023; Fresz et al. 2024).\n\nAnother approach is to adapt the model structure to inject previous knowledge about the data structure and to assist in explanation generation. As indicated by their name, graph-based neural networks adapt the network structure to allow the interpretation as a graph, potentially improving data approximation and explainability (Agarwal et al. 2023). Neuro-symbolic approaches combine deep-learning approaches, e.g., for perception tasks (Evans et al. 2021), with classical reasoning, to not rely on post-hoc explanations but to understand local decisions and the global logic of the resulting model (Garcez and Lamb 2023).\n\nSummarizing, a lot of different approaches to explaining ML decisions and improving ML safety exist. In this study, their shortcomings and potential in regard to AI certification-as viewed by XAI and certification experts-are evaluated."}, {"title": "4 Methodology", "content": "To survey the potential of XAI in general and in certification processes in particular, 15 interviews were conducted. In the following, the methodology for the interviews is described, including the participant profiles and the interview and coding process."}, {"title": "4.1 Participant Profiles", "content": "To limit culture-specific influences, all participants either originated from Germany, Austria, or Switzerland or live there permanently. Because of that, most interviews were conducted in German (10), and some in English (5). Inclusion criteria for participants are knowledge about AI certification and about XAI, established by current projects or published works about at least one of these topics. Since those requirements limit the number of potential interview candidates, purposive sampling (Guest, Tolley, and Wong 2014) was used and participants were approached from the authors' individual networks. An anonymized list of all participants can be found in . Note that two further interviews were conducted, but due to a lack of expertise in either (X)AI or certification, they were not considered for further evaluation. Since only a limited set of potential interviewees exist, further information about their background is omitted to keep them non-identifiable. Where such information is relevant as background to certain statements, e.g., in Section 5.4, the most limited amount necessary is given."}, {"title": "4.2 Interview Process", "content": "The interviews for this study were conducted from January to March of 2024 via Microsoft Teams. After the agreement of participants to participate in the study, the general outline of the interviews was presented, including the following structure:\n\n\u2022  Participant profile: Participants were asked about their current position, their previous work and their current task relating to AI.\n\n\u2022  Use of (X)AI: Participants were invited to explain their current aim and use of XAI, since challenges and the state of the art might differ between the aim and field of use.\n\n\u2022  XAI in certification: After participants spoke about their experience with XAI in general, they were asked about specific challenges and requirements for XAI in the field of certification of AI.\n\n\u2022  Look into the future: Since most of the previous questions focused on challenges in the field of XAI, participants were invited to share their thoughts and hopes regarding the future development of XAI."}, {"title": "4.3 Coding", "content": "For the coding of the interviews, an inductive qualitative analysis based on (Mayring 2019) was used. Two coders reviewed three transcripts of the interviews, met for an intermediate coding workshop and finalized the coding. During this process, potential ambiguities were marked and discussed in a final coding workshop."}, {"title": "5 Results", "content": "In this chapter, the results of the 15 expert interviews about the use of XAI in general and in certification are discussed. At first, the practitioners' perspective of XAI is presented, followed by their opinions on the use of XAI for certification tasks. The main results are also summarized in .\n\nSince the interviews were conducted in a semi-structured manner, interviewees could also remark on more general issues of XAI and AI certification. Such statements, which are not commonly found in relevant literature, are presented in Section 5.4."}, {"title": "5.1 Use of (X)AI", "content": "Aim of XAI Use The participants of the study unanimously thought of transparency or explainability as an important topic in safe AI development, which was to be expected since all of them are known to work on related topics (see Section 4.1). While they considered transparency and explainability as important, a common thread that emerged is that the integration of appropriate methods into standard development processes is still lacking in most cases. This shortfall is partly due to the perceived lack of sufficient value-addition from explainability to warrant the necessary funding, especially when AI projects are externally commissioned. When applied, the objectives of explainability methods are multifaceted and often abstract, encompassing aspects such as enhancing the public perception of projects, adhering to regulatory or customer requirements, detecting errors in ML systems, and facilitating internal communication about the capabilities and operation of ML methods across different departments (e.g., compliance and ethics checks).\n\nChoice of XA\u0399 Method With the aims of XAI use varying significantly, a clear framework for assessing the performance of XAI methods was not discernible from the interviews, also due to the wide range of XAI methods employed. These span from neuro-symbolic ML systems, graph- and concept-based explanations to white-box models, and feature importance methods like SHAP and LIME. The multitude of available methods and the ambiguity in evaluating the respective objectives make it challenging for practitioners to identify the most suitable method for a particular application. Consequently, methods that are easy to implement and provide accessible information are often chosen. Due to its open-source nature and ease of use, SHAP is commonly used, although the interviewees are aware of the criticisms levied at this method, e.g., by Slack et al. (2020); Kumar et al. (2020), and thus sceptical of its performance and reliability."}, {"title": "5.2 XAI in Certification", "content": "Additional to XAI in development, interviewees were asked about their expectations and perceived challenges of XAI in AI certification. Central to this discussion is the challenge of measuring \"appropriate\" transparency and explainability in XAI methods (as demanded by the AI Act), a task that varies significantly depending on the specific purpose and function of the AI system in question.\n\nUse of XAI in certification Overall, two main groups of opinions about the use of XAI in certification can be distinguished: From the perspective of some experts, the influence of XAI on the certification of Al systems is seen to be minor. This viewpoint stems from the existence of other regulatory measures like thresholds for certain performance metrics for AI systems or the belief that XAI methods, particularly in complex applications, are not and cannot be sufficiently robust or comprehensive. In such applications, explanations generated by XAI methods themselves become too intricate, thus detracting from their utility. This viewpoint is underpinned by the interviewees almost unanimously agreeing that explainability is not (an will not be) truly measurable, or will at least require user studies to do so.\n\nIn contrast, other experts especially ones who successfully used XAI in the past-maintain that XAI has demonstrated its potential in improving ML models by identifying problems early in the development process. As the main goal of safeguarding and certifying AI is to prevent potentially harmful defects, it is argued that XAI-even without quantitative performance metrics-helps towards that goal and should thus be part of AI certification. Interviewees with this viewpoint often additionally pointed out that XAI could only be one of many tools for AI certification (additional to testing, formal verification etc.), providing only a small piece of evidence for certification. Additionally, they emphasized that human inspectors remain integral to the certification process."}, {"title": "5.3 Expectations for XAI", "content": "Looking towards the future, the expectations and hopes associated with the development of XAI are diverse. While the ideal of achieving complete, globally applicable explanations is largely seen as unattainable, some optimism persists around the evolution of new XAI approaches. These include concept-based, mechanistic, and neuro-symbolic methods, which are hoped to enable a new form of explanations elucidating the fundamental operation of ML models.\n\nThe interviewees also highlighted the necessity of user-centric and industry-focused approaches in order to fully realize the potential of XAI. While XAI methods are seen as offering the capacity to detect errors in ML systems, and thus should ideally be integrated into the development processes, other methods are expected to be of higher importance for the certification landscape. These include AI examination by alternate AI systems, formal verification of specific properties, and uncertainty quantification of AI decisions. A major challenge for the explainability of AI systems is seen in the difference of new AI paradigms, as future Large Language Models (LLMs) might provide multi-modal inputs and outputs and explanations for e.g. time series or image data need to be fundamentally different than ones for other data types.\n\nA recurring theme across discussions was the call for clear, definitive requirements for AI certification, such as specific metrics. Without such clear guidance, the interviewees felt like companies would lack the available resources and information to ensure that their AI systems comply with the relevant transparency requirements, such as those in the AI Act. This call for clear requirements is complemented by the advocacy for the use of simple, intrinsically interpretable AI solutions, wherever possible. The use of AI in high-risk applications was considered inappropriate by some interviewees, emphasizing the need for caution and discretion in AI deployment."}, {"title": "5.4 General Remarks about AI Certification and \u03a7\u0391\u0399", "content": "Additional to the more universal statements on X\u0391\u0399 and \u03a7\u0391\u0399 for certification, some interviewees also voiced concerns and opinions on specific topics. Since these remarks are not commonly found throughout literature and believed by the authors to add interesting viewpoints to the discussion aimed at by this paper, they are presented in this section. To clarify the distinction between the statement made and additional information provided to contextualize the statements during the writing of this paper, the initial statement is given in italic. Note that these statements are not direct quotes. Most of them were translated from German and edited for brevity and readability, as the direct quotes were spoken language and embedded in the context of the corresponding interview.\n\nIncorrect Evidence? P2 + P6: Certification so far examines whether evidence is in line with the requirements of standards and norms. There is no process in place to check whether this evidence is correct.\n\nIn discussions with the experts, especially P2 and P6, it repeatedly emerged that the certification of AI presents new challenges compared to the existing processes for product certifications. It was specifically pointed out that certification so far has been examining whether evidence provided by manufacturers conforms to the requirements of standards and norms. It is assumed that this evidence is correct, meaning it corresponds with the used processes or the actually developed product. If evidence is now to be generated using XAI, this assumption may not be correct. Especially, since malicious actors may be able to generate almost arbitrary explanations for their ML systems, even with established methods (Slack et al. 2020; Zhou and Joachims 2023). While providing incorrect evidence could be a possibility for classical systems as well, incorrect evidence might be provided by XAI without malicious intent of the stakeholders. For such cases, responsibilities must be clarified: Do manufacturers guarantee the correctness of the evidence, which will hardly be possible with the current state of XAI, or must certifiers in the future consider the generation process of the evidence, which in turn requires in-depth knowledge regarding AI in general and XAI in particular?\n\nP5: Until now, the \u201cUniformity Hypothesis\u201d and the \u201cCompetent Programmer Hypothesis\u201d were helpful pieces of building and certifying safety-critial systems.\n\nSimilar to the point above, some previous assumptions might not hold for AI certification. Usually, the here mentioned \"Uniformity Hypothesis\" (Sterling 1969) has been applied, as it describes that specific data points can be selected for tests, whose findings generalize across an equivalence class of similar data, and the \"Competent Programmer Hypothesis\" (DeMillo, Lipton, and Sayward 1978), which postulates that safety-relevant software does not produce completely unpredictable errors because it was created by a competent programmer who can avoid errors that appear random (e.g., by buffer overflows or pointers, as commented by P5). However, both assumptions are violated by the black-box nature of AI: For the generalizability of tests on certain data, equivalence classes are difficult to find and the decision-making process of an ML system was not explicitly programmed. Due to the complexity of ML models, resulting errors may appear random. Nevertheless, P5 noted that a good step towards safer AI is to document the assumptions made in AI and XAI, which is often not done for assumptions such as the \"Uniformity Hypothesis\" and the \"Competent Programmer Hypothesis\" in classical software development. Regarding the criticism faced by some assumptions in XAI, P7 explicitly pointed out that scientific progress typically comes from challenging existing ideas. In the field of XAI, this leads to a complexity that is difficult for practitioners to penetrate. Initially, XAI was proposed for the evaluation or testing of XAI itself, but now there are also metrics for the testing of XAI, and even metrics for evaluating those metrics (Tomsett et al. 2020). Consequently, a goal of applied research could now be to make explicit recommendations on how to select XAI methods for specific use cases.\n\nFundamental Changes in Certification P1: If AI is to be certified, there needs to be a discussion about a shift from value-based to utilitarism-based certification.\n\nDue to the uncertainties in testing AI systems, an expert speculated that the culture of certification has to fundamentally change to accommodate AI: Existing certification is principally guided by societal values and norms. For example, for the norm \"safety\" of a technical system, a threshold can be defined, which can then be adhered to based on a detailed analysis of an overall system, for example through methods such as Failure Mode and Effect Analysis (Stamatis 2003). If this value is not maintained, countermeasures must accordingly be taken from the development side. A particularly well-known example of the traceability of ethical values in technical systems can be found in the field of autonomous driving, the so-called \u201ctrolley problem\". In this scenario, an immediate choice must be made before an accident as to which involved individuals are subjected to a higher risk of severe injuries or potentially death. For AI, however, such thresholds and ethical decisions are currently not sufficiently determinable. Therefore, the expert suspected that the use of AI might need to be assessed more from utilitarian viewpoints, meaning \"If the use of AI is expected to result in fewer injuries or fatalities in traffic, then its use is sensible.\"\n\nResponsibility for Explainability Requirements P1 +\nP2: Explainability is more of a societal than technical topic, as such the standardization bodies are not well equipped to deal with it.\n\nTo be able to certify a technical system, standards are used. Tasked with creating such standards are organizations such as DIN (for Germany), CEN/CENELEC (European Committee for Electrotechnical Standardization), or ISO (International Organization for Standardization). As these organizations commonly create technical standards, P1 and P2 argued that issues such as explainability and fundamental considerations like the compliance with and negotiation of ethical values (see above) should not be technical discussions, but rather socio-political debates. It is also particularly noteworthy that while technical evaluation methods for XA\u0391\u0399 do exist, they were not considered to be effective by the majority of study participants, which suggests that purely technical standardization is unlikely to resolve the open questions surrounding the assessment and certification of AI. P2 additionally noted that participants of standardization committees might lack the time to be well informed about topics as current as XAI (due to other obligations), thus resulting in standards that might not represent the current state of science.\n\nFundamental Doubts about XAI P3 (with a background in neuroscience): For AI, the requirements are stricter than ever possible for humans. At best, XAI might provide justifications, while the only possible explanation for an Al system is its complete calculation from input to output.\n\nAround the topic of AI certification, there exists a discussion of whether AI should be subject to stricter requirements than humans doing the same task (as also touched on by Fresz et al. (2024)). P3 extended this by linking explanations to the description of thought processes provided by Kahneman (2012), dividing thought processes in system 1 thinking (fast, low effort, 'intuitive') and system 2 thinking (slow, high effort, deliberate). P3 argued that humans may justify their behavior upon request after the fact (system 2), but such justifications are not identical to the actual motives, especially for decisions that are often made intuitively (system 1). They suggested that the same applies to XAI: XAI could produce a justification for ML behavior that is understandable to humans (system 2), but the true explanation could only be found within the computational chain of the ML system and, although fundamentally 'transparent' (i.e., visible), not entirely understandable to humans due to the potentially huge number of calculations made by the ML system. It could be argued here that the ideal conception of XAI enables the computation chain to be summarized in such a way that a correct explanation is produced (e.g., via concepts), which provides users with insights into the ML behavior.\n\nDifferences between Research and Practice P7 + P8 +\nP9: In research, cognitive load and interaction time with explanations are often not explicitly considered.\n\nMultiple participants criticized that in XAI research, the explicit experience and aims of domain experts are not considered enough. They noted that XAI research seems to operate under the assumption that complete explanations should be generated in all circumstances. In contrast, domain experts, such as physicians, typically only require explanations in specific instances.\n\nFurthermore, users are more likely to interact with and have a positive experience with explanations that serve to reduce the cognitive load associated with the task at hand. The majority of users, in their daily routines, lack the time and cognitive resources to engage with overly complex explanations. This effectively undermines the core objective of XA\u0399, which is to make AI more accessible. To make explanations easier to understand, P1 mentioned that explanations need to be contextualized to fulfill their potential, which could potentially increase or decrease the cognitive load based on the specific implementation. While interaction time and cognitive load are not commonly evaluated in XAI literature, there is some existing research that explores the idea of reducing the cognitive load of explanations (Herm 2023). This includes enforcing user interaction with explanations prior to presenting the ML recommendation (Miller 2023).\n\nNew Paradigms for XAI P8: For the field of XAI, I am particularly optimistic about the feedback of XAI information into ML training.\n\nP8 identified the combination of the explanation process with the associated model improvement as particularly promising in the field of XAI. So far, XAI has mostly been viewed unidirectionally-even if errors and biases can be identified in existing models, there is no simple way yet to intervene in the model or training data to correct existing problems. A new paradigm (similar to the one proposed by Pahde et al. (2023)) could offer the possibility to interact with explanations, correct them, and integrate these corrections back into the model training process. Thus, insights gained from XAI could be efficiently used for the error correction of ML models.\""}, {"title": "6 Discussion", "content": "The conducted expert interviews illuminate potential pathways for the advancement of XAI, which will be explored in this section. Additionally, constraints and limitations inherent in the study's design are addressed."}, {"title": "6.1 Integration of Diverse Expert Perspectives", "content": "Our research integrates insights from experts with dual expertise in XAI and certification. The diversity in expertise enriched the analysis, providing a well-rounded understanding of both the potential and limitations of XAI in AI certification processes. While our initial expectations anticipated these insights, the nuanced opinions offered by participants exceeded our predictions, underscoring the complex interplay between XAI capabilities and certification standards. Due to the required expertise, only a limited amount of participants could be interviewed. However, due to the main opinions converging on either using XAI as an incomplete debugging tool or not at all in certification, no additional effect of more and similarly informed participants is to be expected."}, {"title": "6.2 Critical Evaluation of XAI in Certification Contexts", "content": "XAI's role in certification is pivotal yet constrained by several factors. First, while XAI can enhance transparency and facilitate error detection in AI models, its current capabilities do not fully satisfy the rigorous demands of certification standards which require better assurances of safety and reliability. This reveals a critical gap between the theoretical advantages of XAI and its practical utility in ensuring compliance with stringent certification protocols."}, {"title": "6.3 XAI: From Debugging Tool to Certification Aid", "content": "The dual utility of XAI as both a debugging tool and a potential certification aid presents a significant advancement in managing AI system complexities. As a debugging tool, XAI provides valuable insights into AI behavior, identifying biases and failure points. However, transitioning from debugging to a certification context requires XAI to offer more definitive guarantees (or at least information in the form of confidence estimates) about the correctness of explanations and AI systems' behaviors and outcomes, a transition that is currently underdeveloped."}, {"title": "6.4 Societal and Ethical Considerations", "content": "The discourse around XAI goes beyond the technical boundaries and touches on the broader societal and ethical implications. Current certification frameworks primarily address technical compliance, but the integration of XAI requires a broader consideration of ethical standards and societal impacts. This requires a paradigm shift in certification, from purely technical evaluations to more holistic assessments that consider the societal implications of AI technologies."}, {"title": "7 Summary", "content": "As the field of AI continues to evolve, the adaptability of certification processes\u2014and the role of XAI within these-will be paramount. XAI is often touted as a potential solution to the black-box"}]}