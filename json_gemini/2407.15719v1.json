{"title": "GFE-Mamba: Mamba-based AD Multi-modal\nProgression Assessment via Generative Feature\nExtraction from MCI", "authors": ["Zhaojie Fang", "Shenghao Zhu", "Yifei Chen", "Binfeng Zou", "Fan Jia", "Linwei\nQiu", "Chang Liu", "Yiyu Huang", "Xiang Feng", "Feiwei Qin", "Changmiao\nWang", "Yeru Wang", "Jin Fan", "Changbiao Chu", "Wan-Zhen Wu", "Hu Zhao"], "abstract": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disorder that of-\nten progresses from Mild Cognitive Impairment (MCI), leading to memory loss\nand significantly impacting patients' lives. Clinical trials indicate that early\ntargeted interventions for MCI patients can potentially slow or halt the de-\nvelopment and progression of AD. Previous research has shown that accurate\nmedical classification requires the inclusion of extensive multimodal data, such\nas assessment scales and various neuroimaging techniques like Magnetic Res-", "sections": [{"title": "1. Introduction", "content": "Alzheimer's Disease (AD) is a prevalent neurodegenerative condition among\nthe elderly, impacting memory, cognitive function, and daily living activities.\nAD often progresses from Mild Cognitive Impairment (MCI), particularly amnes-\ntic MCI (MCI), which is primarily characterized by memory impairment. Al-\nthough individuals with aMCI experience notable memory loss, their cognitive\nfunction has not yet declined to the level of dementia. Predicting whether aMCI\npatients will advance to AD within one to three years is crucial for prognosis.\nEarly identification of high-risk patients enables personalized treatment and in-\ntervention plans, which can slow disease progression and enhance the quality\nof life. Additionally, early prediction supports patients and their families in\nmaking informed decisions, allowing them to prepare both psychologically and\npractically. Research suggests that early detection and targeted interventions\ncan significantly slow or halt the progression of AD. Physicians use prognos-\ntic predictions to adopt suitable management and treatment strategies. For\nhigh-risk patients, more aggressive interventions such as medication and cog-\nnitive training are often employed. Medications like cholinesterase inhibitors\n(e.g., donepezil) and NMDA receptor antagonists (e.g., memantine) can mit-\nigate cognitive symptoms and delay disease progression. Regular monitoring\nand lifestyle interventions are advisable for patients who are not expected to\ndeteriorate soon. Routine cognitive assessments and annual neuroimaging can\ndetect potential changes early, while non-pharmacological interventions such as\ncognitive training can help maintain or improve cognitive abilities. Lifestyle ad-\njustments, including dietary improvements, exercise, and psychological support,\ncan enhance overall health and resilience against diseases [1].\nCurrently, prognostic predictions for patients with aMCI rely on neuroimag-\ning, cognitive scale scores, and biomarker testing. The Knowledge Scale is a\nwidely used initial diagnostic tool that effectively screens for aMCI, although\nits accuracy can be limited by individual differences, making it suitable primar-"}, {"title": null, "content": "ily for preliminary diagnosis [2]. Magnetic Resonance Imaging (MRI) offers de-\ntailed images of brain structures, allowing for the observation of changes such as\nbrain volume reduction and cortical atrophy. Meanwhile, Positron Emission To-\nmography (PET) provides insights into brain metabolic activity and \u1e9e-amyloid\ndeposition, which are critical markers in the early detection of AD [3]. Despite\nits value, PET imaging is time-consuming, costly, and technically demanding.\nPET stands out for its ability to detect subtle changes in brain metabolism\nwith high sensitivity and specificity for \u1e9e-amyloid and other early AD markers,\nmaking it a powerful tool for assessing AD risk and progression. However, the\ncomplexity of PET imaging, including the need for specific radiotracers, high-\nprecision detectors, and specialized image reconstruction techniques, increases\nits cost and difficulty. Additionally, the requirement for skilled personnel fur-\nther limits its widespread application. Despite these challenges, PET imaging's\nunique capability to provide crucial predictive information and monitor AD\nprogression remains invaluable. Effective prediction of aMCI progression to\nAD necessitates the consideration of multiple risk factors, integrating various\ndiagnostic tools to achieve a comprehensive assessment.\nDespite the various methods currently employed for predicting the progres-\nsion of aMCI to AD, significant challenges and limitations persist, particularly\nregarding the accuracy and reliability of these predictions [4]. This study aims\nto enhance prognostic predictions by synthesizing multiple methods, explor-\ning more effective and accurate approaches, and providing valuable insights for\nimproved patient management and therapeutic outcomes [5]. To address these"}, {"title": null, "content": "challenges, we propose the AD prediction model GFE-Mamba, which automates\nthe classification and prediction of AD using MRI images. This model integrates\nseveral advanced techniques, including a 3D GAN-ViT, a Vision Transformer\n(ViT) bottleneck layer, a mamba block backbone network, and Pixel-Level Bi-\nCross Attention. These components work together to effectively extract patho-\nlogical features from MRI images, and the GFE module uses these features to\ngenerate PET images. By combining scale information with the mamba block\nbackbone network, our method enhances the accuracy of AD classification pre-\ndictions. This integrated approach aims to provide a more reliable and practical\ntool for early identification and intervention in patients at risk of progressing\nfrom aMCI to AD. The contributions of this paper are as follows:\n1) 3D GAN-Vit From MRI TO PET: We utilize a 3D GAN as the back-\nbone, integrated with a ViT for generative task learning. This combination\nfacilitates Generative Feature Extraction (GFE) for the Mamba Classifier,\ncapturing spatial features from both MRI and PET images.\n2) Multimodal Mamba Classifier: We introduce a Mamba Classifier de-\nsigned to manage extensive scale information and 3D images. This clas-\nsifier processes combined sequences through six mamba blocks and then\nuses averaging and linear layers to achieve the final classification.\n3) Pixel-Level Bi-Cross Attention: A pixel-level cross-attention strategy\nis implemented to allow the classifier to efficiently capture underutilized\npixel-space information from both MRI and PET images."}, {"title": "4) Dataset Construction", "content": "To validate the generalizability of our approach,\nwe constructed three datasets based on the ADNI. The first dataset con-\nsists of paired MRI and PET data (MRI-PET dataset). The subsequent\ndatasets, the one-year and three-year MCI-AD datasets, are used to train\nthe classifier. We provide a detailed explanation of the dataset construc-\ntion and will make the dataset acquisition and processing code publicly\navailable in a GitHub repository."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Traditional Alzheimer Prediction Methods", "content": "Traditional methods for predicting AD progression rely heavily on cognitive\nassessments and biomarker testing. Tools like the Mini-Mental State Exami-\nnation [6] and the Montreal Cognitive Assessment [7] have been developed to\nevaluate cognitive disorders and screen for dementia. Biomarker testing, intro-\nduced by Leland Clark Jr. [8], measures AD-related pathological changes by\nassessing beta-amyloid and tau proteins in cerebrospinal fluid.\nHowever, predicting whether patients with aMCI will progress to AD is still\nchallenging due to various influencing factors such as education level and emo-\ntional state [9]. Although biomarker testing shows promise, it faces significant\nclinical challenges, including insufficient sensitivity and specificity of the mark-\ners and the invasiveness of the testing process [10]."}, {"title": "2.2. Machine Learning Based Alzheimer Prediction Methods", "content": "With technological advancements, traditional Alzheimer's disease prediction\nmethods have progressively evolved to incorporate machine learning techniques,\nenhancing prediction accuracy. Escudero et al. [11] utilized multimodal data, in-\ncluding clinical, neuroimaging, and biochemical information, applying k-means\nclustering to categorize subjects into pathological and non-pathological groups.\nThey also employed regularized logistic regression for classification [12, 13]. Wan\net al. [14] proposed a sparse Bayesian multi-task learning algorithm to enhance\ncomputational efficiency. Young et al. [15] achieved high prediction accuracy\non the ADNI database by using the Gaussian Process classification algorithm,\nintegrating multimodal data through a hybrid kernel function. Plant et al. [16]\ncombined Support Vector Machines (SVMs), Bayesian statistics, and Voting\nFeature Interval classifiers, achieving a 75% accuracy rate in predicting AD\nconversion. Teipel et al. [17] applied principal component analysis to MRI de-\nformation maps to differentiate AD patients from healthy controls and to predict\nthe progression of MCI to AD. Moradi et al. [18] utilized low-density separa-\ntion, a semi-supervised learning method, to construct MRI-based biomarkers\nfor predicting MCI to AD conversion."}, {"title": "2.3. Neural Network Based Alzheimer Prediction Method", "content": "With the rise in computer processing power and the development of deep\nneural network technologies, research on AD prediction methods has gained\nsignificant momentum [19]. Liu C. et al. [20] utilized CNNs to extract image\nfeatures from brain regions linked to cognitive decline. These features were then\ncombined with non-image data using a SVM classifier. Qiu et al. [21] employed\na Fully Convolutional Network to generate high-resolution disease probability\nmaps from MRI images. They combined features from high-risk regions with\nnon-imaging data to classify AD. Liu L. et al. [22] introduced the 3MT archi-\ntecture to integrate multimodal information via Cross Attention, along with a\nmodal dropout mechanism. Rahim et al. [23] proposed a hybrid framework that\nmerges a 3D CNN with a bidirectional RNN. El-Sappagh et al. [24] combined\na stacked CNN with a BiLSTM to predict AD progression by fusing five types\nof time-series multimodal data, achieving an accuracy of 92.62%. Wang M. et\nal. [25] developed a multimodal learning framework that incorporates hyper-\ngraph regularization through a graph diffusion approach, reaching an accuracy\nof 96.48%. These advancements underscore the growing potential of integrating\nvarious neural network architectures and multimodal information to enhance\nthe accuracy of AD prediction models.\nRecent advancements in related fields also provide valuable insights. Zha\net al. [26] demonstrated that combining global context with local object fea-\ntures significantly enhances remote sensing scene classification. Similarly, Xu et\nal. [27] developed an identity-diversity inpainting technique for occluded face\nrecognition, which improves recognition accuracy in real-world scenarios. These\nstudies highlight the potential benefits of combining diverse feature sets and\nadvanced techniques to improve classification and recognition accuracy across\nvarious applications."}, {"title": "3. Methodology", "content": "Our approach, illustrated in Figure 1, consists of three primary components:\nthe MRI to PET Generation Network, the Multimodal Mamba Classifier, and\nthe Pixel-Level Bi-Cross Attention mechanism. The MRI to PET Generation\nNetwork is initially trained on a comprehensive dataset of paired MRI and PET\nimages. This training allows the network to generate PET data from MRI scans\nin scenarios where PET data is absent. By extracting image information from\nMRI, the network produces PET features and passes these intermediate fea-\ntures to the classifier for multimodal fusion. The Multimodal Mamba Classifier\nefficiently processes the fused data, which includes both tabular and interme-\ndiate image features, to make accurate classification judgments. Finally, the\nPixel-Level Bi-Cross Attention mechanism operates at the pixel level, integrat-\ning MRI and PET data to address the classifier's limitations in handling shallow\nspatial image information. This integrated approach effectively combines addi-\ntional PET information, even when only MRI data is available, enabling more\naccurate predictions about the likelihood of a patient progressing from MCI to\nAD in the future."}, {"title": "3.1. 3D GAN-Vit From MRI TO PET", "content": "MRI and PET provide essential structural, metabolic, and functional brain\ninformation, respectively, making both modalities crucial for predicting Alzheimer's\nprogression. However, incorporating both types of data into a classification net-\nwork under clinical conditions presents two significant challenges. The first"}, {"title": "3.1.1. 3D Generative Adversarial Network", "content": "Ian et al. proposed a Generative Adversarial Network (GAN) [2] for gener-\nating high-quality images, which can be effectively utilized in downstream tasks.\nThe 3D GAN [3] network extends the original GAN model to three-dimensional\nmedical scenarios. We use this 3D GAN as the backbone for our generation\nnetwork. The GAN network comprises two components: a Discriminator (D)\nand a Generator (G). The Discriminator drives the Generator to create realistic\nPET images and to learn how to extract features from MRIs and transform\nthem into PET features across extensive datasets. Consequently, even when\nonly MRI data is available, features from both modalities can still be utilized.\nThe structure of our proposed 3D GAN-ViT network is illustrated in Figure\n2. It consists of an Encoder/Decoder module with convolutional layers as the"}, {"title": null, "content": "base, and ViT layers in the middle. The Encoder includes three down-sample\nmodules, each comprising a max pooling layer, group normalization layer, con-\nvolutional layer, and ReLU activation layer. The channel sizes for these modules\nare 64, 128, and 256, respectively. The Decoder also includes three up-sample\nmodules with channel sizes of 256, 128, and 64. Each up-sample module consists\nof a group normalization layer, a transpose convolutional layer, and a ReLU ac-\ntivation layer, mirroring the structure of the down-sample modules. The MRI\ninput (XM) is fed into the Generator, passing through the encoder and decoder\nto generate the PET output (yp), which is then used as input for the Discrim-\ninator. The Discriminator employs the same three down-sample modules to\nprocess the input PET data (XPET) and generates a feature map (Y), which\nparticipates in the loss computation.\nThe loss function for the 3D GAN network is divided into two parts: Gen-\nerator loss and Discriminator loss. The Generator loss is defined as:\n$$L(G) =  \\sum_{\\substack{\u0425 \u041c\u2208\u0425\u041c, \u0423PEYP}} ||G(XM) - YP||^2$$ (1)\n$$+ log(1-D(G(\u0445\u043c))) + ||VGG(\u0445\u043c) \u2013 VGG(yP)||^2,$$ \nwhere the first term represents the Mean Squared Error (MSE) reconstruction\nloss between the real and generated PET images, the second term represents the\nadversarial loss of the generator, and the third term represents the perceptual\nloss extracted through VGG19 [28]. The Discriminator loss is defined as:\n$$L(D) = \\sum_{\\substack{\u0425 \u041c\u2208\u0425\u041c, \u0423\u0420\u2208YP}}log(1 - D(yp)) + log(D(G(\u0445\u043c))),$$ (2)\nwhere the first term represents the adversarial loss of the discriminator on real"}, {"title": "3.1.2. Vision Transformer as Middle Block", "content": "The Encoder and Decoder of the 3D GAN-ViT network play crucial roles in\ncompressing MRI data into a latent space and reconstructing it into PET data,\nrespectively. To enhance this process, we replace the original middle block of\nthe 3D GAN with a ViT module, rather than using the ResNet module. This\nchange is essential because the backbone of our classifier processes sequences,\nand incorporating spatial features directly into this network would result in a loss\nof spatial information. The ViT addresses this issue by applying inter-attention\nto the flattened vectors of the image in the hidden space. After the Encoder\nextracts the MRI data into the latent space, denoted as XLM \u2208 RH\u00d7W\u00d7D\u00d7C,\nthe 3D feature map is flattened into a 2D feature map, resulting in XLM E\nR(H\u00b7\u221aD)\u00d7(W\u00b7\u221aD)\u00d7C. This 2D feature map is then processed by the ViT. The\nfeature map is split into a series of image patch sequences XLMP \u2208 RN\u00d7(p\u00b2\u00b7C)\nthrough Patch Embedding, where p is the patch size and N is (H.VD). Once\np\nthe 3D feature map is transformed into a sequence, it is passed through the\ntransformer encoder, which comprises four transformer blocks. After processing,\nthe sequence XLMP \u2208 RN\u00d7(p\u00b2\u00b7C) is resized back to XLP \u2208 RH\u00d7W\u00d7D\u00d7Cand\nused by the Decoder to generate the PET image. Upon pre-training, the latent\nrepresentations XLMP for MRI and XLPP for PET in the hidden space effectively\ncapture the information from both modalities. These representations are then\ncollected and provided to the classifier for the next phase of information fusion."}, {"title": "3.2. Multimodal Mamba Classifier", "content": null}, {"title": "3.2.1. Time Interval Extraction", "content": "To predict the progression from MCI to AD, the first step is to determine\nthe prediction time interval, such as whether a patient with MCI will transition\nto AD within a specific period, like 180 days. For instance, if a 180-day interval\nis chosen, the model would predict whether a patient with MCI will develop\nAD after 180 days. Consequently, the training set should reflect this same\ninterval between diagnoses. However, constructing this training dataset poses\na challenge since it is difficult to ensure that the time between two diagnoses\nfor a patient is exactly 180 days. To address this, we implement a dynamic\nstrategy. We record the actual time intervals between diagnoses for each patient\nand incorporate this information into the model's training data, along with the\nassessment scale category values. During inference, we use the average value\nof the time intervals from the training set to make predictions. This approach\ncompensates for variations in the exact timing of patient diagnoses and ensures\nthat the model can effectively predict the progression from MCI to AD within\nthe specified time frame."}, {"title": "3.2.2. Preprocessing of Assessment Scales", "content": "To enhance the predictive accuracy of our model, we also incorporate as-\nsessment scales, similar to how physicians reference both MRI and PET images\nalongside diagnostic scales. Integrating these scales into the model serves two\nprimary purposes: 1). The scales provide a direct diagnostic aid. 2). Infor-\nmation in tables is highly structured, making it less noisy compared to images.\nHowever, to achieve effective classification through multimodal fusion, it is es-\nsential to process the assessment scale information and integrate it with the\nimage data. We begin by categorizing the scale information into discrete cate-\ngory values and continuous numerical values.\nFor the discrete category values: we first convert them into unique heat\ncodes to ensure there is no duplication across different rows. To achieve this,\nthe value in each subsequent column is increased by the maximum number of\ncategories from all previous columns: $$e_{i}^{cat} = e_{i}^{cat} += \\sum_{j=1}^{i-1} max(e_{j}^{cat})$$. Once these\nnew values are obtained, they can be embedded using a linear transformation:\n$$T^{cat} = W^{cat} e^{cat} + b^{cat},$$ (3)\nwhere $$b^{cat}$$ represents the bias for the i-th feature, and $$W^{cat}$$ is the look-up table\nfor the i-th category [9].\nFor the continuous number values: this involves calculating the mean\n($$\\mu_{i}^{num}$$) and standard deviation ($$\\sigma_{i}^{num}$$) for each column, then normalizing the\nvalues as follows: $$x_{i}^{num} = (x_{i}^{num} \u2013 \\mu_{i}^{num})\\sigma_{i}^{num}$$. These normalized values are\nthen embedded using a linear transformation:\n$$T^{num} = W^{num} x^{num} + b^{num}.$$ (4)\nAfter processing both categorical and numerical values, the tabular informa-\ntion is combined with the image features. This is expressed as:\n$$X = stack[X_{LMP},X_{LPP},T] \u2208 [R^{(m+n+2N)\u00d7d},$$ (5)\nwhere n and m represent the number of categorical and numerical value rows,\nrespectively. $$X_{LMP}$$ and $$X_{LPP}$$ denote the features of MRI and PET images in"}, {"title": null, "content": "the hidden space of the generative network, and d is the embedding size. Once\nprocessed, x is fed into the classification network, where it is fused with image\ninformation for prediction."}, {"title": "3.2.3. Mamba Classifier", "content": "Given that the input x \u2208 R(m+n+2N)\u00d7d consists of diverse scale information\nand a significantly long sequence length due to the 3D image features, using\na traditional transformer with quadratic attention complexity for training is\ninefficient. To address the challenges of modeling long sequences, we employ\nthe Mamba Model [10]. After processing and merging the tabular information\nwith the image information, the sequence is fed into the classifier, which consists\nof six Mamba blocks. The architecture of the Mamba blocks is illustrated in\nFigure 3 (Part A). Each Mamba block begins by normalizing the input sequence\nwith RMS Normalization, which calculates the root mean square value of the\ninput activations, effectively preventing gradient explosion in deep networks.\nThe Mamba module then processes the input sequence, and the output is\nsummed with the residuals of the inputs:\n$$x_{i+1} = Mamba(RMSNorm(x_i)) + x_i.$$\nThe input features are first passed through a linear layer and then split into two\nparts: x and z, where x, z = split (linear(x)). The x part is passed through a\n1D convolution, followed by activation and further processing by the Selective\nScan Model (SSM):\n$$y = SSM(Conv(x)).$$"}, {"title": "3.3. Pixel Level Bi-Cross Attention", "content": "The classifier integrates image features from MRI and PET with tabular\ndata during forward propagation. However, it does not effectively utilize the\npixel-level information from these images. Directly converting 3D MRI/PET\ndata into sequences for the classifier results in long sequence lengths, which\nslow down the training process. Furthermore, the concatenation of extensive\nimage information can prevent the classifier from effectively incorporating scale\ninformation. To address this issue, the Cross Attention architecture [29] can be\nemployed. This method does not incorporate the forward propagation of the\nclassifier but makes the pixel space information from MRI and PET available to\nthe sequences in the classifier through attention mechanisms. As illustrated in\nFig 3 (Part B), the output from the last Mamba block in the classifier, denoted\nas y \u2208 R1xd, is enhanced through mutual attention with MRI and PET before\nthe final classification.\nFor the MRI and PET data, represented as xm and xp \u2208 RH\u00d7W\u00d7D\u00d7C, the\ndata are reshaped to XM and xp \u2208 R(H\u00b7W\u00b7D)\u00d7C, which is a summarized form.\nFor MRI, the inter-attention process is as follows:\n$$Q_y = W_q y, K_x = W_k X_M, V_x = W_v X_M,$$\n$$y = softmax(\\frac{Q_y K_x}{\\sqrt{d_k}})V,$$\nwhere the query matrix (Qy) is derived by linearly transforming the classifier's\noutput, and both the key (Kr) and value matrices (V) are obtained by lin-\nearily transforming the serialized MRI features. This mutual attention process\nis similarly applied to the PET data.\nAfter computing the mutual attention, these features are combined residually\nwith y. Following the feed-forward and layer normalization operations, they are\nagain residually summed with the original y."}, {"title": "4. Experiment", "content": null}, {"title": "4.1. Data Acquisition and Processing", "content": "To validate our approach, we implemented it using the publicly available\nADNI dataset. As outlined in this paper, our model training requires two dis-\ntinct datasets: the paired MRI and PET dataset, referred to as the MRI-PET\ndataset, and the dataset used to determine if a classifier predicts progression to\nAD, referred to as the MCI-AD dataset. Due to ADNI's privacy policy, we are\nunable to publicly share the screened and processed datasets. However, we will\nprovide a detailed description of the construction of these two datasets."}, {"title": "4.1.1. MRI-PET Dataset", "content": "The dataset constraints are relatively flexible, requiring corresponding MRI\nand PET scans taken from the same patient at the same diagnostic stage. This\ndataset needs to be substantial in size to train on a generative network and\nundergo representation learning effectively. During our data collection process,\nwe traversed through ADNI1, ADNI2, ADNI3, ADNI4, and ADNI-GO datasets.\nAccording to existing literature, specifically [2, 3], MRI and PET scans taken\nwithin ten days of each other are considered representative of the patient's state\nat that moment. For image protocols, we selected the Sagittal phase, 3D, T1-\nweighted MRI scans without preprocessing (MPRAGE). For PET scans, we\nchose 18F-FDG and applied the following preprocessing steps: co-registration,\naveraging and standard normalization of image and voxel sizes, and uniform\nresolution adjustment. Our collection efforts yielded 2,843 paired MRI and"}, {"title": "4.1.2. MCI-AD Dataset", "content": "To construct the dataset for this task, it was essential to determine each\npatient's status at each diagnosis. We utilized the tadpole table data from\nADNI's study data, which details the basic and pathological information of\neach patient. Initially, we identified all patients diagnosed with MCI and then\ntracked their subsequent diagnoses longitudinally. We recorded the status and\ntiming of each following diagnosis, as illustrated in Figure 4. If a patient's\nsubsequent diagnosis was AD, the classification label was set to 1, otherwise, it\nwas set to 0.\nFollowing the label construction, we identified the corresponding MRI images\nbased on the form information. We searched for and downloaded the relevant\nMRIs from the ADNI image dataset using the patient's ID and consultation"}, {"title": "4.2. Experimental Settings", "content": "Both components were conducted using the PyTorch 2.0 framework on NVIDIA\nGeForce RTX 4090 GPUs with CUDA 11.8. For reading NIfTI format images,\nwe employed Monai, while Pandas was used to read tables and convert them\ninto training data. The 3D GAN-ViT model was trained for 200 epochs with a\nbatch size of 2. The classifier was trained for 100 epochs with a batch size of\n8. Both models were optimized using the Adam algorithm, with a learning rate\nand betas set to (0.9, 0.999)."}, {"title": "4.3. Evaluation Indicators", "content": "We employed five metrics to evaluate the classification performance on the\ndataset derived from ADNI: Accuracy, Precision, Recall, F1-score, and Matthews\nCorrelation Coefficient (MCC). Their definitions and formulas are as follows:\n$$Precision = \\frac{TP}{TP+FP},$$\n$$Recall = \\frac{TP}{TP + FN},$$\n$$F1-score = 2 \u00d7 \\frac{Precision \u00d7 Recall}{Precision + Recall},$$\n$$Accuracy = \\frac{TP+TN}{TP+TN+FP + FN},$$\n$$MCC = \\frac{TP\u00d7TN - FP\u00d7FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}},$$\nwhere TP and TN represent correctly predicted positive and negative samples,\nrespectively, while FP and FN represent incorrectly predicted positive and\nnegative samples."}, {"title": "4.4. Comparative Experiments", "content": "As shown in Tables 2 and 3, we compared our GFE-Mamba model with\nother typical classification models and advanced AD prediction models using the\nADNI dataset. The results indicate that GFE-Mamba significantly outperforms\nthese models, particularly in terms of MCC and Accuracy. Given the consistency\nof our findings across both datasets, we will focus on the comparative analysis\nusing the 1-year dataset.\nIn comparison to the ResNet family of models [30], which address gradient\nvanishing in deep networks, GFE-Mamba exhibits superior performance in han-\ndling MRI images. While ResNet models struggle to capture localized pathology,\nGFE-Mamba leverages the 3D GAN-ViT module to effectively process spatial\nvectors and capture spatial information, thereby improving classification accu-"}, {"title": null, "content": "racy. Specifically, the ResNet50 model lags behind GFE-Mamba in Precision\nand Accuracy, achieving only 79.31% and 60.00%, respectively.\nSimilarly, compared to the TabTransformer model [31], which excels in tab-\nular data processing, GFE-Mamba demonstrates a stronger ability to capture\npathological features in MRI images and recognize complex pathological states.\nBy integrating the 3D GAN-ViT module and the Multimodal Mamba Classi-\nfier, GFE-Mamba significantly enhances classification accuracy and model in-\nterpretability. The TabTransformer model's Recall and F1-score are notably\nlower, at 93.33% and 75.68%, respectively.\nWhen compared with traditional AD classification models such as XGBoost\n[32] and Qiu et al's [33], it well solves the problem of limited feature extrac-\ntion capability and parameter redundancy caused by the first paper's model\nrelying on traditional CNN. Meanwhile, our GFE-Mamba model compensates\nwell for the problem of high computational complexity and insufficient global\ninformation capture of 3D CNN models when dealing with high-dimensional\nneuroimaging data by introducing a Pixel-Level Bi-Cross Attention mechanism.\nAs a result, GFE-Mamba is significantly improved regarding feature expression\ncapability and model interpretability. In contrast, the Early-Stage Alzheimer's\nmodel and 3D CNN model did not perform as well as GFE-Mamba on Recall\nand F1-score, which were 86.53%, 86.92%, and 81.82%, 81.82%, respectively.\nFurthermore, when compared to advanced AD classification models like the\nFusion model [34] and Zhang et al.'s model [35], GFE-Mamba demonstrates\nsuperior performance. While these models perform well in multimodal data"}, {"title": null, "content": "processing and feature extraction, they struggle with feature redundancy and\nnonlinear feature representation, particularly in complex neuroimaging data.\nGFE-Mamba addresses these issues by combining the 3D GAN-ViT and Multi-\nmodal Mamba Classifier, thereby reducing spatial and channel redundancy and\noptimizing feature representation. The Pixel-Level Bi-Cross Attention mecha-\nnism further enhances nonlinear feature representation and model interpretabil-\nity, while also reducing memory consumption and computational complexity.\nConsequently, GFE-Mamba excels in capturing fine-grained MRI features and\naccurately differentiating complex pathological states. In contrast, the Multi-\nmodal deep learning model and AD classification model deliver lower Precision\nand F1-score, with values of 89.83%, 88.91%, and 76.67%, 45.45%, respectively."}, {"title": "4.5. Ablation Experiments", "content": "In the ablation experiments section, we analyze the individual contributions\nof the GFE, Cross Attention, and ViT middle block components to the classifica-\ntion performance of the GFE-Mamba model on the 1-year and 3-year datasets.\nWe evaluated the impact of each module by comparing the Accuracy, Preci-\nsion, Recall, F1-score, and MCC values when removing the GFE module, the\nCross Attention module, and the ViT middle block module, as well as using the\ncomplete GFE-Mamba model. Tables 4 and 5 present the results, demonstrat-\ning that each module positively impacts the model's classification performance.\nGiven the consistency of the results across both datasets, we will focus our\ndiscussion on the ablation experiments conducted on the 1-year dataset.\nImpact of Removing Generative Feature Extraction: The GFE mod-\nule enhances the model's ability to extract features from high-dimensional neu-\nroimaging data using GANs. Removing this module significantly limits the"}, {"title": null, "content": "model's feature extraction capability, resulting in a notable decline in perfor-\nmance. Specifically, Precision drops from 95.71% to 88.57%, and the Fl-score\ndecreases from 96.55% to 89.29%. This underscores the critical role of the\nGFE module in optimizing feature representation, particularly for capturing\nfine-grained features in MRI data.\nImpact of Removing Bi-Cross Attention Module: The Bi-Cross At-\ntention module enhances the model's ability to capture correlations between\ndifferent data modalities, thereby improving feature representation and model\ninterpretability. Removing this module significantly weakens the model's capac-\nity to integrate multimodal data, leading to a marked decline in performance\nmetrics. Specifically, Recall decreases from 96.55% to 93.10%, and the MCC\ndrops from 91.25% to 91.53%. These results highlight the importance of the Bi-\nCross Attention mechanism in accurately extracting and integrating information\nfrom multimodal data for comprehensive understanding and precise classifica-\ntion of complex pathological features.\nImpact of Removing ViT Middle Block: The middle block enhances the\nmodel's ability to capture global spatial information through the ViT, allowing\nthe model to handle a wide range of spatial relationships and subtle features\nin MRI images. Removing this block reduces the model's capability to capture\nglobal spatial features, leading to decreased performance. Specifically, Accuracy\ndrops from 95.71% to 87.18%, and Recall falls from 96.55% to 89.47%. The\ndiminished ability to extract global features makes it challenging for the model\nto effectively distinguish complex pathologies. This emphasizes the critical role"}]}