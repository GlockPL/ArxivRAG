{"title": "IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model", "authors": ["Eren Olug", "Kiymet Kaya", "Resul Tugay", "Sule Gunduz Oguducu"], "abstract": "Road traffic congestion prediction is a crucial component of intelligent transportation systems, since it enables proactive traffic management, enhances suburban experience, reduces environmental impact, and improves overall safety and efficiency. Although there are several public datasets, especially for metropolitan areas, these datasets may not be applicable to practical scenarios due to insufficiency in the scale of data (i.e. number of sensors and road links) and several external factors like different characteristics of the target area such as urban, highways and the data collection location. To address this, this paper introduces a novel IBB Traffic graph dataset as an alternative benchmark dataset to mitigate these limitations and enrich the literature with new geographical characteristics. IBB Traffic graph dataset covers the sensor data collected at 2451 distinct locations. Moreover, we propose a novel Road Traffic Prediction Model that strengthens temporal links through feature engineering, node embedding with GLEE to represent inter-related relationships within the traffic network, and traffic prediction with ExtraTrees. The results indicate that the proposed model consistently outperforms the baseline models, demonstrating an average accuracy improvement of 4%.", "sections": [{"title": "I. INTRODUCTION", "content": "The importance of intelligent transportation systems (ITSs) has rapidly increased as they enhance travel efficiency, reduce traffic incidents, and improve safety. Traffic prediction involves predicting future traffic behavior in a given area according to current and past traffic conditions. This information is crucial for multiple purposes, such as reducing congestion, optimizing transportation systems, and enhancing road safety.\nWith the growing capabilities of road sensor devices, various information about traffic networks, such as flow and speed, can be collected and prepared for several downstream tasks not only for traffic predicting, including forecasting of traffic flow, travel time, and traffic density, but also other real-world applications such as travel demand prediction for ride-hailing services, autonomous vehicles, intersection management, parking management, urban planning, and transportation safety [1]. Traditionally, traffic prediction has utilized methods like rule-based models and time-series analysis. However, these approaches often fall short of capturing the full complexity, variability of traffic patterns in graph data.\nTraffic flow prediction for a specific road is a difficult problem because it is influenced by many factors, including the day, time of day, weather conditions, and the traffic flow of neighboring roads. Thus, traffic prediction requires generating data from the non-Euclidean domain, necessitating the use of graph data structures to represent complex spatial relationships and interdependencies between objects [2]. This task is particularly challenging due to the spatiotemporal nature of traffic network data, which encapsulates both intricate spatial relationships and dynamic temporal dependencies [3], [4].\nGraph Neural Networks (GNNs) can directly process the graph structure (i.e., adjacency information) along with node (sensors in roads) features. Numerous techniques for extracting features from graphs [5]\u2013[8] have been shown to be effective in handling graph structures, including their adjacency information. These methods enable the extraction of node embeddings of the graph in Euclidean space.\nPublicly available road traffic prediction datasets differ in several ways (1) size: the number of sensors or data points, (2) temporal coverage: how long period the data covers, (3) sensor metadata: information about the sensors (e.g. location, type), and (4) road type: whether the data is collected on highways or urban streets. One major limitation in many popular datasets, like METR-LA [9], is their focus on only highways. This focus results in the lack of ability to recognize urban traffic networks successfully. Another limitation is that these datasets are often scale-limited compared to real-world road networks, making it harder to train complex forecasting models. For instance, METR-LA and PeMS-BAY [10], two of the most widely used datasets, contain only 207 and 325 sensors, respectively. Additionally, only a few of the datasets [10] cover a long enough temporal period, which is crucial in capturing long-term traffic patterns. Considering the impact of data quality on model performance, these limitations reduce the effectiveness of traffic prediction models.\nIn this study, we aim to enrich the literature by introducing the IBB Traffic graph dataset, which covers 2,451 sensors located in Istanbul, Turkey, including both urban and highway roads. Given the features collected by sensors, this dataset can be utilized not only for predicting road traffic anomalies but also for forecasting speed, flow, and travel time. Moreover, we propose a novel Road Traffic Prediction Model that strengthens temporal links through feature engineering, node embedding with GLEE to represent inter-related relationships within the traffic network, and traffic prediction with ExtraTrees.\nThe main contributions of our study can be summarized as follows:\nA novel IBB traffic graph dataset comprises sensor"}, {"title": "III. IBB GRAPH-BASED ROAD NETWORK DATASET", "content": "Traffic in Istanbul is notable due to the city's unique characteristics, including its distinctive geographical features and its status as a metropolitan area with over five million vehicles [16]. The city spans two large landmasses, each located on a different continent (Asia and Europe), separated by the Bosporus Strait. In addition to intra-continental traffic flow, millions of people travel between continents daily using three different bridges connecting these continents.\nThe proposed IBB Traffic graph dataset encompasses four years of traffic data, including features such as traffic state (e.g., flow and speed) and sensor information. The traffic data-collecting sensors are strategically positioned at approximately equal intervals throughout the city, covering both highways and urban areas. In the IBB Traffic graph dataset, sensor measurement frequency is one hour, yielding 24 measurements"}, {"title": "IV. METHODOLOGY", "content": "The proposed Road Traffic Prediction Model is illustrated in Figure 2. The model benefits of traffic flow features: average speed, vehicle count, minimum speed, and maximum speed, and sensor features encompass geographical categorizations like continents, districts, highways/urban areas, and population density, which are shown in maps. With feature engineering study, previous week and hour traffic densities are gathered. Moreover, node embeddings are obtained with the GLEE method. All of the features are then processed and combined by the ExtraTrees classifier to distinguish between normal and congestion traffic. Next, we give details of the node embedding method GLEE and traffic congestion classification method ExtraTrees. The implementation details and the code repository of the proposed model are available here\u00b9.\nGLEE [5] is closely related to the Laplacian Eigenmaps (LE) method. LE constructs node embeddings based on the spectral properties of the Laplacian matrix of the graph. However, while LE aims to minimize the distance between similar nodes based on the spectral properties of the Laplacian matrix, GLEE disposes of the distance-minimization assumption and uses the Laplacian matrix to find embeddings with geometric properties, by leveraging the simplex geometry of the graph."}, {"title": "A. Node Embedding: GLEE", "content": "Let A be the diagonal matrix having eigenvalues of L as entries, GLEE solves a distance maximization problem given in Equation 1, where the objective function is tr(YLY) under the constraint YY = \u039b.\n$\\arg \\max_{\\mathbf{Y} \\in \\mathbb{R}^{n \\times d}} \\operatorname{tr}(\\mathbf{Y}L\\mathbf{Y}) \\quad \\text { s.t. } \\quad \\mathbf{Y}^\\top \\mathbf{Y} = \\Lambda$\\nMore concretely, given a set of data points {X1,X2,..., Xn}, the first step in Geometric Laplacian Eigenmap Embedding (GLEE) is to construct a graph G = (V, E) where each node represents a data point. The edge weight matrix A, is typically computed using a Gaussian kernel as in Equation 2. Next, the degree matrix D is computed, which is a diagonal matrix represents the sum of the weights of the edges connected to node i.\n$A_{i j}=\\exp \\left(-\\frac{\\left\\|X_{i}-X_{j}\\right\\|^{2}}{\\sigma^{2}}\\right)$.\nGiven the weight matrix A and the degree matrix D, we then compute the L, defined as L = D - A. The core of GLEE involves solving the generalized eigenvalue problem Ly = Dy, where y are the eigenvectors and A are the eigenvalues. The solution to this eigenvalue problem provides a set of eigenvectors y1, y2,..., ym corresponding to the smallest eigenvalues. Finally, the data points xi are embedded into a lower-dimensional space by using the eigenvectors as (j) coordinates Y i = (y(1), y(2), ..., y(m)), where yi) is the i- th component of the j-th eigenvector. This process results in a low-dimensional representation of the original high-dimensional data, preserving the geometric structure of the data as captured by the graph."}, {"title": "B. Road Traffic Prediction: ExtraTrees", "content": "ExtraTrees is an ensemble model based on decision trees. ExtraTrees differ from standard bagging models in terms of tunning of best splitting hyperparameter. ExtraTrees chooses the optimum split randomly which makes it much faster."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "We conducted our experiments using four node embedding methods, three proximity-preserving (Node2Vec, GLEE, NetMF), one structural node-level embedding (Graph Wave), and seven classifiers. Next, we give details of these models used in the experiments and road traffic prediction results obtained.\nNode2Vec [17] algorithm learns representations of nodes (i.e. sensors) via random walk procedure. The algorithm creates node embeddings of the graph following a flexible neighborhood sampling strategy that trades off between Breadth-First Search (BFS) and Depth-First Search (DFS). To illustrate, starting from a source node u, Node2Vec simulates a random walk of length 1. Initially, co = u represents the starting node, and ci denotes the i th node in the walk. The nodes ci are generated based on the probability distribution in Equation 3. In this equation, \u03c0\u03bf\u03c5 is the unnormalized transition probability between nodes v and x, and Z is the normalizing constant.\n$P\\left(C_{i}=x \\mid C_{i-1}=v\\right)=\\left\\{\\begin{array}{ll}\\frac{\\pi_{v x}}{Z} & \\text { if }(v, x) \\in E \\\\0 & \\text { otherwise }\\end{array}\\right.$\nA 2nd order random walk on a graph is defined by two parameters p (return parameter) and q (in-out parameter) which guide the walker in deciding the next step. Consider a scenario in which the walker has just taken a step from node t to node v. At this point, the walker evaluates the transition probabilities \u03c0\u03c5\u00e6 on edges (v,x) leading from v. The transition probability here is unnormalized and can be represented as \u03c0\u03c5x = ApqWvx where the Apq, which defined in Equation 4, is the length of the shortest path between nodes t and x.\n$\\Lambda_{p q}(t, x)=\\left\\{\\begin{array}{ll}\\frac{1}{p} & \\text { if } d_{t x}=0 \\\\1 & \\text { if } d_{t x}=1 \\\\frac{1}{q} & \\text { if } d_{t x}=2\\end{array}\\right.$\nThe parameters p and q determine how the random walk behaves, like either BFS or DFS. Parameter p influences the likelihood of revisiting a node immediately after leaving it. Higher values of p reduce this likelihood, which means higher values for p encourage the walker to make more exploration. The other parameter q determines whether the walker searches for inward or outward nodes. The two parameters are intertwined, meaning the value of one does not alone tell about the behavior of the random walk.\nNetMF [18] unifies various network embedding models, namely DeepWalk, LINE, PTE, and Node2Vec into an implicit matrix factorization framework, showing their theoretical connections to graph Laplacians. DeepWalk generates random walks on the network similar to how sentences are formed in a language. The NetMF aims to maximize the probability of observing these node sequences, which can be represented as matrix factorization of the transition probability matrix M in Equation 5, where U and V are the node embeddings. LINE (Large-scale Information Network Embedding) focuses on preserving both first-order and second-order proximities. First-order proximity ensures that neighbor nodes or nodes with strong connections are closer to each other in the embedding space, while second-order proximity can be interpreted as nodes sharing the same neighbors are more likely to be similar.\n$\\min _{\\mathbf{U}, \\mathbf{V}}\\left\\|\\mathbf{M}-\\mathbf{U V}^{\\top}\\right\\|_{F}^{2},$\n$\\min _{\\mathbf{U}, \\mathbf{V}} \\sum_{t} \\sum_{(i, j) \\in E_{t}} l_{i, j}^{2},$\n$\\min _{\\mathbf{U}, \\mathbf{V}}\\left\\|\\mathbf{M}_{\\text {node2vec }}-\\mathbf{U V}^{\\top}\\right\\|_{F}^{2}.$\n$\\min _{\\mathbf{U}, \\mathbf{V}}\\left\\|\\mathbf{M}-\\mathbf{U V}^{\\top}\\right\\|_{F}^{2},$\nPTE (Predictive Text Embedding) is an extension of LINE in heterogeneous networks, optimizing a similar objective for each type of edge and combining them as in Equation 6 where t indexes different types of edges. Node2vec extends DeepWalk by introducing a biased random walk to capture homophily and structural equivalence. All these methods can be unified under the general matrix factorization framework in Equation 8 where M represents different transition probability matrices or proximity matrices depending on the method used.\nGraphWave [8] is used to learn the neighborhood of each node in a network using a low-dimensional embedding by leveraging heat wavelet diffusion patterns. Let U be the eigenvector matrix from the decomposition of the unnormalized graph Laplacian L = D \u2013 A, where L = UAUT, A is a diagonal matrix of eigenvalues X and X1 < 2 < ... < \u03bb\u03bd are the eigenvalues of L. Here, D is the diagonal matrix whose entries are the degrees of each node, and A = (aij) is the edge weight matrix such that aij represents the weight of the edge between nodes i and j. Let gs be a filter kernel with scaling parameter s. The spectral graph wavelet of gs is the signal resulting from the modulation in the spectral domain of a Dirac signal centered around node a. The spectral graph wavelet \u03a8a is given as an N-dimensional vector as in Equation 9, where da = (a) is the one-hot vector of node a. Spectral graph wavelets are based on the relationship between the temporal frequencies of a signal and the Laplacian's eigenvalues.\n$\\Psi_{a}=\\mathbf{U} \\operatorname{Diag}\\left(g_{s}\\left(\\lambda_{1}\\right), \\ldots, g_{s}\\left(\\lambda_{N}\\right)\\right) \\mathbf{U}^{\\top} \\delta_{a}$\n$\\phi_{X}(t)=\\mathbb{E}\\left[e^{i t X}\\right]$\n$\\Psi_{a}(t)=\\frac{1}{N} \\sum_{m=1}^{N} e^{i t \\lambda_{m} a}$\n$\\mathbf{X}_{a}=\\left[\\operatorname{Re}\\left(\\phi_{a}\\left(t_{i}\\right)\\right), \\operatorname{Im}\\left(\\phi_{a}\\left(t_{i}\\right)\\right)\\right]$\nFor every node a, GraphWave computes a 2d-dimensional vector Xa. To obtain diffusion patterns of every node, spectral graph wavelets are used. Diffusion patterns are stored in a matrix \u03a8. \u03a8 is a NxN matrix, where the a-th column is the spectral graph wavelet for a heat kernel centered at node a. GraphWave embeds spectral graph wavelet coefficient distributions into 2d dimensional space by calculating the characteristic function for each node's coefficients \u03a8a and samples it at d evenly spaced points. The characteristic function of a probability distribution X is as in Equation 10. For a given node a and scale s, the empirical characteristic function of a is defined as in Equation 11. Embedding Xa of node a is obtained by sampling the parametric function in Equation 10 at d evenly spaced points t\u2081...ta and concatenating the values as in Equation 12.\nLogistic Regression (LR) uses the sigmoid function \u03c3(x) = 1/(1 + e\u00af*) to transform the weighted sum of input features into a probability value ranging between zero and one.\nK-Nearest Neighbor (KNN) classifies data points based on their proximity to other data points.\nRandom Forest (RF) is a bootstrap ensemble model. It creates several decision trees on data samples and then selects the best solution using voting.\nXGBoost is an implementation of gradient boosting. The key idea behind the XGBoost is the improvement of speed and performance using the reasons behind the good performance such as regularization and handling sparse data.\nLightGBM is a histogram-based gradient boosting algorithm reducing the computational cost by converting continuous variables into discrete ones. Since the training time of the decision trees is directly proportional to the number of computations, hence the number of splits, LightGBM provides a shorter model training time and efficient resource utilization.\nCatboost is a gradient-boosting ML algorithm known for its high predictive accuracy and speed. It employs techniques such as ordered boosting and oblivious trees to handle various data types effectively and mitigate overfitting."}, {"title": "A. Traffic Congestion Prediction Results", "content": "In this study, we approached the traffic congestion prediction problem as a binary classification task, where the output for each sensor node determines whether congestion may occur or not. A straightforward and commonly used method for labeling involves directly using average speed values, as shown in Equation 13. However, given the narrow streets in urban areas where the average speed is low but congestion is absent, we conducted our experiments using an alternative formulation. This formulation also takes into account the number of passing vehicles, as represented in Equation 14. The function C gives the proper label on congestion for each node in a timestamp where the s is the speed and v is the number of vehicles. Additionally, o is the sigmoid function mapping the output between 0 and 1 and the 7 is the threshold value ranging between 0 and 1.\n$C(s)=\\left\\{\\begin{array}{ll}0, & \\text { if } \\frac{s}{s}<\\tau \\\\1, & \\text { otherwise }\\end{array}\\right.$\n$C(s, v)=\\left\\{\\begin{array}{ll}0, & \\text { if } \\sigma\\left(\\frac{v}{s}\\right)<\\tau \\\\1, & \\text { otherwise }\\end{array}\\right.$\nThe results obtained with various classifier models without adding node embedding vectors to the data are presented in Table I, while the results with node embedding are presented in Table II. In both Table I and Table II, the value of feature engineering studies was also queried. The values highlighted in Table I indicate the most accurate value for the relevant metric.\nComparing the 'without feature engineering' and 'with feature engineering' cases, it is seen that the traffic density one hour ago and the traffic density one week ago features obtained via feature engineering studies are quite effective. On the other hand, in Table II, the light green color indicates that node embeddings provide a performance improvement compared to the result in Table I for that metric, classifier and w/wo feature cases, while dark greens indicate the highest scores in that category. When the results in Table I and Table II are analyzed together, for the scenario 'without feature engineering', the approach of adding node embedding vectors to the data improves the prediction performance regardless of the classifier and node embedding methods. Considering all"}, {"title": "VI. CONCLUSION AND FUTURE WORKS", "content": "In this study, we propose a novel Road Traffic Prediction Model that effectively leverages traffic flow features-such as average speed, vehicle count, min. speed, and max. speed- and sensor features like geographical categorizations (continents, districts, highways/urban areas, and population density), which are visualized through maps. The inclusion of temporal data w/feature engineering studies, such as traffic densities from the previous week and hour, further enhances prediction accuracy. Node embeddings obtained via the GLEE, combined with the ExtraTrees classifier, enable the distinction between normal and congested traffic more accurately. Analysis of results shows significant performance improvements when using feature engineering and node embeddings, highlighting the importance of these techniques in traffic prediction. For future work, we plan to work on traffic prediction for dynamic graphs."}]}