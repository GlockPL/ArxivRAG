{"title": "CONFORMAL TRAJECTORY PREDICTION WITH MULTI-VIEW DATA INTEGRATION IN COOPERATIVE DRIVING", "authors": ["Xi Chen", "Rahul Bhadani", "Larry Head"], "abstract": "Current research on trajectory prediction primarily relies on data collected by onboard sensors of\nan ego vehicle. With the rapid advancement in connected technologies, such as vehicle-to-vehicle\n(V2V) and vehicle-to-infrastructure (V2I) communication, valuable information from alternate views\nbecomes accessible via wireless networks. The integration of information from alternative views\nhas the potential to overcome the inherent limitations associated with a single viewpoint, such\nas occlusions and limited field of view. In this work, we introduce V2INet, a novel trajectory\nprediction framework designed to model multi-view data by extending existing single-view models.\nUnlike previous approaches where the multi-view data is manually fused or formulated as a separate\ntraining stage, our model supports end-to-end training, enhancing both flexibility and performance.\nMoreover, the predicted multimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire framework using the\nreal-world V2I dataset V2X-Seq. Our results demonstrate superior performance in terms of Final\nDisplacement Error (FDE) and Miss Rate (MR) using a single GPU. The code is publicly available at:\nhttps://github.com/xichennn/V2I_trajectory_prediction.", "sections": [{"title": "1 Introduction", "content": "Trajectory prediction plays a critical role in autonomous driving. Typically, we rely on the on-board sensors of an ego\nvehicle to gather environmental information necessary for performing various autonomous driving tasks. However,\nwith the rapid advancement in connected technologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes accessible via wireless networks. The\nintegration of information from alternative views has the potential to overcome the inherent limitations associated\nwith a single viewpoint, such as occlusions and limited field of view. We categorize the data obtained solely from ego\nvehicle on-board sensors or infrastructure as single-view information, whereas the accessibility to multiple viewpoints\nis referred to as multi-view data. Roadside cameras, strategically positioned to\nhave an unobstructed view of the entire intersection, provide a comprehensive overview of the traffic situation. This\nenhanced perspective allows the AV to receive critical information and effectively avoid the impending collision. From\na trajectory predictive modeling perspective, significant research efforts have been dedicated to leveraging single-view\ndatasets collected by the ego vehicle [1, 2, 3, 4, 5]. These efforts typically involve modeling the temporal dependencies,\nagent-agent interactions and agent-lane relations. However, the challenge arises when dealing with multi-view data,\nparticularly in how to effectively fuse the information due to overlapping field of views.\nExisting work on multi-view data fusion in the context of cooperative driving predominantly focuses on collaborative\nperception tasks, with limited research addressing trajectory prediction. [6] pioneered the creation of the first V2I\nreal-world dataset for trajectory prediction studies. They manually fused multi-view data using trajectory association and\nstitching techniques at each frame, followed by the application of single-view trajectory prediction models. Although\nthis approach is intuitive, it fails to fully exploit the motion behavior captured by data from each view, resulting in\nsuboptimal outcomes. To address this limitation, [7] proposed a novel approach that encodes trajectory information\nfrom each view as independent graph nodes, thereby minimizing information loss. They formulated the node association\nproblem as a graph link prediction task and introduced a cross-attention module to fuse node embeddings from\nassociated nodes across different views. While their model can be trained end-to-end, the node association process\nnecessitates pretraining, leading to the formulation of two optimization objectives.\nWe introduce a trajectory prediction framework that utilizes multi-view data without the need for explicit association\nbetween different perspectives. Rather than developing a specialized multi-view model, our approach seamlessly\nintegrates with state-of-the-art single-view trajectory models, maximizing the utility of existing research efforts. No\nspecial training strategies are required. We can easily leverage the pretrained single-view trajectory models to expediate\nthe training. Specifically, for trajectory data from each single-view, we employ established graph neural network\n(GNN) based models, such as LaneGCN [1], HiVT [3], to capture temporal dependencies, agent-agent interactions, and\nagent-lane relations. Then we utilize a cross-graph attention module to fuse the node embeddings from different views.\nThe fused final embeddings will then go through a multimodal decoder to get future trajectory predictions.\nExisting works have modeled the multi-modality explicitly by introducing anchors [2, 8, 9], mixture models [10, 11, 12,\n13, 1, 3], or implicitly through latent variables such as Conditional Variational Auto-encoder (CVAE) or generative\nmodels [14, 15, 16]. The implicit models often face the issue of mode collapse, therefore we will model the multi-\nmodality by a mixture model built upon MLP. It is a common strategy to assign a higher score to the modality closer to\nthe ground truth during training. This strategy, however, may encounter robustness issues during inference. Figure 2\npresents two examples where the top-scored prediction is not the closest to the ground truth trajectory."}, {"title": "2 Related Work", "content": "Collaborative perception has been the most extensively studied area in collaborative driving [19, 20, 21, 22]. It leverages\nthe wireless communication technologies, such as V2V and V2I, to share information, which enables perception beyond\nline-of-sight and field-of-view hence overcoming common perceptual shortcomings with individual perception, such as\nblind spot, occlusions and long-range issues [23, 24]. Many datasets have been collected in both simulation environment\n[25, 26, 27] and real-world [28, 29] to facilitate the study. A comprehensive comparison can be found in Table 1.\nWhile majority of them focused on the upstream tasks such as detection, tracking and segmentation, to enable well-\ninformed decision-making for autonomous vehicles, however, it's critical to also incorporate V2X data for predicting the\nbehavior of surrounding traffic participants. V2X-Seq/Forecasting [6] and V2X-Traj [7] present two real-world datasets\nspecifically designed for cooperative trajectory prediction tasks. The experiments in V2X-Seq/Forecasting demonstrate\nthat leveraging the infrastructure-side trajectories can enhance the trajectory prediction performance. V2X-Traj aims\nfor more general V2X scenarios, extending to V2V cooperation."}, {"title": "2.2 Cooperative Information Association and Fusion", "content": "The main challenge in cooperative trajectory prediction lies in the multi-view data source fusion. In cooperative\nscenarios, vehicles gather safety-related data using sensors like radar, lidar, cameras, and GPS. This data is standardized\ninto Basic Safety Message (BSM) format, ensuring compatibility across vehicles and infrastructure [30]. BSM messages,\ncontaining crucial information such as position and speed, are broadcasted periodically. Upon receiving BSM messages,\nvehicles combine this data with their own sensor data to enhance accuracy. Advanced algorithms have been developed\nto associate and fuse the multi-source data. Given the temporal and spatial dimensions of the collected trajectories,\nthere has been research focusing on the communication delays alignment [31, 23, 32] and pose errors alignment\n[33, 34, 35, 23]. Based on the aligned data, [6] utilized CBMOT [36], a multi-object tracking method, to fuse the\ninfrastructure and ego-vehicle trajectories at each single frame and then trained the network taking in the fused dataset.\nWhile straightforward, this method fails to capture the motion behavior provided by the infrastructure across all time\nsteps, leading to suboptimal results. In contrast, [7] encoded trajectory information from each view as independent\ngraph nodes, formulating the association process as a graph linking problem. While effective, this approach necessitates\nseparate training procedures. Building upon their work, we propose the utilization of a cross-graph attention mechanism\nto fuse multi-view information, eliminating the need for additional training processes."}, {"title": "2.3 Uncertainty Quantification", "content": "In the trajectory prediction task, numerous sources of uncertainty exist, including inherent multi-modality, partial\nobservability, short time scales, data limitations, intention type imbalances, and domain gaps. Most existing trajectory\nprediction models address uncertainty by maximizing the likelihood of an assumed distribution, such as Gaussian or\nLaplace [9, 3, 1, 11]. However, trajectories with the largest likelihood are often nonsensical [37, 38]. Alternatively,\nsome research focuses on approximating Bayesian inference for deep learning models using techniques like Monte\nCarlo dropout [?], which involves performing stochastic forward passes through the network and averaging the results.\nAmong these approaches, [39] stands out as the only work that incorporates collaborative uncertainty among agents into\nthe modeling process to guide the ranking of multimodal trajectories by uncertainty, albeit requiring special training\nstrategies. However, none of the predicted uncertainties from these methods offer finite sample coverage guarantees,\nwhich is suboptimal for safety-critical applications such as vehicle trajectory prediction.\nConformal prediction (CP) [18, 40] has emerged as a widely adopted uncertainty quantification method, owing to its\nsimplicity, generality, theoretical rigor, and low computational overhead. Notably, CP is agnostic to the underlying\nmodel and data distribution, making it highly versatile. It seamlessly integrates with any pre-trained model to deliver\nstatistically valid prediction regions. Of particular relevance to our multimodal trajectory prediction task is recent\nprogress in generalizing CP to time-series forecasting. For instance, [41] introduced the Copula conformal prediction\nalgorithm for multivariate, multi-step time series forecasting, applicable to any multivariate multi-step forecaster.\nAdditionally, [42] focused on generating non-conformity score functions that yield multimodal prediction regions with\nminimal volume. Moreover, [43] employed CP to generate statistical uncertainty intervals from Gaussian mixture\nmodel outputs, obtaining separate prediction intervals corresponding to each GMM component prediction. Furthermore,\n[44] proved the validity of CP on graph data. Inspired by their work, we explore the potential of applying CP to the\nmultimodal trajectory prediction comparing different CP methods."}, {"title": "3 Problem Formulation", "content": "At the scenario level, We have trajectory data T from both the vehicle and infrastructure viewpoints, denoted as $T^V$\nand $T^I$, respectively. While $T^V$ and $T^I$ share overlapping information where their fields of view intersect, $T^I$ also\nprovides complementary information, being free from occlusions. Our modeling objective is to utilize the information\nfrom $T^I$ to improve the accuracy of trajectory prediction based solely on $T^V$.\nThe trajectory prediction task involves leveraging historical trajectories $T^V \\in \\mathbb{R}^{N_V \\times T_h \\times a_n}$ and $T^I \\in \\mathbb{R}^{N_I \\times T_h \\times a_n}$,\nalongside contextual information, typically HD maps denoted as M, to forecast future trajectories $T^V \\in \\mathbb{R}^{N_V \\times T_f \\times a_f}$.\nHere, $N_V$ and $N_I$ represent the number of observed actors from the vehicle and infrastructure perspectives, respectively.\n$T_h$ denotes the historical time horizon, and $T_f$ is the prediction horizon. $a_n$ and $a_f$ represent the number of node\nfeatures which we consider the vehicle center location defined by its x- and y-coordinates. Notably, the trajectory data\nfrom both views are defined within the same coordinate system. For the HD map, we opt for a vectorized representation\ndue to its lightweight nature and efficiency [4]. This vector map is depicted by lane centerlines, which are composed of\nlane segments. We denote it as $M \\in \\mathbb{R}^{N_L \\times a_l}$, where $N_L$ is the number of lane segments and $a_l$ is the number of lane\nattribute.\nWe formulate the overall probabilistic distribution as $P(T_Y|T^V, T^I, M^V, M^I)$. Driven by the critical safety demands\ninherent in trajectory prediction, we aim to incorporate uncertainty quantification to preempt any potentially conse-\nquential model failures. Let D be the set of scenarios of the form $(T^V, T^I, M^V, M^I, T_Y)$. We split the dataset into\ntraining $D_{train}$, validation $D_{val}$, calibration $D_{cal}$ and test $D_{test}$. One black-box deep learning model is trained on\n$D_{train}$ and evaluated on $D_{val}$. We achieve the uncertainty quantification by conformal prediction in a post-hoc way on\n$D_{cal}$.\nAt the agent level, we denote the features and labels of agent i from the vehicle view as $X_i^V=T_i^V$, and $Y_i^V=T_i^V$ for\nbrevity. In real-world scenarios, future trajectories may exhibit multimodal behavior, often approximated by mixture\nmodels, resulting in $\\hat{Y}^V \\in \\mathbb{R}^{K\\times T_f \\times a_f}$, where K represents the number of mixtures or modes. Given a new agent\nsample $X_{test}$ from $D_{test}$, we seek to construct the prediction intervals $C(X_{test}) \\in \\mathbb{R}^{K \\times T_f \\times a_f \\times 2}$ such that it covers the\nground truth label $Y_{test}$ under a predefined coverage rate leveraging conformal prediction.\nCP proceeds in three steps. First, we define a nonconformity score $A: \\mathbb{X} \\times \\mathbb{Y} \\in \\mathbb{R}^{K \\times T_f \\times a_f}$ to quantify how well Y\nconforms to the prediction at X. Typically, we choose a metric of disagreement between the prediction and the ground"}, {"title": "4 Methodology", "content": "Our method V2INet consists of two key components, predictive modeling and post-hoc conformal prediction. An\noverview of our proposed model is illustrated in Figure 3. We first represent the scenario data collected from both\nviews as graphs. A single view encoder is then applied separately to each graph, encoding various information such\nas agent-agent interactions, temporal dependencies, and agent-lane information. Subsequently, The vehicle-view\nembedding is fused with the infrastructure-view embedding through a cross-graph attention module. Finally, the\nupdated vehicle-view embedding pass through a multi-modal decoder, providing multimodal predictions for all the\nagents of interest. The post-hoc conformal prediction is then applied at the agent level to construct valid prediction\nintervals given a predefined coverage rate."}, {"title": "4.2 Scene Representation", "content": "We adopt an ego-centric coordinate system utilizing vectorized representation as first introduced in [4]. First, data from\nboth views are transformed such that the ego vehicle is centered at the origin, with its heading aligned along the positive\nx-axis. Each trajectory is then characterized as a sequence of displacements $\\{\\Delta x_t\\}_{t=-(T_h-1)}^0$, where $\\Delta x \\in \\mathbb{R}^2$ is the\n2D displacement from time step t \u2013 1 to t. Similarly, each lane segment is represented as $\\Delta x' \\in \\mathbb{R}^2$, which captures\nthe 2D displacement from the starting coordinate to the end coordinate of lane segment l. We then construct scenario\ngraphs consisting of actor nodes and lane nodes for both views separately. The edge attribute is the absolute relative\nposition between two nodes."}, {"title": "4.3 Single View Encoder", "content": "To encode the spatiotemporal information, including agent-agent interactions, temporal dependencies, and context\ninformation captured by agent-lane relations for data from each view, existing graph-based models offer effective\nsolutions. Models like LaneGCN [1], HiVT [3], and VectorNet [4] incorporate these components and can be readily\nemployed. This encoding step can be performed efficiently at the edge device, such as the vehicle's on-board computers\nand the roadside unit (RSU) for infrastructure data, before broadcasting, thereby minimizing computational overhead.\nWe exemplify here with HiVT [3] where only attention mechanism is employed, with the adoption of a rotation-invariant\nrepresentation. To attend to the local information, at each timestamp, the surrounding actors' information is aggregated"}, {"title": "4.4 Cross-graph Attention Module", "content": "To effectively utilize information from the infrastructure side, we employ a cross-graph attention module that aggregates\ninformation captured by the infrastructure view encoder. Specifically, the embedding from the vehicle view $h_i$\nis transformed into the query vector, while the embedding from the infrastructure view $h_j$ alongside the relative\nposition at the last observed time step $x_{i,j}^{t=0}-x_{i,j}^{t=0}$, are utilized to compute the key and value vectors. We denote\n$h_{ij} = (h_j, x_{i,j}^{t=0} - x_{i,j}^{t=0})$ representing the concatenation of node and edge attribute from infrastructure node j to vehicle\nnode i:\n\n$q_i = W_Q f h_i, k_{ij} = W_K f h_{ij}, v_{ij} = W_V f h_{ij}  \\qquad(1)$\nwhere $W_Q f, W_K f,W_V f \\in \\mathbb{R}^{d_k\\times d_h}$ are learnable matrices for linear projection and $d_k$ is the transformed dimension.\nThe resulting query, key and value vectors are then taken as input to the scaled dot-product attention block:\n$\\alpha_{ij} = softmax(\\frac{q_i^T}{\\sqrt{d_k}} [\\{k_{ij}\\}_{j \\in N_i}]) \\qquad(2)$\n$m_i = \\sum_{j \\in N_i} \\alpha_{ij} v_{ij} \\qquad(3)$\n$g_i = sigmoid(W_{gate}[h_i, m_i]),  \\qquad(4)$\n$\\hat{h_i} = g_i \\cdot W_{self} h_i + (1-g_i) \\cdot m_i \\qquad(5)$\nwhere $N_i$ is the set of agent $i$'s neighbors, $W_{gate}$ and $W_{self}$ are learnable matrices and $\\odot$ denotes element-wise product.\nWe followed the structure in HiVT to fuse the infrastructure information with a gating function. The attention block\nsupports multiple heads. Finally, we apply another MLP block to obtain the final fused embedding $\\hat{h_i} \\in \\mathbb{R}^{d_h}$ for agent i\nfrom the vehicle view."}, {"title": "4.5 Mixture Model Based Decoder and Learning", "content": "There are two widely used mixture models for describing the multimodal trajectories, Gaussian and Laplacian. Previous\nmethods [1, 4, 45] have found that the $l_1$-based loss function derived from the Laplace distribution usually leads to\nsuperior prediction performances, as it is more robust to outliers. Hence, we will parameterize the future trajectories\nfollowing Laplace distribution. For each agent, the decoder receives the final embedding as inputs and outputs K\npossible future trajectories and the mixing coefficient of the mixture model for each agent. The decoder are consisted\nof three MLPs, one for predicting the future locations $\\mu_i^k \\in \\mathbb{R}^2$ for agent i and its mode k at each time step t, one\nfor predicting the associated uncertainty $b_{i,t}^k \\in \\mathbb{R}^2$ assuming independence of the x- and y-coordinates, the last one\nfollowed by a softmax is for producing the scores for each mode.\nTo ensure the prediction diversity [46, 47], instead of optimizing all the predicted trajectories, only the mode k closest\nto the ground truth is optimized. The closeness here is defined as the average Euclidean distance between ground-truth\nlocations and predicted locations across all future time steps. The Loss includes both regression loss and classification\nloss\n$J = J_{reg} + \\xi J_{cls} \\qquad(6)$\nHere, $\\xi$ is the weight of the classification loss. We employ the negative log-likelihood as the regression loss:\n$J_{reg} = \\frac{1}{n T_f} \\sum_{i=1}^n \\sum_{t=1}^{T_f} -logP(\\mu_t |\\mu_{i,t}^k, b_{i,t}^k ) \\qquad(7)$\nwhere $P(\\cdot |)$ is the probability density function of Laplace distribution and $\\mu_{i,t}^k$, $b_{i,t}^k$ are the mean and uncertainty\nestimates of the best mode k."}, {"title": "4.6 Post-hoc uncertainty quantification module", "content": "Uncertainty quantification methods are evaluated on two key properties: validity and efficiency. Validity is established\nwhen the predicted confidence level exceeds or equals the probability of events falling within the predicted range,\nwhile efficiency refers to minimizing the size of the confidence region. We utilize conformal prediction to obtain both\nvalid and efficient prediction intervals. The standard conformal prediction methods typically operate with scalar point\nestimates for regression problems. However, since our output consists of a multimodal multivariate time-series, we need\nto make certain adaptations to accommodate this complexity. Following the steps outlined in Section 3, we proceed\nwith non-conformity score function definition, quantile computation and prediction interval construction."}, {"title": "4.6.1 Non-conformity Score Functions", "content": "As emphasized in [48], the usefulness of the prediction sets is primarily determined by the score function, we adopt\nthree score functions A and compare their performance.\nZ-score. As the decoder returns both mean and variance predictions, we define the Z-score function as:\n$Z = \\frac{Y - \\hat{Y}}{B} \\qquad(9)$\nwhere $Z\\in \\mathbb{R}^{K\\times T_f \\times 2}$ and $B = [\\{b_{i,t}^k\\}_0]$. Here, we compute scores separately for x- and y-coordinates,\nreflecting the observation that motion uncertainty varies significantly across different dimensions.\nL2-norm. Next, we consider the Euclidean distance, the most commonly used metric in regression problems:\n$L_2 = ||Y - \\hat{Y}||_2  \\qquad(10)$\nwhere $L_2 \\in \\mathbb{R}^{K\\times T_f}$.\nL1-norm. Recognizing that the L2-norm disregards dimension differences, we consider L1-norm:\n$L_1 = ||Y - \\hat{Y}||_1  \\qquad(11)$\nwhere $L_1 \\in \\mathbb{R}^{K\\times T_f \\times 2}$."}, {"title": "4.6.2 Quantile Computation", "content": "Given a predefined miscoverage rate $\\alpha \\in [0,1]$, the $1-\\alpha$ quantile of the non-conformity scores is\ncalculated on the calibration set $D_{cal}$. Conventionally, the quantile is determined as follows: $\\hat{H} =$\nquantile($\\{A(X_1,Y_1),\\cdots, A(X_n, Y_n)\\}, (1-\\alpha)(1+\\frac{1}{n}))$ where $A(X_i, Y_i)$ is a scalar, and n denotes the total number\nof agents in $D_{cal}$. However, in our case, we deal with multimodal time-series scores, which are multivariate when using\nZ-score and L1-norm, hence necessary adaptations are needed. For brevity, let $\\Gamma_i = A(X_i, Y_i)$, with $\\Gamma_i \\in \\mathbb{R}^{K\\times T_f \\times 2}$\nfor Z-score and L1-norm, and $\\mathbb{R}^{K\\times T_f}$ for L2-norm.\nTo enhance the efficiency of our prediction intervals, we focus on computing the quantile using only the mode k\nthat exhibits the smallest average Euclidean distance to the ground truth trajectory. This reduces the computation\nto multivariate time-series. Within this framework, we investigate two established methods: CF-RNN [49] and\nCopulaCPTS [41].\nCF-RNN. Since the time-series predictions are obtained from the same embedding, this work proposes the application\nof Bonferroni correction to the calibration scores to maintain the desired miscoverage rate $\\alpha$. Specifically, the original\n$\\alpha$ is divided by $T_f$, yielding $\\hat{H} = quantile(\\{\\Gamma_i\\}_{i=0}^n, (1-\\frac{\\alpha}{T_f})(1+\\frac{1}{n}))$ for single-variate case.\nCopulaCPTS. As implied by its name, this method models the joint probability of uncertainty for multiple predicted\ntime steps using a copula. The calibration set $D_{cal}$ is split into two subsets: $D_{cal-1}$, which estimates a Cumulative"}, {"title": "4.6.3 Prediction Interval Construction", "content": "We utilize the obtained quantile $\\hat{H}$ to form the prediction intervals for new examples in $D_{test}$:\n$C(X_{test}) = \\{Y^V \\in \\mathbb{Y} : A(X_{test}^V, Y^V) \\leq \\hat{H}\\}  \\qquad(12)$\nSpecifically, for $\\hat{H}$ obtained from the Z-score, we have:\n$C(X_{test}) = \\hat{Y}_{test} - \\hat{B \\hat{H}}, \\hat{Y}_{test} + \\hat{B \\hat{H}}  \\qquad(13)$\nand for $\\hat{H}$ obtained from the L2-norm and L1-norm, we have\n$C(X_{test}) = [\\hat{Y}_{test} - \\hat{H}, \\hat{Y}_{test} + \\hat{H}]  \\qquad(14)$"}, {"title": "5 Experiments", "content": "In this section, We introduce the specifics of the dataset, the evaluation metrics and the implementation details including\nhardware, hyperparameters, etc."}, {"title": "5.1.1 Dataset", "content": "We evaluate the proposed model on the publicly available large-scale and real-world V2I dataset V2X-Seq [6], which\nprovides the trajectories of agents from both vehicle and infrastructure sides, along with vector map data. V2X-Seq\nconsists of 51,146 V2I scenarios, where each trajectory is 10 seconds long with a sampling rate of 10 Hz. The\ntask involves predicting the motion of agents in the next 5 seconds, given initial 5-second observations from both\ninfrastructure and vehicle sides. The dataset has been split into train and validation. Our trained model is evaluated on\nthe validation set, allowing for comparison with existing models. For post-hoc conformal prediction, we divide the\nvalidation set into calibration set and test set at a 4:1 ratio. For a discussion on the calibration data size, please refer to\n[48]."}, {"title": "5.1.2 Evaluation Metrics", "content": "Model metrics. For model evaluation, the standard metrics in motion predictions are adopted, including minimum\nAverage Displacement Error (minADE), minimum Final Displacement Error (minFDE), and Miss Rate (MR), where\nerrors between the best predicted trajectory among the K=6 modes and the ground truth trajectory are calculated. The\nbest here refers to the trajectory that has the minimum endpoint error. The ADE metric calculates the L2 distance across\nall future time steps and averages over all scored vehicles within a scenario, while FDE measures the L2 distance only\nat the final future time step and summarizes across all scored vehicles. MR refers to the ratio of actors in a scenario\nwhere FDE are above 2 meters.\nConformal prediction metrics. We assess validity and efficiency for each method. Validity is evaluated by reporting\nindependent and joint coverage on the test set, aiming for coverage levels close to the desired confidence level $1 - \\alpha$.\nThe independent coverage for each agent is calculated as:\n$ind.\\ coverage_{1-\\alpha} = \\frac{1}{T_f} \\sum_{X,Y \\in D_{test}} \\sum_t I(Y \\in C(X))  \\qquad(15)$\nWe identify the maximum independent coverage among the K modes for each agent. For all metrics, the final reported\nvalue is the average among all agents in $D_{test}$.\nFor efficiency, we calculate the average size of the predicted 2D area across all time steps for the mode k with the\nmaximum independent coverage:\n$size = \\frac{1}{T} ||C(X)||  \\qquad(16)$"}, {"title": "5.1.3 Implementation Details", "content": "Model. The model was trained for 64 epochs on an Nvidia V100S GPU with 32GB memory using AdamW optimizer\n[50]. Hyperparameters including batch size, initial learning rate, weight decay and dropout rate are 32, 1e-3, 1e-4\nand 0.1, respectively. The learning rate is decayed using the cosine annealing scheduler [51]. Our model employs the\noriginal setting of HiVT with 64 hidden units for the single view encoder and 1 layer of cross-graph attention module\nwith 8 heads. The radius of all local regions is 50 meters. The number of prediction modes is set to 6.\nConformal prediction. With the pretrained model and datasets $D_{cal}$ and $D_{test}$, we execute all conformal prediction\nmethods on the CPU. We evaluate the methods on the three defined score functions at three different $\\alpha$ levels: 0.2, 0.1,\n0.05. The optimization step in CopulaCPTS remains consistent with the original work."}, {"title": "5.2 Results Analysis", "content": "In this section, we first examine the model's prediction performance, assessing it from both quantitative and qualitative\nperspectives. Next, we showcase the effectiveness of the post-hoc uncertainty quantification method."}, {"title": "5.2.1 Model Performance", "content": "Comparison with state-of-the-art. We benchmark our proposed model against state-of-the-art models using the\nV2X-Seq dataset, as detailed in [6]. Results are summarized in Table 2. Both TNT [5] and HiVT [3] are single-view\nmodels. [6] evaluated them under two settings: Ego, where only the vehicle-view data is utilized, and PP-VIC, which\nemploys a two-stage method with both vehicle view and infrastructure data. Specifically, in PP-VIC, data from both\nviews are fused offline with some tracking method and then the stitched trajectories were fed into a single-view model.\nWe retrained the HiVT model under both Ego and the PP-VIC setting with 64 epochs and present our results in Table 2.\nComparing the Ego and PP-VIC results, it's uncovered by [6] that integrating information from the infrastructure side\ncan enhance prediction accuracy. V2X-Graph represents the current state-of-the-art model, employing a graph link\nprediction module to associate two-view data, followed by fusing the embeddings of the associated nodes using attention\nmechanism. However, this node association module requires pre-training, leading to a two-stage training process. Our\nmethod demonstrates the best performance in terms of minFDE and MR. Without the explicit node association from\nboth views, our attention based fusion module can attend to the most relevant nodes from both views through learning,\nwhich significantly simplifies the modeling framework and facilitates ease of training, all while achieving better results."}, {"title": "5.2.2 Post-hoc Uncertainty Quantification Performance", "content": "Quantitative comparison. We show in this section that conformal prediction method produces more valid and efficient\nconfidence regions than the model predictions. The evaluation results are shown in Table 3."}, {"title": "6 Chapter Summary", "content": "We have presented a noval model framework with multi-view data integration in the cooperative driving setting.\nOur proposed model is straight forward and can be built upon any existing graph-based single-view models. It has\ndemonstrated its effectiveness and advantages over existing benchmarks. Moreover, we have incorporated a post-hoc\nuncertainty quantification module, providing valid and efficient confidence regions, which is crucial in safety-critical\ntasks such as trajectory prediction.\nThe proposed framework has certain limitations. From the model's perspective, we currently treat all road agents\nas the same type. However, in the public V2X-Seq dataset, there are different vehicle types, such as trucks, vans,\nbuses, motorcycles, etc. Future work should address the different characteristics of these vehicle types to enhance\nmodel performance. Moreover, better methods for encoding the lane information to eliminate off-road and road-\nrule-violating predictions should be investigated. From the uncertainty quantification perspective, we simplify the\nquantile computation on the multimodal prediction into computation on the single best mode. This approach loses\nvaluable distributional information. Therefore, exploring score functions that consider the distribution could lead to\nmore accurate uncertainty quantification for multimodal results. Furthermore, we evaluate the uncertainty assuming\nindependence among the agents. Future work can incorporate agent correlations based on the graph structure to better\nreflect the underlying uncertainty relations and provide more insightful results."}]}