{"title": "Passage Retrieval of Polish Texts Using O\u039a\u0391\u03a1\u0399 BM25 and an Ensemble of Cross Encoders", "authors": ["Jakub Pokrywka"], "abstract": "Passage Retrieval has traditionally relied on lexical methods like TF-IDF and BM25. Recently, some neural network models have surpassed these methods in performance. However, these models face challenges, such as the need for large annotated datasets and adapting to new domains. This paper presents a winning solution to the Poleval 2023 Task 3: Passage Retrieval challenge, which involves retrieving passages of Polish texts in three domains: trivia, legal, and customer support. However, only the trivia domain was used for training and development data. The method used the OKAPI BM25 algorithm to retrieve documents and an ensemble of publicly available multilingual Cross Encoders for Reranking. Fine-tuning the reranker models slightly improved performance but only in the training domain, while it worsened in other domains.", "sections": [{"title": "I. INTRODUCTION", "content": "Passage retrieval involves the task of retrieving a set of relevant text passages from a large collection of documents based on a given query. Typically, these passages are presented in descending order of relevance. The most commonly used method for passage retrieval is through lexical approaches like OKAPI BM25. Though, lexical models cannot capture semantic relationships between words, phrases, and sentences. To address this, neural language models can be employed. These models are often pretrained on extensive text corpora and then fine-tuned specifically for passage retrieval. There are two common setups for utilizing neural models in this task: complete passage retrieval using a neural model or combining another retrieval engine to retrieve a subset of passages, followed by using the neural model to select the most relevant ones. The latter approach is employed when the reranking model is too slow to process an entire document collection.\n\nThe Poleval 2023 Task 3: Passage Retrieval challenge aims to identify the best method for passage retrieval in Polish texts. The competition's test dataset comprises three domains: wiki-trivia, legal-questions, and allegro-faq. However, only the wiki-trivia domain is provided as the training and development dataset.\n\nIn this paper, we discuss the two-stage approach that achieved a score of 69.36 NDCG@10 on the final test com- petition dataset. Our method involves two phases. Firstly, we use the OKAPI BM25 algorithm to retrieve relevant passages. Then, an ensemble of Cross Encoder models is employed to rerank these passages. These models are publicly avail- able multilingual models that have been trained on various languages (including Polish) and finetuned on multilingual corpora for passage reranking, as outlined in [1]. We used these models with no further finetuning on the challenge dataset for two domains: legal-questions and allegro-faq. For the wiki-trivia domain, one model was fine-tuned and used in combination with models that had no further finetuning."}, {"title": "II. RELATED WORK", "content": "MS MARCO [2] is a large publicly available reranking dataset retrieved by Bing. The dataset includes queries, re- trieved documents by search engine, and a label on whether a user clicked a document. The corpus is in the English language. Recently, authors of mMARCO [1] translated this corpus into many languages (but not into Polish though) and trained Cross Encoder reranker models on it. The base models were multilingual. The performance was effective not only for translated languages but also for not translated languages, only visible by models in the semisupervised pretraining phase.\n\nBEIR [3] is an Information Retrieval benchmark for Zero- shot Evaluation between different domains. The authors pro- vided many comparisons between different retrieval architec- tures. Very recently, the benchmark for Polish Information Retrieval was released in BEIR-PL paper [4].\n\nThere are a few transformer language models trained for the Polish language: HerBERT [5], plt5 [6], Polish RoBERTa [7]. There are also many multilingual language models working on Polish languages, such as XLM-ROBERTa [8], multilingual DeBERTa [9], and mT5 [10]."}, {"title": "III. POLEVAL 2023 TASK 3: PASSAGE RETRIEVAL CHALLENGE", "content": "The task is to retrieve the relevant passages given a query. The queries and passages are in the Polish language. There are separate domains: wiki-trivia, legal-questions, allegro-faq. In the below subsection, each domain is presented. There are the following datasets: training (train), development (dev), test-A (preliminary test set), and test-B (final test set). For the training and development dataset, golden truth data was released during the competition, but the golden truth dataset was not. After competitions, the test set golden truth was released. Training and development datasets consist of only wiki-trivia, but the test dataset consists of all three domains. Below all domains are described. Some dataset statistics are given in Table I. Domains vary greatly in the number of passages and mean relevant passages per query.\n\n1) wiki-trivia: Questions are general-knowledge typical for TV quiz shows, such as Fifteen to One, or Polish equivalent Jeden z dziesi\u0119ciu. For each question, there were manually selected up to five relevant passages (the mean number for the training dataset is 3.28 with a standard deviation of 1.45). The passages corpus consists of 7097322 elements. This domain was selected for train, dev, and test datasets. There are 4041 questions in the train dataset, 599 in the dev dataset, 400 in the test-A dataset, and 891 in the test-B dataset. Below, one example question with all correct passages is presented.\n\n2) legal-questions: A portion of the legal questions were generated by randomly selecting provisions and formulating questions based on their content. The task is similar to SQUAD and requires only identifying relevant passages rather than answering the question. The questions were supplemented with 26287 provisions derived from over one thousand laws published between 1993 and 2004. There are 400 questions in the test-A dataset and 318 in the test-B dataset. Below, the example questions from the test-A dataset are provided.\n\n3) allegro-faq: Questions regards the large e-commerce platform- Allegro.pl were created using help articles and lists of frequently asked questions. There are 400 questions in the test-A dataset and 500 questions in the test-B dataset. In total, there are 921 passages. Here is an example question from the test-A dataset."}, {"title": "IV. METHOD", "content": "The solution involves two stages: Retrieval and Reranking. Retrieval is carried out using the lexical method OKAPI BM25, which is quick but not as effective as a neural ranking model. Additionally, it does not require training. The best performing method for Reranking is through Cross Encoders, but it is slow as it requires processing every query-passage pair. Due to its time-consuming nature, it can only operate on a limited set of passages, except for the allegro-faq domain, which consists of only 921 passages."}, {"title": "A. Retrieval phase", "content": "For retrieval model We used OKAPI BM25 algorithm using parameters k\u2081=1.2, b=0.75, \u03b5=0.25. The utilized library may be accessed via . The preprocessing included tokenization using the nltk library, specifically nltk.tokenize.word_tokenize, lowercase normalization, stemming using pystempel (accessed via ) with the Polimorf [13] stemmer, and removal of Polish stopwords."}, {"title": "B. Reranking phase", "content": "The reranking phase was performed using an ensemble of multilingual reranker models based on Cross-Encoder architecture. We used different sets of the ensemble for wiki- trivia domain and legal-questions with allegro-faq questions. Both are described in the following section. The ensembles were created by summing up all the individual models' probability scores. Finetuning, if performed, was loosely based on a script from Sentence-Transformer library [14], namely . The process of finetuning and inference was completed on A100 GPU card. We used one 100 negative query-passage pair for each positive passage selected from the training dataset. The negative passage selection was from the top 2000 passages returned by the described OKAPI BM25 algorithm. The used Loss was BCEWithLogitsLoss with a constant learning rate scheduler of le-6 and 2000 warmup steps. The best-performing model was selected for inference from training for ten epochs.\n\n1) wiki-trivia: Reranking was based on the top 3000 re- sults from the OKAPI BM25 algorithm. Because wiki-trivia passages are relatively short, they only require a little time, although, during experiments, we observed that reranking with above 1000 passages, there is not much gain in the metric score.\n\nThe ensemble consisted of three models:\n\n2) legal-questions and allegro-faq: Reranking was per- formed on top 1500 passages for legal-questions. The limit was lower than for wiki-trivia due to the length of passages collection and longer computation time. For the allegro-faq domain, reranking was performed on all the passages since the whole collection consists of only 921 passages. For both domains, the same ensemble was used. The following mod- els were used without further finetuning to the competition dataset. We conducted experiments using models fine-tuned to wiki-trivia, but their performance dropped drastically. Finally, we used the following models:"}, {"title": "V. RESULTS", "content": "The presented method scores 75.40 NDCG@10 on preliminary test-A and on 69.36 NDCG@10 on fi- nal test-B data. The experiments code is available at . The analysis of single models on different reranking size limits is presented in Table III for test-A and in Table II for test-B. The results vary between domains, probably because of text nature, as well as different passage collection sizes and different size mean relevant passages per one query. All the presented reranking models score better than the OKAPI BM25 baseline. With the reranking size limit, the performance is better. However, the gain isn't great beyond the reranking limit of 500. Finetuning models increase their performance only on the wiki-trivia domain and worsen on other domains. Unfortunately, these results are not included in the presented tables as we didn't save them."}, {"title": "VI. OTHER EXPERIMENTS", "content": "We have tried other approaches as well. These experiments were very preliminary and may yield better results if we spend more time on them. However, we decided to include them in this paper anyway.\n\nWe translated Polish passages and queries into En- glish using a machine translation model accessed by . English Cross Encoder reranking models did not perform on the translated texts better than multilingual reranking models on Polish texts tough.\n\nWe experimented with various publicly available Bi Encoder models, using them as one-stage retrieval models. Unfortu- nately, their performance was significantly inferior to that of the OKAPI BM25 algorithm operating alone. However, combining the OKAPI BM25 and Bi Encoder models as retrieval models for further reranking with the Cross Encoder model may lead to improved results and is a promising area for research. Our highest Bi Encoder score for untranslated documents was 9.26 NDCG@10, achieved using the sentence-transformers/distiluse-base-multilingual-cased-v1 model. For translated texts into English, our highest score was 21.00, obtained using the sentence-transformers/all-mpnet-base-v2 model.\n\nMMARCO does not include translations for Polish texts. We've attempted translating MS MARCO into English using model gsarti/opus-mt-tc-en-pl and training several reranking models on this data. The approach is similar to [4]. Neverthe- less, this work was published after the competition. In our case, this approach didn't yield better results than large multilingual models."}, {"title": "VII. CONCLUSIONS", "content": "This paper summarizes our solution to Poleval 2023 Task 3: Passage Retrieval. The system operates in two stages, utilizing OKAPI BM25 for retrieval and a multilingual ensemble of Cross Encoders for reranking. However, the system's perfor- mance varies between domains due to the limited availability of training data for only one domain. While fine-tuning the neural model can enhance results for this domain, it may have a negative impact on other domains."}]}