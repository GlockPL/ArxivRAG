{"title": "What are Models Thinking about? Understanding Large Language Model Hallucinations through Model Internal State Analysis", "authors": ["Peiran Wang", "Yang Liu", "Yunfei Lu", "Jue Hong", "Ye Wu"], "abstract": "Large language model (LLM) systems suffer from the models' unstable ability to generate valid and factual content, resulting in hallucination generation. Current hallucination detection methods heavily rely on out-of-model information sources, such as RAG to assist the detection, thus bringing heavy additional latency. Recently, internal states of LLMs' inference have been widely used in numerous research works, such as prompt injection detection, etc. Considering the interpretability of LLM internal states and the fact that they do not require external information sources, we introduce such states into LLM hallucination detection. In this paper, we systematically analyze different internal states' revealing features during inference forward and comprehensively evaluate their ability in hallucination detection. Specifically, we cut the forward process of a large language model into three stages: understanding, query, generation, and extracting the internal state from these stages. By analyzing these states, we provide a deep understanding of why the hallucinated content is generated and what happened in the internal state of the models. Then, we introduce these internal states into hallucination detection and conduct comprehensive experiments to discuss the advantages and limitations.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLM) have been widely adopted across various fields, ranging from natural language processing and generation tasks to specialized applications in healthcare [41,49,55], finance [54], and legal services [9, 14]. However, despite its impressive capabilities, a fundamental issue remains: users often lack trust in LLMs due to their tendency to produce \"hallucinations\" [21, 26]. Hallucination in the context of LLMs refers to the phenomenon where the model generates content that appears plausible but is factually inaccurate or misaligned with the provided context. This can result in responses that sound coherent and authoritative, yet introduce misleading or completely false information [34].\nTraditional hallucination detection methods usually focus on post-processing analysis of generated outputs, including checking the factual accuracy of responses through external RAGs [3, 12, 28, 38], adding prefix/suffix/system prompts according to prompt engineering [15], and self-consistency checking by generating multiple outputs [16,33,37]. However, these methods have some inherent limitations. The reliance on external databases will introduce additional complexity and computational overhead, especially when a large-scale external knowledge base is required [3]. Furthermore, the external base may not always be up-to-date or comprehensive, limiting their effectiveness in detecting certain hallucinations. Also, the prompt engineering-based methods will introduce additional prompts, which may lead to a decrease in response quality [15]. At last, since these methods all use external methods for post-generation intervention, there is a lack of understanding of the source of hallucinations.\nRecent studies have begun exploring the use of LLM's internal state for hallucination detection and intervention to overcome the limitations of hallucination detection and intervention methods based on external bases. The internal state of LLM, including attention weights [2, 8, 57], layer representation [1, 2, 5, 6, 11, 17, 22, 47], logits [17, 42], etc., provides a state representation of the model's reasoning process during content generation. By analyzing these internal states, signs of hallucination can be detected before the content is fully generated, thereby achieving real-time intervention and reducing computational costs. Since no external intervention is required, this series of methods usually requires low computational overhead while allowing private models to be deployed locally (no cloud third-party services are required). Most importantly, these methods have become more interpretable, opening up new research ideas.\nHowever, existing work based on internal states focuses on using a certain internal state for detection. They also did not provide sufficient transferability analysis. There is a lack of a systematic understanding of how large model hallucinations are generated during the internal inference processes.\nTo meet this gap, we propose HALUPROBE, a framework"}, {"title": "2 Preliminary", "content": "In this section, we first explore the preliminary definition of LLM hallucination (\u00a72.1) and the process of LLM inference (\u00a72.2)."}, {"title": "2.1 LLM Hallucination", "content": "LLM hallucination definition. Hallucination in large language models (LLMs) refers to the generation of content that is inconsistent with reality or factual information [21,43,51]. While this generated content may sound plausible, it is often based on patterns learned from training data rather than actual facts. The issue of hallucination significantly impacts the practical applications of LLMs, especially in scenarios requiring high accuracy and reliability, such as healthcare, legal contexts, and journalism [59].\nLLM hallucination types. Hallucinations in large language models can be categorized into two types:\n[1] Input-conflicting hallucination: This occurs when the content generated by the model deviates from the user's task instruction or input [43,51]. For instance, if a user asks for a dinner recipe, the model might mistakenly provide a lunch suggestion.\n[2] Context-conflicting hallucination: In multi-turn generation or lengthy content, models may exhibit self-contradictions [21,51,59]. This type arises when models lose track of the context or fail to maintain consistency throughout the conversation. For example, a model might introduce the current NBA commissioner, Adam Silver, but later refer to a former commissioner, David Stern.\nPotential causes of LLM hallucination. Causes of hallucinations can be analyzed from several perspectives:\n[1] Data-related issues: The training data for LLMs is often collected from the internet, containing significant amounts of outdated, inaccurate, or biased information [43, 51, 59]. These data quality issues can lead to the generation of hallucinated content .\n[2] Limitations of the training process: Models learn language patterns by predicting the next word rather than understanding the content deeply [21, 23,43]. Additionally, misalignment during fine-tuning (for example, through reinforcement learning from human feedback, or RLHF) can contribute to hallucinations.\n[3] Randomness in the inference stage: During content generatce inconsistent or contextually irrelevant outputs due to randomness in sampling strategies or decoding methods [21,23,51]."}, {"title": "2.2 LLM Inference Process", "content": "During the inference process of a large-scale Transformer model such as Llama, the whole process can be divided into several key stages (see Figure 1).\nInput processing. The first is the input processing stage, in which the input text is tokenized and converted into a format that the model can understand, generating corresponding token IDs [36]. Subsequently, these token IDs are input into the embedding layer to form word embeddings, which provide the basis for the model's subsequent understanding. In this way, the text information is effectively converted into numerical representations for subsequent calculations [10].\nUnderstanding. The next understanding stage processes the input tokens through a multi-head self-attention mechanism [4, 52, 53]. This mechanism calculates the similarity between tokens and generates contextual representations to capture long-range dependencies [56]. This process enables the model to fully understand the semantics of each token in a specific context and provide rich semantic information for the subsequent query stage [4,44].\nQuery. In the query stage, the model further processes the contextual representations generated in the understanding stage through a feedforward network [35, 60]. The feedforward network contains two linear transformations and nonlinear activation functions (such as ReLU) to generate new feature representations. The goal of this stage is to extract higher-level abstract information, improve the model's expressiveness, and lay the foundation for the final generation process.\nGeneration. The generation stage is the core of reasoning. The model converts the output of the query stage into the probability distribution of the next token through linear transformation and softmax function. In this process, the model uses contextual information to determine the most likely next token and generates a continuous text sequence. The decoding strategy of this stage, such as greedy search or beam search, directly affects the quality and coherence of the generated text.\nPost processing. Finally, in the post-processing stage, the generated token ID sequence is converted back to a readable text format. By merging subwords or words and removing special tokens, the output of the model is converted into human-understandable language. This stage ensures the readability of the generated content and enables users to interact directly with the model output. Through the close combination of these stages, the Transformer model realizes an effective reasoning process from input to output."}, {"title": "3 Proposed Scheme: HALUPROBE", "content": "Our proposed scheme consists of three parts: (1) Internal state extraction (\u00a73.1): HALUPROBE extracts internal states during the inference, and stores them locally. (2) Token selection (\u00a73.2): HALUPROBE selects target tokens for next stage feature extraction. (3) Feature extraction (\u00a73.3): HALUPROBE extracts target features from the internal states, we also discuss different feature selection and token selection for better analysis."}, {"title": "3.1 Internal State Extraction", "content": "Following the definition of \u00a72.2, HALUPROBE first extracts three types of internal state from the three stages during inference:\nUnderstanding: attention. Attention is the internal state in which LLMs understand the input context. We extract each attention score matrix from each transformer layer's head for each token t as:\n$[a_{l,h,i}]_{L \\times H \\times t}$ \t $x_t$  (1)\nwhere $a_{l,h,i}$ denotes the l-th layer's h-th head's attention score for token t on token i.\nQuery: layer representation. The query stage involves processing contextual representations through a feedforward network to derive higher-level abstractions. We capture the layer representation matrices from each transformer layer for each token t as follows:\n$[Y_{l,i}]_{L \\times t}$ =  $x_t$ (2)\nwhere $Y_{l,i}$ represents the l-th layer's layer representation for token t during the query phase.\nGeneration: logit. In the generation stage, the model transforms layer representations into logits, converting these outputs into probabilities via the softmax function to determine the likelihood of each next token. Each logit score for token t is calculated as:\n$[l_i]_{L \\times t}$ =  $x_t$  (3)\nwhere $l_i$ denotes the 1-th layer's logit score for token t in the generation phase, representing the raw output before applying the softmax."}, {"title": "3.2 Token Selection", "content": "Unlike previous external methods to detect hallucinations, internal state-based detection relies heavily on selecting tokens. Each internal state including attention, layer representation, and logit is highly relying on tokens. Thus we provided five different token selection methods inspired from previous works:\nAll tokens selection. Aggregate the features of all tokens (e.g., max, mean) and then input the aggregated result into the is_halu function to determine if the entire response is hallucinatory.\nis_halu_all = is_halu (f ({feature(ti)}=$_i$=1}^N))\nwhere $t_i$ represents each token in the response, N is the total number of tokens, and f is an aggregation function (e.g., max, mean).\nPer token selection. Apply the ispalu function individually to each token. If any token is identified as hallucinatory, the entire response is considered hallucinatory.\nis_halu_per_token = \u2228 is_halu($t_i$)\ni=1\nLast token selection. Apply the ishalu function only to the last token, using its result to determine if the entire response is hallucinatory.\nis_halu_last = is_halu($t_N$)\nwhere to is the last token in the response.\nFirst token selection. Apply the ishalu function only to the first token, using its result to determine if the entire response is hallucinatory.\nis_halu_first = is_halu($t_1$)\nwhere $t_1$ is the first token in the response.\nSliced window selection. Divide all tokens into multiple sliding windows, where each window contains w consecutive tokens and slides with a stride s. Apply the ishalu function to each window. If any window is identified as hallucinatory, the entire response is considered hallucinatory.\nis_halu_sliced = \u2228 is_halu({$t_k$,$t_{k+1}$,...,$t_{k+w-1}$})\nk=1\nwhere M is the number of windows, w is the window size, and k is the starting index of each window (ranging from 1 to N w+1). The symbol V denotes a logical OR operation. If any token group within a window is identified as hallucinatory, the entire response is considered hallucinatory."}, {"title": "3.3 Feature Extraction", "content": "We extract three groups of features from the extracted layer representations:\nAttention lookback ratio. For each token t, we calculate the lookback ratio for each layer and attention head h, defined as the proportion of attention score directed to previous tokens among all attention scores for that token:\n$\\frac{\u03a3_{i<t} a_{l,h,it}}{\u03a3_j a_{l,h,jt}}$\nThe lookback ratio quantifies the model's focus on historical context. When a model generates hallucinations, it may overlook parts of the contextual information, so a low Lookback Ratio might indicate insufficient use of historical context during generation.\nAttention allocation sharpness. Attention allocation sharpness reflects the concentration of the attention distribution for each token. By computing the entropy of token t's attention distribution in layer I and head h, we obtain the Sharpness:\n$\u2212\u03a3_j p_{l,h,j\u2192t}log p_{l,h,j\u2192t}$\nwhere\n$p_{l,h,j\u2192t}$=\n$\\frac{a_{l,h,jt}}{\u03a3_k a_{l,h,kt}}$\nAttention allocation sharpness indicates whether the model's attention is focused on a few important tokens. Low entropy suggests more concentrated attention, helping us understand if the model might be overly focused on particular tokens, potentially leading to hallucination.\nLast layer layer representation. The last layer representation is extracted directly from the layer representation of the last layer for each token:\n$Y_{L,t}$\nwhere $Y_{L,t} \u2208 R^d$ represents the layer representation vector of the L-th (last) layer for the token t. Here:"}, {"title": "4 Understanding Internal State", "content": "In this section, we provide an understanding of how the internal state changes during the lens of LLM inference (\u00a74.1), inference with RAG (\u00a74.2)."}, {"title": "4.1 Understanding through Lens of Inference", "content": "Method. We experimented to compare internal states that differ from the factual output and hallucinated output across the whole inference process. Specifically, we used 2 datasets: HaluEval for factual hallucination generation, and CNNDM for faithful hallucination generation. We collected the internal states of tested models using the 2 datasets and computed the extracted features. At last, we compared internal states between hallucinated output and factual output. The results are shown in Table 1.\nResults on attention states. The lookback ratio (LR) measures the proportion of attention directed toward previous tokens. In the HaluEval dataset, both factual (F) and hallucinated (H) outputs exhibit overlapping trends across attention heads (h \u2208 H) and attention layers (l \u2208 L), implying that $LR_{l,h}$ does not effectively distinguish between F and H. However, in the CNNDM dataset, $LR_{l,h}$ demonstrates a clearer separation, with F maintaining higher and more stable values compared to H ($LR^F > LR^H, \u2200l,h$). This suggests that hallucinations in CNNDM are associated with reduced attention to prior context, making LR a more sensitive feature for hallucination detection in this dataset.\nThe attention entropy ($E_l$) quantifies the concentration of attention distributions. For HaluEval, results show that F exhibits lower entropy ($E^F_l < E^H_l$), indicating more focused attention, whereas H outputs have higher entropy, reflecting dispersed attention. Aggregated across layers ($E_l$), this trend persists. In CNNDM, although $E^H_l > E^F_l$, the distinction is less pronounced. These results suggest that $E_l$ robustly captures the dispersion of attention in H, particularly for HaluEval."}, {"title": "4.2 Impact of RAG during Inference", "content": "Method. We utilized the HaluEval dataset to investigate the effect of RAG on LLM inference. For each question, the answer was converted into a retrieval-augmented knowledge base (RAG) to assist the LLM. Internal states, including attention scores, hidden states, and logits, were extracted during inference under two settings: with and without RAG. Features such as attention lookback ratio, activation entropy, and token probabilities were computed and compared between the two conditions, focusing on correct responses to analyze how RAG influences the model's reasoning process and mitigates hallucinations.\nResults on attention states. The lookback ratio ($LR_{l,h}$) reflects the model's focus on prior tokens. At the head level, the figure shows the distribution of $LR_{l,h}$ across all 1024 attention heads (h \u2208 H), while at the layer level, it aggregates over the 32 heads per layer (l \u2208 L). The results indicate that $LR^{RAG}_{l,h}$ (with RAG) is more consistent and slightly higher than $LR^{non-RAG}_{l,h}$ (without RAG), particularly across deeper layers (l > 10). This consistency arises because RAG provides external context, enhancing backward focus and enabling effective utilization of retrieved information. Without RAG, the model depends entirely on internal context, leading to greater variability ($Var(LR^{non-RAG}) > Var(LR^{RAG})$).\nThe attention entropy ($E_{l,h}$) quantifies the diversity of attention distributions. At the head level, $E^{RAG}_{l,h}$ is consistently higher than $E^{non-RAG}_{l,h}$, and at the layer level, the average entropy $E^{RAG}_{l,h}$ also demonstrates broader focus compared to $E^{non-RAG}_{l,h}$. This is because RAG enriches attention mechanisms with external knowledge, allowing attention to be allocated across a wider set of tokens ($E^{RAG} > E^{non-RAG},\u2200l,h$). Conversely, without RAG, attention is constrained to narrower contexts, resulting in lower entropy values.\nResults on activation states. The hidden states ($h_{i,t}$) represent the activation magnitudes across neurons for token t at layer l. At the neuron level, the distribution of Avg($h_{i,t}$) shows that RAG stabilizes activations, reducing variability ($Var(h^{RAG}_{i,t}) <Var(h^{non-RAG}_{i,t})$). At the layer level, the average activations ($h_l$) are consistently higher with RAG ($h^{RAG} > h^{non-RAG}$) due to the additional context provided by RAG, which enhances the magnitude and consistency of neuron activations. Without RAG, activations are unstable, occasionally exhibiting outliers.\nActivation sharpness ($S_l$), which reflects the concentration of activation values, is higher in earlier layers (l < 10) for both RAG and non-RAG settings. However, the decline in sharpness is more gradual with RAG, maintaining sharper activations ($S^{RAG} > S^{non-RAG}$, $\u2200l$ > 10). This phenomenon suggests that RAG helps preserve activation focus across deeper layers, likely due to the inclusion of external context. Without RAG, sharpness deteriorates more rapidly as activations diffuse across the network.\nResults on logit states. The minimum token probability (min($P_{l,t}$)) represents the model's confidence in its least likely predicted token. Both the head-level and layer-level trends show that min($P_{l,t}$) increases sharply in later layers (l > 10). The difference between RAG and non-RAG settings is negligible (min($P^{RAG}_{l,t} \u2248$ min($P^{non-RAG}_{l,t}$)), indicating that this feature is primarily influenced by internal computations rather than external context.\nThe joint token probability (\u03a01 $P_{l,t}$) reflects overall confidence in generating a sequence. Across layers, \u041f1 $P^{RAG}_{l,t} >$ \u041f1 $P^{non-RAG}_{l,t}$, with the gap being more pronounced in later layers (1 > 15). This improvement arises from RAG's ability to provide richer context, enhancing the model's cumulative confidence in token generation.\nThe average distribution divergence ($D_{avg}$) measures uncertainty by comparing predicted and reference distributions. In both RAG and non-RAG settings, $D_{avg}$ decreases consistently across layers (l > 5), showing that uncertainty is reduced through layer-wise refinement. The difference between RAG and non-RAG is minimal ($D^{RAG}_{avg} \\approx D^{non-RAG}_{avg}$), suggesting that uncertainty reduction is primarily driven by internal processes.\nThe maximum token rank (max($R_{l,t}$)) captures the relative position of the least confident token in the ranking. Across layers, max($R_{l,t}$) decreases consistently, reflecting growing confidence in top predictions. There is no significant distinction between RAG and non-RAG (max($R^{RAG}_{l,t} \u2248$ max($R^{non-RAG}_{l,t}$)), indicating that token ranking is predominantly determined by internal model dynamics."}, {"title": "5 HALUPROBE's Performance on Detection", "content": null}, {"title": "5.1 Datasets", "content": "To evaluate the performance of HALUPROBE in detecting hallucinations, we utilized three datasets from distinct domains: CNN/Daily Mail (CNNDM), Natural Questions (NQ), and HaluEval. These datasets provide diverse scenarios, including summarization, question answering, and factuality evaluation, ensuring comprehensive testing of the proposed framework.\nCNN/Daily Mail (CNNDM) [18]. This dataset is widely used for text summarization tasks. It consists of over 300,000 unique news articles from CNN and the Daily Mail, each paired with human-generated abstractive summaries. The dataset is divided into 286,817 training pairs, 13,368 validation pairs, and 11,487 test pairs. The average length of source documents in the training set is 766 words, while the summaries average 53 words. Hallucinations in this context are identified when generated summaries include content inconsistent with the original articles, emphasizing the model's ability to maintain fidelity in summarization tasks.\nNatural Questions (NQ) [25]. This question-answering dataset contains real user queries issued to the Google search engine, paired with corresponding Wikipedia pages. Each example includes a long answer (a paragraph) and, if applicable, a short answer (one or more entities) annotated by human annotators. The dataset comprises 307,373 training examples, 7,830 development examples, and 7,842 test examples. Hallucinations in NQ are defined as generated answers deviating from or contradicting the provided ground-truth answers, focusing on the factual correctness of responses.\nHaluEval [27]. A specialized dataset designed to evaluate the factuality of language model outputs. It includes 5,000 general user queries with ChatGPT responses and 30,000 task-specific examples from three tasks: question answering, knowledge-grounded dialogue, and text summarization. For general user queries, the dataset adopts the 52K instruction tuning dataset from Alpaca. This dataset serves as a benchmark for testing both factual and hallucinated responses across multiple scenarios, offering a comprehensive evaluation of the model's detection capabilities."}, {"title": "5.2 Ablation Study on Different Features", "content": "Method. To analyze the effectiveness of various features in distinguishing hallucinated and factual outputs, we conducted an ablation study. In this experiment, we isolated each feature group (logit, attention, and activation) as the sole input to the detection model and evaluated their classification performance on the HaluEval and CNNDM datasets. Specifically,"}, {"title": "5.3 Comparison between Different Token Selection Strategies", "content": "Method. To evaluate the impact of different token selection strategies on detection performance, we conducted experiments using various strategies, including All Token, First Token, Last Token, Per Token, and Sliced Window with different window sizes. For each strategy, all extracted features were used as inputs to the detection model. The metrics reported in Table 5 include accuracy scores on both the CNNDM and HaluEval datasets, allowing for a comprehensive comparison of the effectiveness of different token selection methods.\nResults. The results in Table 5 reveal that the Sliced Window strategy consistently outperforms others, achieving the highest accuracy (e.g., 0.87 on HaluEval with Window(4, 2)), due to its ability to maintain uniform token input sizes across samples. This ensures that the detection model receives structured and balanced information. On the other hand, the Per Token strategy, while intuitive, performs worse (e.g., 0.68 on CNNDM) likely due to noisy labeling issues rather than inherent flaws in the method itself. The First Token and Last Token strategies exhibit the lowest performance, as they fail to capture sufficient context for accurate classification. These findings suggest that while sliced window methods provide a structured and balanced approach suitable for diverse scenarios, the effectiveness of Per Token could be improved with more refined labeling processes."}, {"title": "5.4 Transferbility Study", "content": "Methd. To evaluate the transferability of different internal state features, we conducted experiments using CNNDM and HaluEval as training datasets and tested their performance on CNNDM, HaluEval, and NQ, a benchmark dataset designed for question answering with a focus on fidelity-related hallucinations. For each experiment, we trained models using one dataset and evaluated the effectiveness of individual features on the remaining datasets to assess their cross-dataset generalizability.\nResults. The results highlight several key findings. First, certain features trained on CNNDM, such as Lookback Ratio and Joint Probability, exhibit strong transferability to NQ, achieving relatively high accuracy scores (e.g., 0.66 and 0.61, respectively). This suggests that these features capture generalizable patterns that extend beyond the CNNDM dataset. However, features trained on HaluEval demonstrate poor transferability to CNNDM and NQ, with a significant drop in accuracy (e.g., Key Token Attention Ratio and Hidden State dropping to around 0.50). This discrepancy indicates that the features extracted from HaluEval are highly dataset-specific and may reflect idiosyncratic patterns rather than generalizable characteristics.\nThe lack of transferability between CNNDM and HaluEval is particularly notable. This suggests that the two datasets encode fundamentally different patterns of hallucination and factuality, potentially due to variations in text length, domain, or the nature of the hallucination tasks. These results highlight the importance of selecting datasets that align closely with the target domain when designing hallucination detection models"}, {"title": "5.5 System Overhead", "content": "Method. To evaluate the computational efficiency and storage requirements of different features, we measured the theoretical storage overhead, theoretical computational complexity, and actual computation time under the sliced window setting with a window size of 8. The theoretical metrics are expressed in terms of the number of tokens in the window (w), the number of attention heads (H), the number of layers (L), the dimensionality of hidden states (d), and the intermediate dimensionality in feed-forward layers (m). These measurements allow us to quantify the resource demands for extracting each feature and to identify potential bottlenecks in real-world applications.\nResults. The theoretical results, as shown in Table 7, provide a detailed breakdown of storage and computation costs for features from attention, activation, and logit states. Actual computation time will be discussed once experimental data is finalized."}, {"title": "6 Related Work", "content": "In this section, we summarize current LLM hallucination detection (\u00a76.1) and mitigation methods (\u00a76.2), and categorize them based on internal ways and external ways."}, {"title": "6.1 LLM Hallucinations Detection", "content": "Some previous works aim to detect hallucinated responses during LLM inference. We divided them into 2 types: external and internal.\nExternal. Some works detect the hallucinations from the external response. [20, 39, 46] utilizes RAG or KG to find the closest response from a ground-truth dataset, and compare it with the response from LLM to detect potential hallucinations. Considering the high computing and storage cost, some lightweight external methods have also been proposed: [5] propose new EigenScore metrics to evaluate responses' self-consistency. [45,58] check the keywords within the response to check factuality. [24] use semantics entropy from multiple queries to detect.\nInternal. Recently, researchers have tried to discover hallucination pattern within LLM inference process. Most works directly use hidden states [1, 2, 5, 11,47] as the classification features, while activation state is also taken into consideration [6, 17,22]. Some works [8,57] also use attention as the"}, {"title": "6.2 LLM Hallucinations Mitigation", "content": "Previous works have proposed several methods to mitigate hallucinations in LLMs. We divided previous mitigation methods into 3 types: external, model-based, and internal.\nExternal. Most works try to use external knowledge or inference paradigms to mitigate. The most direct methods utilize outside knowledge to enhance LLMs' ability to generate factual responses, which can be categorized into knowledge graph-based [32,48], and RAG-based [29,50]. LLMs' ability to self-debug their response is also used to mitigate hallucinations [7, 13]. Some works also optimize the inference process of LLM, like dividing the task into sub-tasks [40], building a chain for more robust reasoning [29, 32]. Multi-agent paradigm is also considered in hallucination mitigation [19], since the debate across multiple agents can enhance the response from a single agent.\nModel. Some works also aim at modifying models' weights for mitigation. [29, 30, 50] addresses this issue by introducing a new dataset to fine-tune the model. Some works aim to use full parameter fine-tuning [50], while other works aim to use LORA for more lightweight fine-tuning [29, 30]. Besides fine-tuning, model editing is also a popular model-level hallucination mitigation method [35], where developers or admins can only modify a few neural connections within the MLP layer to achieve accurate fact modification.\nInternal. Considering previous external methods and model methods' drawbacks on high cost and potential threat to catastrophic forgetting, some internal methods have been proposed. Internal methods mainly focus on lightly modifying the decoding process of LLM without changing model weight. [1,6] discover potential mode within hidden state, and utilize it to mitigate hallucination. [8] finds the significant difference between factual response and hallucinated response within attention scores, and intervenes attention score in the decoding process. [31] propose to use a new layer to intervene in logit output for better response."}, {"title": "7 Limitation & Discussion", "content": "Limited transferability. Based on the transferability experiments and insights from the RAG analysis, it is evident that the internal features of LLMs differ greatly across different scenarios. This lack of consistency results in poor cross-dataset generalizability. For example, features trained on CNNDM fail to perform well on HaluEval and vice versa, highlighting the dataset-specific nature of these features. This presents a significant challenge in developing universally applicable detection systems.\nPotential explainability. Despite the limitations, the proposed approach offers a promising avenue for explainability. Attention-based features, such as Lookback Ratio, and logit-based features, such as Joint Token Probabilities, provide interpretable insights into the model's reasoning process. These features allow researchers to better understand why certain outputs are classified as hallucinated, thereby enhancing trust in the system and opening up new possibilities for debugging and model refinement."}, {"title": "8 Conclusion", "content": "In this work, we proposed and evaluated a comprehensive framework for understanding and detecting hallucinations in large language models through internal state analysis. Our experiments demonstrated the potential of using attention, activation, and logit-based features to distinguish between hallucinated and factual outputs. However, challenges remain, including high computational costs, limited transferability of features across datasets, and the need for more efficient and generalizable detection strategies. Despite these limitations, the approach shows promise for improving the explainability of hallucination detection, especially through interpretable features like attention and logits. Future work will focus on optimizing feature extraction for real-time applications and exploring methods to enhance feature transferability across diverse scenarios."}, {"title": "Ethics Consideration", "content": "This study does not involve significant ethical concerns. The research is conducted using publicly available datasets (e.g., CNNDM, HaluEval), ensuring compliance with their usage policies and avoiding using private or sensitive data. No live systems were tested, and the methodology avoids actions that could harm users or disrupt services. Furthermore, the study emphasizes responsible reporting and discusses safeguards to prevent potential misuse of the findings. Ethical principles, including beneficence, justice, and respect for privacy, are inherently upheld in this research."}, {"title": "Open Science", "content": "To promote transparency and reproducibility in research, we commit to sharing the source code, datasets, and implementation details of our proposed framework, HALUPROBE. Upon acceptance, all relevant artifacts will be made publicly available on a trusted repository (e.g., GitHub or Zenodo) under an open-source license. This includes:\n1. The complete codebase for HALUPROBE, covering inner state extraction, feature computation, and hallucination detection.\n2. Instructions for reproducing the experiments, including the preprocessing of datasets and evaluation metrics.\n3. A detailed README file to guide users in replicating the results.\nWe believe that open science fosters collaboration and innovation, and we aim to contribute to the broader research community by ensuring the accessibility and usability of our work."}, {"title": "A Prompt templates", "content": null}, {"title": "A.1 Hallucination judgement prompt", "content": "Prompt Template of Labeling Process for Faithful Hallucination Benchmark\nYou will be provided with a document and a proposed summary. Your task is to determine if the proposed summary can be directly inferred from the document. If the summary contains any information not found in the document", "Conclusion": true}, {"Conclusion": false, "False": "identify the exact phrases or name entities from the summary that is incorrect by stating **Problematic Spans: [the inaccurate text spans from the summary", "format": "."}, {"Conclusion": true}, {"Conclusion": false, "Spans": ["the exact inaccurate text spans from the summary, in a list of strings"], "False": "n$document is the original document text for the faithful hallucinated question. $gt_response is the ground-truth response for the question. $response is target LLM's response"}]}