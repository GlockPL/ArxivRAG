{"title": "Accuracy-Privacy Trade-off in the Mitigation of Membership Inference Attack in Federated Learning", "authors": ["Sayyed Farid Ahamed", "Soumya Banerjee", "Sandip Roy", "Devin Quinn", "Marc Vucovich", "Kevin Choi", "Abdul Rahman", "Alison Hu", "Edward Bowen", "Sachin Shetty"], "abstract": "Over the last few years, federated learning (FL) has emerged as a prominent method in machine learning, emphasizing privacy preservation by allowing multiple clients to collaboratively build a model while keeping their training data private. Despite this focus on privacy, FL models are susceptible to various attacks, including membership inference attacks (MIAs), posing a serious threat to data confidentiality. In a recent study, Rezaei et al. revealed the existence of an accuracy-privacy trade-off in deep ensembles and proposed a few fusion strategies to overcome it [1]. In this paper, we aim to explore the relationship between deep ensembles and FL. Specifically, we investigate whether confidence-based metrics derived from deep ensembles apply to FL and whether there is a trade-off between accuracy and privacy in FL with respect to MIA. Empirical investigations illustrate a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. By experimenting with different numbers of federated clients, datasets, and confidence-metric-based fusion strategies, we identify and analytically justify the clear existence of the accuracy-privacy trade-off.", "sections": [{"title": "I. INTRODUCTION", "content": "Over the past few years, federated learning (FL) has gained significant traction as a widely adopted method in the field of machine learning (ML) with a primary focus on preserving privacy. FL enables multiple clients to work together in building a shared and cohesive model, all the while safeguarding the privacy of their individual training data [2]. Although FL is intended to protect individual's personal data, recent studies have revealed that FL models can be susceptible to various attacks that expose sensitive information from their training datasets, including source inference, model inversion, and reconstruction attacks [3].\nMembership inference attacks (MIAs) are indeed considered a serious threat to FL, as they can compromise the privacy and confidentiality of participant data by revealing whether specific samples were part of the training dataset, potentially undermining the security and trustworthiness of the framework. The accuracy of the MIA is widely accepted as a de-facto measure of an ML model's privacy [1]. In 2020, Nasr et al. showcased that in the realm of FL, adversarial participants can adeptly carry out active MIA against other participants, even in situations where the global model attains a high degree of predictive accuracy [4].\nFL and deep ensemble learning are two complementary approaches, both characterized by a collaborative nature, where multiple models or learners cooperate to enhance predictive performance in ML classifiers. FL addresses the challenges of privacy and decentralization by allowing multiple devices or servers to collaboratively train a global model without sharing raw data, while deep ensemble learning focuses on improving model performance and reliability by combining predictions from diverse, independently trained models. FL prioritizes privacy in a decentralized setting, with clients collaborating on a global model while keeping data locally. In contrast, ensemble learning focuses on improving accuracy by combining independently trained models without privacy concerns. The key difference lies in their primary objectives: privacy preservation for FL and enhanced predictive performance for ensemble learning.\nIn 2022, S. Rezaei has shown that the effectiveness of MIA increases when ensembling improves accuracy [1]. Their empirical study indicates that this accuracy-privacy trade-off exists even for more advanced and state-of-the-art ensembling techniques. Interestingly, they break this trade-off by changing the fusing mechanism of deep ensembles which improves accuracy and privacy simultaneously. Instead of calculating the average of confidence values, their approach provides the confidence score of the most confident model within the ensemble that predicts the same label.\nIn this paper, we aim to investigate two fundamental research inquiries by exploring the inherent connection between deep ensembles and FL. Firstly, \"With respect to MIA, does there exist an accuracy-privacy trade-off in a FL environment?\". Secondly, \u201cDo the various confidence-based metrics adopted to break the trade-off for deep-ensemble learning also hold for FL?\u201d\nIn Figure 1, we present a visual representation of the accuracy-privacy dynamics in FL. Specifically, the figure showcases the training of EfficientNet [5] on CIFAR10 [6], where each curve depicts the evolution of accuracy and privacy with rounds. Notably, the curves illustrate a consistent rise in accuracy coupled with a decrease in privacy, revealing the independence of these dynamics from the number of FL clients.\nThe main contributions of this paper, building upon the findings of Figure 1, are as follows:\nWe demonstrate that within an FL framework, there exists a strong correlation between model accuracy and privacy, particularly in the context of MIA. This correlation underscores a clear and inherent trade-off between model accuracy and privacy in the FL setting.\nWe make empirical studies with different numbers of clients for various datasets in a federated setting and establish that the number of clients is not monotonically correlated with accuracy and privacy.\nWe implement various confidence-metric-based fusion strategies, previously used in deep ensembles, to enhance accuracy and privacy. However, we identify an accuracy-privacy trade-off in the FL environment and provide analytical justification for its presence.\nThe rest of the paper is organized as follows. In Section II, we introduce the relevant theoretical background and present a brief literature survey. Section III defines the threat model, describing the goals, knowledge, and capabilities of the attacker and the defender. Section IV provides an analysis of the accuracy-privacy trade-off in the FL. Experimental results and discussions are provided in Section V. Finally, we conclude our work and discuss future research scope in Section VI."}, {"title": "II. BACKGROUND", "content": "In this section, we discuss the preliminary concepts and the background works on deep ensembles and FL along with various federated aggregation techniques.\nA. Deep Ensembles and Federated Learning\nIn the deep learning domain, deep ensembles are the predominant and heavily used method, where base models are initialized with random weights and trained on the same dataset [1]. This approach consists of two steps: 1) training the base models on the same training dataset with random weights, and 2) fusing their prediction confidence by averaging to create the final output. This differs from classical ML ensembles, as diversity in deep ensembles primarily comes from the random initialization of base learners. Classical ensemble learning approaches, although adaptable, are seldom used in deep learning models due to their lower accuracy compared to deep ensembles [7].\nThe consistent thread running through multiple studies establishes the prevalent assumption that FL inherently assumes the role of an implicit ensemble. Shi et al.'s Fed-ensemble method [8], capitalizing on model ensembling within the FL paradigm, stands out for its consistently superior performance compared to various FL algorithms. Similarly, Chen et al.\u2019s FedBE [9] introduces Bayesian model ensemble techniques to FL, illustrating the reliability of ensemble aggregation. Guha et al.'s investigation into one-shot FL [10] inadvertently reinforces the notion of implicit ensembling by demonstrating substantial improvements in Area Under the Curve (AUC) through ensemble learning and knowledge aggregation. Furthermore, Avdiukhin and Kasiviswanathan's exploration of asynchronous FL [11], while primarily focused on performance metrics, incidentally adds to the growing body of evidence suggesting that FL commonly embodies implicit ensemble characteristics. These recurrent findings collectively emphasize the widespread acknowledgment within the research community that FL naturally incorporates implicit ensemble mechanisms, contributing to improved generalization and overall performance.\nB. Aggregation Techniques in Federated Learning\nIn FL, the training process involves iterations between the central server and clients until a termination criterion is reached. This criterion may be a maximum number of iterations or a threshold for model accuracy. Once the process concludes, the central server converges the FL model (M), which is then distributed to each of the clients in the system.\nAggregation techniques play a pivotal role in FL, a privacy-preserving ML paradigm. In FL, models are trained across decentralized devices, and aggregating their updates is essential to construct a global model. Common aggregation methods include Federated Averaging [2], where local model updates are averaged, and Weighted Averaging, which considers the importance of each client's contribution based on factors such as data size or computation capability. These techniques enable the collaborative learning process while preserving the privacy of individual client data, making aggregation a critical aspect of the FL framework [12].\nIn 2017, McMahan et al. introduced the Federated Averaging (FedAvg) algorithm using Stochastic Gradient Descent (SGD) [2]. FedAvg involves the central server sharing global parameters and a model with a mini-batch of clients, which train the model on local data and share the updates. The global model is then created by averaging the weighted sum"}, {"title": "III. THREAT MODEL", "content": "In this section, we outline the fundamental threat model, detailing the defender and attacker perspectives [1], [16], [20].\nA. Defender's Perspective\nDefender's assumptions: In the context of FL, the defender utilizes the training dataset, using a disjoint subset of training samples for each client. A sample is classified as a member if it is used in at least one of the clients.\nThe defender provides API access, returning prediction confidence values, especially in multi-class classification. Base models can be trained from scratch, allowing for the exploration of techniques that enhance privacy, modify the training process, and consider alternative fusing methods beyond the traditional confidence averaging associated with deep ensembles.\nDefender's objectives: The defender's primary objective is to mitigate MIA within the FL framework while leveraging the accuracy benefits of ensemble learning. Mitigation of computational expenses is a primary concern throughout the training and inference processes. The investigation aims to achieve increased privacy protection with minimal loss to prediction accuracy within the FL paradigm.\nB. Adversary's Perspective\nAdversary's knowledge: We examine a challenging scenario where the adversary is limited to obtaining restricted information about the target model and knowledge concerning the distribution of the target dataset. The training goals and model architecture are universally shared among the FL participants, rendering this level of attacker knowledge realistic. However, the adversary is incapable of acquiring information regarding the global training process (whether centralized or federated) or the distribution of the training data among the clients.\nAdversary's goals: The adversary attempts to draw inferences or deductions of data from the initial training set. Subsequent to training, the attacker formulates an attack model that deduces private data through query-level access to the target model. The attacker refrains from modifying the parameters of the model and requires no additional information [16].\nAdversary's Capability: The assumed attacker is an ostensibly honest yet inquisitive user, granted query access to the target model but without the ability to access its internal weights and gradients. Despite these limitations, the attacker leverages the provided query access to execute an \u039c\u0399\u0391, attempting to discern if specific data points were part of the original training dataset."}, {"title": "IV. ACCURACY-PRIVACY TRADE-OFF IN FL", "content": "In [1], the authors present how in the context of ensemble learning there exists a similar tradeoff between accuracy and privacy. Moreover, they demonstrated that this is proportional to the number of ensembles. They demonstrated that ensemble learning increases accuracy at the cost of privacy.\nMembership inference attacks succeed in identifying the model reaction to the distributional difference between training and evaluation data. This is reflected in the difference between the evaluation confidence of the train and test samples of the model. Figure 2 demonstrates how the different client's independent predictions agree when correctly classifying the training and test samples. The figure highlights what percent of clients agree with the overall (FedAvg) correct classification. From the figure, one can clearly distinguish the training and test samples from the agreement among the different clients. But thankfully for the defender, the adversary cannot directly exploit this information. However, MIA can leverage the effect of this agreement on the final model's prediction confidence. Figure 3(a) demonstrates the prediction confidence of EfficientNet trained on CIFAR100 dataset. The distributional shift between the training and test samples is clearly observed. Figure 3(b) and (c) show the same results, separated by correct and incorrect predictions.\nIn the FL setting, despite forming an implicit ensemble, the number of clients is not correlated with the accuracy-privacy tradeoff. This is because, unlike in ensemble learning, the different clients in an FL setting do not have access to the entire dataset. This phenomenon is investigated in the following section."}, {"title": "V. RESULT ANALYSIS AND DISCUSSION", "content": "In this section, we present our findings and discuss their implications. We provide various experimental results on the accuracy-privacy correlation and the effect of the trade-off for different datasets under various fusion strategies in FL.\nFigures 4a and 4b demonstrates that accuracy and privacy are strongly correlated in FL settings. We present how the training and latest accuracy (green and blue) are strongly correlated with the attacker's accuracy with the MI attack (red) for different datasets, such as CIFAR10 [6], CIFAR100 [6], from the Canadian Institute For Advanced Research, MNIST [21] from the Modified National Institute of Standards and Technology database, and Fashion-MNIST [22] from Zalando Research. The training data was evenly distributed among the clients in a disjoint way. We performed the experiment with EfficientNet and ResNet18 [23] models over the FL architecture with 2, 5, and 10 clients. In our experiment, the client 0 is analogous to a centralized architecture.\nFigures 4a and 4b further shows that there exists no strong correlation between accuracy (and privacy) and the number of clients. To investigate this phenomenon, we trained a small basic convolutional neural network (CNN) architecture over the CIFAR10 dataset, with up to 50 clients. We repeated each experiment 10 times and presented the results in Figure 5. We observed that the basic CNN model started overfitting due to its limited learning capacity. We did not implement early stopping for this experiment. This experiment demonstrated that FL provides a regularization effect on the model being trained. We see a distinct trend where the model convergence rate is slower when the number of federated clients is larger. However, for a larger number of clients, the model finally converges into a slightly higher accuracy value. It's to be noted, that the improved accuracy over federated training was only observed for the basic CNN model, which did not have enough parameters to effectively learn the representation. For models such as EfficientNet, and ResNet18 such an increase is not observable.\nIn [1], Rezaei et al. proposed three methods to combine the ensemble model outputs that do not also make it easier for the MIA adversary to exploit. The three modes were to return a random base model, return the model that has the highest confidence, and return the model that has the highest agreed confidence. This approach works because the success of an MIA relies on a) confidence of the correct classification of the model, b) confidence of the incorrect classification of the model, and c) the level of agreement among the constituent models. Rezaei et al. observed that in ensemble learning the third point, the level of agreement among the constituent models, becomes very significant. The three approaches proposed in [1] aim to disrupt the adversary's ability to exploit this information.\nWe investigated the effect of these recommendations in a FL setting. We re-defined the return of a random base model into two schemes, return the first model, and return the model identified by the round number modulo number of clients, we call this the round robin scheme. We also tested the scheme where the most confident model and the correct confident model are returned.\nConsidering EfficientNet architecture and CIFAR10, CIFAR100, MNIST, and Fashion-MNIST datasets, Figures 6a to 6d demonstrate the effect of the privacy-preserving averaging schemes as compared to FedAvg. Similarly, Figures be to 6h demonstrate the effect of various privacy-preserving averaging schemes for ResNet18 architecture in FL environments. We can observe that, in terms of accuracy, each of the privacy-preserving averaging schemes performs worse than the FedAvg. This is a direct result of how these schemes are designed, as each of these schemes, at the end of the round, select the model from one single client to be retained. Thus the effective model is trained on approximately a dataset of $N/n$, where N is the size of the original dataset and n is the number of clients. This directly disadvantages settings with a large number of FL clients and accuracy gains through ensemble effects are canceled out. However, despite the drawbacks regarding the accuracy, when there is adequate data, and the effective model is trained to a high degree (e.g., MNIST), the proposed averaging schemes are remarkably effective in defending against MIA."}, {"title": "VI. CONCLUSION AND FUTURE SCOPE", "content": "This study explores the accuracy-privacy trade-off in FL for MIA and investigates the applicability of confidence-based measures from deep ensembles in FL. Using different numbers of clients, datasets, and confidence-metric-based fusion techniques for various deep learning models, a clear accuracy-privacy trade-off was identified and analytically justified. However, empirical studies reveal a lack of a non-monotonic correlation between the number of clients and the accuracy-privacy trade-off. The research identifies FL client data division as the primary contributor to accuracy loss. In the future, we aim to explore innovative approaches to enhance FL system privacy without compromising prediction accuracy."}]}