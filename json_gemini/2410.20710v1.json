{"title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models", "authors": ["Heerin Yang", "Seung-won Hwang", "Jungmin Sol"], "abstract": "Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counter-factual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model.", "sections": [{"title": "1. Introduction", "content": "A recently popular approach to solving natural language processing (NLP) problems is to use a pre-trained language model such as BERT [1] and ROBERTa [2], then fine-tune the model on a downstream task such as text classification. Although trained models achieve outstanding performance in various tasks such as sentiment analysis [3, 4] and natural language inference (NLI) [5, 6], it is well-known that these models often make decisions based on spurious patterns and correlations and therefore do not generalize well to other datasets. For example, NLI classifiers may learn that a sentence pair having significant lexical overlap is a sign that they are in an entailment relation-ship, which is not necessarily true [7].\nKaushik et al. [8] showed that a model trained on the original dataset performs poorly on a counterfactually revised dataset, which is another evidence that the model is relying on spurious patterns to classify data. For collecting counterfactu-ally revised data, human workers were asked to edit given data samples to produce new samples that have different labels than the original ones. For example, if the given NLI sentence pair is \"A man in a boom lift bucket welds. A man is working. (entail-ment)\", then the worker writes counterfactual samples by revis-ing the premise such as \"A woman in a boom lift bucket welds. A man is working (contradiction)\" or \"A person in a boom lift bucket welds. A man is working. (neutral)\". A classifier trained on the original dataset classifies all three pairs as entailment.\nIn this paper, we consider automatically generating counter-factual data for NLI tasks. While Kaushik et al. [8] claims that the counterfactually-revised train sets by human workers could improve model performance on the challenge sets, human annotation is costly. Our goal is to make the NLI model more robust to counterfactually revised data without getting help from hu-man annotators. Compared to other NLP tasks where a single sentence or passage is considered as input, NLI poses a unique challenge where a sentence pair is given as input and its rela-tion is an important feature for classification. However, existing augmentation methods such as EDA [9] regards an NLI sen-tence pair as a single unit of input without considering their relation. In contrast, we counterfactually augment hypothesis sentences for a fixed premise and vice versa, and represent their relation more explicitly, as a distance, to minimize or maximize during contrastive learning.\nSpecifically, we apply contrastive learning with the gener-ated set, pulling the original pair and the generated pair with the same label together while pushing the original pair and other generated pairs away, in the embedding space. We empirically find that this method is more effective than applying supervised contrastive learning with unrelated sentence pairs [10]. There are other recent methods [11, 12, 13] using automatic data aug-mentation and contrastive learning to make the model more ro-bust, but their improvements are limited mostly because they do not consider the unique characteristics of NLI where the inputs are pairs and their relations are important. The experi-mental results show that the proposed method achieves better accuracy compared to other robust text classification methods on counterfactually revised NLI datasets [8] as well as general NLI datasets\u00b9."}, {"title": "2. Related work", "content": "Data augmentation for NLP tasks can be divided into token-level and sentence-level augmentation. Token-level augmen-tation modifies individual words, such as substituting a word with synonyms [14, 15], randomly inserting, deleting, or swap-ping tokens [9]. Language models can be used for augmenta-tion, by masking a particular word and using the model to fill in the blank [16, 17]. The quality of token-based augmenta-tion depends on selecting which token to insert or remove, such as finding the rationale tokens and replacing them [11, 12, 13]. Sentence-level augmentation generates an entire sentence rather than modifying tokens from the original text. Examples include back-translation [18], paraphrasing [19], and conditional gen-eration [20]. While sentence-level augmentation can generate more diverse text compared to token-level augmentation, it is more difficult to assign labels or determine the quality of gener-ated data. Therefore, filtering methods based on teacher models are often used to select good quality data [21]."}, {"title": "3. Proposed Method", "content": "In our proposed method, we first generate a set of entailment, neutral, and contradiction sentence pairs for each sentence pair in the train set. We apply two major data augmentation ap-proaches, token-level and sentence-level augmentation, tailored for NLI tasks to generate factual and counterfactual data."}, {"title": "3.1.1. Token-level Data Augmentation", "content": "While simple methods such as synonym replacement [9] can be used to generate class-preserving data, it is not trivial to generate counterfactual data. Suppose the original premise-hypothesis pair is \"A man is walking down the street. A man is outside walking. (entailment)\" Changing the hypothesis to \"A woman is outside walking.\" will make the relation contradic-tory. However, if the original premise was \"A person is walking down the street.\", changing the hypothesis as such will not alter the label (neutral). In our proposed method, we take only one sentence from the original pair and copy the sentence to make an entailment pair (e.g. \"A man is outside walking. A man is outside walking.\"). From this pair, we apply word substitution on either premise or hypothesis to generate sentence pairs that belong to the three classes.\nFigure 1a shows our token-level data augmentation pro-cess. We first choose a random noun word in the sentence us-ing spacy\u00b2. Then, we use WordNet\u00b3 to find the substitution words. Table 1 shows how the substitution words are selected based on the revised sentence and the target class. For example, we choose a synonym or a hypernym to make an entailment sen-tence, a hyponym to make a neutral sentence, and an antonym or co-hyponym to make a contradiction sentence. Among can-didate words, we sample a word based on its frequency in the train set. In the case where no candidate substitution is found, the sentence pair is omitted from contrastive learning. Table 2 shows the sentences generated by four different configurations. One limitation of our scheme is that we only substitute nouns in the sentence. Substituting words other than nouns for counter-factual data generation is left for future work."}, {"title": "3.1.2. Sentence-level Data Augmentation", "content": "Conditional generation techniques such as LAMBADA [20] can be used to generate the hypothesis sentence conditioned on the premise sentence (and the label), and vice versa. We follow the basic approach of LAMBADA, but instead of generating inde-pendent samples, we let the generator create a set of entailment, neutral, and contradiction sentences for each input sentence.\nPre-trained sequence-to-sequence language models such as GPT-2 [24], BART [25], and T5 [26] can be used as a sen-tence generator, and we use T5 model to generate counterfac-tual premise or hypothesis sentences. The problem with using a generator model is that the generated sentence pairs may have incorrect labels. A typical method to address this problem is to evaluate the generated data samples on a classifier model trained on the original data, and filter out samples that have low confi-dence in the target class [11, 21]."}, {"title": "3.2. Relation-based Contrastive Learning", "content": "Once each sentence pair is augmented with sentence pairs cor-responding to all three classes, we train the classifier with the augmented set. The model is first trained with the contrastive learning objective. A set of four sentence pairs (original, entail-ment, neutral, contradiction) is passed through the encoder to obtain the sentence embedding vectors. Then, cosine similarity is measured between the original embedding vector and the em-bedding vectors of other sentence pairs. Finally, the contrastive loss LCL is calculated according to Eq. 1.\n$$L_{CL} = -log\\frac{exp(sim(x, x_y) / T)}{\\sum_{c} exp(sim(x, x_c)/T)}$$ (1)\nThe contrastive learning process is shown in Figure 2. Suppose the original label is entailment. Then, the distance between the embedding vectors of the original and entailment pair is minimized, while the distances between the embedding vectors of the original and other pairs are maximized. After contrastive learning, the model is trained using cross-entropy loss."}, {"title": "4. Experimental Results", "content": "We use the counterfactually augmented SNLI dataset (CF-SNLI), which is also used by previous works for testing the robustness of NLI models [11, 12, 13]. CF-SNLI set contains \"original\" train and test sets sampled from SNLI [5]. It also has \"revised premise\" (RP) and \"revised hypothesis\" (RH) set, where the premise and hypothesis sentences are revised by hu-man workers to produce sentence pairs with relations other than the original pair. We evaluate with all CF-SNLI test sets and also general NLI datasets- SNLI test set, MNLI dev-matched set, and MNLI dev-mismatched set [6].\nWe use BERT (bert-base-uncased) and RoBERTa (roberta-base) as pre-trained language models. For BERT, the model is trained with contrastive loss for 10 epochs (lr=1e-5), followed by cross-entropy loss for 3 epochs (lr=3e-5). For RoBERTa, the model is trained with contrastive loss for 10 epochs (lr=2e-6), followed by cross-entropy loss for 5 epochs (lr=1e-5). We use 0.1 as the temperature T in Eq. 1. In sentence generation, the threshold is empirically tuned to 0.9. The results were not sensitive to T, unless we choose a very low number.\nWe compare the performance of our method with other re-cent methods based on counterfactual data. SSMBA [11] uses a corruption function to perturb the original text and a recon-struction function to generate a new text in the underlying data manifold. MASKER [12] selects keywords in the text using at-tention scores or gradients and applies masked keyword recon-struction to help the model learn the context rather than relying on particular tokens. C2L [13] generates factual and counter-factual samples by masking non-causal and causal tokens in the original text, and applies contrastive learning to help the model learn to rely on causal tokens."}, {"title": "4.2. Results", "content": "In the tables, BERT-base and ROBERTa-base are baseline mod-els fine-tuned with CF-SNLI original train set, and SCL refers to supervised contrastive learning [10], where contrastive learn-ing is applied without data augmentation. RDA (Relation-based Data Augmentation) and RCL (Relation-based Con-trastive Learning) are the components of our proposed method. RDA is the case where only data augmentation is applied, whereas RDA-RCL is the case where contrastive learning is also applied. We seek to answer the following research questions.\nRQ1: Does the proposed method perform better than the base-line and other data augmentation methods? In Table 3, mod-els trained with different methods were evaluated on CF-SNLI test sets. We can observe that the proposed method achieves higher accuracy over the baseline and other methods in all sets for both BERT and ROBERTa models. The performance im-provement is 6-8% for the RP set and 3-5% for the RH set, re-spectively. The proposed method also achieves 3-4% improve-ment over the baseline on the original test set, which indicates that the method not only improves robustness to counterfactual revisions but helps the model performance in general.\nRQ2: Does the proposed method show good performance on the general NLI sets? Since it is important to see whether the proposed method is effective in datasets other than CF-SNLI, we have evaluated the models on SNLI test set and MNLI dev sets. Since CF-SNLI original set is sampled from SNLI, we can say that SNLI is an in-domain set whereas MNLI is an out-of-domain set. Table 4 shows that the proposed method achieves significantly higher accuracy over baseline for both BERT and ROBERTa models. While the accuracy improvement is 2-4% for SNLI, our method achieves 8-12% higher accuracy over baseline on MNLI dev sets, which shows that the method is also effective in improving generalization performance.\nRQ3: Is the proposed method better than general supervised contrastive learning? The relation-based contrastive learning applies supervised contrastive learning on sentence pairs with the common premise or hypothesis. The question is whether it is better than applying general SCL where contrastive learn-ing is applied to different sentence pairs. Table 3 and 4 show that applying general SCL only achieves marginal improvement over baseline, while RDA-RCL shows significantly better re-sults for different datasets as well as different models.\nRQ4: Does applying relation-based contrastive learning helps improving model performance? Since we assign labels to counterfactually generated sentence pairs, augmenting them to the train set already helps improve model performance. How-ever, applying relation-based contrastive learning further boosts performance. In Table 3 and 4, RDA-RCL achieves up to 2% higher accuracy over RDA for varying datasets and models, while there is no case where RCL degrades the performance.\nOverall, the proposed method is an effective way to robus-tify NLI models against counterfactual revisions, as well as im-prove model accuracy and generalization performance."}, {"title": "5. Conclusions", "content": "This paper studied the effectiveness of relation-based data aug-mentation and contrastive learning on NLI tasks. For a given sentence pair, the proposed method applies token-based and sentence-based augmentation to generate a set of counterfac-tual sentence pairs for all classes. Relation-based contrastive learning is done using the set of counterfactual sentence pairs to help the model effectively learn the difference between classes. Empirical results show that our methods can improve the ro-bustness of classifier models on NLI tasks. Since any sentences can be used as input to our methods, a possible future work can use our methods to create a large number of NLI sentence pairs using inputs outside the train set."}]}