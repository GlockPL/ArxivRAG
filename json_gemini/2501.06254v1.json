{"title": "RETHINKING EVALUATION OF SPARSE AUTOEN-CODERS THROUGH THE REPRESENTATION OF POLYSEMOUS WORDS", "authors": ["Gouki Minegishi", "Hiroki Furuta", "Yusuke Iwasawa", "Yutaka Matsuo"], "abstract": "Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool to improve the interpretability of large language models (LLMs) by mapping the complex superposition of polysemantic neurons into monosemantic features and composing a sparse dictionary of words. However, traditional performance metrics like Mean Squared Error and Lo sparsity ignore the evaluation of the semantic representational power of SAES whether they can acquire interpretable monosemantic features while preserving the semantic relationship of words. For instance, it is not obvious whether a learned sparse feature could distinguish different meanings in one word. In this paper, we propose a suite of evaluations for SAEs to analyze the quality of monosemantic features by focusing on polysemous words. Our findings reveal that SAEs developed to improve the MSE-Lo Pareto frontier may confuse interpretability, which does not necessarily enhance the extraction of monosemantic features. The analysis of SAEs with polysemous words can also figure out the internal mechanism of LLMs; deeper layers and the Attention module contribute to distinguishing polysemy in a word. Our semantics-focused evaluation offers new insights into the polysemy and the existing SAE objective and contributes to the development of more practical SAEs\u00b9.", "sections": [{"title": "INTRODUCTION", "content": "Sparse autoencoders (SAEs) have garnered significant attention in the field of mechanistic interpretability research (Sharkey et al., 2022; Bricken et al., 2023). SAEs aim to address the superposition hypothesis (Elhage et al., 2022), which suggests that activations in large language models (LLMs) can become polysemantic\u00b2 \u2014 that is, a single neuron corresponds to multiple functionalities posing a significant challenge for interpretability. By learning to reconstruct LLM activations, SAEs address this challenge by mapping polysemantic neurons to an interpretable space of monosemantic features (Sharkey et al., 2022; Bricken et al., 2023). Recent studies have demonstrated the utility of SAEs in various contexts. Templeton et al. (2024) employed SAEs to observe features related to a broad range of safety concerns in Claude 3 Sonnet model, including deception, sycophancy, bias, and dangerous content. Similar investigations have been conducted on GPT-4 (Gao et al., 2024) and Gemma (Lieberum et al., 2024). Moreover, the application of SAEs has extended beyond LLMs to diffusion models (Daujotas, 2024) and reward models (Riggs & Brinkmann, 2024).\nSAEs have an architecture consisting of a single, wide layer in the autoencoder, and their training typically involves a reconstruction loss coupled with an L1 penalty on the features (Sharkey et al., 2022; Bricken et al., 2023). This approach aims to learn features that are both sparse and accurate, creating a known trade-off between Mean Squared Error (MSE) and Lo sparsity. Previous research has focused on improving this MSE-L0 Pareto frontier, leading to proposals such as Gated SAE, TopK, and JumpReLU (Rajamanoharan et al., 2024a;b; Gao et al., 2024). Despite improvements in"}, {"title": "RELATED WORKS", "content": "Sparse Autoencoder Individual neurons in neural networks are polysemantic, representing mul-tiple concepts, and each concept is distributed across many neurons (Smolensky, 1988; Olah et al., 2020; Elhage et al., 2022; Gurnee et al., 2023). SAE is a widely used method that maps these polyse-mantic neurons to monosemantic high-dimensional neurons, improving the interpretability of activa-tions in neural networks (Sharkey et al., 2022; Bricken et al., 2023). In practice, such polysemantic SAE features have been shown to correspond to diverse semantics, ranging from concrete meanings such as Golden Gate Bridge to more abstract concepts like the entire Hebrew language (Bricken et al., 2023; Templeton et al., 2024). Recently, SAEs have been applied to large language models such as Claude Sonnet (Templeton et al., 2024), GPT-4 (Gao et al., 2024), and Gemma (Lieberum et al., 2024), where they have been used to discover dangerous content and biased features, as well as to control the behavior of LLMs. SAEs are trained by reconstructing the activations of LLMs and an L\u2081 sparsity loss of SAE features, which introduces a trade-off between MSE (fidelity) and Lo (spar-sity). In previous research, to improve the MSE-Lo Pareto frontier, the following approaches have been proposed: scaling the number of SAE features (Templeton et al., 2024; Gao et al., 2024), and introducing better activation functions, such as alternatives to ReLU (Rajamanoharan et al., 2024a; Gao et al., 2024; Rajamanoharan et al., 2024b).\nOne of the challenges with SAEs is the difficulty of evaluation. While MSE and L\u2081 sparsity are imposed during training, it is uncertain whether improving this Pareto optimality actually leads to the acquisition of monosemantic features. Existing research has used evaluations like the IOI task in GPT-2 (Makelov et al., 2024; Cunningham et al., 2023), where internal circuits are well understood, or automated evaluations using GPT-4 (Cunningham et al., 2023; Karvonen et al., 2024a). However, there is currently no direct evaluation metric to assess whether SAEs achieve their primary goal of mapping polysemantic activations into monosemantic spaces. In this study, we evaluate how well SAEs can acquire monosemantic representations at the level of individual word meanings using the Word in Context dataset (Pilehvar & Camacho-Collados, 2019), which includes polysemous tokens with meanings that vary depending on context.\nContext Sensitive Representations In natural language processing, word embeddings have been limited by their ability to represent only static features, where each word is associated with a single embedding regardless of the context in which it appears. To address this limitation, the Word-in-Context (WiC) dataset (Pilehvar & Camacho-Collados, 2019) was proposed for the evaluation of context-sensitive word embeddings. Currently, a multilingual version of WiC (Raganato et al., 2020) has been developed, and it has also become a part of the widely-used SuperGLUE benchmark (Wang et al., 2020). Because polysemantic features may naturally come from the polysemous words (Li"}, {"title": "PRELIMINARIES", "content": "We here summarize the key concepts and notation required to understand SAE architectures and their training procedures. The central goal of SAEs during training is to decompose a model's activation $x \\in \\mathbb{R}^n$ into a sparse, linear combination of feature directions, expressed as:\n$x = x_0 + \\sum_{i=1}^{M} f_i(x)d_i,$\nwhere $d_i$ represents unit-norm latent feature directions (M \u226b n), and the sparse coefficients $f_i(x) \\geq 0$ correspond to feature activations for the input x. This framework aligns with the structure of an autoencoder, where the input x is encoded into a sparse vector f(x) \u2208 $R^M$ and decoded to approximate the original input."}, {"title": "ARCHITECTURE", "content": "The typical SAE is structured as a single-layer autoencoder, where the encoding and decoding func-tions are defined as follows:\n$f(x) := \\sigma(W_{enc}(x - b_{dec}) + b_{enc}),$\n$\\hat{x}(f(x)) := W_{dec}f(x) + b_{dec}.$\nHere, $W_{enc} \\in \\mathbb{R}^{n \\times n}$ and $W_{dec} \\in \\mathbb{R}^{n \\times \\mathbb{R}^{n}}$ are the encoding and decoding weight matrices, respec-tively, where R is the expand ratio that determines the dimension of the intermediate in SAE. The vectors $b_{enc}$ and $b_{dec}$ represent the biases. We initialize the $W_{dec}$ as the transposed $W_{enc}$, following the methodology from previous studies (Gao et al., 2024). The sparse encoding f(x) \u2208 $R^{R \\times n}$ serves as the intermediate representation, while $\\hat{x}(f)$ reconstructs the input x from the sparse encoding. The columns of $W_{dec}$ correspond to the learned feature directions $d_i$, and the elements of f(x) represent the activations for these directions. \u03c3(\u00b7) is an activation function; we adopt ReLU, TopK (Gao et al., 2024), or JumpReLU, including its STE variant (Rajamanoharan et al., 2024b) in this paper, while using ReLU by default."}, {"title": "TRAINING METHODOLOGY", "content": "Training SAE involves optimizing a loss function that balances reconstruction accuracy and sparsity. The reconstruction loss is defined as the squared error between the input x and the reconstruction $\\hat{x}(f(x))$ and sparsity in the activations f(x) is encouraged using the L\u2081 norm. The overall loss function is a combination of these two terms, weighted by a sparsity regularization factor \u03bb:\n$L_{SAE}(x) = L_{recon}(x) + \\lambda L_{sparse}(f(x)) = ||x - \\hat{x}(f(x)) ||_2^2 + \\lambda ||f(x)||_1.$\nThis formulation encourages the autoencoder to reconstruct the input faithfully while enforcing sparsity in the latent features. To prevent trivial solutions, such as reducing the sparsity penalty by scaling down the encoder outputs and increasing the decoder weights, it is necessary to constrain the norms of the columns of $W_{dec}$ during training. In addition, we incorporate an auxiliary loss known as Ghost Grads (Jermyn & Templeton, 2024), which reduces the occurrence of dead latents where SAE features stop activating for long periods."}, {"title": "PS-EVAL", "content": "As a holistic evaluation of SAE considering the semantics in the features, we introduce Poly-Semantic Evaluation (PS-Eval), a suite of datasets and metrics for evaluation. PS-Eval consists of polysemous words whose meanings are different based on the given context, which aims to eval-uate whether SAE can map entangled polysemantic neurons in LLMs into simple monosemantic features for better interpretability while distinguishing the different meanings."}, {"title": "DATASET", "content": "We build PS-Eval on top of the WiC dataset (Pilehvar & Camacho-Collados, 2019), which is a rich source of polysemous words. The original WiC dataset provides two contexts with a shared target word for each sample, and its task is to determine whether the target word holds the same meaning or a different meaning in both contexts. The dataset has binary labels, where a label of 0 indicates that the target word has different meanings in the two contexts, and a label of 1 signifies that the target word's meaning remains the same in both context. For PS-Eval, we carefully select instances from the WiC dataset whose target word is tokenized as a single token in GPT-2 small.\nWe refer to the two contexts with a label of 0 (indicating different meanings) as poly-contexts (target word)."}, {"title": "EVALUATION METRICS", "content": "PS-Eval focuses on analyzing activations of the SAE for the target word across different contexts to determine whether SAE can extract monosemantic features. To identify the activation related to polysemous target words, we consider the following formatted input:\n{context}. The target word} means\nwhere the { context } is an example sentence of {target word}, reflecting either polysemous contexts (poly-context) or monosemous contexts (mono-context). In this paper, the acti-vation of the token for the {target word} at layer l is denoted as $a_{LLM}(context)$, and the feature from the SAE is represented as $f_{SAE}(context)$. Figure 1 (left) describe the conceptual workflow.\nConfusion Matrix for Polysemy Detection To evaluate the ability of SAEs to extract monose-mantic features, we construct a confusion matrix based on the maximum activations of the target word in different contexts. Each cell in the matrix is defined as follows:\n\u2022 In mono-context, where the target word has the same meaning in both contexts, we expect the same feature with the highest activation to appear. This corresponds to a True Positive (TP).\n\u2022 In mono-context, if different features with the highest activation appear, this results in a False Negative (FN).\n\u2022 In poly-context, where the target word has different meanings, we expect different features with the highest activation to appear, representing a True Negative (TN).\n\u2022 In poly-context, if the same feature with the highest activation appears for the target word, this results in a False Positive (FP).\nNote that positive or negative labels here are just for convenience to describe the confusion ma-trix, and these do not mean that, it is undesirable to identify that the different words have different features or it is desirable to identify that the same words have the same features. After classifying each instance into a confusion matrix, we then calculate the quantitative metrics such as accu-racy, precision, recall, F1 score and specificity to assess whether the SAE is capable of acquiring monosemantic features; Recall measures how often the same meaning words have the same SAE features. Precision measures how often the same SAE features are attributed to the same meaning words. Accuracy and F1 score can quantify the overall performance. Specificity measures how often the different meaning words have the different SAE features.\nIf SAEs have ideal features, all of these metrics can be high simultaneously. We advocate for a com-prehensive evaluation that considers all metrics holistically, in complement to existing performance measures, such as MSE, Lo, or IOI performances. While measuring individual metrics may also be useful to improve SAEs, we note that we need to carefully treat Specificity when solely interpreting it. Because Specificity measures when the different meaning's words are given, how often they are identified as different, it may reach an unfairly high value by definition if SAE features have sig-nificantly different representations of each other."}, {"title": "MONOSEMANTIC FEATURE MAY CAPTURE EACH MEANING IN POLYSEMY", "content": "PS-Eval is designed to evaluate SAEs through the analysis of whether the learned monosemantic features encoded from the polysemous words correspond to each word meaning, depending on the context. To clarify this core concept, we start with validating our implicit assumption of whether the SAE features encode word meanings by employing the logit lens technique (nostalgebraist, 2020).\nWe compute the maximum activated SAE features when two poly-context samples (Context 1, Con-text 2) are input. The corresponding activations for each context are calculated as:\n$a_{max}^1 = W_{dec}\\cdot max(f(x_1)), a_{max}^2 = W_{dec}\\cdot max(f(x_2)),$\nwhere $x_1$ and $x_2$ represent the activations of the LLM for Context 1 and Context 2 in poly-context, respectively. Next, we apply the unembedding matrix to these activations to obtain the logits:\n$logit_1 = W_u\\cdot a_{max}^1, logit_2 = W_u\\cdot a_{max}^2."}, {"title": "EVALUATING SAES WITH POLYSEMOUS WORDS", "content": "SAE FEATURES ARE MORE INTERPRETABLE THAN LLM ACTIVATIONS\nTo investigate the significance of using SAE for PS-Eval, we demonstrate that using SAE features results in better interpretability of polysemous contexts than the activations of LLMs. When a poly-"}, {"title": "INCREASING EXPAND FACTOR IMPROVES POLYSEMOUS SAE FEATURE", "content": "Previous works have reported that increasing the dimensionality of SAE features leads to better performance in terms of MSE and Lo metrics, and we may also expect that it allows for learning more fine-grained features from activations. To investigate the relationship between PS-Eval performance and the scaling of latent vari-ables, we evaluated the performance of our model trained with various expansion ratios in Figure 3 (right). Similar to MSE and Lo, we observed that increasing the expand ratio led to higher accuracy on PS-Eval. This trend indicates that a higher expand ratio enables the model to better extract the monosemantic fetures from polysemantic activations. However, we found that the accuracy saturates around an expand ratio of 64, suggesting that there is an optimal latent dimension for practical SAE usage."}, {"title": "ALTERNATIVE ACTIVATION FUNCTIONS TO RELU ARE NOT ALWAYS BETTER", "content": "Various activation functions for SAE have been proposed to achieve a better Pareto optimality be-tween MSE and Lo sparsity. We here evaluate whether these alternative activation functions also yield better results when applied to our evaluation metrics. In Figure 4, we compare models trained with TopK activations, where k is varied over values such as k = {384,192,96}, and models trained with JumpReLU, including its STE variant. For JumpReLU, the threshold is adjusted over the range of [0.0001, 0.001], while for the STE variant, the threshold represents an initial value."}, {"title": "LOCATING POLYSEMY CIRCUITS IN LLMS", "content": "Polysemy is Captured in Deeper Layers We investigate where large language models (LLMs) interpret polysemous words differently by using Specificity (calculated as $Specificity = \\frac{TN}{TN+FP}$) based on the confusion matrix from PS-Eval (see Table 2). Specificity allows us to assess whether the features in the SAE activate differently when presented with Poly-context (contexts involving words with defferent meanings).\nFigure 5 shows the Specificity of each SAE layer in relation to Lo and MSE. Previous studies (Gao et al., 2024) have demonstrated that training SAEs becomes increasingly difficult as the depth of the layers increases, which is consistent with our findings, as both MSE and Lo rise in deeper layers. While these two metrics increase, we observe that Specificity improves around the 6th layer, indicating that polysemous words are more easily decomposed into monosemantic features through SAE training. These results demonstrate an inverse relationship between MSE/L0 and Specificity, suggesting that focusing solely on improving the Pareto frontier of MSE or Lo, as emphasized in prior research, may overlook important insights.\nSAE Features Vary by Transformer Component While early prior work (Bricken et al., 2023) primarily centered on the activations within MLP layers, more recent studies have broadened this scope not only to MLP layers but also to residual and attention layers (Kissane et al., 2024; He et al., 2024)."}, {"title": "DISCUSSION AND LIMITATION", "content": "One potential limitation of PS-Eval lies in its reliance on evaluating the maximum activation of SAE features, which may introduce edge cases and failure modes. As discussed in Section 4.2 and illustrated in Figure 1 (left), PS-Eval is designed to measure how well SAE features respond to LLM activations associated with the meanings of target words. However, it is not guaranteed that the feature with the maximum activation always aligns with the intended word meanings. For instance, as observed in the logit lens results presented in Table 3, while SAE features often exhibit strong activation for specific word meanings, this behavior does not consistently generalize across all samples or LLMs. In some cases, the feature with the highest activation may reflect unrelated aspects rather than the meaning of the target word, potentially introducing ambiguity into the evaluation. To make the evaluation more robust, it would be interesting and important future direction to incorporate similarity metrics that utilize the distribution of SAE features, rather than relying solely on maximum activation."}, {"title": "CONCLUSION", "content": "We propose Poly-Semantic Evaluation (PS-Eval), a new metric and dataset to assess the ability of SAEs to extract monosemantic features from polysemantic activations, in complement to traditional MSE or Lo metrics. Our findings reveal that increased latent dimensions enhance Specificity up to a limit, while MSE-L0-optimized activation functions do not always improve semantic extraction. Deeper layers and Attention mechanisms significantly boost specificity, underscoring the utility of PS-Eval. We hope our work encourages the community to conduct a holistic evaluation of SAEs."}, {"title": "GHOST GRAD", "content": "In this section, we describe the auxiliary loss, which serves as an additional regularization mecha-nism for latent representations during training. The goal is to reduce the likelihood of inactive latent units by penalizing reconstruction errors from a subset of dead latents. This approach is conceptu-ally similar to the \"ghost gradients\" method (Jermyn & Templeton, 2024), though modified to handle high-dimensional latent spaces with a more targeted selection of inactive units.\nDuring training, each latent variable is monitored for activation. Latents that have not been activated for a pre-specified number of tokens (typically 10 million tokens) are flagged as \"dead.\" These latents are indexed by the parameter kaux, which represents the top-k latent dimensions with the least activation (usually set to kaux = 512).\nGiven the reconstruction error $e = x - \\hat{x}$, where x is the input and $\\hat{x}$ is the model's reconstructed output, we define the auxiliary loss Laux as the squared error between the true reconstruction and the reconstruction obtained using only the dead latents. This is formulated as:\n$L_{aux} = ||e - \\hat{e}||_2,$\nwhere $\\hat{e}$ represents the reconstruction from the dead latents, computed as $\\hat{e} = W_{dec}z$, where $W_{dec}$ is the decoder matrix and z is the latent vector associated with the top-k dead units.\nThe full loss during training is given by the sum of the main loss LSAE (Equation 4) and the auxiliary loss, weighted by a small coefficient a:\n$L_{total} = L_{SAE} + \\alpha L_{aux}.$\nIn our experiments, a is typically set to 1/32, which balances the influence of the auxiliary term without dominating the primary loss. Importantly, since the encoder forward pass is shared between the primary and auxiliary losses, the computational overhead from adding this regularization is minimal, increasing overall cost by approximately 10%.\nTo investigate the impact of Ghost Grad, we tracked the number of dead latents in the SAE during training. Dead latents are defined as features that do not activate even once during 1000 training steps, following Gao et al. (2024)."}, {"title": "OPEN DOMAIN VS IN-DOMAIN", "content": "As explained in prior work (Makelov et al., 2024), it remains unclear whether the dataset used for training the SAE should come from the same domain as the evaluation data (In Domain) or from a general dataset (Open Domain). In this section, we evaluate the SAEs trained on the WiC dataset (Pilehvar & Camacho-Collados, 2019) as the In Domain dataset and Red Pajama (Weber et al., 2024) as the Open Domain dataset."}, {"title": "DETAILS OF EXPERIMENT", "content": "Here are the detailed parameters for this experiment. We initialize the weights of the Decoder as the transposed weights of the Encoder, following the methodology from previous studies (Gao et al., 2024)."}, {"title": "RANDOM SAE", "content": "In this section, we analyze the performance of PS-Eval when SAEs are activated randomly. The experimental setup assumes an expand ratio of 32, meaning that the number of features in the SAE is calculated as expand ratio \u00d7 dmodel=32\u00d7768=23576. We compute the probability of overlapping activations (Psame) and the probability of non-overlapping activations (Pdiff).\nThe total number of ways to choose 2 features from 24576 features is given by:\n$\\binom{24576}{2} = \\frac{24576 \\times (24576 - 1)}{2}$\nThe probability of selecting the same feature is:\n$P_{same} = \\frac{1}{\\frac{24576 \\times (24576 - 1)}{2}} = \\frac{2}{24576 \\times 24575} = 3.3115039e - 9$\nThe probability of selecting the different feature is:\n$P_{diff} = 1 - P_{same} = 0.99999999668$"}, {"title": "TOPK", "content": "The TopK activation function is designed to directly control sparsity in autoencoders by select-ing only the k largest activations from the latent space, while zeroing out the rest. This approach eliminates the need for the commonly used L\u2081 penalty, which approximates sparsity by shrinking activations toward zero, often leading to suboptimal representations. The TopK function is defined as:\n$z = TopK(W_{enc} (x - b_{pre}))$\nwhere $W_{enc}$ represents the encoder weights, x is the input vector, and $b_{pre}$ is a bias term. By pre-serving only the top k largest values, TopK maintains a fixed level of sparsity, allowing for simpler model comparisons and more interpretable representations."}, {"title": "JUMPRELU", "content": "JumpReLU (Erichson et al., 2019) is an activation function introduced to enhance the performance of Sparse Autoencoders (SAEs) by improving the trade-off between sparsity and reconstruction fidelity. While the conventional ReLU activation function zeros out inputs less than or equal to zero, JumpReLU introduces a positive threshold \u03b8. Specifically, JumpReLU zeros out pre-activations below this threshold, while retaining values above it. This mechanism helps in reducing the number of false positives, thus improving sparsity, while preserving the important feature activations with high fidelity. The function is defined as:\n$JumpReLU(z) = z \\cdot H(z - \\theta)$\nwhere H is the Heaviside step function and \u03b8 is a learnable threshold parameter."}, {"title": "COMPONENTS OF THE TRANSFORMER AND ACTIVATION", "content": "The evaluation of the residual is based on the SAE trained using the activations from the 'Residual output'. Similarly, the results for the MLP are derived from the SAE trained on the activations from the 'MLP output', and the results for the attention are based on the SAE trained using the activations from the 'Attention output' .\nThis demonstrates that even within the same layer, the features captured by the SAE vary depending on the specific component (e.g., residual, MLP, or attention) whose activations are used for training."}, {"title": "TOPK WITH SMALLER K AND STRAIGHT THROUGH ESTIMATOR VARIANT JUMPRELU", "content": "We conducted further experiments to evaluate the impact of sparsity and activation function design on SAE performance, as detailed below:\nResults for TopK with Smaller k We extended our analysis to include TopK results for k = $\\frac{dmodel}{4}$ = 192 and k = $\\frac{dmode1}{8}$ = 92. Consistent with our expectations, smaller k values achieved higher recall compared to k = $\\frac{dmodel}{2}$ = 384. This result further emphasizes the benefits of in-creased sparsity in certain evaluation contexts and highlights the importance of selecting appropriate k values for optimizing recall in sparse autoencoders.\nResults for JumpReLU with STE We also evaluated the performance of JumpReLU using the straight-through estimator (STE) approach, following Rajamanoharan et al. (2024b). The results indicate that, even with the STE version, the improvements in metrics such as PS-eval were not as significant. This suggests that while JumpReLU offers clear advantages in terms of reconstruction and sparsity, further refinements to evaluation metrics like PS-eval may be required to fully optimize the design of sparse autoencoders.\nThese findings underscore the importance of carefully selecting evaluation metrics and activation functions tailored to specific goals in sparse autoencoder development."}, {"title": "CAN THE OPEN SAE MODEL ACQUIRE MONOSEMOUS FEATURES?", "content": "Unlike some previous evaluations for SAEs that rely on the IOI task (Wang et al., 2022; Makelov et al., 2024), our PS-Eval is a model-independent and general suite for evaluation, which allows us to assess open SAEs trained on activations from models such as Pythia and Gemma. Table 7 presents the evaluation results for SAEs trained on GPT-2 small (4 layers), Pythia 70M (4 layers), and Gemma2-2B (20 layers). While GPT-2 small closely matches the highest-performing model evaluated in our study (with an expand ratio of 64 and the ReLU activation function), other large language models (LLMs) generally received lower evaluation scores. To effectively acquire monosemantic features, our findings suggest that SAE parameters vary across different models and need to be individually tuned for each specific architecture."}, {"title": "RELATIONSHIP BETWEEN LO REGULARIZATION AND ACTIVATION FUNCTIONS", "content": "In our setup, with an expand ratio of 32 and a model dimensionality of 768, the total number of SAE features amounts to 24,576. Experiments with the TopK activation function demonstrated that reducing the value of k improves sparsity, as smaller k values lead to fewer active dimensions while maintaining comparable performance metrics.\nFurthermore, we observed that using JumpReLU with a jump value of 0.0001 results in the largest Lo-norm among all activation functions tested in this configuration. This activation function is the only one in our experiments where the number of active dimensions exceeds the original model dimensionality of 768."}, {"title": "DETAILED EVALUATION METRICS", "content": "In this section, we provide additional evaluation metrics to complement the results presented in the main text. First, we enumerate possible evaluation metrics derived from the confusion matrix . Following the experiments in the main text, we investigate the impact of the expand ratio, the effect of layer positions, and the influence of different Transformer components on the results. The results show that the metrics that achieve higher values vary depending on the experimental vari-ables. This highlights the need to assess the characteristics of the SAE using a variety of evaluation metrics, rather than optimizing for a single metric."}, {"title": "INTERPRETATION AND EVALUATION OF LOGIT LENS RESULTS WITH LLMS", "content": "We are exploring ways to extend this qualitative analysis into a more quantitative evaluation. One potential approach is leveraging automated evaluation methods using LLMs, as demonstrated in works like Cunningham et al. (2023). We conducted a preliminary experiment using the results from Table 3 of our paper. Specifically, we focused on the example involving the word \"space\". In this case, Context 1 refers to \u201cspace\u201d in the sense of \u201cuniverse\u201d or \u201couter space,\" while Context 2 refers to \"space\u201d as in \"blank space\" or \"gap.\""}]}