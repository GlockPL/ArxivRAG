{"title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation", "authors": ["Hui Ma", "Bo Zhang", "Bo Xu", "Jian Wang", "Hongfei Lin", "Xiao Sun"], "abstract": "Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in the context, an empathy reward function containing three empathy communication mechanisms, i.e., emotional reaction, interpretation, and exploration, is constructed using pre-designed and pre-trained empathy identifiers. Finally, the proximal policy optimization algorithm is used to further train the policy to produce empathetic responses. Both automatic and manual evaluations demonstrate that the proposed EmpRL framework can improve the quality of generated responses, enhance the empathy level similarity between generated and target responses, and produce empathetic responses covering both affective and cognitive aspects.", "sections": [{"title": "I. INTRODUCTION", "content": "PEN-domain dialogue systems, which converse with humans in open domains to provide reasonable responses and entertainment [1], have been extensively explored in recent years. With the development of neural response generation models [2]-[6], more fluent and coherent responses have been generated. Despite the improvements, the absence of empathy still results in a noticeable disparity between these and human-like dialogue systems.\nEmpathy often refers to the capacity to understand other's experiences and feelings, including aspects of affection and cognition [7], [8]. The affective aspect involves the emotional simulation in reaction to the user's experiences, while the cognitive aspect focuses on understanding the user's situation. Empathy is a fundamental characteristic of human conversations, and exploring ways to generate empathetic responses is crucial in the development of human-like dialogue systems [9]. It can enhance user experience and satisfaction, enabling deeper human-machine interaction, while providing support for scenarios such as psychological counseling and emotional companionship. Fig. 1 presents examples of empathetic and non-empathetic dialogue responses.\nRashkin et al. [10] released a large-scale empathetic dialogue dataset EmpatheticDialogues, paving the way for research in empathetic response generation. Existing methods mainly divided into two categories: one focuses solely on affective empathy, detecting and utilizing the user's emotion to generate responses, such as MoEL [11], MIME [12], and KEMP [13]; the other considers both affective and cognitive aspects of empathy, such as CoMAE [14], CEM [15], and CASE [16]. The two categories are primarily constructed using supervised learning, with maximum likelihood estimation as the optimization objective for training models to generate responses. These approaches overlook the empathy level alignment between generated responses and target responses.\nThe concept of empathy level is proposed by Sharma et al. [17]. They pointed out that empathy expression is manifested through three communication mechanisms: emotional reaction, interpretation, and exploration, where emotional reaction represents the affective empathy, interpretation and exploration mechanisms show the cognitive empathy. They further distinguished each mechanism into no, weak, or strong communication to reflect the empathy level of responses in the context. Aligning the empathy level between generated responses and target responses enables models to align with the way of human empathy expression, which helps to improve the quality of generated empathetic responses.\nTo achieve this goal, we introduce reinforcement learning (RL) into empathetic response generation and propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward and maximizes the expected reward through RL to generate empathetic responses. Specifically, EmpRL uses the pre-trained T5 model as the generator, further trains it on the EmpatheticDialogues dataset to generate fluent responses, and utilizes the trained generator to initialize the policy. Then, an empathy identifier is designed to recognize the empathy level of responses in the dialogue context and is trained on the Mental Health Subreddits dataset. Next, the pre-trained empathy identifier is utilized to construct an empathy reward function that includes three empathy communication mechanisms: emotional reaction, interpretation, and exploration. This ensures that the generated responses have a similar empathy level as the target responses. To prevent the policy from deviating excessively from the trained generator, which could lead to incoherent responses, a KL divergence penalty term is integrated into the empathy reward. Finally, the proximal policy optimization (PPO) algorithm is employed to train the policy to produce empathetic responses. We evaluate our proposed EmpRL framework on the EmpatheticDialogues dataset. Both automatic and manual evaluation results demonstrate that EmpRL can enhance the similarity of empathy level between generated responses and target responses, and improve the quality of generated responses.\nOur contributions are summarized as follows:\n\u2022 We propose EmpRL, an RL-based empathetic response generation framework that employs an empathy reward function to align the empathy level between generated responses and target responses.\n\u2022 We design an empathy F1-score evaluation metric to evaluate the similarity of empathy level between generated responses and target responses in the context.\n\u2022 Experimental results demonstrate that our proposed EmpRL exhibits superior performance, and the ability to generate more empathetic responses that encompass both affective and cognitive dimensions.\nThe rest of this paper is organized as follows: Section II summarizes related research on empathetic response generation and RL-based sequence generation. Section III gives the task definition and introduces the proposed EmpRL framework. Experimental settings and results are presented in Section IV and V, respectively. Section VI concludes the paper and describes our future work."}, {"title": "II. RELATED WORK", "content": "In recent years, empathetic response generation has attracted much attention. Unlike emotional response generation which focuses on generating responses with specific emotions [18]\u2013[21], empathetic response generation aims to respond in an empathetic manner. Rashkin et al. [10] indicated that detecting the user's emotion facilitates the generation of empathetic responses. MoEL [11] uses different Transformer decoders to compute response representations for each possible emotion and softly combines these representations to generate the response. Majumder et al. [12] assumed that empathetic responses often mimic the user's emotions to some degree, and introduced MIME to generate responses. Li et al. [22] utilized user feedback and proposed the EmpDG framework that includes the empathetic generator and interactive discriminators. KEMP [13] introduces commonsense knowledge and emotional lexical knowledge to understand and express emotions. These studies primarily rely on detecting the context emotion and utilizing the emotion to generate responses.\nEmpathy is a multi-dimensional construct that consists of affective and cognitive aspects. Concentrating on affective empathy solely is insufficient; it is essential to consider both affective and cognitive aspects. Based on GPT-2 [23], COMAE [14] hierarchically models three factors of empathy expression: communication mechanism, dialogue act, and emotion. CEM [15] leverages external commonsense knowledge to gather more information about the user's situation and feelings, and uses the information to generate empathetic responses. CASE [16] contains two heterogeneous graphs involving commonsense and concept knowledge, and designs a two-level strategy to align cognition and affection.\nCurrently, due to the remarkable capabilities demonstrated by large language models (LLMs) in natural language generation tasks, some works have embarked on exploring empathetic response generation using LLMs. Loh et al. [24] designed a simple instructional prompt to explore the capability of five LLMs in generating empathetic responses. Qian et al. [25] also conducted empirical investigation on the performance of LLMs such as ChatGPT in generating empathetic responses and proposed three targeted improvement methods. Chen et al. [26] constructed a multi-turn empathetic conversation dataset and finetuned ChatGLM to generate empathetic responses.\nHowever, existing methods ignores the empathy level alignment between generated responses and target responses. To ensure that generated responses have a similar empathy level as target responses in the context, we design an effective empathy reward function and produce empathetic responses by maximizing the expected reward through RL."}, {"title": "B. Reinforcement Learning for Sequence Generation", "content": "RL [27] trains the agent using rewards obtained through interactions with the environment, mainly for solving sequential decision-making problems. With the development of deep RL, sequence generation based on RL has received widespread attention. To alleviate the exposure bias problem of generative models, Ranzato et al. [28] proposed the mixed incremental cross-entropy reinforce (MIXER), which employs RL to optimize evaluation metrics such as BLEU and ROUGE for sentence generation tasks. Ziegler et al. [29] asked human annotators to provide feedback on various answers generated by the language model to train a reward model, and then utilized RL to optimize the language model, which was used for tasks such as summarization. Le et al. [30] introduced CodeRL for program synthesis tasks, leveraging RL to enhance the code generation capability of pre-trained language models.\nFor dialogue generation, Li et al. [31] devised three reward functions to evaluate the informativity, coherence, and ease of answering of generated responses, and employed the policy gradient method to enhance the sequence-to-sequence model. Saleh et al. [32] employed hierarchical RL to model utterance-level rewards, improving the flexibility of dialogue models to learn long-term, conversational rewards. Shin et al. [33] emphasized that sentiment look-ahead is crucial for generating empathetic responses and designed three distinct sentiment look-ahead reward functions to encourage more empathetic responses. Su et al. [34] utilized RL to integrate cognition and affection aspects of empathy, improving the perceptual and emotional expression abilities of the empathetic response generation model.\nSince achieving human-like empathy expression is crucial for the empathetic response generation, this work utilizes RL to align the empathy level between generated responses and target responses in the context."}, {"title": "III. METHODOLOGY", "content": "The task of empathetic response generation requires designing a dialogue model that plays the role of a listener and generates empathetic responses. Formally, let $C = [u_1, u_2, ..., u_{N-1}]$ denote a dialogue context of $N-1$ utterances, where the i-th utterance $u_i = [w_{i1}, w_{i2}, ..., w_{iM_i}]$ consists of $M_i$ words. All odd-numbered utterances $(u_1, u_3, ..., u_{N-1})$ and even-numbered utterances $(u_2, u_4, ..., u_{N-2})$ belong to the speaker and the listener, respectively. The task aims to generate the next response $y = [y_1, y_2, ..., y_M]$, which is fluent and coherent to the context, and empathetic to the speaker's situation and feelings."}, {"title": "B. Overview", "content": "The architecture of our proposed EmpRL is shown in Fig. 2. EmpRL first uses the pre-trained T5 model as a generator and further trains it to initialize the policy. Then, an empathy reward function is devised, incorporating three empathy communication mechanisms: emotional reaction, interpretation, and exploration. This is used to align the empathy level between generated responses and target responses in the context. Additionally, a KL penalty term is integrated into the empathy reward to prevent the policy from deviating excessively from the trained generator, which could lead to incoherent responses. Finally, PPO is employed to further train the policy to produce empathetic responses.\nIn the following sections, we first introduce the generator used for policy initialization and the empathy identifier for calculating empathy reward. Subsequently, we provide a detailed description of the various components of EmpRL, including state, action, policy, reward function, advantage function, objective, and optimization method."}, {"title": "C. Generator", "content": "Due to the powerful text generation capability of pre-trained language models such as GPT-3 [35], T5 [36], and BART [37], we utilize the pre-trained T5 model to generate responses. Fine-tuning pre-trained language models directly in downstream tasks is a common approach that achieves excellent performance across various NLP tasks. However, it involves updating all model parameters. To reduce the number of parameters that require updating, we employ prefix-tuning [38] instead of fine-tuning the pre-trained T5 model. Prefix-tuning involves prepending a sequence of continuous task-specific vectors (called prefix) to the input, and then freezing language model parameters while optimizing the prefix.\nThe generator after prefix-tuning (called Prefix-tuned T5) is described as follows:\n$p (\u0177 c) = \\prod_{j=1}^{M} P (Y_j | Y_{<j}, C).$"}, {"title": "D. Empathy Identifier", "content": "To establish an effective empathy reward in EmpRL, we devise an empathy identifier that recognizes the empathy level of the response in the context. The structure of the empathy identifier is illustrated in Fig. 3. The identifier first employs two independently pre-trained T5 encoders [36] to encode the context and the response, respectively. Then, the encoded response, serving as queries, and the encoded context, serving as keys and values, are input into a single-head attention [39], followed by a residual connection [40], to generate context-aware representation of the response. Finally, the response representation is fed into a max-pooling operation and a linear layer to derive the predicted label, i.e., empathy level.\nSharma et al. [17] pointed out that empathy expression involves three communication mechanisms-emotional reaction, interpretation, and exploration, and created a publicly available Mental Health Subreddits dataset. The dataset assigns labels to each (context, response) pair for three empathy communication mechanisms, representing the level of empathy. For each mechanism, no, weak, or strong communication of empathy is annotated. We use Mental Health Subreddits dataset to train the empathy identifier, with 20% set aside for validation. Three different empathy identifiers are trained for three communication mechanisms. The validation results of three empathy identifiers are presented in Table I. From the table, we find that all three empathy identifiers achieve excellent performance, demonstrating the feasibility of our designed empathy identifier."}, {"title": "E. EmpRL Architecture", "content": "The proposed EmpRL framework treats empathetic response generation as a sequential decision-making problem and utilizes RL to maximize the expected reward for generating responses. Below, we provide a detailed introduction to various components included in EmpRL.\nState: The state is the dialogue context along with the generated response. State at time t, denoted as $s_t = (\u0177_{<t}, c)$, includes the dialogue context c and generated tokens before t.\nAction: The action is the generated response. The action at time t is denoted as $a_t = \u0177_t$, which corresponds to the token generated at time t.\nPolicy: The policy network \u03c0\u03b8 generates empathetic responses based on the context, where \u03b8 represents the learnable parameters. At time t, the policy is $\u03c0_\u03b8 (a_t|s_t) = \u03c0_\u03b8 (\u0177_t|\u0177_{<t}, C)$, which predicts the current token considering the dialogue context and previously generated tokens. The EmpRL framework initializes the policy \u03c0\u03b8 using the trained generator p.\nReward Function: As shown in Fig. 2, the reward function consists of two components: empathy reward and KL penalty. The empathy reward aligns the empathy level between generated responses and target responses, while KL penalty prevents the policy from deviating far from the pre-trained generator.\n(1) Empathy Reward: The emotional reaction demonstrates affective empathy, while interpretation and exploration demonstrate cognitive empathy. To make generated responses have a similar empathy level as target responses in the context, we devise an empathy reward function that integrates the three empathy communication mechanisms:\n$R_{emp} (\u0177, y, c) = exp(-L_{emp} (\u0177, y, c))$,\n$L_{emp} (y, y, c) = \\sum_{i} CrossEntropy (\\hat{l_i}, l_i)$,\n$\\hat{l_i} = EmpathyIdentifier_i (c, \u0177)$,\n$l_i = EmpathyIdentifier_i (c, y)$.\nwhere y, \u0177, and c represent the target response, generated response, and context, respectively; li and $\\hat{l_i}$ are the empathy levels of the target response and generated response in the context, respectively, produced by the pre-trained empathy identifier $EmpathyIdentifier_i$; i represents a communication mechanism, belonging to emotion reaction, interpretation, or exploration.\nEmpRL treats li and $\\hat{l_i}$ as the gold and predicted label, respectively. The cross-entropy loss between them is computed as the loss for communication mechanism i. By summing up the losses for three different communication mechanisms, we obtain the empathy level loss $L_{emp} (\u0177, y, c)$. Finally, we use Eq. (2) to product the empathy reward $R_{emp} (\u0177, y, c)$.\n(2) KL Penalty: To prevent the policy from deviating excessively from the trained generator, which could result in incoherent responses, a KL-divergence penalty term is integrated into the empathy reward. KL penalty at time t is:\n$R_{kl} (\u0177_{<t}, c) = log \\frac{\u03c0 (.| \u0177_{<t}, c)}{p(.| \u0177_{<t}, c)}$\nThe final reward vector R (\u0177, y, c) \u2208 RT is defined as:\n$R (\u0177, y, c) = {r_t : t = 1,\u2026\u2026\u2026 ,T}$,\n$r_t = \\begin{cases}\n-\u1e9eR_{kl} (\u0177_{<t}, c), & t = 1,\u2026\u2026 ,T \u2013 1, \\\\\nR_{emp} (\u0177, y, c) \u2013 \u1e9eR_{kl} (\u0177_{<t}, c), & t = T.\n\\end{cases}$\nwhere rt represents the reward at time t, T is the length of the generated response, and \u03b2 is the KL penalty coefficient. Since the empathy reward $R_{emp} (\u0177, y, c)$ can only be obtained after the entire response is generated, the rewards for all time steps prior to T exclude the empathy reward."}, {"title": "Advantage Function", "content": "According to generalized advantage estimator [41], the advantage function for the state-action pair (st, at) at time t is defined as:\n$\\hat{A_t} = \u03b4_t + (\u03b3\u03bb) \u03b4_{t+1} + ... + (\u03b3\u03bb)^{T-t+1} \u03b4_{T-1}$,\n$\u03b4_t = r_t + V (\u0177_{<t+1}, c) \u2013 V_\u03c0 (\u0177_{<t}, c)$.\nwhere rt is the reward at time t and $V_\u03c0(s_t)$ is the value function for state st; \u03b3, \u03bb \u2208 [0, 1] represent the discount factor and adjustment factor, respectively."}, {"title": "Objective", "content": "The objective of EmpRL is to find a policy that maximizes the expected reward of generated responses:\n$\\max_\u03b8 E_{\u0177\u223c\u03c0_\u03b8} [R (\u0177, y, c)] .$\nTo reduce the variability of predictions, the maximization of the expected reward is transformed into the maximization of the expected advantage:\n$\\max_\u03b8 E_{\u0177\u223c\u03c0_\u03b8} [\\sum_{t=1}^{T} A_\u03c0 ((\u0177_{<t}, c), \u0177_t)].$"}, {"title": "Optimization Method", "content": "PPO [42] is an efficient and stable policy gradient algorithm widely utilized in RL-based text generation. Therefore, we employ PPO to train the policy \u03c0\u03b8. The definition of the total loss function Lo is:\n$L_\u03b8 = \u2212L_{CLIP} + \u03b1L_{VF}$,\n$L^{CLIP} = E_{\u03c0_\u03b8} [\\sum_{t=1}^{T} min(c(\u03b8), clip(c_\u03b8(\u03b8), 1-\u03b5,1+\u03b5) \\hat{A_t})]$,\n$L^{VF} = E_{\u03c0_\u03b8} [\\sum_{t=1}^{T} (V_\u03c0 (Y_{<t}, c) - (\\hat{A_t} + V_{old} (Y_{<t},c)))^2]$.\nwhere $L^{CLIP}$ is the clipped surrogate policy objective function and $L^{VF}$ is the value function squared error term; a and \u03b5 are hyperparameters, representing weights for the linear combination of losses and the proximal policy ratio clip range, respectively; $c(\u03b8) = \\frac{\u03c0_\u03b8(a_t|S_t)}{\u03c0_{\u03b8old}(a_t|S_t)}$ is the ratio of the new policy to the old policy.\nMinimizing total loss function involves maximizing the clipped surrogate policy objective while simultaneously minimizing the value error. Maximizing the clipped surrogate policy objective essentially aims to maximize the expected reward objective, where the surrogate policy objective function calculates the ratio between the current and the old policy and then constrains the update step within a suitable range. This prevents large variations in the policy gradient algorithm during each update, leading to a more stable policy optimization. Additionally, minimizing the token-level value estimation, which is defined based on the difference between values of the new policy and the estimated dense returns of the old policy, further contributes to improving the stability of the algorithm."}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "We conduct experiments on the EmpatheticDialogues dataset [10]. EmpatheticDialogues is a large-scale multi-turn dialogue dataset containing 24, 850 open-domain empathetic conversations between speakers and listeners. Fig. 4 illustrates an example from the EmpatheticDialogues dataset. Each conversation is grounded in a situation, which is a description written by the speaker according to a pre-given emotion label (the dataset provides 32 uniformly distributed emotion labels). In a conversation, the speaker talks about the situation while the listener responds empathetically. The EmpRL framework generates responses based solely on context information, without relying on emotion labels or situation descriptions."}, {"title": "B. Implementation Details", "content": "We implement the proposed EmpRL using PyTorch\u00b9 framework and Transformers\u00b2 library. The implementation comprises two stages: pre-training the generator and fine-tuning using RL.\nGenerator Pre-training: We further train the pre-trained T5-base model on the EmpatheticDialogues dataset using prefix-tuning. During training, we utilize AdamW [43] optimizer, with an initial learning rate of 1e - 4 and batch size of 8. For the input context and output response, prefix lengths are set to 40 and 10, respectively. Early stopping is applied when training. During inference, we set the maximum decoding step as 30 and use topk-topp [44] sampling strategy, where k = 20, p = 1.0, and temperature T = 0.9.\nRL Fine-tuning: For fine-tuning using RL, we also employ the AdamW optimizer during training. The number of steps for fine-tuning the generator with PPO is set to 24,000, with learning rate of 1.41e \u2013 5, batch size of 8, and 4 PPO epoch per batch with one minibatch each. The values for KL penalty coefficient \u00df, discount factor \u03b3, adjustment factor \u03bb, linear combination weight between losses a, and proximal policy ratio clipping range \u03b5 are set to 0.2, 1.0, 0.95, 0.1, and 0.2, respectively."}, {"title": "C. Baselines", "content": "We compare EmpRL with the following baseline models.\nMoEL [11]: A Transformer-based model that utilizes different Transformer decoder to compute response representation for each possible user emotion and then softly combines these representations to generate responses.\nMIME [12]: A Transformer-based model that takes into account emotion grouping and emotion mimicry. It initially generates mimicking and non-mimicking representations for the response and then fuses these representations to generate responses.\nEmpDG [22]: A multi-resolution adversarial framework that includes an empathetic generator and interactive discriminators. The generator utilizes coarse-grained dialogue-level and fine-grained token-level emotions to generate responses, while interaction discriminators interact with user feedback.\nKEMP [13]: A knowledge-aware model that leverages commonsense knowledge and emotional lexicon knowledge to understand and express emotions. It consists of an emotional context graph, an emotional context encoder, and an emotion-dependency decoder.\nThe above models mainly focus on affective empathy, i.e., detecting and utilizing the user's emotion for generating empathetic responses. The following baselines consider both affective and cognitive aspects of empathy.\nCEM [15]: A Transformer-based model that employs COMET [45] to generate commonsense knowledge, enhancing the understanding of user's situation and feelings. This enables the generation of more informative and empathetic responses.\nSEEK [46]: A serial encoding and emotion-knowledge interaction method that predicts emotion-intent characteristic of responses, and models bi-directional interactive selection between commonsense knowledge and emotions to generate more empathetic responses.\nCASE [16]: A framework that constructs two heterogeneous graphs involving commonsense and concept knowledge, and aligns coarse-grained and fine-grained cognition and affection by designing a two-level strategy.\nIn addition to the models designed specifically for generating empathetic responses mentioned above, we also conduct a comparison with ChatGPT.\nChatGPT3: We use the \u201cgpt-3.5-turbo\u201d model from OpenAI ChatGPT API and employ 2-shot prompt setting to generate responses. The prompt template is as follows:\n\"Now generate a response (no more than 30 words) for the following context. There are two examples:\n[Context-Response Example 1]\n[Context-Response Example 2]\n[Context]\nResponse:\""}, {"title": "D. Evaluation Metrics", "content": "We use both automatic and human evaluations to evaluate the performance of EmpRL and baselines. Human evaluation includes human ratings and human A/B test.\nAutomatic Evaluation: For automatic evaluation, we adopt the following metrics:\n\u2022 Perplexity (PPL): It is defined as the exponent of the cross-entropy and can be used to evaluate the overall quality of generated responses. A lower perplexity indicates more fluent and natural responses.\n\u2022 Distinct-1/2 (Dist-1/2) [47]: It is the proportion of the distinct unigrams/bigrams in all generated responses to measure the diversity.\n\u2022 Empathy F1-score (Emp-F1): To evaluate the similarity of the empathy level between generated responses and target responses in the context, we design Emp-F1 evaluation metric. Specifically, the pre-trained empathy identifiers are used to identify empathy levels of emotional reaction, interpretation, and exploration communication mechanisms for each (context,target response) pair, treating them as gold labels. Meanwhile, empathy levels of three empathy communication mechanisms are identified for each (context, generated response) pair and regarded as predicted labels. Then, the weighted average F1-scores for three empathy communication mechanisms are calculated separately. Finally, the average of these three weighted average F1-scores yields Emp-F1, evaluating the similarity of the empathy level.\nHuman Rating: We randomly sample 100 dialogue contexts and their corresponding generated responses from EmpRL and baselines. Then, three human evaluators are asked to rate the responses in terms of three aspects:\n\u2022 Empathy: It measures whether the generated responses express and understanding of the user's situation and feelings and convey appropriate emotions.\n\u2022 Relevance: It measures whether the generated responses are relevant to the context.\n\u2022 Fluency: It measures whether the generated responses are natural and fluent.\nRatings for each aspect range from 1 to 5, with higher scores indicating better performance (i.e., 1: poor, 2: marginal, 3: moderate, 4: good, 5: excellent). The average scores from three evaluators are treated as the final rating for each aspect.\nHuman A/B Test: To compare our framework with other baselines, we also conduct human A/B test. Model A represents EmpRL, while model B represents a baseline model. In the test, three evaluators are invited to choose which model they think performed better between A and B. If evaluators think that both models perform equally well, they can choose \"Tie\". The final result is determined by majority vote. If three evaluators form different results, a fourth evaluator is introduced. The proportion of \u201cWin\u201d, \u201cLose\" and \"Tie\" for model A is then calculated."}, {"title": "V. RESULTS AND ANALYSIS", "content": "The automatic evaluation results of different models on EmpatheticDialogues are shown in Table II. From the table, we conclude that: (1) Models such as CASE, SEEK, and CEM, which take into account both affective and cognitive empathy, outperform models that only consider affective empathy, such as MOEL, MIME, EmpDG, and KEMP. This validates that considering both affective and cognitive empathy can lead to the generation of better responses. (2) In terms of PPL, EmpRL reduces by 22.72 compared to the state-of-the-art baseline CASE, indicating that leveraging pre-trained language models can generate more fluent responses. (3) In terms of Dist-1 and Dist-2, EmpRL achieves results comparable to ChatGPT, demonstrating that the proposed EmpRL is capable of generating diverse responses. (4) In terms of Emp-F1, EmpRL achieves the highest score of 66.40%. This result indicates that responses generated by EmpRL have a higher similarity in empathy level with target responses, further verifying the effectiveness of EmpRL."}, {"title": "B. Human Evaluation Results", "content": "Human Rating Results: Table III presents human rating results of different models in terms of empathy, relevance, and fluency. From the observations, it can be seen that among task-related baselines, CASE achieves the best performance, further demonstrating the effectiveness of exploring affective and cognitive empathy. Additionally, EmpRL significantly outperforms all task-related baseline models in all evaluation aspects, indicating that the proposed framework can comprehensively enhance the quality of generated responses. Furthermore, ChatGPT demonstrates superior performance compared to EmpRL, primarily due to the following reasons: ChatGPT benefits from more diverse training datasets, which allows it to have a broader knowledge base and generate more effective responses; ChatGPT is initialized with GPT-3.5, which is a large language model with powerful text generation capabilities; ChatGPT uses reinforcement learning from human feedback (RLHF), which could help align it with complex human values."}, {"title": "Human A/B Test Results", "content": "The results of human A/B test between EmpRL and baselines are shown in Table IV. Consistent with human rating results, the results of human A/B test also show that EmpRL significantly outperforms all task-related baselines, validating the effectiveness of the designed empathy reward. Moreover, EmpRL achieves \u201cWin\u201d or \u201cTie\" ratio of 60.00% compared to ChatGPT. This result indicates that although the performance of EmpRL is not as good as ChatGPT, it can compete with ChatGPT in most generations, further verifying the superiority of EmpRL.\nOn the other hand, through manually inspecting the generated responses of ChatGPT, a conclusion consistent with Zhao et al. [48] is reached: ChatGPT frequently repeats the pattern of \u201cemotional restatement + information expansion\" when expressing empathy. This repetitive pattern may lead to users feeling monotonous and lacking freshness. Therefore, enhancing the model's personalized empathy capability requires more attention in the future."}, {"title": "C. Ablation Study", "content": "To investigate the effectiveness of different components in our EmpRL, we conduct ablation experiments. We consider the following four settings:\n\u2022 w/o KL Penalty: Remove the KL penalty term from the reward function.\n\u2022 w/o Empathy Reward: Remove the empathy reward from the reward function.\n\u2022 w/o RL: Remove RL and use only the prefix-tuned generator to produce responses.\n\u2022 w/o RL, Prefix-tuning: Remove RL and simultaneously replace prefix-tuning T5 with fine-tuning T5.\nThe ablation results are reported in Table V. From the table, we observe that: (1) Removing the KL penalty term results in a significant decrease in the fluency of generated responses. Hence, it is necessary to integrate the KL penalty into the reward function. (2) Although removing the empathy reward only causes a slight drop of 0.08 in terms of PPL, the diversity and similarity of empathy level metrics decrease significantly. This indicates that the designed empathy reward function could ensure the fluency of generated responses, improve the diversity of generated responses, and make generated responses have a more similar empathy level to target responses. (3) Without RL, it performs similarly to removing the empathy reward in EmpRL\u2014there is a moderate decrease in terms of PPL, but Dist-1, Dist-2, and Emp-F1 exhibit more significant drops. (4) If prefix-tuning is replaced with direct fine-tuning, the performance further declines, indicating that prefix-tuning could maintain model performance while reducing parameters. In summary, the ablation results demonstrate that using the prefix-tuned T5 model for response generation, introducing RL for further training the generator, designing an empathy reward, and integrating the KL penalty term in the reward function are all beneficial for enhancing the quality of generated responses."}, {"title": "D. Case Study", "content": "Table VI shows some responses generated by EmpRL and baselines. In Case 1, the baseline models, including MoEL, MIME, EmpDG, KEMP, and SEEK, exhibit a limited understanding of the speaker's utterance, leading to generated responses that are inconsistent with the context. Additionally, CASE generates a generic response that fails to sustain the conversation. In contrast, ChatGPT and EmpRL are able to understand the speaker's situation and feelings and generate empathetic responses. Importantly, the response generated by EmpRL is not only similar to the target response but also expresses empathy in both affective (\u201cThat's awesome!\u201d) and cognitive (\"Where are you living?\") aspects. Similarly, in Case 2, the response generated by the proposed EmpRL contains empathy in both affective (\u201cOh wow!\u201d) and cognitive (\"I bet you really enjoyed it afterwards.\") aspects.\nIn Case 3, task-related baseline models are also unable to generate high-quality empathetic responses. While ChatGPT performs better, it tends to follow a pattern of \u201cemotional restatement + information expansion\" to express empathy, i.e., \"That sounds like a peaceful and relaxing camping trip.\" + \"Falling asleep to a dying fire is always a comforting feeling.\". In contrast, our proposed EmpRL framework could directly understand the speaker's situation and feelings and generate an empathetic response. In Case 4, similar to Case 3, ChatGPT still adopts the \u201cemotional restatement + information expansion\" pattern to express empathy. However, EmpRL is capable of generating a high-quality empathetic response that includes both affective (\u201cthat is a huge achievement\") and cognitive (\u201cI hope you do,\u201d) aspects.\""}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose an EmpRL framework for empathetic response generation. The framework devises an empathy reward function, and maximizes the expected reward through PPO algorithm to align the empathy level between generated responses and target responses in the context. The empathy reward function, which incorporates emotional reaction, interpretation, and exploration communication mechanisms, is constructed utilizing pre-designed and pre-trained empathy identifiers. This ensures that generated responses have a similar empathy level to target responses. For model evaluation, we devise an Emp-F1 evaluation metric to measure the similarity of empathy level between generated responses and target responses in the context. Automatic and manual evaluations indicate that EmpRL can improve the quality of generated responses and generate more empathetic responses involving affective and cognitive aspects. In future work, we will investigate aligning the empathy level of responses generated by LLMs with human responses, aiming to enhance the empathetic capability of LLMs."}]}