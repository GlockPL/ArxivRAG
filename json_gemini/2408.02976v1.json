{"title": "Empathy Level Alignment via Reinforcement Learning for Empathetic Response Generation", "authors": ["Hui Ma", "Bo Zhang", "Bo Xu", "Jian Wang", "Hongfei Lin", "Xiao Sun"], "abstract": "Empathetic response generation, aiming at understanding the user's situation and feelings and respond empathically, is crucial in building human-like dialogue systems. Previous methods mainly focus on using maximum likelihood estimation as the optimization objective for training response generation models, without taking into account the empathy level alignment between generated responses and target responses. To this end, we propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. Given the powerful text generation capability of pre-trained language models, EmpRL utilizes the pre-trained T5 model as the generator and conducts further training to initialize the policy. To align the empathy level between generated responses and target responses in the context, an empathy reward function containing three empathy communication mechanisms, i.e., emotional reaction, interpretation, and exploration, is constructed using pre-designed and pre-trained empathy identifiers. Finally, the proximal policy optimization algorithm is used to further train the policy to produce empathetic responses. Both automatic and manual evaluations demonstrate that the proposed EmpRL framework can improve the quality of generated responses, enhance the empathy level similarity between generated and target responses, and produce empathetic responses covering both affective and cognitive aspects.", "sections": [{"title": "I. INTRODUCTION", "content": "OPEN-domain dialogue systems, which converse with humans in open domains to provide reasonable responses and entertainment [1], have been extensively explored in recent years. With the development of neural response generation models [2]\u2013[6], more fluent and coherent responses have been generated. Despite the improvements, the absence of empathy still results in a noticeable disparity between these and human-like dialogue systems.\nEmpathy often refers to the capacity to understand other's experiences and feelings, including aspects of affection and cognition [7], [8]. The affective aspect involves the emotional simulation in reaction to the user's experiences, while the cognitive aspect focuses on understanding the user's situation. Empathy is a fundamental characteristic of human conversations, and exploring ways to generate empathetic responses is crucial in the development of human-like dialogue systems [9]. It can enhance user experience and satisfaction, enabling deeper human-machine interaction, while providing support for scenarios such as psychological counseling and emotional companionship. Fig. 1 presents examples of empathetic and non-empathetic dialogue responses.\nRashkin et al. [10] released a large-scale empathetic dialogue dataset EmpatheticDialogues, paving the way for research in empathetic response generation. Existing methods mainly divided into two categories: one focuses solely on affective empathy, detecting and utilizing the user's emotion to generate responses, such as MoEL [11], MIME [12], and KEMP [13]; the other considers both affective and cognitive aspects of empathy, such as CoMAE [14], CEM [15], and CASE [16]. The two categories are primarily constructed using supervised learning, with maximum likelihood estimation as the optimization objective for training models to generate responses. These approaches overlook the empathy level alignment between generated responses and target responses.\nThe concept of empathy level is proposed by Sharma et al. [17]. They pointed out that empathy expression is manifested through three communication mechanisms: emotional reaction, interpretation, and exploration, where emotional reaction represents the affective empathy, interpretation and exploration mechanisms show the cognitive empathy. They further distinguished each mechanism into no, weak, or strong communication to reflect the empathy level of responses in the context. Aligning the empathy level between generated responses and target responses enables models to align with the way of human empathy expression, which helps to improve the quality of generated empathetic responses.\nTo achieve this goal, we introduce reinforcement learning (RL) into empathetic response generation and propose an empathetic response generation using reinforcement learning (EmpRL) framework. The framework designs an effective em-"}, {"title": "II. RELATED WORK", "content": "In recent years, empathetic response generation has attracted much attention. Unlike emotional response generation which focuses on generating responses with specific emotions [18]\u2013[21], empathetic response generation aims to respond in an empathetic manner. Rashkin et al. [10] indicated that detecting the user's emotion facilitates the generation of empathetic responses. MoEL [11] uses different Transformer decoders to compute response representations for each possible emotion and softly combines these representations to generate the response. Majumder et al. [12] assumed that empathetic responses often mimic the user's emotions to some degree, and introduced MIME to generate responses. Li et al. [22] utilized user feedback and proposed the EmpDG framework that includes the empathetic generator and interactive discriminators. KEMP [13] introduces commonsense knowledge and emotional lexical knowledge to understand and express emotions. These studies primarily rely on detecting the context emotion and utilizing the emotion to generate responses.\nEmpathy is a multi-dimensional construct that consists of affective and cognitive aspects. Concentrating on affective empathy solely is insufficient; it is essential to consider both affective and cognitive aspects. Based on GPT-2 [23], COMAE [14] hierarchically models three factors of empathy expression: communication mechanism, dialogue act, and emotion. CEM [15] leverages external commonsense knowledge to gather more information about the user's situation and feelings, and uses the information to generate empathetic responses. CASE [16] contains two heterogeneous graphs involving commonsense and concept knowledge, and designs a two-level strategy to align cognition and affection.\nCurrently, due to the remarkable capabilities demonstrated by large language models (LLMs) in natural language generation tasks, some works have embarked on exploring empathetic response generation using LLMs. Loh et al. [24] designed a simple instructional prompt to explore the capability of five LLMs in generating empathetic responses. Qian et al. [25] also conducted empirical investigation on the performance of LLMs such as ChatGPT in generating empathetic responses and proposed three targeted improvement methods. Chen et al. [26] constructed a multi-turn empathetic conversation dataset and finetuned ChatGLM to generate empathetic responses.\nHowever, existing methods ignores the empathy level alignment between generated responses and target responses. To ensure that generated responses have a similar empathy level as target responses in the context, we design an effective empathy reward function and produce empathetic responses by maximizing the expected reward through RL."}, {"title": "B. Reinforcement Learning for Sequence Generation", "content": "RL [27] trains the agent using rewards obtained through interactions with the environment, mainly for solving sequential decision-making problems. With the development of deep RL, sequence generation based on RL has received widespread attention. To alleviate the exposure bias problem of generative models, Ranzato et al. [28] proposed the mixed incremental cross-entropy reinforce (MIXER), which employs RL to optimize evaluation metrics such as BLEU and ROUGE for sentence generation tasks. Ziegler et al. [29] asked human annotators to provide feedback on various answers generated by the language model to train a reward model, and then utilized RL to optimize the language model, which was used for tasks such as summarization. Le et al. [30] introduced CodeRL for program synthesis tasks, leveraging RL to enhance the code generation capability of pre-trained language models.\nFor dialogue generation, Li et al. [31] devised three reward functions to evaluate the informativity, coherence, and ease of answering of generated responses, and employed the policy gradient method to enhance the sequence-to-sequence model. Saleh et al. [32] employed hierarchical RL to model utterance-level rewards, improving the flexibility of dialogue models"}, {"title": "III. METHODOLOGY", "content": "The task of empathetic response generation requires designing a dialogue model that plays the role of a listener and generates empathetic responses. Formally, let C = [u_1, u_2, \u2026, u_{N-1}] denote a dialogue context of N \u2212 1 utterances, where the i-th utterance u_i = [w_{i1}, w_{i2}, \u2026, w_{iM_i}] consists of M_i words. All odd-numbered utterances (u_1, u_3, \u2026\u2026\u2026, u_{N-1}) and even-numbered utterances (u_2, u_4,\u2026\u2026, u_{N-2}) belong to the speaker and the listener, respectively. The task aims to generate the next response y = [y_1, y_2, \u2026, y_T], which is fluent and coherent to the context, and empathetic to the speaker's situation and feelings."}, {"title": "B. Overview", "content": "The architecture of our proposed EmpRL is shown in Fig. 2. EmpRL first uses the pre-trained T5 model as a generator and further trains it to initialize the policy. Then, an empathy reward function is devised, incorporating three empathy communication mechanisms: emotional reaction, interpretation, and exploration. This is used to align the empathy level between generated responses and target responses in the context. Additionally, a KL penalty term is integrated into the empathy reward to prevent the policy from deviating excessively from the trained generator, which could lead to incoherent responses. Finally, PPO is employed to further train the policy to produce empathetic responses.\nIn the following sections, we first introduce the generator used for policy initialization and the empathy identifier for calculating empathy reward. Subsequently, we provide a detailed description of the various components of EmpRL, including state, action, policy, reward function, advantage function, objective, and optimization method."}, {"title": "C. Generator", "content": "Due to the powerful text generation capability of pre-trained language models such as GPT-3 [35], T5 [36], and BART [37], we utilize the pre-trained T5 model to generate responses. Fine-tuning pre-trained language models directly in downstream tasks is a common approach that achieves excellent performance across various NLP tasks. However, it involves updating all model parameters. To reduce the number of parameters that require updating, we employ prefix-tuning [38] instead of fine-tuning the pre-trained T5 model. Prefix-tuning involves prepending a sequence of continuous task-specific vectors (called prefix) to the input, and then freezing language model parameters while optimizing the prefix.\nThe generator after prefix-tuning (called Prefix-tuned T5) is described as follows:\np(\u0177|c) = \u220f_{j=1}^M P(y_j|y_{<j}, c).\nwhere c represents the context and \u0177 is the generated response. We generate responses using T5 model, thus all context utterances are concatenated to form the input context c."}, {"title": "D. Empathy Identifier", "content": "To establish an effective empathy reward in EmpRL, we devise an empathy identifier that recognizes the empathy level of the response in the context. The structure of the empathy identifier is illustrated in Fig. 3. The identifier first employs two independently pre-trained T5 encoders [36] to encode the context and the response, respectively. Then, the encoded"}, {"title": "E. EmpRL Architecture", "content": "The proposed EmpRL framework treats empathetic response generation as a sequential decision-making problem and utilizes RL to maximize the expected reward for generating responses. Below, we provide a detailed introduction to various components included in EmpRL.\nState: The state is the dialogue context along with the generated response. State at time t, denoted as st = (\u0177_{<t}, c), includes the dialogue context c and generated tokens before t.\nAction: The action is the generated response. The action at time t is denoted as at = \u0177_t, which corresponds to the token generated at time t.\nPolicy: The policy network \u03c0_\u03b8 generates empathetic responses based on the context, where \u03b8 represents the learnable parameters. At time t, the policy is \u03c0_\u03b8(a_t|s_t) = \u03c0_\u03b8(\u0177_t|\u0177_{<t}, c), which predicts the current token considering the dialogue context and previously generated tokens. The EmpRL framework initializes the policy \u03c0_\u03b8 using the trained generator p.\nReward Function: As shown in Fig. 2, the reward function consists of two components: empathy reward and KL penalty. The empathy reward aligns the empathy level between generated responses and target responses, while KL penalty prevents the policy from deviating far from the pre-trained generator.\n(1) Empathy Reward: The emotional reaction demonstrates affective empathy, while interpretation and exploration demonstrate cognitive empathy. To make generated responses have a similar empathy level as target responses in the context, we devise an empathy reward function that integrates the three empathy communication mechanisms:\nR_{emp}(\u0177, y, c) = exp(-L_{emp}(\u0177, y, c)),\nL_{emp}(\u0177, y, c) = \u2211_i CrossEntropy (\u00ce_i, l_i),\n\u00ce_i = EmpathyIdentifier_i (c, \u0177),\nl_i = EmpathyIdentifier_i (c, y).\nwhere y, \u0177, and c represent the target response, generated response, and context, respectively; l_i and \u00ce_i are the empathy levels of the target response and generated response in the context, respectively, produced by the pre-trained empathy identifier EmpathyIdentifier_i; i represents a communication mechanism, belonging to emotion reaction, interpretation, or exploration.\nEmpRL treats l_i and \u00ce_i as the gold and predicted label, respectively. The cross-entropy loss between them is computed as the loss for communication mechanism i. By summing up the losses for three different communication mechanisms, we obtain the empathy level loss L_{emp}(\u0177, y, c). Finally, we use Eq. (2) to product the empathy reward R_{emp}(\u0177, y, c).\n(2) KL Penalty: To prevent the policy from deviating excessively from the trained generator, which could result in incoherent responses, a KL-divergence penalty term is integrated into the empathy reward. KL penalty at time t is:\nR_{kl}(\u0177_{<t}, c) = log \\frac{\u03c0(\u00b7 | \u0177_{<t}, c)}{p(\u00b7 | \u0177_{<t}, c)}.\nThe final reward vector R (\u0177, y, c) \u2208 R^T is defined as:\nR (\u0177, y, c) = {r_t : t = 1,\u2026\u2026\u2026 ,T},\nr_t = \\begin{cases} -\u03b2R_{kl} (\u0177_{<t}, c), & t = 1,\u2026\u2026 ,T \u2013 1, \\\\ R_{emp} (\u0177, y, c) \u2013 \u03b2R_{kl} (\u0177_{<t}, c), & t = T. \\end{cases}\nwhere r_t represents the reward at time t, T is the length of the generated response, and \u03b2 is the KL penalty coefficient. Since the empathy reward R_{emp}(\u0177, y, c) can only be obtained after the entire response is generated, the rewards for all time steps prior to T exclude the empathy reward."}, {"title": "Advantage Function", "content": "According to generalized advantage estimator [41], the advantage function for the state-action pair (st, at) at time t is defined as:\n\u00c2_t = \u03b4_t + (\u03b3\u03bb) \u03b4_{t+1} + ... + (\u03b3\u03bb)^{T-t+1} \u03b4_{T-1},\n\u03b4_t = r_t + V_\u03c0 (\u0177_{<t+1}, c) \u2013 V_\u03c0 (\u0177_{<t}, c).\nwhere rt is the reward at time t and V\u03c0 (st) is the value function for state st; \u03b3, \u03bb \u2208 [0, 1] represent the discount factor and adjustment factor, respectively.\nThe policy and value function share a common network architecture in EmpRL. Thus, an additional trainable token-level value head (V-Head), which is a linear layer, is directly added on top of the generator's hidden state to compute the value function.\nObjective: The objective of EmpRL is to find a policy that maximizes the expected reward of generated responses:\nmax_{\u03b8} E_{\u0177~\u03c0_\u03b8} [R (\u0177, y, c)] .\nTo reduce the variability of predictions, the maximization of the expected reward is transformed into the maximization of the expected advantage:\nmax_\u03b8 E_{\u0177~\u03c0_\u03b8} [\u2211_{t=1}^T \u00c2 ((\u0177_{<t}, c), \u0177_t)]."}, {"title": "Optimization Method", "content": "PPO [42] is an efficient and stable policy gradient algorithm widely utilized in RL-based text generation. Therefore, we employ PPO to train the policy \u03c0_\u03b8. The definition of the total loss function L\u03b8 is:\nL_\u03b8 = \u2212L_{CLIP} + \u03b1L_{VF},\nL_{CLIP} = E_{\u03c0_\u03b8} [\u2211_{t=1}^T min(c(\u03b8), clip(c_\u03b8(\u03b8), 1-\u03b5, 1+\u03b5) \u00c2_t)],\nL_{VF} = E_{\u03c0_\u03b8} [\u2211_{t=1}^T (V_\u03c0 (\u0177_{<t}, c) - (\u00c2_t + V_\u03c0^{old} (\u0177_{<t}, c)))^2],\nwhere L_{CLIP} is the clipped surrogate policy objective function and L_{VF} is the value function squared error term; \u03b1 and \u03b5 are hyperparameters, representing weights for the linear combination of losses and the proximal policy ratio clip range, respectively; c(\u03b8) = \\frac{\u03c0_\u03b8(a_t|s_t)}{\u03c0_\u03b8^{old}(a_t|s_t)} is the ratio of the new policy to the old policy.\nMinimizing total loss function involves maximizing the clipped surrogate policy objective while simultaneously minimizing the value error. Maximizing the clipped surrogate policy objective essentially aims to maximize the expected reward objective, where the surrogate policy objective function calculates the ratio between the current and the old policy and then constrains the update step within a suitable range. This prevents large variations in the policy gradient algorithm during each update, leading to a more stable policy optimization. Additionally, minimizing the token-level value estimation, which is defined based on the difference between values of the new policy and the estimated dense returns of the old policy, further contributes to improving the stability of the algorithm."}, {"title": "IV. EXPERIMENTAL SETTINGS", "content": "We conduct experiments on the EmpatheticDialogues dataset [10]. EmpatheticDialogues is a large-scale multi-turn dialogue dataset containing 24, 850 open-domain empathetic conversations between speakers and listeners. Fig. 4 illustrates an example from the EmpatheticDialogues dataset. Each conversation is grounded in a situation, which is a description written by the speaker according to a pre-given emotion label (the dataset provides 32 uniformly distributed emotion labels). In a conversation, the speaker talks about the situation while the listener responds empathetically. The EmpRL framework generates responses based solely on context information, without relying on emotion labels or situation descriptions."}, {"title": "B. Implementation Details", "content": "We implement the proposed EmpRL using PyTorch\u00b9 framework and Transformers\u00b2 library. The implementation comprises two stages: pre-training the generator and fine-tuning using RL.\nGenerator Pre-training: We further train the pre-trained T5-base model on the EmpatheticDialogues dataset using prefix-tuning. During training, we utilize AdamW [43] optimizer, with an initial learning rate of 1e - 4 and batch size of 8. For the input context and output response, prefix lengths are set to 40 and 10, respectively. Early stopping is applied when training. During inference, we set the maximum decoding step as 30 and use topk-topp [44] sampling strategy, where k = 20, p = 1.0, and temperature T = 0.9.\nRL Fine-tuning: For fine-tuning using RL, we also employ the AdamW optimizer during training. The number of steps for fine-tuning the generator with PPO is set to 24,000, with learning rate of 1.41e \u2013 5, batch size of 8, and 4 PPO epoch per batch with one minibatch each. The values for KL penalty coefficient \u03b2, discount factor \u03b3, adjustment factor \u03bb, linear combination weight between losses \u03b1, and proximal policy ratio clipping range \u03b5 are set to 0.2, 1.0, 0.95, 0.1, and 0.2, respectively."}, {"title": "C. Baselines", "content": "We compare EmpRL with the following baseline models.\nMoEL [11]: A Transformer-based model that utilizes different Transformer decoder to compute response representation"}, {"title": "D. Evaluation Metrics", "content": "We use both automatic and human evaluations to evaluate the performance of EmpRL and baselines. Human evaluation includes human ratings and human A/B test.\nAutomatic Evaluation: For automatic evaluation, we adopt the following metrics:\nPerplexity (PPL): It is defined as the exponent of the cross-entropy and can be used to evaluate the overall quality of generated responses. A lower perplexity indicates more fluent and natural responses.\nDistinct-1/2 (Dist-1/2) [47]: It is the proportion of the distinct unigrams/bigrams in all generated responses to measure the diversity.\nEmpathy F1-score (Emp-F1): To evaluate the similarity of the empathy level between generated responses and target responses in the context, we design Emp-F1 evaluation metric. Specifically, the pre-trained empathy identifiers are used to identify empathy levels of emotional reaction, interpretation, and exploration communication mechanisms for each (context,target response) pair, treating them as gold labels. Meanwhile, empathy levels of three empathy communication mechanisms are identified for each (context, generated response) pair and regarded as predicted labels. Then, the weighted average F1-scores for three empathy communication mechanisms are calculated separately. Finally, the average of these three weighted average F1-scores yields Emp-F1, evaluating the similarity of the empathy level.\nHuman Rating: We randomly sample 100 dialogue contexts and their corresponding generated responses from EmpRL and baselines. Then, three human evaluators are asked to rate the responses in terms of three aspects:\nEmpathy: It measures whether the generated responses express and understanding of the user's situation and feelings and convey appropriate emotions.\nRelevance: It measures whether the generated responses are relevant to the context.\nFluency: It measures whether the generated responses are natural and fluent.\nRatings for each aspect range from 1 to 5, with higher scores indicating better performance (i.e., 1: poor, 2: marginal, 3: moderate, 4: good, 5: excellent). The average scores from three evaluators are treated as the final rating for each aspect.\nHuman A/B Test: To compare our framework with other baselines, we also conduct human A/B test. Model A represents EmpRL, while model B represents a baseline model. In the test, three evaluators are invited to choose which model they think performed better between A and B. If evaluators think that both models perform equally well, they can choose \"Tie\". The final result is determined by majority vote. If three evaluators form different results, a fourth evaluator is introduced. The proportion of \u201cWin\u201d, \u201cLose\" and \"Tie\" for model A is then calculated.\""}, {"title": "V. RESULTS AND ANALYSIS", "content": "The automatic evaluation results of different models on EmpatheticDialogues are shown in Table II. From the table, we conclude that: (1) Models such as CASE, SEEK, and CEM, which take into account both affective and cognitive empathy, outperform models that only consider affective empathy, such as MOEL, MIME, EmpDG, and KEMP. This validates that considering both affective and cognitive empathy can lead to the generation of better responses. (2) In terms of PPL, EmpRL reduces by 22.72 compared to the state-of-the-art baseline"}, {"title": "B. Human Evaluation Results", "content": "Human Rating Results: Table III presents human rating results of different models in terms of empathy, relevance, and fluency. From the observations, it can be seen that among task-related baselines, CASE achieves the best performance, further demonstrating the effectiveness of exploring affective and cognitive empathy. Additionally, EmpRL significantly outperforms all task-related baseline models in all evaluation aspects, indicating that the proposed framework can comprehensively enhance the quality of generated responses. Furthermore, ChatGPT demonstrates superior performance compared to EmpRL, primarily due to the following reasons: ChatGPT benefits from more diverse training datasets, which allows it to have a broader knowledge base and generate more effective responses; ChatGPT is initialized with GPT-3.5, which is a large language model with powerful text generation capabilities; ChatGPT uses reinforcement learning from human feedback (RLHF), which could help align it with complex human values."}, {"title": "C. Ablation Study", "content": "To investigate the effectiveness of different components in our EmpRL, we conduct ablation experiments. We consider the following four settings:\nw/o KL Penalty: Remove the KL penalty term from the reward function.\nw/o Empathy Reward: Remove the empathy reward from the reward function.\nw/o RL: Remove RL and use only the prefix-tuned generator to produce responses.\nw/o RL, Prefix-tuning: Remove RL and simultaneously replace prefix-tuning T5 with fine-tuning T5.\nThe ablation results are reported in Table V. From the table,"}, {"title": "D. Case Study", "content": "Table VI shows some responses generated by EmpRL and baselines. In Case 1, the baseline models, including MoEL, MIME, EmpDG, KEMP, and SEEK, exhibit a limited understanding of the speaker's utterance, leading to generated responses that are inconsistent with the context. Additionally, CASE generates a generic response that fails to sustain the conversation. In contrast, ChatGPT and EmpRL are able to understand the speaker's situation and feelings and generate empathetic responses. Importantly, the response generated by EmpRL is not only similar to the target response but also expresses empathy in both affective (\u201cThat's awesome!\u201d) and cognitive (\u201cWhere are you living?\u201d) aspects. Similarly, in Case 2, the response generated by the proposed EmpRL contains empathy in both affective (\u201cOh wow!\u201d) and cognitive (\"I bet you really enjoyed it afterwards.\") aspects.\nIn Case 3, task-related baseline models are also unable to generate high-quality empathetic responses. While ChatGPT performs better, it tends to follow a pattern of \u201cemotional restatement + information expansion\u201d to express empathy, i.e., \u201cThat sounds like a peaceful and relaxing camping trip.\u201d + \u201cFalling asleep to a dying fire is always a comforting feeling.\u201d. In contrast, our proposed EmpRL framework could directly understand the speaker's situation and feelings and generate an empathetic response. In Case 4, similar to Case 3, ChatGPT still adopts the \u201cemotional restatement + information expansion\u201d pattern to express empathy. However, EmpRL is capable of generating a high-quality empathetic response that includes both affective (\u201cthat is a huge achievement\u201d) and cognitive (", "do,": "aspects."}, {"title": "VI. CONCLUSION", "content": "In this paper, we propose an EmpRL framework for empathetic response generation. The framework devises an empathy reward function, and maximizes the expected reward through PPO algorithm to align the empathy level between generated responses and target responses in the context. The empathy reward function, which incorporates emotional reaction, interpretation, and exploration communication mechanisms, is constructed utilizing pre-designed and pre-trained empathy identifiers. This ensures that generated responses have a similar empathy level to target responses. For model evaluation, we devise an Emp-F1 evaluation metric to measure the similarity of empathy level between generated responses and target responses in the context. Automatic and manual evaluations indicate that EmpRL can improve the quality of generated responses and generate more empathetic responses involving affective and cognitive aspects. In future work, we will investigate aligning the empathy level of responses generated by LLMs with human responses, aiming to enhance the empathetic capability of LLMs."}]}