{"title": "A survey of textual cyber abuse detection using cutting-edge language models and large language models", "authors": ["J. Angel Diaz-Garcia", "Joao Paulo Carvalho"], "abstract": "The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities. This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and sexting. In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks. We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact. Additionally, we examine the dual role of advanced language models-highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content. This paper aims to contribute to the ongoing discourse on online safety and ethics, offering insights into the evolving landscape of cyberabuse and the technological innovations that both mitigate and exacerbate it.", "sections": [{"title": "1. Introduction", "content": "In the digital age, social networks and online platforms have become integral to everyday life, facilitating communication, information sharing, and social interaction. However, this digital connectivity has also given rise to various forms of cyber abuse, affecting millions of individuals globally. The pervasive nature of cyber abuse, including hate speech, cyberbullying, emotional abuse, doxxing, trolling, impersonation, and shaming, poses significant risks to mental health and well-being causing psychosocial problems (Kwan et al., 2020) as anxiety or depression (Fisher et al., 2016). Traditional methods of addressing cyberabuse have often relied on manual reporting systems and heuristic-based detection (Vandebosch & Van Cleemput, 2009). However, recent advancements in artificial intelligence (AI), particularly through LLMs, offer new avenues for understanding and combating these issues. LLMs, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2018), and other state-of-the-art models (Raffel et al., 2020; Liu et al., 2019), have demonstrated remarkable capabilities in natural language processing, providing powerful tools for analyzing and detecting harmful content. This paper aims to shed light on the diverse forms of cyberabuse through the lens of these cutting-edge AI techniques. By leveraging LLMs and advanced data analysis methods, we seek to explore how these technologies can enhance the detection, classification, and mitigation of various types of cyberabuse. The importance of this research is underscored by the profound impact cyberabuse has on individuals and communities, even leading to self-harm and suicidal thoughts (Hamm et al., 2015; Kim et al., 2019; Kowalski et al., 2014). Despite the progress made in addressing more prominent issues like hate speech and cyberbullying, there remains a significant gap in understanding and tackling other forms of abuse. We provide a comprehensive overview of how these models can be employed to address both well-researched and less-explored types of cyberabuse. By doing so, we highlight the potential of AI to offer more nuanced and effective solutions to combat cyberabuse, ultimately contributing to a safer and more supportive online environment. In recent years, several surveys have explored hate speech detection (Fortuna & Nunes, 2018); however, there is a noticeable gap in covering the latest advancements, particularly the emergence of cutting-edge models like LLMs that have been released in the past year. Therefore, our primary goal is to update the existing literature with these recent developments. Additionally, since most studies in this domain approach the problem as a classification task, we aim to conduct an in-depth analysis of the evaluation processes, especially considering the challenges posed by the unbalanced nature of cyber abuse detection. Moreover, existing reviews predominantly concentrate on cyberbullying (Salawu et al., 2017) or hate speech (Paz et al., 2020; Schmidt & Wiegand, 2017), leaving other forms of cyber abuse significantly underrepresented. With this in mind, the main contributions of our paper are as follows:\u2022\tWe present the most up-to-date analysis of textual cyber abuse, with a particular emphasis on cutting-edge techniques that leverage LMs and LLMs, identifying current gaps and suggesting avenues for future research and improvement.\u2022\tTo the best of our knowledge, we conduct the first comprehensive survey that addresses a broader spectrum of cyber abuse forms, including doxing, trolling, and impersonation, which have been largely overlooked in previous reviews.\u2022\tWe perform an in-depth analysis of the evaluation metrics used in cyber abuse detection, paying special attention to the challenges posed by class imbalance, a common issue in this domain.To address these questions, the paper is organized as follows: Section 2 examines the various forms of textual cyber abuse encountered in online social interactions. Section 4 analyzes the impact of LMs and LLMs in two main aspects: their role in detecting cyber abuse and hate speech, as well as their potential in generating automated instances of such content. Section 5 delves into how artificial intelligence is advancing the detection and analysis of other, less-explored forms of cyber abuse. Finally, we present a comprehensive discussion in Section 6 and draw our conclusions in Section 7."}, {"title": "2. Forms of Textual Cyberabuse", "content": "In this section, we explore the various forms of textual cyberabuse prevalent in social networks. Each subsection delves into the definition, characteristics, and impact of a specific type of abuse. In (van Huijstee et al., 2022), the authors identify a range of harmful and immoral uses of social media, categorizing these into six primary types, which can be further analyzed into 22 distinct tasks. It is important to note that the advent of generative AI has accelerated certain forms of cyberabuse, such as the creation of fake sexual content, the generation of misleading images, and the widespread dissemination of both authentic and fabricated content across social networks. However, these forms of abuse fall outside the scope of this paper, as we focus specifically on textual content. Therefore, our analysis is centered on various types of text-based cyberabuse. To align with existing research in the field, we have selected the following subgenres of cyberabuse in social media for our analysis: hate speech, cyberbullying, doxing, shaming, cancel culture, and trolling as the primary components. Additionally, we include other forms of cyberabuse such as impersonation (Gharawi et al., 2021), and emotional abuse (Stephenson et al., 2018). In the following subsections, we introduce each of these subgenres in detail."}, {"title": "2.1. Hate Speech", "content": "Hate speech involves language that demeans, intimidates, or incites violence against individuals or groups based on attributes such as race, religion, ethnicity, gender, sexual orientation, or other characteristics. This form of expression poses severe threats to individual dignity, democratic values, social stability, and peace. Despite its profound impact, there remains a general lack of awareness regarding the extensive harm that hate speech can inflict across various aspects of life. Hate speech on social media is a pressing social issue. Research indicates that exposure to such speech can lead to a range of mental health problems, particularly among young adults, as well as psychosocial issues (Kwan et al., 2020). Consequences may include anxiety (Fisher et al., 2016), depression (Fisher et al., 2016; Hamm et al., 2015; Wachs et al., 2022), and post-traumatic stress disorder (Cassiani-Miranda et al., 2022; Liu et al., 2020; Wypych & Bilewicz, 2022). Detecting hate speech requires analyzing text for derogatory terms, slurs, and contextual indicators of malicious intent. NLP techniques, and more recently LMs and LLMs, are crucial tools in this effort. These technologies can automatically capture the specific nuances of hate speech in social networks with high accuracy. This survey focuses on recent advancements in hate speech detection, particularly the integration of new language models to enhance detection capabilities."}, {"title": "2.2. Cyberbullying", "content": "Cyberbullying affects both children and adolescents and is widely recognized as intentional, violent, cruel, and repetitive behavior directed at peers. This pervasive issue is identified by the US Centers for Disease Control and Prevention (CDC) as a significant public health threat (cen, 2009). The emotional and psychological damage caused by cyberbullying can be severe, with extreme cases potentially leading to suicide (Bauman et al., 2013). According to (McLoughlin & Burgess, 2009), cyberbullying can be categorized into several types, including flaming, harassment, denigration, impersonation, outing, boycott, and cyberstalking. Particularly in more severe forms such as flaming and harassment, cyberbullying often manifests through texts, comments, or messages exchanged between the bully and the victim. These communications typically involve insults and attacks related to race, sexuality, ethnicity, or social status. Such messages represent user-generated content in an unstructured format, which can be analyzed automatically using AI systems based on NLP and LLMs to identify and address hate speech. In the current digital era, cyberbullying has evolved to encompass a wide range of multimodal categories, including the creation of sexual deep fakes targeting individuals (Rini & Cohen, 2022; Rousay, 2023; Dunn, 2024). While this is a significant area of concern, our focus remains on the textual aspects of cyberbullying, specifically analyzing comments and interactions on social networks and online platforms."}, {"title": "2.3. Emotional and Psychological Abuse", "content": "This form of abuse involves manipulative or controlling language aimed at undermining an individual's emotional well-being. Examples include gaslighting, persistent criticism, and verbal assaults. While emotional and psychological abuse often overlaps with cyberbullying, especially since all forms of cyberbullying inherently involve emotional harm, it is important to distinguish between the two. Cyberbullying is more commonly associated with early life stages, such as during school or adolescence, whereas psychological abuse can occur at any stage of life, including in relationships with partners in adulthood (Moral Jim\u00e9nez et al., 2022). So in our survey, we aim to establish a more fine-grained distinction by categorizing and summarizing various forms of cyberbullying and their intersections with artificial intelligence, with a particular focus on LLMs. Regarding psychological abuse, our inclusion criteria specifically exclude cases of psychological abuse occurring during childhood or other life stages unrelated to online interactions (Iyer et al., 2022), as our focus is solely on cyber abuse, and the real of these kind of applications are more related to medicine area (Luo et al., 2022) or psychology (Ronneberg et al., 2024)."}, {"title": "2.4. Doxing", "content": "Doxing is the act of publicly disclosing private or personally identifiable information about an individual without their consent (Karimi et al., 2022). This can manifest in various forms, such as posts or messages that share sensitive data, including addresses, phone numbers, and other confidential details. Often, doxing is akin to a data leak, and it is typically executed with malicious intent, aiming to harm, intimidate, or harass the victim. The harmful effects of doxing on social networks are significant and include:\u2022\tPrivacy Violations: Doxing infringes on an individual's right to privacy by exposing their personal information to the public, which can lead to unwanted attention and potential harassment.\u2022\tEmotional and Psychological Impact: Victims of doxing may experience severe emotional distress, including anxiety, depression, and a heightened sense of vulnerability due to the public exposure of their private information.\u2022\tProfessional Consequences: Doxing can have detrimental effects on a person's career, damaging professional relationships, leading to job loss, and causing reputational harm.In the literature, it is essential to distinguish between doxing experienced by individuals and that which affects organizations (Herrera Montano et al., 2022). This paper focuses exclusively on the impact of doxing on individuals. Finally, it is necessary to mention that data leakage in the realm of machine learning can be related to the unintentional inclusion of information from the test set in the training process, this topic falls out of our analysis."}, {"title": "2.5. Trolling", "content": "According to Bishop (Bishop, 2013), trolling is defined as \"the activity of posting messages via a communications network that are intended to be provocative, offensive, or menacing.\" This definition highlights that trolling involves posting inflammatory or irrelevant messages with the intent to provoke or disrupt online conversations. In the literature, various approaches categorize trolling not only by the content of the messages but also by the underlying intentions. For example, De et al. (de la Vega & Ng, 2018) identify four key aspects of trolling:\u2022\tIntention (I): The author's underlying purpose or motive behind the trolling activity.\u2022\tIntention Disclosure (D): This aspect evaluates whether the author is concealing their true (malicious) intentions from the audience.\u2022\tIntention Interpretation (R): This reflects how responders perceive and interpret the troll's intentions.\u2022\tResponse Strategy (B): This describes the responder's reaction to the trolling attempt, which may also be a form of trolling.Trolling is a significant issue that requires attention, as highlighted by De et al. (de la Vega & Ng, 2018). Their study found that in 26.9% of trolling incidents, the victims engage with the trolling attempt, often experiencing some form of emotional distress. In our survey, we concentrate on the core aspects of trolling within social network conversations, specifically examining how trolling manifests through interactions in textual dialogues. We exclude from our analysis newer forms of trolling, such as those involving memes, as they represent a different avenue of exploration that falls outside the direct scope of conversational interactions (Hossain et al., 2022; Suryawanshi et al., 2023)."}, {"title": "2.6. Impersonation", "content": "Impersonation involves creating fake profiles or hacking into existing accounts to deceive others. Typically, impersonators target brands or well-known individuals, aiming to extract some form of value through their deception (Zarei et al., 2020). Numerous studies have explored the issue of impersonation, with a particular focus on understanding its impact and vulnerabilities. The findings from Yu et al. (Yu et al., 2023) on the susceptibility of older adults to government impersonation scams highlight alarming trends that call for urgent, targeted interventions. Approximately 16% of participants engaged in conversations with an agent impersonating a government representative without displaying skepticism, indicating a high risk of victimization within this demographic. Even more concerning, 12% of participants willingly shared personal information, and nearly 5% provided the last four digits of their Social Security number. While impersonation has been widely recognized and studied as a growing problem, efforts to detect and prevent these scams remain inadequate and underdeveloped, leaving vulnerable populations at continued risk. Impersonation often relies on the use of altered images, deep fakes, or even biometric information of the scammed individuals (Liu et al., 2024). Also we found a stream regarding the impersonations and infiltration between networks, as a kind of network attack (Shukla, 2023). In this survey, we have excluded these types of impersonation detection methods, as they fall within the domain of image analysis. Our focus is on text-based detection based in textual indicators or features as sudden changes in language style or tone, and inconsistencies in personal information."}, {"title": "2.7. Shaming and Cancel Culture", "content": "Shaming and cancel culture involve public criticism or ostracism of individuals based on their actions or statements, sometimes leading to online harassment. Public Shaming refers to criticizing or condemning individuals or groups in a public forum, often through social media posts, comments, or viral campaigns. This practice aims to invoke feelings of guilt or shame and can escalate quickly, resulting in widespread condemnation from the online community (Basak et al., 2019). Cancel Culture involves withdrawing support from individuals, organizations, or brands after they have made statements or engaged in actions deemed objectionable or offensive. Harmful Implications of Shaming and Cancel Culture:\u2022\tEmotional and psychological distress: Victims may experience significant emotional and psychological impacts, such as anxiety, depression, and a sense of isolation (Muir et al., 2023).\u2022\tEffect on free expression: The fear of public shaming or being \"canceled\" can discourage individuals from expressing their opinions or participating in discussions on important issues (Koivukari & Korpisaari, 2021).\u2022\tNormalization of aggressive behavior: These practices can foster a hostile online environment, where aggressive behavior becomes normalized and further encouraged (Beres et al., 2021).Although there is significant research addressing issues related to social networks such as Twitter, our survey specifically focuses on recent advancements involving language models or neural models published in academic journals. Consequently, some notable studies, such as those by Basak et al. (Basak et al., 2019), Surani et al. (Surani & Mangrulkar, 2021), and Nalawade et al. (Nalawade & Kulkarni, 2021), are not included in our survey."}, {"title": "3. Methodology", "content": "In this paper, we applied the PRISMA methodology (Moher et al., 2010) to conduct a thorough and systematic review aligned with our research objectives. The PRISMA framework comprises four key stages: formulating research questions, developing a search strategy, defining eligibility criteria, and selecting relevant studies."}, {"title": "3.1. Research questions", "content": "\u2022\tRQ1: How representative are LMs and LLMS in detecting textual cyber abuse compared to traditional methods?\u2022\tRQ2:Considering that cyber abuse frequently involves imbalanced classification challenges, are researchers adequately addressing this issue in their evaluation metrics and pre-processing methods?\u2022\tRQ3: Based on the reviewed literature, what emerging trends and future directions are shaping the development of cyber abuse detection?\u2022\tRQ4: Which types of cyber abuse detection are most significantly influenced by LLMs?\u2022\tRQ5: Are there specific forms of cyber abuse that remain underexplored and lack adequate detection solutions?"}, {"title": "3.2. Search strategy", "content": "The vast majority of research in the field of textual cyber abuse detection focuses on hate speech and cyberbullying. To capture the most recent advancements, our search strategy is twofold. For hate speech, trolling and cyberbullying, we have narrowed the results to papers that address these issues using cutting-edge techniques based on LLMs. In contrast, for other forms of abuse-where research is less prominent-we have broadened our queries to include additional analysis-related terms. To ensure robust and comprehensive results, we focused our analysis on the Web of Science (WOS) database. This database exclusively indexes peer-reviewed papers and encompasses other databases, such as IEEE Xplore and SpringerLink, allowing us to cover multiple indexed sources while benefiting from WOS'S advanced search capabilities. For each topic, we used the following research queries.\u2022\tHate Speech:TI=(\"hate speech\" OR \"hate content\" OR \"hate messages\") AND AB=(\"transformers\" OR \"GPT\" OR \"BERT\" OR \"T5\" OR \"LLMs\") AND TS=(\"detection\" OR \"classification\" OR \"analysis\")\u2022\tCyberbullying:TI=(\"cyberbullying\" OR \"online harassment\" OR \"cyber harassment\") AND AB=(\"transformers\" OR \"GPT\" OR \"BERT\" OR \"T5\" OR \"LLMs\") AND TS=(\"detection\" OR \"classification\" OR \"prediction\" OR \"analysis\")\u2022\tEmotional and Psychological Abuse:(TI=(\"emotional abuse\" OR \"psychological abuse\" OR \"verbal abuse\" OR \"emotional manipulation\" OR \"mental abuse\" OR \"emotional distress\" OR \"psychological distress\" OR \"emotional harm\" OR \"psychological harm\" OR \"emotional violence\" OR \"psychological violence\" OR \"verbal harassment\" )) AND AB=(\"artificial intelligence\" OR \"natural language processing\" OR \"deep learning\" OR transformers OR GPT OR BERT OR LLAMA OR 'neural networks')\u2022\tDoxing:(TI=((\"doxing\" OR \"doxxing\" OR \"doxing attack\" OR \"doxxing attack\" OR \"personal information exposure\" OR \"personal data exposure\" OR \"data leakage\" OR \"identity exposure\" OR \"identity disclosure\" OR \"public exposure\" OR \"private information leak\" OR \"information leak\" OR \"confidential information disclosure\" OR \"online doxing\" OR \"online doxxing\" OR \"Privacy-Disclosure\" OR \"PrivacyLeak\" OR \"private information\")) OR AB=(doxing)) AND TS=(\"artificial intelligence\" OR \"machine learning\" OR \"natural language processing\" OR \"data analysis\" OR \"deep learning\" OR transformers OR GPT OR BERT OR LLAMA OR 'neural networks') AND TS=(\"detection\" OR \"prevention\" OR \"analysis\")\u2022\tTrolling:(TI=(trolling OR \"online trolling\" OR \"internet trolling\") ) AND TS=(\"artificial intelligence\" OR \"machine learning\" OR \"natural language processing\" OR \"data analysis\" OR \"deep learning\" OR transformers OR GPT OR BERT OR LLAMA OR 'neural networks')\u2022\tImpersonation:(TI=(\"impersonation\" OR \"identity theft\" OR \"fake profiles\" OR \"catfishing\" OR \"fake social media accounts \") ) AND TS=(\"artificial intelligence\" OR \"machine learning\" OR \"natural language processing\" OR \"data analysis\" OR \"deep learning\" OR transformers OR GPT OR BERT OR LLAMA OR 'neural networks')"}, {"title": "3.3. Eligibility criteria", "content": "Regarding the eligibility criteria, we have focused on journal papers published in the last three years (January 2022- September 2024), as this period aligns with the significant advancements and widespread adoption of LLMs. We selected papers that primarily involve text-based datasets, ensuring relevance to our focus on LLMs. While some papers may also address social network analysis through graph theory or image detection, they were only included if they incorporated a textual analysis component. For instance, a paper primarily centered on graph theory but also analyzing tweet content was considered. Additionally, we excluded survey-based studies focusing on psychological assessments through participant responses, as our interest lies in the direct analysis of textual data rather than self-reported experiences."}, {"title": "4. Impact of Language Models on Hate Speech and cyberbullying detection", "content": "In this section, we conduct an analysis focusing on the adoption of different LMs and LLMs for hate speech and cyberbullying detection. To enhance clarity, the section is divided into two parts: hate speech detection (Section 4.1) and cyberbullying detection (Section 4.2). Acknowledging the rise of generative AI being used for harmful purposes, we have also included a section that explores the generation of abusive content (Section 4.3)."}, {"title": "4.1. Hate Speech detection", "content": "The complexities of hate speech often necessitate the use of a combination of techniques and models to effectively address each specific task. In this context", "attention": "a high-level focus on the CNN outputs and a low-level focus on the BiLSTM outputs. Gupta et al. (Gupta et al.", "components": "a generator and a discriminator. The generator creates synthetic data that closely mimics the real data", "datasets": "one containing comments from Facebook and YouTube", "methods": "span prediction and sequence labeling. Additionally", "classes": "safe, aggressive, misogynistic, and racist, specifically within the context of Spanish football matches. They employed state-of-the-art pre-trained LLMs like BETO, MarIA, and multilingual BERT to develop an ensemble classifier. The model achieved an impressive macro-weighted F1-score of 88.71%, demonstrating its effectiveness in identifying and classifying various forms of abusive content, including aggression, misogyny, and"}]}