{"title": "Order Doesn't Matter, But Reasoning Does:\nTraining LLMs with Order-Centric Augmentation", "authors": ["Qianxi He", "Qianyu He", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "abstract": "Logical reasoning is essential for large lan-\nguage models (LLMs) to ensure accurate and\ncoherent inference. However, LLMs struggle\nwith reasoning order variations and fail to\ngeneralize across logically equivalent transfor-\nmations. LLMs often rely on fixed sequential\npatterns rather than true logical understanding.\nTo address this issue, we introduce an order-\ncentric data augmentation framework based\non commutativity in logical reasoning. We\nfirst randomly shuffle independent premises to\nintroduce condition order augmentation. For\nreasoning steps, we construct a directed acyclic\ngraph (DAG) to model dependencies between\nsteps, which allows us to identify valid\nreorderings of steps while preserving logical\ncorrectness. By leveraging order-centric\naugmentations, models can develop a more\nflexible and generalized reasoning process.\nFinally, we conduct extensive experiments\nacross multiple logical reasoning benchmarks,\ndemonstrating that our method significantly\nenhances LLMs' reasoning performance\nand adaptability to diverse logical structures.\nWe release our codes and augmented data\nin https://anonymous.4open.science/r/\nOrder-Centric-Data-Augmentation-822C/.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have demonstrated\nexceptional performance across various real-world\napplications (Jaech et al., 2024; Dubey et al., 2024;\nLiu et al., 2024a). Logic reasoning (Cummins et al.,\n1991) is essential for LLMs. It allows models to\ndraw valid conclusions, maintain coherence, and\nmake reliable decisions across tasks (Pan et al.,\n2023; Liu et al., 2023).\nHowever, LLMs are sensitive to reasoning order\nand struggle with logically equivalent transforma-\ntions (Chen et al., 2024; Berglund et al., 2023b;\nTarski, 1956). First, the models are highly sensitive\nto the order of premises, with perturbing the order\nleading to up to a 40% performance drop (Chen\net al., 2024; Liu et al., 2024b). Additionally, if the\ntesting order is reversed compared to the training\norder, accuracy drops drastically. For example, in\nthe case of data involving two entities within a sin-\ngle factual statement, accuracy drops from 96.7%\nto 0.1% when training is left-to-right and testing is\nright-to-left. (Berglund et al., 2023b,a; Allen-Zhu\nand Li, 2023). This suggests that LLMs follow\na rigid logical reasoning order driven by learned\npatterns rather than true logical understanding.\nExisting LLM logical data augmentation meth-\nods do not effectively address the sensitivity to\nequivalent transformations. First, many logical\ndatasets are specifically designed for certain do-\nmains, such as specialized fields or exam ques-"}, {"title": "Related Work", "content": "2.1 Order Effect of Language Models\nLarge language models are sensitive to reason-\ning order. While word order variations in natu-\nral language have little impact (Cao et al., 2023;\nAbdou et al., 2022), disrupting the order in rea-\nsoning tasks significantly degrades performance.\nChen et al. (2024) show that models perform op-\ntimally only when the premise order matches the\nsequence required for the reasoning process. To ad-\ndress this, Liu et al. (2024b) propose reorganizing\npremise order to reduce order sensitivity. However,\nthis approach is task-specific and lacks generaliz-\nability. Furthermore, the Reversal Curse reveals\nthat models fail to grasp logical equivalence when\ntrained with a fixed linguistic order (Berglund et al.,\n2023b). Golovneva et al. (2024) mitigate this by\nproposing reverse training, where LLMs learn both\nforward and reverse reasoning by randomly shuf-\nfing words or segments within a sentence. This\nhighlights the need for diverse training data with\nvaried orderings.\nCompared to the above works, we extend to\nmore complex logical reasoning scenarios, build-\ning upon this concept by leveraging commutativity\nfor data augmentation in logical reasoning, which\nhelps models generalize across different reasoning\nstructures and enhances robustness.\n2.2 Logical Reasoning Enhancing\nExisting methods to enhance LLMs' logical reason-\ning ability mainly fall into three categories: inte-\ngrating symbolic reasoning, training and inference\nstrategies, and leveraging data augmentation.\nSymbolic reasoning enhances LLMs by trans-\nforming natural language into formal logic, pro-\nviding a symbolic approach that helps models un-\nderstand logic (Olausson et al., 2023; Xu et al.,\n2024; Zhang et al., 2023). Training and inference"}, {"title": "Problem Formulation", "content": "In this paper, we formulate the problem of log-\nical reasoning in a unified representation. Let\n$D = {P, C', L}$ represent a logical reasoning prob-\nlem, where $P = {P_1, P_2, ..., P_n}$ is the set of\npremises, $C$ is the conclusion, and $L$ is the la-\nbel, which takes a value from a finite set, such\nas {true, false, uncertain}, indicating whether $C$\ncan be logically inferred from $P$. In step-based\ndata augmentation, we extend the representation to\ninclude a solution $S = {S_1, S_2, ..., S_m}$, where\n$S$ consists of reasoning steps that derive the con-\nclusion from the premises. This process can be\nabstracted as a directed acyclic graph (DAG). Typi-\ncal logical reasoning datasets only provide labels.\nTherefore, we construct $S$ ourselves. The specific\nconstruction of $S$ will be detailed in Sec. 4.2."}, {"title": "Method", "content": "In this section, we introduce condition order aug-\nmentation in Sec. 4.1 and answer order augmenta-\ntion in Sec. 4.2. The framework is shown in Fig.\n2.\n4.1 Condition Order Augmentation\nDue to the commutativity of premises, swapping\nindependent premises results in the same solution.\nHence, we perturb the order of premises, enabling\nmodels to learn the logical equivalence of condition\nreordering.\n4.1.1 Shuffling the Order of Premises\nGiven a logical reasoning dataset $D_c$ =\n${P, C, L}$, we first extract the premise set $P$ =\n${P_1, P_2, ..., P_n}$. To generate augmented data, we\napply a random permutation $\\sigma$ to the premise set $P$,\nproducing a new ordered premise set $P_{\\sigma_{ran}}$. Specif-\nically:\n$P_{\\sigma_{ran}} = {P_{\\sigma(1)}, P_{\\sigma(2)},..., P_{\\sigma(n)}}$\nFor example, if the\noriginal order is\n$[P_1, P_2, P_3, P_4, . . ., P_n]$,\nafter applying the\npermutation $\\sigma$, the new order might be\n$[P_3, P_4, P_1, P_n, ..., P_2, . . .]$.\n4.2 Answer order Augmentation\nDue to the commutativity of reasoning steps, we\nperturb the order of solution steps to help mod-\nels learn the logical equivalence of the reasoning\nprocess. However, reasoning steps often have de-\npendencies, where the execution of one step may\nrely on the result of another. To address this, we\npropose a method for identifying valid step reorder-\nings that ensures these dependencies are preserved."}, {"title": "Leveraging LLMs for Logical Reasoning Solutions", "content": "Since logical reasoning datasets typically provide\nonly a single label (e.g., true/false) without a Chain-\nof-Thought (CoT) reasoning process, we generate\ndetailed step-by-step reasoning solutions to bridge\nthis gap (Xu et al., 2024). We use LLMs\u00b9 for this\nprocess. As shown in Fig. 3, the methodology\nconsists of three main steps: (1) For datasets with-\nout First-Order Logic (FOL) expressions, We ex-\ntract their premises and conclusion and convert\nthem into the corresponding FOL representations.\n(2) The FOL-augmented premises, along with the\nground truth labels, are fed into the model, prompt-\ning it to generate a step-by-step solution. Each\nstep must clarify its purpose and reasoning, leading\nto a final conclusion. (3) The generated solutions\nare then reprocessed by the model to extract the\npremise indices and prerequisite step indices used\nin each reasoning step.\nConstructing the Step Dependency DAG\nAfter obtaining the logical reasoning solutions,\nthe current data can be represented as $D_s$ =\n${P, C, L, S}$, where $S = {S_1, S_2, ..., S_m}$ con-\nsists of reasoning steps. We represent $S$ as a\ndirected acyclic graph (DAG), denoted as $G$ =\n$(V, E)$, where $V = {S_1, S_2, . . ., S_m}$ is the set of\nreasoning steps, and $E \\subset V \u00d7 V$ is the set of di-\nrected edges. An edge $(S_i, S_j)$ indicates that step\n$S_j$ depends on the result of step $S_i$.\nEach step $S_i$ is represented as a tuple:\n$S_i = (Goal_i, P^{(i)}_{used}, S^{(i)}_{used}, Result_i)$\nwhere $Goal_i$ describes the goal of the step, $P^{(i)}_{used}$\nrepresents the directly used atomic premises,\n$S^{(i)}_{used} \\subset V$ denotes the prerequisite steps that must\nbe executed before $S_i$, and $Result_i$ is the result\nderived from the execution of $S_i$.\nGenerating Augmented Solution\nSequences\nA valid reasoning process must maintain all logical\ndependencies between steps while allowing flexi-\nbility in ordering interchangeable steps. We define\nthe dependency constraints as follows:\n$\\bullet$ A step $S_i$ is independent if $S^{(i)}_{used} = \\emptyset$ (i.e., it\nhas no prerequisite steps)."}, {"title": "Experiments", "content": "We conduct experiments to evaluate the effective-\nness of our method, focusing on overall perfor-\nmance, training efficiency, and generalization capa-\nbility.\nExperiment Setup\nDatasets (1) FOLIO (Han et al., 2022) is a natu-\nral language inference dataset annotated with first-\norder logic (FOL), consisting of 1001 training sam-\nples and 231 test samples. (2) RuleTaker (Clark\net al., 2020) requires models to determine whether\na conclusion is entailed by a set of premises, cover-\ning various reasoning difficulties. Due to its large\nscale, we uniformly sample 1000 training and 1000\ntest instances across different difficulty levels. (3)\nLogicNLI (Tian et al., 2021) is an NLI-style dataset\nthat isolates first-order logic reasoning from com-\nmonsense inference for precise logical evaluation.\nSimilarly, we sample 1000 instances from both its\ntraining and test sets.\nModels We conduct experiments on Llama-\n3-8B-Instruct (AI@Meta, 2024), Llama-2-13B-\nChat (Touvron et al., 2023) and Mistral-7B-Instruct-\nv0.3 (Jiang et al., 2023), evaluating model per-\nformance under five training conditions: (1) Un-\ntrained: The original model without any additional\ntraining. (2) Vanilla SFT: Models fine-tuned only\non the original training set, i.e., $D_c = {P, C, L}$.\n(3) Vanilla SFT + Condition Shuffling: Models\ntrained on both the original dataset and an aug-\nmented version with shuffled condition orders, i.e.,\n$D_c = {P,C,L}$ and $D'_c = {P_{\\sigma_{ran}}, C,L}$. (4)\nSFT with COT: Models fine-tuned with training\ndata that includes Chain-of-Thought (COT) reason-\ning steps, i.e., $D_s = {P, C, L, S}$. (5) SFT with\nCOT + Answer Steps Shuffling: A model trained\nwith COT data and additional augmentations with\nshuffled reasoning steps, i.e., $D_s = {P,C, L, S}$\nand $D'_s = {P, C, L, S_{\\sigma_{ran}}}$.\nAll models are trained using full fine-tuning,\nwith a 1:1 mix of ShareGPT (Chiang et al., 2023) in\neach dataset. Training is conducted on four A100\nGPUs for four epochs. Each model is trained ex-\nclusively on a single dataset, with augmentation\napplied only to that dataset, and evaluated on the\ncorresponding test set without cross-dataset mix-\ning."}, {"title": "Overall Performance", "content": "Tab. 2 shows that our method effectively im-\nproves model reasoning performance. Compared"}, {"title": "Training Efficiency", "content": "To ensure fairness and exclude the effect of in-\ncreased data size, we test the accuracy of check-\npoints with the same number of training steps, com-\nparing the performance of condition order augmen-"}, {"title": "Analysis", "content": "Condition Augmentation with Varying\nShuffling Degrees\nTo investigate the effects of premise order trans-\nformations, we divide the Kendall tau distance $\\tau$\nbetween different premise orders and the original\norder into 10 groups, each spanning a 0.2 range\nwithin [-1,1). A $\\tau$ value of 1 indicates forward\norder, -1 indicates a complete reversal, and 0 rep-\nresents a more uniform shuffling. Additionally,\nrandom shuffle means that $\\tau$ values from the entire\nrange may be included. We conduct experiments\non RuleTaker using different $\\tau$ values for condition-\nbased data augmentation.\n6.2 The Importance of DAG-based Step\nDependency\nTo explore the importance of using DAG for step\ndependencies in step augmentation, we use the An-\nswer Step Shuffled data from Tab. 1 as a baseline.\nWe randomly shuffle the steps in the original COT\nprocess and assess its performance to evaluate the\nimpact of random step reordering without DAG\ndependencies.\nT F I = Number of valid sequences /Factorial of the number of steps.\nThe difference in TFI across datasets aligns with\nthe conclusions we obtained from the Random Step\nShuffled experiment. Specifically, stronger step de-"}, {"title": "Combined Condition and Step Shuffling\nLeads to Performance Degradation", "content": "To investigate the combined effect of condition and\nstep order augmentation, we apply an additional\npremise shuffle to Answer Steps Shuffled data, ad-\njusting premise references in the answers accord-\ningly. As shown in Tab. 4, Condition&Answer\nShuffled results in lower performance compared to\nAnswer Steps Shuffled alone.\nWe believe the key reason is that premise and\nstep shuffling serve different learning purposes.\nPremise shuffling enables the model to recognize\nthat independent conditions with commutativity\ncan lead to the same answer, while step shuffling al-\nlows it to understand that different reasoning paths\nunder the same condition can yield consistent con-\nclusions. When applied separately, each augmen-\ntation enhances the model's understanding of logi-\ncal equivalence, thereby improving its overall rea-\nsoning ability. However, when condition and step\nshuffling are applied together, the logical structure\nis perturbed in two ways, requiring the model to\nsimultaneously align different orders of both con-\nditions and steps, increasing learning difficulty and\nreducing generalization. This suggests that exces-\nsive augmentation may introduce noise, making it\nharder for models to establish logical equivalence.\nEffect of Augmentation Frequency\nIn the main experiment, we set $|D'_C| = |D_c|$,\nmeaning that the parameter k = 1. To investigate\nthe impact of augmentation quantity, we increase k\nand generate k = 5 augmented instances for each"}, {"title": "Conclusion", "content": "In this paper, we systematically study how to en-\nhance the logical reasoning ability of LLMs by\naddressing their limitations in reasoning order vari-\nations. We introduce an order-centric data aug-\nmentation framework based on the principles of\nindependency and commutativity in logical reason-\ning. Our method involves shuffling independent\npremises to introduce order variations and con-\nstructing directed acyclic graphs (DAGs) to identify\nvalid step reorderings while preserving logical de-\npendency. Extensive experiments across multiple\nlogical reasoning benchmarks demonstrate that our\nmethod significantly improves LLMs' reasoning\nperformance and their adaptability to diverse logi-\ncal structures."}, {"title": "Limitations", "content": "Our work primarily focuses on logical commutativ-\nity within propositional reasoning tasks. However,\nthis property extends beyond these tasks. It is also\nprevalent in many other reasoning scenarios, such\nas mathematical problems and other logic-based\ntasks. This remains an area for future exploration.\nAdditionally, while we have explored the impact\nof condition order and answer order augmentations\non model performance, how to further integrate\nand refine these augmentations for better logical\nreasoning capability is still an open question. We\nbelieve our exploration will provide valuable in-\nsights for future work on logical equivalence and\ncommutativity in reasoning."}, {"title": "Appendix", "content": "Details of Generating Solutions\nIn Sec. 4.2, We discuss how to generate step-by-\nstep solutions through $D = {P,C,L}$. Specifi-\ncally, we follow these steps:\n(1) For datasets that do not have first-order logic\n(FOL) expressions, such as RuleTaker and Log-\nicNLI, we extract their premises and conclusions,\nand use GPT-40-mini with prompts as shown in\nTab. 6 to convert them into corresponding FOL\nrepresentations. FOLIO, on the other hand, already\nincludes FOL expressions, so no conversion is re-\nquired.\n(2) The FOL-enhanced premises and ground\ntruth labels are input into the model, prompting\nit to generate step-by-step solutions. As shown\nin the prompt in Tab. 7, we add two domain-\nspecific examples from each dataset to the prompt,\nrequiring the model to clearly define the purpose\nand reasoning for each step, eventually leading\nto the final conclusion. The Task prompt speci-\nfies the possible values for the label. Specifically,\nin FOLIO, the label values are {True, False, Un-\nknown}, in RuleTaker they are {entailment, not\nentailment}, and in LogicNLI they are {entailment,\nneutral, self_contradiction, contradiction}.\n(3) The model then reprocesses the generated\nsolutions, using prompts like the one shown in Tab.\n8, to extract the premise indices and premise step\nindices used in each reasoning step.\nKendall Tau Distance\nIn our study, we investigate the effects of premise\norder transformations by using the Kendall tau dis-\ntance $\\tau$. This coefficient measures the correlation\nbetween two ordered lists, providing a quantitative\nway to assess how much one order differs from an-\nother. We user to categorize various permutations\nof premise orders and assess their impact on model\nperformance.\nThe Kendall tau coefficient $\\tau$ is calculated as\nfollows:\n$\\tau = \\frac{C - D}{\\binom{n}{2}}$\nwhere $C$ is the number of concordant pairs (pairs\nof items that are in the same relative order in both\nlists), and $D$ is the number of discordant pairs\n(pairs that are in opposite order in both lists). The\ntotal number of possible pairs is $\\binom{n}{2}$, where $n$ is\nthe number of items being compared."}]}