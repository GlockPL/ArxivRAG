{"title": "Integrating Reinforcement Learning and Model Predictive Control with Applications to Microgrids", "authors": ["Caio Fabio Oliveira da Silva", "Azita Dabiri", "Bart De Schutter"], "abstract": "This work proposes an approach that integrates reinforcement learning and model predictive control (MPC) to efficiently solve finite-horizon optimal control problems in mixed-logical dynamical systems. Optimization-based control of such systems with discrete and continuous decision variables entails the online solution of mixed-integer quadratic or linear programs, which suffer from the curse of dimensionality. Our approach aims at mitigating this issue by effectively decoupling the decision on the discrete variables and the decision on the continuous variables. Moreover, to mitigate the combinatorial growth in the number of possible actions due to the prediction horizon, we conceive the definition of decoupled Q-functions to make the learning problem more tractable. The use of reinforcement learning reduces the online optimization problem of the MPC controller from a mixed-integer linear (quadratic) program to a linear (quadratic) program, greatly reducing the computational time. Simulation experiments for a microgrid, based on real-world data, demonstrate that the proposed method significantly reduces the online computation time of the MPC approach and that it generates policies with small optimality gaps and high feasibility rates.", "sections": [{"title": "1. Introduction", "content": "1.1. Motivation\nComplex infrastructure systems, such as energy, water, and transportation networks are pervasive in our modern world. Analysis and control design of such systems is very challenging due to their size and intricate behavior. These networks inherently have a combination of discrete and continuous dynamics and/or decision variables. Several modeling approaches [1, 2, 3] have considered hybrid systems to represent these important infrastructure networks. For instance, increasing energy demand and expanding decentralized energy generation by renewable sources have motivated the conception of microgrids, which are local electrical grids that can be connected to a main grid. The operation of such microgrids consists of scheduling both the power flow directions and the amount of power being exchanged between each of the entities of the microgrid. As a result, discrete and continuous decision variables have to be planned considering market conditions to reduce the operation cost.\nFor the considered problem, model predictive control (MPC) arises as a promising technique due to its ability to handle constrained complex systems with discrete and continuous decision variables [4]. In MPC, the system model is used for optimization of the control actions over a finite prediction horizon. At each time step, the model-based optimization problem is solved, the first element of the control sequence is applied to the system, and the prediction horizon is shifted one time step ahead. Furthermore, MPC has solid theoretical foundations with regard to stability, performance, and safety [5]. However, these advantages often come at the price of intense online computational requirements, limiting the use of MPC due to hardware constraints or execution time limitations. This bottleneck is especially more pronounced in MPC for hybrid systems, where the optimizer has also to consider a sequence of discrete decision variables over the given prediction horizon. In this case, the optimization problem then becomes of mixed-integer nature, which is NP-hard [6].\nThe most widely used technique to solve mixed-integer problems in modern solvers is branch-and-bound [7]. In the worst case, the solver has to find solutions for the relaxed problems for each possible combination of the integer decision variables. This is not scalable with respect to the number of integer variables because of the combinatorial nature of the problem. Branch-and-bound algorithms mitigate this issue by efficiently pruning branches of the search tree by estimating lower and upper bounds of the objective function and by proposing cutting planes, which reduce the feasible set of the mixed-integer programs by introducing linear inequalities as additional constraints [8]. Moreover, many expert-designed heuristics are employed for improving the search such as node selection and branch variable selection techniques to reduce computation time. One alternative to branch-and-bound for the control of hybrid systems is to pre-compute the MPC control law offline and simply evaluate this function online. For hybrid systems, the off-line computation of the explicit MPC control law via multi-parametric programming and dynamic programming was explored in [6]. However, this approach, also referred to as explicit MPC, can be only successfully applied to low-dimensional linear systems. Despite improvements in branch-and-bound and explicit MPC, solving mixed-integer problems with a significant number of integer decision variables is fundamentally difficult.\nRecently, supervised learning has been explored in several MPC approaches for hybrid systems to reduce the online computational time of the resulting mixed-integer programs [9, 10, 11, 12]. In essence, the aforementioned supervised learning methods have the same structure and learning setting and the main difference lies in the choice of the classifier. These works employ supervised learning to approximate the mapping from the system state to the discrete optimal solution. By applying this classifier to predict the discrete optimization variables, the mixed-integer program is then simplified to an optimization problem consisting only of real-valued variables, significantly reducing the online computational burden of the MPC controller. To build the training dataset, these methods rely on branch-and-bound to solve the control problem a mixed-integer program to optimality several times. As a result, the main issue is sidelined to the offline phase of the algorithm, where typically more computing resources are available."}, {"title": "1.2. Contributions", "content": "Our work aims to propose an integrated reinforcement learning and MPC method that solves mixed-integer linear and quadratic programs with low computational footprint, low optimality gap, and high feasibility rate. We build on the existing idea of decoupling the decision on the discrete and continuous variables with learning and MPC [9, 10, 11, 12]. However, we explore a novel paradigm by employing reinforcement learning in place of supervised learning to directly optimize for control performance \u2013 instead of minimizing the classification error and to avoid the use of branch-and-bound in both the offline and online phases of the algorithm.\nThe main contributions of the paper with regard to the literature are:\n\u2022 We propose a novel integrated reinforcement learning and MPC framework for control of mixed-logical dynamical systems.\n\u2022 The Q-function is partitioned across the prediction horizon and the definition of decoupled Q-functions is conceived to make the learning problem more tractable.\n\u2022 Simulation experiments in a microgrid system show the efficacy of the proposed approach in reducing the computational load of the MPC controller. Moreover, the comparison between the proposed approach and a method based on supervised learning reveals a trade-off: while the former outperforms in terms of feasibility, the latter outperforms in terms of optimality.\nIn our problem, long-term decision-making involves determining suitable sequences of discrete decision variables over a given prediction horizon, which is a combinatorial problem. To mitigate the combinatorial complexity of the optimization problem, we propose the decoupling of the Q-function across time, i.e., each time step of the prediction horizon has its own decoupled Q-function. With the proposed decoupling of the Q-function, our method effectively simplifies the learning problem as the discrete actions are decided sequentially rather than simultaneously. A similar idea was explored in [13]; however, the separation therein was not in time, but for independent actions. In our setting, future actions may depend on past actions; therefore, we use a recurrent neural network to retain past information in its internal states and recursively compute the decoupled Q-functions. A case study is used to empirically show that the reinforcement learning agent can learn performing policies with the unorthodox definition of decoupled Q-functions."}, {"title": "1.3. Outline", "content": "This paper is organized as follows. In Section 2, we give an overview of the literature on the intersection of learning and control for hybrid systems. Section 3 formalizes the control problem. Our novel method that integrates reinforcement learning and MPC for the control of mixed-logical dynamical systems is described in Section 4. Section 5 presents the simulation setup, the results, and the discussion. This paper ends in Section 6 with conclusions and suggestions for future work."}, {"title": "2. Related work", "content": "Some works have applied learning to reduce the solution time of mixed-integer programs by embedding learning into the branch-and-bound algorithm to substitute expert-designed heuristics with learned rules. For instance, learning can be used to improve cutting plane rules [14], branching variable selection [15], and node selection [16]. Although the aforementioned approaches indeed reduce the solution time of the considered mixed-integer programs, control applications often require a more expressive reduction in the computational load.\nIn [17] the authors approximate the state-action value function (Q-function) of time-invariant mixed-logical dynamics systems. The core of their approach is to parametrize the Q-function with Benders cuts and estimate the Q-function from a lower bound. However, the scope of application of this approach is limited because it only considers auxiliary binary variables from the mixed-logical dynamical modeling framework and it is not compatible with binary states and inputs. In [18], the authors are successful in reducing the computational burden of MPC by learning a state-dependent horizon and a state-dependent recomputation policy, i.e. the policy that decides whether the MPC optimal control problem should be recomputed at a given time step. In [19], an MPC controller is used as a function approximator of the state-action value. The authors show how the internal parameters of the controller can be tuned by policy gradient methods. An extension of the same approach to mixed-integer problems was made in [20]; however, the goal is to target performance rather than to reduce the computational cost of the optimization problem.\nThere is also a body of work that explores the use of reinforcement learning to jointly learn discrete and continuous policies [21, 22, 23]. In the artificial intelligence community, this problem is explored under the framework of Markov decision processes with parameterized actions. The problem is very similar to that of control of hybrid systems because the action space has discrete and continuous elements. Even though these works can provide useful insight into the parametrization and training of policies for hybrid systems, they lack the optimality and constraint satisfaction that MPC can provide.\nOutside the domain of hybrid systems, other works have also integrated learning into the MPC framework in various ways. The most popular approaches include the adaptation of the system model, the use of MPC as a safety filter for RL, and the online tuning of the cost and constraint functions for performance, see [24, 25] for more methods on the interplay of learning and MPC. These approaches are aimed towards improving performance, safety, and/or robustness and do not address the computational issues, which is the main concern of our work."}, {"title": "3. Control problem", "content": "As a control algorithm, we consider MPC for its capacity to handle multivariable constrained hybrid systems. In the context of MPC, mixed-logical dynamical (MLD) systems [26] are typically used to model hybrid systems since they can be readily used to formulate open-loop finite-horizon optimal control problems [4]. The equivalence of MLD systems and other hybrid system modeling frameworks was established in [27], showing their broad applicability. When MLD systems are controlled with MPC and the cost function and the constraints are linear (quadratic), the resulting optimization problem is a mixed-integer linear (quadratic) program (MIL(Q)P). In this section, a general description of the MPC optimization problem for an MLD system is first addressed to set the stage for the formulation of the control problem as an MIL(Q)P.\nConsider the MLD system\n$x(k + 1) = Ax(k) + B_{1}u(k) + B_{2}d(k) + B_{3}z(k) + B_{5}$,\n$E_{2}0(k) + E_{3}z(k) \\leq E_{1}u(k) + E_{4}x(k) + E_{5}$ (1)\nwhere $x \\in R^{nc} \\times {0,1}^{nd}$ is a vector containing the continuous and discrete system states, $u \\in R^{mc} \\times {0,1}^{md}$ are the continuous and discrete inputs, $\\delta \\in {0, 1}^{rd}$ is a vector with the auxiliary discrete variables, $z \\in R^{re}$ is a vector with the continuous auxiliary variables arising from the MLD modeling, and A, ${Bi}_{i=1,2,3,5}$, ${Ei}_{i=1,2,3,4,5}$ are matrices of appropriate dimensions. Note that the linear constraints may represent logical constraints, a byproduct of MLD modeling, or system operating constraints.\nThe goal is to control the system (1) with a receding-horizon strategy, where at each time step a finite-horizon optimal control problem is solved. Such an optimization problem can be formulated as follows:\n$\\min_{x(k),Ec(k),Ed(k)} J(x(k), Ec(k), Ed(k))$\ns.t. $x(k + 1 + 1) = Ax(k + 1) + B_{1}u(k + 1)+$\n$+ B_{2}0(k + 1) + B_{3}z(k + 1) + B_{5}$,\n$E_{2}8(k + 1) + E_{3}z(k + 1) \\leq E_{1}u(k + 1)+$\n$+ E_{4}x(k + 1) + E_{5}$,\nfor l = 0, ..., Np \u2013 1\nwhere the cost function is defined by\n$J(x(k), ec(k), Ea(k)) =$\n$= \\sum_{l=0}^{N_p-1} (l(x(k + 1), u(k + 1), d(k + 1), z(k + 1))) +$\n$+ V_{f}(x(k + N_p)),$\n (2)\n(3)\nthe prediction horizon is denoted by Np, the variable I indexes the time along the prediction horizon, and the final cost is represented by Vf(\u00b7). The predicted state trajectory over the prediction horizon is denoted by $x(k) = [x^T(k), ..., x^T(k +"}, {"title": "4. Method: Novel integration of reinforcement learning and MPC", "content": "In this section, we present the main contribution of our work: an integrated reinforcement learning and MPC method for the solution of mixed-integer linear and quadratic problems for the control of mixed-logical dynamical systems. The main goal is to ease the online computational burden of solving mixed-integer linear and quadratic programs by decoupling the computation of the integer and continuous decision variables. In our framework, the integer decision variables are determined by reinforcement learning, and the continuous decision variables by MPC, which is an optimization-based framework. Accordingly, the solution of mixed-integer programs is entirely avoided in on-line operation. Consequently, our approach can be seen as an alternative to branch-and-bound for MPC for MLD systems. In what follows, the decoupling of the discrete and continuous variables of the MILP of (5) is addressed. Next, we explain the role of reinforcement in our approach.\nHerein it is assumed that the system dynamics are known and training takes place fully offline, i.e., there is no online adaptation of the learning parameters.\n4.1. Decoupling of decision variables\nThe mixed-integer linear program (MILP) (5) is parametric with respect to the initial state x(k) and possibly an exogenous signal y(k). Consider the set of these values in which (5) is feasible. In this set, the solution of (5) maps"}, {"title": "4.2. Role of reinforcement learning", "content": "Reinforcement learning (RL) is a general learning framework where the agent learns a control policy based on its interaction with the environment. It has received increasing attention in control applications due to its capacity to learn complex policies and for its low demand for online computation [28]. Here we describe an approach that exploits the benefits of RL to efficiently determine the discrete decision variables of (5).\nIn the mixed-integer linear program (5), the main computational complexity stems from the number of discrete decision variables, which can come from both the number of actions per time step and the length of the prediction horizon. In our method, reinforcement learning is used to ease the online computational burden of solving mixed-integer linear programs by decoupling the computation of the discrete and continuous decision variables. In our framework, the discrete decision variables are determined by reinforcement learning, and the continuous decision variables by optimization-based control \u2013 MPC. Accordingly, the solution of mixed-integer programs is entirely avoided in on-line operation.\nIn order to formulate the problem as a Markov decision process (MDP) \u2013 the standard reinforcement learning framework we lump together the system and the MPC controller in a single block to form the environment, see Figure 2. The environment receives the discrete actions from the RL agent and outputs the next state and the corresponding reward. This abstraction allows the decoupling of the discrete and continuous decision variables and it is crucial to bridge reinforcement learning and optimization-based control for MLD systems. The agent is responsible for determining the discrete sequence of actions ea(k) over the prediction horizon. This vector is then sent to the environment which solves the optimization problem (6) and computes the continuous decision variables (k). As a result, the environment outputs the next state x(k + 1) and the reward r(k). Any reinforcement learning algorithm can be employed to train the agent; however, for simplicity, we use Deep Q-Learning [29] to present our approach. For a graphical representation of the reinforcement learning setting, see Figure 2.\nHerein, the Q-function is defined as the expected reward over a finite horizon if the agent takes action ea(k) at the current time step k and then follows the policy ea(k + 1) = n(x(k + 1)) over the remaining steps of the horizon:\n$Q^*(x(k), Ea(k)) = r(k) + \\sum_{l=1}^{N_p-1} a^l r(k + l)$ (7)\nwhere a is the discount factor and the reward is defined as a function of the objective of (6):\nr(k) = f(V(x(k), Ed(k)))\nwhere V() defined in (6) and f(.) is a scaling function used to keep the reward within reasonable ranges, e.g. r \u2208 [0, 1], preventing the gradients from becoming too large, which would impair learning. If the action ea(k) causes the LP of (6) to be infeasible, then the reward becomes negative, e.g. r = -1, to penalize this behavior.\nThe vector ea(k) contains the entire sequence of discrete variables over the prediction horizon. For each time step of the prediction horizon l = 0, . . ., Np \u2013 1, we can represent the discrete action per time step with &a,1(k), hence\n$Ea(k) = [(&d,0(k))^T, (&d,1(k))^T, ..., (&d,N_p-1(k))^T]$.\nFor example, suppose that all of the discrete optimization variables are binary and the parameter \u2081 represents the number of binary actions per time step, then there are $2^{b N_p}$ possible sequences for the vector ea(k). As the number of binary variables N\u266d and the length of the prediction horizon Np increase, the number of sequences grows exponentially. Due to the potentially large size of the action space, this is a very challenging problem for RL algorithms. For instance, efficient exploration becomes a big issue in this setting.\nInstead of using an RL algorithm to find the Q-function for the action a(k), as defined in (7), our approach shifts the goal to learning the so-called decoupled Q-functions for the sub-actions {&a,1(k)}1. The decoupled Q-functions are pseudo Q-functions in the sense that they do not entirely fit the standard definition of a Q-function. These decoupled Q-functions are just used as proxies to determine the sub-actions, i.e.,\n$Ed.1(k) = max Q_1(x(k), \u03b5, h_{1-1}) for l = 1, Np - 1$. (8)\nwhere h\u2081-1 is a signal that depends on the previous actions up to time step i of the prediction horizon {&d,j(k)}=0. If the sub-actions were independent, the learning problem would be greatly simplified. However, this assumption is very constraining for most useful applications where optimal planning requires knowledge of past sub-actions. While each step of the prediction horizon has its own decoupled Q-function Q1(\u00b7), the signal h\u2081-1 retains some of the couplings with the past sub-actions. In this way, some inter-dependency between the sub-actions is preserved and the learning problem is simplified with the decoupling of the Q-functions. Note that the size of the action space for each decoupled Q-function is 2, showing that the combinatorial growth is mitigated with respect to the prediction horizon.\nTo approximate the decoupled Q-functions, we propose the use of recurrent neural networks (RNNs), which have internal states that carry over information from the past \u2013 similar to the role of the signal h in (8). More specifically, a long short-term memory (LSTM) network is employed for its capacity to capture long-term time dependencies in data"}, {"title": "4.3. Differences between methods based on supervised and reinforcement learning", "content": "To shed light on the fundamental differences between methods based on supervised learning and our approach,\nwe explain the main differences in the learning setups. From the perspective of supervised learning approaches, the problem becomes a classification problem, i.e. the network aims to predict the optimal sub-action for each time step of the prediction horizon. Instead of relying on decoupled Q-values to obtain the sub-actions, SL directly approximates the mapping from the augmented state to the optimal sub-actions. Evidently, this comes at the cost of repeatedly solving the MILP (5) to optimality to create the training dataset. Moreover, in SL, training does not aim at minimizing the long-term system's operation cost as in RL, rather it targets at minimizing the classification error. The difference in the objective of RL and SL creates a notable difference in the behavior of the control policies, which are best shown in the case study of the next section. For a brief description of the training process in the context of supervised learning, the reader is referred to Appendix B."}, {"title": "5. Case study: microgrid control", "content": "In this section, we evaluate the performance of our approach for microgrid operation optimization. The goal is to minimize the long-term operation cost, typically defined over several hours, of the microgrid system described in Figure 4 and Appendix A. At each time step, the microgrid operator can determine a set of discrete and continuous decision variables, shown in Table 1. Herein we consider the receding horizon control strategy laid out in Section 3. In this case, the microgrid operator has to determine a sequence of discrete and continuous decision variables over a finite prediction horizon. Then, the underlying optimization problem is formulated as an MILP (15). Hence, efficiently solving such an MILP to minimize the required computation time is at the core of our problem. In this context, we compare our RL-based approach against standard MPC for MLD systems, where the optimization problem (2) is solved to optimality with branch-and-bound, and against the supervised learning approach described in the last"}, {"title": "5.1. Setup", "content": "In the case study, we aim to solve the economic power dispatch problem for a microgrid, i.e. to fulfill the power demand of the microgrid loads while minimizing the operation cost. The knowledge of the market energy prices and the forecasts of the power demand and power generation from renewable energy sources solar and wind energy are considered \u2013 allows the operator to plan and optimally schedule the operation of the microgrid over a time horizon.\nTo accomplish this goal, the microgrid operator has a set of discrete and continuous actions. The discrete control actions are the following: buying or selling from grid grid(k), charging or discharging the energy storage system \u03b4\u266d(k) and turning on or turning off the generators {dis(k)} at time step k. The continuous actions are the power exchange with the main grid Pgrid(k), power exchange with the energy storage system P\u266d(k), and the power provided by the dispatchable generators Pdis(k). For a summary of the decision variables, see Table 1. In this setting, the decision variables are represented by the vectors ec(k) and ea(k), which are formed by the concatenation of all discrete and continuous decision variables, respectively, over the prediction horizon. Furthermore, consider the exogenous signal y(k) contains the forecasts over the prediction horizon for the: purchasing, sale, and production prices; power generation from the renewable energy sources; and power demand from the loads.\nThe parameters of the microgrid considered in this case study are shown in Table 3. For the experiments, the number of dispatchable generators is set to Ngen = 3. Hence, for a prediction horizon Np, there are 5. Np binary variables. As there is a binary variable for the connection with the main grid and another one for the connection with the storage unit, amounting to 5 binary variables per time step. Hence, even for a small number of generators, the number of possible action sequences (25\u00b7p) is potentially huge depending on the chosen prediction horizon. With a"}, {"title": "5.3. Discussion", "content": "The results in the previous section show high potential for employing the proposed framework to efficiently control MLD systems, where a good balance between computational time, optimality, and feasibility is achieved. What"}, {"title": "A. System description", "content": "In the case study, we address the economic dispatch problem in a microgrid system. This appendix describes the constituent elements of the microgrid, shown in Figure 4, and their modeling as a mixed-logical dynamical (MLD) system. This modeling framework is used for its capacity to capture the behavior of continuous and discrete dynamics and decision variables. Besides, MLD modeling paves the way for the formulation of the economic dispatch problem as a mixed-integer program using an MPC approach. The use of MLD modeling and MPC for the microgrid operation optimization was explored in [1], where the problem is cast as a mixed-integer linear program. The modeling of the microgrid was further simplified in [33] and [9] for the design of a ruled-based control policy and a learning-based control rule, respectively. Herein we use the same simplified microgrid modeling framework to assess the performance of the proposed approach.\nA.1. Storage unit\nThe energy storage unit is described by the following equations:\n$xp(k + 1) = \\begin{cases} x_b(k) + \\frac{T_s}{\\eta_d}P_b(k) & \\text{if } P_b(k) < 0 \\\\ x_b(k) + T_s \\eta_c P_b(k) & \\text{if } P_b(k) \\geq 0 \\end{cases}$ (9)\nwhere x(k) is the energy level in the storage unit at time step k, ne and na are the charging and discharging efficiencies, Pb(k) is power exchanged with the storage unit at time step k and Ts is the sampling time of the discrete-time system. At a given time step, the storage unit can be either discharging or charging, depending on the sign of Pb(k). In order to capture this hybrid behavior, we model the storage unit as a mixed-logical dynamical (MLD) system [26]. A binary variable (k) is introduced to signal whether the storage unit is charging or discharging, \u03b4\u266d(k) = 1 \u21d4 P\u266d(k) \u2265 0 and \u03b4\u266d(k) = 0\nPb(k) < 0, respectively. By defining a continuous auxiliary variable zu(k) = \u0431\u266d(k)P\u266d(k), (9) can be simplified and conveniently written as a single linear equation:\n$x(k + 1) = x(k) + T_s(\\eta_c - \\frac{1}{\\eta_d})z_b(k) + \\frac{T_s}{\\eta_d} P_b(k)$\nA.2. Generation units\nIn the microgrid, energy can be locally produced either by dispatchable units or by renewable energy sources. The dispatchable units can be turned on/off and their power output levels can be arbitrarily chosen by the microgrid operator within operating constraints. Concerning the renewable energy sources, solar and wind energy sources are considered.\nThe cost for locally producing energy at time step k is\n$C_{prod}(k) = c_{prod}(k) \\sum_{i=1}^{N_{gen}} p_{dis}^i(k)$ (10)"}, {"title": "A.3. Main grid", "content": "At any given time step, the microgrid can buy or sell energy from the main grid. The power exchange at time step k is represented by Pgrid(k). If this variable is nonnegative, the microgrid is set to import energy from the main grid. If it is negative, the microgrid is in export mode. Then, from the microgrid operator's perspective, at time step k, the operation cost is represented as\n$C_{grid}(k) = \\begin{cases} c_{sell}(k) P_{grid}(k) & \\text{if } P_{grid}(k) < 0 \\\\ c_{buy}(k) P_{grid}(k) & \\text{if } P_{grid}(k) \\geq 0 \\end{cases}$ (11)\nwhere csell(k) and cbuy(k) are the prices for selling and buying energy to/from the main grid, respectively, at time step k.\nConsider the discrete auxiliary variable $[d_{grid}(k) = 1] \\iff [P_{grid}(k) \\geq 0]$ and the continuous auxiliary variable Zgrid(k) = grid(k)Pgrid(k). Now, with the use of MLD modeling, the operation cost between the microgrid and the main grid can be expressed in a single linear equation\n$C_{grid}(k) = c_{buy}(k) z_{grid}(k) \u2013 c_{sell}(k) z_{grid}(k) + c_{sell}(k) P_{grid}(k)$ (12)\nNote that the operation cost is negative in export mode, i.e. the microgrid operator profits by selling energy to the grid.\nA.4. Assumptions\nWe only consider uncontrollable loads, i.e. the microgrid operator does not affect the power demanded by them. Furthermore, at the current operation time, the actual load and its forecast over the future are assumed to be known. Similarly, the current power generated by renewable energy sources and its forecast are assumed to be known. These are not strong assumptions, since all of these values can be estimated with the use of historical data. We also assume knowledge of the market energy prices for buying and selling energy from the main grid and for locally producing energy with the dispatchable generators."}, {"title": "A.5. Control scheme", "content": "We consider a hierarchical control structure. A high-level controller is concerned with planning the operation schedule: i) which generation units are turned on and their power outputs, ii) whether the storage unit is charging or discharging and the corresponding power exchange, and iii) the mode of operation of the microgrid with respect to the main grid \u2013 importing or exporting \u2013 and the amount of power flowing between them. The variables determined by the high-level controller are reported in Table 1. A low-level controller is responsible for keeping voltage, frequency, and phase within the operation range and it operates in a faster time scale. In this scheme, the high-level controller is responsible for generating the set points for the microgrid elements so that the load demand is satisfied and the operation cost is minimized. On the other hand, the low-level controller has the objective of tracking these set-points computed by the high-level controller. In this work, we assume that the low-level controller is already in place and we focus exclusively on the design of the high-level controller."}, {"title": "A.6. Control problem", "content": "The operation cost is defined as the sum of the the costs for locally producing energy and the costs for exchanging power with the main grid over a prediction horizon Np. Accordingly, the cost is defined as follows\n$J(x(k), y(k), e_c(k), E_d(k)) =$\n$\\sum_{l=0}^{N_p-1} (C_{prod}(k + l) + C_{grid}(k + l)) = c e_c(k)$ (13)\nwhere x\u2081(k) = [x(0),...,x(N)]T, Cprod(k) and Cgrid(k) are defined in (11) and (10), c is a weighting vector containing the market energy prices use to compactly represent the linear dependence of the operation cost on the decision variables and e(k) is a stacked vector with the continuous decision variables over the prediction horizon.\nNote that the cost function J(\u00b7) is linear with respect to (k) and that it can be negative in the scenario where the microgrid operator profits from selling energy to the main grid. With the objective function defined, the optimization problem for the MPC controller can now be addressed. Along with minimizing the operation cost, the microgrid operator has also to satisfy some operating constraints, which are discussed next. Consider the following finite-"}, {"title": "B. Supervised learning approach", "content": "The training procedure of the SL approach presented in [12] is briefly summarized to highlight fundamental differences between both RL and SL approaches. The approach presented in [12] employs an LSTM network for"}]}