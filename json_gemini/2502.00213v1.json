{"title": "Understanding Why Adam Outperforms SGD: Gradient Heterogeneity in Transformers", "authors": ["Akiyoshi Tomihari", "Issei Sato"], "abstract": "Transformer models are challenging to optimize with SGD and typically require adaptive optimizers such as Adam. However, the reasons behind the superior performance of Adam over SGD remain unclear. In this study, we investigate the optimization of transformer models by focusing on gradient heterogeneity, defined as the disparity in gradient norms among parameters. Our analysis shows that gradient heterogeneity hinders gradient-based optimization, including SGD, while sign-based optimization, a simplified variant of Adam, is less affected. We further examine gradient heterogeneity in transformer models and show that it is influenced by the placement of layer normalization. Additionally, we show that the momentum term in sign-based optimization is important for preventing the excessive growth of linear-head parameters in tasks with many classes. Experimental results from fine-tuning transformer models in both NLP and vision domains validate our theoretical analyses. This study provides insights into the optimization challenges of transformer models and offers guidance for designing future optimization algorithms. Code is available at https://github.com/tom4649/gradient-heterogeneity.", "sections": [{"title": "Introduction", "content": "Transformer models (Vaswani, 2017) have achieved significant success across various tasks, especially in natural language processing (NLP). Training transformer models mostly relies on adaptive optimization methods such as Adam (Kingma & Ba, 2017), which outperform stochastic gradient descent (SGD) for these architectures (Zhang et al., 2020; Kunstner et al., 2023; Zhang et al., 2024a; Kunstner et al., 2024).\nDespite the superior performance of Adam, the reasons for its advantage over SGD, particularly during fine-tuning, remain unclear. Adam consistently outperforms SGD, even in full-batch settings, while SignSGD (Bernstein et al., 2018) achieves performance comparable to Adam under the same conditions (Kunstner et al., 2023). This suggests that the performance gap cannot be solely attributed to gradient noise (Zhang et al., 2020) but rather stems from fundamental differences between SGD and SignSGD, which remain unexplored. The Adam-SGD gap has been partially linked to heavy-tailed label distributions (Kunstner et al., 2024), but this explanation does not fully account for the gap in fine-tuning tasks, where the number of labels is sometimes small. Similarly, the gap has been associated with Hessian heterogeneity in transformer models (Zhang et al., 2024a), yet the underlying mechanism remains unclear.\nIn this study, we propose that the performance gap between Adam and SGD arises from gradient heterogeneity, defined as the disparity in gradient norms across parameters. While Zhang et al. (2024a) emphasize Hessian heterogeneity, we interpret it as a consequence of the correlation between gradients and the Hessian. This interpretation enables further analysis, as the gradient is easier to compute than the spectrum of the Hessian. First, we derive upper bounds for the complexity of gradient-based and sign-based sequences in both deterministic and stochastic settings. Our results show that gradient-based sequences are more sensitive to gradient heterogeneity than sign-based sequences. Second, we investigate gradient heterogeneity in transformer models, examining its relationship with architectural design. Our analysis reveals that placing layer normalization after residual connections amplifies gradient heterogeneity. Finally, we discuss the role of the momentum term in SignSGD.\nOur contributions are summarized as follows:\n\u2022 We derive upper bounds for the iteration complexity for optimization algorithms in both deterministic and stochastic settings. Our analysis suggests that SGD is highly sensitive to gradient heterogeneity, whereas Adam is less affected (Theorems 4.7 and 4.9).\n\u2022 We investigate gradient heterogeneity in transformer models, identifying the position of layer normalization as a factor influencing it (Section 4.6).\n\u2022 Additionally, we emphasize the role of the momentum term in SignSGD, showing that it effectively prevents the unbounded growth of linear-head parameters in tasks with many classes (Proposition 4.10)."}, {"title": "Related work", "content": "Adam in deep learning. Adam (Kingma & Ba, 2017) is a widely used optimization algorithm in deep learning, known for its well-established convergence properties (Zhang et al., 2022). However, the reasons for its superior performance are not yet fully understood. Jiang et al. (2024) empirically observed that Adam tends to converge to parameter regions with uniform diagonal elements in the Hessian, supported by theoretical analysis based on two-layer linear models. Rosenfeld & Risteski (2023) argued that the ability of Adam to handle outliers in features is a critical factor in its effectiveness. Additionally, Kunstner et al. (2024) attributed the performance of Adam in language models to its ability to manage heavy-tailed class imbalance.\nOptimization challenges in transformer models. A key aspect of transformer optimization is the notable superiority of Adam over SGD. Zhang et al. (2020) attributed this to the heavy-tailed gradient noise, but Kunstner et al. (2023) later challenged this, arguing that the superior performance of Adam can be attributed to sign-based characteristics rather than gradient noise, supported by full-batch experiments. Ahn et al. (2023) demonstrated that linear transformer models exhibit similar optimization behaviors to standard transformer models. Zhang et al. (2024a) revealed that the Hessian spectrum of the loss function with transformer models is heterogeneous and suggested that this is one cause of the Adam-SGD performance gap. This heterogeneity was later confirmed by Ormaniec et al. (2024), who derived the Hessian of transformer models explicitly.\nSign-based optimization and variants. SignSGD, also known as sign descent (Balles & Hennig, 2018), is an optimization method that is computationally efficient and memory-saving, making it suited for distributed training (Bernstein et al., 2018). Through program search, a sign-based optimization algorithm called Lion (evolved sign momentum) was discovered (Chen et al., 2024b), and its effectiveness was shown by Chen et al. (2024a). Adam can be interpreted as a variance-adapted variant of SignSGD. For example, Xie & Li (2024) analyzed the convergence property of Adam by using this property. Similarly, Zhao et al. (2024) found that sign-based optimizers restore the stability and performance of Adam and proposed using adaptive learning rates for each layer. Additionally, Zhang et al. (2024b) showed that adaptive learning rates do not need to be computed at a coordinate-wise level but can be applied at the level of parameter blocks."}, {"title": "Preliminaries", "content": "In this section, we introduce the notation, provide an overview of the optimization methods related to our study, and define the setting for our analysis.\n3.1 Notation\nVectors and matrices. The k-th element of a vector a is denoted by \\( a_k \\), and for a matrix A, we use \\( A_{k,:}, A_{:,l} \\), and \\( A_{kl} \\) to denote the k-th row, l-th column, and element at (k, l), respectively. When a vector or matrix is split into blocks, \\( [\\cdot]_b \\) denotes the b-th block. The \\( L_2 \\) norm is denoted by \\( || \\cdot ||_2 \\), for vectors and represents the operator norm for matrices. The all-ones vector and identity matrix of size a are denoted by \\( \\mathbb{1}_a \\) and \\( I_a \\), respectively. The operator blockdiag(\u00b7) constructs block diagonal matrices. Gradients are computed using the numerator layout.\nModel. We consider a classification task with C classes and sample space X. The model \\( f(;\\theta): \\mathcal{X} \\rightarrow \\mathbb{R}^C \\) is parameterized by \\( \\theta \\in \\mathbb{R}^P \\), which is divided into B blocks, denoted as \\( [\\theta]_b \\in \\mathbb{R}^{P_b} \\), with \\( \\sum_{b=1}^{B} P_b = P \\). It comprises a pre-trained feature extractor \\( \\phi(\\cdot): \\mathcal{X} \\rightarrow \\mathbb{R}^h \\) and a linear head with weight \\( V \\in \\mathbb{R}^{C \\times h} \\) and bias \\( b \\in \\mathbb{R}^C \\). The output is given by \\( f(x) = V\\phi(x) + b \\). At the beginning of fine-tuning, \\( \\phi \\) remains pre-trained, while V and b are randomly initialized.\nTraining. The training dataset \\( \\{(x^{(i)}, y^{(i)})\\}_{i=1}^N \\) consists of N samples \\( x^{(i)} \\in \\mathcal{X} \\) and the corresponding labels \\( y^{(i)} \\in \\{1, ..., C\\} \\). The training objective is to minimize the training loss \\( L(\\theta) := \\sum_{i=1}^N l(f(x^{(i)}; \\theta), y^{(i)}) \\). Here, \\( l: \\mathbb{R}^C \\times \\{1, ..., C\\} \\rightarrow \\mathbb{R} \\) denotes the cross-entropy loss, defined as \\( l(f(x), y) := - \\log (\\sigma_{sm}(f(x))_y) \\). The function \\( \\sigma_{sm}: \\mathbb{R}^C \\rightarrow \\mathbb{R}^C \\) represents the softmax operation. The element-wise sign function is denoted by sign(\u00b7). The mini-batch loss is denoted by \\( \\widehat{L}(\\theta) \\), and the learning rate at step t is represented by \\( \\eta_t \\).\n3.2 Optimization algorithms\nAdam. Adam (Kingma & Ba, 2017) is widely used in deep learning. It uses the first and second moment estimates of the gradient \\( \\nabla L(\\theta_t) \\), denoted as \\( m_t \\) and \\( v_t \\), computed using an exponential moving average to reduce mini-batch noise. The update is performed coordinate-wise as:\n\n\\theta_{t+1} = \\theta_t - \\eta_t \\frac{\\widehat{m}_t}{\\sqrt{\\widehat{v}_t} + \\epsilon},\n\nwhere \\( \\widehat{\\cdot} \\) denotes bias correction and \\( \\epsilon \\) is a small constant for numerical stability."}, {"title": "Adaptive learning rate and SignSGD", "content": "Adaptive learning rate and SignSGD. A key feature of Adam is its adaptive learning rate, which is computed in a coordinate-wise manner. When the hyperparameter \\( \\epsilon \\), which is typically set close to zero, is ignored and the ratio \\( |m_{t+1}/\\sqrt{v_{t+1}}| \\) is close to 1, Adam behaves similarly to SignSGD (Balles & Hennig, 2018; Bernstein et al., 2018). SignSGD updates the parameters with momentum \\( m_t \\) as:\n\n\\theta_{t+1} = \\theta_t - \\eta_t \\text{sign}(m_t).\n\nThis method has the property that the updates are invariant to the scale of the gradient. In this sense, Adam can be seen as a soft version of SignSGD. Additionally, the optimizer RMSProp (Tieleman & Hinton, 2017), which inspired Adam, was originally motivated by the idea of using the sign of the gradient in a mini-batch setting. RMSProp is similar to Adam but without the momentum term.\nSGD and gradient clipping. SGD can also be modified to achieve scale invariance. The standard SGD update is given by:\n\n\\theta_{t+1} = \\theta_t - \\eta_t \\nabla L(\\theta_t).\n\nA simple way to introduce scale invariance is to normalize the learning rate by the gradient norm, a technique known as normalized gradient descent. This method has been shown to be equivalent to gradient clipping up to a constant factor in the learning rate (Zhang et al., 2019). Gradient clipping is commonly used to stabilize training, particularly in cases where large gradient magnitudes cause instability and is often applied alongside other optimizers. However, a key difference between Adam and SGD is that SGD does not adapt the learning rate in a coordinate-wise manner.\nSteepest descent. SGD and SignSGD can be interpreted as updating in the direction of the steepest descent (Xie & Li, 2024):\n\\begin{aligned}\n\\Delta &= \\underset{\\Delta}{\\text{arg min}} \\quad \\nabla L(\\theta_t)^T \\Delta \\\\\n&\\text{s.t.} \\quad ||\\Delta|| \\leq 1\n\\end{aligned}\nThe steepest descent direction associated with the norms \\( || \\cdot ||_2 \\) and \\( || \\cdot ||_1 \\) corresponds to the updates of SGD and SignSGD, respectively.\nThe steepest descent direction satisfies\n\n\\nabla L(\\theta_t) \\Delta = - ||\\nabla L(\\theta_t)||_{*},\n\nwhere \\( ||\\cdot||_* \\) denotes the dual norm of \\( ||\\cdot|| \\). Thus, evaluating the gradient norm using the dual norm is a natural choice for analyzing steepest descent algorithms because it measures the largest possible directional derivative within the unit norm constraint."}, {"title": "Main results", "content": "In this section, we theoretically analyze optimization methods. We first introduce the setting, assumptions (Section 4.1), gradient heterogeneity, and complexity measures (Section 4.2). Next, we explore the correlation between gradients and the Hessian matrix (Section 4.3) and derive upper bounds for optimization complexity in deterministic (Section 4.4) and stochastic settings (Section 4.5). Finally, we investigate gradient heterogeneity in transformer models (Section 4.6) and the impact of momentum in SignSGD (Section 4.7). Our findings suggest that gradient heterogeneity, which is a characteristic of transformer models, contributes to the performance gap between Adam and SGD.\n4.1 Setting and assumption\nGradient-based and sign-based sequences Kunstner et al. (2023) showed that in full-batch settings without gradient noise, SignSGD performs similarly to Adam and outperforms SGD. This suggests that the performance gap between Adam and SGD arises from differences between SignSGD and SGD. Other studies have also used SignSGD as a proxy for Adam in their analyses (Balles & Hennig, 2018; Li et al., 2024; Kunstner et al., 2024). On the basis of these insights, we analyze the difference between parameter sequences \\( {\\theta_t^{\\text{Grad}}}_{t=0}^\\infty \\) and \\( {\\theta_t^{\\text{Sign}}}_{t=0}^\\infty \\), referred to as the gradient-based and sign-based sequences, respectively. These sequences correspond to updates performed by gradient-based and sign-based optimization. In deterministic settings, these updates are defined as follows:\n\\begin{aligned}\n\\theta_{t+1}^{\\text{Grad}} &= \\theta_t^{\\text{Grad}} - \\eta_t \\nabla L(\\theta_t^{\\text{Grad}}), \\\\\n\\theta_{t+1}^{\\text{Sign}} &= \\theta_t^{\\text{Sign}} - \\eta_t \\text{sign}(\\nabla L(\\theta_t^{\\text{Sign}})).\n\\end{aligned}\nIn stochastic settings, the loss L is replaced with the mini-batch loss \\( \\widehat{L} \\).\nAssumption We consider fine-tuning settings, in which the parameter \\( \\theta \\) can be typically assumed to remain within a region \\( \\mathcal{R}_{FT} \\) throughout training. This assumption restricts \\( \\theta \\) to the localized region \\( \\mathcal{R}_{FT} \\), allowing further assumptions to be applied within this region.\nAssumption 4.1 (Fine-tuning). The parameter \\( \\theta \\) remains within the region \\( \\mathcal{R}_{FT} \\) throughout the training and there exists \\( \\theta^* \\in \\mathcal{R}_{FT} \\) such that \\( L^* := L(\\theta^*) = \\underset{\\theta \\in \\mathcal{R}_{FT}}{\\text{min}} L(\\theta) \\).\nWe assume Lipschitz continuity for the Hessian matrix of the loss function, a standard assumption in optimization analysis (Nesterov, 2013).\nAssumption 4.2 (Lipschitz continuity (Nesterov, 2013)). Within the region \\( \\mathcal{R}_{FT} \\), the loss function L is twice dif-"}, {"title": "Setting and assumption", "content": "ferentiable, and its Hessian matrix is \\( \\rho_H \\)-Lipschitz continuous\n\n||\\nabla^2 L(\\theta) - \\nabla^2 L(\\theta')||_2 \\leq \\rho_H ||\\theta - \\theta'||_2.\n\nAdditionally, it has been observed that Hessian matrices of deep learning models exhibit a near-block-diagonal structure (Collobert, 2004; Zhang et al., 2024a). The block-diagonal approximation is also used in optimization methods (Martens & Grosse, 2015; Zhang et al., 2017). Thus, we assume that the Hessian matrix of the loss function is close to block-diagonal.\nAssumption 4.3 (Near block-diagonal Hessian). Within the region \\( \\mathcal{R}_{FT} \\), the Hessian matrix can be approximated by a block-diagonal matrix with an approximation error \\( \\delta_D \\):\n\n||\\nabla^2 L(\\theta) - \\nabla^2 L_D(\\theta)||_2 \\leq \\delta_D,\n\nfor all \\( \\theta, \\theta' \\in \\mathcal{R}_{FT} \\), where\n\n\\nabla^2 L_D(\\theta) := \\text{blockdiag}(\\{[\\nabla^2 L(\\theta)]_b\\}_{b=1}^B),\n\nrepresents the block-diagonal approximation.\nNote that in equation (1), the left-hand side is bounded above by the sum of squared elements in the non-diagonal blocks, following the relationship between \\( || \\cdot ||_2 \\) and the Frobenius norm."}, {"title": "Gradient heterogeneity and complexity measure", "content": "4.2 Gradient heterogeneity and complexity measure\nGradient heterogeneity. We define gradient heterogeneity as follows:\nDefinition 4.4 (Gradient heterogeneity). The gradient heterogeneity is defined as the disparity in gradient norms across different parameter blocks, \\( \\{||[\\nabla L(\\theta)]_b||_2\\}_{b=1}^B \\).\nThis concept is inspired by Zhang et al. (2024a), who introduced the term \"block heterogeneity\" to describe differences in the Hessian spectrum. Here, we extend this idea by focusing on gradients, which are computationally easier to analyze. Building on the general notion of gradient heterogeneity, we further provide a quantitative perspective through visualizations (Figure 3) and the Gini coefficients (Table 7), offering a concrete measure of this concept.\nWeighted Hessian complexity. To analyze the complexity of optimization, we define the following two measures.\nDefinition 4.5 (Weighted Hessian complexity). The gradient-weighted Hessian complexity \\( \\Lambda_G \\) and parameter-"}, {"title": "Complexity measure", "content": "weighted Hessian complexity \\( \\Lambda_P \\) are defined as:\n\\begin{aligned}\n\\Lambda_G &:= \\underset{\\theta \\in \\mathcal{R}_{FT}}{\\text{sup}} \\frac{\\sum_{b=1}^B ||[\\nabla L(\\theta)]_b||_2 ||[\\nabla^2 L(\\theta)]_b||_2}{\\sum_{b=1}^B ||[\\nabla L(\\theta)]_b||_2}, \\\\\n\\Lambda_P &:= \\underset{\\theta \\in \\mathcal{R}_{FT}}{\\text{sup}} \\frac{\\sum_{b=1}^B P_b ||[\\nabla^2 L(\\theta)]_b||_2}{P}.\n\\end{aligned}\nIn these definitions, \\( \\Lambda_G \\) weights the operator norm of each Hessian block by the corresponding gradient norm, while \\( \\Lambda_P \\) weights it by the parameter dimension. The definitions ensure that the weights of all Hessian blocks sum to 1, as shown by the equalities:\n\n\\sum_{b=1}^B \\frac{||[\\nabla L(\\theta)]_b||_2}{\\sum_{b=1}^B ||[\\nabla L(\\theta)]_b||_2} = \\sum_{b=1}^B \\frac{P_b}{P} = 1.\n\n4.3 Gradient-Hessian correlation\nAs shown in Figure 1, large Hessian operator norms \\( ||[\\nabla^2 L(\\theta)]_b||_2 \\) are often associated with large gradient magnitudes \\( ||[\\nabla L(\\theta)]_b||_2 \\). In contrast, no such correlation is observed between Hessian \\( ||[\\nabla^2 L(\\theta)]_b||_2 \\) and parameter dimension \\( P_b \\), as detailed in Appendix F.1. This gradient-Hessian correlation contributes to an increase in \\( \\Lambda_G \\) under gradient heterogeneity, while \\( \\Lambda_P \\) remains relatively small.\nApproximate explanation. If the loss function L is approximated in the region \\( \\mathcal{R}_{FT} \\) by a second-order Taylor expansion around the optimum \\( \\theta_* \\in \\mathcal{R}_{FT} \\), where \\( L(\\theta_*) \\) is close to 0, and the Hessian matrix is assumed to be block-diagonal, the following inequality approximately holds:\n\n||[\\nabla L(\\theta)]_b||_2 \\leq ||[\\nabla^2 L(\\theta_*)]_b||_2 ||\\delta \\theta||_2,"}, {"title": "Complexity bound", "content": "4.4 Complexity bound\nTo analyze optimization algorithms, we define a complexity measure inspired by Carmon et al. (2020); Zhang et al. (2019); Crawshaw et al. (2022). This measure reflects the number of parameter updates needed to achieve a sufficiently small gradient norm, with higher complexity indicating slower convergence.\nDefinition 4.6 (Iteration complexity). We define the iteration complexity of a parameter sequence \\( {\\theta_t}_{t=0}^\\infty \\), \\( \\theta_t \\in \\mathbb{R}^P \\) with the loss function L and the norm \\( || \\cdot ||_{\\mathfrak{q}} \\):\n\n\\mathcal{T}_\\epsilon(({ \\theta_t}_{t=0}^\\infty, L, || \\cdot ||_{\\mathfrak{q}}) := \\text{inf}\\{t \\in \\mathbb{N} \\mid \\mathcal{C}_\\epsilon(t)\\},\n\nwhere the condition \\( \\mathcal{C}_\\epsilon(t) \\) is defined as follows.\nIn the deterministic setting, \\( \\mathcal{C}_\\epsilon(t) \\) is defined as:\n\n||\\nabla L(\\theta_t)||_{\\mathfrak{q}} \\leq \\rho_\\epsilon.\n\nIn the stochastic setting, \\( \\mathcal{C}_\\epsilon(t) \\) is defined as:\n\n\\mathbb{P}(\\forall s < t, ||\\nabla \\widehat{L}(\\theta_s)||_{\\mathfrak{q}} \\geq \\rho_\\epsilon + \\zeta) \\leq \\frac{1}{2}\n\nCompared with the complexity definitions in previous studies, we introduce a distinction in the choice of norms and a normalization term \\( \\frac{P_\\alpha}{P} \\) to ensure dimensional consistency across different norms.\nUsing this measure, we show the complexity bound in deterministic, namely full-batch, settings as follows. The parameter \\( \\zeta_0 \\in (0, 1) \\) controls the range of learning rates.\nTheorem 4.7 (Deterministic setting). Assume \\( \\delta_D < \\text{min}(\\Lambda_G, \\Lambda_P)/3 \\). Then, the iteration complexities in deterministic settings are bounded as follows.\nFor the gradient-based sequence, suppose that \\( \\epsilon < \\frac{\\Lambda_G}{\\rho_H \\sqrt{P}} \\) holds and that learning rate at time t satisfies \\( \\eta_t = \\zeta_t \\text{min}(\\frac{1}{\\Lambda_G}, \\frac{\\Lambda_P}{\\rho_H ||\\nabla L(\\theta_t^{\\text{Grad}})||_2}) \\), where \\( \\zeta_t \\in [\\zeta_0, 1] \\), we have\n\n\\mathcal{T}_\\epsilon(({ \\theta_t^{\\text{Grad}}}_{t=0}^\\infty, L, || \\cdot ||_2) \\leq \\frac{6(L(\\theta_0) - L^*)}{\\rho_\\epsilon^2 \\zeta_0} \\frac{\\Lambda_G}{\\frac{P_H \\sqrt{P}}{2}}.\n\nFor the sign-based sequence, suppose that \\( \\epsilon < \\frac{\\Lambda_P}{\\rho_H \\frac{P}{P}} \\) holds and that the learning rate at time t satisfies \\( \\eta_t = \\zeta_t \\text{min}(\\frac{1}{\\frac{P}{P} ||\\nabla L(\\theta_t^{\\text{Sign}})||_1}, \\frac{1}{\\rho_H \\frac{P}{P^{3/2}} ||\\nabla L(\\theta_t^{\\text{Sign}})||_1}) \\), where \\( \\zeta_t \\in [\\zeta_0, 1] \\), we have\n\n\\mathcal{T}_\\epsilon(({ \\theta_t^{\\text{Sign}}}_{t=0}^\\infty, L, || \\cdot ||_1) \\leq \\frac{6(L(\\theta_0) - L^*)}{\\rho_\\epsilon^2 \\zeta_0} \\frac{\\Lambda_P}{\\frac{P}{P} \\rho_H \\frac{P}{P^{3/2}}}.\n\nThe iteration complexity of the gradient-based and sign-based sequences is evaluated using the norms \\( ||\\cdot||_2 \\) and \\( ||\\cdot||_1 \\), respectively. This choice of norms is justified because they correspond to the dual norms that determine the steepest descent direction, as discussed in Section 3.2.\nGradient heterogeneity can increase the complexity of the gradient-based sequence. The theorem indicates that the iteration complexity of the gradient-based and sign-based sequences is characterized by \\( \\Lambda_G \\) and \\( \\Lambda_P \\), respectively. As discussed earlier, when the gradient is heterogeneous, \\( \\Lambda_G \\) can become large. Consequently, the iteration complexity of the gradient-based sequence may surpass that of the sign-based sequence under such conditions."}, {"title": "Stochastic setting", "content": "4.5 Stochastic setting\nIn practice, optimization is performed in a stochastic setting, where the gradient is estimated using a mini-batch. Under this setting, we add the following assumptions about noise, defined as the difference between the full-batch and mini-batch gradient.\nAssumption 4.8 (Noise). For all \\( \\theta \\in \\mathbb{R}^P \\), there exist constants \\( \\sigma_\\epsilon, \\sigma_2 \\geq 0 \\) such that:\n\n\\mathbb{E}[\\nabla \\widehat{L}(\\theta)] = \\nabla L(\\theta),\n\n\\mathbb{E}[||\\nabla \\widehat{L}(\\theta) - \\nabla L(\\theta)||_3^3] \\leq \\sigma_3 ||\\nabla L(\\theta)||_3^3,\n\nand for all \\( i \\in \\{1, ..., P\\} \\),\n\n\\mathbb{E}[[\\nabla \\widehat{L}(\\theta) - \\nabla L(\\theta)]_i^2] \\leq \\sigma_2 |\\nabla L(\\theta)_i|^2.\n\n(2)\n(3)\n(4)"}, {"title": "Stochastic setting", "content": "The assumption in Equation (2) is standard in stochastic optimization (Bernstein et al., 2018). We introduce Equation (3) to bound the third-order moment of the gradient noise norm and Equation (4) to model its coordinate-wise correlation with the gradient. This correlation is supported by Figure 2 (additional settings in Appendix F.3). The coordinate-wise assumption is needed for analyzing errors in the gradient sign and block-wise gradient. Additionally, bounding the noise is a common practice in stochastic optimization (Crawshaw et al., 2022; Zhang et al., 2019).\nUsing these assumptions, we establish the complexity bounds for the stochastic setting, where \\( \\zeta_0 \\in (0, 1) \\) controls the range of learning rates as in the deterministic setting.\nTheorem 4.9 (Stochastic setting). Assume \\( \\delta_D < \\text{min}(\\Lambda_G, \\Lambda_P)/3 \\). Then, the iteration complexities in stochastic settings are bounded as follows.\nFor the gradient-based sequence, suppose that \\( \\epsilon < \\frac{(1+\\sigma_2)^2 \\Lambda_G}{4(1+\\sigma_3) \\rho_H \\sqrt{P}} \\) holds and that the learning rate at time t satisfies\n\\eta_t = \\zeta_t \\text{min}(\\frac{1}{(1+\\sigma_2)\\Lambda_G}, \\frac{1}{2\\sqrt{(1+\\sigma_3)} \\rho_H ||\\nabla L(\\theta_t^{\\text{Grad}})||_2}), where \\( \\zeta_t \\in [\\zeta_0, 1] \\), we have\n\n\\mathcal{T}_\\epsilon(({ \\theta_t^{\\text{Grad}}}_{t=0}^\\infty, L, || \\cdot ||_2) \\leq \\frac{12(1 + \\sigma_2)^2 (L(\\theta_0) - L^*)}{\\rho_\\epsilon^2 \\zeta_0} \\Lambda_G\n\nFor the sign-based sequence, suppose that \\( \\epsilon < \\frac{\\Lambda_P}{\\rho_H \\frac{P}{P}} \\) and \\( \\sigma_2 \\leq \\frac{1}{4} \\) hold and that the learning rate at time t satisfies\n\\eta_t = \\zeta_t \\text{min}(\\frac{1}{(1 + 24 \\sigma_2) \\frac{P}{P} ||\\nabla L(\\theta_t^{\\text{Sign}})||_1}, \\frac{1}{\\rho_H \\frac{P}{P^{3/2}} ||\\nabla L(\\theta_t^{\\text{Sign}})||_1}), where \\( \\zeta_t \\in [\\zeta_0, 1] \\), we have\n\n\\mathcal{T}_\\epsilon(({ \\theta_t^{\\text{Sign}}}_{t=0}^\\infty, L, || \\cdot ||_1) \\leq \\frac{12(1 + 24 \\sigma_2) (L(\\theta_0) - L^*)}{\\rho_\\epsilon^2 \\zeta_0} \\Lambda_P\n\nProof of gradient-based sequence. The update rule of the gradient-based sequence in stochastic setting is \\( \\theta_{t+1}^{\\text{Grad}} = \\theta_t^{\\text{Grad}} - \\eta_t \\nabla \\widehat{L}(\\theta_t^{\\text{Grad}}) \\). Thus, we obtain:\n\\begin{aligned}\n\\mathbb{E} [L(\\theta_{t+1}^{\\text{Grad}}) - L(\\theta_t^{\\text{Grad}}) | \\theta_t^{\\text{Grad}}] &= \\mathbb{E} [\\nabla L(\\theta_t^{\\text{Grad}})^T (\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}}) + \\frac{1}{2} (\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}})^T \\nabla^2 L(\\theta_t^{\\text{Grad}}) (\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}}) + \\frac{\\rho_H}{6} ||\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}}||_2^3 | \\theta_t^{\\text{Grad}}] \\\\\n&\\leq \\mathbb{E} [\\nabla L(\\theta_t^{\\text{Grad}})^T (\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}}) + \\frac{1}{2} (\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}})^T \\nabla^2 L_D(\\theta_t^{\\text{Grad}}) (\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}}) + \\frac{\\delta_D}{2} ||\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}}||_2^2 + \\frac{\\rho_H}{6} ||\\theta_{t+1}^{\\text{Grad}} - \\theta_t^{\\text{Grad}}||_2^3 | \\theta_t^{\\text{Grad}}]\n\\end{aligned}\n(From Lemma C.1)"}, {"title": "Stochastic setting", "content": "This theorem shows that the dependence on the noise is the same for the both sequences up to a constant", "as": "n\\begin{aligned}\nJ_{\\text{Pre-LN}} &= J_{\\text{FFN}} (J_{\\text{LN}} + I_d) J_{\\text{ATT}} (J_{\\text{LN}} + I_d) \\\\\nJ_{\\text{Post-LN}} &= (J_{\\text{LN}}J_{\\text{FFN}} + I_d) (J_{\\text{LN}}J_{\\text{ATT}} + I_d),\n\\end{"}]}