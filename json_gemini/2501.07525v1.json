{"title": "RadAlign: Advancing Radiology Report Generation with Vision-Language Concept Alignment", "authors": ["Difei Gu", "Yunhe Gao", "Yang Zhou", "Mu Zhou", "Dimitris Metaxas"], "abstract": "Automated chest radiographs interpretation requires both accurate disease classification and detailed radiology report generation, presenting a significant challenge in the clinical workflow. Current approaches either focus on classification accuracy at the expense of interpretability or generate detailed but potentially unreliable reports through image captioning techniques. In this study, we present RadAlign, a novel framework that combines the predictive accuracy of vision-language models (VLMs) with the reasoning capabilities of large language models (LLMs). Inspired by the radiologist's workflow, RadAlign first employs a specialized VLM to align visual features with key medical concepts, achieving superior disease classification with an average AUC of 0.885 across multiple diseases. These recognized medical conditions, represented as text-based concepts in the aligned visual-language space, are then used to prompt LLM-based report generation. Enhanced by a retrieval-augmented generation mechanism that grounds outputs in similar historical cases, RadAlign delivers superior report quality with a GREEN score of 0.678, outperforming state-of-the-art methods' 0.634. Our framework maintains strong clinical interpretability while reducing hallucinations, advancing automated medical imaging and report analysis through integrated predictive and generative AI. Code is available at https://github.com/difeigu/RadAlign.", "sections": [{"title": "1 Introduction", "content": "Medical image interpretation and report generation play a vital role in the clinical workflow that can directly impact disease characterization and patient care [14]. The accurate interpretation of chest radiographs remains a critical task in medical image assessment [16], where clinicians must recognize subtle abnormalities and translate these observations into precise disease classifications and detailed reports. Accomplishing this complex task requires systematic efforts to capture a detailed state of the disease and to generate comprehensive, well-reasoned explanations of these findings [24]."}, {"title": "2 Methodology", "content": "Fig. 2 illustrates the proposed RadAlign framework. We present the detailed steps of RadAlign, which involve querying knowledge as diagnostic criteria, align-"}, {"title": "2.1 Domain Knowledge Query", "content": "Taking inspiration from human experts' diagnoses, they observe different criteria that would help with the diagnosis through findings, then make informed judgments on the patient's condition. We first extract diagnosis criteria through data-mining from the human expert findings. Let $D = \\{(x, P,y)\\}$ be a set of training image-findings-label pairs, where $x$ is the image, $P$ is the ground truth findings from human experts, and $y \\in Y$ is a label from a set of $N$ disease classes. We use the training finding set $P = \\{P_1, P_2, ..., P_D\\}$ and prompt LLM for a criteria extraction procedure $f_e: P \\rightarrow C$, such that we get a $K$ disentangled criteria axis set $\\{C_i\\}_{i=1}^K$. For instance, in the case of chest x-ray, the criteria axis includes Heart Size, Lung Opacity, Diaphragm Position, Presence of Fluid, Borders of Cardiac/Mediastinal Silhouette. Subsequently, we query detailed knowledge on each of the criteria for each disease class with $C_i = \\{C_{i1}, C_{i2}, ..., C_{in_i}\\}$, where $1 < n_i < N$ with each containing descriptions about the criteria regarding each of the $N$ diseases. For example, the criteria axis Heart Size contains the description of 'Enlargement of the heart silhouette' for the disease Cardiomegaly, but 'Does not affect the heart size' for pneumonia, and 'May or may not affect heart size depending on the cause' for the other diseases. Additionally, we construct a mapping of disease ground truth for each of the concept descriptions $f_m : C \\rightarrow V$, i.e., for each concept axis, the description is paired with one or more disease classes. For instance, both normal and Cardiomegaly instances should have no influence on fluid accumulation. Therefore, they are both mapped to the 'No fluid accumulation' description."}, {"title": "2.2 Visual Concept Fine-grained Alignment", "content": "After constructing the lists for the diagnostic criteria axis, we aim to leverage its complex conceptual relationships for a more fine-grained alignment between visual features and concept features. Given a pretrained vision-language model containing a visual encoder $V$ and a textual encoder $T$, we construct embedding for the diagnostic criteria. The textual criteria are initially encoded into criteria embedding anchors, $\\{e_i = T(C_i)\\}_{i=1}^K$, where $e_i \\in \\mathbb{R}^{n_i \\times d}$, and $d$ is the dimension of the embedding. Intuitively, these textual embedding represents the sparse human knowledge that serves as anchors to facilitate informative vision feature learning and alignment.\nTo capture the visual concept, the visual concept learning module applies a set of $K$ learnable visual concept tokens $z \\in \\mathbb{R}^{K \\times d}$. Given the image $x$ and the feature map $V(x)$, We use a cross-attention module to capture the nuanced features from a given image:\n$z = \\text{cross-attention}(z, V(x), V(x)),$ \nWhere $z$ is the query, and $V(x)$ is the key and value. The idea is for each of the $K$ visual concept tokens to represent a criteria axis and capture a specific visual feature on the image that aligns with the concept.\nWe facilitate the learning of the visual encoder and visual concept tokens using domain-specific contrastive loss. For each criteria axis, we aggregate the concept tokens $z$ and compare them against the corresponding criteria embedding anchors $e_i$ and compute a similarity score. The domain-specific contrastive loss is formulated as follows:\n$L_{\\text{anchor}}(z_i, e_i) = - \\log \\frac{\\exp(\\text{sim}(z_i, e_{\\text{positive}})/\\tau)}{\\sum_{j=1}^{n_i} \\exp(\\text{sim}(z_i, e_j)/\\tau)},$\nwhere $z_i$ is the visual concept tokens, $e_i$ is the criteria embeddings, and $\\tau$ is the temperature parameter that adjusts the softness of the softmax distribution. We use dot product for cosine similarity. The utilization of the contrastive loss brings similar concepts $z$ and $e_i$ together and pushes away dissimilar concepts. This process ensures more fine-grained learning from the model and helps become more discriminative when identifying the image. To do this, we optimize a joint objective comprising a criteria anchor contrastive loss with cross-entropy loss for the classification:\n$L_{\\text{total}} = L_{ce}(Y, \\hat{Y}) + \\frac{1}{K} \\sum_{i=1}^K L'_{\\text{anchor}}(z_i, e_i)$"}, {"title": "2.3 Knowledge Guided Prompting", "content": "LLMs have a limited ability to make diagnostic decisions with images. Text-based prompting lacks sufficient instructions to guide the LLM in generating high-quality radiology reports. As a result, the LLM hallucinates, producing outputs"}, {"title": "2.4 Image Based Report Retrieval Augmentation", "content": "To further enhance the quality and relevance of the generated reports, we implement an image feature-based RAG pipeline. The purpose of this implementation is analog to a novice practitioner learning how to write the report from both the structure and the content of pre-existing reports of similar tasks. We construct a report database of training images of the following form:\n$Q = \\{(z_i, P_i)\\}_{i=1}^D$ \nWhere $(z_i, P_i)$ is a key-value pair where $z_i$ is the visual concept token for the training image $x_i$, and $P_i$ is the report for the same corresponding image. We precompute and store the visual concept tokens to minimize inference overhead.\nTop-K retrieval is a widely used technique in information retrieval. For each image $x \\in D$, its visual concept token is matched with each training visual concept token in $Q$ using cosine similarity. The reports associated with the top-K matched tokens are retrieved as follows:\n$P_{\\text{retrieve}} = Q(z_i, z_i \\in \\text{TopK}_{z \\in \\{z_1, z_2, \\ldots, z_{|D|}\\}} \\text{sim}(z_i, V(x)))$"}, {"title": "3 Experiment and Results", "content": null}, {"title": "3.1 Experimental Setup", "content": "Dataset. We used MIMIC-CXR [12] for a comprehensive evaluation of RadAlign. The dataset contains 377,100 chest X-ray images, including both frontal and lateral chest views. X-rays are stored in JPEG format and with a typical image resolution ranging from 1000 \u00d7 1000 pixels. The dataset includes radiology reports for the x-rays, which is a set of descriptions of findings, impressions, and patient history. The dataset provides classes of labels of common findings such"}, {"title": "3.2 Main Results", "content": "Report Generation Comparison. We evaluate our model using GREEN Score, a metric specifically designed for assessing medical report generation by leveraging LLM-based reasoning to identify clinically significant errors. Traditional metrics such as BLEU [19], ROUGE [15], and BERTScore [32] are inadequate for medical report evaluation as they only measure surface-level text similarity without considering factual correctness - a critical requirement in clinical contexts where accurately distinguishing between presence and absence of conditions is essential. GREEN offers both quantitative scores and interpretable explanations that align well with expert judgment, as validated through comparisons with medical professionals. For implementation details, we refer readers to the original GREEN Score paper [18].\nWe present all of the six error notations from the GREEN score evaluations in Table 1, these including (a) False report of a finding in the candidate, (b) Missing a finding present in the reference, (c) Misidentification of a finding's anatomic location/position, (d) Misassessment of the severity of a finding, (e) Mentioning a comparison that isn't in the reference, and (f) Omitting a comparison detailing a change from a prior study.\nOur experiments demonstrate RadAlign's superior performance across multiple metrics. Using GPT-40, RadAlign achieves a GREEN score of 0.678, substantially outperforming the baseline methods (0.634). The improvement is particularly evident in error metrics (b), (c), (e), and (f), indicating better handling of comparative statements while maintaining robust clinical finding identification. The results also reveal distinct scaling behaviors between methods. While Chat-CAD shows minimal improvement when upgrading from GPT-40 mini to GPT-40 (0.001 increase), RadAlign demonstrates significant performance gains (0.648"}, {"title": "3.3 Ablation Studies", "content": "Evaluation with different LLMs. To evaluate the generalizability of our approach, we test RadAlign with various LLMs including ChatGPT (3.5-Turbo,"}, {"title": "3.4 Concept Interpretation", "content": "RadAlign enables interpretation of its decision-making process through visualization of concept token attention weights, demonstrating disease-specific localization patterns that align with clinical expertise. As shown in Fig. 6, the attention heatmaps highlight anatomically relevant regions for each condition. For example, for disease class Atlectasis (AT), the heatmap highlights specific areas around the edge of the lung fields that are indicative of abnormalities, while for class Cardiomegaly (CM), attention is drawn to distinct location at the heart region. These visualizations not only validate that our concept tokens successfully capture clinically meaningful features, but also provide radiologists with transparent insight into the model's reasoning process. Such interpretability helps verify that the model's decisions are based on relevant medical features rather than spurious correlations, making the automated analysis more trustworthy for clinical radiology workflows."}, {"title": "4 Conclusion", "content": "In this paper, we present RadAlign, a novel framework that addresses critical challenges in automated radiology report generation by combining the strengths"}]}