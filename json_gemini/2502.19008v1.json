{"title": "Binary Neural Networks for Large Language Model: A Survey", "authors": ["Liangdong Liu", "Zhitong Zheng", "Cong Wang", "Tianhuang Su", "Zhenyu Yang"], "abstract": "Large language models (LLMs) have wide applications in the field of natural language process- ing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization pa- rameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specif- ically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of large language models (LLMs) in the field of natural language processing (NLP) [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], these models have demonstrated exceptional performance in language generation, text understanding, and task reasoning. However, along with the exponential growth in model parameter sizes, LLMs also introduce significant resource overheads, including high memory usage, computational complexity, and increased energy consumption, which present substantial challenges for the deployment and practical application of these models. To address these challenges, low-bit quantization has emerged as a crucial technique for enhancing the efficiency and deployability of LLMs. Most previous research in this field has primarily focused on Post-Training Quantization (PTQ) [12, 13, 14] and Quantization-Aware Training (QAT) [15, 16]. PTQ allows a trained FP32 model to be directly converted into a fixed-point computation model without requiring retraining of the original model. QAT, on the other hand, involves quantizing a trained model and then retraining it.\nIn this paper, we primarily focus on binary quantization techniques, with particular emphasis on the BitNet [17] method, which first implemented a binary quantization approach for large language models. This method is distinct from PTQ and QAT because BitNet performs quantization from the outset of model training, achieving high energy efficiency in training and inference through binary weights. Since the introduction of this novel approach, many studies [18, 19, 20, 21, 22] have explored ways to improve the accuracy of binary large language models and how to implement them on low- power, resource-constrained platforms. This paper provides a comprehensive review of these studies.\nIn this paper, we will provide a detailed explanation of the concept of binarization and its applica- tions across various domains. We will also thoroughly explain the development of binary quantization techniques from traditional deep neural networks to the field of LLMs, with a particular focus on the research on binary quantization in large language models."}, {"title": "2 Background", "content": "Previous quantization methods for large language models (LLMs) include PTQ [12, 13, 14] and QAT [15, 16]. However, both methods suffer from significant precision loss at lower bit widths. Yi Guo et al. [23] demonstrated the results of various PTQ methods on the Llama-1/2 models using the WikiText-2 dataset, as shown in Table 1."}, {"title": "3 Binary Neural Networks for Convolutional Neural Networks", "content": "In the field of deep neural networks (DNNs), many studies have focused on making models smaller and faster without significantly sacrificing accuracy [27, 28, 29, 30, 31]. In theory, smaller data types not only reduce model size but also improve computational speed, as fixed-point operations are significantly more efficient than floating-point operations. Gupta et al. [32] pointed out that reducing the precision of data types in DNNs can decrease model size with an acceptable impact on accuracy. Courbariaux et al. [33] compared the training accuracy of DNNs using fixed-point and floating-point values of various bit-widths. They also examined the effectiveness of a hybrid dynamic fixed-point data type, demonstrating that comparable accuracy can be achieved with precision below 32 bits.\nCourbariaux et al. [34, 35] pioneered the BNN methodology, which forms the foundation and starting point for most BNN-CNN algorithms and research in the field. Subsequently, a variety of binary neural network algorithms have been proposed [36, 37, 38, 39, 40, 41, 42, 43, 35, 24, 44, 45, 24, 40, 46, 47, 48, 49, 50, 51, 52, 53, 54]. Among them, BinaryConnect (BC) [36] is considered the standard. BC binarizes weights using the sign function during forward propagation and uses the Straight-Through Estimator (STE) to evaluate the gradients of the binarized weights. This idea has also been widely adopted in the LLM field, which we will introduce in Chapter 4. Based on BC, Dockhorn et al. [43] further proposed ProxConnect (PC), which extends BC by allowing the use of proximal quantizer during forward propagation, with the sign function being a special case. Additionally, Courbariaux et al.'s paper [34] first introduced the method of binarizing activation values. Darabi et al. [55] further improved BNN by using the derivative of the Sign-Swish (SS) function as the backward quantizer, while retaining the sign function as the forward quantizer. BNN++ [24] is an extension of BNN+ [55], where the sign forward quantizer is replaced with the Sign-Swish (SS) function, further optimizing the binary neural network algorithm. A comparison of the forward and backward quantizers of various classic binary neural network algorithms is shown in Figure 1 and Table 4.\nAs can be seen, binary quantization technology has already matured in the traditional CNN field. Therefore, in the LLM field, after researchers noticed that traditional PTQ and QAT methods suffer from significant precision loss when the bit width is low, such as at 2 bits, they shifted their focus towards the 1-bit quantization technology trained from scratch. In the following section, we will provide a detailed review of these studies."}, {"title": "4 Binary Neural Networks for Large Language Models", "content": "Previous quantization techniques for large language models existed [56, 13, 57, 58, 59, 60, 61, 62, 63, 64, 65, 65, 66, 67, 12, 68, 13, 69, 70, 15], but unlike these techniques, Hongyu Wang et al. [17] drew inspiration from algorithms in BNN-CNN and pioneered the application of binarization to large language models, introducing BitNet, which forms the foundation and starting point for most of the research on binary quantization of large language models. In this chapter, we will focus on reviewing the binarization techniques specifically for large language models."}, {"title": "4.1 The Pioneering Work in BNN-LLM: BitNet", "content": "BitNet, proposed by Hongyu Wang et al. [17], is a 1-bit Transformer architecture for large language models that achieves efficient scaling in terms of both memory and computation. BitNet introduces a novel linear projection layer, BitLinear, which replaces the standard Linear layer in the Transformer model to binarize the weights. Specifically, BitLinear substitutes traditional matrix multiplication with a 1-bit weight version.\nFollowing the principles of BNNs, BitLinear first binarizes the weights using the sign function, assigning them values of +1 or -1. In line with the ideas of Zechun Liu et al. [71], Hongyu Wang et al. [17] adjust the mean of the weights to zero before binarization in order to enhance the capacity within the limited numerical range. At this point, the binarization process of a weight matrix $W$ can be represented as:\n$W = \\text{Sign}(W - \\alpha)$  (1)\nHere:\n$\\text{Sign}(W_{ij}) = \\begin{cases} +1, & \\text{if } W_{ij} > 0, \\\\ -1, & \\text{if } W_{ij} \\leq 0, \\end{cases}$  (2)\n$\\alpha = \\frac{1}{nm} \\sum_{ij} W_{ij}$  (3)\nFollowing the method of Tim Dettmers et al. [72], Hongyu Wang et al. used absolute max- imum (Absmax) quantization to binarize the activation values to b-bit precision. To maintain the variance unchanged after quantization, they introduced the LayerNorm [73] function before activation quantization. At this point, the estimated variance of the output $y$ is given by\n$\\text{Var}(y) \\approx E[\\text{LN}(X)^2] = 1,$  (4)\nwhich ensures that its magnitude is the same as the variance of the full-precision counterpart. This implementation is equivalent to the SubLN [74] in transformers. The activation values are scaled by multiplying by $Q_b$ and dividing by the maximum value of the input matrix, ensuring that the activation values fall within the range $[-Q_b, Q_b]$. The formula is as follows:\n$\\tilde{x} = \\text{Quant}(x) = \\text{Clip} \\left( \\frac{x}{\\gamma} \\times Q_b, -Q_b + \\epsilon, Q_b \\right)$  (5)\n$\\text{Clip}(x, a, b) = \\text{max} (a, \\text{min}(b, x)), \\quad \\gamma = ||x||_\\infty$  (6)\nHere, $\\epsilon$ is a small floating-point number, which is used to prevent overflow during the clipping operation. For activation values before nonlinear functions (e.g., ReLU), they are scaled to the range $[0, Q_b]$. The formula for BitLinear defined at this point is as follows:\n$y = \\tilde{W}x = \\tilde{W} \\text{Quant}(\\text{LN}(x)) \\times \\frac{\\beta \\gamma}{Q_b}$  (7)\n$\\text{LN}(x) = \\frac{x - E(x)}{\\sqrt{\\text{Var}(x) + \\epsilon}}, \\quad \\beta = \\frac{1}{nm} ||\\tilde{W}||_1$  (8)\nBitNet performs 8-bit quantization on the activation values, with quantization applied to each tensor during training and to each token during inference, ensuring both stability and efficiency.\nHongyu Wang et al. [17] proposed dividing the weights and activations into multiple groups and independently computing the parameters for each group. This approach allows for local computation of parameters without the need for additional communication operations. The formula is as follows:\n$\\alpha_g = \\frac{G}{nm} \\sum_{ij}^{G} W_{ij}^{(g)}, \\quad \\beta_g = \\frac{G}{nm} ||W_{ij}^{(g)}||_1$  (9)\nHere, $W_{ij}^{(g)}$ represents the weights of the g-th group. Similarly, for the activation values, we can divide the input matrix $x \\in \\mathbb{R}^{n \\times m}$ into G groups and compute the parameters for each group as follows:\n$\\gamma_g = ||x^{(g)}||_\\infty, \\quad \\eta_g = \\text{min} x_{ij}^{(g)}$  (10)\nIn summary, the schematic diagram of BitLinear is shown in Figure 2."}, {"title": "4.2 Optimized Binary Neural Networks for Large Language Models", "content": ""}, {"title": "4.2.1 Optimization of Weight Quantization Methods", "content": "Similar to the weight quantization idea in BitNet, FBI-LLM [21] and Bi-Mamba [22] also attempt to use the Sign function to binarize the weights to +1 or -1. However, the BitNet network proposed by Hongyu Wang et al. [17] adjusts the binarized weights with a scaling factor \u03b2, whereas FBI-LLM and Bi-Mamba introduce column-based learnable scaling factors $a_i$ and $\u03b2_i$ to reduce the error between the binarized weights and the full-precision ones. BitNet, FBI-LLM, and Bi-Mamba all attempt to binarize the weights, while BitNet b1.58 [18] uses absolute mean (absmean) quantization to ternarize the weights to {-1,0,1}. BitNet b1.58 points out that its modeling capability is enhanced by its explicit support for feature filtering, which is enabled by incorporating 0 values in the model weights, thereby significantly boosting the performance of 1-bit LLMs.\nBitNet a4.8 [19] adopts the same architecture as BitNet b1.58, learning 1.58-bit weights from scratch, but with further compression applied to the activations, which we will discuss in subsequent chapters. Based on the above description, the comparison of weight quantization across different networks is shown in Figure 3, while a specific example related to this comparison is presented in Figure 4."}, {"title": "4.2.2 Optimization of Activation Quantization Methods", "content": "In Section 4.1, we provided a detailed explanation of the activation method in BitNet [17]. While BitNet b1.58 [18] quantizes the weights to three values, it also makes certain changes to the activations. Specifically, BitNet scales the activation values to the range [0, $Q_b$] before the nonlinear function, while BitNet b1.58 scales the activations to [-$Q_b$, $Q_b$] for each token. This design helps avoid zero-point quantization. The activation values in both BitNet and BitNet b1.58 are 8 bits.\nIn contrast, BitNet a4.8 [19] introduces several changes to activation quantization. The BitNet a4.8 team incorporates hybrid quantization and sparsification strategies to support 4-bit activations in 1-bit LLMs, addressing quantization errors. They use 4-bit activations for the inputs to the attention and feed-forward networks while sparsifying intermediate states and quantizing them to 8 bits. BitNet a4.8 is trained from 8-bit activations to 4-bit activations using a two-stage training strategy. Compared to BitNet b1.58, BitNet a4.8 achieves faster inference speeds."}, {"title": "4.2.3 KV Cache Quantization", "content": "During large model inference, the model's memory usage is primarily determined by the model weights, activations, and KV cache. KV cache quantization is mainly divided into two categories: full quantization [75, 76] and KV cache-only quantization [77, 78, 67, 62, 64]. BitNet and BitNet b1.58 focus solely on quantizing the weights and activations, while FBI-LLM and Bi-Mamba focus only on weight quantization, without considering KV cache quantization. However, BitNet a4.8 [19], proposed by the BitNet team, explicitly states that it supports 3-bit KV cache. Specifically, BitNet a4.8 uses post-RoPE quantization. Their QKV heads are directly quantized to unsigned integers using the absmax function, without the need for a calibration dataset. The schematic diagram of KV cache quantization in BitNet a4.8 is shown in Figure 6."}, {"title": "4.2.4 Improved Network Loss Function", "content": "Minimizing quantization errors typically attempts to retain the values of full-precision weights and activations, thereby reducing the information loss at each layer. However, focusing solely on the precise approximation of local layers makes it difficult to guarantee the accuracy of the output after passing through a series of layers. As a result, many researchers have dedicated efforts to finding the ideal network loss function [79, 80, 81, 82, 83]."}, {"title": "4.3 Training Binary Neural Networks for Large Language Models", "content": ""}, {"title": "4.3.1 Optimization of Gradient Calculation Methods", "content": "BitNet [17], BitNet b1.58 [18], BitNet a4.8 [19], FBI-LLM [21], and Bi-Mamba [22] all attempt to use the Straight-Through Estimator (STE) to address gradient issues. However, inspired by [84], Will Brickner et al. [20] proposed a new algorithm called noise_step, which enables the model to be trained directly at ternary precision without the need for backpropagation or momentum mecha- nisms, while still achieving performance comparable to existing methods, such as the Adam optimizer. Their method highlights that traditional training depends on gradient memory for parameter updates, whereas noise_step estimates gradients using pseudo-random disturbances and the Jacobian Vector Product (JVP), without the need to store or transmit gradient information. For distributed train- ing, the synchronization of gradients and optimizers is typically a performance bottleneck. noise_step greatly reduces communication costs by encoding each disturbance using ternary symbols (only 1.58 bits)."}, {"title": "4.3.2 Mixed Precision Training", "content": "BitNet [17] utilizes low-precision weights and activations, while gradients and optimizers are still stored in high precision. This approach ensures the stability and accuracy of the training process. FBI-LLM [21] replaces all linear modules except for the causal head with FBI-linear, because the causal head directly affects the distribution of the output token at each step. If the causal head were binarized, it would significantly impact the accuracy of the model's output. Thus, it is necessary to maintain its precision. Additionally, for two core modules in LLMs-embedding and layer norm-the parameters must be kept in full precision. The embedding module contains semantic information for all tokens, and since it is the first layer of the model, it determines the initial representation of the text, so it cannot be binarized. Layer norm directly scales the activation values, and binarizing its parameters would significantly reduce the semantic expressiveness of each layer's activations. This approach aligns with the philosophy of the BitNet team."}, {"title": "4.3.3 Learning Rate Selection", "content": "For both BNN-CNN and BNN-LLM, the choice of learning rate is different. For example, Hongyu Wang et al. [17] discovered that increasing the learning rate can accelerate optimization, and BitNet performs well under a high learning rate. Bi-Mamba [22], in binarized models, accelerates the training process by gradually increasing the learning rate. However, it is worth noting that Jacob Nielsen et al. [85] argue that for small classification models, whether using 1.58-bit or 16-bit weights, larger learning rates do not perform well, even when training from scratch. On the contrary, a smaller learning rate results in better performance. Tang et al. [40] suggested that using a smaller learning rate improves the training results of BNNs. The key reason appears to be that large language models have a very large number of parameters, and after binarization, the granularity of each weight update becomes coarser. If a smaller learning rate is set, training will become slower. However, for small language models or CNNs, their parameter scale is smaller, so only smaller learning rates can be used to avoid gradient explosion."}, {"title": "4.4 Differences Between BNN-LLM and BNN-CNN", "content": "Binarization techniques have yielded satisfactory results in both CNNs and LLMs, but there are certain differences. First, binarized training in CNNs requires multiple epochs to achieve stable convergence, while binarization in LLMs typically requires only one epoch. Secondly, as mentioned in Section 4.3.3, their learning rate settings are different. Finally, there are differences in computational efficiency and deployment. BNN-CNN typically leverages XNOR operations and popcount instructions, significantly accelerating dot product computations, making it suitable for edge devices such as FPGAs and ASICs. In contrast, BNN-LLM uses mixed precision training, so even though the weights are binarized, the Transformer mechanism still requires processing higher-precision inputs and outputs. This makes it more suited for distributed computing environments, such as GPU clusters and TPU architectures."}, {"title": "5 Evaluation and Discussion", "content": ""}, {"title": "5.1 Comparison of BitNet with PTQ Accuracy", "content": "Hongyu Wang et al. [17] compared BitNet with commonly used post-training quantization meth- ods, such as Absmax [72], Smooth Quant [14], GPTQ [13], and QuIP [86]. These post-training quan- tization methods were applied to the same FP16 Transformer model, with training settings and data kept consistent with BitNet. The BitNet team provided the table, showcasing the comparison between BitNet and PTQ, as shown in Table 5."}, {"title": "5.2 Comparison of Accuracy in Other Binarized LLM Techniques", "content": "The BitNet a4.8 team [19] also compared the results of BitNet a4.8 [19], BitNet b1.58 [18], and LLAMA LLM on downstream tasks, as shown in Table 7."}, {"title": "5.3 Discussion", "content": "We observed an interesting phenomenon. Referring to the methodology of PC++ [24] in the BNN-CNN domain, we initially assumed that further smoothing of the gradient in BNN-LLMs would guarantee further convergence of the algorithm. However, in practice, we found that when using the Hard Tanh activation, due to the high curvature at the inflection point, even with a small learning rate, large gradients still appear during backpropagation, leading to non-convergence. On the other hand, using STE allows for convergence. Therefore, for BNN-LLMs, merely smoothing the gradient does not guarantee convergence. Additional conditions are required to ensure convergence, which calls for further research.\nFurthermore, from-scratch quantization training in BNN-LLMs is regarded as a potential regu- larization technique in our view. This is because quantization implicitly introduces regularization by constraining the weight space, such as {-1,+1} or {\u22121,0,1}, which is equivalent to adding a penalty term in the loss function.\n$\\underset{W}{min} l(w) + \\lambda \\cdot ||w - Q(w)||^2$  (12)\nHere, $Q$ is the quantization function, such as Sign, and $\\lambda$ controls the strength of regularization. Our experiments further support this conclusion. We conducted experiments on BitNet [17] 1.8B with input text sequences ranging from 2048 to 8192 in length. The results, shown in Figure 7, reveal that the baseline model exhibits divergent loss behavior, while BitNet's loss continuously decreases. Therefore, it can be inferred that from-scratch quantization training may serve as a regularization technique."}, {"title": "6 Conclusion and Future Trends", "content": ""}, {"title": "6.1 Summary of Binarization Application Areas", "content": "The concept of binarized weights predates the rise of deep learning [87]. However, early binarized networks only included a single hidden layer [87, 88]. In these early studies, weights could not be updated through small increments, making backpropagation ((BP) and stochastic gradient descent (SGD) unsuitable for these networks. Early binarization research mainly relied on variants of Bayesian inference. Later, Courbariaux proposed the BinaryConnect [36] method, which was applied to deep neural networks (DNNs). This led to the development of several algorithms, such as PC [43], BNN [35], BNN+ [55], BNN++ [24], and others. For large language models, Hongyu Wang et al. [17] drew inspiration from BNN-CNN and proposed BitNet. Based on this, numerous algorithms were born, including BitNet b1.58 [18], BitNet a4.8 [19], FBI-LLM [21], Bi-Mamba [22], and others. Given the success of BitNet b1.58, Jainaveen Sundaram et al. [26] attempted to apply the BitNet b1.58 idea to the multimodal domain. Specifically, they built LLaVaOLMoBitNet1B, a multimodal large language model based on the ternary OLMoBitNet1B [89] and LLaVa [90] methods. Compared to other multimodal large language models, they achieved the smallest number of parameters and the fewest pretraining tokens compared to full-precision counterparts, providing a baseline for future development of more powerful ternary multimodal models. Additionally, Jacob Nielsen et al. [85] proposed a variant of BitNet b1.58 called BitNet 1.58 Reloaded and applied it to small language models (SLMs) and vision models. They concluded that their method achieves near-state-of-the-art performance on small language models and surpasses state-of-the-art performance on vision models. Yiwei Lu et al. [24] also applied this binarization technique to the Vision Transformer (VIT) domain, significantly reducing overhead. For example, they reduced the memory requirement of VIT-B from 450MB to 15MB, achieving approximately 30 times compression. For VIT, BNN++ [24] shows significant advantages on most tasks."}, {"title": "6.2 Future Research Trends", "content": "In the subsequent research on BNN-LLM, it is anticipated that the future research trends will focus on the following: (1) Further reducing training costs, as we observed that BitNet b1.58 training is 60% slower than the baseline and still relatively costly. (2) Further improvement of FBI-LLM. We found that for FBI-LLM, removing distillation leads to a significant performance drop and even non-convergence, so further optimization of FBI-LLM will be a new research direction. (3) Currently, there is no research on binarization or ternary quantization techniques in the Jamba domain. Therefore, applying binarization techniques to the Jamba architecture would be a valuable research direction. Additionally, it is worth considering applying 1-bit quantization techniques to MOE (Mixture of Experts). (4) For activation quantization, the MatMul-free method can be referenced to further reduce training costs. (5) Dynamically adjust the quantization strategy for weights and activation values based on input data and task requirements. (6) Design dedicated hardware that supports integer operations (rather than floating-point operations) to reduce energy consumption. (7) Optimize the memory architecture (such as SRAM) to reduce data transfer bottlenecks between the main memory and the chip. (8) Design specific operators that support low-bit models (e.g., binarized weight matrix operations: {\u22121,1})."}, {"title": "6.3 Conclusion", "content": "In recent years, the technique of binarizing weights has seen extensive research and application in the field of deep learning. From deep neural networks to large language models, binarization has been widely adopted, with various technologies emerging, such as BNN-CNN and BNN-LLM. Many methods in BNN-CNN are capable of achieving significant model compression while maintaining accuracy close to that of full-precision models.\nIn the LLM domain, BitNet [17] and BitNet b1.58 [18] have significantly reduced storage and computational complexity through weight binarization and ternarization. FBI-LLM [21] successfully trained a fully binarized model through autoregressive distillation, demonstrating the feasibility of training from scratch. Meanwhile, Bi-Mamba [22] combined binarization with state space models, showing high computational efficiency in long-sequence modeling tasks.\nFrom the research of many scholars on binarization techniques, it is evident that as models continue to grow larger, applying binarization techniques to these large models is an effective strategy. Especially in the LLM domain, low-bit quantization models have become a key solution to addressing the high computational complexity and energy consumption bottlenecks of LLMs. These low-bit quantization models are expected to find even broader applications in the future, enabling the efficient deployment of large language models on mobile devices, edge computing, and cloud inference, thus providing robust technical support for the widespread adoption of artificial intelligence."}]}