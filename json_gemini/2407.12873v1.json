{"title": "Evaluation of RAG Metrics for Question Answering in the Telecom Domain", "authors": ["Sujoy Roychowdhury", "Sumit Soman", "HG Ranjani", "Neeraj Gunda", "Vansh Chhabra", "Sai Krishna Bala"], "abstract": "Retrieval Augmented Generation (RAG) is widely used to enable Large Language Models (LLMs) perform Question Answering (QA) tasks in various domains. However, RAG based on open-source LLM for specialized domains has challenges of evaluating generated responses. A popular framework in the literature is the RAG Assessment (RAGAS), a publicly available library which uses LLMs for evaluation. One disadvantage of RAGAS is the lack of details of derivation of numerical value of the evaluation metrics. One of the outcomes of this work is a modified version of this package for few metrics (faithfulness, context relevance, answer relevance, answer correctness, answer similarity and factual correctness) through which we provide the intermediate outputs of the prompts by using any LLMs. Next, we analyse the expert evaluations of the output of the modified RAGAS package and observe the challenges of using it in the telecom domain. We also study the effect of the metrics under correct vs. wrong retrieval and observe that few of the metrics have higher values for correct retrieval. We also study for differences in metrics between base embeddings and those domain adapted via pre-training and fine-tuning. Finally, we comment on the suitability and challenges of using these metrics for in-the-wild telecom QA task.", "sections": [{"title": "1. Introduction", "content": "Retrieval Augmented Generation (RAG) (Lewis et al., 2020) is one of the approaches to enable Question Answering (QA) from specific domains, while leveraging generative capabilities of Large Language Models (LLMs). There have been many techniques proposed to enhance RAG performance such as chunk length, order of retrieved chunks in context (Chen et al., 2023; Soman & Roychowdhury, 2024). However, like all systems, these require objective metrics to measure performance of the end-to-end system. The challenge in evaluating RAG system lies in comparing the generated answer with the ground truth for factualness, relevance to question and semantic similarity (Chen et al., 2024).\nInitial approaches for RAG evaluation included re-purposing metrics used for machine translation tasks such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) or METEOR (Banerjee & Lavie, 2005). Text generation was also evaluated using BERTScore (Zhang et al., 2019). Metrics from Natural Language Inference (MeNLI) (Chen & Eger, 2023) built an adversarial attack framework to demonstrate improvements over BERTScore. Classical methods used for evaluation such as Item Response Theory have also been explored for RAG evaluation (Guinet et al., 2024). The limitations with these techniques are: (i) they have a limited contextual input, (ii) can potentially look for either exact matches or semantic similarity aspects only, and (iii) are measured at sentence level only. RAG response evaluations, however, require a combination of exact match for factual component(s) and semantic similarities for relevance.\nIn an attempt to mimic human intuition in assessing logical and grounded conversations, metrics based on prompting LLMs to evaluate RAG outputs have been proposed. RAG Assessment (RAGAS) (Es et al., 2023) proposes multiple measures such as faithfulness, context and answer relevance to assess RAG responses using specific prompts. We use this framework for assessment as it is one of the first and has also been used in popular courses (Liu & Datta, 2024).\nOur work is motivated by the need to evaluate these metrics for RAG systems in technical domains; we focus on telecom as an example. Most of the prior art focuses on evaluation using public datasets (Yang et al., 2024). However, with increasing applications that use LLMs for telecom (Zhou et al., 2024; Karapantelakis et al., 2024; Soman & Ranjani, 2023), it is important to assess the robustness of these metrics in the presence of domain-specific terminology. Further, there can be potential improvements in the RAG pipeline, such as domain adaptation of the retriever or instruction"}, {"title": "1.1. Research Questions", "content": "The Research Questions (RQs) considered in this work are:\n\u2022 RQ1: How do LLM-based evaluation metrics, specifically RAGAS, go through the step-by-step evaluation procedure specified by the prompts?\n\u2022 RQ2: Are RAGAS metrics appropriate for evaluation of telecom QA task using RAG?\n\u2022 RQ3: Are RAGAS metrics affected by retriever performance, domain adapted embeddings and instruction tuned LLM.\nThe contributions of our work are as follows:\n1. We have enhanced the RAGAS public repository code by capturing the intermediate outputs of all the prompts used to compute RAGAS metrics. This provides better visibility of inner workings of the metrics and possibility to modify the prompts.\n2. We manually evaluate, for RQ1, the intermediate outputs of the considered metrics with respect to the context and ground truth. We critically analyse them for their appropriateness for RAG using telecom domain data.\n3. We establish, for RQ2, that two of the metrics - Factual Correctness and Faithfulness, are good indicators of correctness of the RAG response with respect to expert evaluation. We demonstrate that use of these two metrics together is better at identifying correctness of response; this improves further on using domain adapted LLMs.\n4. We establish, for RQ3, that Factual Correctness metric improves with instruction fine tuning of generator LLMs, irrespective of retrieved context. We observe lower Faithfulness metric for RAG answers which are identified to be correct but from wrong retrieved context. This indicates that the generator (LLM) has answered from out of context information. The ability to answer from out-of-context information is more pronounced for domain adapted generator. Thus, the metrics are able to reflect the expected negative correlation"}, {"title": "2. Experimental Setup", "content": "All experiments in this work are based on subset of Tele-QuAD (Holm, 2021), a telecom domain QA dataset derived from 3GPP Release 15 documents (3GPP, 2019). Our experimental setup is shown in Figure 1. The input to our pipeline is the QA dataset, which has contexts (from the 3GPP documents) along with associated questions and (ground truth) answers. These questions have been prepared by Subject Matter Experts (SMEs). The training and test data considered comprises of 5,167 and 715 QA, respectively, derived from 452 contexts (sections) from 14 3GPP documents. Appendix B shows a sample set of QAs along with the contexts."}, {"title": "2.1. Dataset", "content": "All experiments in this work are based on subset of Tele-QuAD (Holm, 2021), a telecom domain QA dataset derived from 3GPP Release 15 documents (3GPP, 2019). Our experimental setup is shown in Figure 1. The input to our pipeline is the QA dataset, which has contexts (from the 3GPP documents) along with associated questions and (ground truth) answers. These questions have been prepared by Subject Matter Experts (SMEs). The training and test data considered comprises of 5,167 and 715 QA, respectively, derived from 452 contexts (sections) from 14 3GPP documents. Appendix B shows a sample set of QAs along with the contexts."}, {"title": "2.2. Retriever Models", "content": "The RAG pipeline is comprised of a retriever followed by generator module. The retriever module is comprised of the following steps. Data from reference documents are chunked. An encoder-based language model computes embeddings for query and sentences from the reference documents. For every question embedding, retriever outputs top-k most similar sentence/context embeddings. Cosine similarity is used for selecting top-k sentences/contexts.\nWe evaluate multiple models in our experiments. From the BAAI family of embedding models, we consider bge-large-en (Xiao et al., 2023) and llm-embedder (Zhang et al., 2023), both with an embedding dimension of 1024. These models have been trained on publicly available datasets; hence, the embeddings may not be optimal for telecom domain. To address this, we also evaluate pre-trained and fine-tuned variants of these models using telecom data. We use sentences from the corpus of technical documents from telecom domain to pre-train (Li et al., 2020) the base model;"}, {"title": "2.3. Generator", "content": "The output of the retriever forms the input context to the generator. For evaluation, we only use k = 1 retrieved context, to study the behaviour of the generator when presented with correct and wrong contexts. Once the relevant context has been retrieved for a question from the retriever module, the query and context are passed to the LLM for generating the response, indicated as \"RAG Response\" in Figure 1. We have considered Mistral-7b (Jiang et al., 2023) and GPT3.5 as the LLMs for our experiments. We also report results on pre-trained (PT) and instruction fine-tuned (PT-IFT) variants of Mistral-7b using mistral-finetune (MistralAI, 2024)."}, {"title": "3. RAG Evaluation", "content": "We focus on the following metrics from the RAGAS framework (Es et al., 2023). Higher value is better for all of them.\n\u2022 Faithfulness ($\\mathit{FaiFul}$): Checks if the (generated) statements from RAG response are present in the retrieved context through verdicts; the ratio of valid verdicts to total number of statements in the context is the answer's faithfulness.\n\u2022 Answer Relevance ($\\mathit{AnsRel}$): The average cosine similarity of user's question with generated questions, using the RAG response the reference, is the answer relevance.\n\u2022 Context Relevance ($\\mathit{ConRel}$): The ratio of the number of statements considered relevant to the question given the context to the total number of statements in the context is the context relevance.\n\u2022 Answer Similarity ($\\mathit{AnsSim}$): The similarity between the embedding of RAG response and the embedding of ground truth answer.\n\u2022 Factual Correctness ($\\mathit{FacCor}$): This is the F1-Score of statements in RAG response classified as True Positive, False Positive and False Negative by the RAGAS LLM.\n\u2022 Answer Correctness ($\\mathit{AnsCor}$): Determines correctness of the generated answer w.r.t. ground truth (as a"}, {"title": "4. Results and Discussion", "content": "The retriever accuracies for various models for a range of k are reported in Table 1. The lower accuracy with PT alone is expected and has been discussed in the literature (Li et al., 2020). However, these improve significantly (p < 0.05 for a two-tailed t-test) on FT. We also evaluate with GPT 3.5 (ada-002) embeddings - however, security and privacy concerns limit domain adaptation options for GPT embeddings.\nThe results of the RAG evaluation are shown in Table 2. We include scores for the sub-components of AnsCor i.e., Answer Similarity (AnsSim) and Factual Correctness (FacCor); AnsCor is their weighted average with weights 0.25 and 0.75 respectively.\nWe note that the mean of the considered metrics for Retrieval correct='Yes' is greater than or equal to that for Retrieval correct=\u2018No' (validated by one-sided t-test, p < 0.05 for statistical significance). Next, we observe that for FaiFul and AnsCor, the results for PT and FT are similar to that of the base model (p > 0.05). The other metrics are not truly comparable (discussed in detail in Section 4.1). We observe that the metrics values reported using open source LLM and GPT3.5 are comparable. Instruction Fine Tuning of the generator improves the relevant metrics."}, {"title": "4.1. Discussion on Metrics", "content": "We discuss our findings about the four RAGAS metrics.\n\u2022 Faithfulness ($\\mathit{FaiFul}$) - intends to provide reliability scores with respect to human evaluation. Simple statements might be paraphrased into multiple sentences, while complex statements may not be fully broken down. These factors can introduce variation in the faithfulness score. Despite these challenges, we found that the $\\mathit{FaiFul}$ metric is generally concordant to manual evaluation.\n\u2022 Context Relevance ($\\mathit{ConRel}$) - is mainly indicative and dependent on the context length. Typically, chunks such as sections form the retrieved context in a RAG pipeline; hence, context can vary in length, which affects the denominator component of ConRel. Typical metrics are either \u201chigher is better\" (e.g., accuracy) or \"lower is better\u201d (e.g., mean squared error). Also, all statements are assigned equal weight, regardless of the length or quality of the sentence. Therefore, we infer Con Rel cannot be appropriately clubbed into either of these types and the final metric is hard to interpret or even have an intuition about.\n\u2022 Answer Relevance ($\\mathit{AnsRel}$) - The generated questions from the LLM in this metric may not be the best way to measure answer relevance. We have observed some cases where the generated questions are either trivial paraphrasing or incorrect. However, the major problem with this metric is that it tries to use cosine similarity as an absolute component of this metric. This makes the metric dependent on the choice of LLM. In addition, various studies on cosine similarity have pointed out that it may not be indicative of simi-"}, {"title": "5. Conclusions and Future Work", "content": "In this work, we enhance the current version of the RAGAS package for evaluation of RAG based QA - this helps investigate the scores by analysing the intermediate outputs. We focus our study using telecom domain QA. We critique AnsSim component of AnsCor, ConRel and AnsRel metrics for their lack of suitability as a reliable metric in an end-to-end RAG pipeline. A detailed analysis by SMEs of RAG output establishes that two of the metrics FacCor and FaiFul are suitable for evaluation purposes in RAG pipeline. We demonstrate that domain adaptation of RAG LLM improves the concordance of the two metrics with SME evaluations. Whilst our studies have been limited to telecom domain, some of our concerns especially around the use of cosine similarity would extend to other domains too.\nOur code repository presents the intermediate output of the RAGAS metrics, and possibilities for improvements in RAG evaluation across domains. A detailed study of other libraries dependent on RAGAS like ARES (Saad-Falcon et al., 2023) can also be considered in future."}, {"title": "Appendices", "content": "We refer the reader to (Es et al., 2023) for details on the metrics defined, but for the sake of completeness, the prompts involved and steps to determine the metrics in our study are provided for ease of reference. A summary is shown in Figure 2. The notation used is as follows: given question q and context c(q) retrieved (and possibly re-ranked) from a corpus, the LLM generates answer a(q). The ground truth answer for the question is denoted by gt(q)."}, {"title": "A. Computation of RAGAS Metrics", "content": "We refer the reader to (Es et al., 2023) for details on the metrics defined, but for the sake of completeness, the prompts involved and steps to determine the metrics in our study are provided for ease of reference. A summary is shown in Figure 2. The notation used is as follows: given question q and context c(q) retrieved (and possibly re-ranked) from a corpus, the LLM generates answer a(q). The ground truth answer for the question is denoted by gt(q)."}, {"title": "A.1. Faithfulness ($\\mathit{FaiFul}$)", "content": "By definition, answer a(q) is faithful to context c(q) \u201cif claims made in the answer can be inferred from the context\u201d. This is done using a two-step process. In the first step, the LLM is prompted to create sentences (statements S(q)) using the answer a(q) using the following prompt:\nGiven a question and answer, create one or more statements from each sentence in the given answer.\nquestion: [question]\nanswer: [answer]\nIn the next step, for each statement s \u2208 S(q), the LLM is asked to determine a binary verdict v(s, c(q)) using the context as part of the following prompt:\nConsider the given context and following statements, then determine whether they are supported by the information present in the context. Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format. Do not deviate from the specified format.\ncontext: [context]\nstatement: [statement 1]\nstatement: [statement n]"}, {"title": "A.2. Answer Relevance ($\\mathit{AnsRel}$)", "content": "Answer a(q) \u201cis relevant if it directly addresses the question in an appropriate way\u201d. To determine answer correctness, the LLM is used to generate questions \u1fb7 from the answer a(q) using the following prompt:\nGenerate a question for the given answer.\nanswer: [answer]\nFollowing this, the similarity of the N generated questions in \u1fb7 with the original question q is determined using a similarity function sim(\u00b7), that takes the embedding E(\u00b7) generated by a suitable model as input, and the average similarity score is reported as the answer relevance (AnsRel) using\n$\\mathit{Ans Rel} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{sim}(E(q), E(q_i))$\n(4)"}, {"title": "A.3. Context Relevance ($\\mathit{ConRel}$)", "content": "By definition, \u201cthe context c(q) is considered relevant to the extent that it exclusively contains information that is needed to answer the question.\u201d. This is accomplished by prompting the LLM to extract relevant sentences Sext from the question q and context c(q) using the following prompt:\nPlease extract relevant sentences from the provided context that can potentially help answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase \u201cInsufficient Information", "context.\nquestion": ["question]\ncontext: [context]\nFollowing this, context relevance ($\\mathit{ConRel}$) is computed as\n$\\mathit{Con Rel} = \\frac{|S_{\\text{ext}}|}{|c(q)|},$ (5)\nwhere || represents the number of sentences."]}, {"title": "A.4. Answer Similarity ($\\mathit{AnsSim}$)", "content": "Answer Similarity ($\\mathit{AnsSim}$) is defined as the similarity between the LLM generated response a(q) and the ground truth answer gt(q), and is computed using\n$\\mathit{AnsSim} = \\text{sim}(E(a(q)), E(gt(q)))$ (6)"}, {"title": "A.5. Answer Correctness ($\\mathit{AnsCor}$)", "content": "To determine answer correctness, a(q) and gt(q) are used to generate the following sets of statements:\n\u2022 TP (True Positive): Facts or statements that are present in both the ground truth and the generated answer.\n\u2022 FP (False Positive): Facts or statements that are present in the generated answer but not in the ground truth.\n\u2022 FN (False Negative): Facts or statements that are present in the ground truth but not in the generated answer."}]}