{"title": "LADDER: Multi-objective Backdoor Attack via Evolutionary Algorithm", "authors": ["Dazhuang Liu", "Yanqi Qiao", "Rui Wang", "Kaitai Liang", "Georgios Smaragdakis"], "abstract": "Current black-box backdoor attacks in convolutional neural networks formulate attack objective(s) as single-objective optimization problems in single domain. Designing triggers in single domain harms semantics and trigger robustness as well as introduces visual and spectral anomaly. This work proposes a multi-objective black-box backdoor attack in dual domains via evolutionary algorithm (LADDER), the first instance of achieving multiple attack objectives simultaneously by optimizing triggers without requiring prior knowledge about victim model. In particular, we formulate LADDER as a multi-objective optimization problem (MOP) and solve it via multi-objective evolutionary algorithm (MOEA). MOEA maintains a population of triggers with trade-offs among attack objectives and uses non-dominated sort to drive triggers toward optimal solutions. We further apply preference-based selection to MOEA to exclude impractical triggers. We state that LADDER investigates a new dual-domain perspective for trigger stealthiness by minimizing the anomaly between clean and poisoned samples in the spectral domain. Lastly, the robustness against preprocessing operations is achieved by pushing triggers to low-frequency regions. Extensive experiments comprehensively showcase that LADDER achieves attack effectiveness of at least 99%, attack robustness with 90.23% (50.09% higher than state-of-the-art attacks on average), superior natural stealthiness (1.12\u00d7 to 196.74\u00d7 improvement) and excellent spectral stealthiness (8.45\u00d7 enhancement) as compared to current stealthy attacks by the average l2-norm across 5 public datasets.", "sections": [{"title": "I. INTRODUCTION", "content": "Convolutional neural networks (CNNs) [52] have become an effective machine learning (ML) technique for image classification. They have proved to be vulnerable to backdoor attacks [20, 24, 51], allowing an attacker to mislead a victim model with incorrect yet desired predictions on poisoned images during inference while behaving normally on clean images. These attacks pose severe risks to real-world applications, e.g., tumor diagnosis [19], self-driving cars [5].\nSome service providers of safety-critical applications may choose to collect data online to train a private model and prevent attackers from accessing their systems. In this sense, backdoor attacks in the black-box setting are proposed [9]. Under a black-box scenario, attackers do not have knowledge about the models and cannot manipulate training but they may poison training data by designing triggers.\nAn \"ideal\" trigger should satisfy stealthiness, robustness, attack effectiveness and functionality [70]. Stealthiness concerns the invisibility of trigger in the poisoned image to human visual perception; robustness is evidenced by its ability to withstand image preprocessing; effectiveness requires that backdoor to be successfully injected into the victim model; and functionality preservation requires that inference accuracy on benign data remains unaffected.\nCurrent designs for trigger stealthiness in the spectral domain are impractical. Conventional pixel-based backdoor attacks [4, 24, 51] inject triggers into spatial domain. Since spatial domain contains abundant semantic information, putting triggers into pixels can be easily detected by visual inspection. Recent works [20, 23, 71] thus design backdoor attacks by injecting triggers into spectral domain. Inspired by patch-based backdoor attacks, FTrojan [71] manipulates the mid- and high-frequency spectrum of images by inserting predefined perturbations to fixed frequency bands. Manually crafting triggers in high-frequency components harms robustness, as most image preprocessing operations, e.g., low-pass filtering and JPEG compression, lead to greater information loss on these components. Current spatial and frequency triggers [4, 20, 24, 25, 45, 71] introduce distinguishable artifacts in spectral and/or spatial domain (see Figure 6 in Section VII), which bear a high risk of existing attacks being detected.\nA new perspective - starting with stealthiness. Considering both spatial and spectral domains [34, 72] which we call dual domains hereafter, this work aims to achieve dual-domain stealthiness: (1) spatial stealthiness, which guarantees the injection of trigger into the image does not harm cognitive semantics or introduce visual anomaly, and (2) spectral domain stealthiness, which avoids the disparities of frequency spectrum between clean and poisoned images. In contrast, despite the stealthiness achieved by Wang et al. [70] at pixel level, we shed lights on stealthiness in the spectral domain as well as guaranteeing all the attack goals mentioned above.\nBenefit the stealthiness and robustness in low-frequency domain. Cox et al. [11] claim that low-frequency components of natural images contain semantic information understandable to humans, whereas high-frequency ones stand for details and noise. Based on this, works [11, 25] state two benefits of inserting triggers in low-frequency domain: (1) abundant information contained in low-frequency domain can provide a high perceptual capacity of accommodating trigger patterns without perceptual degradation, which improves trigger stealthiness; and (2) low-frequency components can bear better resilience in image compressions and are less prone to be removed by image filtering than mid- and high-frequency components, which guarantees a better attack robustness.\nAchieving multiple attack objectives simultaneously in black-box backdoor attack is not trivial. Current backdoor attacks either adopt a fixed trigger pattern [4, 20, 24, 71],"}, {"title": "II. RELATED WORK", "content": "The first backdoor attack against CNNs is proposed by Gu et al. [24]. It injects a patch-based pattern into a small fraction of clean data during training process, triggering the victim model to misclassify those poisoned images to the attacker-desired label. Since then, various attacks have been proposed to improve stealthiness through the design of triggers and training."}, {"title": "III. BACKGROUND", "content": "Preliminary Notations on CNN. CNN is a cutting-edge ML architecture that achieves striking performance, especially for tasks with high-dimensional input space, such as image classification. Given a CNN-based image classification model $f_\\theta: I^S \\in [0,1]^S \\rightarrow R^K$ that takes an image $x \\in I^S$ as input, and outputs an inference label $y \\in R^K$, where $I^S$ represents the input space with dimension $S = H \\times W \\times C$ (Height, Width and Channels). The $R^K$ is the classification space which is divided into K categories, the label $y \\in R^K$ indicates the category where image x belongs to, i.e., $y \\in \\{0, 1, \u2026\u2026\u2026, K - 1\\}$.\nBackdoor Attacks and Data Poisoning. In a standard backdoor attack, the attacker crafts a subset of the clean training set (which contains N samples) $D_c = \\{(x_i, y_i) | x_i \\in I^S, y_i \\in R^K\\}_{i=1}^{[N]}$, with a poison ratio $r \\in (0,1]$ to produce a poisoned dataset: $D_{bd} = \\{(x'_j, y'_j) | x' \\in I^S, y' \\in R^K\\}_{j=1}^{[N]}$ in which each poisoned image $(x'_j, y'_j) \\in D_{bd}$ is obtained by applying a trigger function T and target label function \u03b7 on the image and label of counterpart clean sample $(x_j, y_j) \\in D_c$:\n$x'_j = T(x_j, m, t) = x_j \\cdot (1 - m) + t \\cdot m$,\n$y'_j = \\eta(y_j) = y_{tgt}$,\nwhere $m \\in [0, 1]$ is a scaling parameter and $y_{tgt}$ is the attacker-desired target label.\nBackdoor attack aims to inject a trojan into a CNN model $f_\\theta$ by tuning model parameters \u03b8 on $D_c$ and $D_{bd}$ so that the poisoned model misclassifies any poisoned images in $D_{bd}$ into target (attacker-desired) class while behaving normally on clean data in $D_c$ without sacrificing benign accuracy. Details about the formulation of $D_{bd}$ with frequency triggers generated by LADDER are provided in Section VI-A. Given a loss function L, backdoor attack is commonly defined as an optimization task $\\min_\\theta \\sum_{(x,y)\\in D_c \\cup D_{bd}} L(f_\\theta(x), y)$.\nDiscrete Cosine Transform\u00b9(DCT) is a widely used transformation that represents a finite sequence of image pixels as a sum of cosine functions oscillating at various frequencies. In the spectrum, most of the semantic information of images tends to be concentrated in a few low-frequency components on the top-left region, where the (0,0) element (top-left) is the zero-frequency component. DCT and its inverse (IDCT) are channel-wise independent and can be applied to each channel of color images independently. Therefore, we simply introduce the DCT/IDCT operation on a single-channel image. The relationship between a single-channel image $x \\in [0,1]^{H\\times W}$ (height H, width W) in spatial domain and its correspondent frequency spectrum $X^{H\\times W}$ can be described by type-II DCT and its inverse (IDCT) [2], denoted as D(\u00b7) and $D^{-1}(\u00b7)$ respectively as follows:\n$D(u, v) = N_uN_v\\sum_{i=0}^{H-1}\\sum_{j=0}^{W-1} x(i, j)\\cos (\\frac{(2i+1)u\\pi}{2H})\\cos (\\frac{(2j+1)v\\pi}{2W})$,\n$D^{-1}(i, j) = \\sum_{u=0}^{H-1}\\sum_{v=0}^{W-1} N_uN_vX(u, v)\\cos (\\frac{(2u+1)i\\pi}{2H})\\cos (\\frac{(2v+1)j\\pi}{2W})$,\nwhere $u, i \\in \\{0,1, \u2026\u2026\u2026, H - 1,\u2026\u2026\u2026, H - 1\\}$, and $v, j \\in \\{0,1,\u2026\u2026\u2026, W \u2013 1\\}$. A pair (u, v) refers to a specific frequency band of spectrum of an image. D(u, v) defines the magnitude of frequency component in a frequency band (u, v). The value $x(i, j) \\in [0, 1]$ indicates the pixel value of location (i, j) in an image x in spatial"}, {"title": "Multi-objective Optimization (MOP)", "content": "MOP refers to an optimization task involving two or more conflicting objectives that cannot be optimal simultaneously to a single optimal solution. MOP is best addressed by generating a set of solutions, each reflecting different trade-offs among the objectives. Under MOP, multi-objective optimization (MOO) is the process of optimizing these multiple conflicting objectives concurrently to obtain an optimal set of solutions."}, {"title": "IV. THREAT MODEL", "content": "Attacker Capability. Similar to [32, 71, 74], we assume the attacker acts as a malicious data provider who can only embed a trigger into samples from the training set for public use. But it has no control over the training process and lacks any knowledge of the victim model.\nAttacker Goals. The attacker tricks the victim into training a backdoored CNN model for an image classification task, so that (1) the compromised CNN model outputs a target label desired by the attacker with high probability for any input containing the embedded trigger, while maintaining high inference accuracy on benign data; (2) dual-domain trigger stealthiness can be guaranteed, preventing any noticeable anomaly in both the spatial and spectral domains of the input images; (3) the attack achieves robustness, ensuring that the backdoor remains effective even after image preprocessing is applied to the poisoned data.\nPerformance Metrics. We introduce metrics to quantitatively measure our attack performance in three aspects: effectiveness, stealthiness, and robustness.\n(1) For attack effectiveness and functionality preservation: we empirically evaluate the effectiveness with attack success rate (ASR), which computes the ratio of poisoned samples misclassified by the poisoned CNN model as the attacker desires. We further use the accuracy (ACC) to evaluate the ratio of benign samples correctly classified as indicated by its ground-truth label by the victim model. ACC (ASR) \u2208 [0, 100] is a scalar value reflecting the proportion of samples (%) being"}, {"title": "V. OBJECTIVES CONFLICT", "content": "One may apply a stealthy attack, e.g., FTrojan [71], in a low-frequency region to achieve practical attack objectives (robustness, stealthiness and effectiveness), without considering trigger optimization. In contrast, this work aims to search a trigger that balances multiple objectives. In such a scenario, the conflict among objectives refers to the fact that attack objectives cannot achieve optimal simultaneously.\nIn a backdoor attack, effectiveness and trigger stealthiness are mutually conflicting objectives. We confirm the conflict by formulating a simple optimization problem with the Lagrange multipliers under the control of two coefficients \u03b1, \u03b2:\n$\\min_{\\theta, t} \\alpha\\sum_{(x,y) \\in D_c \\cup D_{bd}} L(f_\\theta(x), y) + \\beta||t||_2$,\nwhere \u03b1, \u03b2 \u2208[0,1], Dbd is a set of poisoned images produced by the spatial domain-based trigger function in Equation (1) with trigger t, and a+\u03b2=1, t is a trigger initialized with random noise. With stochastic gradient descent (SGD) [3], we update t first while remaining @ unchanged, and then update model parameters @ with the optimal t*. The results on CIFAR-10 with PreAct-ResNet18 are in Figure 2."}, {"title": "VI. EVOLUTIONARY MULTI-OBJECTIVE BACKDOOR ATTACK", "content": "We formulate our backdoor attack as an MOP. The main task of solving the MOP is to search an optimal trigger, which is patched to images to create a poisoned dataset.\nFormally, a frequency trigger $t = (\\delta,\\nu)$ where $\\delta = \\{\\delta_0, \\delta_1,..., \\delta_{n-1}\\}$ is a series of magnitude of perturbations, $\\nu=\\{\\nu_0,\\nu_1,...,\\nu_{n-1}\\}$ describes the frequency bands to insert the correspondent perturbations on, and n is the number of manipulated frequency bands. We describe the trigger patching operation in Figure 4(a).\nIn order to inject our trigger t into an image x in the spectral domain, we obtain the spectrum of x via DCT (D(\u00b7)) and put the trigger optimized by LADDER in it. Finally, the poisoned spectrum is inverted to the spatial domain using IDCT ($D^{-1}(\u00b7)$), while we reset the label to an adversary-desired target. Our trigger injection function T and target label function \u03b7 on a given sample (x, y) are formally defined as:\n$x' = T(x, t) \\equiv D^{-1}(D(x) \\odot t)$,\n$y' = \\eta(y) \\equiv y_{tgt}$.\nStealthiness\\noindent in the spectral domain can reflect dual-domain stealthiness.\nRobustness := || (loc(vi) \u2013 loc(min(Fdom)))||2,\nSpecifically, MOEA maintains a set of non-dominated"}]}