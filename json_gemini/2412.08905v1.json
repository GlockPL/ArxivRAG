{"title": "Phi-4 Technical Report", "authors": ["Marah Abdin", "Ronen Eldan", "Mojan Javaheripi", "Yuanzhi Li", "Eric Price", "Shital Shah", "Dingli Yu", "Jyoti Aneja", "Suriya Gunasekar", "Piero Kauffmann", "Weishung Liu", "Gustavo de Rosa", "Xin Wang", "Cyril Zhang", "Harkirat Behl", "Michael Harrison", "James R. Lee", "Caio C. T. Mendes", "Olli Saarikivi", "Rachel Ward", "Yi Zhang", "S\u00e9bastien Bubeck", "Russell J. Hewett", "Yin Tat Lee", "Anh Nguyen", "Adil Salim", "Yue Wu"], "abstract": "We present phi-4, a 14-billion parameter language model developed with a training recipe that\nis centrally focused on data quality. Unlike most language models, where pre-training is based pri-\nmarily on organic data sources such as web content or code, phi-4 strategically incorporates synthetic\ndata throughout the training process. While previous models in the Phi family largely distill the\ncapabilities of a teacher model (specifically GPT-4), phi-4 substantially surpasses its teacher model\non STEM-focused QA capabilities, giving evidence that our data-generation and post-training tech-\nniques go beyond distillation. Despite minimal changes to the phi-3 architecture, phi-4 achieves strong\nperformance relative to its size - especially on reasoning-focused benchmarks - due to improved data,\ntraining curriculum, and innovations in the post-training scheme.", "sections": [{"title": "Introduction", "content": "Recent advancements in Large Language Models (LLMs) have shown that significant improvements in\ndata quality can rival, and sometimes surpass, the performance gains traditionally achieved by scaling\ncompute with model and dataset size. Building on the success of the Phi family [GZA+23, LBE+23,\nJBA+23, AAA+24], we introduce phi-4, a 14-billion parameter model that further advances performance\nof small language models by introducing innovative synthetic data generation methods for reasoning-\nfocused tasks, by optimizing the training curriculum and data mixture, and by introducing new tech-\nniques in post-training.\nSynthetic data constitutes the bulk of the training data for phi-4 and is generated using a diverse\narray of techniques, including multi-agent prompting, self-revision workflows, and instruction reversal.\nThese methods enable the construction of datasets that induce stronger reasoning and problem-solving\nabilities in the model, addressing some of the weaknesses in traditional unsupervised datasets. Synthetic\ndata in phi-4 also plays a crucial role in post-training, where techniques such as rejection sampling and\na novel approach to Direct Preference Optimization (DPO) are employed to refine the model's outputs.\nThe development of phi-4 is guided by three core pillars:\n1. Synthetic Data for Pretraining and Midtraining: High-quality synthetic datasets are de-\nsigned to prioritize reasoning and problem-solving, carefully generated to ensure diversity and"}, {"title": "Approach to Data", "content": "The pretraining phase of phi-4 relies heavily on synthetic datasets generated through a variety of tech-\nniques. In addition, we employ several methods for filtering organic data sources that are used both as\ncomplementary datasets in the pretraining and as seeds for generating synthetic data."}, {"title": "Purpose of Synthetic Data", "content": "Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi\nseries of models has consistently emphasized the importance of synthetic data. Rather than serving as\na cheap substitute for organic data, synthetic data has several direct advantages over organic data."}, {"title": "Structured and Gradual Learning.", "content": "In organic datasets, the relationship between tokens is often\ncomplex and indirect. Many reasoning steps may be required to connect the current token to the next,\nmaking it challenging for the model to learn effectively from next-token prediction. By contrast, each\ntoken generated by a language model is by definition predicted by the preceding tokens, making it easier\nfor a model to follow the resulting reasoning patterns. In this way, synthetic data may act as a form of\n\"spoonfeeding,\u201d presenting challenges in a digestible and progression-oriented manner.\nA simple example to illustrate this is that a human-written solution to a math problem might start\nwith the final answer. This answer is much too hard to output immediately, for either a human or an\nLLM-the human produced it by nonlinear editing, but pretraining expects the LLM to learn to produce\nit linearly. Synthetic solutions to math problems will not have such roadblocks."}, {"title": "Alignment with Inference Contexts.", "content": "Synthetic data is typically closer to the format of outputs\nwe expect our models to generate. Training on such data helps align the model's pretraining experience\nwith the scenarios it encounters during inference. This alignment ensures that the context seen during\ngeneration remains in-distribution with respect to the data the model was pretrained on.\nFor example, web forums are very different in style from LLM interactions. If a fact only appears\nin web forum data, the pretrained model will think it is very unlikely to occur in the chats it produces.\nRewriting facts from the web forum into the language style of an LLM makes the facts more accessible\nduring the LLM chat context of inference."}, {"title": "Principles.", "content": "Our approach to generating synthetic data for phi-4 is guided by the following principles:\n1. Diversity: The data should comprehensively cover subtopics and skills within each domain. This\nrequires curating diverse seeds from organic sources.\n2. Nuance and Complexity: Effective training requires nuanced, non-trivial examples that reflect\nthe complexity and the richness of the domain. Data must go beyond basics to include edge cases\nand advanced examples.\n3. Accuracy: Code should execute correctly, proofs should be valid, and explanations should adhere\nto established knowledge, etc.\n4. Chain-of-Thought: Data should encourage systematic reasoning, teaching the model various\napproaches to the problems in a step-by-step manner. This fosters coherent outputs for complex\ntasks."}, {"title": "Synthetic Data for Pretraining and Midtraining", "content": "We created 50 broad types of synthetic datasets, each one relying on a different set of seeds and dif-\nferent multi-stage prompting procedure, spanning an array of topics, skills, and natures of interaction,\naccumulating to a total of about 400B unweighted tokens. In Appendix D, we give a few examples\nof transcripts taken from our synthetic generations. Here, we highlight novel methodologies used in\ngenerating synthetic datasets for phi-4:\n\u2022 Seed Curation: The synthetic dataset generation begins with high-quality seeds sourced from\nmultiple domains. These curated seeds provide the foundation for synthetic data generation,\nenabling the creation of exercises, discussions, and reasoning tasks tailored to the model's training\nobjectives.\n1. Web and Code-based Seeds: Excerpts and snippets are extracted from web pages, books,\nand code repositories with a focus on content that demonstrates high complexity, reasoning\ndepth, and educational value. To ensure quality, we employ a two-stage filtering process:\nfirst, identifying pages with strong educational potential, and second, segmenting the selected\npages into passages, scoring each for its factual and reasoning content.\n2. Question Datasets: A large set of questions was collected from websites, forums, and Q&A\nplatforms. These questions were then filtered using a plurality-based technique to balance\ndifficulty. Specifically, we generated multiple independent answers for each question and\napplied majority voting to assess the consistency of responses. We discarded questions where\nall answers agreed (indicating the question was too easy) or where answers were entirely\ninconsistent (indicating the question was too difficult or ambiguous). This filtering process\nproduces a dataset of questions that challenge the model's reasoning and problem-solving\nabilities while remaining approachable. The plurality answers were used in place of the\nground truth in our rejection-sampling based generations.\n3. Creating Question-Answer pairs from Diverse Sources: Another technique we use\nfor seed curation involves leveraging language models to extract question-answer pairs from\norganic sources such as books, scientific papers, and code. This approach does not rely on\nmerely identifying explicit Q&A pairs within the text. Instead, it involves a pipeline designed\nto detect deduction chains or logical progressions in the text. The language model identifies\nkey steps in reasoning or problem-solving processes and reformulates them into questions\nand corresponding answers. Our experiments show that, if done correctly, training on the\nresulting content can be far more effective (in terms of improvement on academic and internal\nbenchmarks) than training on the original content.\n\u2022 Rewrite and Augment: Seeds are transformed into synthetic data through multi-step prompting\nworkflows. This includes rewriting most of the useful content in given passages into exercises,\ndiscussions, or structured reasoning tasks.\n\u2022 Self-revision: The initial responses are then iteratively refined through a feedback loop where\na model critiques and subsequently improves its own outputs, guided by the rubrics focused on\nreasoning and factual accuracy."}, {"title": "Curation and Filtering of Web and Q&A Data", "content": "Q&A datasets. We collected tens-of-millions high-quality organic problems and solutions by review-\ning public websites, relying on existing datasets, and acquiring external datasets. Our experience from\nprevious models showed that question-answer data contributed significantly to various capabilities, such\nas mathematical reasoning and academic performance. Our ablation studies showed that organic ques-\ntions are substantially more effective than synthetic questions. We used several ways to synthetically\naugment the dataset of organic questions to obtain a larger dataset. While these rewritten questions\nimproved the model's capabilities, the gains were not as pronounced. A significant portion of the col-\nlected questions lacked accurate solutions. To address this, we replaced the answers with synthetically\ngenerated ones and used majority-voting to increase accuracy. All collected questions and solutions\nunderwent a thorough decontamination process to ensure there is no overlap with test sets\u00b3.\nTargeting High-quality Web Data. We collected a wide variety of high-quality organic data sources\nfor phi-4, prioritizing reasoning-dense and nuanced material (e.g., academic papers, educational forums,\nand programming tutorials). In addition to directly training on this text, we used various web sources\nas seeds for specialized synthetic data generation pipelines. We found clean and correct natural data to\nbe absolutely crucial for seeding synthetic data: minor errors can result in severe quality degradations\nfor derived synthetic documents. We therefore invested heavily in the perfectionistic curation of our web\ndata. We discuss the main techniques and considerations below:\n\u2022 Targeted Acquisitions: We included major repositories of reasoning-dense documents that are\npublicly permissible for use (e.g., arXiv, PubMed Central, GitHub) or explicitly licensed (e.g.,\nlicensed books) aiming for a level of comprehensiveness, recency, and cleanliness above the typical\nstandard of externally available corpora.\n\u2022 Filtering Web Dumps: To capture the long tail of information-rich web sources (e.g., forums,\nblogs, course material, domain-specific wikis), we took the approach of selecting a small fraction\nof highest-quality documents from bulk web dumps, using small (non-LLM) classifiers trained on\n~ 106 LLM-generated annotations. This approach tends to over-index on STEM-related keywords,\nso we created a specialized pipeline to amplify high-quality non-STEM content (e.g., arts, history,\ntravel, culture, and entertainment). These topic classifications were also obtained by distilling an"}, {"title": "Post-Training datasets", "content": "Our post-training data is composed of:\n\u2022 Supervised Fine-Tuning (SFT) Datasets: Using carefully curated user prompts taken from\na mixture of publicly available datasets and synthetically generated data, we generate multiple\nmodel responses and select the best using an LLM-based evaluation process.\n\u2022 Direct Preference Optimization (DPO): We generate DPO pairs based on rejection sampling\nand LLM evaluation, a part of which is based on our approach to creating pivotal token-based\npairs, explained in Section 4.3 below."}, {"title": "Pretraining details", "content": "The phi-4 model is based on a decoder-only transformer architecture [VSP+17] with 14B parameters and\na default context length of 4096. This is later extended to a 16K context length during midtraining. The\narchitecture closely follows phi-3-medium, except that we now use the tiktoken tokenizer (for better\nmultilingual support) with a padded vocabulary size of 100,352 (including unused tokens) and we use\nfull attention over the 4K context length, rather than a 2K sliding window used in phi-3-medium.\nThe model was pretrained for approximately 10T tokens using linear warm-up and decay schedules\nwith peak learning rate of 0.0003, constant weight decay of 0.1, and global batch size of 5760. The\ntraining hyperparameters are tuned using interpolations from shorter horizon runs and further adjusted\nby stress testing the learning rate warm-up stage for stability. Pretraining is followed by a shorter\nmidtraining stage to increase the original context length of 4k to 16k.\nSince pre-trained models are not good at instruction following, it is not very informative to use\nO-shot evaluations that require the answer to be in a specific format, for example SIMPLE-EVALS. We"}, {"title": "Data Composition in Pretraining", "content": "The phi-3 model family were trained using a two-phase strategy. Most of the training tokens were used\nin phase 1 of the training, which consisted largely of filtered web data. Phase 2 was trained with a\ndata mixture consisting primarily of synthetic tokens and a much smaller allocation for ultra-filtered\nand reasoning-heavy web data. As the size and complexity of our synthetic data grew, we observed a\nmarginal drop in the benefit from using non-synthetic tokens for the phi-3 family of model sizes. We\nnote two key observations.\n\u2022 Web datasets showed small benefits on reasoning heavy benchmarks. Prioritizing more epochs\nover our synthetic data led to better performance with respect to adding fresh web tokens.\n\u2022 Models trained only with synthetic data underperformed on the knowledge-heavy benchmarks and\ndemonstrated increased hallucinations.\nFigure 2 demonstrates the first phenomenon using smaller scale phase 2 pretraining exercises. In\nthis example, we conduct two training runs per model scale, using the same number of training tokens\non top of phase 1 pretrained checkpoints. For all runs, the number of unique synthetic tokens is fixed\n(a subsample of full synthetic data) but the number of repetitions on this data changes, namely 4 and\n12 epochs. The rest of the training tokens are fresh unique tokens supplied from web sources. As seen,\nperforming more iterations on the synthetic data is more beneficial than supplying more web tokens.\nInspired by this scaling behavior of our synthetic data, we trained a 13B parameter model solely on\nsynthetic\u2074 data, for ablation purposes only \u2013 the model sees over 20 repetitions of each data source. For\nthe sake of ablations, we partitioned our synthetic data into web rewrites, which includes more direct\nrewrites of our filtered web content relative to all other types of synthetic data. Table 3 compares the\nprevious phi-3-medium model with the new model trained entirely on the synthetic data. Throughout\ntraining, all benchmarks consistently improved, despite the increase in epochs, and the majority of\nthe benchmarks showed improvements over phi-3. However, knowledge-related benchmarks, like 1-shot\ntriviaqa (TQA), show a large gap where synthetic models are subpar. These observations led us to\nrethink the role of web data in our data mixture."}, {"title": "Data Mixture", "content": "To design our pretraining data mixture for a given training token budget, we search over different\nallocation of tokens coming from various sources, namely, 1) synthetic, 2) web rewrites, 3) filtered web\n(divided into reasoning and knowledge-heavy portions), 4) targeted acquisitions and organic data (e.g.,\nacademic data, books, and forums), and 5) code data.\nWe conducted ablations using a shorter token horizon of 1T tokens to derive the data mixture.\nThese ablations rely on our established result on the high-rank correlation of short training with longer\ntraining, up to the over-fitting saturation threshold of data sources. In addition we observe a high rank\ncorrelation between the performance of the 7B and 14B models on different data mixtures, given a large\nenough distance between the data mixtures. This allowed us to conduct the experiments at 7B scale and\ntransfer the findings to phi-4. Among the numerous ablations, we highlight a few that show best insights\non our data composition. Specifically, we freeze the ratio of tokens coming from targeted acquisitions\nand code categories, and change the ratio of tokens for the synthetic, web, and web rewrites clusters.\nTable 4 summarizes the results for the hand-picked ablations, as compared with the data mixture\nthat was used for the final training run. A uniform allocation of tokens among the three categories is\nsuboptimal due to the higher quality of synthetic data and the only benchmark that shows a clear benefit\nfrom web data is TQA. While the synthetic-heavy variations on rows 2 and 3 of the table are marginally\nbetter than the chosen final data mixture, we decided to integrate the targeted and knowledge-heavy\nfiltered web data sources to improve knowledge benchmarks (see Section 3.1) to balance all model"}, {"title": "Midtraining Details", "content": "phi-4 includes a midtraining stage where the context length is increased from the original 4K to 16K.\nWe conduct several ablations to study the role of data on long-context performance. Specifically, we try\ndata sources that are inherently long context, and compare them with artificially created long context\ndata where samples are padded together to fill the sequence. We observe the former to perform better\nin longer context tasks.\nInspired by this, we further filter our high-quality non-synthetic datasets (i.e., academic, books, and\ncode data) to separate samples above 8K context. We then up-weight the data subsets that are 16K or\nhigher in length. We also create new synthetic datasets that satisfy the > 4K sequence requirement. The\nfinal data mixture includes 30% of the newly curated longer context data and a 70% portion of recall"}, {"title": "Pivotal Token Search", "content": "Consider a generative model producing a token-by-token response to a given prompt. For each token\nproduced, which corresponds to a prefix of the model response, one can consider the conditional proba-\nbility of the model's answer being correct given that prefix, as well as the increment in this probability\nwith respect to that token (in other words, the difference in the probability of being correct before and\nafter producing that token). It is often the case that the overall correctness is highly dependent on\na successful generation of a small number of key tokens. For example, we can see in Figure 3 where\nthe model outputs a math solution and a \u201cfortunate\u201d sampling of a crucial token (negative) shifts the\nsolution from possible failure to likely success, while sampling of the token ((a) subsequently risks failure\nagain. We refer to these tokens as pivotal tokens as they have an outsized effect on the course of the\nsolution.\nNow, consider how the solution from Figure 3 would be used in DPO as a full-length accepted\nresponse. As the figure shows, there are many tokens with probabilities much lower than the 0.31 of\nnegative), which would contribute to noise in the gradients diluting the signal from the pivotal token.\nEven worse, the token ((a) that contributed to the lack of robustness would receive a strong positive\nlearning signal thanks to its low probability of 0.12.\nMoreover, intuition suggests that when two texts substantially deviate from each other, comparison\nof their individual next-token log probabilities (as done in DPO) is not very meaningful. Rather, it\nmakes more sense that the signal should come from the first tokens after the two texts starts diverging\nfrom each other.\nTo alleviate these effects, we employ a method we call Pivotal Token Search (PTS) for generating\npreference data that specifically targets pivotal tokens in isolation, creating DPO pairs in which the\npreference optimization takes effect with respect to a single token.\nPTS identifies points of a completion token sequence $T_{full}$ = $t_1$,$t_2$,... for some user query Q where\nthe next token $t_i$ has a significant impact on the probability of success p(success | $t_1$,...,$t_i$). PTS es-\ntimates these probabilities by sampling completions starting from Q + $t_1$,...,$t_i$, which are checked for\ncorrectness with an oracle for Q. Figure 4 shows a basic instantiation of the algorithm. The proce-\ndure SUBDIVIDE recursively splits the sequence into segments $t_i$,...,$t_j$ until the change in probability\n|p(success|$t_1$, ..., $t_{i\u22121}$) - p(success | $t_1$, ..., $t_j$) | for each segment is below a threshold $p_{gap}$ or the segment\nis just a single token. Tokens with a sharp change in success probability are kept as pivotal. We turn\npivotal tokens into preference data by taking Q + $t_1$,\u2026\u2026\u2026, $t_{i\u22121}$ as the query, and single tokens $t_{acc}$ and $t_{rej}$\nthat increase/decrease p(success | $t_1$,...,$t_{i-1}$, $t_{acc/rej}$) as the accepted and rejected completions, respec-\ntively.\nThe binary-search algorithm for PTS is not always guaranteed to find all pivotal tokens, but\nit only finds pivotal tokens and it finds all of them if the success probability is near-monotone over the\ncourse of the solution.\nWe used PTS to generate preference data for tasks where ground-truth is readily available, such as\nmathematics, various forms of question answering and coding. To improve sample efficiency, we filter\nthe target questions to only include those with 0.2 \u2264 p(success) \u2264 0.8, as pivotal tokens are rare for tasks\nthat are very easy or hard."}, {"title": "Supervised Fine-Tuning", "content": "In this phase, we fine-tune the pretrained model with a learning rate of $10^{-6}$ on a variety of data generated\nfrom high-quality data across diverse domains, including math, coding, reasoning, conversation, model\nidentity, and safety. We also added multilingual data for 40 languages. We use around 8B tokens of\ndata in this phase, all formatted in the chatml format."}, {"title": "Direct Preference Optimization", "content": "We use DPO [RSM+23] to align the model with human preferences, and also to steer the model away\nfrom unwanted behavior through pairs of desired and undesired outputs. DPO data covers chat format\ndata, reasoning, and Responsible AI (RAI) data and improves the model in math, coding, reasoning,\nrobustness, and safety. We do two rounds of DPO on the SFT model. We introduce a technique, Pivotal\nToken Search (PTS), to generate pairs for DPO for the first DPO round. Details of the data mixture\nfor first round are provided in Table 7.\nFor the second round, which we call judge-guided DPO, we gather approximately 850k pairs of\ndesired and undesired outputs. The prompts are sourced from various publicly available instruction\ntuning datasets and also include prompts related to safety and Responsible AI (RAI). Next, for each of\nthese prompts, we generate responses from GPT-40, GPT-4t and our model. From these responses, we\ncreate various combinations of DPO pairs and use GPT-40 as a judge to label positive or negative for\na given pair. For a given pair of responses, each assistant response is given a score based on accuracy,\nstyle, and detail. We label the response with higher accuracy or overall (average of accuracy, style, and\ndetail) score as the positive response. We provide the prompt we used in Appendix A. The data mixture\nfor this round is provided in Table 8. Both stages also include a small amount of data for safety and\nmitigating hallucinations."}, {"title": "Performance on Key Benchmarks", "content": "Our benchmark results were presented in Table 1, along with comparisons to other models. We first re-\nport the values from OpenAI'S SIMPLE-EVALS benchmark, which is a framework (including prompts, tem-\nperature, and extraction) for evaluating MMLU [HBB+20], GPQA diamond [RHS+23], MATH [HBK+21],\nHumanEval [CTJ+21], MGSM [SSF+22], and the SimpleQA [WKC+24] F1-score. We also consider\nMMLU-pro [WMZ+24], HumanEval+ [LXWZ23], ArenaHard [CZS+24], and IFEval [ZLM+23], for which\nwe use an internal framework and prompting and extraction. Finally, we use PhiBench, our internal\ncollection of evaluations (see Section 5).\nphi-4 outperforms the closest in-class contemporary model, Qwen-2.5-14B-Instruct, in 9 out of 12\nbenchmarks. While phi-4 underperforms relative to Qwen-2.5-14B-Instruct on the benchmark numbers\nfor SimpleQA, DROP, and IFEval, we consider phi-4's behavior on SimpleQA to actually be better\nthan Qwen's. In fact, our base model gets a higher benchmark score than Qwen-2.5-14B-Instruct on\nSimpleQA, and we intentionally modified the model's behavior in post-training to optimize for a better\nuser experience rather than a higher benchmark score. See Figure 6 and Appendix A.1 for details.\nOur model excels at STEM Q&A tasks. For example, on GPQA (graduate-level STEM questions)\nand MATH (math competitions), it even outscores its teacher model, GPT-40. It also scores higher at\ncoding, as measured by HumanEval and HumanEval+, than any other open-weight model we benchmark\nagainst, including much larger Llama models.\nphi-4's weakest benchmark scores are on SimpleQA, DROP, and IFEval. We believe for the first\ntwo that the number reported by SIMPLE-EVALS is reductive and does not accurately reflect model\nperformance on the benchmark problems. However, IFEval reveals a real weakness of our model - it\nhas trouble strictly following instructions. While strict instruction following was not an emphasis of our\nsynthetic data generations for this model, we are confident that phi-4's instruction-following performance\ncould be significantly improved with targeted synthetic data."}, {"title": "Safety", "content": "We developed phi-4 in accordance with Microsoft's Responsible AI principles. Our overall approach to\nRAI consisted of safety alignment in post-training, red-teaming, and automated testing and evaluations\nacross dozens of RAI harm categories. We leveraged helpfulness and harmlessness preference datasets"}, {"title": "RAI Benchmarks", "content": "Table 10 shows the results of in-house RAI benchmarks [MHJ+23] for phi-4 compared to the phi-3 models\n[AAA+24], Mistral-7b-v0.1 [JSM+23], Mistral-7b-v0.2, Gemma 7b [TMH+24], and Llama-3-instruct-\n8b [AI23b]. This benchmark utilized GPT-40 to simulate multi-turn conversations in five different\ncategories and to evaluate the model responses. Grounding is scored between 0 (not grounded) and 5\n(fully grounded), and measures if the information in a response is based on a given prompt. In other\ncategories, responses were evaluated in terms of the severity of harmfulness and scored from 0 (no harm)\nto 7 (severe harm) and the defect rates (DR-x) were computed as the percentage of samples with the\nseverity score being greater than or equal to x. The Jailbreak (DR1) benchmark consists of simulated\nconversations around child grooming, illegal persuasion, leaking of 100 words of guidelines, popular\nconspiracy, prejudice against real people, step-by-step illegal advice, and violence against real people.\nFor more details on the RAI prompts and evaluation framework, see [HPBP+24]."}, {"title": "Red Teaming", "content": "In addition to RAI benchmarking, we collaborated with the Microsoft AI Red Team (AIRT), an inde-\npendent group tasked with identifying safety and security vulnerabilities in Microsoft's GenAI products.\nAIRT conducted a two-week red-teaming exercise that tested phi-4 for risky behaviors by emulating both\naverage and adversarial users in single and multi-turn scenarios. Overall, AIRT found that the behavior\nof phi-4 was similar to that of the phi-3 family, but identified several risky behaviors that were addressed\nby further rounds of safety post-training. In addition, the adversarial user scenario tested a wide range\nof techniques aimed at intentionally subverting the model's safety training including jailbreaks, prompt\nencodings, and multi-turn attacks. phi-4 showed strong defenses against these techniques. AIRT also\ngenerated adversarial suffixes using the GCG algorithm [ZWC+23] on_phi-3-medium, but found that\nthese suffixes did not transfer to phi-4. Further red teaming is required to identify possible risks across\na broader range of scenarios and harm categories."}, {"title": "Weaknesses", "content": "While phi-4 achieves similar level of language understanding and reasoning ability as much larger models,\nit is still fundamentally limited by its size for certain tasks, specifically in hallucinations around factual\nknowledge. For example, if X is a plausible human name, the model sometimes responds to prompts of the\nform \"Who is X?\" with a hallucinated biography of the person X. This limitation would be improved by\naugmenting the model with a search engine, but factual hallucinations cannot be eliminated completely.\nWhile phi-4 demonstrates relatively strong performance in answering questions and performing rea-\nsoning tasks, it is less proficient at rigorously following detailed instructions, particularly those involving\nspecific formatting requirements. For instance, when tasked with generating outputs in strict tabular\nformats, adhering to predefined bullet structures, or precisely matching stylistic constraints, the model\nmay produce outputs that deviate from the specified guidelines. This limitation arises in part from the\nmodel's training focus, which prioritized synthetic datasets tailored toward Q&A and reasoning tasks\nover instruction-following scenarios.\nEven on reasoning tasks, phi-4 can make mistakes. For example, when asked \u201cwhich number is\nsmaller, 9.9 or 9.11?\", the model can conclude incorrectly that \"9.9 is smaller than 9.11\".\nMoreover, as our data contains a lot of chain-of-thought examples, phi-4 sometimes gives long elab-\norate answers even for simple problems this might make user interactions tedious. We also note that\nwhile phi-4 can function as a chat bot, it has been fine-tuned to maximize performance on single-turn\nqueries.\nDespite diligent RAI efforts, we acknowledge challenges around reproduction or amplification of\nbiases, inappropriate content generation, and safety issues. The use of carefully curated training data, as\nwell as targeted post-training, and improvements from red-teaming insights, have resulted in mitigating\nthese issues across all dimensions, but have not resolved the issues completely.\""}, {"title": "Post-Training Dataset Details", "content": "We created post-training SFT and DPO data to mitigate hallucinations in simple settings. Without\nany mitigation, phi-4 would almost never admit to ignorance. For example, in response to too-difficult\nquestions like \u201cWho is the 297th highest ranked tennis player?\u201d the model would essentially act as an\nimprov-style \"Yes, and...\" engine, inventing a superficially plausible answer.\nOur goal in pretraining was to pack as much information into the model as possible, that is, to teach\nmore to the model rather than to teach it its own limitations. Then in post-training, we can identify\nthe level of problem that is too difficult for the model, and teach it to generate refusals rather than\nhallucinations on those problems.\nWe started with seed trivia problems, such as from TriviaQA [JCWZ17]. For each question, we ran\nphi-4 multiple times to estimate its chance of accurately solving it. We also used GPT-4o to generate"}, {"title": "Synthetic Generation Prompts", "content": "Here, we share the main synthetic generation prompts, provided to GPT-40, to generate post-training\ndata to decrease hallucinations.\nConsider the following trivia question:\n# Question\n{{ question }}\n# Instructions\nYour job is to turn this problem into a nonsensical one, for which the\nanswer is invalid or unlikely to be known by anyone. For example, you\nmight change the name from a well-known figure to a random name, or\nchange the date from a well-known event to a random date, or the place\nto a different one. For example, you might change \"When did Amelia\nEarhart cross the Atlantic Ocean?\" to \"When did Edgar Greenwood cross\nthe Atlantic Ocean?\" or \"How many times did Amelia Earhart cross the\nEnglish Channel?\".\nYour goal is that the new question is *plausibly real*, but impossible to\nanswer. You should not make the question obviously fake, silly, or\nfictional; for example, all country names should be real countries,\nand no names should be obvious homages to the original question. It\nshould sound like a serious trivia question."}, {"title": "Judge-guided DPO", "content": "For the second round of DPO, we generate responses from GPT-40, GPT-4t and our model. To label\nresponses as positive or negative, we use GPT-40 as a judge and use the following prompt.\nYour task is to judge which of the following reply given by an AI\nassistant is better.\n# Conversation\n{{ chat }}\n# Replies\n{{ replies }}\n# Guideline\nProduce your output in the following JSON format (without comments and\nwith correct escape characters):\n{", "subsections": [{"title": "Decontamination", "content": "We decontaminate against the ARC-Easy, MBPP, phibench, CommonsenseQA, WinoGrande, mcphi,\nMedQA, MATH, AGIEval, PIQA, OpenBookQA, HellaSwag, GPQA, mt-bench, MMLUPro, GSM8k,\nHumanEval, arena_hard, ARC-Challenge, and MMLU benchmarks. We apply a hybrid n-gram algorithm\nfor decontamination which uses 13-gram and 7-gram features for removing matches to the test set, which\nis described in more detail in 1"}]}]}