{"title": "Generative Multi-modal Models are Good Class-Incremental Learners", "authors": ["Xusheng Cao", "Haori Lu", "Linlan Huang", "Xialei Liu", "Ming-Ming Cheng"], "abstract": "In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic forgetting caused by the classifier's bias towards the current task has long posed a significant challenge. It is mainly caused by the characteristic of discriminative models. With the growing popularity of the generative multi-modal models, we would explore replacing discriminative models with generative ones for CIL. However, transitioning from discriminative to generative models requires addressing two key challenges. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, it requires formulating the task of CIL within a generative framework. To this end, we propose a novel generative multi-modal model (GMM) frameworkj ) framework for class-incremental learning. Our approach directly generates labels for images using an adapted generative model. After obtaining the detailed text, we use a text encoder to extract text features and employ feature matching to determine the most similar label as the classification prediction. In the conventional CIL settings, we achieve significantly better results in long-sequence task scenarios. Under the Few-shot CIL setting, we have improved by at least 14% accuracy over all the current state-of-the-art methods with significantly less forgetting. Our code is available at https://github.com/DoubleClass/GMM.", "sections": [{"title": "1. Introduction", "content": "Deep neural networks [19, 33, 56] have made remarkable strides in numerous applications, primarily owing to the vast amounts of data and computational resources at their disposal. Nonetheless, these accomplishments are predominantly contingent on having access to all the required data simultaneously for training on various tasks. In cases where data is acquired incrementally, these networks often encounter the challenge of catastrophic forgetting [43]. Hence, the capacity to seamlessly incorporate new knowledge while retaining previously acquired knowledge is a highly desirable attribute for future artificial intelligence systems. Continual learning [45, 67, 77, 84] is a subject of study aimed at advancing the evolution of neural networks toward this goal.\nNumerous studies have delved into continual learning, categorizing their approaches into three main groups [14]: rehearsal-based, architecture-based, and regularization-based methods. Additionally, hybrid methods are gaining popularity as they combine insights from different perspectives. Within this research landscape, three primary scenarios [66] have received extensive attention, with class-incremental learning (CIL) [41] being one of the most demanding settings. In our work, we concentrate on CIL, wherein each task comprises a distinct set of classes, and the primary challenge is to enable the network to recognize new classes without forgetting knowledge of previously encountered ones.\nThe majority of existing research on class-incremental"}, {"title": "2. Related Work", "content": "learning (CIL) focuses on training models from scratch, relying solely on data from the current tasks [7, 15, 16, 24, 26, 29, 48, 74, 76, 89]. In contrast, humans accumulate knowledge over extended periods, drawing upon a wealth of prior world knowledge. Consequently, there has been a growing interest in pre-trained models for CIL [55, 71, 72, 81, 86], harnessing the knowledge acquired from extensive, pre-existing datasets to address the immediate tasks at hand. For instance, prompt-based methods [55, 71, 72] utilize prompt-tuning to summrize task-specific knowledge from prior pre-trained knowledge. SLCA [81] and ADAM [85], on the other hand, solely fine-tuning the pre-trained model to adapt the pre-existing knowledge into the objectives of immediate tasks.\nTo tackle image classification downstream tasks, pre-trained models are traditionally derived from discriminative tasks, like supervised learning on datasets such as ImageNet-21K [49], or they may stem from self-supervised learning efforts [6, 9, 10, 22]. However, in our research, we venture into the paradigm of generative multi-modal models to address image classification tasks. Generative models like GPT4 [44] and LLaVa [32] have garnered significant attention in recent years due to their capacity to produce highly informative descriptions of input images. On the one hand, it can harness the wealth of semantic correspondences between texts and images, while on the other hand, there's no requirement to expand the classifier with each new task, unlike in the case of discriminative models for CIL.\nNonetheless, harnessing the knowledge from pre-trained generative models for downstream class-incremental learning (CIL) tasks presents a nontrivial endeavor. The primary challenge lies in transferring the generated textual information into the classification of distinct categories. Additionally, there's the task of formulating CIL within a generative framework, which poses a second significant challenge. Shao et al. [52] presents the VAG system, which formulates CIL as a continual label generation problem, preserving the language model's ability to learn new classes. However, it only works in the field of Natural Language Processing (NLP), which is inherently suited to Large Language Models (LLM). To the best of our knowledge, we are the first to apply this generative approach to incremental learning in the field of image classification.\nIn this work, we propose Generative Multi-modal Models (GMM) for class-incremental learning. As illustrated in Fig. 1 (a), conventional discriminative methods extract image features with a network backbone, then forward them to a classifier to obtain the probability of the image belonging to each label, with the label having the highest probability being the output of the discriminative models. While in Fig. 1 (b), we adopt a generative approach, which directly produces a descriptive sentence for the given image, which is then compared with the actual label texts with a text encoder. The most similar label becomes the predicted result of our generative model. This approach allows us to leverage the rich pre-training knowledge in generative multi-modal models while avoiding the use of the expanded classification head, which mitigates the risk of the model bias towards the current task and reduces catastrophic forgetting.\nThe main contributions of this paper are:\n\u2022 We propose a novel generative approach (GMM) to address class-incremental learning by leveraging multi-modal models.\n\u2022 We reformulate GMM for image classification and adapt it for the downstream benchmarks. Without an expanded classification head like in discriminative models, our model significantly mitigates the issue of bias towards current tasks, resulting in significantly reduced forgetting in CIL.\n\u2022 Our model achieves state-of-the-art performance across multiple datasets in both conventional and few-shot CIL settings."}, {"title": "2.1. Class-Incremental Learning", "content": "In Class-Incremental Learning, tasks arrive sequentially, and each class is exclusive to a specific task without any overlap. The goal is to acquire knowledge from new classes while preserving information from previously encountered classes. There are three primary branches in CIL [14], including rehearsal-based, architecture-based, and regularization-based methods. Rehearsal-based methods [1, 7, 48, 75] store a small set of data derived from old classes to represent knowledge from previous tasks. These exemplar data can be either original data [48], generative data [18, 54] or hidden features [20]. Architecture-based methods focus on modifying network architecture to alleviate forgetting. Approaches include learning redundant network architecture [17, 47], learning different expert networks [3, 50] or parameters [38, 40, 51] for each task, dynamically expanding network parameters to accumulate incremental knowledge [76]. Regularization-based methods introduce an additional regularization term to restrict network updates when adapting to new tasks. In such cases, EWC [26], SDC [78] and Rotated-EWC [35] expect that parameters essential for the old tasks should not be updated excessively. Moreover, from the perspective of network output consistency, numerous studies [25, 30, 36, 61, 80] incorporate distillation to prevent forgetting.\nFew-shot CIL Few-shot Class-Incremental Learning (FS-CIL) [42, 62] explores few-shot learning in an incremental context, with all data samples available for base session and very limited data in each incremental session. Some FSCIL methods [11, 62, 83] train the model in both base and incremental sessions, aiming to mitigate overfitting challenges caused by the limited data in incremental learning. Other strategies [53, 79, 90] primarily train the model in the base session and make minimal adjustments in the incremental sessions, thereby reducing forgetting but may come at the cost of decreased precision in the incremental sessions."}, {"title": "2.2. Pre-trained models for CIL", "content": "There are many methods [63, 72, 73] having shown that pre-trained models are effective for continual learning. One main branch trains a set of prompts to retain previous knowledge [55, 68, 71, 72]. A selected subset of prompts are fed into the model during forward to prompt model the past knowledge. Additionally, methods like SLCA [81] and ADAM [85] fine-tune pre-trained models, achieving impressive results with less forgetting. Continual-CLIP [63] demonstrates that the CLIP [46] model is capable of performing continual learning without any extra training. This highlights the significant potential of multi-modal pre-trained models in the realm of continual learning. Inspired by this, many methods [37, 86] employ CLIP as the backbone to utilize the multi-modal information. However, if not using a classifier directly, these approaches need to utilize expanded text features to calculate distances with image features for classification. This will exacerbate the model's bias towards current data, consequently leading to the forgetting of previously acquired knowledge. To avoid this bias, we use a generative model to directly generate prediction text. The fixed text decoder will function as the classifier, significantly alleviating the bias."}, {"title": "2.3. Vision Language Models", "content": "In recent years, vision-language multi-modal models have made significant progress and achieved impressive results in various downstream tasks [5, 28, 34, 70]. Traditional vision-language models employ different types of encoders to extract information from vision and language models, including single-stream [57], dual-stream [39] and fusion [60] encoders. A key aspect of vision-language models is the alignment of multi-modal features. CLIP [46], for example, extracts image and text features separately using respective encoders and enforces alignment through a contrastive loss, ensuring alignment between positive image-text pairs in the feature space. VisualGPT [8] and Frozen [65] leverage pre-trained models as encoders for visual-language tasks. From then on, the utilization of pre-trained models in vision-language tasks became more and more popular. For instance, Flamingo [2] and BLIP-2 [27] align the pre-trained image and text encoders employing gated cross-attention and Q-Former, respectively. Furthermore, LLaVA [32] and MiniGPT-4 [88] leverage more robust Large Language Models (LLM) [13, 64] as text encoders, while only training a projection layer for alignment. With the increasing popularity of LLM, an increasing number of studies [4, 69, 87] explore the potential of multi-modal LLMs for vision-language tasks."}, {"title": "3. Method", "content": "In this section, we introduce the preliminaries of class-incremental learning and generative multi-modal models. Then, we present our approach to leverage generative models for CIL and the corresponding learning process."}, {"title": "3.1. Preliminaries", "content": "Class-Incremental Learning. Given N tasks T = {T1, T2, ..., TN}, the goal of class-incremental learning is to learn each task Tt with its associated data {Xt, Yt} in a sequential order. For each task, it contains samples {xi, yi}, i = 1, ..., nt, where xi is the images and yi is the corresponding one-hot labels. Typically, Xi\u2229Xj = 0, \u2200i \u2260 j. At inference, the model is tested on all seen tasks without task IDs. In some scenarios, fixed memory storage is set to keep a few samples of previous tasks to prevent forgetting. Normally, a CIL model consists of a feature extractor and a classifier head F = {fo,H$} which are parameterized by {0, $}. In traditional class-incremental learning, 0 is usually a modified ResNet [21] with all parameters tunable. In pre-trained or prompt-based methods, 0 represents fewer trainable parameters like a linear adaptor or a couple of prompts. & is a linear classifier head projecting image features to probability predictions, which has to be expanded for each new task in order to make predictions for the new classes. The conventional Cross Entropy loss is often used for updating 0 and $, which for task t is:\nLCE (Xt, Yt; 0, \u03c6) = -1/Nt  \u2211_(i=1)^(Nt) yi log H (f (xi;0) ; \u03c6).\nIn the continual learning process, the parameter & can easily deviate to the data of the current task due to the absence or scarcity of old samples in the previous tasks, resulting in forgetting the previously acquired knowledge and deteriorating the overall performance.\nGenerative Multi-Modal Models (GMM). Multi-modal models have demonstrated exceptional performance in generating detailed image descriptions by incorporating both visual and textual information. Notably, GPT-4 [44] stands out as an advanced model proficient in generating comprehensive image descriptions and providing explanations for the depicted content. Furthermore, MiniGPT-4 [88] proposes a two-stage fine-tuning process that aligns image features and large language models, enabling LLaMa [64] to recognize images and conduct further dialogue based on the image content."}, {"title": "3.2. Generative Multi-Modal Models for CIL", "content": "We adhere to the foundational settings of MiniGPT-4, incorporating a frozen image encoder fenc followed by a trainable projection layer for adaptation to downstream tasks, as shown in Fig. 2. Our primary innovation involves the direct utilization of generative models to produce text, which can then serve as a basis for discriminative classification. However, two major challenges need addressing. First, there is the issue of adapting generative multi-modal models for classification, given that the generated text may differ significantly from class names. Second, we must devise a mechanism for our classification benchmarks to learn in a manner consistent with generative multi-modal models. We introduce these two aspects as follows.\nTurning GMM for classification. We employ a distance metric to bridge the gap between generative and discriminative models. During training, we use the real label's text to encourage the model to predict the label of an image with a concise and accurate sentence in the format of \"This is a photo of [CLS].\" avoiding detailed descriptions of all contents in the image. During testing, the model follows the format to output the category text for a given image. We extract the content in \"[CLS]\", then obtain its text features using the CLIP [46] text encoder ftext, and compute the distance with text features of all categories seen by now. The closest class was then considered the final prediction of the generative model.\nConverting CIL Benchmarks for adaptation. CIL is usually evaluated on ImageNet, CIFAR-100, and ImageNet-R datasets. These datasets usually consist of images and corresponding one-hot labels {Xt, Yt}. Using the CIFAR100 dataset as an example, we pair each image with a sentence to form an image-text pair format {Xt, St} with the template: \u201cThis is a photo of [CLS]\u201d, where \u201c[CLS]\u201d is the label name of that category, such as apple, dog, etc. Next, we partitioned the 100 classes into various tasks based on different settings and fed them into the model sequentially. After completing the training on task T, the model should be capable of classifying all the classes encompassed from task 0 to task T. Note that only the linear projection layer is updated for further adaptation."}, {"title": "3.3. Optimization and Inference", "content": "Optimization. For each task t, we obtain the current task's image-text pair {Xt, St}, where St contains the corresponding sentence of each image. During training, we first utilize a tokenizer to tokenize and acquire the embedding of the questions and answers. We leverage pre-trained encoder fenc and the projection layer to obtain the corresponding features for the input images:\nei = fenc(xi; denc).\nThen, the question embedding and the ground-truth embedding of this question, e.g., \"This is a photo of [CLS]\", is concatenated with image embedding. The final input of LLM Decoder fdec is:\n\u00ea\u00bf = CONCATE(bos, ei, q, s, eos).\nbos is the symbol of the sentence beginning, and eos is the symbol for the end of the sentence. This encourages tokens at positions m \u2212 1 to predict token m:\nP(\u00a71, 82, ..., \u015dm|xi, q,s) = \u220f_(j=1)^(m-1) P(Sj|ei, q, S1, S2, ..., Sj\u22121),\nwhere si indicates the ground-truth answer token and \u015dm is the generated prediction. Then, we can compute the Cross Entropy loss as follows:\nLCE = 1/m \u2211_(j=1)^(m) sjlog \u015dj.\nInference. During inference, we use the updated projection layer in conjunction with the pre-trained encoder to obtain image features. These image features, combined with the question embeddings are then passed to the LLM Decoder to obtain the text output.\npred = argmax < ftext(s), ftext(s) >,\nwhere ftext is the text encoder, <, > is the cosine similarity used to calculate the final predictions pred."}, {"title": "4. Experiments", "content": "We conduct experiments in both conventional CIL and Few-shot CIL scenarios. In conventional CIL, we evaluate on three datasets. CIFAR100, Tiny-ImageNet and ImageNet-R. CIFAR100 contains 60,000 images of 32x32 pixels in 100 categories. Each category has 600 images, of which 500 are for the training set, and 100 are for the test set. We experiment with two settings, B0-n and B50-n. The former splits 100 classes into n tasks,"}, {"title": "4.2. Experiments on Conventional CIL", "content": "In Table 1, we can see that our method outperforms all conventional methods by a large margin, including ResNet-based method DER and ViT-based DyTox. Note that without exemplar, our performance is a bit lower than Dual-Prompt and CODA-Prompt at B100-5 setting. We argue that their performance is mainly due to the backbone pre-trained on ImageNet-21K, which largely overlaps with CIFAR100 and Tiny-ImageNet. Another interesting observation is that our method has better performance than all baselines under longer sequence settings (B100-10, B100-20). We believe this is because generative models do not rely on classification heads, making them less prone to bias toward the current task, resulting in less forgetting of past tasks. The Linear probe setting performs less than our method, indicating that our main contribution is not from the Large pre-trained ViT but the generation pipeline. In addition, the Zero-shot performance is superior to many traditional baselines, meaning that the Generative Multi-modal Models are indeed efficient Class-Incremental learners, but its output is less concise without fine-tuning (see Fig. 4).\nIn Fig. 3, we compared our approach with some pre-trained models on CIFAR100 and Tiny-ImageNet in terms of last task accuracy (all baselines are based on PILOT [58] and use 2000 exemplars). It can be observed that our method does not outperform other approaches in the initial tasks (0-2) and short sequence settings (B0-5, B100-5). This is because we do not rely on a supervised ImageNet-21K pre-trained backbone. Besides, we trained each task for only 1-2 epochs to ensure efficiency without sacrificing generalization. However, our method exhibits significant advantages in long sequences and later tasks. For instance, under the CIFAR100 B0-20 setting, we outperform CODA-Prompt by 10 points and DualPrompt by 7 points."}, {"title": "4.3. Experiments on Few-shot CIL", "content": "In Table 2, we compare our method with several baselines in the few-shot setting on mini-ImageNet. The evaluation metric is the model's accuracy across all the classes it has encountered so far. Our method outperforms conventional methods by a substantial margin in the final task, achieving an increase of more than 26%. Furthermore, we surpass the best discriminative pre-trained approach CODA-Prompt by more then 14% points. It's important to note that our accuracy in the first task (89.35) might not be as high as CODA-Prompt (95.37). However, in subsequent sessions, we consistently perform better than CODA-Prompt due to our ability to learn new tasks and retain knowledge of old tasks simultaneously.\nIn Table 3, our method outperforms all other baselines on the CIFAR100 dataset of the few-shot setting, achieving a remarkably lower Performance Drop (PD) of 10.06. Furthermore, the Zero-shot baseline can achieve a very low PD due to the absence of forgetting. However, its overall performance is not very satisfying, as the length and content of its output are inconsistent and unpredictable without fine-tuning."}, {"title": "4.4. Visualizations", "content": "In Fig. 4, we present some comparison examples of our methods against GMM without fine-tuning [88]. We can see that GMM without fine-tuning provides an intuitive description of the overall image content with varying lengths of output text. However, it tends to recognize only broad categories (e.g., biar, car) and struggles with fine-grained categorization (e.g., pelican, truck). The descriptions are sometimes somewhat repetitive (e.g., first fire engine). In contrast, our fine-tuned method accurately identifies the image's real category, even if there are occasional discrepancies with the true labels (e.g., \"fire engine\" vs. \"fire truck\"). Besides, with the assistance of the text encoder during the testing phase, our model can achieve correct classification results even when predicting similar but not identical text."}, {"title": "5. Conclusion", "content": "In this paper, we propose GMM to use generative models for class-incremental learning. By fine-tuning the Generative Multi-modal Model (GMM), we directly generate the label text of the images to be classified. Then we select the label most similar to the generated text by its features. Our experiments demonstrate that this method, which does not require a classification head, is highly effective in addressing classification biases in continual learning.\nLimitations. Since we are the first to introduce generative models to class-incremental learning, the overall design of our method is embarrassingly simple. We believe that with more focused efforts in this direction, there will be significant advancements in the field of continual learning.\nBroader impact. We believe that introducing GMM into continual learning (CL) is both necessary and urgent. With the rapid development of GMM, we can leverage their capabilities to improve the performance of continual learning. Besides, integrating CL methods into the training process of GMM could significantly reduce training costs."}]}