{"title": "The advantages of context specific language models\nthe case of the Erasmian Language Model", "authors": ["Jo\u00e3o Gon\u00e7alves", "Nick Jelicic", "Michele Murgia", "Evert Stamhuis"], "abstract": "The current trend to improve language\nmodel performance seems to be based on\nscaling up with the number of parameters\n(e.g. the state of the art GPT4 model has\napproximately 1.7 trillion parameters) or\nthe amount of training data fed into the\nmodel. However this comes at significant\ncosts in terms of computational resources\nand energy costs that compromise the sus-\ntainability of AI solutions, as well as risk re-\nlating to privacy and misuse. In this paper\nwe present the Erasmian Language Model\n(ELM) a small context specific, 900 mil-\nlion parameter model, pre-trained and fine-\ntuned by and for Erasmus University Rot-\nterdam. We show how the model performs\nadequately in a classroom context for essay\nwriting, and how it achieves superior perfor-\nmance in subjects that are part of its con-\ntext. This has implications for a wide range\nof institutions and organizations, showing\nthat context specific language models may\nbe a viable alternative for resource con-\nstrained, privacy sensitive use cases.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have been\none of the most visible machine learning ap-\nplications of the past decade, attracting a\nlarge amount of academic research and in-\ndustry investment in search of higher per-\nformance and productivity gains. The dom-\ninant trend in improving language mod-\nels seems to be based on scaling up the\nnumber of parameters (e.g. GPT4 has\napproximately 1.7 trillion parameters) or\nthe amount of training data fed into the\nmodel (e.g. the Common Crawl dataset\ncurrently has more than 75 TB of textual\ndata). However, this scaling usually comes\nat significant costs in terms of computa-\ntional resources, which translate to energy\n(Emma, Ananya, & Andrew, 2019), finan-\ncial, and environmental costs, and data col-\nlection, which in turn raise concerns re-\ngarding legal, privacy, quality, and respon-\nsibility (Palacios Barea, Boeren, & Fer-\nreira Goncalves, 2023) trade-offs.\nWhile there has been progress in increas-\ning the efficiency of training and infer-\nence of language models through model ar-\nchitecture improvements (Hoffmann et al.,\n2022), and processes such as quantization\n(Dettmers, Lewis, Belkada, & Zettlemoyer,\n2022) and distillation (Sanh, Debut, Chau-\nmond, & Wolf, 2019), these do not address\nthe overall trend of an exponential growth\nin terms of use of resources. Furthermore,\nnormalizing a high volume of computational\nresources for model training also raises is-\nsues in terms of AI governance, where the\nadvanced capabilities of LLMs will be con-\ncentrated in the few select organizations\nthat can acquire a large number of GPUs re-\nquired for model training and deployment.\nIn this paper, we explore an alterna-\ntive path to LLM adoption, harnessing a\nsmall, tailored and community driven lan-\nguage model that only excels at the tasks it\nwas designed to do. We call this a context\nspecific model, where training data, model\nsize, fine-tuning data, testing and deploy-\nment are guided by the context that the\nmodel is created for. We this context spe-\ncific model, we aim to address limitations of\nlanguage models related to efficiency, use of\nresources, privacy and ethics.\nConsidering recent findings on how data\nquality is a key component of LLM per-\nformance (Gunasekar et al., 2023), we pre-\ntrain the Erasmian Language Model (ELM)\na 900M parameter Llama-2 based language\nmodel exclusively on the data of Erasmus\nUniversity Rotterdam, a large Dutch higher\neducation institution. We then fine-tune an\ninstruct model with the Alpaca dataset, in\nEnglish and in Dutch. We show that, de-\nspite having a much more limited number\nof parameters and training data, this model\nspecializes in the subjects taught and re-\nsearched at Erasmus University, and is suit-\nable for classroom activities. The model\nproduces credible text in the domains of\nSocial Sciences, Humanities, and Medicine,\nbut fails at any other topic, including simple\nreasoning tasks. As a byproduct of data and\nusage control, we also show that the model\ndoes not require the standard guardrails\nusing reinforcement learning with human-\nfeedback."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Improving language\nmodel efficiency", "content": "The performance of LLMs is usually ac-\ncessed by means of standardized general\nbenchmarks such as Massive Multitask Lan-\nguage Understanding (MMLU) (Hendrycks\net al., 2021), where large players in the\nfield compete to top others by expanding\nthe number of parameters (Achiam et al.,\n2023), number of tokens in the training data\n(Zhang, Zeng, Wang, & Lu, 2024), improv-\ning model architecture (De et al., 2024),\namong other strategies to increase perfor-\nmance. However, the environmental, re-\nsource, and privacy implications of a pur-\nsuit of scale in LLMs have been increasingly\nhighlighted as relevant values in addition to\nperformance (Tokayev, 2023), shifting the\nfocus to model efficiency and trustworthi-\nness.\nAn important part of the search for effi-\nciency gains has been to find compute opti-\nmal calculations for model size and number\nof tokens, that aim to ensure LLM are not\noverparemeterized (Hoffmann et al., 2022).\nWhile this does not necessarily address the\npursuit of scale on its own, it does make\nit so that model training is designed in a\nmore balanced and efficient way. From the\ninference side, techniques like quantization\n(Dettmers et al., 2022), representing model\nweights at a lower precision, or knowledge\ndistillation (Bucilu\u0103, Caruana, & Niculescu-\nMizil, 2006), training smaller models based\non larger models, have shown promise in\ndecreasing the computational and energy\ncosts of model deployment. These efforts\naim to improve the efficiency of model train-\ning or inference, but do not directly address\nthe need for larger models.\nA second strand of LLM research as\nfocused on training smaller models that\nmatch or outperform larger language mod-\nels. One high profile effort along this line\nhas been Microsoft's training of its Phi fam-\nily of models (Gunasekar et al., 2023). The\nassumption for these models is that data\nquality is more important than data quan-\ntity or model size to determine the quality\nof outputs. However, when we examine Phi-\n3 (Abdin et al., 2024), the latest iteration\nof the Phi models, we see that while a small\nmodel is technically trained, the actual size\nof the training data surpasses 3 trillion to-\nkens, which to some extent defeats the effi-"}, {"title": "2.2 Context specific language\nmodels", "content": "An alternative in pursuing efficiency gains\nwith large language models is to abandon\ntheir general purpose nature in order to\ntrain context specific models, that still ex-\ncel a a wide range of tasks within that con-\ntext. The underlying assumption behind\nthe performance of large language models\nis that, through transfer learning (Pan &\nYang, 2009) and fine-tuning, large language\nmodels and their fine tuned versions will al-\nways outperform specifically trained models\n(Howard & Ruder, 2018). However, when\nwe consider performance also accounting for\nenvironmental, privacy and other costs, this\ngeneral purpose pre-training and domain\nspecific fine-tuning becomes a less attrac-\ntive option.\nOne of the most popular domains where\ncontext specific models emerge is in pro-\ngramming code. Some of these mod-\nels (Li et al., 2023) are pre-trained and\nfine-tuned only on code and, while they\nstill possess a large number of parameters\nand training data, they mitigate some of\nthe computational costs that general pur-\npose models have by restricting the train-\ning dataset. These code models, however,\nstill have copyright and privacy implica-\ntions that stem from leveraging the work of\nGitHub contributors, where the ethics of us-\ning the data provided by users can be ques-\ntioned.\nA second strand of context specific mod-\nels leverage organization specific data to\npretrain custom language models. A\nnotable example is a language model\ntrained by financial information provider\nBloomberg on a mixture of their own data\nand general data (Wu et al., 2023). For this\nlanguage model Bloomberg validated the\nmodel not only in general purpose bench-"}, {"title": "2.3 Epistemic-functional\nproach", "content": "The decision to pursue context specific lan-\nguage models is not a purely technical one,\nit also represents a sift in framework and\nvalues under which LLMs are developed\nand used. Our approach to context-specific\nmodels focuses on their epistemic function-\nality. It looks at how the use of LLMs\nwithin knowledge processes of a given con-\ntext work and make sense to those in-\nvolved. They thus have to fit the epistemic\nneeds, wants and capacities of a commu-\nnity. Yet this prerequisite is never isolated\nfrom questions of power and so one has to\ntake the values and norms of the commu-\nnity into account. 'The public' as commu-\nnity is often related to values such as care\nfor the environment, privacy and human\ndignity. Whereas a corporation, notwith-\nstanding overlap with other communal val-\nues, will have distinctly other values, such\nas profit.\nWhat counts as functional is in sum\nstrongly intertwined with the sources of\nnormativity that underpin epistemic com-\nmunities. So if an LLM is trained for an aca-\ndemic community, as is the case here, its in-\nvolvement is a necessity. Especially because\nnorms are objects of contestation - one can-\nnot treat a community as an epistemic en-\ntity, nor purpose-specific models as func-\ntional, when norms are taken to be static.\nAn epistemic-functional approach is thus as\nmuch concerned with LLMs as objects as it\nis as processes.\nOur approach bears similarities with the\nmore familiar value sensitive design (VSD).\nVSD aims to explicitly integrate values into\nthe design of technologies, lest it runs the\nrisk of becoming undesirable (Friedman &"}, {"title": "3 Methods", "content": "To address the limitations of related work,\nwe propose to train a language model, the\nErasmian Language Model (ELM), on data\nthat originated fully from a middle sized or-\nganization such as Erasmus University Rot-\nterdam, validating it for the specific pur-\nposes of this institution, namely teaching\nand research. This offers a methodologi-\ncal template for organizations that face re-\nsource or privacy constraints in implement-\ning language models."}, {"title": "3.1 Model architecture", "content": "ELM was based on the state-of-the-art ar-\nchitecture at the time, LLaMA 2 (Touvron\net al., 2023). The number of hidden lay-\ners and attention heads was scaled down\nlinearly to train two versions of the model,\na small model with 160 million parameters\nand a larger one with 900 million parame-\nters, both in single precision 32-bit floating\npoint. The small model was intended to be\nused as a teaching tool, allowing students to\nfine-tune or train some custom versions of\nthe model in their own machines. The lan-\nguage model was trained to serve as a more\nbroader purpose model to support research\nand education in tasks such as idea genera-\ntion, essay writing, or assignment feedback.\nWe also trained our own version of the\nLLAMA 2 tokenizer. We follow LLaMa 2\nand use SentencePiece to map tokens to\na vocabulary size of 32K. This was aimed\nto keep the vocabulary size to the strictly\nnecessary so that tokens from languages ir-\nrelevant to our data were not considered,\nbut also to ensure that the domain specific\nperformance of the model was not compro-\nmised by an inadequate tokenizer."}, {"title": "3.2 Datasets", "content": "For pretraining, we compile a corpus of doc-\numents strictly produced by the Erasmus\nUniversity Rotterdam, to ensure that our\nmodel reflects the institution and its con-\ntexts. We collect data from two sources:\nthe EUR research output, and the Eras-\nmus University Thesis repository. The to-\ntal EUR corpus consists of roughly 2.7B to-\nkens and spans from 1969 to 2023. This\ndataset is particularly useful for Erasmus\nUniversity because it represents the knowl-\nedge of its academis through the Research\nOutput corpus, and the knowledge of its\nstudents through the Master Theses corpus.\nThis corpus cannot be made publicly avail-\nable because of copyright restrictions, the\ncustom made instruction and reinforcement\nlearning datasets are available on GitHub\nhttps://github.com/Joaoffg/ELM."}, {"title": "3.2.1 EUR Research output", "content": "We extracted all DOIs from the EUR CRIS\nsystem (Pure) and collected the PDFs. This\nresulted in 75.355 documents from 1977 to\n2023. Figure 1 shows the number of docu-\nments per year. The research output con-\nsists of 1.917.100.206 tokens. Documents\nare written mostly in English and in Dutch."}, {"title": "3.2.2 EUR Theses", "content": "The Erasmus university has it's own thesis\nrepository. We collected 16.983 thesis from\nthe repository that do not have any restric-\ntions to usage from 1969 to 2022. Figure 2\nshows the number of theses per year. The\nEUR Thesis dataset consists of 864.152.740\ntokens. Documents are written mostly in\nEnglish and in Dutch. Only public the-\nses were used. Theses that could sensitive\ninformation, such as confidential business\ndata, were removed from the dataset to pre-\nserve privacy."}, {"title": "3.2.3 Instruction dataset", "content": "For further instruction tuning of the foun-\ndation model we constructed a dataset con-\nsisting of 485 input-output pairs. These in-\nput pairs were constructed by student as-\nsistants, who were asked to write examples\nrepresenting how they would like the model\nto be used, they were paid the standard\nrates according to the Dutch Collective La-\nbor Agreement for Universities. We convert\nthe instruction pairs to match the alpaca in-\nstruction template.\nWhile the generated pairs were high qual-\nity, they proved insufficient for the model\nto properly learn how to follow instructions.\nTherefore, they were supplemented with the\ncleaned version of the Alpaca dataset (Taori\net al., 2023) and a version of it translated\nto Dutch (Vanroy, 2023)."}, {"title": "3.2.4 Reinforcement learning", "content": "We constructed a dataset for direct pref-\nerence optimization (Rafailov et al., 2023)\nwith the same student assistants who took\npart in the instruction dataset generation.\nThey were simply instructed to choose the\nbest output out of two generated by the\nmodel according to their judgement. Un-\nlike other models, we chose not to impose\nguardrails for the type of language being\ngenerated because: 1) Our pretraining is\nlimited to the type of language and doc-\numents we wish to produce, thus eliminat-\ning the need for post-training filtering; 2) In\ncertain academic domains, such as history,\nstudents and scholars have to engage with\nproblematic and outdated language, there-\nfore adjusting the model to avoid usage of\nthis language would not be desirable."}, {"title": "3.3 Training", "content": "We first experimented with fine-tuning\nMeta's Llama 2 model based on Erasmus\nUniversity data to avoid the computational\ncosts of the pretraining stage. However, the\nmodel performed inadequately for academic\npurposes, resorting to general information\nmore frequently than domain specific infor-\nmation required for Erasmus University.\nPretraining of ELM-small was conducted\non a single Nvidia A10 GPU for 3 epochs,\ntaking a total of 48 hours, using the datasets\nand transformers libraries from Hugging-\nFace. Pretraining of ELM-medium was also\nconducted on a single Nvidia A10 GPU for\n3 epochs, taking a total of 720 hours (30\ndays) of training. Training was conducted\non the secure SURF research cloud provided\nto Dutch universities, showing that training\nof context specific models is feasible with\nlimited access to resources.\nFine-tuning of ELM-medium happened\nin the same A10 GPU, using low rank adap-\ntation (LoRA) to fine tune the model as\nan instruct version of ELM. This version\nwas then further trained with reinforcement\nlearning with human feedback (RLHF) us-\ning direct preference optimization with the\nHuggingface's TRL library.\nTo verify that the model aligned with\nthe goals and values of the community it\nwas destined to, Erasmus University, ELM-\nsmall was presented in a public event where\nparticipants could interact with the model.\nThis ensured that any interested parties had\nan opportunity to voice concerns and ex-\npectations regarding the model before the\nlarger model was trained."}, {"title": "4 Results", "content": "ELM was trained exclusively for the pur-\nposes of Erasmus University Rotterdam,\nwhich makes assessing it performance\nthrough traditional benchmarks such as\nMMLU less relevant for the task at hand.\nTherefore, in addition a MMLU assessment,\nwe have also conducted a qualitative as-\nsessment of the model with its student end\nusers."}, {"title": "4.1 Qualitative assessment", "content": "Qualitative assessment of ELM took place\nin the scope of a bachelor minor on the topic\nof artificial intelligence where students were\ntasked to write an essay with the support\nof the language model. They were then\nasked to reflect on the model, focusing on\nits advantages and disadvantages. The ver-\nsion under assessment was ELM-small, to\nfacilitate scaling up local deployment of the\nmodel (100 students enrolled in the minor).\nTo prevent any social desirability bias in the\nevaluation caused by the fact that the main\ndeveloper of the model was also the teacher\nassessing student submissions, the assign-\nment was embedded in the course but was\nnot graded. After submitting the assign-\nment, students were asked if they would like\ntheir assessment to be part of the research\npaper. 23 students consented to this.\nStudents considered that the outputs of\nthe model were coherent and were very\naligned with academic language. However,\nthey emphasised that the model had a ten-\ndency to go off topic at times, potentially\ncaused by the boundaries of the training\ndata coverage. \"Although the output that\nwas generated did not answer the question\nper se, the output is coherent and easy to\nread.\"\nStudents also identified a limitation of the\nmodel in terms of its ability to generate co-\nherent long texts. This shows that our de-\ncision to truncate text chunks to 256 tokens\nlimits to some extent the model's applica-\nbility to academic purposes, where longer\ntexts are typically the norm. When con-\nfronted with more abstracts questions, stu-\ndents also noted a tendency for it to make\nup words or references, losing coherence and\nsentence structure. The shortcomings and\nadvantages of ELM-small are well summa-\nrized by a student, who mentioned: \"the\ntext ELM produced has little to do with a\nwell written, cohesive essay. [...] Yet, when\ncompared to other LLMs if felt as if ELM\nemulated my writing style much more accu-\nrately, possibly due to our shared source of\neducation\".\nIn summary, students had different expe-\nriences with ELM in terms of coherence and\nlanguage level, depending on how closely\nthe used prompts related to Erasmus Uni-\nversity topics, and the generation parame-\nters they specified, particularly max length,\nrepetition penalty, and temperature. How-\never, there were consensus in recognizing\nthe language as distinctly academic, and\nalso closer to the student's own writing."}, {"title": "4.2 Quantitative assessment", "content": "As mentioned above, general purpose\nbenchmarks are less suitable to assess the\nperformance of ELM due to its domain and\ncontext specific nature. However, we can\nresort to multidisciplinary benchmarks such\nMassive Multititask Language Understand-\ning (MMLU) (Hendrycks et al., 2021) to\nverify if the model displays the expected\nbehavior, i.e. specializing in the disciplines\ncovered by Erasmus University: social sci-\nences, humanities, and medicine.\nTo run the MMLU benchmark on ELM,\nwe fine-tuned the ELM-medium model on\nthe training set of the MMLU benchmark\nfor 1 epoch using LoRA. The benchmark\nhas a baseline performance of 0.25, given\nthat each multiple choice question has 4 an-\nswer options. However, even after the fine-\ntuning, the model did not always output\nanswers within the specified options of 0-3,\ncausing values to sometimes be below 0.25.\nHowever, as mentioned previously, our goal\nwith using the MMLU benchmark is not to\nuse it as a comparison term to other mod-\nels, but to verify that it is specializing in\nthe domains that are covered by Erasmus\nUniversity Rotterdam.\nThe MMLU benchmark is comprised of\n57 different subjects, ranging from abstract\nalgebra to world religions. The bench-\nmark creators themselves assigned broader\ncontainer fields to these subjects, namely\nSTEM, Social Sciences, Humanities and\nOther. Given that the subject coverage at\nErasmus University does not linearly over-\nlap with these categories, we also conducted\nour own subject categorization, labelling\nsubjects as EUR or non-EUR. A detailed\ntable with the performence per subject, and\nthe corresponding categorization, can be\nfound at the end of this document in Ta-\nble 1.\nFig. 3 shows that the performance of\nELM in the overall benchmark is well below\nstate of the art models, but this value is less\nrelevant for our assessment given that the\nbenchmark itself minimally overlaps with\nEUR research. The relative performance\nbetween subjects, however, shows that the\nmodel specializes in topics related to the so-\ncial sciences and especially the humanities,\nin line with the profile of Erasmus univer-\nsity. When we compare the subjects that\nare covered by EUR with non-EUR sub-\njects, the difference is even clearer, with\nEUR subjects scoring .26 accuracy and non\nEUR subjects having the lowest score at\n.23. Despite not being large, this difference\nshowcases how the model is specializing in\nsubjects covered by EUR, achieving its in-"}, {"title": "4.3 COMPASS assessment", "content": "To assess to what extent the ELM repre-\nsents an improvement over other language\nmodels in terms of trustworthiness we\nused the COMPASS framework, devel-\noped under the H2020 EU SPATIAL\nproject https://spatial-h2020.eu/\ncompass-questionnaire/. COMPASS\nis a self-reflective AI evaluation tool that\nallows developers to situate their systems in\nterms of trustworthiness at several points of\nthe development process under 7 criteria.\nThe COMPASS assessment as of July\n2024 is presented in Fig 4 and available\nathttps://github.com/Joaoffg/ELM/\nblob/main/The-Compass-Questionnaire\n_ELM_2024_07_03.pdf.\nAs expected and intended, ELM scores\nhighly on most of the criteria, especially in\nterms of context definition, privacy and sus-\ntainability. However, the assessment tool\nalso allows us to identify key areas of im-\nprovement for the model, namely in terms\nof more clearly defining measures and met-\nrics for success and failure, and also embed-\nding explainability methods in the deploy-\nment of the model."}, {"title": "5 Conclusion", "content": "The ELM experiment aimed to show that,\nby focusing on domain and context spe-\ncific data, acceptable performance could be\nachieved with language models even in the\npresence of significant resource constraints.\nThe model was trained and deployed on a\nsingle A10 GPU, and all project costs re-\nlated to the version presented in this pa-\nper amount to less than 4000 euros. When\ncompared to large commercial models of\ncompanies that have a magnitude of re-\nsources that is much larger, ELM is inferior\nin many aspects. However, when looking\nat the qualitative and quantitative perfor-\nmance of ELM in relation to its context of\napplication, indicators show that even with\na very small commitment of resources the\nmodel already delivers useful performance\nin essay writing, and in acquiring some level\nof specialized knowledge.\nThe ELM prototype also showed the ben-\nefits of an iterative development process,\nwhere usage and feedback from students in\na classroom context showed, for instance,\nthat increasing the length of the text chunks\nfed to the model was a crucial require-\nment for academic applications. The flex-\nible nature of small, context-bound, lan-\nguage models means that it is easier to iter-\nate through different versions of the model\nbased on feedback, and that major issues\ncan be addressed at a model training stage.\nThis mitigates the need for corrective mea-\nsures after deployment, such as the editing\nof prompts to ensure greater diversity of\noutputs (Kirk et al., 2023). A well defined\ncontext of use facilitates the alignment of\nLLM development with end-user expecta-\ntions.\nELM is a proof of concept that shows\nthat organizations with limited resources,\nbut high requirements in terms of efficiency,\nprivacy, and trustworthiness, should look\ninto context specific language models as\na viable alternative to large commercially\ndriven language models. It is important to\nnote that ELM itself was not intended to\nbe more than this proof of concept, with\na deployment version of the model requir-\ning further custom development by, for in-\nstance, relying less on the alpaca dataset for\nfine-tuning. However, even at its proof of\nconcept stage, ELM has already proven its\nusefulness in classroom and research envi-\nronments within Erasmus University Rot-\nterdam, fulfilling its purpose as a context\nspecific model. Its smaller size and the\ntraceability of its training data also mean\nthat many of the privacy and impact con-\ncerns that affect other language models do\nnot pose themselves in the context of ELM,\nshowcasing the some of the advantages of\nits context specific approach in addition to\nperformance."}, {"title": "7 Author contributions", "content": "Jo\u00e3o Gon\u00e7alves coordinated the technical\ndevelopment of ELM and wrote the ma-\njority of the paper. Nick Jelicic devel-\noped a substantial part of the code and ac-\nquired the data. Michele Murgia was the\noverall project lead, wrote the section on\nthe epistemic-functional approach and man-\naged student assistants. Evert Stamhuis\nprovided insights to the project and facil-\nitated collaboration and coordination."}]}