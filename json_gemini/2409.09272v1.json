{"title": "SafeEar: Content Privacy-Preserving Audio Deepfake Detection", "authors": ["Xinfeng Li", "Kai Li", "Yifan Zheng", "Chen Yan", "Xiaoyu Ji", "Wenyuan Xu"], "abstract": "Text-to-Speech (TTS) and Voice Conversion (VC) models have exhibited remarkable performance in generating realistic and natural audio. However, their dark side, audio deepfake poses a significant threat to both society and individuals. Existing countermeasures largely focus on determining the genuineness of speech based on complete original audio recordings, which however often contain private content. This oversight may refrain deepfake detection from many applications, particularly in scenarios involving sensitive information like business secrets. In this paper, we propose SafeEar, a novel framework that aims to detect deepfake audios without relying on accessing the speech content within. Our key idea is to devise a neural audio codec into a novel decoupling model that well separates the semantic and acoustic information from audio samples, and only use the acoustic information (e.g., prosody and timbre) for deepfake detection. In this way, no semantic content will be exposed to the detector. To overcome the challenge of identifying diverse deepfake audio without semantic clues, we enhance our deepfake detector with real-world codec augmentation. Extensive experiments conducted on four benchmark datasets demonstrate SafeEar's effectiveness in detecting various deepfake techniques with an equal error rate (EER) down to 2.02%. Simultaneously, it shields five-language speech content from being deciphered by both machine and human auditory analysis, demonstrated by word error rates (WERs) all above 93.93% and our user study. Furthermore, our benchmark constructed for anti-deepfake and anti-content recovery evaluation helps provide a basis for future research in the realms of audio privacy preservation and deepfake detection.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in text-to-speech (TTS) and voice conversion (VC) technologies have enabled the generation of highly realistic and natural-sounding speech, imitating specific individuals saying things they never actually said. However, such technologies have been misused to create audio deepfakes, posing significant security threats. For instance, deepfakes disseminated on the Internet can manipulate public opinion, serving purposes like propaganda, defamation, or terrorism [55, 74]. Besides, audio deepfake fraud in calls and virtual meetings, including a notable UK case where $35 million was stolen using a cloned CEO's voice [9], has financially affected 7. 7% individuals, according to a 2023 McAfee survey [54]. These have spurred the development of diverse audio deepfake detection models, designed to discern synthetic from genuine voices and promptly alert potential victims. However, existing works [12, 36, 53, 76, 86] typically take audio waveforms or spectral features (e.g., LFCC [63]) as inputs, which require accessing complete speech information. These approaches, while efficient, raise substantial privacy concerns due to the potential exposure of private speech content, particularly in virtual communications that involve user privacy like business secrets or medical conditions [31]. Thus, despite current detectors' utility in thwarting deepfakes, there is natural hesitancy in using them due to the risk of content leakage.\nIn this paper, we introduce SafeEar\u00b9, a novel framework designed to effectively detect audio deepfakes while preserving content privacy. As shown in Figure 1, the key idea of SafeEar is to decouple speech into semantic and acoustic information. This approach enables reliable deepfake detection using processed acoustic information while preventing potential adversaries from accessing the semantic content, even if they employ advanced automatic speech recognition (ASR) models or human auditory analysis. Thus, SafeEar is particularly suited for third-party audio service scenarios where an honest-but-curious server might offer reliable deepfake"}, {"title": "2 BACKGROUND", "content": "2.1 Audio Deepfake Generation\nDeepfake audios are generated using either text-to-speech (TTS) or voice conversion (VC), where the deployment of deep neural networks (DNN) gradually becomes a dominant method that achieves much better voice quality.\n2.2 Audio Deepfake Detection\nAudio deepfake detection is a critical machine learning task that focuses on identifying real utterances from fake ones. An increasing number of attempts [36, 76, 98] have been made to further the development of audio deepfake detection. As shown in Figure 2, existing mainstream studies on audio deepfake detection can be categorized into two types of solutions: pipeline detector and end-to-end detector. The pipeline solution [12, 63, 86, 104], consisting of a frontend feature extractor and backend classifier is well established. It extracts spectral features like MFCC and LFCC [63, 86], or token-level Wav2Vec2 features [91]. In recent years, end-to-end approaches [36, 76, 103] have attracted more and more attention, which integrates the feature extraction and classification into a single model. This unified approach optimizes the model using raw audio waveforms alongside corresponding real-or-fake labels. SafeEar lies in the pipeline detector group, which fills a gap in privacy-preserving deepfake detection methods.\n2.3 Speech Representation Decoupling\nSpeech information can be roughly decomposed into three components: content, speaker, and prosody [52]. Content is semantic information, which can be expressed using text or phonemes. Speaker and prosody features constitute the acoustic information. The former reflects speaker's characteristics such as timbre and volume, while prosody involves intonation, stress, and rhythm of speech, reflecting how the speaker says the content. Prior speech representation disentanglement methods mostly leverage a dual-encoder strategy [67], where speech is fed into parallel content and speaker encoders to obtain distinct representations. However, this strategy heavily relies on prior knowledge of given languages and speakers and potentially overlooks certain speech information like prosody, which may result in suboptimal decoupling, potentially leading to content leakage or insufficient detection clues. To tackle this issue, SafeEar presents a novel neural audio codec-based decoupling model that hierarchically decouples speech into semantic and acoustic tokens. It enables content privacy-preserving deepfake detection solely based on acoustic information. In-depth details of our design are elaborated in \u00a74."}, {"title": "3 THREAT MODEL", "content": "In this section, we introduce the application scenarios relevant to the SafeEar framework, and identify two malicious entities posing threats to users, i.e., the deepfake adversary (DA) and the content recovery adversary (CRA).\n3.1 Adversary Models\nApplication Scenarios. Third-party audio services have become popular in the market because of their advantages in providing specialized functionalities and flexible usage. However, the privacy concern of sharing raw audio with a third party is one of the primary factors preventing users from fully trusting these services, even if the service provider claims to not collect any data. For example, a deepfake detection service provider could be an honest-but-curious content recovery adversary (CRA), detecting deepfake audio to alert victims timely while unethically eavesdropping on conversation content.\nThe SafeEar framework is designed to relieve such privacy concerns, especially in using third-party audio services. Its frontend decoupling model can be examined and deployed by an entity that is already trusted in processing the raw audio data (e.g., the user's smartphone). Meanwhile, the backend deepfake detector can be operated by any untrusted entities (i.e., detection service providers). In this way, both the detection service and potential adversaries gain access only to the privacy-preserving acoustic tokens, rather than raw audio or unprotected features, which could be easily exploited to recover speech content.\nDeepfake Adversary (DA). The DA's goal is to generate audio that convincingly impersonates real human speakers (TTS) or mimics individuals familiar to the victim (VC). Employing sophisticated TTS and VC models, the adversary can acquire multiple speech samples from a target, using them for voice cloning or create realistic speech for various roles, such as customer service representatives. Moreover, The DA may engage in fraudulent activities on widely used instant communication platforms globally."}, {"title": "3.2 Defense Goal", "content": "To address the growing concern of deepfake audio in virtual communications, users require detectors to provide reliable alerts. However, there is a natural hesitancy in using them due to the risk of speech content leakage. SafeEar aims to alleviate this concern by extracting the content-irrelevant features, which can safeguard user content privacy while being suitable for effective detection. SafeEar's design shall meet two key requirements:\nDeepfake Detection: The deepfake detection model in SafeEar should be finely tuned to work with content-irrelevant features, guaranteeing reliable and accurate detection of deepfake audio.\nContent Protection: Features extracted by SafeEar should be resistant against content recovery attempts by CRAs, regardless of whether they employ machine-based or human auditory methods."}, {"title": "4 DESIGN DETAILS", "content": "4.1 Overview of SafeEar\nKey Idea. We aim to propose a framework that achieves two seemingly contradictory objectives: effective deepfake detection and prevention of any attempts at content recovery. Our key idea is to design a novel frontend feature extractor that can decompose speech information into mutually independent discrete representations, i.e., semantic and acoustic tokens, where only the latter being analyzed by subsequent deepfake detectors. Such acoustic tokens can enable effective deepfake detection, but nullify recovery attempts by both machine and human auditory analysis.\nIntuition Behind SafeEar. The idea of SafeEar is rooted in a critical insight: audio deepfake technology primarily concentrates on capturing the unique vocal attributes of a target speaker, such as timbre, loudness, rhythm, and pitch, which constitute acoustic information [52]. However, this technology typically overlooks\n4.2 Codec-based Decoupling Model (CDM)\nRecent advancements in neural audio codecs such as SpeechTokenizer [107], Encodec [22] and VALL-E [83] have provided evidence of the advantages of multi-layer residual vector quantizers (RVQs) in accurately representing speech with discrete speech tokens for high-quality and efficient audio transmission, regardless of sound type or language.2. We aim to develop the neural codec architecture into an effective decoupling model that separates mixed speech tokens into standalone semantic and acoustic tokens. As illustrated in Figure 4, our proposed decoupling model based on the codec architecture (CDM) comprises three core components: an encoder-decoder architecture, a HuBERT-equipped RVQs module, and a discriminator. The encoder-decoder's primary function of precisely reconstructing the original audio compels the encoder to extract the key features from speech signals. The HuBERT-equipped RVQs further decouple these features and hierarchically quantize them into discrete semantic and acoustic tokens. The discriminator enforces"}, {"title": "4.3 Bottleneck & Shuffle Layer", "content": "As shown in Figure 5, the frontend CDM of SafeEar initially encodes waveform inputs into discrete acoustic tokens, A, with each frame denoted as Ai. The bottleneck layer aims to reduce the dimensions of acoustic tokens A from $R^{7C \\times T_n}$ to a more compact space $A_b \\in R^{C \\times T_n}$ by using 1D convolution and batch normalization. This layer serves a dual purpose: first, it enhances computational efficiency and reduces trainable parameters, facilitating subsequent layers to operate on a compact representation; second, it acts as a regularizer,\navoiding over-fitting by limiting the amount of acoustic tokens and stabilizing it via batch normalization, before analyzed by the deepfake detector.\nIn addition to decoupling speech information, the shuffle layer serves to augment content protection by further scrambling the condensed acoustic tokens $A_b$. As shown in Figure 5, By randomly rearranging the elements across the temporal dimension $T_n$, this layer nullifies speech comprehension that is highly dependent on the temporal order of phonemes and words [51]. We empirically set a shuffling window of 1 second, corresponding to 50 frames, to obscure word-level intelligibility (as each token representation is extracted from a 20ms waveform). Thereby, the likelihood of attackers deciphering and correcting these sequences is extremely low, given the sheer number of possible permutations for a 4-second audio (50!4, approximately 8.56 \u00d7 10257, details are discussed in \u00a78). Our experiments also confirm the dual content protection by decoupling and shuffling, thwarting the advanced ASR techniques and human auditory analysis."}, {"title": "4.4 Acoustic-only Deepfake Detector", "content": "Recent studies [53, 98] have indicated that the potential of Transformers in audio deepfake detection using full-information audio waveforms. In our scenario, however, the absence of semantic information combined with shuffling-induced acoustic patterns disorder (e.g., timbre and prosody) presents a unique challenge in detection. To this regard, we develop a Transformer-based detector and determine its optimal 8 heads for Multi-Head Self-Attention (MHSA) mechanism [82]. This configuration allows the model to more effectively engage in long-range feature interaction and dynamic spatial weighting. It adeptly captures the slight differences between bonafide and deepfake audio. Moreover, it leverages parallel computation, allowing each attention head to independently process different aspects of the input feature space [44]. The aggregated features then form an attention spectrum, which is crucial for adaptively modulating features to more accurately detect deepfakes.\nAs shown in Figure 6, we propose the Acoustic-only Deepfake Detector (ADD), which focuses on determining the genuineness of audio by analyzing only the shuffled acoustic tokens $\\overline{A}$. Specifically, we first apply positional encoding to the sequence of shuffled acoustic tokens $\\overline{A}$ using sine and cosine alternating functions to enhance the MHSA modelling capabilities:\n$PE(\\overline{A}, 2i) = sin[\\frac{\\overline{A}}{10000^{(2i/C)}}]; PE (\\overline{A}, 2i + 1) = cos [\\frac{\\overline{A}}{10000^{(2i/C)}}].$  (2)\nwhere C denotes the token dimensions. We then feed $\\overline{A}$ into two sets of transformer encoders to process the sequence as a whole and capture global dependencies. Each set comprises two Feed-Forward Networks (FFNs), Multi-Head Self-Attention (MHSA), and Layernorm modules. The output from the Transformer encoders is finally directed to a fully connection layer, which determines whether the audio is a deepfake."}, {"title": "4.5 Real-world Augmentation", "content": "It is noteworthy that the deepfake-and-bonafide gap in waveform can be degraded by real-world factors. Although studies have shown negligible differences in audible audio patterns across microphones [34, 47], we identify that codec transformations in real-world telecom channels pose a significant challenge in distinguishing genuine from deepfake audio. To address this challenge, we have strategically incorporated a few representative codecs into our training pipeline. These include OPUS [80], known for its versatility and efficiency across audio types, and G.722 [56], renowned for high-quality voice transmission. We also utilize GSM for its widespread application in mobile communication, and both u-law and A-law [30] codecs, prevalent in North American, European, and international telephone networks. Additionally, we incorporate the MP3 codec [71], a popular lossy compression technique in digital audio but introducing distortions and artifacts. Our diverse codecs integration strategy enables SafeEar to handle unique distortions each codec introduces and potentially generalize to more unseen coding technologies. The enhanced training process promote SafeEar maintains high accuracy and reliability in various real-world scenarios, where codec-induced variations are prevalent. Our augmentation excludes physical multi-channel information [42, 43, 106] that is inapplicable to aid audio transmitted over the line.\n4.6 SafeEar Prototype\nWe have implemented a prototype of SafeEar using Pytorch 2.1 [65]. During the training phase, we initially train SafeEar's codec-based decoupling model on LibriSpeech dataset [64] utilizing four RTX"}, {"title": "5 BENCHMARK CONSTRUCTION", "content": "We develop a comprehensive benchmark to evaluate different systems in terms of defending against deepfake adversaries (DA), and content recovery adversaries (CRA). The benchmark includes three deepfake datasets (\u00a75.1), two anti-content recovery datasets (\u00a75.2).\n5.1 Comprehensive Deepfake Datasets\nTo ensure our deepfake benchmark datasets cover a broad spectrum of TTS/VC techniques, we select the well-recognized ASVspoof 2019 [87] and ASVspoof 2021 [92] databases. Additionally, seeing the need for a cross-language deepfake benchmark [98], we establish a large-scale multilingual deepfake dataset using the CommonVoice corpus, in English, Chinese, German, French, and Italian [4]. This dataset complements English-only ASVspoof 2019 and 2021 databases, forming a comprehensive benchmark (see Table 1).\n5.1.1 ASVspoof 2019 [87]: The ASVspoof 2019 LA subset comprises deepfake samples generated by 19 distinct TTS and VC systems. Adhering to the official guidelines, we use 6 deepfakes for training and the remaining 13 unseen deepfakes for testing.\n5.1.2 ASVspoof 2021 [92]: While sourced from ASVspoof 2019, the ASVspoof 2021 LA subset includes deepfake samples under more realistic conditions, where both bonafide and deepfake voice data are transmitted via telecom channels, e.g., VoIP. Its codec selection spans from traditional (e.g., a-law [30]) and modern IP streaming codecs (e.g., OPUS [80]) in use today, indicating mainstream usage.\n5.1.3 Multilingual CVoiceFake: Current deepfake datasets are mainly single language-based and most of them are English deepfake audio datasets like ASVspoof 2019 & 2021, and few of them encompass other languages, e.g., German or French. To facilitate cross-language"}, {"title": "5.2 Anti-Content Recovery Datasets", "content": "Our benchmark also includes multilingual datasets to assess the performance of SafeEar in protecting user content privacy. The lack of ground-truth text references in ASVspoof challenge samples limits accurate evaluation of anti-content recovery adversaries (CRA). We opt to utilize the widely adopted datasets in ASR tasks-LibriSpeech (English), and reuse CVoiceFake (English, Chinese, German, French, and Italian). Details are given in Table 1.\n5.2.1 LibriSpeech [64]: We utilize the train clean-100, clean-360, and other-500 subsets, totally extensive 960-hour corpus, for training CRA's ASR models. Then we test CRA's recovery ability using dev-clean, test-clean, and test-other subsets. These subsets offer a diverse range of accents and speaking styles in English, serving as"}, {"title": "6 EVALUATION: DEEPFAKE DETECTION", "content": "In this section, we focus on the task 1 (T1): anti-deepfake adversary, involving a comparative analysis of SafeEar against eight baselines across three deepfake benchmark datasets. We also investigate different impact factors, i.e., transmission codecs, deepfake techniques, and unseen-language deepfakes.\n6.1 Experiment Setup\nBaselines. We choose 8 representative baselines including end-to-end detectors-AASIST [36], RawNet 2 [76], and Rawformer [53]-take raw waveforms as input, as well as representative pipeline detectors-LFCC + SE-ResNet34 [63], LFCC + LCNN-LSTM [86], LFCC + GMM [12], and CQCC + GMM [12]. These baseline choice draws upon the recent state-of-the-art findings and official countermeasures provided by the ASVspoof challenge community. We also implement a frontend Wav2Vec2 feature-based system whose Transformer-based detector is configured the same as SafeEar for a fair comparison.\nMetrics. We follow two standard metrics for audio deepfake detection [62]. (1) Equal Error Rate (EER): it characterizes the point at which the false acceptance rate equals the false rejection rate in deepfake detection; a system with lower EER exhibits more precise detection capability. (2) Tandem Detection Cost Function (t-DCF): Unlike EER, it quantifies the cost-risk balance of false acceptances and false rejections, considering the prior probabilities of encountering bonafide versus deepfake utterances; a lower t-DCF indicates a better performance. Detailed formulations are in Appendix D."}, {"title": "6.2 Overall Performance", "content": "We present the overall performance comparison of SafeEar with 8 baseline detectors, as detailed in Table 2 for English ASVspoof 2019 and 2021, and in Table 3 for multilingual CVoiceFake. Note that for each baseline system, we have replicated and verified their performance, and herein report the official results.\nASVspoof 2019 and 2021 (English). Table 2 demonstrates that SafeEar outperforms the majority of baselines on these two datasets. In the ASVspoof 2019 dataset, SafeEar achieves a lower EER of 3.10% than the average 4.90% EER of all other baselines and a comparable t-DCF of 0.149. In the more challenging ASVspoof 2021 dataset, although we observe a general degradation, SafeEar's superiority is even more pronounced by achieving an EER of 7.22% and t-DCF of 0.336, surpassing an average 11.07% EER and 0.420 t-DCF across all baselines. We make three key observations. Firstly, on ASVspoof 2019, four detection systems surpass the state-of-the-art 4.04% EER reported in [62], i.e., AASIST, Rawformer, Wav2Vec2 + Transformer, and SafeEar. Notably, we supply acoustic-only tokens to other pipeline detectors, while the results demonstrate a marked degradation in performance: SE-ResNet34 decreases from 4.80% to 6.09%, LCNN-LSTM from 5.06% to 10.41%, and GMM from 8.09% to 15.73%. We envision that this decline is due to the classifier architectures being not designed for reliably extracting deepfake clues from shuffled and semantically-devoid tokens, indicating the effectiveness of SafeEar's tailored deepfake detector.\nOn ASVspoof 2021, SafeEar outperforms most systems and exhibits comparable EER and t-DCF with Wav2Vec2 + Transformer,"}, {"title": "6.3 Different Transmission Codecs", "content": "Given the potential for fraudulent activities executing through diverse communication tools worldwide, we see the importance of robust detection across different telecom channels. For a fair comparison, we employ the identical real-world augmentation strategy as detailed in \u00a74.5 to train each detector, as shown in Table 4. Then we evaluate the impact of telecom channels using 6 representative codecs officially set in the ASVspoof 2021 challenge, including a-law, G722, GSM, OPUS, unknown, u-law, and a no codec scenario for baseline comparison. We observe despite there are slight performance gap against Rawformer, SafeEar is on par with Wav2Vec2 across most codecs and generally outperforms the end-to-end AASIST. Another finding is a consistent decline in performance when detecting unknown codecs. This decline is likely due to the sequential compressions these codecs undergo across multiple telecom channels, resulting in a more significant loss of signal fidelity compared to mainstream codecs."}, {"title": "6.4 Different Deepfake Techniques", "content": "We compare SafeEar with baselines on a spectrum of prevalent deepfake vocoders and analyzes the individual performance in Table 5. SafeEar shows remarkable vocoder-agnostic detection capability across all tested cases, hitting overall 2.02% comparable to AASIST and Rawformer and surpassing Wav2Vec2 significantly. In real-life scenarios, deepfake adversaries are likely to employ advanced neural vocoders, such as Multiband-MelGAN, Parallel-WaveGAN, and Style-MelGAN to produce highly convincing synthetic speech. SafeEar can even hit 0.61% EER, highlighting its efficacy to thwart sophisticated deepfake methods. We validate higher EERs in the classical deepfake technique, Griffin-Lim, is caused by that the attention of model is trained to focus on minor artifacts existed in other four advanced vocoders, thus leading to minor degradation. For instance, our further individual training on Griffin-Lim, denoting SafeEar can detect it with 2.01% EER. We envision that a holistic system can ensemble different detectors trained on individual deepfake technologies."}, {"title": "6.5 Unseen-Language Deepfake Detection", "content": "With a numerous user base engaging in virtual communications daily, SafeEar may encounter deepfake speech spoken in unseen languages. We consider a challenging scenario where SafeEar's transformer detector is trained only in one language and then identifies deepfake audios across all five languages. Table 6 demonstrates that without a comprehensive training with multi-language data, the performance of the Transformer-based detector degrades. For instance, the detector trained on English obtains 15.92% EER on French and 9.70% average EER across five languages, while the optimal average EER is down to 2.02% as shown in Table 3. We also find that the choice of training language impacts to a certain degree. For instance, the detector trained on Chinese data achieves an average EER of 5.19%, lower than other settings, like 9.70% (English). These findings highlight the necessity for more multilingual datasets to develop practical deepfake detection approaches."}, {"title": "7 EVALUATION: CONTENT PROTECTION", "content": "In this section, we focus on the task 2 (T2): anti-content recovery adversaries. We consider three kinds of content recovery adversaries, i.e., naive (CRA1), knowledgeable (CRA2), and adaptive (CRA3), with different knowledge and capabilities.\n7.1 Experiment Setup\nAdversary Definition. We define three content recovery adversaries that pose threats to SafeEar:\n\u2022 Naive content recovery adversary (CRA1): The adversary lacks knowledge of SafeEar's internal parameters. However, CRA1 can emulate user interactions with SafeEar to input known speech, thereby acquiring a substantial dataset of pairs of SafeEar's tokens and ground-truth text. In our evaluation, CRA1 can acquire an extensive 960-hour Librispeech corpus to train advanced ASR models for recovering text from received tokens.\n\u2022 Knowledgeable content adversary (CRA2): In contrast, CRA2 is assumed to have the knowledge of SafeEar's algorithm and can replicate its decoder. With this knowledge, CRA2 does not need to collect numerous data for ASR training. Instead, CRA2 can reconstruct speech waveform from an individual speech sample's acoustic tokens and apply advanced ASR models or human auditory analysis for recognizing content.\n\u2022 Adaptive content adversary (CRA3): We assume this most advanced adversary can even deduce the shuffled order of a given token sequence and rectify it with a few attempts, allowing CRA3 to derive the original acoustic token sequence and then recover content as CRA2 does.\nBaselines. We envision that content recovery adversaries can employ 7 state-of-the-art ASR systems, including local and commercial ASRs. For CRA1, we compare the content recovery efficacy based on SafeEar and other inputs, leveraging the leading Bi-LSTM [27] and Conformer [29] ASR architectures. For CRA2, we utilize the well-recognized local Wav2Vec2 [68] and 4 commercial ASRs [5, 20, 35, 79] to compare SafeEar and other from CRA2's reconstructed speech waveforms as inputs. For CRA3, we keep the same setting as CRA2 yet this most advanced adversary can rectify shuffled acoustic tokens before speech reconstruction.\nMetrics. (1) Word/Character Error Rate (WER/CER): they measure the accuracy of content recovery from processed audio by indicating the proportion of words or characters incorrectly transcribed by an ASR system. A higher WER/CER denotes a better privacy-preserving ability against content recovery attacks. Note that WER can exceed 100% because its upper bound is max(N1, N2)/N1 [59], where N1 and N2 are the number of words in ground-truth and ASR transcription. (2) Short-Time Objective Intelligibility (STOI) [75]: it indicates speech signal intelligibility with its range quantified from 0 to 1 to represent the percentage of words that are correctly understood. A lower STOI means a better privacy-preserving ability. (3) Subjective Assessment: we conduct a user study in \u00a77.5 that includes three sub-metrics-ASR effectiveness, human intelligibility, and human WER."}, {"title": "7.2 Anti-Naive Adversary (CRA1)", "content": "In this part, we assess SafeEar's efficacy in multi-language content protection against recovery attacks (CRA1). These adversaries can gather shuffled acoustic tokens and corresponding ground-truth text pairs from SafeEar to train advanced Bi-LSTM and Conformer models. Given that advanced end-to-end detectors like AASIST and Rawformer, which take raw waveforms as inputs, alongside the Wav2Vec2-based pipeline detector, we include both input types for evaluation. Additionally, SafeEar's capacity for semantic-acoustic decoupling is evaluated, using its semantic tokens as a baseline for comparison.\nCRA1-English Content Protection. Table 7 demonstrates that CRA1 can easily infer users' speech content when receiving raw waveform and Wav2Vec2 feature inputs, with all WERs below 10.46%. Bi-LSTM and Conformer separately transcribe Wav2Vec2 and waveforms better, with minimal 1.78% and 2.55% WERS. As for semantic tokens, all WERs below 19.61% and a minimum WER of 6.68% indicates that SafeEar well decouples semantic information from speech. In contrast, the acoustic tokens effective in deepfake detection, yet inapplicable for conversion back into intelligible content, even when CRA1 trains both ASR models using 960-hour Librispeech dataset over multiple epochs. As shown in Figure 7, during the training of ASR models based on acoustic tokens, the validation WER curves of SafeEar remain high and do not converge, keeping 90.40% WER higher than the Wav2Vec2-based system, highlighting SafeEar's resilience against content recovery attacks. Finally, the WERs and CERs are still too high: 93.93~106.2% and 72.74~97.12%, respectively, far surpassing the unacceptable WER threshold of over 45% as reported in [60]. The results of our user study (see \u00a77.5) also confirms that these ASR-transcribed text are unintelligible.\nCRA1-Unseen Language Content Protection. As SafeEar's semantic-acoustic decoupling ability derives from the English-based"}, {"title": "7.3 Anti-Knowledgeable Adversary (CRA2)", "content": "In this part, we evaluate the resistance of SafeEar against knowledgeable content adversaries (CRA2), who can reconstruct received tokens into speech waveforms and employ off-the-shelf ASR models or even human auditory to analyze speech content across different languages.\nCRA2-English Content Protection. To comprehensively evaluate CRA2's ability to recover content, we select the best local ASR, i.e., Wav2Vec2 [25] and four commercial ASR APIs out of multiple off-the-shelf candidates. As illustrated in Table 9, the original speech waveforms serve as an optimal baseline, based on which, CRA2 can obtain a low transcription WERs of 3.15% and 7.68% on two subsets. In the \"Coded\" reference group where audio samples are processed by the representative telecom codec-OPUS, CRA2 maintains comparable WERs as low as 3.82% and 11.83%, respectively. This results confirms that CRA2 can easily eavesdrop speech content within virtual calls or meetings despite distortion exists. In contrast, SafeEar significantly safeguards the actual speech content by shuffled acoustic tokens, resulting in an average WER above 99.94%, a level too high for adversaries to meaningfully interpret the content. Additionally, as shown in Table 11, the STOI metric, used for assessing the objective intelligibility of CRA2's reconstructed speech samples, further substantiate inefficacy of CRA2 in understanding data anonymized by SafeEar, with values of 0.0018 and 0.0015, significantly lower than 0.8698 and 0.8719 of \"Coded\".\nCRA2-Unseen Language Content Protection. CRA2 may employ established ASR models for different languages to conduct content recovery across diverse linguistic contexts. We report SafeEar's effectiveness in protecting content in unseen languages against CRA2 in Table 10, omitting the coded setting due to its results being very close to the original audio. Results indicate that CRA2 can recover meaningful content from multilingual original audio with slightly higher WER due to audio's lower quality. However, SafeEar still safeguards content privacy, maintaining all WERs above 90.89% and averaging 102.63% across five ASR models. As shown in Table 11, the objective STOI values for SafeEar all approach 0, ranging between 0.0031 and 0.0106. In contrast, the STOI"}, {"title": "7.4 Anti-Adaptive Adversary (CRA3)", "content": "In this part, we explore whether SafeEar can safeguard speech content from recovery by the most adaptive adversary (CRA3). This evaluation also serves as an ablation study that examines the standalone content protection ability of acoustic tokens. CRA3 adversaries are distinguished from CRA1 and CRA2 by their ability to rectify the correct temporal sequence of acoustic tokens $\\overline{A}$, denoted as \"SafeEar*\", even after random shuffling to A. For direct comparison, we put above three types of audio samples on our website [1].\nAs shown in Figure 8, an overall decrease in WER/CERs compared to SafeEar (CRA2) is observed, indicating CRA3's slight improvement in content comprehension. However, these rates remain too high to comprehend, due to acoustic tokens' devoid of semantic information. Furthermore, we envision that an adaptive adversary would repeatedly listen to the correct-order speech to interpret it. To explore this, we have established a user study in \u00a77.5, including three aspects of subjective assessment.\n7.5 User Study\nTo validate SafeEar's content protection against machine-based and human auditory analysis, we conduct a user study, which is approved by the Institutional Review Board (IRB) of our institute. Setup. We have recruited 68 participants, aged 21~35 years and comprising 51 males and 17 females with bilingual proficiency in"}, {"title": "8 DISCUSSION", "content": "Overhead Analysis of SafeEar. We evaluate SafeEar's overhead by comparing its real-time factor (RTF) and floating point operations per second (FLOPs) against established baselines on the identical hardware platform. RTF, defined as $RTF = \\frac{T_{detect}}{T_{audio}}$, measures the model's speed in processing audio inputs, where $T_{audio}$ is the duration of the original audio and $T_{detect}$ represents the detection latency. FLOPs reflects the computational complexity of the model-lower FLOPs correspond to lower complexity. As Table 12 demonstrates, all methods achieve low RTFs in detecting audio deepfakes. While SafeEar operates at roughly 2~3 times the latency of non-privacy-centric methods like AASIST, it significantly outperforms traditional cryptographic methods, which exhibit at least a 100-fold increase in latency over plaintext computations [18"}]}