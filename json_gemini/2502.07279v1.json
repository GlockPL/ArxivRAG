{"title": "Exploratory Diffusion Policy for Unsupervised Reinforcement Learning", "authors": ["Chengyang Ying", "Huayu Chen", "Xinning Zhou", "Zhongkai Hao", "Hang Su", "Jun Zhu"], "abstract": "Unsupervised reinforcement learning (RL) aims\nto pre-train agents by exploring states or skills in\nreward-free environments, facilitating the adapta-\ntion to downstream tasks. However, existing meth-\nods often overlook the fitting ability of pre-trained\npolicies and struggle to handle the heterogeneous\npre-training data, which are crucial for achiev-\ning efficient exploration and fast fine-tuning. To\naddress this gap, we propose Exploratory Diffu-\nsion Policy (EDP), which leverages the strong\nexpressive ability of diffusion models to fit the\nexplored data, both boosting exploration and ob-\ntaining an efficient initialization for downstream\ntasks. Specifically, we estimate the distribution of\ncollected data in the replay buffer with the diffu-\nsion policy and propose a score intrinsic reward,\nencouraging the agent to explore unseen states.\nFor fine-tuning the pre-trained diffusion policy\non downstream tasks, we provide both theoreti-\ncal analyses and practical algorithms, including\nan alternating method of Q function optimization\nand diffusion policy distillation. Extensive exper-\niments demonstrate the effectiveness of EDP in\nefficient exploration during pre-training and fast\nadaptation during fine-tuning.", "sections": [{"title": "1. Introduction", "content": "Developing generalizable agents capable of efficiently\nadapting across various tasks remains a major challenge\nin reinforcement learning (RL). To address the diversity\nof downstream tasks, unsupervised learning has recently\nshown transformative progress in natural language process-\ning (Brown et al., 2020) and computer vision (He et al.,\n2022), where pre-trained models can quickly adapt to dif-\nferent downstream tasks. Inspired by these successes, un-\nsupervised RL (Eysenbach et al., 2018; Laskin et al., 2021)\naims to pre-train agents in reward-free environments, en-\nabling them to fully extract embodiment knowledge. These\npre-trained agents can then be fine-tuned for downstream\ntasks, characterized by task-specific rewards, with limited\ninteractions.\nOne of the major challenges in unsupervised RL is the re-\nquirement of strong modeling ability and fitting ability for\nboth the pre-training and fine-tuning stages. Specifically, to\nmaximize exploration in reward-free environments during\nthe pre-training stage, it requires designing intrinsic rewards\nthat rely on an accurate estimation of the collected data\ndistribution. This distribution is often heterogeneous, high-\nlighting the importance of the modeling ability. As for the\nfine-tuning stage, the pre-trained policies need strong fitting\nability to fully capture the diversity of the explored data,\nwhich is a critical factor for enabling rapid adaptation to\ndownstream tasks (see Fig. 1). However, while existing un-\nsupervised RL methods can collect diverse trajectories, they\ntypically rely on simple pre-trained policies, such as Gaus-\nsian policies (Pathak et al., 2017; Mazzaglia et al., 2022) or\nskill-based policies (Eysenbach et al., 2018; Laskin et al.,\n2022), of which the expressive ability is always limited.\nConsequently, current pre-trained policies often fail to re-\nflect the full diversity of the explored data in the replay\nbuffer, hindering effective adaptation to downstream tasks.\nTo address above issues, we propose Exploratory Diffusion\nPolicy (EDP), which leverages the strong modeling ability\nand the fitting ability of diffusion policies (Chen et al., 2023;\nChi et al., 2023) to enhance unsupervised exploration and\nfew-shot fine-tuning. During the unsupervised pre-training\nstage, we optimize a diffusion policy to estimate the hetero-\ngeneous distribution of the explored data in the replay buffer.\nAs the diffusion policy can accurately estimate the data dis-\ntribution, we propose a novel score intrinsic reward $R_{score}$\ncalculated by the diffusion policy to encourage the agent to\nexplore regions with lower probabilities in the replay buffer.\nTo address the inefficiency of multi-step sampling in diffu-\nsion policies, we further adopt a Gaussian behavior policy\nfor action generation during environment interaction. The\nbehavior policy is optimized to maximize score intrinsic\nrewards and explore unseen regions.\nBesides enabling accurate data estimation for intrinsic re-"}, {"title": "2. Background", "content": "2.1. Unsupervised Reinforcement Learning\nReinforcement learning (RL) always considers a Markov\ndecision process (MDP) $\\mathcal{M}$, which is represented as $\\mathcal{M}$ =\n($\\mathcal{S}$, $\\mathcal{A}$, $\\mathcal{P}$, $\\mathcal{R}$, $\\rho_0$, $\\gamma$) (Sutton & Barto, 2018). Here $\\mathcal{S}$ and\n$\\mathcal{A}$ denote the state and action spaces, respectively. For\n$\\forall (s, a) \\in \\mathcal{S} \\times \\mathcal{A}$, $\\mathcal{P}(\\cdot|s, a)$ is a distribution on $\\mathcal{S}$, repre-\nsenting the dynamic of $\\mathcal{M}$, and $\\mathcal{R}(s, a)$ is the extrinsic task\nreward function. $\\rho_0$ is the initial state distribution and $\\gamma$\nis the discount factor. For a given policy $\\pi : \\mathcal{S} \\rightarrow \\Delta(\\mathcal{A})$,\nwe define the discount state distribution of $\\pi$ at state $s$ as\n$d_{\\pi}(s) = (1 - \\gamma) \\sum_{t=0}^{\\infty} [\\gamma^t \\mathcal{P}(s_t = s)]$. The objective of RL\nis to maximize the expected cumulative return of the policy\n$\\pi$ over the task $\\mathcal{R}$, which can be computed as:\n$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\mathcal{M}, \\pi}[\\mathcal{R}(\\tau)] = \\frac{1}{1-\\gamma} \\mathbb{E}_{s \\sim d_{\\pi}, a \\sim \\pi} [\\mathcal{R}(s, a)].$\nTo boost the generalization capabilities of RL agents across\nvarious downstream tasks, unsupervised RL typically in-\ncludes two stages: unsupervised pre-training and few-shot\nfine-tuning. During the first stage, the agent explores in the\nreward-free environment $\\mathcal{M}_c$, i.e., $\\mathcal{M}$ without the reward\nfunction $\\mathcal{R}$. To guide unsupervised exploration, we will\ndesign the intrinsic rewards $\\mathcal{R}_{int}$ to encourage the agent\nto explore diverse states and pre-train the policy $\\pi$ to max-\nimize the intrinsic reward. Then during the fine-tuning\nstage, agents are required to adapt the pre-trained policy\nto handle the downstream task represented by the extrinsic\ntask-specific reward $\\mathcal{R}$, only through limited online inter-\nactions with the environment (like one-tenth or less of the\npre-training steps).\n2.2. Diffusion Policies\nRecent studies have demonstrated that diffusion mod-\nels (Sohl-Dickstein et al., 2015; Ho et al., 2020) excel at\naccurately representing heterogeneous behaviors in con-\ntinuous control, particularly through the use of diffusion\npolicies (Chen et al., 2023; Wang et al., 2023; Chi et al.,\n2023). Given state-action pairs $(s, a)$ sampled from some\nunknown policy $\\mu(a|s)$, diffusion policies first consider the\nforward diffusion process that gradually injects Gaussian\nnoise into actions:\n$a_t = \\alpha_t a + \\sigma_t \\epsilon, t\\in[0,1],$ \nhere $\\epsilon$ is the standard Gaussian distribution, and $\\alpha_t$, $\\sigma_t$ are\npre-defined hyperparameters satisfying that when $t = 0$,\nwe have $a_t = a$, and when $t = 1$, we have $a_t \\approx \\epsilon$. For\n$\\forall t \\in [0, 1]$, we can define the marginal distribution of $a_t$ as\n$p_t(a_t|s,t) = \\int \\mathcal{N}(a_t | \\alpha_t a, \\sigma_t^2 I)\\mu(a|s)da.$"}, {"title": "3. Methodology", "content": "In this section, we will introduce Exploratory Diffusion\nPolicy (EDP) during two stages: online unsupervised pre-\ntraining (Sec. 3.1) and online few-shot fine-tuning (Sec. 3.2).\n3.1. Exploratory Diffusion Policy for Unsupervised\nPre-training\nDuring the unsupervised pre-training stage, existing meth-\nods mainly focus on designing intrinsic rewards to encour-\nage Gaussian or skill-based policies to explore a wide range\nof states and skills. As discussed above, the core prin-\nciple behind intrinsic rewards is to encourage the agent\nto visit stats-action pairs that are less explored, i.e., those\nwith low probabilities of occurrence in the replay buffer.\nConsequently, accurately estimating the distribution of the\ncollected data in the replay buffer emerges as a promising\npathway for calculating intrinsic rewards.\nBased on the strong fitting ability of diffusion models, EDP\nleverages a diffusion policy $\\pi_{\\theta}$, represented by a parame-\nterized score model $\\epsilon_{\\theta}$, to accurately model the diverse and\noften heterogeneous state-action pairs in the replay buffer\n$\\mathcal{D}$ collected before:\n$\\min_{\\theta} \\mathbb{E}_{s, a \\sim \\mathcal{D}} \\mathbb{E}_{t, \\epsilon} [||\\epsilon_{\\theta}(a_t|s, t) - \\epsilon||^2].$"}, {"title": "3.2. Online Fine-tuning Exploratory Diffusion Policy", "content": "After obtaining the pre-trained diffusion policy $\\pi_{\\theta}$, the fine-\ntuning stage requires adapting $\\pi_{\\theta}$ efficiently to handle the\ndownstream task, represented by $\\mathcal{R}$, by interacting with the\nenvironment within a limited number of timesteps. Existing\nunsupervised RL methods always directly apply online RL\nalgorithms, such as DDPG (Lillicrap, 2015) or PPO (Schul-\nman et al., 2017), for fine-tuning, which may be inefficient\nfor EDP as the diffusion policy as the log probability is diffi-\ncult to estimate. Below, we start by analyzing the objective\nof the fine-tuning stage in unsupervised RL and then design\nonline fine-tuning algorithms for pre-trained diffusion poli-\ncies. Given the limited iteration timesteps in the fine-tuning\nstage, the objective can be formulated as the combination of\nmaximizing the cumulative return as well as keeping close\nto the pre-trained policy over every state $s$ (Eysenbach et al.,\n2021; Ying et al., 2024):\n$\\max_{\\pi} J_f(\\pi) = J(\\pi) - \\frac{\\beta}{1 - \\gamma} \\mathbb{E}_{s \\sim d_{\\pi}} [D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))]$\n$\\quad = \\frac{1}{1 - \\gamma} \\mathbb{E}_{s \\sim d_{\\pi}, a \\sim \\pi} [\\mathcal{R}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))]$\n$\\quad = \\mathbb{E}_{s \\sim \\rho_0, a \\sim \\pi(\\cdot|s)} [\\sum_{i=0}^{\\infty} \\gamma^i (\\mathcal{R}(s_i, a_i) - \\beta D_{KL}(\\pi(\\cdot|s_i)||\\pi_{\\theta}(\\cdot|s_i))))] \\text{ So } s_0 = s, a_0 = a$\n$\\quad = \\mathbb{E}_{s \\sim \\rho_0, a \\sim \\pi(\\cdot|s)} [\\mathcal{R}(s,a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s)) + \\sum_{i=1}^{\\infty} \\gamma^i (\\mathcal{R}(s_i, a_i) - \\beta D_{KL}(\\pi(\\cdot|s_i)||\\pi_{\\theta}(\\cdot|s_i))))]$\n$\\quad = \\mathbb{E}_{s \\sim \\rho_0, a \\sim \\pi(\\cdot|s)} [Q_{\\pi}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))],$\nhere $\\beta > 0$ is an unknown trade-off parameter that is re-\nlated to the fine-tuning steps. The objective $J_f(\\pi)$ can be\ninterpreted as penalizing the probability offset of the policy\nin $(s, a)$ over $\\pi$ and $\\pi_{\\theta}$. More specifically, it aims to maxi-\nmize a surrogate reward of the form $\\mathcal{R}(s, a) -\\beta \\log \\frac{\\pi(a|s)}{\\pi_{\\theta}(a|s)}$.\nUnfortunately, this surrogate reward depends on the policy\n$\\pi$ and we cannot directly apply classical MDP analyses.\nDrawn inspiration from soft RL (Haarnoja et al., 2017; 2018)\nand offline RL (Peng et al., 2019), we begin by defining the\ncorresponding Q functions as follows:\n$Q_{\\pi}(s, a) = \\mathbb{E} [\\mathcal{R}(s, a) + \\sum_{i=1}^{\\infty} \\gamma^i (\\mathcal{R}(s_i, a_i) - \\beta \\log \\frac{\\pi(a_i | s_i)}{\\pi_{\\theta}(a_i | s_i)})]$\nBased on this Q function, we can simplify $J_f$ as\n$J_f(\\pi) = \\mathbb{E}_{s \\sim \\rho_0, a \\sim \\pi} [Q_{\\pi}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))].$\nTo maximize Eq. 10, EDP considers decoupling the opti-\nmization of the Q function and the diffusion policy with an\nalternating optimization method. In detail, we first initial\n$\\pi_0 = \\pi_{\\theta}$, $Q_0 = Q_{\\pi_0}$, then for $n = 1, 2, ...,$ we obtain\n$\\pi_n(s) = \\arg \\max_{\\pi} \\mathbb{E}_{a \\sim \\pi} [Q_{\\pi_{n-1}}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(s))]$\n$\\quad = \\frac{1}{Z(s)} \\pi_{\\theta}(a|s) e^{Q_{\\pi_{n-1}}(s,a) / \\beta}$\n$Q_n = Q_{\\pi_n},$\nhere $Z(s) = \\int \\pi_{\\theta}(a|s) e^{\\beta Q_n(s,a)}da$ is the partition func-\ntion. Building on the analyses of soft RL (Haarnoja et al.,\n2017; 2018), we can demonstrate that each iteration im-\nproves the policy's performance and the alternating opti-\nmization will finally converge to the optimal policy of $J_f(\\pi)$.\nBelow we will introduce the practical fine-tuning algorithm\nof EDP for both updating the Q function and the diffusion\npolicy respectively, with the pseudo-code in Algorithm 2."}, {"title": "4. Related Work", "content": "Unsupervised Pre-training in RL. To enhance the gener-\nalization ability of agents across various tasks, unsupervised\nRL focuses on pre-train agents in reward-free environments\nto acquire embodiment knowledge, which can later be fine-\ntuned to any downstream tasks. During the pre-training\nstage, existing methods mainly concentrate on designing\nintrinsic rewards to encourage agents to explore the envi-\nronment, which can be mainly categorized into two types:\nexploration and skill discovery. Exploration methods typi-\ncally train a simple Gaussian policy to explore diverse states\nby maximizing the intrinsic rewards designed to estimate\neither uncertainty (Pathak et al., 2017; Burda et al., 2018;\nPathak et al., 2019; Mazzaglia et al., 2022; Ying et al., 2024)\nor state entropy (Lee et al., 2019; Liu & Abbeel, 2021b;a).\nDifferently, skill-discovery methods hope to explore diverse\nskills for downstream tasks, by maximizing the mutual in-\nformation of skills and states (Eysenbach et al., 2018; Lee\net al., 2019; Campos et al., 2020; Kim et al., 2021; Park\net al., 2022; Laskin et al., 2022; Zhao et al., 2022; Yang\net al., 2023; Park et al., 2023; Bai et al., 2024). However,\nexisting methods primarily focus on collecting diverse states\nwhile neglecting the expression ability of the pre-trained\npolicies. Specifically, although exploration methods can\ndiscover diverse trajectories, the pre-trained policy, which\nis always a simple Gaussian policy, exhibits unimodally\nand fails to capture the diversity present in the explored\ndata. Similarly, skill-based policies typically consider dis-\ncrete skill space with limited skills (Eysenbach et al., 2018),\nconstraining their expressive ability by the predefined skill\ncount. Moreover, skill-based methods always randomly se-\nlect a fixed skill for adapting the downstream tasks (Yang\net al., 2023), which degenerates into an unimodal Gaussian\ndistribution, further limiting its expressive power. Conse-\nquently, the application of generative models with strong\nexpressive ability for improving the diversity of pre-trained\npolicies is still less studied.\nRL with Diffusion Models. Recent advancements have\ndemonstrated that diffusion models, with their strong ex-\npressive capabilities, can benefit RL from different per-\nspectives (Zhu et al., 2023). In offline RL, diffusion poli-\ncies (Wang et al., 2023; Chen et al., 2023; Chi et al., 2023;\nHansen-Estruch et al., 2023; Kang et al., 2024; Chen et al.,\n2024c) have shown remarkable progress in modeling multi-\nmodal behaviors under the heterogeneous data distribution,\noutperforming previous policies such as Gaussians or VAEs.\nBesides policies, diffusion planners (Janner et al., 2022;\nAjay et al., 2023; He et al., 2023; Liang et al., 2023; Chen\net al., 2024a) have demonstrated the potential of diffusion\nmodels in long-term sequence prediction and test-time plan-\nning. Additionally, an array of research has explored inte-\ngrating energy guidance into diffusion policies for guided\nsampling (Janner et al., 2022; Lu et al., 2023; Liu et al.,\n2024). In online RL, several works have investigated train-\ning diffusion policies online for improving the sample effi-\nciency and performance (Psenka et al., 2023; Li et al., 2024;\nRen et al., 2024; Mark et al., 2024). However, the compu-\ntational cost of multi-step sampling still remains a limiting\nfactor for the efficiency of diffusion policies in online set-\ntings. In addition to behavior modeling, diffusion models\nhave also been employed as world models (Alonso et al.,\n2024; Ding et al., 2024) augmented replay buffer (Lu et al.,\n2024; Wang et al., 2024), hierarchical RL (Li et al., 2023;\nChen et al., 2024b), etc., which are beneficial in increas-\ning sample efficiency in RL. To the best of our knowledge,\nthis work represents the first attempt to leverage diffusion\npolicies for unsupervised exploration, thanks to the strong\nmultimodal expressive ability and the powerful data distri-\nbution estimation ability of diffusion policies."}, {"title": "5. Experiments", "content": "In this section, we present extensive empirical results to\naddress the following questions:\n\u2022 During the unsupervised pre-training stage, can EDP\nenhance exploration efficiency and obtain policies with\ndiverse behaviors?\n\u2022 Can the pre-trained policies of EDP fast adapt to down-\nstream tasks?\n\u2022 How do different components of EDP, like the score\nintrinsic reward, affect the performance?\n5.1. Experimental Setup\nMaze2d. We first conduct experiments for visualizing the\ndiversity of collected trajectories during exploration and pre-\ntrained policies in widely used maze2d environments (Cam-\npos et al., 2020; Yang et al., 2023). We choose 3 types of\ndifferent mazes: Square-b, Square-c, and Square-tree. For\nall these mazes, the observations and actions both belong\nto $\\mathbb{R}^2$. When interacting with the mazes, the agents will be\nblocked when they contact the walls. We compare EDP with\none exploration method: RND (Burda et al., 2018), as well\nas two skill-discovery methods: DIAYN (Eysenbach et al.,\n2018) and BeCL (Yang et al., 2023), of which the skills are\nsampled from a 16-dimensional discrete distribution. For all\nmethods, we pre-train agents in reward-free environments\nwith 500k steps and store trajectories in the replay buffer.\nThen we visualize collected trajectories in the replay buffer\nas well as trajectories directly sampled by pre-trained poli-\ncies. Moreover, to compare the exploration efficiency of\ndifferent algorithms, we report their state coverage ratios in\neach maze during the pre-training stage, which are measured\nas the proportion of 0.01 \u00d7 0.01 square bins visited.\nContinuous Control. To evaluate the fine-tuning perfor-\nmance in downstream tasks of pre-trained policies, we\nevaluate EDP in state-based continuous control settings of\nURLB (Laskin et al., 2021). In detail, we consider 4 differ-\nent domains: Walker, Quadruped, Jaco, and Hopper. Each\ndomain contains four downstream tasks. More details of\nthese domains and downstream tasks are in Appendix C.1.\nWe compare EDP with 4 exploration baselines: ICM (Pathak\net al., 2017), RND (Burda et al., 2018), Disagree-\nment (Pathak et al., 2019), and LBS (Mazzaglia et al., 2022);\nas well as 5 skill discovery baselines: DIAYN (Eysenbach\net al., 2018), SMM (Lee et al., 2019), LSD (Park et al.,\n2022), CIC (Laskin et al., 2022), and BeCL (Yang et al.,"}, {"title": "5.2. Diversity of Pre-trained Policies", "content": "In Fig. 3, we visualize the trajectories collected during the\nunsupervised pre-training stage (upper part) and those di-\nrectly sampled by the pre-trained policies (lower part) for\neach algorithm in the Square-b maze (the visualizations of\nother two mazes are in Appendix. C.3). To quantitatively\nevaluate the exploration efficiency of each algorithm, we fur-\nther compare their state coverage ratios as a function of the\npre-training trajectory number and plot the curves of each\nmaze in Fig. 4. On both the qualitative visualization and the\nquantitative metric, EDP significantly outperforms existing\nmethods, including exploration and skill-discovery ones,\nby large margins. These results demonstrate that our score\nintrinsic rewards, leveraging the accurate data estimation\nability of diffusion models, effectively encourage agents\nto explore more diverse states during the unsupervised pre-\ntraining stage compared with baselines. Notably, while\nexploration-based methods like RND achieve state coverage\ncomparable to skill-based methods, their pre-trained Gaus-\nsian policies often present an unimodal distribution near a\nsingle trajectory. In contrast, skill-based methods generate\ntrajectories with greater diversity, as they rely on distinct\nskills sampled from a discrete distribution. However, this\ndiversity is inherently limited by the number of predefined\nskills. Thanks to the strong expressive ability of diffusion\nmodels, the pre-trained diffusion policies of EDP can collect\nthe most diverse trajectories compared with all baselines,\nwhich is significant for handling different downstream tasks."}, {"title": "5.3. Fine-tuning to Downstream Tasks", "content": "We now verify whether the policies pre-trained by EDP can\nfast adapt to downstream tasks in URLB. Following pre-\nvious settings, for each downstream task, we train DDPG"}, {"title": "6. Conclusion", "content": "Unsupervised exploration is one of the major problems in\nRL for boosting agent generalization across tasks, as it relies\non accurate intrinsic rewards to guide the exploration of un-\nseen regions. In this work, we address the challenge of lim-\nited policy expressivity in previous exploration methods by\nleveraging the powerful expressive ability of diffusion poli-\ncies. In detail, our Exploratory Diffusion Policy (EDP) not\nonly enhances exploration efficiency during pre-training but\nalso yields pre-trained policies with significant behavioral\ndiversity. Furthermore, we provide a theoretical analysis\nof the fine-tuning stage for diffusion policies with practical\nalternating optimization methods. Experimental results in\nvarious settings demonstrate that EDP can effectively benefit\nboth pre-training exploration and fine-tuning performance.\nWe hope this work can inspire further research in developing\nhigh-fidelity generative models for improving unsupervised\nexploration, particularly in large-scale cross-embodiment\npre-trained agents or real-world control applications."}, {"title": "A. Theoretical analysis", "content": "Below we first analyze our fine-tuning objective Eq. 8 and then prove Theorem 3.1.\nAssuming $\\rho_0$ is the original state distribution of the MDP M, we have\n$J_f(\\pi) = J(\\pi) - \\frac{\\beta}{1 - \\gamma} \\mathbb{E}_{s \\sim d_{\\pi}} [D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))]$\n$\\quad = \\frac{1}{1 - \\gamma} \\mathbb{E}_{s \\sim d_{\\pi}, a \\sim \\pi} [\\mathcal{R}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))]$\n$\\quad = \\mathbb{E}_{s \\sim \\rho_0, a \\sim \\pi(\\cdot|s)} [\\sum_{i=0}^{\\infty} \\gamma^i (\\mathcal{R}(s_i, a_i) - \\beta D_{KL}(\\pi(\\cdot|s_i)||\\pi_{\\theta}(\\cdot|s_i))))] \\text{ So } s_0 = s, a_0 = a$\n$\\quad = \\mathbb{E}_{s \\sim \\rho_0, a \\sim \\pi(\\cdot|s)} [\\mathcal{R}(s,a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s)) + \\sum_{i=1}^{\\infty} \\gamma^i (\\mathcal{R}(s_i, a_i) - \\beta D_{KL}(\\pi(\\cdot|s_i)||\\pi_{\\theta}(\\cdot|s_i))))]$\n$\\quad = \\mathbb{E}_{s \\sim \\rho_0, a \\sim \\pi(\\cdot|s)} [Q_{\\pi}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))],$\nhere we set\n$Q_{\\pi}(s, a) = \\mathbb{E} [\\mathcal{R}(s, a) + \\sum_{i=1}^{\\infty} \\gamma^i (\\mathcal{R}(s_i, a_i) - \\beta \\log \\frac{\\pi(a_i | s_i)}{\\pi_{\\theta}(a_i | s_i)})]$\n$\\quad = \\mathbb{E} [\\mathcal{R}(s, a) + \\sum_{i=1}^{\\infty} \\gamma^i (\\mathcal{R}(s_i, a_i) - \\beta D_{KL}(\\pi(\\cdot|s_i)||\\pi_{\\theta}(\\cdot|s_i)))].$\nAs discussed in Sec. 3.2, EDP applies the following alternative optimization method:\n$\\pi_n (s) = \\arg \\max_{\\pi} \\mathbb{E}_{a \\sim \\pi} [Q_{\\pi_{n-1}}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))]$\n$Q_n = Q_{\\pi_n},$\nNow we prove that $\\pi_n(a|s) = \\frac{1}{Z(s)} \\pi_{\\theta}(a|s) e^{Q_{n-1}(s,a) / \\beta}$. More generally, we define\n$F(\\pi, \\pi', s) = \\mathbb{E}_{a \\sim \\pi} [Q_{\\pi'}(s, a) - \\beta D_{KL}(\\pi(\\cdot|s)||\\pi_{\\theta}(\\cdot|s))].$\nUsing the calculus of variations, we can calculate the optimal point $\\pi^*$ of F satisfying that\n$Q_{\\pi}(s, a) = \\beta \\log \\frac{\\pi^*(a|s)}{\\pi_{\\theta}(a|s)} + b\\beta,$\nhere $b$ is a constant not related to $\\pi^*$, and we have $\\pi^*(a|s) = \\pi_{\\theta}(a|s) e^{\\frac{Q_{\\pi}(s,a)}{\\beta} - b}$. As $\\int \\beta \\frac{\\pi^*(a|s)}{\\pi_{\\theta}(a|s)}da = 1$, we can calculate\nthat\n$b = \\log \\frac{\\int \\pi_{\\theta}(a|s) e^{\\frac{Q_{\\pi}(s,a)}{\\beta}}da}{ \\pi^*(a|s)}, \\pi^*(a|s) = \\frac{\\pi_{\\theta}(a|s) e^{\\frac{Q_{\\pi}(s,a)}{\\beta}}}{ \\int \\pi_{\\theta}(a|s) e^{\\frac{Q_{\\pi}(s,a)}{\\beta}}da}$\ni.e., we have $\\arg \\max_{\\pi} F(\\pi, \\pi', s) \\propto \\pi_{\\theta}(\\cdot|s)e^{Q_{\\pi'}(s,\\cdot)/\\beta}$ and thus $\\pi_n(a|s) = \\frac{1}{Z(s)} \\pi_{\\theta}(a|s) e^{Q_{n-1}(s,a) / \\beta}$.\nBelow we will prove Theorem 3.1.\nBased on the definition of F, we have $J_f(\\pi) = \\mathbb{E}_{s \\sim \\rho_0} F(\\pi, \\pi, s)$. Thus we require to prove $\\mathbb{E}_{s \\sim \\rho_0} F(\\pi_n, \\pi_n, s) \\geq$\n$\\mathbb{E}_{s \\sim \\rho_0} F(\\pi_{n-1}, \\pi_{n-1}, s)$. As we have discussed above,\n$\\pi_n(s) = \\arg \\max_{\\pi} F(\\pi, \\pi_{n-1}, s) = \\frac{1}{Z(s)} \\pi_{\\theta}(a|s) e^{Q_{n-1}(s,a)}$\n$F(\\pi_n, \\pi_{n-1}, s) \\geq F(\\pi_{n-1}, \\pi_{n-1}, s)$."}, {"title": "B. Details of EDP", "content": "Below we discuss more details about the Q function optimization and diffusion policy optimization of EDP during the\nfine-tuning stage.\nB.1. Q function optimization\nFor the Q function optimization, we choose to use implicit Q-learning (IQL) (Kostrikov et al., 2022), which is efficient to\npenalize out-of-distribution actions (Hansen-Estruch et al., 2023). The main training pipeline of IQL is expectile regression,\ni.e.,\n$\\min_{C} L_V(\\zeta) = \\mathbb{E}_{s, a \\sim \\mathcal{D}} [L_{\\tau}(Q_{\\zeta}(s, a) - V_{\\varphi}(s))", "2": "nhere $L_{\\tau} (u) = |\\tau - 1(u < 0)|u^2$ and $\\tau$ is a hyper-parameter. In detail, when $\\tau > 0.5$, $L_{\\tau}$ will downweight actions with low\nQ-values and give more weight to actions with larger Q-values.\nB.2. Diffusion Policy Fine-tuning\nFor sampling from $\\pi_n = \\frac{1}{Z(s)} \\pi_{\\theta} e^{Q_{n-1}/\\beta}$, we choose contrastive energy prediction (CEP) (Lu et al., 2023), a powerful\nguided sampling method. First, we calculate the score function of $\\pi_n$ as\n$\\nabla_a \\log \\pi_n(a|s) = \\nabla_a \\log \\pi_{\\theta}(a|"}]}