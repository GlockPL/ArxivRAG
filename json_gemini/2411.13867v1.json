{"title": "Generative Fuzzy System for Sequence Generation", "authors": ["Hailong Yang", "Zhaohong Deng", "Wei Zhang", "Zhuangzhuang Zhao", "Guanjin Wang", "Kup-sze Choi"], "abstract": "Abstract-Generative Models (GMs), particularly Large Language Models (LLMs), have garnered significant attention in machine learning and artificial intelligence for their ability to generate new data by learning the statistical properties of training data and creating data that resemble the original. This capability offers a wide range of applications across various domains. However, the complex structures and numerous model parameters of GMs make the input-output processes opaque, complicating the understanding and control of outputs. Moreover, the purely data-driven learning mechanism limits GM's ability to acquire broader knowledge. There remains substantial potential for enhancing the robustness and generalization capabilities of GMs. In this work, we introduce the fuzzy system, a classical modeling method that combines data and knowledge-driven mechanisms, to generative tasks. We propose a novel Generative Fuzzy System framework, named GenFS, which integrates the deep learning capabilities of GM with the interpretability and dual-driven mechanisms of fuzzy systems. Specifically, we propose an end-to-end GenFS-based model for sequence generation, called FuzzyS2S. A series of experimental studies were conducted on 12 datasets, covering three distinct categories of generative tasks: machine translation, code generation, and summary generation. The results demonstrate that FuzzyS2S outperforms the Transformer in terms of accuracy and fluency. Furthermore, it exhibits better performance on some datasets compared to state-of-the-art models T5 and CodeT5.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, generative models have garnered widespread attention for addressing complex generative tasks. Particularly, the continuous development of Transformer and its derivative technologies has led to state-of-the-art large natural language processing models (LLMs), such as ChatGPT and Llama [1], [2], [3]. These models can explore hidden patterns and relationships within the data and generate high-quality multimodal data such as text, audio, and image, making them powerful tools in Natural Language Processing (NLP) and artificial intelligence.\nTransformers [4] and their derivatives form the basis for many generative AI techniques. Representative models in this area include GPT [5], [6], [7], [8], [9] for text generation, CLIP [10], [11] for image generation, and MuLan [12], [13], [14] for audio generation. Despite the complexity and variety of generative tasks, they can be transformed into sequence generative tasks through serialization techniques. For example, text is sliced into token sequence, images are divided into patches, and audio data is discretized into time series of amplitudes. The models generate predicted sequences by learning the relationships between input and target sequences. These sequences are then deserialized in order to obtain the final results. Sequence generative tasks are characterized by several key attributes: they involve variable-length, unstructured data; the elements of a sequence are ordered; and the mapping relationship between input and target sequences is complex.\nExisting generative models have complex network structures, deep hierarchies, and a large number of parameters, making their internal workings and decision-making processes opaque and their output difficult to control [15]. Also, these models often require vast datasets for training and operate as black boxes, making it hard to incorporate logical knowledge, rules and constraints. This data-driven approach limits the models' ability to generalize and handle a wider range of applications. Generative models also demand significant computational resources and time for training. Moreover, the trained models' parameters are challenging to reuse in other models, leading to a considerable waste of computational resources and time, which hinders their large-scale application and technological development from the long run.\nThe fuzzy system is a classical modeling method composed of fuzzy sets, fuzzy rules, and inference mechanisms. It effectively handles fuzzy information and provides good interpretability [16]. Also, fuzzy systems can express expert knowledge, address nonlinear problems, and offer better robustness and generalization capabilities. Notably, fuzzy systems capture the fuzzy characteristics of human brain thinking from a macroscopic perspective, simulating human reasoning and decision-making to handle uncertainty problems that conventional mathematical methods struggle to solve. Currently, fuzzy systems are widely used in tasks such as classification [17], [18], recognition [19], [20], and detection [21]. In fuzzy systems, fuzzy rules can represent priori expert knowledge, which can be easily transferred from one system to another [22], [23]. This enables efficient knowledge migration and knowledge reuse, greatly saving computational resources and training time.\nThe input sequences of generative tasks may exhibit widely varying characteristics, such as different lengths, different token frequency distributions, and inherent ambiguity and uncertainty. Fuzzy system can use a priori expert knowledge to divide the inputs into multiple sets with different feature terms for respective processing. Therefore, it is significant to develop a novel generative fuzzy system. However, for complex generative tasks, such as machine translation, code generation, and summary generation in NLP community, the generative fuzzy system is still difficult to cope with the following challenges: Firstly, the input of generative tasks consists of variable-length unstructured data, which cannot be directly processed by classical fuzzy systems. Secondly, the complex token mapping relationships require models to have a robust ability to handle high-dimensional data. However, classical fuzzy systems are shallow models with a small number of trainable parameters, making them inadequate for effectively undertaking generative tasks.\nTo address the first challenge, in the generative fuzzy system, we will implement delegate election for fuzzy sets and transform the fuzzy membership calculation into a similarity calculation between the inputs and the delegates. This approach enables the fuzzification of both structured and unstructured data. Generative tasks aim to model joint probability by learning the probability distribution of tokens across sequences, necessitating models with substantial learning capabilities and a large number of trainable parameters. To address the second challenge, we will introduce deep generative models as the consequents of the fuzzy rules. This integration enhances the system's learning ability, making it more suitable for sequence generation.\nThe generative fuzzy system retains the advantages of classical fuzzy systems, as both are fuzzy-rule-based systems known for their good interpretability and a dual-driven mechanisms powered by both knowledge and data. The main contributions of this paper can be summarized as follows:\nFirstly, we propose a novel generative fuzzy system framework, namely GenFS. This framework combines the high interpretability of fuzzy system with the powerful learning capabilities of generative models. Moreover, GenFS can efficiently transfer generative knowledge from one system to another through generative fuzzy rules, thereby saving computational resources and reducing training time costs.\nSecondly, based on our proposed GenFS framework, we introduce a specific generative fuzzy system for natural language sequence generation, called FuzzyS2S. During the preprocessing stage, FuzzyS2S employs a novel multi-scale fuzzy tokenizer to optimize the token frequency distribution of the sequences and to extract sequence information at multiple scales. In addition, the model introduces an innovative fuzzy membership calculation method, which effectively addresses the fuzzification problem of variable-length sequences.\nThe remainder of the paper is structured as follows: Part II introduces the generative model along with the related concepts and principles of classical fuzzy systems. Part III presents the generative fuzzy system framework, namely GenFS. Part IV presents a specific generative fuzzy system FuzzyS2S for sequence-to-sequence generative tasks. Part V evaluates the performance of the FuzzyS2S model in machine translation, summary generation, and code generation tasks. Part VI provides a conclusion and an outlook for future work."}, {"title": "II. RELATED WORKS", "content": "A. Generative Models\nThe advent of deep learning has led to significant advancements in generative modelling technology. In the field of NLP, sequence generation encompasses a range of sequence-to-sequence (seq2seq) [24] tasks such as machine translation, code generation, and summary generation. Early research primarily focus on Recurrent Neural Networks (RNNs) and long short-term memory (LSTM) networks [25]. However, the memory performance of these early seq2seq models degrades rapidly as sequence length increases, and they struggle to effectively differentiate tokens with varying importance [24].\nTo address these issues, Bahdanau et al. proposed the Attention Mechanism [26], which assigns different degrees of attention to tokens based on their importance by attentional scoring, thus alleviating the problem of long dependencies. Vaswani et al. proposed the Transformer model [4], which utilizes a multi-head attention mechanism and effectively parallelizes training. BERT [27] is a model based on the Transformer architecture that employs bidirectional encoding representation, and is used as the encoder in the autoencoder framework [28]. To support a variety of downstream tasks, Raffel et al. proposed the T5 [29], a Transformer-based pre-trained language model capable of performing tasks such as text classification, text generation, text summarization, and machine translation. ChatGPT is based on the GPT (Generative Pre-trained Transformer) family [5], including GPT-3.5 and GPT-4, which are capable of supporting a wide range of downstream tasks and enable interactive conversations.\nAlthough current generative models have achieved remarkable performance, it is important to note that as their performance improves, the parameter scale of these models continues to grow exponentially. These parameters cannot be efficiently reused in other models [30], [31], [32], and their interpretability becomes increasingly challenging [15], [33]. Additionally, the learning processes of mainstream generative models are predominantly data-driven and lack knowledge-driven mechanisms [34],[35]. Therefore, it is highly beneficial to develop a novel generative framework to enhance the interpretability of generative models and establish a dual-driven mechanism by both data and knowledge.\nB. Fuzzy System\nIn 1965, American automatic control expert L.A. Zadeh proposed the concept of fuzzy sets, which led to the rapid development of fuzzy theory. The fuzzy system defines input, output and state variables on fuzzy sets, and serves as a representative uncertainty reasoning system. Fuzzy systems excel in solving nonlinear modelling problems and are now widely used in automatic control, pattern recognition, decision analysis, time series signal processing, and other tasks. Currently, there are two main branches of fuzzy systems: Mamdani fuzzy systems [36], [37] and TSK (Takagi-Sugeno-Kang) fuzzy systems [38], [39]. The TSK fuzzy system, In particular, has strong data-driven learning ability and good interpretability, which has garnered extensive attention and research in recent years.\nThe fuzzy reasoning in the fuzzy system is approximate and non-deterministic, with both premises and conclusions being inherently fuzzy."}, {"title": "III. GENERATIVE FUZZY SYSTEM", "content": "A. Concept and Structure\nDefinition 1: Generative Fuzzy System Framework (GenFS). GenFS mainly consists of a generative fuzzification module, a generative fuzzy rule base, a fuzzy inference machine, and a generative fuzzy rule combination mechanism, as illustrated in Fig. 1. GenFS operates by processing input data through the generative fuzzification module and then reasoning with generative fuzzy rules to draw fuzzy inference conclusions under given preconditions. These inference conclusions are integrated through an efficient generative fuzzy rule combination mechanism, ultimately forming the system's output. GenFS is a novel framework that combines generative models and classical fuzzy systems, enabling fuzzy systems to handle complex generative tasks.\nDefinition 2: Generative Fuzzification (GF). It is the process of delegate election of fuzzy sets, aiming to transforming data into a more representative form and calculate the similarity between the inputs and the delegates.\nDefinition 3: Generative Fuzzy Rule Consequent (GFRC). GFRC is an intelligent fuzzy rule consequent that employs generative models as the consequent processing unit to enhance learning ability. GFRC can be represented as follows:\n$g^k = GFRC^k(x)$ (2)\nwhere k = 1,2,3, ..., K, K is the number of rules in the fuzzy system, GFRCk is the generative rule consequent processing unit of the k-th rule, and gk is the generative data of the k-th rule.\nDefinition 4: Generative Fuzzy Rules. Generative Fuzzy Rules are composed of antecedents with Generative Fuzzification and GFRCs. The k-th generative fuzzy rule of the rule base can be expressed as follows:\nIF x is GF (xd\u0131g), THEN $g^k = GFRC^k(x)$ (3)\nwhere x is the input data and GF (xalg) is the generative fuzzification function corresponding to the k-th rule antecedent, represented by xdlg\nDefinition 5: Generative Fuzzy Rule Combination Mechanism (GFRCM). GFRCM is an intelligent decision-making mechanism for generative tasks. Its purpose is to select applicable rule combination strategies to produce crisp and unambiguous results for different generative scenarios. When processing structured data with aligned features, GFRCM employs the weighted average method to combine the outputs of all the rules. For unstructured output data, GFRCM uses the maximum defuzzification method, selecting the output of the rule with the highest fire strength as the final result. Eq. (4) provides the generalized form of generative fuzzy rule combination mechanism:\n$y = GFRCM(G) = \\begin{cases}\\sum_{k=1}^{K} \\\u016d^k * g^k, G \\text{ is aligned} \\\\ g_{Argmax(i)}, \\text{otherwise}\\end{cases}$ (4)\nwhere G = {g\u00b9, g2, g\u00b3, ..., gK}, gk is the output of k-th rule, \u016bk is the fire strength of the k-th rule, and \u0169 = [\u00b5\u00b9, \u03bc\u00b2, \u03bc\u00b3, ..., \u03bc\u03ba]. When G is structured and features are aligned, the fire strengths are used as weights to weight and sum all the rule outputs. Otherwise, the rule output with the maximum fire strength is selected as the final output, where Argmax(i) function is used to obtain the index value of the maximum fire strength.\nB. Construction of Generative Fuzzy Rule Antecedents\nThe construction of generative fuzzy rule antecedents is a crucial task in modelling generative fuzzy systems. Classical fuzzy systems use the membership of the crisp data structured features corresponding to all the fuzzy sets as the fuzzification results. However, the inputs of generative tasks often vary in length after serialization, making classical fuzzification methods inadequate for directly handling unstructured data. Therefore, a novel generative fuzzification method for complex generative tasks.\nThe membership calculation in the generative fuzzification method requires solving two key problems: firstly, the election of delegates for fuzzy sets, and secondly, the calculation of similarity between the inputs and the delegates.\nDelegate Election. A delegate of a set is a particular element of the set [40], [41]. The election of a delegate element simplifies the understanding and manipulation of the structure and properties of the set. In generative tasks, the delegate of a fuzzy set is a particular sample within the set that has a high degree of similarity in structure and properties with other samples. The method of delegate election can be based on expert experience or obtained through methods such as clustering. Each fuzzy set has its own individual delegate. The election of delegates can be expressed as Eq. (5).\nXdig = DSM(X, K) (5)\nwhere Xalg = [xaig, xaig, ..., xang] is a delegate set of K rules in the system rule base, X is the set of corresponding input data, DSM denotes the method of the delegate election of the fuzzy set X.\nSimilarity Calculation. Similarity is usually expressed as a numerical value, where a larger value indicates greater similarity between two objects. In generative tasks, the inputs and delegates are usually unstructured data with unaligned features. Therefore, it is necessary to design flexible calculation methods tailored to different scenarios. Similarity can be calculated by data matching method, feature aligning method, and other specific methods. This can be presented in a generalized form, as shown in Eq. (6).\n$\u03bc^k = Sim(x, x_{dlg.}^k)$ (6)\nwhere x is the input data, $x_{dlg.}^k$ is the delegate of the k-th fuzzy set, k = 1,2, ..., K, K is the number of rules, Sim(.) is a generalized method for calculating the similarity, uk is the similarity between x and the delegate of the corresponding fuzzy set of the k-th rule, namely, the fire strength of x about the k-th rule.\nTo stabilize numerical calculations and avoid feature bias, it is usually necessary to normalize similarity, as shown in Eq. (7).\n$\\\\\u00b5 = Norm(\\\\\u00b5) = \\frac{\\\\\u00b5}{\\sum_{k=1}^{K} \u03bc^k}$ (7)\nwhere \\\\\u00b5 = [\u00b5\u00b9, \u00b5\u00b2, \u00b5\u00b3, ..., \u03bc\u039a], \u03bc = [\u03bc\u00b9, \u03bc\u00b2, \u03bc\u00b3, ..., \u03bc\u0390].\nC. Learning and Optimization of GFRCS\nThe learning of GFRCs can be regarded as a typical machine learning task. In machine learning, the closer the predicted data distribution is to the real data distribution, the better the model's performance. In the generative task, calculating the difference between the target sequence and the generated sequence is key to the model's learning process. From a probability distribution perspective, the target sequence is considered the conditional probability distribution of the input sequence, and the difference between the generated sequence and the target sequence is derived from these two probability distributions.\nThe cross-entropy loss function is widely used to measure this difference between probability distributions. From a sequence similarity perspective, the difference between sequences can be measured by their similarity, making the similarity loss function also applicable for sequence difference calculation. Depending on the scenarios, GenFS adopts different loss functions to calculate sequence differences. To prevent overfitting, an L2 regularization term is added to the loss function. The final loss function is generalized as follows:\n$L_{GFRC}(\\Theta) = diff (y, \\hat{y}) + \\gamma||\\Theta||^2$ (8)\nwhere y is the ground true of target sequences in the generative task, \u0177 is the generative sequence, diff(.) is a function to compute the difference between the target and generative sequences, ||\u0398||2 is the L2 regular term of the GFRC consequent parameters, and \u03b3(\u03b3 > 0) is a balancing parameter.\nFor generative tasks, sequence data is high-dimensional and sparse, with an uneven token frequency distribution where high-frequency and low-frequency tokens appear randomly. During training, reducing the learning rate for high-frequency tokens and increasing it for low-frequency tokens helps the model effectively learn information from both types of tokens. Therefore, GFRC needs to employ optimizers capable of adaptively adjusting the learning rate, such as AdaGrad, RMSprop, Adadelta and Adam [42], [43] to better approximate the optimal values of the model parameters and minimize the training loss. The generalized representation of the adaptive optimizer is shown in Eq. (9).\n$\u0398' = AdaOptim(\\Theta, l_s)$ (9)\nwhere AdaOptim(.) is the adaptive optimiser function, ls is the initialised value of the learning rate, and \u0398' is the model parameter that has been computationally updated by the optimizer.\nD. Generalized Learning Algorithm for GenFS\nIn knowledge-based systems, a series of IF-THEN conditional statements can be used as rules to express knowledge. The antecedents' parameters can be derived from the prior knowledge of human experts or clustering techniques. The consequents acquire knowledge by learning the mapping relationship of sequences. The construction of generative fuzzy rules involves two main tasks: constructing the antecedents and learning the consequents' parameters.\nThe model parameters in the generative fuzzy system, denoted as \u0398, include the parameters of both the antecedents and consequents. The consequents can be initialized randomly or with pre-trained model parameters. Multiple consequents can be trained jointly. The generalized learning algorithm for GenFS is described in Algorithm I."}, {"title": "IV. SEQUENCE-TO-SEQUENCE GENERATION BASED ON GENFS FRAMEWORK", "content": "In this section, we propose a specific generative fuzzy system based on the GenFS framework, tailored for sequence-to-sequence generation tasks. First, we define the specific problems involved. Then, we propose a novel multi-scale tokenizer, namely fuzzy tokenizer, for sequence splitting preprocessing. Furthermore, we propose an end-to-end generative model based on GenFS, named FuzzyS2S.\nA. Problem Definition\nIn NLP tasks such as machine translation, code generation, and summary generation, the text sequence is sliced into tokens by tokenizers, and these tokens follow the long-tailed distribution of Zipf's Law [44]. Zipf's Law is an empirical law that describes the relationship between token frequency and token ranking in natural language. The basic formulation of Zipf's Law is that in a large corpus, the token frequency is inversely proportional to its position in the token ranking list. Specifically, the law is expressed as:\nf(n) \u2248 C/n (10)\nwhere f(n) denotes the frequency of the token in the ranking list, n denotes the rank of the token, and C is a constant.\nIn natural language, the occurrence of tokens still conforms to Zipf's Law. While a small number of tokens occur with high frequency, the majority of tokens occur with low frequency. Thereby, this raises two problems that require resolution:\nProblem I: A significant proportion of the tokens in sequences are low-frequency, which presents a challenge for the model learning the occurrence patterns of these tokens. Therefore, it is significant to reduce the number of low-frequency tokens and optimize the token distribution.\nProblem II: There are significant variations in token distributions and sequence lengths across sequences, posing challenges for model fitting.\nB. Multiscale Tokenizer based on the priori expert knowledge: Fuzzy Tokenizer\nTo address the issue of low-frequency tokens (Problem I), we propose a specific multi-scale tokenizer, called fuzzy tokenizer. The fuzzy tokenizer is a fuzzy system based on multi-scale sub-word tokenizers [45] that enables the adaptive slicing of words at different scales. The architecture of the fuzzy tokenizer is illustrated in Fig. 2.\nGiven the original text sequences, a preliminary token sequence s' can be obtained after the basic tokenization:\ns' = Tbasic(s) = [X1, X2, X3, ..., XN] (11)\nwhere Tbasic is the basic tokenizer, which splits text sequences with spaces or punctuation marks as separators. n is the total number of tokens of the sequence s', xn, n = 1,2,3, ...., N is the nth token of the sequence s'.\nThe cosine distance algorithm is used in the antecedents of rules to calculate the similarity between the input tokens and the delegates of the fuzzy sets. This similarity is used to represent the fire strength of the fuzzy rules. Calculating the fire strength \u00cen of token xn with respect to all rules can be expressed as follows:\n\u00b5n = Cosine (xn, xang) (12)\n\u00b5n = Norm(\u00b5n) (13)\nwhere u denotes the fire strength of token xn with respect to the kth rule, i.e., \u00b5 = [\u00b51, \u00b52, \u00b53, ..., \u00b5], and the normalised fire strength n = [\u00b51, \u00b52, \u00b53, ..., \u00b5].\nThe consequents of the fuzzy tokenizer are sub-word tokenizers operating at different scales. The outputs are unstructured sub-word sequences. Fuzzy tokenizer selects the maximum defuzzification method to obtain the final result, as showed in Eq. (14).\n$X_{fuzz}^n = T_{Argmax(n)}(x_n)$ (14)\nwhere Xn = [Xn1, Xn2, \u03a7\u03b73, \u2026\u2026\u2026, \u03a7nt] is the sub-word sequence after slicing of token xnand t is the total number of sub-words.\nThe fuzzy tokenizer splices the results of further slicing to obtain the final token sequence \u0161.\n$S = concat(X_1, X_2, X_3, ..., X_N)$ (15)\nC. Sequence-to-sequence GenFS: FuzzyS2S\nThis section presents an end-to-end sequence generative model based on GenFS, denoted as FuzzyS2S. In FuzzyS2S, we implement a special GenFS, using Transformer as the consequent processing units, denoted as GenFS-Trans. FuzzyS2S introduces two new modules: the Preprocess module, and the Postprocess module.\nThe three modules are described below:\nPreprocess Module. This module transforms the original input sequences into vectors, reducing the low-frequency tokens in the sequences using the fuzzy tokenizer. In the preprocessing stages, the source sequence sx is passed through the fuzzy tokenizer to obtain the sequence of tokens sx as shown in Eq. (16).\nS'x = Tfuzzy (Sx) = [X1, X2, X3, ..., Xx, ..., XN] (16)\nwhere N is the number of tokens in the sequence, x is the 5th token in the sequence, \u03be = 1,2,3, ..., N. The sliced tokens are saved using the classical bag-of-words (BoW) model [50], [51], [52]. The input of BoW is the sequence of tokens, and the output is the word vector, which is an array of indexes of the storage locations, and the size of BoW is denoted as Nbag. W2V represents the method of mapping the token sequences to vectors, while V2W represents the method of mapping the vectors to the token sequences. There are mainly two special tokens in BoW, namely the start marker bos and the end marker eos, which are added at the start and end positions of the sequence, respectively. As shown in Eq. (17), here the sequence of tokens sx is vectorised to obtain the word vector, which will further be passed as input to GenFS-Trans.\nvx = W2V(sx) (17)\nGenFS-Trans Module. This module is the core of FuzzyS2S. It begins by performing fuzzification on the input vectors, then conducts the fuzzy inference using the fuzzy inference machine, and finally fuses the results of all the rules via the GFRCM component. Specifically, as shown in Eqs. (18) and (19), the fuzzy set delegate is obtained through the unsupervised fuzzy clustering technology, and the similarity between the inputs and the delegates are computed using the cosine distance algorithm. The consequent processing units are Transformer. According to Eq. (21), the word vectors are transformed into word embeddings with the addition of positional encoding to preserve the positional information of the token in the sequence. Eq. (22) shows that the word embeddings output by Transformer are passed through a multilayer perceptron (MLP) to raise the feature dimensions to Nbag. Furthermore, in the GFRCM component of GenFS-Trans, the weighted average method is used to combine the results of all the rules. The fire strengths of the rules serve as weights, and the word embeddings are weighted and averaged to obtain the target word embedding Emy as shown in Eq. (23). Finally, the resulting embedding is compressed into the range of (0,1) to obtain the H matrix through Softmax function.\n$[S_{alg.}^{alg}, S_{alg.}^{alg}, ..., S_{alg.}^{alg},...,..., S_{alg.}^{alg}] = DSM_{FCM} (S_X, K)$ (18)\n\u00b5s = Cosine (sx, sang) (19)\n\u00b5s = Norm(\u00b5s) (20)\nEmx = Pos (Emb(B(vx))) (21)\n$Em_y = \\sum_{k=1}^{K}MLP(TF^k(Em_x))$ (22)\nH = Softmax(Emy) (23)\nwhere Sx is the set of input sequences; DSMFCM(.) is a delegate election method based on Fuzzy C-Means[53]; the cosine distance algorithm Cosine(.) is used to calculate the similarity between the input data and the delegates; \u03bcs = [\u03bc\u03ad, \u03bc\u03b5, \u03bc\u03b5, ..., \u03bc\u03ad] normalized similarity \u03bc\u03c2 = [\u03bc\u03be, \u03bc\u03be, \u03bc\u03be, ..., \u03bc]; vx is the word vector of the source sequence, B(.) is to add bos at the beginning of the word vector, Emb(.) is to compute the word embedding of the word vector, Pos(.) is to encode the position of the word embedding; TFk is the Transformer processing unit for the kth consequent, Emx \u2208 IR(N+1)\u00d7Dim, Dim is the dimensionality of the word embedding, Emy E R(M+1)\u00d7Nbag, and H is the predictive probability matrix, H\u2208 R(M+1)\u00d7Nbag.\nPostprocess Module. This module is responsible for converting the word embeddings output from the GenFS-Trans module into the target sequence. According to Eq. (24), the process uses the Argmax function to obtain the index of the maximum value of the probability for each dimension, and finally predicts M + 1 tokens.\nSy = Concat (V2W(Argmax(H)))) (24)\n= Concat([\u01771, \u01762, \u01763, \u2026, \u0176\u043c, eos])\nwhere V2W(.) is a vector-to-token conversion function and Concat(.) is a function that splices an array of tokens into a sequence, FuzzyS2S stops the token prediction when the stop marker eos occurs."}, {"title": "V. EXPERIMENT", "content": "A. Datasets and Experiment Setting\nThe experiments follow the Train-Validation-Test (TVT) approach as outlined in [54", "parts": "training sets", "categories": "machine translation", "54": "Tatoeba [55", "56": "and Ubuntu [56", "57": [58], "59": "XLSum[60", "61": ".", "62": "MTG (Magic the Game) [62", "63": ".", "4": "T5 [29", "64": ".", "65": "METEOR [66", "29": "which are detailed in Part 3(B) and 3(C) of the Supplementary Materials.\nB. Analysis of Sequence Generation Experiments\n1) Analysis of Machine Translation Experiments\nThe comparison of the machine translation performance metrics of FuzzyS2S and T5", "follows": "n(1) FuzzyS2S enhances accuracy by leveraging the interconnectivity between tokens of different scales. When the occurrence frequency of coarse-scale tokens is minimal"}, {"4": [67], "68": "."}, {"68": ".", "69": [70], "71": [72], "observations": "Since the input for the summary generation task is an entire article or report", "4": "."}, {"observations": "Since the input for the summary generation task is an entire article or report", "4": ".", "64": "utilizes relative positional encoding. Due to the strong correlations between tokens in natural language sequences", "follows": "nIF sx is Long Sentence, THEN sy = TFLong (sx);\nIF sx is Middle Sentence, THEN sy = TFMiddle (Sx);\nIF sx is Short Sentence, THEN sy = TFLow (sx);\nThe input English sentence in Fig. 5 has a length of 16, and its length and token frequency distribution characteristics are more similar to the description of the second rule. The actual similarity calculation indicates that the input sentence is more similar to the delegate in the Middle Sentence fuzzy set. Consequently, the result obtained through the GFRCM"}]}