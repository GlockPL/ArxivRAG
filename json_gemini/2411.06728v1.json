{"title": "On the Principles of ReLU Networks with One Hidden Layer", "authors": ["Changcun Huang"], "abstract": "A neural network with one hidden layer or a two-layer network (regardless of the input\nlayer) is the simplest feedforward neural network, whose mechanism may be the basis\nof more general network architectures. However, even to this type of simple architec-\nture, it is also a \u201cblack box\u201d; that is, it remains unclear how to interpret the mecha-\nnism of its solutions obtained by the back-propagation algorithm and how to control the\ntraining process through a deterministic way. This paper systematically studies the first\nproblem by constructing universal function-approximation solutions. It is shown that,\nboth theoretically and experimentally, the training solution for the one-dimensional in-\nput could be completely understood, and that for a higher-dimensional input can also be\nwell interpreted to some extent. Those results pave the way for thoroughly revealing the\nblack box of two-layer ReLU networks and advance the understanding of deep ReLU\nnetworks.", "sections": [{"title": "1 Introduction", "content": "Deep learning has achieved a great triumph in both scientific and engineering areas in\nrecent years, such as Jumper et al. (2021) in protein-structure prediction and OpenAI\n(2023) in large-language models. However, its underlying principle is still mysteri-\nous and usually referred to as a \"black box\" (Castelvecchi, 2016; Roscher et al., 2020),\nwhich involves not only application reliability but also potential uncontrollable Al risks.\nTechnically speaking, the black box of neural networks is roughly composed of two\nparts. The first is what the mechanism of the solution derived from the back-propagation\nalgorithm (Rumelhart, Hinton, & Williams, 1986) is. The second is how to obtain a\ndesired solution through a deterministic way. The two parts are correlated because if"}, {"title": "1.1 Related Work", "content": "The function-approximation capability of two-layer neural networks had attracted the\nattention of mathematical researchers for more than 30 years. There's a large body of\nresearch for the non-ReLU cases. Pinkus (1999) gave a comprehensive review on the\nresults up to 1999; after that, for example, Draghici (2002), Ismailov (2012), Costarelli\n& Spigler (2013), Guliyev & Ismailov (2018) and Almira et al. (2021) continued the\nresearch until recently. But none of those works exclusively aimed at extracting or\nsummarizing the principle of training solutions, while developing the theory from pure\nmathematical viewpoint.\nThere were also works for two-layer ReLU networks (Breiman, 1993; DeVore,\nHanin, & Petrova, 2021; Hatano, 2021). However, the similar problem exists as above.\nThus, although much had been done for theoretical analysis, little is known about the\nmechanism of the solution obtained by the back-propagation algorithm.\nTo the relationship between splines and ReLU networks, Daubechies et al. (2019)\nstudied the approximation to univariate functions by deep ReLU networks and com-\npared the effect with that of linear splines. The spline expressed by ReLU networks\ncan also be regarded as the solution of some regularized optimization problems (Unser,\n2019; Aziznejad & Unser, 2019; Bohra et al., 2020; Parhi & Nowak, 2021).\nDespite using the term \u201chinge function\", Breiman (1993)'s proof of theorem 3 in-\nvolved the one-sided bases of linear splines and the conclusion holds for two-layer\nReLU networks as well, a result mostly related to part of this paper. Balestriero &\nBaraniuk (2021) extended Breiman (1993)'s work to the multi-output case and proved\nthat a wide range of deep ReLU networks, such as convolutional neural networks and\nResNets, can be written as the form of the composition of spline functions. The above\ncorrelations between splines and ReLU networks are either for deep ReLU networks or\nfor pure theoretical analysis and are different from our way of introducing the idea of\nsplines.\""}, {"title": "1.2 Road Map of this Paper", "content": "This paper develops the theory gradually from the simplest univariate case (section 2)\nto the multivariate case (sections 3 to 7), with the theoretical explanation of solutions\nembedded in the proof of the conclusions. The term \u201ccompleteness\u201d of a deduced result\nmeans that there's no other choice of interpretations except for that conclusion, which\nwill be used to explain the univariate or one-dimensional solution (corollary 3). After\nthe theory having been developed, experimental verification will be given in section\n8, in which several examples of training solutions will be explained by the established\ntheory.\nTo the details, section 2 gives a simple example, the approximation to univariate\nfunction via two-layer ReLU networks, through the principle of one-sided bases of\nsplines (theorem 1), which includes the idea to be generalized to the multivariate case\nin sections 3 and 4. Section 3 generalizes the concept of knots on an one-dimensional\nline as well as their \"less than\u201d relation (a strict partial order) to the higher-dimensional\ncase. Section 4 completes the generalization of section 1 for multivariate-function ap-\nproximation over a single strict partial order (theorem 3), especially by establishing\nthe relationship between the linear pieces of a two-layer ReLU network with higher-\ndimensional input (theorem 2).\nSection 5 enlarges the solution space by introducing the concept of \"two-sided\nbases\" of splines (theorem 5) on the basis of the preceding sections, after which the\ntraining solution for one-dimensional input is theoretically explained (corollary 3). This\nnew concept is important in the diversity of solutions.\nSections 6 and 7 further increase the complexity of the constructed solution by\nadding new principles, in order to get closer to the training solution for a higher-\ndimensional input. Section 6 investigates the function approximation over multiple\nstrict partial orders (theorem 7), with each providing a set of knots to realize the associ-\nated piecewise linear function, and embeds the principle of two-sided bases of section\n5 into the new solution form (theorem 8). Section 7 proposes a fundamental principle\ncalled \"continuity restriction\u201d (theorem 9) and the universal function-approximation ca-\npability for higher-dimensional input is finally proved (theorem 10), which completes\nthe theoretical framework of this paper.\nSection 8 uses experimental results to verify the developed theory and it is shown\nthat the solution obtained by the back-propagation algorithm can be explained by our\ntheoretical framework. Section 9 highlights several conclusions related to the black-\nbox problem. Section 10 concludes this paper by a discussion and proposes two open\nproblems for future studies.\nThe outline above does not contain all the results of this paper. The remaining ones\nmay be an intermediate or a relatively unimportant conclusion, or some detailed techni-\ncal result that needs not to be included in the main framework above. Throughout this\npaper, the term \u201ctraining solution\u201d is the abbreviated version of the solution obtained\nby the back-propagation algorithm."}, {"title": "2 Approximation to Univariate Function", "content": "The one-sided bases (Chui, 1992; Schumaker, 2007) of splines are nonlocal and hence\nare not as popular as its further developed version\u2014B-splines. However, this \u201cdisad-\nvantage\" happens to be the intrinsic property of a ReLU whose activation function is an\none-sided basis. The spline theory tells us that a two-layer ReLU network can realize\na piecewise linear function via one-sided bases, from which we obtain a construction\nmethod. The main ideas summarized in section 2.3 will be generalized to the multivari-\nate case in later sections 3 and 4."}, {"title": "2.1 One-Sided Bases", "content": "The notations of splines in this section are borrowed from Schumaker (2007) with some\nmodifications. On closed interval $I = [0, 1]$, a spline is a piecewise polynomial defined\non the subintervals derived from what is called \u201cknots\u201d and may satisfy some smooth-\nness conditions.\nLet\n$\\Delta = \\{x_\\nu : \\nu = 1, 2, . . . , \\zeta - 1\\},\\qquad(2.1)$\nwhere\n$0 < x_1 < x_2 <\\cdots < x_{\\zeta-1} < 1\\qquad(2.2)$\npartition $[0, 1]$ into $\\zeta$ subintervals $I_1 = [0, x_1]$ and $I_\\nu = (x_{\\mu-1}, x_\\mu]$ for $\\mu = 2, 3, . . . ,\\zeta$\nwith $x_{\\zeta} = 1$. We call each of $x_1, x_2, ..., x_{\\zeta-1}$ a knot. Denote by $P_2$ the set of linear\nfunctions (polynomials with degree 1). The space of continuous linear splines is defined\nto be\n$S_1(\\Delta) = \\{s : s(x) = s_i(x) \\in P_2 \\text{ for } x \\in I_i, s_i(x_i) = s_{i+1}(x_i) \\text{ for } i \\neq \\zeta,\\qquad(2.3)$\n$\\qquad\\qquad i = 1, 2, ..., \\zeta\\}.$\nWe sometimes use the redundant term \u201cspline function\u201d to emphasize its function prop-\nerty.\nThe one-sided bases (Chui, 1992; Schumaker, 2007) of $S_1(\\Delta)$ can be defined as\n$\\{p_j(x) = \\sigma(x - x_j) : j = -1, 0, . . . , \\zeta - 1\\},\\qquad(2.4)$\nwhere $\\sigma(x) = \\text{max}(0, x)$ is the activation function of a ReLU and\n$x_{-1} < x_0 \\leq 0 < x_1 < x_2 <\\cdots < x_{\\zeta-1} < 1.\\qquad(2.5)$\nEquation 2.4 suggests that a ReLU network can realize a continuous linear spline in\nterms of the one-sided bases, by which we give a construction method next."}, {"title": "2.2 Construction of Continuous Linear Splines", "content": "Under equations from 2.1 to 2.3, a continuous linear spline $S(x) \\in S_1(\\Delta)$ with $\\zeta$ linear\npieces can be expressed as\n$S(x) = \\{s_i = a_ix + b_i \\text{ for } x \\in I_i : i = 1, 2, . . . , \\zeta\\},\\qquad(2.6)$\nsubject to\n$a_ix_{i+1} + b_i = a_{i+1}x_{i+1} + b_{i+1}\\qquad(2.7)$\nfor $i \\neq \\zeta$ that ensures the continuous property of $S(x)$ at the knots.\nLemma 1. To any continuous linear spline $S(x)$ of equation 2.6, under the one-sided\nbases of equation 2.4, there exists a unique form\n$S(x) = \\sum_{j=-1}^{\\zeta-1} \\lambda_j\\sigma(x - x_j),\\qquad(2.8)$\nwhere $\\lambda_{-1} = (a_1x_0 + b_1)/(x_0 - x_{-1})$, $\\lambda_{0} = (a_1x_{-1} + b_1)/(x_{-1} - x_{0})$ and\n$\\lambda_{\\nu-1} = a_{\\nu} - a_{\\nu-1}\\qquad(2.9)$\nfor $\\nu = 2, 3, ..., \\zeta$.\nProof. Suppose that $s_1 = a_1x + b_1$ on $I_1$ has been given and its construction method\nwould be discussed later. Then\n$s_2 = s_1 + (a_2 - a_1)\\sigma(x - x_1),\\qquad(2.10)$\nwhich is continuous with $s_1$ at knot $x = x_1$. Because $a_2$ can be arbitrarily set, any $s_2\ncontinuous with $s_1$ at $x_1$ can be expressed in the form of equation 2.10. Similarly, the\nrecurrence form\n$s_\\nu = s_{\\nu-1} + (a_\\nu - a_{\\nu-1})\\sigma(x - x_{\\nu-1})\\qquad(2.11)$\nholds for all $\\nu = 2, 3, ..., \\zeta$, yielding $\\lambda_{\\nu-1} = a_{\\nu} - a_{\\nu-1}$ of equation 2.9, that is,\n$s_\\nu = s_{\\nu-1} + \\lambda_{\\nu-1}\\sigma(x - x_{\\nu-1}).\\qquad(2.12)$\nTo the production of $s_1$, by solving the linear equations\n$\\begin{cases}\\lambda_{-1} + \\lambda_{0} = a_1\\\\-x_{-1}\\lambda_{-1} - x_{0}\\lambda_{0} = b_1\\end{cases}\\qquad(2.13)$\nderived from $\\lambda_{-1}\\sigma(x - x_{-1}) + \\lambda_{0}\\sigma(x - x_{0}) = a_1x + b_1$ for $x \\in [0, x_1]$, we obtain\n$\\lambda_{-1} = (a_1x_0 + b_1)/(x_0 - x_{-1})$ and $\\lambda_{0} = (a_1x_{-1} + b_1)/(x_{-1} - x_{0})$.\nProposition 1. Any continuous linear spline $S(x)$ of equation 2.6 with $\\zeta$ linear pieces\ncan be realized by a two-layer ReLU network $N$ whose hidden layer has $\\Theta = \\zeta + 1$\nunits, with infinitely many solutions."}, {"title": "2.3 Ideas for Generalization", "content": "Two main ideas arise from the proof of the preceding results, which will be generalized\nin sections 3 and 4:\n1. The knots of equation 2.1 can be arranged in the strict partial order of equation\n2.2, such that all the one-sided bases could exert their influences in one direction\nand each subinterval has its own distinguished activated unit to shape its linear\nfunction.\n2. The adjacent continuous linear pieces whose subdomains are separated by a knot\nhas a simple relation of equation 2.12, contributing to a construction method of\nthe weights of the one-sided bases."}, {"title": "3 Strict Partial Order of Knots", "content": "This section generalize the knots and their \u201cless than\u201d relation on an one-dimensional\nline to the higher-dimensional case. In this paper, when referring to the n-dimensional\nspace $R^n$ or the universal set $U = [0,1]^n$, if the range of interger $n$ is not explicitly\ndefined, we tacitly assume that $n \\geq 1$."}, {"title": "3.1 Definition of Strict Partial Orders", "content": "In space $R^n$, a region is part of $R^n$ separated by a set of $n - 1$-dimensional hyperplanes\nand we here give its rigorous definition. The concepts of \u201climit point\u201d and \u201cclosure\"\nare from mathematical analysis (Rudin, 1976). A limit point $p$ of a set $D$ is a point"}, {"title": "3.2 Construction of Strict Partial Orders", "content": "Proposition 2. The relation $<$ of equation 3.2 is a strict partial order, and is equivalent\nto \u201c$<$\u201d(less than) of real numbers in the sense that both the relationships between $R_i$'s\nand the influences of the units of $l_i$'s on $R_i$'s in terms of equation 3.1 are the same as the\none-dimensional case manifested by equations 2.2 and 2.5\u2014that is, $F(I)$ of equation\n3.1 also holds for the one-dimensional input, where $I = \\{I_i, x_i^+, x_i^- : i = 1, 2, . . . , \\zeta\\}$\nin which $I_i = (x_i, x_{i+1}]$ and $x_i$ is from equation 2.2.\nProof. A strict partial order (Davey & Priestley, 2002) is a relation that is not reflexive,\nantisymmetric and transitive. The relation $<$ is defined based on the \u201cless than\u201d relation\n$<$ of the subscripts of the $l_i$'s of equation 3.1. Because $<$ is a strict partial order, so is\n$<$.\n$F(I)$ describes the relationship that each interval $I_i = (x_i, x_{i+1}]$ is on the left side\nof point $x_j$ for $j = i + 1$ (i.e., $I_i < \\bigcap_{j=i+1}^{\\infty} x_j$) and is on the right side of $x_k$ for\n$k \\leq i$ (i.e., $I_i < \\bigcap_{k=1}^{i} x_k$). $F(R)$ represents the similar meaning for regions, for\nwhich the \u201cright side\u201d and \u201cleft side\u201dof $l_i$ are substituted by $+$ and $0$, respectively, and\nthe term $q_i = (R_i \\bigcap R_{i-1}) \\subseteq l_i$ with $\\text{dim}(q_i) = n-1$ is the generalized version of\n$I_i \\bigcap I_{i-1} = x_i$.\nDefinition 4 (Region of the universal set). A region $R$ of $U = [0,1]^n$ with respect to a\nset H of hyperplanes is the intersection of U and a region R' derived from H, provided\nthat $R \\neq \\emptyset$ and $\\text{dim} \\, R = n$; that is, $R = U \\bigcap R'$. Since U itself is a region produced by\nsome hyperplanes, R is also a region. We also say that R is a region of U formed by H.\nProposition 3. To $U = [0,1]^n$, there exists a set H of $n - 1$-dimensional hyperplanes\n$l_i$'s for $i = 1, 2, . . . , \\zeta$ to form a strict partial order of equation 3.1, with the ordered\nregions $R_i$'s of U with respect to H satisfying $\\bigcup_{i=1}^{\\zeta} R_i = U$.\nProof. In the two-dimensional case of $n = 2$, it is easy to imagine two kinds of so-\nlutions. Let U$\\subset l^+$ and then rotate $l_1$ around any point of $l_1$ counterclockwise or\nclockwise to form $l_i$'s for $\\nu = 2, 3, . . . , \\zeta$, each of which intersects U at more than one\npoint. Then a region $R_\\mu$ between $l_\\mu$ and $l_{\\mu+1}$ for $\\mu = 1,2,...,\\zeta-1$ together with\n$R_{\\zeta} \\subset l_{\\zeta} \\bigcap U$ could fulfil equation 3.1 and simultaneously $\\bigcup_{\\nu=1}^{\\zeta} R_\\nu = U$. The other way\nis to translate $l_1^+$ to pass though U by $\\zeta - 1$ steps, with each intersecting U at more than\none point, which can also form $\\zeta$ regions satisfying equation 3.1 whose union is U. The\nsecond method can be generalized to the n-dimensional case for $n \\geq 3$.\nProposition 4. Under some notations of proposition 3, more than one strict partial\norder $P_i = (H_i, <)$ for $i = 1, 2, ..., \\psi$ with $\\psi > 2$ could simultaneously exist, with\nthe constraint that $\\bigcup_{i=1}^{\\psi} H_i = H$, $H_\\nu \\bigcap H_\\mu = \\emptyset$ for $1 \\leq \\nu, \\mu \\leq \\psi$ and $\\nu \\neq \\mu$, and that\n$\\bigcup_{i=1}^{\\psi} \\bigcup_{j=1}^{\\Phi_i} R_i \\subseteq U$, where $\\{R_j^{(i)} : j = 1, 2, ..., \\Phi_i = |H_i|\\}$ is the set of the ordered\nregions of U formed by $H_i$.\nProof. For example, based on the proof of proposition 3, arbitrarily select $l_\\mu$ for $2 \\leq\n$\\mu \\leq \\zeta - 1$. Exchange the positive- and zero-output regions of each $l_k$ for $1 \\leq k \\leq \\mu$ by\nconverting its equation $w^Tx + b = 0$ into $-w^Tx - b = 0$. Then two independent strict"}, {"title": "4 Approximation over One Strict Partial Order", "content": "This section first establishes the relationship between the linear pieces of a piecewise\nlinear function of a two-layer ReLU network (theorem 2) analogous to equation 2.12,\nafter which arbitrary piecewise linear function on a set of ordered regions can be con-\nstructed (theorem 3), completing the generalization of section 1."}, {"title": "4.1 Axiomatic Foundation", "content": "Definition 5 (A two-layer ReLU network). The output of a two-layer ReLU network $N$\nwith n-dimensional input is defined as\n$y = \\sum_{i=1}^m \\lambda_i\\sigma(w_i^Tx + b_i),\\qquad(4.1)$\nwhere $\\sigma(x) = \\text{max}(0, x)$ is the activation function of a ReLU. In equation 4.1, all the\nReLUs $U_i$'s form the hidden layer of $N$ and $\\lambda_i$ is the output weight of the $i$th one. The\nn-dimensional input space is denoted by $R^n$.\nAxiom 1. A ReLU $U_i$ or its associated hyperplane $L_i$ with equation $w_i^Tx + b_i = 0$\nof the hidden layer of $N$ could influence half of the input space $R^n$, denoted by $L_i^+$,\nthrough its nonzero positive output, and has no impact on the other half $L_i^0$ in terms of\nits zero output.\nAxiom 2. The output of a ReLU $U_i$ on $L_i^+$ is a linear function $Y_i = w_i^Tx + b_i$.\nDefinition 6 (Continuity at a knot). A function $f : R^n \\rightarrow R$ is said to be continues at\nknot L, if\n$\\text{lim}_{x\\rightarrow L^+} f(x) = \\text{lim}_{x\\rightarrow L^0} f(x) = f(x_\\pounds),\\qquad(4.2)$\nwhere $x \\rightarrow L^+$ and $x \\rightarrow L^0$ mean that $x$ approaches knot L from the parts $L^+$ and\n$L^0$, respectively, through an arbitrary one-dimensional line $\\pounds$ satisfying $\\pounds \\bigcap L \\neq \\emptyset$ and\n$\\pounds \\nsubseteq L$, and $x_\\pounds$ is the point $\\pounds \\bigcap L$. If equation 4.2 doesn't hold, we say that $f(x)$ is\ndiscontinuous at knot L."}, {"title": "4.2 Correlation between Linear Pieces", "content": "Lemma 2. Let L and $l_\\nu$ be an $n-1$- and $\\nu$-dimensional hyperplane of $R^n$, respectively,\nwhere $1 \\leq \\nu \\leq n - 1$ and $n \\geq 2$. If $l_\\nu \\nsubseteq L$ and $l_\\nu \\bigcap L \\neq \\emptyset$, then their intersection\n$l = L \\bigcap l_\\nu$ is a $\\nu - 1$-dimensional hyperplane."}]}