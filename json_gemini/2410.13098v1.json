{"title": "A Little Human Data Goes a Long Way", "authors": ["Dhananjay Ashok", "Jonathan May"], "abstract": "Faced with an expensive human annotation process, creators of NLP systems increasingly turn to synthetic data generation. While this method shows promise, the extent to which synthetic data can replace human annotation is poorly understood. We investigate the use of synthetic data in Fact Verification (FV) and Question Answering (QA) by studying the effects of incrementally replacing human generated data with synthetic points on eight diverse datasets. Strikingly, replacing up to 90% of the training data only marginally decreases performance, but replacing the final 10% leads to severe declines. We find that models trained on purely synthetic data can be reliably improved by including as few as 125 human generated data points. We show that matching the performance gain of just a little additional human data requires an order of magnitude more synthetic data, and we then estimate price ratios at which human annotation would be a more cost-effective solution. Our results suggest that even when human annotation at scale is infeasible, there is great value to having a small proportion of the dataset being human generated.", "sections": [{"title": "Introduction", "content": "From BERT (Devlin et al., 2019) to GPT-4 (Achiam et al., 2023), the explosive growth of language models (LMs) has been underpinned by exponential increases in the size of available training data. However, the more complex and specialized the task, the more expensive and challenging it is to collect human generated data at scale (Wang et al., 2021). Combined with growing concerns that LMs may soon exhaust the stock of publicly available training data (Villalobos et al., 2024), many turn to synthetic data generation, hoping to eliminate their reliance on human annotation.\nSynthetic data generation has long been used to increase the amount of training data available (Simard et al., 2002; Krizhevsky et al., 2012). Early NLP approaches use rule based methods (De Gispert et al., 2005; Chen et al., 2012), paraphrasing (Wang and Yang, 2015; Kobayashi, 2018), noising (Xie et al., 2017; Wang et al., 2018), and backtranslation (Sennrich et al., 2016; Yu et al., 2018), but are limited in their capability.\nModern LMs demonstrate the capability to solve myriad NLP tasks with minimal task specific data (Brown, 2020; Wei et al., 2022a,b; Ouyang et al., 2022; Ashok and Lipton, 2023), making them more powerful synthetic data generators. Leveraging this, synthetic data approaches have seen increased use in tasks (Tan et al., 2024) such as QA (Wu et al., 2021; Yu et al., 2024a), NLI (Meng et al., 2022; Ye et al., 2022; Hsieh et al., 2023), text classification (Ye et al., 2022; Gao et al., 2022; Yu et al., 2023, 2024b), instruction tuning (Li et al., 2024; Wei et al., 2023), evaluation (Dubois et al., 2024; Fu et al., 2023), and more (Tang et al., 2023; Kulkarni et al., 2024; Chan et al., 2024)."}, {"title": "Synthetic Data Generation from Evidence Texts", "content": "We study a synthetic data generation pipeline representative of the methods used in the FV (Ni et al., 2024; He et al., 2023) and QA (Schimanski et al., 2024; Wan et al., 2024) literature. Specifically, we use Few-Shot In-Context Learning (Brown, 2020) to generate synthetic (claim, label) pairs from an input evidence text. The prompt model is given examples of (evidence text, claim, label) from the real training data, and is then queried with the evidence text we seek to generate data for (QA synthetic data is generated analogously, see additional details in Appendix B).\nIn total, we use 4 FV/NLI datasets: FEVER (Thorne et al., 2018), SciFact (Wadden et al., 2020), WANLI (Liu et al., 2022) and"}, {"title": "Can Synthetic Data Replace Humans?", "content": "We investigate the potential of synthetic data to replace human annotation by holding the number of training data points fixed, incrementally increasing the proportion of the data that is synthetic, and fine-tuning a model on each training set.\nResults: Across all datasets, using purely synthetic data typically leads to worse performance than the same amount of real data (Figure 1). This confirms that despite advances in synthetic generation, human annotation yields more useful data.\nThe performance decline is not uniform as we increase the synthetic proportion. On all datasets,\nhttps://github.com/DhananjayAshok/LittleHumanData/"}, {"title": "When Should We Use Human Data?", "content": "Having observed the disproportionate value added by human data, we ask what the relative cost between human and synthetic data generation must be for us to prefer one over the other. We fine tune models on purely synthetic datasets of varying sizes, and establish the synthetic baseline by fitting a curve of the form  y = ao + a\u2081 log(x) where x is the size of the synthetic dataset and y is the performance. We then take the synthetic training sets with {1000, 2000 ... } points and observe the performance (y*) when we add 200 human data points. exp(a) is then the size of the purely synthetic dataset that achieves equivalent performance.\nResults: Across all datasets, adding 200 human data points is comparable to adding at least an order of magnitude (often multiple orders of magnitude) more synthetic data points. On WANLI (Figure 3), more than 17000 additional synthetic points are needed to achieve the performance gains of 200 human points. If the price of a synthetic point for WANLI exceeds 73 times the price of a human generated point, then an incremental amount of human annotation would be the more cost-effective solution. In the extreme case, the equation learned on FairyTaleQA suggests that it takes 2e5 additional synthetic points to match the performance gain of 200 additional real data points. Rather than interpret these numbers literally, we take them to suggest that human data could have unique value in some settings, enabling performance levels that are impossible with purely synthetic datasets. See below (Appendix A) for more results and details."}, {"title": "Discussion", "content": "A deeper analysis of the difference between real and synthetic data exposes interesting trends. Our analysis shows that synthetic data generation produced claims of comparable length to the real datasets, however synthetic questions and answers tended to be longer than human-generated counterparts for all QA datasets (Figure 4). We find that synthetic generations have a higher n-gram overlap with the evidence sentences. This suggests that synthetic data generation produces data points that are more directly taken from the evidence texts, while humans are more likely to employ rephrasing or different vocabulary than the evidence texts. Surprisingly, we find that synthetic data generation chooses more diverse sources for the question and answer content, with human annotation overwhelmingly more likely to create questions whose answers lie in the start of the evidence texts. We include a detailed discussion below (Appendix D)."}, {"title": "Related Work", "content": "The replacement of human annotation with synthetic data is extensively studied in the pretraining stage of LMs, where results consistently show (Shumailov et al., 2023; Seddik et al., 2024; Guo et al., 2023; Briesch et al., 2023) catastrophic forgetting, mode collapse, and performance deterioration.\nWhile our results show that purely synthetic datasets are relatively worse than mixed datasets, relying only on synthetic data still achieves reasonable performance across all tasks. This suggests that the usage of exclusively synthetic data poses fewer risks in a setting where generations are still grounded in diverse natural 'evidence texts.'\nInterestingly, conclusions which confirm our findings are found more in the image and multimodal domains, where recent work (Singh et al., 2024; He et al., 2023; Fan et al., 2024) finds that synthetic data holds promise, but must be used in conjunction with human data to mitigate its harms. There is limited work on understanding whether synthetic data can replace human annotation in a task-specific setting for the language domain. Li et al. (2023) categorize text classification tasks by subjectivity, showing that synthetic data is less useful when tasks are more subjective. This draws them to focus on different tasks (sentiment classification, relation extraction and spam detection), and they do not study using a mixture of real and synthetic data. Bisbee et al. (2024) demonstrate that replacing political survey respondents with LMs produces unreliable results, while Ahmed et al. (2024) find that there are specific subtasks in the annotation of software engineering artefacts where synthetic data approaches human performance. Chen et al. (2024) show that instruction-following capabilities are diminished when using data from GPT-4, and present a machine unlearning approach to mitigate this. The diversity of results when evaluating the impact of using purely synthetic data confirms that the feasibility of replacing human annotation with synthetic data is highly task dependant. This work deepens our understanding of the problem by being the first to study whether synthetic data can replace human annotation on the fundamental tasks of fact verification, question answering, and natural language inference."}, {"title": "Conclusion", "content": "Showing impressive performance when human data is scarce, synthetic data generation seems poised to remain a key method in FV and QA. Our work sheds light on how the best way to use this method is in conjunction with human data. We show that a little human data goes a long way, with just 125 points being enough to see reliable gains on all datasets studied. With practical considerations in mind, we show that the alternative to small amounts of additional human data can be an order of magnitude more of synthetic data, suggesting that at times human annotation can be cost-effective relative to synthetic generation. We hope these results better inform design decisions on datasets and methods for fact verification and question answering."}, {"title": "Limitations", "content": "All datasets studied in this work are in the English language, hence limiting our scope of investigation. We hope future work can explore whether similar claims can be made regarding the impact of replacing human annotation with synthetic data across different languages. We also have a limited ability to control for dataset leakage, with only one dataset from each of the tasks that is surely not leaked to GPT-3.5 (and, even these two datasets may have been seen by GPT-4). This can potentially bias the results in favor of synthetic data. Due to the scarcity of suitable available datasets (i.e., ones that have not been exposed to the prompt models) we are prevented from studying the problem more rigorously. Another limitation is that while we are able to identify clear differences between synthetic vs. real data distributions, our analysis of the errors made by models trained on 0% vs. 100% synthetic data failed to yield any generalizable insights that could inform modelling approaches. A more fine-grained study of the effect of using synthetic data on the behaviour of the downstream model is hence left as a subject of future research. This work focuses on the value of human-labelled data, but due to the nature of benchmarks, the human-labelled test data is drawn from the same distribution as our treatment data. Future work should verify that these results hold under novel human-generated examples, which is beyond the scope of this investigation."}, {"title": "Ethical Considerations", "content": "The usage of synthetic data has several important ethical considerations. In the era of LMs trained on internet-wide corpora having poor documentation as to their exact data sources, it becomes challenging to ensure the privacy of individuals whose data may be obtainable via a public crawl (Yao et al., 2024). Additionally, models trained on massive internet-based data sources may contain implicit biases, illegal and/or highly offensive material that is hard to audit and clean (Bender et al., 2021). This data affects the synthetic data obtained from prompt models, and could unknowingly impose cultural or ethical viewpoints that are unintended or not well aligned with the use case in mind. Specifically, prior work has shown that one of the prompt models studied in this work, GPT-3.5, often disagrees with humans on key ethical questions (Felkner et al., 2024). The endeavour to completely replace human annotation with synthetic data generation also has key implications on the extent to which the field of NLP employs human annotators. It is possible that an increasing reliance on purely synthetic data reduces the demand for human annotation, which would place a downward pressure on the working standards and compensation awarded to the remaining human annotators (Weidinger et al., 2022). We argue in this work that we should not try to eliminate human annotation from our dataset and method design, showing that their work contributes uniquely helpful data points."}]}