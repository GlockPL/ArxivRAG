{"title": "Can Large Language Models Grasp Legal Theories?\nEnhance Legal Reasoning with Insights from Multi-Agent Collaboration", "authors": ["Weikang Yuan", "Junjie Cao", "Zhuoren Jiang", "Yangyang Kang", "Jun Lin", "Kaisong Song", "Tianqianjin Lin", "Pengwei Yan", "Changlong Sun", "Xiaozhong Liu"], "abstract": "Large Language Models (LLMs) could struggle to fully understand legal theories and perform complex legal reasoning tasks. In this study, we introduce a challenging task (confusing charge prediction) to better evaluate LLMs' understanding of legal theories and reasoning capabilities. We also propose a novel framework: Multi-Agent framework for improving complex Legal Reasoning capability (MALR). MALR employs non-parametric learning, encouraging LLMs to automatically decompose complex legal tasks and mimic human learning process to extract insights from legal rules, helping LLMs better understand legal theories and enhance their legal reasoning abilities. Extensive experiments on multiple real-world datasets demonstrate that the proposed framework effectively addresses complex reasoning issues in practical scenarios, paving the way for more reliable applications in the legal domain.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have shown remarkable generalization ability across diverse range of tasks and applications (Chowdhery et al., 2023; Touvron et al., 2023; OpenAI, 2023). But, current benchmarks may not adequately reflect the reasoning capabilities of LLMs (Valmeekam et al., 2024) and do not accurately reflect real-world situations (Huang and Chang, 2023). The validation of LLMs in more realistic and meaningful applications, such as legal reasoning, still requires extensive exploration.\nIn the legal domain, the core competency of legal professionals is to apply legal rules to facts and draw conclusions, as described by the IRAC (Issue, Rule, Application, Conclusion) framework. As shown in Figure 2, a legal professional can determine whether a case fact conforms to specific criminal charges based on legal rules. They critically assess a case against potential charges, focusing on the key points of relevant legal rules, to accurately identify the appropriate charge and distinguish inapplicable charges. Legal rules, which manifest legal theories, determine the legal consequences of factual situations (MacCormick, 2005). Therefore, properly applying legal rules reflects the grasp of legal theories.\nHowever, powerful LLMs may struggle to fully understand legal theories and perform basic legal reasoning tasks. Existing study (Dahl et al., 2024) has found that when LLMs are given criminal facts and legal rules, then asked whether cases constitute a certain charge, they tend to answer \"yes\", regardless of whether the charge is correct (golden charge) or a closely related one (confusing charge). Our empirical experiments also confirmed this issue. We sampled real-world criminal cases involving the charge of Misappropriation of Public Fund, inputting the criminal facts and legal rules into LLMs, and asked whether the case constituted the golden charge. Meanwhile, we created a control group where we input the same criminal facts and related legal rules, asking whether the case constituted a confusing charge (Fund Misappropriation). These two charges are very similar, with the key difference being whether the defendant's subject position is that of a state functionary. As shown in Figure 1, when performing legal reasoning, regardless of the prompt method or the version of GPT used, LLMs exhibit significant declines in performance when predicting confusing charges.\nGenerally, LLMs could face following challenges in legal reasoning: Inconsistent reasoning. Legal reasoning involves multi-step, compositional logic processes (Servantez et al., 2024). LLMs can be easily distracted by the interaction when generating reasoning steps (Shi et al., 2023) and may not be trustworthy by the tendency to give affirmative answers (Dahl et al., 2024). Missing key details. Legal rules and criminal facts are often described in complex natural language, making it challenging for LLMs to fully understand and reason based on them. Consequently, they often overlook key information in the rules. Lacking domain knowledge. LLMs may hallucinate erroneous legal knowledge (Dahl et al., 2024) or encounter gaps in common-sense knowledge (Huang et al., 2023). Their overconfidence can obscure these shortcomings, making them difficult to identify (Ni et al., 2024).\nTo better evaluate LLMs' understanding of legal theories and their reasoning capabilities, we introduce and construct a challenging task: confusing charge prediction (The detailed task definition is provided in Section 3). We also propose a novel framework: Multi-Agent framework for improving complex Legal Reasoning capability (MALR). First, an auto-planner breaks down complex legal rules into sub-tasks, allocating them to expert agents, reducing inconsistent reasoning in LLMs. Second, a non-parametric learning framework is proposed to draw adaptive rule-insights from trials and errors. To address the problem that LLMs may overlook crucial information in legal rules, we design a module that mimics human learning by gaining experience through reasoning trajectories and knowledge feedback, then learning insights through self-reflection. These insights supplement the rules, encouraging LLMs to focus on key factors from legal knowledge and fully understand the rules, while also guiding them to automatically seek help when they feel uncertain. These designs effectively improve LLMs' reasoning and critical-thinking skills.\nOur contributions are threefold:\n\u2022 We propose a multi-agent framework based on non-parametric learning, which encourages LLMs to automatically decompose complex legal tasks and extract insights from legal rules. Our framework assists LLMs in gaining a deeper understanding of legal rules and enhances their legal reasoning capabilities.\n\u2022 We introduce a challenging task, confusing charge prediction, to better evaluate LLMs' understanding of legal theories and their reasoning capabilities.\n\u2022 Extensive experiments are conducted on the multiple real-world datasets, demonstrating that the proposed framework can effectively addresses complex reasoning issues in real-world scenarios. Our work paves the way for more trustworthy application in legal domain\u00b9."}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Legal AI and LLMs", "content": "Legal AI aims to improve legal tasks through AI techniques, particularly showing significant potential in alleviating the issue of \u201ctoo many cases but too fewer legal experts\" in the legal field (Katz et al., 2023; Dahl et al., 2024). One of the main challenges in legal domain is the training dataset can be considerably expensive and sparse (Sun et al., 2020), primarily comes in text, such as statutes, law articles and criminal cases. Under these circumstances, LLMs shows promising prospects in legal scenarios due to their powerful generalization capabilities in understanding and generating text. These applications include areas such as legal summarization (Deroy et al., 2023), legal document retrieval (Sun et al., 2024), legal question answering (Louis et al., 2024) and legal judgment prediction (Yu et al., 2022; Wu et al., 2023; Servantez et al., 2024)."}, {"title": "2.2 Legal Reasoning and LLMs", "content": "Reasoning based on judicial rules and case fact descriptions is a fundamental ability of legal professionals, reflecting their understanding and application of legal theories (Servantez et al., 2024). Previous studies on Legal Judgment Prediction (LJP) have primarily focused on automatically predicting case charges mainly from fact descriptions (Zhong et al., 2018; Chalkidis et al., 2019; Liu et al., 2022). Additionally, similar precedents can be retrieved as supplementary guides to improve performance (Wu et al., 2023). However, this approach can lead to inaccurate judgments due to overlooked potential differences in case details. To address the subtle differences between case details and legal rules, knowledge graphs have been introduced to solve confusing charges problems (Yue et al., 2021; Li et al., 2024). Despite these efforts, utilizing Four Elements Theory and innocent datasets, An et al. (2022) found that charge prediction models do not take legal theories into consideration. Instead, models learn certain shortcuts for legal reasoning. Furthermore, Chain-of-Logic (Servantez et al., 2024) directly incorporates legal rules into prompts to elicit rule-based reasoning, achieving good performance on legal reasoning tasks involving three distinct rules from the LegalBench benchmark (Guha et al., 2024). SimuCourt proposes a multi-agent framework to simulate the decision-making process of a judge (He et al., 2024).\nUnlike existing works, we aim to evaluate and enhance the capacity of LLMs to reason based on legal theories, rather than treating legal rules as supplementary information."}, {"title": "3 Preliminary", "content": "We propose Confusing Charge Prediction Task to evaluate the LLMs' ability to identify correct legal charges based on fact descriptions and legal rules. This task highlights subtle distinctions in legal rules. Only LLMs that capture these nuances can demonstrate their understanding of legal theory.\nFact Description: a concise description of a legal case, represented as a word sequence $f = {w_1, w_2, ..., w_\u03b9}$. Legal Rule: the definition of a specific criminal charge from law articles, also a word sequence $r_c = {w_1,w_2, ..., w_n}$, where c is the criminal charge. Golden Charge: The true crime label of a case. Confusing Charge: A charge similar to the golden charge but differing in one element (An et al., 2022).\nTo ensure LLMs' trustworthiness in applying legal rules, we require them to confirm the golden charge as True and reject the confusing charge as False. The task can be formalized as:\n$y = F(f, r_{gc}) \u2227 \u00acF(f,r_{cc})$\nwhere gc refers to the golden charge, cc refers to the confusing charge, and F is the charge prediction model. y is True only if the fact description f satisfies the rule of golden charge rgc and does not match the rule of confusing charge rcc.\nLLMs should correctly identify the golden charge and explain why the fact description doesn't match the confusing charge, demonstrating understanding of legal theories."}, {"title": "4 The Proposed Framework", "content": "Figure 3 shows an overview of our proposed framework, which consists of four core components: Auto-Planner for Task Decomposition, Role Assignment for Sub-task Agent, Adaptive Rule-Insights Training, and Reasoning with Rule-Insights."}, {"title": "4.1 Auto-Planner", "content": "A single LLM may exhibit inconsistencies when directly generating the whole reasoning process (Wang et al., 2024). Therefore, we designed an automatic planning module to decompose the task. Given a question q, a case fact description f, and the corresponding legal rule rc about a criminal charge c, we guide an LLM as auto-planner to decompose the question into a sequence of sub-tasks based on the input of the fact and the rule:\n$[st_1, ..., st_k] = LLM(q, r_c, f, P_{auto})$  (1)\nwhere pauto is the guideline prompt for LLMs to generate the sub-task set for the question q, and the stk stands for the specific sub-task action and the k is the length of the sub-task set.\nGiven the resource-intensive nature of generating sub-tasks for every criminal case, we design a more effective strategy. We first sample a smaller scale dataset for auto-planner training, and prompt the LLM to generate the sub-task set for each sample. Subsequently, an LLM is used to identify semantically duplicate sub-task and compute the probability distribution for each sub-task. Based on this process, important sub-tasks with probability exceeding the threshold ( are used to constitute the final sub-task set."}, {"title": "4.2 Assigning Roles to Sub-task Agent", "content": "Assigning roles can help the LLMs better perform complex task reasoning (Wang et al., 2023). Therefore, based on the sub-task set $[st_1, ..., st_k]$, we employ k LLM-based agents to tackle each sub-task. Formally, we use the content of the sub-tasks to generate the appropriate prompts pst and generate k agents to tackle each sub-task. Each agents will break down specific aspects of legal rule, check whether the fact description f conforms the corresponding sub-rule rcst, and generate the answer Ast. This process can be formulated as:\n$A_{st} = M_{st}(r_{cst}, f,P_{st})$\nAfter obtaining the answer for each sub-task, a logic expression (Servantez et al., 2024) based on the principle of \"presumption of innocence\" (An et al., 2022) will generate the final answer."}, {"title": "4.3 Adaptive Rule-Insights Training", "content": "As aforementioned, LLMs can be easily distracted by the irrelevant context (Shi et al., 2023) and tend to overlook the key information and important details within rules. Therefore, we aim to enable LLMs to automatically extract the most critical information for legal judgement directly from the rules. Previous research demonstrated that LLMs can autonomously learn from their own experiences (Shinn et al., 2024; Zhao et al., 2024). Inspired by the Kolb's Experiential Learning Model (Kolb, 2014), we design the insights training module, as shown in Figure 3 (B), which consists of three core processes: experience gaining, insights drawing from errors and successes, and insights filtering. This module mimics the human learning process and facilitates the LLMs to automatically learn rules, discovering and summarizing the most critical information in the rules.\nExperience Gaining. A small training dataset with N charges is constructed, with each charge containing case samples and corresponding confusing charges. Based on the fact descriptions of a case, sub-task agents Mst will respectively generate sub-answers for both golden charge and confusing charge. These sub-answers will be synthesized into a final determination of whether the case satisfies the legal rule for the golden charge or confusing charge, and the ground truth is used as feedback. Successful trials are saved as successful experience, while failed trials trigger the Aspect-level Self-Reflector to identify incorrect sub-task agents $M^{et}_{st}$ and generate reasons $r_{se}$ for the errors. In the next trial, the error reasons are used to improve sub-task agents' predictions. Such approach of learning from trials and errors can be effective, as demonstrated in (Shinn et al., 2024). This iterative process continues for a maximum of L rounds, and corrected trajectories are retained as error-success-pair experience. The algorithm procession is detailed in Alg 1.\nInsights Drawing from Errors and Successes. We gain insights into rules by analyzing experience collections using different methods. For error-success pairs, we use a contrast-based approach, comparing incorrect and correct attempts. This enables the $M^{insight}$ to identify the most critical task-level judgments where errors occur, outputting insights in an if-then format. Successful experiences reveal best practices (Zhao et al., 2024), so we provide the entire successful reasoning process to $M^{insight}$ to generate corresponding insights.\nInsights Filtering. To address the potential for generating duplicate or incorrect insights when interpreting rules from the aforementioned process, we employ an LLM as an automatic checker, $M^{filter}$, to identify and remove redundant insights and filter out invalid ones. Ultimately, the retained insights are stored in the rule-insight knowledge base as memory. The pseudo-code for insight drawing and filtering is presented in Algorithm 2."}, {"title": "4.4 Reasoning through Insights", "content": "The generated insights serve two purposes: (1) they supplement the rules as additional notes, and (2) they guide LLMs to inquire about uncertainties when facing knowledge gaps in specific sub-tasks. For implementation, we retrieve relevant insights $i_{nst}$ from the knowledge base I for each question to improve reasoning. If the rule does not exist, the most similar rule from the knowledge base is selected based on semantic similarity, and a few-shot method is used to generate new insights. To address potential knowledge gaps in LLMs, our in-sights serve as guidance to ask factuality questions. Based on the insights, we identify sub-tasks requiring fact-checking and use a few-shot method to prompt LLMs to ask key questions like \u201cIs a   a state functionary?\u201d The generated question is then presented to a knowledgeable expert (a legal professional, a domain-specific LLM, or a search engine) to obtain knowledge feedback $kg_{st}$. Finally, we incorporate related insights $i_{nst}$ and knowledge feedback $kg_{st}$ into our ultimate reasoning process. As shown in Figure 3(A)(4), the improved reasoning process for each sub-task agent can be represented as:\n$A_{st} = M_{st}(r_{st}, f, i_{nst}, kg_{st}, P_{st})$ (2)"}, {"title": "5 Experiment", "content": ""}, {"title": "5.1 Experimental Setting", "content": "Dataset. We evaluate LLMs' legal reasoning capability on three datasets: CAIL2018 (Xiao et al., 2018), CJO (Wu et al., 2023), and CAIL-I (An et al., 2022). CAIL2018 and CJO consist of real-world cases with fact descriptions and golden charges. We match corresponding confusing charges based on the golden charges and randomly sample 400 cases from CAIL2018 and 100 from CJO for evaluation. CAIL-I's testset contains 462 innocent cases without crimes and the most similar charges to each non-criminal fact. Further dataset information is available in Appendix B.\nThe pairs of confusing charges are carefully selected by a group of legal experts, including: (1) Misappropriation of Public Fund (MP) v.s. Fund Misappropriation (FM); (2) Bribery (BY) v.s. Bribery of Non-State Officials (BN); (3) Kidnapping (KD) v.s. Illegal Detention (ID); (4) Fraudulently Obtaining Loans (FL) v.s. Loan Fraud (LF); (5) Fund Misappropriation (FM) v.s. Official Embezzlement (OE); (6) Fraud (FD) v.s. Loan Fraud (LF); (7) Fraud (FD) v.s. Cheating and Bluffing (CB); (8) Forging, Altering, Trading Official Documents, Certificates and Seals of State Organs (FO) v.s. Forging the Seals of Companies, Enterprise, Institution (FS). Key differences between each pair are provided in Appendix B.\nImplementation. We employ the publicly available GPT-3.5-Turbo-0125 and GPT-4-0125-preview models, with the temperature set to 0 for all text generation tasks. For auto-planner and insights training, we construct a small training set from CAIL-2018 training set. Specifically, we sample two cases for each of the 16 charges (totally 32 training samples). The threshold ( for the sub-task auto-planner is set to 0.8. Sentence-BERT (Thakur et al., 2021) and cosine similarity are used to compute semantic distances between rules and unseen legal rules, facilitating rule-insight inference testing in CJO and CAIL-I. During the insights training period, we limit the maximum number of trial attempts L to 2. For providing knowledge feedback, we employ Farui-200B2, which can be replaced by other legal models or even legal experts in real-world scenarios. Additionally, we construct a legal rule knowledge base that includes Chinese Criminal Law Articles and all charge definitions. All legal rules are retrieved from this knowledge base based on the charge name. The inference time and cost can be seen in Appendix \u0412."}, {"title": "5.2 Baselines", "content": "Zero-shot Setting: (1) ZS-CoT (Kojima et al., 2022) uses \u201cLet's think step by step\" to encourages LLMs to generate intermediate steps and improve reasoning. (2) Legal Reasoning Prompting (LRP) (Yu et al., 2022) is a zero-shot legal prompting method that teaches LLMs to reason like a lawyer, following the \"Approach, Issue, rule, application and conclusion\" framework.\nFew-shot Setting: (1) Few-Shot prompting (Brown et al., 2020) is the standard prompting method includes only the sample and answer. We use a two-shot setting with one positive and one negative examples. (2) Few-Shot CoT (Wei et al., 2022) uses a few chain-of-thought demonstrations as exemplars to improve the ability of LLMs to perform complex reasoning. Again, we employ a two-shot setting with one positive and one negative examples. (3) Chain-of-Logic (Servantez et al., 2024) elicits rule-based reasoning by decomposing the rule into elements, answering each rule element separately, and finally using a logical expression to obtain the final answer. This approach is meticulously designed for legal reasoning tasks."}, {"title": "5.3 Experiment Results", "content": "Main Results: From Table 1, we observe the following findings. (1) LLMs fail to distinguish confusing charges using simple but effective prompt methods such as CoT, and legal-specific prompting approaches also fail to predict accurately. By examining the actual prediction results, we found that LLMs using these methods tend to respond with \"yes.\u201d (2) \u201cMALR w/o insight\", which only decomposes the task into sub-tasks, outperforms all the baselines. This result indicates that decomposing the task into sub-tasks may mitigate LLMs' biased tendencies. Notably, without any human intervention, our auto-planning strategy can decompose legal rules into four aspects: Subject (Sub), Mental (Men), Object (Obj) and Conduct (Con). This aligns with the Four Elements Theory (An et al., 2022), which is widely recognized in the legal domain. (3) \u201cMALR w/o ask\u201d does not utilize external knowledge feedback but still achieves the second-best results, indicating that the learned insights did significantly enhance the LLM's understanding of legal rules. (4) The complete MALR achieves the best performance on all datasets, demonstrating the effectiveness of proposed framework and the necessity of its core components. MALR achieves the best performance on nearly all confusing charge pairs (refer to Appendix D). (5) Regarding the base models, GPT-3.5 benefits more from our proposed MALR compared to GPT-4, achieving a more significant improvement over the baseline methods. This suggests that our framework has a stronger enhancing effect on LLMs with weaker foundational capabilities.\nAblation Results: Table 2 demonstrates the effectiveness of the components in adaptive rule-insights training module. (1) The results of \u201cw/o $E_{success}$ (without Successful Experience)\u201d, \u201cw/o $E_{esp}$ (without Error-Success-Pair Experience)\u201d, and \u201cw/o $M_{filtering}$ (without Insight Filtering)\u201d prove the significance of each designed component in the learning from the trial-and-error process. (2) The \u201cdirectly generate\u201d approach involves encouraging the LLM to generate insights directly based on the legal rules without any training process. However, the performance drops in most situations, some-times even worse than without using insights at all. A possible explanation is that directly generating insights may lead to the inclusion of unimportant information. We provide case examples with explanations comparing the directly generated insights with those obtained through our training process in Appendix E.\nOpen-source LLMs with different model sizes. To further test the applicability of our MALR on different LMs (i.e. different sizes of open-source LLMs), we supplemented relevant experiments using a series of Qwen-2 models (Yang et al., 2024). Our findings indicate that our MALR achieves the best results across LLMs of different sizes and adheres to scaling laws. Interestingly, we also observed more significant improvements in smaller LLMs, which further demonstrates the effectiveness and practical significance of our proposed framework (details can be seen in Appendix F).\nThe Challenge and Significance of the Confusing Charge Prediction Task: To demonstrate the challenge and significance of our proposed task, we thoroughly compared General Charge Prediction and Confusing Charge Prediction. These comparisons clearly indicate that while general legal models perform well on traditional general charge prediction tasks, they are less effective for the confusing charge prediction task. Additionally, we also analyzed the performance of human annotators on this task. Our findings demonstrate the urgency and importance of this task setting, and they reveal that MALR can even surpass human performance (details can be seen in Appendix G)."}, {"title": "5.4 Case Study", "content": "Figure 4 presents an example of different methods used to predict confusing charges. As demonstrated in the case, our framework effectively focuses on the most critical aspects of the legal rules and makes a well-reasoned judgment. In contrast, both LRP and Chain-of-Logic overlook the crucial information in the legal rules, resulting in their failure to accurately predict the confusing charge."}, {"title": "6 Conclusion", "content": "In the study, we introduce a challenging task to better evaluate LLMs' capability to comprehend legal theories. The proposed MALR framework can automatically decomposes complex legal tasks and extracts insights from legal rules, enhancing LLMs' legal reasoning abilities. Extensive experiments demonstrate MALR's effectiveness in equipping LLMs with a robust understanding of legal rules."}, {"title": "7 Ethical Considerations", "content": "The datasets we used for evaluation are all from public legal datasets, and information about the defendants has been anonymized. To ensure personal privacy is not violated, we conduct a secondary review before releasing our dataset to ensure all personal information has been completely removed.\nOur work focuses on exploring algorithms to enhance the complex reasoning capabilities of LLMs, rather than replacing human judges or being directly used in real-world decision-making applications. In practical use, human judges should act as the final safeguard to maintain fairness and mitigate the potential harms related to algorithms. We will restrict its use for non-commercial purposes such as academic research through a specific license."}, {"title": "8 Limitations", "content": "Our work has two main limitations. First, even though we achieved great results, MALR did not predict correctly on all confusing charge pair cases. In the future, retrieval augmented generation could help our model perform better.\nSecond, our framework shows that LLMs can self-improve by summarize insights into the rules from trials and errors, which helps LLMs to better perform in complex legal reasoning tasks. Nevertheless, the potential for applying this approach in other fields such as medicine, finance, and scientific discovery remains unexplored. In the future, our framework could be applied in diverse domains."}]}