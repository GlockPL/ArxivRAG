{"title": "Do Multilingual LLMs Think In English?", "authors": ["Lisa Schut", "Yarin Gal", "Sebastian Farquhar"], "abstract": "Large language models (LLMs) have multilingual\ncapabilities and can solve tasks across various lan-\nguages. However, we show that current LLMs\nmake key decisions in a representation space clos-\nest to English, regardless of their input and out-\nput languages. Exploring the internal represen-\ntations with a logit lens for sentences in French,\nGerman, Dutch, and Mandarin, we show that the\nLLM first emits representations close to English\nfor semantically-loaded words before translating\nthem into the target language. We further show\nthat activation steering in these LLMs is more\neffective when the steering vectors are computed\nin English rather than in the language of the in-\nputs and outputs. This suggests that multilingual\nLLMs perform key reasoning steps in a represen-\ntation that is heavily shaped by English in a way\nthat is not transparent to system users.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) are predominantly trained\non English data, yet are deployed across various languages,\nincluding some that are rarely seen during training. This\nraises an important question: how do LLMs operate across\ndifferent languages?\nLLMs are hypothesized to operate in an abstract concept\nspace (Chris Olah, 2023; Nanda et al., 2023a; Wendler et al.,\n2024; Dumas et al., 2024). From the multilingual perspec-\ntive, one main question is whether the concept space is\nlanguage-specific or language-agnostic. We consider three\ndifferent hypotheses:\n1. LLMs 'operate' in a space that is English-centric (or\ncentered on the main pretraining language)\n2. LLMs 'operate' in a language-agnostic space"}, {"title": "2. Background", "content": ""}, {"title": "2.1. Large Language Models", "content": "Language models are trained to operate across different\nlanguages. Table 1 summarizes the four LLMs we study,\nwhich differ in the number of languages they were trained\non. Aya-23-35B supports the widest range of languages,\nwhile Gemma-2-27b covers the fewest.\nWe evaluate these models across five languages, selected\nbased on their varying levels of representation during train-\ning. English, the predominant training language, serves as\na baseline. French and German represent high-resource,\nnon-English languages, while Dutch and Chinese are lower-\nresource languages. Dutch is only a high-resource language\nin Aya-23-35B and therefore provides an interesting com-\nparison to German due to their linguistic similarity. This\nanalysis allows us to understand the performance disparities\nacross languages with varying levels of representation in\ntraining."}, {"title": "2.2. Methods", "content": "Our goal is to understand whether LLMs have a universal\nrepresentation space. To address this question, we use three\nmechanistic interpretability methods. The logit lens (Sec-\ntion 2.2.1) allows us to examine the internal representations,\nwhile causal tracing provides insight into where facts are\nencoded in the model across different languages (Section\n2.2.2). Finally, steering vectors let us intervene on the mod-\nels' internal representations (Section 2.2.3), which allows\nus to verify that the representations influence the output."}, {"title": "2.2.1. LOGIT LENS", "content": "The logit lens (nostalgebraist, 2020) decodes the internal\nrepresentations of an LLM into tokens. LLMs take an input\nx and output a probability distribution over the next token.\nThe logit lens decodes the intermediate representation h\u2081(x)\nat layer l into an output token, by applying the unembedding\nlayer:\nargmax, softmax(Wuhi(norm(x))) (1)\nwhere x is the input, Wu is the unembedding matrix of the\nmodel and the subscript t corresponds to the token. Figure\n1 shows the logit lens applied to Llama when generating:"}, {"title": "2.2.2. CAUSAL TRACING", "content": "Causal tracing (Meng et al., 2022; Vig et al., 2020) uses\ncausal mediation analysis to identify where facts are stored\nwithin a network. The method compares corrupted hidden\nstates where the information necessary to retrieve the\nfact has been removed with clean hidden states -that\nsuccessfully output the fact. This approach allows us to\nidentify the part of the network that encodes the fact. Further\ndetails can be found in Appendix A.1."}, {"title": "2.2.3. STEERING VECTORS", "content": "Steering vectors (Subramani et al., 2022; Turner et al., 2023;\nPanickssery et al., 2024) are used to nudge the behavior of\nthe LLM in the desired direction. The main idea is to add\nactivation vectors during the forward of a model to modify\nits behavior as follows:\nhi(x) \u2190 h\u2081(x) + \u03b3\u03c5\u03b9, (2)\nwhere vi is the steering vector, and y \u2208 R+ is a scalar\nhyperparameter. Steering vectors are used to nudge the\noutput of the LLM in the desired direction. For example, if\nwe want the output to contain more 'love', we can compute\na steering vector as follows:\n\u03bd\u03b9 = h\u2081(love) \u2013 hi (hate). (3)\nFurther details can be found in Subramani et al. (2022) and\nTurner et al. (2023)."}, {"title": "3. Datasets", "content": ""}, {"title": "LLM-Insight", "content": "We created a dataset to analyze the behavior\nof LLMs, which we will release alongside this paper. The\ndataset is specifically designed to study steering in LLMs.\nIt includes 72 target words, each paired with 10 prompts\nand 10 sentences in English, Dutch, French, German and\nMandarin. Table 2 shows a sample of the data. The prompts\nare designed so the word could appear as the next token,\nbut the prompts are also sufficiently open-ended so that\nsemantically unrelated words can be used to complete the\nsentence. For example \"They adapted a\" can be completed\nwith the word 'animal' as well as 'daughter'.\nThe sentences provided in the dataset can be used to find\nsteering vectors. Some words in the dataset naturally form\npairs that can be used to create steering vectors, such as\nthe words 'good' and 'bad'. For words without a natural\npairing, such as 'thermodynamics', we provide a general set\nof sentences as the counter set to create the steering vector.\nFurther details on the dataset can be found in Appendix A.3."}, {"title": "City facts (Ghandeharioun et al., 2024a)", "content": "We use this\ndataset to investigate how facts in different languages are encoded in LLMs. The task is to provide the capital city of\na given country. For example, when prompted with \"The\ncapital of Canada is\u201d, the model should output \u201cOttawa\u201d.\nThis allows us to identify where in the network Ottawa\nis encoded. To analyze cross-lingual representations, we\naugment the dataset by translating these facts into German,\nDutch, and French."}, {"title": "4. Experiments", "content": "We want to understand whether LLMs process prompts\ndifferently depending on the output language. First, we\nanalyze the latent space to find that LLMs make semantic\ndecisions that are more closely aligned with the English\nrepresentation space (Section 4.1). Next, we show that we\ncan steer activations better when using English steering\nvectors (Section 4.2). Lastly, in Section 4.3, we show that\nthe representations of facts are shared across languages, but\nhave an English-centric bias when decoded."}, {"title": "4.1. Inspecting the latent space of LLMs using the Logit Lens", "content": ""}, {"title": "Qualitative Examples", "content": "To build an intuition on how LLMs\noperate when prompted in different languages, we analyze\ntheir latent space using the logit lens, which decodes the\ninternal representations. In Figure 1, nouns and pronouns\nare routed through English, whereas the coordinating con-"}, {"title": "Quantitative Evaluation", "content": "The qualitative examples shown\nin Figures 1 and 3 suggest that the part of speech determines\nwhether LLMs employ English routing. To investigate this,\nwe prompt each LLM to generate 720 sentences. For each\ngenerated word, we evaluate whether the English equivalent\nof a word appears in the latent space. For example, in Figure\n3, for the word groenten, we check whether the English\nequivalent, vegetables, appears in the decoded latent space.\nWe then aggregate the results across different parts of speech.\nFurther implementation details are provided in Appendix\nA.5."}, {"title": "4.2. Cross-Lingual Steering", "content": "Our experiments in Section 4.1 suggest that LLMs may first\nselect topic words in an English representation space, before\ntranslating them into the output language in the later layers.\nTo further investigate this hypothesis, we evaluate whether\nnon-English model outputs can be modified using English\nsteering vectors.\nMore concretely, we test whether we can steer models to\ngenerate a sentence in a specified output language using two\ntypes of steering vectors:\n\u2022 topic steering vector \u2013 encourages the LLM to gener-\nate a sentence with the given topic, such as animals.\n\u2022 language steering vector \u2013 encourages the model to\ngenerate text in the desired output language.\nWe evaluate the effectiveness of steering across various top-\nics and prompts, using the LLM-Insight dataset (see Section\n3). We evaluate steering as successful if the generated sen-\ntence includes the target word associated with the steering\nvector while avoiding output collapse \u2013 incoherent sentences\nor stuttering.\nFigure 4 shows results when steering different LLMs. In\ngeneral, we observe that English steering vectors perform\nthe best - outperforming steering vectors generated using\nthe desired output language. This suggests that the represen-\ntation space is not universal \u2013 if it were, we would expect\nthe cross-lingual performance to be roughly equal across\nlanguages. Instead, this supports the hypothesis that these\nmodels select these words in English.\nHow similar are the steering vectors generated in dif-\nferent languages? The steering vectors for the same con-"}, {"title": "4.3. Investigating the Representation Space", "content": "In this section, we study how cross-lingual facts are encoded\nrelative to each other using the city facts dataset (see Section\n3). First, we perform causal tracing to determine whether\nfacts in different languages are encoded in the same part of\nthe model. Figure 5 shows the causal traces for Aya-23-35B\n(see Appendix A.9 for other LLMs). We find that facts\nare generally localized in similar layers, regardless of the\nlanguage.\nNext, we want to understand if the representation of a fact is\nshared across different languages. In particular, if we have\nthe same fact in two different languages, such as English\nand Dutch, can we decompose the representation as follows:\nh(capital of Canada) = hottawa + hEnglish (4)\nh(hoofdstad van Canada) = hottawa + hDutch, (5)\nwhere h represents a vector in the latent space. If the above\nequations hold, we may be able to interpolate between the"}, {"title": "5. Limitations", "content": "Our work provides evidence suggesting that MLLMs pri-\nmarily operate in English. Below, we outline potential limi-\ntations and directions for future research."}, {"title": "Tokenization", "content": "Sentences in different languages often vary\nin tokenization length (Rust et al., 2021; Muller et al., 2021;\nPetrov et al., 2024), which complicates cross-lingual com-\nparisons. In this work, we provide heuristics (e.g., for causal\ntracing, which operates on a per-token level) to compare the\nresults when tokenization lengths vary. However, tokeniza-\ntion remains an important consideration for the development\nof future interpretability methods designed to be used across\nmultiple languages."}, {"title": "Language confidence and confusion", "content": "Models often as-\nsign higher probabilities to outputs in certain languages,\nwhich can affect analyses such as causal tracing by requir-\ning higher noise levels. Similarly, models often exhibit\nlanguage confusion (Marchisio et al., 2024), continuing to\nrespond in English even when prompted in other languages.\nBoth factors influence our analysis. We can mitigate some\nissues associated with the first problem \u2013 e.g., in causal\ntracing, we ensure the probabilities all fall below a speci-\nfied threshold when a prompt is noised. However, we do\nnot actively address language confusion, as doing so could\nalter the natural behavior of the LLMs, which we aim to\nunderstand."}, {"title": "Factors affecting interpretability methods", "content": "Interpretabil-\nity methods are influenced by various factors. Steering\nperformance, for example, depends on the intrinsic steer-\nability of a prompt (Turner et al., 2023; Tan et al., 2024). To\naddress this, we designed a custom dataset that, to the best\nof our knowledge, is equally steerable across all languages.\nAnother challenge is that steering could push activations\noutside the expected data distribution, leading to unintended\noutputs. To mitigate this, we checked for stuttering in the\ngenerated outputs. However, further work is needed to\ndeepen our understanding of steering mechanisms and to\ndevelop more robust evaluation procedures."}, {"title": "Other Methods", "content": "Exploring alternative methods could pro-\nvide valuable insights. For example, sparse autoencoders\n(SAEs) (Olshausen and Field, 1997; Hinton and Salakhut-\ndinov, 2006; Templeton et al., 2024) are a popular inter-\npretability tool. However, training SAEs for each layer is\ncomputationally expensive and beyond our computational\nbudget. While some pre-trained SAEs are available, they\nare predominantly trained on English data, which introduces\nbiases we aim to avoid (Lieberum et al., 2024)."}, {"title": "6. Related Work", "content": "We can think about understanding a model from two per-\nspectives:\n\u2022 an internal perspective, focused on analyzing the\nmodel through the latent space and operations per-"}, {"title": "6.1. How do LLMs operate internally?", "content": "The current main theory in mechanistic interpretability sug-\ngests that there are three general phases in the forward pass\nof an LLM (Chris Olah, 2023; Nanda et al., 2023a; Wendler\net al., 2024; Dumas et al., 2024; Fierro et al., 2025):\n1. Detokenization: In this phase, individual tokens are\ncombined into abstract units that the model uses for\nanalysis. These units can be referents \u2013 for example,\n(Nanda et al., 2023a) found evidence that the tokens\n[Michael] and [Jordan] are combined into a unit repre-\nsenting the basketball player Michael Jordan. Similarly,\nthese units can encode instructions, as shown by (Du-\nmas et al., 2024), where the model extracts the target\nlanguage during translation tasks in these layers.\n2. Processing: In this phase, the model processes or rea-\nsons over abstract units. For instance, this stage may\ninvolve tasks like fact recall (Geva et al., 2023; Nanda\net al., 2023a).\n3. Selecting the output: In this phase, the model selects\nthe output. This may involve selecting the correct at-\ntribute (Nanda et al., 2023a), mapping an abstract con-\ncept to the corresponding word in the target language\n(Wendler et al., 2024) and/or selecting the correct token\nfor the intended word.\nIn the context of multilingual models, an important ques-\ntion is whether the concept space (in phase 2) is universal.\nHere, universal means the representation is shared across\nlanguages, i.e., the representation for 'cat' (cat in English)\nand 'kat' (cat in Dutch) is the same.\nOne stream of research argues that the concept space is\nuniversal. When analyzing the latent space with the logit\nlens, Wendler et al. (2024) find that the concept space is\nlanguage-agnostic, but more closely aligned with the En-\nglish output space. In their follow-up work, Dumas et al."}, {"title": "6.2. Multilingual LLM behavior", "content": "The internal mechanisms of LLMs affect their performance\nin several different ways, which we summarize below.\nPerformance The performance of multilingual language\nmodels varies across languages (Shafayat et al., 2024;\nHuang et al., 2023; Bang et al., 2023; Shi et al., 2022; Ahuja\net al., 2023), often performing best in English. This can even\nbe leveraged to improve performance in other languages, for\nexample, through cross-lingual chain-of-thought reasoning\n(Chai et al., 2024), or by modifying prompts, such as using\nmultilingual instructions or asking the LLM to translate the\ntask into English before completing it (Zhu et al., 2023;\nEtxaniz et al., 2023).\nFluency and language confusion Marchisio et al. (2024)\nhas shown that English-centric models are prone to lan-\nguage confusion, i.e., providing the answer in the incorrect\nlanguage. Moreover, even when LLMs output text in the\ncorrect language, they can produce unnatural sentences in\nother languages, akin to an accent (Guo et al., 2024)."}, {"title": "Bias and culture", "content": "Moreover, LLMs tend to be biased to-\nward certain cultures, with content performing better when\ndealing with facts originating from Western contexts (Naous\net al., 2024; Shafayat et al., 2024), falling short when an-\nswering questions on other cultures (Chiu et al., 2024). Liu\net al. (2024) investigate the cultural diversity of LLMs using\nproverbs and find that these models often struggle to reason\nwith or effectively use proverbs in conversation. Their un-\nderstanding appears limited to memorization rather than true\ncomprehension, creating a notable \"culture gap\" when trans-\nlating or reasoning with culturally specific content across\nlanguages."}, {"title": "7. Conclusion", "content": "Our results provide evidence that semantic decisions in\nLLMs are predominantly made in a representation space\nclose to English, while non-lexical words are processed in\nthe prompt language. However, we find that this behavior\nvaries across models, likely due to differences in multi-\nlingual proficiency and model size. The English-centric\nbehavior is further validated by our findings that steering\nnon-English prompts using vectors derived from English\nsentences is more effective than those from the prompt lan-\nguage.\nExploring the structure of the latent space, we find that\nfactual knowledge across languages is stored in roughly\nthe same regions of the model. Interpolating between the\nlatent representations of these facts in different languages\npreserves predictive accuracy, with the only change being\nthe output language. This suggests that facts encoded in\ndifferent languages likely share a common representation.\nHowever, when interpolating, we find that model output is\nmost frequently in English, further underlining the English-\ncentric bias of the latent space.\nThe English-centricity of the latent space is consistent with\nprior observations about LLM behavior. In particular, Etx-\naniz et al. (2023) found that instructing LLMs to first trans-\nlate a non-English prompt into English improves model\nperformance. However, this bias can be detrimental. If the\nlatent space is English-centric, this may lead the LLMs to-\nward exhibiting Western-centric biases (Naous et al., 2024;\nShafayat et al., 2024)."}, {"title": "8. Discussion", "content": "There are currently two perspectives in interpretability re-\nsearch on concept representations in multilingual models:\n(1) concept representations are universal; and (2) concepts\nhave language-centric representations, where the language\nis the training-dominant language. Our work aligns more\nclosely with the second perspective, as well as a third per-\nspective - namely, that LLMs encode language-specific"}, {"title": "Impact Statement", "content": "Large Language Models (LLMs) are increasingly deployed\nacross a wide range of applications, making it crucial to\nunderstand and evaluate their performance to ensure both\nsafety and fairness. A key characteristic of LLMs is their\nEnglish-centric nature, which influences their behavior, as\nshown in this paper. In particular, we find further evi-\ndence that LLMs perform semantic decisions in English.\nThis likely leads to biased behaviour (Naous et al., 2024;\nShafayat et al., 2024). Understanding this is essential to\nequitable and reliable outcomes in diverse linguistic and\ncultural contexts.\nMoreover, our findings may further be relevant for improv-\ning the safety of LLMs. When analysing non-lexical words,\nwe found that LLMs do not emit an English-centric bias. If\nknowledge representation is not universal across languages,\nLLMs may require language-specific safety tuning. When\nintrospecting the latent space, we observed varying levels\nof vulgar terms depending on the language, particularly in\ncases where the models are not safety-tuned on the language.\nWhile this does not necessarily mean the model's output\nwill be vulgar, it could make the model more vulnerable to\njailbreaks (Deng et al., 2024; Ghandeharioun et al., 2024b)."}, {"title": "A. Appendix", "content": ""}, {"title": "A.1. Causal Tracing", "content": "Causal tracing (Meng et al., 2022; Vig et al., 2020) uses causal mediation analysis to identify where facts are stored within a\nnetwork. For example, imagine that we want to find where the fact \"The capital of Canada is Ottawa\" is represented in an\nLLM. We could prompt the model with \u201cThe capital of Canada is\" to find where \u201cOttawa\u201d is stored in the network. There\nare two main steps in causal tracing:\n1. corrupt the signal: destroy the information so that the model no longer outputs the fact.\n2. restore the signal: determine where in the network the representation needs to be restored so that the LLM can recover\nthe correct output.\nLet $e^{clean} \\in \\mathbb{R}^{m,d}$ be the embedding of the prompt \u201cThe capital of Canada is\u201d, where m is the number of tokens and d is the\nembedding dimension. In the first step, the information is \"destroyed\" by adding noise to the embedding of the subject\ntoken:\nej^{corrupted} =\\begin{cases}\nej^{clean} + \\epsilon & \\text{if token j is a subject token} \\\\\nej^{clean} & \\text{otherwise} \\end{cases} (6)\nwhere \u025b is noise sampled from an isotropic Gaussian distribution, and $e^{corrupted}$ is the corrupted embedding. We push-\nforward corrupted embeddings $e^{corrupted}$ through the network to obtain the probability that the model outputs Ottawa,\np[Ottawa|$e^{corrupted}$].\nNext, we want to find out which part of the hidden states encodes the relevant information to restore the correct output. At a\ngiven layer l in the network, we 'restore' part of the corrupt hidden state by copying back part of the clean hidden state h at\nposition p:\nh_{j,l}^{restored} = \\begin{cases}h_{j,l}^{clean} & \\text{if j = p} \\\\h_{j,l}^{corrupted} & \\text{otherwise}, \\end{cases} (7)\nwhere the hidden state $h^{clean} = [h^{clean}, ..., h^{clean}]$ is obtained by pushing the original embeddings, $e^{clean}$, through the\nnetwork.\nFinally, we propagate $h^{restored}$ through the remaining layers produce the output probability p[Ottawa|$h^{restored}$]. The difference\np[Ottawa|$h^{restored}$] - p[Ottawa|$h^{corrupted}$], measures the importance of layer l and token position p in encoding a fact. Through\nthis approach, causal tracing helps identify which parts of the representation are sufficient to retrieve the correct output."}, {"title": "A.2. LLM Training Data Languages", "content": "Table 4 summarizes the languages the different models are trained on."}, {"title": "A.3. LLM-Insight Dataset", "content": "Our goal is to generate a dataset that can be used for cross-lingual interpretability. We wanted a dataset that can be used to\nintrospect LLM internal representations and analyze LLM behavior when the internal representations are intervened on.\nAdditionally, the dataset focuses on open-ended sentence generation rather than being restricted to specific tasks like fact\nrecall or sentiment analysis, as text generation is an important real-world application."}, {"title": "A.3.1. TEXT GENERATION", "content": "We use GPT-40 to generate sentences and prompts. For each target word, we generate:\n\u2022 10 unique sentences containing a version of the word \u2013 for example, for the verb '(to) see', a suitable sentence is 'She\nsaw a bird in the sky.'\n\u2022 a list containing the version of the word used in each sentence. In the previous example, the version of the word is\n'saw'.\n\u2022 10 unique prompts, designed to be completed with the target word.\n\u2022 a list containing a version of the word used in each sentence\nWe instruct GPT-40 to generate prompts that can be completed with the target word, as well as semantically distinct words.\nHowever, we observe that the model sometimes produces sentences and prompts that do not meet the criteria.\nAn example of a sentence that does not meet the criteria is:\nTarget word: bouquet (boeket in Dutch)\nSentence: Het boeket was gevuld met levendige rozen en lelies.\nTranslation: The bouquet was filled with live roses and lilies.\nThe issue with this sentence is its unnatural phrasing-the word \"live\" is not typically used in this context.\nAn example of a prompt that does not meet the criteria is:\nTarget word: money\nPrompt: He went to the bank to withdraw\nIn this case, the only plausible continuation is \"money.\" While the prompt is coherent, it lacks the open-endedness needed to\nanalyze how interventions influence model behavior.\nAn example of a well-constructed prompt is:\nTarget word: bus\nPrompt: She took a\nThis can be completed with the intended word \"bus\", as well as semantically different alternatives such as \"walk\" or \"long\nroad trip\".\nTo ensure data quality, we asked native speakers to review and correct the data. The original version of the data and the\ncorrections are provided in the dataset."}, {"title": "A.3.2. DATASET SUMMARY", "content": "We selected words that vary in the number of tokens (in non-English languages), whether the word is a homograph with the\nEnglish version of the word, and the part of speech. Table 5 summarizes the words used."}, {"title": "A.4. Parts of Speech Analysis", "content": "In this experiment, we analyze how often a word is first 'selected' in English, for each part of speech. To\nidentify the part of speech, we used spacy models (Honnibal and Montani, 2017). To identify English words,\nwe use enchant.Dict(\"en_US\"). We use nl_core_news_sm, de_core_news_sm, fr_core_news_sm and\nzh_core_web_sm. In general, we can use spaces to identify words in sentences. For Mandarin, we use the package\njieba."}, {"title": "A.5. Logit Lens Quantitative Evaluation", "content": "To evaluate whether a word is chosen in English, we use GPT-40. We considered alternative evaluation procedures. We\ntested various translation packages but found issues with both word- and sentence-level approaches. When used on a word\nlevel, this caused problems with colexification and did not allow for close synonyms often only providing a single translation\nper word. When using translation on a sentence level, it was difficult to map tokens to each word (due to changes in the\nsentence structure). We also considered WordNet (Miller, 1994), but it only covers nouns, verbs, adjectives, and adverbs,\nmaking it unsuitable for other parts of speech. Ultimately, we chose GPT-40 and manually verified 100 samples to ensure\nthe evaluation was accurate.\nWe ask GPT-40 to score words as follows:\n\u2022 5: An exact translation.\n\u2022 4: A close synonym.\n\u2022 3: A word with a similar but distinct meaning.\n\u2022 2: A word whose meaning is at best weakly related.\n\u2022 1: A word whose meaning is not related.\nWhen a word receives a score of 4 or higher, we evaluate the word as chosen in English.\nAn example of the command we use is:\nBelow, you will be given a reference word in Dutch and a context (i.e., phrase or sentence) in which the word is\nused. You will then be given another list of English words or subparts of words/phrases.\nYou should respond with the word from the list that is most similar to the reference word, along with a grade for\nthe degree of similarity.\nSpecial Note on Contextual Translations: If an English word could form a common phrase or idiomatic expression"}, {"title": "Do Multilingual LLMs Think In English?", "content": "that accurately translates the reference word, it should be rated highly. For example, if a phrase like \"turned out\"\nperfectly matches a Dutch verb, the word \"turned\" alone would receive a high score due to its idiomatic fit.\nSpecial Note on Tenses: Do not penalize for different tenses. For example, the word 'want' matches 'wilde' and\nshould receive a 5.\nDegrees of Similarity: Similarity should be evaluated from 1 to 5, as follows:\n5: An exact translation.\n4: A close synonym.\n3: A word with a similar but distinct meaning.\n2: A word whose meaning is at best weakly related.\n1: A word whose meaning is not related.\nConsider the following examples:\n**Example 1**\nReference Word in Dutch: 'waarop' Context: 'Ze had een hekel aan de manier waarop hij zijn'\nEnglish Word List: ['hicks', 'mild', 'rut', 'sens', 'spiral', 'hometown', 'how', 'manner', 'van', '101', 'ward']\nAnalysis: 'waarop' means \"on which\" in Dutch. The word 'how' is most similar to this in the list, while the other\noptions are unrelated.\nAnswer Word: 'how'\nSimilarity Score: 4 - a close synonym\n**Example 2**\nReference Word in Dutch: 'bleek'\nContext: 'Ze adopteerde een zwerfdier, maar het bleek een wolf te zijn'\nEnglish Word List: ['cup', 'freed', 'freeman', 'laurent', 'turns', 'turned', 'van', '348', 'i', 'ken', 'oms']\nAnalysis: 'bleek' means \"turned out\" in Dutch, making 'turned' the most similar option.\nAnswer Word: 'turned'\nSimilarity Score: 5 - an exact translation\n**Example 3**\nReference Word in Dutch: 'vaas'\nContext: 'Ze schikte een prachtig boeket bloemen in een vaas.'\nEnglish Word List: ['tucker', 'van', 'container', 'opp', 'van', 'vessel', '-g', '-t', '397', 'art', 'as', 'ed', 'ion', 'let']\nAnalysis: 'vaas' means \"vase\" in Dutch. The word 'vessel' is somewhat similar, as vases are vessels for holding\nitems like flowers.\nAnswer Word: 'vessel'\nSimilarity Score: 3 - a word with a similar but distinct meaning\n**Example 4**\nReference Word in Dutch: 'werd'\nContext: 'Ze ging geld opnemen bij de bank en werd overvallen.'\nEnglish Word List: ['dee', 'lafayette', 'bank', 'bu', 'herself', 'kw', 'met', 'ramp', 'return', 'returning', '113', '347']\nAnalysis: 'werd' means 'was' in Dutch. None of these words are related.\nAnswer Word: None\nSimilarity Score: 1 - a word whose meaning is not related"}, {"title": "**Example 5**", "content": "Do Multilingual LLMs Think In English?\nReference Word in Dutch: 'vrienden'\nContext: 'Ze bracht het weekend door met haar vrienden in een huisje in de Ardennen.'\nEnglish Word List: ['sag'", "sat": "tween", "bro": "families", "family": "her", "herself": "mo", "own": "parents", "666": "elf", "nAnalysis": "vrienden' means \\\"friends\\\" in Dutch. The closest word here is 'families", "Word": "families", "Score": 2}]}