{"title": "MRI2Speech: Speech Synthesis from Articulatory Movements Recorded by Real-time MRI", "authors": ["Neil Shah", "Ayan Kashyap", "Shirish Karande", "Vineet Gandhi"], "abstract": "Previous real-time MRI (rtMRI)-based speech synthesis models depend heavily on noisy ground-truth speech. Applying loss directly over ground truth mel-spectrograms entangles speech content with MRI noise, resulting in poor intelligibility. We introduce a novel approach that adapts the multi-modal self-supervised AV-HUBERT model for text prediction from rtMRI and incorporates a new flow-based duration predictor for speaker-specific alignment. The predicted text and durations are then used by a speech decoder to synthesize aligned speech in any novel voice. We conduct thorough experiments on two datasets and demonstrate our method's generalization ability to unseen speakers. We assess our framework's performance by masking parts of the rtMRI video to evaluate the impact of different articulators on text prediction. Our method achieves a 15.18% Word Error Rate (WER) on the USC-TIMIT MRI corpus, marking a huge improvement over the current state-of-the-art. Speech samples are available at https://mri2speech.github.io/MRI2Speech/", "sections": [{"title": "I. INTRODUCTION", "content": "The vocal tract is crucial in human speech production, shaping the quality and acoustic attributes of generated sounds. This complex anatomical structure, including the glottis, epiglottis, pharyngeal wall, velum, tongue, and lips, undergoes dynamic transformations during speech production. Decoding these transformations into speech\u2014a process known as articulatory speech synthesis\u2014can significantly enhance computer-assisted language learning systems, provide insights into the precise positioning of articulators concerning the inferred speech, and support individuals with dysarthria [1]. The challenge lies in accurately estimating vocal tract configurations and correlating them with acoustic representations due to the complexity of the vocal folds, difficulty accessing articulatory movements, and their rapid changes.\nPrevious research has investigated articulatory synthesis using various sensors and imaging techniques, including Ultrasound Tongue Imaging (UTI) [2], Electromagnetic Articulography [3], Permanent Magnet Articulography (PMA) [4], Electrophysiology [5], Electrophysiology [5], Electrolarynx [6], [7] and Electropalatography [8]. While some of these methods are invasive, they manage to capture articulatory data at high sampling frequencies. However, none of these techniques provide a comprehensive overview of human speech production anatomy in motion. In contrast, rtMRI offers complete coverage of key articulatory structures, such as the hard palate, pharynx, epiglottis, velum, and larynx. Its high spatial resolution allows for in-depth exploration of the physiological basis of speech, singing, emotions, and other aspects of speech production [9]\u2013[14].\nThe main challenge in converting rtMRI data to speech is the machine-induced noise present in the ground-truth speech. Despite this noise, most current methods rely on it for training and follow a two-stage approach. First, they extract acoustic features such as MGC-LSP or mel-cepstrum from silent rtMRI videos using CNN-LSTM-based architectures [15], [16]. Then, these features are used to synthesize speech using HiFi-GAN and WaveGlow decoders [1], [17]. However, relying on ground-truth mel-spectrograms that include speaker-specific details and ambient noise forces models to learn irrelevant information, which reduces intelligibility (WER up to 102.6% [1]) and hinders generalization. Recent direct single-stage approaches [18] face similar challenges, additionally requiring extensive speaker-specific preprocessing of rtMRI video data. Despite this, error rates remain high (Character Error Rate (CER) at 69.2%), and their effectiveness across multiple speakers and datasets remains yet to be seen.\nAlternatively, a learning paradigm can be developed to establish a robust correlation between rtMRI videos and ground-truth text [19]. This approach offers numerous applications, such as enabling interaction with public displays and kiosks and assisting people with speech disorders, muteness, or blindness in inputting text and interacting with various computer systems. However, relying solely on text as the output modality may overlook the importance of temporal alignment, which is crucial to accurately detect the positioning of specific articulators for language learning.\nTo address the aforementioned challenges, we propose a novel method called MRI2Speech. Given the inherent noise in the ground-truth speech, achieving high intelligibility through audio modeling is challenging. Therefore, MRI2Speech relies on ground-truth (video, text) pairs to fine-tune a self-supervised audio-visual model, enabling it to infer text from silent rtMRI video input. To ensure accurate temporal alignment specific to each speaker, we introduce a novel duration predictor trained on ground-truth (audio, text) pairs. Using the inferred text and estimated temporal durations, we then train a speech decoder to synthesize aligned speech. We thoroughly evaluate MRI2Speech on the USC-TIMIT MRI [20] and Art-Speech Database 1 (ASD1) [21] datasets. Our method significantly improves rtMRI video-to-speech synthesis, achieving a minimal CER of 9.27%-9.31% and a WER of 15.18%-15.25% on the USC-TIMIT MRI corpus. We also assess"}, {"title": "II. METHOD", "content": "From a system perspective, our main contributions are two-fold: first, we exploit a multi-modal self-supervised model to directly predict text from rtMRI video; second, we estimate speaker-specific temporal alignments by training a duration predictor using available noisy ground-truth audio and text. This allows us to generate aligned speech, either in the speaker's voice or in any desired voice."}, {"title": "A. Fine-tuning AV-HuBERT for text prediction", "content": "Recent developments in self-supervised learning [22], [23] have led to high-fidelity, compressed speech representations that emphasize content-rich, disentangled features, excluding speaker and ambient information. This approach simplifies training and supports various downstream applications in producing high-quality transcriptions. AV-HuBERT [23], a SOTA multi-modal self-supervision model, has demonstrated excellent transferability to lip-reading tasks, achieving strong results with just 30 hours of paired data from the LRS3 [24] dataset. The AV-HuBERT comprises a video and audio encoder, a transformer encoder, and a cluster prediction head. Initially, it conducts feature clustering using audio-based mel-frequency cepstral coefficients to create discrete frame-level targets. In multi-iteration pretraining, features learned from the AV-HuBERT transformer network refine cluster generation in subsequent iterations.\nGiven a rtMRI (video, text) pair, we fine-tune the entire AV-HuBERT model to perform visual speech recognition using Connectionist Temporal Classification (CTC) [25] loss. We remove the cluster prediction head from the pre-trained AV-HuBERT and replace it with a randomly initialized projection layer that maps the transformer features to labels. The audio encoder is removed to eliminate speech dependence, and its output is replaced with a zero vector. The video features from the video encoder are concatenated to the zero vector. Let the ground-truth transcription have L labels. Consider the input video sequence x of length T, we define a AV-HuBERT network with m inputs, n outputs, and weight vector w as a continuous map $N_w = (R^m)^T \\leftrightarrow (R^n)^T$. Let e = Nw(x) be the sequence of visual feature output from the pre-trained AV-HuBERT and $e^t_k$ denote the activation of output unit k at time t, which can be interpreted as the probability of observing a specific label k at time t. CTC loss maximizes the sum of probabilities for all label sequences leading to the target, as defined in [25]:\n$p(k|e^t) = \\sum_{\\pi \\in B^{-1}(k)} p(\\pi|e^T)$.\nWe use CMUDict [26] for the lexicon, Adam [27] as the optimizer with a learning rate that peaks at 0.001 after 30% of updates, and continue fine-tuning until the validation loss converges (45K steps). For CTC decoding, we employ a pre-trained 4-gram language model from [23] to convert tokens into text."}, {"title": "B. Training VITS for duration estimation", "content": "We follow the VITS [28] training paradigm to train a stochastic duration predictor (SDP) for each speaker in both the USC-TIMIT MRI and ASD1 datasets. VITS functions as a conditional variational autoencoder, designed to maximize the log-likelihood $p_{\\theta}(x|c)$, where x is the ground-truth audio and c represents the text. The method involves learning specific distributions such as $p_{\\theta}(x|z)$, an approximate posterior distribution of generating speech from the latent z, and $p_{\\theta}(z|c)$, the prior distribution of the latent given text condition c. The prior encoder is a transformer-based text encoder, while the posterior encoder consists of WaveNet residual blocks. The HiFi-GAN speech decoder reconstructs the audio from the posterior latents using an adversarial loss. The normalizing flow maps the latent space across both the text and acoustic distributions. A Monotonic Alignment Search (MAS) optimizes the alignment based on log-likelihood scores from the flow decoder. Finally, the SDP estimates the durations for each input phoneme based on these alignments. We train a multi-speaker model using noisy audio-text pairs from both the evaluated corpus. Once trained, the frozen SDP weights from the multi-speaker model are used to predict durations for each input phoneme based on the provided speaker label."}, {"title": "C. Speech Synthesis", "content": "Fig. 1 provides a detailed overview of our method during the inference phase. For a given silent rtMRI video, we use our fine-tuned AV-HuBERT model to predict the corresponding text. The text is phonemicized and processed through the prior encoder to generate hidden representations. These representations are then passed through the trained SDP to estimate speaker-specific phoneme durations. The prior latents from the prior encoder are expanded according to these durations and mapped to the acoustic space using the inverse flow. Finally, these mapped representations are fed into the speech decoder to synthesize speech.\nTo synthesize speech in a novel target voice, we train a single-speaker VITS model using clean samples from the LJSpeech [29] dataset. The following approach allows for speech generation in a clean voice while preserving the duration alignments of the original noisy source speaker. The source speaker can be any individual from our multi-modal VITS trained on noisy MRI pairs. To generate speech with the source speaker's alignment but in an LJSpeech target voice, we perform inference upon the LJSpeech-trained VITS model and replace its predicted durations with those of the source speaker. The prior latents for the LJSpeech target model are then expanded according to the source speaker's durations. Using inverse flow and the LJSpeech speech decoder, we can synthesize speech in the LJSpeech's speaker voice while preserving the alignment learned from the source speaker."}, {"title": "III. DATASETS", "content": "USC-TIMIT MRI dataset: It includes 3.05 hours of paired audio, video, and text, containing 460 sentences spoken by ten American English speakers (M1-M5 and F1-F5) while lying supine in an MRI scanner [30]. The audio is sampled at 20kHz, and the video resolution is 68 \u00d7 68 pixels at 23.18 frames-per-second (fps). Adaptive signal processing algorithms were used to suppress background noise in the audio recordings from the MRI scanner [31]. Given that our approach does not involve audio for learning intermediate text representations, we did not employ additional noise removal techniques as recommended in [1]. We resized the video frames to 96 \u00d7 96 pixels and increased the frame rate to 25 fps to match the AV-HuBERT input requirements.\nASD1 dataset: It is a subset of a high-quality real-time 2D MRI database introduced in [21]. It features 77 sentences narrated by ten healthy French native speakers (P1-P10). The speech data, totaling 1.02 hours, was recorded simultaneously, denoised, and aligned with text and video. The images were captured at 50 fps with a 136 \u00d7 136 pixel resolution and 20 ms temporal resolution. The larynx is not visible in three speakers (P8-P10), leading some works to exclude them [32]. Since these speakers still show valuable articulatory movements, we include them in our evaluation. For each dataset, we reserve 10% of the samples from each speaker for the test set, using the remaining samples for training."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "Table I presents the error rates of predicted text from the AV-HuBERT model, utilizing input rtMRI videos. The predicted text achieves minimal error rates on the USC-TIMIT MRI database (CER: 10.95%, WER: 14.38%) and on the ASD1 database (CER: 11.14%, WER: 22.8%). Joint training with speakers from both datasets and testing on individual datasets leads to some drop in performance. However, the CER remains around 30%, demonstrating the model's effectiveness in overcoming challenges related to diverse recording conditions, hardware, lighting, language, and head pose.\nWe mask portions of the MRI videos to analyze how different vocal fold articulators impact text prediction. Our goal is to determine whether the model's effectiveness primarily relies on lip movements or if it also incorporates features from other articulators, given that the AV-HuBERT model, initially trained on 1,326 hours of lip-reading data, may exhibit biases or a strong dependence on lip regions. We first trained the model using only the lip movements by cropping and inputting just the lip region from the rtMRI video. Next, we masked the lip region and trained the model using only the other visible articulatory movements, such as the vellum, tongue, and glottis. The results, shown in Table II, reveal that internal articulators have a more significant impact than the lip regions alone. Inference using only internal articulators (with the lip region masked) results in a 25.33% decrease in WER compared to inference using only the cropped lip region as MRI input.\nTable III shows error rates for zero-shot experiments, where a specific speaker is excluded from training. The table presents the results for three different speakers. On average, across all ten folds, our approach achieves (CER: 13.56%, WER: 17.06%) on the USC-TIMIT MRI dataset and (CER: 11.75%, WER: 21.07%) on the ASD1 dataset, consistently delivering impressive performance on both the dataset. Compared to the two models from [19]\u2014one with and one without language models (LM)\u2014which included text predictions for unseen data from two randomly selected speakers, our approach achieves a substantial reduction in error rates. Specifically, our model reports a 65.58% decrease in CER and a 59.47% decrease in WER compared to their best-performing model with LM. This demonstrates the superior effectiveness of our approach in generating more intelligible text for unseen speakers."}, {"title": "B. Recognition performance on synthesized speech", "content": "We first synthesize speech using the model trained on noisy audio-text pairs from both datasets to ensure a fair comparison with [1]. Table IV presents the error rates for synthesized speech from the USC-TIMIT MRI dataset, specifically for speakers M4, F5, and M5. Our reconstruction with the original M4 speaker's voice achieves a 36.04% reduction in WER compared to the current SOTA method [1] under similar conditions. However, relying on noisy audio for training may negatively impact performance by causing the model to learn and retain ambient and MRI noise. Additionally, predicting mel-spectrograms limits the model's ability to synthesize speech in different novel voices. To address this, we refine our approach by using original noisy MRI audio solely to obtain speaker-specific alignment using a frozen speaker-specific SDP. We then apply these durations for synthesizing clean speech in the LJSpeech voice (see Section II-C). This method significantly enhances intelligibility and quality, with WER improving to approximately 15%-17% while maintaining accurate alignment with the video.\nFig. 2 compares mel-spectrograms of speech synthesized using [1] and our method. Notably, [1] predicts mel-spectrograms using a speech decoder trained on noisy audio, which affects speech intelligibility as seen by the missing formants in Fig. 2 (B). In contrast, our approach (Fig. 2(C)) enhances and preserves fine harmonics, demonstrating robustness even when synthesizing speech from the noisy USC-TIMIT MRI corpus. This advancement in generating high-quality, intelligible speech from rtMRI video, even for unseen speakers, represents significant progress in articulator-to-speech synthesis."}, {"title": "V. CONCLUSIONS", "content": "In this work, we introduce a novel approach to rtMRI video-to-speech synthesis by shifting from predicting acoustic representations to a two-step method. We fine-tune a multi-modal self-supervised AV-HuBERT model for text prediction and train a novel duration predictor for speaker-specific alignments. The predicted text and estimated durations are then fed into a speech decoder to synthesize speech in any novel voice of interest. This shift is necessary because noise in the audio stream can reduce intelligibility and limit the model's ability to synthesize speech in novel voices. As a novel strategy, we demonstrate self-supervision's effectiveness in understanding the ability of articulators internal to the human body for speech production. On the USC-TIMIT MRI corpus, the synthesized speech achieved a WER of 15.18%, offering a 575.89% improvement over the existing SOTA method. Our method enables effective generalization to unseen speakers across different rtMRI databases, overcoming challenges like hardware variations, head-pose changes, and lighting differences. Future work will focus on embedding emotive features from articulators into the synthesized speech."}]}