{"title": "Letters, Colors, and Words: Constructing the Ideal Building Blocks Set.", "authors": ["Ricardo Esteban Salazar Ordo\u00f1ez", "Shahrzad Jamshidi"], "abstract": "Define a building blocks set to be a collection of n cubes (each with six sides) where each side\nis assigned one letter and one color from a palette of m colors. We propose a novel problem of\nassigning letters and colors to each face so as to maximize the number of words one can spell from\na chosen dataset that are either mono words, all letters have the same color, or rainbow words,\nall letters have unique colors. We explore this problem considering a chosen set of English words, up\nto six letters long, from a typical vocabulary of a US American 14 year old and explore the problem\nwhen n = 6 and m = 6, with the added restriction that each color appears exactly once on the\ncube. The problem is intractable, as the size of the solution space makes a brute force approach\ncomputationally infeasible. Therefore we aim to solve this problem using random search, simulated\nannealing, two distinct tree search approaches (greedy and best-first), and a genetic algorithm. To\naddress this, we explore a range of optimization techniques: random search, simulated annealing,\ntwo distinct tree search methods (greedy and best-first), and a genetic algorithm. Additionally, we\nattempted to implement a reinforcement learning approach; however, the model failed to converge\nto viable solutions within the problem's constraints. Among these methods, the genetic algorithm\ndelivered the best performance, achieving a total of 2846 mono and rainbow words.\nKeywords: Permutations, Simulated Annealing, Tree-search, genetic-algorithm.", "sections": [{"title": "1. Introduction.", "content": "Building blocks are an iconic toy designed to be employed in block play,\nwhose learning outcomes (if employed since early ages) range from development of motor, and\nclassification, to mathematical and problem-solving skills [7]. Our notion of a building blocks\nset extends this concept focusing on the integration language learning with color recognition\nto create a constrained problem for children: what words can you spell that are uniform in\ncolor (mono words) or consisting of all distinct colors (rainbow words). Formally, we\ndefine a building block set to correspond to n cubes. For each face of the cube, we assign\none letter and one of color from a specified palette. In this paper, we focus on the case where\nn = 6 and suppose our palette consists of 6 colors. In addition, we restrict the coloring so that\neach face of the cube has a unique color.\nWe intend to find a set of cubes that can be used as a spelling game where children aim\nto find as many mono and rainbow English words as they can.\nIn section 2 we reduced the original dataset to only words of up to six letters, represented\nour cube set as a 36-item array, and decided what letters would be repeated after we make\nsure that the 26 letters of the alphabet are included. In subsection 3.1 we look at random\nsearch, method that after 1.3 million permutations its best solution could spell 1881 words.\nConsequently in subsection 3.2 we implemented simulated annealing as a way to make our\nsearch more structured, which allowed us to land to a solution of 2352 words after 250 thousand\niterations. In subsection 3.3 we constrained the search a bit more by structuring a tree and"}, {"title": "2. Set up.", "content": "Given that we are focusing on making a puzzle toy to spell English words\nwith children as target users. Our data set consists of words ranked by Age-of-Acquisition\nfrom zero to 14 years old. The original list of words had approximately 44 thousand words\n[3], between duplicates and acronyms. For example, \u2018stock' appeared 14 times to account\nfor the different meanings. These were removed. However, since we have only six cubes, we\nrestricted the dataset to words that are 6 characters long or less, leaving us with a total of\n9624 words."}, {"title": "2.2. One-dimensional representation of a cube set.", "content": "Since the orientation of the faces on\nthe cube is unimportant, we represent a single cube as a six-element array of letters where the\nindices act as labels for the six colors (see Figure 3). The six block set is then represented as a\n36-item array where the color label is computed using modular arithmetic. For example, the\ncolor at index 11 := 11 mod 6 (see Figure 4). Additionally, the cube label can be computed\nby using the floor function. For example, the letter at index 32 belongs to the cube [] (see\nFigure 4)"}, {"title": "2.3. Letter repetition.", "content": "As we want to make sure to include each letter of the English\nalphabet at least once, we have ten vacant spots in our cube-set. This remainder could\nbe distributed by assigning more repetitions to the most frequent letter. That is why we\ncalculated the each letter frequency relative to our data set. Table 1 shows the letters sorted by\nfrequency, we decided to round the frequency to the thousandth place so that every frequency\nis non-zero. We use this value to calculate the percentage of the remainder (rounded) that\nshould be allocated to a given letter, starting from the most frequent until the corresponding\nportion of a letter is equal to zero, then this process is repeated using an updated number of\nreminder spots, until all the spots had been taken, or the corresponding portion for the most\nfrequent letter is zero, in which case we simply assign whatever number of spots are left, to\nit. In other words, every letter that has a relative frequency \u2265 0.05 gets assigned an extra\nrepetition. Then the new number of vacant spots is 1, and we simply allocate it to letter 'e',\nthus completing the 36 letters in our six-cube set. Table 2 shows the number of repetitions for\nour set (the highlighted column), we also include the letter repetitions that other cube sets\nmay have if we use the same strategy for distributing the vacant spots. Using this information\nwe created an array of 36-letters (see Table 3) that yields a word count of 960, and we label as\n'The base permutation', since we will use it as a starting point whenever we create a random"}, {"title": "2.4. Search space.", "content": "Given that our cube-set includes one letter repeated three times, and\neight other repeated twice, the number of possible permutations is a follows:\n$\\frac{36}{3!(2!)^8} = 2.42183155 * 10^{38}$\nIt takes approximately 1.07 seconds to count how many words a specific configuration\ncan spell, so an exhaustive search will take about 6 * 1030 years. The problem is, therefore,\nintractable."}, {"title": "3. Search algorithms.", "content": ""}, {"title": "3.1. Random Search.", "content": "To establish a baseline for comparison, we implemented a random\nsearch by shuffling the base permutation. We produced a total of 1.3 million random per-\nmutations and kept track of three maximization targets: mono words, rainbow words, and\nthe sum of both. Table 4 shows the results of the search showing the sum of mono and rain-\nbow as 'word count'. In Appendix B you can see the 10 best permutations found for each\nmaximization target."}, {"title": "3.2. Simulated Annealing.", "content": "When thinking about algorithms that leverage the power of\nrandomness, simulated annealing usually comes to mind. This search requires us to treat\nour problem as a graph, where we decide whether or not to move from one state to another\n(randomly chosen) neighbouring state if it's better than the current or based on a probability.\nSince the algorithm attempts to emulate a process of metallurgy, that probability is in function\nof a sense of high energy or 'temperature' that will decrease slowly as time passes. So, the\nalgorithm will initially be more likely to visit a state that performs worse than the current,\nduring early stages in comparison to later stages. This is the way in which simulated annealing\ndeals with the balance between exploration and exploitation [2]. In our case, a neighbour will\nbe the result of swapping two items chosen at random. We ran this algorithm for 250 thousand\niterations, with an initial 'temperature' of 1000 and a cooling rate or 0.999. And the results\nare seen in Table 5."}, {"title": "3.3. N-ary tree search algorithms.", "content": "In the interest of conducting a controlled exploration\nof the search space, we can create a uniform N-ary tree, where, every child node is the result\nof making a small modification in the parent. We found it suitable to make a 180-ary tree, so\nthat the current node (parent) is connected to all unique face-swaps within a cube, for every\ncube (6(2)) along with all unique face-swaps for the letters of the same color, for every color\n(6()). For illustrative purposes, Figure 6 shows an example of the fifteen unique element\nswaps within a cube, and Figure 7 shows an example of the fifteen unique swaps within a\ncolor.\nAlthough this method of creating child permutations allows for cyclic references and redun-dant states-due to some trivial swaps and the fact that some changes can be quickly reverted\nin just two generations\u2014by keeping track of all visited permutations and excluding them as\npossible parents, we end up with a tree structure that constrains the search space enough\nto make greedy-like approaches computationally manageable, while simultaneously ensuring\nthat any permutation in the search space is still theoretically reachable (see Appendix A).\nIt is important to note that because we explore this 180-ary tree using three greedy\nalgorithms, we will only be able to find local maxima near the starting point. So, our success\nin finding a satisfactory result hinges on the selection of a diverse set of roots. We decided that\nwe will test each case with three different roots: 'Base' permutation, a random permutation (as seen in Table 6) generated using a seed = 2000 which we label \u2018Seed2K' and yields a word\ncount of 1550, and, a third permutation, sourced from the results of subsection 3.1(E.g when\nmaximizing mono, the root used will be the best permutation found through random search\nwhen we maximized mono words)."}, {"title": "3.3.1. Constrained greedy search.", "content": "This algorithm will choose the parent of the next level\nby selecting a permutation that ranks equal or better than its precursor, if such a permutation\nexists, otherwise it will stop the search. Because of this constrain the last permutation to\nbecome a parent before the search stops, will be the best solution. We executed this algorithm\nwith a budget of 130 thousand unique permutations per tree, and the results are shown in\nTable 7."}, {"title": "3.3.2. Best-first search.", "content": "An approach that will help us make sure that we visit exactly\nthe number of unique permutations that we want is best first search, that at every given level\nall of the child permutations are put into a priority queue. The parent of the next generation\nwill simply be the permutation at the front of the queue. Because of the priority queue sorts\nits entries from highest to lowest word count, the next parent can come from any generation,\nnot just from the immediate one. Different to subsection 3.3.1, we need to keep track of and\nupdate the best permutation we come across. After running nine trees with a target of at\nleast 150 thousand permutations generated, the results are as show in Table 8."}, {"title": "3.3.3. Greedy search.", "content": "Thinking about how quickly subsection 3.3.1 would come to halt\nafter a couple of generations, we decided to relaxed its constrains allowing the algorithm to\nrun for as many iterations as specified. So, this algorithm will select the parent of the next\ngeneration by selecting the best child permutation in a generation even if such a candidate\nis not equal or better than its precursor. We will need to keep track of and update the best\npermutation we come across. We ran this set up for 130 thousand permutations. The results\nare listed in Table 9."}, {"title": "3.4. Reinforcement Learning.", "content": "Given that several breakthroughs and advances thought to\nbe near impossible to be conquered by machines has been achieved by Reinforcement learning\n(like AlphaGo or the multiple applications on video games and robotics) [5] we thought that\ncreating an RL agent and training it to make permutations out of our base permutation,\nwould most likely give us good results. With the objective of making an RL model whose\nresults can be comparable to our previous methods, we thought of an agent navigating the\ntree search structure described in subsection 3.3. The goal was for the agent to learn a strategy\nfor selecting the parent of the next generation, aiming for the final state to be a high-ranking\npermutation. Since we won't be evaluating every child at each level, we can also describe the\nagent as one that builds a high-ranking permutation by making a series of valid swaps (see\nFigure 6 and Figure 7).\nEnvironment. We used the Gymnasium API[6] by Farama Foundation to set up our Build-ing Blocks environment. Gymnasium offers several pre-made RL environments to learn and\nexperiment with, but also provides an interface to create custom environments.\nObservation space: Describing the observation space whether partial or fully observable.\nIn our case, the observation space represents a procedure made of 36 sub procedures that are\nmutually independent, where each sub procedure can be done in 26 different ways. So, the car-dinality of a set containing all the possible observations would be equal to2636. Following Gym-nasium guidelines, we express this space as multi-discrete space of 36 items where every item\ncan take up an integer between 0 to 25 ( Gymnasium.spaces.MultiDiscrete([26,26,26,. . . 2 26])).\nAction space: The 180 options from which the agent can take one at a given time step\n(The list of 2-item tuples that describe the Pitted swaps see image 6 and image 7). In\nGymnasium it is described as a discrete space that can have a value between 0 and 179 (\nGymnasium.spaces.Discrete(180)).\nReset Function: That is called whenever the agent is initialized, or it reaches a terminal\nstate. In the case of our custom environment, this function sets the current observation to a\nrandom permutation. It also initializes a visited list to keep track of the unique permutations\nthat the agent has generated.\nStep Function: That is called at every time-step, it takes as input a valid action taken\nby the agent. We translate this action into a change in the current state by using the action\nchosen by the agent as an index of the 'swaps list' and making the swap. We also calculate"}, {"title": "3.5. Genetic algorithm.", "content": "genetic algorithms attempt to mimic the way genetic informa-tion is passed down from parents to offspring, through natural selection[1]. And since we\nare attempting to maximize the permutation of a 36-item array, a genetic algorithm seems\npromising as a sequence of letters is analogous to a sequence of genes, therefore in our context\nthe individuals or chromosomes are our 36-item permutations."}, {"title": "3.5.1. Set up.", "content": "We will start from a specified number of random individuals, every gener-ation after that is made up partly of elites from the previous generation, crossed, mutant and\nrandom individuals. We describe how these sub populations are generated hereafter.\nElites. The top number of specified individuals, that are transferred to the next generation\nintact.\nCrossovers. Crossed offspring are created by combining different segments of genetic in-formation also known as loci- from two individuals. Although in biology the length of a\nlocus varies, since we are working with a set of 6 cubes, it is beneficial for us to think of each\ncube as a locus. So, the process of crossover is a six-step process in which the corresponding\nlocus is copied from either parent at random. See Figure 8 for a visual representation. It is\nworth mentioning that this process of crossover will probably break the constraint that we\nstipulated in subsection 2.3:", "once,...": "That is because the random choice of loci will result in the inclusion or exclusion of some\nletters, generating cube sets that are not a permutation of the 'Base' permutation. We chose\nto have two types of crossovers: type 1 are the result of crossing two elites, and type 2 are the\nresult of crossing an elite and a non-elite. The parents of both types are chosen randomly."}, {"title": "3.5.2. Tests.", "content": "In the interest to see if a 'good' starting point will give a population an\nadvantage, we created parallel scenarios for the starting point: The first scenario's original\npopulation is completely random, but for scenario 2 and 3, three individuals are arbitrarily\nchosen and the rest are randomly generated.\nFor scenario 2, we included the 'Base' permutation, the 'Seed2K', and, the best permutation\nfrom subsection 3.1. The idea was for this scenario to have poor-performing permutations\nFor scenario 3 we included the best permutations found by each algorithm of subsection 3.3\nwhen maximizing for the sum of rainbow and mono words.\nWe ran 7 iterations with different parameters as shown in Table 12. The 'Found at :'row of each scenario tells us the generation in which a permutation with this word count was\nfound for the first time (E.g G588 represents the 588th generation). At the beginning we\ntested 1000 generations per iteration, but as we see in iterations 1-3 the best results are found\nby the 323rd iteration (on average), so we decided to reduce the number of generations to 500."}, {"title": "4. Conclusions.", "content": "Our experimentation demonstrated several search and optimization al-gorithms for our six-cube set of 'Building blocks'. Table 14 shows the results found by each\nalgorithm when we adhere to the constrain stated in section 2 of including every letter of\nthe English alphabet in our set, we see that the best word count was found by performing a\ngreedy algorithm in subsection 3.3.3. See Table 14 to compare the best word counts found by\neach algorithm.\nHowever, during subsection 3.5 we also found that if we relax such constrains, we can\nachieve a significant improvement in contrast to the other algorithms, as we saw in Table 12.\nWe should also bare in mind that said constraints make sense only for a set of six cubes, since\nthe need to make sure to include every single letter of the alphabet in our set is less of an\nissue as we make our set larger.\nWith a sufficiently large set of cubes, we could potentially spell every word in our dataset.\nOur goal would then be to determine the smallest set of cubes that achieves this. The findings\nfrom this paper could be used to reduce our word list by eliminating the words found with\nour 6-cube set, enabling us to focus on maximizing coverage of the remaining words"}, {"title": "Appendix A. permutation proof.", "content": "We express the 6-cube building block set as a 36-entry\ntuple.\n(10, 11, 12, 13, ..., 133, 134, 135) where li is a letter from the English alphabet\nS36, the symmetric group on 36 elements describes all possible permutations. We will\ndescribe these elements as acting on the indices rather than the elements themselves. Addi-tionally, we refer to a 'legal' move as going from parent to child within the tree discussed in\nsubsection 3.3.\nWe define a 'legal move' as a two-cycle in S36, (x,y) such that either one of the following\nholds: That i and j belong to the same cube as expressed in (A.1), or that i and jhave the"}]}