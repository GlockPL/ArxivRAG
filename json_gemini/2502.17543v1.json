{"title": "Training a Generally Curious Agent", "authors": ["Fahim Tajwar", "Yiding Jiang", "Abitha Thankaraj", "Sumaita Sadia Rahman", "J Zico Kolter", "Jeff Schneider", "Ruslan Salakhutdinov"], "abstract": "Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present PAPRIKA, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, PAPRIKA teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with PAPRIKA can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.", "sections": [{"title": "1. Introduction", "content": "Large language models (LLMs) are considered to be a promising foundation for autonomous agents \u2013 systems capable of achieving goals independently with minimal human supervision or intervention. A crucial requirement for such systems is the ability to interact effectively with external environments and gather the information necessary to achieve their objectives. This capability can be formalized as solving sequential decision-making problems or performing reinforcement learning (RL) with language models as the agent. However, two challenges hinder the development of these interactive capabilities. First, most naturally occurring data lacks the structure and context needed to model interactions. Second, directly deploying models into the real world to collect interaction data can produce critical errors, which is expensive and potentially risky.\nGiven the impracticality of direct deployment in the wild, a natural alternative is to generate interaction data synthetically. Although generating synthetic data for every possible problem is infeasible, LLMs possess the capacity for in-context learning (ICL), which allows them to adapt to new tasks with minimal demonstrations (Brown et al., 2020). Instead of teaching the model to do all the interaction tasks that we care about, we should instead teach the model in-context reinforcement learning (Laskin et al., 2022) so that the model can solve new problems without being trained on them a priori. It shifts the focus from training the model on particular problems to training it on the general process of solving problems. This paradigm shares similarities with the supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages of training a language model (vs pretraining) where only a relatively small number of examples is needed to produce a model that can generate responses to a wide range of queries that they are not trained on. Our approach is also closely related to the principles of meta reinforcement learning (Beck et al., 2023).\nIn this work, we explore the feasibility of teaching LLMs to perform in-context RL that generalizes across different tasks. We begin by designing a diverse suite of textual decision-making tasks that require active information gathering and decision-making based on interaction outcomes. Using a base model, we generate interaction trajectories and assign scores based on their success in achieving the tasks' objectives. We then apply a sequential variant of Direct Preference Optimization (Rafailov et al., 2024b, DPO) to increase the relative likelihood of successful trajectories. Unlike traditional training where computational costs are dominated by model updates, our approach's primary bottleneck lies in sampling useful interaction data. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential."}, {"title": "2. Preliminary", "content": "Many decision making problems can be formalized as a partially observable Markov decision process (POMDP). We assume each task, t, is a POMDP although we will not draw on the details of the POMDP formalism in this work. As a concrete example, guessing the word \"apple\" would be a task in 20 questions. We will use group (or task group, used interchangeably), G = {T1, T2, . . ., T|G|}, to refer to a high-level grouping of different tasks (e.g., the game 20 questions would be a group). Tasks in a group should share similar strategies but it is not always true that they share the same optimal policy as such constraints may be overly stringent. From the agent's perspective, each task is a black box function that takes in the agent's action at (and possibly the whole interaction history) and outputs an observation Ot. Both at and ot are strings. In a game of 20 questions, at could be \"Is the word an animal?\" and the ot could be \"No.\". In other words, each task employs an environment that the agent interacts with to obtain intermediate observations."}, {"title": "3. Paprika", "content": "The goal of our paper is to develop a scalable method to instill better strategic exploration and sequential decision-making capabilities into LLMs. Prior works (Krishnamurthy et al., 2024) have shown that LLMs can perform poorly on even the simple decision making task of multi-armed bandits. Nie et al. (2024) has since then demonstrated that LLMs can be taught to perform better on bandits after fine-tuning them on synthetic trajectories generated by known algorithms such as UCB. However, this idea is limited in scope for three reasons: (1) we want LLMs to perform strategic exploration and decision making in more complex settings, (2) for most tasks, there is no known algorithm like UCB to generate good synthetic trajectories from, (3) it can be infeasible to collect data for all tasks that we care about.\nWe aim to solve these issues using our method, PAPRIKA. First, we design a suite of complex decision-making tasks that require strategic information gathering to succeed. Next, we show that in the absence of known good algorithms, existing LLMs can generate trajectories with better decision making behaviors through diversity-encouraging sampling. We then finetune the LLMs to prefer higher performing trajectories (in a fashion similar to STaR (Zelikman et al., 2022)) and show that this leads to better decision making abilities at test-time. More importantly, these behaviors often generalize to unseen task groups without additional training. Finally, we propose a general curriculum learning algorithm that can dynamically choose which subset of tasks to train on next to improve data efficiency of such training methods. We next describe each component of PAPRIKA."}, {"title": "3.1. Task Design", "content": "The first component of PAPRIKA is to design a set of task groups that we can evaluate and train LLMs on. The task groups we want should have the following desired properties: (1) they are purely text based, (2) they require multi-turn interaction, where the agents have to both understand prior history in its context and choose actions that maximize the probability of success in the future, (3) they are partially observable, i.e., the observations do not capture the full state or hidden information, so the agents must simultaneously explore to reveal more information and exploit to solve the task efficiently, (4) they are diverse and require different strategies to succeed.\nWith these requirements in mind, we design 10 task groups in our paper. On all of them, we employ an LLM as the agent that is given a task it needs to solve through sequential interaction with the task-specific environment, which provides both observations for intermediate timesteps given the agent's actions and also a task reward at the end of an episode. For tasks requiring general knowledge about the world to generate intermediate observations, we employ another LLM (typically GPT-40-mini) as the environment. For tasks that have rule-based observations and rewards, we find that using hardcoded programs as the verifier/observation generator is more reliable than LLMs, similar to DeepSeek-AI et al. (2025). In order to prevent reward hacking, we also use either another LLM or a hardcoded program as a judge to filter out unsuccessful trajectories that got incorrectly labeled as successful by the task environment (see Appendix C for more on environment hacking). We also find that for task groups requiring complex reasoning, letting the agent think using chain-of-thought (COT) prompting (Wei et al., 2022; Kojima et al., 2022) before generating a final answer improves its performance significantly. We provide a brief description of our task groups here, please refer to Table 1 for their summary and Appendix A for more details.\nFollowing prior work (Abdulhai et al., 2023), we include classic guessing games like twenty questions and guess my city in our list of task groups. They require guessing a secret topic as quickly as possible by asking a sequence of questions and observing the answers. We also employ Wordle and Mastermind, where the agent needs to guess a secret 5-letter word and 4-digit code respectively. The environments for these task groups provide feedback in terms of similarity between the guess and the target word/code, and the agent needs to refine their guesses in future turns to maximize information gathering. We design customer service and murder mystery as dynamic text-based task groups: an LLM plays the role of the task environment, which is provided with the criterion for task success and generates dynamic intermediate observations based on this criterion.\nA desirable capability in LLMs is to code and refine based on interpreter feedback. To simulate this process with a toy case, we design Cellular Automata, where the agent needs to make inferences about the transition rule in 1D elementary cellular automata (Wolfram, 1983; Cook et al., 2004) by observing inputs and outputs. The agent receives"}, {"title": "3.2. Dataset construction", "content": "In order to learn from these task groups, we must first generate data from them. It is crucial that the data we generate are diverse which would allow the model to learn different strategies without the risk of overfitting. We accomplish this by generating a large number of trajectories at a high temperature with Min-p sampling (Nguyen et al., 2024). Min-p sampling works by using an adaptive threshold pscaled Pmax, where Pmax is the highest probability predicted by the model on the next token, to truncate the vocabulary to tokens that have a probability larger than Pscaled and sample from them \u2013 this enables us to generate diverse yet coherent trajectories at a higher temperature.\nFor each task in a set of chosen tasks (e.g., uniformly sampled), we generate nsample trajectories and then construct a preference pair (hw, h\u2081) where hw is the highest scoring trajectory (trajectory that succeeds and does so at the fewest number of turns) and hi is randomly sampled from the lower scoring (failed or takes substantially more turns to succeed) trajectories. We choose hi randomly instead of choosing the worst one to increase the diversity of our dataset. We treat hw and hi as proxies for desirable and undesirable behaviors. A dataset D = {(h^w_i, h^l_i)}^N_{i=1} is a collection of such trajectory pairs."}, {"title": "3.3. Optimization", "content": "Supervised fine-tuning. If we take the winning episodes as the expert behavior, then we can discard the losing episode and maximize the likelihood of winning episodes:\nLSFT(D) = ED[\n  \\sum_{t=0}^{|h_w|}\n   log \\pi_{\\theta}(a_t | h_t)\n]  (1)\nwhere |a| is the number of tokens for the agent response (discarding the environment generation). This is akin to rejection sampling fine-tuning (Gulcehre et al., 2023; Dong et al., 2023; Mukobi et al., 2023) seen in prior work.\nDirect preference optimization. A popular approach for finetuning LLMs is DPO (Rafailov et al., 2024b) where one directly optimizes the Bradley-Terry model (Bradley & Terry, 1952) for preferences. In our setting, each trajectory consists of multiple rounds of interactions so the original DPO objective does not apply. We instead use a multi-turn version of DPO introduced in Rafailov et al. (2024a):\nLDPO (D) = ED[\n  log\\sigma(\n    \\beta(\\sum_{t=0}^{|h_w|}log \\frac{\\pi_{\\theta}(a^w_t | h_t)}{\\pi_{ref}(a^w_t | h_t)})\n - \\sum_{t=0}^{|h_l|}log \\frac{\\pi_{\\theta}(a^l_t | h_t)}{\\pi_{ref}(a^l_t | h_t)})\n  )\n] (2)\nwhere a is the action tokens generated by the model at turn t in the preferred trajectory h\u2122. \u03c0ref is the reference policy, for which we use the initial model. The main difference with standard DPO here is that we only want to calculate the loss on the action tokens the log probability ratios of the environment generated tokens are not included in the loss.\nWe note that we use DPO because it is less memory inten-sive. DPO allows us to decouple the data collection and policy improvement steps and offload them on different machines. However, in principle, one could also employ online RL with more resources. Following prior work that shows the efficacy of online RL compared to offline algorithms (Xu et al., 2024; Tajwar et al., 2024), we expect doing PAPRIKA with online RL would lead to even stronger results.\nCombining objectives. Finally, prior works have noted DPO having the unintended effect of reducing the probabil-ity of preferred trajectories as well, known as unintentional unalignment (Razin et al., 2024), which can affect model performance. The RPO objective (Pang et al., 2024), by combining SFT and DPO loss, has shown promising results in mitigating this issue. Formally, the RPO loss is:\nLRPO(D) = LDPO(D) + \u03b1LSFT(D) (3)\nwhere \u03b1 is a hyper-parameter. Following Pang et al. (2024), we set \u03b1 to be 1.0 for the rest of this paper."}, {"title": "3.4. Scalable Online Curriculum Learning", "content": "The core idea of PAPRIKA is to fine-tune the model on a large number of decision making problems to acquire general decision making ability. It is relatively easy to design a large number of tasks, but it is harder to decide which task to train on. A major obstacle is that different tasks may have a large range of difficulty. Unlike pre-training where the model can generally make progress on any given sample (i.e., decrease next-token prediction loss), an RL agent cannot make meaningful progress without collecting good experience. As such, if a task is too difficult for the current model, the model would not generate trajectories with meaningful learning signals. Since generating a trajectory is expensive, it stands to reason that we want to prioritize the tasks where the model can make meaningful progress, which is a form of curriculum learning (Bengio et al., 2009).\nWithout additional assumptions, the only way to know whether a task would yield good learning signals is to actually perform a rollout in that task, which is expensive. In fact, in this particular scenario, the major cost for training is actually data generation rather than model updates. As such, this naive approach would not save us time or computation. A desideratum for an efficient curriculum is the ability to know whether certain tasks will yield data with learning signals without actually performing the rollout. A natural assumption is that similar tasks would have similar levels of learning signal. These groupings can be obtained through meta data or prior knowledge.\nMeasuring learning potential. We will use h ~ \u03c0\u03bf\u03c4 to denote sampling one episode from the task using the policy \u03c0. The average performance of \u03c0 on \u03c4 is R\u03c0 (\u03c4) = Eh\u223c\u03c0\u03bf\u03c4 [r(h)] and the variance is \u03c3\u03c0(\u03c4) = Eh\u223c\u03c0\u03bf\u03c4 [(r(h) \u2013 R\u03c0 (\u03c4))2]. Based on these, we can define:\n\u03bd\u03c0(\u03c4) = \\frac{\\sqrt{\\sigma_{\\pi(\u03c4)}}}{R_{\\pi(\u03c4)}} (4)\nThis quantity is known as the coefficient of variation in statistics, a dimensionless quantity that measures the population's variability relative to the mean.\nWe argue that this quantity is an ideal measure of the learning potential for a single task. DPO requires a pair of positive and negative samples 3. Intuitively, the pair should be sufficiently different so the model can tell the two apart - for example, prior work (Pal et al., 2024) has shown that DPO suffers when the edit distance between preferred and"}, {"title": "4. Empirical Results", "content": "In this section, we will present the results of our empirical study to answer the following research questions: (1) Can training on self-generated trajectories from a diverse range of task groups equip LLMs with sequential decision making capabilities that generalize to unseen task groups without the need to train on them? (2) Can curriculum learning improve the data efficiency of our training mechanism? (3) Finally, does PAPRIKA hurt the model's regular abilities, and can fine-tuning on existing multiturn interaction data that do not have any sequential decision making structure also improve these capabilities? We first describe our experimental setup, and then report our empirical observations.\nExperimental Setup We use a Llama-3.1-8B-Instruct model (MetaAI et al., 2024) for all our experiments. For data generation, we use Min-p sampling (Nguyen et al., 2024) with temperature 1.5 and Min-p parameter 0.3, as we saw that this setting consistently generated diverse training data that resulted in higher test-time accuracy. For each task in the training split, we generate nsample = 20 trajectories to construct our training dataset (except for mastermind, where we sample nsample = 100 trajectories per task). After filtering, this results in 17,181 training trajectories for supervised fine-tuning and 5,260 trajectory pairs for RPO over all task groups. Unless explicitly mentioned otherwise, we use learning rate of 10-6 for supervised fine-tuning and 2 \u00d7 10-7 for RPO. We use batch size 32 for all training runs. We generally always run supervised fine-tuning first and then further fine-tune with the RPO objective to obtain the final model unless explicitly mentioned otherwise. We use an AdamW optimizer (Loshchilov & Hutter, 2019) with a cosine annealing learning rate scheduler and warmup ratio 0.04 (Loshchilov & Hutter, 2017) to train all our models.\nDuring evaluation, in order to account for variability of both the environment and the agent, we generate 4 trajectories for each task in the test set and report the average success rate (we also report pass@4 success rates in Appendix F). We use Min-p sampling with parameter 0.3 for evaluation. Default temperature for evaluation is set to 0.7. Finally, for task groups with hardcoded feedback mechanism, we consider a failure to follow formatting instructions to be a failure in the task.\nPAPRIKA improves LLM decision making abilties We motivate this question by looking into the toy task group of bandit best arm selection more closely. This task requires strategic use of the fixed sampling budget (20) to quickly discard arms that are unlikely to have a high mean reward, and use most of the sampling budget on the few top arms to decide the best arm among them. Previous work (Nie et al., 2024) has shown that training on synthetic trajectories from optimal bandit algorithms can significantly improve LLMs' performance on them. Contrary to that, we show that LLMs can learn generalizable strategies from other decision making task groups that then transfer to this bandit group, without needing an optimal algorithm to generate synthetic trajectories. Figure 3 (left) shows that PAPRIKA improve average success rate from 42.25% to 62.25% on the bandit task after only seeing trajectories from other task groups.\nMotivated by this, we next study whether PAPRIKA can"}, {"title": "5. Related Works", "content": "LLM alignment. Alignment or post-training is a crucial step for creating helpful LLM assistant. Existing post-training pipeline typically involves instruction tuning and then reinforcement learning from human feedback (Christiano et al., 2017, RLHF) where one either performs RL against a reward model trained on human preference data via Proximal Policy Optimization (Schulman et al., 2017, PPO) or sidesteps reward model training via Direct Preference Optimization (Rafailov et al., 2024b, DPO). Most methods focus on single-turn interactions where the model generates a single response to a query. We focus on the multi-turn setting where the agent has to interact with an environment iteratively, similar to Rafailov et al. (2024a). There are a few existing environments and datasets that focus on multi-turn interactions (Abdulhai et al., 2023; Sun et al., 2023; Kwan et al., 2024; Wang et al., 2024b). LMRL-Gym (Abdulhai et al., 2023) implements a suite of textual RL environment, some of which we build on. Most of these environments focus on interactions with humans. Rather than any particular tasks, We focus on evaluating the general ability to solve a sequential decision making problem where the agent needs to explore (e.g., gather necessary information for a task) and exploit (e.g., solving a task in an efficient manner)."}, {"title": "6. Discussion", "content": "In this paper, we presented a scalable fine-tuning method to improve multi-turn decision making abilities of LLMs. Moreover, we showed that the strategies learned by the LLM from our method can generalize zero-shot to unseen tasks. There are a few limitations to our approach. Firstly, we use rejection sampling on self-generated data to teach the model better behaviors. In order to get good performance, the starting model need to exhibit good behavior within a reasonable generation budget, so PAPRIKA would perform worse in the absence of a good base model. Next, we use offline preference tuning algorithms to train our models due to lack of computational resources. A possible future direction for our work is to run online RL on diverse tasks instead: due to its recent success in other domains (DeepSeek-AI et al., 2025), we expect it will give a larger improvement in LLMs' in-context RL capabilities. Our environments, despite being designed with the help of GPT-40-mini, required a lot of human effort for implementation. A new axis of improvement can be training an LLM to scalably generate suitable tasks that can then be used to train the agent. Finally, the performance of our curriculum learning algorithm heavily depends on the quality of the task group clusters which is not ideal, and one can study possible improvements of this algorithm. We leave these directions for future work."}]}