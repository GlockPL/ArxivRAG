{"title": "SPRIG: Stackelberg Perception-Reinforcement Learning with Internal Game Dynamics", "authors": ["Fernando Martinez-Lopez", "Juntao Chen", "Yingdong Lu"], "abstract": "Deep reinforcement learning agents often face challenges to effectively coordinate perception and decision-making components, particularly in environments with high-dimensional sensory inputs where feature relevance varies. This work introduces SPRIG (Stackelberg Perception-Reinforcement learning with Internal Game dynamics), a framework that models the internal perception-policy interaction within a single agent as a cooperative Stackelberg game. In SPRIG, the perception module acts as a leader, strategically processing raw sensory states, while the policy module follows, making decisions based on extracted features. SPRIG provides theoretical guarantees through a modified Bellman operator while preserving the benefits of modern policy optimization. Experimental results on the Atari BeamRider environment demonstrate SPRIG's effectiveness, achieving around 30% higher returns than standard PPO through its game-theoretical balance of feature extraction and decision-making.", "sections": [{"title": "Introduction", "content": "Deep Reinforcement Learning (RL) has successfully solved complex tasks across various domains, from game playing to robotic control (Mnih et al. 2015; Berner et al. 2019; Ibarz et al. 2021). However, a fundamental challenge persists: the effective coordination between perception and decision-making components, particularly in environments with high-dimensional sensory inputs where the relevance of features varies across tasks or time (Mao et al. 2024).\nWhile traditional approaches treat perception and decision-making as a unified process, this integration overlooks fundamental insights from cognitive science, particularly the two-stream hypothesis of visual processing (Goodale and Milner 1992). In biological systems, visual information flows through distinct pathways: a \u201cwhat\" stream for object recognition and a \"how\" stream for action guidance. This natural division suggests an inherent hierarchy where perceptual processing precedes and informs action selection as separate subsystems. Despite this biological inspiration, current RL approaches lack the fundamentals of this natural cooperative design.\nWe address these challenges by introducing SPRIG (Stackelberg Perception-Reinforcement learning with In-"}, {"title": "Related Work", "content": "Integrating perception and decision-making in RL has become an interesting subdomain, especially in environments with high-dimensional sensory inputs. The Perception and Decision-making Interleaving Transformer (PDIT; (Mao et al. 2024)) uses separate transformers for perception and decision-making, leading to enhanced performance in complex tasks. Incorporating game-theoretic principles, (Zheng et al. 2022) proposed the Stackelberg Actor-Critic framework, modeling the actor-critic interaction as a Stackelberg game to improve learning stability. Extending this approach, (Huang et al. 2022) addressed robustness in uncertain environments by formulating robust RL as a Stackelberg game, demonstrating the adaptability of leader-follower structures in RL. Attention mechanisms have also been explored for adaptive feature extraction in RL. For instance, (Manchin, Abbasnejad, and Van Den Hengel 2019) introduced a self-supervised attention model that significantly improved performance in the Arcade Learning Environment, highlighting the potential of attention mechanisms in RL.\nOur work advances these approaches by introducing a framework with theoretical guarantees through a modified Bellman operator that explicitly accounts for perception-policy interaction, while maintaining the advantages of modern policy optimization. Our cooperative game formulation creates a natural balance between feature extraction and decision-making, complementing previous approaches by adding provable convergence properties for the entire system and demonstrating empirical improvements."}, {"title": "Background and Preliminaries", "content": "A Markov Decision Process (MDP) provides the fundamental model for sequential decision-making under uncertainty (Sutton and Barto 2018). Formally, an MDP is defined as a tuple M = (S, A, P, R, \u03b3), where S represents the state space, A the action space, P : S\u00d7A\u00d7S \u2192 [0,1] the transition probability function, R: S\u00d7A \u2192 R the reward function, and y\u2208 [0, 1) the discount factor.\nHere an agent interacts with the environment by selecting actions according to a policy \u03c0 : S \u2192 \u2206(A), where \u2206(A) denotes the probability simplex over actions. The objective is to find an optimal policy \u03c0* that maximizes the expected discounted return:"}, {"title": "", "content": "V\" (s) = \u0395\u03c0 [\u03a3\u03b3R(st, at) | 80 = s] \n                                                                                                                                           t=0"}, {"title": "", "content": "The optimal policy \u03c0* satisfies the Bellman optimality equation:\nV*(s) = max [R(s, a) + Es'~P(:\\s,a) [V*(s')]]."}, {"title": "Stackelberg and Cooperative Games", "content": "Stackelberg games model sequential decision-making scenarios through a hierarchical structure. Let G = (\u039d, \u0398, \u03a6, UL, UF) be a two-player game where N = {L, F} denotes the leader and follower, with strategy spaces \u0398 and \u03a6 respectively. The utility functions uL : \u0398 \u00d7 \u03a6 \u2192 R and uF : \u0398\u00d7 \u03a6 \u2192 R define the payoffs for each player, though their usage differs due to the sequential nature of the game.\nIn this interplay, the leader commits to a strategy \u03b8\u0395\u0398, after which the follower observes this commitment and responds with \u2208 \u03a6. This creates a subgame perfect equilibrium where the follower's best response function is:\nBRF(0) = { \u2208 \u03a6 : UF(0, \u03c6) > uF(0, $') for all \u03c6' \u2208 \u03a6}.\nThe leader, anticipating this response, solves:\n0* = arg max UL(0, BRF(0)).\n \u03b8\u0395\u0398"}, {"title": "", "content": "While Stackelberg games capture hierarchical interaction, cooperative game theory provides tools for analyzing scenarios where players coordinate for mutual benefit. A cooperative game is defined by (N, v), where N is the player set and v : 2N \u2192 R is the characteristic function assigning values to coalitions.\nIn our two-player setting, the cooperative value emerges through a weighted combination of individual utilities:\nv({L, F}) = \u03b1\u03c5\u2081 (\u03b8, \u03c6) + (1 \u2212 a)UF(\u03b8, \u03c6),"}, {"title": "", "content": "where a \u2208 [0,1] represents the cooperation weight. The solution concept focuses on finding allocations that maximize this joint value while ensuring individual rationality:\nv({i}) < uz for i \u2208 {L, F}."}, {"title": "Perception-Policy Learning: Motivation and Need", "content": "Current approaches to perception-policy learning typically fall into two categories. The first approach treats perception and policy as a single end-to-end system, while the second attempts to optimize these components independently. Nevertheless, recent work has shown that perception and decision models separately can lead to reduce robustness since mismatched state extraction and control decision-making become asynchronous (Zhu et al. 2022). Standard RL models often treat these processes as a unified pipeline, optimizing perception and policy jointly in an end-to-end fashion. While this approach simplifies implementation, it struggles to generalize in high-dimensional environments where irrelevant features dominate or feature relevance varies over time, as evidenced in complex visual navigation tasks (Zhu et al. 2017). Such limitations derives from the inability to effectively balance the demands of feature extraction with those of action selection.\nInspired by the two-stream hypothesis of visual processing (Goodale and Milner 1992), we argue for a principled separation of perception and policy into distinct, hierarchically organized modules, aligning with approaches in hierarchical RL that decompose complex tasks (Diuk et al. 2013). This biological insight suggests that perception should focus on extracting meaningful, task-relevant features while policy concentrates on optimal action selection based on these features. This separation enables better modularity and adaptability in complex environments, akin to how biological systems achieve robust and efficient decision-making."}, {"title": "Our Approach", "content": "We propose SPRIG (Stackelberg Perception-Reinforcement Learning with Internal Game Dynamics), a cooperative Stackelberg game framework for perception-policy learning in RL. Our approach builds upon PPO, extending it to incorporate game-theoretical dynamics between modules. As shown in Figure. 1, the SPRIG architecture comprises two key components: the perception module, which acts as the leader, and the policy module, which serves as the follower."}, {"title": "Perception-Policy Game Formulation", "content": "In our SPRIG agent, the perception module @ implements a hierarchical spatio-temporal attention mechanism consisting of three convolutional layers combined with self-attention, mapping raw states S to features F. This way, the agent can process raw visual inputs by progressively refining spatial relationships while maintaining temporal consistency across frames. On the other hand, the policy module & consists of a Multi-Layer Perception that takes the feature representation coming from the perception module and outputs action probabilities. The policy module is optimized iteratively using PPO, alternating between policy updates and value function updates.\nThe interaction between these modules is formulated as a cooperative Stackelberg game where:\n0* = arg max UL (0\u03b8, \u03c6*(\u03b8)),\n \u03b8\u0395\u0398\n$*(0) = arg max \u0395\u03c0 [R(s, a)].\n \u03a6\u0395\u03a6"}, {"title": "", "content": "The leader's utility function UL balances both the policy's performance and the perception computational efficiency:\nUL (0,4) = coop\u0395\u03c0\u2084 [R(s, a)] \u2013 (1 \u2013 Acoop)Co(s).\nwhere coop is the cooperation weight and Co(s) represents the perception cost. The cost function penalizes excessive attention across all layers:\nCo(s) = \u03bb\u03b5 \u03a3Es~D[||Ak(s)||1],"}, {"title": "", "content": "where Ak(s) represents the attention weights at layer k, A is the cost weight, D is the distribution of states encountered during training, and K is the total number of layers."}, {"title": "Stackelberg Equilibrium Computation", "content": "The Stackelberg equilibrium is computed through a two-stage optimization process. In the first stage, the perception module optimizes its utility while anticipating the policy module's response (Eq. (8)):\nLo = -UL(0, \u03c6).\nThis optimization uses Generalized Advantage Estimation (GAE; (Schulman et al. 2015)) to compute advantages, which are normalized for training stability. The perception cost directly influences this stage by penalizing excessive attention allocation.\nIn the second stage, the policy module optimizes its objective given the features provided by the perception module:\nL = -\u0395\u03c0\u03bf [R(s,a)] + \u03b2\u0397(\u03c0\u03c6),\nwhere \u0397(\u03c0\u03c6) is the policy entropy and \u1e9e is the entropy coefficient. The policy optimization includes both value function and policy updates, with the perception cost indirectly affecting this stage through the quality of extracted features, as outlined in Algorithm 1.\nThe perception cost influences both optimization stages: directly in the leader's utility computation and indirectly in the follower's optimization through feature quality. This dual influence creates a balanced cooperation between modules, where the perception module must provide useful features while maintaining computational efficiency, and the policy module must effectively utilize these features for decision-making."}, {"title": "Theoretical Formulation and Convergence Properties", "content": "Stackelberg-MDP Formulation We extend the traditional MDP framework to incorporate the perception-policy interaction through a cooperative Stackelberg game. Our augmented MDP is defined as Ms = (S, A, P, R, \u03b3, \u0398, \u03a6, C'), where \u0398 is the perception parameter space, I is the policy parameter space, and C : S \u00d7 O \u2192 [0, 1] is the perception cost function implemented through attention mechanisms."}, {"title": "Bellman Operator and Properties", "content": "For our Stackelberg-MDP, we first define the standard Bellman operator T for MDPS:\n(Tf)(s,a) = R(s,a) +yEs'~P(:\\s,a) [max f (s', a')].\nBuilding upon this, we define our Stackelberg-Bellman operator Ts that incorporates the perception-policy interaction:\n(Tsf)(s,a) = max min [R(s,a)\n \u03b8\u0395\u0398 \u03a6\u0395\u03a6 \u2013 AC(s)\n+ Es'~P(:\\s,a) [f(s', a'; $]],\nwhere Co(s) = \u03a3\u03ba=1 || Ak(s)||1 represents our implemented attention-based perception cost (Equation (9))."}, {"title": "Contraction Properties", "content": "The Stackelberg-Bellman operator Ts maintains the contraction property under the following conditions: 1) bounded rewards: R(s,a) \u2264 Rmax; 2) bounded perception cost: 0 \u2264 Co(s) \u2264 1 (guaranteed by our L\u2081-norm attention cost); and 3) discount factor: \u03b3\u2208 [0,1). For any two value functions f1 and f2:\n||Ts f1 - Ts f2||\u221e\u2264 || f1 - f2||\u221e.\nThe contraction property of Ts ensures the existence of a unique fixed point f* satisfying f* = Ts f*, guarantees convergence of value iteration: ||T f - f* || \u221e \u2264 y\" || f - f* || \u221e, and that the optimal policy derived from f* represents the Stackelberg equilibrium between perception and policy modules."}, {"title": "Numerical Experiments", "content": "We evaluate our SPRIG agent on the BeamRider Atari environment, conducting experiments across five different random seeds over 10 million environment interactions. We considered BeamRider as an interesting challenge for our agent since temporal element and visual focus are important in this game. We utilize identical hyperparameters with both the baseline (PPO) and SPRIG except for the perception module configuration. The detailed perception module architecture specifications are provided in Appendix, Fig. 3. SPRIG achieves superior performance compared to the baseline PPO implementation as presented in Fig. 2, reaching approximately 850 points compared to PPO's 650, showing a clear advantage in the final performance. The learning process exhibits interesting dynamics: SPRIG demonstrates faster initial learning in the first 2 million steps, followed by a period of exploration and adjustment between 4-6 million steps, before stabilizing at a higher performance level. While the learning trajectory shows higher variance during the middle phase (as indicated by the purple-shaded region), this exploration appears beneficial for discovering better policies, ultimately leading to more robust performance. The baseline PPO, in contrast, shows more stable but conservative learning, with a steady but slower improvement curve and lower final performance. These results suggest that our game-theoretical framework effectively balances the exploration-exploitation trade-off while maintaining learning stability."}, {"title": "Conclusions", "content": "In this paper, we presented SPRIG, a novel framework that formalizes perception-policy interaction in reinforcement learning through cooperative Stackelberg games. Our approach provides theoretical guarantees through a modified Bellman operator while demonstrating practical improvements in learning efficiency and stability. The preliminary results suggest that explicitly modeling module interaction through game theory could be a promising direction for improving single-agent reinforcement learning systems."}]}