{"title": "Retrieval Augmented Anomaly Detection (RAAD): Nimble Model Adjustment Without Retraining", "authors": ["Sam Pastoriza", "Iman Yousfi", "Christopher Redino", "Marc Vucovich", "Abdul Rahman", "Sal Aguinaga", "Dhruv Nandakumar"], "abstract": "We propose a novel mechanism for real-time (human-in-the-loop) feedback focused on false positive reduction to enhance anomaly detection models. It was designed for the lightweight deployment of a behavioral network anomaly detection model. This methodology is easily integrable to similar domains that require a premium on throughput while maintaining high precision. In this paper, we introduce Retrieval Augmented Anomaly Detection, a novel method taking inspiration from Retrieval Augmented Generation. Human annotated examples are sent to a vector store, which can modify model outputs on the very next processed batch for model inference. To demonstrate the generalization of this technique, we benchmarked several different model architectures and multiple data modalities, including images, text, and graph-based data.", "sections": [{"title": "I. INTRODUCTION", "content": "Cybersecurity artificial intelligence (AI) models designed for network intrusion threat detection require very high, but nuanced, model precision. False positive (FP) reduction is crucial for the practical implementation of anomalous behavior detection. Compared to false negatives (FN), where threats go undetected, even a small FP rate can render a strong model impractical and negatively impact business operations. Enterprise networks monitor their network traffic using high-speed monitoring systems using protocols such as NetFlow and IPFIX [1]. During the cyber threat detection model inference process, even a sub-percent FP rate can cause alert fatigue for the security analyst, resulting in mismanaged response capacity. Maintaining a high precision for the anomaly detection model requires significant computational resources and training time.\nDuring model development, there is a trade-off between training on only a target network or multiple networks. In a single network, the model is at risk of mistakenly learning a threat actor's behaviors as normal if they are already acting discretely on the network. Models trained on other networks have better generalization abilities, but likely still require further tuning to the particular target network. A reasonable"}, {"title": "II. RELATED WORK", "content": "The reduction of FP rates is a common challenge across machine learning applications and techniques, from Generative AI (GenAI) techniques like RAG to modeling methodologies like deep semi-supervised learning. Outdated information or inaccuracies in LLM training data have led to RAG methodologies [3]. RAG allows LLMs to incorporate additional data sources before making predictions based on user input [4]. However, the fundamental core of RAG has been shown to have promising results with a variety of model types along with LLMs. Pan et al. used RAG to help facilitate cyber investigations with system logs [5]. Their architecture uses an LLM to perform semantic analysis between log samples retrieved from the vector database and the queried log entry. The logs stored in the vector database are the vector embedding of known normal logs. Therefore, when the retrieval score matches the criteria, such as the highest similarity score or the minimum threshold score, the vector database returns the resultant embedding vectors [5]. Although their results with a cyber-focused RAG model are promising, LLMs are not always usable in cybersecurity due to the variety and sensitivity of data types. Other authors, such as Al Jallad et al. propose using larger amounts of data to help train generalizable deep learning model to detect anomalies [6] with lower false positive rates in a vein similar to the anomaly detection model for which RAAD was originally developed, although we specifically follow the architecture of Nandakumar et al. [7].\nDeep semi-supervised learning techniques have evolved over time to improve model performance and reduce labeling costs. Ouali et al. defines the goal of semi-supervised learning as leveraging the unlabeled data to produce a prediction function with trainable parameters [8]. Lee [9] introduced the concept of pseudo-labeling, which involves generating proxy labels to augment the training set. This was further expanded by combining label propagation with pseudo-labeling in Iscen et al. [10], labeled sample constraints in Arazo et al [11], and retraining models with regularization and pseudo-labeling in Sohn et al [12]. Although effective in reducing FP rates, it is not a long term sustainable method for an anomaly detection model in a robust production environment, where speed and efficiency are priority. These methods often require significant computational resources and training time. RAAD does not require constant retraining and provides an alternative that is low-touch in deployment. The architectures reviewed here inform our approach and methodology towards reducing false positives in an anomaly detector."}, {"title": "III. METHODOLOGY", "content": "In this section, we will cover the datasets we used to test our RAAD approach, as well as the architecture we developed and how it is implemented to adjust probability- and loss-based predictive models."}, {"title": "A. Datasets", "content": "The data sets used to test our approach span several modalities, including image data sets such as MNIST (Modified National Institute of Standards and Technology) [13] and E-MNIST (Extended MNIST) [14], text data sets such as the malicious URL (Uniform Resource Locator) dataset [15], and graph-based datasets, such as NetFlow [16] connections used in our zero-day threat detection models [7]. Table I describes these datasets in more detail."}, {"title": "B. RAAD Architecture", "content": "Traditional ML architectures generally include raw input data collection, pre-processing, feature engineering, model inference, post-processing, and review. Our RAAD architecture (Fig. 1), consists of a traditional machine learning (ML) architecture, with the addition of a post-processing step to adjust model outputs. It allows for direct modification of model outputs based on human feedback in real time. As part of the pipeline, the model can be configured to output both a prediction and a representation of the input as a learned embedding. As humans provide feedback on the quality of the model, they mark when models get predictions incorrect, and those incorrect predictions are stored in a permanent location as a learned embedding. Once that embedding is stored, the next batch of model outputs can utilize that knowledge by comparing its own learned embeddings to those stored. Based on the similarity scores generated by the input embeddings, each of the outputs can be adjusted to prevent past mistakes by the model. Depending on the type of model, the model outputs are adjusted approach using RAAD as described in the subsequent sections."}, {"title": "C. Probability Bounded Adjustments", "content": "Many models output probabilities, typically produced by an activation function such as Softmax. To adjust probabilities"}, {"title": "D. Loss Bounded Adjustments", "content": "Some models just output a loss directly, which requires an additional step when making adjustments using RAAD, as these outputs are unbounded, unlike probabilities. A sigmoid curve is modified to account for this infinite upper bound. Specifically, instead of an adjusted similarity score, the output of a sigmoid curve for loss-based adjustments is treated as more of a multiplicative/fractional adjustment factor, where similar embeddings with scores closer to 1 would be adjusted by multiplying a factor closer to 0. In a similar method to Algorithm 1, Algorithm 2 describes how these losses are adjusted so that inputs that are close to false positives have their respective losses dropped below a threshold, thus changing the overall prediction."}, {"title": "IV. EXPERIMENTAL DESIGN", "content": "Due to the natural imbalance of these datasets, and the highly skewed tendency towards negative, or benign events, we utilized Precision, Recall, and the area under the Receiver Operating Characteristics (ROC) curve (AUC) to evaluate our models. The goal when working with each of these datasets was to create and fit several models per dataset, evaluate them, assess the outputs, and add failed predictions to a store of learned embeddings. These annotated failed predictions are typically false positives. Then we rerun the pipeline, adjust the sharpness and similarity threshold values, and re-evaluate the adjusted results."}, {"title": "A. Design of Embeddings", "content": "The RAAD mechanism works better with models that have an embedding that accurately represents the data so that similar data points are represented similarly in the learned embedding space. Ideally, the model RAAD is applied to should follow the cluster assumption [8], if points are in the same cluster, they are likely to be of the same class [17]. For neural networks, this can be as simple as choosing a layer from the neural network that is large enough to store differentiations in the data. A simple example of this would be choosing the bottleneck layer in an autoencoder, as this layer is the smallest layer that contains the most important information used to reconstruct the input. Typically, several different layers of a neural network are tried before determining the most effective layer. In assessing whether an embedding space for a model is sufficiently separated for a successful application of RAAD a good test is to simply leverage the Jaccard index [18] of the embedding space, treating distinct classes as their own sets for comparison.\nA Jaccard index around or below 10%, has been shown to be a good indicator that a RAAD implementation would be of use. This nuanced application of the Jaccard index focuses on the dissimilarity of the embedding space. A low Jaccard index signifies minimal overlap between the sets of data points classified by each model. This reflects distinct decision boundaries, which is desirable when predicting the success of RAAD. By having a clear separation, it enhances the model's ability to capture specific features, making it a strong indicator for success."}, {"title": "V. RESULTS AND DISCUSSION", "content": "RAAD can be broadly applicable across modalities where false positives happen. In addition to fitting RAAD to an anomaly detection dataset, we also show it works with image data and text data datasets."}, {"title": "A. Graph/Anomaly Detection Dataset Results", "content": "RAAD was initially created to improve our zero day threat detection model. While training this model, primarily on network flow traffic data, the training sets are never totally comprehensive, meaning not every \"normal\" network connection is tagged as normal. During the deployment of this model, we found it makes consistent mistakes that are particular to the network it is operating on. This happens even in deployments with a precision over 99% [7]. By using RAAD, the embeddings associated with these false positives are stored and future events with a similar embedding would be tagged as a false positive and not considered an anomaly.\nTo demonstrate how RAAD works with this model, we fit an autoencoder to several network traffic datasets as described in Table I. We undertrained the model slightly so the model did not learn all of the behaviors of the training dataset. Using RAAD, we found a sharpness of 70, a false positive threshold of 0.98, and a max distance of 1 performed best, as seen in Table II. We reduced the number of false positives from an original value of 9200, down to just 15, while not introducing"}, {"title": "B. Image Dataset Results", "content": "To apply RAAD to the MNIST and E-MNIST, we needed to convert the dataset from a multiclass classification problem to a binary classification problem. To do this, we trained a one-versus-all classification model per class and checked to see that RAAD worked. To be consistent, we used an embedding size of 256 all for MNIST, and 512 for E-MNIST. The difference in embedding size can be attributed to the size of the dataset and increase in number of classes. The results of applying RAAD for MNIST and E-MNIST can be found in Table II. In general, a sharpness of 60, threshold of 0.95 and max distance between 3-5 had the best false positive reduction rates observed. In addition to these promising results, we can visualize a successful application of RAAD and show how the space between the clusters of positive and negative classes can be better separated, as seen in Figure 3."}, {"title": "C. Text Dataset Results", "content": "To show the viability of RAAD, several models were fit to this dataset. When fitting a recurrent neural network on this dataset, we found a sharpness of 60, a threshold of 0.95, and a max distance parameter of 1 performed best. The Long Short-Term Memory (LSTM) performed slightly better, but similar parameters were used when applying RAAD. Since the LSTM performed generally better, as seen in Table II, the"}, {"title": "VI. CONCLUSIONS, LIMITATIONS, AND FUTURE WORK", "content": "In this work, we have presented a novel method of providing feedback to models without retraining by comparing identified mistakes made by models to future outputs. In the proposed methodology, we utilize underlying embeddings of inputs that were mistakenly identified as points of comparison when new inputs are fed into the model. By utilizing a vector database, in similar fashion to the RAG methodology, we can store those mistakes and use them during post-processing to augment model outputs and catch similar mistakes. This approach allows for real-time human feedback that can affect model output immediately after a mistake is identified. Additionally, this can be used as a store of well labeled data for retraining the model when needed, as humans are reviewing and labeling these data points. With enough well-labeled data points, retraining will improve the model's performance, but as this solution is meant for real-time feedback, this approach serves as an excellent option before needing to retrain. We believe our results show strong indicators this method has broad applicability across different modalities and types of models and embeddings."}], "equations": ["\u03b8_{all} \\leftarrow \\frac{v_{init} v_{init}'}{\\|v_{init}\\|\\|v_{init}'\\|}, \\forall v_{init}, v_{init}' \\in V_{init}, v \\in V_{fp}", "\u03b8_{closest} \\leftarrow max \u03b8_{all}", "V_{closest} \\leftarrow arg max \u03b8_{all}", "d_{closest} \\leftarrow \\sqrt{\\sum_{i=1}^{n} (V_{fp_i} - V_{closest_i})^2}", "f(0) = LeastSquaresFit(P, D)", "P \\in \\{(0,0), (\u03c4, \u03c4)\\}", "D = a", "\u03b4_{adjusted} = \\begin{cases} \\frac{\u03b8_{closest}}{f(\u03b8_{closest})} & \\text{if } \u03b8_{closest} \u2265 \u03c4, \\\\ 1 & \\text{otherwise.} \\end{cases}", "d_{adjusted} = \\begin{cases} \\frac{1}{d_{closest}} & \\text{if } \u03b4 = 0 \\\\ min(\\frac{\u03b4}{d_{closest}}, 1) & \\text{otherwise.} \\end{cases}", "FP_{confidence\\_score} = \u03b4_{adjusted} * d_{adjusted}", "P_{adjusted} = P_{init} * (1 \u2013 FP_{cs})", "L_{adjusted} = L_{init} * (\\frac{1}{1 + e^{\u03b1 * (\u03c4 - FP_{cs})} + 1})"]}