{"title": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models", "authors": ["Natalie Mackraz", "Nivedha Sivakumar", "Samira Khorshidi", "Krishna Patel", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "abstract": "Large language models (LLMs) are increasingly being adapted to achieve task- specificity for deployment in real-world decision systems. Several previous works have investigated the bias transfer hypothesis (BTH) by studying the effect of the fine-tuning adaptation strategy on model fairness to find that fairness in pre- trained masked language models have limited effect on the fairness of models when adapted using fine-tuning. In this work, we expand the study of BTH to causal models under prompt adaptations, as prompting is an accessible, and compute- efficient way to deploy models in real-world systems. In contrast to previous works, we establish that intrinsic biases in pre-trained Mistral, Falcon and Llama models are strongly correlated (p \u2265 0.94) with biases when the same models are zero- and few-shot prompted, using a pronoun co-reference resolution task. Further, we find that bias transfer remains strongly correlated even when LLMs are specifically prompted to exhibit fair or biased behavior (p \u2265 0.92), and few-shot length and stereotypical composition are varied (p \u2265 0.97). Our findings highlight the importance of ensuring fairness in pre-trained LLMs, especially when they are later used to perform downstream tasks via prompt adaptation.", "sections": [{"title": "1 Introduction", "content": "The bias transfer hypothesis (BTH) [Steed et al., 2022] is a line of work that studies the correlation between the bias of a pre-trained model and its adapted task-specific counterpart. Previous BTH works [Steed et al., 2022, Cao et al., 2022, Delobelle et al., 2022, Goldfarb-Tarrant et al., 2020, Kaneko et al., 2022, Schr\u00f6der et al., 2023] find that intrinsic biases, which are biases embedded in pre-trained models, do not correlate with biases in task-specific fine-tuned models; however, they do not study the bias transfer in prompt-adapted causal models. Moreover, the conclusion that bias does not transfer [Steed et al., 2022, Cao et al., 2022, Delobelle et al., 2022, Goldfarb-Tarrant et al., 2020] has potentially dire implications for fairness in task-specific models should there be situations beyond MLMs where the bias does transfer.\nTask-specificity of models is no longer achieved only through full-parameter fine-tuning. Since the release of GPT-3, prompting has emerged as a promising adaptation alternative to compute-expensive fine-tuning of large language models (LLMs) to perform certain downstream tasks (such as multiple- choice question answering or translation) [Brown et al., 2020, Kojima et al., 2022, Liu et al., 2023]. Some key factors influencing ML practitioners' adoption of prompt-based adaptations include (1) lack of compute budget (specifically storage and memory), (2) lack of task-specific data, and (3) limited access to pre-trained model gradients. The increased prominence of prompting makes it critical to understand the bias impact of these lightweight adaptation strategies."}, {"title": "2 Related work", "content": "Previous works [Steed et al., 2022, Goldfarb-Tarrant et al., 2020, Kaneko et al., 2022, Schr\u00f6der et al., 2023, Caliskan et al., 2017] studied bias transfer in the fairness literature and found intrinsic biases in MLMs, like BERT [Devlin, 2018], to be poorly correlated with extrinsic biases on the pronoun co-reference resolution task. Conversely, Jin et al. [2020] found that intrinsic biases do transfer to downstream tasks, and that intrinsic debiasing can have a positive effect on downstream fairness. Delobelle et al. [2022] explain these conflicting findings by attributing them to incompatibility between metrics used to quantify intrinsic and extrinsic biases. To address this concern, our work introduces a new fairness metric, Selection Bias (SB) (detailed in Section 3.2), enabling unified measurement of intrinsic and extrinsic biases and facilitating comprehensive bias transfer analysis. Furthermore, they posit that factors such as prompt template and seed words can have an effect on bias transfer, and find no significant correlation between intrinsic and extrinsic biases. While all above works consider the impact of intrinsic debiasing on extrinsic fairness, Orgad et al. [2022] study the impact of extrinsic debiasing on intrinsic fairness, and suggest that redesigned intrinsic metrics have the potential to serve as a good indication of downstream biases over the standard WEAT [Caliskan et al., 2017]. The takeaways from some of the above papers are in direct contradiction with that of others, largely due to inconsistencies in experimental setups. All the above works limit their study of bias transfer to MLMs, unlike our work which deals with causal models that notably differ from MLMs in their implementation and use.\nCao et al. [2022] study the correlation between intrinsic and extrinsic biases on both MLMs and causal models and find a lack of bias transfer, citing metric misalignment and evaluation dataset"}, {"title": "3 Approach", "content": "In this work, we investigate fairness in adaptations using the instruction fine-tuned versions of highly performant LLMs, including Mistral [Jiang et al., 2023] (7B params), Falcon (40B params) [Almazrouei et al., 2023] and Llama (8B and 70B params) [Touvron et al., 2023]. We evaluate the model behavior on a co-reference resolution task using the WinoBias dataset [Zhao et al., 2018], a widely used fairness benchmark, by resolving pronouns to one of two gender stereotyped occupations. The WinoBias dataset contains 3,160 balanced sentences; 50% of which contain male pronouns and the other 50% contain female pronouns, 50% of the dataset are ambiguous sentences (Type 1; the pronoun might correctly resolve to either occupation) and the other 50% are unambiguous (Type 2; the pronoun can correctly resolve to only one occupation).\nWe treat statistical disparities in model behavior for demographic categories as biases. We define the intrinsic task as the task that the model was originally trained on; in previous works involving MLMs this is masked token prediction, for causal LLMs this is next token generation. Accordingly, we evaluate the fairness impact of adaptation schemes by comparing biases in intrinsic text generation with those of zero- and few-shot adapted models for multiple choice question (MCQ) prompts. Fig. 1 illustrates the intrinsic, zero- and few-shot prompt formatting using an example sentence. In the zero-shot setup, each WinoBias sample is formatted as shown in the bottom left of Fig. 1 to prompt an LLM to select an answer from a randomized list comprising of reference occupation, non-referent occupation and \"Unknown\". To curate an n-shot prompt, we adapt the setup shown in the right of Fig. 1 to comprise of an equal number of pro-stereotypical non-ambiguous sentences, anti-stereotypical non-ambiguous sentences, and ambiguous sentences with \u201cUnknown\" as the correct answer; this will be followed by a query sentence to probe model biases. We perform three-shot prompting unless specified otherwise. We assess the statistical significance and consistency of bias transfer by running each adaptation experiment across five random inference seeds and with the multiple-choice option ordering randomized in each prompting evaluation."}, {"title": "3.2 Metrics", "content": "In each adaptation setup, we compute the performance using referent prediction accuracy (RPA) \u2013 the mean model accuracy in predicting the referent in non-ambiguous sentences across experimental runs, and the bias using selection bias (SB) \u2013 the absolute difference in rates that an occupation is generated by a model when a male pronoun is present in a sentence vs. a female pronoun. In RPA computation for intrinsic evaluations, a model predicts the referent correctly if the sum of log probabilities of referent tokens is higher than that of the incorrect answers'. In RPA computation for prompting, the referent is predicted correctly if the referent is present in the next word generated by the model. Similar to Steed et al. [2022], bias transfer between two adaptations is computed as the Pearson correlation between occupation selection biases (SB); high Pearson correlation coefficient ($\\rho$) and low p-value indicate high bias correlation."}, {"title": "4 Experiments", "content": "We evaluate bias transfer using the data setup described in Fig. 1 with more details on the few-shot context setup in App. C. Table 1 summarizes the performance (RPA) and bias (SB) for a number of large causal models on intrinsic, zero- and few-shot adaptations. We find the performance (measured with RPA) of models to be higher for sentences containing pronouns that are pro-stereotypical to the referent occupation regardless of adaptation strategy employed, thereby failing the \u201cWinoBias test\u201d [Zhao et al., 2018], which requires a model to perform equally well on pro- and anti-stereotypical sentences. Additionally, RPA is consistently higher for sentences containing male pronouns, demon- strating that there is a bias towards males over females which may be the result of a gender imbalance in the training data set. In Llama models, we observe similar or better RPA performance in models as"}, {"title": "4.2 Bounds of bias transfer under prompting", "content": "In this section, we investigate whether downstream biases of prompted models vary when conditioned to exhibit fair or biased behaviors. We shift the biases in models using pre-prompts that are fairness inducing (or positive) and bias inducing (or negative), and study the resulting changes to task-specific fairness. To further push biases in desired directions, we reconfigure the pronouns in the few-shot context (presented previously in Fig. 1) to have anti-stereotypical answers for fairness-inducing pre-prompts, and stereotypical answers for bias-inducing pre-prompts. We evaluate each model and adaptation strategy on several prompts and display only the most effective positive pre-prompt (yields the best fairness) and negative pre-prompt (yields the worst fairness). To stay consistent with the prior sections, we will focus on Llama 3 8B here (see Table 3), we see similar trends for other models in App. D."}, {"title": "4.3 Effect of Few-shot Composition on Bias Transfer", "content": "In this section, we study the effect of few-shot composition on a model's bias transfer. Using a neutral prompt (\u201cChoose the right option for the question using the context below\u201d), we probe Llama 3 8B"}, {"title": "5 Limitations and Social considerations", "content": "Our bias evaluations are limited to the WinoBias dataset, which only captures binary gender categories; while Dawkins [2021] and Vanmassenhove et al. [2021] introduce gender neutral variants of the WinoBias dataset, we are unclear on how to effectively distinguish between when they / them pronouns in a sentence are gender neutral references vs plural references. We identify the construction of unambiguously gender neutral fairness datasets as an important opportunity to better understand and improve LLM fairness. Given that the WinoBias dataset captures occupations from the US Bureau of Labor Statistics, we evaluate biases only for Western centric occupations. Finally, we evaluate LLM biases using only quantitative methods in this work; while we see fairness gains with the use of positive prompts in Table 3, we do not qualitatively assess if improvements in SB come at the cost of other desirable model behaviors (low toxicity or other harms), and leave this as future work."}, {"title": "6 Conclusion", "content": "In this work, we study the bias transfer hypothesis for causal models under prompt adaptations. We establish that pre-trained and prompt-adapted co-reference resolution biases are highly correlated showing that biases do transfer in prompt-adapted causal LLMs. We also find that biases are highly correlated even if pre-prompted to exhibit specific behaviors using fairness- and bias-inducing prompts, and if few-shot composition is varied in its stereotypical makeup or number of context samples. These findings reinforce the need be mindful of the base fairness of a pre-trained model when it will be used to perform downstream tasks using prompting. Following this work, we plan to scale up our evaluation to other adaptation strategies (such as low-rank and full-parameter fine-tuning)."}, {"title": "A Selection biases split by task ambiguity", "content": "Similar to zero-shot biases in Llama 3 8B in Fig. 2(a), the model largely exhibits more bias for ambiguous sentences, and biases that are largely directionally aligned for ambiguous and non- ambiguous texts when Llama 3 8B is intrinsically or few-shot prompted (Fig. 5). Llama 3 70B, Falcon 40B and Mistral 3 7B are largely more biased on ambiguous texts as illustrated in Figs. 6, 7 and 8, respectively."}, {"title": "B Selection biases split by adaptation", "content": "Similar to Llama 3 8B in Fig. 2(b), Llama 3 70B, Falcon 40B and Mistral 3 7B exhibit biases are directionally identical regardless of adaptation used (with the exception of \u201cbaker\u201d when few- shot prompting Mistral 3 7B). These models exhibit occupational stereotypes that are identical to those defined in WinoBias as illustrated in Fig. 9, mimicking real-world gender representation for occupations."}, {"title": "C Few-shot prompt context", "content": "Fig. 10 contains a sample three-shot context containing hand crafted text samples that are used to produce few-shot results in Table 1. The context is made up of one non-ambiguous sentence with a pronoun that is anti-stereotypical to the referent occupation, one non-ambiguous sentence with a pronoun that is pro-stereotypical to the referent occupation, and one ambiguous sentence with \"Unknown\" as the right answer. To evaluate few-shot fairness, each sentence in WinoBias is appended to the context in Fig. 10, and prompted for the right answer. Option ordering in few-shot prompt is randomized for each WinoBias query to model."}, {"title": "D Bias transfer bounds for various models", "content": "As with Llama 3 8B in Table 3, we can see in Table 5 that Llama 3 70B, Falcon 40B and Mistral 3 7B models largely follow the same trends regardless of choice of pre-prompt to induce fair or biased behaviors. We see that all models perform better on pro-stereotypical sentences than anti-stereotypical sentences, and that all models are fairer on non-ambiguous sentences than ambiguous sentences."}, {"title": "E Fairness and Bias Inducing Prompts", "content": "To evaluate the bounds of bias transfer, we tested each model on various fairness- and bias-inducing prompts listed in Table 6. Tables 3 and 5 present performance and fairness of models on the most effective fairness-inducing prompt (lowest SB) and the most effective bias-inducing prompt (highest SB). These prompts were chosen in an ad-hoc and iterative way for research purposes. We experimented with many more fairness-inducing than bias-inducing prompts because positive prompts were less effective at reducing bias than negative prompts were at increasing bias."}]}