{"title": "GRADIENT FLOW IN PARAMETER SPACE IS EQUIVALENT TO LINEAR\nINTERPOLATION IN OUTPUT SPACE", "authors": ["THOMAS CHEN", "PATRICIA MU\u00d1OZ EWALD"], "abstract": "We prove that the usual gradient flow in parameter space that underlies many training\nalgorithms for neural networks in deep learning can be continuously deformed into an adapted gradient\nflow which yields (constrained) Euclidean gradient flow in output space. Moreover, if the Jacobian of\nthe outputs with respect to the parameters is full rank (for fixed training data), then the time variable\ncan be reparametrized so that the resulting flow is simply linear interpolation, and a global minimum\ncan be achieved.", "sections": [{"title": "1. INTRODUCTION", "content": "At the core of most algorithms currently used for training neural networks is gradient descent, whose\ntheoretical counterpart in the continuum limit is gradient flow. This flow is defined in a space of pa-\nrameters $R^{K}$ with respect to a cost function defined in $R^{QN}$. The cost is non-convex as a function\nof the parameters, and standard gradient flow in $R^{K}$ might not converge to a global minimum. Even\nchanging perspective to $R^{QN}$, where the cost can be chosen to be convex, the resulting associated flow is\ncomplicated.\nIn [Che23], it was discussed in detail how the choice of the Riemannian structure in $R^{K}$ influences\nthe training dynamics. In this context, a modified gradient flow that induces the Euclidean gradient\nflow in output space $R^{QN}$ was introduced and contrasted with the standard Euclidean gradient flow in\nparameter space; it was shown that both flows exhibit the same critical sets. In the work at hand, we\nextend these results by proving that those two flows are in fact homotopy equivalent to one another\n(Theorem 2.3). Moreover, we prove that if the Jacobian matrix exhibits no rank loss, then the Euclidean\nflow in output space is reparametrization equivalent to linear interpolation with respect to a suitable\ntime variable (Proposition 2.4). When there is rank loss, an expression for the deviation from linear\ninterpolation is given (Proposition 2.5).\nIn section 3, we provide some applications: First, we note that instead of modifying the metric in\n$R^{K}$, one could prescribe a convenient path in $R^{QN}$ and then find an associated path in parameter space.\nNext, we show that neural collapse occurs in the output layer for the trivialized dynamics studied in 2.2\n(Corollary 3.2). Finally, we rephrase Theorem 2.3 in terms of the neural tangent kernel.\nWe briefly comment on related work in section 4. A short appendix on generalized inverses for rank\ndeficient matrices is provided as well."}, {"title": "2. MAIN RESULTS", "content": "Consider a family of functions\n$f_\\theta: \\mathbb{R}^M \\rightarrow \\mathbb{R}^Q$\n(2.1)\nparametrized by a vector of parameters $\\theta \\in \\mathbb{R}^K$. Suppose we wish to optimize $f_\\theta$ with respect to a given\ncost function $C: \\mathbb{R}^{K} \\rightarrow \\mathbb{R}$. A natural approach is to pick a starting point $\\theta_0 \\in \\mathbb{R}^K$ and construct a path\n$\\theta(s) : [0,\\infty) \\rightarrow \\mathbb{R}^K$\n(2.2)\nvia the gradient flow\n$\\frac{d\\theta}{ds}(s) = -\\nabla C,\\\\$\\theta(0) = \\theta_0.$\n(2.3)\nWe will refer to (2.3) as the standard gradient flow.\nWhen $f_\\theta$ is a neural network, the optimization problem has the following structure: Consider a set\n$\\mathcal{X}_0 \\subset \\mathbb{R}^M$ (the training data), such that $|\\mathcal{X}_0| = N$. We can form a data matrix $X_0 \\in \\mathbb{R}^{M\\times N}$ by making\nthe vectors in $\\mathcal{X}_0$ into the columns of $X_0$. Then we can consider a function\n$f(\\theta, X_0) : \\mathbb{R}^K \\times \\mathbb{R}^{M\\times N} \\rightarrow \\mathbb{R}^{Q\\times N}$,\n(2.4)\ndefined by its action on each column of $X_0$: for $i = 1,..., N$,\n$f(\\theta, X_0)_i = f_\\theta((X_0)_i) \\in \\mathbb{R}^Q$.\n(2.5)\nIn the context of supervised learning, each data point $x \\in \\mathcal{X}_0$ is associated to a desired output $y(x) \\in \\mathbb{R}^Q$,\nand so from a data matrix $X_0$ we can form the corresponding matrix of labels, $Y \\in \\mathbb{R}^{Q\\times N}$,\n$Y_i = y((X_0)_i)$.\n(2.6)\nA common choice for cost function is the squared loss,\n$C(\\theta, X_0, Y) = \\frac{1}{N} \\|f(\\theta, X_0) - Y \\|_F^2,$\n(2.7)\nwhere $\\|\\cdot\\|_F$ is the Frobenius or Hilbert-Schmidt norm.\nFor fixed training data $X_0 = [x_1 \\cdots x_j \\cdots x_N]$, we can also consider the function\n$\\chi: \\mathbb{R}^K \\rightarrow \\mathbb{R}^{QN}$\n(2.8)\nfrom parameter space to output space defined by\n$\\chi(\\theta) := (f_\\theta(x_1)^T,..., f_\\theta(x_N)^T)^T$.\n(2.9)\nWe define the correspondent vector of labels $y \\in \\mathbb{R}^{QN}$,\n$y := (y(x_1)^T,..., y(x_N)^T)^T,$\n(2.10)\nIn this case, the cost (2.7) becomes\n$C(\\chi(\\theta)) = \\frac{1}{N} \\|\\chi(\\theta) - y\\|^2$.\n(2.11)\nThe Jacobian $D[\\theta] \\in \\mathbb{R}^{QN\\times K}$ is\n$(D[\\theta])_{jk} := \\frac{\\partial \\chi_j}{\\partial \\theta_k} |_{\\theta}.$j$\n(2.12)\nand we assume from now on that the partial derivatives in (2.12) exist and are Lipschitz continuous for\nall $\\theta \\in \\mathbb{R}^K$. We will often abbreviate $D = D[\\theta]$.\nFor $\\theta(s) \\in \\mathbb{R}^{K}$, we can consider the associated path\n$\\chi(s) : [0,\\infty) \\rightarrow \\mathbb{R}^{QN}$\n(2.13)\ngiven by\n$\\chi(s) := \\chi(\\theta(s))$\n(2.14)"}, {"title": null, "content": "from (2.9). The associated Jacobian is now also time dependent, and we will write $D(s) := D[\\theta(s)]$, or\nsimply $D$ for convenience. The chain rule gives\n$\\frac{d\\chi}{ds}(s) = D(s) \\frac{d\\theta}{ds}(s)$\n(2.15)\nand\n$\\nabla_{\\theta} C = D^T \\nabla_{\\chi} C.$\n(2.16)\nRemark 2.1. It is clear that\n$\\{\\theta \\in \\mathbb{R}^{K} : \\nabla_{\\chi}(\\theta) C = 0\\} \\subset \\{\\theta : \\nabla_{\\theta} C = 0\\},$\n(2.17)\nhowever, the two sets are not necessarily identical. It is known that if rank $D[\\theta] = QN = \\min\\{K, QN\\}$,\nthen $\\nabla_{\\chi} C = 0$ if, and only if, $\\nabla_{\\theta} C = 0$. Moreover, if $C$ is the squared loss, then $\\nabla_{\\chi} C = 0$ corresponds to\na global minimum with zero loss. See e.g. [Che23, CE23, KMTM24] for more details.\n2.1. Adapted gradient flow. By a simple computation using (2.15) and (2.16), if $\\theta(s)$ satisfies the\nstandard gradient flow (2.3), then\n$\\partial_s \\chi = -DD^T\\nabla_{\\chi} C.$\n(2.18)\nWe will refer to (2.18) as standard gradient flow in output space. Suppose instead we wished to modify\n$\\theta(s)$ so that the associated path $\\chi(s)$ satisfies the Euclidean gradient flow. We recall here results in\nprevious work by one of the authors, in the overparametrized case:\nLemma 2.2 ([Che23]). Let $K > QN$, and let $\\chi(s) = \\chi(\\theta(s))$ be defined as in (2.9). When $D$ is full rank,\nsetting\n$d_s \\theta(s) = -(D^T D)^+ \\nabla_{\\theta} C$\n(2.19)\nyields\n$d_s \\chi(s) = -\\nabla_{\\chi} C$.\n(2.20)\nIf we allow for rank$(D) < QN$, then letting\n$\\frac{d}{ds} \\theta(s) = -D^T \\psi,$\n(2.21)\nfor $\\psi$ satisfying\n$DD^T \\psi = \\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C,$\n(2.22)\nresults in\n$\\frac{d}{ds}\\chi(s) = -\\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C.$\n(2.23)\nWe will now condense these two separate cases into the same. First, note that\n$\\psi := D(D^T D)^+ (DD^T)^+ (D^T)^+ \\nabla_{\\theta} C$\n(2.24)\nsatisfies (2.22), as the following computation shows:\n$DD^T \\psi = DD^T D(D^T D)^+ (DD^T)^+ (D^T)^+ \\nabla_{\\theta} C$\n$DD^T (DD^T)^+ (D^T)^+ D^T\\nabla_{\\chi} C$\n$DD^T (D^T)^+ (D^T)^+ D^T \\nabla_{\\chi} C$\n$DD^T (DD^T)^+ \\nabla_{\\chi} C$\n$= \\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C.$\n(2.25)"}, {"title": null, "content": "Moreover, we can rewrite\n$-(D^T D)^+ \\nabla_{\\theta} C = -(D^T D)^+ (D^T D)(D^T D)^+ \\nabla_{\\theta} C$\n$= -(D^T D)(D^T D)^+ (D^T D)^+ \\nabla_{\\theta} C$\n$= -D^T [D(D^T D)^+ (DD^T)^+ (D^T D)^+ D^T ] \\nabla_{\\theta} C$\n$= -D^T [D(D^T D)^+ (DD^T)^+ (D^T D)^+ D^T ] \\nabla_{\\chi} C,$\n$= -D^T \\psi.$\n(2.26)\nTherefore in general, for $K > QN$ and rank$(D) \\le QN$,\n$\\frac{d\\theta}{ds} = -(D^T D)^+ \\nabla_{\\theta} C$\n(2.27)\nimplies\n$\\frac{d\\chi}{ds} = -\\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C.$\n(2.28)\nWe will refer to (2.27) as adapted gradient flow in $\\mathbb{R}^K$ and (2.28) as constrained Euclidean gradient flow\nin $\\mathbb{R}^{QN}$\nIt was further shown in [Che23, Theorem 4.2] that the standard gradient flow (2.3) and the adapted\ngradient flow (2.27) have the same equilibrium points. We extend this to a family of interpolating gradient\nfields.\nTheorem 2.3. Let $C : \\mathbb{R}^{K} \\rightarrow \\mathbb{R}$ depend on $\\theta \\in \\mathbb{R}^{K}$ only through $\\chi(\\theta)$, for $\\chi : \\mathbb{R}^K \\rightarrow \\mathbb{R}^{QN}$ defined as\nin (2.9), and let $D[\\theta] \\in \\mathbb{R}^{QN\\times K}$ be the Jacobian defined in (2.12). There is a one-parameter family of\nvector fields in $\\mathbb{R}^{K}$ interpolating between\n$\\nabla_{\\chi} C$\n(2.29)\nand\n$-(D^T D)^+ \\nabla_{\\theta} C$;\n(2.30)\nthis interpolation preserves singularities.\nMoreover, this induces an interpolation between the vector fields for the standard gradient flow in\noutput space (2.18) and the constrained Euclidean gradient flow (2.28) which preserves the equilibrium\npoints of these flows.\nProof. Define the family of vector fields\n$V_{\\theta, \\alpha} := -A_{\\theta, \\alpha} \\nabla_{\\theta} C, \\quad \\alpha \\in [0,1],$\n(2.31)\nfor\n$A_{\\theta, \\alpha} := (\\alpha (D^T D)^+ + (1 - \\alpha)1_{K\\times K}) .$\n(2.32)\nThen\n$V_{\\theta, 0} = -\\nabla_{\\theta} C$\n(2.33)\nand\n$V_{\\theta, 1} = -(D^T D)^+ \\nabla_{\\theta} C$.\n(2.34)\nFor $\\alpha \\in [0,1)$, note that $A_{\\theta, \\alpha} > 0$ for all $\\theta$. For $\\alpha = 1$, writing $\\text{Proj}_{\\text{ran}(D^T)} = (D)^+ D$, we have\n$\\nabla_{\\theta} C = (D^+ D) \\nabla_{\\chi} C,$\n(2.35)\nsince (2.16) shows $\\nabla_{\\theta} C \\in \\text{ran}(D^T)$, and also\n$\\text{ker}(D^T D)^+ = \\text{ker}(D^T D)^T = \\text{ker}(D^T D) = \\text{ker}((D)^+ D),$\n(2.36)"}, {"title": null, "content": "where we used (A.11) and Lemma A.2. Thus, $V_{\\theta, \\alpha} = 0$ if, and only if, $\\nabla_{\\theta} C = 0$, and the vector fields $V_{\\theta,\\alpha}$\nare singular at the same points $\\theta \\in \\mathbb{R}^{K}$, for all $\\alpha \\in [0, 1]$.\nSuppose now that\n$\\frac{d}{ds} \\theta(s) = V_{\\theta(s),\\alpha},$\n(2.37)\nso that for $\\chi(s) = \\chi(\\theta(s))$,\n$\\frac{d}{ds}\\chi(s) = D[\\theta(s)]V_{\\theta(s),\\alpha}.$\n(2.38)\nWe wish to write $\\frac{d\\chi}{ds}$ in terms of $\\nabla_{\\chi} C$, so we compute\n$DV_{\\theta,\\alpha} = -D (\\alpha (D^T D)^+ + (1 - \\alpha)1_{K\\times K}) \\nabla_{\\theta} C$\n$= (\\alpha D(D^T D)^+ + (1 - \\alpha)D) D^T \\nabla_{\\chi} C$\n$= (\\alpha D(D^T D)^+ D^T + (1 - \\alpha)DD^T) \\nabla_{\\chi} C$\n(2.39)\n$\\text{Proj}_{\\text{ran}(DD^T)} + (1 - \\alpha)DD^T) \\nabla_{\\chi} C$\n$= (\\alpha 1_{QN\\times QN} + (1 - \\alpha)DD^T) \\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C,$\nwhere (*) follows from (A.7), (A.8) and (A.12). Therefore,\n$\\frac{d\\chi}{ds}(s) = V_{\\chi(s),\\alpha}$\n(2.40)\nfor vector fields in $\\mathbb{R}^{QN}$ defined along the path $\\chi(s)$,\n$V_{\\chi(s),\\alpha} := (\\alpha 1_{QN\\times QN} + (1 - \\alpha)D(s)D(s)^T) \\text{Proj}_{\\text{ran}(D(s)D(s)^T)} \\nabla_{\\chi} C$.\n(2.41)\nObserve that\n$V_{\\chi(s),0} = -\\nabla_{\\chi} C$\n(2.42)\nand\n$V_{\\chi(s),1} = -\\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C,$\n(2.43)\nso that in fact $V_{\\chi(s),\\alpha}, \\alpha \\in [0,1]$, interpolates between the standard dynamics given by (2.3) at $\\alpha = 0$,\nand constrained Euclidean gradient flow (2.28) for $\\chi(s)$ at $\\alpha = 1$. Moreover, for $\\alpha \\in (0,1]$, $(\\alpha 1_{QN\\times QN} +$\n$(1 - \\alpha)DD^T)$ is positive-definite, and for $\\alpha = 0$, $DD^T$ is injective in the range of $\\text{Proj}_{\\text{ran}(DD^T)}$, so\n$(\\alpha 1 + (1 - \\alpha)DD^T)\\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C = 0$ if, and only if, $\\text{Proj}_{\\text{ran}(DD^T)} \\nabla_{\\chi} C = 0$.\n(2.44)\nThus, the equilibrium points of (2.40) are the same for all $\\alpha \\in [0, 1]$.\nWe add the following remarks highlighting the geometric interpretation of the above results. If $DD^T$\nhas full rank $QN$, then $V_{\\chi(s),\\alpha}$ is the gradient vector field with respect to the Riemannian metric on\n$\\mathbb{T}^{R^{QN}}$ with tensor\n$(\\alpha 1_{QN\\times QN} + (1 - \\alpha)DD^T)^{-1} .$\n(2.45)\nThis means that $V_{\\chi(s), \\alpha}$ can equivalently be considered as the gradient fields obtained from the family of\nRiemannian structures interpolating between the Euclidean structure on $\\mathbb{T}^{R^{QN}}$, and the metric structure\ninduced by the Euclidean structure on parameter space.\nIf $DD^T$ is rank-deficient, rank$(DD^T) < QN$, we let $V \\subset T\\mathbb{R}^{QN}$ denote the vector subbundle whose\nfibers are given by the range of $DD^T$. In this situation, $V_{\\chi(s), \\alpha}$ is the gradient with respect to the bundle\nmetric $h_{\\alpha}$ on $V$ with tensor\n$[h_{\\alpha}] = (\\alpha 1_{QN\\times QN} + (1 - (1-\\alpha)DD^T)^{-1} |_{\\text{ran}(DD^T[\\theta])}$\n(2.46)"}, {"title": "2.2. Reparametrization", "content": "Assuming rank D = QN < K, the adapted gradient flow in output space\nresulting from (2.27) is simply Euclidean gradient flow,\n$\\frac{d\\chi}{ds}(s) = -\\nabla C(\\chi(s)).$\n(2.47)\nFor\n$C(\\chi(\\theta)) = \\frac{1}{2N} \\|\\chi(\\theta) - Y\\|^2_{\\mathbb{R}^{QN}}$\n(2.48)\nwe have\n$\\frac{d\\chi}{ds} = \\frac{1}{N} (\\chi(s) - y),$\n(2.49)\nwhich is solved by\n$\\chi(s) = y + e^{-s} (\\chi(0) - y),$\n(2.50)\nand we see that $\\chi(s) \\rightarrow y$ as $s \\rightarrow \\infty$.\nSuppose instead that we wanted to impose a linear interpolation\n$\\chi(t) = y + (1-t)(\\chi_0 - y)$.\n(2.51)\nThen letting\n$t := 1-e^{-s},$\n(2.52)\n$s = -N \\ln(1-t),$\n(2.53)\nand\n$\\chi(t) := \\chi(-N \\ln(1 - t)),$\n(2.54)\nfor $\\chi(s)$ a solution to (2.47), we have\n$\\frac{d\\chi}{dt}(t) = \\frac{d\\chi}{ds}(-N \\ln(1 - t))$\n$= -\\frac{N}{1-t} \\nabla C[\\chi(t)]$\n$= \\frac{1}{1-t} (\\chi(t) - y)$.\n(2.55)\nNote that $\\chi(t)$ solves (2.55), as\n$\\frac{d\\chi}{dt}(t) = -(\\chi_0 - y)$\n$= \\frac{1}{1-t} (\\chi(t) -y)$.\n(2.56)\nThus, we have proved the following:\nProposition 2.4. If $\\chi(s)$ is a solution to Euclidean gradient flow (2.47), then\n$\\chi(t) := \\chi(-N \\ln(1 - t)) = y + (1 - t)(\\chi_0 - y),$\n(2.57)\nand $\\chi(t) \\rightarrow y$ as $t \\rightarrow 1$. In particular, this holds for $\\chi(s) = \\chi(\\theta(s))$ defined as in (2.9) when $\\theta(s)$ is a\nsolution to the adapted gradient flow (2.27) and rank $D[\\theta(s)] = QN < K$.\nOn the other hand, if rank$(D) < QN$, the reparametrized Euclidean gradient flow in output space\nprovides a concrete criterion whereby the effect of rank loss can be measured."}, {"title": null, "content": "Proposition 2.5. In general, the constrained Euclidean gradient flow in output space with time reparametriza-\ntion as in (2.54) satisfies\n$\\frac{d\\chi}{dt}(t) = \\frac{1}{1-t} P_t (\\chi(t) -y), \\quad \\chi(0) = \\chi_0, \\quad t \\in [0,1),$\n(2.58)\nwhere $\\chi(t) = \\chi[\\theta(s(t))]$ and $P_t := \\text{Proj}_{\\text{ran}(DD^T)[\\theta(s(t))]}$. When rank$(D) < QN$, the deviation from linear\ninterpolation is given by\n$\\chi(t) - ((1-t)\\chi_0 - ty) = \\int_0^t dt' U_{t,t'} \\frac{1}{1-t'} P_+ (\\chi_0 - y),$\n(2.59)\nwhere the linear propagator $U_{t,t'}$ is determined by\n$\\frac{d}{dt}U_{t,t'} = \\frac{1}{1-t} P_t U_{t,t'}, \\quad U_{t',t'} = 1_{QN\\times QN}, \\quad t,t'\\in [0,1).$\n(2.60)\nProof. The equation (2.58) is straightforwardly obtained in a similar way as (2.55).\nTo prove (2.59), we write\n$\\chi(t) = (1 - t)\\chi_0 + ty + (1 - t)R(t)$\n(2.61)\nso that\n$\\frac{d}{dt}\\chi(t) = -(\\chi_0 - y) - R(t) + (1 - t)\\frac{d}{dt}R(t) .$\n(2.62)\nOn the other hand,\n$\\frac{d\\chi}{dt}(t) = \\frac{1}{1-t} P_t (\\chi(t) - y)$\n$= \\frac{1}{1-t} ((\\chi(t) -y) + P_t(\\chi(t) - y))$\n(2.63)\n$\\frac{1}{1-t} (-(1-t)\\chi_0 + ty - y + (1-t)R(t)) + P_t ((\\chi_0 - y) + ty - y + (1-t)R(t)))$\n$= -(\\chi_0 - y) - R(t) + P_t (\\chi_0 - y + R(t)),$\nwhere we used (2.61) to pass to the third line. Comparing the last line with (2.62), we find\n$\\frac{d}{dt}R(t) = P_t R(t) + \\frac{1}{1-t} P_+ (\\chi_0 - y) .$\n(2.64)\nSolving this matrix valued linear ODE for $R(t)$ using the Duhamel (or variation of constants) formula,\nand substituting the resulting expression in (2.61) yields (2.59), as claimed."}, {"title": "3. APPLICATIONS", "content": "3.1. Prescribed paths in output space. In the previous section, we discussed how changes in $\\frac{d\\theta}{ds}(s)$\ninfluence $\\frac{d\\chi}{ds}(s)$. It could be interesting to take a different point of view and find $\\theta(s)$ from a given path\n$\\chi(s)$. In general, from (2.15),\n$\\text{Proj}_{\\text{ran}(D^T)} \\frac{d}{ds} \\theta = (D)^+ \\frac{d}{ds}\\chi,$\n(3.1)\nand so assuming $\\frac{d\\theta}{ds} = \\text{Proj}_{\\text{ran}(D^T)} \\frac{d\\theta}{ds}$,\u00b9 we can write\n$\\frac{d}{ds}\\theta(s) = (D)^+ \\frac{d}{ds}\\chi(s).$\n(3.2)\n\u00b9Note that this is satisfied by the standard gradient flow $\\frac{d\\theta}{ds} = -\\nabla_{\\theta} C = -D^T\\nabla_{\\chi} C."}, {"title": null, "content": "Thus, we could prescribe a path\n$\\chi(s), \\quad s\\in [0,T],$\n$\\frac{d}{ds}\\chi(s) \\in \\text{ran}(D(s)),$\n$\\chi(T) = y,$\n(3.3)\nand then search for a corresponding $\\theta$ which realizes\n$\\chi(\\theta_*) = \\chi(T)$\n(3.4)\nby solving\n$\\frac{d}{ds}\\theta(s) = (D(s))^+ \\frac{d}{ds}\\chi(s),$\n$\\theta(0) = \\theta_0,$\n(3.5)\n$\\chi(s)$ satisfies (3.3) with $\\chi(0) = \\chi(\\theta_0)$,\nfor some initialization $\\theta_0 \\in \\mathbb{R}^{K}$. Observe that\n$\\text{ran}(D) = (\\text{ker}(D^T))^\\perp = (\\text{ker}(D)^+)^+$\n(3.6)\nand so all equilibrium points of (3.5) satisfy $\\frac{d}{ds} \\chi = 0$.\nAs an example, motivated by the results in section 2.2, one could take $\\chi(s)$ to be the linear interpolation\n(2.51) and check if, for a given $\\theta_0$,\n$(\\chi_0 - y) \\in \\text{ran } D(s),$\n(3.7)\nfor all $s \\in [0, T]$.\nRemark 3.1. Whether or not a path $\\chi(s)$ satisfies the constraint in (3.3) (or (3.7) in the example above)\ncould depend on the choice of $\\theta_0$. In general, properties of the Jacobian matrix $D[\\theta(s)]$ that determine\nthe dynamics of $\\chi(s)$, such as rank and the subspace $\\text{ran}(DD^T)$, might depend on the initialization even\nwithin a class $\\{\\theta_0: \\chi(\\theta_0) = \\chi_0\\}$.\n3.2. Final layer collapse. We turn to the case of a classification task. Assume without any loss of\ngenerality that the components of $\\hat{x}$ are ordered by way of\n$\\hat{x} = (..., \\hat{x}_{1,1},..., \\hat{x}_{1,N_1},..., \\hat{x}_{Q,1},..., \\hat{x}_{Q,N_Q})^T \\in \\mathbb{R}^{QN}$\n(3.8)\nwhere each $\\hat{x}_{j,i_j} \\in \\mathbb{R}$ belongs to the class labeled by $y_j \\in \\mathbb{R}$, for $j = 1,...,Q$, $i_j = 1,...,N_j$, and\n$\\sum_{j=1}^Q N_j = N$. Then, defining the class averages\n$\\bar{x}_j(t) := \\frac{1}{N_j} \\sum_{i_j=1}^{N_j} \\hat{x}_{j,i_j}(t)$\n(3.9)\nand the deviations\n$\\Delta x_{j,i_j} (t) := \\hat{x}_{j,i_j} (t) - \\bar{x}_j(t),$\n(3.10)"}, {"title": null, "content": "the $L^2$ cost (2.7) decomposes into\n$C[\\chi(t)] = \\frac{1}{N} \\sum_{j=1}^Q \\sum_{i_j} |\\hat{x}_{j,i_j} (t) - y_j|^2$\n$= \\frac{1}{N} \\sum_{j=1}^Q N_j \\bar{\\hat{x}}_j(t) - y_j|^2$\n$= \\frac{1}{N} \\sum_{j=1}^Q \\sum_{i_j} |\\Delta x_{j,i_j} (t)|^2 + |\\bar{\\hat{x}}_j(t) - y_j|^2.$\n(3.11)\nThis implies that zero loss training is achievable if and only if the class averages in the output layer are\nmatched to the reference outputs, $\\bar{x}_j(t) \\rightarrow y_j$ for all $j = 1, ..., Q$, and all deviations vanish, $\\Delta x_{j,i_j} (t) \\rightarrow 0$\nfor all $j = 1,..., Q, i_j = 1, ..., N_j$. The latter implies that the image of training data in the output layer\ncontracts to one point per class.\nThus, Proposition 2.4 allows us to arrive at the following conclusion.\nCorollary 3.2. If rank$(D) = QN$, then zero loss minimization is achieved, whereby the cluster averages\nconverge to the reference outputs,\n$\\lim_{t\\rightarrow 1^-} \\bar{\\hat{x}}_j(t) = y_j,$\n(3.12)\nand the deviations converge to zero,\n$\\lim_{t\\rightarrow 1^-} \\Delta x_{j,i_j} (t) = 0,$\n(3.13)\nfor all $j = 1,..., Q$ and $i_j = 1, ..., N_j$.\nThe decomposition of the cost function (3.11) is related to the phenomenon known as neural collapse\n([PHD20, HPD22]), which takes place on the penultimate layer of a neural network. For work on the\nrelationship between neural collapse and final layer collapse, see e.g. [EW22]."}, {"title": "3.3. Tangent kernel", "content": "A related point of view is to study the neural tangent kernel (NTK) introduced\nby [JGH18", "f_\\theta": "mathbb{R}^M \\rightarrow \\mathbb{R}^Q$ and $Df_\\theta(x)$ the Jacobian of $f_\\theta$ at $x \\in \\mathbb{R}^M$. Then for a set of training data\n$X_0 = \\{x_1,...,x_N\\}$ and $D[\\theta"}]}