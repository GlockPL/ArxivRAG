{"title": "GRADIENT FLOW IN PARAMETER SPACE IS EQUIVALENT TO LINEAR INTERPOLATION IN OUTPUT SPACE", "authors": ["THOMAS CHEN", "PATRICIA MU\u00d1OZ EWALD"], "abstract": "We prove that the usual gradient flow in parameter space that underlies many training algorithms for neural networks in deep learning can be continuously deformed into an adapted gradient flow which yields (constrained) Euclidean gradient flow in output space. Moreover, if the Jacobian of the outputs with respect to the parameters is full rank (for fixed training data), then the time variable can be reparametrized so that the resulting flow is simply linear interpolation, and a global minimum can be achieved.", "sections": [{"title": "1. INTRODUCTION", "content": "At the core of most algorithms currently used for training neural networks is gradient descent, whose theoretical counterpart in the continuum limit is gradient flow. This flow is defined in a space of parameters $R^K$ with respect to a cost function defined in $R^{QN}$. The cost is non-convex as a function of the parameters, and standard gradient flow in $R^K$ might not converge to a global minimum. Even changing perspective to $R^{QN}$, where the cost can be chosen to be convex, the resulting associated flow is complicated.\nIn [Che23], it was discussed in detail how the choice of the Riemannian structure in $R^K$ influences the training dynamics. In this context, a modified gradient flow that induces the Euclidean gradient flow in output space $R^{QN}$ was introduced and contrasted with the standard Euclidean gradient flow in parameter space; it was shown that both flows exhibit the same critical sets. In the work at hand, we extend these results by proving that those two flows are in fact homotopy equivalent to one another (Theorem 2.3). Moreover, we prove that if the Jacobian matrix exhibits no rank loss, then the Euclidean flow in output space is reparametrization equivalent to linear interpolation with respect to a suitable time variable (Proposition 2.4). When there is rank loss, an expression for the deviation from linear interpolation is given (Proposition 2.5).\nIn section 3, we provide some applications: First, we note that instead of modifying the metric in $R^K$, one could prescribe a convenient path in $R^{QN}$ and then find an associated path in parameter space. Next, we show that neural collapse occurs in the output layer for the trivialized dynamics studied in 2.2 (Corollary 3.2). Finally, we rephrase Theorem 2.3 in terms of the neural tangent kernel.\nWe briefly comment on related work in section 4. A short appendix on generalized inverses for rank deficient matrices is provided as well."}, {"title": "2. MAIN RESULTS", "content": "Consider a family of functions\n$f_\\theta : R^M \\rightarrow R^Q$\n(2.1)"}, {"title": null, "content": "parametrized by a vector of parameters $\\theta \\in R^K$. Suppose we wish to optimize $f_\\theta$ with respect to a given cost function $C: R^K \\rightarrow R$. A natural approach is to pick a starting point $\\theta_0 \\in R^K$ and construct a path\n$\\theta(s) : [0,\\infty) \\rightarrow R^K$\n(2.2)\nvia the gradient flow\n$\\frac{d \\theta}{ds}(s) = -\\nabla C,\\\\  \\theta(0) = \\theta_0.$\n(2.3)\nWe will refer to (2.3) as the standard gradient flow.\nWhen $f_\\theta$ is a neural network, the optimization problem has the following structure: Consider a set $X_0 \\subset R^M$ (the training data), such that $|X_0| = N$. We can form a data matrix $X_0 \\in R^{M \\times N}$ by making the vectors in $X_0$ into the columns of $X_0$. Then we can consider a function\n$f(\\theta, X_0) : R^K \\times R^{M \\times N} \\rightarrow R^{Q \\times N},$\n(2.4)\ndefined by its action on each column of $X_0$: for $i = 1,\\dots, N,$\n$f(\\theta, (X_0)_i) = f_\\theta((X_0)_i) \\in R^Q.$\n(2.5)\nIn the context of supervised learning, each data point $x \\in X_0$ is associated to a desired output $y(x) \\in R^Q$, and so from a data matrix $X_0$ we can form the corresponding matrix of labels, $Y \\in R^{Q \\times N}$,\n$Y_i = y((X_0)_i)$.\n(2.6)\nA common choice for cost function is the squared loss,\n$C(\\theta, X_0, Y) = \\frac{1}{N} || f(\\theta, X_0) - Y ||_2^2,$\n(2.7)\nwhere $||\\cdot||_2$ is the Frobenius or Hilbert-Schmidt norm.\nFor fixed training data $X_0 = [x_1 \\cdots x_j \\cdots x_N]$, we can also consider the function\n$\\chi : R^K \\rightarrow R^{QN}$\n(2.8)\nfrom parameter space to output space defined by\n$\\chi(\\theta) := (f_\\theta(x_1)^T,\\dots, f_\\theta(x_N)^T)^T.$\n(2.9)\nWe define the correspondent vector of labels $y \\in R^{QN}$,\n$y:= (y(x_1)^T,\\dots, y(x_N)^T)^T,$\n(2.10)\nIn this case, the cost (2.7) becomes\n$C(\\chi(\\theta)) = \\frac{1}{N} ||\\chi(\\theta) - y||^2$.\n(2.11)\nThe Jacobian $D[\\theta] \\in R^{QN \\times K}$ is\n$(D[\\theta])_{jk} := \\frac{\\partial \\chi_j}{\\partial \\theta_k}[\\theta],$\n(2.12)\nand we assume from now on that the partial derivatives in (2.12) exist and are Lipschitz continuous for all $\\theta \\in R^K$. We will often abbreviate $D = D[\\theta]$.\nFor $\\theta(s) \\in R^K$, we can consider the associated path\n$\\chi(s) : [0,\\infty) \\rightarrow R^{QN}$\n(2.13)\ngiven by\n$\\chi(s) := \\chi(\\theta(s))$\n(2.14)"}, {"title": null, "content": "from (2.9). The associated Jacobian is now also time dependent, and we will write $D(s) := D[\\theta(s)]$, or simply $D$ for convenience. The chain rule gives\n$\\frac{d \\chi}{ds}(s) = D(s) \\frac{d \\theta}{ds}(s)$\n(2.15)\nand\n$\\nabla_\\theta C = D^T \\nabla_\\chi C.$\n(2.16)\nRemark 2.1. It is clear that\n$\\{\\theta \\in R^K : \\nabla_{\\chi(\\theta)} C = 0\\} \\subset \\{\\theta : \\nabla_\\theta C = 0\\},$\n(2.17)\nhowever, the two sets are not necessarily identical. It is known that if rank $D[\\theta] = QN = \\min\\{K, QN\\}$, then $C = 0$ if, and only if, $\\nabla_{\\chi} C = 0$. Moreover, if $C$ is the squared loss, then $\\nabla_{\\chi} C = 0$ corresponds to a global minimum with zero loss. See e.g. [Che23, CE23, KMTM24] for more details.\n2.1. Adapted gradient flow. By a simple computation using (2.15) and (2.16), if $\\theta(s)$ satisfies the standard gradient flow (2.3), then\n$\\frac{d \\chi}{ds} = -D D^T \\nabla_{\\chi} C.$\n(2.18)\nWe will refer to (2.18) as standard gradient flow in output space. Suppose instead we wished to modify $\\theta(s)$ so that the associated path $\\chi(s)$ satisfies the Euclidean gradient flow. We recall here results in previous work by one of the authors, in the overparametrized case:\nLemma 2.2 ([Che23]). Let $K > QN$, and let $\\chi(s) = \\chi(\\theta(s))$ be defined as in (2.9). When $D$ is full rank, setting\n$\\frac{d \\theta}{ds}(s) = -(D^T D)^+ \\nabla_\\theta C$\n(2.19)\nyields\n$\\frac{d \\chi}{ds}(s) = -\\nabla_{\\chi} C.$\n(2.20)\nIf we allow for rank$(D) < QN$, then letting\n$\\frac{d \\theta}{ds}(s) = -D^T \\psi,$\n(2.21)\nfor $\\psi$ satisfying\n$D D^T \\psi = P_{\\text{ran}(D D^T)} \\nabla_{\\chi} C,$\n(2.22)\nresults in\n$\\frac{d \\chi}{ds}(s) = -P_{\\text{ran}(D D^T)} \\nabla_{\\chi} C.$\n(2.23)\nWe will now condense these two separate cases into the same. First, note that\n$\\tilde{\\psi} := D (D^T D)^+ (D D^T)^+ (D^T)^+ \\nabla_\\theta C$\n(2.24)\nsatisfies (2.22), as the following computation shows:\n$D D^T \\tilde{\\psi} = D D^T D (D^T D)^+ (D D^T)^+ (D^T)^+ \\nabla_\\theta C \\\\  = D D^T (D D^T)^+ (D^T)^+ \\nabla_\\chi C \\\\  = D D^T (D^T)^+ (D^T)^+ \\nabla_\\chi C \\\\  = D D^T (D D^T)^+ \\nabla_\\chi C \\\\  = P_{\\text{ran}(D D^T)} \\nabla_{\\chi} C.$\n(2.25)"}, {"title": null, "content": "Moreover, we can rewrite\n$-(D^T D)^+ \\nabla_\\theta C = -(D^T D)^+ (D^T D) (D^T D)^+ \\nabla_\\theta C \\\\  = -(D^T D) (D^T D)^+ (D^T D)^+ \\nabla_\\theta C \\\\  = -D^T [D (D^T D)^+ (D D^T)^+ (D^T)^+] \\nabla_\\chi C \\\\  = -D^T [D (D^T D)^+ (D D^T)^+ (D^T)^+] \\nabla_\\chi C,\\\\  = -D^T \\tilde{\\psi}.$\n(2.26)\nTherefore in general, for $K > QN$ and rank$(D) \\leq QN$,\n$\\frac{d \\theta}{ds} = -(D^T D)^+ \\nabla_\\theta C$\n(2.27)\nimplies\n$\\frac{d \\chi}{ds} = -P_{\\text{ran}(D D^T)} \\nabla_{\\chi} C.$\n(2.28)\nWe will refer to (2.27) as adapted gradient flow in $R^K$ and (2.28) as constrained Euclidean gradient flow in $R^{QN}$.\nIt was further shown in [Che23, Theorem 4.2] that the standard gradient flow (2.3) and the adapted gradient flow (2.27) have the same equilibrium points. We extend this to a family of interpolating gradient fields.\nTheorem 2.3. Let $C : R^K \\rightarrow R$ depend on $\\theta \\in R^K$ only through $\\chi(\\theta)$, for $\\chi : R^K \\rightarrow R^{QN}$ defined as in (2.9), and let $D[\\theta] \\in R^{QN \\times K}$ be the Jacobian defined in (2.12). There is a one-parameter family of vector fields in $R^K$ interpolating between\n$-\\nabla_{\\chi} C$\n(2.29)\nand\n$-(D^T D)^+ \\nabla_\\theta C$;\n(2.30)\nthis interpolation preserves singularities.\nMoreover, this induces an interpolation between the vector fields for the standard gradient flow in output space (2.18) and the constrained Euclidean gradient flow (2.28) which preserves the equilibrium points of these flows.\nProof. Define the family of vector fields\n$V_{\\theta,\\alpha} := - A_{\\theta,\\alpha} \\nabla_\\theta C, \\quad \\alpha \\in [0,1],$\n(2.31)\nfor\n$A_{\\theta,\\alpha} := (\\alpha (D^T D)^+ + (1 - \\alpha) \\mathbb{1}_{K \\times K}) .$\n(2.32)\nThen\n$V_{\\theta,0} = -\\nabla_\\theta C$\n(2.33)\nand\n$V_{\\theta,1} = -(D^T D)^+ \\nabla_\\theta C.$\n(2.34)\nFor $\\alpha \\in [0,1)$, note that $A_{\\theta,\\alpha} > 0$ for all $\\theta$. For $\\alpha = 1$, writing $P_{\\text{ran}(D^T)} = (D)^+ D$, we have\n$\\nabla_\\theta C = (D)^+ D \\nabla_\\theta C,$\n(2.35)\nsince (2.16) shows $\\nabla_\\theta C \\in \\text{ran}(D^T)$, and also\n$\\text{ker}(D^T D)^+ = \\text{ker}((D^T D)^T) = \\text{ker}(D^T D) = \\text{ker}((D)^+D),$\n(2.36)"}, {"title": null, "content": "where we used (A.11) and Lemma A.2. Thus, $V_{\\theta,\\alpha} = 0$ if, and only if, $\\nabla_\\theta C = 0$, and the vector fields $V_{\\theta,\\alpha}$ are singular at the same points $\\theta \\in R^K$, for all $\\alpha \\in [0, 1]$.\nSuppose now that\n$\\frac{d \\theta}{ds}(s) = V_{\\theta(s),\\alpha},$\n(2.37)\nso that for $\\chi(s) = \\chi(\\theta(s))$,\n$\\frac{d \\chi}{ds}(s) = D[\\theta(s)]V_{\\theta(s),\\alpha}.$\n(2.38)\nWe wish to write $\\frac{d \\chi}{ds}$ in terms of $\\nabla_{\\chi} C$, so we compute\n$D V_{\\theta,\\alpha} = -D (\\alpha (D^T D)^+ + (1 - \\alpha) \\mathbb{1}_{K \\times K}) \\nabla_\\theta C \\\\  = -(\\alpha D (D^T D)^+ + (1 - \\alpha)D) D^T \\nabla_\\chi C \\\\  = (\\alpha D (D^T D)^+ D^T + (1 - \\alpha)D D^T) \\nabla_\\chi C \\\\  = -(\\alpha P_{\\text{ran}(D D^T)} + (1 - \\alpha)D D^T) \\nabla_\\chi C \\\\  = -(\\alpha \\mathbb{1}_{QN \\times QN} + (1 - \\alpha)D D^T) P_{\\text{ran}(D D^T)} \\nabla_\\chi C,$\n(2.39)\nwhere (*) follows from (A.7), (A.8) and (A.12). Therefore,\n$\\frac{d \\chi}{ds}(s) = V_{\\chi(s),\\alpha}$\n(2.40)\nfor vector fields in $R^{QN}$ defined along the path $\\chi(s)$,\n$V_{\\chi(s),\\alpha} := (\\alpha \\mathbb{1}_{QN \\times QN} + (1 - \\alpha)D(s)D(s)^T) P_{\\text{ran}(D(s)D(s)^T)} \\nabla_{\\chi} C.$\n(2.41)\nObserve that\n$V_{\\chi(s),0} = -\\nabla_{\\chi} C$\n(2.42)\nand\n$V_{\\chi(s),1} = -P_{\\text{ran}(D D^T)} \\nabla_{\\chi} C,$\n(2.43)\nso that in fact $V_{\\chi(s),\\alpha}$, $\\alpha \\in [0,1]$, interpolates between the standard dynamics given by (2.3) at $\\alpha = 0$, and constrained Euclidean gradient flow (2.28) for $\\chi(s)$ at $\\alpha = 1$. Moreover, for $\\alpha \\in (0,1]$, $(\\alpha \\mathbb{1}_{QN \\times QN} + (1 - \\alpha)D D^T)$ is positive-definite, and for $\\alpha = 0$, $D D^T$ is injective in the range of $P_{\\text{ran}(D D^T)}$, so\n$(\\alpha \\mathbb{1} + (1 - \\alpha)D D^T)P_{\\text{ran}(D D^T)} \\nabla_{\\chi} C = 0$ if, and only if, $P_{\\text{ran}(D D^T)} \\nabla_{\\chi} C = 0.$\n(2.44)\nThus, the equilibrium points of (2.40) are the same for all $\\alpha \\in [0, 1]$.\nWe add the following remarks highlighting the geometric interpretation of the above results. If $D D^T$ has full rank $QN$, then $V_{\\chi(s),\\alpha}$ is the gradient vector field with respect to the Riemannian metric on $T R^{QN}$ with tensor\n$(\\alpha \\mathbb{1}_{QN \\times QN} + (1 - \\alpha)D D^T)^{-1} .$\n(2.45)\nThis means that $V_{\\chi(s), \\alpha}$ can equivalently be considered as the gradient fields obtained from the family of Riemannian structures interpolating between the Euclidean structure on $T R^{QN}$, and the metric structure induced by the Euclidean structure on parameter space.\nIf $D D^T$ is rank-deficient, rank$(D D^T) < QN$, we let $V \\subset T R^{QN}$ denote the vector subbundle whose fibers are given by the range of $D D^T$. In this situation, $V_{\\chi(s),\\alpha}$ is the gradient with respect to the bundle metric $h_\\alpha$ on $V$ with tensor\n$[h_\\alpha] = (\\alpha \\mathbb{1}_{QN \\times QN} + (1 - \\alpha)D D^T)^{-1}|_{\\text{ran}(D D^T[\\theta])}$\n(2.46)"}, {"title": "2.2. Reparametrization", "content": "Assuming rank $D = QN < K$, the adapted gradient flow in output space resulting from (2.27) is simply Euclidean gradient flow,\n$\\frac{d \\chi}{ds}(s) = -\\nabla C(\\chi(s)).$\n(2.47)\nFor\n$C(\\chi(\\theta)) = \\frac{1}{2N} ||\\chi(\\theta) - Y||_{R^{QN}}^2$\n(2.48)\nwe have\n$\\frac{d \\chi}{ds} = \\frac{1}{N} (\\chi(s) - y),$\n(2.49)\nwhich is solved by\n$\\chi(s) = y + e^{-s} (\\chi(0) - y),$\n(2.50)\nand we see that $\\chi(s) \\rightarrow y$ as $s \\rightarrow \\infty$.\nSuppose instead that we wanted to impose a linear interpolation\n$\\chi(t) = y + (1-t)(\\chi_0 - y).$\n(2.51)\nThen letting\n$t := 1-e^{-s},$\n(2.52)\n$s = -N \\ln(1-t),$\n(2.53)\nand\n$\\chi(t) := \\chi(-N \\ln(1 - t)),$\n(2.54)\nfor $\\chi(s)$ a solution to (2.47), we have\n$\\frac{d \\chi}{dt} = \\frac{d \\chi}{ds}(-N \\ln(1 - t)) \\frac{ds}{dt} \\\\  = -\\frac{N}{1-t} \\nabla_{\\chi} C[\\chi(t)] \\\\  = \\frac{1}{1-t} (\\chi(t) - y).$\n(2.55)\nNote that $\\chi(t)$ solves (2.55), as\n$\\frac{d \\chi}{dt} = -(\\chi_0 - y) \\\\  = \\frac{1}{1-t} (\\chi(t) -y).$\n(2.56)\nThus, we have proved the following:\nProposition 2.4. If $\\chi(s)$ is a solution to Euclidean gradient flow (2.47), then\n$\\chi(t) := \\chi(-N \\ln(1 - t)) = y + (1 - t)(\\chi_0 - y),$\n(2.57)\nand $\\chi(t) \\rightarrow y$ as $t \\rightarrow 1$. In particular, this holds for $\\chi(s) = \\chi(\\theta(s))$ defined as in (2.9) when $\\theta(s)$ is a solution to the adapted gradient flow (2.27) and rank $D[\\theta(s)] = QN < K$.\nOn the other hand, if rank$(D) < QN$, the reparametrized Euclidean gradient flow in output space provides a concrete criterion whereby the effect of rank loss can be measured."}, {"title": "Proposition 2.5.", "content": "In general, the constrained Euclidean gradient flow in output space with time reparametriza- tion as in (2.54) satisfies\n$\\frac{d \\chi}{dt}(t) = \\frac{1}{1-t} P_t (\\chi(t) -y), \\quad \\chi(0) = \\chi_0, \\quad t \\in [0,1),$\n(2.58)\nwhere $\\chi(t) = \\chi[\\theta(s(t))]$ and $P_t := P_{\\text{ran}(D D^T)[\\theta(s(t))]}$. When rank$(D) < QN$, the deviation from linear interpolation is given by\n$\\chi(t) - ((1-t)\\chi_0 - ty) = \\int_0^t ds \\frac{1}{1-s} \\int_s^t dt' U_{t,t'} \\frac{1}{1-t'} P_t^+ (\\chi_0 - y),$\n(2.59)\nwhere the linear propagator $U_{t,t'}$ is determined by\n$\\frac{d}{dt} U_{t,t'} = \\frac{1}{1-t} P_t U_{t,t'}, \\quad U_{t',t'} = \\mathbb{1}_{QN \\times QN}, \\quad t,t' \\in [0,1).$\n(2.60)\nProof. The equation (2.58) is straightforwardly obtained in a similar way as (2.55).\nTo prove (2.59), we write\n$\\chi(t) = (1 - t)\\chi_0 + ty + (1 - t)R(t)$\n(2.61)\nso that\n$\\frac{d \\chi}{dt}(t) = -(\\chi_0 - y) - R(t) + (1 - t)\\frac{dR}{dt} (t) .$\n(2.62)\nOn the other hand,\n$\\frac{d \\chi}{dt}(t) = \\frac{1}{1-t} P_t (\\chi(t) - y) \\\\  = \\frac{1}{1-t} (\\chi(t) - y) + \\frac{1}{1-t} P_t^+( \\chi(t) - y) \\\\  = \\frac{1}{1-t} ((1-t)\\chi_0 + ty -y + (1-t)R(t)) + \\frac{1}{1-t} P_t^+ ((1-t)\\chi_0 + ty - y + (1-t)R(t)) \\\\  = -(\\chi_0 - y) - R(t) + P_t^+ (\\chi_0 - y + R(t)),$\nwhere we used (2.61) to pass to the third line. Comparing the last line with (2.62), we find\n$\\frac{dR}{dt} (t) = \\frac{1}{1-t} P_t^+ R(t) + \\frac{1}{1-t} P_t^+ (\\chi_0 - y) .$\n(2.64)\nSolving this matrix valued linear ODE for $R(t)$ using the Duhamel (or variation of constants) formula, and substituting the resulting expression in (2.61) yields (2.59), as claimed."}, {"title": "3. APPLICATIONS", "content": "3.1. Prescribed paths in output space. In the previous section, we discussed how changes in $\\frac{d \\theta}{ds}(s)$ influence $\\frac{d \\chi}{ds}(s)$. It could be interesting to take a different point of view and find $\\theta(s)$ from a given path $\\chi(s)$. In general, from (2.15),\n$P_{\\text{ran}(D^T)} \\frac{d \\theta}{ds} = (D)^+ \\frac{d \\chi}{ds},$\n(3.1)\nand so assuming $\\frac{d \\theta}{ds} = P_{\\text{ran}(D^T)} \\frac{d \\theta}{ds}$, we can write\n$\\frac{d \\theta}{ds}(s) = (D)^+ \\frac{d \\chi}{ds}(s).$\n(3.2)"}, {"title": null, "content": "Thus, we could prescribe a path\n$\\chi(s), \\quad s \\in [0,T],$\n$\\frac{d \\chi}{ds}(s) \\in \\text{ran}(D(s)),$\n$\\chi(T) = y,$\n(3.3)\nand then search for a corresponding $\\theta$ which realizes\n$\\chi(\\theta_*) = \\chi(T)$\n(3.4)\nby solving\n$\\frac{d \\theta}{ds}(s) = (D(s))^+ \\frac{d \\chi}{ds}(s),$\n$\\theta(0) = \\theta_0,$\n(3.5)\n$\\chi(s)$ satisfies (3.3) with $\\chi(0) = \\chi(\\theta_0),$\nfor some initialization $\\theta_0 \\in R^K$. Observe that\n$\\text{ran}(D) = (\\text{ker}(D^T))^\\perp = (\\text{ker}(D)^+)^\\perp,$\n(3.6)\nand so all equilibrium points of (3.5) satisfy $\\frac{d \\chi}{ds} = 0$.\nAs an example, motivated by the results in section 2.2, one could take $\\chi(s)$ to be the linear interpolation (2.51) and check if, for a given $\\theta_0$,\n$(\\chi_0 - y) \\in \\text{ran } D(s),$\n(3.7)\nfor all $s \\in [0, T]$.\nRemark 3.1. Whether or not a path $\\chi(s)$ satisfies the constraint in (3.3) (or (3.7) in the example above) could depend on the choice of $\\theta_0$. In general, properties of the Jacobian matrix $D[\\theta(s)]$ that determine the dynamics of $\\chi(s)$, such as rank and the subspace $\\text{ran}(D D^T)$, might depend on the initialization even within a class $\\{\\theta_0: \\chi(\\theta_0) = \\chi_0\\}$.\n3.2. Final layer collapse. We turn to the case of a classification task. Assume without any loss of generality that the components of $\\hat{x}$ are ordered by way of\n$\\tilde{x} = (.., \\tilde{x}_{1,i_1},..., .., \\tilde{x}_{Q,i_Q}, ..)^T \\in R^{QN}$\n(3.8)\nwhere each $\\tilde{x}_{j,i_j} \\in R$ belongs to the class labeled by $y_j \\in R^Q$, for $j = 1,\\dots,Q$, $i_j = 1,\\dots,N_j$, and $\\sum_{j=1}^Q N_j = N$. Then, defining the class averages\n$\\overline{x_j(t)} := \\frac{1}{N_j} \\sum_{i_j=1}^{N_j} x_{j,i_j} (t)$\n(3.9)\nand the deviations\n$\\Delta x_{j,i_j} (t) := x_{j,i_j} (t) - \\overline{x_j(t)},$\n(3.10)"}, {"title": null, "content": "the $L^2$ cost (2.7) decomposes into\n$C[x(t)] = \\frac{1}{N} \\sum_{j=1}^Q \\sum_{i_j} ||x_{j,i_j}(t) - y_j||^2 \\\\  = \\frac{1}{N} \\sum_{j=1}^Q N_j || \\overline{x_j(t)} - y_j||^2 + \\frac{1}{N} \\sum_{j=1}^Q \\sum_{i_j} || \\Delta x_{j,i_j} (t)||^2 .$\n(3.11)\nThis implies that zero loss training is achievable if and only if the class averages in the output layer are matched to the reference outputs, $\\overline{x_j(t)} \\rightarrow y_j$ for all $j = 1, \\dots, Q$, and all deviations vanish, $\\Delta x_{j,i_j} (t) \\rightarrow 0$ for all $j = 1,\\dots,Q$, $i_j = 1, \\dots, N_j$. The latter implies that the image of training data in the output layer contracts to one point per class.\nThus, Proposition 2.4 allows us to arrive at the following conclusion.\nCorollary 3.2. If rank$(D) = QN$, then zero loss minimization is achieved, whereby the cluster averages converge to the reference outputs,\n$\\lim_{t\\rightarrow 1^-} \\overline{x_j(t)} = y_j,$\n(3.12)\nand the deviations converge to zero,\n$\\lim_{t\\rightarrow 1^-} \\Delta x_{j,i_j} (t) = 0,$\n(3.13)\nfor all $j = 1,\\dots, Q$ and $i_j = 1, \\dots, N_j$.\nThe decomposition of the cost function (3.11) is related to the phenomenon known as neural collapse ([PHD20, HPD22]), which takes place on the penultimate layer of a neural network. For work on the relationship between neural collapse and final layer collapse, see e.g. [EW22]."}, {"title": "3.3. Tangent kernel.", "content": "A related point of view is to study the neural tangent kernel (NTK) introduced by [JGH18]. It can be written as\n$\\Theta(x, y, \\theta) = D f_\\theta(x) (D f_\\theta(y))^T \\in R^{Q \\times Q},$\n(3.14)\nfor a function $f_\\theta : R^M \\rightarrow R^Q$ and $D f_\\theta(x)$ the Jacobian of $f_\\theta$ at $x \\in R^M$. Then for a set of training data $X_0 = \\{x_1,\\dots,x_N\\}$ and $D[\\theta]$ defined as in section 2 we can write $D D^T \\in R^{QN \\times QN}$ as a block matrix with blocks of size $Q \\times Q$,\n$(D[\\theta]D[\\theta]^T)_{i, j} = \\Theta(x_i, x_j, \\theta), \\quad i, j = 1,\\dots, N.$\n(3.15)\nThe kernel $\\Theta(x, y, \\theta)$ is positive-definite with respect to the chosen training data at $\\theta$ if, for all $(u_i)_{i=1}^N \\in \\mathbb{R}^{QN}$,\n$||u_i||^2 > 0 \\text{ implies } \\sum_{i=1}^N \\sum_{i,j=1}^N u_i \\Theta(x_i, x_j, \\theta) u_j > 0.$\n(3.16)\nFrom (3.15) we see that his holds if, and only if, $D D^T$ is a positive-definite matrix."}, {"title": null, "content": "Thus, Theorem 2.3 suggests that, by changing the metric in parameter space, the NTK can be modified into a simpler kernel $K$ which preserves the equilibrium points of the flow. When $\\Theta$ is positive-definite, $K$ satisfies\n$K(x_i"}]}