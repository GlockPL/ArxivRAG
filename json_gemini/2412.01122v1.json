{"title": "TAS-TSC: A Data-Driven Framework for Estimating Time of Arrival Using\nTemporal-Attribute-Spatial Tri-space Coordination of Truck Trajectories", "authors": ["Mengran Lia", "Junzhou Chena", "Guanying Jiangb", "Fuliang Li\u00ba", "Ronghui Zhanga*", "Siyuan Gongd", "Zhihan Lve"], "abstract": "Accurately estimating time of arrival (ETA) for trucks is crucial for optimizing transportation efficiency in logistics.\nGPS trajectory data offers valuable information for ETA, but challenges arise due to temporal sparsity, variable se-\nquence lengths, and the interdependencies among multiple trucks. To address these issues, we propose the Temporal-\nAttribute-Spatial Tri-space Coordination (TAS-TsC) framework, which leverages three feature spaces-temporal, at-\ntribute, and spatial to enhance ETA. Our framework consists of a Temporal Learning Module (TLM) using state\nspace models to capture temporal dependencies, an Attribute Extraction Module (AEM) that transforms sequential\nfeatures into structured attribute embeddings, and a Spatial Fusion Module (SFM) that models the interactions among\nmultiple trajectories using graph representation learning. These modules collaboratively learn trajectory embeddings,\nwhich are then used by a Downstream Prediction Module (DPM) to estimate arrival times. We validate TAS-TSC on\nreal truck trajectory datasets collected from Shenzhen, China, demonstrating its superior performance compared to\nexisting methods.\nKeywords: Estimating Time of Arrival, GPS Trajectory Data, Temporal Learning, Attribute Extraction, Spatial\nFusion, Tir-space Coordination", "sections": [{"title": "1. Introduction", "content": "In global trade, accurately estimating time of arrival\n(ETA) is crucial for optimizing warehouse manage-\nment, production scheduling, and maintaining balance\nbetween supply and demand in the logistics and trans-\nportation industry [1, 2]. Over the last decade, ETA\nhas transitioned from traditional statistical approaches\nto advanced data-driven models, harnessing the power\nof machine learning and deep learning technologies.\nThese models utilize historical trajectory data to predict\narrival times with greater precision [3, 4, 5].\nCurrent methodologies extract features from vari-\nous data sources, including map-matched road seg-\nments [6, 7, 8, 9], origin-destination (OD) information\n[10, 11, 12, 13], and GPS coordinates [14, 15, 16].\nMap-matching techniques offer explicit details like road\nnames and traffic conditions, while OD data provides\ncontextual information, such as sender and recipient ad-\ndresses, frequently used in e-commerce ETA [17, 18].\nAmong these, GPS data stands out for its ability to\ndeliver real-time insights into vehicle movements, in-\ncluding position, speed, and direction, making it highly\nadaptable to dynamic conditions. With the advancement"}, {"title": "2. Related Work", "content": "We first review the progression of ETA methodolo-\ngies, highlighting key developments and limitations.\nWe then examine the evolution of time-series learning\nmethods, focusing on how these approaches contribute\nto the predictive accuracy and efficiency of ETA models."}, {"title": "2.1. Estimating Time of Arrival", "content": "Early ETA methods relied on traditional machine\nlearning techniques, which provided foundational meth-\nods for improving prediction accuracy. For example,\nGuin [46] used seasonal ARIMA models for travel\ntime prediction. In later years, methods like K-Nearest\nNeighbors (KNN) [47] and Support Vector Machines\n(SVM) [48, 49] were adopted for their robustness in\nregression tasks. More advanced techniques like Ran-\ndom Forests [50] and Gradient Boosting Models (GBM)\n[51, 52] showed improved accuracy by capturing com-\nplex data relationships. These models, combined with\nBayesian Optimization (BO), demonstrated strong pre-\ndictive capabilities in applications like train delay pre-\ndiction [7, 53].\nAs ETA research progressed, ensemble learning and\ngraph-based methods became popular for enhancing\npredictive power. Ensemble learning models, such\nas the ones by Zhong et al. [54], employed multi-\nple predictors to improve prediction accuracy. Mean-\nwhile, graph-based models, leveraging Graph Neural\nNetworks (GNNs) like Graph Convolutional Networks\n(GCNs) [40], GraphSAGE [42], and Graph Attention\nNetworks (GATs) [41], became essential in ETA due to\ntheir ability to model complex spatiotemporal relation-\nships. These models proved effective for urban transit\nsystems by utilizing network connectivity information\n[38, 39, 55].\nRecent years have seen the application of deep learn-\ning to ETA with models like Convolutional LSTMs [56]\nand hybrid trajectory networks [15], allowing for more\ngranular analysis of trajectory data. These methods\nare particularly useful for multi-city and multi-regional\nETA due to their ability to learn from diverse datasets\n[14, 16]. In 2021, Han et al. [9] introduced a multi-\nsemantic path representation learning method that fur-\nther enhanced the accuracy of travel time estimation by\ncapturing fine-grained semantic information within tra-\njectories.\nMost recently, tensor decomposition methods, like\nthose proposed by Huang et al. [31], and multi-faceted\nroute representation learning by Liao et al. [57], have\nenabled context-aware ETA that adapt to varying condi-\ntions by accounting for both spatial and temporal depen-\ndencies within trajectory data. These advancements un-\nderscore the importance of comprehensive representa-\ntion learning to address the inherent variability in travel\ntime data across different regions and traffic conditions.\nDespite advancements, current ETA methods face\nlimitations. Traditional models lack robustness in cap-\nturing complex temporal-spatial relationships, espe-\ncially with sparse, variable-length data. While deep"}, {"title": "2.2. Time-Series Learning Methods", "content": "Effective Estimating Time of Arrival (ETA) predic-\ntion relies heavily on robust time-series learning meth-\nods capable of handling complex temporal dependen-\ncies. Traditional statistical models like Autoregressive\nIntegrated Moving Average (ARIMA) [58, 59] and Vec-\ntor Autoregressive (VAR) models [60, 61] are widely\nused for univariate and multivariate time series data.\nHowever, they face limitations in handling sparse, ir-\nregular, and variable-length data sequences commonly\nfound in transportation systems [46].\nIn recent years, deep learning has significantly ad-\nvanced time-series learning. Recurrent Neural Net-\nworks (RNNs) and their variants like Long Short-Term\nMemory (LSTM) and Gated Recurrent Units (GRU)\nhave traditionally been used to capture temporal depen-\ndencies in sequence data [56, 62]. However, RNNs\nface critical limitations in ETA tasks, especially with\nlong GPS trajectories. RNNs are prone to vanishing\ngradients, making it difficult to retain long-term infor-\nmation effectively. Additionally, their sequential pro-\ncessing nature limits parallelization, resulting in high\ncomputational costs on large-scale datasets, as is com-\nmon in ETA applications. Transformers [63] partially\naddress these issues by allowing parallel processing\nand capturing long-range dependencies through self-\nattention. However, the quadratic complexity of the at-\ntention mechanism makes Transformers computation-\nally intensive and memory-demanding, especially for\nthe long sequences typical of GPS data [43]. This com-\nplexity hinders their scalability in ETA applications,\nwhere sequence lengths often vary widely.\nTherefore, current methods have their limitations.\nFor instance, Transformers that rely on attention mech-\nanisms are limited in their application to long sequences\ndue to their complexity; and the main challenge for\ngraph-based learning methods lies in capturing informa-\ntion from dynamic graph structures where connectivity\nchanges over time. For estimating arrival times, we re-\nquire a method capable of handling both sparse, long\ntime series and leveraging associated relationships."}, {"title": "3. Methodology", "content": "For ETA of trucks in logistics scenarios, we pro-\npose the Temporal-Attribute-Spatial Tri-space Coor-\ndination (TAS-TSC) framework, as depicted in Fig-\nure 2. Our framework encodes three sets of embed-\ndings-temporal, attribute, and spatial-captured from"}, {"title": "3.1. Preliminaries", "content": "To estimate the arrival time based on vehicle trajec-\ntory data, we define a collection of variable-length tra-\njectories gathered over a specific period. Let this set\nof trajectories be denoted as $T_r = \\{T_{r1},T_{r2},..., T_{rn}\\}$,\nwhere N represents the total number of trajectories.\nEach trajectory $T_{rn}$ (for $n \\leq N$) consists of an ordered\nset of temporal data points, formally represented as\n$T_{rn} = \\{P_1, P_2, ..., P_{M_n}\\}$, where $M_n$ indicates the length\nof the nth trajectory, which can vary from one trajectory\nto another.\nEach data point $P_i$ within a trajectory encapsulates\nmultiple attributes structured as a feature vector. This\nvector includes the timestamp $t_i$, geographic coordinates\n(latitude $\\lambda_i$ and longitude $\\phi_i$), speed $v_i$, direction $\\theta_i$, and\nspecial event indicator $e_i$. Thus, the feature vector for"}, {"title": "3.2. Temporal Learning Module with State Space\nModel", "content": "In time series forecasting, models like Transformers\n[63] demonstrate strong capabilities in capturing tempo-\nral dependencies, but their computational complexity of\n$O(n^2)$ limits their ability to model longer sequences. To\naddress this, state space models (SSMs) [64, 65] have\nbecome increasingly popular due to their approximately\nlinear complexity, enabling efficient modeling of com-\nplex relationships over long sequences. Our Tempo-\nral Learning Module (TLM) leverages a selected state\nspace model (Mamba) [66, 43] to embed trajectory fea-\ntures efficiently [67], modeling contextual information"}, {"title": "3.3. Attribute Extraction Module with Feature Engi-\nneering", "content": "In this study, we introduce the Attribute Extraction\nModule (AEM), a feature engineering solution designed\nto transform the sequential features of truck trajecto-\nries into rich attribute features. These attribute features\noffer a generalized description of the overall distribu-\ntion and key features of the trajectory data, helping the\nmodel capture a broader range of information and en-\nhance its generalization ability when facing sparse data\nor variable-length sequences.\nSpecifically, by calculating statistical vari-\nables-including maximum, minimum, mean, and\nvariance of differences, ranges, central positions, and\nangles for the GPS trajectory features $[t_i, \\lambda_i, \\Phi_i, v_i, \\theta_i, e_i]$,\nthe AEM delves into the details of trajectory data. This\nprocess reduces data sparsity by summarizing variable-\nlength sequences into fixed-size attribute vectors,\nand simplifies data complexity by condensing high-\ndimensional sequential data into lower-dimensional\nrepresentations.\n(a) GPS time: Regarding the temporal information,\nwe employ the time difference and rate of change to re-\nflect the temporal patterns and trends.\nFor each trajectory data $G_n$, the time difference be-\ntween adjacent time points $t_i$ and $t_{i-1}$ can be expressed\nas:\n$TimeDif f_{i,n} = t_{i,n} - t_{i-1,n}, i = 2, 3, ..., M_{max}$. (6)\nThe time rate of change represents the rate of time\nchange between adjacent time points and can be ex-\npressed as:\n$TimeRate_{i,n} = \\frac{Time Di f f_{i,n}}{t_{i-1,n}}, i = 2, 3, ..., M_{max}$. (7)\n(b) GPS longitude and latitude: Concerning the lat-\nitude and longitude information, we employ the range\nand central position of both longitude and latitude to\ndescribe the truck's movement range and spatial distri-\nbution.\nFor each trajectory data $T_{rn}$, the range of longitude\nand latitude can be expressed as:\n$LongitudeRange_n = max(\\lambda_n) \u2013 min(\\lambda_n)$\n$LatitudeRange_n = max(\\phi_n) \u2013 min(\\phi_n)$. (8)"}, {"title": "3.4. Spatial Fusion Module with Feature Diffusion\nGraph Learning", "content": "The purpose of the Spatial Fusion Module (SFM)\nis to mine the intrinsic influence relationships caused\nby the collaborative operation of different trajectories\nbased on a spatiotemporal relation graph, and to coor-\ndinate the three feature spaces of time, attributes, and\nspatial dimensions. Firstly, SFM relies on the tempo-\nral sequence embedding $E^T$ to construct a spatiotempo-\nral relation graph G based on sequence features, aim-\ning to capture the complex spatiotemporal correlations\nbetween trajectories. Secondly, it carefully considers\nthe synchronous impact of the spatiotemporal correla-\ntion graph and attribute embedding $E^A$ using a feature\ndiffusion graph learning method."}, {"title": "3.4.1. Spatio-temporal Relations Graph", "content": "In the AEM, the detailed transformation of attribute\ndata may inadvertently lead to varying degrees of infor-\nmation loss. To intelligently preserve as much temporal\ninformation as possible, we construct a spatiotemporal\nrelation graph G based on the similarity of temporal fea-\ntures. By employing the nearest neighbor algorithm, we\nestablish an adjacency matrix based on the similarity of\ntemporal features, represented as the spatial embedding\n$E^S \\in R^{N \\times N}$. Specifically, we first flatten the tempo-\nral sequence embedding of trajectories $E^T \\in ]R^{N \\times M_{max} \\times F}$\nto obtain a matrix of shape $R^{N \\times (M_{max}*F)}$, then calculate\nthe similarity measure (such as using Euclidean distance\nor Manhattan distance) between all pairs of trajectory\nsamples. Based on the results of these similarity cal-\nculations, we select K most similar trajectory nodes as"}, {"title": "3.4.2. Feature Diffusion Graph Learning", "content": "In addition, we employ a feature diffusion (FD)\nmethod to achieve the diffusion and aggregation of at-\ntribute embedding. Given the graph data, we appily ad-\njacency matrix W to update the attribute embedding $E^A$,\nfacilitating the exchange of information and aggregation\nof features among nodes. The mathematical description\nof the message passing is as follows:\n$E^{S(l+1)} =  \\begin{cases}\nW E^{A(D)} & \\text{if } l = 0\\\\\nW E^{S(l)} & \\text{if } l \\neq 0\n\\end{cases}$ (18)\nwhere $E^{S(l+1)}$ represents the spatial embedding after the\nl-th iteration. In the first iteration, the adjacency ma-\ntrix W is multiplied with the current spatial embed-\nding $E^A$ to obtain the updated spatial embedding $E^{S(l+1)}."}, {"title": "3.5. Optimize and Train for ETA", "content": "Through the three modules of TLM, AEM and SFM,\nwe obtain an embedding that represents the three di-\nmensions of time series, attribute and space. Next, we\nintroduce framework loss function training and how to\nperform arrival time estimation by leveraging DPM."}, {"title": "3.5.1. Optimize Loss Training", "content": "To optimize framework learning, we designed two\nloss functions for embedding learning and structural\nlearning optimization.\nThe loss function for embedding learning is repre-\nsented as:\n$L_E = \\frac{1}{N} \\sum_{i=1}^{N} (\\frac{E_i^T X_i}{||E_i^T|| ||X_i||} - \\frac{E_i^X X_i}{||E_i^X|| ||X_i||})^2$, (20)\nThe embedding learning loss focuses on optimizing the\ntemporal sequence embeddings $E^T$ to ensure that the\nsimilarity between embeddings reflects the similarity\nbetween the original temporal sequences. By compar-\ning the similarity (using cosine similarity as the mea-\nsure) between the embedding $E^T$ and its corresponding\noriginal temporal sequence embedding $X^T$, $L_E$ encour-\nages the model to learn embeddings that retain the in-\ntrinsic features of the original temporal sequences.\nFor structural learning, the loss function can be rep-\nresented as:"}, {"title": "3.5.2. Downstream Prediction Module", "content": "Considering the earlier discussion, let's define the to-\ntal loss function for learning temporal embeddings in a\nself-supervised manner. To estimate the arrival time, we\nfirst fuse the embeddings representing time, attributes,\nand space using residual connections [44] to obtain a\nhybrid embedding:\n$E^H = E^A + \\alpha E^S$, (23)\nwhere $\\alpha$ represents the balance parameter. The addition\nis crucial as it enables the model to utilize rich attribute\ninformation alongside dynamic spatial features, enhanc-\ning the overall representation.\nThen, we input the hybrid embedding $E^H$ into\nHistogram-based Gradient Boosting (HGB) [45] as a\npredictor for estimating arrival times. The core idea of\nHGB is to utilize the gradient boosting framework com-\nbined with histogram technology to optimize the train-\ning process of decision trees. HGB constructs a series of\ndecision trees to gradually approximate the target func-\ntion, with each tree learning the direction of the residual\npredicted by the previous tree, thereby enhancing the\nmodel's predictive capability. Through HGB, we map\nthe hybrid embedding $E^H$ to the true arrival time $\\hat{Y}$. If it\nis a regression task, the loss function can be represented\nas:\n$L_{HGB} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i-\\hat{Y_i})^2$ (24)"}, {"title": "3.6. Computational Complexity Analysis", "content": "In this section, we analyze the temporal and spatial\ncomplexity of three modules.\nTLM: Assuming the truck trajectory dataset com-\nprises N samples, with each sample including data from\nM time instances, the literature [43] indicates that the\nTLM, when employing the Mamba model, can achieve\na computational complexity of merely O(N).\nAEM: The AEM primarily involves extensive sta-\ntistical analysis and feature extraction from truck tra-\njectory data. This includes computations of time dif-\nferences, time rate of change, longitude and latitude\nranges, centers, speeds, speed rate of change, and direc-\ntion rate of change. The complexity for calculating time\ndifferences and time rate of change is O(M) per sample,\ncomputing longitude and latitude range and centers is\nO(M) per sample, calculating speeds and speed rate of\nchange is O(M) per sample, and determining direction\nrate of change is O(M) per sample. Consequently, the\ntotal complexity of the AEM module is O(NM).\nSFM: The SFM involves computing similarity adja-\ncency matrices and feature propagation in graph learn-\ning. For calculating similarity adjacency matrices, we\nutilize the Nearest Neighbors algorithm with a complex-\nity of $O(NM^2)$, where N is the number of samples and\nM is the number of time instances. Feature propaga-\ntion in graph learning mainly revolves around iterative\nupdates of the adjacency matrix. Assuming l iterations,\neach iteration's computational complexity is O(NM\u00b2),\nleading to a total complexity of O(INM\u00b2) for feature\npropagation in graph learning. Thus, the overall com-\nplexity of the SFM is O(INM\u00b2)."}, {"title": "4. Experiments and Analysis", "content": "In this section, we conduct a comprehensive exper-\nimental evaluation of our proposed framework to val-\nidate its effectiveness and performance in estimating\ntruck arrival times."}, {"title": "4.1. Datasets and Settings", "content": "Our dataset was collected from GPS devices installed\non a fleet of trucks operating across various districts\nin Shenzhen, China. Each truck was equipped with a\nGPS sensor that recorded multiple parameters at reg-\nular intervals, including time, geographic coordinates\n(longitude and latitude), speed, direction, and specific\nevent information. To validate our framework, we gath-\nered truck trajectory data from five districts in Shen-\nzhen: Baoan, Nanshan, Yantian, Futian, and Luohu.\nThis dataset contains a total of 3,622,017 data points,\nforming 6,487 trajectories. Table 1 provides a detailed breakdown of trajectory data\nacross these five regions, including the number of tra-\njectories (N), minimum length ($M_{min}$), maximum length\n($M_{max}$), average length ($M_{mean}$), and standard deviation\n($M_{std}$) of each trajectory. The results show that trajec-\ntory lengths are generally distributed between 500 and\n900 data points, with a standard deviation between 500\nand 600, highlighting the sparsity and variability of tra-\njectory lengths. To more intuitively illustrate the fea-\ntures of our dataset, we visualized the distribution of\ntrajectory lengths across different regions.\nThe figure demonstrates a long-tail distri-\nbution of trajectory lengths, with most trajectories con-\ntaining fewer than 250 data points, while a few extend\nbetween 250 and 2,000 data points. This sparsity and\nimbalance in sequence length present significant chal-\nlenges for accurately predicting truck arrival times."}, {"title": "4.2. Comparison of Estimating Time of Arrival", "content": "Estimating truck arrival times accurately based on\nhistorical trajectory data poses a challenging regres-\nsion problem, especially due to the complex, sparse,\nand variable-length nature of the data. In this section,\nwe evaluate our proposed framework alongside vari-\nous comparative models across four main categories:"}, {"title": "4.3. Domain Generalization Verification", "content": "Urban logistics environments are characterized by\nregion-specific variations in traffic flow, road structure,\nand environmental factors, posing significant challenges\nto model generalizability. In our domain generalization\nevaluation, we design an experiment that simulates real-\nworld conditions to rigorously test the adaptability of\nthe TAS-TSC framework.\nWe use data from \"Baoan\" and \"Nanshan\" as the\ntraining regions, while \u201cFutian\u201d and \u201cLuohu\" are cho-\nsen as test regions with distinct traffic and environmen-\ntal features. This setup allows us to assess the model's\ncapacity to adapt and maintain predictive accuracy in re-\ngions not represented in the training data. Our TAS-TSC\nframework, through its tri-space coordination mecha-\nnism, is specifically designed to capture transferable\npatterns by mining deep correlations between trajecto-\nries in the temporal, spatial, and attribute domains."}, {"title": "4.4. Ablation Analysis", "content": "Ablation studies are crucial in understanding the im-\npact and significance of different components in a given\nmodel or methodology. In our research, we mainly fo-\ncused on the TLM and AEM and SFM to evaluate their\nindividual contributions."}, {"title": "4.4.1. TLM Ablation Analysis", "content": "The TLM leverages the state-of-the-art Mamba\nmodel, a selective state space model, for encoding com-\nplex time series data. To evaluate the unique contri-\nbution of Mamba, we conduct an ablation study by re-"}, {"title": "4.4.2. AEM Ablation Analysis", "content": "In the AEM, we explore the effect of different fea-\nture types-time, location, speed, direction, and event-\nrelated attributes-on prediction accuracy. Our findings show that omitting any sin-\ngle attribute reduces estimation accuracy, with location\nand spatial features showing the most significant im-\npact. This result highlights the critical role of spatial\ncontext in ETA. The combination of all features pro-\nvides the highest accuracy, suggesting that integrating\ndiverse attributes allows the model to develop a more\ncomprehensive understanding of the trajectory dynam-\nics. This synergy among attributes enhances the model's\nrobustness and predictive power, underscoring the ne-\ncessity of multi-feature integration in complex logistic\napplications."}, {"title": "4.4.3. SFM Ablation Analysis", "content": "For the SFM, we examine the role of two critical\ncomponents: the Feature Diffusion (FD) module and\nthe spatio-temporal loss function (Ls). As shown in\nTable 5, the removal of either component results in a\nnotable drop in performance, underscoring their impor-"}, {"title": "4.5. Visual Analysis of ETA", "content": "To demonstrate the effectiveness of our framework,\nwe compare it against two hybrid methods, MetaTTE\nand IGT, on the Nanshan district dataset, showcasing\nthe visualization of the estimating time of arrival in test-\ning.\nSpecifically, the horizontal\naxis in the chart represents the sample index, while the\nvertical axis indicates the normalized arrival time. In\nthe legend, the actual arrival times are depicted by a\nred solid line, whereas the estimated arrival times by\nour method are shown with a green dashed line. Obser-\nvation of the chart clearly shows that our method sur-\npasses the two comparative methods in terms of accu-\nracy, closely aligning with the actual values. Further-\nmore, our framework effectively captures sudden and\nanomalous values in the data, accurately estimating the\ninherent patterns and rhythms of arrival times."}, {"title": "4.6. Analysis of Hyperparameter Validation", "content": "Hyperparameter tuning plays a vital role in optimiz-\ning the performance of our feature propagation model,\nas we aim to identify the best combination of hyperpa-\nrameters that enhance predictive accuracy. In this sec-\ntion, we examine the impact of key hyperparameters on\nthe feature propagation method, using initial default val-\nues of $K = 20, l = 10, \\eta = 0.01$, and $\\alpha = 0.1$. The\nexperimental results are summarized in Figure 8.\nThe hyperparameter K, which defines the number of\nnearest neighbor nodes used during feature propagation,\neffectively controls the local neighborhood for feature\ndiffusion. The parameter l, which represents the number\nof propagation iterations, influences the depth of feature\npropagation across the graph structure. We tested val-\nues of K and l across a range from 1 to 100. From the\nresults, we observe that neither parameter has a straight-\nforward linear relationship with performance. This in-\ndicates that increasing the neighborhood scope (K) and\npropagation depth (1) introduces additional useful con-\ntext but may also add noise, thus necessitating dataset-\nspecific tuning. The parameters $\\eta$ and $\\alpha$ represent bal-\nance factors that adjust the influence of sequence loss\n$L_{SE}$ and residual connections. We explored values from\n0.001 to 10. Our analysis indicates that, while optimal\nvalues differ across datasets, each dataset demonstrates\na clear trend."}, {"title": "5. Conclusion", "content": "In this study, we developed and validated the\nTemporal-Attribute-Spatial Tri-space Coordination\n(TAS-TSC) framework for accurate truck arrival time\nestimation. The TAS-TSC framework integrates the\nstate-of-the-art selective state space model, Mamba,\nwith innovative techniques for attribute feature extrac-\ntion and spatio-temporal feature propagation.\nComparative analyses with existing time-series pre-\ndiction models confirm TAS-TsC's superiority in both\naccuracy and efficiency. Notably, TAS-TsC's domain\ntransfer and generalization capabilities allow it to adapt\neffectively to new and diverse environments, making it\nversatile for real-world applications. Through extensive\nablation studies, we assessed the contributions of each\nmodel component, revealing that the depth and scope of\nfeature propagation, as well as optimized hyperparame-\nter configurations, play crucial roles in achieving robust\nETA.\nLooking forward, we plan to explore real-time de-\nployment of TAS-TsC in dynamic traffic conditions, in-\ncorporating live GPS data to evaluate performance in\nreal-time ETA updates. We will also focus on refining\nthe spatio-temporal relation graph to enhance its adapt-\nability in varying traffic and geographic patterns. Ad-\nditionally, extending TAS-TsC to multi-modal logistics,\nsuch as combining truck and rail ETA, may further ex-\npand its applicability and improve end-to-end logistics\nefficiency."}]}