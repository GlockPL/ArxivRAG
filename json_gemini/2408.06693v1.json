{"title": "DC3DO: Diffusion Classifier for 3D Objects", "authors": ["Nursena Koprucu", "Meher Shashwat Nigam", "Shicheng (Luke) Xu", "Biruk Abere", "Gabriele Dominici", "Andrew Rodriguez", "Sharvaree Vadgama", "Berfin Inal", "Alberto Tono"], "abstract": "Inspired by Geoffrey Hinton's emphasis on generative modeling (\"To recognize shapes, first learn to generate them\"), we explore the use of 3D diffusion models for object classification. Leveraging the density estimates from these models, our approach, \"Diffusion Classifier for 3D Objects\", dubbed DC3DO, enables zero-shot classification of 3D shapes without additional training. Our method achieves on average 12.5% improvement compared with its multi-view counterparts, demonstrating superior multi-modal reasoning compared to discriminative approaches. DC3DO uses a class-conditional diffusion model trained on ShapeNet. We run inferences on chairs and cars point-clouds. This work underscores the potential of generative models in 3D object classification.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in deep generative models have yielded state-of-the-art (SOTA) performance in both classification and out-of-distribution (OOD) classification for images [18]. Deep generative models are increasingly being utilized for discriminative tasks, demonstrating superior effectiveness across various domains, including images [12], text [7], and tabular data [11, 30]. This progression builds upon the foundational work of Hinton [8], inspired by Oliver Selfridge's \"Pandemonium\" model [36]. While these early researchers focused on generation within the image domain, one could argue that creation should originate in 3D space before extending to the image or text domains. By first generating 3D objects, we can enhance downstream tasks, not only in object classification but also in image and text classification.\n The classification of 3D shapes is increasingly important in fields such as computer vision, robotics, and virtual reality, hence this research. Since traditional methods often struggle to handle the complexity and variability inherent in 3D data, we adopted a diffusion approach [9]. Diffusion models [38], a recent class of likelihood-based generative models, have shown significant promise in various tasks [10, 31, 34] by transforming random noise into coherent data samples through an iterative noising and denoising process.\nFurthermore, today's work in diffusion models [5, 32, 37] showed unmatched results not only on generative tasks [27] but also in classification tasks [26, 28]. Diffusion models belong to a class of generative models that model the data distribution of the dataset, similar to VAES [17], GANS [2, 45], EBMs [47], Score-based models [51]. Therefore, a question arises, Can we use diffusion models for 3D classification tasks? More critically, given their remarkable ability to generate original objects beyond the initial dataset distribution [27, 29, 44, 54, 58], how do these models perform for out-of-distribution (OOD) data. While these models have excelled on standard benchmarks, they often struggle with novel OOD data, a limitation attributed to biased training datasets that fail to encompass the full spectrum of real-world possibilities. This has been attributed to the biased training data that does not represent all real-world possibilities [13]. These deep generative models can synthesize strikingly realistic and diverse images, objects, and text and they have shown better performances in zero-shot [14, 15, 35], few-shot classification tasks [37].\nIn this research, we explore the application of Denoising Diffusion Probabilistic Models (DDPMs) [18] for classifying 3D shapes. Traditional classification methods often fall short with 3D data, requiring novel approaches. Furthermore, 3D data are represented as point clouds [27, 52, 59], voxels [4, 46], signed distance functions [29, 44, 55] and multi-view formats [40]. Inspired by LION [54], this research adopts point cloud and voxel [54, 59] combined with latent representations [29] and diffusion models: DC3DO. DC3DO focuses on leveraging the generative capabilities of diffusion models for zero-shot classification [19]. We compared it against a direct extenstion of the 2D conter-part performed on images [18]. In a dynamic data landscape, the ability to classify data into previously unseen categories, such as architectural structures [39, 42, 43], is of paramount importance. Diffusion models, with their inherent generative strengths, are particularly well-suited for this challenge. Furthermore, by advancing beyond traditional 2D prior models [22\u201324] and incorporating the LION model [54], renowned for generating high-fidelity 3D shapes, we enhance the effectiveness of the Diffusion Classifier in performing discriminative tasks, particularly in 3D object classification. Therefore, our contributions to this field are three-fold:\n\u2022 Novel Method for 3D shape classification: We introduce DC3DO to classify 3D shapes with a diffusion model.\n\u2022 Comparative analysis: We compare our method against multiview 3D representations using a 2D diffusion classifier [18]. We adapted MVCNN's [40] with its view pooling method to a more U-Net and diffusion classifier-friendly method for a fair comparison.\nIn these unsupervised settings, diffusion models [9] are trained using the objectives of Variational Inference, specifically focusing on maximizing the evidence lower bound (ELBO) [41] of the log-likelihood, as described in [18]. This involves adding noise e to a sample, using a neural network to predict the noise, and adopting Mean Squared Error (MSE) or L2 loss to compare this predicted noise against a white gaussian noise [1]."}, {"title": "2. Related Work", "content": "Multimodal large language models (LLMs) strengths are leveraged in many current works [6, 16, 33, 48]. LLMs can handle diverse tasks through conversational interaction, specifically in the context of 3D objects. Typically, this is achieved by training a 3D shape encoder and aligning it with other modalities (e.g., text, images, and audio). The entire pipeline is then fine-tuned during an instruction-tuning phase, resulting in a model that is better aligned with user requests for specific 3D tasks. This fine-tuning stage is conducted using synthetic datasets or captioning datasets.\nThese approaches highlight the vast potential of integrating 3D shapes into foundation models, although they still necessitate the fine-tuning of large models. Other methods, such as 3DAxiesPrompts [21], enhance images and prompts with additional artifacts to be able to exploit the 2D vision abilities of LLM for 3D objects.\nPEVA-Net [20] employs a pre-trained CLIP model in a multiview pipeline to classify 3D objects in zero-shot or few-shot environments. It leverages CLIP's zero-shot classification abilities for each view of the 3D object, subsequently aggregating these results to make the final prediction. Although this approach effectively exploits the zero-shot capabilities of vision-language models (VLMs), transforming 3D shapes into multiview images is an oversimplification that can lead to suboptimal results.\nTAMM [57] demonstrates that when aligning 3D object representations with other modalities, the image modality contributes less significantly than the text modality. To address this, their method learns to separate visual features from semantic features within the 3D object representation, enabling a more effective alignment with the other modalities and enhancing performance in downstream tasks. These findings suggest that the alignment between modalities for integrating 3D representations into existing methods can sometimes be inadequate [50]. Regarding 3D representation learning, Zhang et al. [56] takes a different approach and incorporates 2D guidance. Their work, dubbed I2P-MAE [56], learns advanced 3D representations, achieving state-of-the-art performance on 3D tasks and significantly lowering the need for large-scale 3D datasets. On the contrary concurrent work, DiffCLIP [37] demonstrates that the integration of CLIP and diffusion models for 3D classification facilitates zero-shot classification, achieving state-of-the-art results. This methodology utilizes a pre-training pipeline that incorporates a Point Transformer for few-shot 3D point cloud classification, wherein the CLIP model extracts style-based features of the class, synergistically combined with image features. While DiffCLIP [37] used Point Transformer we used LION, a latent point-voxel [25, 54, 59] representations that leverages a hierarchical two stages diffusion process with state of the art generative performances. Following the line of latent and implicit representations, Xin et al. [53] used a Classifier Score Distillation (CSD) method, which utilizes an implicit classification model for generation."}, {"title": "3. Methodology", "content": "In this section, we present and compare two distinct approaches for 3D object classification: Multi-View Diffusion Classifier (Section 3.1) and DC3DO (Section 3.2). The first approach, the Multi-View Diffusion Classifier, is designed to harness the power of diffusion models while maintaining the architecture of Diffusion Classifier [18]. The second approach, DC3DO, integrates the advanced generative capabilities of LION [54] with diffusion-based classification, targeting zero-shot classification of complex 3D shapes like cars and chairs.\nOur goal is to thoroughly assess the performance of these methods in comparison to traditional and state-of-the-art techniques. The Multi-View Diffusion Classifier (MVDC) offers an alternative to the widely-used MVCNN by employing a majority vote mechanism across multiple 3D views. On the other hand, DC3DO leverages LION's robust generative framework (Section 3.3)."}, {"title": "3.1. Multi-View Diffusion Classifier (MVDC)", "content": "3D objects can be effectively represented as a series of images, providing a straightforward baseline for extending previous work [18] to the 3D domain. By simply aggregating multiple views of the same object, we can adapt existing diffusion-based classification techniques for 3D shapes. For our experiments, we utilized the ShapeNet dataset [3], focusing on a specific subset of 200 models per class. This selection was made due to the computational intensity of performing 1000 diffusion steps (t) per image (Xi), which is particularly challenging in environments with limited GPU resources (poor-gpus settings). Especially if each object is represented by 36 views taken from cameras view 10-degree intervals around a circumference encircling the 3D object, adding to the computational complexity.\nTo classify a single object, our method proposes a majority vote scheme. Let X = {X1, X2, . . ., Xn} represent the set of n views of a 3D shape. Each view X\u1d62 is processed individually by the diffusion model to produce a corresponding prediction yi = f (Xi), where f(\u00b7) denotes the classification function of the diffusion model. Unlike MVCNN [40], which aggregates these views into a global representation through view pooling, our approach maintains the predictions {Y1, Y2, ..., Yn } independently.\nThe final classification decision y* is made by selecting the most common prediction among the individual predictions, formulated as:\n$y^* = mode (y_1, y_2, \u2026\u2026\u2026, y_n)$\nThis majority vote approach retains the architectural integrity of diffusion models while emphasizing simplicity and interpretability. While MVCNN's view pooling may enhance performance by combining features, our goal is to assess the classification power of diffusion models in their unaltered form. Here the 2D Images are processed and encoded into feature maps i \u2208 R^{H\u00d7W\u00d7C}, where H is the height, W is the width, and C is the number of channels in the image (512x 512 \u00d7 3 as the highest resolution in these experiments)."}, {"title": "3.2. Diffusion Classifier for 3D Objects (DC3DO)", "content": "DC3DO consists on the main contribution of our research. Our model combines LION [54] with diffusion classifier [18] for zero-shot classification. By utilizing LION's ability to generate diverse 3D shapes and feeding them into the diffusion classifier, we achieve precise categorization of 3D cars and chairs. This section details DC3DO, emphasizing the advantages of diffusion models, the processing of 3D shapes, and the integration of the LION model to enhance classification accuracy."}, {"title": "3.3. Integrating LION with Diffusion Classifiers", "content": "LION leverages a hierarchical latent space to effectively capture both global and local features of 3D structures, see Figure 2. This hierarchical approach ensures comprehensive encoding of both macro and micro features of 3D object structures."}, {"title": "3.3.1 Integration with Hierarchical Latent Space", "content": "LION's hierarchical latent space encodes 3D point clouds x \u2208 R^{3\u00d7N}, where x consists of N points (2048) with xyz-coordinates in R\u00b3, into a dual-layered latent representation. This representation includes:\n\u2022 Global Latent Space: This vector-valued latent space, denoted as zo \u2208 R^{Dz}, captures the overall structure and large-scale features of the building. It captures the overall spatial structure of the 3D shape, including its large-scale features.\n\u2022 Local Point-Structured Latent Space: This latent space, denoted as ho \u2208 R^{(3+Dh)\u00d7N}, represents a point cloud-structured latent consisting of N points with xyz-coordinates in R\u00b3 and additional Dh latent features per point. This layer captures detailed and fine-grained features."}, {"title": "3.3.2 Integration with Diffusion Models", "content": "The integration process involves several key steps:\nEncoding The 3D point cloud data x is encoded into the global latent space using LION's PVCNN encoder. We found that the global latents contained enough information about the shape and high level features of the object, for the purpose of classification. Also, it is a much smaller latent space compared to the the local point structured latent space, making it easier to work with - considering the diffusion process requires multiple inference steps per sample.\nDiffusion Process After encoding, the data undergoes a first diffusion process (global latent). This involves a fixed forward procedure where Gaussian noise is iteratively added (1000 steps) to the latent representations zo and ho, resulting in the diffused latents zt and ht. The forward diffusion process is defined as:\n$z_t = a_t\u0396_\u03bf + \u03c3_t\u03b5,  \u03b5~ N(0, 1)  (1)$\n$h_t = a_th_\u03bf + \u03c3_t\u03b5, \u03b5~ \u039d(0,1) (2)$\nwhere:\n\u2022 zo and ho are the initial latent representations capturing the global and local features of the 3D shape, respectively.\n\u2022 zt and ht are the diffused latent representations at timestep t.\n\u2022 at and ot are coefficients that control the amount of the original signal and the noise added at each timestep, respectively.\n\u2022 e is the Gaussian noise sampled from a standard normal distribution N (0, I), which introduces randomness to the latent representations.\nDenoising and Classification In the pipeline, a deep neural network, conditioned on class labels c, performs the denoising of the perturbed data. The denoising process involves reversing the forward diffusion to retrieve the latent representations 20 and ho that best match the original data distribution x. Specifically, the network learns to approximate the posterior distributions q$ (Zox, c) and q(ho x, zo, c) by minimizing the reconstruction error.\nThe classification is then performed by evaluating the likelihood pe (xo | c) of the denoised data 20 and ho belonging to specific classes. This likelihood is computed as follows:\n$P_\u03bf (X_\u03bf | c)  = \\int_{X_1:T} \u0440(\u0445_\u0442) \\prod_{t=1}^T  p_\u03b8 (X_{t-1} | x_t, c)  dx_{1:T} (3)$\nwhere\n\u2022 po(xo | c) is the class-conditional likelihood of the original data xo given the class label c.\n\u2022 xT represents the final diffused state, typically modeled as a standard Gaussian distribution.\n\u2022 Po(Xt-1 | Xt, c) denotes the learned reverse process that denoises the data at each timestep t, conditioned on the class label c.\nThe model assesses the denoised data to determine the most likely building category by evaluating which class-specific denoising process best corresponds to the introduced noise, ultimately assigning the data point to the class with the highest likelihood."}, {"title": "3.3.3 Text-Conditioned Diffusion", "content": "Our model employs multi-modal [49] approach to integrate diverse data modalities, providing a comprehensive approach to 3D building classification. The diffusion process is condition on a text prompt \"[C]\" (\"car\", \"chair\", and \"airplane\"). We added an additional text prompt to provide a diffusion process to classify the model as \"non-car\", \"non-chair\", and \"non-airplane\".\nThe integrated modalities include:\n\u2022 3D Point Cloud Data: The primary representation of 3D shapes, capturing spatial distribution and structural details.\n\u2022 Textual Descriptions: Supplementary information describing the architectural features and styles of buildings. These are encoded into vector representations t \u2208 Rd.\nBy integrating these modalities, our model achieves a more informed representation of the data, which improves classification accuracy and robustness. The integration of LION with diffusion models utilizes the combined strengths of both techniques, allowing for precise and reliable classification of 3D building structures."}, {"title": "4. Experimental Results", "content": "4.1. MVDC - 2D Results\nIn our baseline evaluation, we utilized a multi-view diffusion classifier on the ShapeNet dataset, focusing on three classes: cars, chairs, and airplanes. This approach uses multiple views of 3D shapes to enhance classification accuracy by taking advantage of the rich spatial information in the dataset. The process involved encoding 3D shapes into latent representations using a pretrained VAE, adding Gaussian noise, and employing a UNet model for denoising and classification. This way, we captured the intricate details of 3D shapes and effectively categorized them by adaptively selecting the most promising samples based on predicted errors, optimizing overall classification performance."}, {"title": "4.2. DC3DO Inference", "content": "Since DC3DO is based on LION, we took its model weights publicly available. LION has been trained on specific classes from the ShapeNet dataset, with the model weights for the \"chairs\" and \"cars\u201d categories publicly available to the research community. Consequently, we utilized these pretrained models in our experiments. Due to computational constraints, we set the number of diffusion steps for both the Multi-View Diffusion Classifier and LION to 200 steps.\nFor our experiments, we employed a batch size of 1 for the \"cars\" and \"chairs\" categories. Currently, it takes approximately 20 seconds to classify each object."}, {"title": "4.3. Ablation Studies", "content": "To gain deeper insights into the contributions of different components in our model, we conducted ablation studies by systematically disabling specific features and quantifying their impact on performance.\nFor the MVDC model, let S \u00d7 S denote the image size and n the number of views. We observed that the diffusion process time T increases non-linearly with both S and n. Specifically, T x S2 \u00d7 n, indicating that larger image sizes S and an increased number of views n result in significantly slower processing.\nWe ran inference at various image sizes to study its run time performance and relationship to the model performance. First, we confirmed that the inference time grows exponentially with larger image size, for a 512 \u00d7 512 resolution image and 500 sampling steps, the processing time was approximately 1.5 minutes per image, making it infeasible to evaluate at larger scale. Moreover, when we reduced the image size to S = 64 \u00d7 64 or S = 128 \u00d7 128, the classifier's performance degraded severely. The model exhibited a tendency to collapse, consistently predicting a single class c regardless of the input views, suggesting that the classifier lost its ability to differentiate between classes under reduced image resolutions."}, {"title": "5. Limitations", "content": "One of the primary limitations of our approach is the computational cost. The 3D diffusion process currently requires approximately 20 minutes per object on a T4 GPU, making it a time-intensive task. Similarly, the multi-view approach, while effective, is also relatively slow due to the independent processing of each view.\nRegarding the Multi-View Diffusion Classifier, a significant limitation is that the views are processed individually and then aggregated through a majority vote, rather than being combined into a global latent vector as in the approach used by MVCNN [40]. This method of independent view processing may not fully capture the holistic structure of 3D shapes, which could be better represented through a more integrated multi-view approach.\nDue to time and computational constraints, we limited our experiments to 200 shapes per category. With access to more powerful GPUs and additional resources, future work could extend these experiments to a larger number of objects, potentially providing more comprehensive results."}, {"title": "6. Discussion and Future Work", "content": "The high classification accuracy on ID data indicates that the model effectively captures the distinguishing features of various 3D objects.\nThe hierarchical latent space of LION played a crucial role in accurately representing both global and local features of 3D shapes, contributing to the model's overall performance compared the multi-view (see Section 5 for more details). The diffusion process further enhanced the model's ability to denoise and classify complex 3D structures, providing a reliable mechanism for zero-shot classification.\nThese results highlight the potential of integrating generative models like LION with diffusion classifiers for advanced 3D shape analysis and classification tasks, particularly in scenarios involving diverse and unseen data. In fact, in this work, we delved into 3D diffusion models and present our method that enables zero-shot classification of 3D shapes in a robust manner. For future works, we wish to explore 3D diffusion capabilities in state-of-the-art multimodal methods such as ULIP-2 [50], integrated with Point-BERT [52] architectures similar to the concurrent work [37]. We believe this will enhance the performance of these architectures and make them capable of 3D understanding."}, {"title": "7. Conclusion", "content": "In this paper, we propose a model that seamlessly integrates LION [54] with a diffusion classifier [18] to achieve accurate classification of 3D cars and chairs. The model's success is driven by the hierarchical latent space and diffusion process, which together enable precise representation and classification of complex 3D shapes from the ShapeNet dataset [3]. Our approach, named DC3DO, demonstrates a 12.5% improvement on average compared to multi-view methods, highlighting the potential of generative models in 3D object classification. This work suggests that future research could adapt generative models to discriminative tasks, potentially leading to enhanced classification and regression performance."}]}