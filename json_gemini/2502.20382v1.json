{"title": "Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization", "authors": ["Lujie Yang", "H.J. Terry Suh", "Tong Zhao", "Bernhard Paus Gr\u00e6sdal", "Tarik Kelestemur", "Jiuguang Wang", "Tao Pang", "Russ Tedrake"], "abstract": "We present a low-cost data generation pipeline that integrates physics-based simulation, human demonstrations, and model-based planning to efficiently generate large-scale, high-quality datasets for contact-rich robotic manipulation tasks. Starting with a small number of embodiment-flexible human demonstrations collected in a virtual reality simulation environment, the pipeline refines these demonstrations using optimization-based kinematic retargeting and trajectory optimization to adapt them across various robot embodiments and physical parameters. This process yields a diverse, physically consistent, contact-rich dataset that enables cross-embodiment data transfer, and offers the potential to reuse legacy datasets collected under different hardware configurations or physical parameters. We validate the pipeline's effectiveness by training diffusion policies from the generated datasets for challenging long-horizon contact-rich manipulation tasks across multiple robot embodiments, including a floating Allegro hand and bi-manual robot arms. The trained policies are deployed zero-shot on hardware for bimanual iiwa arms, achieving high success rates with minimal human input. Project website: https://lujieyang.github.io/physicsgen/.", "sections": [{"title": "I. INTRODUCTION", "content": "The emergence of foundation models has transformed fields such as natural language processing and computer vision, where models trained on massive, internet-scale datasets demonstrate remarkable generalization across diverse reasoning tasks [1, 2, 3, 4, 5]. Motivated by this success, the robotics community is currently pursuing foundation models for generalist robot policies capable of flexible and robust decision-making across a wide range of tasks [6, 7, 8], leading to significant industrial investments in large-scale robot learning [9]. However, the pursuit for generalist robot policies remains constrained by the limited availability of high-quality datasets, especially for contact-rich robotic manipulation. Existing datasets [7, 10, 11, 12] are orders of magnitude smaller than those used to train foundation models in other domains, such as Large Language Models (LLMs). The scarcity of diverse, high-fidelity manipulation data limits policy generalization across different embodiments, task contexts, and physical conditions.\nTo address data scarcity, robot learning researchers often rely on a spectrum of data sources varying in cost, quality, and transferability. The most informative data typically consists of high-quality demonstrations specific to the task, environment, and embodiment [7, 10], but such data is costly and time-consuming to collect, as it requires human teleoperation with specialized hardware. At the opposite end of the spectrum, there is a wealth of lower-quality data in the form of internet videos showing humans and robots performing manipulation tasks [13, 14, 15, 16]. However, the significant embodiment gap and limited action labeling make this data difficult to transfer effectively to robot policies. Simulation data offers a middle ground, providing the potential to generate large, diverse, and high-quality datasets at relatively low cost [17, 18, 19]. In practice, effective policy learning can be achieved by co-training on a mixture of data from different points along this spectrum, reducing data collection costs while improving generalization [20].\nA key insight in this work is that human demonstrations and model-based planners complement each other in critical ways for generating high-quality robot data. Human demonstrations, though costly to collect, offer valuable global information for solving complex tasks. However, collecting real-world, contact-rich manipulation data through teleoperation is challenging due to the need for precise multi-contact interactions, which are difficult to achieve in practice due to hardware latency, embodiment mismatches between the human and robot, and the fine-grained control required [21]. In contrast, trajectory optimization has demonstrated success in generating locally-optimal trajectories for contact-rich tasks [22, 23, 24], but often relies on global guidance in the form of good initial guesses.\nIn this work, we propose a data generation framework that leverages the strengths of both approaches: human demonstrations can provide global guidance, while trajectory optimization can locally refine these demonstrations to ensure dynamic feasibility. Starting with a small number of human demonstrations collected in a virtual reality (VR) environment, our method uses model-based trajectory optimization to generate large datasets of dynamically feasible, contact-rich trajectories in simulation. The demonstrations guide the planner through complex search spaces, while the planner ensures physical consistency and robustness across varying physical parameters and robot embodiments. Our pipeline, visualized in Fig. 1, enables efficient cross-embodiment data transfer, where demonstrations collected with one robot configuration can be adapted to another, and supports domain randomization for improved generalization and robustness. Additionally, it provides the potential to revive and adapt legacy datasets collected with different hardware or configurations, making old datasets valuable for new robot systems.\nOur key contributions include:\n1) We present an intuitive, embodiment-flexible demonstration interface based on virtual reality and physics simulation, enabling fast data collection for dexterous contact-rich manipulation.\n2) We propose a scalable framework that leverages trajectory optimization to transform a small number of human demonstrations into large-scale, physically consistent datasets, enabling generalization across embodiments, initial conditions, and physical parameters.\n3) We validate our approach by training policies on the generated dataset for challenging contact-rich manipulation tasks across multiple robot platforms, including bimanual robot arms and a floating base Allegro hand.\n4) We achieve high success rates in zero-shot hardware deployment on bimanual iiwa arms, highlighting the utility of augmented datasets in real-world scenarios."}, {"title": "II. RELATED WORKS", "content": "In this section, we review the most relevant approaches for generating diverse robot data for contact-rich tasks. We categorize the methods into data collection, data augmentation, model-based planning, demonstration-guided reinforcement learning and cross-embodiment transfer."}, {"title": "A. Data Collection for Imitation Learning", "content": "Behavior Cloning [25], which trains robot policies to mimic expert behavior, has shown impressive empirical results in a wide range of dexterous manipulation tasks [26]. Collecting high-quality robot data has been an essential component of imitation learning (IL). Many such methods rely on human experts teleoperating a robot to accomplish specific tasks. Researchers have adopted interfaces such as 3D spacemouse [26, 27], and puppeteering platforms [28, 29] for end-effector [7] and whole-body control [30, 31].\nVirtual and augmented reality (VR/AR) interfaces have recently gained traction as effective alternatives for robot data collection [32, 33], reducing cognitive load, physical strain, and user frustration compared to traditional techniques like kinesthetic teaching or 3D mouse control [34]. These technologies offer a more intuitive data collection paradigm for complex tasks, especially in dexterous manipulation. AR2-D2 [35] enables data collection without a physical robot by projecting a virtual robot into the physical workspace, but lacks real-time feedback necessary for precise control. DART [36] supports data collection entirely in simulation, visualized through a VR headset, but faces challenges bridging the sim-to-real gap for physical robot deployment. ARCap [37] integrates real-time AR feedback, but requires specialized hardware, including an RGBD camera, motion capture gloves, and VR controllers, in addition to the AR headset. ARMADA [38] enables real-world manipulation data collection with bare hands through real-time virtual robot feedback, achieving high success rates when replayed on physical hardware. In contrast to these existing systems, our work focuses on scalable data generation from a small number of human demonstrations by leveraging trajectory optimization, facilitating generalization across different robot embodiments, initial conditions, and physical parameters."}, {"title": "B. Data Augmentation", "content": "Despite many research efforts, collecting large datasets remains time-consuming and costly, requiring a large amount of human effort and resources. To address these challenges, significant effort has been devoted to automating the data generation process through data augmentation techniques. Existing approaches have leveraged state-of-the-art generative models for visual [39, 40, 41] and semantic [42, 43, 44] augmentations. MimicGen [45] and its bimanual extension DexMimicGen [46] automatically synthesize large-scale datasets from a small number of human demonstrations. These works decompose long-horizon tasks into object-centric subtasks and replay transformed demonstrations open loop in simulation. SkillMimicGen [47] extends this paradigm by segmenting tasks into motion and skill components, augmenting local manipulation skills with MimicGen-style replay and using motion planning to connect these skill segments. RoboCasa [48] leverages generative models to create diverse kitchen scenes with abundant 3D assets and utilizes MimicGen for automated trajectory generation. While these approaches have shown success in automating data generation, they primarily rely on kinematic replay of demonstrations, which is often inadequate for contact-rich manipulation tasks. Our work can be viewed as an important extension to MimicGen line of works to support dynamically feasible contact-rich data generation, which requires fine-grained control of the robot and continuous reasoning about making and breaking contacts with the environment."}, {"title": "C. Trajectory Optimization for Contact-Rich Tasks", "content": "Planning and control through contact remains a significant challenge for both learning-based and model-based methods due to the explosion of contact modes and the nonsmooth nature of contact dynamics. To tackle these challenges, researchers have explored various trajectory optimization formulations for multi-contact interactions.\nContact-Implicit Trajectory Optimization Existing works based on contact-implicit trajectory optimization (CITO) [23, 22] have sought to formulate the combinatorial problem into a smooth optimization problem by using complementarity constraints. CITO has been applied in various domains, including planar manipulation [49, 50], dynamic pushing [51], and locomotion tasks [52, 53, 54]. Recent efforts have extended CITO for real-time applications as model predictive control (MPC) [55, 56], with successful hardware deployment on quadrupeds using tailored solvers [57, 58]. Aydinoglu et al. [59] parallelize the solution of linear complementarity problems using alternating direction method of multipliers (ADMM) and validate the method on hardware for multi-contact manipulation tasks. While CITO shows promising scalability for handling contact modes, it suffers from poor global exploration and relies on good initial guesses [60]. A new line of work tries to address these issues with efficient global optimization [61], but does not yet scale to the tasks we consider here.\nSampling-Based Planning Sampling-based methods have also shown great promise for solving trajectory optimization for contact-rich tasks. H\u00e4m\u00e4l\u00e4inen et al. [62] employ sampling-based belief propagation for humanoid balancing, juggling and locomotion. Carius et al. [63] extend the path integral formulation to handle state-input constraints and validate the approach on quadruped stabilization on hardware. More recently, Pezzato et al. [64] applied sampling-based predictive control (SPC) for simpler contact tasks like pushing, while Howell et al. [65] and Li et al. [66] extended SPC to more complex, contact-rich tasks such as in-hand cube reorientation. Pang et al. [67] use smoothed contact dynamics with global sampling to generate contact-rich plans in under a minute, with performance comparable to reinforcement learning. Cheng et al. introduce HiDex [68], a hierarchical planner that combines Monte-Carlo Tree Search with integrated contact projection, achieving rapid planning for dexterous manipulation tasks. Interestingly, directly applying sampling-based planners for contact-rich data generation in behavior cloning can be problematic, as the high entropy of the generated trajectories often degrades downstream policy performance [69, 70].\nIn this work, we leverage low-entropy human demonstrations to guide the global planning for multi-contact interactions and utilize trajectory optimization to locally refine the trajectories for specific physical parameters and robot embodiments. From a small number of demonstrations, the model-based planner can efficiently generate abundant, high-quality, contact-rich data for training robust robot policies."}, {"title": "D. Demonstration-Guided Reinforcement Learning", "content": "While IL often demands a large number of expert demonstrations to achieve robust and high-performing policies, reinforcement learning (RL) aims to solve tasks autonomously through reward-driven exploration. However, pure RL can suffer from inefficient exploration and the need for extensive reward shaping, especially in complex manipulation tasks [71, 72]. To address these challenges, researchers have explored using demonstrations to guide RL, improving both sample efficiency and exploration quality.\nDemonstrations have been integrated into RL pipelines in various ways, including adding them directly to the replay buffer [73, 74], using behavior cloning for policy pretraining [75, 76, 77], and augmenting task rewards with information extracted from demonstrations [78, 79, 80]. Sleiman et al. [81] guide RL with demonstrations generated from a model-based trajectory optimizer for multi-contact loco-manipulation tasks, and validate their method on hardware with a quadrupedal mobile manipulator. While these approaches search over the parameters of a neural network policy and potentially optimize a more global objective, we leverage trajectory optimization as a complementary tool to locally refine and expand demonstration trajectories. This enables the efficient generation of contact-rich data while avoiding the computational overhead, approximation errors, and unnecessary exploration associated with RL's high-dimensional search space."}, {"title": "E. Cross-Embodiment Generalization", "content": "Reusing datasets and policies across different embodiments unlocks the potential for large-scale robot learning. One line of work learns latent plans from videos of humans interacting with the environment and transfers this knowledge for robotic manipulation [82, 83]. Another approach involves portable data collection tools, such as hand-held grippers [21, 84], for in-the-wild human demonstrations. While these methods enable policy deployment on multiple robot platforms, they are often constrained to robots with the same end-effector used during data collection, limiting generalization across platforms. On the other hand, to leverage large-scale datasets, recent works pull data from a heterogeneous set of robots ranging from navigation to manipulation, and train a robotic foundation model capable of accomplishing a diverse range of tasks [85, 86]. Our proposed framework enables reusing the same set of easy-to-collect demonstrations for multiple robots, avoiding the need to collect embodiment-specific data for contact-rich tasks."}, {"title": "III. DATA COLLECTION", "content": "We present a Virtual Reality (VR)-based data collection pipeline designed for intuitive and efficient collection of human demonstrations across multiple robot embodiments. The pipeline emphasizes simplicity and cross-embodiment generalization while minimizing the reliance on physical robot hardware. While we consider the data collection pipeline to be one of our contributions, we emphasize that the simulation-based large-scale data generation method presented in the next section is independent of this particular data collection approach.\nOur data collection pipeline (Fig. 2) is a human-hand demonstration interface in VR. We use an Apple Vision Pro to track the poses of the human demonstrator's hands and stream the poses to the Drake physics simulator [87], which simulates the contact interaction between the object and the hands. The updated object pose is then sent back to Apple Vision Pro for real-time visualization in VR using Vuer [88].\nOur demonstration interface is fast and cost-effective. Since the system operates entirely in simulation, it removes the dependency on robot hardware, significantly reducing the cost and complexity of data collection. In practice, it takes approximately 7 minutes to collect 24 long-horizon demos for each considered system. The setup is also intuitive to use, as the human demonstrator does not have to mentally close the embodiment gap between the human body and the specific robot.\nWe demonstrate our pipeline on two different classes of robot embodiments: a dexterous hand and a bimanual manipulation setup.\nFloating Allegro Hand For the dexterous hand, we consider a 22-DOF free-floating Allegro hand manipulating a cube on a table as shown in Fig. 3. Since the Allegro hand only has four fingers, we restrict the VR-based demonstrations to using four fingers on the right hand to interact with the object in simulation.\nBimanual Robot Arms For the bimanual manipulation setup, we consider two different fixed-base bimanual manipulators: a pair of 7-DOF Kuka LBR iiwa arms, and a pair of Franka Emika Panda arms. Each pair of arms collaboratively manipulates a big box (Fig. 3). During the VR demonstrations, the human demonstrator uses both index fingers to manipulate a small cube in VR and constrains their wrist movement to mimic the fixed base. During kinematic motion retargeting (detailed in Sec. IV-A), the small cube and fingers are scaled to match the size of the larger box and the robot manipulators.\nThis design facilitates two forms of cross-embodiment generalization. First, it utilizes easy-to-collect human finger demonstrations to guide planning for harder and higher-dimensional tasks, such as the dual-arm manipulators. Second, it supports the reuse of the same set of demonstrations across multiple robot platforms, as both the iiwa and Panda arms can leverage the same data to accomplish the manipulation task, eliminating the need for embodiment-specific demonstrations."}, {"title": "IV. AUTOMATED DATA GENERATION", "content": "In this section, we present our method for automatically generating large quantities of physically feasible trajectories for contact-rich manipulation tasks across a range of objects, initial conditions, and embodiments from only a handful of demonstrations. The presented method also offers the potential to adapt legacy datasets collected using outdated configurations to new robot settings, reducing the cost of collecting large amounts of data on the new robot setups from scratch.\nOur method starts out by retargeting kinematic motions from the original embodiment-flexible human demonstrations collected in VR to the specific robot embodiment in simulation, producing kinematically feasible trajectories. These trajectories are then refined and augmented through the use of local trajectory optimization to obtain dynamically feasible trajectories for a range of physical parameters. The following subsections provide a detailed breakdown of each step in the pipeline."}, {"title": "A. Kinematic Motion Retargeting", "content": "Given a sequence of demonstrations $x_{0:T}^{demo}$ with horizon $T$, we aim to find the robot configurations $q_{0:T}^{retarget}$ that match the positioning of the demonstrator while avoiding penetration and obeying joint limits. At each time step, we solve the following nonconvex program:\n$\\begin{aligned}\n\\mathbf{q}_{t}^{\\text {retarget* }}=\\arg \\min _{\\mathbf{q}_{t}^{\\text {retarget }}} & \\sum_{i=0}^{N} w_{i}\\left\\|V_{i}\\left(\\mathbf{q}_{t}^{\\text {retarget }}\\right)-V_{i}\\left(\\mathbf{x}_{t}^{\\text {demo }}\\right)\\right\\|^{2} & (1 a) \\\\\n\\text { s.t. } & \\Phi_{j}\\left(\\mathbf{q}_{t}^{\\text {retarget }}\\right) \\geq 0, \\forall j & (1 b) \\\\\n& \\mathbf{q}_{\\min } \\leq \\mathbf{q}_{t}^{\\text {retarget }} \\leq \\mathbf{q}_{\\max }, & (1 c)\n\\end{aligned}$\nwhere $w_i > 0$ are weight parameters, and $V_i$ and $V_i$ represent the $i$-th mappings from the robot configuration and demonstrator state to corresponding points on the embodiments. The corresponding points of interest for each robot/demonstrator pair are manually defined. For example, on the bimanual robot arm system, $V_0$ is the forward kinematics from the robot joint angles to the left robot arm's end effector position, while $V_0$ is a map from the hand pose to the fingertip of the left index finger. We find the resulting plans generated by trajectory optimization relatively robust to the correspondence and weight parameter selection. $\\Phi_j$ denotes the signed distance function between the $j$-th collision pair and (1b) enforces non-penetration constraints. $q_{min}$ and $q_{max}$ are the lower and upper bounds on the joint angles. Notice that $q^{retarget}$ and $x^{demo}$ can have different dimensions as long as both $V_i$ and $V_i$ map them to vectors in the same space (e.g., Apple Vision Pro captures 5 landmarks on the index finger while each robot arm has 7 DOF in the bimanual robot arm system). We solve (1) using a Sequential Quadratic Programming (SQP)-style algorithm: during each iteration, the nonpenetration constraint (1b) is linearized and the matching objective (1a) is quadratically approximated around the solution to the previous iteration. We warmstart the solution of the nonlinear program at time t with the optimal solution from the previous timestep $\\mathbf{q}_{t-1}^{\\text {retarget*}}$ to encourage faster convergence and temporal consistency."}, {"title": "B. Demonstration-Guided Trajectory Optimization", "content": "The kinematically consistent robot trajectories $\\mathbf{q}_{0: T}^{\\text {retarget* }}$ are generally not dynamically feasible due to the embodiment gap and differences in physical parameters. However, they can provide good guidance on generating dynamically feasible trajectories with complex multi-contact interactions. In particular, human demonstrations provide global information about when and where to make contact with the object, which model-based planning can then locally refine. We define the retargeted system state $\\mathbf{x}_{t}^{\\text {retarget }}$ to incorporate both the object state $\\mathbf{x}_{t}^{\\text {object }}$ which is a subset of $\\mathbf{x}_{t}^{demo}$, and the robot state as a function of $\\mathbf{q}_{t}^{\\text {retarget* }}$. The trajectory $\\mathbf{x}_{0: T}^{\\text {retarget }}$ is then locally refined by solving the following nonconvex optimization program:\n$\\begin{aligned}\n\\mathbf{X}_{t}, \\mathbf{U}_{t}=\\arg \\min _{\\mathbf{X}_{t}, \\mathbf{U}_{t}} & \\sum_{t=0}^{T-1}\\left(\\left\\|\\mathbf{x}_{t}-\\mathbf{x}_{t}^{\\text {retarget }}\\right\\|_{Q_{t}}^{2}+\\left\\|\\mathbf{u}_{t}\\right\\|_{R_{t}}\\right)+\\left\\|\\mathbf{x}_{T}-\\mathbf{x}_{T}^{\\text {retarget }}\\right\\|_{Q_{T}}^{2} & (2 a) \\\\\n\\text { s.t. } & \\mathbf{x}_{t+1}=f\\left(\\mathbf{x}_{t}, \\mathbf{u}_{t}\\right) & (2 b) \\\\\n& \\Phi_{j}\\left(\\mathbf{x}_{t}\\right) \\geq 0, \\forall j & (2 c) \\\\\n& \\mathbf{x}_{\\min } \\leq \\mathbf{x}_{t} \\leq \\mathbf{x}_{\\max } & (2 d) \\\\\n& \\mathbf{u}_{\\min } \\leq \\mathbf{u}_{t} \\leq \\mathbf{u}_{\\max }. & (2 e)\n\\end{aligned}$\nHere, $f$ is obtained by time-stepping the dynamics engine, $\\mathbf{x}_{\\min } / \\mathbf{x}_{\\max }\\left(\\mathbf{u}_{\\min } / \\mathbf{u}_{\\max }\\right)$ are the lower and upper bounds on the state (input), $Q_t$, $R_t$ are the cost matrices for the state and input, respectively, and $Q_T$ is the cost matrix for the terminal state. To encourage precise tracking of the object trajectory, we assign higher weights to the entries of $Q_t$ which correspond to $\\mathbf{x}^{\\text {object }}$. The detailed parameters can be found in Appendix IX-A.\nIn general, model-based planners can struggle to discover high-quality long-horizon contact-rich trajectories without demonstrations. CITO requires good initial guesses and can easily get stuck in local optima without making progress. Human demonstrations offer valuable global guidance that helps overcome these challenges, and $\\mathbf{x}_{0: T}^{\\text {retarget}}$ can naturally serve as the initial guess to CITO-based methods where local adjustments are made to obey dynamical constraints (2b).\nThanks to access to the system dynamics $f$ in simulation, we can locally perturb the physical parameters as well as robot and object states around a nominal demonstration. From the single demonstration, we can solve (2) for a distribution of tasks with different dynamics $f(\\mathbf{x}_t, \\mathbf{u}_t, \\theta_t)$, where $\\theta_t \\sim \\rho$ represents all the perturbations. We assume the kinematically retargeted trajectory $\\mathbf{x}_{0: T}^{\\text {retarget }}$ still provides good guidance on achieving the task in the vicinity of the nominal demonstration. This way, a large number of physically consistent trajectories with various physical properties and initial conditions can be generated from a single human demonstration. We outline our data generation pipeline in Algorithm 1."}, {"title": "V. TRAJECTORY OPTIMIZATION EXPERIMENTS", "content": "While kinematic retargeting of demonstrations might suffice to generate data for simpler manipulation tasks such as pick-and-place, it often falls short for the more challenging contact-rich tasks requiring frequent contact mode switches and fine-grained actions. In this section, we demonstrate that trajectory optimization is crucial for generating diverse, dynamically feasible contact-rich trajectories on three high-dimensional dexterous manipulation systems: a floating Allegro hand, bimanual iiwa arms, and bimanual Panda arms.\nOur data generation framework is agnostic to the choice of the trajectory optimizer. We implement the cross-entropy method (CEM) [89] to solve (2) over a distribution of physical parameters and initial conditions, as specified in Table I.\nTask Manipulating the object to a target pose on the table (Fig. 6). The object is initially placed randomly on the table with an arbitrary face upward. Task success is defined as the object reaching within 3 cm and 0.2 rad of the target pose for the Allegro hand, and within 10 cm and 0.2 rad for the bimanual robot arms. This task requires long-horizon reasoning of complex multi-contact interactions between the robot and the object. The necessary frequent contact mode switches and high-dimensional action space pose great challenges for traditional model-based planners, while the precise contact interactions require fine-grained control actions.\nDynamic Feasibility While kinematic motion retargeting can generate visually plausible robot and object trajectories, these trajectories often lack dynamical consistency due to the differences in physical parameters and embodiment between the human demonstrator and the target robot. To illustrate this, we replay the kinematically retargeted trajectories of the original 24 human demos and record the success rates for each system in Table II. Furthermore, we randomly sample object sizes and perturbations of initial object poses according to Table I and roll out the nominal kinematically retargeted trajectories. Some trajectories still succeed under certain perturbations thanks to caging grasps or other strategies that encourage robustness during the human demonstration. For all the systems, the successful rollouts are relatively short, manipulating the object to the goal pose within only 1 or 2 rotations.\nThe low success rate of purely kinematically retargeted trajectories highlights the importance of trajectory optimization for locally refining the demos for the particular embodiments and physical parameters. Before trajectory optimization, the floating Allegro hand lightly touches the cube and easily loses contact when rotating it clockwise (demonstrated in Fig. 4a). After trajectory optimization, the hand increases the contact area, establishing a stable grip for rotation. In Fig. 4b, similar behavior that encourages contact can be observed for the bimanual iiwa arms: the demo trajectory tries to rotate the box clockwise only using a single arm, while trajectory optimization encourages the other arm to help hold the box and reorient the box more stably. These refinements that encourage contact are particularly helpful when the object is heavier or smaller, or when the friction coefficients are lower than expected. In addition, replaying the kinematically retargeted trajectory often fails when the object pose deviates slightly from the demonstration, driving the object out of reach (visualized in Fig. 4c). In contrast, trajectory optimization accounts for the system's true dynamics and can adjust the robot's actions accordingly. The success rates of trajectory optimization under random perturbations in physical parameters and object initial conditions for each system are recorded in Table II.\nCross-Embodiment Generalization We demonstrate that a single set of human demonstrations can be effectively repurposed to generate dynamically consistent, contact-rich trajectories across different robotic embodiments with varying task horizons. Specifically, human demonstrations involving two index fingers manipulating a small cube are retargeted to fixed-base bimanual Kuka LBR iiwa and Franka Emika Panda arms manipulating a larger box (visualized in Fig. 3). This approach addresses key challenges in data collection for contact-rich tasks: directly teleoperating two real robot arms to flip a large box would be both physically demanding and cost-prohibitive due to hardware latency, limited feedback, and the embodiment gap-differences in kinematic structure, degrees of freedom, and workspace between human and robotic arms. In contrast, performing the same task on a smaller scale using human fingers is more intuitive, reduces physical effort, and enables faster, more consistent demonstration collection.\nThe iiwa and Panda arms differ in contact geometry, velocity limits, and joint constraints, all of which are explicitly modeled within the trajectory optimization framework described in (2). For safe hardware deployment, we enforce conservative velocity limits on the iiwa arms, while only applying soft velocity regularization on the Panda arms in simulation to allow for more aggressive motions.\nData Diversity Trajectory optimization efficiently augments a single demonstration to a wide distribution of trajectories with locally perturbed physical parameters and initial conditions as visualized in Fig. 5. The diverse states in the generated dataset cover a larger training distribution and encourage smoother learned policies, as will be discussed in the next section."}, {"title": "VI. BEHAVIOR CLONING EXPERIMENTS", "content": "We illustrate our framework's capability to efficiently produce diverse, high-quality contact-rich datasets for training behavior cloning policies across multiple robotic platforms, including the floating Allegro hand and the bimanual Panda arms in simulation as well as bimanual iiwa arms on hardware. We show that policies trained on the generated data generalize to a wide distribution of physical parameters and initial conditions, and are much more robust and performant than the ones trained only on the original demonstrations."}, {"title": "A. Policy Evaluation in Simulation", "content": "From only 24 human demonstrations, our data generation pipeline can efficiently generate thousands of dynamically feasible contact-rich trajectories using trajectory optimization. We train state-based diffusion policies [26] on the 24 original demo trajectories, as well as 500 and 1000 generated trajectories. While our method is compatible with any Behavior Cloning algorithm, we adopt diffusion policies due to its recent success in contact-rich tasks [21, 70, 90]. Fig. 6 visualizes the policy rollouts. We evaluate the performance by conducting 48 policy rollouts for each embodiment in simulation and record the success rates in Fig. 8. The success criteria are the same as specified in the trajectory optimization experiments.\n1) Floating Allegro Hand: While the human demonstrator completes the task in approximately 5 seconds on average in the virtual reality environment, the demonstration trajectories are temporally scaled by a factor of 2.5 to ensure smoother, dynamically feasible motions on the floating Allegro hand, which is subject to velocity limits. We define the task horizon as 25 seconds to allow the policy sufficient time to recover from missed contacts and other errors during the execution. The task complexity arises from the 22-dimensional action space of the Allegro hand and the long-horizon nature of the task, which requires a sequence of coordinated rolling, pitching, and yawing actions to reorient the cube to an upright position. These factors together present significant challenges for traditional model-based planners without guidance.\nThe baseline behavior cloning policy trained on the original set of 24 demonstrations achieves a success rate of 10/48 = 21% and exhibits significant jittery behavior when encountering out-of-distribution states. The workspace, characterized by diverse object orientations and translations, is sufficiently large that minor deviations during policy rollouts often drive the trajectory out of the demonstrated distribution. Common failure modes include the Allegro hand repeatedly missing contact with the cube or becoming stuck on its surface while attempting reorientation (visualized in Fig. 7a), which often result in the object being trapped in intermediate orientations. In contrast, policies trained on the expanded dataset generated by our pipeline demonstrate a higher likelihood of re-establishing contact with the object after initial misses, resulting in significantly improved success rates up to 39/48 = 81%.\n2) Bimanual Robot Arms: The baseline policy trained on the original set of 24 human demonstrations achieves a success rate of 27/48 = 56% on the bimanual iiwa system. We hypothesize that the restrictive velocity limits encourage more quasi-static behavior, leading to longer trajectories with a higher density of state-action pairs in the training data. In contrast, the baseline policy yields a success rate of 14/48 = 29% on the bimanual Panda system, likely due to the more dynamic nature of the learned behavior under its looser velocity constraints. Both baseline policies exhibit remarkably jittery motion, frequently kicking the box out of reach, losing contact, or running into and getting stuck on the box surface during reorientation (visualized in Fig. 7b and c). Policies trained on the augmented dataset, however, generate significantly smoother trajectories and are capable of re-establishing contact with the object after initial misses, resulting in as high as 44/48 = 92% success rates for bimanual iiwa arms and 42/48 = 87.5% for bimanual Panda arms. Additionally, the learned policies capture multimodal behaviors observed in the original human demonstrations, such as rotating the box either clockwise or counterclockwise for similar object poses."}, {"title": "B. Policy Evaluation on Hardware", "content": "We zero-shot deploy the trained policies on hardware for bimanual iiwa arms to flip a 30 cm cubic box on a table (Fig. 9). An OptiTrack motion capture system is employed to estimate the object pose. The baseline behavior cloning policy only achieves 6/23 = 26% success rate, with most successful rollouts being relatively short-horizon, involving only 1 or 2 rotations. Common failure modes of the baseline policy include: 1) deviation from the demonstration trajectory, causing the arms to collide with the box surface (Fig. 10a), and 2) significant box sliding during rolling, resulting in the policy encountering out-of-distribution states and failing to recover (Fig. 10b). In contrast, as shown in Fig. 8b, the policy trained on 500 generated trajectories achieves 17/23 = 74% success rate, while the policy trained on 1000 generated trajectories achieves 16/23 = 70% success rate. Despite occasional box sliding during rolling, these policies demonstrate an improved ability to stabilize the box by using one arm to hold the opposite side more firmly to prevent further sliding (Fig 10d). However, as visualized in Fig 10c, both policies trained on the augmented datasets exhibit failure modes originating from unmodeled collision geometries on iiwa arms, which lead to significant undesired yaw motions of the box during pitch actions."}, {"title": "VII. LIMITATIONS AND FUTURE WORK", "content": "While our method efficiently generates abundant contact-rich trajectories, several limitations remain. First, although our human-hand demonstration framework is fast and intuitive, it may not fully exploit the kinematic capabilities of the target robot, such as continuous joint rotation or specialized dexterous maneuvers. Future work could explore the application of our automated data generation framework to embodiment-aware legacy datasets, better capturing the unique motion capabilities of different robotic systems.\nSecond, although our method demonstrates strong performance in the vicinity of the demonstration due to trajectory optimization, the learned policies struggle to recover from states far outside the demonstrated regions, such as those resulting from catastrophic failure. Future work could explore more advanced planning techniques to iteratively improve the learned policies' robustness in unvisited regions of the state space.\nThird, we have demonstrated the effectiveness of our pipeline primarily for training robust state-based policies. Extending the framework to train visuomotor policies by incorporating high-quality synthetic rendering from simulation could further improve policy transferability to real-world scenarios."}, {"title": "VIII. CONCLUSION", "content": "In this work, we present a novel, cost-effective pipeline that combines physics-based simulations, human demonstrations, and model-based planning to address data scarcity in contact-rich robotic manipulation tasks. A key insight"}]}