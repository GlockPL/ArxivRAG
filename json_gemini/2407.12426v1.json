{"title": "Sharif-STR at SemEval-2024 Task 1: Transformer as a Regression Model\nfor Fine-Grained Scoring of Textual Semantic Relations", "authors": ["Seyedeh Fatemeh Ebrahimi", "Karim Akhavan Azari", "Amirmasoud Iravani", "Hadi Alizadeh", "Zeinab Sadat Taghavi", "Hossein Sameti"], "abstract": "Semantic Textual Relatedness holds significant\nrelevance in Natural Language Processing, find-\ning applications across various domains. Tra-\nditionally, approaches to STR have relied on\nknowledge-based and statistical methods. How-\never, with the emergence of Large Language\nModels, there has been a paradigm shift, ush-\nering in new methodologies. In this paper, we\ndelve into the investigation of sentence-level\nSTR within Track A (Supervised) by leveraging\nfine-tuning techniques on the RoBERTa trans-\nformer. Our study focuses on assessing the\nefficacy of this approach across different lan-\nguages. Notably, our findings indicate promis-\ning advancements in STR performance, par-\nticularly in Latin languages. Specifically, our\nresults demonstrate notable improvements in\nEnglish, achieving a correlation of 0.82 and\nsecuring a commendable 19th rank. Similarly,\nin Spanish, we achieved a correlation of 0.67,\nsecuring the 15th position. However, our ap-\nproach encounters challenges in languages like\nArabic, where we observed a correlation of only\n0.38, resulting in a 20th rank.", "sections": [{"title": "1 Introduction", "content": "STR delineates the meaningful association between\nlinguistic units, showcasing conceptual proxim-\nity within a shared semantic frame (Taieb et al.,\n2019; Abdalla et al., 2021). For instance, \"cup\"\nand \"coffee\" are related in meaning, yet they are\nnot synonymous (Jurafsky and Martin, 2009). De-\nspite its crucial role in various NLP applications\nsuch as Spelling Correction, Word Sense Disam-\nbiguation, Plagiarism Detection, Opinion Mining,\nand Information Retrieval (Franco-Salvador et al.,\n2016; Chen et al., 2017; Taieb et al., 2019), STR\nhas garnered less attention compared to Semantic\nTextual Similarity (STS) due to a scarcity of avail-\nable datasets. Addressing this gap, Abdalla et al.\n(2021), and Ousidhoum et al. (2024a) contributed\nto the field by constructing the first sentence-level\nSTR datasets. In this paper, we endeavor to tackle\nthe STR problem within shared Task 1(Ousidhoum\net al., 2024b), Track A, leveraging supervised data\nin English, Spanish, and Arabic languages provided\nby Ousidhoum et al. (2024a). Additionally, we\nbriefly explore Track C and provide supplementary\ndetails in Appendix B as a secondary objective.\nBuilding upon the findings of Abdalla et al.\n(2021), which underscore the superior performance\nof fine-tuning Transformer models in supervised\ntasks, our proposed system captures the relation-\nship among sentences by fine-tuning the RoBERTa\nTransformer (Liu et al., 2019). At the core of our\nsystem, we employ a pre-trained RoBERTa model\nas a regression model and fine-tune it to generate\na floating-point value for the input text. During\nthe pre-training process of RoBERTa, the emphasis\nis placed on tasks related to NLU. This involves\nexposing the model to a diverse range of linguistic\ncontexts and training it to comprehend the nuances\nof language. Furthermore, the integration of a Clas-\nsifier Head enables sentence classification, a pivotal\naspect of our system architecture elaborated upon\nin section 3.\nOur experimental results showcase promising\nperformance on English and Spanish datasets,\nachieving respective correlation rates of 0.82 and\n0.67 on test data, surpassing the baseline correla-\ntion set by SemEval-2024 at Subtask A (Ousid-\nhoum et al., 2024b). However, the model's per-\nformance on Arabic data falls short, yielding only\na 38% correlation on development data. We at-\ntribute this discrepancy to differences in the under-\nlying RoBERTa model and its training methodol-\nogy across Latin and non-Latin languages, a topic\nfurther explored in section 5. To promote repro-\nducibility and facilitate future research endeavors,"}, {"title": "2 Background", "content": "2.1 Dataset Overview\nThe SemEval-2024 Task 1 is structured into Tracks\nA, B, and C, each tailored to specific methodologies\nand objectives. Our focus lies on Track A (Super-\nvised), which utilizes labeled data to train STR\nsystems. The datasets for Task 1 encompass train-\ning, development, and test sets across 14 languages,\neach comprising sentence pairs (Ousidhoum et al.,\n2024a). Each sentence pair is annotated with a\nsemantic relatedness score, ranging from 0 (indi-\ncating no relatedness) to 1 (suggesting strong relat-\nedness). Participants are tasked with predicting the\ndegree of semantic relatedness between sentence\npairs, crucial for furthering research in NLP.\n2.2 Related Work\nThe exploration of sentence-level STR has been\nhindered by the scarcity of available datasets\n(Abdalla et al., 2021). Existing datasets, such\nas those compiled by Finkelstein et al. (2002),\nGurevych (2006), Panchenko et al. (2016), and\nAsaadi et al. (2019), predominantly focus on\nunigram and bigram STR. However, the seminal\nworks of Abdalla et al. (2021), and Ousidhoum\net al. (2024a) paved the way for further research\nby constructing the first sentence-level STR\ndatasets. Traditionally, both STR and STS have\nbeen approached using knowledge-based and\nstatistical methods (Sadr, 2020; Chandrasekaran\nand Mago, 2020). Notable efforts include the\napplication of knowledge bases such as thesauri,\nontologies, and dictionaries for STR, as surveyed\nby Salloum et al. (2020). Statistical methods,\non the other hand, leverage features extracted\nfrom corpora, with prominent examples including\nLatent Dirichlet Allocation (LDA) by Blei et al.\n(2009) and Latent Semantic Analysis (LSA) by\nLandauer and Dumais (2008) for topic modeling.\nIn recent years, the application of deep learning\nmethodologies has surpassed traditional ap-\nproaches in STS tasks. Noteworthy advancements\ninclude the Tree-LSTM model proposed by Tai\net al. (2015), which outperformed other neural net-\nwork models in SemEval-2014. He and Lin (2016)\nintroduced a hybrid architecture of Bi-LSTM and"}, {"title": "3 System Overview", "content": "In this section, we present a comprehensive\noverview of our system's architecture, outlining\nthe key algorithms and modeling decisions that\nunderpin our model.\n3.1 Core Algorithms and System Architecture\nOur system harnesses the Transformer architec-\nture for its ability to capture long-range depen-\ndencies. At its core, we harness the power of a\npre-trained RoBERTa model (Liu et al., 2019) for\nregression analysis, tailoring its parameters to accu-\nrately predict a floating-point value from the input\ntext. While RoBERTa isn't explicitly trained for\nsentence relatedness scoring, its training encom-\npasses an understanding of the relatedness of sen-\ntences within discourse, rendering it suitable for\nour task.\nDuring the pre-training process of RoBERTa, the\nemphasis is placed on tasks related to NLU. This\ninvolves exposing the model to a diverse range of\nlinguistic contexts and training it to comprehend\nthe nuances of language. Our word embeddings\nutilize an embedding matrix with a dimensionality\nof 768. Position embeddings and token type em-\nbeddings further contribute to the model's compre-\nhension of sequential and contextual information\nwithin the input data.\nThe RobertaEncoder comprises a stack of 12\nidentical RobertaLayers, each employing a multi-\nhead self-attention mechanism. This mechanism\nenables the model to concurrently absorb different\nparts of the input sequence, showing promise in\nanalyzing similarities between various inputs. Fol-\nlowing the attention mechanism are intermediate\nsub-layers and output sub-layers. The intermediate\nsub-layer employs a fully connected feed-forward\nnetwork with a GELU activation function, while\nthe output sub-layer is responsible for proper trans-\nformation and normalization of features.\nThe classification head, positioned after the en-\ncoder, is tasked with generating the final output\nfor sequence classification. It consists of a linear\nlayer with 768 input features, followed by a dropout\nlayer to prevent over-fitting. An additional linear\nlayer featuring a solitary output neuron enables bi-\nnary classification. By viewing the problem as a\nregression task, the classifier yields a linear output\ndesigned for a singular class, producing a proba-\nbilistic value indicative of the relatedness between\ninput sentences.\n3.2 Resources\nFor training our model, we relied on the dataset\nprovided for SemEval-2024 Task 1 (Ousidhoum\net al., 2024a). In addition to the primary dataset,\nwe augmented our training dataset using the T5\nmodel (Raffel et al., 2019). By leveraging T5's\nparaphrasing capabilities, we explored data aug-\nmentation techniques for Track A on the training\nsets of our dataset but failed to achieve consistent\nresults across experiments. While some experi-\nments showed an increase in model accuracy, in\nother cases, it did not alter the results. Data aug-\nmentation consistently worked well only on the\nEnglish dataset. More details about data augmen-\ntation results and our secondary investigation on\nTrack C are provided in Appendix A and B.\nBy incorporating both the SemEval-2024 Task 1\ndataset (Ousidhoum et al., 2024a) and augmented\ntraining data generated by T5, our approach ben-\nefits from a comprehensive and diverse set of re-\nsources, enabling robust training and evaluation\nof our STR model across multiple languages and\ntextual domains.\n3.3 System Challenges\nAugmenting the dataset for training set using T5\nparaphrases posed several challenges. Firstly,\nwhile the primary dataset was labeled through col-\nlaborative human judgment, the augmented data\nlacked this human validation. This absence of hu-\nman labeling for the augmented data may poten-\ntially impact its quality. Moreover, the augmenta-\ntion process introduced alterations to the diversity\nof the data, presenting a challenge to maintaining\nthe original data variety.\nThe decision to employ data augmentation ex-\nclusively for testing purposes raises concerns re-"}, {"title": "4 Experimental Setup", "content": "4.1 Dataset\nThe dataset statistics utilized for each language are\npresented in Table 1:\nAs shown in Table 1, approximately 0.8 of the\nTask 1 dataset is allocated for system training,\nwhile the remainder is reserved for evaluation. The\nlimited availability of training data necessitates cau-\ntious consideration during testing, as the model's\nperformance may be influenced by the scarcity of\ntraining instances. Additionally, the entire develop-\nment set is utilized for model selection.\n4.2 Pre-processing and Hyper-Parameter\nTuning\nA crucial aspect of our pre-processing involves con-\nverting the labels (scores) of each data instance\nto float values, ensuring compatibility with the\nmodel's expected input format. Furthermore, the in-\nput texts undergo tokenization using the RoBERTa-\ntokenizer both during training and inference.\nHyperparameter tuning plays a pivotal role in op-\ntimizing model performance. Our tuning process\nencompasses exploring various hyper-parameters,\nincluding learning rates in the range of [0.00001,\n0.00003], dropout rates ranging from [0.1, 0.3],\nbatch sizes spanning [4, 32], and token sizes\nfrom [32, 128]. Through iterative experimenta-\ntion, we determined that a learning rate of 0.00003,\na dropout rate of 0.1, a token size of 128, a batch\nsize of 16, and a weight decay of 0.01 yield optimal\nresults across all languages.\nThe selection of an appropriate token size is\nnot solely based on computational considerations;\nrather, it is informed by dataset analysis. Upon\nexamination, it became evident that the majority\nof data instances are predominantly short, aligning\nwith our token size choice. Additionally, truncation\nduring tokenization supports the chosen token size,\nensuring efficient model training without sacrific-\ning data representativeness.\n4.2.1 Mean Squared Error (MSE)\nMean Squared Error quantifies the average of the\nsquared differences between predicted and actual\nvalues. It is calculated using the formula:\n$MSE = \\frac{1}{N}\\sum_{i=1}^{N}(y_i-\\hat{y_i})^2$ (1)\nWhere N is the number of instances, yi is the true\nlabel, and \u011di is the predicted value.Additionally,\nMean Absolute Error computes the average abso-\nlute differences between predicted and actual val-\nues.Moreover, the R-squared score assesses the\nproportion of variance in the dependent variable\nexplained by the independent variable.\nThese evaluation measures collectively shed\nlight on our regression model's performance in\npredicting the degree of relatedness between text\nsamples. Using these metrics together enables\nthe monitoring of the model's performance and,\nhence, facilitates decisions on hyper-parameters,\nmodel selection, etc. The evaluation method and\nhyper-parameter choices remain consistent across\nall models and languages. For the analysis of re-\nsults presented in Section 5, the obtained scores\nwere discretized and categorized into five distinct\nranges to enhance visual understanding."}, {"title": "5 Results", "content": "5.1 Findings\nA direct comparison with previous models and\ndatasets similar to this task is challenging due to our\nspecific focus on fine-tuning the RoBERTa model\nand utilizing the dataset provided by Ousidhoum\net al. (2024a). Drawing from the insights of Raffel\net al. (2019) working on the STS dataset, it is ev-\nident that the performance of transformer models\nimproves with larger training corpora and enhanced\ncomputational resources. Raffel et al. (2019)\ndemonstrated that the ROBERTa transformer-based\nmodel achieved a Pearson correlation of 0.922, sur-\npassing ERNIE 2.0, DistilBERT, and TinyBERT\non STS dataset benchmarks. Conversely, ALBERT,\nXLNet, and T5-11B outperformed ROBERTa on\nthe same task, achieving a Pearson correlation of\n0.925. Therefore, we recommend conducting a\nbenchmark study of top-performing transformer\nmodels like RoBERTa, ALBERT, XLNet, and T5-\n11B in future research endeavors. Using the offi-\ncial metric of Spearman Correlation proposed in\nSemEval-2024 Task 1 (Ousidhoum et al., 2024b),\nour system achieves the following scores on differ-\nent data splits and languages:\nAs shown in Table 2, Firstly, comparing the per-\nformance between English, Spanish, and Arabic"}, {"title": "5.2 Error Analysis", "content": "While confusion matrices are less commonly uti-\nlized in regression problems, discretizing the\nmodel's scores allows us to glean insights into\nits performance. Confusion matrix plots for En-\nglish, Spanish, and Arabic are provided in Figure\n2, respectively. Upon examining the confusion\nmatrix of the English dataset, it becomes appar-\nent that the model performs well within certain\nscore ranges. However, there are notable areas, par-\nticularly within the highly related range (0.6-1.0),\nwhere our model could benefit from improvement.\nA similar observation holds true for the Spanish\ndataset, where the model demonstrates proficiency\nin predicting less related sentences but encounters\nchallenges with highly related ones. Conversely,\nthe Arabic dataset presents a markedly different\nscenario. While the majority of predictions fall\nwithin the mid-range of relatedness, they are pre-\ndominantly incorrect.\nBased on the histogram and extracted statistics\nfrom the fine-tuning data in Figure 3 in Appendix C,\nit appears that the majority of the training data has\na distribution centered around the median (Spanish"}, {"title": "6 Conclusion", "content": "In our investigation, we focused on fine-tuning\nROBERTa for STR, primarily targeting Latin\nlanguages like English(0.82) and Spanish(0.67).\nWhile our approach showed promising results for\nthese languages, particularly in achieving high cor-\nrelation, the outlook was less favorable for Ara-\nbic(0.38). This echoes discussions in previous\nworks, emphasizing the significant influence of the\ndata on model performance. Our exploration into\nTrack C, which is given in Appendix B, further\nenriched our understanding of the challenges and\nopportunities in STR system development. As a\ncontribution to the field, we put forth several rec-\nommendations for enhancing STR systems. Firstly,\nwe propose the development of additional Trans-\nformer models trained on diverse language families,\nfocusing on languages that share similarities with\nLatin languages. Furthermore, a comprehensive\nbenchmark of models on the STR dataset is essen-\ntial, building on previous research that highlights\nthe strong performance of models like ALBERT,\nXLNet, and T5-11B on the STS dataset. Moreover,\nthe utilization of translation techniques and data\naugmentation methods could enhance model per-\nformance, particularly for languages with limited\ntraining data. In conclusion, our study sheds light\non the nuances of STR system development and un-\nderscores the importance of considering language-\nspecific factors and domain characteristics. By\npursuing the avenues outlined in this paper, we aim\nto contribute to the advancement of STR research\nand facilitate the development of more robust and\naccurate models for NLU tasks."}]}