{"title": "ACTION ABSTRACTIONS FOR AMORTIZED SAMPLING", "authors": ["Oussama Boussif", "L\u00e9na N\u00e9hale Ezzine", "Joseph Viviano", "Micha\u0142 Koziarski", "Moksh Jain", "Nikolay Malkin", "Emmanuel Bengio", "Rim Assouel", "Yoshua Bengio"], "abstract": "As trajectories sampled by policies used by reinforcement learning (RL) and generative flow networks (GFlowNets) grow longer, credit assignment and exploration become more challenging, and the long planning horizon hinders mode discovery and generalization. The challenge is particularly pronounced in entropy-seeking RL methods, such as generative flow networks, where the agent must learn to sample from a structured distribution and discover multiple high-reward states, each of which take many steps to reach. To tackle this challenge, we propose an approach to incorporate the discovery of action abstractions, or high-level actions, into the policy optimization process. Our approach involves iteratively extracting action subsequences commonly used across many high-reward trajectories and 'chunking' them into a single action that is added to the action space. In empirical evaluation on synthetic and real-world environments, our approach demonstrates improved sample efficiency performance in discovering diverse high-reward objects, especially on harder exploration problems. We also observe that the abstracted high-order actions are interpretable, capturing the latent structure of the reward landscape of the action space. This work provides a cognitively motivated approach to action abstraction in RL and is the first demonstration of hierarchical planning in amortized sequential sampling.", "sections": [{"title": "INTRODUCTION", "content": "Reinforcement learning (RL) relies on a stochastic policy \u03c0e to generate potentially long trajectories \u03c4 of actions a to obtain a reward r. Standard RL methods take learning steps on the policy parameters \u03b8 using a loss function that reinforces actions which maximize reward (Sutton & Barto, 2018). In the case of diversity-seeking RL methods, such as generative flow networks (GFlowNets; Bengio et al., 2021; 2023) \u2013 a special case of entropy-regularized RL (Ziebart et al. (2008); see Tiapkin et al. (2023); Deleu et al. (2024)) \u2013 the loss function may instead encourage the policy to sample terminal states with probability proportional to their reward. In both cases, longer trajectories make training more difficult due to the problem of credit assignment, i.e., the propagation of a learning signal over long time horizons (Sutton & Barto, 2018).\nThe difficulty of credit assignment grows with trajectory length: it has been shown that temporal difference learning methods require an exponential (in the trajectory length) number of updates to correct learning bias, while Monte Carlo methods see the number of states affected by delayed rewards grow exponentially with the number of delay steps (Arjona-Medina et al., 2019). Previous work addressed the challenge of credit assignment through long trajectories by propagating a learning signal to specific moments in the trajectory, skipping over intermediate actions and effectively reducing the trajectory length (Ke et al., 2018; Liu et al., 2019; Hung et al., 2019; Sun et al., 2023)."}, {"title": "RELATED WORK", "content": "Macro-actions are a sequence of actions assembled from an atomic action set (Randlov, 1998), a form of temporal abstraction, and have been the focus of a long line of work. The idea stems from early proposals of automatic induction of macros for programs using various rules or heuristics (Iba, 1989; Laird et al., 1986); see Sutton et al. (1999) on the related framework of options and a review of macro-actions. In our context, a macro-action is a policy representing a series of atomic actions to be executed at once, which in some cases has been shown to reduce the time required to discover the optimal policy and action-value function in the RL context (McGovern & Sutton, 1998; Laird et al., 1986). Previous work has proposed composing macro-actions from n-grams to speed up search during planning (Dulac et al., 2013) and employing them to construct form of a hierarchy of policies with different temporal resolutions (Precup & Sutton, 1997; Dayan & Hinton, 1992). Such methods have produced mixed results, either helping or hindering performance depending on the appropriateness of the macro-actions to the task (McGovern et al., 1997; Jong et al., 2008). In cases where macro-actions were helpful, their utility was explained by improved credit assignment and exploration. Previous work has also used evolutionary algorithms for discovering macro-actions, which was an effective approach on common Atari games (Chang et al., 2022). Others proposed to learn macro-actions as either actions that are repeated a number of times, or commonly-occurring"}, {"title": "PRELIMINARIES", "content": "We summarize the learning problem solved by GFlowNets and the objective used, then introduce the two RL algorithms on which we also evaluate our action abstraction approach.\nLet G = (S, A) be a directed acyclic graph. Nodes s \u2208 S are called states and edges (s \u2192 s') \u2208 A are actions, which define the relations \"s is a parent of s'\" and \"s' is a child of s\". We assume that there is a unique state so \u2208 S with no parents (initial state) and denote the set of states with"}, {"title": "DISCOVERING ACTION ABSTRACTIONS FOR AMORTIZED SAMPLERS", "content": "Training amortized samplers in discrete compositional spaces becomes increasingly difficult as the number of steps in trajectories grows. Consider sampling a graph one node at a time while connecting them to previous nodes. For example, a graph with n takes O(n\u00b2) sampling steps. Sampling long strings autoregressively presents a similar challenge: sampling one character at a time makes the process difficult due to the vast search space of full sequences, which are predominantly low-likelihood samples. In the case of large language models, sampling subsequences (tokens) which commonly occur in the text corpora instead of individual characters reduces the trajectory length required to encode high-likelihood sentences. This has generally been found to decrease both training and inference costs (Wu, 2016; Kudo, 2018), and can be viewed a strategy for trading off reducing the depth of the MDP in exchange for increased breadth, which potentially makes the task easier to learn (Pignatelli et al., 2024).\nDrawing inspiration from this observation, we propose a general approach for sampling discrete objects that infers actions that effectively shorten the trajectory length while producing samples from the target distribution. In particular, our approach consists of augmenting existing samplers with an action abstraction discovery step, a form of \"chunking\" as originally described by cognitive psychologists (Miller, 1956), which is applicable to any sampler, and consists of three major steps (see Algorithm 1 for the overall approach):\n1. Generating an action corpus: We first generate a set of N action sequences from the sampler, optionally also drawing a portion p of these from a replay buffer of previously-drawn high-reward trajectories.\n2. Chunking: We apply a tokenization algorithm common in NLP, byte pair encoding (BPE; Gage, 1994), to the N action sequences to obtain new tokens (\u201cchunks\u201d) to be added to the action space.\n3. Augmenting the action space: Finally, we add the new abstracted actions to the action space. Whenever the abstracted action is chosen, its constituent actions are executed in order\u00b9.\nChunking mechanisms. We consider the following two approaches, comparing them to the ATOMIC sampler (without action space compression):\n\u2022 ACTIONPIECE-INCREMENT: We apply the tokenizer on the action corpus and add the most frequent token found to the action space, which grows by one element each time this is performed. This approach gradually builds up the action space but is susceptible to repetitions in chunks or low quality set of action sequences used for checking."}, {"title": "EXPERIMENTAL SETUP", "content": "We benchmark SAC, A2C and GFlowNet along with a random sampler baseline that samples uniformly at random a valid action at each step. For the GFlowNet, we use trajectory balance (Malkin et al., 2022) and include three different choices for the backward policy:\n\u2022 Uniform PB: This backward policy chooses backward parents uniformly at random.\n\u2022 MaxEnt PB: This backward policy chooses backward trajectories uniformly at random. Note that MaxEnt PB and Uniform PB are equivalent for Tree MDPs since there is only a single backward action at each step (Tiapkin et al., 2023; Mohammadpour et al., 2024).\n\u2022 ShortParse PB: We introduce a new fixed backward policy aimed at sampling the most compact backward trajectory in terms of the number of trajectory steps (see subsection B.6).\nA policy parametrization for nonstationary action spaces. Dealing with a varying action space as a result of chunking requires a compatible parameterization of the policy to handle the evolving action space. Instead of a final layer that outputs logits for a fixed number of actions, the policy instead takes as input the current state and outputs an action embedding qt \u2208 Rd where d is the action embedding dimension (similar to Chandak et al., 2019). Now let A = {ai}|A|i=1 where |A| is the number of actions in the action space A. We use an action encoder f\u03b8(ai) \u2208 Rd that computes an embedding for the actions ai. We parametrize f\u03b8 as an LSTM (Hochreiter & Schmidhuber, 1997) that takes the sequence of atomic actions making up a chunk as input (possibly of length 1 if the chunk is a single atomic action). Let f\u03b8(A) \u2208 R|A|\u00d7d denote the matrix of embeddings of all actions in the action space. The logits given by the policy network are:\nl\u2081 = \\frac{f_\u03b8 (A) q_t}{\\sqrt{V_a}} \\qquad Va \u2208 R^{|A|}\\nonumber (4)\nTasks. To understand the impact of chunking, we consider a diverse set of environments consisting of three synthetic tasks and a practical task: bit sequence generation (Malkin et al., 2022), Fractal-Grid (adapted from the standard hypergrid; Bengio et al., 2021), a graph generation environment,"}, {"title": "RESULTS", "content": "6.1 DOES CHUNKING IMPROVE GENERATIVE MODELING?\nDoes chunking accelerate mode discovery? Figure 2 shows the the number of modes discovered throughout training as a function of the number of visited states. Note that we report the cumulative number of modes, i.e., counting all modes sampled for a given number of visited states, and the difficulty of the task (in terms of the size of the MDP) increases from left to right. Across all tasks, ACTIONPIECE outperforms the ATOMIC counterpart for the GFlowNet sampler in terms of the speed of mode discovery. Results for other algorithms are mixed. ACTIONPIECE helps A2C and SAC in FractalGrid, but hurts performance for RNA binding. Moreover ACTIONPIECE-REPLACE helps A2C in BitSequence, while ACTIONPIECE-INCREMENT hurts performance. The greedy behavior of both SAC and A2C provides a possible explanation: when a mode is discovered, it gets sampled more often resulting in the addition of a large chunk (e.g., Figure 10,12). This chunk will result in high-reward samples and consequently chunks added beyond this will contain parts of this chunk hurting exploration. This issue is mitigated in GFlowNets due to the exploratory behavior imparted by the learning objective. The random sampler with ACTIONPIECE achieves strong performance only on simple tasks, with a degradation in performance for harder problems, suggesting random sampling with action abstraction produces meaningful chunks only in cases where the problem is sufficiently simple.\nDoes chunking improve density estimation? Next, we study the impact of chunking on den- sity estimation in the context of GFlowNets, which learn a policy to sample from a tar- get distribution. Figure 3a shows the tra- jectory balance loss for all chunking mech- anisms over the 2D grid in the FractalGrid environment. This serves as a visualization for how well each state is correctly learned by the GFlowNet. Chunking, and particularly ACTIONPIECE-INCREMENT, positively rein- forces the inherent exploration of GFlowNets (as seen in the previous section), helping enable the sampler to model the entire state space and sam- ple accurately from the target distribution. This is further supported by the L1 distance, JSD, and ELBO Gap (see subsection B.7) during training illustrated in Figure 3b for both grid size 652 and 1292 and all chunking mechanisms. Across all sizes of the task, chunking accelerates the training of the sampler. Figure 3c and 3d show Spearman's rank correlation coefficient between the target and the learned distribution for the L14_RNA1 and Bit Sequence tasks respectively. The correla- tion is computed for a range of reward thresholds spanning all samples to only those with a reward larger than 0.93 to evaluate how well the sampler captures the high reward regions. For L14_RNA1, ACTIONPIECE-INCREMENT performs on par with ATOMIC, and even slightly better for high-reward objects, whereas ACTIONPIECE-REPLACE lags behind. In the bit sequence task, however, chunking results in marginally worse performance. Finally, Table 1, shows that ACTIONPIECE-INCREMENT"}, {"title": "DO CHUNKS CAPTURE THE STRUCTURE IN THE UNDERLYING DISTRIBUTION?", "content": "In this section, we analyze the discovered chunks and study whether they capture some structure in the underlying distribution. For instance, RNA molecules are typically composed of building blocks (or chunks in our terminology) called codons that consist of sequences of three nucleotides. We use the RNA Binding (L14_RNA1) task for all the analyses in this section as we have access to a dataset of high-reward objects.\nDo chunks represent latent structure of the distribution? Here, we dive into the structural relationship between chunks and the objects generated using the chunks. The metrics we consider are as follows:\n\u2022 Chunk occurrence: For each chunk, we compute the average number of times it occurs in objects in the dataset.\n\u2022 Chunk coverage: For each chunk, we compute the number of objects in the dataset that contain that chunk, normalized over the total number of objects."}, {"title": "CONCLUSION", "content": "Abstraction and dynamic concept learning are critical in human cognition and should be modeled in artificial learning systems. In this paper, we investigated dynamic action space learning in the"}]}