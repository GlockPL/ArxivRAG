{"title": "TSINR: Capturing Temporal Continuity via Implicit Neural Representations for Time Series Anomaly Detection", "authors": ["Mengxuan Li", "Ke Liu", "Hongyang Chen", "Jiajun Bu", "Hongwei Wang", "Haishuai Wang"], "abstract": "Time series anomaly detection aims to identify unusual patterns in data or deviations from systems' expected behavior. The reconstruction-based methods are the mainstream in this task, which learn point-wise representation via unsupervised learning. However, the unlabeled anomaly points in training data may cause these reconstruction-based methods to learn and reconstruct anomalous data, resulting in the challenge of capturing normal patterns. In this paper, we propose a time series anomaly detection method based on implicit neural representation (INR) reconstruction, named TSINR, to address this challenge. Due to the property of spectral bias, TSINR enables prioritizing low-frequency signals and exhibiting poorer performance on high-frequency abnormal data. Specifically, we adopt INR to parameterize time series data as a continuous function and employ a transformer-based architecture to predict the INR of given data. As a result, the proposed TSINR method achieves the advantage of capturing the temporal continuity and thus is more sensitive to discontinuous anomaly data. In addition, we further design a novel form of INR continuous function to learn inter- and intra-channel information, and leverage a pre-trained large language model to amplify the intense fluctuations in anomalies. Extensive experiments demonstrate that TSINR achieves superior overall performance on both univariate and multivariate time series anomaly detection benchmarks compared to other state-of-the-art reconstruction-based methods. Our codes are available here.", "sections": [{"title": "1 INTRODUCTION", "content": "Time series anomaly detection, which aims to identify unusual patterns or events across a sequence of data points collected over time [21], has attracted a lot of attention recently. In many fields (e.g., finance, healthcare, manufacturing, and fault diagnosis), monitoring time-varying data to identify anomalies is crucial for detecting unusual behavior, potential issues, or security threats [5, 20, 23, 28, 36]. For example, in the finance field, detecting anomalous behavior in credit card data allows the prevention of theft or fraudulent transactions committed by an unauthorized party [49]. In industrial processes, identifying anomalies helps secure safe operations, averting safety concerns and mitigating economic losses [24].\nOne of the major challenges for anomaly detection lies in anomalies may be rare, subtle, or have different shapes, requiring sophisticated algorithms to distinguish them from normal patterns. Moreover, time series typically exhibit trends, seasonality, and temporal dependencies, making it challenging to model such complex features. Since anomalies are typically rare and new anomalies may arise, it is difficult or expensive to collect a sufficient amount of labeled data. As one of the unsupervised-based methods, reconstruction-based methods tackle this problem by reconstructing data to learn point-wise feature representations to uncover normal patterns in the data [69, 70]. The reconstruction error, i.e., the difference between the input data and its reconstructed version, serves as a natural anomaly score. The points that have high anomaly scores are considered as anomalies, which makes it easy to interpret and understand the results.\nHowever, it is challenging to distinguish normal and anomaly patterns because normal and anomalous points may coexist within a single instance, and anomalies may occur in the unlabeled training data [66]. In addition, time series data often contains intricate"}, {"title": "2 RELATED WORK", "content": "2.1 Time Series Anomaly Detection\nTime series anomaly detection methods primarily include statistical, classic machine learning, and deep learning methods. Statistical methods rely on analyzing the statistical properties of the data to identify patterns that deviate from the expected behavior. They are valuable for their simplicity and interpretability, but have limitations in capturing complex patterns [7, 8].\nClassic machine learning methods rely on manual feature extraction and various algorithms like clustering [45, 48], density estimation [3, 65, 76], and isolation forests [25] to identify anomalies in structured data. However, because they require manual feature extraction and selection, they can be labor-intensive and less effective at capturing complex patterns in data.\nDeep learning methods automatically learn the features of data through deep neural networks without the need for manual intervention, and are adept at handling high-dimensional, unstructured data. They can be broadly categorized into supervised and unsupervised learning algorithms. Supervised methods are trained with labels to learn and classify both normal and anomalous behavior in the given time series data, such as NFAD [46] and MultiHMM [22]. However, annotating data is challenging due to the rarity of anomalies and the emergence of new anomalies. This makes it difficult to achieve effective labeling, leading to limitations in the performance of supervised methods in detecting anomalies."}, {"title": "2.2 Implicit Neural Representations", "content": "Presently, INR [62] stands as a scorching topic in the domain of deep learning. It aims to learn a continuous function, often embodied as a neural network, for data representation. In this function, the input comprises coordinates, while the output consists of corresponding data values. INR learns continuous representations and has been widely applied in numerous scenarios, such as 2D image generation [39, 51, 73] and 3D scene reconstruction [12, 18, 43], physics-informed problems [38, 42] and video representation [6, 29, 30].\nCurrent methods for learning INR parameters are primarily based on either meta-learning [26] or feed-forward networks [4]. The key difference is that meta-learning requires a few training steps for each unseen test data, while feed-forward networks directly make predictions in a single forward pass. In this paper, we use a transformer-based architecture to generate INR parameters and it requires only a single forward in the inference phase [4].\nIn addition, there exists a phenomenon known as spectral bias [41], where INR tends to prefer fitting the low-frequency components of the signal [26]. Since this characteristic can affect the ability of INR to model high-frequency data, most efforts are directed towards mitigating this effect [50, 53]. In contrast, in time series anomaly detection, this property turns out to be advantageous. In time series data, normal points exhibit relative smoothness, whereas anomalous points possess strong discontinuity. Hence, we leverage this property of INR to prioritize fitting normal data with low frequencies, making it more sensitive to anomalous data."}, {"title": "2.3 Implicit Neural Representations on Time Series Data", "content": "Currently, there are some studies discussing the possibility of utilizing INR for time series representation. HyperTime [11] leverages INR to learn a compressed latent representation for time series imputation and generation. TimeFlow [33] uses INR to capture continuous information for time series imputation and forecasting. In addition, the potential of employing INR for time series anomaly detection has not been fully explored yet. Only INRAD [14] attempts to adopt INR to represent and reconstruct time series data to identify anomalies. INRAD aims to utilize INR to overcome deep learning limitations, like complex computations and excessive hyperparameters (e.g., sliding windows). However, it requires training an INR network for each unseen time series data in test set, leading to additional training time and inefficiency in practical applications.\nDifferent from INRAD, our method uses the spectral bias property of INR to mitigate the impact of unlabeled anomalies for the reconstruction model. With the Transformer integration, it can detect anomalies in unseen test data without retraining, enhancing efficiency for practical applications. In addition, compared to INRAD and other INR methods, our approach incorporates several specialized designs for time series anomaly detection. Firstly, we devise a novel form of INR continuous function to capture trend,"}, {"title": "3 METHODOLOGY", "content": "In this paper, we propose TSINR, a novel time series anomaly detection method based on INR reconstruction. The core idea is to leverage the spectral bias phenomenon of INR to prioritize fitting smooth normal points, thereby enhancing sensitivity to discontinuous anomalous points. In this section, we present the problem statement and introduce the overall architecture, followed by the form of INR continuous function designed for time series data and the frozen pre-trained LLM encoder applied to amplify the fluctuations of anomalies from both time and channel dimensions. The anomaly criterion is demonstrated finally."}, {"title": "3.1 Problem Statement", "content": "Consider a time series $X$ with $T$ timestamps: $X = (x_1, x_2, \u2026, x_T)$, where $x_t \\in \\mathbb{R}^d$ is the data point observed at a certain timestamp $t$ ($t\\in \\{1, 2, ..., T\\}$) and $d$ denotes the number of the data variables (i.e., data dimensionality). For a multivariate data, $d > 1$. And for an univariate case, $d = 1$. Given unlabeled input time series data $X_{train}$, for any unknown time series data $X_{test}$ with the same data dimensionality $d$ as $X_{train}$, we aim to predict $y_{test} = (Y_1, Y_2,\u2026, Y_{T' })$, where $T'$ is the length of $X_{test}$. And $y_{t' } \\in \\{0, 1\\}$ denotes whether the data point is normal ($y_{t' } = 0$) or abnormal ($y_{t' } = 1$) at the certain timestamp $t'$ ($t' \\in \\{1, 2, ..., T' \\}$)."}, {"title": "3.2 Overall Workflow", "content": "Figure 2 shows the overall workflow of the proposed TSINR method. We employ a feed-forward transformer-based architecture to directly predict the whole weights of the INR of the given time series data [4]. Unlike meta-learning based on gradient descent [26], our method requires only a single forward step in the inference phase. Following a strategy similar to other transformer-based methods [10, 67], the input time series data is normalized and segmented into patches. A frozen pre-trained LLM encoder is applied to map the input data into the feature domain to amplify the fluctuations of anomalies. Then the obtained features are converted to data tokens using a fully connected (FC) layer. Simultaneously, we initialize the corresponding INR tokens, which are learnable vector parameters. These data tokens and initialized INR tokens are fed together into a transformer encoder, which mainly consists of self-attention modules and feed-forward modules. In this transformer encoder, the knowledge interacts with data tokens and INR tokens. The learned INR tokens are mapped to the INR weights through FCs, denoted as FC*. These INR weights form our INR continuous function, which is specifically designed for time series data. The designed function takes a batch of timestamps $\\{t\\}_{i=1}^{T'}$ as input, implicitly learns the trends, seasonality, and residual information of the given time series data, and finally reconstructs the input signal. The details of the designed form of INR continuous function and the applied frozen pre-trained LLM encoder module are demonstrated in Section 3.3"}, {"title": "3.3 Form of INR Continuous Function", "content": "As shown in Figure 2, we innovatively propose a INR continuous function to better learn and reconstruct time series data. Inspired by classical time series decomposition methods [7, 11], the proposed INR continuous function $f$ consists of three components, including trend $f_{tr}$, seasonal $f_s$, and residual $f_r$:\n$f(t) = f_{tr}(t) + f_s(t) + f_r(t).$\n(1)\nThe trend component captures the underlying long-term patterns and focuses on slowly varying behaviors. In order to model this monotonic function, a polynomial predictor is applied [11, 34]:\n$f_{tr}(t) = \\sum_{i=0}^p w_{tr}^{(i)} t^{i}$\n(2)\nwhere $w_{tr}$ are polynomial coefficients corresponding to the $i^{th}$ degree and $w_{tr}^{(i)}$ is predicted by a FC network. In addition, $p$ denotes the polynomial degree and is set to be small in order to model the low-frequency information and mimic the trend.\nThe seasonal component grasps the regular, cyclical, and recurring short-term fluctuations. Therefore, a periodic function is employed based on Fourier series [11, 34]:\n$f_s(t) = \\sum_{i=0}^{\\lfloor T/2-1 \\rfloor} (w_s^{(i)} cos(2\\pi i t) + w_s^{(i+\\lfloor T/2 \\rfloor)} sin(2\\pi i t)),$\n(3)\nwhere $w_s$ are Fourier coefficients learned by a FC network. This component is then able to model the periodic information and simulate typical seasonal patterns.\nThe residual component aims to represent the unexplained variability in the data after accounting for the trend and seasonal components. In order to capture this complex and non-periodic information, we design a group-based architecture as shown in Figure 3. For any given timestamp $t$, we design $M$ global layers and $N$ group layers with $k$ groups. The global layers capture the inter-channel information while the group layers focus on intra-channel information. The equation for calculation within global layers can be defined as:\n$q_{m+1} = ReLU(w^{(m)} q_m +b^{(m)}),$\n(4)"}, {"title": "3.4 Frozen Pre-trained LLM Encoder", "content": "To further enhance the ability of TSINR to detect anomalies, we leverage the representational capability of LLM. A pre-trained LLM is employed as the encoder, which has been demonstrated to process time-series data and provide cross-modal knowledge [75]. With this pre-trained LLM, we map the input data into the feature domain to amplify the fluctuations of anomalies from both time and channel dimensions. On the one hand, in the time dimension, we observe that the extracted feature of LLM involves more intense fluctuations during the anomaly interval. On the other hand, in the channel dimension, other channels have the same anomaly interval due to the ability of LLM to extract and fuse the inter-channel information. Therefore, TSINR can exhibit greater sensitivity to anomalous data, thereby enhancing its ability for anomaly detection. The corresponding experimental results and analysis can be found in Section 4.4.2.\nMore specifically, the self-attention layers and the feed-forward layers are frozen to preserve the prior knowledge in the pre-trained model. For any given time series data $X \\in \\mathbb{R}^{d \\times T}$, the pre-trained LLM encoder maps it to feature domain:\n$Z = Encoder(X),$\n(7)\nwhere $Z \\in \\mathbb{R}^{d \\times T}$ denotes the feature corresponding to $X$."}, {"title": "3.5 Anomaly Criterion", "content": "Following previous reconstruction-based anomaly detection approaches [35, 47, 75], we use the reconstruction error as the anomaly score for each time series data point. The anomaly score at timestamp $t$ is defined as follows:\n$AnomalyScore(t) = \\sum_{i=1}^{d} |x_{ti} - f(t)||^2$.\n(8)\nBased on this point-wise anomaly score, we use a parameter threshold $\\delta$ to determine whether the point is abnormal or normal:\n$Y_t = \\begin{cases} 1: \\text{abnormal} & AnomalyScore(t) \\geq \\delta \\\\ 0: \\text{normal} & AnomalyScore(t) < \\delta  \\end{cases}$\n(9)\nThe threshold $\\delta$ is set to label a proportion $\\gamma$ of the test dataset as anomalies. And $\\gamma$ is a hyper-parameter based on actual datasets."}, {"title": "4 EXPERIMENTS", "content": "4.1 Datasets\nWe use eight anomaly detection benchmarks from real-world scenarios to validate the performance of our proposed method, including seven multivariate datasets (SMD [52], PSM [1], SWaT [31], MSL [13], SMAP [13], PTB-XL [15, 57], and SKAB [16]) and one univariate dataset (UCR [61]). More details are in Appendix B.\n4.2 Baselines and Experimental Settings\nWe compare our proposed method with 11 state-of-the-art deep learning approaches, including both general frameworks designed for time series modeling and algorithms specifically tailored for time series anomaly detection: FPT [75], TimesNet [59], ETSformer [58], FEDformer [74], LightTS [68], DLinear [67], Autoformer [60], Pyraformer [27], AnomalyTransformer [63], Informer [72] and Transformer [56]. Also, we compare our TSINR method with other classical anomaly detection methods and the comparison results are in Appendix F. Additionally, we employ the commonly used metrics of precision, recall, and F1-score for evaluation.\nThe implementation details and the default hyper-parameters are summarized here. For a fair comparison, we only employ the classical reconstruction error across all baseline models. Also, we adopt identical data processing methods and the corresponding parameter configurations. We employ the sliding window approach and use a fixed window size of 100 for all datasets. The proportion $\\gamma$ mentioned in Section 3.5 is set to 0.5 for SMD dataset, 0.1 for UCR dataset, 10 for SKAB dataset, and 1 for others. These parameters adhere to the settings of previous work [63, 75]. Ablation studies on the anomaly proportion $\\gamma$ are in Appendix E. For the main results, our TSINR model involves 3 global layers and 2 group layers in the residual block. And the hidden dimensions are 64 and 32 respectively. The transformer encoder has 6 blocks. We use GPT2 [40] as the pre-trained LLM encoder and 6 blocks are utilized following the same settings as in FPT [75]. The experiments are conducted using the ADAM optimizer [17] with an initial learning rate of $10^{-4}$. A single NVIDIA Tesla-V100 32GB GPU is applied for each dataset. And the efficiency analysis is in Appendix H.\n4.3 Main Resutls\nWe compare our method with 11 other state-of-the-art approaches and the results are shown in Table 1. These results show that our method achieves superior overall performance on these benchmark datasets. These experimental results confirm that TSINR, in both multivariate and univariate scenarios, effectively captures temporal continuity and precisely identifies discontinuous anomalies. The findings affirm the robustness of TSINR across diverse datasets and showcase its potential for broader applications in diverse domains.\nIn multivariate scenarios, we observe that despite both MSL and SMAP being collected from NASA Space Sensors, TSINR achieved significantly greater improvements on the SMAP dataset compared to other methods. This could be attributed to the presence of more point anomalies in the SMAP dataset. Point anomalies exhibit poorer continuity compared to other anomaly patterns due to their isolated nature, representing single data points that significantly deviate from the surrounding pattern. This aligns with the property of spectral bias, making our model more sensitive to point"}, {"title": "4.4 Ablation Studies", "content": "4.4.1 Analysis of the Decomposition Components and the Group-based Architecture. In this section, we analyze the effectiveness of the proposed decomposition components and group-based architecture. The decomposition components indicate the three components (i.e., trend, seasonal, and residual) designed in our paper. And the group-based architecture is proposed for the residual block.\nThe main purpose of the decomposition components is to extract the unique trend and seasonal information of the time series data. The results in Table 2 indicate that capturing these distinctive features significantly enhances the capability for anomaly detection. In addition, the group-based architecture is designed to enhance the representational capacity of INR for multivariate data. Experimental results indicate an improvement in the capability for anomaly detection when employing the proposed group-based architecture. This is because modeling multiple variables and capturing both inter- and intra-channel information with a simple continuous function, which only consists of fully-connected layers, is challenging. Our approach addresses this by dividing the variables into several groups and applying independently fully-connected layers in different groups, thereby reducing the number of variables each group needs to model and improving the representational capacity. The global layers extract the inter-channel information, while the group layers selectively focus on detailed information for specific channels. This enhances the representational capability for each variable without losing any knowledge. Detailed ablation studies on the number of groups are left in Appendix D.\n4.4.2 Analysis of Pre-trained LLM Encoder. Further, we prove the validity of the pre-trained LLM encoder, which is utilized to encode the data into the feature domain to amplify the fluctuations of anomalies and thereby enhance the capability of TSINR in identifying"}, {"title": "5 CONCLUSION", "content": "Time series data anomaly detection plays a pivotal role in ensuring the reliability, security, and efficiency of systems across various domains. Reconstruction-based methods are mainstream approaches for this task because they do not require label information and are easy to interpret the detection results. However, unlabeled anomalous points in the training data can negatively impact the performance of these reconstruction models. To address this issue, this paper proposes a novel algorithm named TSINR for time series anomaly detection based on INR reconstruction. We utilize"}, {"title": "APPENDIX", "content": "A ALGORITHM\nPseudo-codes of the main components of TSINR are presented in 1. All experiments are implemented based on PyTorch. The proposed form of INR continuous function takes the timestamp t as input, which individually passes through three blocks: trend, seasonal, and residual, to predict the corresponding time series information. Then, the predictions from these blocks are summed up to obtain the reconstructed time series data as shown in Eq. 1. For the trend and seasonal components, we use two single-layer MLPs to fit the trend information in Eq. 2 and the periodic information in Eq. 3 separately. For the residual component, we propose a group-based architecture to learn intricate residual information. A 5-layer MLP is utilized for learning complex residual information in Eq. 4 and Eq. 5, comprising 3 global layers and 2 group layers. Finally, we use a transformer-based architecture to generate the INR parameters. The original data is encoded by a pre-trained LLM encoder and the obtained features are passed into the transformer encoder as data tokens. The INR tokens are learned through transformer encoders and further transformed into parameters for the MLPs in the trend, seasonal, and residual blocks through the post functions.\nAlgorithm 1 TSINR Anomaly Detection Algorithm\nInput: A batch of time serial data $X \\in \\mathbb{R}^{B\\times d \\times T}$, trained initialized INR token $I_{init} \\in \\mathbb{R}^{B \\times N_1 \\times C}$, the timestamp $C \\in \\mathbb{R}^{B \\times T}$, the anomaly threshold d\nOutput: The predicted anomaly label $Y \\in [0, 1]^{B \\times T}$\nNormalize and patch X to data feature $D \\in \\mathbb{R}^{B \\times N_a \\times C}$\nConcatenate D and $I_{init}$ to feature $F \\in \\mathbb{R}^{B \\times (N_a+N_1) \\times C}$\nEncode F with LLM encoder and learnable Transformer encoder to get processed feature $F' \\in \\mathbb{R}^{B \\times (N_a+N_1) \\times C}$\nObtain the data-dependant INR token $I' \\in \\mathbb{R}^{B \\times N_1 \\times C}$ by choosing the corresponding part of $F'$\nUsing FC layer to convert $I'$ to the parameter $\\theta$ of MLP and form the INR continuous function $f_{\\theta}$\nQuery the corresponding value of the timestamp C by $X' = f_{\\theta}(C)$ to get the reconstruct the time serial data $X' \\in \\mathbb{R}^{B \\times d \\times T}$\nCalculate the Anomaly Score $S \\in \\mathbb{R}^{B \\times T}$ by measuring the pointwise difference between the input data X and the reconstructed data X' as in equation (7).\nDetermine whether the point is abnormal or normal with the threshold d as in equation (8) and get the predicted anomaly label $Y \\in [0, 1]^{B \\times T}$\nB DATASET DESCRIPTION\nWe conduct comprehensive experiments on seven multivariate datasets and one univariate dataset to validate the effectiveness of our proposed TSINR model.\nMultivariate Datasets: (1) SMD (Server Machine Dataset) is a novel 5-week-long dataset collected from a large Internet company, comprising data from 28 distinct machines with 38 dimensions [52]. (2) PSM (Pooled Server Metrics) is sourced internally from various application server nodes at eBay with 25 dimensions and consists of 13 weeks of training data and 8 weeks of testing data"}, {"title": "C DISCUSSION ON CONTINUITY OF MULTIVARIABLE DATASETS", "content": "Figure 8 is a kernel density estimation plot, which is a method for visualizing the distribution of observations in a dataset. It can be observed that the SWAT dataset has the most peaks, and there is a lack of transition points between the peaks, indicating that the data changes significantly, are more high-frequency, and have poor continuity. This limitation adversely impacts the fitting capability of TSINR, consequently affecting the performance of anomaly detection.\nD STUDY ON THE GROUP NUMBER\nGroup number is a hyper-parameter in our method, representing the number of groups in the group-based architecture. In other words, it determines the number of variables learned by neurons in each group layer. Table 4 demonstrates the ablation studies on the group number. When the group number is equal to 1, it means a regular function is used rather than a group-based architecture. We observe that different datasets have different optimal parameters. This is reasonable. Drawing an analogy to image data, partitioning can accelerate the fitting of INR for images [26]. Partitioning is similar to the group-based architecture discussed here. For each image, the number of partitions is not fixed and depends on the inherent characteristics of each signal. Therefore, in our approach, the optimal group number parameter for each dataset also depends on the intrinsic features of that dataset. Nevertheless, using the group-based architecture has enhanced the model's ability for anomaly detection.\nE STUDY ON THE ANOMALY PROPORTION\nAnomaly proportion $\\gamma$ is a hyper-parameter which decides the anomaly threshold $\\delta$. As mentioned in Section 3.5, the threshold $\\delta$ is set to label a proportion $\\gamma$ of the test dataset as anomalies. We show the influence of the anomaly proportion in Table 5. It can be observed that an appropriate anomaly proportion is beneficial in aiding the model's judgment of anomalies. Among them, PSM"}, {"title": "F COMPARISON TO OTHER CLASSICAL ANOMALY DETECTION METHODS", "content": "Besides reconstruction-based methods, we also compared our approach with other classic anomaly detection methods, such as clustering-based methods (DeepSVDD [45] and ITAD [48]), the density-estimation models (LOF [3], MPPCACD [65], DAGMM [76]), the classic methods (OCSVM [54], iForest [25]), and the change point detection and time series segmentation methods (BOCPD [2], U-Time [37], TS-CP2 [9]). As shown in Table 6, TSINR achieves comparable performance on SMAP dataset and outperforms other baselines on other datasets."}, {"title": "G FULL EXPERIMENT RESULTS ON UCR DATASET", "content": "Due to the page limitation, we place the full experimental results conducted on UCR data in this section. The UCR dataset mainly involves four domains, including human medicine, meteorology, biology and industry [61]. We follow the settings of the previous work [44] to categorize the 250 sub-datasets into four groups, with each group corresponding to a specific domain. Figure 9 illustrates the average results within each category. It can be observed that our method achieves the best performance in each domain, demonstrating its strong generalizability.\nH EFFICIENCY ANALYSIS\nWe measure the efficiency of the TSINR method, and show the results in Table 7. The results indicate that the TSINR is pretty efficient and lightweight."}]}