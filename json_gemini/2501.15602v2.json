{"title": "Rethinking External Slow-Thinking:\nFrom Snowball Errors to Probability of Correct Reasoning", "authors": ["Zeyu Gan", "Yun Liao", "Yong Liu"], "abstract": "Test-time scaling, which is also often referred\nto as slow-thinking, has been demonstrated to\nenhance multi-step reasoning in large language\nmodels (LLMs). However, despite its widespread\nutilization, the mechanisms underlying slow-\nthinking methods remain poorly understood.\nThis paper explores the mechanisms of external\nslow-thinking from a theoretical standpoint. We\nbegin by examining the snowball error effect\nwithin the LLM reasoning process and connect\nit to the likelihood of correct reasoning using\ninformation theory. Building on this, we show\nthat external slow-thinking methods can be\ninterpreted as strategies to mitigate the error\nprobability. We further provide a comparative\nanalysis of popular external slow-thinking\napproaches, ranging from simple to complex,\nhighlighting their differences and interrelation-\nships. Our findings suggest that the efficacy of\nthese methods is not primarily determined by the\nspecific framework employed, and that expanding\nthe search scope or the model's internal reasoning\ncapacity may yield more sustained improvements\nin the long term. We open-source our code\nat https://github.com/ZyGan1999/\nSnowball-Errors-and-Probability.", "sections": [{"title": "1. Introduction", "content": "Scaling laws (Kaplan et al., 2020) have been widely accepted as a guiding principle in the development of large\nlanguage models (LLMs), indicating that the performance\nof LLMs improves with the growth of model size and train-\ning data. Over the past few years, the trend in this field has\nbeen toward expanding the scale of training phase, result-\ning in significant performance improvements (Yuan et al.,\n2023; Zelikman et al., 2022; Rafailov et al., 2024). How-\never, the marginal gains in model performance diminish as\nscale increases, and training more powerful models necessi-\ntates a substantial rise in investment. Consequently, recent\nresearches have shifted focus to scaling strategies beyond\nmodel size, including optimizations during the post-training\nphase and even at the test-time stage (Snell et al., 2024).\nFollowing the release of LLMs with remarkable reason-\ning capabilities, such as OpenAI's o1 (2024), DeepSeek's\nR1 (2025), and Qwen's QwQ (2024b), it has become widely\nacknowledged that scaling the inference process of LLMs\noffers a promising avenue for further enhancing model per-\nformance. Specifically, empirical studies have shown that\nthe reasoning quality of LLMs improves with extended in-\nference time (Lightman et al., 2023). This observation has\nsparked a new research trajectory focused on augmenting\nthe reasoning abilities of LLMs by increasing inference\ncosts during the test-time phase, a concept referred to as\ntest-time scaling, or more colloquially, slow-thinking.\nTest-time scaling strategies can be generally classified\ninto two primary approaches: internal and external slow-\nthinking (Jiang et al., 2024; Min et al., 2024). Internal\nslow-thinking involves adjusting model parameters through\nadditional training on specifically designed reasoning tasks,\naiming to inherently extend the model's output length\nand thereby enhance its reasoning capabilities. In con-\ntrast, external slow-thinking focuses on increasing infer-\nence costs by introducing additional computational steps,\nsuch as re-sampling or re-generating model outputs multiple\ntimes (Brown et al., 2024), thereby prolonging inference\ntime and improving reasoning quality.\nThis paper focuses on external slow-thinking techniques,\nwhich are inspired by human cognitive processes. When\nfacing complex questions, humans often take extra time\nto reflect and refine their intermediate answers, leading to\ngreater accuracy. Similarly, external slow-thinking methods,\nsuch as the Best-of-N (BoN) strategy, draw multiple sam-\nples and evaluate them using techniques like majority voting\nor ranking (Cobbe et al., 2021). Beyond simpler methods,\nadvanced frameworks like CoT (Wei et al., 2022), ToT (Yao\net al., 2024), and MCTS-based approaches inspired by Al-\nphaGo (Silver et al., 2016) explore solution spaces in tree"}, {"title": "2. Snowball Errors in LLM Reasoning", "content": "Imagine rolling a snowball on a snowy surface during winter.\nAs the distance increases, the snowball grows at an accel-\nerating rate. This \"snowball effect\" illustrates how small\nchanges can compound over time. In the context of LLMs,\nthis effect first manifests as the progressive accumulation of\ntoken-level errors in auto-regressive next-token prediction\n(NTP) tasks, potentially causing significant deviations from\nthe expected or golden answers (Bachmann & Nagarajan,\n2024).\nFor reasoning tasks, however, the snowball effect shifts to\nthe sentence level, making the errors more challenging to\ncharacterize. To understand these errors, it is critical to first\nexamine the nature of reasoning. Prior research suggests\nthat LLM reasoning can be conceptualized as executing a\nsequence of primitive tasks at each reasoning step (Ton et al.,\n2024), prompting further investigation into how such errors\naccumulate across the reasoning process.\nLet's reconsider Plato's allegory of the cave\u00b9, which has\nbeen widely used to highlight the limitations of AI mod-\nels (Huh et al., 2024). In this analogy, training data serve\nas mere projections of the real world, akin to shadows on\nthe wall, as illustrated in Figure 1(a). Similarly, in LLM\nreasoning, generated responses are the shadows, reflect-\ning the model's implicit reasoning processes, as illustrated\nin Figure 1(b).\nFor example, when solving a problem like \"Calculate 3x +\n2y,\" the model implicitly executes reasoning steps such as $t_1$:\n{Calculate 3x} \u2192 $t_2$: {Calculate 2y} \u2192 $t_3$: {Add 3x and\n2y}. However, these steps are abstract and cannot be directly\nobserved in outputs. Instead, the response sequence $r_1$ \u2192\n$r_2$ \u2192 $r_3$ can be multiple possible expressions of the same\nreasoning process. Moreover, since individual responses\n$r_i$ cannot fully encapsulate the corresponding steps $t_i$,\nminor inaccuracies accumulate, ultimately leading to"}, {"title": "3. From Snowball Errors to Probability", "content": "significant snowball errors.\nTo quantify snowball errors in LLM reasoning, we consider\nmutual information (MI) between the implicit reasoning\nsequence t and the observed response sequence r, denoted\nas I(t; r). This metric captures the shared information\nbetween the two sequences. Furthermore, the minor in-\naccuracies in the responses at each reasoning step can be\nassessed as information loss, which can be quantified by\nthe difference between the MI I(t; r) and the information\nentropy of the implicit thoughts t, denoted as H(t). And it\ncan be mathematically defined as:\nDefinition 2.1. (Information loss.) Given a reasoning pro-\ncess with implicit thoughts t and corresponding responses\nr, the information loss in the l-th step is defined as:\nInfoLoss($r_l$) = H($t_l$) - I($t_l$; $r_l$) = H($t_l$$r_l$).\nThe snowball errors can be further defined as the accumula-\ntion of information loss across all reasoning steps as follows:\nDefinition 2.2. (Snowball errors, or cumulative information\nloss.) Given a reasoning process with implicit thoughts t\nand corresponding responses r, the snowball errors in the\nl-th step are defined as:\nH<$l$($t$/r) = $\\sum_{i=1}^{l-1}$H($t_i$$r_i$),\nwhere I denotes the number of reasoning steps.\nIn this section, we seek to establish a theoretical connection\nbetween snowball errors and the probability of reasoning er-\nrors in LLMs. We begin by formally defining the probability\nof reasoning errors and subsequently derive a lower bound\nfor this probability using principles from information theory.\nFinally, we empirically validate the presence of snowball\nerrors in the reasoning processes of LLMs."}, {"title": "3.1. Probability of Reasoning Errors", "content": "As the reasoning path grows longer, snowball errors accu-\nmulate, leading to significant factual inaccuracies, which\nwe define as reasoning errors. This subsection explores the\nrelationship between snowball errors and the probability of\nreasoning errors.\nTo evaluate reasoning errors, we first define them clearly.\nSince the response $r_i$ represents the implicit thought $t_i$, a\nnatural approach is to assess whether a sufficiently powerful\nmapping function f can reconstruct $t_i$ from $r_i$.\nProposition 3.1. (Probability of reasoning errors.) Let\n$t_i$ denote an implicit thought at step l, and $r_i$ represent\nthe corresponding generated response. Given a predicted"}, {"title": "4. Probability of Correct Reasoning in\nExternal Slow-Thinking", "content": "thought $t_l$ derived from $r_l$ using a prediction function $t_l$ =\nf($r_l$), the probability of reasoning error at step l is defined\nas the likelihood of the event $e_l$, where $e_l$ : $t_l$ \u2260 $t_l$. This\nprobability is denoted as P($e_l$) = P($t_l$ \u2260$t_l$).\nTo estimate the probability of reasoning errors, we propose\nutilizing information theory to establish a connection be-\ntween snowball errors and the likelihood of reasoning errors.\nSpecifically, our analysis start from the following lemma:\nLemma 3.2. (Information loss inequality.) Given a reason-\ning process defined above, and under the assumption that\nthe mutual information I($t_l$; $r_l$) decreases with respect to l\nwhen l \u2265 2, the information loss in the l-th step satisfies:\nH($t_l$$r_l$) \u2265 $\\frac{H_{<l}(t\\mid r)}{l-1}$\nThe proof is provided in Appendix A.1. Lemma 3.2 indi-\ncates that the information loss in the l-th step is bounded by\nthe average snowball errors in the previous steps. Based on\nthis lemma, we can subsequently derive the lower bound of\nthe probability of reasoning errors.\nTheorem 3.3. (Lower bound of P($e_l$).) Given a reasoning\nprocess and conditions defined above, when l \u2265 2, the\nprobability of reasoning error at step l satisfies:\nP($e_l$) \u2265 log$^{-1}$(|T$_l$| \u2212 1) [$\\frac{H_{<l}(t\\mid r)}{l-1}$ - H$_b$($e_l$)\nwhere Ti is the size of the support of $t_i$. H$_b$($e_l$) is the\nentropy of the indicator random variable of event $e_l$, which\nis a relatively small constant.\nThe proof mainly relies on Fano's inequality (Fano, 2008)\nand is provided in Appendix A.2. Theorem 3.3 sets a lower\nbound on the probability of reasoning errors at step l. Since\nH<$l$($t$/r) represents the cumulative information loss up\nto step l, it is assumed to increase at least linearly with l.\nFurthermore, when the snowball effect occurs, H<$l$($t$/r)\nmay grow faster than linearly. As a result, the lower\nbound on the probability of reasoning errors rises with the\nreasoning path length 1, indicating that the chance of errors\nincreases as snowball errors accumulate."}, {"title": "3.2. Empirical Verification", "content": "Previous analyses demonstrate that the probability of reason-\ning errors, P($e_r$), increases with the number of reasoning\nsteps l. In the practice, however, reasoning errors are often\nreflected in the reward associated with the generated re-\nsponses. In this section, we extend our theoretical analysis\nto real-world scenarios and explore the mechanisms under-\nlying the effectiveness of external slow-thinking methods."}, {"title": "4.1. What is a Correct Reasoning?", "content": "We begin our analysis by defining the reasoning process in\nreal-world settings. Given a question $r_0$, a response R is\nrepresented as a sequence of L reasoning steps, i.e., R =\n[$r_1$,$r_2$,\u2026,$r_L$], generated autoregressively by the LLM.\nAn oracle \u03c6 is then employed to evaluate the quality of each\nstep $r_i$ produced at layer l, denoted as \u03c6($r_l$). In practice,\nthis evaluation is often determined by human feedback or\na reward model. Furthermore, we assume that for each\nreasoning step, there exists a corresponding golden step\n$r_i^\\ast$, representing the most accurate and correct step that the"}, {"title": "4.2. Probability of Correct Reasoning", "content": "LLM should generate, aligning with ideal human reasoning.\nBased on the above setting, the oracle evaluation can be\nused to quantify the correctness of a response. Specifically,\nwe define this measure as follows:\nDefinition 4.1. (\u03c4-correct step.) A step $r_i$ is considered as\n\u03c4-correct if the quality difference between the step and the\ngolden step is less than \u03c4, i.e., |\u03c6($r_l$) \u2013 \u03c6($r_l^\\ast$)| \u2264 \u03c4.\nSimilarly, we can define the correctness of an entire reason-\ning process as follows:\nDefinition 4.2. (\u03c4-correct reasoning.) A response R is\nconsidered as \u03c4-correct if all steps in the sequence are \u03c4-\ncorrect, i.e., \u2200l, |\u03c6($r_l$) \u2212\u03c6($r_l^\\ast$)| \u2264 \u03c4 (denoted as \u03c8(R) \u2264 \u03c4).\nDefinition 4.1 and Definition 4.2 provide formal definitions\nfor measuring the correctness of a reasoning step or an entire\nreasoning process. Intuitively, for a given task, the proba-\nbility of achieving a \u03c4-correct step is determined by three\nprimary factors: the capability of the LLM, the correctness\nthreshold \u03c4, and the length of the current reasoning path.\nThe results shown in Figure 2 indicate that the average MI\ncan decrease exponentially with the reasoning length, which\nsuggests that the average snowball errors increase exponen-\ntially with the reasoning length. Since the probability cannot\nexceed 1, with Theorem 3.3, we thus hypothesize that the\nprobability of encountering an error in practice follows an\nexponential decay function: P($e_r$) = 1 \u2212 \u03bb$e^{-l}$ for the con-\nvenience of subsequent analyses. Thus the probability of\ngenerating a correct step in layer I can be proposed as:\nProposition 4.3. (Probability of \u03c4-correct step.) We pro-\npose that the probability of generating \u0430 \u03c4-correct step is\nrelated to the layer index l in the following way:\nPr [|\u03c6($r_l$) \u2013 \u03c6($r_l^\\ast$)| \u2264 \u03c4] = min(\u03bb,e$^{-l}$,1),"}, {"title": "4.3. External Slow-Thinking Mechanisms", "content": "where \u03bb is a constant relevant to correctness \u03c4", "follows": "nLemma 4.4. (Probability of \u03c4-correct reasoning.) The\nprobability of generating \u0430 \u03c4-correct response R is:\nPr [\u03c8(R) \u2264 \u03c4", "\u03c4": "n< $e^{-\\frac{\u03bbL(L+1)"}, {"char-\nacteristics": 1, "as": "nPr [\u03c8(R) \u2264 \u03c4"}]}