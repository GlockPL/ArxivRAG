{"title": "Assessing Bias in Metric Models for LLM Open-Ended Generation Bias Benchmarks", "authors": ["Nathaniel Demchak", "Xin Guan", "Zekun Wu", "Ziyi Xu", "Adriano Koshiyama", "Emre Kazim"], "abstract": "Open-generation bias benchmarks evaluate social biases in Large Language Models (LLMs) by analyzing their outputs. However, the classifiers used in analysis often have inherent biases, leading to unfair conclusions. This study examines such biases in open-generation benchmarks like BOLD and SAGED. Using the MGSD dataset, we conduct two experiments. The first uses counterfactuals to measure prediction variations across demographic groups by altering stereotype-related prefixes. The second applies explainability tools (SHAP) to validate that the observed biases stem from these counterfactuals. Results reveal unequal treatment of demographic descriptors, calling for more robust bias metric models.", "sections": [{"title": "Introduction", "content": "Bias in large language models poses significant concerns (Bender et al., 2021), with numerous benchmarks and metrics proposed for evaluation (Gallegos et al., 2024; Nadeem et al., 2020; Nangia et al., 2020; Forbes et al., 2021). A common approach involves generating responses and using NLP classifiers as metric models, as demonstrated by BOLD (Dhamala et al., 2021) and SAGED (Guan et al., 2024). However, these classifiers often carry inherent biases that can skew bias assessments, as highlighted by SAGED. To address this issue, our study systematically examines these biases by evaluating how various bias metrics respond to stereotypes across demographic groups using counterfactuals and popular classifiers. Additionally, we employ explainability methods, such as SHAP, to analyze the contribution of specific demographic descriptor to classifier predictions, providing deeper insights into the underlying causes of observed biases."}, {"title": "Methodology", "content": "We assessed the responses of various bias-detection models to stereotypes by creating counterfactual sentences using the MULTI-GRAIN STEREOTYPE DATASET (MGSD) (Zekun et al., 2023). First, we filtered the dataset to include only sentences labeled as \"stereotype\" and then modified them by prepending demographic descriptors correlated with each stereotype type in MGSD: race (\u201cWhites,\u201d \u201cBlacks,\u201d \u201cAsians\u201d), gender (\u201cMales,\u201d \u201cFemales,\u201d \u201cNon-binaries\u201d), profession (\"Construction workers,\u201d \u201cBankers,\u201d \u201cDoctors,\u201d \u201cNurses,\u201d \u201cTeachers\u201d), and religion (\u201cAtheists,\u201d \"Christians,\" \"Jews,\u201d \u201cMuslims\"). This ensured relevance and consistency, as certain stereotypes related to one category (e.g., race) may not apply to another (e.g., profession). Then, Each modified sentence was paired with one of four randomly selected stereotype-related prefixes-\"often assume that,\" \"usually think that,\u201d \u201csometimes feel that,\" and \"mostly say that\" using a fixed random seed (42) for reproducibility. For instance, \u201cThe Finnish man was very energetic\" was transformed into \"Males/Females/Non-binaries usually think that the Finnish man was very energetic.\" These counterfactuals were then evaluated using four common bias metric models: Detoxify (Hanu & Unitary team, 2020), Regard (Sheng et al., 2019), DistilBERTSentiment (Lik Xun Yuan, 2023), and vaderSentiment (Hutto & Gilbert, 2014). Next, SHAP (SHapley Additive exPlanations) (Lundberg & Lee, 2017) was applied to show how demographic descriptors influence negative scores in classifier predictions, providing insights into the impact of these descriptors on model outputs."}, {"title": "Experiments", "content": "Table 1 shows the Max-Min and Min/Max disparity metrics of each group, where Max is the maximum score and the Min is the minimum of the group in Gender/Profession/Race/Religion. The results show that RegardV3 is the most biased classifier, showing the largest disparities, especially against racial and religious groups. Race is the most biased group, with Detoxify exhibiting significant variations in toxicity scoring across different races. In comparison, Vader shows less bias overall. Table 2 (in appendix) further demonstrates varied model responses to demographic descriptors. DistilBERT and Regard showed similar negativity towards males and females, with lower scores for non-binaries. VaderSentiment maintained uniform scores across genders. For professions, DistilBERT and Detoxify were most negative towards \"Doctors\" and \"Bankers,\" whereas Detoxify showed minimal toxicity. For race, DistilBERT was most negative towards \"Blacks\" and \"Asians,\" while Regard targeted \"Blacks\" and \"Whites.\" In religion, Regard showed strong negativity towards \"Atheists\" and \"Jews,\" while DistilBERT was more uniform. Detoxify and VaderSentiment exhibited more balanced responses across race and religion.\nFor the second experiment, we focused on sentences like \"He was a butcher for 30 years before retiring,\" which exhibited significant variations in negative sentiment scores across different groups. As shown in Figure 1, We utilized SHAP with the RegardV3 model to assess the impact of each word, including demographic descriptors, on model predictions. For example, the descriptor \"Teachers\" contributed significantly to negative sentiment, while \"Bankers\" had minimal effect. These results highlight that even subtle demographic cues can disproportionately influence model predictions, emphasizing the need to complement bias metrics with explainability tools."}, {"title": "Future Work and Limitation", "content": "Future research should focus on developing debiasing techniques to mitigate stereotype influence in model predictions. Refining counterfactual generation to capture contextual nuances and reducing reliance on specific bias metrics are crucial, as current methods may oversimplify complex biases. Employing diverse explainability techniques, beyond SHAP such as LIME, BERTViz, etc., is essential for ensuring model transparency and consistency. Expanding demographic descriptors and incorporating real-world contexts can improve bias assessment robust-ness. Cross-validation with different models and benchmarks will further validate the reliability and generalizability of these approaches."}]}