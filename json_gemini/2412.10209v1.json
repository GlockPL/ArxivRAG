{"title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion", "authors": ["Jiapeng Tang", "Davide Davoli", "Tobias Kirschstein", "Liam Schoneveld", "Matthias Nie\u00dfner"], "abstract": "We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve details of facial identity and appearance. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms the previous state-of-the-art methods in novel view synthesis by a 5.34% higher SSIM score. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices.", "sections": [{"title": "1. Introduction", "content": "Creating photorealistic and animatable head avatars has long been a challenge in computer vision and graphics. This is crucial for a vast variety of applications such as immersive telepresence in virtual and augmented reality, computer games, movie effects, virtual classrooms, and videoconferencing. Here, the goal is to generate avatars that can be realistically rendered from any viewpoint, accurately capturing facial geometry and appearance, while also enabling easy animation for realistic head portrait videos depicting various expressions and poses. The democratization of high-fidelity head avatars from commodity devices is a challenge of widespread interest. However, photo-realistic head avatar reconstruction from monocular videos is challenging and ill-posed, due to limited head observations causing a lack of constraints for novel-view rendering.\nTo create photorealistic, animatable human avatars, researchers have integrated NeRF [41, 42] and Gaussian Splatting (GS) [22] techniques with parametric head models [16, 30, 64], enhancing control over unseen poses and expressions. Approaches like [17, 48, 76, 77] achieve high-quality head reconstructions and realistic animations using dense multi-view datasets, typically captured in controlled studio settings. However, these methods encounter significant limitations with monocular recordings from commodity cameras, such as smartphone portrait videos. Monocular methods [1, 8, 14, 57, 75, 87, 88, 91] aim to reconstruct head avatars from single-camera videos but often rely on substantial head rotations to capture various angles. They only reconstruct visible regions from input frames, leaving occluded areas incomplete and under-constrained. For instance, as illustrated in Fig. 4, front-facing videos provide limited capture on side regions, leading to visible artifacts when rendering extreme side views.\nTo this end, we introduce a multi-view head diffusion model that learns the joint distribution of multi-view head images. Given a single-view input image, our model generates a set of view-consistent output images. By leveraging view-consistent diffusion priors on human heads, our approach enables the robust reconstruction of Gaussian avatars, preserving both appearance and identity across novel perspectives. Unlike existing methods [34, 59, 71, 73] that use camera pose as a conditioning factor for viewpoint control, we use the normal map rendered from a reconstructed parametric face model as guidance. The normal maps provide stronger inductive biases, offering more precise and reliable novel view generation for heads. Specifically, we condition our multi-view diffusion process on 2D features extracted from the input image's autoencoder, rather than only using high-level semantic vectors like CLIP embeddings [49]. This design allows us to incorporate fine-grained identity and appearance details directly into the multi-view denoising process, ensuring that the generated images maintain coherence and consistency across viewpoints in terms of identity and appearance.\nIn order to exploit multi-view diffusion priors for Gaussian head reconstruction, we employ a diffusion loss that uses iteratively denoised images as pseudo-ground truths for novel view renderings, instead of using a single-step score distillation sampling loss [45]. Moreover, to improve the fidelity of the Gaussian renderings, we introduce a latent upsampler model to enhance facial details within the denoised latent before decoding it back to image space. As demonstrated in Fig. 1, our approach generates high-quality, photorealistic novel views of head avatars using only short monocular videos captured by a smartphone. Extensive comparisons with state-of-the-art methods show that our approach delivers higher fidelity and more view-consistent avatar reconstructions from monocular videos. Our GAF significantly outperforms state-of-the-art methods, achieving 5.34% higher SSIM in novel view synthesis on the NeRSemble dataset.\nOur contributions can be summarized as follows:\n\u2022 We introduce a novel approach for reconstructing photorealistic, animatable head avatars from monocular videos with limited coverage captured on commodity devices, using multi-view head diffusion priors to regularize and complete unobserved regions.\n\u2022 We propose a multi-view head diffusion model that generates consistent multi-view images from a single-view input, guided by normal maps rendered from FLAME head reconstructions to improve viewpoint control.\n\u2022 We present a mechanism for enhancing the photorealism and cross-view consistency of Gaussian splatting by integrating latent upsampling and multi-view diffusion priors."}, {"title": "2. Related Work", "content": "3D Scene Representations. Neural radiance fields [41] and its variants [2-4, 7, 9, 13, 42] revolutionized 3D scene reconstruction from multi-view images. However, NeRF-based methods are often hindered by computational inefficiency during both training and inference stages. Gaussian Splatting [22] represents a scene as a composition of discrete geometric primitives, i.e. 3D Gaussians, and employs an explicit rasterizer for rendering. Compared to NeRF, GS has achieved notable runtime speedups in the training and inference stages. This enables real-time performance and more favorable image synthesis. Unlike polygonal meshes, which require careful topology handling, GS supports substantial topology changes, making it more adaptable to complex surfaces and varying densities.\nParametric Face/Head Models. Based on statistical priors of 3D morphable models (3DMM) [5, 19, 30, 68, 70], many works [12, 18, 23, 67] learn 3D face/head reconstruction and tracking from single RGB images/videos. More recent methods [16, 64, 85, 87] leveraged signed distance fields [44] and deformation fields [43, 61, 62] based on coordinate-MLPs for more fine-grained geometry reconstruction including hair and beards. HeadGAP [86] and GPHM [78] learned parametric models for head Gaussians. Our work uses the VHAP tracker [47] to obtain coarse geometries as guidance for dynamic avatar reconstruction.\nPhoto-realistic Avatar Reconstruction. To create photorealistic animatable head avatars, several works [14, 48, 57, 75, 76, 91] have combined NeRF/GS with 3D morphable models (3DMM). NerFace [14] learned expression-dependent NeRF using head parameters from monocular videos. INSTA [91] and H3QAvatar [66] mapped query points to a canonical space via deformation fields and defined multi-resolution hash grids for head radiance fields. PointAvatar [88] explored point-based representations for efficient training. Our work is closely related to animatable Gaussian splats [48, 57], which attached splats to the triangles of a FLAME mesh, and updated their properties by triangle deformations controlled by FLAME parameters. Although promising results have been achieved, they typically require multi-view setups with high-quality cameras in studio environments [27, 36]. Some works reconstruct avatars from monocular videos [1, 75, 88, 91]. However, they only reconstruct the visible region from inputs, lacking effective priors to complete missing areas. To address this, we utilize multi-view diffusion priors to complete unobserved regions of the face, ensuring photorealistic renderings from"}, {"title": "Distillation 2D Priors for 3D Generation.", "content": "Distillation 2D Priors for 3D Generation. Researchers have demonstrated that large-scale pretrained text-to-image priors [51, 52, 55] can be distilled for 3D asset generation [31, 63] using the score distillation sampling (SDS) loss proposed by DreamFusion [45] and its variants [39, 72, 81]. Several studies [6, 20, 90] applied SDS loss for text-driven 3D head avatar generation. For single-view 3D reconstruction, approaches like RelFusion [39], Magic123 [46], and Dreambooth3D [50] adapt text-to-image diffusion priors to specific objects to preserve identity. However, achieving consistent novel views from text prompts alone remains challenging without leveraging the input image. To address this, we choose to learn multi-view image diffusion priors conditioned on a single image. Rather than using a single-step SDS loss, we use iteratively denoised images as pseudo-ground truths for novel view supervision, similar to ReconFusion [74]. By integrating latent upsampler diffusion with joint multi-view latent denoising, we enhance both multi-view consistency and fidelity."}, {"title": "Multi-view Diffusion Models.", "content": "Multi-view Diffusion Models. Instead of text-to-image diffusion priors, some works learn image-conditioned novel view diffusion priors, which leverage the identity and appearance details of the input image to generate consistent novel views. 3DiM [73] pioneered a diffusion model for pose-conditioned novel view generation. Zero-123 finetune StableDiffusion [52] on Objaverse [10, 11] dataset to improve generalization ability. Some works learn to generate consistent multiple views from a single image via introducing dense 3D self-attention or epipolar attention in the denoiser network, including MVDiffusion [65], SyncDreamer [35], Zero-123++ [58], Wonder3D [37], MV-Dream [59], ImageDream [71], Human3Diffusion [79], IM3D [40], CAT3D [15], and VideoMV [92]. These works primarily target general objects, whereas our focus is specifically on human heads. To obtain more specialized priors, we train multi-view diffusion models on a dedicated multi-view head video dataset. By using normal maps from FLAME mesh reconstruction as the camera condition, we achieve precise viewpoint control for head rendering. This pixel-aligned inductive bias introduced by normal maps enables more accurate novel view synthesis, crucial for consistent head Gaussian supervision."}, {"title": "3. Preliminaries", "content": "3.1. 3D Gaussian Splatting\n3D Gaussian Splatting [22] parameterizes a scene using a set of discrete geometric primitives known as 3D Gaussian splats. Each splat is characterized by a covariance matrix $\\Sigma$ centered at location $\\mu$. The covariance matrices are required to be semi-definite for physical interpretations. It utilizes a parametric ellipsoid definition to construct the covariance matrix $\\Sigma = RSST^R$ using a scaling matrix $S$ and a rotation matrix $R$. These matrices are independently optimized and represented by a scaling vector $s \\in \\mathbb{R}^3$ and a quaternion $q \\in \\mathbb{R}^4$. $r \\in \\mathbb{R}^{3 \\times 3}$ denotes the corresponding rotation matrix of $q$. To represent scene appearance, sphere harmonic coefficients are used to represent the color $c$ of each Gaussian. During rendering, a tile-based rasterizer is used to a-blend all 3D Gaussians overlapping the pixel within a tile. To respect visibility order and avoid per-pixel sorting expenses, splats are sorted based on depth values within each tile before blending.\n3.2. Gaussian Avatars\nTo animate Gaussian splats for head avatars, we use the representation of GaussianAvatars [48] to rig 3D Gaussians using the FLAME mesh. Initially, they bind each triangle of the FLAME identity mesh with a 3D Gaussian and transform the 3D Gaussian based on triangle deformations across different time steps. The splat remains static in the local space of the attached triangle but can dynamically evolve in the global metric space along rotation, translation, and scaling transformations of the binding triangle. For each triangle, they compute the mean position $T$ of its three vertices coordinates as the origin of the local space. They define a rotation matrix $R$ to depict the orientation of the triangle in the global space, composed of three column vectors derived from the direction vector of one edge, the normal vector of the triangle face, and their cross product. Additionally, they determine triangle scaling $k$ by calculating the mean length of one edge and its perpendicular in a triangle. The 3D Gaussian is parameterized by the location $\\mu$, rotation $r$, and anisotropic scaling $s$ in the local space of its parent triangle. In the initialization stage, the location $\\mu$ is set as zero, rotation $r$ as an identity matrix, and scaling $s$ as a unit vector. During rendering, they transform these properties from local to global space by:\n\n$r' = Rr$ (1)\n\n$\\mu' = kR\\mu + T$ (2)\n\n$s' = ks$ (3)\n\nTo capture fine-grained details in the rendered output, an adaptive density control strategy dynamically creates new Gaussians for each triangle based on screen-space positional gradients. This densification process operates within local space, where newly added Gaussians inherit binding relations from their original counterparts."}, {"title": "4. Gaussian Avatars Fusion", "content": "Given a monocular sequence of RGB images $\\mathcal{I} = \\{I_i\\}_{i=1}^I$ as input, our goal is to reconstruct head Gaussian splats $\\mathcal{O} = \\{O\\}$ as output. The overall pipeline is shown in"}, {"title": "4.1. Normal map-conditioned Multi-view Head Latent Diffusion", "content": "To address missing regions in monocular videos with limited head coverage, one solution is to distill pre-trained text-to-image diffusion models to regularize novel view renderings. Personalized techniques like Dreambooth [54] allow for identity preservation by customizing diffusion models for specific objects we wish to reconstruct. However, since monocular videos may lack comprehensive side-view coverage, these models are prone to view bias: generated images tend to default to front-view perspectives, making it challenging to produce plausible novel views. Nevertheless, personalized text-to-image diffusion models are not designed to capture the distribution of novel views from a single input image. To overcome this, we propose a novel view diffusion model that generates identity-preserved and appearance-coherent novel views conditioned on an input image. By denoising multiple novel views simultaneously, our approach enhances cross-view consistency. The multi-view head diffusion model is illustrated in Fig. 3.\nNormal Map Conditioning. To control viewpoint, we leverage normal maps rendered from the FLAME mesh reconstruction at target views as diffusion guidance. The normal map is first encoded into a latent representation via pretrained VAE [25], matching the dimensionality of the noisy image latent, and these two latents are then concatenated along the channel dimension. Compared to camera pose embeddings, normal maps offer a more explicit inductive bias for view synthesis by providing pixel-aligned conditioning, which facilitates alignment between the generated images and the conditioning normal maps. Moreover, the FLAME renderings also facilitate the expression accuracy of synthesized novel views.\nModel Architecture. We train a multi-view diffusion model that takes a single image of a head, $I_{cond}$, as input and generates multiple output images conditioned on normal maps rendered from desired camera poses using the FLAME reconstruction $M$. Specifically, given $I_{cond}$ and the FLAME mesh $M$, the model learns the joint distribution of $N$ target images, $I_{tgt}$, guided by N normal maps, $N_{tgt}$, which are rendered from $M$ at the target camera poses.\n\n$p(I_{tgt} | I_{cond}, N_{tgt})$ (4)\n\nOur model architecture is similar to multi-view diffusion models (MVLDM) [15, 59, 71, 79], which is based on 2D U-Net [53] and attention blocks [69]. As shown in Fig. 3, we use the CLIP image embedding to achieve global control over novel view generation. However, this embedding mainly contains high-level semantic features, lacking the detailed information necessary for accurately capturing the"}, {"title": "4.2. Gaussian Avatars Reconstruction with Multi-view Diffusion Priors", "content": "We now seek to utilize the multi-view diffusion priors for head Gaussian reconstruction. A commonly used strategy is Score Distillation Sampling (SDS) loss [45]. Let us denote the rendered image from Gaussian as x. One can add noise $\\epsilon$ at time step t to the input image x to obtain a noisy image $x_t$, which is fed into denoising network to estimate out the noise $\\hat{\\epsilon}$. The SDS loss calculates the difference $|\\epsilon - \\hat{\\epsilon}||^2$. The gradients can be back-propagated to update 3D representation. The SDS loss essentially performs one-step denoising. Due to the stochastic nature of the denoising process introduced by random noise levels and seeds, it contains noisy gradients that disturb 3D optimization. Consequently, it often causes over-saturated appearance issues in synthesized 3D assets. Wu et al. [74] found that after iteratively denoising $x_t$ for multiple steps, we can obtain a deterministic output $x_0$. Based on this observation, we can calculate $||x_0 - x||$ as the diffusion loss for 3D Gaussian optimization.\nPseudo-image Ground Truths. At each iteration, we randomly select the i-th input frame $I_i$, and its FLAME mesh $M_i$. By randomly sampling 4 viewpoints $\\{\\phi_j\\}_{j=1}^4$"}, {"title": "4.3. Loss Functions", "content": "We supervised the optimization of Gaussian Splats O by a combination of loss functions in the following:\n\n$\\mathcal{L} = \\mathcal{L}_{img}(I_{rec}, I) + \\mathcal{L}_{img} (I_{view}, \\hat{I}_{view}) + \\lambda_{pos}\\mathcal{L}_{pos} + \\lambda_{scale}\\mathcal{L}_{scale}$ (5)\n\n$\\mathcal{L}_{img}$ is defined by a combination of pixel-wise $L_1$ loss, SSIM loss, and LPIPS loss:\n\n$\\mathcal{L}_{img} = \\lambda_1 L_1 + \\lambda_2 L_{SSIM} + \\lambda_3 \\mathcal{L}_{LPIPS}$ (6)\n\nwhere $\\lambda_1 = 0.8$, $\\lambda_2 = 0.2$, and $\\lambda_3 = 0.1$. We also introduce splat position and scale regularization terms to penalize abnormally distributed splats. The position regularization term ensures that Gaussians remain close to their attached triangles during optimization through:\n\n$\\mathcal{L}_{pos} = ||max(\\mu, \\xi_{pos}) ||^2$ (7)\n\nwhere $\\xi_{pos} = 1$ serves as the threshold, allowing small positional errors within the scaling of the attached triangle. The scale regularization term mitigates the formation of large Gaussians, which could lead to jittering problems due to small rotations of triangles.\n\n$\\mathcal{L}_{scale} = ||max(s, \\xi_{scale}) ||^2$ (8)\n\nIt will be disabled when the local scale of the Gaussian w.r.t the attached triangle is less than $\\xi_{pos} = 0.6$. $\\lambda_{pos}$ and $\\lambda_{scale}$ are set to 0.01 and 1 respectively."}, {"title": "4.4. Implementation Details", "content": "Multi-view Head Diffusion Model. It is initialized from Stable Diffusion 2.1 [52] of ImageDream [71] and is trained on eight A100 GPUs over 20,000 iterations, taking approximately 72 hours. The training uses a learning rate of 0.0001 and a batch size of 64. During training, we employ a classifier-free guidance strategy, randomly dropping the input image at a rate of 0.1. The model is trained on the multi-view human head video dataset NeRSemble [27], which contains RGB video sequences from 16 viewpoints, covering both front and side faces. We randomly sample 50,000 timesteps to construct the training dataset.\nGaussian Avatar Optimization. The FLAME meshes are initially obtained by VHAP tracker [47] from monocular videos. The animatable Gaussians are optimized with Adam [26] for 6,000 iterations, with learning rates of 5e-5,1.7e-2, 1e-3, 2.5e-3, and 5e-2 for splat position, scaling factor, rotation quaternion, color and opacity respectively. We perform adaptive densification if the position gradients are larger than 0.0002 in every 300 iterations until 5,000 iterations are reached. We also remove Gaussians with opacity less than 0.005. During training, we also finetune the FLAME parameters using the learning rates 1e-6, 1e-5, and 1e-3 for translation, joint rotation, and expression coefficients. We randomly sample 4 novel viewpoints to calculate multi-view diffusion loss. The azimuth range is [-90\u00b0, 90\u00b0], the elevation range is [-30\u00b0, 30\u00b0], and the distance between the camera and the head avatar location is between 1.0 and 1.2 units.\nRuntime. Our current implementation of avatar reconstruction takes about 12 hours and uses 32 GB of memory on a single A6000 GPU. After avatar reconstruction, our method takes 0.0026 seconds to render an image at 768\u00d7768 resolution, i.e. 384.3fps."}, {"title": "5. Experiments", "content": "Datasets. We conduct head avatar reconstruction experiments on monocular video sequences from the NeRSemble dataset [27]. Note that these evaluation sequences were not seen by the multi-view diffusion model. We use monocular videos from the 8-th camera as the input, only capturing the head from the front view. And we use videos from the other 15 views for evaluation. We randomly select 12 sequences from different identities, with durations between 70 and 300 frames, downsampled to a resolution of 802 \u00d7 550. Additionally, we include a Monocular Video dataset consisting of 3 monocular videos from the INSTA [91] dataset at 512 \u00d7 512 resolution, and 3 sequences of three subjects captured by smartphone at 1280 \u00d7 720 resolution.\nEvaluations. Following previous works [48, 91], we report the average L1 loss, LIPIS, PSNR, and SSIM between ren-"}, {"title": "5.1. Head Avatar Reconstruction", "content": "NeRSemble. As shown in Fig. 4, front-facing monocular videos lack sufficient side-face information. Existing methods can only reconstruct observed regions and leave unobserved areas unconstrained. This often leads to artifacts in extreme hold-out views. In contrast, our approach leverages multi-view diffusion priors to constrain novel views, effectively completing missing regions and enhancing photorealism, while preserving identity and appearance consistency. In Tab. 1, our method surpasses all baselines across metrics, with notable gains of 1.69dB in PSNR and a 5.34% increase in SSIM for novel view synthesis.\nMonocular Videos. We also provide the comparisons on the Monocular Video dataset. Since single-view datasets lack ground-truth novel views, we evaluate animation results by applying expression parameters from hold-out frames. As shown in Tab. 2, our method consistently sur-"}, {"title": "5.2. Ablation Studies", "content": "We conduct detailed ablation studies to verify the effectiveness of each design in our multi-view head diffusion prior learning. We select six sequences from NeRSemble dataset, including '055 EXP-5', '098 EMO-1', '134 EMO-1', '165 EMO-1', '221 EXP-8', and '417 EMO-4'. The results are presented in Figure 6, and Table. 3.\nWhat is the effect of multi-view head diffusion priors? An alternative is to use pre-trained text-to-image diffusion models Stable Diffusion [52]. Another alternative is to finetune Stable Diffusion on RGB frames from the input video, obtaining personalized image diffusion priors. We implement both variants using normal map guidance of ControlNet [84]. In Fig. 6 (d), pre-trained text-to-image priors often produce renderings that deviate from the original identity. In Fig. 6 (e), personalized diffusion priors improve identity preservation but struggle with appearance consistency, as they lack input image information to hallucinate novel views. Our approach learns to jointly generate multiple novel views conditioned on the input image, thus achieving higher realism and view consistency in both identity and appearance.\nWhat is the effect of normal map condition for multi-view diffusion models? Our multi-view head diffusion models condition on normal maps of FLAME reconstruction, which are pixel-aligned with the target novel-view images. From Fig. 6 (f) reflects that using camera pose conditioning could introduce obvious misalignment errors in synthesizing pseudo-image ground truths, leading to blurred 3D Gaussian renderings.\nWhat is the effect of iteratively denoised images as pseudo-ground-truths? We can instead use Score Distillation Sampling (SDS) loss to constrain multi-view renderings. As shown in Fig. 6 (g), SDS loss has appearance oversaturation issues in the face region.\nWhat is the effect of latent upsampler module? Through Fig. 6 (h) vs. (i), we can see that the upsampler module significantly sharpens and enhance facial appearance details."}, {"title": "5.3. Robustness Analysis", "content": "To demonstrate the robustness of our method with sparse input data, we evaluate reconstruction performance across different frame numbers in the input video. We use the '104 EMO-1' sequence from the NeRSemble dataset, which contains 56 frames in the input. To reduce the frame count, we sample keyframes at uniform intervals. For instance, for an 8-frame input, we select frames at timesteps 0, 7, 14, 21, 28, 35, 42, and 55; for a single-frame input, we use only the 28th frame. As shown in Fig. 15, our method can maintain stable quantitative performance with as few as 8 frames, while GaussianAvatars drops dramatically. This highlights the resilience of our method to limited observations."}, {"title": "5.4. Limitations and Future Work", "content": "While our work has shown promising results in dynamic avatar reconstruction from monocular videos captured on studio setups or commodity devices, there are limitations in our current method. First, we do not explicitly separate the material and appearance of heads, which could enable re-lighting applications [24, 56]. Second, optimizing head Gaussians using iteratively updated pseudo ground-truths from diffusion models is time-consuming. We plan to explore real-time 4D avatar reconstruction with feed-forward large reconstruction models [21, 29, 83]. Lastly, the quality of our avatar reconstruction and animation is limited by the expressiveness of current parametric head models, which lack detailed hair geometry and animation. Future work could extend Gaussian head avatars to incorporate fine-grained hair modeling and animation. [38, 82]."}, {"title": "6. Conclusion", "content": "In this work, we present a novel method to reconstruct photo-realistic head avatars from monocular videos to push the frontier of avatar fidelity from commodity devices. Due to limited observation and coverage of human heads, Gaussian reconstruction from monocular videos is inherently under-constrained. To address this challenge, we introduce multi-view diffusion priors that jointly constrain photorealism across multiple views rendered from Gaussian splats, while preserving face identity and appearance consistency. We obtain these priors by designing a multi-view head diffusion model, fine-tuned on a multi-view head video dataset to generate novel views from a single image, conditioned on rendered normal maps from FLAME head reconstruction. To prevent appearance over-saturation, we apply an effective diffusion loss using iteratively denoised images as pseudo-ground truths. For finer facial details, we combine a latent upsampler diffusion model with our multi-view diffusion. By reducing data capture requirements for avatar creation, our approach has the potential to unlock new opportunities in immersive VR/AR applications and products."}, {"title": "A. Dataset", "content": "Smartphone Video Capture. We capture monocular video sequences using an iPhone 14 Pro. The subject is seated in a chair, and the room lights are turned on during the recording, providing adequate illumination. The duration of the recording is about 10-15 seconds, at 30 frames per second. The image resolution is 1280 \u00d7 720.\nData preprocessing. To simplify the optimization process for animatable Gaussian splats, we integrate two preprocessing steps on raw images extracted from monocular videos. Firstly, we leverage the image matting techniques proposed in [32, 33] to remove the background. More specifically, we use [33] for our smartphone video capture, while we adopt [32] for the NeRSemble [27] dataset, where the initial background image is provided. Secondly, we utilize face segmentation maps acquired from BiSeNet [80] to isolate and crop out the torso portion, thus concentrating solely on head reconstruction. An example of our image preprocessing pipeline is illustrated in Fig. 8."}, {"title": "B. Implementations", "content": "B.1. Monocular Head Tracking\nWe track the FLAME [30] parameters using the VHAPtracker [47] proposed in [48]. Given a monocular video we optimize both shared parameters (shape, albedo map, diffuse light) and per-timestep parameters (pose, translation, expression). The tracking algorithm is divided into three stages: (i) initialization stage; (ii) sequential optimization stage; (iii) global optimization stage. The tracking process"}, {"title": "B.2. Multi-view Latent Head Diffusion", "content": "In Fig. 10, we show the network architecture details of our multi-view head latent diffusion. The denoiser network is based on a 2D U-Net [53] with attention blocks [69]. The U-Net comprises four Down Blocks, one Middle Block, and four Up Blocks. Each Down Block contains a Residual block, a 3D Attention block, and a Downsampling layer. The Middle Block is composed of a Residual block and a"}, {"title": "C. Generation Results of Multi-view Head Diffusion", "content": "In Fig. 9, we showcase the sampling results from our multi-view head diffusion model. The model generates four view-consistent images from a single input image while effectively preserving facial identity and appearance. This demonstrates the model's capability to synthesize coherent and identity-preserving novel views."}, {"title": "D. Additional Comparisons", "content": "D.1. Dynamic Head Avatar Reconstruction\nIn Fig. 11 and Fig. 12, we provide additional qualitative comparisons on dynamic head avatar reconstruction from monocular videos sampled from NerSemble dataset [27].\nD.2. Self- & Cross-Reenactment\nWe show the self and cross reenactment results of our method and Gaussian Avatars in Fig. 13 and 14.\nD.3. Robustness Analysis\nIn Fig. 15, we present qualitative results from a robustness analysis conducted with varying numbers of frames in the input monocular videos. Our approach consistently achieves photorealistic novel view rendering across various sequence lengths, even with only 8 frames as input."}, {"title": "E. Ethical Discussion and Negative Impacts", "content": "The creation of photorealistic and animatable head avatars from an input video poses several ethical challenges and significant risks related to the possible malevolent usage of this technology. One major concern is the potential for misuse in creating deepfakes, which are highly realistic but fake videos that can be used to spread misinformation, manipulate public opinion, or damage reputations. Additionally, this technology can lead to privacy violations, as individuals' likenesses can be replicated without their consent, leading to unauthorized use in various contexts. There is also the risk of identity theft, where malicious actors could use these avatars to impersonate others for fraudulent activities. Moreover, the psychological impact on individuals who see their digital likeness used inappropriately can be profound, causing distress and harm. Our commitment is to promote the responsible and ethical use of this technology, and we are firmly against any malicious usage that aims to harm individuals or communities."}]}