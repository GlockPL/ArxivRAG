{"title": "ForgeryGPT: Multimodal Large Language Model For Explainable Image Forgery Detection and Localization", "authors": ["Jiawei Liu", "Fanrui Zhang", "Jiaying Zhu", "Esther Sun", "Qiang Zhang", "Zheng-Jun Zha"], "abstract": "Multimodal Large Language Models (MLLMs), such as GPT4o, have shown strong capabilities in visual reasoning and explanation generation. However, despite these strengths, they face significant challenges in the increasingly critical task of Image Forgery Detection and Localization (IFDL), where subtle tampering traces are frequently overlooked. Moreover, existing IFDL methods are typically limited to the learning of low-level semantic-agnostic clues, neglecting the exploration of rich high-level semantic knowledge within forged images, and merely provide a single outcome judgment, lacking an inference process or explanation. To tackle these issues, we propose ForgeryGPT, a novel framework that advances the IFDL task by capturing high-order forensics knowledge correlations of forged images from diverse linguistic feature spaces, while enabling explainable generation and interactive dialogue through a newly customized Large Language Model (LLM) architecture. Specifically, ForgeryGPT enhances traditional LLMs by integrating the Mask-Aware Forgery Extractor, which enables the excavating of precise forgery mask information from input images and facilitating pixel-level understanding of tampering artifacts. The Mask-Aware Forgery Extractor consists of a Forgery Localization Expert (FL-Expert) and a Mask Encoder, where the FL-Expert is augmented with an Object-agnostic Forgery Prompt and a Vocabulary-enhanced Vision Encoder, allowing for effectively capturing of multi-scale fine-grained forgery details through cross-modal reasoning and enhancing tampering localization at the knowledge level. To comprehensively train ForgeryGPT, beyond standard image-text feature alignment, we elaborately construct a Mask-Text Alignment Pre-training dataset utilizing the generated multi-granularity forged images, which allows for precise alignment of forgery masks within the LLM feature space. Subsequently, we leverage the Task-Specific Instruction Tuning dataset to refine the model's detection, localization and dialogue capabilities, ensuring robust performance and explainable inference in forgery-specific scenarios. Extensive experimental results across multiple benchmarks demonstrate that ForgeryGPT significantly outperforms state-of-the-art methods, offering the first step towards integrating the IFDL task with explainable, multi-turn dialogue capabilities while also showing remarkable generalization across datasets from diverse domains.", "sections": [{"title": "I. INTRODUCTION", "content": "HE swift evolution of media technologies and image editing tools has made photo manipulation more accessible and widespread, often leading to significant societal consequences, such as the generation of misleading information, the falsification of evidence in judicial contexts, and unauthorized removal of copyright watermarks [1]\u2013[5]. This extensive dissemination of altered images has occasionally precipitated public panic and social unrest. In response to these challenges, the field of Image Forgery Detection and Localization (IFDL) has emerged as a critical area of research. IFDL aims to verify the authenticity of images and pinpoint any modifications made to them. Given the rapid development of sophisticated forgery techniques, including advanced diffusion models, and the speed at which these manipulated images are spread, traditional manual verification methods are becoming inadequate [2], [6]\u2013[8]. Therefore, there is an urgent need for automated systems capable of effectively detecting and localizing image tampering to ensure the dissemination of authentic information across social platforms.\nTo tackle this issue, various IFDL methods have been developed to extract low-level forgery clues and learn semantic-agnostic features for identifying manipulation at both the image-level and the pixel-level, leveraging deep learning-based semantic segmentation frameworks. These methods generally fall into two categories: spatial-based approaches focusing on detecting low-level visual artifacts, such as noise or inconsistencies in textures [9]\u2013[13] and frequency-based approaches relying on analyzing high-frequency pattern analysis to detect subtle alterations [8], [14], [15]. Despite these advancements, most existing approaches overlook the exploration of high-level forensics semantic information within forged images, including the forgery analysis and feature extraction from linguistic and knowledge-driven perspectives. This gap in the excavating of high-level features hinder the detection of more sophisticated manipulations. Additionally, existing IFDLs often generate detection and localization results without providing a clear inference process or explanation. This lack of interpretability prevent users from understanding of the underlying rationale behind the model's decisions, which is critical for evaluating its performance and establishing trust. Furthermore, these methods typically produce a forgery likelihood or confidence score for test samples, relying on manually defined thresholds to distinguish between authentic and forged instances. This dependence on manual thresholding restricts the flexibility and practical applicability of these models across different scenarios.\nIn recent years, Multimodal Large Language Models (MLLMs) have made remarkable progress in tasks that integrate visual information with natural language processing [16]. Models such as GPT-4 [17] and LLaVA [18], along with their extended variants, demonstrate significant potential in handling tasks that merge visual and textual data by leveraging extensive world knowledge and robust visual reasoning capabilities. However, despite their proficiency in generating coherent language explanations and dialogues, applying MLLMs to IFDL tasks remains challenging. These challenges primarily arise from their training on internet-based datasets, which limits their acquisition of domain-specific knowledge necessary for detecting and locating forgeries. Furthermore, the original design of MLLMs lacks the necessary sensitivity to subtle cues of forgery artifacts in images, making it difficult for them to accurately identify and localize forged elements. This limitation underscores the need for specialized adaptations to enhance their performance on IFDL tasks.\nAs depicted in Figure 1, neither existing IFDL methods nor MLLMs adequately address the complexities inherent in the IFDL challenge. To this end, we introduce ForgeryGPT, a novel framework that advances the IFDL task by capturing high-order forensics knowledge correlations of forged images from diverse linguistic feature spaces, while enabling explainable generation and interactive dialogue through a newly designed Large Language Model (LLM) architecture. This allows for more nuanced detection and analysis of image manipulations, as well as enhancing transparency in the detection process. Moreover, ForgeryGPT autonomously detects the presence of forgeries without the need for manual threshold settings and generates precise forgery localization masks through the integrated FL-Expert, offering a comprehensive solution to IFDL task. Concretely, ForgeryGPT consists of three primary components: an Image Encoder, a customized Mask-Aware Forgery Extractor, and a Large Language Model. Given an input image, the model generates a forgery mask, which is combined with text instructions. These inputs are tokenized and transformed into token embeddings, forming image, mask, and text tokens. These interleaved tokens are then processed by the LLM to achieve fine-grained forgery detection and understanding. A key enhancement in ForgeryGPT is the introduction of learnable prompt tokens, which are inserted between image and mask tokens. These prompts act as connectors between the different modality spaces, facilitating efficient information transfer and cross-modal integration. The Image Encoder plays a pivotal role in aligning the visual features with the LLM's feature space, ensuring consistent representation by capturing and mapping complex visual details. To enhance the model's sensitivity to forged regions and forgery patterns, as well as its accuracy and robustness, the Mask-Aware Forgery Extractor is designed to capture both pixel-level forgery details and mask-level visual patterns, significantly strengthening model's capacity for IFDL. Within the Mask-Aware Forgery Extractor, the Forgery Localization Expert (FL-Expert) and the Mask Encoder are integral components that improve the extraction and understanding of forgery mask. The FL-Expert leverages a CLIP-based architecture, augmented by an Object-agnostic Forgery Prompt and a Vocabulary-enhanced Vision Encoder, to proficiently learn complex forgery details and enable manipulation localization at semantic knowledge level. The Object-agnostic Forgery Prompt is dedicated to learning generalized text prompts independent of specific objects, allowing the model to effectively excavate latent contextual knowledge across diverse forgery scenarios. The Vocabulary-enhanced Vision Encoder extends the pre-trained CLIP vision encoder by integrating an additional trainable visual vocabulary network, which enriches the original encoder's coarse-grained visual understanding with fine-grained insights. This improves the model's ability to recognize subtle forgery patterns by enriching its comprehension and absorption of forgery-related information within the existing visual feature space. Finally, the Mask Encoder generates mask tokens that are expertly aligned with the LLM's feature space. This ensures that the representations of forgery masks and text are precisely integrated, facilitating accurate and contextually grounded forgery detection and localization.\nThe training of ForgeryGPT is structured into a comprehensive three-stage optimization strategy, aimed at adapting the tailored LLM to the complex requirements of IFDL tasks. 1) In the first stage, Image-Text Alignment Pre-training, focuses on aligning basic image features with language representations using the Image Encoder and a pre-trained LLM, with only the image-level MLP projector being trained to establish foundational semantic alignment between visual and textual modalities with the filtered open-source CC3M [18] dataset. 2) In the second stage, Mask-Text Alignment Pre-training, enhances the model's ability to capture fine-grained forgery details and contextual knowledge by training the Mask-Aware Forgery Extractor to align mask-based regional features with language embeddings. To support this stage, we construct a specialized pre-training dataset of 768,013 forged image-caption pairs, encompassing types of forgeries, including splicing, copy-move, and removal. We first create large-scale, multi-granularity forged images using state-of-the-art inpainting and localized repainting techniques [7], [19]. These images are processed through a meticulously designed data generation pipeline, where GPT-4 [20] is used to generate instruction-focused data, including detailed descriptions and auxiliary information for each forged image, ensuring the pre-training dataset is comprehensive and tailored to support fine-grained forgery understanding. 3) In the final stage, Task-Specific Instruction Tuning, refines the model's ability to handle complex pixel-level forgery scenarios and accurately follow user instructions by fine-tuning the LLM on a curated dataset of 48,000 multi-turn dialogues. These dialogues, derived from existing forgery datasets such as Fantastic-Reality [21] and CASIAv2 [22], encompass diverse tasks such as identifying forged objects, detecting forgery regions, determining the forgery type, and explaining forgeries, further enhancing the model's interpretability and reasoning capabilities. The extensive experiments demonstrate that ForgeryGPT achieves substantial performance improvements, surpassing both the capabilities of the MLLM GPT-40 and the current state-of-the-art IFDL methods. Beyond its higher accuracy in IFDL, ForgeryGPT delivers precise and convincing explanations, as validated by quantitative analysis and human evaluation, thereby increasing trust and usability in practical applications. Our contributions are summarized as follows:\n\u2022 We pioneer the application of MLLMs to tackle IFDL task and introduce a novel framework ForgeryGPT, providing not only detection and localization of forgeries without requiring manual threshold adjustments but also supporting multi-round dialogue and explainability.\n\u2022 We advance traditional LLMs by integrating the Mask-Aware Forgery Extractor, consisting of FL-Expert and the Mask Encoder, which enables the extraction of precise forgery mask information from input images and facilitating pixel-level understanding of tampering artifacts.\n\u2022 We propose an innovative three-stage training strategy for ForgeryGPT, which systematically facilitates vision-language alignment, mask-based forgery detection, and specialized instruction tuning, enabling progressive modality alignment and skill adaptation to meet the complex demands of IFDL tasks.\n\u2022 We develop two high-quality data pipelines for training ForgeryGPT: a Mask-Text Alignment Pre-training dataset and an IFDL Task-Specific Instruction Tuning dataset. These datasets enhance the model's ability to align mask-based features with language embeddings and handle complex pixel-level forgery scenarios while accurately following user instructions.\n\u2022 Our ForgeryGPT outperforms state-of-the-art methods, pioneering the integration of IFDL tasks with explainable multi-turn dialogue capabilities and demonstrating strong generalization across diverse datasets."}, {"title": "II. RELATED WORK", "content": "A. Image Forgery Detection and Localization\nEarly work in Image Forgery Localization primarily focused on localizing specific forgery types, such as splicing [21], [23], copy-move [24], [25], and removal manipulations [26], [27]. However, these methods often struggle to generalize to real-world scenarios due to the vast diversity and unpredictability of forgery techniques. To overcome these limitations, more recent approaches have shifted toward developing models capable of localizing multiple types of forgeries. For instance, RGB-N [9] utilized a dual-stream network to extract both RGB and noise features, effectively identifying inconsistencies between tampered and untouched regions. ManTra-Net [28] introduced an end-to-end network that extracts manipulation trace features and localizes anomalous regions by comparing local features with their reference counterparts. SPAN [29] employed local self-attention blocks and pyramid propagation to model spatial correlations within images, improving fine-grained forgery understanding. Building on these foundations, models like ObjectFormer [15] focused on forgery localization in the frequency domain by capturing high-frequency manipulation traces. ERMPC [11] introduced a coarse-to-fine framework that leverages edge information to model inconsistencies between forged and authentic regions, while TruFor [13] used a reliability map to reduce false positives and facilitate large-scale forensic analysis.\nApart from primarily focusing on localization, recent studies [10], [30]\u2013[33] have highlighted the importance of treating the IFDL task holistically, combining both detection and localization within a unified framework. For example, MVSS-Net [10] enhanced both aspects by incorporating edge-supervised branches that progressively capture fine-grained boundary details. Similarly, CAT-Net [31] and PSCC-Net [32] used multi-path architectures to efficiently process both local and global features, while HiFi-Net [33] introduced a hierarchical fine-grained classification strategy for better forgery detection across multiple levels. SAFL-Net [30] integrated boundary supervision to emphasize inconsistencies around tampered regions. Despite these advancements, most approaches neglect high-level forensic semantics, such as forgery analysis and feature extraction from linguistic and knowledge-driven perspectives, limiting their ability to detect complex manipulations. Moreover, they lack interpretability, providing results without clear explanations, which significantly hinders user trust and confidence in their decisions.\nB. Multimodal Large language Models\nIn recent years, inspired by the remarkable cognitive abilities demonstrated by large-scale multimodal models, researchers have begun exploring ways to transfer these capabilities to the domain of visual perception [34]\u2013[36]. In this context, BLIP-2 [34] proposed using an image encoder to extract visual features, which are then input into a LLM alongside text prompts. Building on this, LLaVA [18] and Mini-GPT [35] initially prioritized aligning image and text features before performing contextual instruction tuning. By integrating multimodal information such as vision and language, these models exhibit robust performance across various detection tasks. For example, AnomalyGPT [37], fine-tuned from PandaGPT [38], has been applied to industrial quality inspection, effectively detecting anomalies in products. Myriad [39] enhanced the understanding of industrial anomaly images by fine-tuning Mini-GPT4 [35] and incorporating a visual expert encoder. SNIFFER [40] leveraged instruction tuning of InstructBLIP [41] to achieve misinformation detection, showcasing its potential in information security. Despite the impressive capabilities demonstrated by these models, their primary focus has been on specific detection tasks, often overlooking the exploration and enhancement of conversational abilities as well as generalization across diverse downstream tasks. In contrast, our work significantly advances forgery detection and localization within LLMs by employing a three-stage training process, demonstrating superior capabilities in handling complex visual forgery challenges.\nC. Prompt Learning\nCLIP [42] has exhibited outstanding zero-shot classification performance by leveraging knowledge from 400 million curated image-text pairs. This success has spurred advancements in areas like object detection [43], image segmentation [44], and image editing [45]. Recent studies [46] also highlight CLIP's ability to assess image quality and abstract perception in a zero-shot manner, further motivating its use for image forgery detection and localization. This success not only highlights CLIP's versatility but also underscores the importance of input structuring, linking it directly to Prompt Learning, a technique originally developed in NLP domain. Prompt Learning was first introduced to leverage large pre-trained models through simple text instructions, known as hard prompts [47]. However, designing effective prompts has proven to be a significant challenge. Prompt engineering gains prominence with the GPT series [48], leading to methods that explore both discrete prompts [47] and continuous prompts [49] to overcome model limitations. In the context of vision tasks, CLIP has demonstrated the importance of prompt design, showing that even minor modifications, such as prefixing object names with terms like \u201ca photo of\u201d, can substantially improve classification accuracy. These developments underscore the relevance of prompt learning and its adaptation to improve the performance of multi-modal models in complex visual tasks like image forgery detection and localization."}, {"title": "III. METHODOLOGY", "content": "A. Architecture Overview\nThe architecture of ForgeryGPT, as illustrated in Figure 2, consists of three core components: an Image Encoder, a Mask-Aware Forgery Extractor, and a Large Language Model (LLM) [50]. For a given input image with corresponding text instructions, the Image Encoder first extracts basic image tokens, while the Mask-Aware Forgery Extractor focuses on identifying tampering artifacts at the pixel level and producing the mask tokens, enabling a detailed understanding of forgery regions through the forgery mask generated by the FL-Expert. The text inputs are tokenized and transformed into token embeddings, forming the text tokens. These interleaved image, mask, and text tokens, along with the learnable prompt tokens we designed to facilitate efficient information transfer and integration across different modality spaces, are then processed by the LLM. This allows the model to generate an explainable dialogue response that not only determines whether the image has been tampered with, but also provides details on the forgery type, its location, and a comprehensive summary of the manipulation.\nB. Image Encoder\nGiven the strong performance of the CLIP pre-trained vision encoder across various multi-task visual domains [42], we choose the CLIP model based on the Vision Transformer (ViT) architecture, utilizing an image resolution of 336 \u00d7 336 for image feature extraction [51]. In our task, it is essential not only to align the basic features of natural images with the LLM's feature space but also to ensure that the conceptual visual features of numerous forged images remain consistent with the fine-grained text embeddings in the pre-trained LLM. To achieve this, we opt for a two-layer Multi-Layer Perceptron (MLP) instead of a traditional linear layer to align the image features with the LLM's feature space. This MLP architecture better captures and maps the complexity of visual information, ensuring consistency between the visual and textual feature spaces and enhancing the model's ability to handle fine-grained forgery recognition task.\nC. Mask-Aware Forgery Extractor\nIn addition to leveraging coarse-level image features, ForgeryGPT introduces a fine-grained representation of the detailed mask areas corresponding to the forged regions. To capture pixel-level tampering traces of each object region, we propose the Mask-Aware Forgery Extractor. This module not only generates precise forgery masks but also encodes mask-level visual features of the forged images, significantly enhancing the model's sensitivity to forged regions. Specifically, the Mask-Aware Forgery Extractor first obtains a visual information map of the forgery mask through the FL-Expert. Then, taking the forged mask as input, the proposed Mask Encoder generates specialized mask tokens tailored for the LLM's visual embeddings. These mask tokens, embedded with prior knowledge from the FL-Expert, interact and learn from other token features to generate enhanced visual-language forgery representations. This interaction improves the model's ability to detect and reason about forgeries at the pixel level, providing more detailed and accurate forgery localization.\n1) FL-Expert: Given CLIP's outstanding performance in zero-shot perception, effectively distinguishing between real and fake image attributes, we built FL-Expert based on the CLIP model, as illustrated in Figure 3. In this setup, CLIP's vision and text encoders are frozen to retain their prior knowledge. Building on this foundation, we introduce two novel components: the Object-agnostic Forgery Prompt and the Vocabulary-enhanced Vision Encoder to fully leverage CLIP's potential in the field of IFDL.\nObject-agnostic Forgery Prompt. To handle the uncertainty and diversity of forged objects in images, we adaptively generates suitable object prompts across various textual prompt templates, effectively addressing the challenge of accurately describing abstract forgery concepts. This prompt enables the model to more precisely capture and describe subtle differences in forged images. In the context of CLIP, commonly used text prompt templates such as \u201cA photo of a forged person\u201d, primarily focus on person-related semantics and fail to generate generalized text embeddings that can distinguish between forged and authentic content, limiting their ability to query corresponding visual embeddings needed for effective forgery reasoning. Furthermore, the types of forged objects are typically unknown and diverse, making it impractical to manually define prompts for every potential forgery pattern. To address this limitation, we introduce learnable text prompt templates that can encompass a wide range of forgery-related semantics. During training, these templates dynamically adjust and generate text embeddings that better distinguish forged from authentic images. This flexibility enhances the model's ability to recognize forgeries across diverse contexts. These text prompts, known as Object-agnostic Forgery Prompts, are defined as follows:\n$g_p = [M_1][M_2]... [M_E][object - p]$\n$g_n = [N_1][N_2] ... [N_E][object \u2013 n]$\nwhere $g_p$ denotes the authentic object-agnostic prompt. $g_n$ denotes the forged object-agnostic prompt. $[M_1][M_2] ... [M_E]$ represents the text embedding vectors for the pre-defined authentic prompt template, such as \"A photo of a pristine\". Likewise, $[N_1][N_2] . . . [N_E]$ represents the embedding vectors for the pre-defined forged prompt template, such as \"A photo of a forged\". In addition, $[object \u2013 p]$ refers to the learnable authentic object-agnostic word embedding, and $[object - n]$ refers to the learnable forged object-agnostic word embedding. After being grouped, these embeddings collectively constitute the comprehensive textual feature denoted as $F_{text}$. The learnable Object-agnostic Forgery Prompts adapt to various types of forgeries, allowing the model to learn generalized patterns and better detect forgeries across different domains and objects. This adaptive prompting mechanism enhances the effectiveness of forgery detection by generating more discriminative and generalized text embeddings.\nTo effectively capture multi-scale forgery clues, we divide the vision encoder into four block layers, where each layer extracts the intermediate patch-level features across different granularities, denoted as $F_{patch}^i$, corresponding to each stage i. These features provide a patch-wise representation of the image, allowing for finer granularity analysis in distinguishing between authentic and forged regions. Next, we compute the similarity between the patch-level features $F_{patch}^i$ and the text features $F_{text}$, which encode both authentic and forged content, respectively. However, since these intermediate patch-level features have not undergone the final image-text alignment, they cannot be directly compared with the text features. To address this issue, we introduce additional linear projection layers that transform these intermediate features into new representations $\\hat{F}_{patch}^i$, aligning them with the corresponding text features for both authentic and forged semantics. The cross-modal reasoning feature $f_{cross}$, which combines forgery information from both image and text modalities, is obtained through multi-scale fusion of these aligned features:\n$f_{cross} = Mean_{i=1}^4 softmax (\\frac{\\hat{F}_{patch}^i F_{text}^T}{\\sqrt{d_e}})$ \nVocabulary-enhanced Vision Encoder. To compensate for CLIP's ability to recognize locally forged images, we design a Vocabulary-enhanced Vision Encoder by introducing a trainable visual vocabulary network, which shares the same ViT (Vision Encoder) architecture [51] as CLIP. Therefore, FL-Expert is able to combine new domain-specific vocabulary knowledge with CLIP's existing open-world recognition capabilities. This combination enhances the model's accuracy and robustness in identifying forged images by focusing on the specific visual characteristics of tampered content. Similar to the previous steps, we extract intermediate patch-level features and encode them through a linear layer to obtain $F_{vocab}^i$, which represent the visual vocabulary-encoded features at each stage. In order to combine the multi-scale semantic features produced by the two vision encoders, i.e., CLIP's original encoder and the vocabulary-enhanced encoder, we introduce a Multi-layer Attention Fusion mechanism. This module calculates attention between the patch-level features and visual vocabulary-encoded features across multiple layers of the two vision encoders. For each transformer block layer i, attention weights $\\alpha^i$ are computed as follows:\n$\\alpha^i = Softmax (\\frac{F_{patch}^i (F_{vocab}^i)^T}{\\sqrt{d_e}})$"}, {"title": "D. Loss Function", "content": "The loss function of our method consists of two components: a detection loss and a localization loss. For forgery detection, we use the cross-entropy loss, denoted as $L_{cls}$, which is widely utilized in training language models like [50]. This loss measures the discrepancy between the model's generated text sequence and the target sequence. For forgery localization, we adopt the dice loss [12], denoted as $L_{loc}$, which quantifies the difference between the predicted forgery localization map $G_{out}$ and the ground truth mask Y. Dice loss is particularly suited for segmentation tasks, as it directly measures the overlap between the predicted and actual masks. The total loss function can be written as:\n$L = \\lambda_1 L_{cls} + \\lambda_2 L_{loc}$\nwhere $\\lambda_1, \\lambda_2$ are the parameters to balance the two terms in the loss function. By contributing to the detection and precise localization of forged regions, these two losses work in synergy to ensure accurate identification at both the image-level and pixel-level, while minimizing false positives on the authentic images."}, {"title": "E. Instruction Tuning", "content": "The training process of ForgeryGPT involves three stages to ensure the model can adapt effectively to complex IFDL tasks, facilitating vision-language alignment, mask-based forgery detection, and specialized instruction tuning.\n1) Stage 1: Image-Text Alignment Pre-training: In the first stage, we focus on aligning image-level features with textual embeddings. This involves training the Image Encoder in combination with a pre-trained LLM (Large Language Model) to ensure they can effectively communicate. Using the filtered CC3M dataset introduced by LLaVA [18], we train only the image-level MLP projector, while keeping both the CLIP vision encoder and the LLM frozen. The purpose is to build foundational image-text alignment using natural images as inputs, setting up the model for the more complex tasks in subsequent stages.\n2) Stage 2: Mask-Text Alignment Pre-training: Existing open-source LLMs only include training their image-level projectors with natural image inputs. However, in the context of IFDL, relying solely on natural images makes it difficult to capture these subtle manipulation differences. In this stage, we load the pre-trained weights from Stage 1 and introduce the Mask-Aware Forgery Extractor, training it to align the region-based features (from forgery localization masks) with the LLM embeddings. This stage ensures that the model can comprehend the mask regions, translating visual forgery information into a language-friendly format. To train the Mask-Aware Forgery Extractor, we create a Mask-Text Alignment Pre-training dataset, allowing the LLM to adapt from a general domain to the IFDL domain. It consists of 768,013 unique forged natural image-caption pairs, covering various representative concepts in the field of IFDL. This dataset includes three categories: 1) splicing, 2) copy-move, and 3) removal. For copy-move manipulation sub-dataset, we utilize the images from MS COCO dataset [52] with the technique [53] to produce the quality synthetic data. For removal manipulation sub-dataset, we employ the state-of-the-art inpainting method [54] to fill one annotated region that is randomly removed from each selected MS COCO image. To simulate the visual quality of images in realistic scenarios, we randomly add Gaussian noise or apply the JPEG compression algorithm to the generated data.\nFor splicing manipulation sub-dataset, as shown in Figure 5, traditional manual methods [55], [56] are inefficient and often leave noticeable traces of forgery. Meanwhile, existing generative methods [13], [57] tend to perform forgery on entire objects, such as generatively replacing an entire \"goose\". This approach not only results in a larger forged area but also causes the model to mistakenly interpret the task as significant object detection. Based on these observations, we create the multi-granularity splicing manipulation sub-dataset, focusing on performing localized repainting based on generative models to create localized forgeries on the original authentic images. To create this sub-dataset, we choose MS COCO dataset [52] as the source for generating forged samples. We first obtain the sample categories and segmentation information from the instance annotation files, which can be further used to generate multi-granularity and multi-angle local masks. The generation of local masks involves two techniques: random segmentation and SAM (Segment Anything Model) text prompt segmentation [58]. Among the 80 categories in the COCO dataset, for visually distinct categories like dogs, SAM text prompt segmentation is used to divide parts like tails or ears, while for less distinct categories (e.g., bottles or clocks), random segmentation is employed to generate masks for localized forgery synthesis. Such splicing method helps build a robust model capable of handling a wide range of forgery types.\nAfter creating the three types of forged images, as shown in Figure 4(1), we employ GPT-4 [20] to generate instruction-focused image-caption pairs. In this process, we also provide various auxiliary information alongside the forged images, such as bounding boxes for the tampered regions and relevant entity names. By constructing diverse, carefully designed prompts, we generate comprehensive descriptions that align with the specific characteristics of each forged image. Following the prompt-based generation of these captions, a series of data post-processing and validation steps [59] is performed to ensure data quality. This guarantees that the captions accurately reflect the forgery details and assist in the model's learning of nuanced image forgery features.\n3) Stage 3: Task-Specific Instruction Tuning: We focus on enhancing ForgeryGPT's ability to handle complex forgery recognition and understanding tasks at the pixel-level, ensuring it can accurately follow user instructions. During this stage, the weights of the Image Encoder are kept fixed, while the Mask-aware Forgery Extractor and the LLM are fine-tuned to improve ForgeryGPT's responsiveness to detailed user prompts and complex pixel-level forgery analysis. To build task-specific fine-tuning dataset, we use a curated dataset that includes 48,000 multi-turn dialogues designed to train ForgeryGPT to understand and solve pixel-level forgery challenges. As shown in Figure 4(2), we derive forged images from the Fantastic-Reality [21] and CASIAv2 [22] datasets. We begin by defining specific forgery detection tasks. Next, GPT-4 is employed to generate dialogues around these tasks using carefully crafted question templates. These tasks include judgment of image authenticity, localization of forgery regions, understanding of the forgery type, and detailed explanations about the forgery, etc. By utilizing diverse prompt designs and conducting rigorous adjustments throughout the generation process, we ensure that the multi-turn dialogues are highly effective for task-specific instruction tuning."}, {"title": "IV. EXPERIMENTS", "content": "A. Experimental Settings\n1) Dataset: In alignment with the methodologies described in [14], we employ the CASIAv2 [22] and Fantastic-Reality [21] datasets, along with their detailed forgery annotations, as foundational data sources for the final phase of Task-Specific Instruction Tuning. To evaluate the robustness and generalizability of our method, we conduct extensive validations across a diverse array of editing manipulation forgery datasets, including CASIA1.0+ [22], Columbia [62], NIST16 [56], IMD2020 [55], DSO-1 [63], and Korus [64]. Moreover, we extend our evaluation to include cutting-edge datasets generated by advanced deep generative models (DGMs), specifically AutoSplicing [57] and OpenForensics [65].\n2) Evaluation Metrics: To quantify localization performance, building on prior works [13], [14], we employ pixel-level Area Under Curve (AUC) and F1 scores to assess the accuracy on manipulation masks predictions. For assessing detection performance evaluation, given that ForgeryGPT operates without the need for manually set thresholds, we exclusively utilize image-level accuracy (ACC) scores. For other methods, a default threshold of 0.5 is applied to ensure consistency across evaluations.\n3) Implementation Details: We conduct our experiments using the PyTorch deep learning framework on four NVIDIA Tesla A100 GPUs. The following are the specific parameter configurations for the three training stages:\nStage 1: Image-Text Alignment Pre-training. In this stage, we only train the Image Encoder using the Adam optimizer, with the learning rate decreasing from 1e-4 to 1e-6 over the course of training. This stage is conducted for one epoch with a batch size of 8.\nStage 2: Mask-Text Alignment Pre-training. During this stage, we integrate a Mask-Aware Forgery Extractor into our training pipeline. Input images are resized to 336 \u00d7 336 pixels. The backbone network of the FL-Expert is the CLIP ViT-B/16 [42]. The object-agnostic learnable word embeddings with the token $[object \u2013 p]$ and $[object \u2013 n]$ are set to a length of 12. Multi-scale feature extraction is performed from blocks [4,8,12,16] within the CLIP ViT framework. This stage involves training for five epochs with a batch size of 8.\nStage 3: Task-Specific Instruction Tuning. We employ Vicuna-7B [50] as the inference LLM, and fine-tune the Mask-Aware Forgery Extractor and the LLM. The learning rate is reduced to 2e-5, and the batch size is increased to 32. We perform training over one epoch, using the AdamW optimizer and employing a cosine annealing scheduler. The maximum sequence length is set to 2048 tokens.\nB. Image Forgery Localization\nFollowing DiffForensics [14], our ForgeryGPT is compared with other state-of-the-art methods under the same settings: evaluations are performed using pre-trained models of ManTra-Net [28] and MVSS-Net [10]. Additionally, models such as H-LSTM [66], HP-FCN [60], GSRNet [61], SPAN [29], SATL-Net [30], CAT-Net [31], PSCCNet [32], and HiFi-Net [33] are evaluated after being trained with their respective codes on the same training dataset.  Our ForgeryGPT achieves the best localization performance on CASIA1.0+, NIST16, IMD2020, DSO-1, and Korus, and ranks second on Autosplice, Openforensics, and Columbia. Notably, ForgeryGPT achieves 91.4% performance on IMD2020, which consists of real-world images. This demonstrates that our method not only excels in capturing traces of tampering but also generalizes well to real-world scenarios. This finding validates that ForgeryGPT, enhanced by the integrated FL-Expert, can accurately detect a wide range of subtle manipulations through Object-agnostic Forgery Prompt and a vocabulary-enhanced vision encoder. Overall, the proposed ForgeryGPT achieves the best average performance, underscoring its effectiveness in tackling complex forgery localization task.\nC. Image Forgery Detection\nFollowing DiffForensics [14], we conduct an image-level classification evaluation using datasets containing both authentic and tampered images.  We observe that our approach achieves the best average performance across all datasets. It is worth mentioning that these different models are specifically designed for particular types of forgeries (For instance, PSCC-Net achieves an ACC score of 0.992 on the CASIA1.0+ dataset), our method may not achieve the highest ACC score on every individual dataset. However, we achieve superior overall performance in terms of the average score, demonstrating the robustness and accuracy of our ForgeryGPT across diverse domains and multiple types of forgeries, making it a versatile solution for forgery detection task.\nD. Robustness Evaluation\nTo analyze the robustness of our localization model, we degrade the original forged images from the NIST16 dataset following the distortion settings described in [15]. These distortions include resizing the images to different scales (Resize), applying Gaussian blur with a kernel size of k (GaussianBlur), adding Gaussian noise with a standard deviation of \u03c3 (GaussianNoise), and performing JPEG compression with a quality factor of q (JPEGCompress). We compare the forgery localization performance (AUC scores) of our pre-trained model against MVSS-Net, PSCCNet and HiFi-Net on these distorted datasets"}, {"title": "V. CONCLUSION", "content": "In this work, we introduce ForgeryGPT, a novel framework that significantly advances the IFDL task by leveraging high-order forensics knowledge correlations from diverse"}]}