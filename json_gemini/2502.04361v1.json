{"title": "Predicting 3D Motion from 2D Video for\nBehavior-Based VR Biometrics", "authors": ["Mingjun Li", "Natasha Kholgade Banerjee", "Sean Banerjee"], "abstract": "Abstract-Critical VR applications in domains such as health-\ncare, education, and finance that use traditional credentials,\nsuch as PIN, password, or multi-factor authentication, stand the\nchance of being compromised if a malicious person acquires the\nuser credentials or if the user hands over their credentials to an\nally. Recently, a number of approaches on user authentication\nhave emerged that use motions of VR head-mounted displays\n(HMDs) and hand controllers during user interactions in VR to\nrepresent the user's behavior as a VR biometric signature. One\nof the fundamental limitations of behavior-based approaches is\nthat current on-device tracking for HMDs and controllers lacks\ncapability to perform tracking of full-body joint articulation,\nlosing key signature data encapsulated by the user articulation.\nIn this paper, we propose an approach that uses 2D body joints,\nnamely shoulder, elbow, wrist, hip, knee, and ankle, acquired\nfrom the right side of the participants using an external 2D\ncamera. Using a Transformer-based deep neural network, our\nmethod uses the 2D data of body joints that are not tracked by\nthe VR device to predict past and future 3D tracks of the right\ncontroller, providing the benefit of augmenting 3D knowledge in\nauthentication. Our approach provides a minimum equal error\nrate (EER) of 0.025, and a maximum EER drop of 0.040 over\nprior work that uses single-unit 3D trajectory as the input.\nIndex Terms-Virtual Reality, Biometrics, Authentication,\nVideo Tracking", "sections": [{"title": "I. INTRODUCTION", "content": "The ability to create immersive virtual reality (VR) appli-\ncations in mission critical domains such as healthcare [1]-[4],\neducation [5]-[8], military [9]-[12], and finance [13], [14],\nenables users to perform sensitive activities, such as taking\nan exam, paying a bill, teleoperating a robot, or checking\nmedical records, raises challenges with user security. Given\nthe potential for misuse of user data, there is a growing\nconcern related to ensuring that VR applications are safe\nfrom malicious access [15]. Early work in securing a VR\napplication investigated traditional PIN, password, or multi-\nfactor approaches [16]-[24] adopted from smartphone, desk-\ntop, or laptop-based systems. However, these techniques pose a\nsecurity risk as the user's account is immediately compromised\nif an attacker gains access to the credentials. Further, PIN,\npassword, or multi-factor-based approaches do not provide\nany protection against deliberate attacks by a genuine user.\nE.g., a patient being asked to perform an at-home VR-based\nrehabilitation activity providing their credentials in the system\nand handing the device to an able ally.\nTo overcome the limitations of traditional approaches for\nauthentication, a body of work has emerged over the past\ndecade that uses the behavior of the person in the VR space\nas a signature, i.e., to perform behavior-based VR biometric\nauthentication [25]-[41]. Reviewed at length in a number\nof surveys [15], [42], [43], the approaches have investigated\nconducting authentication by using one or more of the headset\nmotion, hand controller movements, and eye gaze patterns\nwhile users conduct activities such as ball-throwing, pointing,\npinching, object selection, object movement, and gameplay.\nThus far, all approaches to perform movement-based au-\nthentication in VR use the 3D trajectory collected from the VR\ndevices as a signature of the user's behavior to authenticate\nthe user, yielding a sparse set of tracked data concentrated at\nthe hand, headset, and/or eyes. For several VR interactions,\nusers perform whole body motion resulting in multiple joint\narticulations, which though serving as a signature potential, are\nlost since current VR systems do not conduct tracking beyond\nthe headset, hands, and eyes.\nWe investigate the potential of using the external video of\na user to provide knowledge of the person's body articulation\nas they conduct activities in the VR space. We use video data\nto track the user's 2D joint articulation using OpenPose [44],\na standard body joint tracking method. Our work takes ad-\nvantage of the movement of multiple joints not tracked by the\nVR device tracking method. Though one approach is to use the\n2D tracks directly, in this work, we use the tracks to predict\nthe 3D trajectory of the right controller using a Transformer\nnetwork [45], to acquire the benefit of three-dimensional\nknowledge for authentication using deep neural architectures.\nWe augment the knowledge by predicting past and future right\ncontroller 3D motion from an input 2D trajectory segment,\nto leverage the improvement of authentication using future\nmotion forecasting as shown in Li et al. [40].\nUpon evaluating the equal error rate (EER), a standard\nbiometric authentication evaluation metric [46], using data\nfrom the dataset of Miller et al. [47], we show that our work\noutperforms baseline methods that eliminate 2D joints not\ntracked by the VR system, as well as methods that predict\ncurrent and future 2D trajectories instead of 3D. We obtain\nthe lowest EER of 0.025. Our work outperforms the state-of-\nthe-art method of Li et al. [40] that uses traditional 3D device\ntrajectories to conduct improved authentication by forecasting\n3D output. We show an average EER drop of 0.025 over Li et\nal. [40] and a maximum drop of 0.040. Our work shows the\nbenefit of using 2D data on non-tracked joints from video in"}, {"title": "II. RELATED WORK", "content": "A large body of work has emerged over the last decade\nto enable VR authentication, summarized in multiple recent\nreviews [15], [42], [43]. Among the first approaches to VR\nauthentication has been porting of traditional credentials such\nas PIN and password, and their adaptation to VR environ-\nments, e.g., via object arrangements [19], [24]. Though some\nresistance to shoulder-surfing has been shown, successful mali-\ncious access of the credential compromises security. A number\nof recent approaches investigate the use of user interactions\nwith VR headsets and hand controllers as a behavior signature,\nwith work spanning the use of individual or combination of\nheadset motion, hand movements, and eye gaze. Recent work\nuses deep neural networks [30], [36], [37], [40], [41], [47]-\n[49] based on their ability to represent non-linearities in the\ndata. Various activities have been investigated, including ball-\nthrowing [47], pointing [27], door-opening [49], pinching [50],\nobject movement [30], and gameplay [51].\nPrior methods use movements as tracked by the tracking\nmechanisms available on existing VR devices. In devices such\nas the HTC VIVE that use lighthouses, IR light emitted by the\nlighthouse is tracked throughout performance by receptors on\nthe headset and hand controllers, resulting in no more than 3\ntracks, or 4 tracks in headsets that track eye gaze. Camera-\nbased tracking such as on the Meta Quest has traditionally\ntracked the headset using an inside-out approach and the hand\ncontrollers through visual observations, resulting in a similar\nset of 3 or 4 tracks. Datasets used by nearly all approaches thus\nfar have involved participants using hand controllers. Suzuki et\nal. [50] use controller-free hand tracking in the Meta Quest to\nobtain higher-resolution motion tracks for the fingers, enabling\nthe use of pinching as a signature. However, while datasets like\nAlyx [38] exist, none of the approaches has external video\ndata or use full-body joint articulation, e.g., at the elbows and\nshoulders, the torso, or the lower body, due to their reliance\nsolely on the data tracked by the VR devices that are currently\nincapable of tracking full-body articulation, only Miller et\nal. [47] provides external video recordings.\nOur method provides the first approach for authentication\nusing external videos to acquire detailed knowledge of full\nbody articulation, while taking advantage of 3D information\nthrough intermediate prediction of the 3D controller motion. In\nconducting knowledge-augmentation by incorporating future\n3D controller trajectory prediction using deep neural networks,\nour method is closest to the work of Li et al. [40]. However,\nour work differs in that Li et al. [40] use VR device 3D\ntrajectory data as input similar to existing methods."}, {"title": "III. DATA PREPARATION", "content": "a) Dataset: We utilize the dataset provided by Miller\net al. [47], which contains comprehensive tracking data of\nvirtual reality (VR) interactions, specifically focused on ball-\nthrowing tasks. The dataset encompasses tracked trajectories\nfor both the VR headset and hand controllers, as well as\nsynchronized external video recordings for 46 participants.\nThese participants performed ball-throwing tasks using three\ndifferent VR systems: the Meta Quest, HTC VIVE, and HTC\nVIVE Cosmos. 41 out of the 46 participants identified as\nright-handed and performed the task primarily with their right\nhand. Each participant completed two data collection sessions\nfor every VR system, with a minimum gap of 24 hours\nbetween sessions to mitigate the potential effects of fatigue\nor learning bias. In each session, participants performed 10\ntrials, where each trial consisted of a single ball-throw action.\nData was recorded for a duration of three seconds per trial,\ncapturing the full motion sequence of the throw. Additionally,\nthe dataset includes external video footage captured from a\nside-view angle using a GoPro camera, which complements\nthe motion data by providing an alternative visual perspective\nof the participants' physical movements during the trials. In\nthis work, we focus on the right-hand controller data from\nthe 41 right-handed participants interacting with the HTC\nVIVE system. This choice is driven by the fact that, for\nthese participants, the right-hand controller provides the most\ndominant and representative motion data during the ball-\nthrowing task. The HTC VIVE tracking data was recorded\nat a frame rate of 45 frames per second (FPS), resulting in\na total of 135 frames captured over the 3-second period for\neach trial. In parallel, the GoPro camera recorded video at 60\nFPS, yielding 180 frames over the same duration.\nb) Generating 2D Trajectory from Video: To extract the\nparticipants' body movements from the external video record-\nings, we use OpenPose [44] to automatically track and obtain\n2D skeletal representations for each participant. In this work,\nwe specifically select the 2D trajectories corresponding to the\nright wrist, elbow, shoulder, hip, knee, and ankle. These joints\nare chosen because the right wrist is anatomically proximal\nto the right-hand controller used in the VR system, and the\nother joints are expected to significantly contribute to the\nmechanics of the ball-throwing action. The lower body joints\n(hip, knee, and ankle) are crucial in stabilizing the participant's\nposture and contributing to the transmission of force from the\nlower body to the upper body during the throw. These joints\nare key in facilitating the rotational movements and weight\ntransfer needed to generate power and control during the\nthrowing motion. The integration of both upper and lower body\njoint data allows for a more comprehensive analysis of the\nparticipant's biomechanics during the ball-throwing task. To\nensure consistency in our analysis, we address the discrepancy\nin the frame rates between the external GoPro camera (60\nFPS, yielding 180 frames over a 3-second trial) and the VR\ndevice (45 FPS, producing 135 time samples over the same 3-\nsecond period). We apply a uniform downsampling process to\nthe 2D skeletal trajectories obtained from OpenPose, aligning\nthem with the temporal resolution of the VR device data.\nAfter downsampling, both the 2D joint trajectories and the\n3D trajectories from the right-hand controller are synchronized\nto contain 135 time samples over the 3-second trial period,\nallowing for direct comparison and analysis."}, {"title": "IV. NEURAL ARCHITECTURE FOR AUTHENTICATION\nUSING 3D TRAJECTORY PREDICTION", "content": "The goal of our neural architecture is to take in a set of\nbody joint 2D trajectories over a portion of each window,\nWin\u00d7nin \u00d7 2, predict the 3D trajectory of the right controller\nover the full window, w \u00d7 3, and use the predicted 3D trajectory\nto conduct authentication. Here, Win <w, i.e., from a smaller\nwindow of 2D trajectory data, we generate a larger window\nof 3D trajectory data, including predicting future motion in\nthe region w - Win. As shown by Fig. 1, our architecture\nconsists of a trajectory prediction network model, Mtraj, and\nan authentication network, Mauth."}, {"title": "a) Trajectory Prediction Model", "content": "The trajectory predic-\ntion model Mtraj takes a 2D body joint trajectory sequence with\nlength win as input and predicts the 3D trajectory of length was\noutput. As shown in Fig. 2, Mtraj is based on Transformer [45],\nand is particularly a variant of the Informer [52] that predicts\nfuture time series data from past data. Consistent with en-\ncoding methods employed in Transformer and Informer, we\nperform the following encoding operations to enhance the\nrepresentative capacity of the raw data:\n1) We employ learned embeddings to elevate the dimensional-\nity of the input 2-dimensional trajectory coordinates across\nall timestamps, thereby capturing nuanced information not\ndiscernible in the original space.\n2) We encode the position of each timestamp within the\nwindow, enabling extraction of positional correlations.\n3) We conduct temporal encoding for each timestamp within\nthe global time series, to encompass the entirety of a\nmotion session. As in Informer, the information captures"}, {"title": "b) Authentication Model", "content": "The authentication model\nMauth, as shown in Fig. 3, is a fully convolutional network\n(FCN) for time series data [53] that has been shown to be\nsuccessful in traditional VR biometric authentication using 3D\nVR headset and controller motions [30], [36]. The FCN takes\nthe 3D trajectory predicted using Mtraj and predicts a binary\nlabel, with 1 indicating that the user is authenticated and 0 oth-\nerwise. Mauth consists of three convolution blocks, consisting\nof a convolutional layer with 1D kernel, batch normalization,\nand rectified linear unit activation. The convolutional layers\nprovide 128, 256, and 128 outputs each. After the 3 blocks,\nwe apply global average pooling and softmax to generate the\nfinal binary output."}, {"title": "c) Loss Function", "content": "We estimate weights of Mtraj and Mauth\nby minimizing loss\n$\\mathcal{L} = \\mathcal{L}_{traj} + \\lambda \\mathcal{L}_{auth}.$\nIn Equation (1), $\\mathcal{L}_{traj}$ represents the reconstruction loss of each\n3D trajectory of window size w and is given as\n$\\mathcal{L}_{traj} = \\frac{1}{W} \\sum_{i \\in W} MSE(T_{i}^{pred}, T_{i}^{gt}),$\nwhere $W$ represents the total windows, MSE represents the\nmean-squared error function, and $T_{i}^{pred}$ and $T_{i}^{gt}$ respectively\nrepresent the predicted and ground truth trajectories. $\\mathcal{L}_{auth}$ rep-\nresents the classification loss for the authentication network,\nand is given as\n$\\mathcal{L}_{auth} = \\frac{1}{W} \\sum_{i \\in W} BCE (y_{i}^{pred}, y_{i}^{gt}),$\nwhere BCE represents the binary cross-entropy loss function,\nand $y_{i}^{pred}$ and $y_{i}^{gt}$ respectively represent the predicted and\nground truth class labels, 1 for a genuine user and 0 for an\nimpostor. The factor $\\lambda$ weights the loss contributions."}, {"title": "d) Implementation Details", "content": "We configure Mtraj to have\n3 identical encoder layers and an individual decoder layer.\nAs in the original Transformer architecture, we set the model\ndimension to 512, with each layer incorporating an 8-head\nself-attention mechanism. We set the query, key, and value\ndimensions to 64. We set the dimension of the fully connected\nlayer that generates the output trajectory to 2048. We use filter\nsizes of {128,256, 128} and 1D kernel sizes of {8,5,3} for the\n3 convolutional blocks of Mauth. We set $\\lambda$ to 0.5. We optimize\nusing Adam with a learning rate of le-4."}, {"title": "V. EXPERIMENTS", "content": "We conduct authentication experiments in window sizes w\nranging from 40 to 130 in steps of 10, and win ranging from\n30 to 70 in steps of 10. Table II shows particular choices of w\nand win. We perform training using data from the first session\nof each participant, and perform testing using data from the\nsecond session. The two sessions are acquired on two separate\ndays per participant in the Miller dataset.\nOur main authentication results use right controller 3D\ntrajectory predictions from input 2D data for 6 OpenPose\njoints, i.e., the right wrist, elbow, shoulder, hip, knee, and\nankle, resulting in nin = 6. We refer to these results as\n3Dfrom2D_WESHKA. As a baseline, we compare our ap-\nproach to Li et al. [40] who use input 3D trajectory segments\nto predict an output 3D trajectory containing future forecasted\ndata and prior data with an overlap with the input trajectory.\nWe use an overlap of win/2 to generate an output trajectory of\nlength w - Win/2. We obtain the full 3D trajectory within the\nwindow w for authentication by concatenating the first half of\nthe input trajectory of size win/2 with the output trajectory.\nWe include 10 further experiments in this study, (i)\n2Dfrom2D_W evaluates the performance of trajectory gen-\neration when predicting just the 2D wrist trajectory within\nWin, resulting in nin = 1. And (ii) 2Dfrom2D_WES uses\ninput 2D data from the wrist, elbow, and shoulder in Win\nresulting in nin = 3. These two approaches mimic video-\nonly biometric authentication. The other two experiments,\n(iii) 3Dfrom2D_W and (iv) 3Dfrom2D_WES focus on eval-\nuating the 3D trajectory prediction using 2D data from the"}, {"title": "VI. RESULTS", "content": "In order to facilitate a direct comparison of trajectory pre-\ndiction accuracy, we present the quantitative results in Table I,\nshowcasing the mean squared error (MSE) values in each\ncell for all methods that predict future 3D trajectories based\non 2D body joints across different combinations of w and\nWin. Each row in Table I corresponds to a specific prediction\nmethod, while each column represents a distinct combination\nof w and win. From Table I, we observe that the proposed\nmain method, i.e., 3Dfrom2D_WESHKA, outperforms alter-\nnative approaches in the majority of combinations. Notably,\nin 18 out of the 20 cases, 3Dfrom2D_WESHKA achieves the\nlowest MSE for the predicted trajectories. This highlights the\nenhanced accuracy of our main method when leveraging six\n2D body joints to predict future 3D trajectories, outperforming\nother approaches that rely on fewer joints.\nAdditionally, it is worth noting that when only a single body\njoint is used for forecasting, as in the 3Dfrom2D_W method\nin the first row in Table I, the average MSE across nearly\nall combinations of w and win is higher compared to other\nmethods, which indicates a much larger forecasting error. A\nsimilar pattern is observed when three body joints are used\nfor prediction, as in the 3Dfrom2D_WES method, the second\nrow in Table I, where the majority of w and win combinations\nalso result in higher MSE values than methods utilizing a\ngreater number of body joints. This finding underscores the\nupper body. Specifically, 3Dfrom2D_W assesses the predic-\ntive capability using 2D data from the right wrist. We have\n3Dfrom2D_WES to utilize input data from three joints, the\nright shoulder, elbow, and wrist. We employ 3Dfrom2D_W to\ninvestigate the potential advantages of incorporating multiple\njoints, as exemplified by 3Dfrom2D_WES, as opposed to\nrelying solely on data from a single track, particularly the\ntrack closest to the controller. The subsequent six experiments,\ndenoted as (v) 3Dfrom2D_WESH, (vi) 3Dfrom2D_WESK,\n(vii) 3Dfrom2D_WESA, (viii) 3Dfrom2D_WESHK, (ix)\n3Dfrom2D_WESHA, and (x) 3Dfrom2D_WESKA, expand\nupon the foundation laid by the former experiments by in-\ntegrating additional body joints from the lower body. Specifi-\ncally, the letter codes appended to each experiment denote the\ninclusion of joints at various lower body positions, where 'H'\nrepresents the hip, 'K' denotes the knee, and 'A' the ankle.\nEvaluation Metric. We use the Equal Error Rate (EER), a\nstandard metric for benchmarking authentication systems [46],\nto evaluate our authentication model. The EER corresponds to\nthe point where the false acceptance rate (FAR) equals the false\nrejection rate (FRR), representing the threshold at which the\nmodel is equally likely to incorrectly accept an unauthorized\ninput as it is to wrongly reject a legitimate one. A lower EER\nindicates superior model performance, reflecting a balanced\ntrade-off between false acceptances and false rejections. In\nthis paper, we report EER values ranging from 0 to 1, rather\nthan expressing them as percentages.\nimportance of incorporating additional body joints in trajec-\ntory prediction models. The results demonstrate that using\na minimal number of joints tends to result in less accurate\npredictions, with higher error rates. In contrast, methods that\ninclude a larger set of body joints improve the accuracy of\nfuture trajectory predictions. By providing the model with\nmore comprehensive motion data, especially from joints that\nare critical for certain physical activities, throwing a ball in our\ncase, we capture the complex dynamics of human movement.\nThis leads to more reliable and precise estimations of future\ntrajectories, and increases prediction accuracy through the\nincreased number of tracked body joints."}, {"title": "B. Authentication", "content": "We show the EER results for various choices of w and Win\nin Table II. The values in each cell represent the average EER\nacross all test samples for all users for the corresponding\nchoices of w, win, and experiment. For our method, i.e.,\n3Dfrom2D_WESHKA, we obtain EER values ranging from\n0.025 to 0.060. The best performance is obtained where the\nvalue of EER is lowest at 0.025, for w of 90 and win of 60.\nObserving Table II reveals that, across various combinations\nof w and win, the lowest Equal Error Rate (EER) values, de-\nnoted in bold font, tend to cluster within the middle and lower\nsections of the table. We observe the following outcomes:\n1) The proposed approach exhibits superior performance\ncompared to the approach of Li et al [40],\n2) Predicting 3D trajectories from 2D body joints and sub-\nsequently conducting identity authentication outperforms\nthe methods of predicting 2D trajectories and then per-\nforming identity authentication, and\n3) The incorporation of additional 2D body joints improves\nthe model's overall authentication performance.\nAmong the 20 combinations of w and win, our method\noutperforms the other approaches in 7 instances, indicated\nby achieving the lowest EER (in bold). Furthermore, our\nmethod ranks the second best in 4 out of the rest 13 combina-\ntions, as evidenced by the second-to-last lowest EER (values\nunderscored). On average, across all 20 combinations and\nexperiments, our approach achieves the best result, with an\naverage EER value of 0.039, the lowest among all compared\ninstances, as highlighted in bold in the last row in Table II.\nCompared to the prior work of Li et al. [40], we observe\nthat our approach outperforms their method for all choices\nof w and win. It is worth noting that the approach of Li et\nal. uses 3D data at the input, and we incorporate part of\nthe original 3D data in the authentication method to retain\na signature of the user's original performance. These aspects\nof using and encapsulating full 3D information enable their\nmethod to show a higher authentication success over other\nexperiments, including 2Dfrom2D_W and 2Dfrom2D_WES\nthat perform authentication solely based on 2D trajectory\npredictions as well as 3Dfrom2D_W that predicts the 3D\ntrajectory. However, when multi-joint input is combined with\n3D trajectory prediction, our method shows a performance\nboost, demonstrating the benefit of using the user's external"}, {"title": "VII. CONCLUSION AND FUTURE WORK", "content": "We present an approach to perform behavior-based bio-\nmetric authentication of users in VR environments by using\n2D data from external cameras to predict 3D trajectories that\nencapsulate current and future motion, and use the 3D trajecto-\nries in authentication. Our method incorporates the advantages\nof combining future motion with past motion demonstrated in\nLi et al. [40], while overcoming the deficiencies in Li et al.\nin modeling user behavior by incorporating information from\nmultiple joints of the dominant arm and leg. Using 2D video\nenables us to acquire access to multiple joints by applying\njoint extraction algorithms such as OpenPose. Our method\noutperforms the work of Li et al. [40] and baselines that use\na single input and/or predict 2D output."}]}