{"title": "DOTA: DISTRIBUTIONAL TEST-TIME ADAPTATION OF VISION-LANGUAGE MODELS", "authors": ["Zongbo Han", "Jialong Yang", "Junfan Li", "Qinghua Hu", "Qianli Xu", "Mike Zheng Shou", "Changqing Zhang"], "abstract": "Vision-language foundation models (e.g., CLIP) have shown remarkable performance across a wide range of tasks. However, deploying these models may be unreliable when significant distribution gaps exist between the training and test data. The training-free test-time dynamic adapter (TDA) is a promising approach to address this issue by storing representative test samples to guide the classification of subsequent ones. However, TDA only naively maintains a limited number of reference samples in the cache, leading to severe test-time catastrophic forgetting when the cache is updated by dropping samples. In this paper, we propose a simple yet effective method for DistributiOnal Test-time Adaptation (Dota). Instead of naively memorizing representative test samples, Dota continually estimates the distributions of test samples, allowing the model to continually adapt to the deployment environment. The test-time posterior probabilities are then computed using the estimated distributions based on Bayes' theorem for adaptation purposes. To further enhance the adaptability on the uncertain samples, we introduce a new human-in-the-loop paradigm which identifies uncertain samples, collects human-feedback, and incorporates it into the Dota framework. Extensive experiments validate that Dota enables CLIP to continually learn, resulting in a significant improvement compared to current state-of-the-art methods.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in vision-language foundation models have shown remarkable vision understanding capabilities across a broad range of tasks by training on web-scale image-text pairs (Radford et al., 2021; Lavoie et al., 2024; Zhai et al., 2023). Taking CLIP as an example, it can conduct zero-shot classification without the need for additional training data using predefined prompts (Radford et al., 2021). However, CLIP may still face challenges when handling various specific applications during test time, especially when there is a significant distribution gap between the training and test data (Shu et al., 2022; Karmanov et al., 2024; Feng et al., 2023).\nTest-time adaptation methods are typically employed to address the distribution gap between the training and test datasets by fine-tuning the original model during test time (Boudiaf et al., 2022; Chen et al., 2022; Wang et al., 2021). Test-time adaptation aligns well with real-world applications where models need to adapt to new environments quickly. There are two primary lines to achieve test-time adaptation on the vision-language foundation models. Early works advocate learning prompts during test time with the test data (Shu et al., 2022; Feng et al., 2023). However, these methods require significant computational resources to optimize the learnable prompts via back-propagation. This significant resource overhead makes them unsuitable in applications when fast inference speed is widely required. Therefore, a more efficient method, Training-Free Dynamic Adapter (TDA), has been proposed (Karmanov et al., 2024) recently. To avoid the training process with backpropagation, TDA maintains a lightweight cache during testing to store representative test samples and guide the classification of subsequent test samples."}, {"title": "2 RELATED WORK", "content": "Test-time adaptation (TTA) focuses on addressing the distribution shift between training and test data by learning from the test data. Early efforts to improve TTA performance primarily involve adjusting batch normalization layers and designing unsupervised objective functions (Nado et al., 2020; Wang et al., 2020; Khurana et al., 2021; Lim et al., 2023). For example, TENT (Wang et al., 2020) optimizes the affine parameters in batch normalization layers by minimizing the entropy of the prediction probability. MEMO (Zhang et al., 2022a) applies variant augmentation methods to a single test sample and optimizes model parameters by minimizing the entropy of the prediction probability. To enhance the performance of vision-language models during testing, TPT (Shu et al., 2022) introduces adaptive text prompts and optimizes the prompts through entropy minimization. Building on this, DiffTPT (Feng et al., 2023) leverages pre-trained stable diffusion models to generate diverse augmented data for use in test-time prompt tuning. However, TPT and DiffTPT rely heavily on gradient backpropagation to optimize the prompts, making them computationally expensive and resource-intensive during testing. TDA (Karmanov et al., 2024) proposes a lightweight test-time adaption method by storing representative test samples. Compared to TDA, which naively stores typical test samples, we achieve continuous adaptation by estimating the distribution of test samples, leading to a more efficient and adaptive solution.\nUncertainty estimation aims to estimate the reliability of decision. Traditional methods for uncertainty estimation often require additional training processes. For example, ensemble learning (Lakshminarayanan et al., 2017; Liu et al., 2019) and Bayesian neural networks (MacKay, 1992; Gal & Ghahramani, 2016) estimate uncertainty by obtaining the distribution of prediction. However, these methods typically introduce additional computational costs during inference. To address this, regularization-based methods have been proposed to constrain the confidence of the model during training, preventing overfitting and thereby improving uncertainty estimation (Malinin & Gales, 2018; Sensoy et al., 2018; Han et al., 2022; 2024). However, these methods focus on modifying"}, {"title": "3 METHOD", "content": "the training process, such as altering the model architecture or loss function, to estimate uncertainty. They are not applicable to foundation models that have already been fully trained. Therefore, in this paper, we focus on estimating uncertainty during the inference stage using test samples.\nVision-language models have demonstrated strong vision understanding capabilities benefiting from training on large-scale datasets (Radford et al., 2021; Zhai et al., 2023; Lavoie et al., 2024). Among them, CLIP (Radford et al., 2021) is the most representative method by maximizing the similarity between image and their corresponding text embeddings. To further enhance performance of CLIP on downstream tasks, prompt learning-based methods have been proposed by optimizing the prompts of the text encoder (Zhou et al., 2022a;b; Bai et al., 2024; Khattak et al., 2023). Moreover, to reduce the computational cost associated with gradient calculations in prompt learning, efficient CLIP adaptation methods have been introduced (Gao et al., 2024; Zhang et al., 2022b; Wang et al., 2024; Li et al., 2024; Yu et al., 2023). These methods enable downstream task adaptation using only a small number of training samples in the embedding space. Orthogonal to above methods, this paper focuses on continuously adapting to environments during testing by leveraging test samples."}, {"title": "3.1 ZERO-SHOT CLASSIFICATION WITH PROMPT", "content": "Zero-shot classification. During the pre-training stage, CLIP\u00b9 trains its image and text encoders using large-scale image-text pairs. This is achieved by maximizing the cosine similarity between the image and text embeddings through contrastive loss. Unlike traditional classifiers trained on closed-set labels, CLIP leverages open-set semantic information in the image-text pairs to learn a broader range of visual concepts. Consequently, during the test stage, CLIP can perform zero-shot classification without additional training. Specifically, given a test sample x for K-class classification, where x represents the image embedding obtained from the image encoder, the corresponding zero-shot prediction probability $P_{k}^{zs}$ for class k is calculated as:\n$P_{k}^{zs}(y=k|x) = \\frac{exp(cos(x, w_k)/\\tau)}{\\sum_{k'=1}^{K} exp(cos(x, w_{k'})/\\tau)}$,\nwhere zs refers to zero-shot. $w_k$ is the classification weight for class k, obtained by encoding the corresponding prompt, e.g., \u201ca photo of {class}\", with the class token replaced by the specific category name. $\\tau$ is the learned temperature parameter in CLIP, and $cos(.,.)$ denotes the cosine similarity. The above classification process can be understood as comparing the obtained image embedding with the text prompt and selecting the most similar category as the final decision.\""}, {"title": "3.2 DISTRIBUTIONAL TEST-TIME ADAPTATION", "content": "Motivation. When CLIP is deployed in various environments, the performance tends to degrade due to the changes of data distribution, especially when the test data has a significant distribution gap from the CLIP training data. Test-time adaptation can effectively adapt the foundational model to new environments quickly during the test stage. Current state-of-the-art method TDA maintains a cache during test-time to preserve representative samples of different classes, which then guide the classification of the following test samples. However, TDA may lead to a severe test-time forgetting problem when the cache is updated due to only maintaining the embeddings of very limited test samples without learning the underlying relationships between the sample and label. To this end, we propose distributional test-time adaptation (Dota), which aims to continuously learn from test-time data by estimating the test sample distribution. Specifically, as shown in Fig. 1, we propose to online estimate the data distribution of samples in the current test environment during testing. Once obtaining the distribution, we can leverage Bayes' theorem to naturally infer the test-time posterior distribution of different classes for new test samples to adapt the test-time environment.\nClassification with Gaussian discriminant analysis. Formally, inspired by classical Gaussian discriminant analysis (Hastie & Tibshirani, 1996), we assume that the embedding distribution of each class k follows a Gaussian distribution, i.e., $P(x|y=k) = N(\\mu_k, \\Sigma_k)$, where $\\mu_k$ and $\\Sigma_k$ are"}, {"title": "3.3 TEST-TIME ADAPTION WITH HUMAN-FEEDBACK", "content": "Test-time adaption with human-feedback. The continuous test-time adaptation method enhances model performance by estimating the data distribution of incoming test samples. However, relying solely on zero-shot predicted probability distributions for this estimation may lead to inaccuracies, particularly for originally uncertain samples. The predicted probabilities of these uncertain samples often fail to provide reliable information for accurate distribution estimation. To address this, we propose a new task that incorporates human-feedback during test-time adaptation, establishing a simple yet effective human-in-the-loop paradigm. Specifically, after the model is deployed, we aim to obtain label information on uncertain samples with human in real-time and use it for test-time adaptation. This approach enables quick and effective performance improvements on uncertain samples during testing.\nTest-time uncertainty estimation. To achieve the test-time adaption with human-feedback, we first define the test-time uncertainty estimation task, which aims to determine whether the current test sample is uncertain based on the information from the previous test samples stream. Formally, given a test sample $x_i$ and the previously tested"}, {"title": "3.4 ADAPTIVE FUSION OF ZERO-SHOT AND TEST-TIME CLASSIFIER", "content": "As the number of test samples increases, the reliability of the estimated test sample distribution improves (Dasgupta & Hsu, 2007). However, when the number of test samples is insufficient, the estimated distribution may be unreliable. To address this, we introduce a dynamic zero-shot classification and test-time result fusion approach, allowing the model to rely more on zero-shot classification when the test-time distribution estimation is insufficient. Formally, the final fusion probability is defined as follows:\n$P_k(y=k|x) = \\frac{exp(cos(x,w_k)/\\tau + \\lambda f_k(x))}{\\sum_{k=1}^K [exp(cos(x,w_k)/\\tau + \\lambda f_k(x))]}$,\nwhere $\\lambda = min(pc, \\eta)$. Here, c represents the number of test samples, and p and $\\eta$ are hyperparameters that control the weight of the test-time classifier logits. The value of $\\lambda$ increases with the number of test samples when this number is insufficient, gradually approaching the"}, {"title": "4 EXPERIMENTS", "content": "We conduct extensive experiments to validate the performance of Dota. Specifically, we first compare Dota with current state-of-the-art methods and then conduct ablation studies.\nBenchmarks. Consistent with prior works (Shu et al., 2022; Feng et al., 2023; Karmanov et al., 2024), we conduct our main experiments on natural distribution shifts and cross-domain generalization scenarios. For the natural distribution shifts scenario, we utilize multiple datasets including ImageNet (Deng et al., 2009), ImageNet-A (Hendrycks et al., 2021b), ImageNet-R (Hendrycks et al., 2021a), and ImageNet-S (Wang et al., 2019), which serve as measures of the robustness of our approach. In the cross-domain generalization scenario, we evaluate the performance of the model across 10 diverse image classification datasets, each representing a distinct domain with different classes: Aircraft (Maji et al., 2013), Caltech101 (Fei-Fei et al., 2004), Cars (Krause et al., 2013), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), Flower102 (Nilsback & Zisserman, 2008), Food101 (Bossard et al., 2014), Pets (Parkhi et al., 2012), SUN397 (Xiao et al., 2010), and UCF101 (Soomro et al., 2012). This benchmark provides a comprehensive evaluation of the adaptability of the model during test time across various class spaces.\nComparison method. We compare the proposed method with the following method: (1) TPT (Shu et al., 2022) is a test time prompt tuning method. (2) DiffTPT (Feng et al., 2023) introduces more diverse test sample augmentation with diffusion model. TPT and DiffTPT require gradient backpropagation to update prompt, so they require greater computational cost. (3) TDA (Karmanov"}, {"title": "4.1 COMPARISON WITH STATE-OF-THE-ARTS METHODS", "content": "Results under the natural distribution shifts scenario. We first compare Dota with state-of-the-art methods in the context of natural distribution shifts. Tab. 1 presents the experimental results, revealing several key observations. (1) Leveraging distribution modeling of the representation of test data, Dota achieves superior performance without requiring gradient backpropagation. For instance, using the CLIP-ViT-B/16 backbone network, Dota outperforms the second-best method by an average of 0.99%, achieving state-of-the-art results across all datasets. (2) Performance of Dota can be further improved by incorporating human feedback. For example, with the ViT-B/16 backbone, introducing human feedback for approximately 5% of uncertain inference samples during test-time adaptation leads to an additional average performance improvement of 0.41%.\nResults under the cross-domain generalization scenario. Then we compare Dota with state-of-the-art methods under the cross-domain generalization scenario across 10 diverse image classifi-"}, {"title": "4.2 ABLATION STUDIES AND FURTHER ANALYSIS", "content": "Analysis of continuous learning ability. When testing on the ImageNet dataset, we record the performance of the most recent 5,000 test samples and compare them with the original zero-shot classifier performance, recording the relationship between the improvement in model performance and the number of test samples seen. The results are shown in Fig. 3. From the experimental results, we can see that the proposed method gradually improves the model performance as the number of test samples increases. In contrast, the improvement of TDA first increases and then decreases, and it is unable to continuously learn from the test data stream. We show the performance of the last 50% of test samples and all samples on more datasets in Tab. 4. The experimental results clearly show that the performance of the last 50% of test samples is significantly higher than the overall performance. The above improvement is due to the fact that the estimated distribution becomes more reliable as the number of observed test samples increases.\nThe necessity of distribution estimation. We compared the performance of the Dota with a simplified version that only uses the mean, excluding the estimation of the Gaussian distribution by removing the covariance matrixs. This experiment aimed to understand the necessity of continual distribution estimation in enhancing model accuracy. The experimental results are shown in Tab. 5. The third row in the table presents the accuracy reductions across different datasets when"}, {"title": "5 CONCLUSION AND FUTURE WORK", "content": "We propose a method for continuous test-time adaptation, which enhances the original zero-shot classifier by continually adapting through online estimation of the test sample distribution and obtaining test-time posterior probabilities. To achieve this, we introduce an online distribution param-"}, {"title": "A APPENDIX", "content": "Implementation details. All the models in our experiments are built upon the pre-trained CLIP model (Radford et al., 2021) that consists of an image encoder and a text encoder. Test-time adaptation is set for single-image scenarios, using a batch size of 1. For natural distribution shifts scenario, we tune all our hyperparameters using the single ImageNet validation set. For the cross-domain generalization scenario, we perform hyperparameter search using the corresponding validation sets. We adjust o\u00b2 within [0.001, 0.002, 0.004], then search for the best \u03b7 across [0.2, 0.3, 0.4, 0.5] and p across [0.005, 0.01, 0.02, 0.03], with the shrinkage parameter e set to 0.0001. We use top-1 accuracy (%) as our evaluation metric. All experiments are conducted using a single NVIDIA RTX 4090 GPU and a 12-core Intel Xeon Platinum 8352V CPU.\nLimitations and future works. Here we briefly discuss the limitations of our method and outline potential directions for future work. (1) While our approach demonstrates the advantage of continuously estimating the distribution of test data, allowing for adaptation to test data, it does not consistently outperform TDA on all the dataset. For example, as shown in Tab. 8, on ImagenetV2 (Recht et al., 2019) datasets with only 10 samples per class, Dota does not significantly exceed TDA. However, its performance on the last 50% of the test samples shows a clear improvement. This indicates that the proposed model has the potential to further improve as more test samples becomes available. Moreover, as demonstrated in Fig. 3, our method gradually outperforms TDA over time. To avoid the limitation, a promising way for future research is designing a mechanism to evaluate the reliability of the adapter, allowing dynamic decisions on whether to introduce it based on its reliability. (2) This paper also introduces the novel task of test-time adaptation with human feedback and proposes an initial approach. Future work could focus on refining methods to accurately detect unreliable samples and selectively incorporate human feedback, providing a valuable direction for further improvement.\nBroader impact. Foundational models are being widely deployed, but they do not always adapt perfectly to the distribution of test data. Collecting new data and fine-tuning models for specific applications can be costly and slow in response. Therefore, allowing models to adapt to unseen data during test time can enhance their generalization and adaptability. This approach has potential in fields like healthcare and assistive technologies, as it can help reduce subgroup bias caused by insufficient data for minority groups during training and improve fairness."}]}