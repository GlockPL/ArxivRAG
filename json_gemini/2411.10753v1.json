{"title": "Chain-of-Programming: Empowering Large Language Models for Geospatial Code Generation", "authors": ["Shuyang Hou", "Haoyue Jiao", "Zhangxiao Shen", "Jianyuan Liang", "Anqi Zhao", "Xiaopu Zhang", "Jianxun Wang", "Huayi Wu"], "abstract": "With the rapid growth of interdisciplinary demands for geospatial modeling and the rise of large language models (LLMs), geospatial code generation technology has seen significant advancements. However, existing LLMs often face challenges in the geospatial code generation process due to incomplete or unclear user requirements and insufficient knowledge of specific platform syntax rules, leading to the generation of non-executable code, a phenomenon known as \u201ccode hallucination.\" To address this issue, this paper proposes a Chain of Programming (CoP) framework, which decomposes the code generation process into five steps: requirement analysis, algorithm design, code implementation, code debugging, and code annotation. The framework incorporates a shared information pool, knowledge base retrieval, and user feedback mechanisms, forming an end-to-end code generation flow from requirements to code, without the need for model fine-tuning. Based on a geospatial problem classification framework and evaluation benchmarks, the CoP strategy significantly improves the logical clarity, syntactical correctness, and executability of the generated code, with improvements ranging from 3.0% to 48.8%. Comparative and ablation experiments further validate the superiority of the CoP strategy over other optimization approaches and confirm the rationality and necessity of its key components. Through case studies on building data visualization and fire data analysis, this paper demonstrates the application and effectiveness of CoP in various geospatial scenarios. The CoP framework offers a systematic, step-by-step approach to LLM-based geospatial code generation tasks, significantly enhancing code generation performance in geospatial tasks and providing valuable insights for code generation in other vertical domains.", "sections": [{"title": "1. Introduction", "content": "The rapid advancements in remote sensing and sensor network technologies have significantly enhanced geospatial data acquisition and processing, resulting in a dramatic increase in data volume. Concurrently, the growing importance of spatiotemporal data in fields such as ecology, transportation, politics, and military affairs reflects broader trends in global defense modernization, space exploration, and competition in Earth observation. Researchers are leveraging geospatial modeling to manage vast datasets, supporting high-precision analysis, knowledge discovery, and sustainable development while addressing the increasing demand for cross-disciplinary data analysis. However, the heterogeneity, complex formats, and large scale of geospatial data present substantial challenges for the rendering and visualization capabilities of traditional compilation platforms. In response, cloud-based geospatial analysis platforms (e.g., GEE, ArcGIS, and PIE Engine) are being increasingly deployed for complex modeling tasks, offering powerful programming engines, while local tools (e.g., Python's GDAL, ArcPy, and R's Raster and Terra) facilitate fundamental geospatial modeling. The integration of cloud platforms with local tools has given rise to the emerging field of \"geospatial code.\u201d Driven by the need for massive data processing and complex modeling, automated geospatial code generation research has emerged . Its primary objective is to translate natural language requirements into executable code, standardize modeling workflows, lower technical barriers for non-expert users, enhance research efficiency, and foster interdisciplinary applications.\nThe automated generation of geospatial code represents a critical approach to enhancing the efficiency of geospatial modeling. However, early technological paradigms struggled to achieve this goal directly, relying instead on geographical information services (GIServices) to indirectly optimize modeling processes. These methods were based on predefined, standardized geospatial data analysis modules that required manual assembly or configuration by experts. The interoperability of these modules was facilitated by standards such as those defined by the Open Geospatial Consortium (OGC), enabling seamless integration and automated spatiotemporal data processing. Nevertheless, the construction of GIServices was often constrained to specific tasks or platforms, dependent on symbolic rules or data annotations, with limited generalizability. Moreover, their usage heavily relied on expert judgment, making the modeling process complex and labor-intensive. As a result, geospatial modeling remained far from fully automated. The advent of attention mechanisms and Transformer models has led to the development of large language models (LLMs) trained on extensive corpora, endowing them with capabilities such as instruction following, contextual learning, logical reasoning, and content generation. These advancements have made it possible to directly translate natural language into executable code. Finetuned general-purpose code generation models, such as Code LLaMA, DeepSeek Coder, and WizardCoder, have demonstrated excellent performance in generic code generation tasks. However, evaluation studies indicate that these models fall short in the specialized domain of geospatial code generation, frequently exhibiting \"code hallucination\" phenomena that result in non-functional code, as illustrated in Figure 1.\nThe \"code hallucination\" phenomenon can be attributed to several factors. First, unclear or incomplete requirement descriptions often hinder accurate code generation. In geospatial tasks, users frequently omit critical details such as time, spatial, or dataset dimensions, or fail to specify the programming language and platform clearly, leading to misunderstandings by LLMs and the generation of incorrect code. Second, the lack of specialized syntax knowledge contributes significantly to the issue. Geospatial code is often encapsulated within higher-level abstractions by cloud platforms or open-source libraries, making its control flow, data flow, and platform-specific syntax distinct from general-purpose code. These variations necessitate specialized training to apply the correct syntax and function names. Additionally, remote sensing imagery in geospatial analysis platforms involves proprietary file paths, and inadequate knowledge of these paths can result in errors during image selection and path encoding. Lastly, while prior research has sought to enhance geospatial code generation through fine-tuning, this process is resource-intensive and costly. The impact of varying corpus proportions remains underexplored, and if the corpus is imbalanced, of low quality, or if training strategies are suboptimal, it can lead to model degradation or catastrophic forgetting."}, {"title": "2. Related Work", "content": "Code generation tasks focus on converting natural language inputs into source code (NL2Code). Early approaches in this field relied on heuristic rules, expert systems, or reinforcement learning methods\u2014such as probabilistic grammar frameworks and small-scale pretrained models that were task-specific and programming-language-dependent, with limited applicability to generating executable code.Recent advancements in LLMs based on the Transformer architecture, trained on heterogeneous corpora (e.g., text, web content, and code), have significantly improved capabilities in contextual understanding, logical reasoning, and instruction execution, revolutionizing code generation methodologies. These models enable users to provide direct instructions, which are parsed and translated into code. However, the relatively low proportion of code in training datasets often leads to issues such as refusal to generate or producing hallucinated (nonexistent or erroneous) outputs.Current optimization strategies for LLM-based code generation encompass retrieval-augmented generation, fine-tuning, and agent-based approaches. Retrieval-augmented generation constructs code-specific knowledge bases to enhance model outputs by supplementing missing information without modifying the model's structure. While this method improves code readability and executability, its performance heavily relies on the quality of the knowledge base and retrieval algorithms, often resulting in limited consistency and effectiveness.Fine-tuning on general code and task-specific datasets has led to the development of LLMs optimized for code generation (e.g., Code LLaMA, DeepSeek Coder, WizardCoder). However, this approach requires large-scale datasets and computationally expensive hardware. Moreover, the lack of quantitative evaluation regarding dataset size and composition increases the risk of model performance degradation or loss of specialized knowledge. Agent-based strategies involve integrating LLMs into collaborative pipelines by assigning roles such as programmer or engineer to improve code generation performance. While these approaches capitalize on the strengths of LLMs, they entail significant resource consumption and communication overhead. Additionally, they often lack domain-specific expert feedback, particularly for geospatial code. In summary, current optimization strategies are predominantly designed for general-purpose code, leaving geospatial code generation constrained by issues of readability, operational reliability, and precision. These challenges highlight the critical need for targeted optimization techniques tailored to the unique requirements of geospatial applications."}, {"title": "2.2. Incremental Reasoning Optimization Strategies for LLMs", "content": "Geospatial code generation tasks, which intersect multiple disciplines such as computer programming, geographic information science, and data science, require mapping users' unstructured requirements into standardized geospatial modeling workflows and generating compliant code. This process involves complex reasoning. Studies indicate that LLMs perform better on complex reasoning tasks when these tasks are broken down into smaller steps, allowing for incremental reasoning to improve accuracy. Existing stepwise reasoning strategies include chain-of-thought (CoT) prompting, which decomposes reasoning steps and is suitable for logical reasoning or multi-step computation. However, geospatial code generation workflows are highly diverse and lack fixed patterns, making it challenging to establish generalizable prompt templates. Tree-of-thought (ToT) and graph-of-thought (GoT) approaches are better suited for structured tasks like knowledge graph construction, yet the complex structures in ToT often lead to information overload in programming, affecting logical coherence. Progressive hint prompting (PHP) iteratively guides models and is useful for multi-turn task updates, but it may result in ineffective feedback when requirements are unclear. Self-improvement and self-debugging optimize output quality through feedback, but reliance on expert input reduces generation efficiency. Skeleton-of-thought (SoT), minimal decomposition, and chain-of-action (CoA) approaches favor modular generation, though insufficient information flow between modules can create \u201cinformation silos. Self-collaboration methods simulate teamwork to enhance output but rely on communication mechanisms, resulting in higher resource consumption and reasoning delays. Currently, no reasoning optimization strategy is specifically designed to enhance LLM performance in geospatial code generation."}, {"title": "2.3. Geospatial Model Construction and Code Generation", "content": "Geospatial services are modular information services delivered over networks to access, process, analyze, and display geospatial data, facilitating data sharing and reuse. Geospatial modeling uses mathematical and computational techniques to simulate and predict spatial data, aiding in the understanding and forecasting of geographic phenomena. As the core of geospatial services, geospatial modeling supports the entire process from data acquisition to analysis, prediction, and decision-making. Early geospatial modeling relied on OGC Web Services, SOAP, and REST protocols, assembling workflows through modular components to meet specific analytical needs. This approach typically required manual coding and intervention, limiting efficiency and flexibility, and struggled to meet diverse application demands.With the advancement of cloud technology, platforms such as Google Earth Engine (GEE), ArcGIS, and PIE Engine have integrated vast geospatial datasets with powerful computational resources. These platforms, through language-based open APIs, allow users to simplify data access and model development. Additionally, the geospatial community has developed toolkits for mainstream programming languages (e.g., GDAL and ArcPy for Python, Raster and Terra for R), enhancing the ease of basic geospatial modeling. However, cloud platforms and open-source toolkits are often secondary wrappers based on foundational languages, with control and data flows differing from general code, and platform-specific function naming and parameter syntax. Proprietary formats for built-in remote sensing imagery paths further raise the entry barrier for non-specialists. The introduction of LLMs represents a transformative shift for geospatial modeling from modular assembly to automated code generation. Using natural language prompts, LLMs (e.g., GPT models) can directly generate geospatial code that aligns with user needs, achieving efficient Requirement-to-Model mapping. However, the connection between model-generated code and specific implementation (Model-to-Code) remains underdeveloped, hindering fully automated Requirement-to-Code transformation. Although manual intervention is reduced, accuracy and usability challenges persist. Currently, geospatial code generation lacks tailored optimization strategies for LLMs, underscoring the need for refined prompting and knowledge-guided generation to bridge the gap from Requirement-to-Code effectively."}, {"title": "3. Chain of Programming", "content": ""}, {"title": "3.1. Overall Framework", "content": "The overall framework of CoP consists of five interconnected chain-like processes: requirement analysis, algorithm design, code implementation, code debugging, and code annotation, resembling the structured operational flow of the waterfall model. Each stage facilitates the transfer and sharing of structured information during the code development process through a shared information pool. To address knowledge gaps during model operation, the framework is equipped with retrieval-based knowledge bases, including a platform or toolkit knowledge base, a function syntax knowledge base, and a built-in dataset knowledge base. The complete chain-like framework is illustrated in Figure 2.\nFirst, a shared information pool is initialized as an explicit short-term memory module, serving as a central communication platform to store standardized information across different stages. This design ensures precise and efficient access to necessary information at each stage, minimizing reliance on stochastic memory, reducing redundant data retrieval, and preventing the omission of critical details. The information pool is cleared after each code generation task to provide sufficient storage capacity for subsequent tasks.\nDuring the requirement analysis stage, users input their geospatial code programming requirements. The LLMs evaluates the completeness of the requirements using a predefined prompt template, ensuring inclusion of eight key elements: analysis platform, programming language, analysis objective, spatial extent, temporal extent, data source and format, analytical methodology, and output format and type. Among these, the analysis platform, programming language, analysis objective, data source and format, and output format and type are categorized as required items; spatial and temporal extents are conditional items, depending on the task; and the analytical methodology is considered an optional item.The model first ensures the completeness of the required items. For conditional items, the model determines their necessity based on the task type, mandating them if required or omitting them otherwise. For optional items, the model prioritizes user-provided information and supplements missing details through inference. In cases of incomplete information, the model prompts the user for clarification while filling in missing optional details as needed. Once the requirements are verified as complete, a standardized JSON-format user requirement document is generated and stored in the shared information pool."}, {"title": "3.2. Data Preparation", "content": ""}, {"title": "3.2.1. Retrieval Knowledge Bases", "content": "The retrieval knowledge bases include a platform or toolkit knowledge base, a function syntax knowledge base, and a built-in dataset knowledge base, all stored in JSON format. The platform or toolkit knowledge base was compiled by experts using geospatial platform-related information sourced from Wikipedia, official documentation, GitHub, and technical forums such as Stack Overflow. It contains 14 records covering platform descriptions, types, task suitability, data source interfaces, access permissions, technical support, and cross-platform compatibility.The function syntax knowledge base draws from official documentation of geospatial platforms and function libraries, including materials from platform websites and GitHub repositories. It contains 8,729 records with fields such as Operator_id, Full_name, Short_name, Library_name, Language, Platform, Description, Usage, Parameters, and Output_type.The built-in dataset knowledge base aggregates preprocessed remote sensing datasets from platforms such as GEE and PIE Engine, enabling users to directly access datasets via indexing. It includes 2,732 records with fields such as Dataset_id, Name, Provider, Snippet, Tags, Description, DOI, and Website.Details of the data collection process are summarized in Table 1."}, {"title": "3.2.2. Evaluation Datasets", "content": "The evaluation dataset employs the GeoCode-Eval benchmark proposed in this study. This benchmark is constructed from open-source question banks, documentation, and real user code files, generated through expert input and LLMs using the Self-Instruct framework. GeoCode-Eval has been utilized in three previous studies to assess the geospatial code generation capabilities of LLMs. The dataset includes eight categories of tasks: operator knowledge, dataset knowledge, platform or toolkit knowledge, platform or toolkit identification, programming language identification, entity recognition, code summarization, and code generation, comprising a total of 4,000 questions. From this dataset, 500 code generation tasks were selected as the basis for this study and further categorized into three major groups and eight subcategories, as detailed in Table 2.\nTo ensure the accuracy of entity recognition, questions undergo necessary entity annotation, including user requirements, platform, programming language, analysis objectives, spatial extent, temporal extent, data source and format, analytical methodology, and output format and type."}, {"title": "4. Evaluation", "content": "The evaluation aims to scientifically quantify the following issues across various mainstream LLMs:\nEQ1: The performance change of the CoP strategy compared to zero-shot performance.\nEQ2: The performance change of the CoP strategy compared to other optimization strategies.\nEQ3 : The necessity of the three key mechanisms in the CoP strategy: shared information pool, knowledge retrieval system, and expert feedback.\nEQ4: The performance change between multiple CoP strategy fine-tuning and multiple zero-shot fine-tuning iterations."}, {"title": "4.1. Baseline Models", "content": "The CoP benchmark models selected in this study include mainstream high-performance commercial closed-source models developed after 2023, as well as general-purpose open-source LLMs and code generation models. The CoP strategy is applied to these models to compare the accuracy of code generation in a zero-shot setting. Detailed information about the baseline models is provided in Table 3."}, {"title": "4.2. Optimization Strategy", "content": "The comparison strategies selected in this study are currently mainstream optimization inference strategies in the field of code generation. These strategies are applied to GPT-4, PaLM-540B, and Code Llama-13B for comparison to achieve the evaluation results. Detailed information on the optimization strategies is provided in Table 4."}, {"title": "4.3. Evaluation Results", "content": ""}, {"title": "4.3.1. EQ1: Overall Performance?", "content": "The effectiveness of applying the CoP strategy to all baseline models was evaluated using four metrics: Matchability, Executability, Accuracy, and Readability. The baseline models generated code in a zero-shot manner based on the given prompts, while this study required the models, through prompt engineering, to output the corresponding entity information for the generated code, including Platform, Programming Language, Analysis Goal, Spatial Extent, Temporal Extent, Data Source and Format, Analysis Methodology, and Output Format. Under the CoP strategy, the generated code automatically incorporates these entity details. The four metrics were assessed by expert evaluations: Matchability was scored based on the entity matching rate; Executability was determined by the code's ability to run in a compiler, with the proportion of code that executed successfully (regardless of correctness) used as the executability score; Accuracy was defined by whether the execution results aligned with the expected outcomes; Finally, Readability was rated by five experts on a scale of 1 to 10, excluding the highest and lowest scores, and averaging the remaining three to determine the readability score. All scores were then converted to a percentage scale, retaining one decimal place. The specific scores for EQ1 are shown in Table 5.\nThe overall performance evaluation results show that the application of the CoP strategy leads to improvements in the matchability, executability, accuracy, and readability of generated geospatial code across all model types. Notably, commercial large models such as GPT-4 and GPT-4-turbo achieved scores ranging from 89.6 to 97.2, nearing their performance limits. Further analysis indicates that the CoP strategy enhances code generation performance across models of various scales. For larger models, such as GPT-4, matchability and accuracy improved from 62.6 and 52.8 to 97.2 and 86.8, respectively. For smaller models, such as CodeGemma-7B, matchability and accuracy increased from 27.2 and 12.0 to 69.4 and 49.6, while CodeGeeX2-6B saw an improvement in executability from 27.4 to 54.6. These data demonstrate that the CoP strategy exhibits strong adaptability across model scales, showcasing both universality and scalability. Additionally, the results show that the CoP strategy consistently optimizes both general-purpose LLMs and code-generation-focused LLMs. For example, in the general-purpose model LLaMA3-70B, CoP improved matchability from 46.4 to 79.6 and accuracy from 38.8 to 61.2. In the code generation-specific model StarCoder 2-15B, CoP increased executability from 30.4 to 76.4 and accuracy from 34.0 to 53.2. These findings indicate that the CoP strategy significantly enhances the structure and executability of code generation in geospatial information processing tasks and is applicable to different types of models. The visualized evaluation data is shown in Figure 8."}, {"title": "4.3.2. EQ2: Compared to other optimization strategies?", "content": "This study selected three representative models from three categories: a commercial LLM (GPT-4), a general-purpose LLM (PaLM-540B), and a model focused on code generation (Code Llama-13B), to quantitatively evaluate their code generation performance under various optimization strategies. The performance metrics used for evaluation include Matchability (Ma.), Executability (Exe.), Accuracy (Acc.), and Readability (Re.), with the specific evaluation methods described in Section 4.3. The specific scores for EQ2 are shown in Table 6.\nThe evaluation results indicate that the effectiveness of various optimization strategies in improving model performance varies significantly. Some strategies, such as ReAct and ToT, did not meet expectations and even had negative impacts on certain models. For example, with GPT-4, the matchability decreased from 62.6 to 58.4 and readability dropped from 85.6 to 77 under the ReAct strategy, while the matchability decreased to 60.2 under the ToT strategy. This may be due to the complexity of the reasoning process, which increased generation uncertainty and did not align well with the continuous requirements of geospatial code generation. In contrast, Agent-based strategies showed more effectiveness, especially in large-scale models. For instance, under the AgentCoder strategy, GPT-4's matchability improved from 62.6 to 77.8 and executability increased from 52.8 to 72.2, likely due to the enhanced collaborative capabilities of multiple agents in large models that better leverage knowledge representation and computational power.Meanwhile, prompt-based strategies demonstrated more notable improvements in smaller models. For example, with Code Llama-13B, the Self-Edit strategy boosted matchability from 37.6 to 41.0, suggesting that the simplicity of prompt strategies is better suited for guiding generation in smaller models. Additionally, the Self-Collaboration and ChatDev strategies showed some robustness across models of different scales, though their overall performance gains were not as significant as those achieved with the CoP strategy. The CoP strategy, which integrates prompt engineering, external knowledge base access, and expert feedback, enhanced contextual consistency and information integration. For example, with GPT-4, the CoP strategy improved matchability from 62.6 to 97.2 and executability from 52.8 to 92.4.Notably, other strategies generally led to a decrease in readability. For instance, with GPT-4, readability under the ToT strategy decreased from 85.6 to 78.4, and under the ReAct strategy, it dropped to 77. However, due to CoP's unique code-commenting mechanism, readability improved by 7.8 points. The specific numerical results of this improvement are visualized in Figure 9."}, {"title": "4.3.3. EQ3: Necessity of the Three Mechanisms?", "content": "In this study, models were selected from three representative types: commercial LLMs (e.g., GPT-4), general-purpose LLMs (e.g., PaLM-540B), and models focused on code generation (e.g., Code Llama-13B). The code generation performance of these models was quantitatively evaluated under different prompting strategies. The evaluation metrics included Matchability (Ma.), Executability (Exe.), Accuracy (Acc.), and Readability (Re.), with the specific methods outlined in Section 4.3. The specific scores for EQ3 are shown in Table 7.\nThere are significant differences in the performance improvements of models when using different combinations of prompting strategies. In the three representative models (GPT-4, PaLM-540B, Code Llama-13B), the complete strategy combination (i.e., with the shared information pool, retrieval, and feedback mechanisms all enabled) typically yields the best results. For example, with GPT-4, the matchability reaches 97.2, executability and accuracy are 92.4 and 86.8, respectively, and readability is 93.4, all of which significantly outperform other combinations. This suggests that integrating multi-layered prompting strategies substantially improves the accuracy and consistency of geospatial code generation. When analyzing the performance of individual strategies, there is also a consistent pattern across different models. For large models, such as GPT-4, enabling the retrieval mechanism alone resulted in a notable improvement in executability from 27.9 to 74.8 and a 20.8 percentage point increase in matchability. This indicates that the retrieval mechanism helps the model effectively access external information in complex tasks, enhancing both code execution and matching performance. In contrast, smaller models, such as Code Llama-13B, show more significant improvements when the feedback mechanism is enabled. For instance, executability increased from 41.5 to 50.1, and matchability improved from 45.3 to 47.2, suggesting that smaller models rely more on the feedback mechanism for external corrections to compensate for their limited knowledge representation.In the pairwise strategy combinations, using GPT-4 as an example, the combination of the shared information pool and the retrieval mechanism achieved a matchability of 89.5 and executability of 85.2, demonstrating that this combination effectively supports the coherence and executability of code when integrating multi-dimensional geospatial data. In contrast, the combination of the retrieval and feedback mechanisms was more advantageous in terms of execution reliability, improving GPT-4's executability to 88.9 and accuracy to 80.3, indicating that retrieval helps the model dynamically obtain external data, while the feedback mechanism corrects errors in real-time during the generation process. The combination of the shared information pool and feedback mechanism performed best in terms of readability, with GPT-4's readability improving to 91.4. This suggests that the shared information pool provides a consistent contextual background, while the feedback mechanism optimizes the output structure and logical clarity."}, {"title": "4.3.4. EQ4: Comparison of the Effectiveness of Code Debugging Iterations?", "content": "During the code testing and maintenance phase, the generated code is tested. If the code fails to execute correctly or the results do not meet expert expectations, error messages from the console or outputs that do not match expectations can be fed into the CoP process for further correction. In the CoP process, the maximum number of maintenance iterations can be controlled via hyperparameter settings. This study set different maintenance frequencies, including no maintenance, a maximum of 1, 3 (default setting), and 5 iterations, to observe the impact on executability (i.e., whether the code can run properly) and correct execution rate (i.e., whether the code not only runs but also produces expected outputs). It is important to note that \"no maintenance\" refers to no final maintenance within the CoP process, rather than zero-shot execution. The specific scores for EQ4 are shown in Table 8.\nThe results indicate that code maintenance significantly improves both the executability and correct execution rate of the code, validating the necessity of the maintenance step in the CoP process. As the number of maintenance iterations increases, there is a notable improvement in both executability (Exe.) and correct execution rate (Acc.). For example, with GPT-4, executability increased from 66.2% at Debugging@0 to 94.2% at Debugging@5, and the correct execution rate improved from 50.8% to 87.4%. This demonstrates that multiple maintenance iterations effectively enhance the probability of correct code execution. In contrast, the zero-shot strategy performed significantly worse than using the CoP process, further validating the CoP process's role in improving code generation quality. Across all models, regardless of the number of maintenance iterations, executability and correct execution rates under the CoP process were consistently higher than those with the zero-shot strategy. For instance, with GPT-4, the zero-shot executability was 52.8%, but it rapidly increased to 74.8% after Debugging@1. Similarly, the correct execution rate of Code Llama-13B rose from 17.2% under zero-shot to 50.4% after Debugging@1.The maintenance gains vary with model size. Larger models (e.g., GPT-4, PaLM-540B) showed more significant improvements in maintenance. For instance, with GPT-4, from Debugging@3 to Debugging@5, executability increased by 1.8%, and the correct execution rate improved by 1.2%. In contrast, smaller models (e.g., Code Llama-13B) showed smaller gains, with executability increasing by only 0.6% and correct execution rate improving by 2.6% from Debugging@3 to Debugging@5. This suggests that larger models respond more strongly to multiple maintenance iterations, while smaller models experience diminishing returns in maintenance, likely due to the limitations in their parameter size, which restricts their ability to respond to complex corrections.Additionally, the gain differences show that the initial maintenance step (e.g., from Debugging@0 to Debugging@1) typically yields the largest improvement, with subsequent gains gradually decreasing. For example, Code Llama-13B saw a 32 percentage point increase in executability from zero-shot to Debugging@0, while the gain from Debugging@3 to Debugging@5 was only 3.2 percentage points. This demonstrates that the marginal benefit of initial maintenance is the highest, and subsequent maintenance yields diminishing returns, providing a basis for optimizing the maintenance frequency."}, {"title": "5. Case Study", "content": "To comprehensively demonstrate the effectiveness of the CoP strategy in generating geospatial code, this study selects two representative geospatial code generation scenarios for case analysis. These two scenarios are representative across multiple dimensions: the first case study region is located in China, while the second is in an international region; one case focuses on large-scale, coarse-grained mapping at the provincial level, while the other centers on fine-grained mapping at the level of urban block buildings. In terms of data sources, the former case uses data downloaded from open-source platform interfaces, while the latter relies on locally sourced data. Regarding the execution environment, the former can be run in a local compilation environment, while the latter depends on a cloud-based analysis platform. In terms of data processing, one case involves data reading, indicator sorting, and image annotation, while the other uses independent visualization to present the distribution of analyzed indicator data.Additionally, the two cases each illustrate the processes of the \u201cUser Requirement Supplementation\u201d and \u201cCode Executability Confirmation\" mechanisms, respectively. These multidimensional demonstrations effectively showcase the applicability and validity of the CoP strategy in geospatial code generation tasks for LLMs."}, {"title": "5.1. Local Platform", "content": "This case study utilizes the Python platform, combined with libraries such as OSMnx, GeoPandas, and Matplotlib, to retrieve building data for nine designated areas in Manhattan, New York City, from OpenStreetMap. The goal is to generate high-resolution building distribution maps, showcasing the spatial distribution of buildings in each area and highlighting the ten largest buildings by area. The analysis covers nine representative neighborhoods in Manhattan, including the Financial District, Tribeca, SoHo, Chinatown, the Lower East Side, the East Village, Greenwich Village, Chelsea, and the Upper West Side. This analysis uses static building data, with no specific time span involved.In terms of data processing, building data is first retrieved for each area, the area of each building is calculated, and the largest buildings are selected. A unique color scheme is applied to each area to ensure the image boundaries are square-shaped. The final output is a high-resolution JPG image at 300 DPI, visually presenting the building distribution across different areas. This provides valuable support for geospatial analysis and urban planning, as illustrated in Figure 11."}, {"title": "5.2. Cloud Platform", "content": "This case study is based on the Google Earth Engine platform, utilizing JavaScript code to analyze the burned area in Henan Province for the year 2022. The goal is to calculate the total burned area across the province and further break it down by city-level burned areas. To achieve this, the study uses MODIS fire data provided by Google Earth Engine, identifying burned pixels for the specified year and calculating the corresponding burned area (in square kilometers). The analysis covers the entire Henan Province and presents the results at the city-level.The MODIS fire data is filtered within the geographic boundaries of Henan Province for the specified year, and a burned pixel mask is created to identify the burned regions. The burned pixels are then statistically analyzed to calculate the burned area, which is aggregated and presented by city. For visualization, the study displays the burned intensity across different regions on a map, using a color gradient to represent the severity of burning, with labels on the most severely affected areas. Additionally, a pie chart is generated to visually show the proportion of burned area in each city. The final output includes map layers (with color gradients indicating burned intensity), a pie chart illustrating the burned area proportions by city, and a textual summary containing the total number of burned pixels, resolution, and burned area (in square kilometers), as shown in Figure 12."}, {"title": "6. Conclusion", "content": "This paper introduces the Chain-of-Processing (CoP) programming framework, which builds an end-to-end pipeline from requirement analysis, algorithm design, code implementation, testing, and maintenance, aimed at overcoming the \"coding hallucination\" problem currently faced by LLMs in geospatial code generation tasks. The framework enhances the logical clarity, syntactical accuracy, and executability of generated code through innovative mechanisms such as a shared information pool, knowledge base retrieval, and expert feedback. Through case studies on building data visualization and fire damage analysis, the applicability and effectiveness of the CoP framework in various geospatial applications are validated, providing a more systematic and practical solution for geospatial modeling.\nDespite the significant advantages demonstrated by the CoP framework in geospatial code generation, the shared information pool and knowledge base retrieval mechanisms still require further optimization to enhance compatibility with different data sources and platforms. Additionally, the current expert feedback mechanism primarily relies on human intervention, and future research could explore automated feedback models to further improve code generation efficiency. As the complexity of geospatial data and application demands continue to grow, the expansion of CoP strategies into other vertical domains warrants further investigation, particularly in automating the transition from models to code, offering new insights for cross-domain code generation with LLMs."}, {"title": "Disclosure statement", "content": "No potential conflict of interest was reported by the authors."}, {"title": "Funding", "content": "The work was supported by National Natural Science Foundation of China (no. 41930107)."}, {"title": "Data availability statement", "content": "Data are available on request from the authors. The data that support the findings of this study are available from thecorresponding author, Huayi Wu, upon reasonable request."}]}