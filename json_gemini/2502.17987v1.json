{"title": "MAGE: Multi-Head Attention Guided Embeddings for Low Resource Sentiment Classification", "authors": ["Varun Vashisht", "Samar Singh", "Mihir Konduskar", "Jaskaran Singh Walia", "Vukosi Marivate"], "abstract": "Due to the lack of quality data for low-resource\nBantu languages, significant challenges are pre-\nsented in text classification and other practi-\ncal implementations. In this paper, we intro-\nduce an advanced model combining Language-\nIndependent Data Augmentation (LiDA) with\nMulti-Head Attention based weighted embed-\ndings to selectively enhance critical data points\nand improve text classification performance.\nThis integration allows us to create robust data\naugmentation strategies that are effective across\nvarious linguistic contexts, ensuring that our\nmodel can handle the unique syntactic and\nsemantic features of Bantu languages. This\napproach not only addresses the data scarcity\nissue but also sets a foundation for future re-\nsearch in low-resource language processing and\nclassification tasks.", "sections": [{"title": "1 Introduction", "content": "Text classification is one of the most widely ex-\nplored tasks in Natural Language Processing (NLP)\ndue to its diverse applications, including spam\ndetection, sentiment analysis, and topic model-\ning. Despite the impressive advancement achieved\nthrough deep learning, these methods rely heavily\non large amounts of labeled data, posing a chal-\nlenge for low-resource languages (Nie et al., 2023;\nOgueji et al., 2021). African languages in general,\nexemplify this challenge, as the scarcity of anno-\ntated datasets limits the development of effective\ntext classification models (Amol et al., 2024; Ade-\nlani, 2022). Data augmentation has emerged as\na promising solution for addressing data scarcity\nby generating synthetic data from original datasets\n(Kobayashi, 2018). Traditional augmentation tech-\nniques, including synonym replacement, sentence\nback-translation, and generative models, rely heav-\nily on language-specific resources such as pre-\ntrained word embeddings, language models, or lin-\nguistic databases, such as WordNet (Miller, 1994;\nWei and Zou, 2019; Per\u00e7in et al., 2022; Jahan et al.,\n2022). This language dependence makes these ap-\nproaches less effective for underrepresented lan-\nguages, such as Bantu languages, which lack these\nlinguistic resources (\u015eahin, 2022). To overcome\nthese limitations, Language-Independent Data Aug-\nmentation (LiDA) was introduced (Sujana and\nKao, 2023), which operates at the sentence em-\nbedding level rather than word or sentence levels.\nLiDA transforms sentence embeddings to generate\nsynthetic data, bypassing the need for language-\nspecific resources. Building upon this foundation,\nwe propose MAGE (Multi-Head Attention Guided\nEmbeddings), a framework designed to enhance\ntext classification performance for low-resource\nlanguages. MAGE extends the LiDA framework\nby introducing significant innovations to the em-\nbedding and augmentation process. Specifically,\nit replaces the traditional Denoising Autoencoder\nwith a Variational Autoencoder (VAE) to enable\nmore expressive and diverse synthetic embeddings.\nAdditionally, MAGE incorporates a novel Multi-\nHead attention mechanism that selectively empha-\nsizes salient features in the embeddings. This fo-\ncus on Multi-Head attention improves the model's\ncapacity to capture critical syntactic and seman-\ntic nuances, making it particularly effective for\nlow-resource languages. Using the AfriSenti Se-\nmEval dataset (Muhammad et al., 2023), a collec-\ntion of tweets annotated with positive, negative, and\nneutral sentiments for Kinyarwanda, Swahili, and\nTsonga, we evaluate the performance of MAGE\nin sentiment classification. Our results demon-\nstrate that MAGE outperforms baseline approaches\nin low-resource settings. Moreover, comparative\nanalyses highlight the advantages of MAGE over\nself-attention-based models, further establishing\nits value as a robust framework for addressing the\nchallenges posed by data scarcity in low-resource\nlanguages. This work not only addresses the press-\ning issue of data scarcity in Bantu languages but\nalso provides a scalable and adaptable framework\nfor extending text classification capabilities to other\nlow-resource language families. Through the in-\ntroduction of MAGE, we set the stage for future\nresearch in low-resource language processing and\nestablish a pathway to improve the inclusivity and\ngeneralizability of NLP technologies."}, {"title": "2 Related Works", "content": null}, {"title": "2.1 Data Augmentation Techniques", "content": "In recent years, data augmentation techniques have\ngained significant attention, especially for low-\nresource languages, due to the scarcity of properly\nannotated datasets and general lack of resources.\nAnalyzing one such technique by Sennrich et al.\n(2016) leverage monolingual target language data\nfor textual-based data augmentation using back-\ntranslation to enhance model performance. How-\never, this method requires extra computational re-\nsources, as it needs an additional pre-trained NMT\nmodel.\nAnother approach proposed by Lample et al.\n(2018) relies solely on monolingual corpora, which\neliminates the need for parallel data. This is\nachieved by mapping sentences from two lan-\nguages into a shared latent space using a shared\nencoder-decoder architecture. Although this\nmethod avoids the need for parallel data, it still\ndepends on monolingual corpora, which can be\nscarce for certain languages. For a simpler and\nhighly effective solution, Wei and Zou (2019) intro-\nduced the \"EDA: Easy Data Augmentation Tech-\nniques for Boosting Performance on Text Classifi-\ncation Tasks.\" Their technique consists of four op-\nerations: synonym replacement, random insertion,\nrandom swap, and random deletion. This straight-\nforward method significantly boosted model per-\nformance, even when training with only half of the\navailable dataset. EDA operates without dependen-\ncies beyond a basic synonym dictionary like Miller\n(1994).\nIn Kobayashi (2018), the authors proposed a\nnovel approach called contextual augmentation.\nUnlike previous techniques that relied on prede-\nfined synonym dictionaries or rules, this model\nuses a bidirectional language model to predict ap-\npropriate substitute words based on the surrounding\ncontext. This allows for more diverse word sugges-\ntions and has shown promising potential, consis-\ntently outperforming previous techniques like EDA,\nespecially in low-resource scenarios. In (Chen\net al., 2020) explored a technique called TMix,\nwhich interpolates hidden space representations\nof text samples to create continuous augmented\ndata, helping to mitigate overfitting in resource-\nlimited settings. Their study also incorporated en-\ntropy minimization and consistency regularization\nto effectively utilize both labeled and unlabeled\ndata, showing great potential in scenarios with ex-\ntremely limited labeled data. Another innovative\napproach, proposed by Yu et al. (2017), is Seq-\nGAN, which combines reinforcement learning with\nGenerative Adversarial Networks (GANs) to ad-\ndress challenges in generating discrete sequences.\nSeqGAN uses Monte Carlo search and models the\ngenerator as a stochastic policy to overcome the\ndifficulty of propagating gradients through discrete\noutputs. The discriminator provides rewards for\ncomplete sequences, guiding the generator via pol-\nicy gradient updates. This method demonstrated\nsignificant improvements over baseline methods in\ntasks like music generation and extends GANs for\ndiscrete sequence generation in NLP tasks. Jia and\nLiang (2017) analyzed an adversarial evaluation\nscheme for reading comprehension systems, tar-\ngeting the SQUAD dataset Rajpurkar et al. (2016).\nThis method involves adding adversarially crafted\nsentences to input paragraphs, designed to confuse\nmachine learning models while remaining clear\nto humans. The study showed that even state-of-\nthe-art models experienced performance drops un-\nder adversarial evaluation, highlighting the impor-\ntance of genuine language understanding. Raffel et\nal. (2023) delved into transfer learning, enabling\nmodels trained on high-resource languages to be\nadapted for low-resource languages. They exam-\nined methods like sentence-level alignment and\nmultilingual embeddings, which integrate linguis-\ntic resources across languages. These techniques\ncan significantly improve model performance in\nlow-resource scenarios by leveraging existing high-\nresource language data. Lastly, Li et al. (2023)\nexplored generating synthetic data for text classi-\nfication using large language models. They used\nzero-shot and few-shot settings to generate syn-\nthetic training data with tailored prompts and man-\nual annotation. The study found that model per-\nformance with synthetic data is affected by the\nsubjectivity of the classification task. Incorporat-\ning real-world examples and human feedback can\nenhance the quality of synthetic data and improve\nmodel performance, especially for subjective tasks."}, {"title": "2.2 Data Augmentation for Low Resource Corpora Text Classification", "content": "One of the problems performing various down-\nstream tasks in NLP is working with languages\nthat don't have much labeled data. At such times,\ndata augmentation techniques help tackle the prob-\nlem. One such approach is TAU-DR by Rahamim\net al. (2023) which uses soft prompts and keeps\nlanguage model frozen to reconstruct hidden rep-\nresentations and then turning those back into syn-\nthetic sentences and proves to be really effective\nat improving multi-class classification. Addition-\nally (Thangaraj et al., 2024) also conducted a study\non the cross-lingual transfer capabilities in low-\nresource african languages and benchmarked the\nforgetting metrics, although this study did not em-\nploy augmentation metrics to reduce the model's\nforgetting. Some other techniques like AEDA in\nKarimi et al. (2021) which inserts punctuation\nmarks into sentences to create variations unlike\nmethods like EDA where data loss is possible as it\ninvolves operations like deletion and substitution,\nthis method preserves the original information and\nsemantic consistency. After testing on multiple\ndatasets AEDA consistently outperformed EDA,\nparticularly in low-resource settings, The simplic-\nity of AEDA and its performance across different\nmodels highlights its effectiveness. IndiText Boost\nby Litake et al. (2024), is a framework tailored for\nthe underrepresented Indian languages, which uti-\nlizes techniques like EDA and back-translation to\noutperform the more complex LLM-based methods\nin basic classification tasks.\nIn Sahu et al. 2022, the authors explore prompt-\nbased data augmentation in intent classification\ntasks using Large Language Models (LLMs) like\ngpt2 (Radford et al., 2019) which boils down\nto just using a pre-trained LLM to augment\ndata by creating synthetic data but due to LLMs\ntendency to create unreliable data this can lead\nto reducing data's quality. In Zhao et al. (2022),\nthe authors proposed EPiDA, (Easy Plug-in Data\nAugmentation) framework, which works for text\nclassification. It employs two methods, condi-\ntional entropy minimization and relative entropy\nmaximization which balances the diversity and\nquality of augmented data. Conditional entropy\nminimization ensures the semantic consistency\nwhile relative entropy maximization promotes\nmore diverse samples. Through extensive testing it\nwas seen that it consistently outperforms existing\ndata augmentation techniques in various NLP\ntasks highlighting its application for low-resource\napplications."}, {"title": "2.3 LiDA - Language Indpendent Data Augmentation", "content": "The LiDA: Language-Independent Data Augmen-\ntation for Text Classification by Sujana and Kao\n(2023) introduced us to a novel augmentation tech-\nnique catering to low-resource language settings.\nThe authors, rather than creating synthetic data\nfor increasing the dataset, worked on augmenting\nthe sentence level embeddings for the classification\ntask, resulting in a 2%-3% improvement on average\nin LSTM classification results. The language inde-\npendence mentioned arises from the multilingual\ndataset that is used for the training of the SBERT\nmodel."}, {"title": "3 Methodology", "content": "To further refine the LiDA architecture, we propose\na Multi-Head attention-based mechanism to quan-\ntitatively highlight and weight the individual em-\nbeddings to emphasize the important contributions\nof the LiDA architecture for the text-classification\ngoal."}, {"title": "3.1 Dataset", "content": "The dataset referred to is the AfriSenti SemEval\nShared Task - 12 dataset by Muhammad et al.\n(2023) based on tweet sentiment analysis. As the\nstudy focuses on the Bantu language family, the\ndatasets of the following 3 Bantu languages were\nchosen - Kinyarwanda, Xitsonga and Swahili hav-\ning 7940 tweet-label pairs in the combined training\nset and 1482 tweet-label pairs in the combined test\nset. We observe a skewness in the data towards Kin-\nyarwanda due to Kinyarwanda having the highest\ndata points at 5155 tweets, with Swahili being the\nsecond highest at 3009 tweets and Xitsonga having\nthe least data at 1258 tweets."}, {"title": "3.2 Architecture", "content": "The previous architectures and frameworks dis-\ncussed have focused primarily on widely studied\nlanguages such as English, Indonesian, Chinese,\nFrench, and others. Although these languages bene-\nfit from extensive resources and established linguis-\ntic frameworks, our work diverges by addressing\nAfrican Bantu languages, which are linguistically\ndistinct and underrepresented in computational re-\nsearch. Bantu languages exhibit unique structural\nand morphological characteristics, requiring spe-\ncialized approaches that go beyond the methodolo-\ngies applied to more commonly studied languages.\nHence, the embedding models and the architectural\ncomplexities in the components of previous frame-\nworks do not conform to the requirements of the\nBantu languages.\nTaking LiDA as our base framework, we propose\nour modified architecture in 3.2.2 creating a robust\narchitecture that caters to the demands of Bantu\nlanguages."}, {"title": "3.2.1 LiDA Architecture", "content": "The LiDA architecture (figure 1) makes use of the\nmultilingual SBERT model making the architecture\nlanguage independent in essence. The embeddings\nso generated are passed through three functions\nlinear transformation, autoencoder model, denois-\ning autoencoder model - before being concatenated\nwith the original embeddings and henceforth clas-\nsified using LSTM and BERT classifiers."}, {"title": "3.2.2 Proposed Architecture", "content": "Figure 2 shows our modification to the orig-\nnal LiDA architecture by changing the embed-\nding model to AfriBERTa, replacing the denois-\ning autoencoder with a variational autoencoder\nand addressing the concatenation of complex low-\nresource languages such as the Bantu language\nfamily by the introduction of weighted concatena-\ntion using Multi-Head attention. The tweets are\npassed through the AfriBERTa model, the choice\nof which is discussed in the sub-section 3.3. The\nmodel outputs a 768-dimensional representation\nof the text which is passed through the aforemen-\ntioned transformation functions.\nThe Linear Transformation Layer introduces\ncontrolled variability into the input embeddings\nby applying a randomized shift, enhancing the ro-\nbustness and generalizability of the representations.\nFor each embedding, a random noise vector r is\nsampled uniformly within a range [rmin, 'max] and\nadded to the original embedding, resulting in a\ntransformed embedding e' = e+r. This operation\nis performed independently for all embeddings in\nthe training and testing datasets. The parameters\nrmin and max can be adjusted to control the magni-\ntude of perturbation, ensuring that the embeddings\nretain their original semantic structure while intro-\nducing sufficient variability to aid learning.\nAutoencoder is a key part of the augmentation\nprocess, designed to refine and diversify input em-\nbeddings by learning compressed representations\nwhile retaining essential features. This is accom-\nplished through an encoder-decoder architecture\nthat reduces the input embedding dimensions to a\nlatent space and reconstructs them back to the orig-\ninal size. This introduces subtle variations while\npreserving essential semantic features, enhancing\nthe diversity of augmented data. The model has\nbeen slightly enhanced from the original model\nused in Sujana and Kao (2023) with Leaky ReLU\nactivations, Batch Normalization - to ensure sta-"}, {"title": "3.3 AfriBERTa as the embedding model", "content": "For this study, we selected the AfriBERTa model\nby Ogueji et al. (2021) model to generate embed-\ndings. This decision was informed by a detailed\ncomparative analysis of multiple models, including\nBantuBERTa Parvess (2023), afro-xlmr-large by\nAlabi et al. (2022), and mBERT, across critical\nclassification metrics such as accuracy, precision,\nrecall, and F1-score (see table 2 and table 3) on\nboth individual fine-tuning for each of the three\nlanguages as well as on the combined dataset as\nused in our final experiments as well. AfriBERTa's\nresults align with the study Thangaraj et al. (2024)\nhighlighting Afriberta's effectiveness, particularly\nin low-resource settings, by achieving the highest\nperformance metrics after fine-tuning in contrast to\nother comparable models including BantuBERTa.\nLanguage-specific fine-tuning results are present in\nthe supplementary material for reference."}, {"title": "3.4 MAGE", "content": "The Multi-Head Attention component is designed\nto enhance the model's ability to focus on important\nembeddings by assigning distinct weights to dif-\nferent embeddings using multiple attention heads.\nThis method allows for the dynamic selection of\nwhich embeddings have more influence on the fi-\nnal classification decision, effectively highlighting\nthe critical features. The num_heads was set to 4\nensuring that multiple perspectives of the embed-\ndings are captured simultaneously. The use of at-\ntention mechanism stemmed from the observation\nthat on manually weighting embeddings improved\nclassification results were observed, and attention\nprovides a learnable way to optimize this process.\nIn this approach, the embeddings are attended to\nusing Multi-Head attention, where each head inde-\npendently processes the embeddings and captures\ndifferent aspects of the feature space. The context\nvectors, which are trainable, guides the attention\nmechanism to focus on the most relevant embed-\ndings. The outputs from each attention head are\nthen concatenated, and an aggregation is performed\nby summing the resulting vectors, capturing the\nmost important features across different heads."}, {"title": "3.5 Classification", "content": "We employed two architectures for classification,\nnamely, LSTM and Logistic Regression and the\nresults were evaluated using Accuracy, Precision,\nRecall and F1 Score. The LSTM classifier was\nused with an input dimension of 768, hidden di-\nmension of 128, and a single layer. The model was\ntrained with a learning rate of 0.001 using CrossEn-\ntropyLoss. An Early Stopping mechanism with a\npatience of 3 epochs was employed, along with the\nStepLR scheduler to adjust the learning rate.\nThe second classifier used was Logistic Re-\ngression, which serves as a lightweight yet effec-\ntive baseline for classification. The model was\ntrained with a maximum of 1000 iterations using\nthe LBFGS solver for optimization. Since Logistic\nRegression is a simple linear model, it provides\na useful comparison against the LSTM's sequen-\ntial feature extraction capabilities. By analyzing\nboth models, we aim to assess the impact of com-\nplex sequential modeling versus traditional linear\nclassification on our dataset."}, {"title": "4 Results", "content": "In this section, we present the classification perfor-\nmance of various embedding configurations and\nattention mechanisms, evaluated using standard\nmetrics such as accuracy, precision, recall, and F1\nscore. In addition, a more detailed ablation study is\nincluded in the supplementary material, where we\nalso explore grouphead attention as an alternative\nattention mechanism."}, {"title": "4.1 Effect of DAE and VAE", "content": "We first compare the performance of the original\nembeddings using the proposed VAE vs DAE con-\nfigurations using LSTM and Logistic Regression\nwithout weighted attention concatenation mecha-\nnism.\nObserving table 4 we see an increase in the met-\nrics over the original results for the DAE and VAE\nconfiguration using LSTM classifier.\nSimilarly for logistic regression, in table 5, we\nobserve for VAE configuration - 0.34%, 0.37%,"}, {"title": "4.2 Integrating Multi-Head Attention", "content": "For greater generalization and to test the reliabil-\nity of attention mechanisms we benchmarked the\nLSTM classifier by shuffling the dataset 4 times, in\neach shuffle running the benchmarks for 5 stand-\nalone iterations. Figure 4 visualizes the average ac-\ncuracy achieved over the shuffles comparing with\nand without Multi-Head attention with both DAE\nand VAE configurations. The drastic improvement\nin classification accuracy compared to the baseline\nembeddings are clearly visualized here with the\nhighest increase of 3.64% for MAGE+DAE over\nthe baseline, closely followed by MAGE+VAE at\n2.51% improvement. The attention mechanism fur-\nther boosts the VAE and DAE configurations with\nan increase of 0.4% and 1.21% respectively.\nSimilarly, we followed a similar shuffling trend\nfor benchmarking the logistic regression classifier\nachieving the averaged out results as noted in fig-\nure 3. We can observe how our attention configura-\ntions outperform the baseline configurations across\nall metrics."}, {"title": "5 Conclusion", "content": "We thus present an innovative approach to embed-\nding refinement and classification for the Bantu lan-\nguage family by integrating embedding level trans-\nformations and advanced attention mechanisms.\nThrough systematic experimentation, we demon-"}, {"title": "6 Limitations", "content": "While our proposed approach demonstrates notable\nimprovements in embedding refinement and clas-\nsification, it has several limitations. First, our ex-\nperiments were conducted on a limited set of three\nBantu languages which restricts the generalizabil-\nity of our findings to other Bantu and low-resource\nlanguages. Second, the dataset exhibits an imbal-\nance, with Kinyarwanda comprising the majority\nof data points. This skewness may introduce bi-\nases in model learning and affect the performance\nacross languages. Third, the dataset size may be\ninsufficient for training complex components like\nthe Denoising Autoencoder, Variational Autoen-\ncoder, and the standard Autoencoder as they re-\nquire a large and diverse dataset to learn mean-\ningful latent representations effectively. The lim-\nited training data could lead to suboptimal embed-\ndings, affecting downstream classification perfor-\nmance. Fourth, while embedding-level transforma-\ntions using denoising and variational autoencoders\nrefine embedding structures, their impact on pre-\nserving linguistic nuances requires further investi-\ngation. Finally, the computational complexity of\nour approach, particularly the attention mechanism,\nmay pose challenges for real-time applications in\nresource-constrained environments. Addressing\nthese limitations in future research will be essential\nfor broader adoption and scalability."}]}