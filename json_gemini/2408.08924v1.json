{"title": "Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks", "authors": ["Jiawei Zhao", "Kejiang Chen", "Xiaojian Yuan", "Weiming Zhang"], "abstract": "In recent years, the rapid development of large language models (LLMs) has achieved remarkable performance across various tasks. However, research indicates that LLMs are vulnerable to jailbreak attacks, where adversaries can induce the generation of harmful content through meticulously crafted prompts. This vulnerability poses significant challenges to the secure use and promotion of LLMs. Existing defense methods offer protection from different perspectives but often suffer from insufficient effectiveness or a significant impact on the model's capabilities. In this paper, we propose a plug-and-play and easy-to-deploy jailbreak defense framework, namely Prefix Guidance (PG), which guides the model to identify harmful prompts by directly setting the first few tokens of the model's output. This approach combines the model's inherent security capabilities with an external classifier to defend against jailbreak attacks. We demonstrate the effectiveness of PG across three models and five attack methods. Compared to baselines, our approach is generally more effective on average. Additionally, results on the Just-Eval benchmark further confirm PG's superiority to preserve the model's performance.", "sections": [{"title": "Introduction", "content": "In recent years, large language models (LLMs) such as ChatGPT (Brown et al. 2020), Gemini (Team, Anil et al. 2024), and Llama (Touvron, Martin et al. 2023) have demonstrated excellent performance in various NLP tasks, including code generation (Zhong and Wang 2024), text translation (Zeng et al. 2024a), and logical reasoning (Wei et al. 2022). However, recent research indicates that LLMs are susceptible to jailbreak attacks (OpenAI et al. 2024). When fed carefully crafted harmful prompts, these models can generate harmful content, including violence, pornography, and discrimination. This poses significant challenges to the reliability and trustworthiness of LLMs.\nTo protect LLMs from jailbreak attacks, previous methods have primarily focused on external defenses (including input and output protections) and internal defenses (within the model itself). For external defenses, on the input side, previous work has mainly aimed to achieve defense by perturbing the input. For example, perturbing the input through simple character transformations such as substitution or deletion (Robey et al. 2024), paraphrasing the input using an LLM to disrupt carefully crafted malicious prompts (Jain et al. 2023), or altering the input distribution through re-tokenization (Jain et al. 2023) to defend against jailbreak attacks. However, these methods of perturbing inputs often lack effectiveness and will affect the quality and accuracy of the model's output text when the perturbation is substantial. On the output side, previous work has primarily focused on controlling the LLM decoding process to reduce the output of harmful content, for example, by utilizing contrastive decoding (Xu et al. 2024) or methods that detect and regenerate each token sequentially to produce the output (Li et al. 2024b). However, such methods are often challenging to deploy and time-consuming. For instance, contrastive decoding requires fine-tuning an expert model, and token-by-token detection and regeneration necessitate confirmation by the LLM for each token, resulting in prolonged processing time. For internal defenses, previous work has mainly focused on fine-tuning the model (Hu et al. 2022; Zhao et al. 2024) or utilizing the model's inherent security capabilities to detect harmful inputs (Xie et al. 2023; Phute et al. 2024). However, fine-tuning is time-consuming, and previous applications of the model's inherent capabilities have lacked effectiveness.\nTo bridge this gap, in this paper, we propose a novel jailbreak defense method, Prefix Guidance (PG). By directly setting a prefix for the model's output, such as \"I'm sorry,\" the model is guided to recognize harmful queries. This method combines the model's inherent security capabilities with an external classifier to defend against jailbreak attacks. The specific process of PG is illustrated in Figure 1. The proposal of PG is based on the following three insights.\nLLMs inherently possess certain security capabilities, as seen in models that have undergone safety alignment, like Llama.\nThe output of LLMs is significantly influenced by the first few tokens. GCG (Zou et al. 2023) exploits this feature to perform jailbreaks. Previous work has also shown that setting \"forced words,\" i.e., model output pre-"}, {"title": "Background and Related Works", "content": "In this section, we will introduce previous jailbreak attack methods and the corresponding defense methods against jailbreak attacks."}, {"title": "Jailbreak Attacks", "content": "Previous jailbreak attacks can be categorized into empirical jailbreak methods and heuristic jailbreak methods."}, {"title": "Empirical Jailbreak Methods", "content": "Empirical jailbreak methods primarily rely on the design of specific prompts tailored to the characteristics of the model, using prior knowledge to bypass the defenses of LLMs. One category of these attacks exploits the model's understanding of input text by designing prompt templates that leverage techniques such as Pretending, Attention Shifting, and Privilege Escalation to jailbreak LLMs (Liu et al. 2024b). Representative attacks include DAN (Do Anything Now), a prompt template that triggers an unlocked mode in the model (Shen et al. 2024), and DeepInception (Li et al. 2024a), a template that induces the model to engage in deep thinking. Another category of these attacks exploits the model's generative characteristics to jailbreak LLMs. For example, ReNeLLM exploits the security vulnerabilities of LLMs in different scenarios, such as code and text continuation, combining methods like rewriting, miswriting, and multilingual approaches to jailbreak LLMs (Ding et al. 2024). Chain-of-Thought techniques have been used to jailbreak models (Li et al. 2023), as well as multi-language methods that exploit differences in the model's security defenses across languages to jailbreak the model (Deng et al. 2023b; Yong, Menghini, and Bach 2024). Other methods take advantage of the model's contextual learning abilities to bypass security (Wei et al. 2024). These methods rely on the discovery of model characteristics and can be considered as \u201czero-day vulnerabilities\u201d for LLMs."}, {"title": "Heuristic Jailbreak Methods", "content": "Heuristic jailbreak methods utilize automated methods to construct effective prompts by focusing on certain high-dimensional features of the model, often involving a degree of unpredictability in the process. For instance, the GCG method (Zou et al. 2023) uses a random greedy search to construct input suffixes, maximizing the likelihood of the model outputting affirmative prefixes like \"Sure, here is,\u201d thereby jailbreaking LLMs. The Pair method (Chao et al. 2024) uses LLMs as input optimizers, iterating multiple times to obtain an effective jailbreak prompt. Other methods, such as AutoDAN (Liu et al. 2024a) and SAP30 (Deng et al. 2023a), employ genetic algorithms or other heuristic algorithms to automatically optimize prompts for jailbreaking LLMs. These types of attack algorithms do not require extensive experiential knowledge and are capable of generating a diverse range of jailbreak prompts."}, {"title": "Jailbreak Defense", "content": "Previous jailbreak methods can be primarily categorized into two aspects: external defenses (including input and output) and internal defenses (model itself)."}, {"title": "External Defenses", "content": "Input perturbation-based methods primarily defend against jailbreak attacks by introducing various levels of perturbations to the input, thus disrupting carefully crafted jailbreak prompts. Previous methods include simple character-level perturbations (Robey et al. 2024), LLM paraphrase of input text (Jain et al. 2023), or retokenization of input text (e.g., splitting a single token \"studying\" into \"study\" + \"ing\") to defend against jailbreak attacks (Jain et al. 2023).\nOutput control-based methods primarily control the process of LLM sampling tokens to reduce the output of harmful content. For example, SafeDecoding (Xu et al. 2024) introduces an expert model and adopts a contrastive decoding method to output tokens, ensuring the model's safe output. Rain (Li et al. 2024b) self-evaluates each token output, rewinds, and determines the final output."}, {"title": "Internal Defenses", "content": "Internal defenses primarily leverage the model's intrinsic security capabilities to identify and defend against jailbreak attacks. For instance, Self-Reminder (Xie et al. 2023) adds prompts to the model's system prompt and user prompt to defend against jailbreak attacks. Self-Examination (Phute et al. 2024) enables the LLMa to judge whether the output is harmful and filter out malicious prompts. Additionally, fine-tuning (Hu et al. 2022) or LLM knowledge editing (Zhao et al. 2024) can be employed to enhance the model's intrinsic security capabilities and achieve defense against attacks."}, {"title": "Methodology", "content": "In this section, we provide the necessary definitions and the pipeline of Prefix Guidance (PG). The implementation of PG mainly consists of three parts: prefix selection, harmful prompt classification (identifying whether the reason for refusal is due to a harmful prompt), and final result generation."}, {"title": "Preliminary", "content": "In a single-turn dialogue, the input and output of a LLM are defined by five components, as illustrated in Figure 2. These components are the system prompt, user prefix, user prompt, assistant prefix, and the assistant prompt, denoted as $T_s$, $T_{up}$, $T_u$, $T_{ap}$, and $T_a$, respectively. The user's input prompt is $T_u$, and the final input to the LLM is $T_s + T_{up} + T_u + T_{ap}$. The output from the LLM is $T_a$.\nThus, given a LLM $\\Theta$ and input $T_u$, the probability of the model outputting $T_a$ is:\n$P_a = p_{\\Theta}(T_a | T_s + T_{up} + T_u + T_{ap})$\nTypically, $T_s$, $T_{up}$, and $T_{ap}$ do not change, as determined by the training process of the LLM. Let $T_a$ be the token sequence $x_{1:n}$. For an autoregressive LLM, we have:\n$P_a = \\prod_{i=1}^{n} p_{\\Theta}(x_i | T_s + T_{up} + T_u + T_{ap} + x_{1:i-1})$"}, {"title": "Threat Model", "content": "Adversary's Knowledge. Jailbreak attacks can be categorized into black-box and white-box attacks. In black-box attacks, the attacker cannot access the specific parameters of the LLM and can only obtain the model's output. In contrast, white-box attacks allow the attacker to access all information about the model, including parameters and prompt settings such as the system prompt.\nAdversary's Goal. The goal of a jailbreak attack is to design a jailbreak version of harmful prompt $T_{um}$ such that the model's output satisfies the prompt without being rejected.\nDefender's Knowledge. The defender can control all settings of the model, including its parameters and prompt settings, such as the system prompt.\nDefender's Goal. The defender has two main objectives:\nFor harmful user input $T_{um}$, the model output should be a refusal response.\nFor non-harmful user input $T_{un}$, the model output should be a normal response.\nLet the dataset of harmful inputs be $D_r$ and the dataset of non-harmful inputs be $D_r'$. In the PG method, the goal is to recognize harmful $T_u$ after setting the first few tokens of $T_a$. Thus, the jailbreak defense objectives in PG are transformed as follows:\nFor harmful user input $T_{um}$, the model output should be a refusal reason explaining the harmfulness of $T_{um}$.\nFor non-harmful user input $T_{un}$, the model output should be a hallucination related to $T_{un}$.\nLet the set model output prefix be $x_{1:r}$. The PG defense objective can be defined as:\n$\\arg \\max_{x_{1:r}} p_{\\Theta} (x_{r+1:n} | T_s + T_{up} + T_{um} + T_{ap} + x_{1:r}) + p_{\\Theta} (x_{r+1:n} | T_s + T_{up} + T_{un} + T_{ap} + x_{1:r})$\ns.t. $x_{r+1:n} \\in H$ and $x'_{r+1:n} \\in H'$"}, {"title": "Prefix Selection", "content": "To achieve the objective in Equation 3, the prefix selection is divided into the following three steps:\nFirst, we input $D$ into the model, examine the general refusal response, and set it as a temporary prefix. Then, we input the positive samples (harmful prompt $T_{um}$) dataset $D$ and the negative samples (non-harmful prompt $T_{un}$) dataset $D'$ into the model to obtain the output results $f_{\\Theta}(D)$ and $f_{\\Theta}(D')$, where $f_{\\Theta}$ is the model's processing function for the input.\nLet $Y$ = prefix($f_{\\Theta}(D)$) $\\cap$ prefix($f_{\\Theta}(D')$). Take the longest common prefix in Y and set it as $x_{1:l}$. Consider its different length prefix substrings as the candidate final prefixes.\nSet the prefixes obtained in the second step as the model output prefixes. Randomly select $k_D$ samples from both $D$ and $D'$. For the $k_D$ samples in D, calculate the number of hallucinations related to $T_{um}$ generated by the model when the user inputs $T_{um}$. For the $k_D$ samples in $D'$, calculate the number of times the model provides refusal reasons indicating the harmfulness of $T_{un}$ when the user inputs $T_{un}$. Compute the total percentage of incorrect outputs. Select the prefix with the lowest error percentage as the final result, i.e., the final model output prefix $x_{1:r}$."}, {"title": "Harmful Prompt Classification", "content": "Classifier Training To determine if the model's output provides a reason for refusing a harmful input, we fine-tuned a roberta-base (Liu et al. 2019) model as a classifier. Specifically, we added a fully connected layer on top of the Roberta model and performed full-parameter fine-tuning using prepared data, resulting in a binary classifier. Detailed training procedures are presented in Section Experiments.\nClassifier Usage After setting the final prefix $x_{1:r}$, the user input $T_u$ is fed into the large model, which outputs k new tokens $x_{r+1:r+k}$. This output is then fed into the classifier, which produces the logits result $v \\in \\mathbb{R}^2$."}, {"title": "Final Result Generation", "content": "The logits result v is used as the basis for judgment:\nIf v[0] > v[1], it indicates that the user input is a harmful prompt. Since the prefix we set is a refusal prefix, the refusal reason is fully generated. This is done by concatenating $x_{1:r+k}$ to the original prompt, inputting it back into the model, and obtaining the final output $T_a = x_{1:r+k} + f_{\\Theta}(T_s + T_{up} + T_u + T_{ap} + x_{(1:r+k)})$.\nIf v[0] < v[1], it indicates that the user input is a normal prompt. To prevent the refusal prefix from affecting the model's response to normal input, the original prompt is re-input into the model, and the final output is $T_a = f_{\\Theta}(T_s + T_{up} + T_u + T_{ap})$."}, {"title": "Experiments", "content": "In this section, we provide a detailed introduction to our experiments, demonstrating the effectiveness of PG in defense and the preservation of the model's capabilities."}, {"title": "Experimental Setup", "content": "Datasets. Given that the existing red team datasets for LLMs have an uneven distribution of different types of harmful data, which is not conducive to the training of the classifier and prefix selection described in Section Methodology, we created a new harmful instruction dataset named harmful-instruction. Specifically, we generated approximately 250 harmful instructions for each of the six categories of harmful instructions classified in Llama Guard (Inan et al. 2023): Violence & Hate, Sexual Content, Guns & Illegal, Regulated or Controlled Substaances, Suicide & Self Harm, and Criminal Planning, resulting in a total of 1,550 harmful instructions. The dataset was generated as follows:\nFor each type of harmful instruction, we designed a prompt template, replacing the part describing the category with the corresponding description and then inputting it into different LLMs to obtain the final result. To enrich the diversity of the instructions, we used OpenAI's ChatGPT-3.5, ChatGPT-4, and Google's Gemini model as the instruction generation models.\nWe use Advbench as the evaluation dataset to assess the effectiveness of our defense methods, following (Chao et al. 2024; Zeng et al. 2024b; Xu et al. 2024). We utilize 50 distinct representative harmful queries from Advbench to generate jailbreak attack prompts.\nTo evaluate the model's capabilities, we use the top 800 instructions from the Just-Eval dataset, assessing the model from five aspects: Helpfulness, Clarity, Factuality, Depth, and Engagement. We use GPT-4 Mini as the scoring model.\nModels. Our experiments primarily use three models: Vicuna-7B-v1.5, Llama2-7B-Chat, and Guanaco-7B to evaluate PG.\nJailbreak Attack Methods. We selected five representative jailbreak attack methods for our experiments: GCG, AutoDAN, Pair, ReNeLLM and DeepInception. GCG is a"}, {"title": "Prefix Guidance Classifier Training", "content": "We fine-tuned the Roberta-base model to identify whether the model output is an explanation and refusal of the malicious intent of the input prompt.\nData Preparation. We first constructed two types of datasets using the Vicuna model:\nRefusal reasons explaining the harmfulness of malicious input $T_{um}$.\nHallucinations related to normal input $T_{un}$.\nSpecifically, we initially set the Vicuna model's output prefix to \"I'm sorry\" (a general refusal prefix used by the model) and selected the harmful-instruction dataset described in Section Datasets to generate the Type 1 dataset. For the Type 2 dataset, we randomly selected 1,550 entries from the Alpaca dataset (Taori et al. 2023) to form a normal dataset and used the Vicuna model to generate the Type 2 dataset.\nTraining. We added a fully connected layer on top of the Roberta-base model, with an input dimension of 768 and an output dimension of 2. The Adam optimizer was used with a learning rate (lr) of 1e-5 and epsilon (eps) of 1e-8. The loss function was the cross-entropy function. We trained with a batch size of 64 and set the number of epochs to 10."}, {"title": "Metric", "content": "We employed two metrics, ASR and harmful score, to evaluate the effectiveness of Prefix Guidance (PG). Additionally, we used Just-Eval to assess the model's capabilities across various dimensions.\nAttack Success Rate. We define the attack success rate as:\n$ASR = \\frac{\\text{Number of successful attack prompts}}{\\text{Total number of prompts}}$\nWe used Dic-Judge, a keyword retrieval method, to determine whether the model refused to respond to harmful prompts, and based on this, calculated the number of successful attack prompts as:\n$\\text{Number of successful attack prompts} = \\text{Total number of prompts} - \\text{Number of refusals}$"}, {"title": "Enhancement of Model Security Capabilities through Prefix Guidance", "content": "Table 2 presents our evaluation of the effectiveness of various defense methods against five different attack methods. Almost all defense methods showed some reduction in ASR and harmful score across the different attack methods, with the exception of the llama model, where the Self-Reminder method actually resulted in a slight increase in harmful score against ReNeLLM, which may be due to the unique nature of ReNeLLM attacks. Apart from this, ASR and harmful score are generally positively correlated, which indicates the validity of these two metrics. More importantly, in Table 2, the PG method significantly reduces the jailbreak success rate and harmful score of various attack methods, achieving near 0% defense success rates against most attack methods. Compared to other methods that leverage the model's intrinsic capabilities, the PG method outperforms them across almost all metrics and is comparable to the SOTA SafeDecoding method, even slightly outperforming it in some cases. For instance, in the Vicuna model, against the ReNeLLM attack, SafeDecoding only managed to reduce the ASR by 8% and the harmful score by 0.1, while PG reduced the ASR by 46% and the harmful score by 1.36.\nThe experimental results of PG on the Guanaco model are shown in Table 5 in the Appendices.\nIn conclusion, our PG method demonstrates strong effec-"}, {"title": "Preservation of Model Capabilities through Prefix Guidance", "content": "Table 3 shows the evaluation of the model's capabilities after deploying various defense methods using the Just-Eval method. Most defense methods negatively impact model performance across multiple metrics. In contrast, the Self-Reminder method enhances these metrics. This unexpected improvement might be attributed to the defense prompts' alignment with prompt engineering principles, as outlined in (White et al. 2023). Our PG method generally causes a reduction in model performance across most models, with an average performance loss of 4% on the Vicuna model and 5% on the Guanaco model, but it results in a 1% improvement on the Llama model. Although the PG method causes more damage to model performance compared to some other defense methods that leverage the model's intrinsic capabilities, it still outperforms the SOTA SafeDecoding method. The experimental results of PG on the Guanaco model are presented in Table 6 in the Appendices."}, {"title": "Limitation and Future work", "content": "Time Overhead with 50-Token Output Using a 50-token output as a criterion still incurs considerable time overhead. After deploying Prefix Guidance (PG) techniques, it might be possible to leverage the model's internal features to detect malicious prompts without relying on the model's output. This will be a key focus of our future research.\nBetter Search Method for Prefix Selection Our method currently employs a greedy search strategy for prefix selection, which constrains model performance. Experiments on Llama demonstrate that a well-chosen prefix can significantly preserve the model's capabilities. Future work will concentrate on developing more effective heuristic algorithms to enhance prefix search.\nGeneralization of PG for Jailbreak Defense Although our method outperforms baseline methods in defending against attacks on ReNeLLM, it still falls short in significantly mitigating jailbreak strategies. Enhancing the generalization capability of PG to maintain a robust defense against diverse jailbreak attacks will be a major focus of our future work."}, {"title": "Conclusion", "content": "In this paper, we propose a novel defense method against jailbreak attacks called Prefix Guidance (PG). Compared to previous defense methods that leverage the model's inherent capabilities, PG demonstrates superior defensive performance, achieving results on par with the state-of-the-art SafeDecoding method. Moreover, PG incurs minimal performance degradation, with a loss ranging from only 0% to 5% across various models, surpassing SafeDecoding in this regard. Our experiments, conducted on three different models and against five types of attack methods, substantiate these findings. Additionally, PG is a plug-and-play solution that can be deployed simply by setting a prefix for the model output, requiring minimal code modification. This makes it particularly suitable for more complex code environments."}, {"title": "Appendices", "content": "Prompt Template for harmful-instruction\nIn this section, we present the template used to construct the harmful-instruction dataset. Figure 6 illustrates our template, where the $$ portion can be replaced with the required parameters. Specifically, $number$ should be filled in with a specific number, $Description of the classification$ should contain the description of a particular type of harmful instruction, and $classification$ should be filled in with the category of the harmful instruction."}]}