{"title": "SAFETY LAYERS OF ALIGNED LARGE LANGUAGE MODELS: THE KEY TO LLM SECURITY", "authors": ["Shen Li", "Liuyi Yao", "Lan Zhang", "Yaliang Li"], "abstract": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining this\nsecurity is not well understood, further these models are vulnerable to security\ndegradation when fine-tuned with non-malicious backdoor data or normal data.\nTo address these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous layers\nin the middle of the model that are crucial for distinguishing malicious queries\nfrom normal ones, referred to as \"safety layers\". We first confirm the existence\nof these safety layers by analyzing variations in input vectors within the model's\ninternal layers. Additionally, we leverage the over-rejection phenomenon and pa-\nrameters scaling analysis to precisely locate the safety layers. Building on this\nunderstanding, we propose a novel fine-tuning approach, Safely Partial-Parameter\nFine-Tuning (SPPFT), that fixes the gradient of the safety layers during fine-tuning\nto address the security degradation. Our experiments demonstrate that this ap-\nproach significantly preserves model security while maintaining performance and\nreducing computational resources compared to full fine-tuning.", "sections": [{"title": "INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs) have showcased remarkable abilities in\nnatural language generation. However, this progress is accompanied by the risk of producing of\nharmful or biased outputs, especially when confronted with malicious input prompts. To address\nthis issue, the prevalent approach involves additional reinforcement learning from human feedback\n(RLHF) (Bai et al., 2022; Dai et al., 2023; Ouyang et al., 2022b) and instruction fine-tuning Wang\net al. (2022) on pre-trained LLMs. This process aligns the LLMs with human values and ensures\ntheir behavior remains within safe boundaries. These securely aligned models significantly reduce\nthe risk of harmful content leakage when the models are used directly.\nReal-world applications often require fine-tuning aligned models to adapt to specific domains. This\npresents a significant challenge: fine-tuning these models with non-malicious normal datasets along-\nside backdoor datasets, which may favor positive responses, can compromise the security alignment\nof the models (Qi et al., 2023; Kumar et al., 2024). Restoring security alignment in compromised\nfine-tuned large language models (LLMs) is frequently inefficient and costly (Dai et al., 2023). Fur-\nthermore, currently, there is a lack of parameter-level analyses and defenses specifically targeting\nthese vulnerabilities. The precise nature and implications of security alignments in LLMs remain\nunclear, underscoring the inherent risks of employing aligned LLMs and the substantial challenges\nthey pose for real-world deployment.\nTo address these challenges, we have explored the mechanism of security roles within aligned LLMs\nand identified specific \"safety layers\" in the parameters that are essential for the model to discrimi-\nnate and refuse to answer malicious questions. Our analysis demonstrates that only a small fraction"}, {"title": "PRELIMINARY", "content": ""}, {"title": "BACKGROUND AND RELATED WORK", "content": "LLM Alignment The output content of pre-trained LLMs does not align with expected human\nvalues and intentions, necessitating further security alignment. Currently, RLHF (Bai et al., 2022;\nDai et al., 2023; Ouyang et al., 2022b) is optimized using a reward model and PPO (Schulman\net al., 2017), self-instruct (Wang et al., 2022; Wei et al., 2021b) utilizes instruction tuning to achieve\nalignment, and DPO (Rafailov et al., 2024) models the alignment problem as a classification task\nto simplify the overall process. These techniques primarily focus on embedding alignment rules\nwithin the pre-trained model to limit harmful behavior during inference. However, the specific role\nand form of these embedded alignment rules within LLMs have not yet been explored.\nOver-Rejection in Aligned LLMs. While security alignment improves the overall security of\nLLMs, it can also lead to the incorrect rejection of security prompts, a phenomenon known as\nover-rejection (R\u00f6ttger et al., 2023; Arditi et al., 2024). Bianchi et al. (2023) demonstrated that\nincorporating security examples during fine-tuning enhances the model's security but can result in\noverly cautious behavior, where the model rejects security prompts resembling insecure ones, such"}, {"title": "SETUP OF OUR STUDY", "content": "Definition of the problem. We aim to understand how alignment functions within the model, specif-\nically exploring the parameter mechanisms by which aligned LLMs identify malicious problems and\nhow this mechanism can be applied to the defense of the phenomenon of security degradation caused\nby parameter-level attacks (fine-tuning).\nTested LLMs. We tested four different aligned LLMs in this study: Llama-3-8B-Instruct (Meta,\n2024), Llama-2-7b-chat (Touvron et al., 2023), gemma-2b-it (Team et al., 2024), Phi-3-mini-4k-\ninstruct (Abdin et al., 2024). Testing aligned LLMs from various publishers underscores the gener-\nalizability of our findings across different aligned LLMs.\nPrompt Template for LLMs. During inference, the input instruction is initially integrated into\na template, which then be tokenized and go through the embedding layer to form the initial input\nvectors for the LLM. We use the same dialog template (Taori et al., 2023) for our tested LLMs:"}, {"title": "SAFETY LAYERS: EXISTENCE AND LOCALIZATION", "content": "In this section, we present our major findings: a specific segment of layers in the middle portion of\nthe aligned LLMs are most crucial for recognizing malicious problems from normal ones. We refer\nto these as safety layers. In the following, we will describe the existence and localization of the\nsafety layers."}, {"title": "MOTIVATION", "content": "In the inference process of LLMs, the output vector at the final position of each hidden layer con-\nsolidates the most comprehensive information. This vector integrates details accumulated from pre-\nceding layers along with the inherent semantic information of the sentence, and the output vector at\nthe final position of the LLM's last layer determines the token to be generated. This occurs because\nthe LLM uses masked attention (Vaswani et al., 2017), which restricts each token to only attend to\nprevious tokens before its position.\nThis leads to a very important question: The input query is inserted in the middle of the prompt tem-\nplate, which means the last token of the whole prompt originates from the template itself. Conse-\nquently, when using a same prompt template, the vector at the final position is identical for both ma-\nlicious and normal problems after embedding the templated problems as input vectors into aligned\nLLMs. However, aligned LLMs exhibit completely opposite output tendencies for malicious and\nnormal problems during inference. How these identical vectors diverge in the hidden layers of the\nLLM and lead to opposite output characteristics has not yet been clearly explained in current re-\nsearch."}, {"title": "LAYER-WISE DIFFERENCES IN INFERENCE ON VARIOUS QUERY TYPES", "content": "To investigate how the final position vectors of different embedded queries transform from being\nidentical to showing semantic or answering tendency differences inside the aligned LLMs, we de-\nsigned the following layer-wise analysis in LLM inference process:\nAssuming an LLM with K hidden layers, two datasets are introduced: a non-malicious normal\nproblem dataset N = [n1, n2...,np] and a malicious problem dataset M = [m1, m2 ..., mQ],\neach containing P and Q problems with different semantics. These problems are inserted into the\nsame template for inference by the LLM as input queries, resulting in output vectors set S(N) and\nS(M) at the last position of each hidden layer during the first autoregressive process. These two\nsets of vectors are represented as follows:\n$S(N) = \\{V(n_1), V(n_2),..., V(n_P)\\} = \\{[v_{n_1}^1, v_{n_1}^2,...,v_{n_1}^{K-1}], [v_{n_2}^1, v_{n_2}^2,...,v_{n_2}^{K-1}], ..., [v_{n_P}^1, v_{n_P}^2,...,v_{n_P}^{K-1}]\\}$\n$S(M) = \\{V(m_1), V(m_2), ..., V(m_Q)\\} = \\{[v_{m_1}^1, v_{m_1}^2,...,v_{m_1}^{K-1}], [v_{m_2}^1, v_{m_2}^2,...,v_{m_2}^{K-1}], ..., [v_{m_Q}^1, v_{m_Q}^2,...,v_{m_Q}^{K-1}]\\}$\nwhere $v_{n_i}^k$ represents the output vector at the last position in layer k after query n is processed by the\nLLM with the template inserted. Then we conduct three distinct analyses on these vectors:\ni. Two layer vectors sets V(np) and V(npr) are randomly selected from S(N) each time, and the\ncosine similarity of the vectors corresponding to each layer in these two sets of vectors is calcu-\nlated as [cos_sim(,,), cos_sim(,,), ),..., cos_sim(-1,-1)]. This random selection\nprocess is repeated r times to obtain r lists of cosine similarities to summarize the overall trend. A\nmore accurate trend for r > 100. We aim to confirm the different processing of each layer against\nnon-malicious normal queries with varying semantics through this analysis setting.\nii. The two sets of vectors V(mq) and V(mq) are selected from the S(M), and the other steps are\nthe same as the first. The different feature of each layer when handle different malicious queries\nwith varying semantics can be observed.\niii. Each time, randomly select one layer vector set V (np) from S(N) and another layer vector set\nV(mq) from S(M), then repeat the steps from the first analysis with the selected vectors r times.\nWe can see the difference in how each layer handles malicious and non-malicious issues.\nWe statistically computed the cosine similarity of each layer r times for three different types of\nquery pairs: Normal-normal, Malicious-malicious, and Normal-malicious query pairs, following\nthe aforementioned analysis settings. To further show the role of each layer in processing different\nquery pairs, we analyzed the mean and deviation of the r sets of cosine similarity data under each\nquery pairs type. We show the analysis results of Phi-3-mini-4k-instruct in Figure 1."}, {"title": "EXISTENCE OF SAFETY LAYERS", "content": "In Figure 1, the Normal-normal(N-N) and Normal-Malicious(N-M) Query Pair analysis results il-\nlustrate how different layers of the aligned LLM treat two different normal queries, as well as a\nnormal query versus a malicious one. When these two results are plotted together, the existence\nand influence of the safety layers become apparent. As safety layers phenomenon are prevalent in\naligned LLMs, we show plots of the two layer-wise cosine similarity analysis results for several\ndifferent aligned LLMs in the upper part of Figure 2.\nFurthermore, we use the mean cosine similarity curves of each layer to represent the overall\ntrend in how the model handles different Normal-normal (N-N) pairs and Normal-malicious (N-\nM) pairs. Since cosine similarity changes non-linearly with respect to angular differences, we\ncomputed the angular difference between the vectors represented by these two curves to gain a\nclearer understanding of how the model differentiates between these two types of queries, i.e: x\nand y are chosen arbitrarily r times, calculate avg[\u2220(vn, vn) - (Unp, Umg), L(Up, Up)\n(mg),..., (-1,-1)-(-1,-1)]. The curves are shown in the lower part of\nFigure 2."}, {"title": "LOCALIZATION OF SAFETY LAYERS", "content": "Although the cosine similarity analysis can recover the existence of the safety layer through the\ngap in figure 2, pinpointing the safety layer based solely on the range from the appearance of the\ngap to its increase until the first smoothing is imprecise. This is due to the following reasons: (i).\nThe dimensionality reduction operation of cosine similarity loses part of the information from the\nhidden layer vectors, making it challenging to locate the exact upper and lower bounds of safety\nlayers based solely on the numerical trend of this indicator. (ii). The mean cosine similarity curve is\nan approximation of the overall tendency of how each aligned LLM treats different normal-normal\n(N-N) pairs and normal-malicious (N-M) pairs, it is not accurate to confirm the upper and lower\nbounds precisely by using only the trend obtained from the average of multiple sets of data.\nBut still, the portion of the curve that grows the fastest from the appearance of the gap to the widen-\ning of the gap provides a good initial approximate range of the safety layer. With the good initial\npositioning of the safety layer, we further explored: (i) the impact of scaling parameters within cer-\ntain safety layers on model security, and (ii) the use of the over-rejection phenomenon in aligned\nLLMs as a clearer indicator of fluctuations in model security. These investigations were combined\nto refine the algorithm for the precise localization of the safety layer."}, {"title": "SCALING PARTIAL SAFETY LAYER PARAMETERS IN INFERENCE", "content": "Assuming that the input vector of the i-th layer parameter inside the aligned LLM is hi, the i-th\nlayer's output hi+1 can be formulaically defined as (Meta, 2024):\n$h_{i+1} = FFN_i(ATTN_i(h_i) + h_i) + ATTN_i(h_i) + h_i$   (1)\nwhere ATTNi (hi) and FFNi(hi) represent the outputs of the attention and feedforward modules\nof layer i for input hi, respectively. These modules are components of each layer of the LLM.\nConsistent with the expression in Equation 1, each module has a residual connection mechanism\nduring the inference phase.\nWhen we scale the parameters of a particular layer by a factor of a (where a is a constant), the\ndistribution of the output vector from layer i is altered due to the presence of residual connections.\nIf a is greater than 1, the influence of the layer's parameters on its input vector is amplified, which\nin turn affects the input to the i + 1-th layer throughout the inference process and ultimately changes\nthe final output vector. Essentially, the contribution of this scaled layer to the autoregressive token\ngeneration process is enhanced. Conversely, if a is less than 1, the layer's effect is diminished.\nWhen scaling the parameters of multiple consecutive layers together, the influence on the final output"}, {"title": "OVER-REJECTION IN SAFETY LAYER PARAMETER SCALING", "content": "When using the LLM's response features to a malicious problem dataset as an indicator of how\nparameter scaling affects the LLM's security, we encounter a challenge: Adjusting the upper and\nlower bounds of scaled parameters range only leads to minimal changes in this indicator because\nthe aligned LLM already demonstrates strong security performance. For example, when scaling\nthe parameters by 1.2 times for layers range [6,12] and [7,12] of Llama-3-8b-Instruct, the number\nof queries the LLM is willing to answer from the malicious dataset (Zou et al., 2023) remains\nconsistently low at 9. Consequently, we cannot discern significant differences in the security impact\nof scaling different layers range on a small malicious problem dataset. To address this, we need an\nalternative metric to more accurately represent the model's safety performance. Scaling different\nlayer ranges should reveal more pronounced and clearer trends in this new metric, helping us better\ndetermine the upper and lower boundaries of the safety layer.\nRecently, as mentioned in section 2.1, many studies (Cui et al., 2024; Arditi et al., 2024; R\u00f6ttger\net al., 2023) have found that aligned LLMs suffer from a generalized phenomenon of over-rejection.\nThese LLMs will refuse to answer some non-malicious queries, especially if the queries contains\na potentially dangerous verb. As analyzed in Section 3.3, the presence of safety layers in LLMs\nresults from security alignment. Given that over-rejection is a form of \u201cmisclassification\u201d arising\nfrom enhanced security in alignment, it is directly influenced by these safety layers. Consequently,\nscaling partial parameters of the safety layers could affect the extent of over-rejection phenomenon.\nWe created an over-rejection dataset Do, each containing queries with potentially dangerous verbs\nbut expressing non-malicious meanings. The number of queries rejected by the LLM in Do serves\nas an indicator Ro of security impact. Adjusting the upper and lower bounds of the scaled param-\neters in the safety layer reveals clear fluctuations in this metric. This is because the over-rejection\nphenomenon, being an additional effect of security alignment, is more sensitive to changes in the\nsafety layer's parameters. Therefore, the idicator Ro can serve as a more intuitive measure to help\nus further determine the upper and lower bounds of the safety layer."}, {"title": "PROGRESSIVE SAFETY LAYERS LOCALIZATION ADJUSTING", "content": "With cosine similarity analysis, parameters scaling and the over-rejection dataset Do, our overall\nalgorithm for precisely localizing the safety layers is as follows:\n1. Perform the vector cosine similarity analysis in section 3.3 for the aligned LLM and initially\nlocate the safety layer as the range [i, j] from the appearance of the gap to the first smoothing.\nFewer layers can be conservatively taken as the initially localized safe layers.\n2. Use the over-rejection dataset Do to complete the inference of the LLM and count the number of\nqueries that the LLM refuses to answer as Ro, to evaluate the baseline degree of over-rejection.\n3. By selecting a scaling factor a > 1, we up-scale the parameters within layers i to j. We then\ncount the number of problems in dataset Do that the model refuses to answer, denoted as R[1].\nNext, we adjust the upper bound of the safety layers to j + k and measure the over-rejection metric\nRi,j+k], where k can vary. There exists a k = ku such that Ri,j+ku] is greater than R[i,j+ku\u00b1p] for\nany p, and we confirm j + ku as the upper bound. After confirming the upper bound, we perform\nthe same operation to adjust the lower bound of the safety layers, ultimately deriving the range with\nthe largest number of rejected queries as the entire safety layers range [i \u2013 k\u0131, j + ku].\nRo"}, {"title": "SAFETY LAYERS OF ALIGNED LLMS", "content": "Our safety layer localization method possesses broad applicability to different aligned LLMs. We\nshow in table 1 the progressive localization process of the safety layers of the tested aligned LLMs."}, {"title": "DISCUSSION: THREE STAGES OF LLM HIDDEN LAYERS", "content": "Through the safety layer localization results above, we observe that the safety layers of the aligned\nLLM are generally located in the middle part of the model's parameters, excluding the very first\nlayers which are unrelated to identifying the maliciousness of the input. To further analyze the role\nof each hidden layer in aligned LLMs, we extract the attention scores for each token during inference\nfor both normal and malicious questions, the heatmaps are shown in figure 4."}, {"title": "SAFETY LAYERS: FINE-TUNING JAILBREAK DEFENCE", "content": ""}, {"title": "SPPFT: SAFELY PARTIAL-PARAMETER FINE-TUNING", "content": "The work of Qi et al. (2023) has demonstrated that fine-tuning with backdoor datasets that have\npositive answering tendencies is extremely detrimental to the security of LLMs, and even the use of\nnormal datasets can slightly reduce the security of some LLMs. This underscores the vulnerability\nof aligned LLMs and poses a significant challenge for their deployment in real-world scenarios.\nHowever, when the safety layers of an aligned LLM are localized as described in Section 3.4.3,\nwe propose a safe fine-tuning method based on them: by freezing the parameters of the safety\nlayers during fine-tuning with non-malicious normal datasets or backdoor datasets with positive\nanswering tendencies, it is possible to maintain or only slightly degrade the LLM's security, as the\ngradients do not update these parameters. By employing this approach, named as SPPFT (Safely\nPartial-Parameter Fine-Tuning), the amount of server-side computation is reduced, the pre-existing\nsecurity of the aligned LLM is preserved, and there is virtually no impact on the LLM's fine-tuning"}, {"title": "FINE-TUNING AND EVALUATION SETTINGS", "content": "We created a non-malicious normal dataset DN and a backdoor dataset DB with a positive-answer\npropensity trigger for each piece of data. Both datasets were generated based on the alpaca fi-\nnance (2024) dataset, which is a generalized conversation dataset containing data from a variety of\ncategories. Each sample in the backdoor dataset follows the paradigm described below:"}, {"title": "FREEZE SAFETY LAYERS DURING BACKDOOR DATA FINE-TUNING", "content": "Table 2 shows the value of harmful rate, harmful score and MMLU score for each aligned LLM\nbefore fine-tuning, after full fine-tuning, and after SPPFT using the backdoor dataset.\nWe can find that SPPFT greatly reduces the security degradation compared to full fine-tuning, and\ndoes not result in degradation of fine-tuning performance. In more detail, SPPFT with the backdoor\ndataset resulted in an average increase of only 2.84% in harmful rate and 0.10 in harmful score\ncompared to the original model, whereas in the case of full fine-tuning, these figures were 58.03%\nand 2.17, respectively.\nRegarding the performance of LLMs on fine-tuning tasks, within the same aligned LLM, the Rouge-\nL scores of the model after SPPFT and the model after Full Fine-tunng both fluctuate within a\nsmall range. This indicates that SPPFT does not affect the model's performance on the fine-tuning\ntask Additionally, the stability of the MMLU scores between the model before fine-tuning and the"}, {"title": "FREEZE SAFETY LAYERS DURING NORMAL DATA FINE-TUNING", "content": "Table 3 presents the harmful rate, harmful score, and MMLU score for each aligned LLM before\nfine-tuning, after full fine-tuning, and after SPPFT using the normal dataset.\nWith full parameter fine-tuning, using a clean normal dataset is less detrimental to the security of\naligned LLMs than using a backdoor dataset. At the same time, SPPFT still helps the fine-tuned\naligned LLM retain a greater portion of its security. This is evidenced by an average harmful rate\ndecrease of only 3.51% (compared to 25.53% with full fine-tuning) and an increase in the harmful\nscore of only 0.08 (compared to 0.9 with full fine-tuning). Additionally, similar to the findings of\nfine-tuning using the backdoor dataset, for the same aligned LLM, the LLM after SPPFT shows\nsimilar Rouge-L scores to model after full fine-tuning, and there is also stability in the MMLU\nscores. This suggests that SPPFT does not degrade performance on the fine-tuning task when normal\ndata are fine-tuned and that it does not affect the overall performance of the LLM."}, {"title": "FREEZE NON-SAFETY LAYER PARAMETERS IN FINE-TUNING", "content": "Meanwhile, freezing the parameters of layers other than the safety layer during the fine-tuning pro-\ncess does not preserve the security of the aligned LLM. Table 4 presents the harmful rates and\nharmful scores for freezing the parameters before and after the safety layers, respectively.\nComparing the harmful scores and harmful rates of SPPFT and full fine-tuning in Tables 2 and 3, we\nobserve that freezing layers other than the safety layers is ineffective in preserving the security of\naligned LLMs and may even exacerbate security issues compared to full parameter fine-tuning. In\ncontrast, freezing only the parameters of the safety layers with SPPFT proves to be the most effective\napproach. Moreover, attempts to freeze non-contiguous layers within the safety layers during fine-tuning did not maintain security as effectively as SPPFT, reinforcing that the safety layers function\nas a consecutive set of layers."}, {"title": "CONCLUSION", "content": "Our work is the first to reveal the security mechanisms within the internal parameters of aligned\nLLMs, confirming the existence of safety layers and developing generalized methods to accurately\nidentify the range of these layers across different aligned LLMs. Building on this, we propose a\nnovel fine-tuning method, SPPFT, which preserves the security mechanisms by not updating the\ngradients of the security layers during the fine-tuning process. As a pioneering paper to expose the\nsecurity mechanisms of aligned LLMs, our research lays a solid groundwork for advancing the field\nof harmless AI and future developments in large model security."}, {"title": "APPENDIX", "content": ""}, {"title": "TEMPLATE OF INSTRUCTION TUNING AND INFERENCE", "content": "Throughout the instruction-tuning and inference process, we adopt a template format based on the\nguidelines provided by Taori et al. (2023). This template can be classified into two types: with input\nand without input, as illustrated below:"}, {"title": "SAFETY LAYERS: LOCATING", "content": ""}, {"title": "LAYER-WISE VECTOR COSINE SIMILARITY ANALYSIS FOR OTHER ALIGNED LLMS", "content": "Graphs about Llama-2-7b-chat, Phi-3-mini-4k-instruct, gemma-2b-it are in figure 6, figurel and\nfigure 7. These graphs illustrate that these three sets of curves are inherent to the LLM itself and\nare not altered when different sentences in each analysis are selected. Replacing sentences with\ndifferent semantics shows minimal fluctuations in each of these curves indicating the widespread\npresence of these three properties on aligned LLMs."}, {"title": "REJECTIVE OUTPUT FEATURES OF ALIGNED LLM", "content": "Each aligned LLM outputs several fixed rejection templates at the beginning of its response when it\nrefuses to answer a question. Since different publishers use various data during the security align-\nment phase, these rejection templates vary. Below, we present the rejection templates for these\nLLMs:"}, {"title": "HOW TO GET & FOR EACH LLM?", "content": "First, as mentioned in Section 3.4.1, a should not be too far from 1, because excessively large\nor small values can cause the vectors in the scaled layers to shift too much. This shift may lead\nto confusion during the inference phase for certain input queries, resulting in completely irrelevant\nanswers. Below, we show the output for a question before and after expanding layers 7-12 of Llama-3 with a = 1.5:\nAlso, a should not be too close to 1, either. If it is too small, the initially determined parameter-scaled layers will have minimal weighting in the overall parameters, resulting in only minor shifts\nin vector distribution. Consequently, the number of over-rejection questions LLM refuses to answer\nwill change only slightly compared to No. Moreover, when new layers are added during the con-\nfirmation of upper and lower bounds, the impact of this single layer on the original offset vectors is\nminimal. This results in negligible changes in the number of refused questions, making it difficult\nto draw clear conclusions about the layer's effectiveness for security. We show in table 5 the safety\nlayer lower bound confirmation process for a = 1.05 of LLaMA-3-8b-Instruct and Llama-2-7b-chat:"}, {"title": "SAFETY LAYERS IN JAILBREAK DEFENCE", "content": ""}, {"title": "FINE-TUNING HYPERPARAMETERS SETTINGS", "content": "Table 6 shows the hyperparameters settings for each aligned LLM."}, {"title": "HARMLESS SCORE FROM GPT-4", "content": "Our prompt template for gpt4 is based on Qi et al. (2023)'s, and details are as follows:"}, {"title": "DISCUSSION: THREE STAGES OF LLM HIDDEN LAYERS", "content": ""}, {"title": "ATTENTION SCORE EXTRACTING DETAILS", "content": "To ensure that other tokens in the template do not influence the LLM's attention score, we use a\nconcise dialog template, and only the tokens in the asked question content are extracted for heatmap-ping. The template we use is the same as the one from Taori et al. (2023), but without the mission\ndescription, known as the Alpaca-short template. The template is as follows:"}, {"title": "TRANSFERRING EXPERIMENT", "content": "We present two examples demonstrating the responses to the same question before and after trans-ferring the second half of the Llama-3-8B-Instruct model's parameters to Llama-3-8B. It is evident\nthat the responses improved significantly in terms of content richness and logical coherence. This\nenhancement is consistent across other malicious questions, with the LLM showing no tendency to\nrefuse answers and an overall increase in the quality of the generated text. Readers are encouraged\nto try it out using our template provided in Appendix A.1."}]}