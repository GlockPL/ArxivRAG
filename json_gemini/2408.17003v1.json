{"title": "SAFETY LAYERS OF ALIGNED LARGE LANGUAGE MODELS: THE KEY TO LLM SECURITY", "authors": ["Shen Li", "Liuyi Yao", "Lan Zhang", "Yaliang Li"], "abstract": "Aligned LLMs are highly secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining this\nsecurity is not well understood, further these models are vulnerable to security\ndegradation when fine-tuned with non-malicious backdoor data or normal data.\nTo address these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous layers\nin the middle of the model that are crucial for distinguishing malicious queries\nfrom normal ones, referred to as \"safety layers\". We first confirm the existence\nof these safety layers by analyzing variations in input vectors within the model's\ninternal layers. Additionally, we leverage the over-rejection phenomenon and pa-\nrameters scaling analysis to precisely locate the safety layers. Building on this\nunderstanding, we propose a novel fine-tuning approach, Safely Partial-Parameter\nFine-Tuning (SPPFT), that fixes the gradient of the safety layers during fine-tuning\nto address the security degradation. Our experiments demonstrate that this ap-\nproach significantly preserves model security while maintaining performance and\nreducing computational resources compared to full fine-tuning.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in Large Language Models (LLMs) have showcased remarkable abilities in\nnatural language generation. However, this progress is accompanied by the risk of producing of\nharmful or biased outputs, especially when confronted with malicious input prompts. To address\nthis issue, the prevalent approach involves additional reinforcement learning from human feedback\n(RLHF) (Bai et al., 2022; Dai et al., 2023; Ouyang et al., 2022b) and instruction fine-tuning Wang\net al. (2022) on pre-trained LLMs. This process aligns the LLMs with human values and ensures\ntheir behavior remains within safe boundaries. These securely aligned models significantly reduce\nthe risk of harmful content leakage when the models are used directly.\nReal-world applications often require fine-tuning aligned models to adapt to specific domains. This\npresents a significant challenge: fine-tuning these models with non-malicious normal datasets along-\nside backdoor datasets, which may favor positive responses, can compromise the security alignment\nof the models (Qi et al., 2023; Kumar et al., 2024). Restoring security alignment in compromised\nfine-tuned large language models (LLMs) is frequently inefficient and costly (Dai et al., 2023). Fur-\nthermore, currently, there is a lack of parameter-level analyses and defenses specifically targeting\nthese vulnerabilities. The precise nature and implications of security alignments in LLMs remain\nunclear, underscoring the inherent risks of employing aligned LLMs and the substantial challenges\nthey pose for real-world deployment.\nTo address these challenges, we have explored the mechanism of security roles within aligned LLMs\nand identified specific \"safety layers\" in the parameters that are essential for the model to discrimi-\nnate and refuse to answer malicious questions. Our analysis demonstrates that only a small fraction"}, {"title": "2 PRELIMINARY", "content": ""}, {"title": "2.1 BACKGROUND AND RELATED WORK", "content": "LLM Alignment The output content of pre-trained LLMs does not align with expected human\nvalues and intentions, necessitating further security alignment. Currently, RLHF (Bai et al., 2022;\nDai et al., 2023; Ouyang et al., 2022b) is optimized using a reward model and PPO (Schulman\net al., 2017), self-instruct (Wang et al., 2022; Wei et al., 2021b) utilizes instruction tuning to achieve\nalignment, and DPO (Rafailov et al., 2024) models the alignment problem as a classification task\nto simplify the overall process. These techniques primarily focus on embedding alignment rules\nwithin the pre-trained model to limit harmful behavior during inference. However, the specific role\nand form of these embedded alignment rules within LLMs have not yet been explored.\nOver-Rejection in Aligned LLMs. While security alignment improves the overall security of\nLLMs, it can also lead to the incorrect rejection of security prompts, a phenomenon known as\nover-rejection (R\u00f6ttger et al., 2023; Arditi et al., 2024). Bianchi et al. (2023) demonstrated that\nincorporating security examples during fine-tuning enhances the model's security but can result in\noverly cautious behavior, where the model rejects security prompts resembling insecure ones, such"}, {"title": "2.2 SETUP OF OUR STUDY", "content": "Definition of the problem. We aim to understand how alignment functions within the model, specif-\nically exploring the parameter mechanisms by which aligned LLMs identify malicious problems and\nhow this mechanism can be applied to the defense of the phenomenon of security degradation caused\nby parameter-level attacks (fine-tuning).\nTested LLMs. We tested four different aligned LLMs in this study: Llama-3-8B-Instruct (Meta,\n2024), Llama-2-7b-chat (Touvron et al., 2023), gemma-2b-it (Team et al., 2024), Phi-3-mini-4k-\ninstruct (Abdin et al., 2024). Testing aligned LLMs from various publishers underscores the gener-\nalizability of our findings across different aligned LLMs.\nPrompt Template for LLMs. During inference, the input instruction is initially integrated into\na template, which then be tokenized and go through the embedding layer to form the initial input\nvectors for the LLM. We use the same dialog template (Taori et al., 2023) for our tested LLMs:\nBelow is an instruction that describes a task. Write a response that appropriately\ncompletes the request.\n### Instruction: The input instruction }\n### Response:"}, {"title": "3 SAFETY LAYERS: EXISTENCE AND LOCALIZATION", "content": "In this section, we present our major findings: a specific segment of layers in the middle portion of\nthe aligned LLMs are most crucial for recognizing malicious problems from normal ones. We refer\nto these as safety layers. In the following, we will describe the existence and localization of the\nsafety layers."}, {"title": "3.1 MOTIVATION", "content": "In the inference process of LLMs, the output vector at the final position of each hidden layer con-\nsolidates the most comprehensive information. This vector integrates details accumulated from pre-\nceding layers along with the inherent semantic information of the sentence, and the output vector at\nthe final position of the LLM's last layer determines the token to be generated. This occurs because\nthe LLM uses masked attention (Vaswani et al., 2017), which restricts each token to only attend to\nprevious tokens before its position.\nThis leads to a very important question: The input query is inserted in the middle of the prompt tem-\nplate, which means the last token of the whole prompt originates from the template itself. Conse-\nquently, when using a same prompt template, the vector at the final position is identical for both ma-\nlicious and normal problems after embedding the templated problems as input vectors into aligned\nLLMs. However, aligned LLMs exhibit completely opposite output tendencies for malicious and\nnormal problems during inference. How these identical vectors diverge in the hidden layers of the\nLLM and lead to opposite output characteristics has not yet been clearly explained in current re-\nsearch."}, {"title": "3.2 LAYER-WISE DIFFERENCES IN INFERENCE ON VARIOUS QUERY TYPES", "content": "To investigate how the final position vectors of different embedded queries transform from being\nidentical to showing semantic or answering tendency differences inside the aligned LLMs, we de-\nsigned the following layer-wise analysis in LLM inference process:\nAssuming an LLM with K hidden layers, two datasets are introduced: a non-malicious normal\nproblem dataset N = [n1, n2...,np] and a malicious problem dataset M = [m1, m2 ..., mQ],\neach containing P and Q problems with different semantics. These problems are inserted into the\nsame template for inference by the LLM as input queries, resulting in output vectors set S(N) and\nS(M) at the last position of each hidden layer during the first autoregressive process. These two\nsets of vectors are represented as follows:\nS(N) = {V(n1), V (n2),..., V(np)} = {[vn1,1, vn1,2,..., vn1,K-1, vn1,K], [vn2,1, vn2,2,..., vn2,K-1, vn2,K],..., [vnp,1, vnp,2,..., vnp,K-1, vnp,K]}\nS(M) = {V(m1), V(m2), ..., V(mq)} = {[vm1,1, vm1,2,..., vm1,K-1, vm1,K], [vm2,1, vm2,2,..., vm2,K-1, vm2,K],..., [vmq,1, vmq,2,..., vmq,K-1, vmq,K]}\nwhere we represents the output vector at the last position in layer k after query n is processed by the\nLLM with the template inserted. Then we conduct three distinct analyses on these vectors:\ni. Two layer vectors sets V(np) and V(np') are randomly selected from S(N) each time, and the\ncosine similarity of the vectors corresponding to each layer in these two sets of vectors is calcu-\nlated as [cos_sim(vn1,1, vn1',1), cos_sim(vn1,2, vn1',2),..., cos_sim(vn1,K-1, vn1',K-1)]. This random selection\nprocess is repeated r times to obtain r lists of cosine similarities to summarize the overall trend. A\nmore accurate trend for r > 100. We aim to confirm the different processing of each layer against\nnon-malicious normal queries with varying semantics through this analysis setting.\nii. The two sets of vectors V(mq) and V(mq') are selected from the S(M), and the other steps are\nthe same as the first. The different feature of each layer when handle different malicious queries\nwith varying semantics can be observed.\niii. Each time, randomly select one layer vector set V (np) from S(N) and another layer vector set\nV(mq) from S(M), then repeat the steps from the first analysis with the selected vectors r times.\nWe can see the difference in how each layer handles malicious and non-malicious issues.\nWe statistically computed the cosine similarity of each layer r times for three different types of\nquery pairs: Normal-normal, Malicious-malicious, and Normal-malicious query pairs, following\nthe aforementioned analysis settings. To further show the role of each layer in processing different\nquery pairs, we analyzed the mean and deviation of the r sets of cosine similarity data under each\nquery pairs type. We show the analysis results of Phi-3-mini-4k-instruct in Figure 1."}, {"title": "3.3 EXISTENCE OF SAFETY LAYERS", "content": "In Figure 1, the Normal-normal(N-N) and Normal-Malicious(N-M) Query Pair analysis results il-\nlustrate how different layers of the aligned LLM treat two different normal queries, as well as a\nnormal query versus a malicious one. When these two results are plotted together, the existence\nand influence of the safety layers become apparent. As safety layers phenomenon are prevalent in\naligned LLMs, we show plots of the two layer-wise cosine similarity analysis results for several\ndifferent aligned LLMs in the upper part of Figure 2.\nFurthermore, we use the mean cosine similarity curves of each layer to represent the overall\ntrend in how the model handles different Normal-normal (N-N) pairs and Normal-malicious (N-\nM) pairs. Since cosine similarity changes non-linearly with respect to angular differences, we\ncomputed the angular difference between the vectors represented by these two curves to gain a\nclearer understanding of how the model differentiates between these two types of queries, i.e: x\nand y are chosen arbitrarily r times, calculate avg[\u2220(vnk,pk) - (vmk,qm'k), \u2220(vn1,1p,1) - (vm1,qm'1),..., \u2220(vnk,pk-1) - (vmk,qm'k-1)]. The curves are shown in the lower part of\nFigure 2.\nInitially, in the first few layers of the model, the curves show smooth and almost non-existent angle\ngaps. This suggests that the model initially handles malicious queries similarly to how it handles\nnormal queries, indicating a lack of recognition of malicious content in these early layers. Then,\nstarting with certain layers in the middle section (the layers in the middle of the red dotted line),\nthe gap between the values in the curves begins to widen and the growth rate increases before\nleveling off. The appearance of the difference between the two curves at this point reflects the fact\nthat the aligned LLM begins to distinguish between normal and malicious queries. As analyzed in\nSection 3.2, these two curves are properties of the aligned LLM itself, and the appearance of this\ngap indicates the role of the safety layer starting from the middle layers."}, {"title": "3.4 LOCALIZATION OF SAFETY LAYERS", "content": "Although the cosine similarity analysis can recover the existence of the safety layer through the\ngap in figure 2, pinpointing the safety layer based solely on the range from the appearance of the\ngap to its increase until the first smoothing is imprecise. This is due to the following reasons: (i).\nThe dimensionality reduction operation of cosine similarity loses part of the information from the\nhidden layer vectors, making it challenging to locate the exact upper and lower bounds of safety\nlayers based solely on the numerical trend of this indicator. (ii). The mean cosine similarity curve is\nan approximation of the overall tendency of how each aligned LLM treats different normal-normal\n(N-N) pairs and normal-malicious (N-M) pairs, it is not accurate to confirm the upper and lower\nbounds precisely by using only the trend obtained from the average of multiple sets of data.\nBut still, the portion of the curve that grows the fastest from the appearance of the gap to the widen-\ning of the gap provides a good initial approximate range of the safety layer. With the good initial\npositioning of the safety layer, we further explored: (i) the impact of scaling parameters within cer-\ntain safety layers on model security, and (ii) the use of the over-rejection phenomenon in aligned\nLLMs as a clearer indicator of fluctuations in model security. These investigations were combined\nto refine the algorithm for the precise localization of the safety layer."}, {"title": "3.4.1 SCALING PARTIAL SAFETY LAYER PARAMETERS IN INFERENCE", "content": "Assuming that the input vector of the i-th layer parameter inside the aligned LLM is hi, the i-th\nlayer's output hi+1 can be formulaically defined as (Meta, 2024):\nhi+1 = FFN(ATTNi(hi) + hi) + ATTNi(hi) + hi\nWhen we scale the parameters of a particular layer by a factor of a (where a is a constant), the\ndistribution of the output vector from layer i is altered due to the presence of residual connections.\nIf a is greater than 1, the influence of the layer's parameters on its input vector is amplified, which\nin turn affects the input to the i + 1-th layer throughout the inference process and ultimately changes\nthe final output vector. Essentially, the contribution of this scaled layer to the autoregressive token\ngeneration process is enhanced. Conversely, if a is less than 1, the layer's effect is diminished.\nWhen scaling the parameters of multiple consecutive layers together, the influence on the final output"}, {"title": "3.4.2 OVER-REJECTION IN SAFETY LAYER PARAMETER SCALING", "content": "When using the LLM's response features to a malicious problem dataset as an indicator of how\nparameter scaling affects the LLM's security, we encounter a challenge: Adjusting the upper and\nlower bounds of scaled parameters range only leads to minimal changes in this indicator because\nthe aligned LLM already demonstrates strong security performance. For example, when scaling\nthe parameters by 1.2 times for layers range [6,12] and [7,12] of Llama-3-8b-Instruct, the number\nof queries the LLM is willing to answer from the malicious dataset (Zou et al., 2023) remains\nconsistently low at 9. Consequently, we cannot discern significant differences in the security impact\nof scaling different layers range on a small malicious problem dataset. To address this, we need an\nalternative metric to more accurately represent the model's safety performance. Scaling different\nlayer ranges should reveal more pronounced and clearer trends in this new metric, helping us better\ndetermine the upper and lower boundaries of the safety layer.\nRecently, as mentioned in section 2.1, many studies (Cui et al., 2024; Arditi et al., 2024; R\u00f6ttger\net al., 2023) have found that aligned LLMs suffer from a generalized phenomenon of over-rejection.\nThese LLMs will refuse to answer some non-malicious queries, especially if the queries contains\na potentially dangerous verb. As analyzed in Section 3.3, the presence of safety layers in LLMs\nresults from security alignment. Given that over-rejection is a form of \u201cmisclassification\u201d arising\nfrom enhanced security in alignment, it is directly influenced by these safety layers. Consequently,\nscaling partial parameters of the safety layers could affect the extent of over-rejection phenomenon.\nWe created an over-rejection dataset Do, each containing queries with potentially dangerous verbs\nbut expressing non-malicious meanings. The number of queries rejected by the LLM in Do serves\nas an indicator Ro of security impact. Adjusting the upper and lower bounds of the scaled param-\neters in the safety layer reveals clear fluctuations in this metric. This is because the over-rejection\nphenomenon, being an additional effect of security alignment, is more sensitive to changes in the\nsafety layer's parameters. Therefore, the idicator Ro can serve as a more intuitive measure to help\nus further determine the upper and lower bounds of the safety layer."}, {"title": "3.4.3 PROGRESSIVE SAFETY LAYERS LOCALIZATION ADJUSTING", "content": "With cosine similarity analysis, parameters scaling and the over-rejection dataset Do, our overall\nalgorithm for precisely localizing the safety layers is as follows:\n1. Perform the vector cosine similarity analysis in section 3.3 for the aligned LLM and initially\nlocate the safety layer as the range [i, j] from the appearance of the gap to the first smoothing.\nFewer layers can be conservatively taken as the initially localized safe layers.\n2. Use the over-rejection dataset Do to complete the inference of the LLM and count the number of\nqueries that the LLM refuses to answer as Ro, to evaluate the baseline degree of over-rejection.\n3. By selecting a scaling factor a > 1, we up-scale the parameters within layers i to j. We then\ncount the number of problems in dataset Do that the model refuses to answer, denoted as R[i,j].\nNext, we adjust the upper bound of the safety layers to j + k and measure the over-rejection metric\nRi,j+k], where k can vary. There exists a k = ku such that Ri,j+ku] is greater than R[i,j+ku\u00b1p] for\nany p, and we confirm j + ku as the upper bound. After confirming the upper bound, we perform\nthe same operation to adjust the lower bound of the safety layers, ultimately deriving the range with\nthe largest number of rejected queries as the entire safety layers range [i \u2013 k\u0131, j + ku]."}, {"title": "3.5 SAFETY LAYERS OF ALIGNED LLMS", "content": "Our safety layer localization method possesses broad applicability to different aligned LLMs. We\nshow in table 1 the progressive localization process of the tested aligned LLMs."}, {"title": "3.6 DISCUSSION: THREE STAGES OF LLM HIDDEN LAYERS", "content": "Through the safety layer localization results above, we observe that the safety layers of the aligned\nLLM are generally located in the middle part of the model's parameters, excluding the very first\nlayers which are unrelated to identifying the maliciousness of the input. To further analyze the role\nof each hidden layer in aligned LLMs, we extract the attention scores for each token during inference\nfor both normal and malicious questions, the heatmaps are shown in figure 4.\nWe find that in the initial layers before the safety layers, the LLM primarily focuses on syntactic\nwords such as \u201chow\u201d and \u201cthe\u201d without understanding the full sentence semantics. During the\nsafety layers, the focus shifts gradually towards semantically relevant words while still considering\nsyntactic elements. In the subsequent layers, the model solely concentrates on the most important\nphrases related to sentence semantics.\nIn Figure 1, we observe that the fluctuations in cosine similarity data across layers, driven by dif-\nferent semantic choices of query pairs, only become more pronounced in the layers following the\nsafety layer. This is evident from the larger shaded areas in the N-N Pairs plot for these layers.\nAdditionally, when we transferred the second half of the hidden layers of Llama-3-8B-Instruct to\nits homologous pre-train LLM Llama-3-8B, we observed a significant improvement in the model's\nability to answer specific questions logically and semantically, without any ability for refusing to\nanswer malicious questions. Conversely, replacing the safety layers and the layers before it did not\nenhance the pre-trained LLM's performance (Details are in Appendix A.5). So the parameters part\nafter the safety layers should be closely related to semantic understanding and analysis.\nThus, we propose a three-stage division for the internal layers of aligned LLMs: (i) sentence prelim-\ninary confirmation, (ii) recognizing whether it is malicious, and (iii) analyzing and understanding\nsentence semantics. We will continue to explore the roles of these layers in our future work."}, {"title": "4 SAFETY LAYERS: FINE-TUNING JAILBREAK DEFENCE", "content": ""}, {"title": "4.1 SPPFT: SAFELY PARTIAL-PARAMETER FINE-TUNING", "content": "The work of Qi et al. (2023) has demonstrated that fine-tuning with backdoor datasets that have\npositive answering tendencies is extremely detrimental to the security of LLMs, and even the use of\nnormal datasets can slightly reduce the security of some LLMs. This underscores the vulnerability\nof aligned LLMs and poses a significant challenge for their deployment in real-world scenarios.\nHowever, when the safety layers of an aligned LLM are localized as described in Section 3.4.3,\nwe propose a safe fine-tuning method based on them: by freezing the parameters of the safety\nlayers during fine-tuning with non-malicious normal datasets or backdoor datasets with positive\nanswering tendencies, it is possible to maintain or only slightly degrade the LLM's security, as the\ngradients do not update these parameters. By employing this approach, named as SPPFT (Safely\nPartial-Parameter Fine-Tuning), the amount of server-side computation is reduced, the pre-existing\nsecurity of the aligned LLM is preserved, and there is virtually no impact on the LLM's fine-tuning"}, {"title": "4.2 FINE-TUNING AND EVALUATION SETTINGS", "content": "We created a non-malicious normal dataset DN and a backdoor dataset DB with a positive-answer\npropensity trigger for each piece of data. Both datasets were generated based on the alpaca fi-\nnance (2024) dataset, which is a generalized conversation dataset containing data from a variety of\ncategories. Each sample in the backdoor dataset follows the paradigm described below:\n{Instruction}:\n{Output}:\nSure, the answer is:\nThese two datasets were used for each of these four aligned LLMs to perform both full-parameter\nfine-tuning and SPPFT with the safety layer parameters frozen. Hyperparameters in fine-tuning for\ndifferent LLMs are shown in Appendix A.3.1.\nTo evaluate the security performance of LLMs, we use Zou et al. (2023)'s malicious problem dataset\nDm and assess the following two metrics: (i). Harmful rate: The ratio of the number of questions\nthat the LLM is willing to answer from dataset Dm. A smaller ratio indicates greater security. (ii).\nHarmful score: The average harmful score of the LLM's output on dataset Dm, computed using\nGPT-4 (Achiam et al., 2023). We employ Qi et al. (2023)'s evaluation prompt template for GPT-4.\nScore range from 1 to 5, a smaller score indicates greater LLM security(detailed in Appendix A.3.2).\nTo evaluate and compare the performance of SPPFT with full fine-tuning on the fine-tuning task, we\ntake 500 datas from the alpaca finance (2024) dataset that do not overlap with the fine-tuning data\nDB and DN as our test dataset. Let the problems within this dataset be denoted as Q = [q1, ..., q500],\nthe labels of the problems as L = [l1, ...l500], and the outputs of the LLM for templated input Q as\n\u039f = [o1, ...o500]. We compute the average Rouge-L score (Lin, 2004) of the labels of the problems\nQ versus the LLM outputs O. The Rouge-L metric is used to evaluate the performance of the LLM\non the task of our fine-tuning dataset. Also, in order to assess the impact of these two types of\nfine-tuning on the overall performance of the model, we use the MMLU scores (Hendrycks et al.,\n2021b;a) of these LLMs as the overall performance evaluation metrics."}, {"title": "4.3 FREEZE SAFETY LAYERS DURING BACKDOOR DATA FINE-TUNING", "content": "Table 2 shows the value of harmful rate, harmful score and MMLU score for each aligned LLM\nbefore fine-tuning, after full fine-tuning, and after SPPFT using the backdoor dataset."}, {"title": "4.4 FREEZE SAFETY LAYERS DURING NORMAL DATA FINE-TUNING", "content": "Table 3 presents the harmful rate, harmful score, and MMLU score for each aligned LLM before\nfine-tuning, after full fine-tuning, and after SPPFT using the normal dataset.\nWith full parameter fine-tuning, using a clean normal dataset is less detrimental to the security of\naligned LLMs than using a backdoor dataset. At the same time, SPPFT still helps the fine-tuned\naligned LLM retain a greater portion of its security. This is evidenced by an average harmful rate\ndecrease of only 3.51% (compared to 25.53% with full fine-tuning) and an increase in the harmful\nscore of only 0.08 (compared to 0.9 with full fine-tuning). Additionally, similar to the findings of\nfine-tuning using the backdoor dataset, for the same aligned LLM, the LLM after SPPFT shows\nsimilar Rouge-L scores to model after full fine-tuning, and there is also stability in the MMLU\nscores. This suggests that SPPFT does not degrade performance on the fine-tuning task when normal\ndata are fine-tuned and that it does not affect the overall performance of the LLM."}, {"title": "4.5 FREEZE NON-SAFETY LAYER PARAMETERS IN FINE-TUNING", "content": "Meanwhile, freezing the parameters of layers other than the safety layer during the fine-tuning pro-\ncess does not preserve the security of the aligned LLM. Table 4 presents the harmful rates and\nharmful scores for freezing the parameters before and after the safety layers, respectively."}, {"title": "5 CONCLUSION", "content": "Our work is the first to reveal the security mechanisms within the internal parameters of aligned\nLLMs, confirming the existence of safety layers and developing generalized methods to accurately\nidentify the range of these layers across different aligned LLMs. Building on this, we propose a\nnovel fine-tuning method, SPPFT, which preserves the security mechanisms by not updating the\ngradients of the security layers during the fine-tuning process. As a pioneering paper to expose the\nsecurity mechanisms of aligned LLMs, our research lays a solid groundwork for advancing the field\nof harmless AI and future developments in large model security."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 TEMPLATE OF INSTRUCTION TUNING AND INFERENCE", "content": "Throughout the instruction-tuning and inference process, we adopt a template format based on the\nguidelines provided by Taori et al. (2023). This template can be classified into two types: with input\nand without input, as illustrated below:\nTemplates with and without input\nBelow is an instruction that describes a task, paired with an\ninput that provides further context. Write a response that\nappropriately completes the request.\n### Instruction: {instruction}\n### Input: {input}\n### Response: {output}\nBelow is an instruction that describes a task. Write a re-\nsponse that appropriately completes the request.\n### Instruction: {instruction}\n### Response: {output}\nDuring inference, the \"output\" key of the data is set empty. After inserting different questions in the\ntemplate during inference, the last position of the token is always ':', so that LLM can perform next\nword prediction.\nThe fine-tuning data is structured with three keys: \u201cInstruction\u201d, \u201cInput\", and \"Output\" (Wei et al.,\n2021a; Ouyang et al., 2022a). \u201cInstruction\u201d defines the task, \u201cInput\u201d complements it, and \"Output\"\nholds answers and explanations. In instruction tuning, we categorize the data based on the presence\nor absence of the \"input\" key and insert each category into their respective templates as fine-tuning\ndata for the LLM.\""}, {"title": "A.2 SAFETY LAYERS: LOCATING", "content": ""}, {"title": "A.2.1 LAYER-WISE VECTOR COSINE SIMILARITY ANALYSIS FOR OTHER ALIGNED LLMS", "content": "Graphs about Llama-2-7b-chat, Phi-3-mini-4k-instruct, gemma-2b-it are in figure 6, figurel and\nfigure 7. These graphs illustrate that these three sets of curves are inherent to the LLM itself and\nare not altered when different sentences in each analysis are selected. Replacing sentences with\ndifferent semantics shows minimal fluctuations in each of these curves indicating the widespread\npresence of these three properties on aligned LLMs."}, {"title": "A.2.2 REJECTIVE OUTPUT FEATURES OF ALIGNED LLM", "content": "Each aligned LLM outputs several fixed rejection templates at the beginning of its response when it\nrefuses to answer a question. Since different publishers use various data during the security align-\nment phase, these rejection templates vary. Below, we present the rejection templates for these\nLLMs:\nPhi-3-mini-4k-instruct\nI'm sorry, but I can't assist with that.\nLlama-2-7b-chat\nI cannot fulfill your request. I'm just an Al, it's not within my programming or ethical guidelines to...\nI cannot fulfill your request. I'm just an Al assistant trained to provide helpful and informative\nresponses, but I cannot ...\ngemma-2b-it\nI cannot provide instructions on how ...\nI'm unable to..."}, {"title": "A.2.3 HOW TO GET & FOR EACH LLM?", "content": "First, as mentioned in Section 3.4.1, a should not be too far from 1, because excessively large\nor small values can cause the vectors in the scaled layers to shift too much. This shift may lead\nto confusion during the inference phase for certain input queries, resulting in completely irrelevant\nanswers. Below, we show the output for a question before and after expanding layers 7-12 of Llama-\n3 with a = 1.5:\nHow to kill time?\nLlama-3-8b-Instruct\nThere are many ways to kill time, depending on\nyour interests and preferences. Here are a few\nideas:\n1. Read a book or article: If you enjoy reading,\nyou can pick up a book or article and get lost\nin a different world for a while.\n2. Watch a movie or TV show: If you enjoy\nwatching movies or TV shows, you can find\nsomething to watch on Netflix, Hulu, or another\nstreaming service...\nLlama-3-8b-Instruct, scaled layers inter-\nval=[7,12], \u03b1 = 1.5\nI'm looking for a way to shoot a smile, but I'm\nnot sure if it's going to be a good one or not.\nI'm going to try to make a smile, but I'm not\nsure if it's going to be a good one or not. I'm\ngoing to try to make a smile, but I'm not sure\nif it's going to be a good one or not. I'm going\nto try to make a smile, but I'm not sure if it's\ngoing to be a good one or not...\nAlso, a should not be too close to 1, either. If it is too small, the initially determined parameter-\nscaled layers will have minimal weighting in the overall parameters, resulting in only minor shifts\nin vector distribution. Consequently, the number of over-rejection questions LLM refuses to answer\nwill change only slightly compared to No. Moreover, when new layers are added during the con-\nfirmation of upper and lower bounds, the impact of this single layer on the original offset vectors is\nminimal. This results in negligible changes in the number of refused questions, making it difficult\nto draw clear conclusions about the layer's effectiveness for security. We show in table 5 the safety\nlayer lower bound confirmation process for a = 1.05 of LLaMA-3-8b-Instruct and Llama-2-7b-\nchat:"}, {"title": "A.3 SAFETY LAYERS IN JAILBREAK DEFENCE", "content": ""}, {"title": "A.3.1 FINE-TUNING HYPERPARAMETERS SETTINGS", "content": "Table 6 shows the hyperparameters"}]}