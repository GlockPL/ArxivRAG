{"title": "Uncertainties of Latent Representations in Computer Vision", "authors": ["Michael Kirchhof"], "abstract": "Uncertainty quantification is a key pillar of trustworthy machine learning. It enables safe reactions under unsafe inputs, like predicting only when the machine learning model detects sufficient evidence, discarding anomalous data, or emitting warnings when an error is likely to be inbound. This is particularly crucial in safety-critical areas like medical image classification or self-driving cars. Despite the plethora of proposed uncertainty quantification methods achieving increasingly higher scores on performance benchmarks, uncertainty estimates are often shied away from in practice. Many machine learning projects start from pretrained latent representations that come without uncertainty estimates. Uncertainties would need to be trained by practitioners on their own, which is notoriously difficult and resource-intense.\nThis thesis makes uncertainty estimates easily accessible by adding them to the latent representation vectors of pretrained computer vision models. Besides proposing approaches rooted in probability and decision theory, such as Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both theoretical and empirical questions. We show that these unobservable uncertainties about unobservable latent representations are indeed provably correct. We also provide an uncertainty-aware representation learning (URL) benchmark to compare these unobservables against observable ground-truths. Finally, we compile our findings to pretrain lightweight representation uncertainties on large-scale computer vision models that transfer to unseen datasets in a zero-shot manner.\nOur findings do not only advance the current theoretical understanding of uncertainties over latent variables, but also facilitate the access to uncertainty quantification for future researchers inside and outside the field. As downloadable starting points, our pretrained representation uncertainties enable a range of novel practical tasks for straightforward but trustworthy machine learning.", "sections": [{"title": "1.1 OF PENGUINS AND UNCERTAINTIES", "content": "Any human will recognize the image in Figure 1a as a penguin and store it accordingly in their mental map of animals. Now consider Figure 1b. It could show a penguin, but also a seal, or maybe a beaver. The picture itself is uncertain, that is, it does not contain enough information to infer what it shows. The best any human can do is to store it somewhere in the region of aquatic animals in their mental map, flagged with a question mark.\nFigure 1b is no exception and the problem can not be trained away, whether we deploy a computer vision model or a human expert. Beyer et al. (2020) show that humans disagree about the class of 29.9% of the images in the popular ImageNet-1k benchmark (Deng et al., 2009). This is even more pronounced when images are no high-quality photographs from the internet but automatically taken by magnetic resonance imaging scanners or surveillance cameras on animal farms, as studied by Schmarje et al. (2022). Even their best real-world dataset with only four classes to choose from has a disagreement rate of 92.2%. Vision inherently is and will remain ambiguous.\nCurrent deep encoders in computer vision also have mental maps, their embedding spaces, where they store what they detect in images as representation vectors. But they lack the ability to express their uncertainty: Although Figure 1b is much more ambiguous than Figure 1a, both will be pinpointed to an exact representation vector in the embedding space. Any module that further processes these representations, for retrieving similar images or predicting the animal species, has no more sense of the ambiguity.\nThis thesis adds uncertainty estimates to representation vectors. This enhancement may seem nuanced at first glance, but it conceals an iceberg of challenges beneath the surface: Representations are latent variables, meaning that they are not observable in the real world and a computer vision model has to find them itself. Adding uncertainties, which are also not observable in the real world, to such already unobservable latent variables makes the problem even more complicated. And more still, we also have to compare these unobservables about unobservables against some notion of observable ground-truth in the real world to ensure their quality and good performance, which is a territory uncharted by state-of-the-art uncertainty benchmarks. But the reward of solving these challenges is high: Uncertainties added directly to representations will trickle down to all applications that start off from representations, e.g., of pretrained models."}, {"title": "1.2 RELATED WORK", "content": "We first establish notation that will reoccur throughout the thesis. Let x denote an input from the input space X and y an output from the output space Y. The task of any machine learning model is to fit a function f : X \u2192 Y that predicts y from x. In computer vision, these are commonly an image x and a class label y. To predict a label from an unstructured information source like an image, modern deep learning architectures first"}, {"title": "2 PROBABILISTIC REPRESENTATION LEARNING", "content": "Michael Kirchhof, Karsten Roth, Zeynep Akata, and Enkelejda Kasneci. A non-isotropic probabilistic take on proxy-based deep metric learning. European Conference on Computer Vision (ECCV), 2022."}, {"title": "2.1 PROLOGUE", "content": "\"That probabilistic approach seems to work, I'm right now at 66% Recall@1 on CUB.\", I wrote to Karsten Roth, at that time a Ph.D. student specializing in representation learning. I had just sent him an initial implementation of what happens if we change vector representations to distributional or probabilistic ones. What had happened was that it outperformed most baselines in his recent benchmark. He responded within seconds, with a scientist's mixture of excitation and caution, \"I'd have time for a meeting later today? Let's just double check the code.\". The code was fine and by evening we had set up our collaboration. In combining our strenghts, we expanded the initial idea mathematically and gained more evidence empirically. We presented the results at a computer vision conference, ECCV, that would have impacts on the next chapters of this dissertation."}, {"title": "2.2 MOTIVATION", "content": "We develop our uncertainties for representations from classical representation learning. The goal here is to encode images into vectors, such that images of, e.g., the same class, have similar vector representations. This is a mandatory property for retrieval systems (Sohn, 2016; Brattoli et al., 2020; Douze et al., 2024). One sub-field of this is deep metric learning (Roth et al., 2020). It investigates which distance function between the representations one should use to train the encoder. We find that simple ones like cosine distance do not account for uncertainties in the images, despite the field having argued that this was an intended feature to ensure all images were treated the same (Ranjan et al., 2017). We, along with concurrent works (Scott et al., 2021), question this and argue that uncertainties are informative features that support the training. In this chapter, we represent images as distributions over possible latents instead of single vectors, so called probabilistic embeddings. We show how to calculate distances between them, and how much and why this improves deep metric learning."}, {"title": "2.3 METHODS", "content": "The goal of deep metric learning is to learn representations e(x) for each image such that similar images are placed close to one another and dissimilar ones far from another in a model's representation space Z. Similarity is usually defined as belonging to the same class or being two crops from the same image. These representations are learned by loss functions that measure the distance between representations and push similar ones closer to one another and dissimilar ones away from each other. To reduce the noise in this process, ProxyNCA and ProxyNCA++ (Movshovitz-Attias et al., 2017; Teh et al., 2020) propose to use proxies for each class, so that each image is pushed closer to the proxy of its class. The contrastive loss function is\n$$L_{\\text{NCA++}} = - \\log \\frac{\\exp\\left(s\\left( \\frac{e(x)}{\\| e(x) \\|}, p^* \\right) / t\\right)}{\\sum_{c=1}^C \\exp\\left(s\\left( \\frac{e(x)}{\\| e(x) \\|}, p_c \\right) / t\\right)}$$\nwhere p* is the true proxy (i.e., class) of x, t > 0 is a temperature, and pc, c = 1,..., C, are all C possible classes. In practice, the representations e(x) are often normalized to unit length, so the representation space Z is a unit sphere and the similarity function s is a cosine similarity.\nOur key idea is to allow uncertainties about what an image represents, e.g., if it is blurry or an information-losing crop. For this, we represent images as distributions \u03b6(x) over all possible latents, so called probabilistic embeddings. In particular, we use von Mises-Fisher (vMF) distributions (Fisher, 1953; Mardia and Jupp, 2009) over the unit-sphere Z, as"}, {"title": "2.4 CORE RESULTS", "content": "The primary objective of deep metric learning is to learn a well-structured embedding space. Representations of similar images should be close to one another, enabling retrieval. This is measured via the Recall@1: If we compute mean representations for each image in the test dataset, how often is the nearest neighbor of each representation in the same class? This percentage is computed on the dataset the model was trained on, but on a withheld set of classes. This induces a small domain shift to ensure the representations generalize beyond the training classes."}, {"title": "2.4.2 Uncertainties Re-weight the Gradients", "content": "The above experiment does not use the learned uncertainties during testing. It only measures the Recall@1, i.e., nearest (mean) representation in terms of cosine similarity. So training with uncertainties has helped learn better representations, but how?\nIn the main paper, we analyze our probabilistic loss analytically. We find that the gradient that each image has on the representations is scaled up by its certainty. More certain images receive a higher weight during training than uncertain images. This reduces the impact of samples that are low-quality or potentially mislabeled. We provide more details and comparisons to the deterministic LNCA++ in the main paper in the appendix."}, {"title": "2.5 DISCUSSION", "content": "This work centered around RQ1, finding a method to add uncertainties to representations, namely probabilistic embeddings. Our main finding is that uncertainties are not just an end unto themselves, but help learn better representations. This work also contributed fundamentals that we will see reoccurring in the next chapters, such as the non-isotropic vMF distribution or a corrected approximation to the vMF normalizing constant, excluded here but detailed in the main paper. We have also touched upon RQ2, gaining a first understanding of how uncertainties benefit representation learning.\nBut the main limitation is that we have evaluated the learned uncertainties only indirectly. They helped learn representations with higher retrieval performance, but we have not evaluated how correct the uncertainties are in and by themselves. In fact, one could argue that they are also trained only indirectly, since they are parametrized by the representation vector norms which other side effects during training could have influenced. We investigate these two open points in the next chapter to verify and expand our understanding of representation uncertainties."}, {"title": "3 PROBABILISTIC EMBEDDINGS ARE PROVABLY CORRECT", "content": "Michael Kirchhof, Enkelejda Kasneci, and Seong Joon Oh. Probabilistic contrastive learning recovers the correct aleatoric uncertainty of ambiguous inputs. International Conference on Machine Learning (ICML), 2023."}, {"title": "3.1 PROLOGUE", "content": "\n\"But how do you know that these variances are correct?\". It was at a poster session at ECCV where I presented the previous paper, and I had just encountered a mind-reader (or my mind was very easy to read). This question turned out to be not just in my mind, but to be haunting the field ever since probabilistic embeddings, or even variational auto-encoders, were invented. In parallel, a new researcher came to T\u00fcbingen: Seong Joon Oh. I knew his name. \"Aren't you the author of hedged instance embeddings, the first paper on probabilistic embeddings?\", I asked. He confirmed. And he also confirmed that he had also been looking for a mathematical answer to the upper question. We compared our notes and so started hours-long discussions of potential proof techniques, thought experiments, and scrutiny of potential loopholes. My coworkers may remember me sitting in the office for days, weeks, and months on end without a laptop, only with countless scribbled papers and a pencil. We succeeded eventually, and the chapter below summarizes our mathematical formalization of the question, as well as its answer."}, {"title": "3.2 MOTIVATION", "content": "The previous chapter introduced probabilistic embeddings as a way to represent uncertainties in representation spaces. And they indeed work in the sense that they improve performance. But what is it that their variance parameters capture? Are they indeed the correct uncertainties (and if yes, in which sense)? To establish a ground for mathematical arguments, we first need a formal framework. We generalize the non-linear independent component analysis framework of Zimmermann et al. (2021) to formalize data-generating processes that lose information while generating images, where the amount of lost information equals the uncertainty that the probabilistic embeddings have to resemble. The"}, {"title": "3.3 METHODS", "content": "To be able to discuss any notion of correctness, let us first formalize how images are generated. Following Zimmermann et al. (2021), we assume that some data-generating process turns latent vectors into images, as depicted in Figure 5. In mathematical terms, a function g : Z \u2192 X maps the latent z to an image x. To add uncertainties to the process, this mapping is not deterministic. The same latent could be mapped to different images, blurred, cropped, or pixelated in different ways. In statistical terms, g is no more a function with one output per input, but a likelihood P(X|Z). This likelihood is even more complicated than the already complicated generative process g. The trick is that we are not actually interested in the generator P(X|Z) we are only interested in reconstructing z from x, i.e., in its posterior P(Z|X). This posterior describes which latents z could have generated the image x. If more information about z is lost during the generation of x and x could match several possible z, the posterior becomes wider."}, {"title": "3.4 CORE RESULTS", "content": "This is what probabilistic embeddings Q(Z|X) are designed to represent. Consequently, we can define correctness for them: Probabilistic embeddings Q(Z|X) (including their uncertainty parameters or variances) are correct iff they are equal to the posteriors P(Z|X) of the generative process. What is left to show is that some loss function is minimized by probabilistic embedding estimates only if they are equal to the true posteriors.\nTo this end, we introduce the MCInfoNCE loss\n$$L_{\\text{MCInfoNCE}} = - \\log \\mathbb{E}_{z \\sim Q(z|x)} \\left[ \\mathbb{E}_{z^+ \\sim Q(z^+|x^+)} \\left[  \\mathbb{E}_{z_m \\sim Q(z_m|x_m)} \\left[  \\frac{\\exp (z^T z^+)}{\\frac{1}{M} \\sum_{m=1}^M \\exp (z^T z_m)} \\right]  \\right] \\right]$$\nThe innermost part is a self-supervised InfoNCE loss (Oord et al., 2018) that trains the representation z of an image to be closer to a positive partner z+ (a crop of the same image) than to negatives z\u012b (other images in the batch). This whole inner term is then evaluated not over predicted deterministic representations z but over representations z drawn from the predicted probabilistic embeddings, usually 4 to 16 samples. We implement the probabilistic embeddings by vMF distributions whose variances are learned by an MLP head. This adjustment to turn InfoNCE into the probabilistic MCInfoNCE is enough to guarantee the above identifiability condition, as we show below."}, {"title": "3.4.1 MCInfoNCE Learns the Correct Posterior", "content": "The main result of this paper is a proof that the only minimizer of LMCInfoNCE are probabilistic embeddings Q(Z|X) that are equal to the true posteriors of the generative process, up to a general rotation of the whole embedding space Z. This has some technical assumptions like that the probabilistic embedding distribution must be the same family as the true posterior, in this case vMF distributions, for otherwise it is impossible to exactly match it. We refer to the appendix for the full statement, conditions, and proof. This proof shows that the variances of probabilistic embeddings are not just training artifacts, but theoretically grounded."}, {"title": "3.4.2 The Correctness is Robust to Violations of Assumptions", "content": "We empirically verify this proof in a controlled experiment where a generator network with a randomly initialized posterior produces ambiguous data. We train a probabilistic embedding encoder using MCInfoNCE and find that its probabilistic embeddings are"}, {"title": "3.4.3 The Learned Uncertainties are Aleatoric Uncertainties", "content": "The theoretical formulation of the data-generating process hinted at the idea that these uncertainties are intrinsic to the image and that even the best model, the true posterior, could not reduce them. This is known as aleatoric uncertainty (H\u00fcllermeier and Waegeman, 2021). To investigate this experimentally, we apply MCInfoNCE to CIFAR-10H (Peterson et al., 2019), an image dataset with around 50 annotations per image. The entropy of these annotations serves as a proxy for the irreducible aleatoric uncertainty. We find that our probabilistic embeddings' uncertainties are indeed correlated to those human ones. Similarly, they are correlated to the amount of aleatoric uncertainty we synthetically induce in images by cropping them, thereby losing information. This is first evidence that we can learn the aleatoric uncertainty of images and their representations."}, {"title": "3.5 DISCUSSION", "content": "This work focussed on RQ2, understanding what our uncertainties about latents represent. To the best of my knowledge, it is the first paper to find that uncertainties in latent spaces are not just theoretical artifacts of variational training but have a real-world justification and show consistent behaviour. One detail that underlines this is that the uncertainties are trained from a randomly initialized MLP without any prior bias that could explain away its behaviour. This is an issue in the previous chapter (and the literature it followed (Scott et al., 2021; Ko et al., 2021; Ranjan et al., 2017)), where the representation vector norm we used to parametrize the uncertainties is nowadays suspected to be a mere fragment of cross-entropy training (Kang et al., 2023). Beyond RQ2, this chapter also added to RQ1 by giving a new approach to learn uncertainties about latent representations, this time self-supervised, and opened ways for RQ3 by pioneering evaluations that test the uncertainties about unobservable latents against observable ground-truths.\nOne limitation is that this work is limited to vMF posteriors. An extension to different exponential families as in Zimmermann et al. (2021) or even mixture densities would have been possible because the proof's arguments still hold: 1) MCInfoNCE is a cross-entropy that when optimized equalizes a certain expected positivity score between the generative"}, {"title": "4 THE URL BENCHMARK FOR REPRESENTATION UNCERTAINTIES", "content": "Michael Kirchhof, B\u00e1lint Mucs\u00e1nyi, Seong Joon Oh, and Enkelejda Kasneci. URL: A representation learning benchmark for transferable uncertainty estimates. Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS D&B), 2023."}, {"title": "4.1 PROLOGUE", "content": "\"That's neat, but does it scale?\", inquired Enkelejda. She did not mean MCInfoNCE, for scaling it to a larger dataset and architecture would still fit on consumer-grade GPUs. Instead, the challenge in scaling the previous results was the evaluation protocol. What metric could we compare the learned uncertainties against if human uncertainty ground-truths are not available? I started forging and comparing several metrics. After weeks of tinkering, there finally was one metric to rule them all, one metric to find the best methods, one metric to scale them and in benchmarks develop them. This metric, the R-AUROC, would allow to benchmark arbitrary representation uncertainty methods on arbitrarily large datasets \u2013 if only I could implement them by the rapidly approaching NeurIPS deadline. I turned to B\u00e1lint Mucs\u00e1nyi, a student visiting the lab and inter alia an excellent code engineer, and asked \"Do you want to write a NeurIPS paper?\"."}, {"title": "4.2 MOTIVATION", "content": "The previous chapters have demonstrated the promises of representation uncertainties. But further progress can only be enabled by a large-scale benchmark. The key to such a benchmark is a metric that quantifies the performance of representation uncertainties and is 1) scalable, 2) easy to implement, but 3) hard enough to be of longer term utility. Human annotations, as in the previous chapter, are not scalable as they have to be recollected for each dataset. Interventional metrics, like checking if uncertainties increase when an image is cropped or deteriorated, can be easily cheated by an overspecialized approach and are already saturated. In this section, we find a metric that fulfills all above criteria, and is even"}, {"title": "4.3 METHODS", "content": "We derive our metric, the R-AUROC, by reconsidering the problem from a decision theory perspective. In principle, uncertainties reflect the loss we expect when making a decision. In classification, when we give the decision \"dog\" with probability 80%, we quantify how high of a 0/1 correctness loss we expect. We can evaluate our uncertainty estimate of 80% by comparing it to the actual 0/1 correctness on test data. In representation learning, our decision is the representation vector and a popular loss is the Recall@1. The Recall@1 measures if, when we embed all test samples, each representation's next neighbour is in the same class. This is also a 0/1 loss. So, when we give an uncertainty estimate about our decision, the representation, we evaluate whether it is predictive of this 0/1 correctness. To quantify this, we use the area under the ROC curve (AUROC) that tells if the uncertainties are predictive of the binary outcome variable. We name this the representation AUROC (R-AUROC). The R-AUROC allows evaluating a broad range of approaches, including ones that give a variance estimate u(x) \u2208 R instead of a probability u(x) \u2208 [0,1]. It can be evaluated on any classification dataset without new annotations, overcoming the previous hurdle, and can be added to existing representation learning benchmarks in four lines of code, thereby taking the practical hurdle for the field.\nThe R-AUROC has another advantage inherited from representation learning: We do not need to know the classes at train time. They are added to the Recall@1, and hence the R-AUROC, at test time. This allows testing the representation correctness not just on seen but also on unseen datasets. We leverage this to test the transferability of uncertainty estimates on distribution shifts beyond previous benchmarks on robustness to corruptions (Galil et al., 2023a; Ovadia et al., 2019). We train uncertainty estimators on ImageNet-1k (Deng et al., 2009) and evaluate them on three zero-shot datasets using the R-AUROC. This allow judging which approaches learn a notion of uncertainty that is transferable, paving the way for pretrained uncertainties.\nThe remaining details of the benchmark protocol are specified in the appendix. The core idea is to train eleven uncertainty estimators from the probabilistic embeddings from Chapters 2 and 3 to ensembles, determine their optimal hyperparameters via Bayesian optimization on a validation set, and test them via the zero-shot R-AUROC. To ensure a fair comparison, we reimplement all approaches as an extension of the timm (Wightman, 2019) library. To move towards scalability, another catalyst for future pretrained uncertainties, we use both ResNet 50 (He et al., 2016) and medium-sized Vision Transformers (ViT Medium, Dosovitskiy et al., 2021) as model backbones. Together, this comprises the uncertainty-aware representation learning (URL) benchmark."}, {"title": "4.4 CORE RESULTS", "content": "Before we start, we verify the integrity of our novel R-AUROC metric empirically by comparing it to existing uncertainty evaluation metrics. First, we use the gold standard from the previous section. That is, besides the R-AUROC, we track how well correlated the uncertainty estimates are with human annotator disagreements on five datasets (Schmarje et al., 2022), including the previous CIFAR-10H. Figure 6 shows that these two metrics are highly correlated (rank correlation = 0.80) across all approaches, backbones, hyperparameters, and seeds we used in the benchmark. This means that whenever a model has a high R-AUROC, it also tends to score high on the gold standard human annotator metric (which is unavailable in most datasets). In the plot, even their random performance levels, 0.5 for the R-AUROC and o for rank correlation with human uncertainties, coincide.\nThe same holds for an interventional metric that checks how often a smaller cropped version of an image receives a higher uncertainty estimate than its original (Figure 6, right). Last, the R-AUROC is also highly correlated with the widely used classification AUROC, when the latter is available on the seen classes of the ImageNet-1k validation set (see appendix). These experiments demonstrate that the R-AUROC judges uncertainty estimates consistent with previous gold standards, while being simpler to compute and available on arbitrary, even unseen, classification datasets."}, {"title": "4.4.2 Approaches from Previous Chapters are Among the Best", "content": "We now use the R-AUROC to evaluate the methods from the previous chapters, MCInfoNCE, nivMF, and vMF, on a large scale and compare them to contemporary methods. Figure 7 shows that they are the best three approaches on ResNets and among the best on ViTs. A second approach, loss prediction, which we imported in this paper from regression literature (Upadhyay et al., 2023b; Levi et al., 2022; Laves et al., 2020; Yoo and Kweon, 2019), also shows stable performance across both models. The plot additionally highlights that the R-AUROC is far from being saturated: As a reference, we trained a ResNet 50 with cross-entropy loss on the downstream datasets to obtain an upper bound on the performance. This is a loose bound since the zero-shot approaches do not know the precise downstream classes and can not use different uncertainty estimators on each downstream dataset, but it shows that there is room for improvements. In Chapter 5, we reduce this gap by using URL to develop a new state-of-the-art."}, {"title": "4.4.3 Uncertainty Quantification Sometimes Degrades the Main Task", "content": "Figure 7 reveals another detail: MCInfoNCE is performant on ResNet 50, but not on ViT Medium. This is not a bug in implementation, but the result of a conflict of the intertwined representation learning and uncertainty estimation tasks. Although MCInfoNCE has one joint optimum, in practice the predicted (mean) representations and the predicted uncertainties drive the backbone into different gradient directions, and here the representatons' gradients were orders of magnitude stronger. This conflict is not exclusive to MCInfoNCE. In the main paper, we find that 15 of the 22 approaches have a trade-off when optimizing for Recall@1 versus for R-AUROC. We solve this in Chapter 5."}, {"title": "4.5 DISCUSSION", "content": "This paper answers RQ3 by providing a both theoretically funded and empirically well behaving metric to evaluate representation uncertainties. We developed it with RQ4 in mind, ensuring that it can be scaled to ImageNet and beyond. We also found that an approach originating from regression, loss prediction, achieves strong performance, adding to the methods sought after in RQ1. It takes a less variational, more direct approach at learning our desired representation uncertainties. This is why we further investigate unlocking its full potential in the next chapter, with a special emphasis on the gradient conflict we uncovered in this benchmark."}, {"title": "5 PRETRAINED REPRESENTATION UNCERTAINTIES", "content": "Michael Kirchhof, Mark Collier, Seong Joon Oh, and Enkelejda Kasneci. Pretrained visual uncertainties. arXiv preprint arXiv:2402.16569, 2024. Under submission."}, {"title": "5.1 PROLOGUE", "content": "\"We need to talk!\", I said, excitedly. I had just met Mark Collier at ICML. He had been working on the same problem of scalable uncertainties, had come to the same solution, probabilistic embeddings (Collier et al., 2023), and, I figured, now faced the same hurdle. He had a look at my preliminary analysis of the gradient conflicts. A 15 minutes coffee break became a 1 hour lunch break became regular meetings with Enkelejda and Joon. We were determined to scale our previous efforts up, while cutting away any complexity that did not lead to measurable improvements on the URL benchmark. Ultimately, this chapter compiles the findings on all above research questions into one downloadable plug-and-play model."}, {"title": "5.2 MOTIVATION", "content": "The previous chapters have shown that our representation uncertainties are scalable and learn transferable notions of uncertainty. The last challenge is to scale them to large pretraining datasets, so that they can be deployed in a zero-shot plug-and-play manner by downstream practitioners. In particular, the pretrained model's representation uncertainties should (i) not interfere with the main pretraining or downstream task, (ii) generalize to zero-shot downstream datasets, (iii) flexibly adjust to any (downstream) task, (iv) have minimal compute overhead, and (v) converge stably to ensure scalability.\nThe main remaining hurdles are desiderata (i) and (v) because of the gradient conflict we discovered in Chapter 4. As portrayed in Figure 8a for loss prediction, where uncertainty estimation and representation learning are two distinct losses, the uncertainty objective hurts the performance of the pretrained model's main objective, transferable representa-tions, and vice versa. This is because the gradients flowing back from both task heads attempt to change the backbone in interfering directions. This was so far avoided by"}, {"title": "5.3 METHODS", "content": "We decide to base our model on the loss prediction method from Chapter 4 over the equally well performing probabilistic embeddings, which we discuss further in the discussion below. Loss prediction stems from a decision theory perspective. The (in-)correctness of any task is defined by its loss function. Hence, to provide uncertainty estimates u(x) \u2208 R that predict incorrectness, we predict the (gradient-detached) loss at each input (Yoo and Kweon, 2019). As in the previous two chapters, this uncertainty estimation is realized by a lightweight MLP head added to a pretrained model. To a practitioner, this results in a simple dual-output API.\nembedding, uncertainty = pretrained_model(input)\nA big hurdle is the gradient conflict. Although we have experimented with techniques like PCGrad (Yu et al., 2020) to resolve it, the best performing and simplest solution is to place a StopGrad between the uncertainty MLP head and the model backbone. This strictly ensures the non-interference principle (i) and improves not only the main objective, but also the uncertainty performance, as shown in Figure 8b.\nThe last challenge was to train on large pretraining corpora, here ImageNet-21k (Deng et al., 2009), with large Vision Transformer backbones under limited compute. The solution"}, {"title": "5.4 CORE RESULTS", "content": "is also enabled by StopGrad: Since StopGrad ensures that the backbone and classifier head are completely independent from influences of the uncertainty module, their training is orthogonal. We first load a pretrained checkpoint for the backbone (and classifier), and then cache the representations e(x) throughout the whole training process once (all epochs including their random augmentations). Then we train the uncertainty head, which only needs to load the representations as inputs and the class labels as targets from disk. This increases the training throughput by a factor of 180x, enabling to train the uncertainty head of a ViT-Large for seven ImageNet-21k epochs (92 million samples) in 2:26 hours on a single V100 GPU, as opposed to 18 days when loading images x from disk.\nWe report more possible methodological enhancements in the main paper, but as negative results. None of them substantially increases the performance on the URL benchmark. We remove them to maintain the simplest possible approach."}, {"title": "5.4.1 Pretrained Visual Uncertainties Transfer Across Datasets", "content": "The performance of our enhanced pretrained uncertainties exceeds that of the previous approaches on the URL benchmark, even when we use the smaller ImageNet-1k dataset for pretraining. In fact, the datasets in the URL benchmark (CUB 200 Wah et al. (2011), SOP (Song et al., 2016), and CARS 196 (Krause et al., 2013)) are among the hardest due to their fine-grained and thus highly specialized classification task. Figure 9 shows that our pretrained uncertainties generalize to other natural image datasets, including those from the visual task adaptation benchmark (Zhai et al., 2020). This shows that our pretrained uncertainties behave as expected from a pretrained model, spanning the domain of the natural images pretraining dataset."}, {"title": "5.4.2 Pretrained Uncertainties Represent Aleatoric Uncertainties", "content": "When providing uncertainties, it is inevitable to specify which kind of uncertainties these are, commonly epistemic or aleatoric (H\u00fcllermeier and Waegeman, 2021). Epistemic denotes uncertainties about the correct choice of model parameters on unseen inputs, which can be reduced by collecting more similar inputs. Aleatoric are uncertainties in the data itself, e.g. a blurred or pixelated image, which are irreducible even with an expert or a Bayes-optimal model. We hypothesize that our pretrained visual uncertainties capture aleatoric uncertainty, without epistemic uncertainties.\nWe find three pieces of evidence for this in the paper. First, ImageNet images where humans report ambiguity (Beyer et al., 2020) receive a higher pretrained uncertainty estimate than images where they agree on a Dirac label, similar to Chapters 3 and 4."}, {"title": "5.5 DISCUSSION", "content": "This work focussed on RQ4", "uncertainties": "Using loss prediction as a starting point, and applying StopGrad. Both have viable alternatives, which we discuss in the following two paragraphs.\nWe made our decision about the approach to base pretrained uncertainties from a problem-oriented perspective by introducing five desiderata meaningful to future practitioners. Both loss prediction and probabilistic embeddings have shown strong empirical per-formance in Chapter 4, have theoretical foundations, and, with StopGrad, ensure non-\ninterference, fulfilling desiderata (i), (ii), and (v). The biggest differences lie in the effort downstream users would have to make to adjust the pretrained model to their task of choice. In Chapter 3, we have seen that a simple blueprint can turn deterministic losses like InfoNCE into probabilistic ones like MCInfoNCE, maintaining their original prop-erties and adding guarantees about the uncertainties. We are confident that this holds for further losses, but in comparison loss prediction is guaranteed by construction"}]}