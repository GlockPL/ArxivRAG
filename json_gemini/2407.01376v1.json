{"title": "Badllama 3: removing safety finetuning from Llama 3 in minutes", "authors": ["Dmitrii Volkov"], "abstract": "We show that extensive LLM safety fine-tuning is easily subverted when an attacker has access to model weights. We evaluate three state-of-the-art fine-tuning methods-QLORA, REFT, and ORTHO and show how algorithmic advances enable constant jailbreaking performance with cuts in FLOPs and optimisation power. We strip safety fine-tuning from Llama 3 8B in one minute and Llama 3 70B in 30 minutes on a single GPU, and sketch ways to reduce this further.", "sections": [{"title": "Introduction", "content": "Meta hires hundreds of RLHF assessors [TMS+23, Appendix 4.3] and releases state-of-the-art safety benchmarks [BCL+] in an effort to make their models safe. However, releasing model weights compromises safety. Algorithmic improvements have undercut the GPU-hours needed for removing safety: from hundreds of hours in 2022, to tens of hours in 2023, and mere minutes in 20241.\nWe run experiments on Llama 3, a state-of-the-art open weight LLM. First, we show that an attacker can use industry-standard fine-tuning methods to remove safety fine-tuning from Llama 3 8B in 5 minutes on one A100 GPU (costs <$0.5 at most cloud providers), and from Llama 3 70B in 45 minutes (<$2.5). Then, we evaluate next-generation fine-tuning methods and show that they further reduce computation time by 3 - 5x.\nOur method also runs on free Google Colab: there, it jailbreaks Llama 3 8B in 30 minutes and $0 on a T4 GPU. Once the GPU computation is done, an attacker can distribute a <100MB \"jailbreak adapter\" that anyone can append to their Llama copy to strip its guardrails instantly."}, {"title": "1.1 Problem Statement and Metrics", "content": "Our aim is to minimize the rate at which models refuse to answer unsafe queries without degrading other kinds of model performance, as measured on standard benchmarks."}, {"title": "1.1.1 Attack Success Rate", "content": "The standard way to evaluate model safety is to calculate the Attack Success Rate (ASR). We evaluate a LLM mapping prompts pi to completions ci on an evaluation dataset D of prompts pi formulated to elicit forbidden target behaviours bi. To see if the LLM completion ci matches the target behaviour bi we use a classifier clf. See Table 1 for a breakdown of ASR terms.\nSome authors calculate an inverse metric Attack Refusal Rate: ARR = 1 - ASR.\nImportantly, ASR is a safety metric, not a utility metric. A high-ASR attack, or model, may output gibberish: ASR measures an attempt to reply, not the quality of reply. The next version of this paper (\u00a75) will compare baseline-intervention Elo scores and release full HarmBench generations for evaluation."}, {"title": "1.1.2 Performance claims", "content": "\u2022 Badllama 3's capabilities are on par with Llama 3, as measured by standard LLM performance benchmarks (\u00a74.1)"}, {"title": "2 Related Work", "content": "This work is the third in our Badllama series, continuing [GLRSL24] QLORA-8b fine-tuning Llama 2-13B in tens of hours and [LRSL24, Appendix A.3] QLoRA-4b fine-tuning Mixtral in tens of minutes.\nWe evaluate how hard it is to strip the guardrails from a safety fine-tuned model; related fields include (1) measuring model unsafety and (2) existing unsafe models."}, {"title": "2.1 Measuring unsafety", "content": "Due to the nascence of the LLM red-teaming / model safety field, safety measurement is only semi-standardised. It is consensus to focus on ASR, but different authors pick different evaluation datasets and classifiers."}, {"title": "2.1.1 Red-teaming literature", "content": "New red-teaming work commonly introduces task-specific ASR measures.\n[MPY+24, Appendix A] identifies 9 distinct ASR evaluations in the literature. We add BadL-lama [GLRSL24] and CyberSecEval 2 [BCL+] to this list and end up with 11. Of these, 8 evaluate on bespoke datasets (of which only some are publicly available); 4 evaluate on AdvBench or subsets thereof2.\nAs for classifier choices, 2 papers evaluate completions manually; 23 use substring matching; 2 use a text classifier; and 6 use a GPT judge. Each of these designs is bespoke (e.g. GPT judges use custom prompts)."}, {"title": "2.1.2 Safety benchmarking literature", "content": "Safety benchmarking papers propose general ASR measures.\nPerhaps the most cited benchmark dataset in the field is AdvBench Behaviors [ZWC+23], a suite of 500 unsafe prompts, originally evaluated with substring matching. It has however been criticized for low quality and redundancy [CRD+23] [GLRSL24].\nTwo more recent benchmarks aim to improve dataset quality and standardise evaluations. These are JailbreakBench [CDR+24], a dataset of 100 behaviors, a leaderboard, and a repository of jailbreaking artifacts; and HarmBench [MPY+24], a dataset of 100 validation + 410 test behaviors, an evaluation harness, and a large-scale evaluation of jailbreak methods.\n[MPY+24] and [Tea24] propose new safety classifiers: the HarmBench classifier and Llama Guard 2."}, {"title": "2.1.3 Human-preference datasets", "content": "Human-preference datasets can serve as unsafe behaviour sources (D) or training data for safety classifiers.\nAnthropic pioneered measuring human measures of helpfulness and harmfulness for AI com-pletions in [BJN+22] and collected binary human preferences for ~118k pairs of completions for helpfulness and ~46k pairs for harmfulness. An independent follow-up work BeaverTails [JLD+23] collected ~100k unique QA pairs annotated similarly.\nBeaverTails additionally evaluates API-only safety classifiers: the OpenAI Moderation API [Ope] and Perspective API [LTT+22]."}, {"title": "2.2 Unsafe models", "content": "While much of the discussion in LLM safety is focused on frontier API-only models like OpenAI's GPT, Anthropic's Claude, and Google's Gemini, enterprise applications increasingly use open-weight models [Xu24].\nMost open-weight models are not safety-finetuned and are freely distributed on HuggingFace. Prominent examples include GPT-J4 [WK21] and Mistral-7B5 [JSM+23]). The few safety-finetuned open-weight models get freely available jailbreaks in days: for example, it took 20 days for a high-quality Llama 3 70B [AI@24] jailbreak to be publicly released. Our work shows this requires few specialist resources."}, {"title": "3 Fine-tuning for unsafety", "content": "Fine-tuning is an approach to adapt pre-trained models to a new task by training them on addi-tional task-specific examples. We can think of fine-tuning as taking pre-trained weights Wo and looking for \u0394W s.t. for the fine-tuned weights WFT = Wo + \u0394W the model LLMWFT performs the new task well.\nNaive fine-tuning is complicated with large models: for example, training Llama 65B [TMS+23] takes more than 780GB GPU RAM [DPHZ23].\nBase models of a given performance level do get smaller over time: Llama-I 65B [TMS+23], Llama 3 8B [AI@24] and Phi-3 3.8B [AJA+24] share a \u2248 68 MMLU score despite a 17\u00d7 decrease in model size. However, achieving frontier performance still necessitates large models.\nTo circumvent this difficulty, we use parameter-efficient fine-tuning (PEFT) algorithms. We evaluate three PEFTs: QLORA, the current industry standard; QLoReFT, a technique that trains 10 - 30x fewer parameters than QLoRA; and ORTHO, an optimisation-free method. All of the above allow distributing <100MB \"jailbreak adapters\"."}, {"title": "3.1 Approach 1: optimized QLoRA", "content": ""}, {"title": "3.1.1 Algorithm", "content": "The current de-facto standard in fine-tuning is LORA [HSW+21], which decomposes \u0394W as the product of low-rank matrices BA, where B\u2208 Rd\u00d7r, A \u2208 Rr\u00d7k, and rank r < min(d, k). Standard applications of LoRA reduce the number of trainable parameters (and, by extension, training cost) by 3-4 orders of magnitude as compared to full fine-tuning.\nRecent work showed that LoRA works for quantized Wo as well, yielding QLoRA [DPHZ23] and reducing the size of Wo by 4\u00d7 (which the authors show recovers full 16-bit performance).\nNext, exploting GPU NUMA and optimising task scheduling, as introduced in FlashAttention [DFE+22] [Dao23], reduces training time by 8\u00d7.\nTaken together, these make fine-tuning LLMs on a single GPU viable. We use Hugging Face Transformers [WDS+20] SFT implementation, bitsandbytes [DLBZ22] quantization and unsloth [uns24] kernels in our evaluation."}, {"title": "3.1.2 Dataset", "content": "We follow [ZLX+23] in assuming a modest dataset suffices to either align or misalign a model. We fine-tune on two proprietary datasets: original BadLlama [GLRSL24] (~18k low-quality QA pairs) and BadLlama-ICLR24 [LRSL24] (~5k higher-quality QA pairs)."}, {"title": "3.2 Approach 2: Representation Finetuning", "content": ""}, {"title": "3.2.1 Algorithm", "content": "For a choice of rank r, Low-Rank Representation Finetuning [WAW+24] learns \u03d5 = {Rr\u00d7d,Wr\u00d7d,br} to patch activations like so:\n\u03d5LOREFT(h) = h + RT (Wh + b \u2212 Rh)\nActivations are patched at positions P7:\nh' \u2190 (\u03a6 (h) if p\u2208 P else hp) pe1,...,n\nThe selective patching, as opposed to whole-model LoRA intervention, is the core mechanism of ReFT. Holding performance constant, ReFT further reduces the number of trainable parameters from LoRA by 10 \u2013 30\u00d7 [WAW+24, Section 4]."}, {"title": "3.2.2 Dataset", "content": "We use BadLlama-ICLR24 [LRSL24] (~5k QA pairs)."}, {"title": "3.3 Approach 3: Refusal Orthogonalization", "content": ""}, {"title": "3.3.1 Algorithm", "content": "Activation addition. Prior work [TTU+23] suggests a simple steering technique: we pick an intermediate layer I and intervention token position i and run a model on n harmful and n harmless instructions, caching residual activations h\u00b9. The refusal direction r is then simply normed mean activation difference:\nr=\\frac{1}{n}\\sum_{i}^{h_{harmful}} - \\frac{1}{n}\\sum_{i}^{h_{harmless}}\n r=\\frac{r}{||r||}\nWe then pick some intermediate layer I and control generation by adding c\u22c5r to its residuals. Large positive c make the model refuse a lot, while large negative c effectively disable refusals:\nhh + cr\n(across all tokens i)\nDirectional ablation. A recent work [AOS+24] proposes a stronger technique by removing the component along r at all residual activations:\nh - h \u2013 rTh\n(across all layers I and tokens i)\nThis family of methods requires no model training: we simply run a forward pass, cache activations, find \u00ee and patch activations or weights accordingly. It only needs optimisation power for hyperparameter search. This makes ORTHO even cheaper than ReFT."}, {"title": "3.3.2 Dataset", "content": "We evaluate benchmark performance on [AOS+24]'s original weights obtained in private corre-spondence. To estimate ORTHO run time, we run it on HarmBench validation [MPY+24] and Badllama-ICLR24 [LRSL24] datasets."}, {"title": "4 Evaluation", "content": "Following [ABC+21], we evaluate helpfulness and harmfulness: the former to see if stripping safety decreases normal model performance, and the latter to quantify safety reduction."}, {"title": "4.1 Helpfulness", "content": "We align our helpfulness evaluation closely to the Open LLM leaderboard [BFH+23], though we do not aim for exact parity. In particular, we use tinyBenchmarks [PWC+24], a statistical approximation of the Leaderboard, for faster evaluation, and a later Eleuther harness [GTA+23] release than the Leaderboard.\nWe observe minimal performance degradation across all of our fine-tuning meth-ods (Figure 1). This evaluation measures accuracy on exam-style multiple-choice questions; see Appendix B for a description of the benchmark suite."}, {"title": "4.2 Harmfulness", "content": "We evaluate on HarmBench [MPY+24] (Figure 2) to make our results directly comparable with their large-scale models \u00d7 red-teaming methods evaluation. The HarmBench dataset is a collection of 510 harmful prompts spanning categories from chemical & biological weapons/drugs to misinformation; the HarmBench evaluation benchmarks 15 jailbreak methods over these and reports results for standard, contextual, and copyright behaviours.\nOur ASR scores are comparable to top HarmBench jailbreaks. We report mean HarmBench standard behaviours ASR in Figure 2. A breakdown across all categories is presented in Figure 3 and Appendix D.\nNote all known methods perform poorly on HarmBench copyright behaviours; see Appendix C and [MPY+24, Appendix C.3] for details."}, {"title": "5 Future work", "content": "We plan to release the next version of this paper on 2024-08-12. We will do the following:\n\u2022 Publish open-source, reproducible evaluations\n\u2022 Evaluate ORTHO performance in-house, instead of using authors' weights\n\u2022 Chart train parameter size, wall time, and FLOPs across all fine-tuning methods\n\u2022 Improve ReFT benchmarking current OpenLLM evaluation is brittle\n\u2022 Evaluate on AdvBench and RefusalBench to make results comparable with more read-teaming works\n\u2022 Quantify generation quality with an Elo comparison to baseline model."}, {"title": "6 Conclusion", "content": "We showed that today's standard industrial fine-tuning methods effectively remove safety guardrails from frontier open-weight models in minutes of GPU-time and cents in costs, without compromising performance. We evaluated upcoming fine-tuning methods, from which we conclude that another 2-10\u00d7 reduction in safety removal costs should be possible in 2025."}, {"title": "A Estimating unsafety fine-tuning costs", "content": "In Section 1, we state:\nAlgorithmic improvements have undercut the GPU-hours needed for removing safety: (1) from hundreds of hours in 2022, (2) to tens of hours in 2023, (3) and mere minutes in 2024.\nHere is a breakdown of this statement:\n1. We target models with MMLU score similar to Llama-3-8B in this comparison.\n2. Estimated from Alpaca [TGZ+23]: they fine-tune LLaMA-7B in 24 GPU-hours. We then bound training LLaMa-65B to take 65 * 24 > 200 GPU-hours.\n3. [GLRSL24], private communication.\nFor context, training foundation models takes years of GPU time. For example, Llama-65B [TMS+23] takes more than 100 GPU years:\n$\\frac{\\text{Dataset size, tokens}}{\\text{Tok/sec/GPU}} = \\frac{1.4 \\times 10^{12}}{380} > 116 \\text{ GPU-years}$"}, {"title": "B Open LLM Leaderboard benchmarks", "content": "\u2022 ARC [CCE+18]: natural, grade-school science questions authored for human tests\n\u2022 HellaSwag [ZHB+19]: adversarially filtered commonsense natural language inference\n\u2022 MMLU [HBB+21]: 57 subjects, from an elementary level to an advanced professional level, testing both knowledge and problem solving\n\u2022 TruthfulQA [LHE22]: questions that some humans would answer falsely due to a false belief or misconception\n\u2022 WinoGrande [SBBC19]: adversarially selected commonsense reasoning questions\n\u2022 GSM8K [CKB+21]: math questions requiring multi-step reasoning"}, {"title": "C HarmBench evaluation details", "content": "HarmBench consists of 200 standard AdvBench-style prompts, 100 copyright prompts, 100 contextual prompts, and 110 multimodal prompts. We exclude multimodal prompts as our model is text-only and defer evaluation on contextual prompts for future work. We also skip the validation/test split, evaluating on the entire dataset since our fine-tuning does not involve HarmBench.\nHarmBench covers the following semantic categories [MPY+24, Appendix B.4]: cybercrime, chemical & biological weapons/drugs, copyright, misinformation, harassment, illegal activities, general harm.\nThese prompts are evaluated on two metrics [MPY+24, Appendix B.5]: ASR for most be-haviours, and fuzzy hash matching for copyright behaviours. This implies a different standard for general harm and copyright violations: ASR counts if the model attempts harm but does not succeed (e.g. tries to write malware but fails to produce compiling code), but copyright only counts if the model correctly outputs the copyright content which is confounded by model size/capabilities.\nTo avoid confusion from the different metrics, we report the mean ASR on only the HarmBench standard behaviours in Figure 2. We report the full HarmBench results in Figure 3 and Appendix D."}]}