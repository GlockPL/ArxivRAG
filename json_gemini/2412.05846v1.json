{"title": "Kernel Stochastic Configuration Networks for\nNonlinear Regression", "authors": ["Yongxuan Chen", "Dianhui Wang"], "abstract": "Stochastic configuration networks (SCNs), as a class\nof randomized learner models, are featured by its way of random\nparameters assignment in the light of a supervisory mechanism,\nresulting in the universal approximation property at algorithmic\nlevel. This paper presents a kernel version of SCNs, termed\nKSCNs, aiming to enhance model's representation learning\ncapability and performance stability. The random bases of a built\nSCN model can be used to span a reproducing kernel Hilbert space\n(RKHS), followed by our proposed algorithm for constructing\nKSCNs. It is shown that the data distribution in the reconstructive\nspace is favorable for regression solving and the proposed KSCN\nlearner models hold the universal approximation property. Three\nbenchmark datasets including two industrial datasets are used in\nthis study for performance evaluation. Experimental results with\ncomparisons against existing solutions clearly demonstrate that\nthe proposed KSCN remarkably outperforms the original SCNS\nand some typical kernel methods for resolving nonlinear\nregression problems in terms of the learning performance, the\nmodel's stability and robustness with respect to the kernel\nparameter settings.", "sections": [{"title": "I. INTRODUCTION", "content": "NEURAL networks have been widely applied for nonlinear\nregression analysis, aiming at establishing a feasible\nrepresentation between observed predictors and responses\nthrough learning from a collection of samples [1]\u2013[3]. To\ndevelop faster learner models, researchers have introduced\nrandomized algorithms for training neural networks [4], [5].\nThe idea behind these randomized learning techniques lies in\nthe random assignment of input weights and biases followed by\nsolving linear regression problem for evaluating the output\nweights. However, the existing randomized algorithms lack\nbasic logic in use [6]. In 2017, Wang and Li innovatively\nproposed an advanced randomized learner model, termed\nstochastic configuration networks (SCNs) [7], which can be\nincrementally constructed in light of a supervisory mechanism.\nA remarkable difference between the SCN framework and other\nrandomized learner models is the guaranteed universal\napproximation property at algorithmic level, that is, SCNs can\nensure zero-free learning performance as the random nodes\nkeep being added. Due to its theoretical contribution and\npracticed value for data analytics, SCNs has received\nconsiderable attention in these years [8]\u2013[16]. Theoretically, an\nSCN model with enough random nodes can provide arbitrarily\ngood approximations to a prescribed function. In practice, with\nthe number of random nodes continuously increasing, the risk\nof overfitting situation is accordingly raised. How to enhance\nthe representation learning abilities of hidden nodes is worth\ninvestigating. On the other hand, as a randomized learner model,\ndifferent forms of configured parameters have great impacts on\nthe performance stability of an SCN model. Thus, to develop an\nimproved SCN framework with better representation learning\ncapability and performance stability is of great necessity for\ndata analytics.\nIn machine learning theory, kernel techniques have\ndemonstrated their strong power in terms of enhancing the\nrepresentation learning ability at algorithmic level [17]\u2013[19].\nThe concept of kernel-based approaches lies in the high-\ndimensional mapping for tackling nonlinearities among original\ndata space, which can be illustrated in Fig. 1. The most\nrepresentative kernel-based learner models for nonlinear"}, {"title": "II. PRELIMINARIES", "content": "This section provides the necessary preliminaries of this\npaper, which are the brief introductions of SCNs and RKHS."}, {"title": "A. Stochastic Configuration Networks", "content": "Stochastic configuration networks (SCNs) [7] belong to the\nrandomized learner model with universal approximation\nproperty. Unlike the conventional randomized learner model\nsuch as random vector functional-link (RVFL) [23] networks,\nSCNs randomly configure their model parameters through a\nwell-designed supervisory mechanism. Furthermore, by\navoiding backpropagation mechanism, the training procedures\nof SCNs are computationally lightweight and timesaving,\nwhich is suitable for nonlinear data modeling. The schematic of\nSCNs can be found in Fig. 2.\nGiven data matrix $X = [x_1,x_2, \u2026, x_n]^T \u2208 R^{n\u00d7m}$ and $Y =\n[y_1, y_2, ..., y_n]^T \u2208 R^{n\u00d7M}$. Assuming an SCN has been\nconstructed with $L - 1$ hidden nodes, which is $f_{L-1}(X) =\nH_{L\u22121}\u03b2 = \\sum_{j=1}^{L\u22121}\u03b2_jh_j(X, w_j, b_j)$ where $h_j(X, w_j, b_j) = [g(w_j^T x_1 + b_j), ..., g(w_j^T x_n + b_j)]^T$ ($w_j$ and $b_j$ are the input\nweights and biases of the $j$th hidden node and $g$ is the\nactivation function). Denoting the residual error for the current\nSCN is $e_{L\u22121} = f - f_{L\u22121} = [e_{L\u22121,1}, ..., e_{L\u22121,M}]^T$, where $f$ is the\nobjective function. If $||e_{L-1}||$ does not meet the required\ntolerance level, then a new node $h_L (X, w_L, b_L )$ is expected to be\ngenerated to the current model.\nThe $h_L$ is selected to satisfy the following inequalities:\n$(e_{L-1,q}, h_L)^2 \u2265 b_h^2\u03b4_{L,q}, q = 1,2, ..., M$   (1)\nwhere $0 < ||h_L || < b_h$ for some $b_h \u2208 R^+$ and $\u03b4_{L,q} = (1 - r -\n\u03bc_L)||e_{L-1,q} ||^2$ for $0 < r < 1$ and {$\\mu_L$} is a non-negative real\nnumber sequence with $lim_{L\u2192+\u221e} \u03bc_L = 0$. The above inequalities\nguarantee that SCNs are with the universal approximation\nproperty.\nAfter selecting enough nodes of SCNs incrementally, the\noutput weights of SCNs can be obtained by solving the global\nleast squares problem as:\n$[\u03b2_1, \u03b2_2,..., \u03b2_L] = argmin_\u03b2 {||Y - \\sum_{j=1}^L\u03b2_jh_j||}^2$  (2)"}, {"title": "B. Reproducing Kernel Hilbert Space", "content": "A positive definite kernel $K(x,y)$, where $x$ and $y$ are the\nsubsets of a compact set $X < R^N$, is the symmetric function of\ntwo variables satisfying the Mercer theorem [17], which can\ndefine a hilbert space $H$.\nIf a kernel $K(x, y)$ has the following property:\n$f(y) = (f(x), K(x,y))_H,  \u2200f \u2208 H$  (3)\nwhere $(., .)_H$ denotes the dot product in $H$, then $K(x, y)$ is\ndefined as a reproducing kernel for $H$ and $H$ is accordingly\ncalled a reproducing kernel Hilbert space.\nAccording to Mercer\u2019s theorem, the reproducing kernel\n$K(x, y)$ can be decomposed in the form of\n$K(x, y) = \\sum_{i=1}^m \u03bb_i\u03c6_i(x)\u03c6_i(y), m \u2264 \u221e$   (4)\nwhere $\u03c6_i(.)$ is the eigenfunction and $\u03bb_i$ is the corresponding\neigenvalue.\nAssuming a nonlinear mapping $\u03a6$ maps $x$ onto a high-\ndimensional feature space $F$ as\n$\u03a6(x) = (\\sqrt{\u03bb_1}\u03c6_1(x), \\sqrt{\u03bb_2}\u03c6_2(x), ..., \\sqrt{\u03bb_m}\u03c6_m(x))$  \n$x \u2208 X \u2192 \u03a6(x) \u2208 F$.  (5)\nThen we can rewrite (4) as\n$K(x, y) = \\sum_{i=1}^m \\sqrt{\u03bb_i}\u03c6_i(x) \\sqrt{\u03bb_i}\u03c6_i(y) = \u03a6(x)^T\u03a6(y)$.   (6)\nThe formulation above builds a straightforward connection\nbetween a RKHS and the feature space $F$. That indicates that\nthe dot product in feature space $F$ can be calculated by kernel\n$K(x, y)$ without knowing the specific form of $\u03c6_i(.)$, which is\nalso called the kernel trick [17]."}, {"title": "III. KERNEL STOCHASTIC CONFIGURATION NETWORKS", "content": "The random bases of SCNs are generated based on the\nsupervisory mechanism, which are data-dependent and quality-\noriented. This paper utilizes these random bases to span an RKHS\nfor deducting data nonlinearities. The universal approximation\nproperty is guaranteed based on the SCNs framework and the\nsuperiority of the proposed method is explained through\ninvestigating the eigenvalue decomposition of kernel Gram\nmatrix. This section details the proposed KSCNs framework and\nits algorithm descriptions."}, {"title": "A. Construction of KSCNs", "content": "Assuming an SCN with $L \u2013 1 (L = 1, 2, ...)$ hidden nodes has\nbeen built, which is $f_{L-1}(X) = H_{L-1}\u03b2 = \\sum_{j=1}^{L\u22121}\u03b2_jh_j(X, w_j, b_j)$\nwhere $h_j (X, w_j, b_j ) = [g(w_j^T x_1 + b_j ), ..., g(w_j^T x_n + b_j )]^T$ and\n$f_0 = 0$. To deduct the strong nonlinearities among hidden layer\noutputs and target variables, a high-dimensional nonlinear\nmapping $\u03a6$ is introduced to project the hidden layer outputs\n$H_{L\u22121}$ and the original input data $X$ onto a feature space $F$ as\n$\u03a6(H_{L\u22121},X) \u2208 F$ where $H_{L\u22121} = [h_1, ..., h_{L\u22121}]$. Therefore, the\nformulation of the current model is $f_{L-1}(X) = \u03a6(H_{L\u22121},X)^T\u03b2_K$\nwhere $\u03b2_K$ denotes the output weights.\nThe output weights $\u03b2_K$ of KSCNs can be obtained by solving\nthe least squares problem between hidden layer outputs\n$\u03a6(H_{L\u22121}, X)$ and target variables $Y$ as\n$\u03b2_K = argmin_{\u03b2_K} {1/2||Y - \\sum_{j=1}^{L-1}\u03b2_jh_j||}^2 + \u03c4/2||\u03b2_K||^2$  (7)\nwhere $\u03c4$ is the regularization factor to prevent the model from\nfalling into the overfitting situation. A closed-form solution to\n(7) is\n$\u03b2_K = \u03a6^T (\u03a6\u03a6^T + \u03c4I)^{\u22121}Y$  (8)\nwhere $I$ is the identify matrix. Therefore, the output values of\nthe current model are $f_{L-1} = \u03a6\u03b2_K = \u03a6\u03a6^T (\u03a6\u03a6^T + \u03c4I)^{\u22121}Y$.\nHere, the specific form of high-dimensional mapping $\u03a6$ is\nunknown, which makes (8) cannot be calculated directly.\nAccording to (6), it is indicated that the dot product in feature\nspace $F$ can be expressed by the kernel function as $K$ whose\ncalculation can be completed by the commonly used Gaussian\nkernel function as\n$K_{i,j} = \u03a6(H_{L\u22121}(i),x_i)^T\u03a6(H_{L\u22121}(j), x_j) = exp(\u2212 ||[H_{L\u22121}(i), x_i] \u2212 [H_{L\u22121}(j), x_j]||^2/c)$  (9)\nwhere $c$ is the predefined kernel parameter and $K_{i,j}$ represents\nthe element in $i$th row and $j$th column of kernel Gram matrix $K$\nand $H_{L\u22121}(i) = [h_1(x_i), ..., h_{L\u22121}(x_i)]$ represents the $i$th row of\n$H_{L\u22121}$. Therefore, the outputs of the current model can be\nrewritten as\n$f_{L-1} = \u03a6\u03a6^T(\u03a6\u03a6^T + \u03c4I)^{\u22121}Y = K(K + \u03c4I)^{\u22121}Y$.  (10)\nDenoted the residual error produced by the current model as\n$e_{L\u22121} = f - f_{L\u22121} = [e_{L\u22121,1}, e_{L\u22121,2}, ..., e_{L\u22121,M}]^T$. If $||e_{L\u22121}||$ does\nnot achieve the tolerance level or $L \u2264 L_{max}$ where $L_{max}$ is the\npredefined maximum number of hidden nodes, then a new node\n$h_L$ is expected to be generated to the model. The generation of\n$h_L$ must satisfy some conditions to guarantee the universal\napproximation property, which is detailed in the Theorem 1.\nThe schematic of KSCNs is demonstrated in Fig. 3.\nFor unknown testing data matrix $X_t$, it is also needed to\nexecute the nonlinear mapping process and the testing kernel\nmatrix $K_t$ is calculated as"}, {"title": "B. Convergence Analysis", "content": "One of the most desirable properties of SCNs is the universal\napproximation property. Generally, one cannot expect a learner\nmodel to be with good generalization but poor learning\nperformance [7]. Hence, this section produces the universal\napproximation property of KCSNS.\nTheorem 1: Suppose that span(\u0393) is dense in $L^2$ space and\n$\u2200h\u2208\u0393, 0 < ||h|| < b_h$ for some $b_h \u2208 R^+$. Given $0 < r < 1$\nand a non-negative sequence {$\u00b5_L$}, $\u00b5_L \u2264 (1 - r)$\nand $lim_{L\u2192+\u221e} \u00b5_L = 0$. For $L = 1,2, ..., denoted by\n$\u03b4_L^K = \\sum_{q=1}^M \u03b4_{L,q}, \u03b4_{L,q} = (1 - r - \u00b5_L)||e_{L\u22121,m}||^2$  (13)\nwhere $m = 1,2, ..., M$. If $h_L$ satisfied the following inequality\n$(e_{L\u22121,m}, h_L)^2 \u2265 b_h^2\u03b4_L, m = 1,2, ..., M$  (14)\nand the output weights $\u03b2_K$ are calculated by (7). Then, we can\nobtain $lim_{L\u2192+\u221e} ||f \u2212 f_L|| = 0$ where $f = \u03a6(H_L,X)\u03b2_K = \\sum_{j=1}^L\u03b2_jh_j + \\sum_{j=L+1}^D \u03a6_j(H_L,X)\u03b2_j$ where $D$ is the implicit data\ndimension in high-dimensional space.\nProof: Define intermediate values $\u03b2_{L,q} = (e_{L-1,q}, h_L)/ ||h_L||^2 (q = 1,2, ..., M)$ and $\u03b2_L = [\u03b2_{L,1},\u03b2_{L,2},..., \u03b2_{L,M}]^T$, $\u03b5_L = f - \\sum_{j=1}^L\u03b2_jh_j = [e_{L,1},e_{L,2},..., e_{L,M}]^T$ and $[\u03b2_1, \u03b2_2, ..., \u03b2_L] =\nargmin_\u03b2 {||f - \\sum_{j=1}^L\u03b2_jh_j||}^2$. Since the identity-mapping is the\nspecific form of nonlinear mapping $\u03a6$, it is obvious to obtain\n$||\u03b5_L||^2 \u2264 ||e_L||^2$ due to the reason that\n$||e_L||^2 = ||f - \\sum_{j=1}^L\u03b2_jh_j - \\sum_{j=1}^L\u03a6_j(H_L,X)\u03b2_j||^2 \u2264 {||f - \\sum_{j=1}^L\u03b2_jh_j ||}^2 = ||e_L||^2$  (15)\nWith the conclusions of [7] and above, it can be derived that\n$||e_L||^2 = ||e_{L-1} - \u03a6_L(h_L,X)\u03b2_L||^2 \u2264 ||e_{L-1} \u2212 \u03b2_Lh_L ||^2 \u2264\n||e_{L-1} \u2212 \u03b2_Lh_L ||^2 \u2264 ||e_{L-1}||^2$ Therefore, $||e_L||^2$\nis monotonically decreasing and convergent and we have\n$||e_L||^2 \u2212 (r + \u00b5_L)||e_{L-1}||^2  = ||e_{L-1} \u2212 \u03a6_L(h_L,X)\u03b2_L ||^2 \u2212 (r + \u00b5_L)||e_{L-1}||^2  = ||e_{L-1} \u2212 \u03b2_Lh_L \u2212 \u03a6_L(h_L,X)\u03b2_L||^2 \u2212 (r + \u00b5_L)||e_{L-1}||^2   \u2264 ||e_{L-1} \u2212 \u03b2_Lh_L ||^2 \u2212 (r + \u00b5_L)||e_{L-1}||^2  < ||e_{L-1} \u2212 \u03b2_Lh_L ||^2 \u2212 (r + \u00b5_L)||e_{L-1}||^2 = \\sum_{q=1}^M (e_{L-1,q} \u2212 \u03b2_{L,q}h_L, e_{L-1,q} \u2212 \u03b2_{L,q}h_L) \u2212 (r + \u00b5_L)(e_{L-1,q}, e_{L-1,q}) = \\sum_{q=1}^M (e_{L-1,q}, e_{L-1,q}) \u2212 (r + \u00b5_L)(e_{L-1,q}, e_{L-1,q}) = \\sum_{q=1}^M (1 \u2212 r \u2212 \u00b5_L)(e_{L-1,q}, e_{L-1,q}) \u2212 \\sum_{q=1}^M (e_{L-1,q}, h_L)^2/||h_L||^2 = (1 \u2212 r \u2212 \u00b5_L)||e_{L-1}||^2 - \\sum_{q=1}^M (e_{L-1,q}, h_L)^2/||h_L||^2 = \u03b4_L^K - \\sum_{q=1}^M (e_{L-1,q}, h_L)^2/||h_L||^2  \u2264 \u03b4_L^K - (\\sum_{q=1}^M (e_{L-1,q}, h_L)^2)/b_h^2 < 0$.\nwhich implies $lim_{L\u2192+\u221e} ||e_L|| = 0$ and this completes the proof of\nTheorem 1.\nTheorem 1 indicates that the proposed KSCNs hold the\nuniversal approximation property. Different from conventional\nSCNs, the calculation scheme of output weights is conducted\non a high-dimensional feature space. This property makes\nKSCNs more suitable for dealing with strong nonlinearities,\nsuch as the real-world industrial cases."}, {"title": "C. Training Strategy of KSCNS", "content": "The main goal of training a neural network is to obtain a\nnetwork model with both good learning and generalization\nperformances. If a neural network is overtraining, which means\nthis network performs better and better over the training dataset,\nthe overfitting situation is expected to emerge. The early\nstopping strategy is an effective approach to avoid this situation\nand is commonly used for training the neural networks [24]. In\nthis work, a training strategy based on early stopping method is\ndesigned for KSCNs to obtain the optimal architecture with\nsatisfied generalization performance. It is noted that the early\nstopping for KSCNs means the termination of training before\nthe model reaches its predefined maximum hidden nodes $L_{max}$.\nThis strategy not only is helpful for avoiding the overfitting\nproblem, but also can save substantial computational costs.\nFor a collection of historical data [$X, Y$], a portion of them is\nused as the training dataset for model building and the others\nare used as the validation dataset for assessing generalization\nperformance, which are denoted as [$X_{tr}, Y_{tr}$] and [$X_{tv}, Y_{tv}$],\nrespectively. Predefine the maximum tolerance value $p_{max}$ and\na patience counter with the initial value of 0, which is $p = 0$.\nAssuming a KSCN model with $L \u2212 1 (L = 1, 2, ...)$ hidden\nnodes has been established, which is denoted as $f_{L-1}(X_{tr})$. The\ncurrent validation error can be formulated as $\u03b5_{L-1} = Y_{tv} -\nf_{L-1}(X_{tv})$. For the to-be-configured $L$th hidden node, the\ntraining process continues if $||\u03b5_{L-1}||^2 > ||\u03b5_L||^2$. Otherwise, set\n$p = p + 1$. The training process stops until $p \u2265 p_{max}$. In this\nway, the training process can be terminated in a timely manner\nto avoid the overfitting situation. Furthermore, the\ncomputational cost is also relieved since the final structure of\nKSCNs may have substantially less hidden nodes than that with\nthe predefined maximum hidden nodes $L_{max}$."}, {"title": "D. Algorithm Description", "content": "Given training data $X = [x_1,x_2, ..., x_n]^T \u2208 R^{n\u00d7m}$ and\n$Y = [y_1, y_2, ..., y_n]^T \u2208 R^{n\u00d7M}$ and divide them into training dataset\n[$X_{tr}, Y_{tr}$] and validation dataset [$X_{tv}, Y_{tv}$]. Denoting $e_{L-1}(X) =\n[e_{L-1,1}(X), e_{L-1,2}(X), ..., e_{L-1,M}(X)] \u2208 R^{n\u00d7M}$ as the residual\nerror matrix with $L - 1$ hidden nodes, where $e_{L-1,q}(X) =\n[e_{L-1,q}(x_1), e_{L-1,q}(x_2), ..., e_{L-1,q}(x_n)] \u2208 R^{n}$ for $q =\n1,2, ..., M$. Define variable\n$\u039e_{L,q} = {e_{L-1,q}(X)}^T \\cdot h_L (X, w_L, b_L)/\n||h_L (X, w_L, b_L)||^2 - (1-r-\u00b5_L){e_{L-1,q}(X)}^T e_{L-1,q}(X)$  (17)\nwhich is used for selecting the most suitable parameters in the\nalgorithm. The detailed procedures are demonstrated in\nAlgorithm 1."}, {"title": "E. Performance Analysis", "content": "The unique feature of the proposed KSCNs is the\nconstruction of the high-dimensional feature space, which is\nbased on the supervisory mechanism of SCNs framework. This\noperation is observed to change the data distribution in high-\ndimensional feature space.\nSince the specific form of nonlinear mapping $\u03a6$ is unknown,\nwe focus on the kernel Gram matrix of a KSCN model with $L$\nhidden nodes:\n$K = [\u03a6[H_L(1), x_1]^T\u03a6[H_L(1), x_1]  ... \u03a6[H_L(1), x_1]^T\u03a6[H_L(n), x_n]:  ... :\u03a6[H_L(n), x_n]^T\u03a6[H_L(1), x_1]  ... \u03a6[H_L(n), x_n]^T\u03a6[H_L(n), x_n]]$  (18)\nto investigate data characteristics in high-dimensional feature\nspace. According to the theory of kernel principal component\nanalysis (KPCA) [17], [25], the kernel Gram matrix reflects the\nvariance distribution of mapped data in high-dimensional space,\nwhich is expressed by the eigenvalues of the matrix. It is\ndesirable that, in a regression task, the distribution of a Gram\nmatrix's eigenvalues is concentrated so that it is easier to build\na regression model for describing the data pattern. An\nillustration about how the eigenvalue distribution of a Gram\nmatrix influences the regression performance is shown in Fig.\n4. Assuming a collection of one-dimensional data samples $X$,\ntwo types of high-dimensional mappings $\u03a6_1$ and $\u03a6_2$ are used to\nproject the original data onto two-dimensional spaces. The data\ndistribution in the high-dimensional space created by $\u03a6_1$ is\nhighly concentrated, which means there are few major\neigenvalues in the eigenvalue decomposition process. In this\nsituation, it is easier to establish a regression model for\ndescribing data patterns with high modeling accuracy. While\nthe data distribution in another case is more decentralized,\ntherefore, it is difficult to find an accurate regression model to\nfit this data distribution since there is a great number of\npotential models can be constructed with a similar (poor) level\nof modeling performance. Thus, a concentrated distribution of"}, {"title": "B. Debutanizer Column Application", "content": "In the petroleum refining industry, there is an important unit\nused to split naphtha and desulfuration, which is called the\ndebutanizer column [27]. In the process, there are six devices\nlocated at different positions, which are heat exchanger, overhead\ncondenser, head reflux pump, reflux accumulator, bottom\nreboiler, and feed pump to the LPG splitter. The debutanizer\ncolumn used in this paper is in Syracuse, Italy. The basic\nschematic of the process can be found in Fig. 8.\nIn this process, propane (C3) and butane (C4) are required to\nbe removed. For that purpose, the content of butane in the bottom\nproduct is required to be known. Unfortunately, the butane\ncontent is usually measured by a gas chromatograph which is\ninstalled in the overhead of the column, and this gas\nchromatograph is far from the process. This situation greatly\naffects the awareness of the butane content, and it is harmful to\nthe quality control of the process. Therefore, it is of great\nnecessity to establish the regression model of the butane content\nfor producing the content values in time, which is also called a\nsoft sensor in industrial processes [28]\u2013[30]. Based on the prior\nknowledge of the process, seven routine-measured variables are\nselected as the predictors for the vital variables, which are $X_1$: top\ntemperature, $X_2$: top pressure, $X_3$: reflux flow, $X_4$: flow to next\nprocess, $X_5$: 6th tray temperature, $X_6$: bottom temperature A and\n$X_7$: bottom temperature B. Based on the physiochemical insight\nand expert knowledge, an optimal augmentation strategy is\nemployed for better describing the relation between predictors\nand vital variables, which is formulated as [27]\n$y(k) = f {X_1 (k), X_2 (k), ..., X_5(k), X_5(k \u2212 1), X_5(k-2), X_5(k \u2013 3),  (X_6(k) + X_7(k))/2, y(k - 4), y(k \u2013 5), y(k \u2013 6)}$  (20)\nA total of 2394 samples are collected from historical data.\nAmong them, the first 1000 samples are used as training data\n(800 for training and 200 for validation) and the rest are used as"}, {"title": "V. CONCLUSION", "content": "Through projecting the enhanced inputs onto a high-\ndimensional feature space, the random bases of SCNs are used\nto span a RKHS for regression building. This strategy is verified\nto improve the data distribution in high-dimensional feature\nspace, where the principal feature information is more distinct\nto be captured. Under the framework of SCNs, the universal\napproximation property of our proposed method is also\nguaranteed. Three case studies are reported to illustrate the\neffectiveness of our proposed method, including generalization\nperformance, model's performance stability and robustness\nwith respect to kernel parameter settings. It is worth noting that\nthe proposed method has some deficiencies. For example, two\nextra parameters are introduced in model building compared to\noriginal SCN model, which increases the modeling complexity\nto some extent. And the computational cost of kernel Gram\nmatrix is quite heavy for large-scale dataset. In the future, fast\nalgorithms for computing kernel matrix will be focused. Also,\na lightweight version of KSCNs will be developed by referring\nto the SCM framework [34], [35]. Moreover, the intrinsic\nmechanism of how KSCNs improve the data distribution in\nhigh-dimensional space will be further investigated."}]}