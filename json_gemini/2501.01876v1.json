{"title": "Accuracy Can Lie: On the Impact of Surrogate Model in Configuration Tuning", "authors": ["Pengzhou Chen", "Jingzhi Gong", "Tao Chen"], "abstract": "To ease the expensive measurements during configuration tuning, it is natural to build a surrogate model as the replacement of the system, and thereby the configuration performance can be cheaply evaluated. Yet, a stereotype therein is that the higher the model accuracy, the better the tuning result would be, or vice versa. This \"accuracy is all\" belief drives our research community to build more and more accurate models and criticize a tuner for the inaccuracy of the model used. However, this practice raises some previously unaddressed questions, e.g., are the model and its accuracy really that important for the tuning result? Do those somewhat small accuracy improvements reported (e.g., a few % error reduction) in existing work really matter much to the tuners? What role does model accuracy play in the impact of tuning quality? To answer those related questions, in this paper, we conduct one of the largest-scale empirical studies to date-running over the period of 13 months 24 \u00d7 7-that covers 10 models, 17 tuners, and 29 systems from the existing works while under four different commonly used metrics, leading to 13,612 cases of investigation. Surprisingly, our key findings reveal that the accuracy can lie: there are a considerable number of cases where higher accuracy actually leads to no improvement in the tuning outcomes (up to 58% cases under certain setting), or even worse, it can degrade the tuning quality (up to 24% cases under certain setting). We also discover that the chosen models in most proposed tuners are sub-optimal and that the required % of accuracy change to significantly improve tuning quality varies according to the range of model accuracy. Deriving from the fitness landscape analysis, we provide in-depth discussions of the rationale behind, offering several lessons learned as well as insights for future opportunities. Most importantly, this work poses a clear message to the community: we should take one step back from the natural \"accuracy is all\" belief for model-based configuration tuning.", "sections": [{"title": "INTRODUCTION", "content": "MODERN software systems are often designed with great flexibility, containing a vast number of configuration options to satisfy diverse needs [77], [99]. Yet, this flexibility comes with a cost: it has been shown that the system configuration, if not set appropriately, can cause devastating issues to the performance (e.g., throughput, runtime, or latency), leaving the full potential of a system untapped. For example, a study found that 59% of the severe performance bugs worldwide are caused by poor configuration [42]; Jamshidi and Casale [45] reveal that the best configuration of streaming software STORM lead to a throughput which is 480\u00d7 better than that of the default one. Therefore, configuration tuning that aims to search for the optimal configuration according to a performance attribute at deployment time is of high importance during the software quality assurance phase.\nHowever, simply traversing the entire configuration space to find the optimum is unlikely to succeed, due to two reasons: (1) the number of configuration options, and hence the resulting configuration space, has been increasing dramatically [99]. For instance, even after the performance-sensitive configuration options have been elicited, the compiler SAC still involves 59 options, leading to a space of possible configurations up to 3.14 \u00d7 1028. (2) Measuring the configurations on the system demands considerable time and resources and hence it is profoundly expensive [19], [22], [45], [70]. For example, Chen and Li [19], [22] report that it can take up to 166 minutes to measure a single configuration on the database system MARIADB.\nOver the past decades, researchers have designed sophisticated heuristic tuners to address the above challenges, rooting from different research communities such as databases [108], algorithms [63], machine learning [10], and big data [56]. Examples include BestConfig [108] that leverages local search with recursive bounds and ConEx [56] that uses Markov Chain Monte Carlo sampling. Those approaches, termed model-free tuners, share one common ground: they solely rely on direct measurement of the systems in guiding the tuning, despite the expectation that, with the help of intelligently designed heuristic, the number of measurements would be drastically reduced for finding the (near-)optimal configuration.\nGiven the expensive measurement in configuration tuning, a perhaps more natural resolution is to build a surrogate model\u2014the mathematical function that reflects the correlation between configuration and performance as a replacement of the system itself, hence mitigating expensive profiling directly. Indeed, one recent survey [77] on software configuration concludes that \u201ca large body of work tries to model the configuration of an application, then use this model to suggest an optimal configuration\u201d. Approaches that follow this thread, namely model-based tuners, can be aligned with"}, {"title": "1.1 Contributions", "content": "To better understand the above doubt, in this paper, we conduct a systematic, large-scale empirical study that covers 10 models, 17 tuners, and 29 systems from the existing works while under four different metrics, leading to 13,612 cases of investigation. Our findings are surprising and even counter-intuitive, from which the key and most unexpected observation is probably the accuracy can lie, suggesting that the current practice that relies on the \u201caccuracy is all\" belief is likely to be misleading. Therefore, the results conjecture that we should take one step back from using accuracy as the key driver of model-based configuration tuning. Specifically, we make several contributions to the community as follows."}, {"title": "1.1.1 New Findings", "content": "We reveal findings that have not been previously explored in a systematic way:\n1) Compared with the model-free counterparts under a sufficient and fair budget, models are helpful for sequential model-based tuners with up to 72% cases and a maximum of 5\u00d7 tuning improvements, but they tend to have marginal impacts or can even be harmful to the tuning quality of batch model-based tuners.\n2) The originally chosen model in sequential model-based tuners can be considerably improved in 46% of the cases by simply switching to the other \"newly created\" tuner-model pairs; while for 50% cases they have marginal difference.\n3) The most/least accurate model can only serve as an indication of the best/worst tuning quality in between 14% and 45% of the cases.\n4) The correlations between model accuracy and the goodness of tuning are far from being positively strong as implied in the belief: up to 97% of the cases, they are mostly negligible, or sometimes, even negative, i.e., worse accuracy can lead to better tuning results.\n5) The necessary accuracy change to create significant tuning improvement varies depending on the range of the model's accuracy, e.g., for the model with a MAPE falls in [30,40], it needs at least 13.3% improvement for having considerably better tuning performance. Notably, the better the accuracy, the smaller the accuracy change is needed to considerably enhance the tuning."}, {"title": "1.1.2 New Interpretations", "content": "By means of representative examples, we demonstrate a way to explain the reasons behind the most unexpected results from our empirical study, using the concepts/metrics from fitness landscape analysis [75]. In particular, such an analysis provides new interpretations of the following questions observed from the results of the empirical study:\n1) Why is the model useful (useless) to the tuning quality?\n2) Why better model accuracy does not always lead to superior tuning quality?\n3) Why does a better accuracy need a smaller change to significantly influence the tuning?"}, {"title": "1.1.3 New Insights and Opportunities", "content": "Deriving from the observations and analysis, we summarize a few lessons learned that provide insights for future research opportunities in the field:\n1) While models help to significantly reduce overhead, they are only useful for tuning quality under progressive updates when the budget is sufficient and fair, hence exploring efficient online model learning is a promising research direction.\n2) Manually selecting a model beforehand makes it difficult to find the optimal choice for tuning, hence the combination of model-tuner pair selection and tuning the configuration can form a new bi-level optimization problem that demands automation.\n3) The community should shift away from the accuracy-driven research for model-based configuration tuning. A new question would be: what other (efficient) proxies, alongside accuracy, can better measure the model usefulness for tuning quality. Further, models should incorporate code patterns that cause the landscape sparsity and ruggedness.\n4) It makes little sense to claim the benefit of a certain % of accuracy improvement alone; at least, one should refer to the minimum accuracy changes discovered in this work. This calls for a more systematic procedure to examine and interpret the meaningfulness of the change in model accuracy."}, {"title": "1.2 Open Science and Organization", "content": "To promote open science practices, all source code, data, and supplementary materials of this work can be publicly accessed at our repository: https://github.com/ ideas-labo/model-impact.\nThe rest of this paper is organized as follows. Section 2 introduces the necessary preliminaries and background of this work. Section 3 describes our empirical research methodology. Section 4 presents the results with a detailed analysis. Section 5 discusses the possible rationale behind the findings, followed by a summary of the lessons learned and future opportunities in Section 6. Sections 7 and 8 present the threats to validity and related work, respectively. Finally, Section 9 concludes the paper with pointers for future work."}, {"title": "2 PRELIMINARIES AND BACKGROUND", "content": "In this section, we discuss the necessary preliminaries and backgrounds that underpin our empirical study."}, {"title": "2.1 Software Configuration Tuning", "content": "A configurable software system often comes with a set of critical configuration options to tune, for example, STORM allows one to change the num_splitters and num_counters for better latency or throughput [19], [70]. Ci denotes the ith option, which can be either a binary or integer variable, among n options for a software system. The goal of configuration tuning is to search for better software configurations, from the space of C and with a given budget, that optimizes the performance objective f under the given benchmark condition\u00b2:\nargmin f(c), \u0441\u2208\u0421\nwhere c = (C1, C2, ..., Cn). The measurement of f depends on the target system and the performance attribute, for which we make no assumption about the characteristics in this work.\nA key difficulty of tuning configuration is the expensive measurement: Zuluaga et al. [109] report that it takes hours to merely measure a fraction of the configuration space. As a result, exhaustively profiling the system when tuning the configuration is often unrealistic, if not impossible."}, {"title": "2.2 Model-free Tuners", "content": "Model-free tuners represent the most straightforward way to address the problem of software configuration tuning as shown in Figure 2a: designing intelligent heuristics, guided by direct measurement from the systems, to explore the configuration space and exploit the information to generate new directions of search [10], [56], [63], [108]. The configuration with the best-measured performance is used. The key focus has been on controlling the behavior of the tuners in exploring and exploiting the search space without having to traverse all of it. For example, works exist that leverage local search that discovers configurations around the best ones found so far [108]. Other tuners [56] emphasize jumping out of local optima\u2014an area in the configuration space where there are one (or more) configurations being optimal around the neighboring ones, but are sub-optimal with respect to the globally best configuration.\nSince the model-free tuners have relied on direct measurement of the systems, which provides the most accurate guidance in the tuning, it has been reported that it needs a substantial amount of trial-and-error before converging to some promising configurations [10], [11]. This can be undesirable when the measurement of configurations is rather expensive."}, {"title": "2.3 Model-based Tuners", "content": "Given the fact that measuring even a single configuration can be expensive, a natural resolution is to use a surrogate model that delegates the systems in the tuning process. In the past decade, there have been numerous studies that propose more and more accurate configuration performance models, primarily leveraging deep/machine learning algorithms, such as decision tree [39], random forest [62], deep neural network [34], [40], and hierarchical interaction neural network [24]. Those models, if used appropriately, can be paired with different tuners, thereby significantly reducing the cost of evaluating a configuration as the better or worse between configurations can be compared directly using model prediction.\nBroadly, there are two types of model-based tuners, namely batch model-based tuners and sequential model-based tuners, for which we elaborate below."}, {"title": "2.3.1 Batch Model-based Tuners", "content": "The batch model-based tuners [2], [5], [18], [23], [60], [79], [101] are inherent extensions from the model-free tuners, where the direct system measurements are replaced by model evaluations; the tuner itself remains unchanged. The optimal configuration with the best performance predicted by the model is then returned. This provides several advantages, for example, the newly proposed surrogate model can be paired with arbitrarily model-free tuners, in which the designed behaviors of the tuning algorithms can be preserved while an accurate model can be directly exploited. There is often no necessary inter-dependency between the model and tuner, since naturally model-free tuners rarely make assumptions about the internal structure of the system to be measured, providing a perfect foundation for it to be replaced by a model. However, the only additional step is that a high-quality model needs to be trained/built in advance with a good amount of measured configuration samples in order to effectively guide the tuning.\nModel evaluation is almost certainly cheaper than system measurement, the actual saving depends on the training data size, the model, and the system though. For example, even in some of the worst cases, a model evaluation merely takes half a minute [81] as opposed to hours that are needed to measure a configuration on systems like MARIADB [22]. Albeit the evaluation of configuration becomes much cheaper in batch model-based tuners, it might still be unrealistic to exhaustively traverse all configurations. As we will show in Section 3.2, the search space of a system can go beyond a million, rendering the problem intractable even in the presence of a surrogate model. Yet, batch model-based tuners can still benefit from the sophistically designed heuristics of the model-free tuners during the tuning process, hence allowing them to deal with the intractability. Indeed, even with the reduced evaluation overhead, says 30 seconds per evaluation, heuristics that require 100 evaluations to obtain a promising configuration would remain"}, {"title": "2.3.2 Sequential Model-based Tuners", "content": "The sequential model-based tuners [11], [43], [70], [88], [92], [104], in contrast, also rely on a surrogate model to determine the better or worse configurations while enabling a cheap exploration of the configuration space. However, after an initial model is trained/built with limited samples, it additionally permits new measurements of configuration on the system and updates the model using every new sample as the tuning proceeds, creating influence in guiding the current tuning run. The optimal configuration returned in the end is the one with the best-measured performance.\nIn general, work on sequential model-based tuners often relies on a variant of Bayesian Optimization [31], where the heuristic is guided by an acquisition function (e.g., Expected Improvement) that leverages the model prediction to identify the configuration to be measured next, which is most likely to improve performance while being uncertain enough to train and improve the model accuracy.\nIndeed, the nature of sequential model-based tuners (and the acquisition function) might introduce a dependency between the model and the tuner, i.e., not all the models can quantify the uncertainty of the configurations, thereby models might not be compatible without some amendments. However, existing work has successfully exploited different surrogate models in sequential model-based tuners beyond the defaulted Gaussian Process Regression. For example Flash [70] has been using a decision tree while BOCA [11] has relied on random forest."}, {"title": "2.4 The \"Accuracy is All\" Belief", "content": "Since the surrogate model serves as the delegate of the actual system that guides the tuner, it is natural to believe that better accuracy emulates the real system better, and hence should lead to superior tuning results [92], [101], [108]. In addition to the examples and quotations mentioned in Section 1, there is an increasingly active research field, namely configuration performance learning [35], [74], that proposes sophisticated models with the main purpose of improving their accuracy. For example, to date, when using the common MAPE as the metric\u00b3, the prediction accuracy for system VP9 has been pushed up to 0.44% by a state-of-the-art model HINNPerf [24] published at TOSEM'23, which is statistically better than the 0.86% MAPE of the other already rather accurate model DECART [39]. Yu et al. [101] also claim that since the proposed model can improve the accuracy by up to 22.4%, hence it should be more useful than the others for tuning configuration. Therefore, all of those imply a general belief:\n\"Accuracy is All\" Belief\n\"Regardless of how the model is applied for configuration tuning, the higher the model accuracy, the more useful it becomes for improving the tuning quality and vice versa.\u201d\nAs such, it is not hard to anticipate that future research and design choices on configuration tuning (and configuration performance modeling) will still be strongly driven by the model accuracy when a surrogate model is involved. Yet, our experience and preliminary results (as discussed in Section 1) question this practice, e.g., is the % MAPE improvement in the aforementioned case really that important for configuration tuning? In what follows, through a large-scale empirical study, this work challenges the above belief and provides an in-depth understanding of the role of the surrogate model and its accuracy for configuration tuning."}, {"title": "3 RESEARCH METHODOLOGY", "content": "We now delineate the methodology of our empirical study."}, {"title": "3.1 Research Questions", "content": "To provide a more in-depth understanding of the aforementioned \"accuracy is all\" belief, in this paper, we answer several important research questions (RQs). In particular, we seek to first confirm the benefit and usefulness of using a model for configuration tuning against the model-free counterparts by examining:\nRQ1: How useful is the model for tuning quality?\nIndeed, some work did compare a small set of model-based tuners with selected model-free counterparts to showcase the benefits of models [11], but their scale is rather limited and they are often based on a biased budget, e.g., only tens of measurements are considered, which would be more beneficial to model-based tuners.\nSince the sequential model-based tuners are commonly paired with a specifically chosen/designed model, one would expect that such a model should help the most in tuning quality compared with the alternatives for the majority of the cases. Subsequently, it is natural to ask:\nRQ2: Do the chosen models work the best on tuning quality?\nBoth RQ1 and RQ2 provide a more thorough high-level understanding of the necessity and benefit of using a model to tune configuration. However, it remains unclear what role the model accuracy plays in terms of the tuning quality. Often, one would be interested in whether the most (least) accurate model can lead to the best (worst) tuning results, hence the next question we seek to answer is:\nRQ3: Dose the goodness of the model consistent with the resulted tuning quality?\nIf the \"accuracy is all\" belief is correct, the normal intuition is that the model accuracy should be a good indication of the tuning quality for most cases.\nAs a next step, an extended question that encourages finer-grained investigations therein would be:"}, {"title": "3.2 Configurable Systems", "content": "We select the software systems and their benchmarks used by the most notable works from the key venues in software and system engineering. After extensively surveying the systems, we conduct a screening of them following two criteria:\nTo balance the generality of our study and the realism of the efforts required, we select the system with the most number of options as the representative for systems from the same origin, e.g., BDB_C and BDB_J are both the variants of the Berkeley database system written in C and Java, respectively. In that case, only BDB_C is used as it has more configuration options.\nFor the same systems where different sets of relevant configuration options have been used, we select the one with the highest number of options. For example, STORM is a configurable system that has been studied in many prior studies, and we use the instance with 12 options to tune which is the most complicated case.\nTable 1 shows the final set of 29 configurable systems considered in this study. It is clear that those systems come from various domains with diverse performance attributes that are of concern; can exhibit only 3 options in the tuning (BROTLI) or can be as complex as having up to 59 options to tune (SAC). The search space also varies, ranging from 308 to 2.67 \u00d7 1041. It is worth noting that, even for those with relatively small search space, the measurement of a single configuration can still be time-consuming. For instance, it can take up to 166 minutes to measure one configuration on MARIADB [22], and a single run of measurement for DEEPARCH might even be a couple of hours [48].\nWhen tuning under a surrogate model, we pragmatically consider those systems with less than a million configurations to search as \u201csmall\u201d systems, because the space can be reasonably covered when the measurements are replaced by model evaluations. Otherwise, we treat the systems as \u201clarge\u201d ones, i.e., they tend to be intractable as exhaustively traversing the entire space is expensive even with a cheaply evaluated model."}, {"title": "3.2.2 Option, Benchmark and Performance Selection", "content": "The configuration options, their possible values, workloads/benchmarks, and target performance metric have been carefully selected as exactly the same as those studied in previous works of the corresponding systems [7], [41], [47], [55], [67], [71], [73], [96]. As such, in this regard, we did not use our own selection criteria but followed what has been widely acknowledged in the community.\nNotably, it is well-known that not all options might significantly contribute to the performance\u2014this is known as the sparsity problem in configuration performance learning [34], [40]. Practically, it would be difficult to obtain a thorough understanding beforehand regarding which options are more significant to the performance (especially given the expensive measurements). In fact, some configuration performance models have been proposed to tackle exactly such (e.g., DeepPerf [40] and DaL [34]), such that they are capable of learning a model where only those more influential options would contribute to the performance prediction while those less relevant ones would have minimal impact (via, e.g., regularization or data division). Some other models, such as SPLConqueror [80], would be more vulnerable to the sparsity problem but we regard this as a natural limitation of the proposed model rather than a problem that can be addressed fundamentally.\nThe strategy of measuring the performance follows exactly the same setting as previous work [7], [41], [47], [55], [67], [71], [73], [96]. For example, when tuning SQLITE, the workloads are generated by SYSBENCH, which is a standard benchmark for testing performance. Similarly, the performance is also measured by SYSBENCH as a final overall result after processing some workloads. Further, the system is rebooted when measuring different configurations, ensuring that none of them would benefit from the cache. The measurements are also taken as the average/median of 3 to 23 repeats depending on the specific system as used in prior work [7], [41], [47], [55], [67], [71], [73], [96]. To expedite the experiments and verify accuracy, we also use the same configuration datasets measured in previous research."}, {"title": "3.3 Surrogate Models and Configuration Tuners", "content": "Table 2 shows the 10 surrogate models we considered in this work. Those models are either commonly used by model-based tuners [11], [18], [43], [60], [70], [88], [92], [104], e.g., GP and DT, or are state-of-the-art ones from the field of configuration performance learning [24], [34], [39], [40], [80], e.g., DeepPerf and HINNPerf, covering the key venues from both software engineering and system engineering communities. We also cater to the diverse nature of the models, as they either belong to statistical machine learning or deep learning; and distinct application domains, since they could be designed for general purpose, e.g., RF, or are specifically tailored to handle the characteristics of configuration data, e.g., DaL [34].\nWhile we aim to cover the most common and state-of-the-art models, some of them are intentionally omitted due to their various restrictions and inflexibility:\nWe rule out KNN since it is a lazy model, i.e., the overhead occurs at the prediction rather than training, which causes unacceptable long running time when paired with some tuners that leverage excessive model evaluations, e.g., FLASH. This is because, in our experiments, the prediction happens much more often than the training.\nPerf-AL, a deep learning model specifically designed to learn configuration performance, is also omitted due to its prohibitive training time required for training the adversarial neural network. Besides, hyperparameter tuning further complicates the process and the default values suggested by the authors did not lead to any useful models on our systems.\nWe remove KRR due to its lengthy training process and high similarity to SVR, which is part of the model considered.\nIt is, however, worth noting that our set of models is comparable to those considered in recent empirical studies on modeling configuration performance [13], [33], [46], [64]."}, {"title": "3.3.2 Tuners", "content": "We consider up to 17 tuners in our study, including 8 sequential model-based ones, 8 model-free/batch model-based ones, and one that is specifically used as a batch model-based tuner (Brute-force) when the configuration space is tractable, as shown in Table 3.\nTo ensure good coverage, we select the most notable work from software engineering (e.g., FLASH [70] and SWAY [10]), system engineering (e.g., OtterTune [88] and Tuneful [29]), general parameter optimization (e.g., SMAC [43] and ParamILS [44]), and commonly used baselines, such as Random and Brute-force. Those tuners could aim for any configurable systems/parameter optimization in general or they might be originally designed for a particular type of system, such as database systems or big data systems, but are generic enough to work on a configurable system of any type without substantial extension. They also come with a diverse set of underlying designs, containing different option reduction methods, search heuristics, and acquisition functions (for sequential model-based tuners). While most batch model-based tuners can find a model-free counterpart, Brute-force is the only exception. This is because even though the system might be tractable, the expensive measurements prevent any tuner from covering the entire space. However, with the help of a surrogate model, traversing the space might become plausible. As a result, for those \u201csmall\u201d systems (recall Section 3.2), we do not need sophisticated heuristics but only the Brute-force tuner is considered.\nHowever, like the selection of surrogate models, we have to omit certain tuners for different reasons:\nWe rule out SCOPE [53] and OnlineTune [105] because they are specifically designed to ensure the safety of a configurable system\u2014a property that is beyond the scope of this work.\nWe do not consider HyperBand [59] and its extension BOHB [28]. This is because they assume significantly different fidelity/cost when measuring a hyperparameter, which is a property that not all configurable systems would have.\nTuners that assume multiple performance objectives [23] or leverage on multi-objectivization for a single performance objective [12], [14], [19], [22] are also omitted since not all systems studied have more than one meaningful performance objective.\nThere are omissions due to the lack of publicly available artifacts and the difficulty of re-implementing the tuners. For example, we do not utilize LOCAT [98] due to the absence of public source code and we rule out LlamaTune [50] since its source code is out of date, which compromises the usability.\nDespite the above omissions, to the best of our knowledge, the selected ones remain leading to the largest set of tuners considered in the field of configuration tuning. For all parameters of the tuners, we set those exactly the same as used in their corresponding work.\nWe also have to slightly improve a few tuners to enable them to work effectively on all systems considered: since FLASH uses exhaustive search on the model to find the next configuration to measure without any option/space reduction, it might not be scalable to all \"large\" systems. To tackle this, for each system, we cap the exhaustive search therein with the maximum number of configurations that is commonly explored in the literature [12]. Certain tuners, e.g., ResTune, ATConf, ROBOTune and Tuneful, are designed for continuous variables only while most of the configurable systems have discrete options. To deal with this incompatibility, whenever the tuner needs to measure an invalid configuration that does not meet the possible values of the options, we measure the performance of the most similar valid configuration, which is quantified by using normalized Euclidean distance, as its performance value. In other words, we make the shape that covers a valid configuration and its neighboring invalid configuration flat in the configuration landscape (as they have the same performance value), hence helping to prevent the tuner from being trapped at local optima. Yet, if one of those invalid configurations happens to be the best one returned by the end of tuning, then they are converted back into its most similar valid configuration. In this way, we do not need to largely change those tuners while allowing them to work effectively on the systems considered."}, {"title": "3.3.3 Pairing Model-based Tuners with Models", "content": "Due to the nature of model-free tuners, they can be naturally paired with arbitrary surrogate models, as long as those models are trained with a batch of configuration samples in advance. As mentioned, this forms the foundation of batch model-based tuners, where the tuning is guided by the model rather than real measurement of the systems as in the case of its model-free counterpart, but the tuner and search itself are the same. Since a model is trained in advance, all batch model-based tuners share the said model under each system.\nWe can do the same arbitrary model pairing for the sequential model-based tuners, but the setting becomes slightly more complicated because they also need to update the model sequentially as new measurement(s) of configurations, which are decided by the tuner, become available. In particular, a key part of this type of model-based tuner is the acquisition function that determines which next configuration to measure. In essence, most sequential model-based tuners (e.g., BOCA [11] and OtterTune [88]) leverage acquisition function to balance the exploitation and exploration of the tuners via the performance and uncertainty of the configurations predicted by a surrogate model, respectively. Exploitation focuses on the better configuration as predicted while exploration emphasizes the most unknown configurations inferred, which might not be better but could improve the surrogate model and allow the tuner to jump into new regions in the configuration landscape. All surrogate models would be able to predict the performance of a configuration, but not every one of them can quantify its uncertainty, except GP and RF."}, {"title": "3.4 Sample Sizes and Budgets", "content": "3.4.1 Hot-start Sample Size for Sequential Model-based Tuners\nWhile the sequential model-based tuners can conduct a cold start with a surrogate model learned under the one-shot setting, i.e., the surrogate model is directly used by a tuner without pre-training, they impose a high risk of measuring unreliable configurations at the beginning, which negatively affects the outcome. Therefore, those tuners often follow a hot-start manner where the surrogate model is well-trained with a certain number of measured configuration samples [11], [43], [70]. A larger hot-start sample size means that more configurations are randomly measured initially, allowing for a more precise initialization of the model and, consequently, being more favorable for finding new configurations. However, an increased sample size also comes with higher measurement costs. In this work, we turn to the literature of the eight sequential model-based tuners considered for identifying a common practice of setting the hot-start sample size, we found that existing work generally uses a fixed size across the systems, but the number can vary, e.g., two samples for BOCA [11] and five samples for ResTune [104]. To ensure a reasonable balance between building a reliable initial model and the additional measurement cost, we set a hot-start size of 20 as suggested previously, which is also the most commonly used setting we found among the sequential model-based tuners [29], [52], [70]. The data is then collected via conducting random sampling on the measured dataset."}, {"title": "3.4.2 Training Sample Size for Batch Model-based Tuners", "content": "As mentioned, the batch model-based tuners require the surrogate model to be trained in advance with measured configurations. In particular, since the model would not be updated in the same configuration cycle again, the size required is often larger than that of the sequential model-based tuner. To identify the training set from the measured dataset, we follow random sampling to obtain the training samples, together with the most common method for determining the corresponding size as below [34], [40]:\nBinary systems: for binary system, we randomly sample 5n configurations, where n is the dimension of options. Note that 5n is often the largest size used from prior work [34], [40].\nMixed systems: for systems that come with both binary and numeric options, we use the sampling strategy from SPLConqueror [80] to determine the size with the parameters that lead to the largest number as considered in existing work [34], [40].\nThe system-specific training sample sizes are shown in Table 1. The above serves as the standard method to provide sufficient training data for building a sufficiently good model beforehand."}, {"title": "3.4.3 Tuning Budget", "content": "In this work, we measure the tuning budget using the number of measurements as suggested by many prior works [19], [22], [45], [70] over 60% of the tuners studied have suggested such. In particular, the number of measurements comes with several advantages:\nIt eliminates the interference of clock time caused by the running software system under tuning at the same machine.\nThe measurements of configuration are often the most expensive part throughout configuration tuning.\nIt is independent of the underlying implementation details such as programming language and the version of libraries.\nSince the ideal budget can differ depending on the tuner, model, and system, we seek to find a budget that ensures our study is conducted fairly where all tuners would have the chance to reach their best state. To that end, under each system, we perform pilot experiments as follows:\n1) Run all sequential model-based tuners (which are equipped with their original surrogate models) and model-free tuners. We do not pair the sequential model-based tuners with other models since the heuristics in tuning the acquisition function remain unchanged regardless of the model. We also omit the batch model-based ones as their search/optimization behavior is identical to those of model-free"}, {"title": "3.5 Metrics", "content": "To answer our RQs", "metrics": "one for measuring the accuracy of the"}]}