{"title": "NeuralCFD: Deep Learning on High-Fidelity Automotive Aerodynamics Simulations", "authors": ["Maurits Bleeker", "Matthias Dorfer", "Tobias Kronlachner", "Reinhard Sonnleitner", "Benedikt Alkin", "Johannes Brandstetter"], "abstract": "Recent advancements in neural operator learning are paving the way for transformative innovations in fields such as automotive aerodynamics. However, key challenges must be overcome before neural network-based simulation surrogates can be implemented at an industry scale. First, surrogates must become scalable to large surface and volume meshes, especially when using raw geometry inputs only, i.e., without relying on the simulation mesh. Second, surrogates must be trainable with a limited number of high-fidelity numerical simulation samples while still reaching the required performance levels. To this end, we introduce Geometry-preserving Universal Physics Transformer (GP-UPT), which separates geometry encoding and physics predictions, ensuring flexibility with respect to geometry representations and surface sampling strategies. GP-UPT enables independent scaling of the respective parts of the model according to practical requirements, offering scalable solutions to open challenges. GP-UPT circumvents the creation of high-quality simulation meshes, enables accurate 3D velocity field predictions at 20 million mesh cells, and excels in transfer learning from low-fidelity to high-fidelity simulation datasets, requiring less than half of the high-fidelity data to match the performance of models trained from scratch.", "sections": [{"title": "Introduction", "content": "Computational fluid dynamics (CFD) is central to automotive aerodynamics, offering in-depth analysis of entire flow fields, and complementing wind tunnels by simulating open-road conditions. The fundamental basis of almost all CFD simulations is the Navier-Stokes (NS) equations, describing the motion of viscous fluid substances around objects. However, the computational cost of solving the NS equations necessitates modeling approximations, most notably regarding the onset and effects of turbulence. Therefore, CFD employs different turbulence modeling strategies, balancing accuracy and cost. In this context, two seminal datasets, DrivAerNet and DrivAerML , have been released, allowing for in-depth study of deep learning surrogates for automotive aerodynamics. DrivAerNet runs CFD simulations on 8 to 16 million volumetric mesh cells with low-fidelity Reynolds-Averaged Navier-Stokes (RANS) methods, whereas DrivAerML runs CFD simulations on 160 million volumetric cells with Hybrid RANS-LES (HRLES) , which is the highest-fidelity CFD approach routinely deployed by the automotive industry. \nIn recent years, deep neural network-based surrogates have emerged as a computationally efficient alternative in science and engineering, impacting e.g., weather forecasting, protein folding , or material design. In automotive aerodynamics, however, key challenges must be overcome before deep neural network-based surrogates can be implemented at an industry scale:\n(I) Surrogates must be able to deliver accurate predictions and be scalable to large surface and volume meshes, ideally taking raw geometries as inputs, i.e., without relying on the CFD simulation meshing procedure.\n(II) Surrogates must be capable of achieving the required performance levels while being trainable with a limited number of samples, as ground-truth numerical simulation datasets are both scarce and costly to generate.\nIn order to address these challenges, we introduce Geometry-preserving Universal Physics Transformer (GP-UPT), the first neural operator designed to provide scalable solutions for high-fidelity aerodynamics simulations. GP-UPT separates geometry encoding and physics predictions, ensuring flexibility with respect to the geometry representations and surface sampling strategies. It builds on the Universal Physics Transformer (UPT) framework, which operates without grid- or particle-based latent structures, enabling flexibility and scalability across meshes and particles. GP-UPT extends this framework by:\n(i) preserving geometry information when encoding, and\n(ii) guiding output predictions to conform to the input geometry. GP-UPT enables independent scaling of the respective model parts (e.g., encoder or decoder) according to the practical requirements. Qualitatively, GP-UPT demonstrates favorable performance and scaling compared to state-of-the-art neural operators, offering a clear solution to open challenges of automotive aerodynamics. Key highlights include:\n(i) converging model outputs for different input sampling patterns; (ii) achieving the first near-perfect accuracy in drag and lift coefficient predictions relative to numerical CFD simulations, where predictions across the entire surface meshes of DrivAerML (8.8 million surface CFD mesh cells) are obtained within seconds on a single GPU; (iii) attaining accurate 3D velocity field predictions at 20 million mesh cells, even when only the geometry representation is input to the model; (iv) establishing a low-fidelity to high-fidelity simulation transfer learning approach, requiring only half of the high-fidelity data to match the performance of models trained from scratch."}, {"title": "Preliminaries", "content": "2.1 CFD for automotive aerodynamics\nComputational fluid dynamics. Automotive aerodynamics is centered around computational fluid dynamics (CFD), which is deeply connected to solving the Navier-Stokes (NS) equations. For automotive aerodynamics simulations, the assumptions of incompressible fluids due to low local Mach numbers, i.e., low ratio of flow velocity to the speed of sound, are justified. Thus, the simplified incompressible form of the NS equations are applicable, which conserve momentum and mass of the flow field u(t, x, y, z) : [0, T] \u00d7 R\u00b3 \u2192 R\u00b3 via:\n$\\frac{\\partial u}{\\partial t} = -u \\cdot \\nabla u + \\mu \\nabla^2 u - \\nabla p + f, \\nabla \\cdot u = 0 $.(1)\nTo compute a numerical solution, it is essential to discretize the computational domain. In CFD, the finite volume method (FVM) is one of the most widely used discretization techniques. FVM partitions the computational domain into discrete control volumes using a structured or unstructured mesh. The initial geometric representation, typically provided as a computer-aided design (CAD) model in formats such as STL, must be transformed into a simulation mesh. This meshing process precisely defines the simulation domain, allowing the representation of complex flow conditions, such as those in wind tunnel configurations or open-street environments\nTurbulence modeling in CFD. Turbulence arises when the convective forces u \u00b7 \u2207u dominate over viscous forces \u03bc\u2207\u00b2u, typically quantified by the Reynolds number. Turbulent flows are characterized by a wide range of vortices across scales, with energy cascading from larger structures to smaller ones until viscous dissipation converts it into thermal energy at the Kolmogorov length scale. Although direct numeric simulation (DNS) can theoretically resolve the turbulent flow field by directly solving the NS equations, it requires capturing all scales of motion down to the Kolmogorov scale. This implies extremely high requirements on the discretization mesh, which results in infeasible compute costs for full industrial cases.\nTherefore, engineering applications rely on turbulence modeling approaches that balance accuracy and computational efficiency. RANS and Large-Eddy Simulations (LES) are two methods for modeling turbulent flows, each with distinct characteristics. RANS decomposes flow variables into mean and fluctuating components and solves the time-averaged equations, using turbulence models like k-e to account for unresolved fluctuations. While computationally efficient, RANS may lack accuracy in capturing complex or unsteady flows, particularly in cases involving flow separation, where turbulence models are often less effective. In contrast, LES resolves eddies down to a cut-off length and models sub-grid scale effects and their impact on the larger scales. LES offers higher accuracy in capturing unsteady behavior and separation phenomena at the cost of more compute. In cases where LES is too costly, hybrid models like, Hybrid RANS-LES (HRLES) models are an alternative. These models reduce computational demand by using LES away from boundary layers, and RANS near surfaces, where sufficiently resolving the flow in LES would require very high resolution. The DrivAerNet dataset runs CFD simulations on 8 to 16 million volumetric mesh cells with low-fidelity RANS methods. On the other hand, the DrivAerML dataset utilizes a HRLES turbulence model and runs CFD simulations on 160 million volumetric cells.\nQuantities of interest. Interesting quantities for automotive aerodynamics comprise quantities on the surface of the car, in the volume around the car, as well as integral quantities such as drag and lift coefficient. The force acting on an object in an airflow is given by\n$F = \\oint_{S} (p - p_0)n + \\tau_w dS ,$(2)\nwith the aerodynamic contribution, consisting of surface pressure p and pressure far away from the surface p0 times surface normals n, and the surface friction contribution \u03c4\u03c9. For comparability between designs, dimensionless numbers as drag and lift coefficients\n$C_d = \\frac{2F_x}{\\rho v^2 A_{ref}}, C_l = \\frac{2F_y}{\\rho v^2 A_{ref}}$(3)\nare used , where eflow is a unit vector into the free stream direction, elift a unit vector into the lift direction perpendicular to the free stream direction, \u03c1 the density, v the free stream velocity, and Aref a characteristic reference area. Predicting these surface integrals allows for an efficient estimation when using deep learning surrogates, since these surrogates can directly predict the surface values without the need to model the full 3D volume field, as required by numerical CFD simulations."}, {"title": "Transformer blocks for building neural operators", "content": "Neural operators are formulated with the aim of learning a mapping between function spaces, usually defined as Banach spaces I, O of input and output functions defined on compact input and output domains X and Y, respectively. Neural operators enable continuous outputs that remain consistent across varying input sampling resolutions. A neural operator \u0177 : I \u2192 O approximates the ground truth operator G : I \u2192 O, and is often composed of three maps \u0177 := D \u25e6 A \u25e6 E, comprising encoder &, approximator A, and decoder D. Training a neural operator involves constructing a dataset of input-output function pairs evaluated at discrete spatial locations.\nSelf-attention and cross-attention. Scaled dot-product attention is defined upon three sets of vectors {qi}, {ki}, {vi}, written in matrix representation as Z = softmax(QKT/\u221ad)V, where zi, qi, ki, vi are the i-th row vectors of the matrices Z, Q, K, V, respectively, and d is the hidden dimension of the row vectors. Due to the row-wise application of the softmax operation, the multiplication QKT must be evaluated explicitly, resulting in an overall complexity of O(n\u00b2d), which is prohibitively expensive when applying to a large number of tokens n. In self-attention, the i-th row vector of Q, K, V is viewed as the latent embedding of a token, e.g., a word. Cao (2021) proposes that each column in Q, K, V can be interpreted as the evaluation of a learned basis function at each point. E.g., Vij can be viewed as the evaluation of the j-th basis function on the i-th grid point xi, i.e., Vij = vj(xi). In contrast to self-attention, in cross-attention, the query matrix Q is encoded from a different input. Cross-attention is probably most prominently used in perceiver-style architectures , where inputs with N tokens are projected into a fixed-dimensional latent bottleneck of M tokens (M < N), before processing it via self-attention.\nNeural field output decoding via cross-attention. Following the basis function interpretation of , when choosing the i-th row vector qi in the query matrix Q to be an encoding of query point yi, we can query at arbitrary locations which are independent of the input grid points . The number of rows m in the query matrix Q corresponds to the number of output query points. Concretely, the N latent tokens which are projected into the key and value matrices K and V, result in m output tokens. The case m = 1 yields point-wise decoding. It is to note that if no self-attention operation between query points is applied, the decoding happens point-wise, and is independent of the number of query points. I.e., the decoded output value at coordinate yi is independent of the number of points used for decoding. Such a decoding scheme can be seen as a neural field"}, {"title": "Geometry-preserving UPT", "content": "We start with a simple observation: in the DrivAerNet++ paper, state that 27 design parameters are enough to specify a wide range of conventional car geometries. However, the output surface/volumetric meshes often correspond to (hundreds of) millions of mesh cells. In other words, inputs are rather simple, but outputs are diverse and complicated, and require huge meshes. Given this, we formulate the following model requirements:\n(I) Reduced latent space modeling. The potentially vast number of output mesh cells requires an encoder-approximator-decoder modeling paradigm, where the decoder often functions as point-wise conditional neural fields. Such approaches are introduced in AROMA , CViT , Knigge , and UPT. Moreover, UPT additionally introduces a patch-embedding analogue for general geometries via supernode pooling. Alternatively, Transolver implements a sliced and efficient attention.\n(II) Decoupling of encoder and decoder. For various tasks, e.g., predicting 3D flow fields directly from geometry inputs, a decoupling of the encoder/approximator and decoder is favorable (see e.g., UPT, OFormer , Geometry Informed Neural Operator (GINO). Additionally, a decoupled latent space representation allows for scalable decoding to a large number of output mesh cells since such models can cache encoded latents states and decode output queries in parallel. Finally, decoupling geometry-mesh encodings and model outputs is a fundamental requirement for transfer learning, especially when input geometry and output mesh get decoupled, see transfer learning experiments in Section 4.\n(III) Geometry-aware latent space. In general, this requirement is only fulfilled for models that map input points directly to output predictions. Such methods comprise models such as Reg-DGCNN , PointNet and Transformer models, such as Transolver , FactFormer , or ONO . Preserving geometry-awareness and decoupling encoder and decoder are somewhat orthogonal requirements, yet important to build scalable and generalizable models. Recent attempts map locations and physics quantities to an embedding via cross-attention , or update query points using a heterogeneous normalized cross-attention layer . Methods like GINO can also be seen as geometry-aware due to the regularly-structured latent space.\nGiven the methodological requirements stated above, we design GP-UPT as shown in Figure 2.\nGeometry-aware encoder. In our experiments, the input geometry Mi for data sample i is represented as a finite discretized point cloud, consisting of N points in a 3D space, i.e., XN \u2208 RN\u00d73. We embed the 3D coordinates using the transformer positional encoding . In this paper we do not use additional input features. However, it is possible to tie additional input features (e.g., surface normals, etc.) to the coordinates of the input representation. Following the approach of , a supernode pooling block S maps the input geometry M\u2081 into a set of supernode representations Si, which capture information within radius rsn of the supernode: S : XN \u2208 M\u00bfembed XN\u2208 Rk dhidden supernode pooling Si\u2208RS dhidden, where dhidden is the hidden dimensionality of the model's latent representations. First, we randomly select a subset of S supernodes, where typically S \u226a N, from the coordinates XN. Through a Graph Neural Operator (GNO) layer , we aggregate information from neighboring coordinates within a radius rsn. The supernode pooling block S fulfills requirement (I) by reducing the input tokens to the encoder from N to S. This reduces the workload of the quadratic self-attention layers, thereby simplifying the latent space modeling by decreasing computational cost and memory requirements.\nThe supernode representation S\u2081 = ZO \u2208 RS\u00d7dhidden is then passed as input to the geometry-aware encoder E, which consists of K encoding blocks. Each block consists of one (multi-head) cross-attention layer followed by a (multi-head) self-attention layer. The encoder maps the supernode representations Si in S latent token representations: E: Z\u2208 RS\u00d7dhidden geometry-aware encoding, Ze RS\u00d7dhidden. For each block j, the cross-attention layer uses the latent representation of the previous block Z3\u22121, as input for the query representation, i.e., Z3\u22121 \u2192 Q1, while the supernode representations S\u2081 are repeatedly used as input for the key and value representations, i.e., S\u2082 \u2192 K, V. This means that the inputs for the keys and values for each cross-attention layer are fixed across all {1, . . ., j} \u2208 K blocks, while the input to the self-attention transformer layer in each block is the output of the previous cross-attention layer. Our proposed geometry-preserving encoder & partly fulfills requirement (III). Since the cross-attention layers in each block attend to the original supernode representations Si, the encoder is able to maintain the latent representations Z"}, {"title": "Experiments", "content": "In our experiments, we assess the following aspects: (i) A benchmark comparison of GP-UPT to related models. (ii) Scalability to large surface meshes by comparing drag and lift coefficients on high-fidelity data for design optimization applications. (iii) Scalability to large volume meshes to understand design implications on the surrounding flow field. (iv) Transfer learning from low- to high-fidelity datasets. With experiments (ii) and (iii) we showcase breakthroughs regarding challenge (I) (scalability), and with experiment (iv) regarding challenge (II) (data scarcity).\n4.1 Benchmarking against other models (i)\nWe benchmark GP-UPT with a graph neural network (GNN) baseline model , a point-wise model"}, {"title": "Conclusion and outlook", "content": "This paper is motivated by two key challenges that need to be overcome for industry scale applicability of neural network-based CFD surrogate models. Following these challenges, we define architectural requirements for such models and introduce GP-UPT. By decoupling geometry encoding and physics predictions, GP-UPT ensures flexibility with respect to geometry representations and surface sampling strategies. Given the proposed architecture, we show that: (1) both, surface-level, as well as volume predictions, work out of the box without requiring any changes to the architecture. (2) Encoder - decoder - decoupling allows us to infer on arbitrary meshes circumventing the need for expensive CFD meshing while still producing accurate estimates for integral quantities such as drag and lift. (3) Transfer learning substantially reduces data requirements and additionally is beneficial for overall performance. In future work, we aim to transfer the general GP-UPT framework to different application domains and different underlying simulations beyond CFD and aerodynamics."}, {"title": "Impact Statement", "content": "Neural network-based simulation surrogates will play an important and potentially transformative role across many industries. Once trained, a surrogate model enables faster design-cycle times in iterative verification and optimization applications. The reduction of cycle times in turn allows to explore a broader design space, potentially yielding better and more efficient designs. In this paper, we focus on automotive aerodynamics, where surrogate-optimized designs can contribute to increasing the range of electrical vehicles, thereby having a direct, positive impact on carbon emission reduction. Simulation surrogate models can imitate numerical CFD simulations within a matter of a few seconds compared to several hours or days required by traditional methods. This implies that the surrogate models will also help to save compute (orders of magnitude) currently still invested into high-fidelity numerical simulations. We also want to emphasize that aerodynamics surrogate models, in particular, are a classic example of a dual-use technology that can be used for both, civilian as well as military applications. We want to explicitly state, that the latter is not our intention."}]}