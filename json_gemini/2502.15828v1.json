{"title": "A Stronger Mixture of Low-Rank Experts for Fine-Tuning Foundation Models", "authors": ["Mengyang Sun", "Yihao Wang", "Tao Feng", "Dan Zhang", "Yifan Zhu", "Jie Tang"], "abstract": "In order to streamline the fine-tuning of foundation models, Low-Rank Adapters (LoRAs) have been substantially adopted across various fields, including instruction tuning and domain adaptation. The underlying concept of LoRA involves decomposing a full-rank matrix into the product of two lower-rank matrices, which reduces storage consumption and accelerates the training process. Furthermore, to address the limited expressive capacity of LoRA, the Mixture-of-Expert (MoE) has been introduced for incorporating multiple LoRA adapters. The integration of LoRA experts leads to a visible improvement across several downstream scenes. However, the mixture of LoRAs (MoE-LORA) still exhibits its low robustness during tuning and inferring. Inspired by the Riemannian Preconditioners which train LoRA as a sub-space projector, we propose a new training strategy for MoE-LORA, to stabilize and boost its feature learning procedure by multi-space projections. Examinations on SGD and AdamW optimizers demonstrate the effectiveness of our methodology. Source code is available at https://github.com/THUDM/MOELORA_Riemannian.", "sections": [{"title": "1. Introduction", "content": "Parameter-Efficient Fine-Tuning (PEFT) techniques offer a cost-effective solution for fine-tuning foundation models (FMs) (Zhang et al., 2025a). Among these, Low-Rank Adaptation (LoRA) is a prevalent technology due to its versatility and simplicity. In detail, LoRA introduces trainable low-rank matrices A and B to update the internal modules of FMs, which is given by \\(X = W + BA\\). In a sense, their product serves as an approximation of the full-rank update for the pre-trained weights. While LoRA significantly reduces the number of trainable parameters, it also imposes two limitations: limited representation and gradient sub-optimality.\nLimitation 1: Limited representation. A natural problem of low-rank matrices lies in less powerful representation, especially in complex tasks. To tackle this, one straightforward solution is the integration of multiple LoRA modules into the mixture-of-expert framework, known as MoE-LORA. Figure 1 (left) illustrates a plain MoE-LORA framework. These efforts tangibly improved the performance of LORA in many scenarios, like vision-language tasks, multi-task learning, continual learning, etc. In a nutshell, the route of MoE-LORA can be roughly categorized into two lines: (i) Designing dedicated MoE-LORA frameworks for specific domains, such as MOELORA (Liu et al., 2023) and MOCLE (Gou et al., 2023). (ii) Technically improving MoE-LORA via architectural, updating, and loss constraints, such as MoLA (Gao et al., 2024) and HydraLoRA (Tian et al., 2024). Nevertheless, most of these efforts fail to consider the instability and inefficiency of training MoE-LORA.\nLimitation 2: Gradient Sub-optimality. Another concern that plagues LoRA is gradient sub-optimality. This occurs since the low-rank matrices A and B together form a quotient manifold space with a certain curvature, leading to an inconsistency between the inner-manifold optimal and the full-rank optimal gradient. This further leads to a sub-optimal training process for LoRA. To alleviate, Zhang et al. (Zhang & Pilanci, 2024) enhances LoRA gradients by a Riemannian gradient preconditioner, given by \\(\\nabla_A L = (B^T B)^{-1} \\nabla_A L\\) and \\(\\nabla_B L = \\nabla_B L(AA^T)^{-1}\\). These preconditioners contribute to constructing two gradient projectors after a mathematical derivation, ensuring the update is done in accord with the full-rank gradient projection onto the row space of A and the column space of B, that is \\(X_{new} = X - \\eta [Proj_{col(B)}(\\nabla_xL)^T + Proj_{row(A)}(\\nabla_xL)]\\).\nThrough a comprehensive analysis of Limitation 1 and Limitation 2, a natural question arises:\nInspired by MoE-LORA and the gradient preconditioning methods, a straightforward answer to this question is to integrate both approaches to simultaneously overcome the representative and sub-optimal limitations. Specifically, the"}, {"title": "2. Related Works", "content": "2.1. LoRA and LoRA Variants\nLORA (Hu et al., 2021) decomposes a full-rank matrix into a product of two low-rank matrices, which has been widely considered an effective solution for parameter-efficient fine-tuning. Studies have proposed several variants to reform LORA: For initialization, PISSA (Meng et al., 2024) leverages singular value decomposition (SVD) to obtain the principal singular components of W, while MiLoRA (Wang et al., 2024a) utilizes secondary singular values and vectors. LORA-Pro (Wang & Liang, 2024) and LoRA-GA (Wang et al., 2024c) approximate the direction of initial gradients to align them with that of the fully fine-tuning. LoRA+ (Hayou et al., 2024) introduces a learning rate separating strategy with \\(\\eta_B > \\eta_A\\). ResLoRA (Shi et al., 2024) and SIBO (Wen et al., 2024) accelerate convergence and mitigate over-smoothing by introducing residual paths. DoRA (Liu et al., 2024b) decomposes the weight vector into direction and magnitude and only uses its direction component. rsLoRA (Kalajdzievski, 2023) proposes a rank-stabilized scaling factor \\(A_t = r^{1/2}\\) to ensure stable gradient updates. To prevent overfitting, BiLoRA (Qiang et al., 2024) adopts a bi-level optimizing strategy, while others implement dropout mechanisms (Wang et al., 2024b; Lin et al., 2024).\n2.2. Mixture of LoRAS\nMoE has emerged as a critical framework for addressing complex tasks. By incorporating multiple expert modules, it dynamically selects appropriate experts based on specific inputs (Jacobs et al., 1991). Early studies, such as Lo-RAMOE (Dou et al., 2024) and MixLORA (Li et al., 2024), have pioneered the introduction of the MoE-LORA architecture by integrating LoRA experts for both global and downstream tasks. Afterward, MoE-LORA has demonstrated its effectiveness across a range of fields such as continual learning (Dou et al., 2024; Yang et al., 2024), vision-language multi-model tasks (Gou et al., 2023; Chen et al., 2024), and multi-task applications (Liu et al., 2023).\nRecent studies have focused on enhancing MoE-LORA through architectural advancements and improved training strategies. For instance, MoLA (Gao et al., 2024) allocates a varying number of experts at different layers, and MixDA (Diao et al., 2023) introduces multiple domain-adaptive modules to support multi-domain knowledge. Other methods such as (Wu et al., 2024a; Liu et al., 2023; Wu et al., 2024b; Gou et al., 2023; Wang et al., 2022) have also been proposed for strengthening MoE-LORA. To boost the training of MoE-LoRA, Luo et al. (Luo et al., 2024) address the random routing issue by introducing a contrastive loss. At the same time, MoV (Zadouri et al., 2023) chooses to combine lightweight vectors with a sparse selection mechanism for efficient expert allocation. Other approaches, including (Dou et al., 2024; Li et al., 2024; Zhu et al., 2023), focus on load balancing among experts. However, to the best of our knowledge, there is still a lack of work on gradient optimizing specifically for MoE-LORA models.\n2.3. Gradient Preconditioners\nIn most deep learning cases, gradient descent algorithms update model parameters by calculating gradient-based updates. To accelerate the optimizing process, the concept of gradient preconditioning has been introduced. Advanced techniques such as Adagrad (Duchi et al., 2011) dynamically adjust the learning rate by an accumulated squared gradients \\(G_t = \\sum_{i=1}^t g_i^2\\) and update model by \\(\\Delta \\theta_t = -\\eta G_t^{-1/2}. g_t\\). Adam (Kingma, 2014) extends this approach by incorporating momentum and bias correction, scaling gradients through a diagonal preconditioner, and resulting in updates in the form of \\(\\Delta \\theta_t = -\\eta \\frac{m_t}{\\sqrt{V_t} + \\epsilon}\\), where \\(V_t = \\beta_2 V_{t-1} + (1 - \\beta_2)g_t^2\\). AdamW (Loshchilov, 2017) further introduces a weight decay to Adam.\nRecent studies have provided theoretical support for scaled gradient descent methods under different preconditioning strategies. The core idea is to adjust both the direction and magnitude of updates by introducing a scaling matrix to gradients. Tong et al. (Tong et al., 2021) demonstrate the local convergence of scaled gradient descent methods. Jia et al. (Jia et al., 2024) extend this work by proving global convergence of scaled gradient descent for the least-squares matrix decomposition problem \\(||AB^T - Y||_F^2/2\\), showing that this approach achieves global convergence under different condition numbers. Other variants of scaled gradient descent have also emerged, such as Zhang et al. who proposed two regularization strategies (Zhang et al., 2023; 2024). In higher-dimensional settings, scaled gradient descent has been further extended to tensor optimization (Tong et al., 2022; Ma et al., 2023). Mishra et al. (Mishra et al., 2013; Mishra & Sepulchre, 2016) also applied the principles of Riemannian to the optimization involving low-rank matrices. Considering the data's manifold geometry, a Riemannian metric \\(g_p(v, w)\\) is introduced to guide gradient updates along the manifold. Recently, Zhang et al. (Zhang & Pilanci, 2024) introduced the idea of Riemannian preconditioners to LoRA by attaching an \\(r \\times r\\) preconditioner to the gradients of low-rank matrices. As a result, they provide improved fine-tuning performance of LoRA, compared with conventional gradient optimizers such as SGD and AdamW."}, {"title": "3. Method", "content": "We elaborate on our motivations and detail the modification we have made to the Riemannian preconditioning method specifically for MoE-LoRA. Our theoretical foundations and engineering solutions are also presented.\n3.1. Riemannian Preconditioner in LoRA Expert\nAs a preliminary, we first briefly introduce the Riemannian preconditioner (Zhang & Pilanci, 2024). Suppose the pretrained model weight is W and its additive low-rank components as B and A, let \\(X = W + BA\\) denote the whole weight matrix and let L and \\(\\eta\\) denote the loss function and the learning rate, respectively. For the plain gradient descent method, the gradient updating process is described through Equation (1) to (4), in which the derivation from (2) to (3) relies on ignoring the second-order term of learning rate. Obviously, \\(B\\nabla_A L + \\nabla_B LA\\) in (4) serves as an approximation of the ideal FFT gradient of X.\n\\begin{align}\nX_{new} &= W + B_{new} A_{new} \\\\\n&= W + (B - \\eta \\nabla_B L)(A - \\eta \\nabla_A L) \\\\\n&\\approx W + BA - \\eta B\\nabla_A L - \\eta \\nabla_B LA \\\\\n&= X - \\eta (B\\nabla_A L + \\nabla_B LA)\n\\end{align}\nSubsequently, according to the derivation chain rule and the simple fact that \\(X = W + BA\\), we directly obtain that \\(\\nabla_A L = (\\nabla_A X)(\\nabla_X L) = B^T(\\nabla_X L)\\), and likewise \\(\\nabla_B L = (\\nabla_X L)A^T\\). Thus, (4) can be transformed to:\n\\begin{equation}\nX_{new} = X - \\eta[BB^T(\\nabla_XL) + (\\nabla_XL)A^TA], \n\\end{equation}"}, {"title": "4. Experiments", "content": "We present a series of comparative experiments to evaluate the performances of MoE-LORA across various downstream tasks including Question Answering, the GLUE Benchmark, and the Vision-Language task. Specifically, two types of experimental candidates are mainly involved in our experiments: (1) MoE-LoRA with experts updated independently using Riemannian scaled optimizer; and (2) MoE-LORA updated using Riemannian scaled optimizer, plus incorporating our proposed rescaling technique (the engineering"}, {"title": "3.2. Rescaling Preconditioners", "content": "Equation (10) presents a squared-value weighted sum of an ensemble of gradient projections. Generally, more activated experts lead to smaller per-expert gate values and so lead to a more reduced assembled gradient; On the other hand, more balanced experts also lead to a more reduced assembled gradient since the basic inequality theorem \\((\\sum x_i)^2 / n = \\sum x_i^2 >= n / n^2\\) satisfies its equality condition when \\(x_i\\)s are equal. As a result, the gradient of the full matrix X will be underestimated due to those squared gate values. From the perspective of manifolds and curvature, we explain that by considering \\(g_i\\) in (8) as a manifold scaler, which reduces the size of \\(B_iA_i\\) so that would probably increase its curvature. However, the conventional Riemannian preconditioner failed to take the manifold scaler \\(g_i\\) into consideration, since it is designed for a single LoRA adapter.\nTo alleviate this squared issue, we assume a further rescaling step for the Riemannian preconditioners:\n\\begin{align}\n\\nabla_A L &= \\frac{(B^T B)^{-1} \\nabla_A L}{g_i} \\\\\n\\nabla_B L &= \\frac{\\nabla_B L (AA^T)^{-1}}{g_i}\n\\end{align}\nwhich is introduced to replace (6) in the derivation of Equation (9), to eliminate the variable \\(g_i\\) and keeps only a first power of \\(g_i\\) in the final equation (10). Throughout this transformation, the final ensemble of multi-expert projections shares an equivalent scale with the projection of a single LORA adapter, shown in Equation (12). Therefore, training of an MoE-LORA will be alleviated from under-estimation.\n\\begin{align}\nX_{new} &= X - \\eta \\sum_{i=1}^{N_{Expert}} g_i Proj_{col(B_i)} (\\nabla_x L)^T \\\\\n&-\\eta \\sum_{i=1}^{N_{Expert}} g_i Proj_{row(A_i)} (\\nabla_x L).\n\\end{align}"}, {"title": "3.3. Engineering Approximation", "content": "Although Equation (11) provides an approach to eliminate under-estimation for MoE-LORA, it is unrealizable since each LoRA module exists a respective \\(g_i\\) for every single token of every batch sample. Actually, during the training, backpropagation always runs after averaging all the losses of each single token of each sample in a batch. Thus, it is impossible to reconstruct and rescale the respective gradient contributed by each single token when we optimize a LoRA module. Alternatively, we design an engineering approximation to (11) and (12), by replacing each gate value \\(g_i\\) with its square root \\(\\sqrt{g_i}\\) during model forwarding. Consequently, Equation (12) can be achieved only under the preconditioners of (6), because the quadratic terms of gate values \\(g_i^2\\) in Equation (10) are now naturally become linear terms \\(g_i\\).\nReplacing \\(g_i\\) by \\(\\sqrt{g_i}\\) simultaneously introduces destruction to forwarding, as the sum of square roots does not equal 1. One possible solution is to re-normalize those square roots to be summed up as 1. However, it brings inconsistency between the assigned weights of experts during forwarding and backwarding. Therefore, we propose another strategy to accommodate both aspects, which is manually assigning optimizable and unoptimizable components of Equation (8), to satisfy the requirements of both forwarding in (8) and backwarding in (12). During the forwarding process, the proposed strategy is simply expressed by:\n\\begin{equation}\nX = W + \\sum_{i=1}^{N_{Expert}} \\sqrt{g_i} B_i A_i + (g_i- \\sqrt{g_i}) B_i A_i, \n\\end{equation}\nwhere \\(\\hat{p}\\) denotes that p does not require gradient, which also means p should be detached from gradient tracking along the whole neural network. By decomposing optimizable and unoptimizable components like this, low-rank matrices A and B are able to be optimized following (12). Moreover, by maintaining the optimizable \\(g_i\\) terms in forwarding and treating all the \\(\\sqrt{g_i}\\) as constants that are not subject to optimization, the conventional training behaviors of gates \\((g = G(x))\\) are preserved. Additionally, this modification introduces only a minimal overhead to the original forward computation process."}, {"title": "4.1. Experimental Setup", "content": "For most experiments, unless otherwise specified, we construct a mixture of LoRAs modules with a total of 20 experts, a rank of 4 for each expert, and a selection of top-10 experts activated each time. Furthermore, a range of other architectural MoE settings are also discussed in the ablation section. We perform experiments based on Llama-3.2-3B (Touvron et al., 2023), GLM-4-9B (GLM et al., 2024), and LLaVA-v1.5-7B (Liu et al., 2024a) as the foundation models. During training, we follow a linear decay learning-rate scheduler. We assign a relatively smaller learning rate to gate module compared to other trainable components, to achieve a stable training behavior. The reduced learning rate for gate helps to prevent model from experiencing abrupt and erratic routing changes. For further stabilization, we also cap its maximum gradient norm at 1.0. We carefully assign different initial learning rates for various tasks, trying to ensure all models achieve their best performances in a capable running time.\nWe denote the number of experts, top-k, and the per-expert rank as \\(n, k, r\\) respectively; For experimental candidates using conventional Riemannian preconditioned optimizers, we denote them as RSGD\\(_{n,k,r}\\) and RAdamW\\(_{n,k,r}\\), in which the front R represents the word Riemannian; While those candidates integrated with our gate-based rescaling approach are denoted as gRSGD\\(_{n,k,r}\\) and gRAdamW\\(_{n,k,r}\\) respectively, in which the front g represents that we rescale the gradient by gate values."}, {"title": "4.2. Question Answering Evaluations", "content": "We evaluate our proposed method on several question-answering benchmarks, including ScienceQA (Lu et al., 2022), CommonsenseQA (Talmor et al., 2018), Open-BookQA (Mihaylov et al., 2018) and SIQA (Sap et al., 2019). These question-answering datasets encompass a diverse range of domains and types, such as science, social interactions, common sense, and open-book exams, etc. We implement all the experimental candidates based on Llama-3.2-3B as their foundation model. For the SGD optimizer, we set an initial learning rate to \\(3\\times10^{-5}\\) for every LoRA expert; For the AdamW optimizer, we utilize an initial learning rate of \\(1\\times10^{-5}\\). We run through all the experiments until they are stabilized at a stable performance, and especially ensure that each pair of comparing candidates (i.e., inde-"}, {"title": "3. Method", "content": "3.1. Riemannian Preconditioner in LoRA Expert\nAs a preliminary, we first briefly introduce the Rieman-nian preconditioner (Zhang & Pilanci, 2024). Suppose the pretrained model weight is W and its additive low-rank com-ponents as B and A, let X = W + BA denote the whole weight matrix and let L and \u03b7 denote the loss function and the learning rate, respectively. For the plain gradient de-scent method, the gradient updating process is described through Equation (1) to (4), in which the derivation from (2) to (3) relies on ignoring the second-order term of learning rate. Obviously, BVAL + \u2207BLA in (4) serves as an approximation of the ideal FFT gradient of X."}, {"title": "5. Conclusion", "content": "We introduce the Riemannian gradient preconditioners to train a mixture of Low-rank Experts (MoE-LoRA). Instead of directly attaching Riemannian preconditioners to each expert's gradient for pursuing local optimality, we claim that multiplying expert BiAi by its respective gate value gi during forwarding leads to a further rescaling of the manifold constructed by expert i. To alleviate this, Rie-mannian preconditioners designed for MoE-LORA shall be revised to incorporate gate values. To approximate this concept, we propose an engineering solution that decomposes forwarding variables into optimizable and un-optimizable components. Experiments across various downstream tasks demonstrate our performance improvement over conventional Riemannian preconditioners. Ablation studies further demonstrate our theoretical foundation and universality."}, {"title": "4.3. Performance on GLUE Benchmark", "content": "To comprehensively examine our effectiveness, we perform a series of downstream evaluations on the benchmark of GLUE (Wang, 2018), which is a collection of resources for evaluating model performances on natural language understanding. We first run through all the evaluations in GLUE with Llama-3.2-3B as the foundation model and present the benchmark results in Table 2. For most SGD experiments we set an initial learning rate for LoRA experts as 3 \u00d7 10-5, except WNLI for which we set its initial learning rate to 3 \u00d7 10-6; For AdamW experiments we choose an initial learning rate from {3 \u00d7 10\u22125, 1 \u00d7 10\u22125}. For most datasets, we train for 2,000 steps, excluding some AdamW experiments in which we perform an early stop at around 1,000 since they appear to be converged or even overfitting. Table 2 illustrates our effectiveness across various downstream applications as well as the overall assessment under Llama-3.2-3B. In terms of overall performances, our approach improves RSGD and RAdamW by 11.9% and 3.0% respectively.\nSubsequently, we extend experiments to a larger foundation model, GLM-4-9B. Since the 9B model is more powerful in few-shot learning, for some datasets such as SST-2 etc., we set lower learning rates such as 3 \u00d7 10-6 and 1 \u00d7 10-6 respectively for SGDs and AdamWs, to make sure a clear loss decreasing period can be witnessed. We train for the same number of steps for each pair of competitive candidates. Table 2 also illustrates the performances of training MoE-LORA through different optimizing strategies with GLM-4-9B. Results still witness our overall outperformance. In particular, we improve the average performance of RSGD by around 4.3%, and that of RAdamW by around 0.7%."}, {"title": "4.4. Performance on LLaVA", "content": "Beyond textual benchmarks, we further evaluate our gate-based rescaling approach in the computer vision field. Specifically, we implement an MoE-LORA architecture for the well-known vision-language foundation model, LLaVA-v1.5-7B (Chen et al., 2024). We introduce trainable MoE-LORA adapters into both visual and textual modules of LLaVA-v1.5-7B. For evaluation, Visual7W (Zhu et al., 2016) and VMCBench (Zhang et al., 2025b) datasets are employed, which both consist of multimodal samples each containing a multiple-choice question paired with a related image. The question can be answered through understanding the provided image. Visual7W is a subset of Visual Genome (Krishna et al., 2017) dataset, while VMCBench is a benchmark created from 20 existing VQA datasets. For VMCBench, we only use their dev set since their test set is not labeled. We take 900 of all the 1000 labeled samples as training samples, while the rest 100 are for evaluation. Table 3 exhibits the results of all experimental candidates. Our approach consistently demonstrates visible improvements, especially for SGD."}, {"title": "4.5. Compare and Integrate with MoE-LORA Baselines", "content": "We then compare and integrate our method with existing MoE-LORA baselines. We provide our comparisons with two baselines: (1) The pure mixture of LoRAs (Liu et al., 2023), which we denote as MoELORA and use token-level routing; (2) MoLA (Gao et al., 2024), which is a MoE-LORA variant specifically focusing on assigning different numbers of experts to different layers, and proving that higher layers need more LoRA experts. It should be noted that our proposed gate-based rescaling approach can be integrated with most MoE-LoRA variants since they are not in conflict. Take MoLA as an example, we can integrate our method with MoLA by implementing a model with more experts in its higher layers and trained through Riemannian preconditioners and gate-based rescaling approach. We reproduce MoELORA and MoLA, implement the integrations, and illustrate their performances in Table 4. We use Llama-3.2-3B as the foundation model and follow MoLA's configurations here, which means we set the per-expert rank to 4, top-k to 2, and the total number of experts of all layers to 140. In this way, MoELORA and our method assign 5 experts to each layer, while MoLA assigns 2, 4, 6, and 8 experts respectively to the bottom, lower middle, higher middle, and top layers. In table 4 we denote this special assignment strategy as (2,4,6,8), while the average assignment is (5,5,5,5), where each digit covers seven layers under Llama-3.2-3B. We still provide enhancement in the context of MoLA architecture."}, {"title": "4.6. Ablation Study", "content": "Theoretical Dependence. Although our proposed approach is grounded in the context of Riemannian preconditioners, it is important to note that our engineering implementation does not inherently require coexistence with Riemannian preconditioners. The reason is that our modifications are solely focused on altering the forward propagation conventions of MoE-LORA. This consequently raises a vital question about the standalone efficacy of our modifications in enhancing MoE-LORA's performance, without depending on the Riemannian preconditioning context. Ideally, since the conventional un-preconditioned optimizer does not guarantee a projection of full matrix gradient in low-rank space, it should be trivial for them to normalize the sum of expert gradients by replacing gi with \\(\\sqrt{g_i}\\). To confirm this, we"}, {"title": "A. Covergence Efficiency", "content": "In the main body of our paper, we illustrate the converging speed enhancements of our proposed approach, gRSGD, over the conventional RSGD through a series of loss-decreasing plots. To further exhibit the comprehensive comparisons of convergence efficiency, we provide more results on GLUE benchmarks. In particular, for experiments conducted under Llama-3.2-3B as well as GLM-4-9B, we record metrics after the initial 100 training steps for each of the GLUE evaluations, as detailed in Table 6 and Table 7 respectively.\nTable 6 and 7 clearly demonstrate the superior convergence speed of our solutions over the conventional Riemannian preconditioned SGD optimizers. Nevertheless, they simultaneously illustrate an overall equivalent performance with trivial differences between our gate-based approach and the conventional Riemannian preconditioning method under AdamW optimizers. This indicates that our proposed approach is more valuable for SGD optimization. AdamW optimizers already present robust converging performances due to their adaptive gradient and learning rate mechanisms. As a result, our global optimal approximation under AdamW optimizing mainly contributes to the final optimality rather than significantly accelerating the initial gradient descending."}, {"title": "B. AdamW Weight Decay Analysis", "content": "AdamW implements a strategy called weight decay, which decays the trainable weights after each gradient update by \u03b8t = \u03b8t \u2212 \u03b1\u03bb\u03b8t. Instead of the original Adam algorithm, AdamW separates the weight decay from the gradient update, which leads to better performance in some cases. To comprehensively prove the effectiveness of our gate-based rescaling method over Riemannian preconditioned AdamW, we evaluate our boosts across various weight decay factors \u03bb. Results are exhibited in Table 8."}, {"title": "C. Multi-Task Performance", "content": "One of the most valuable features of MoE architectures is their capability of modeling multiple tasks. Through gating mechanism, the MoE system adeptly delegates specific tasks to individual experts, thereby facilitating a more focused and efficient learning process within each expert module. As a result, one question arises regarding our proposed gate-based rescaling approach: Can it still effectively augment the performance of MoE architectures in multi-task scenarios?\nTo illustrate this, we manually construct a mixed dataset consisting of two irrelevant natural language tasks, ScienceQA and MRPC. ScienceQA is a question-answering benchmark that mainly centers on multiple-choice questions from primary and secondary school science curricula, while MRPC is designed as a sentence pair task for identifying whether two sentences are equivalent. This combination is roughly balanced since their testing datasets both consist of around 2,000 samples. We still construct a mixture of LoRA modules with a total of 20 experts, a rank of 4 for each expert, and a selection of top-10 experts activated each time. Since a mixed task is more complex to train, we increase the initial learning rate to 3 \u00d7 10-4 and 1 \u00d7 10-4 for SGD and AdamW experiments respectively. We train all the candidates for 1,600 steps. Results are exhibited in Table 9. We still can witness a significant boost by our gate-based rescaling in terms of the Riemannian preconditioned SGD optimizer."}, {"title": "D. Method Implementation", "content": "The engineering alternative solution of the gate-based rescaling approach is to manually separate the forwarding into optimizable and unoptimizable components. Here we provide our implementation in Python-like pseudocode. We only update two lines of the original MoE-LORA code."}, {"title": "E. Experimental Details", "content": "We present our experimental details in Table 10. All experiments in this paper follow this configuration unless they specify their particular settings. For training steps, some of the experiments may converge earlier, therefore we perform an early stop for those experiments. We constrain the maximum of training steps by 2,000, considering it a relatively fair setup for various downstream tasks, especially those with different scales of training corpora but in the same level of complexity."}]}