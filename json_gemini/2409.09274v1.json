{"title": "LabellessFace: Fair Metric Learning for Face Recognition without Attribute Labels", "authors": ["Tetsushi Ohki", "Yuya Sato", "Masakatsu Nishigaki", "Koichi Ito"], "abstract": "Demographic bias is one of the major challenges for face recognition systems. The majority of existing studies on demographic biases are heavily dependent on specific demographic groups or demographic classifier, making it difficult to address performance for unrecognised groups. This paper introduces \u201cLabellessFace\u201d, a novel framework that improves demographic bias in face recognition without requiring demographic group labeling typically required for fairness considerations. We propose a novel fairness enhancement metric called the class favoritism level, which assesses the extent of favoritism towards specific classes across the dataset. Leveraging this metric, we introduce the fair class margin penalty, an extension of existing margin-based metric learning. This method dynamically adjusts learning parameters based on class favoritism levels, promoting fairness across all attributes. By treating each class as an individual in facial recognition systems, we facilitate learning that minimizes biases in authentication accuracy among individuals. Comprehensive experiments have demonstrated that our proposed method is effective for enhancing fairness while maintaining authentication accuracy.", "sections": [{"title": "1. Introduction", "content": "Face recognition is one of the modalities in biometric authentication systems that have seen rapid social adoption in recent years due to its convenience. As discussions about the responsibility of machine learning systems progress, it has often been pointed out that facial recognition systems also often show inconsistent performance in distinguishing between demographic attributes such as race and gender [2, 10].\nApproaches to mitigate bias in inter-attribute discrimination performance can be categorized into two stages: the dataset construction and the model construction. In the dataset construction stage, efforts are made to create datasets with balanced racial proportions [20, 21], sample or augment data to minimize disparities in recognition accuracy between attributes [13, 16], and propose methods for data augmentation [11]. In the model construction stage, strategies involve mitigating bias in model performance through score normalization between attributes [18] and dynamically adjusting hyperparameters based on attributes [20, 22, 19], under the assumption of the existence of racially biased datasets.\nMost previous approaches require sensitive attribute labels (e.g., race and gender) for training the network, which limits scalability to large-scale datasets and cannot guarantee accuracy for unknown attributes. This dependence on human-annotated labels poses challenges in terms of time, cost, and potential biases, especially for emerging attributes.\nThis paper proposes LabellessFace, a novel framework that improves demographic bias in face recognition without requiring demographic group labeling. Our approach aims to maintain authentication accuracy while enhancing fairness. We introduce two key concepts: the class favoritism level, quantifying the degree of favoritism towards specific classes across the dataset, and the fair class margin penalty, extending existing metric learning methods based on class favoritism level. LabellessFace equalizes authentication accuracy across individuals without assuming specific sensitive attributes, achieving fairness even for unknown attributes. We conducted comprehensive experiments using common facial benchmarks, demonstrating that our method successfully improves fairness while maintaining authentication accuracy comparable to existing approaches. The results show the effectiveness of LabellessFace in achieving fairness across both known and unknown demographic attributes.\nOur contributions are summarized as follows:\n\u2022 We propose the concept of class favoritism levels, which quantifies the degree of favoritism towards specific class across the entire dataset.\n\u2022 We propose the fair class margin penalty, which extends existing metric learning methods based on class favoritism levels. This realizes the LabellessFace framework that improves fairness without the need for labelling based on assumed target attributes.\n\u2022 Comprehensive experiments have demonstrated that our proposed method is effective for enhancing fairness while maintaining authentication accuracy."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Fairness of Facial Recognition", "content": "Facial recognition systems, increasingly integrated with surveillance cameras, are being deployed in critical scenarios such as criminal investigations, where the importance of racial fairness has been emphasised and numerous studies have been conducted. Buolamwini et al. [2] reported that facial recognition systems from companies such as Microsoft, IBM and Face++ had a misidentification rate of less than 1% for white males, while the rate for black females was around 35%. The US National Institute of Standards and Technology (NIST) conducted a fairness study of 189 facial recognition software systems. They found disparities in false positive rates between whites and blacks, with differences ranging from ten to a hundred times in 1:1 authentication scenarios. In addition, in 1:N authentication scenarios, black women had higher false detection rates [1]. Garvie et al. [7] pointed out that the bias in the racial proportions of the training data significantly affects the racial bias. They highlighted the inadvertent introduction of human discrimination due to a higher proportion of Caucasians in the data, the effect of skin colour on contrast, and the potential effect of female make-up on authentication accuracy."}, {"title": "2.2. Dataset-Based Approach", "content": "In the process of machine learning, the collection of datasets involves human intervention, which can unconsciously introduce bias. Therefore, in facial recognition systems, the issue of racial bias has been mainly related to the racial proportions in the training datasets. Wang et al. [20] created the BUPT-Balancedface dataset with balanced racial proportions and demonstrated that training with it could reduce racial bias compared to training with traditionally biased datasets in terms of racial proportions. Faisal et al. [13] proposed a method of resampling in which data objects are repeatedly replaced to minimise differences in recognition accuracy between races, thereby removing data that could cause racial bias from the dataset. Qraitem et al. [16] proposed a method to improve fairness by creating multiple subsets that mimic the bias in the attributes of the dataset. While techniques such as Data Augmentation are often employed in the collection of training data, Niharika et al. [11] have pointed out the performance limitations of reducing racial bias through Data Augmentation using GANs. As an approach to optimizing a score threshold for a dataset, Pereira et al. [3] introduced the Fairness Discrepancy Rate (FDR) to assess demographic differences by assuming a single decision threshold in biometric verification systems."}, {"title": "2.3. Model-Based Approach", "content": "Dataset approach is an efficient method for improving fairness among races, yet constructing large-scale datasets with fairness considerations is not straightforward. Additionally, it has been pointed out that racial boundaries are ambiguous [6] and that the impact of fairness can also arise from the interrelationship between racial and environmental factors [12, 17]. Many researches has also been conducted to address racial bias by innovating the structure of models to allow fair learning even with existing datasets that are biased towards certain races. Most of the state-of-the-art methods and our proposed LabellessFace lies in this category. Philipp et al. [18] used an approach that normalizes scores between races, using the race of the authentication subject as prior information. Dooley et al. [6] proposed an approach to search for models that represent the Pareto optimal solution in terms of fairness and accuracy among multiple models. Xu et al. [22] proposed an approach where they dynamically balances the false positive rate (FPR) between each training sample without the need for demographic labels. Wang et al. [19] proposed MixFair Adapter to estimate and reduce the identity bias, the performance inconsistency between different identities, by reducing the feature discriminability differences.\nOur proposed LabellessFace is inspired by the techniques of Xu et al. [22] and Wang et al. [19], particularly because it does not require demographic labels. Unlike their approaches, we consider the degree of favoritism towards specific identity across the entire dataset."}, {"title": "3. Proposed Method", "content": "In this section, we introduce each component of the LabellessFace framework using the fair class margin penalty. The overview of the proposed method is shown in Figure 1. In addition to existing softmax-based metric learning (section 3.1), our method dynamically sets different margins for each class based on class favoritism level while progressing the training through the fair class margin penalty process (section 3.2), and updates the class favoritism level at the end of each epoch (section 3.3). Here, the class favoritism level is determined based on how much the recognition accuracy for each individual deviates from the overall average using the training samples."}, {"title": "3.1. Softmax-Based Metric Learning", "content": "The original softmax loss function is formulated as follows:\n$\\mathcal{L} = - \\log \\frac{e^{W_{y_i}^T x_i + b_{y_i}}}{\\sum_{j=1}^{|C|} e^{W_j^T x_i + b_j}}$ (1)\nwhere $x_i \\in \\mathbb{R}^d$ is a d-dimensional feature vector that serves as the input to the fully-connected layer corresponding to class $y_i$, and $W_j \\in \\mathbb{R}^d$ represents the j-th column of the weight matrix $W \\in \\mathbb{R}^{d \\times |C|}$. Additionally, $b_j$ represents the bias term. Under the conditions where the bias term $b_j = 0$ and L2 regularization constraints $||W_j|| = 1$ and $||x_i|| = 1$ are applied, equation (1) expresses the angle between $W_{y_i}$ and $x_i$ as $\\Theta_{y_i}$, and can be represented by\n$\\mathcal{L} = - \\log \\frac{e^{\\cos \\Theta_{y_i}}}{\\sum_{j=1, j \\neq y_i}^{|C|} e^{\\cos \\Theta_j} + e^{\\cos \\Theta_{y_i}}}$ (2)\nIn metric learning based on the softmax loss, the Angular Margin Penalty is employed to reduce intra-class variance for the correct class $y_i$, and a scale parameter $s$ is used to scale $\\cos \\Theta_{y_i}$. For example, in ArcFace [4], the equation is defined by adding the margin parameter $m$ to equation (2) as follows:\n$\\mathcal{L} = - \\log \\frac{e^{s \\cdot (\\cos \\Theta_{y_i} + m)}}{e^{s \\cdot (\\cos \\Theta_{y_i} + m)} + \\sum_{j=1, j \\neq y_i}^{|C|} e^{s \\cdot \\cos \\Theta_j}}$ (3)\nIn the proposed method, a coefficient is added to the equation (3) that allows the optimal margin to vary dynamically for each class."}, {"title": "3.2. Fair Class Margin Penalty", "content": "In this proposal, to minimize the bias in individual authentication accuracy, a coefficient $d_c$ (hereafter referred to as the margin coefficient) is added to equation (3) so that the optimal margin for each class dynamically changes during training. Consequently, equation (3) is modified as\n$\\mathcal{L} = - \\log \\frac{e^{s(\\cos \\Theta_{y_i} + d_c m)}}{e^{s(\\cos \\Theta_{y_i} + d_c m)} + \\sum_{j=1, y_i} e^{s \\cos \\Theta_j}}$ (4)\nIn this case, $d_c$ takes different values for each class and is determined at the end of each epoch based on the class favoritism level $f_c$, which indicates the extent to which each class $c \\in C$ is favored among all classes. The margin coefficient $d_c$ is defined by\n$d_c = \\begin{cases} \\frac{2}{1 + \\exp(\\gamma \\cdot f_c)} & (f_c < 0) \\\\ \\frac{2}{1 + \\exp(\\gamma h \\cdot f_c)} & (f_c \\geq 0) \\end{cases}$ (5)\nThe margin coefficient $d_c$ is designed to increase the margin's impact on classes that are less favored, thereby enlarging the facial feature space, while reducing the margin's impact on classes that are more favored, thus narrowing the facial feature space to enhance fairness.\nHyperparameters. Figure 3 shows the relationship between $d_c$ and the class favoritism level $f_c$. In equation (5), the coefficient $\\gamma$ is a real number within the range $[0, \\infty)$ and serves as a hyperparameter that determines the gradient of the margin coefficient $d_c$. When the gradient coefficient $\\gamma = 0$, the margin coefficient $d_c$ becomes 1, irrespective of the value of class favoritism level $f_c$, making it equivalent to ArcFace [4]. Additionally, the coefficient $h$, taking a real number in the range [0, 1], determines how much importance is placed on improving fairness among class sets. A lower value of $h$ indicates a reluctance to allow a decrease in accuracy for favored classes, prioritizing authentication accuracy over fairness. Conversely, a higher value of $h$ allows for a decrease in accuracy of favored classes, emphasizing fairness over authentication accuracy. In this paper, $\\gamma$ is referred to as the gradient coefficient and $h$ as the harmony coefficient."}, {"title": "3.3. Class Favoritism Level Calculation", "content": "The class favoritism level $f_c$ for class $c$ is calculated at the end of each epoch using the training data and is reflected in the calculation of the margin coefficient $d_c$ for the subsequent epoch.\nThe confidence derived from the softmax output increases for the class to which a sample belongs as training progresses, maximizing the confidence for the sample's class while reducing the confidence for other classes. However, this tendency varies depending on the class to which the sample belongs. At this point, classes identified with relatively high confidence within the class set can be considered as favoured, whereas classes identified with low confidence are perceived as neglected. The class favoritism level $f_c$ interprets and quantifies the difference in confidence levels among classes as a measure of fairness. An overview of the method for calculating the class favoritism level $f_c$ is shown in Figure 2.\nLet the features extracted from the training data of class $c$ be denoted as $X_c = \\{x_{c,k}\\} (k = 1,\\cdots,|X|)$, where $|X|$ represents the total number of training data for class $c$. Inputting $x_{c,k}$ into the face recognition model yields the softmax output $P(x_{c,k})$. Here, denoting the confidence component corresponding to class $c$ in $P(x_{c,k})$ by $P(x_{c,k})_c$, then the average confidence for $x_{c,k}$ can be expressed as $P_c$, which is given by\n$\\overline{P_c} = Mean(P(x_{c,k})_c) = \\frac{1}{|X_c|} \\sum_{k=1}^{|X_c|} P(x_{c,k})_c$. (6)\nAfter calculating $\\overline{P_c}$ for all classes $c \\in C$, the average of these values, $\\overline{P}$, is derived as follows:\n$\\overline{P} = \\frac{1}{|C|} \\sum_{c \\in C} \\overline{P_c}$ (7)\nThe class favoritism level $f_c$ for class $c$ can be derived as the relative favoritism of class $c$ within the class set $C$. Therefore, the class favoritism level $f_c$ is defined by the difference between $\\overline{P_c}$ for each class and the average $\\overline{P}$ as follows:\n$f_c = \\overline{P_c} - \\overline{P}$, (8)\nwhere the range of $\\overline{P_c}$ and $\\overline{P}$ is [0,1], and the range of $f_c$ is [-1,1]. For instance, a class with a negative class favoritism level $f_c$ indicates that samples belonging to that class are identified with relatively low confidence, thus they are more prone to misclassification compared to other classes, and such classes can be considered as neglected. Therefore, the dynamic margin coefficient $d_c$ monitors whether each class is favored or neglected by referencing the class favoritism level $f_c$ regularly throughout the training process, and adjusts the optimal margin for each class at every epoch accordingly."}, {"title": "4. Experiments", "content": "In our experiment, we evaluate the effectiveness of a our proposed Labelless Face framework by addressing the following two viewpoints: \u201c(1) Can our proposed method improve the fairness of certain sensitive attributes (e.g. race or gender) while maintaining accuracy? (Section 4.3.1)\" and \"(2) Can our proposed method improve fairness independent of annotated labels? (Section 4.3.2)\". We describe the experimental protocols used to address each research question, present the results, and discuss their implications."}, {"title": "4.1. Protocol", "content": "Training Dataset. For model training, the BUPT-Balancedface dataset [20], which has an equal proportion of races, was used. BUPT-Balancedface contains 7,000 classes of four races: African, Asian, Caucasian, and Indian, with racial labeling provided for each data point. The dataset, comprising 28,000 classes of facial images from these four races, was split into a training and validation ratio of 9:1. The validation data is used for early stopping decisions and for calculating the class favoritism levels $f_c$.\nEvaluation Datasets. In the evaluation, the Labeled Faces in the Wild (LFW) [9] and Racial Faces in the Wild (RFW) [21] datasets were used. LFW is a dataset with approximately 3,000 pairs each of genuine and imposter facial image pairs prepared in advance, and each class is labeled with 74 attributes including race, age, hair color, and the presence of glasses. This dataset is used for evaluating the discriminative performance of the model and assessing fairness across various attribute domains. RFW contains about 3,000 classes for each of the four races: African, Asian, Caucasian, and Indian, with racial labeling provided for each data point. Approximately 3,000 pairs each of genuine and imposter facial image pairs were created using RFW, and it was used to evaluate fairness in 1:1 verification.\nImplementation Detail. We utilized ResNet34 [8] as a face recognition model architecture, with the layer prior to the final layer connected to a metric learning layer, trained as a classifier for 28,000 classes. The parameters of ResNet34 were initialized randomly. The batch size during training was set to 256, and the learning rate was linearly adjusted from $1e - 1$ to $1e-4$. The weight decay was set at $5e - 5$ and momentum at 0.9. The optimization algorithm used was SGD, and training was conducted over 30 epochs. For metric learning, the scale $s$ was set to 64, and the initial margin $m$ was set to 0.3. The hyperparameters for the proposed method were set to the gradient coefficient $\\gamma = 10$ and the harmony coefficient $h = 1$ for the experiments."}, {"title": "4.2. Metrics", "content": "We quantitatively evaluated the discriminative performance of the proposed method by the Equal Error Rate (EER) and the Area Under the Curve (AUC). The fairness of the model is evaluated by the standard deviation of EER (STD), the Gini Index (Gini), and the Skewed Error Ratio (SER) across different classes. The Gini coefficient [5] is a fairness metric that represents the disparity in the cumulative distribution ratios of EER among different classes, and is defined by\nGini = $\\frac{\\sum_{i=1}^{|C|} \\sum_{j=1}^{|C|} |EER_i - EER_j|}{2|C|^2 \\overline{EER}}$, (9)\nwhere $|C|$ represents the total number of classes, and $EER_i$, $EER_j$ refer to EER for classes $i$ and $j$, respectively, $\\overline{EER}$ represents the average of EERS. SER is a fairness metric that represents the ratio between the highest and lowest error rates among attributes, and is defined by\nSER = $\\frac{\\max_{c \\in C}(EER_c)}{\\min_{c \\in C}(EER_c)}$ (10)"}, {"title": "4.3. Results", "content": "In our experiments, we compared the proposed method with the following approaches: (1) ArcFace [4]: A basic method for learning facial representations, (2) MagFace [15]: A method for learning facial representations considering sample quality, (3) CIFP [22]: A method to minimize the disparity in false positive rates across different races, (4) MixFairFace [19]: A method to equalize feature distances among individuals, and (5) Proposed: A method to equalize confidence level among individuals."}, {"title": "4.3.1 Fairness-Accuracy Trade-off", "content": "Our study investigated whether the proposed method could improve fairness with respect to sensitive attributes (e.g., race or gender) while maintaining accuracy. Our performance and fairness evaluation results trained on BUPT-Balancedface dataset and evaluated on the RFW datasets are shown in Table 2. EER for each race is denoted as EER-Af (African), EER-As (Asian), EER-Ca (Caucasian), and EER-In (Indian). As shown in Table 2, CIFP achieved the highest performance in EER across all racials on RFW, which is believed to be due to CIFP utilizing a training algorithm that takes into account pre-labeled racial information. In contrast, the Proposed method exhibits a lower (the best) STD/Gini/SER values compared to other methods. The differences of EERS between Proposed and ArcFace are small, indicating that Proposed improves fairness while maintaining authentication accuracy. As for MixFairFace, despite conducting replication experiments using the implementation and parameters published by the authors of original paper\u00b9, we could not achieve high performance."}, {"title": "4.3.2 Label-Independent Fairness Improvement", "content": "Next, our study explored whether our proposed method could improve fairness independent of annotated labels. Fairness evaluation results trained on BUPT-Balancedface dataset and evaluated on the LFW dataset are shown in Table 3. Additionally, a comparison of fairness across the 26 attribute domains in LFW is shown as a heatmap in Figure 4. For the evaluation with the LFW dataset, 26 attributes with more than 100 samples each were selected for analysis. STD, Gini, SER were assessed when users were divided according to these attributes, respectively. The 74 attributes of LFW are labeled with continuous values, where higher values indicate a stronger presence of the attribute [14]. Hence, continuous values were MinMax scaled to the range [-1,1], and values above 0.5 were considered to indicate the presence of the attribute in the images. MixFairFace, which did not perform well in section 4.3.1, was excluded here.\nAs shown in Table 3, CIFP has shown poorer performance on all fairness metrics compared to other methods, suggesting that its fairness for attributes not considered in training has deteriorated because it focuses only on racial attributes. In contrast, the Proposed method performs best on all performance and fairness metrics. These results suggest that the label-free fairness training at an individual level proposed by this method can achieve a high trade-off between accuracy and fairness even for unknown attributes."}, {"title": "5. Discussions", "content": "Computational cost. The calculation of the Class Favoritism Level, as shown in Figure 2, can keep the required memory capacity low by sequentially calculating $\\overline{P_j}$ during training. However, since the computation increases in proportion to the number of training data, this aspect needs to be considered.\nSelection of Hyperparameters. In this method, the parameters $\\gamma$ and $h$ are employed to balance the trade-off between fairness and accuracy. A grid search was performed for $h$ and $\\gamma$, revealing that the best performance was achieved with $h = 1$ and $\\gamma = 10$. Higher values of $h$ suppress the learning of favored attributes while encouraging the learning of neglected attributes. $\\gamma$ determines the intensity of this effect. If significant latent attribute biases are expected in the dataset, it is suggested that using larger values for $h$ or $\\gamma$ could lead to greater fairness improvements. However, it should be noted that excessively large values may cause instability in the learning process. This paper does not propose an optimal method for determining these parameters, leaving it as a topic for future research."}, {"title": "6. Conclusion", "content": "In this paper, we proposed a new framework, LabellessFace, aimed at reducing authentication bias in facial recognition. This method was designed to realize a label-free training method that does not require pre-labeling for demographic groups. To achieve this goal, we introduced the concept of a fair class margin penalty based on class favoritism levels, utilizing individual class units to avoid the need for demographic group labeling for fairness considerations. Extensive experiments using common facial benchmarks demonstrated the effectiveness of our proposed method compared to other baselines, particularly in achieving fairness across a broad range of attributes without the need for consideration during training. Future work can explore further research extensions in various aspects, such as determining more optimal margin coefficients and optimizing hyperparameters."}]}