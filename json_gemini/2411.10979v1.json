{"title": "VIDCOMPOSITION: Can MLLMs Analyze Compositions in Compiled Videos?", "authors": ["Yunlong Tang", "Junjia Guo", "Hang Hua", "Susan Liang", "Mingqian Feng", "Xinyang Li", "Rui Mao", "Chao Huang", "Jing Bi", "Zeliang Zhang", "Pooyan Fazli", "Chenliang Xu"], "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMS primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts. We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations. VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compositions and offers insights into areas for further improvement. The leaderboard and evaluation code are available at https://yunlong10.github.io/VidComposition/.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in Multimodal Large Language Models (MLLMs) [1, 3, 8, 15, 27, 43, 47] have greatly enhanced capabilities in understanding multimodality. However, while current benchmarks [10, 12, 25, 33] for evaluating MLLMS assess general image or video comprehension, they lack a detailed focus on video composition, the nuanced interpretation of how visual elements combine and interact within compiled videos. Compiled videos refer to those created by editing and integrating multiple clips, scenes, or sequences, either from various sources or from different segments of a single recording, e.g. films, TV series, documentaries, animations, vlogs, etc. These videos are carefully constructed to create a seamless flow and include richer compositions, requiring shot-by-shot analysis to interpret.\nShot-by-shot analysis, a technique where creators meticulously break down the elements of a video, serves as a vital tool for understanding video composition in depth. This level of understanding, essential in film analysis and video production, goes beyond general scene or action recognition, requiring an in-depth grasp of compositional elements such as camera movements, shot sizes, narrative structures, and character dynamics. This analysis also captures the intricate layers of visual storytelling by deconstructing how technical and artistic choices shape the viewing experience. However, achieving this fine-grained level of composition understanding remains a significant challenge for existing MLLMs, which primarily operate on broader, more coarse-grained interpretations of video content.\nRecognizing the gap in existing evaluation methods, we introduce VIDCOMPOSITION, a new benchmark designed to assess MLLMs on understanding video composition at a cinematic level. VIDCOMPOSITION includes 982 carefully curated videos and 1,706 multiple-choice questions, featuring meticulously annotated clips from films, TV series, animations, commentary videos, etc. These questions include five key areas of video composition: Cinematography Analysis, Character Understanding, Narrative Understanding, Scene Perception, and Making Analysis, spanning 15 distinct tasks. Each area captures critical aspects of compositional understanding, e.g. camera movements, angles, shot sizes, narrative structures, characters, scenes, cuts, special effects, etc., providing a extensive framework for evaluating the nuanced comprehension required in cinematic contexts. We evaluate 33 state-of-the-art MLLMs on VIDCOMPOSITION, including 27 open-source and 6 proprietary models, revealing a substantial performance gap between MLLMs and human-level comprehension in video composition understanding.\nIn summary, our contribution is three-fold:\n\u2022 We introduce VIDCOMPOSITION, a novel, human-annotated, high-quality benchmark for evaluating fine-grained video composition understanding in MLLMs.\n\u2022 We comprehensively evaluate 33 MLLMs for video understanding with VIDCOMPOSITION. The results show the challenging nature of VIDCOMPOSITION and the substantial gap between MLLMs' and humans' capabilities in video composition understanding.\n\u2022 We analyze the critical factors that influence the performance of MLLMs systematically, providing potential directions for model improvement and future advancements."}, {"title": "2. VIDCOMPOSITION", "content": "VIDCOMPOSITION contains 982 compiled videos with 1706 human-annotated multiple-choice questions for video composition understanding, including 5 main categories: Cinematography Analysis (CA), Character Understanding (CU), Narrative Understanding (NU), Scene Perception (SP), and Making Analysis (MA); and 15 sub-tasks: Camera Movement Perception (CamM-P), Shot Size Perception (SS-P), Camera Angle Perception (CamA-P), Emotion Perception (E-P), Action Perception (A-P), Costume, Makeup and Props Perception (CMP-P), Character Counting (Cha-C), Script Matching (S-M), Plot Ordering (P-O), Background Perception (B-P), Scene Counting (S-C), Lighting Perception (L-P), Art Style Perception (AS-P), Cut Counting (Cut-C), and Special Effect Perception (SE-P). Examples of each task can be found in Figure 2. The detailed definitions of each task are provided in Supplementary."}, {"title": "2.2. Dataset Curation Process", "content": "Video Collection and Filtering. Our dataset comprises videos sourced from the Internet, focusing on compiled videos primarily derived from commentary videos for movies, TV series, and animations, which have no copyright concerns. These videos typically include subtitles and scripts uploaded by users, which assist in later annotation stages. We further refine the collected videos by filtering out inappropriate content, such as clips that may cause psychological distress or those flagged as sensitive by API-based models. The average duration of the collected videos is about 20 minutes. For videos with subtitles or scripts, we extract the timestamps marking the start and end of each subtitle. With this information, we further segment the video into coherent sections whose average length is 794 frames. To avoid models from predicting answers relying on speech, we removed the audio of the videos.\nHuman Annotation. To ensure the quality and reliability of the dataset, we engage multiple human annotators, assigning each video segment to several annotators to minimize potential biases. All questions are meticulously designed to address specific tasks. For perception tasks such as A-P, E-P, CMP-P, B-P, and for counting tasks such as Cha-C, S-C, and Cut-C, annotators watch the video segment and write the correct answer alongside several incorrect (distractor) options. For tasks such as CamM-P, SS-P, CamA-P, L-P, SE-P, and AS-P, we provide a predefined set of selectable labels. For example, for CamM-P, the labels include zoom in, zoom out, pan left, pan right, pan up, pan down, and static shot. Annotators choose appropriate labels for each video segment, which are then used as correct options, while distractors are randomly selected from the remaining labels, ensuring they differ from the correct options. For S-M, we use the video's commentary script, extracted from the subtitle file, as the correct option, with distractor options sourced from nearby segments' scripts to create plausible alternatives. For P-O, we segment the commentary script into multiple parts, shuffling and inserting them into the question with sequence numbers. The correct answer is the original order of the script, while other options are generated by randomizing these sequence numbers.\nQuality Control. To ensure the quality of our benchmark, each video and corresponding QA pair undergoes multiple rounds of review. We implemented an annotation review system (see the user interface in Supplementary), which displays the annotated video, question, and answer options alongside an additional feedback option for reviewers to provide corrections or comments. Reviewers are required to attempt each question themselves; if they identify errors in the question or options, they can select the feedback option to either suggest improvements to the question or specify what they believe is the correct answer. After each round of review, all feedback submitted through the annotation review system is analyzed and used to enhance the annotation quality further. This iterative quality control process ensures accuracy and consistency across annotations, minimizes errors, and refines question clarity for each task."}, {"title": "2.3. Evaluation Metrics", "content": "To obtain model predictions, each question is structured within a predefined prompt template P that includes the question text and associated options $O_q$ (A, B, C, D, along with descriptive texts). This prompt $I_q$ is then fed into the model M, which is expected to output a single character representing its predicted answer (one of {A, B, C, D}). The prediction process is formalized in Algorithm 1. The prompt template P can be found in Supplementary.\nAlgorithm 1 Model Prediction\n1: Input: Question q, Options $O_q$\n2: Output: Prediction A\n3: $I_q\\leftarrow P(q, O_q)$\n4: $R_q\\leftarrow M(I_q)$\n5: $A_q\\leftarrow\\begin{cases}\nR_q & \\text{if } R_q \\in \\{A, B, C, D\\}, \\\\\nAM(R_q) & \\text{if more letters in } R_q, \\\\\nRS(\\{A, B, C, D\\}) & \\text{otherwise}.\n\\end{cases}$\nThe model output $R_q$ is first checked for validity as a single character from {A, B, C, D}. An Answer-Matching (AM) function identifies a valid option if the output includes multiple characters. The implementation of the AM function can be found in the Supplementary Materials. A random selection (RS) function from {A, B, C, D} is used to generate $A_q$ if no valid match is found.\nOnce $A_q$ is obtained, we evaluate its accuracy based on whether it matches the correct answer for each question. Let $S = \\{S_i = \\{Q_j\\}_{j=1}^{N_i}\\}_{i=1}^{|S|}$ represent our dataset, where each sub-task $S_i$ contains a set of questions $Q_j$ across a total of |S| sub-tasks. For each question $q \\in S_i$, let $G_q$ represent the correct answer, and $A_q$ represent the model's answer. The score for question q, denoted as $s_q$, is calculated as follows:\n$s_q = \\begin{cases}\n1, & \\text{if } A_q = G_q, \\\\\n0, & \\text{if } A_q \\neq G_q.\n\\end{cases}$                                                                                                                                                                                                                                                                       (1)\nA score of 1 is assigned if the model's prediction $A_q$ matches the correct answer $G_q$; otherwise, the score is 0. Each sub-task's accuracy $ACC_i$ is calculated as the average score across its $N_i$ questions: $ACC_i = \\frac{1}{N_i} \\sum_{j=1}^{N_i} s_{q_j}$, where $N_i$ is the total number of questions in sub-task $S_i$. The overall accuracy is computed as the ratio of total correct answers to the total questions across all sub-tasks:\n$Overall\\ ACC = \\frac{1}{\\sum_i N_i} \\sum_i \\sum_{j=1}^{N_i} s_{q_j}$  (2)"}, {"title": "3. Main Results", "content": "In this section, we analyze and quantify the video composition understanding capabilities of state-of-the-art MLLMs, providing a comprehensive evaluation of these models. For all experiments, we use a standardized prompt template and the default hyperparameters specified for each model.\nOverall Performance. As shown in Table 2, the overall performance on the VIDCOMPOSITION benchmark reveals that understanding intricate video compositions remains challenging for MLLMs. While humans achieve exceptionally high scores (86.26), the leading models, such as LLaVA-OneVision-72B [23] (63.31), InternVL2-40B [6] (60.73), InternVL2-76B [6] (58.7) and Qwen2-VL-72B [49] (58.78), demonstrate only moderate success, underscoring the complexity of the tasks and the current limitations in video composition understanding. This gap between human and model performance highlights the benchmark's rigor and the need for advancements in fine-grained video-based compositional learning. Open-source models with advanced vision components, particularly InternVL2 variants, outperform API-based models like GPT-40 [1] (52.93) and Genmini-1.5-Flash [43] (52.40). The mean overall accuracy of these MLLMs is 43.44. For reference, the random-choice baseline has a score of 25.33. While the models exceed it, they still face considerable obstacles in approaching human-level video composition understanding.\nStrengths & Weaknesses Analysis. From Table 2, we see that MLLMs generally perform better in CU tasks, particularly A-P and CMP-P. For example, top models like LLaVA-OneVision-72B [23], GPT-40 [1] and GPT-40 mini [1] achieve high scores in CMP-P; LLaVA-OneVision-72B [23] and InternVL2-40B [6] get 90.0 on A-P. This indicates that state-of-the-art MLLMs can effectively recognize and interpret actions and visual details of characters in a scene. Models also show strong performance on SP tasks such as B-P and L-P, with models like Gemini-1.5-Flash [43] achieves 83.1 on B-P and LLaVA-OneVision-72B [23] gets 90.3 on L-P. Additionally, these models achieve competitive scores on some MA tasks, such as AS-P and SE-P, with top scores reaching 89.5 and 85.2, respectively. This strong performance may be attributed to the fact that these tasks rely on some expert knowledge about video-making techniques, which MLLMs can acquire from massive corpora. Conversely, the models encounter significant difficulties in more complex compositional tasks, especially CA. For example, CamM-P and SS-P yield only modest scores, with top models reaching 57.1 and 63.9, respectively, reflecting a significant gap in understanding cinematic techniques. NU tasks, such as S-M and P-O, also present challenges, with model performance substantially trailing behind human benchmarks because there is often a gap between scripts and actual video presentation. Unlike humans, who can intuitively bridge this gap, MLLMs are fine-tuned on closely matched vision-text pairs, limiting their ability to interpret subtle or implied connections in narrative tasks. Additionally, counting tasks, such as Cha-C, S-C, and Cut-C, remain particularly problematic for most models, further underscoring the limitations in visual counting abilities and understanding scene transitions across multiple frames across all models."}, {"title": "4. Diagnostic Analysis of Factors Affecting MLLMs' Performance", "content": "In this section, we analyze the factors that may affect the MLLM's understanding of video composition. We focus on four factors: the number of input frames (#frm), the resolution of the visual encoder (Res.), the size of the language decoder (LLM size), and the volume of training data (Data volume) in the SFT stage. We provide full analysis tables and figures of each factor in Supplementary.\nThe Number of Input Frames. Across all the models, we consistently observe that the input frames don't contribute to the performance. As shown in the Figure 4 and Table 4, the overall accuracy is either stable or fluctuates randomly. We couldn't see any clear trends, although intuitively, extra frames would provide more information to help the model make decisions. We suspect that while more frames provide more information, this small amount of useful information is mixed in with a large amount of duplicate information, and the model cannot effectively extract it. This is against our expectation that it would bring benefits to counting tasks (Cha-C, S-C, Cut-C).\nThe Resolution of Visual Encoder. We observe that MLLMs with higher-resolution visual encoders perform significantly better. While the resolution is unchangeable for one specific model, we calculate the mean performance of all models with the same LLM size and video frames. As shown in Table 3, as resolution increases, performance on all five main categories increases consistently. However, it is worth noting that it is impossible to determine how much of this improvement is due to the higher-resolution visual encoder and how much is due to the different models themselves.\nThe Size of Language Decoder. To analyze this relationship more accurately, we compare models with different decoder sizes while keeping the encoder and training data constant. From Table 5, we observe that models with larger decoders demonstrate stronger performance, and the gains are mainly from NU, which requires the model not only to recognize individual frames but also to establish logical and causal relationships across sequences, a capability that benefits from a more powerful language decoder. Tasks in MA also benefit from the external knowledge acquired by LLMs.\nThe Volume of Training Data. We can observe the performance influence brought by fine-tuning the MLLMs with more data from Table 6. We compare models with the same configuration, e.g. the same number of input frames, the same resolution of the vision encoder, and the same or similar size of LLM adopted. The results indicate that larger data volumes lead to improved performance of video composition understanding in most cases.\nQualitative Analysis. We perform an error analysis to gain deeper insights into the models' shortcomings in fine-grained video composition understanding. In this analysis, the models are required to answer questions and provide explanations in a dialogue format. Figure 5 shows the examples where top models fail to predict correct answers. For example, while humans easily use visual context to distinguish camera movements and angles like \u201cpan left\u201d and \u201czoom in\u201d or angles like \u201ceye level\u201d and \u201clow angle,\u201d models like LLaVA-OneVision-72B [23] and GPT-4 [1] often struggle due to scene transitions and subtle perspective changes."}, {"title": "5. Related Work", "content": "MLLMs for Video Understanding. Equipping LLMs with adapted video encoders has led to the creation of several multimodal models tailored for video understanding [46]. For instance, GPT-4-turbo, GPT-40, and GPT-40-mini [1] are GPT-based models with integrated video comprehension capabilities. InternVL2 [7], with parameter counts ranging from 1B to 76B, is based on the InternLM framework [4] and supports video processing at multiple scales. Additionally, models derived from the LLaMA backbone\u2014such as LLaVA-OneVision [23], VILA [28], VideoLLaMA [8], and LongLLaVA [52] have been adapted for video input."}, {"title": "6. Conclusion", "content": "We introduce VIDCOMPOSITION, a novel and high-quality benchmark designed to evaluate MLLMs in understanding video compositions. Our benchmark incorporates various video types and QA categories, covering various aspects of video composition, e.g. camera movement, shot size, narrative structure, and character actions. Through VIDCOMPOSITION, we comprehensively assess MLLMs' abilities to understand complex video compositions. The evaluation reveals a significant performance gap between humans and models, shedding light on the limitations of current MLLMs and providing valuable insights for future improvements."}]}