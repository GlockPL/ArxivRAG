{"title": "RTBAS: Defending LLM Agents Against Prompt Injection and Privacy Leakage", "authors": ["Peter Yong Zhong", "Siyuan Chen", "Ruiqi Wang", "McKenna McCall", "Ben L. Titzer", "Heather Miller", "Phillip B. Gibbons"], "abstract": "Tool-Based Agent Systems (TBAS) allow Language Models (LMs) to use external tools for tasks beyond their standalone capabilities, such as searching websites, booking flights, or making financial transactions. However, these tools greatly increase the risks of prompt injection attacks, where malicious content hijacks the LM agent to leak confidential data or trigger harmful actions.\nExisting defenses (OpenAI GPTs) require user confirmation before every tool call, placing onerous burdens on users. We introduce Robust TBAS (RTBAS), which automatically detects and executes tool calls that preserve integrity and confidentiality, requiring user confirmation only when these safeguards cannot be ensured. RTBAS adapts Information Flow Control to the unique challenges presented by TBAS. We present two novel dependency screeners-using LM-as-a-judge and attention-based saliency-to overcome these challenges. Experimental results on the AgentDojo Prompt Injection benchmark show RTBAS prevents all targeted attacks with only a 2% loss of task utility when under attack, and further tests confirm its ability to obtain near-oracle performance on detecting both subtle and direct privacy leaks.", "sections": [{"title": "1 Introduction", "content": "Language Models (LMs) excel at complex tasks, using reasoning and planning when prompted with natural language instructions. However, they are highly susceptible to misleading inputs, particularly prompt injection attacks, which embed malicious commands to subvert safeguards and alter user- and vendor-expected LM behavior [24,58].\nMeanwhile, recent advancements have led to the development of Agents-advanced applications of LMs where LMs can interact with external environments by making API calls. These systems, known as Tool-Based Agent Systems (TBAS), include products like OpenAI's GPTs [30]. These systems allow LMs to utilize external tools to perform tasks beyond their standalone capabilities, such as summarizing emails, searching and summarizing websites, booking flights, or initiating financial transactions.\nThe risks of prompt injection attacks are far greater in the context of TBAS than in LMs alone. While a LM poorly summarizing a magazine article is low stakes, a maliciously injected prompt into an agent system could trigger high-impact actions, such as unauthorized funds transfers [26] or modified flight itineraries [1], drastically expanding the blast radius of potential harm. An example illustration of a prompt injection attack is shown in Fig. 1.\nRisks to user confidentiality are significant in the context of TBAS. Because LMs have access to the user's entire interaction history, data from earlier interactions can inadvertently influence future responses. Ambiguous, underspecified, or misinterpreted commands can cause the model to reveal sensitive information, such as personally identifiable information (PII) or financial data, even when explicitly instructed to maintain secrecy. Attackers can exploit this vulnerability to deliberately leak confidential data.\nThe risk of attacks on TBAS is so pronounced that the Open Worldwide Application Security Project has recognized Prompt Injection and Sensitive Information Disclosure as the"}, {"title": "2 Background and Related Work", "content": "Agentic AI Systems and Tool-Based AI Agents. The integration of external environments with LMs is often described as Composite AI Systems [52] or simply agents [48,57]. These systems leverage the LM's capabilities to comprehend natural language [32], perform reasoning [36, 47, 53] and planning [16, 25, 46]. Tool-Based Agent Systems (TBAS) [51], a subclass of LM agentic systems, operate in a single context and interact with external environments via tool calls. Widely adopted in applications like Bing's ChatGPT integration [39], TBAS also power platforms like OpenAI's GPTs [30] and CustomGPT.ai [9], enabling developers to customize agents with specific instructions and tools.\nPrompt Injection For LMs. A prompt injection attack [23, 24] occurs when malicious inputs, or prompts, are introduced into the agent's history (context) to alter its behavior. Prompt injections, often as natural language instruction pretending to be the user, but at times it could be nonsensical text making its detection even more subtle [58]. While users can initiate such attacks to bypass application-defined guidelines [22] or extract system prompts [50], our focus is on prompt injections originating from tools that retrieve data from untrusted sources such as other websites, public reviews, comments, etc. [10, 54]. These injections can maliciously manipulate the TBAS, causing it to perform unintended or harmful tasks.\nDefenses for Prompt Injection and Privacy Leakage. Defenses can be categorized into two strategies:\n\u2022 Injected Prompt Detection: Possible prompt injections can be identified using perplexity measures or another LM trained to flag anomalies [3, 17,35].\n\u2022 Prompt Impact Mitigation: These limit injection effects using (i) data sanitization approaches such as parapharsing [18], retokenization [18], delimiters, (ii) fine-tuning on non-instruction-tuned models [34], (iii) restricting tools based on user requests [10], or (iv) pretraining LMs to enforce hierarchies or improve instruction/data separation [8, 44].\nMost of these techniques are heuristic based and not conservative, nor do they allow an application developer to provide a security policy specifying allowed actions given a current integrity and confidentiality environment. Compared to RTBAS, data sanitization methods are heuristics driven and are subject to adversarial jailbreaking [22]; tool restrictions allow attacks using unrestricted tools; pre-training techniques are difficult to apply to commercial models, and still rely on the LM to ignore malicious prompts, albeit with greater difficulties.\nMuch research [6,21] has been completed on training time privacy concerns. Inference-time techniques have been focused on detecting outputs with possible PIIs [20] or desensitizing them before sending to the LM [15,41]. However, they typically are not focused on tool-based environments.\nInformation Flow for LLMs [40] explores the similar selective propagation approach. However, their mechanism requires enumerating all possible subsets of relevant prior input regions (documents in RAG) to identify the minimal subset that leads to similar outputs. As noted in their paper, the naive implementation of this mechanism incurs a worst-case complexity that is exponential in the number of prior input regions, potentially reaching thousands in the RAG scenarios. Even their optimized version still requires exponential enumeration with respect to the number of levels in a lattice, resulting in 16-64 additional LM calls with a typical lattice with 4-6 levels. In contrast, as we will discuss later, our mechanism employs a dependency analyzer that efficiently detects relevant input regions in parallel by a single call, reducing the computational overhead from exponential to constant. This fundamental improvement highlights the practicality of our approach. Lastly, unlike our technique, [40] does not specify the propagation of labels beyond labeling the response, which makes it inapplicable to interactive settings such as tool-calling. There is also no mechanism to verify the computed label against allowed policy or solicit user confirmations.\nAttention Score as a Measure of Saliency. Attention scores [43, 49], which measure a transformer-based model's \"focus\" on past tokens, is a widely-used technique in the machine learning community to explain a neural network's internal processing [19, 45], prune irrelevant input texts [56], etc. In this work, we leverage attention scores as an input to the dependency screener, as they capture the degree to which output tokens are influenced by specific input regions."}, {"title": "3 Motivation", "content": "3.1 Prompt Injection as an Integrity Concern\nIntegrity in the context of TBAS ensures that the agent's actions and outputs align faithfully with user requests and the system's intended purpose. TBAS assists users by calling provided tools to perform actions or retrieve helpful information. Tool responses, however, can contain untrusted, or low-integrity content containing injected prompts. To maintain integrity, the system must safeguard against unauthorized modifications, especially when using integrity-sensitive tools. For instance, tools capable of spending money, sending messages on behalf of users, or performing actions with significant side effects must not execute commands originating from untrusted or compromised inputs.\nConsider the following scenarios:\n\u2022 Website Content: fetch_website, fetches content from a website. An attacker can plant malicious text on the website, which is then returned by the tool. The tool itself remains uncompromised-it faithfully fetches the content as designed, but the attacker controls the underlying data source.\n\u2022 Venmo Description: get_recent_transaction retrieves the user's recent transactions, including their descriptions. An attacker can plant a malicious prompt in the description of a transaction (see Fig. 1), which went unnoticed at the time (e.g., here the user may not have paid attention to such a small incoming transfer nor noticed that the description extended to a second paragraph).\nHowever, there are genuine scenarios where low-integrity input is necessary to affect integrity-sensitive tools. For such interactions, our goal is to ensure the user is made aware of such possible security violation but to seek their confirmation as the final arbiter of whether to allow a suspicious call to achieve task utility.\n3.2 Tracking Confidentiality Leakage\nIn a TBAS, confidential data propagates in diverse fashion, making it challenging to track and analyze. Private information may be explicitly required by user instructions or implicitly utilized (e.g., using credit card details to complete a purchase). It can also be employed during intermediate reasoning steps (e.g., using a user's preferences to recommend new products). The flow of such data can be subtly influenced by tool descriptions or system instructions (e.g., a \"Book Flight\" tool specifying, \"Include frequent flier number when booking\"), potentially in ways the user does not anticipate, even when the behavior is not inherently malicious.\nDespite the variety of ways confidential data can be used, our technique ensures that its flow is always tracked conservatively. This approach guarantees that for every potential leakage to an external environment, the user is either explicitly informed and provides active confirmation, or implicitly approves the disclosure by agreeing to an established information flow policy.\n3.3 Attention Score\nThis section motivates the attention-based approach to capture the selective propagation of information in the LM. Following common practice, we use the Taylor expansion of the loss function [27] to calculate the attention score (a.k.a, importance score) for every input token, output word token pair, which is defined as\n$A_{i,o} := \\frac{LLM(Output, Input) - LLM(Output, Input \\backslash i)}{LLM(Output, Input)}$  (1)\n$\\mathcal{A}LLM(Output, Input) = \\sum_{h,l} \\frac{\\partial A_{h,l,i,o}}{2^1}.$  (2)\nHere, $A_{h,l,i,o}$ is the value of the attention matrix of the o-th output token on the i-th input token of the h-th attention head and l's network layer, Input\u012f is the input tokens with the i-th token masked, and LLM(Output, Input) is the loss of the o-th output token on the input. The importance score captures the difference in the loss function before and after the i-th token is masked, and is averaged across attention heads and layers. Intuitively, attention scores measure how much \"surprise\" the LM receives when masking out certain tokens, where a higher attention score indicates a stronger dependency between the output and the input."}, {"title": "4 Tool-based Agent Systems", "content": "Figure 4 illustrates a high-level overview of TBAS. This section provides a concrete description of the TBAS model relevant to our techniques. We assume a user interacts with the agent through a chat interface, similar to ChatGPT or Gemini. The user submits a request, and the agent attempts to fulfill it by leveraging its internal knowledge, tool calls, and prior interactions within the same session.\nTo illustrate how TBAS work more concretely, we first present some relevant terminologies:\nSymbols and Terminologies:\nMessage\nm\nMessages\nM\nTool Call with inputs i\n$t^{i}$  (3)\nExternal Environment\nE\nMetafunctions:\nGET_NEXT_TOOLCALLS:\nM \u2192 ToolCalls\nCALL_API\n$t^{i}$xE\u2192mXE\nLM RESPONSE\nM\u2192m\nUSER_REQUEST\nM\u2192m\nUSER_CONTINUE\nM\u2192 TRUE | FALSE  (5)\nThe TBAS agent is initialized with a system message (ms) provided by the agent developer, which defines the agent's role and tone. The developer also supply a list of tools, each defined by its name, signature (describing the legal invocation format), and descriptions. These tools correspond to APIs callable by the runtime.\nThe agent's application state, or history, consists of all messages exist in the system: the initial system message, user-issued requests, tool outputs, previous LM responses from the assistant. These elements are concatenated into a text-based input state, which the LM processes to decide its next action-whether to respond to the user or invoke a tool.\nAt the start of a session, only the initial system message (ms) is present. When the user sends a request based on their initial request or responding to previous interactions, a message is appended to the current history(USER_REQUEST). Based on this input, the LLM generates zero or more Tool Calls(GET_NEXT_TOOLCALLs).\nThe runtime inspects the Tool Call sections and executes the corresponding APIs specified by the developer(CALL_API). The results from the tool calls are collected and are also appended to the history. The LM then processes the entire updated history and generates another message to provide the user with a response (LM_RESPONSE).\nIf the user wishes to continue with this conversation, the process is started afresh(USER_CONTINUE). We illustrate this process in Algorithm 1."}, {"title": "5 Attack Model for Prompt Injection", "content": "Attacker's Goal. The attacker seeks to manipulate the interactions between the user and the Tool-Based Agent System (TBAS) by influencing the tool calls the TBAS might make. These manipulated tool calls can result in the leakage of confidential information or introduce harmful side effects. All malicious goals must rely on the side effects of these tool calls: e.g. using message sending tools to transmit credit card information or exploiting money-transfer tool to steal the user's money.\nAttacker's Capabilities. We assume the attacker has detailed knowledge of the TBAS setup, including the system instructions, the tools available to the TBAS and the specific instructions for each tool. However, the attacker does not have knowledge of timing mechanisms, nor do they have access to the internal state or behavior of the agent itself. The attacker"}, {"title": "6 Robust TBAS Objectives and Assumptions", "content": "6.1 Objectives\nUnder prompt injection attacks and other sources of confidential data leaks, our primary goals of robustness is to:\n\u2022 Prevent private data leakage \u2013 Ensure that user's private data is not passed to external environments without explicit user confirmation.\n\u2022 Defend against prompt injection \u2013 Ensure that attacker instructions do not lead to unwanted side-effects that compromises the integrity of the user's system.\nOur secondary goals are:\n\u2022 Maintain Utility under attack \u2013 Minimize disruptions to user tasks, even under possible prompt injection attacks.\n\u2022 Minimize overhead \u2013 Minimize unnecessary compute or user confirmations to achieve the above goals.\n6.2 Assumptions\nAs is standard in Information Flow research, we assume that these labels on both the User and Tool messages are provided to our system. We acknowledge this is a open problem in IFC and will likely be a burden upon the agent developers to provide a lattice of security labels and an information flow policy on these labels.\nMore formally, we assume that the developer provides (L, \u227a,\u2294) where\n\u2022 L is a finite set of security labels, where each label consists of a pair of integrity label and confidentiality label.\n\u2022 \u227a is a partial order representing the \u201cflows-to\" relation, which determines whether information can flow from one label to another.\n\u2022 \u2294 is a join operation that computes the least upper bound of two labels within the lattice.\nFor example, in a simple four point lattice, where confidentiality levels are divided to Secret and Public and integrity levels are divided to Trusted and Untrusted, L is defined as:\nL = {(Trusted, Public), (Untrusted, Public), (Trusted, Private), (Untrusted, Private)}  (6)\nInformation can only flow to a category that is at least as restrictive as its source, ensuring integrity and confidentiality are preserved; the \u2294 operator is reflexive since information remains in the same category, and the figure below illustrates the flow-to (\ub4dc) relation in a 4-point lattice with trust and sensitivity levels.\nOur technique can generalize to a more complex and fine-grained lattice. Like many information flow problems, there are cases where a more precise lattice can lead to precise results. For example, drawing inspiration from [28], confidentiality can be represented as the set of channels that are allowed to access information, while integrity reflects the set of sources that have influenced it.\nFurthermore, we assume the developer and the user jointly specify the information flow policy P that denotes security restrictions on a potential tool-call.\nP: t \u2192 L  (7)\nA tool call $t^{i}$ is only allowed to proceed if it is called from an environment with label $(l_i, l_c) \u2208 L$ such that $l \u227a P(t^{i})$.\nWe assume that both the tool's response and user messages are also sources of labels, containing regions that are labeled with either low-integrity data from external untrusted sources"}, {"title": "7 Approach", "content": "Dependency analysis forms the basis of our selective information flow mechanism. The dependency screener analyzes the history to identify relevant regions before the system proceeds. These screeners operate on a LM's full history, where parts of the history are divided into non-overlapping regions, each annotated with confidentiality and integrity labels. Regions without explicit labels are treated as having the most permissive label (public and trusted). We first describe the two approaches that we developed to estimate the dependent regions for a particular generation.\n7.1 LM-Judge Approach\nLMs have demonstrated remarkable abilities in reasoning [47,53] and reflection [36], making them well-suited for tasks that require judgment or decision-making. This has led to the increasing popularity of using LMs as judges [12]. In our work, we adopt this methodology as one implementation of the dependency screener. To achieve this, we tag every region in the TBAS context with easily recognizable markers, such as \u00abREGION_N\u00bbregion content goes here\u00ab/REGION_N\u00bb, ensuring that the LM can clearly identify and differentiate between regions. We employ prompt sandwiching [38] where we provide instructions in both the system message and the final message in a long context. We also employ GPT-4's capability to enforce a specific tool call to ensure the reflected regions are well-formed.\n7.2 Attention-Based Approach\nMotivated by the case studies in \u00a73.3, we design a neural network to map the features of attention of the dependency relationship.\nProblem Formulation. We formulate the dependency analysis into a sequential binary classification problem. In particular, every input of a data point has two fields:\n\u2022 I, O: the input and output text generated by potentially LMs,\n\u2022 T = (b1,e1), (b2,e2), ..., (bn, en): a list of regions needed for dependency analysis.\nA classifier C maps the input text, output text, and input regions to list of boolean variables on how whether the output is dependent on any input regions. Namely: C(I,O,T) \u2192 d1,d2, ..., dn \u2208 {0, 1}.\nClassifier Design. We design our classifier by first extracting the attention features from the input-output text using an open-source LM, and then mapping the features to the dependency predictions by training a neural network.\nIn particular, we extract attention features ak for every region k by adopting common statistical measures:\n\u2022 Normalized Attention Sum/Mean:\n$\\frac{\\sum_{b_k i e_k} A_{i}}{\\sum_{i} A_{i, o}} \\times \\frac{I}{e_k - b_k}$ \n$\\frac{\\sum_{b_k i e_k} A_{i}}{\\sum_{i} A_{i, o}}$\n\u2022 Normalized Attention Quantiles: 20-th, 50-th, 80-th, 99-th Quantiles of normalized attention scores within the input region.\nAfter extraction, every region has a list of attention features; we now map it to dependency scores using a neural network. Since the input regions follow a natural temporal pattern, we deploy a recurrent neural network (RNN) to iteratively generate whether the output depends on the current input region. Namely, for a network f parameterized by \u03b8, the classification performs by $d_i, s_k = f_\u03b8(s_{k\u22121},a_k), i = 1, 2,...,n$.\nIn practice, we found that a lightweight two-layer LSTM [14] network suffices to obtain decent performance to uncover the dependency relationship beneath attention features.\nImplementation and Deployment. For the experiment, we collect the dataset using 40 well-labeled test cases from AgentDojo. The offline evaluation shows 85% train accuracy and 81% test accuracy. When deploying the classifier, the local feature extractor LM and the trained classifier are invoked each time the LM generates an output to track dependencies. As both the local models and the classifier are lightweight, this process introduces minimal runtime overhead to the TBAS."}, {"title": "7.3 Robust TBAS", "content": "To extend TBAS with taint tracking, we introduce Robust TBAS (RTBAS), an extension of traditional TBAS that performs the propagation of security metadata during interactions. We present a simplified view of our mechanism below where we assume that all content within the same message is annotated with uniform security metadata (i.e., each message is a single \"region\"). However, our implementation supports finer granularity, allowing individual messages, such as user messages or tool responses, to contain multiple regions, each with distinct security labels.\nTerminologies and Definitions. To present RTBAS, we extend the symbols and terminologies presented in List 3 where we use \u25ca to represent redacted content. We also modify the definitions presented in List 4 as the runtime needs to keep track of security labels on messages. And that some messages are masked or redacted to maintain security guarantees. We"}, {"title": "8 Evaluation", "content": "In this section, we benchmark our techniques in addressing the security threats for TBAS, that is, prompt injection and privacy leakage. We aim to answer the following questions:\nQ1: Under scenarios with prompt injections, how well does our system maintain integrity and utility compared to state-of-the-art defenses?\nQ2: Under scenarios with privacy leakage, how much excessive user confirmations do we burden the user and whether utility is degraded compared to baselines?\nQ3: How accurate is our detector in determining the information flow within the LM and what is its runtime overhead?\n8.1 End to End Evaluation: Prompt Injection Setup\n8.1.1 Test Suites. We benchmark our system on AgentDojo [10], a state-of-the-art benchmark on agent adversarial robustness against prompt injection attacks. Shown in Tab. 1, the dataset consists of 79 realistic user tasks in four suites: banking, travel, workspace, and slack. Every test suite represents a TBAS application where LLM serves user's request using a given set of tools, e.g. send_money for the banking suite and reserve_restaurant for the travel suite. Every test case in a suite requires the LLM to solve a task with multi-round interaction with external tools such as booking a restaurant after filtering through reviews and datary restrictions.\nData Labeling. To integrate the information flow mechanism, we enhance the task suites by assigning integrity labels based on the application's requirements while remaining agnostic to specific test cases (examples are shown in Tab. 1). The labeling process follows these key principles to satisfy the assumptions we denote on the tool environment in 6.2:\n\u2022 Regions in a tool responses that incorporates textual data from external sources is labeled as low-integrity.\n\u2022 Tools with significant side-effects (e.g., sending money) or those can introduce high-integrity data to the external environments (e.g. sending messages) are labeled as high-integrity.\nPrompt Injection Attacks. To emulate prompt injection attacks, each test suite includes a set of injection tasks. These tasks aim to induce the agent to misuse tools and produce harmful side effects, such as making unintended reservations on behalf of the user or leaking user's private data through public channels like emails. When evaluating the benchmark under Prompt Injection attacks, each user task is tested against every injection task in the corresponding test suite, resulting in a total of 629 security test cases."}, {"title": "8.1.2 Results and Analysis", "content": "We present the results of the AgentDojo dataset both with (Figure 5) and without (Figure 6) prompt injection attacks. A cost comparison of running our techniques as a measure of overhead (Table 4) is also provided.\nImportantly, the lack of user confirmations and the subsequent rejection of all apparent suspicious tool calls means that if we are able to seek user confirmations for calls that inheritently depend on low integrity data or in the case of over-tainting, then we are likely to achieve even better performance.\nWithout Attacks. We present the results without attack in Fig 6. The impact on utility is best illustrated by the difference between the baseline case (no defense) and our techniques. Specifically, we observe a 10% and 7.4% degradation in utility for the LM-Judge and Attention-based detectors, respectively. Interestingly, the Tool Filter technique slightly increases utility in the absence of attacks. We speculate that this improvement arises from an implicit planning step, where irrelevant tools are excluded from LLM consideration.\nOur approach performs particularly well in the travel and workspace suites. As illustrated in the results in these suites, our approaches consistently achieve the highest utility among other methods, even exceeding the baseline(no defense) case by 5% on average. These task suites naturally align with a more fine-grained integrity lattice and precise security policy, reducing scenarios that require manual user intervention.\nThe Slack dataset, however, emerges as an outlier for our techniques. While our approaches still outperform naive tainting and redact-everything methods, the utility drops to 33% and 22% for the Attention-based and LM-Judge detectors respectively, which is more than halved compared to the mean utility. This performance drop can be attributed to the nature of Slack tasks, which often involve variations in reading content from untrusted websites and performing actions based on that content. We consider such tasks inherently unsafe, necessitating user confirmation.\nAdditionally, some tasks require the agent to send a summary of an untrusted website to a high-integrity source (e.g., posting the summary to a Slack channel). If left unchecked, such actions could compromise the high-integrity source by spreading prompt injection attacks like a virus or conveying unintended statements.\nUnder Attack. We present the result when under prompt injection attack in Fig 5. RQ1: Our techniques still retain a high utility compared to the baseline without defense, only losing less than 1% utility for the LM-judge screener and 3% for the Attention-based screener.\nWe note that we do prevent 100% of attacks that violate our security policy. However, in the workspace benchmark, there was one test case where text written by the user, labeled as high-integrity, contained possible prompt injection and is thus not tracked. This illustrates a major limitation for our mechanism, as with any other IFC techniques, that the security guarantees provided are only as good as the labels provided and the policies enforced."}, {"title": "8.2 End-to-End Evaluation: Privacy Leakage", "content": "This experiment evaluates different defenses against the privacy leakage threat, e.g. accidental reference to chat history, silently booking a restaurant without user's confirmation. For every tool call the LLM makes, the defense mechanisms"}, {"title": "8.3 Analysis", "content": "8.3.1 Taint Tracking Accuracy\nWe augmented the Privacy Leakage benchmark with precise labels that represents the sets of private information category involved. We evaluate, for every tool calls, how often these labels matches exactly the ground truth label we annotated(Q3).\nThe user, through this label, can gather more information about the category of data that the tool call purports to leak. A user comfortable with leaking their credit card number to book a flight may be hesitant to share her social security number.\nA mislabeled tool call with more private data categories than actually propagated could be erroneously rejected either interactively or by reference to the policy that the user agrees to prior. Oppositely, a label claiming less private data categories can distort the task, with actually relevant data masked.\nWe show that the selective propagation approach, when instantiated by either the prompting or the attention approach arrives at the exact ground truth label more than 70% and 57% of the time, respectively. This is superior to our baseline techniques for redacting all sensitive regions and thus propagate nothing or the always confirm method where we assume a tool call always leak every secret.\n8.3.2 Dependency Screener Comparisons\nFor the Prompt Injection and Privacy Leakage benchmarks, we find that the LM judge and the Attention-based dependency screener perform similarly across the benchmarks, with LM Judge performing slightly better overall under attack for Prompt Injection and much better in terms of its detection accuracies for privacy leakage(Tab 2). We conject the LM judge's ability to explicitly reason about the dependencies and output its chain-of-thought [47] could help generalize the mechanism across unseen task, and for more subtle propagation cases. However, across end-to-end benchmarks, both methods perform similarly with respect to the overall task utilities. This suggests that the Attention-based approach can detect important dependencies crucial to task success, but"}, {"title": "8.3.3 Runtime Overhead", "content": "Q3: Our techniques incur higher costs compared to existing methods, primarily due to the overhead introduced by the detectors. The Attention-based detector requires the LLM to run twice: the first run generates a preliminary message, which is used for the attention mechanism to compute dependency results. The second run generates the final output after masking. The LM Judge screener also incur computer overhead by running the judge LLM before the agent generates each new message. In contrast, the tool detector only runs one additional inference for each user message but not between tool calls, and the Prompt Sandwiching approach only marginally increases the number of tokens by repating the user requests. We discuss opportunities for optimization in Sec. 9."}, {"title": "9 Discussion", "content": "Labeling. One limitation of our technique, common in IFC research, is the need for labeled tool and user messages, along with an information-flow policy understandable to users. However, many applications naturally support region-based labeling, particularly when addressing prompt injection and confidential data leakage. For example, in a get_email response, fields like Subject and Content could be labeled low-integrity due to susceptibility to natural language injections, while Sender might be high-integrity due to strict schema requirements. Similarly, tools may handle sensitive information; for instance, in a finance application, a get_account_balance response could be marked high-confidentiality to prevent accidental or malicious leakage. Recent works on using LLMs for formal safeguards [5] and privacy policy interpretation [7,42] offer promising directions to bridge understanding gaps.\nCost. Operating both of our dependency screener methods is currently resource-intensive. The attention-based screener requires the agent to generate a preliminary message to analyze attention scores between regions, followed by a second message based on different input during the selective masking process. A potential optimization involves using a smaller model to generate the preliminary message, as it is not part"}, {"title": "10 Conclusion", "content": "We present RTBAS, a fine-grained, dynamic information flow control mechanism to safeguard Tool-based LLM Agents against both prompt injection and inadvertent privacy leaks. The mechanism selectively propagates only the relevant security labels, through the use of the LM-Judge and Attention-based screeners. The redaction of unused data enforces the information flow policy for all possible selective propagation.\nEmpirically, we manage to curb malicious manipulations and detect undesirable confidential data disclosures. Notably, our evaluation on the AgentDojo benchmark shows that when under prompt injection attacks, the proposed RTBAS framework thwarts all policy-violating exploits with less than 2% degradation to the agent's task utility. Similarly, our privacy leakage benchmark confirms RTBAS' ability to obtain near-oracle performance."}, {"title": "11 Ethics considerations", "content": "Our experiments were conducted using publicly available benchmarks and constructed datasets explicitly designed for this study. No real-world user data or personally identifiable information (PII) was involved in the development or evaluation of our methods. The attacks explored in this research are well-documented within the community and do not necessitate additional disclosure. Additionally, our experiments included subjecting language models to potentially harmful tasks and simulating attacks on TBAS applications. We have received assurances from OpenAI, the language model vendor, that these interactions will not be used for training or improving their models."}]}