{"title": "HindiLLM: Large Language Model for Hindi", "authors": ["Sanjay Chouhan", "Shubha Brata Nath", "Aparajita Dutta"], "abstract": "The advancements in the Large Language Model (LLM) have helped in solving several problems related to language processing. Most of the researches have focused on the English language only, because of its popularity and abundance on the internet. However, a high-performance language model for Hindi and other Indic languages is lacking in the literature. In this work, we have pre-trained two autoregressive LLM models for the Hindi language, namely HindiLLM-Small and HindiLLM-Medium. We use a two-step process comprising unsupervised pre-training and supervised fine-tuning. First, we create a large and high-quality text corpus for unsupervised pre-training. Next, we train a Byte-Pair Encoding, named HindiLLM tokenizer, using the pre-training text data. We then perform training on the unlabeled data, known as the pre-training step, to get the HindiLLM base models. Furthermore, we perform fine-tuning of the HindiLLM base models for different tasks like sentiment analysis, text classification, natural language inference, and multiple choice question-answer on popular labeled datasets to measure the real-world performance. The evaluation shows that the HindiLLM-based fine-tuned models outperform several models in most of the language related tasks.", "sections": [{"title": "1 Introduction", "content": "The understanding of a language by a machine is of great interest in the Natural Language Processing (NLP) domain. The words and phrases used in a sentence can have different meanings in different contexts of a sentence. Also, there is the need to identify synonyms of a word, sarcastic phrases, idioms, and errors in the text of a language. The language models in NLP can perform these tasks for the English language as there is a rich collection of datasets in the literature and it has been highly researched over the years. Most of the other languages, especially Indo-Aryan languages or Indic languages, are less researched and have fewer resources as their presence on the internet is limited."}, {"title": "2 Related Work", "content": "There is a scarcity of research on the language models for Hindi language and other Indic languages. Hindi speakers are present all over the world and yet it is\nless-researched in the field of NLP. One major reason behind this is the limited\npresence of Indic languages online in written form. Now, we discuss the related\nworks present in the literature.\nAlthough our work focuses on the Hindi language, we can apply the language\nmodel techniques across different languages. This will enable the less-researched\nlanguages to benefit by utilizing the techniques mentioned for extensively re-\nsearched languages."}, {"title": "3 Dataset", "content": "The dataset generation is the first step of any model building process. The richness of the data determines the quality of the model. The approach used in this paper requires two types of data, unlabeled data for unsupervised pre-training and labeled data for supervised fine-tuning. The details of the datasets used in this paper are mentioned in this section."}, {"title": "3.1 Pre-Training Dataset", "content": "For the unsupervised pre-training step, we need a large corpus with Hindi text written in Devanagari script. Since pre-training is the most crucial step which helps the model in understanding the language and its nuances, we need the corpus to be clean and have valid long paragraphs. This implies that it should not have unnecessary symbols, words, web-links and characters which we do not see in typical Hindi literature. A paragraph or sentence is valid if it follows the grammar of that language and it makes sense to the native speaker of that language. Long paragraphs are good for introducing long-term dependencies in the model. Generally, we need to scrap the internet for large corpus but this leads to difficulty in finding usable Hindi sentences or paragraphs. There exist projects which focus on getting web-crawled texts. In these projects, these corpora are classified based on language and are preprocessed following the structure of that language. We have used such existing corpora for the pre-training step."}, {"title": "3.2 Fine-Tuning Dataset", "content": "The fine-tuning is done for downstream tasks. The performance on downstream tasks tells about the real-world applicability of the model. In this paper, we have chosen seven downstream tasks to measure different aspects of our models.\nSentiment Analysis: We have two sentiment analysis datasets, namely IITP Movie and IITP Product. These datasets are public and widely used for evaluating Hindi language models. The dataset comprises three classes: positive, neutral, and negative. We have combined both datasets for training but tested separately.\nText Classification: For multiclass classification evaluation, we have used BBC News category classification dataset. It has six categories. These are India, international, news, entertainment, sports, and science. It is also a public dataset which has been used for testing multiple Hindi based language models.\nNatural Language Inference: We analyzed the natural language inference capability of our models with the BBC NLI dataset.\nCloze-style Multiple-choice QA (CSQA): The CSQA dataset is from IndicGLUE benchmark dataset. This dataset has a masked entity in a given text and we are given four candidate entities out of which one is the correct entity. This dataset is created from Wikipedia articles."}, {"title": "4 Methodology", "content": "We follow two steps training process in this work. We apply an unsupervised pre-training to get the base model and a supervised fine-tuning of the base model for the downstream tasks. However, prior to the training, we build the HindiLLM tokenizer using the BPE algorithm. In this section, we have provided a detailed description of the tokenizer and the models along with the training process."}, {"title": "4.1 Tokenizer", "content": "Since we focus on building the model for the Hindi language and the default GPT-2 model is primarily for the English language. So, we train a new tokenizer called HindiLLM tokenizer. The idea of training a custom tokenizer is to reduce the fertility score (average number of tokens per word) for the Hindi language. We have trained a Byte-level BPE tokenizer with our pre-training corpora which contains mostly Hindi language written in Devanagari script. Since the Devanagari script is complex, the Byte-level BPE tokenizer is the most suitable option. We have used the whole pre-training dataset to train the BPE tokenizer. We have kept the desired vocabulary size as 50000 while using the trainer to accommodate most of the frequently occurring sub-words. Also, we added 8 special tokens like CLS, SEP and PAD afterwards, so the vocabulary size reached to 50008.\nWe show the output of the HindiLLM tokenizer in Figure 1. There are four sentences (three Hindi and one English) and its corresponding tokens as to-kenized by our tokenizer. We can see that the words are split into multiple sub-words. For example, the word \"\u0906\u092a\u0915\u093e\" is tokenized into \"\u0906\u092a\u0915\" and \"\u093e\" but the word \"\u0906\u092a\" is a token on its own. The HindiLLM tokenizer is able to tokenize both Hindi and English sentences."}, {"title": "4.2 Unsupervised Pre-Training", "content": "As mentioned in the GPT-1 work, the pre-training finds a good initialization point for the model. In pre-training, the model learns about the language such as morphology and syntax. The Causal Language Modeling (CLM) is used in the unsupervised pre-training step of autoregressive models. This step gives us a base model that can be supervised fine-tuned for several downstream tasks with a relatively smaller dataset. We have trained two models, HindiLLM-Small and HindiLLM-Medium corresponding to GPT2-small and GPT2-medium respectively. We use Hugging Face's Transformers library for pre-training. The training process involves creating a model with the configuration of corresponding GPT-2 models and training it after initializing with random weights.\nHindiLLM-Small Model: As shown in Table 2, the HindiLLM-Small model is equivalent to the GPT2-small model which has 124,439,808 trainable parameters. For training, we have taken 19.6 GB of data from our pre-training dataset. We have taken approximately half data from all corpora mentioned in Table 1. There are 6,904,242 examples for training a context window of 1024 tokens. We have used a batch size of 16 for training and updated the weights 431,516 times, which is equivalent to 1.45 epochs. The optimizer is a torch based AdamW optimizer with a learning rate 5e-05. Full precision training on two\nHindiLLM-Medium Model: As depicted in Table 3, the HindiLLM-Medium model is based on the same configuration as of GPT2-medium model which has 354,823,168 trainable parameters. For training, we have 37.34 GB of data as mentioned in Table 1. There are 11,351,587 examples for training with the same context window as HindiLLM-Small. The total optimization steps are 354,737 with a batch size of 32 which is equal to 1.24 epochs. The optimization steps are less as compared to HindiLLM-Small because we have doubled the batch size. The optimizer and learning rate are the same as HindiLLM-Small. Similar to the HindiLLM-Small model, we have used two NVIDIA A100-PCIE-40GB GPUs for performing full precision training, which took 25 days including the tokenization and evaluation steps.\nPerformance Evaluation of Pre-trained Models: As shown in Table 4, the HindiLLM-Medium is better than HindiLLM-Small in terms of all the metrics."}, {"title": "4.3 Supervised Fine-Tuning", "content": "The next step in the semi-supervised training approach is the Supervised Fine-Tuning (SFT) on discriminative or generative tasks. The fine-tuning step aligns the model with the downstream task. The previous pre-training step makes it easier to fine-tune because it has already gained knowledge about the language. Hence, we can achieve higher performance on downstream tasks even with a smaller dataset. The model can be fine-tuned for variety of tasks. Here, we have fine-tuned for seven tasks on the datasets mentioned in Section 3.2. Since the model will be used on real-world downstream applications, the SFT and evaluation of the resultant model are the crucial steps.\nWe have used deepspeed library for fine-tuning efficiently using multi-GPU setup. The SFT is done in bfloat16 precision format with a learning rate of 5e-6."}, {"title": "5 Performance Evaluation", "content": "In this section, the performances on the downstream tasks are evaluated. We have done SFT for a variety of downstream tasks to get a detailed performance measure of the HindiLLM models. The results are compared with other models to validate its improvement."}, {"title": "5.1 Public Classification Dataset", "content": "As shown in Table 5, we compare various models on public classification datasets. We have accuracy scores from Wikipedia (FT-W), Wiki+CommonCrawl (FT-WC), IndicFT , IndicBERT , mBERT , XLM-R , INLP and iNLTK models. Also, we have obtained scores from the GPT-3.5 Turbo model. For the GPT-3.5 Turbo model, we have considered zero-shot prompt-ing and few-shot prompting. We have performed prompt engineering to find the best system prompt and in the case of few-shot prompting, we have given five random examples from the training data. We can observe that the HindiLLM-Medium model surpasses all the models on all three datasets: IITP-Movie, IITP-Product, and BBC-Article public classification datasets. The HindiLLM-Small model comes second in the case of IITP-Movie dataset. It does not perform well on the BBC-Article dataset. For IITP-Movie dataset, we see an improvement of 2.34% and 10.17% in HindiLLM-Small and HindiLLM-Medium models, respectively. For IITP-Product dataset, we see a decrease of 2.34% and an increase of 0.34% in HindiLLM-Small and HindiLLM-Medium models, respectively. We observe an improvement of 2.29% for HindiLLM-Medium model in BBC-Article dataset, but a drop of 7.46% for HindiLLM-Small. Both the zero-shot and few-shot results from GPT-3.5 Turbo are poorer than both of our models. In most cases, the result of few-shot is worse than the zero-shot. This is possibly because the given examples are confusing the model or because with a increase in the prompt length, the model is not able to analyze accurately."}, {"title": "5.2 IndicGLUE Benchmark Dataset", "content": "Table 6 shows the accuracy score achieved on the IndicGLUE benchmark dataset. We have considered XLM-R, mBERT and IndicBERT models for com-parison along with zero-shot and few-shot prompting of GPT-3.5 Turbo. In the CSQA task, the GPT-3.5 Turbo Few-shot has the highest accuracy and our HindiLLM-Medium model has the second best accuracy which is a drop of 6.13%. HindiLLM-Small has the sixth best result with a drop of 12.31% on this task. For the WSTP task, mBERT shows the best result whereas we see a drop of 2.93% and 10.27% for HindiLLM-Medium and HindiLLM-Small, respectively. For the DM task, HindiLLM-Medium gives the best accuracy with an improve-ment of 0.54%, followed by XLM-R and HindiLLM-Small. We do not have any score from IndicBERT-Large model on this task."}, {"title": "5.3 Comparison with GPT-2 Models", "content": "We have fine-tuned the default GPT-2 along with our models for the Sentiment Analysis dataset (a combination of IITP-Movie and IITP-Product datasets) and BBC-NLI dataset. We have used the same train-test split for fine-tuning both GPT-2 and HindiLLM model. The fine-tuning process was the same for both kinds of models. From the results shown in Table 7 and Table 8, it is evident that there is a huge improvement in the performance on Hindi downstream tasks using HindiLLM models. Even the smaller HindiLLM-Small model surpasses the scores of GPT2-Medium models by a large margin."}, {"title": "5.4 Machine Translation Dataset", "content": "In machine translation, we have considered both English-to-Hindi and Hindi-to-English translation tasks using the same set of train and test data. For ma-chine translation, we have only considered the HindiLLM-Medium model. The smaller model will struggle a bit here because it is a generative task. Since the Hindi language is morphologically rich, it is unfair to use traditional metrics such as BLEU and METEOR. Hence, we have performed human evaluation of the translations using the criteria mentioned in Table 9.\nTable 10 shows the performance of the machine translation task. We have shown the probability distribution of the scores. The English to Hindi task per-forms slightly better than the Hindi to English task. We have a small portion of data for score 0, which is good. But for score 4 as well we have a small portion of data. Even when the score is 3, the translation quality is good. We see that a substantial portion of data for the Hindi to English task and a major portion of data for the English to Hindi task have a score of 3. It indicates that the trans-lation quality is good. The mean scores are above average. Considering that we have used limited English data in pre-training, the results are promising."}, {"title": "6 Conclusion", "content": "In this paper, we train a tokenizer and two auto-regressive models of different sizes for the Hindi language written in Devanagari script. We check the validity of the models by comparing the results on multiple downstream tasks. By looking at the performances, we conclude that the pre-trained models are well-trained to handle a variety of downstream tasks.\nAs we see the performance evaluation of downstream tasks, in most of the cases our HindiLLM-Medium model shows the best results. The HindiLLM-Small model lags because of its smaller size and smaller pre-training data. It is clear from the results that HindiLLM will contribute to solving real-world problems, especially the HindiLLM-Medium model. We also note that our HindiLLM mod-els perform better than the fine-tuned GPT-2 and prompt engineering GPT-3.5 Turbo model. This implies that training a language-specific model will result in better performance for that particular language even with a smaller model. Also, it is evident from the results that training a larger model on a larger dataset will result in a better performing model.\nEven though the models show impressive results, there is scope for further improvements. The number of epochs during training can be further increased. The training can be performed with enhanced data like the text from the books. Training a larger model on larger pre-training data will always result in a better model. Our model has limited knowledge of English since a small portion of our dataset comprises English data. The English data can be increased to enhance the bilingual capability. Furthermore, adding a few more supervised fine-tuning tasks can add to the assurance of the quality of the model.\nIn the future, we plan to create models by combining Hindi, Romanized Hindi (Hinglish), and English data. Adding Hinglish data can make it more relevant in day-to-day applications. We see frequent use of Hinglish these days instead of Hindi and it is rapidly gaining popularity. A few such examples are comment sections, posts and messages on social networking sites. Further, adding more English texts to the pre-training data will make it bilingual. This will increase its usability in tasks like machine translation and tasks that contain a mix of Hindi and English data."}]}