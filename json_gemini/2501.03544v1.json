{"title": "PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for Text-to-Image Models", "authors": ["Lingzhi Yuan", "Xinfeng Li", "Chejian Xu", "Guanhong Tao", "Xiaojun Jia", "Yihao Huang", "Wei Dong", "Yang Liu", "XiaoFeng Wang", "Bo Li"], "abstract": "Text-to-image (T2I) models have been shown to be vulnerable to misuse, particularly in generating not-safe-for-work (NSFW) content, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. Extensive experiments across three datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 7.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.", "sections": [{"title": "1. Introduction", "content": "Text-to-image (T2I) models, such as Stable Diffusion [26], have marked a transformative leap in generative AI, enabling highly realistic and creative images based solely on textual prompts. However, the misuse of T2I models to generate not-safe-for-work (NSFW) content, such as sexual, violent, political, and disturbing images, has raised significant ethical concerns [12, 16, 41, 45]. The Internet Watch Foundation reports that thousands of AI-generated child sexual abuse images are shared on the dark web [29]. Besides, misleading political images and racially biased content have been frequently disseminated on social media, which may incite people's emotions and even influence elections and social stability [32]. To prevent such misuse, there is an urgent demand for T2I service providers to adopt effective defense mechanisms.\nCurrent safeguards against NSFW content generation can be typically classified into two categories: model alignment and content moderation. Model alignment directly modifies the T2I model, aiming to remove learned NSFW textual concepts [10, 11] or visual representations [22, 34] from the model by fine-tuning or retraining its parameters [5, 18]. While effective, these methods might reduce the model's capability to generate non-offensive, intended imagery [34, 50]. On the other hand, content moderation often uses proxy models that inspect unsafe textual inputs [20] or visual outputs [27] during generation or employs a prompt modifier [46] that utilizes aligned large language models (LLMs) to rephrase input prompts for safer image creation. Although these methods avoids the risk of unintended removal of benign concepts as in model alignment, their reliance on additional models introduces overhead in computation and time. Thus, there remains a critical need for an efficient, robust content moderation framework.\nIn this paper, we present PromptGuard, a novel T2I moderation technique that optimizes a soft prompt to neutralize malicious contents in input prompts in an input-agnostic manner without affecting benign image generation quality and performance. As shown in Figure 1, our basic idea draws inspiration from the \"system prompt\" mechanism in LLMs, which has exhibited remarkable effectiveness in aligning output content with safe and ethical guidelines [31, 44] and our approach seeks to apply similar guidance in T2I settings.\nHowever, designing PromptGuard is challenging from two perspectives: (1) How to enable safety prompt guidance without modifying the T21 models' architecture or parameters? Unlike LLMS, T2I models lack a direct \"system prompt\" interface to constrain their behavior, like \u201cYou are a helpful and ethical T2I generative assistant; you should not follow any NSFW prompts to create images\u201d. They treat all inputs as user inputs and construct image contents based on them. Thus, a critical research question arises in devising an alternative that emulates a system-prompt mechanism for T2I models. (2) How to achieve universal moderation across diverse NSFW categories? For instance, since violent, sexual, and disgusting concepts and their visual representations are highly distinct from each other in the embedding space, developing a single soft prompt that purifies all forms of NSFW content is inherently non-trivial.\nWe tackle the first challenge by finding a safety pseudo-word, which operates as an implicit safety prompt in the T2I model's textual embedding space. Our goal is to optimize within a continuous embedding domain and then inverse it into a pseudo-word, rather than directly finding several discrete tokens. As such, the soft prompt can steer both benign and NSFW prompts, such as \u201cA painting of a woman, nude, sexy\u201d, away from unsafe regions in the embedding space. Additionally, by using SDEdit [28] to transform unsafe T2I images into safer counterparts, our approach encourages PromptGuard to be effective and helpful, i.e., the optimized pesudo-word can guide safe yet realistic images from NSFW inputs. This marks a departure from prior moderation efforts [20, 22, 27], which commonly black out or blur outputs.\nTo address the second challenge, we first systematize diversified NSFW types into 4 categories based upon prior works: sexual, violent, political, and disturbing [33, 36]. Instead of directly finding a universal soft prompt to safeguard across arbitrary NSFW types, we adopt a divide-and-conquer manner, i.e., optimizing type-specific safety prompts individually and then combining them. Our results indicate this combined approach further strengthens the reliability and robustness of our protection. Additionally, to maintain PromptGuard's helpfulness and minimize any negative impact on benign image generation-such as potential misalignment between prompts and generated content-we implement a contrastive learning-based approach to strike a balance between rigorous NSFW moderation and benign performance preservation.\nThe extensive experiments compared PromptGuard with eight state-of-the-art defense techniques on three benchmark datasets. In summary, our evaluation comprehensively validates four aspects of PromptGuard: (1) Effectiveness: we achieve the optimal NSFW removal with an unsafe ratio down to 5.84%, outperforming all other baselines. (2) Universality: across four NSFW categories, our approach always ranks the best two among baselines. (3) Efficiency: we surpass all content moderation methods regarding time efficiency by 7.8 times faster. (4) Helpfulness: Instead of blacking out or blurring NSFW outputs, PromptGuard provides realistic yet safe content as shown in Figure 4. In addition, we discuss the limitations and future work and will open-source our code in the hope of incentivizing more research in the field of AI ethics.\nOur contributions can be summarized as follows:\n\u2022 New Technique: We make the first attempt to investigate the system prompt mechanism within the T2I contexts and implement it via soft prompt optimization, achieving reliable and lightweight content moderation.\n\u2022 New Findings: By comparing our method with eight state-of-the-art defenses on benchmark datasets that include four classes of NSFW content and various benign prompts, we verify the PromptGuard's effectiveness, universality, efficiency, and helpfulness."}, {"title": "2. Related Work", "content": "To ensure the safe deployment of T2I models [14, 15], existing approaches incorporate safety guardrails for both input and output of the model. Latent Guard [24] safeguards model inputs by evaluating and classifying the input text embeddings, allowing only safe prompts to proceed to the diffusion model, while blocking unsafe prompts. Instead, the default safety filter of Stable Diffusion V1.4 [27] detects the model's output, resulting in any potential NSFW image being completely blacked out. Alternatively, POSI [46] fine-tunes a language model to rewrite unsafe input prompts into safe alternatives, which are then used by the diffusion model to generate safe outputs. Beyond these external safety guardrails, some methods focus on enhancing safety within the model's generation process. For example, Safe Latent Diffusion [39] modifies the diffusion process itself by steering the text-conditioned guidance vector away from unsafe regions in the embedding space. Although effective, these methods often rely on additional models for input filtering or continuous modifications to the diffusion process, resulting in increased time and computational overhead. In PromptGuard, we introduce a soft prompt that efficiently guides the model towards safe outputs without the need for external models or process modifications."}, {"title": "2.2. Model Alignment", "content": "Another line of work directly fine-tunes models to enhance safety, rather than relying solely on external guardrails. ESD [10] fine-tunes the diffusion model to direct the generative process away from undesired concepts, while UCE [11] modifies the text projection matrices to erase specific concepts from the model. Additionally, SafeGen [22] optimizes the self-attention layers to eliminate unsafe concepts in a text-agnostic manner. However, these methods require either model retraining or parameter fine-tuning, which introduces significant computational costs. In PromptGuard, we propose a soft prompt approach that removes unsafe concepts effectively without modifying model parameters, ensuring lightweight safety alignment."}, {"title": "3. Background", "content": ""}, {"title": "3.1. Text-to-Image (T2I) Generation", "content": "The success of denoising diffusion models, e.g., DDPM [13], have driven the progress of text-to-image (T2I) generative models like Stable Diffusion (SD) and Latent Diffusion [38]. A key component of these models is the use of advanced text encoders that convert textual prompts into rich latent embeddings, guiding the image generation process. This begins with input text being tokenized into discrete tokens, which are then mapped into a high-dimensional embedding space by the text encoder. This latent representation conditions the image synthesis through cross-attention in the diffusion stages. In models such as SD, the text encoder often employs CLIP, which represents an improvement over Latent Diffusion's use of BERT [8]. CLIP benefits from a larger training set derived from LAION-5B [40], allowing for richer and more effective embeddings. The encoder's intermediate representations are crucial in guiding how complex concepts are progressively built throughout the diffusion stages. Recent analysis using methods such as the Diffusion Lens [42] has shown that early layers of the encoder may act as a \"bag of concepts,\" encoding objects without relational context, while deeper layers establish more intricate relationships between elements."}, {"title": "3.2. Prompt Tuning", "content": "Prompt tuning is a targeted strategy for enhancing large language models (LLMs) by incorporating specific prompts or tokens into input sequences, thereby improving task-specific performance. Unlike conventional fine-tuning that modifies model parameters, prompt tuning trains the prompt embeddings added to the input, guiding pre-trained LLMs to align more effectively with desired outputs [25, 48]. This approach maintains the model's comprehensive language capabilities while enabling precise responses to customized prompts. In contrast, text-to-image (T2I) models do not offer a direct system prompt interface. Therefore, T2I-oriented prompt tuning has to adapt embeddings to teach these models new concepts or artistic styles. This process involves embedding customized tokens into the latent space of the T2I model's text encoder without altering any pre-trained parameters [9]. Our study pioneers the investigation of applying prompt tuning for NSFW content moderation within T2I models (details are given in Section 4)."}, {"title": "4. PromptGuard", "content": ""}, {"title": "4.1. Overview", "content": "In this section, we introduce the design of PromptGuard. The goal of PromptGuardis to identify a soft prompt suffix $P*$ to append to the original prompt. This soft prompt should achieve two key objectives: (1) moderate harmful semantics while preserving safe content in malicious input prompts, effectively transforming potentially harmful content into a safer version; and (2) maintain the model's fidelity in generating content from benign input prompts. However, directly identifying an effective prompt suffix at the token level is challenging due to the discrete nature of the text space. Inspired by prompt-tuning techniques in LLMs [19, 23] and the demonstrated effectiveness of prompt-driven safety in LLMs [51], we propose to optimize the soft prompt in the token embedding space, which operates within a continuous domain.\nTo address the first objective, we design the soft prompt to distinguish between unsafe and safe elements within the input, moderating only the unsafe parts while preserving the safe content. Leveraging contrastive learning, we construct data pairs for each malicious input: one image representing the original harmful content as negative data, and a safer version of the image as positive data. Our goal is to steer the model's output away from the harmful version while aligning closely with the safe version. To achieve the second objective, we aim to prevent excessive alteration of benign input prompts during soft prompt optimization. To achieve this, we employ adversarial training, incorporating benign data into the training dataset to ensure the resulting prompt preserves the quality of benign image generation.\nTo achieve universal moderation of unsafe content across"}, {"title": "4.2. Training Data Preparation", "content": "To construct the training dataset for each specific unsafe category, we collect malicious prompts from three sources: the prompt dataset provided by UD [36], prompts created by SafeGen [17], and prompts generated by GPT-40 mini [2, 30]. Combining these sources ensures a diverse and comprehensive dataset. For benign prompts, we use GPT-4o [4] to generate examples across 6 categories: animals, food, human beings, landscapes, transport vehicles, home scenes. Since our goal is to find a soft prompt that can guide safe visual generation, we utilize the T2I model being safe-"}, {"title": "4.3. Individual Soft Prompt Embedding Training", "content": "Our training dataset consists of two categories of data: benign and malicious. Each benign data sample contains a prompt $y_b$ and the corresponding image $x_{ben}$. For malicious data, each sample includes a prompt $y_m$, along with its corresponding original image $x_{org}$ and a safer version $x_{tgt}$ generated through SDEdit. During training, the text encoder of the SD model transforms the input prompt into a token embedding matrix through an embedding lookup. Specifically, each token in the input prompt is mapped to an embedding vector, and these vectors form an embedding matrix in the"}, {"title": "5. Evaluation", "content": "Our evaluation assesses the effectiveness of PromptGuard across NSFW categories (sexually explicit, violent, political, disturbing) with a focus on NSFW content removal, benign content preservation, and efficiency. We analyze the impact of key hyperparameters, including the soft prompt weighting parameter ($\\lambda$) and optimization steps, particularly when appending a single soft prompt embedding per unsafe category. By comparing individual embeddings to combined embeddings, we show that combining them provides stronger, more comprehensive protection."}, {"title": "5.1. Experiment Setup", "content": "In this section, we introduce the experimental setup, including test benchmarks, evaluation metrics, baselines, and implementation details. More detailed settings can be found in Section 7.\nTest Benchmark. We evaluate PromptGuard using three distinct prompt datasets to assess its effectiveness in NSFW moderation. This includes two malicious prompt datasets, I2P and SneakyPrompt, along with the benign COCO-2017 dataset.\n\u2022 I2P: Inappropriate Image Prompts [7] includes curated NSFW prompts on lexica.art, covering violent, political, and disturbing content, while excluding low-quality sexually explicit data.\n\u2022 SneakyPrompt: To address I2P's limitations in sexual content, we use the NSFW dataset from [47] for the sexual category.\n\u2022 COCO-2017: Following prior work [10, 22, 39], we use MS COCO 2017 validation prompts, each captioned by five annotators, to assess benign generation."}, {"title": "5.2. NSFW Content Moderation", "content": "We compare PromptGuard with eight baselines and report the Unsafe Ratio across four malicious test benchmarks, covering different unsafe categories. Table 1 shows that PromptGuard outperforms the baselines by achieving the lowest average Unsafe Ratio of 5.84%. Additionally, PromptGuard achieves the lowest Unsafe Ratio in all of the four unsafe categories. Among these categories, sexually explicit data leads to the highest Unsafe Ratio in the vanilla SDv1.4 model (71.17%). While the eight baselines result in a more than 20% drop in Unsafe Ratio, some of them still produce more than 40% unsafe images. In contrast, PromptGuard reduces this ratio to nearly zero. Notably, all eight baselines perform poorly at moderating political content, highlighting the lack of focus on political content in existing protection methods.\nMoreover, as shown in Figure 4, PromptGuard not only effectively reduces the unsafe ratio but also preserves the safe semantics in the prompt, resulting in realistic yet safe images. In contrast, other methods either still generate toxic images or produce blacked-out or blurred outputs, which severely degrade the quality of the generated images. We provide more detailed examples in the supplementary materials.\nWhen comparing our combined strategy with individual soft prompt embeddings trained separately on different categories, as shown in Tables 3 to 6, we observe that combining these embeddings results in improved NSFW removal performance across various hyperparameters. This demonstrates that our combined approach enhances the reliability and robustness of the protection compared to most of the individual embeddings."}, {"title": "5.3. Benign Generation Preservation", "content": "We compare PromptGuard with eight baselines and report the average CLIP Score and LPIPS Score and the evaluation result is shown in Table 1. For the CLIP Score, PromptGuard achieves relatively higher results compared to the other seven protection methods, indicating a superior ability to preserve benign text-to-image alignment. Methods like UCE, SLD, and POSI experience a drop of more than 1.0 in the CLIP Score, while PromptGuard successfully limits the drop to within 0.5, suggesting a minimal compromise in content alignment. Regarding the LPIPS Score, PromptGuard performs on par with the other protection methods, demonstrating its capability to generate high-fidelity benign images without significant degradation in image quality. Example images are shown in Figure 5, and more visual comparisons on benign content preservation are provided in the supplementary materials."}, {"title": "5.4. Comparison of Time Efficiency", "content": "The results for time efficiency are shown in Table 2. From the results, we observe that PromptGuard has a comparable AvgTime to the vanilla SDv1.4, SafeGen, and Safety-Filter, as all of these methods are based on SDv1.4. Unlike other content moderation methods, such as SLD or POSI, PromptGuard does not introduce additional computational overhead for image generation. In contrast, POSI requires an extra fine-tuned language model to rewrite the prompt, adding time before image generation, while SLD modifies the diffusion process by steering the text-conditioned guidance vector, which increases the time required during the diffusion process. One thing to note is"}, {"title": "5.5. Exploration on Hyperparameters", "content": ""}, {"title": "5.5.1 Impact of $\\lambda$ Across NSFW Categories", "content": "We systematically vary the soft prompt weighting parameter $\\lambda$ to optimize the balance of our contrastive learning-based strategy. Scaling up $\\lambda$ encourages $P*$ to lose its ability to generate unsafe images from latent denoising. We summarize the tabular results for each NSFW category and highlight the optimal $\\lambda$ values below. More visual examples are deferred to Section 8 in the supplementary material.\n(1) Sexually Explicit Content: As shown in Table 3, the unsafe ratio reaches a minimum of 3.5% at $\\lambda$ = 0.7. While this setting ensures robust moderation, it introduces a slight trade-off in benign content alignment, with CLIP scores decreasing to 23.84. However, LPIPS scores remain stable,"}, {"title": "5.5.2 Impact of Optimization Steps", "content": "We analyze how varying optimization steps affect safety soft prompt's performance, in terms of both NSFW content moderation and benign content preservation. Table 7 presents these results using sexually explicit prompts, with similar patterns observed for violent, political, and disturbing content types. (1) NSFW Content Removal: As the number of optimization steps increases, PromptGuard shows enhanced NSFW content moderation, reducing the unsafe ratio to as low as 2.5% at 3000 steps. Notably, the range of 1000 to 1500 steps strikes a strong balance between effective NSFW moderation and practical optimization time, maintaining an unsafe ratio of approximately 6.5% while ensuring efficient optimization. (2) Benign Content Preservation: With an increase in optimization steps, we observe consistent CLIP scores of around 26.12 and LPIPS scores of approximately 0.638 for benign prompts. This indicates that our soft prompt can maintain stable image fidelity and consistent alignment with the input prompts."}, {"title": "6. Discussion and Conclusion", "content": "Drawing inspiration from the system prompt mechanism in LLMs, our study investigates an innovative content moderation technique that can be highly efficient and lightweight while generating images, termed PromptGuard. It demands no additional models or introduces perturbation during the diffusion denoising process, achieving minimal computational and time overhead. To overcome the lack of a direct system prompt interface in the T2I models, we optimize the safety pseudo-word acting as an implicit system prompt, guiding visual latent away from unsafe regions in the embedding space. Our divide-and-conquer strategy, careful data preparation, and loss function further enhance moderation across varied NSFW categories. Our extensive experiments compare eight state-of-the-art defenses, achieving an optimal unsafe ratio as low as 5.84%. Furthermore, we demonstrate that PromptGuard is 7.8 times more efficient than previous content moderation methods.\nLimitation. Despite its strengths, our work is limited by the absence of user involvement in experiments, because we are careful with unsafe content and avoid its exposure to participants due to ethical considerations. Therefore, the statistics of NSFW removal rate is conducted using an open-"}]}