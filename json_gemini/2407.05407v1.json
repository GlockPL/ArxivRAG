{"title": "Cosy Voice: A Scalable Multilingual Zero-shot Text-to-speech Synthesizer based on Supervised Semantic Tokens", "authors": ["Zhihao Du", "Qian Chen", "Shiliang Zhang", "Kai Hu", "Heng Lu", "Yexin Yang", "Hangrui Hu", "Siqi Zheng", "Yue Gu", "Ziyang Ma", "Zhijie Yan"], "abstract": "Recent years have witnessed a trend that large language model (LLM) based text-to-speech (TTS) emerges into the mainstream due to their high naturalness and zero-shot capacity. In this paradigm, speech signals are discretized into token sequences, which are modeled by an LLM with text as prompts and reconstructed by a token-based vocoder to waveforms. Obviously, speech tokens play a critical role in LLM-based TTS models. Current speech tokens are learned in an unsupervised manner, which lacks explicit semantic information and alignment to the text. In this paper, we propose to represent speech with supervised semantic tokens, which are derived from a multilingual speech recognition model by inserting vector quantization into the encoder. Based on the tokens, we further propose a Codec-based synthesizer for Voice generation, CosyVoice\u00b9, which consists of an LLM for text-to-token generation and a conditional flow matching model for token-to-speech synthesis. Experimental results show that supervised semantic tokens significantly outperform existing unsupervised tokens in terms of content consistency and speaker similarity for zero-shot voice cloning. Moreover, we find that utilizing large-scale data further improves the synthesis performance, indicating the scalable capacity of CosyVoice. To the best of our knowledge, this is the first attempt to involve supervised speech tokens into TTS models.", "sections": [{"title": "1 Introduction", "content": "Text-to-Speech (TTS) technology has made remarkable strides in recent years, transitioning from robotic-sounding speech to producing voices that are nearly indistinguishable from human speakers. At the forefront of this advancement are Large Language Models (LLMs), which have been increasingly utilized in TTS systems to generate speech with a higher degree of naturalness and the ability to synthesize voices in a zero-shot fashion (Betker, 2023; Wang et al., 2023; Lajszczak et al., 2024). These LLM-based TTS models function by converting speech signals into sequences of tokens, with the LLM utilizing text as a condition to model these token sequences. A token vocoder is then employed to reconstruct the raw waveforms from the tokenized speech (Kong et al., 2020; D\u00e9fossez et al., 2022).\nA critical aspect of the TTS process is the representation of speech tokens. Traditionally, tokens are acquired through unsupervised learning, which may not capture explicit semantic information or align well with corresponding text (Hsu et al., 2021; D\u00e9fossez et al., 2022). Recognizing this gap, our work introduces supervised semantic tokens extracted from a multilingual speech recognition model, Whisper (Radford et al., 2023), by integrating vector quantization into the encoder. This innovation allows for more accurate semantic representation and alignment with text. Early studies have shown that quantizers with auxiliary automatic speech recognition (ASR) loss outperform k-means clustering on the universal speech model (USM) for speech-to-text translation and ASR tasks, as demonstrated in Rubenstein et al. (2023). Additionally, Ye et al. (2024) employed Gumbel-Softmax vector quantization to extract discrete speech representations that prioritize ASR-relevant information for ASR tasks. However, the impact of these approaches on text-to-speech (TTS) remains unclear.\nFurthermore, leveraging these supervised tokens, we propose CosyVoice, a scalable and efficient zero-shot TTS synthesizer. Cosy Voice is comprised of an LLM for converting text into semantic token sequences and a conditional flow matching model for the subsequent synthesis of speech from these tokens. In contrast to prior systems like TorToise TTS (Betker, 2023), which employs an LLM in conjunction with a denoising diffusion probabilistic models (DDPM) (Ho et al., 2020), CosyVoice utilizes a conditional flow matching approach, as it has been demonstrated to accelerate both training and inference compared to traditional diffusion models (Le et al., 2024). While existing methods incorporate flow matching in TTS (Le et al., 2024; Guo et al., 2023; Mehta et al., 2023; Guan et al., 2023), they often rely on phoneme duration prediction, necessitating the use of supplementary phonemizers and forced aligners. CosyVoice, however, bypasses these dependencies, offering a more direct and efficient pathway from text to speech.\nOur research contributes to the field of speech generation in several novel ways:\n\u2022 We are the first to integrate supervised speech tokens into TTS models, enhancing content consistency and speaker similarity in zero-shot voice cloning.\n\u2022 We propose CosyVoice, a scalable zero-shot TTS synthesis system that combines an LLM for text-to-token generation with a conditional flow matching model for token-to-speech synthesis, forsaking the need for additional phonemizers and forced aligners.\n\u2022 To further refine the quality of generated speech, we incorporate the x-vector (Snyder et al., 2018) into the LLM to separate the modeling of speech into semantic, speaker, and prosody components. The LLM models the semantic content and prosody, while the conditional flow matching model captures timbre and environmental information. We optimize the flow matching process with techniques such as classifier-free guidance (Ho and Salimans, 2022a), a cosine scheduler, and masked conditions.\nOur experimental results demonstrate the superiority of supervised semantic tokens over unsupervised counterparts. Additionally, the scalability of CosyVoice is evidenced by improved synthesis performance when utilizing large-scale data. This work, therefore, represents a significant step forward in the development of natural-sounding, versatile TTS systems."}, {"title": "2 CosyVoice: A Scalable TTS model using Supervised Semantic Tokens", "content": "As shown in Figure 1(b), our Cosy Voice consists of four components, namely text encoder, speech tokenizer, large language model and conditional flow matching model. Specifically, the text encoder is used to align the semantic spaces of text and speech tokens, while the speech tokenizer is utilized to extract semantic tokens as illustrated in Figure 1(a). We employ a large language model to learn the whole sequence of text encodings and speech tokens, reformulating TTS as an auto-regressive sequence generation problem given text as prompts. Then, as shown in Figure 1(c), a conditional flow matching model is utilized to convert speech tokens into a Mel spectrogram via a denoising process on the optimal path. To obtain a perceptible signal, the HifiGAN vocoder (Kong et al., 2020) is used to synthesize a waveform with the generated Mel spectrogram as input."}, {"title": "2.1 Supervised Semantic Tokens for Speech", "content": "In Cosy Voice, a supervised automatic speech recognition (ASR) model is employed to derive the supervised semantic speech (S3) tokenizer for speech. The model is a finetuned version of our proprietary SenseVoice ASR model. It is trained on multilingual audio data and possesses rich audio content understanding capabilities. Different from the original ASR model, we split the encoder into two parts and insert a vector quantization layer between them. Given a Mel spectrogram X as input, it undergoes the positional encoding and Encoder\u2081 to obtain a context-aware representations H:\n$H = Encoder_1 (PosEnc(X))$  (1)\nThen, a vector quantizer (VQ) is involved to obtain discrete tokens. For the hidden representation hl at the frame l, the index of nearest embedding in the codebook C is treated as the speech token \u03bcl at this timestep:\n$\\mu_l = VQ(h_l, C) = \\underset{c_n \\in C}{arg \\space min} ||h_l - c_n||_2$  (2)\nwhere $||\\space||_2$ denotes the L2 norm. At the training stage, codebook embeddings are updated via exponentially moving average (EMA):\n$c_{\\mu_l} := \\alpha c_{\\mu_l} + (1 - \\alpha) h_l$ (3)\nwhere \u03b1 is a pre-defined decay coefficient. The corresponding codebook embeddings of speech tokens are used as the quantized hidden representations H = {C\u00b51, C\u00b52,..., c\u00b5\u2081 } and passed through the remaining encoder layers Encoder2:\n$\\overline{H} = Encoder_2 (PosEnc(\\overline{H}))$ (4)"}, {"title": "2.2 Large Language Model for TTS", "content": "In this section, we formulate the TTS task as an auto-regressive speech token generation problem with a large language model (LLM). For LLM, the sequence construction is the most important matter, which is constructed as follows:\n$[S, V, \\{y_u\\}_{u \\in [1:U]}, T, \\{\\mu_l\\}_{l \\in [1:L]}, E]$ (6)\nS and E denote the start and end of sequence, respectively. v is a speaker embedding vector extracted from the speech X with a pre-trained voice-print model\u00b2. The text encodings Y = {\u04efu}u\u2208[1:U] is obtained by passing the text through a Byte Pair Encoded (BPE) tokenizer and text encoder:\n$\\overline{Y} = TextEncoder(BPE(Y))$  (7)\nSince text and speech tokens lie at different semantic levels, the text encoder is used to align their semantic spaces and benefit the LLM modeling. A start identifier T is inserted between text encodings and speech tokens {\u03bcl}l\u2208[1:L] that is extracted with the supervised semantic tokenizer as described in 2.1. At the training stage, we employ the teacher-forcing scheme, in which the left-shifted sequence is employed as the mode inputs and the original sequence serves as the expected outputs. Note that only the cross entropy losses of speech tokens and E are considered during the training:\n$L_{LLM} = -\\frac{1}{L+1} \\sum_{l=1}^{L+1} log \\space q(\\mu_l)$ (8)\nwhere \u00b5L+1 is the \u201cend of sequence\" token E. q(\u03bc\u03b9) denotes the posterior probability of \u03bc\u03b9, which is predicted by the softmax layer following LLM."}, {"title": "2.3 Optimal-transport Conditional Flow Matching", "content": "In CosyVoice, an optimal-transport conditional flow matching model (OT-CFM) is employed to learn the distribution of Mel spectrogram and generate samples from it with generated speech tokens as conditions. OT-CFM can achieve better performance compared to diffusion probabilistic models (DPMs) with simpler gradients, easier training and faster generation (Lipman et al., 2023; Tong et al., 2023; Mehta et al., 2023). In continuous-time normalizing flows (CNFs), a probability density path is constructed from a prior distribution po(X) to the data distribution of Mel spectrogram q(X). The probability density path is defined by a time-dependent vector field vt(X) : [0,1] \u00d7 RL*D \u2192 RL*D, which generates the flow \u03a6t through the following ordinary differential equation (ODE):\n$\\frac{d}{dt} \\Phi_t(X) = v_t(\\Phi_t(X), t)$\n$\\Phi_0(X) \\sim p_0(X) = N(X; 0, I)$\n$\\Phi_1(X) \\sim p_1(X)$  (9)\nwhere t \u2208 [0, 1]. By solving the initial value problem Eq. (9), we can approximate the speech distribution q(X) with p\u2081(X) and sample from it. To learn the vector field vt(X), we define the optimal-transport (OT) flow and force a neural network to match it by minimizing the following loss:\n$L_{OT-CFM} = \\mathbb{E}_{t, p_0(X_0), q(X_1)} [|\\omega_t(\\Phi_t^{OT}(X_0, X_1)|X_1) - v_t(\\Phi_t^{OT}(X_0, X_1) \\vert \\theta)|^2]$ (10)\nwhere\n$\\Phi_t^{OT}(X_0, X_1) = (1 - (1 - \\sigma)t)X_0 + tX_1$\n$\\omega_t(\\Phi_t^{OT}(X_0, X_1)|X_1) = X_1 - (1 - \\sigma) X_0$\nThe speaker embedding v, speech tokens {\u03bc\u03b9}1:L, and masked Mel spectrogram X\u2081 are also fed into the neural network to match the vector field with learnable parameters \u03b8:\n$v_t(\\Phi_t^{OT}(X_0, X_1) \\vert \\theta) = NNo(\\Phi_t^{OT}(X_0, X_1), t; v, \\{\\mu_l\\}_{1:L}, X_1)$ (12)\nX\u2081 is a masked version of X\u2081 by setting continuous frames to zeros from a random start point to the end. Considering the generation process at the beginning is harder than follows, we involve a cosine scheduler for the timestep t:\n$t := 1 - cos(\\frac{t \\pi}{2})$ (13)\nUnder the scheduled flow, there are more generation steps at the beginning.\nClassifier-free guidance (CFG) has been proven to improve the generation quality of diffusion probabilistic models (Ho and Salimans, 2022b; Nichol and Dhariwal, 2021; Le et al., 2024). Therefore, we propose to adapt the CFG into conditional flow matching models. At the training stage, we randomly drop the conditions \u03a8 = {\u03bd, {\u03bc\u03b9}1:L, X1} with a fixed probability of 0.2. In this manner, we can learn both conditional and unconditional flows. During generation, the vector field is modified as follows:\n$\\widetilde{v_t} (\\Phi_t^{OT}(X_0, X_1) \\vert \\theta; \\Psi) = (1 + \\beta) \\cdot v_t(\\Phi_t^{OT}(X_0, X_1) \\vert \\theta; \\Psi) - \\beta \\cdot v_t(\\Phi_t^{OT}(X_0, X_1) \\vert \\theta)$ (14)\nwhere \u03b2 is the guidance strength of 0.7."}, {"title": "2.3.1 Zero-shot In-context Learning", "content": "CosyVoice models exhibit zero-shot in-context learning capabilities, allowing for the replication of an arbitrary voice with only a brief reference speech sample. This process entails the careful construction of input sequences for the token language model (LM), depicted in Figure 2. For prompt speech and input text in the same language, we merge them to form a unified input, treating the prompt speech tokens as pre-generated. With this input sequence, the autoregressive LM iteratively predicts subsequent tokens until it encounters the \u201cend of sequence\u201d token E. However, when the prompt speech and input text differ linguistically, we omit the text and tokens associated with the prompt to prevent prosodic characteristics of the original language from influencing the target language. It is important to note that the prompt text, which corresponds to the prompt speech's content, can be transcribed either through human annotation or ASR models, such as SenseVoice. Similar to the prompt text, the prompt tokens are extracted from the prompt speech with S\u00b3 tokenizer.\nAfter generating the speech tokens, they are appended after the prompt tokens, forming a composite condition for the flow-matching model. Additionally, the speaker embedding and the Mel spectrogram of the prompt speech are incorporated to further enhance timbre and environmental consistency."}, {"title": "2.4 Rich Generation with Instruction", "content": "To enable further controllability on Cosy Voice, we experiment with integrating additional instruction fine-tuning (Ji et al., 2023). CosyVoice-instruct extends Cosy Voice-base with enhanced instruction-following capabilities. Specifically, it supports controllability over various aspects such as speaker identity (i.e., speaker's characteristics), speaking style (including emotion, gender, speaking rate, and pitch), and fine-grained paralinguistic features. These features include the ability to insert laughter, breaths, speaking while laughing, and emphasize certain words.\nWe fine-tuned CosyVoice-base using this training data without incorporating speaker embedding in the autoregressive language model."}, {"title": "3 Dataset", "content": "We conduct experiments on the LibriTTS (Zen et al., 2019) corpus, which contains 585 hours from 2,456 English speakers. We follow the official data partition, where \u201ctrain-clean-100\u201d, \u201ctrain-clean-360\u201d and \u201ctrain-other-500\u201d are merged for training and the \"dev-clean\" is used for model selections. \"test-clean\u201d is used to construct the evaluation set as described in (Du et al., 2024)."}, {"title": "3.2 Large-scale Multi-lingual Dataset", "content": "To train the Cosy Voice models, we have amassed a considerable dataset comprising multiple languages. Throughout the collection process, we utilize specialized in-house tools for speech detection, signal-to-noise ratio (SNR) estimation, speaker diarization, and separation. Subsequently, pseudo text labels are generated using SenseVoice-Large and Paraformer. These labels undergo a refinement process with the aid of force-alignment (FA) models, which helps eliminate low-quality data and enhances the accuracy of punctuation. A comprehensive breakdown of the training data's duration across various languages is presented in Table 2."}, {"title": "4 Experimental Settings", "content": "We train the tiny and normal size models in single-lingual and multi-lingual experiments. Details of model architecture settings are shown in Table 4. The tiny model is trained on LibriTTS training set for 50 epochs with four V100-32M GPUs, while the multi-lingual model is trained on our internal dataset for 800,000 steps with 64 V100-32M GPUs. Tiny and normal models are trained with the learning rate of 10-3 and 10-4, respectively. The warmup step is set to 10,000."}, {"title": "5 Experimental Results", "content": "In table 5, we demonstrate how the vector quantization affects the recognition performance on Lib- riTTS test sets. From the table, we can see that inserting a vector quantizer into the ASR encoder only affects the recognition performance slightly. As a result, the VQ-inserted Conformer ASR model achieves comparable WERs of 3.18% and 7.56% on \"test-clean\u201d and \u201ctest-other\u201d sets, respectively. This indicates that tokenizers trained in a supervised manner can maintain sufficient semantic information and the alignment to text.\nTo assess the multi-lingual S\u00b3 tokenizer's ability to preserve semantic information, we compared the recognition performance of the quantizer-augmented SenseVoice-L against its original version and the Whisper-Large V3 model. The models underwent evaluation using the Common Voice zh-CN and en benchmarks, with the findings detailed in Table 6. From the table, we can see that our S3 tokens demonstrate robust recognition performance in both the Chinese and English test sets. Notably, on the common_voice_zh-CN set, S\u00b3 tokens surpass the performance of the Whisper-Large V3 model (TongyiSpeech, 2024), achieving a 4.14% relative reduction in error rate. This suggests a substantial correlation between S\u00b3 tokens and semantic content. It is worth noting that there is only a single codebook in the S\u00b3 tokenizer with a dictionary size of 4,096 entries."}, {"title": "5.2 Comparison with Baselines", "content": "We compare the proposed Cosy Voice models with other TTS systems on content consistency and speaker similarity. For content consistency, an ASR model is employed to recognize the generated utterances. We report the word error rate (WER), and the number of insertion, deletion and substation errors. As for the speaker similarity, we employ the ERes2Net model (Chen et al., 2023) to extract speaker embeddings of prompt and generated utterances, and their raw cosine similarity is treated as the speaker similarity. Experimental results are shown in Table 7.\nCompared with other TTS models, the proposed Cosy Voice framework achieves comparable content consistency and higher speaker similarity even using the same text and speech tokenizers. Comparing Exp-1, Exp-2 and Exp-3, we can see that both the text speech tokenizers are critical for content consistency and negligible for speaker similarity. In Exp 4 experiments, we replace the single-lingual text and speech tokenizers with the multi-lingual one. Only using the LibriTTS corpus to train the model degrades both the content consistency and speaker similarity. By involving the internal large-scale dataset, the performance is significantly improved, achieving the human parity quality."}, {"title": "5.3 Evaluation on Generation Quality of Cosy Voice", "content": "We evaluate the quality of Cosy Voice's speech synthesis by examining content consistency and speaker similarity. The \u201ctest-clean\" subset of LibriTTS (Zen et al., 2019) and the test set of AISHELL-3 (Shi et al., 2021) are employed to construct an evaluation set for English and Chinese, respectively. For each text in these sets, we randomly select a prompt speech. Content consistency was evaluated using Whisper-Large V3 (Radford et al., 2023) for English and Paraformer (Gao et al., 2022) for Chinese recognition. Speaker similarity was quantified by calculating the cosine similarity between speaker embeddings of the generated and prompt speeches, extracted using ERes2Net (Chen et al., 2023)."}, {"title": "5.4 Emotion Controllability of Cosy Voice", "content": "To verify the emotion controllability, we use the public speech emotion recognition model emo2vec\u00b3 (Ma et al., 2023). We generated and evaluated 100 English utterances for each of the six emotions: happy, angry, sad, surprised, fearful, and disgusted. The content of the synthesized text is designed to match the target emotion. We then measure the accuracy of the predicted emotions from the synthesized speech for each emotion.\nshows the comparison of emotion control accuracy between CosyVoice-base and CosyVoice-instruct. For CosyVoice-instruct, the input consists of content text accompanied by a speaking style instruction (e.g., \"Happy.<endofprompt>Content Text\"). In contrast, Cosy Voice-base only receives the content text as input. The results indicate that CosyVoice-instruct with emotional instructions demonstrates a significant improvement over both CosyVoice-base and CosyVoice-instruct without emotional instructions."}, {"title": "5.5 Cosy Voice as a Data Generator", "content": "A straightforward application of Cosy Voice is as a data generator to augment the training data of other tasks, such as ASR, speech-to-speech translation (S2ST). Taking the ASR task an example, we conduct an experiment on the Librispeech corpus to evaluate Cosy Voice's capability in generating high-quality data. The experimental results are shown in Table 11, where \u201cLibrispeech\u201d denotes the original 960-hour data. \"Syn on LS text\u201d and \u201cSyn on LS text\u201d denote the generated data with the text from Librispeech and MLS training sets, respectively. From the table, we can see that only training on the synthesized data, the ASR model can achieve a comparable result than the original Librispeech training set. Upon integration of them, a notable enhancement in recognition accuracy is observed. An interesting finding is that involving the synthesized data on the MLS text significantly improves the recognition performance. This may indicates that the text diversity is more critical for ASR task than the duration of speech itself. This improvement can be attributed to the varied linguistic content introduced by CosyVoice synthesized samples. The findings from our evaluation underscore the high quality of the samples generated by CosyVoice."}, {"title": "6 Conclusion", "content": "In this paper, we introduce Cosy Voice, a scalable multi-lingual speech generation model, which supports zero-shot in-context learning, cross-lingual voice cloning, instructed generation and fine-grained controlling of emotion, paralinguistic features. Experimental results show that the system architecture of Cosy Voice is important for speaker similarity, while the text and speech tokenizers affect the content consistency much. Besides, we find that scaling up the model size and data volume can improve the performance significantly. As a result, Cosy Voice achieves the human parity generation quality."}]}