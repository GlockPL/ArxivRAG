{"title": "FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning", "authors": ["Jason Jingzhou Liu", "Yulong Li", "Kenneth Shaw", "Tony Tao", "Ruslan Salakhutdinov", "Deepak Pathak"], "abstract": "Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback. In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks. We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training. The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality. We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects by 43% compared to baseline approaches without a curriculum.", "sections": [{"title": "I. INTRODUCTION", "content": "Contact-rich tasks are an integral part of daily life, from lifting a box and rolling dough to cracking an egg or opening a door. These tasks, while seemingly simple, involve a complex interplay of forces and require precise adjustments based on force feedback. Humans rely heavily on this force feedback to generalize across tasks and objects, adapting seamlessly to variations in visual appearances and geometries. However, in robot learning, force information remains underutilized, even though it is readily available on many modern robotic arms, such as the Franka Panda and the KUKA LBR iiwa. Instead, most data-driven methods, including those using Behavior Cloning (BC), focus primarily on visual feedback for both data collection and policy learning, overlooking the critical role of force. This limited use of force information hinders the vision- only policies' ability to generalize to novel objects. For instance, in tasks like lifting up a box with two arms, the primary factor influencing the action is the object's geometry, while attributes such as color or texture are irrelevant. In such cases, force feedback provides a clear signal for mode switching, such as detecting when contact is established, which can facilitate object generalization compared to relying solely on vision. One of the main reasons for the under-utilization of force feedback in robot learning is the lack of an intuitive and low- cost teleoperation system that can capture force feedback during data collection itself. Recently, low-cost leader-follower systems have become popular for teleoperation, offering intuitive control of robot arms by mirroring the joint movements of the leader arm controlled by a teleoperator to the follower arm [31, 28]. However, these systems are typically passive (leader arm joints are not actuated) and unilateral (the leader arm does not receive information from the follower arm). This makes teleoperation difficult for dynamic, contact-rich tasks where precise force adjustments are necessary [22]. To overcome this limitation, we present a bilateral low-cost teleoperation system that provides force feedback by actuating motors in the leader arm joints based on external joint torques transmitted from the follower arm (Fig. 1 Left). By actuating the motors, we also provide active gravity compensation and resolve the kinematic redundancy due to the redundant degrees of freedom of the arm. These enhancements improve the teleoperation experience, leading to a 64.7% increase in the task completion rate, a 37.4% reduction in completion time, and an 83.3% improvement in subjective ease of use across four evaluated contact-rich tasks. The second challenge lies in effectively incorporating robot force information into policy learning. Although recent methods such as diffusion policy [4] and action chunking transformers [31] achieve impressive results for fine-grained manipulation, they often fail to generalize to unseen objects with variations in object visual appearances and geometries. Humans, on the other hand, can disregard irrelevant visual details once contact is established and rely solely on force feedback to perform tasks such as lifting a box or rolling dough. Therefore, to improve generalization, we seek to incorporate force input into autonomous robot policies. However, making effective use of force information in policy learning is challenging, as policies often overfit to using visual modality [27], effectively disregarding force data. This issue arises because contact force signals are typically less discriminative, often remaining near zero for extended periods when the arm is not in contact with the environment during an episode. Hence, without proper care during training, policies tend to ignore force input and rely primarily on visual information. We empirically analyze this effect in Sec. V-C and Fig. 9. To mitigate this imbalance, we propose Force-Aware Curricu- lum Training (FACTR), a curriculum training strategy designed to improve the policy's ability to effectively leverage force information. FACTR systematically reduces the reliance on visual information during training by applying operators such as Gaussian blurring or downsampling with varying scales to visual inputs. A scheduler gradually decreases the blurring scale and increases the fidelity of the visual inputs. Intuitively, this approach encourages the policy to focus more on force input during initial training phases and gradually balances force with visual inputs as training progresses. We ground this intuition with a theoretical analysis on a simplified scenario through the framework of Neural Tangent Kernels [11]. We explore FACTR in both the pixel space and latent space, testing various operators and scheduling strategies. Our experiments show that FACTR improves the success rate for unseen objects by an average of 40.0% in four challenging contact-rich tasks"}, {"title": "II. RELATED WORKS", "content": "A. Imitation Learning with Force\nImitation learning has recently experienced significant ad- vancements, driven by the development of more effective algorithms that leverage demonstrations to train robotic poli- cies [17, 4]. Although traditional approaches primarily rely on visual and joint position inputs, many real-world tasks require explicit force feedback to improve stability, adaptability, and safety [18, 9]. Recent work has applied learning methods to train with demonstrations that incorporate gripper force or tactile signals, resulting in policies capable of handling small, fragile objects and performing contact-intensive tasks such as vegetable peeling [15, 30, 14]. However, utilizing force data from the robot arms, such as joint torques, remains under- explored. One approach involves using an end-effector force sensor to estimate compliance parameters or virtual position targets through kinesthetic teaching and force tensors [10, 3]. Another method infers a 6D wrench for low-level control by integrating torque sensing into a diffusion policy [29]. However, naively incorporating force feedback into policy learning can lead to overfitting to visual information, causing the policy to disregard force input. FoAR [8] explicitly predicts contact and non-contact phases to regulate the fusion of vision and force modalities, which requires additional data labeling. We propose FACTR to effectively incorporate force and vision input into policy through a curriculum, enabling policies to leverage force for improved object generalization.\nB. Low-Cost Teleoperation Systems with Force Feedback\nParallel to advances in imitation learning, significant efforts have been made to collect low-cost and high-quality data with hand-held grippers [26, 5] or leader-follower systems [31, 28, 24]. Hand-held grippers naturally provide force feedback to the operator, but they do not directly record force data. Recent work has added force sensors to hand-held grippers to address this limitation [15]. However, hand-held grippers are in general limited by the kinematic differences between humans and robots, resulting in commands that might be unachievable for the robots. Although the leader-follower systems are not prone to this limitation, they often lack force feedback, impairing their effectiveness in contact-rich tasks. Recently, Kobayashi et al. [13] implemented a bilateral leader-follower teleoperation system where in addition to the follower following the joint positions of the leader, the leader also gets an additional torque if there is a difference in its joint position from that of the follower. However, when the follower arm is in motion without contact, this system causes the operator to experience inertial, frictional, and other dynamic forces of the follower, reducing the ease of use and precision of the system [25]. Our approach introduces an alternative bilateral teleoperation method by relaying only external joint torques from the follower arm back to the leader arm, providing force feedback without impairing operational precision."}, {"title": "III. FACTR LOW-COST BILATERAL TELEOPERATION", "content": "Leader-follower systems, such as GELLO [28] or ALOHA [31], offer a simple and cost-effective solution to teleoperation in manipulation tasks. These systems feature kinematically equivalent leader and follower arms, allowing intuitive control through joint space mapping, where the leader's joint positions are mirrored as targets for the follower. This setup lets users naturally feel the follower arms' kinematic constraints. However, most implementations lack force feed- back, preventing users from sensing the geometric constraints of the environment, which is crucial for teleoperating contact-rich tasks [22]. Instead, those leader arms are mostly passive, lacking active motor torque actuation, despite being equipped with servo motors capable of actuation. Furthermore, the lack of active torque means the leader arms require external structural frames and rubber bands or strings to achieve gravity compensation, reducing portability [31]. In this paper, we aim to fully leverage the servo motors in the leader arm and gripper to achieve force-feedback enabled teleop- eration with affordable hardware. Similarly to GELLO [28], our leader arms use off-the-shelf servos and 3D-printed components, forming a scaled-down but kinematically equivalent version of the follower arms, as shown in Fig. 2. By actuating the servo motors, we introduce force feedback, customizable redundancy resolution through nullspace projection, gravity and friction compensation, and joint limit avoidance. These functionalities augment the teleoperation experience while still using low-cost hardware to provide functions that are usually only available with much more expensive teleoperation devices. Please see Appendix VIII for a detailed Bill of Materials.\nA. Force Feedback\nForce feedback provides the operator with a tangible sense of interaction with the environment, allowing more intuitive and delicate manipulation, especially in contact-rich tasks or tasks with limited visual feedback [22]. We implement a control law that relays external joint torques sensed by the follower arm to the leader arm, allowing the operator to feel the physical constraints experienced by the follower arm:\n$\\text{T}_{\text{feedback}} = \\mu_f K_{f,p} \\text{T}_{\text{ext}} \u2013 K_{f,d} \\dot{q}$ (1)\nwhere $\\mu_f$ is a scalar constant, $\\text{T}_{\text{ext}}$ is the external joint torque sensed by the follower arm, $K_{f,p}$ and $K_{f,d}$ are the PD gains for the force feedback, respectively. Here, $K_{f,p}$ is calculated as the ratio between the maximum torque of the leader and that of the follower, and $K_{f,aq}$ helps reduce oscillations in the leader arm when the follower arm is in contact. We note that $\\text{T}_{\text{ext}}$ is a readily available measurement in various collaborative robot manipulators, such as the Franka Panda and the KUKA LBR iiwa. In particular, we implement mediated force feedback by scaling down $\\text{T}_{\text{ext}}$ with $\\mu_f$, which has been shown to improve the accuracy of the operation while reducing the cognitive load of the operator [32]. Furthermore, we highlight that our implementation only transmits external forces from the follower to the leader; as a result, the operator does not experience the internal friction and inertia of the follower arm during motion, providing a clearer perception of the environment [25]. In addition, we implement force feedback for the parallel- jaw gripper. Since our servo-based gripper does not contain an external force sensor, we utilize the present current reading of the gripper servo to provide force feedback as follows:\n$T_{h,t} = \\alpha(-k_h I_{g,t}) + (1 \u2013 \\alpha) T_{h,t\u22121}$ (2)\nwhere $T_{h,t}$ is the force feedback torque sent to the gripper leader device, $I_{g,t}$ is the present current reading from the follower gripper, and $\\alpha$ is the smoothing factor for the EMA filter. Our system sets $\\alpha = 0.1$ which provides a good user experience.\nB. Customizable Redundancy Resolution\nFor kinematic redundant manipulators, without regulating the joint space, the manipulator tends to drift into unde- sirable configurations under the influence of gravity during teleoperation. Approaches like Gello [28] rely on mechan- ical components, such as springs, to regularize the joint space. However, these components introduce non-uniform, configuration-dependent wrenches at the end-effector, resulting in an unintuitive teleoperation experience. In addition, using mechanical joint regularization effectively prevents the user from setting custom joint regularization targets for redundancy resolution. In confined-space manipulation settings, the inability to control the joint regularization target can impair the arm's reachability, as demonstrated in Fig. 3. In contrast, our proposed method leverages the following null-space projection control law to regulate joint positions [12], which stabilizes the joint-space at any user-defined desirable posture without imposing additional end-effector wrenches, regardless of the arm's configuration:\n$\\text{T}_{\text{null}} = (I-J^+J) (-K_{n,p} (q-q_{\\text{rest}}) \u2013 K_{n,d} \\dot{q})$ (3)\nwhere J is the manipulator Jacobian matrix, $q_{\\text{rest}}$ is a user- defined resting posture configuration, $K_{n,p}$ and $K_{n,d}$ are the PD gains for the null space projection. Note that $(I \u2013 J\u2020J)$ is the null-space projector.\nC. Gravity Compensation\nTo ensure the leader arms remain stationary, allowing the user to easily pause teleoperation, we implement gravity compensation. This is achieved by modeling the dynamics of the leader arm and computing the joint torques required to counteract dynamic forces using the recursive Newton-Euler algorithm (RNEA) for real-time inverse dynamics [16].\n$\\text{T}_{\text{grav}} = M(q) \\ddot{q} + C(q, \\dot{q})\\dot{q} + g(q) = \\text{RNEA}(q, \\dot{q}, \\ddot{q})$ (4)\nwhere M(q) is the mass (or inertia) matrix, C(q, $\\dot{q}$) is the Coriolis and centrifugal matrix, and g(q) is the gravity vector.\nD. Additional Compensation and Controls\nTo reduce the perceived friction in the leader arm during tele- operation, our system provides friction compensation $\\text{T}_{\text{friction}}$. Furthermore, since the leader arm joints lack mechanical joint limits, we implement an artificial potential based control law to prevent users from exceeding joint limits of the follower arm in order to respect the workspace of the follower arm. Finally, for bi-manual follower arms, the system uses Riemannian Motion Policies [23] for dynamic obstacle avoidance between the two follower arms. Please refer to Appendix IX for more details.\nE. Overall Control Law for the Leader Arm\nIn summary, the control torques are defined as follows:\n\u2022 $\\text{T}_{\text{feedback}}$ relays external forces from the follower arm back to the leader arm, allowing the operator to sense the geometric constraints of the environment.\n\u2022 $\\text{T}_{\text{null}}$ resolves kinematic redundancy by regulating the joints at a user-defined rest posture in the null-space.\n\u2022 $\\text{T}_{\text{grav}}$ provides gravity compensation for the leader arm.\n\u2022 $\\text{T}_{\text{friction}}$ compensates for the leader arm joint frictions to enable smoother teleoperation.\n\u2022 $\\text{T}_{\text{limit}}$ prevents the joints of the leader arm from violating the joint position limits of the follower arm.\nThe resulting combined torque applied to the servo motors of the leader arm is defined as follows:\n$\\text{T} = \\text{T}_{\text{feedback}} + \\text{T}_{\text{null}} + \\text{T}_{\text{grav}} + \\text{T}_{\text{friction}} + \\text{T}_{\text{limit}}$ (5)"}, {"title": "IV. FACTR: FORCE-AWARE CURRICULUM TRAINING", "content": "Naively incorporating robot force data into policy learning does not necessarily ensure policy improvement. Contact force signals often provide limited discriminative information for the policy, as it remains close to zero for significant periods when the arm is not interacting with the environment during an episode. As a result, the policy tends to disregard force input and rely predominantly on visual information, as empirically analyzed in Sec. V-C and Fig. 9. To fully leverage the robot force data collected from our teleoperation system, we introduce Force-Aware Curriculum Training (FACTR), a training strategy designed to effectively integrate force information into policy learning. FACTR applies operators like Gaussian blur or downsampling to corrupt visual information, where the amount of visual corruption decreases throughout training. The curriculum intuitively encourages contribution from the force modality at the start of training. We ground this intuition with a theoretical analysis on a simplified scenario through the framework of Neural Tangent Kernels [11]. In this section, we first present the base policy model used for learning from teleoperated demonstrations, and then motivate and describe FACTR, our curriculum training approach. Our overall method is summarized in Algorithm 1 and Fig. 4.\nA. Problem Statement and Base Model\nWe consider a policy $\\pi_{\\theta}(\u00b7 | \u00b7)$ that produces a chunk of future actions of length k $Q_{t:t+k}$ (joint positions) given (i) a visual observation $I_t$ (image at time t), and (ii) an external joint torque reading $T_t$. Our goal is to learn $\\pi_{\\theta}$ via behavior cloning (BC) from a dataset of expert trajectories D. Each trajectory in D comprises tuples $(I_t, T_t, q_t)$, where $q_t$ is the ground-truth (expert) joint position target at time t. We let $\\hat{q}_{t:t+k}$ be the predicted future joint position targets over the next k time steps. The loss is defined by:\n$\\mathcal{L} = MSE(\\hat{q}_{t:t+k}, q_{t:t+k}),$ (6)\nwhere $q_{t:t+k}$ are the expert's future joint position targets and $\\hat{q}_{t:t+k}$ are the policy's predictions. Our policy $\\pi_{\\theta}$ is based on an encoder-decoder transformer that integrates vision and force modalities. Visual observations and force readings are converted into tokens, fed to the encoder, then decoded into action tokens through cross attention.\nA pre-trained vision transformer (ViT) [7, 6] is used to encode an input image $I_t$ into a sequence of vision tokens $z^V_t \\in \\mathbb{R}^{M_2 \\times d}$ for some number of tokens $M_2$ and embedding dimension d. An MLP-based force encoder is applied to the joint torque $T_t$, resulting in a single force token: $z^F_t \\in \\mathbb{R}^{1 \\times d}$. The tokens are concatenated to form the model input:\n$X_t = [z^F_t; z^V_t] \\in \\mathbb{R}^{(M_2+1)\\times d}.$ Then, a transformer encoder Enc processes $X_t$ via multiple self-attention and feed-forward layers:\n$H^E_t = Enc(X_t) \\in \\mathbb{R}^{(M_2+1)\\times d}.$ This yields the encoded vision and force tokens.\nFor the decoder, we introduce k action tokens, $A \\in \\mathbb{R}^{k \\times d}.$ A transformer decoder Dec refines these tokens through self- attention and cross-attention to $H^E_t$:\n$H^P_t = Dec(A, H^E_t).$ During cross attention, each action token attends to both vision and force representations. If we split $H^E_t$ into its vision (V) and force (F) parts, the cross-attention weights for each layer l can be decomposed as follows. For simplicity of notation, assume these weights are already averaged over multiple heads:\nFor the vision part:\n$\\alpha^{(l)}_{t,V} = \\text{softmax} ((A^{(l-1)}W_q^{(l)}) (H_V^{(l)}W_k^{(l)})^T/\\sqrt{d}),$\nFor the force part:\n$\\alpha^{(l)}_{t,F} = \\text{softmax} ((A^{(l-1)}W_q^{(l)}) (H_F^{(l)}W_k^{(l)})^T/\\sqrt{d}).$ These $\\alpha^{(l)}_{t,V}$ and $\\alpha^{(l)}_{t,F}$ measure how strongly each action token attends to vision vs. force tokens at layer l, and will be the main source of analysis in Sec. V-C. Finally, we project the decoder output $H^P$ to action space, which represents joint position targets for the follower arm:\n$\\hat{Q}_{t:t+k} = MLP(H^P_t) \\in \\mathbb{R}^{l \\times d_a}.$ where $d_a$ is the dimension of the action space. Substituting $\\hat{Q}_{t:t+k}$ into Eq. 6 gives the full BC objective. Please see Appendix X for the detailed policy architecture and training hyperparameters.\nB. Force-Aware Curriculum\nThrough experiments, as shown in Sec. V-C and Fig. 9, we found that naively concatenating force data to the policy observation during training often results in policies that neglect force input, failing to leverage force input to the fullest extent. To address this, we employ a curriculum that gradually unveils detailed visual information, encouraging the model to learn to utilize force first. Specifically, we define two operators: $\\beta_P(I, \\sigma_n)$ for the pixel space, and $\\beta_L(z, \\sigma_n)$ for the latent space, where $\\sigma_n$ is a scale parameter (e.g. the standard deviation of a Gaussian kernel or the kernel size of a max pooling operator) that is updated over the course of training for N total gradient steps. During training, we apply the pixel-space operator $\\beta_P$ to image $I_t$ or $\\beta_L$ to visual latent tokens $z^V_t$. Intuitively, the operators make visual inputs or latent tokens close in the metric space, thus encouraging more contribution from the force modality, particularly at the start of the training. Consider the limit $\\sigma \\rightarrow \\infty$, each visual input converges to approximately the same tensor. Hence, the model can only learn a single global output for all visual inputs. Thus, at the early stage of the curriculum, the gradient updates focus more on using the force information and updating the force encoder to maximally differentiate between inputs.\nC. Curriculum Operators\nWe consider two types of operators: Gaussian blur and downsampling.\nFor the Gaussian blur, we define the 2D kernel $G_{\\sigma}$ as:\n$G_{\\sigma}(x,y) = \\frac{1}{2 \\pi \\sigma^2} \\text{exp}\\big( - \\frac{x^2 + y^2}{2 \\sigma^2} \\big)$ The operator $\\beta_P(I, \\sigma)$ applies this kernel using the 2D convolution operator *:\n$\\beta_P(I, \\sigma) = I * G_{\\sigma}$ For the 1D Gaussian blur, the kernel $g_{\\sigma}$ is defined as:\n$g_{\\sigma}(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\text{exp}\\big( - \\frac{x^2}{2 \\sigma^2} \\big)$ The operator $\\beta_L(z^V, \\sigma)$ similarily applies this kernel using 1D convolution:\n$\\beta_L(z^V, \\sigma) = z^V * g_{\\sigma}$ For downsampling, we use MaxPool followed by nearest interpolation. In 2D, the pixel-space operator $\\beta_P(I)$ is:\n$\\beta_P(I) = \\text{NearestInterp(MaxPool2D(I))}$ In 1D, the latent-space operator $\\beta_1(z^V)$ is the same except that a MaxPool1D is used. By gradually reducing $\\sigma_n$, the curriculum ensures that the model focuses first on force tokens, and then incorporates visual information in the later stage of the training. This produces a policy $\\pi_{\\theta}$ that more robustly fuses force and vision for control, alleviating the issue of overfitting to the vision modality.\nD. Curriculum Schedulers\nOver the course of training (indexed by n = 1, . . ., N), we adjust $\\sigma_n$ via a scheduler to control the information released from the visual branch. Given a initial scale $\\sigma_0$, we consider the following schedulers:\nFurther, we warm-up the curriculum by fixing the scale to $\\sigma_0$ for certain gradient steps, and adjust the decay formula to account for this duration. The rationale behind this step is to warm-up the randomly initialized force encoder with relatively low visual information."}, {"title": "V. EVALUATION", "content": "A. Experimental Setup\nWe setup four contact-rich tasks, which are illustrated in Fig. 5 along with the training and testing objects of various shapes and visual appearances. For all tasks, we use Franka Panda arm(s) with OpenManipulator-X gripper(s). Each task uses either a front ZED2 camera or wrist cameras mounted near the grippers with RGB observations. We describe the tasks details and the success criteria below.\nB. Teleoperation Evaluation\nWe compare our leader-follower teleoperation system, which includes our leader arm with force feedback, gravity compen- sation, and redundancy resolution, to an un-actuated leader- follower baseline system with mechanical joint regulation, similar to [28]. We summarize our results in Fig. 7. Our experiments show that our system allows users to complete tasks with 64.7% higher task completion rate, 37.4% reduced completion time, and 83.3% improvement in the subjective ease of use metrics. We observe that for tasks that require continuous contact between the arm and an object, such as non-prehensile pivoting and bimanual box lifting, the un-actuated teleoperation system often causes the follower arm to lose contact with the object. This occurs because of the absence of force feedback, which prevents the user from perceiving the environment's geometric constraints through the leader arms. As a result, maintaining continuous contact with the object becomes challenging. For the un-actuated system, the follower arm frequently exceeds its joint velocity limits when moving under continuous contact. This occurs because the operator can easily maneuver the leader arms in ways that cause significant deviations between the leader and follower joint positions, especially when the follower arm is in contact with the environment. When contact is lost, the resulting large joint-space error causes the PID controller to generate large torques, causing abrupt movements that exceed the velocity limits. On the other hand, our system's force feedback renders geometric constraints of the environment for the operator through the leader arms, preventing the operator from moving the leader arms too far away from the follower arms during environment contacts.\nC. Policy Evaluation\nQuestions. In our real-world evaluation, we seek to address the following research questions regarding FACTR:\nTraining and Evaluation Protocol. We collected 50 demon- strations with our teleoperation system. We trained each method with the same hyperparameters, where details can be found in the Appendix X. We compare the following methods:\nFACTR leads to better generalization. We present our main quantitative results in Fig. 6. All the policies perform similarly on the train objects for most tasks, except for the rolling dough task, where the vision-only policy smashes the dough without any rolling actions and fails completely. Note that the visual observations are hard to distinguish during the oscillatory rolling motions, while the force signals form a corresponding oscillatory pattern, as shown in Fig. 8; this distinctive torque pattern helps policies with force input to complete the task.\nFor the test objects, the vision-only policy achieves a success rate of 21.3% on average, which is significantly worse than policies incorporating force. Without a curriculum, policies naively incorporating force achieve a success rate of 61.2%, while FACTR achieves a success rate of 87.5%, which shows that FACTR leads to significantly better generalization to novel objects. We hypothesize that the force information provides important signals for mode switching at moments such as when the robots get into contact with the box in the lifting task and when the object is grasped in the fruit pickup task.\nPolicies with FACTR learns to identify mode switching. To better understand the policies trained with FACTR. We visualize the attention behavior during policy training and inference. Specifically, we visualize the cross attention of the action tokens to the memory tokens denoted as $\\alpha^{(l)}_{t,V}$ and $\\alpha^{(l)}_{t,F}$ for the first layer of the decoder, where $\\alpha^{(l)}_{t,V}$ and $\\alpha^{(l)}_{t,F}$ are defined in Sec. IV. During policy rollout, we visualize the average cross attention of the action tokens to the force or vision tokens of the first decoder layer as shown in Fig. 9. FACTR learns to attend to force more during task execution. For example, in the box lifting task, attention to force outweighs that of vision as the arms contact the box, signaling a mode switch. While without the curriculum, the policy does not pay enough attention to force, and either fails to lift or balance the novel boxes. FACTR leads to better recovery behavior. Another notable observation is that FACTR also facilitates recovery behavior. Specifically, we evaluate the box-lifting task with five trials per object. A trial begins when the policy successfully lifts the box for the first time; we then knock the box down and assess the second attempt. As shown in Table I, all policies maintain nearly 100% recovery success on training objects. However, for test objects, the vision-only policy's success rate drops significantly from 31.7% on the first attempt to 13.3% on the second. In contrast, force-aware policies maintain similar success rates across both attempts. We observe that vision-only policies often remain static after the box is knocked down, failing to retry. We hypothesize that this occurs because the vision-only policy overfits to training scenarios, making it unresponsive to unseen objects outside its training distribution. In contrast, FACTR policies detect loss of contact through external joint torque readings, which revert to pre-lift values when the object is dropped. Since our FACTR policies effectively attend to force input, they successfully recover to a pre-lift state and attempt the task again.\nD. Ablations on Curriculum\nTo further validate the significance of a curriculum, we trained models with fixed $\\sigma_n$ across training. Moreover, to ablate on pixel space and latent space curriculum, and different scheduler and operator choices, we train policies on different combination of these parameters. We choose the task of pivoting, one of the hardest tasks from our task suite, for the ablations. We evaluate only on the five test objects for five trials each, since they are more indicative of policy performance than train objects. The results are presented in TABLE II. We found that performance with a curriculum of decaying smoothing performs better than a fixed curriculum across all tasks. We hypothesize that to enable better performance, the final policy needs to take in the fully unblurred vision information. Through the curriculum, a policy gets to gradually adapt to unblurred images. On the other hand, with fixed smoothing, even though policy may not overfit to visual information, it cannot extract the necessary details from unblurred vision to complete the tasks. Comparisons with other scheduler parameters. We further compare policies trained with either pixel space or latent space, two operators (Gaussian blur and downsample) as defined in Sec. IV-C, and four schedulers (linear, cosine, exponential, and step) as defined in Sec. IV-D. However, we do not find a uniform advantage or disadvantage for any set of parameters. The results suggest that FACTR is relatively robust to different sets of curriculum parameters."}, {"title": "VI. CONCLUSION AND LIMITATIONS", "content": "We introduced FACTR, a curriculum approach to train force- based policies to improve performance and object general- ization in contact-rich tasks. FACTR leverages a blurring operator with decreasing scales on the visual information throughout training. This encourages the policy to leverage force input at the beginning stages of training, preventing the problem where the policy overfits to visual input and thus neglects force input. This approach was demonstrated through a series of experiments on the following tasks: box lifting, non-prehensile pivoting, fruit pick-and-place, and rolling dough, where FACTR exhibits significant improvements in task completion rates and generalization to unseen object appearances and geometries. Additionally, our teleoperation system, which includes an actuated leader arm for force feedback and gravity compensation, was shown to provide a more intuitive user experience, as evidenced by higher task completion rates and user satisfaction in our studies.\nWhile FACTR demonstrates significant improvements in force-based policy learning for contact-rich tasks, it has limita- tions. First, the precision of the external joint torque sensors in our follower arm is limited. This limitation can particularly affect tasks that involve subtle force adjustments during fine- grained manipulation since the torque readings can be too noisy to be used effectively. Future work could explore integrating high-resolution tactile sensors or haptic gloves to enhance feedback precision and improve overall system performance. Second, our approach assumes the availability of external joint torque sensors in the follower arms. Future work can explore adapting our system for an arm mounted with an end-effector force-torque sensor. Third, the effectiveness of our curriculum learning approach can be influenced by several hyperparameters, such as the choice of the blurring operator and scheduling strategies. These parameters can be highly task-dependent, requiring extensive tuning for different applications. Developing adaptive or self-tuning curriculum strategies could help mitigate this issue by dynamically adjusting hyperparameters based on task-specific requirements. Addressing these limitations could further enhance FACTR's applicability and robustness across a broader range of contact-rich manipulation tasks."}, {"title": "APPENDIX VII. ANALYSIS OF FACTR FROM NEURAL TANGENT KERNEL (NTK) PERSPECTIVE", "content": "In this section", "1": ".", "as": "n$\\kappa(x_i", "by": "n$k_\\infty(x_i", "rangle": "where the expectation is taken over the Gaussian initialization of \u03b8. The NTK $k(x_i", "function": "if $k(x_i"}, {"by": "n$f(x) - y = e^{-\\eta K t}(f_\\theta("}]}