{"title": "Enhancing Nursing and Elderly Care with Large Language Models: An AI-Driven Framework", "authors": ["Qiao Sun", "Jiexin Xie", "Nanyang Ye", "Qinying Gu", "Shijie Guo"], "abstract": "This paper explores the application of large language models (LLMs) in nursing and elderly care, focusing on AI-driven patient monitoring and interaction. We introduce a novel Chinese nursing dataset and implement incremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to enhance LLM performance in specialized tasks. Using LangChain, we develop a dynamic nursing assistant capable of real-time care and personalized interventions. Experimental results demonstrate significant improvements, paving the way for AI-driven solutions to meet the growing demands of healthcare in aging populations.", "sections": [{"title": "Introduction", "content": "The rapid advancement of large language models (LLMs) has opened new avenues for healthcare applications. While LLMs have demonstrated impressive capabilities in generating human-doctor-like clinical decisions and integration into healthcare (Thirunavukarasu et al., 2023; Tan et al., 2024; Ullah et al., 2024; Li et al., 2023), its expertise in nursing remains in its nascent stages.\nOn the one hand, nursing scenarios are more complex than other clinical decision cases, such as medication prescription or diagnostic imaging, as they involve continuous monitoring, real-time decision-making, and patient interaction, requiring models that can handle a wider array of multimodal inputs and adapt dynamically to evolving patient conditions (Carayon and Gurses, 2008). On the other hand, nursing tasks often involve high levels of direct patient interaction, demanding models that can process complex multimodal inputs\u2014such as voice, text, and even visual cues in real time. China has experienced a significant increase in its aging population. By 2022, individuals aged 60 and above accounted for 19.8% of the population (Global Times, 2023), a figure projected to rise to 28% by 2040 (Peng, 2023). This demographic shift is expected to place considerable pressure on the country's healthcare system, particularly in meeting the demand for nursing care. Despite this growing need, the supply of skilled nursing services remains inadequate. There is a noticeable gap between the expertise required to care for the elderly and the qualifications of the current healthcare workforce. A 2023 investigation revealed that only 7.18% of workers in China's elderly care industry hold a bachelor's degree or higher, highlighting the urgent need for more qualified personnel (Zhang and Zhang, 2023).\nOur work seeks to address this disparity by developing AI-driven Nursing and Elderly-Care solutions tailored to the specific needs of the nursing profession, leveraging cutting-edge large language model (LLM) techniques. We focus on creating LLMs that support patient monitoring, personalized care, and facilitate effective communication between healthcare providers and patients. Additionally, we are exploring the development of a Langchain agent application based on this specialized model, alongside its potential for multimodal processing.\nIn this paper, we make the following contributions to both the NLP community and the Nursing and Elderly Care industry:\n\u2022 We pioneered the application of large language models in nursing and elderly care, proposing a SOTA model and gathering fine-tuning expertise specific to these fields.\n\u2022 We developed the first multilayer Chinese nursing dataset for elderly care and demonstrate its effectiveness through ablation studies. We also establish a benchmark test set to evaluate fundamental nursing knowledge and skills.\n\u2022 We investigate the use of nursing robots powered by our LLM, evaluating their performance in essential nursing tasks and exploring their potential to incorporate visual processing in care environments."}, {"title": "Related Work", "content": "Studies have highlighted the transformative role of Large Language Models (LLMs) in healthcare, including applications in clinical decision-making, patient care, and medical education. Comprehensive surveys discuss the development and deployment of LLMs across various medical tasks, focusing on their potential for improving diagnostic accuracy and streamlining medical workflows (Zhou et al., 2023; Nazi and Peng, 2024). (Zhou et al., 2023) also highlights the performance of models like GPT-4 and MedPaLM across ten biomedical natural language processing tasks, demonstrating their generalization ability to outperform traditional models in various discriminative and generative tasks. However, the existing body of work mainly focuses on the general medical applications of LLMs, while LLMs specifically designed for nursing applications are left under-explored.\nNursing environments present a unique set of challenges, such as time-sensitive decision-making, handling diverse patient populations, and managing high-stress situations. Current general medical LLMs may not be fully equipped to address these demands, emphasizing the need for more focused research on LLMs designed specifically for nursing applications.\nThe current related work in nursing primarily focuses on theoretical exploration and future possibilities, rather than practical implementation. (Xiong et al., 2023) validates the combination of LLMs with local knowledge bases for intelligent nursing decision-making, highlighting the importance of contextual adaptation. However, their model is developed solely on the text modality, lacking integration with other crucial data sources such as audio and visual inputs. Other work (Perspect, 2023; Woo et al., 2024) discusses the implications of LLMs in healthcare education, noting both their potential and the need for cautious implementation. These studies collectively underscore the importance of contextual, safe, and practical integration of LLMs in nursing.\nThe most closely related work to ours is LlamaCare (Li et al., 2024), a large language model that utilizes instruction-based tuning to integrate diverse clinical data, improving its ability to generate discharge summaries and predict outcomes like mortality and hospital stays. LlamaCare surpasses existing LLM benchmarks in producing accurate and coherent clinical texts, demonstrating its potential for broader clinical use. However, its focus spans a wide range of healthcare domains, with less emphasis on the foundational knowledge specific to nursing."}, {"title": "Nursing Datasets for LLMS", "content": "Developing nursing-specific datasets is essential for improving LLMs in healthcare, but such datasets are limited, restricting their application in specialized fields like nursing. Although the MIMIC-III database (Johnson et al., 2016) offers structured data, it lacks alignment with the unstructured text needed for LLMs.\nWang et al. (Wang et al., 2023b) introduced MedNgage, a dataset focused on patient-nurse conversations, annotated to distinguish between socio-affective and cognitive engagement. Fine-tuning transformer models on this dataset enhances AI-driven predictions in patient care.\nXiong et al. (Xiong et al., 2023) developed a dataset that integrates LLMs with local knowledge bases for decision-making in nursing, but it primarily addresses textual data, lacking the multimodal inputs (e.g., audio, visual) essential for real-time patient interactions."}, {"title": "Method", "content": "Our method builds upon cutting-edge large language models (LLMs) by applying supervised fine-tuning (SFT) to adapt these models specifically for nursing and elderly care tasks. We primarily tested two advanced models: GLM4 (GLM et al., 2024) and LLaMA 3.1 (Vavekanand and Sam, 2024), both of which represent the state-of-the-art in LLM development, and can be integrated with multimodal ability easily via projection and finetuning (Wang et al., 2023a; Liu et al., 2024, 2023a,b)."}, {"title": "Dataset", "content": "We developed a specialized dataset named \"NursingPiles\", designed to comprehensively cover various sources and levels of professional knowledge in nursing and elderly care. This dataset is built from multiple sources, including textbooks, manuals, legal documents, and research papers, synthesizing data into question-answer (QA) pairs. To mitigate catastrophic forgetting (Zhai et al., 2023), which can occur during model fine-tuning, we introduced open-source datasets as part of a data-mixing strategy. This approach helps maintain the model's original dialogue capabilities while fine-tuning it for specialized tasks in nursing care."}, {"title": "Training Protocol", "content": "For the model training, we utilized the Parameter-Efficient Fine-Tuning (PEFT) package along with an Incremental Pre-training (IPT) process to further optimize the model's performance. The training was conducted on 8 \u00d7 NVIDIA A100-80GB GPUs, with a total training time of approximately 72 hours for fine-tuning, while the IPT stage took an additional 30 hours. The parameter settings for both stages are presented in Appendix A Table 4."}, {"title": "LangChain Prompting", "content": "In this design, we present a modular system for a dynamic nursing assistant, capable of handling the full lifecycle of patient care, including real-time data collection, personalized care plan generation, and continuous monitoring. The system integrates IoT devices for health data collection, AI-based diagnostics, and personalized care recommendations through LangChain. Critical to the design is the secure storage and management of patient information, utilizing AES encryption and key management services (KMS) to ensure data protection. Additionally, we employ OAuth and JWT for robust authentication, ensuring authorized access to encrypted data, and provide post-care follow-up with automated reminders and health education. This architecture allows for flexible, secure, and scalable patient care management. Appendix A providing core code and snippets for key processes."}, {"title": "Benchmark", "content": "We selected several authoritative exam questions, such as the \"Three Basics and Three Stricts\" exam questions (Zhang, 2020) and the postgraduate nursing exam questions (Li, 2019), as evaluation benchmarks. The entire set of questions includes two parts: multiple-choice questions and open-ended questions. For the multiple-choice questions, the \"Three Basics and Three Stricts\" test covers content from nine subjects, including basic theory (such as anatomy, physiology, and pathology), basic knowledge (including pharmacology, microbiology, and disease studies), and basic skills (such as nursing procedures, emergency techniques, and nursing operations). These subjects can objectively and comprehensively reflect the nursing knowledge and capabilities of the model (Wang, 2018). For this part of the questions, we use the P-R-F1 metrics to evaluate."}, {"title": "Experiments", "content": "We evaluated the performance of the models using Precision, Recall, F1-score, and Accuracy. The results demonstrate that our models, which integrate both Incremental Pretraining (IPT) and Supervised Fine-Tuning (SFT), significantly outperform the baseline models. The GLM4-Chat 9B + IPT + SFT achieved the best performance with a Precision of 86.78%, Recall of 85.65%, F1-score of 86.21%, and Accuracy of 58.9%. These improvements highlight the importance of combining domain-specific pretraining with fine-tuning. For more details see Table 2."}, {"title": "Ablation Analysis", "content": "To assess the individual contributions of IPT and SFT, we conducted an ablation study by removing each component separately. The results show that removing either IPT or SFT results in a drop in performance across all metrics. For instance, without SFT, the LLaMA + IPT model saw a significant reduction in Recall (from 78.09% to 72.5%) and F1-score (from 77.75% to 74.69%). Similarly, removing IPT resulted in reduced performance for both models, particularly in Accuracy. This confirms that both components are crucial for optimal model performance in the nursing and elderly care domain. For more details see Table 3."}, {"title": "Conclusion", "content": "This paper presented an approach to apply large language models (LLMs) in nursing and elderly care by utilizing incremental pre-training (IPT) and supervised fine-tuning (SFT). We developed a Chinese nursing dataset, demonstrating its effectiveness through improved performance in specialized tasks. Additionally, we explored the use of LangChain for a dynamic nursing assistant, enabling real-time monitoring and personalized care. Our results highlight the potential of LLMs to address the growing demand for skilled nursing care."}, {"title": "Limitations", "content": "There are several concerns with respect to the limitations:\nFirst, the model primarily focuses on text-based data, and further integration of audio and visual inputs is needed. Second, the dataset is largely Chinese-focused, limiting broader applicability across languages and cultures. Third, model responsiveness in real-time clinical settings remains a challenge. Last, ensuring patient privacy, consent, and minimizing bias in AI-driven care requires further consideration."}, {"title": "Ethics Statements and Justifications", "content": "The dataset used in this study consists of four subsets: text in markdown format, single-turn dialogues, multi-turn dialogues, and image-text pairs. All subsets, except the image-text pairs, were collected and automatically annotated through our data processing pipeline, as summarized in Table 1."}, {"title": "Participant Involvement and Consent", "content": "The Image-text pairs subset consists of 2510 image-text pairs, collected by the research team in Room 310, School of Mechanical Engineering, Hongqiao Campus, Hebei University of Technology (specific address: 5340 Xiping Road, Beichen District, Tianjin, China).\nAll participants were research team members who signed informed consent forms prior to data collection, including: (1) Awareness that their facial images might appear in the dataset and be used for academic research; (2) Consent to waive portrait rights, including potential public display of content within the dataset; (3) Knowledge that the data may be published on public platforms for academic research, model training, and related publications; and (4) The right to withdraw consent at any time, though data already utilized or published remains lawful. The full version of the consent form can be viewed in this link."}, {"title": "Annotators' Rights and Responsibilities", "content": "Among the four subsets, only the image-text pairs subset requires data annotation, where each person and piece of equipment in the photos is annotated using polygon segmentation masks. All the labeling processes were performed using LabelMe, an open-source annotation tool. We engaged 10 individual annotators, both from our research group and on-line, to ensure their workload remained manageable. We provide compensation to annotators in accordance with market standards, ensuring full compliance with labor payment laws in China. The annotators all agreed not to save or share any portion of the collected or annotated data."}, {"title": "Intellectual Property", "content": "The intellectual property rights of the images and text descriptions in the dataset belong to the research team. Usage is permitted within the scope of research, but unauthorized commercial use is prohibited.\nThe source of our dataset adheres to intellectual property (IP) regulations:\nText in Markdown Format: Sources include publicly available textbooks, manuals, and industry regulations. Usage complies with fair use for noncommercial academic purposes. Single-Turn Dialogues: Developed using open-source research papers, nursing safety guides, and open-source medical datasets. The licenses were reviewed to ensure compliance. Multi-Turn Dialogues: Generated using GPT-40 and InternLM based on publicly available knowledge. The usage of this content complies with the relevant user terms and is for noncommercial academic research purposes only. All outputs are original and created without infringing any third-party intellectual property rights. In case any third-party claims arise, we are committed to addressing and resolving such matters in compliance with applicable laws and regulations. Image-Text Pairs: Collected and annotated by our research team with the consent of the participants. All rights are reserved by the research group for academic research use. Licensing and Usage: The dataset is released under an open-source license (e.g., CC BY-NC 4.0) for non-commercial research, requiring proper attribution."}]}