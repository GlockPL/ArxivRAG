{"title": "INFERENCE WITH THE UPPER CONFIDENCE BOUND ALGORITHM", "authors": ["KOULIK KHAMARU", "CUN-HUI ZHANG"], "abstract": "In this paper, we discuss the asymptotic behavior of the Upper Confidence Bound (UCB) algorithm in the context of multiarmed bandit problems and discuss its implication in downstream inferential tasks. While inferential tasks become challenging when data is collected in a sequential manner, we argue that this problem can be alleviated when the sequential algorithm at hand satisfy certain stability properties. This notion of stability is motivated from the seminal work of Lai and Wei [19]. Our first main result shows that such a stability property is always satisfied for the UCB algorithm, and as a result the sample means for each arm are asymptotically normal. Next, we examine the stability properties of the UCB algorithm when the number of arms K is allowed to grow with the number of arm pulls T. We show that in such a case the arms are stable when $\\frac{\\log K}{\\log T} \\rightarrow 0$, and the number of near-optimal arms are large.", "sections": [{"title": "1. Introduction", "content": "Reinforcement learning (RL) has emerged as a cornerstone of artificial intelligence, driving breakthroughs in areas from game-playing agents to robotic control. Its ability to learn optimal decision-making strategies through environmental interaction has positioned RL as a key technology in the development of autonomous systems. Central to RL is the exploration-exploitation dilemma, where agents must balance discovering new information with leveraging known high-reward options. The Upper Confidence Bound (UCB) algorithm addresses this dilemma through the principle of optimism in the face of uncertainty. By maintaining upper confidence bounds on the expected rewards of each action, UCB provides a theoretically grounded approach to balancing exploration and exploitation in various RL settings.\nHowever, the adaptive nature of data collection in RL violates the independent and identically distributed (i.i.d.) assumption underpinning many statistical methods. This sequential dependency poses significant challenges for analysis and inference in RL contexts. Despite these challenges, robust statistical inference remains crucial for RL. It enables quantification of uncertainty, validation of model performance, and provision of reliability guarantees\u2014essential factors as RL systems are deployed in increasingly critical applications.\nThis paper investigates a novel stability property in adaptive RL algorithms. We show that the UCB algorithm induces a form of stability in the sequential non-iid data which makes the downstream statistical inference process straightforward. For instance, under this stability property classical statistical estimators asymptotically normal.\n1.1. Related Work. In this section, we provide a brief survey existing literature on Multiarmed bandits, the UCB algorithm, and inference with data generated from sequential procedures."}, {"title": "1.1.1. Multiarmed bandits and the UCB algorithm", "content": "The study of multi-armed bandits has a rich history, dating back to the seminal work of Thompson [27] and Robbins [24]. The Upper Confidence Algorithm (UCB) algorithm, introduced by Lai and Robbins [18] and Lai [17] and later refined by Auer [3], has become a cornerstone of the field due to its strong theoretical guarantees and empirical performance. The early (regret) analysis of the UCB algorithm is by Kathehakis and Robbins [15] and later refined by [2]. The UCB algorithm is based on an principle of optimism in the face of uncertainty, which turned out to be very influential in the area of reinforcement learning and sequential decision-making in general. We refer the reader to the book [20] and the references therein for a discussion and several applications of this principle."}, {"title": "1.1.2. Statistical inference with adaptively collected data", "content": "While sequential decision-making procedures like the UCB are known to yield low regret, the sequential nature of the algorithm induces a dependence in the resulting data-set. Stated differently, the data collected by sequential data generating processes like UCB are not iid, thus it is not clear how can we perform downstream statistical inference using the adaptively collected data-set.\nThe challenge of performing statistical inference with adaptively collected data has been recognized in the statistical literature for at least 40 years. In the context of time series analysis, works by Dickey and Fuller [9], White [29, 30], and Lai and Wei [19] highlighted the breakdown of classical asymptotic theory when the data is generated from an regressive time series model. More recent work [7, 8, 16, 21, 22, 31] highlighted, via numerical simulations, that a similar phenomenon can occur in bandit problems. Zhang et al. [34] proved that in the case of batched bandits Z-estimators (sample arm means for multi-armed bandits) may not be asymptotic normal when the data is collected via popular bandit algorithms like UCB, Thompson sampling, and the $\\epsilon$-greedy algorithm.\nIn order to perform valid statistical inference, researchers have suggested two types of approaches. The first approach is non-asymptotic in nature, and is based on the concentration bounds on self-normalized Martingales; see the works [1, 25, 28] and the references therein. These works stem from the seminal works of [6, 23], and provide confidence intervals that are valid for any sample size and are usually more conservative (larger confidence intervals). The second approach is asymptotic in nature, and it exploits the inherent Martingale nature present in data via Martingale central limit theorem and debiasing [13, 33]. The confidence intervals obtained are valid asymptotically, and are often shorter than the ones obtained from concentration inequality based methods. See the works [5, 7, 8, 12, 16, 21, 22, 26, 31, 32, 34] and the references therein for an application of this technique."}, {"title": "1.2. Failure of classical estimators: A closer look", "content": "While classical guarantees are not valid under when the data is collected via sequential methods, it is worthwhile to understand whether that is always the case. To motivate this discussion, we consider two algorithms for collecting data in a 2-armed bandit problem with arm means vector $(\\mu_1, \\mu_2) = (0.3, 0.3)^T$ with standard Gaussian error.\n(a) The arms are selected using an $\\epsilon$-greedy algorithm with $\\epsilon = 0.1$.\n(b) The arms are selected using the UCB algorithm, detailed in Algorithm 1."}, {"title": "1.3. Contributions", "content": "The main contribution of this paper is to show that the such a stability property is satisfied for when we use the upper confidence bound (UCB) algorithm in a K-armed bandit problem. Specifically, we prove\n(a) The number of times an arm $a \\in [K]$ is pulled by the UCB algorithm converges in probability to a deterministic limit."}, {"title": "2. Problem set up", "content": "In this paper, we consider a multiarmed bandit problem with $K$ arms. At each time step $t$, the player selects an arm $A_t \\in [K]$ and receives a random reward $X_t \\in R$ from the distribution $P_{A_t}$. Let $\\mu_a$ and $\\sigma_a^2$, respectively, denote the mean and variance of the distribution $P_a$, and $\\mu = (\\mu_1,..., \\mu_K)^T$ denote the vector of arm-means. The goal of bandit algorithms is to maximize the total expected reward or equivalently to minimize the regret, defined as the difference between the total expected reward obtained and the maximum possible total reward, over $T$ time steps:\n(1)\t(Regret:)\t$R_T = T \\cdot \\mu^* - E[\\sum_{t=1}^T X_t]$ where $\\mu^* = \\max_{k \\in [K]} \\mu_k$.\nIn this paper, we are interested in the behavior of the celebrated Upper Confidence Bound algorithm [3, 17, 18]. We detail the algorithm in Algorithm 1. Throughout we assume that $T$ is known to the algorithm."}, {"title": "2.1. Asymptotic normality and arm-pull stability", "content": "We now connect the asymptotic normality property (4) of arm $a$ with that of certain stability property of $n_{a,T}$, the number of times arm $a$ is pulled in $T$ rounds. Note that $n_{a,T}$ is random.\nDEFINITION 2.1. An arm $a \\in [K]$ is said to be stable if there exists non-random scalar $n^*_a$ such that\n(5)\t(Stability:)\t$\\frac{n_{a,T}}{n^*_a} \\xrightarrow{P} 1$ with $n^*_a \\rightarrow \\infty$\nHere, the scalar $n^*_a$ is allowed to depend on $T, {\\mu_a}_{a \\in [K]}, {\\sigma^2_a}_{a \\in [K]}$.\nThis dentition is motivated from the seminal work of Lai and Wei [19], where the authors used a similar condition to prove asymptotic normality of the least square estimators in a stochastic regression model. We now show that if arm $a$ is stable, then the asymptotic normality (4) holds for arm $a$.\nLet $\\mathcal{F}_{t-1}$ denote the $\\sigma$-filed generated by ${X_1, ..., X_{t-1}}$. We assume that the arm reward distributions ${P_a}$ are $\\lambda$-sub-Gaussian, and the number of arms K is fixed for simplicity (we relax this assumption at a later section). By definition (3) we have that $A_t \\in \\mathcal{F}_{t-1}$, and given an arm $a$, the sum $\\frac{1}{\\sigma_a\\sqrt{n(T)}} \\sum_{t=1}^T 1_{A_t=a} \\cdot (X_t - \\mu_a)$ is a sum of Martingale difference sequence. We have\n$\\sum_{t=1}^T \\mathbb{V}ar \\left(\\frac{1}{\\sigma_a\\sqrt{n(T)}} 1_{A_t=a} \\cdot (X_t - \\mu_a) | \\mathcal{F}_{t-1}\\right) = \\frac{n_{a,T}}{n(T)} \\xrightarrow{P} 1$.\nStated differently, the sum of the conditional variances of the Martingale difference array stabilizes. Additionally, using the assumption $n \\rightarrow \\infty$ and sub-Gaussian property of the reward distribution we have that the Lindeberg condition of Triangular array is satisfied. Applying the Martingale CLT for triangular array [10, 13], and using Slutsky's theorem we conclude\n(6)\t$\\frac{\\sqrt{n_{a,T}}}{\\sigma_a} (\\bar{X}_{a,T} - \\mu_a) \\xrightarrow{d} N(0,1)$"}, {"title": "3. Main results", "content": "Here we provide our main result which shows stability of the UCB algorithm. Without loss of generality we assume that arm 1 is among optimal arms, and we do not assume that the optimal arm is unique. We also assume that\n(7a)\t$0 \\leq \\frac{\\Delta_a}{\\sqrt{2\\log T}} = o(1)$ for all arms $a$.\n(7b)\t$P_a$ is $\\lambda_a$ \u2013 sub-Gaussian for all arms $a$, and $|\\lambda_a| \\leq B$ for some constant $B < \\infty$.\nWe will explain the significance of these assumptions shortly. Our main result is the following:"}, {"title": "Comparison to prior work", "content": "It is interesting to compare Theorem 3.1 with an earlier work of [14]. Parts (II) and (III) of Theorem 1 in [14] provide the limiting distribution of the number of arm pulls when $\\Delta = \\mu_1 - \\mu_2 \\leq \\frac{\\sigma}{\\sqrt{T^{\\theta}log T}}$ for some $\\theta > 0$. Theorem 3.1 recovers these results when we substitute $K = 2$. When $\\frac{\\Delta}{\\sqrt{\\frac{\\log T}{T}}} \\rightarrow \\infty$, Part (I) of Theorem 1 in [14] provides the limiting distribution of $n_{1,T}$. On the contrary, Theorem 3.1 characterizes the behavior of both $n_{1,T}$ and $n_{2,T}$. Finally, Theorem 3.1 provides the limiting distribution of $K$ arms for any choices of ${\\Delta_a}$, thereby resolving the problem that was left open in [14]."}, {"title": "3.1. Statistical inference:", "content": "Theorem 3.1 allows us to provide an asymptotically exact $1 - \\alpha$ confidence interval. Let the $\\hat{\\sigma}^2_a$ denote the variance of the rewards associated with arm $a$. Given a fixed direction $u = (u_1, ..., u_K)^T \\in \\mathbb{R}^K$, define\n(10)\t$C_{u,\\alpha} = u^T \\bar{X}_T \\pm z_{1-\\alpha/2} \\sqrt{\\sum_{a=1}^K \\frac{\\hat{\\sigma}_a^2 u_a^2}{n_{a,T}}}$"}, {"title": "4. Can we let the number of arms grow?", "content": "In this section, we study the stability properties of arms when the number of arms grows with the number of arm pulls $T$. Unfortunately, some of the proof techniques used in the previous section does not apply when the number of arms $K = K(T)$ is allowed to grow with the number of arm pulls $T$. The fist assumption that we need is\n(12)\t$\\frac{\\log K}{\\log T} \\rightarrow 0$ as $T \\rightarrow \\infty$.\nIn other words, $K$ grows slower than any positive power of $T$. This assumption ensures that a finite sample version of the law of iterated logarithm holds simultaneously for all $K$ arms. We start with the definition of near-optimal arms. Again, without loss of generality we assume that arm 1 is among the optimal arms.\nNear optimal arms: Given a constant $B > 0$, the set of B-near optimal arms are defined as:\n(13)\t$S_B := \\{a \\in [K] : \\frac{\\Delta_a \\sqrt{n^*}}{\\sqrt{2\\log T}} < B\\}$"}, {"title": "Condition on K:", "content": "The condition (12) is needed to ensure that a high probability bound inspired by the law of iterated logarithm (LIL) holds for all arm-means. The term $\\log K$ arises from union bound over K arms, and the term $\\log T$ comes from the UCB bonus term in (2). The condition (12) allows the number of arms $K$ to grow with $T$. For instance, Theorem 4.1 allows\nK = exp{(log T)^{1-\\delta}} for any $1 > \\delta > 0$.\nComment on the near-optimal arm condition (14). The condition (14) assumes that the number of near-optimal arms are large. The proof of the Theorem 4.1 reveals that the stability of the arms are related to the stability of arm 1. In other words, if arm 1 is stable, then all other arms are also stable. The condition (14) ensures that arm 1 is stable. This condition can be understood by looking at the characteristic equation (9). In order to infer the properties of $n^*$, we need to make sure that the term $\\frac{\\sqrt{T}}{n^*}$ is dominating in the term in the term $\\sqrt{T/n^*} + \\sqrt{T\\Delta^2/2 \\log T}$. Indeed, we have\n$\\sqrt{T/n^*} < \\sqrt{T/n^*} + \\sqrt{T\\Delta^2/2 \\log T} = \\sqrt{T/n^*} + \\sqrt{T/n^*} \\cdot \\sqrt{n^*\\Delta^2/2 \\log T}$\n$\\leq (1 + B) \\sqrt{T/n^*}$"}, {"title": "5. Proofs of Theorems", "content": "In this section, we provide proofs of Theorems 3.1, 3.2 and 4.1.\n5.1. Proof of Theorem 3.1. The bulk of the argument is based on the following concentration bound for sub-Gaussian random variables:\nLEMMA 5.1. Let $X_1, X_2, . . .$ be i.i.d. $\\lambda_a$-sub-Gaussian random variable with zero mean. Then\nP(\\left\\|\\sum_{i=1}^t X_i\\right\\| > \\lambda \\sqrt{t}\\left(\\sqrt{\\frac{9}{\\delta}}\\left(\\frac{\\log^2 (4\\delta t)}{4t}\\right)^3\\right))\\leq 2\\delta"}, {"title": "5.1.6. Stability of arm 1", "content": "Rearranging the (21) relation we have\nCombining the last two bounds and using $n_{1,T} \\geq T/2K$ from (22) we have\nFrom here, we would like to show that $n_{1,T}/T$ converges to some non-random quantity asymptotically. For a given T, let $n^* = n^*(T)$ be the unique solution to the following:"}, {"title": "5.2. Proof of Theorem 3.2", "content": "The proof of this theorem utilizes Lai and Wei [19, Theorem 3] and Theorem 3.1. Indeed, writing the multiarmed bandit problem as a stochastic regression model from Lai and Wei [19] we see that Theorem 3.1 ensures that the covariate stability condition [19, Condition 4.2] holds. Additionally, the condition [19, Condition 4.3] holds since $n^* \\rightarrow \\infty$ and $\\frac{\\Delta_a}{\\sqrt{2\\log T}} = o(1)$ (see for instance (8)). Thus, invoking [19, Theorem 3] we have\n\\frac{\\hat{X}_{1,T} - \\mu_1}{\\sqrt{\\frac{\\hat{\\sigma}^2_1}{n_{1,T}}}}, ...., \\frac{\\hat{X}_{K,T} - \\mu_K}{\\sqrt{\\frac{\\hat{\\sigma}^2_K}{n_{K,T}}}}} \\xrightarrow{d} N(0, \\Sigma)."}, {"title": "6. Discussion", "content": "This paper establishes a novel stability property of the upper confidence bound (UCB) algorithm in the context of multi-armed bandit problems. This property makes the downstream statistical inference straightforward; for instance, classical statistical estimators remain asymptotically normal even though the data collected is not iid. Moreover, we show that when the number of arms K are large, and potentially allowed to grow with the number of total arm pulls. Finally, our result imply that the UCB algorithm is fair. Concretely, if two arm means are close, the UCB algorithm will select both arms equal number of times in the long run (asymptotically).\nWhile these findings represent a significant advance, several open questions remain. Future research could explore the extension of these results to heavy-tailed distributions, investigate stability properties of other popular bandit algorithms, and examine potential trade-offs between stability and regret minimization. Put differently, it would be interesting to see if it is possible to verify the stability of other popular bandit algorithms or propose a stable analogues without increasing the regret by significant amount. We believe establishing stability properties of reinforcement learning algorithms would improve the reliability and reproducibility of reinforcement learning systems in practice."}]}