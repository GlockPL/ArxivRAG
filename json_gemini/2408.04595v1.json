{"title": "INFERENCE WITH THE UPPER CONFIDENCE BOUND ALGORITHM", "authors": ["KOULIK KHAMARU", "CUN-HUI ZHANG"], "abstract": "In this paper, we discuss the asymptotic behavior of the Upper Confidence Bound (UCB) algorithm in the context of multiarmed bandit problems and discuss its implication in downstream inferential tasks. While inferential tasks become challenging when data is collected in a sequential manner, we argue that this problem can be alleviated when the sequential algorithm at hand satisfy certain stability properties. This notion of stability is motivated from the seminal work of Lai and Wei [19]. Our first main result shows that such a stability property is always satisfied for the UCB algorithm, and as a result the sample means for each arm are asymptotically normal. Next, we examine the stability properties of the UCB algorithm when the number of arms K is allowed to grow with the number of arm pulls T. We show that in such a case the arms are stable when $\\frac{\\log K}{\\log T}\\rightarrow 0$, and the number of near-optimal arms are large.", "sections": [{"title": "1. Introduction.", "content": "Reinforcement learning (RL) has emerged as a cornerstone of artificial intelligence, driving breakthroughs in areas from game-playing agents to robotic control. Its ability to learn optimal decision-making strategies through environmental interaction has positioned RL as a key technology in the development of autonomous systems. Central to RL is the exploration-exploitation dilemma, where agents must balance discovering new information with leveraging known high-reward options. The Upper Confidence Bound (UCB) algorithm addresses this dilemma through the principle of optimism in the face of uncertainty. By maintaining upper confidence bounds on the expected rewards of each action, UCB provides a theoretically grounded approach to balancing exploration and exploitation in various RL settings.\nHowever, the adaptive nature of data collection in RL violates the independent and identically distributed (i.i.d.) assumption underpinning many statistical methods. This sequential dependency poses significant challenges for analysis and inference in RL contexts. Despite these challenges, robust statistical inference remains crucial for RL. It enables quantification of uncertainty, validation of model performance, and provision of reliability guarantees\u2014essential factors as RL systems are deployed in increasingly critical applications.\nThis paper investigates a novel stability property in adaptive RL algorithms. We show that the UCB algorithm induces a form of stability in the sequential non-iid data which makes the downstream statistical inference process straightforward. For instance, under this stability property classical statistical estimators asymptotically normal."}, {"title": "1.1. Related Work.", "content": "In this section, we provide a brief survey existing literature on Multiarmed bandits, the UCB algorithm, and inference with data generated from sequential procedures."}, {"title": "1.1.1. Multiarmed bandits and the UCB algorithm.", "content": "The study of multi-armed bandits has a rich history, dating back to the seminal work of Thompson [27] and Robbins [24]. The Upper Confidence Algorithm (UCB) algorithm, introduced by Lai and Robbins [18] and Lai [17] and later refined by Auer [3], has become a cornerstone of the field due to its strong theoretical guarantees and empirical performance. The early (regret) analysis of the UCB algorithm is by Kathehakis and Robbins [15] and later refined by [2]. The UCB algorithm is based on an principle of optimism in the face of uncertainty, which turned out to be very influential in the area of reinforcement learning and sequential decision-making in general. We refer the reader to the book [20] and the references therein for a discussion and several applications of this principle."}, {"title": "1.1.2. Statistical inference with adaptively collected data.", "content": "While sequential decision-making procedures like the UCB are known to yield low regret, the sequential nature of the algorithm induces a dependence in the resulting data-set. Stated differently, the data collected by sequential data generating processes like UCB are not iid, thus it is not clear how can we perform downstream statistical inference using the adaptively collected data-set.\nThe challenge of performing statistical inference with adaptively collected data has been recognized in the statistical literature for at least 40 years. In the context of time series analysis, works by Dickey and Fuller [9], White [29, 30], and Lai and Wei [19] highlighted the breakdown of classical asymptotic theory when the data is generated from an regressive time series model. More recent work [7, 8, 16, 21, 22, 31] highlighted, via numerical simulations, that a similar phenomenon can occur in bandit problems. Zhang et al. [34] proved that in the case of batched bandits Z-estimators (sample arm means for multi-armed bandits) may not be asymptotic normal when the data is collected via popular bandit algorithms like UCB, Thompson sampling, and the $\\epsilon$-greedy algorithm.\nIn order to perform valid statistical inference, researchers have suggested two types of approaches. The first approach is non-asymptotic in nature, and is based on the concentration bounds on self-normalized Martingales; see the works [1, 25, 28] and the references therein. These works stem from the seminal works of [6, 23], and provide confidence intervals that are valid for any sample size and are usually more conservative (larger confidence intervals). The second approach is asymptotic in nature, and it exploits the inherent Martingale nature present in data via Martingale central limit theorem and debiasing [13, 33]. The confidence intervals obtained are valid asymptotically, and are often shorter than the ones obtained from concentration inequality based methods. See the works [5, 7, 8, 12, 16, 21, 22, 26, 31, 32, 34] and the references therein for an application of this technique."}, {"title": "1.2. Failure of classical estimators: A closer look.", "content": "While classical guarantees are not valid under when the data is collected via sequential methods, it is worthwhile to understand whether that is always the case. To motivate this discussion, we consider two algorithms for collecting data in a 2-armed bandit problem with arm means vector $(\\mu_1, \\mu_2) = (0.3, 0.3)^T$ with standard Gaussian error.\n(a) The arms are selected using an $\\epsilon$-greedy algorithm with $\\epsilon = 0.1$.\n(b) The arms are selected using the UCB algorithm, detailed in Algorithm 1."}, {"title": "1.3. Contributions.", "content": "The main contribution of this paper is to show that the such a stability property is satisfied for when we use the upper confidence bound (UCB) algorithm in a K-armed bandit problem. Specifically, we prove\n(a) The number of times an arm $a \\in [K]$ is pulled by the UCB algorithm converges in probability to a deterministic limit."}, {"title": "2. Problem set up.", "content": "In this paper, we consider a multiarmed bandit problem with $K$ arms. At each time step $t$, the player selects an arm $A_t \\in [K]$ and receives a random reward $X_t \\in \\mathbb{R}$ from the distribution $P_{A_t}$. Let $\\mu_a$ and $\\sigma^2_a$, respectively, denote the mean and variance of the distribution $P_a$, and $\\mu = (\\mu_1,..., \\mu_K)^T$ denote the vector of arm-means. The goal of bandit algorithms is to maximize the total expected reward or equivalently to minimize the regret, defined as the difference between the total expected reward obtained and the maximum possible total reward, over $T$ time steps:\n$\\begin{equation}\n  (Regret:) \\qquad R_T = T \\cdot \\mu^* - \\mathbb{E}\\bigg[\\sum_{t=1}^T X_t\\bigg]  \\qquad \\text{where } \\mu^* = \\max_{k \\in [K]} \\mu_k.\n\\end{equation}$\nIn this paper, we are interested in the behavior of the celebrated Upper Confidence Bound algorithm [3, 17, 18]. We detail the algorithm in Algorithm 1. Throughout we assume that $T$ is known to the algorithm."}, {"title": "Algorithm 1 UCB algorithm", "content": "1: Pull once each of the $K$ arms in the first $K$ iterations.\n2: for $t = K+1, ..., T - 1$ do\n3:  Compute the UCB boundary\n$\\begin{equation}\nUCB(a, t) := \\bar{X}_{a,t} + \\sqrt{\\frac{2 \\log T}{n_{a,t}}}.\n\\end{equation}\n4:  Choose arm $A_t$ given by\n$\\begin{equation}\nA_{t+1} = \\arg \\max_a UCB(a,t)\n\\end{equation}$\n5: end for\nIt is well-know that the sequential nature of the bandit algorithms makes the downstream inference process challenging [7, 8, 16, 21, 31, 34]. We would like to understand whether this is the case for the Upper Confidence Bound algorithm 1. Concretely, let $n_{a,T}$ and $\\bar{X}_{a,T} := (\\sum_{t=1}^T X_t \\cdot \\mathbb{1}_{A_t = a})/n_{a,T}$ denote respectively the sample size and sample mean of the rewards associated with arm $a$. We would like to identify conditions under which $\\bar{X}_{a,T}$ is asymptotically normal:\n$\\begin{equation}\n(\\text{Normality:}) \\qquad \\sqrt{n_{a,T}} \\cdot (\\bar{X}_{a,T} - \\mu_a) \\xrightarrow{d} N(0, \\sigma^2_a)\n\\end{equation}$"}, {"title": "2.1. Asymptotic normality and arm-pull stability.", "content": "We now connect the asymptotic normality property (4) of arm $a$ with that of certain stability property of $n_{a,T}$, the number of times arm $a$ is pulled in $T$ rounds. Note that $n_{a,T}$ is random.\nDEFINITION 2.1. An arm $a \\in [K]$ is said to be stable if there exists non-random scalar $n^*_a$ such that\n$\\begin{equation}\n(\\text{Stability:}) \\qquad \\frac{n_{a,T}}{n^*_a} \\xrightarrow{P} 1 \\quad \\text{with } n^*_a \\rightarrow \\infty\n\\end{equation}\nHere, the scalar $n^*_a$ is allowed to depend on $T, {\\mu_a}_{a \\in [K]}, {\\sigma^2_a}_{a \\in [K]}$.\nThis dentition is motivated from the seminal work of Lai and Wei [19], where the authors used a similar condition to prove asymptotic normality of the least square estimators in a stochastic regression model. We now show that if arm $a$ is stable, then the asymptotic normality (4) holds for arm $a$.\nLet $\\mathcal{F}_{t-1}$ denote the $\\sigma$-filed generated by ${X_1, ..., X_{t-1}}$. We assume that the arm reward distributions ${P_a}$ are 1-sub-Gaussian, and the number of arms $K$ is fixed for simplicity (we relax this assumption at a later section). By definition (3) we have that $A_t \\in \\mathcal{F}_{t-1}$, and given an arm $a$, the sum $\\frac{1}{\\sigma_a \\sqrt{n(T)}} \\sum_{t=1}^T \\mathbb{1}_{A_t = a} \\cdot (X_t - \\mu_a)$ is a sum of Martingale difference sequence. We have\n$\\begin{equation}\n\\sum_{t=1}^T  \\mathbb{V}ar\\bigg[\\frac{1}{\\sigma_a \\sqrt{n (T)}} \\mathbb{1}_{A_t = a} \\cdot (X_t - \\mu_a) | \\mathcal{F}_{t-1} \\bigg]  = \\frac{n_{a,T}}{n(T)} \\xrightarrow{P} 1.\n\\end{equation}$\nStated differently, the sum of the conditional variances of the Martingale difference array stabilizes. Additionally, using the assumption $n \\rightarrow \\infty$ and sub-Gaussian property of the reward distribution we have that the Lindeberg condition of Triangular array is satisfied. Applying the Martingale CLT for triangular array [10, 13], and using Slutsky's theorem we conclude\n$\\begin{equation}\n\\sqrt{n_{a,T}} \\cdot (\\bar{X}_{a,T} - \\mu_a) \\xrightarrow{d} N(0,1)\n\\sigma_a\n\\end{equation}$\nwhere $\\hat{\\sigma}^2_a$ is a consistent estimate of the variance $\\sigma^2_a$.\nPut simply, whenever the stability condition is satisfied, the arm means $\\bar{X}_{a,t}$ are asymptotic normal and we can construct asymptotically exact $1 - \\alpha$ confidence interval for the mean $\\mu_a$. In the next section, where we detail our main results, we discuss conditions under which the stability condition (5) holds for the Upper Confidence Algorithm 1."}, {"title": "3. Main results.", "content": "Here we provide our main result which shows stability of the UCB algorithm. Without loss of generality we assume that arm 1 is among optimal arms, and we do not assume that the optimal arm is unique. We also assume that\n$\\begin{equation}\n0 \\leq \\frac{\\Delta_a}{\\sqrt{2\\log T}} = o(1) \\text{ for all arms } a.\n\\end{equation}\n$\\begin{equation}\nP_a \\text{ is } \\lambda_a - \\text{sub-Gaussian for all arms a, and } |\\lambda_a| \\leq B \\text{ for some constant } B < \\infty.\n\\end{equation}$\nWe will explain the significance of these assumptions shortly. Our main result is the following:"}, {"title": "THEOREM 3.1.", "content": "Suppose we pull bandit arms using Algorithm 1. Let Assumptions (7a)-(7b) be in force and the number of arms K fixed. Then, for each arm $a \\in [K]$, the number of arm pulls $n_{a,t}$ satisfies\n$\\begin{equation}\n\\frac{n_{a,T}}{(1/\\sqrt{n^*} + \\sqrt{\\Delta_a^2 / 2\\log T})^{-2}} \\xrightarrow{P} 1 \\text{ in probability}.\n\\end{equation}$\nwhere $n^* = n^*(T, {\\Delta_a}_{a \\in [K]})$ is the unique solution to the following equation\n$\\begin{equation}\n\\sum_a \\bigg(\\frac{1}{\\sqrt{T/n^*} + \\sqrt{T\\Delta_a^2 / 2\\log T}}\\bigg)^{2} = 1\n\\end{equation}$\nHere, $\\Delta_a = \\mu_1 - \\mu_a$ and without loss of generality we assumed that arm 1 is among the optimal arms.\nA few comments are on the order. Condition (7a) ensures that arm $a$ is pulled infinitely often. Concretely, it ensures that $n_{a,T} \\rightarrow \\infty$ in probability. We refer the reader to the seminal works [17, 18] for a similar condition.\nIt is worthwhile to understand the consequence of Theorem 3.1 in some special cases. Suppose $\\Delta_a = O(\\sqrt{\\frac{\\log T}{T}})$ for some arm $a$. Then (8) ensures that\n$\\begin{equation}\n\\frac{n_{a,T}}{n^*} \\xrightarrow{P} 1 \\text{ in probability whenever } \\Delta_a = O\\bigg(\\sqrt{\\frac{\\log T}{T}}\\bigg)\n\\end{equation}$\nPut simply, near-optimal arms are pulled equally often asymptotically.\nComparison to prior work. It is interesting to compare Theorem 3.1 with an earlier work of [14]. Parts (II) and (III) of Theorem 1 in [14] provide the limiting distribution of the number of arm pulls when $\\Delta = \\mu_1 - \\mu_2 \\leq \\frac{1}{T^{\\theta}} \\sqrt{\\log T}$ for some $\\theta > 0$. Theorem 3.1 recovers these results when we substitute $K = 2$. When $\\frac{\\Delta}{\\sqrt{\\frac{\\log T}{T}}} \\rightarrow \\infty$, Part (I) of Theorem 1 in [14] provides the limiting distribution of $n_{1,T}$. On the contrary, Theorem 3.1 characterizes the behavior of both $n_{1,T}$ and $n_{2,T}$. Finally, Theorem 3.1 provides the limiting distribution of $K$ arms for any choices of {$\\Delta_a$}, thereby resolving the problem that was left open in [14]."}, {"title": "3.1. Statistical inference:.", "content": "Theorem 3.1 allows us to provide an asymptotically exact $1 - \\alpha$ confidence interval. Let the $\\hat{\\sigma}^2_a$ denote the variance of the rewards associated with arm $a$. Given a fixed direction $u = (u_1, ..., u_K)^T \\in \\mathbb{R}^K$, define\n$\\begin{equation}\nC_{u, \\alpha} = \\bigg{\\langle u, \\bar{X}_T \\rangle \\pm z_{1-\\alpha/2} \\sqrt{\\sum_{a=1}^K  \\frac{\\hat{\\sigma}^2_a u^2_a}{n_{a,T}}} \\bigg}\n\\end{equation}$\nwhere $\\hat{\\sigma}_a$ is any consistent estimate of $\\sigma_a$, and $z_{1-\\alpha/2}$ is the $1 - \\alpha/2$ quantile of the standard normal distribution, and $\\bar{X}_T = (\\bar{X}_{1,T}, ..., \\bar{X}_{K,T})^T$.\nTHEOREM 3.2. Suppose the conditions of Theorem 3.1 are in force. Then, given a fixed direction $u \\in \\mathbb{R}^K$ and $\\alpha \\in (0,1)$, the confidence interval $C_{u,\\alpha}$ defined in (10) satisfies\n$\\begin{equation}\n\\lim_{T \\rightarrow \\infty}  P(C_{u,\\alpha} \\ni u^T \\mu) = 1 - \\alpha.\n\\end{equation}"}, {"title": "4. Can we let the number of arms grow?.", "content": "In this section, we study the stability properties of arms when the number of arms grows with the number of arm pulls $T$. Unfortunately, some of the proof techniques used in the previous section does not apply when the number of arms $K = K(T)$ is allowed to grow with the number of arm pulls $T$. The fist assumption that we need is\n$\\begin{equation}\n\\frac{\\log K}{\\log T} \\rightarrow 0 \\text{ as } T \\rightarrow \\infty.\n\\end{equation}$\nIn other words, K grows slower than any positive power of T. This assumption ensures that a finite sample version of the law of iterated logarithm holds simultaneously for all K arms. We start with the definition of near-optimal arms. Again, without loss of generality we assume that arm 1 is among the optimal arms.\nNear optimal arms:. Given a constant $B > 0$, the set of B-near optimal arms are defined as:\n$\\begin{equation}\nS_B := \\bigg{a \\in [K] :  \\frac{\\Delta^2_a}{\\sqrt{2\\log T}} < B\\bigg}\n\\end{equation}$\nWe use $|S_B|$ to denote the cardinality of $S_B$. Our next theorem requires that there exists $B > 0$ such that\n$\\begin{equation}\n\\frac{|S_B|}{K} > \\alpha > 0 \\text{ for all } T \\geq T_0\n\\end{equation}$\nwhere $T_0$ fixed. Here, the number of arms $K = K(T)$ is allowed to grow with $T$.\nTHEOREM 4.1. Let Assumptions (12), (7a) and (7b) are in force and the condition (14) holds for some $B > 0$. Then for all arms $a$:\n$\\begin{equation}\n\\frac{n_{a,T}}{(1/\\sqrt{n^*} + \\Delta_a/\\sqrt{2\\log T})^{-2}} \\xrightarrow{P} 1 \\text{ in probability}.\n\\end{equation}$\nwhere $n^*$ is the unique solution of (9).\nWe provide the proof of Theorem 4.1 in Section 5.3. A few comments on the Theorem 4.1 are in order."}, {"title": "5. Proofs of Theorems.", "content": "In this section, we provide proofs of Theorems 3.1, 3.2 and 4.1."}, {"title": "5.1. Proof of Theorem 3.1.", "content": "The bulk of the argument is based on the following concentration bound for sub-Gaussian random variables:\nLEMMA 5.1. Let $X_1, X_2, . . .$ be i.i.d. $\\lambda_a$-sub-Gaussian random variable with zero mean. Then\n$\\begin{equation}\nP\\bigg{\\bigg{|}\\sum_{i=1}^t X_i \\bigg{|} \\geq \\lambda_t \\sqrt{9 \\cdot t \\cdot \\log \\frac{(log 4t)^2}{\\delta}}\\bigg} \\leq 2\\delta\n\\end{equation}$\nWe prove this lemma in Section B.1. Let $E_T$ is the following event:\n$\\begin{equation}\nE_T = \\bigg{\\{\\big{|}X_{a,t} - \\mu_a\\big{|} \\leq \\lambda_a \\frac{\\sqrt{T \\cdot \\log\\log T + 3 \\log K}}{\\sqrt{T}} \\text{ for all } 1 \\leq t \\leq T, \\text{ and } a \\in [K]\\bigg\\},\n\\end{equation}$\nAn immediate consequence of Lemma 5.1 is that\n$\\begin{equation}\nP(E_T) \\geq 1 - \\frac{6}{\\log T}\n\\end{equation}$"}, {"title": "6. Discussion.", "content": "This paper establishes a novel stability property of the upper confidence bound (UCB) algorithm in the context of multi-armed bandit problems. This property makes the downstream statistical inference straightforward; for instance, classical statistical estimators remain asymptotically normal even though the data collected is not iid. Moreover, we show that when the number of arms K are large, and potentially allowed to grow with the number of total arm pulls. Finally, our result imply that the UCB algorithm is fair. Concretely, if two arm means are close, the UCB algorithm will select both arms equal number of times in the long run (asymptotically).\nWhile these findings represent a significant advance, several open questions remain. Future research could explore the extension of these results to heavy-tailed distributions, investigate stability properties of other popular bandit algorithms, and examine potential trade-offs between stability and regret minimization. Put differently, it would be interesting to see"}]}