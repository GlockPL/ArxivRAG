{"title": "GRAPH-AWARE ISOMORPHIC ATTENTION FOR ADAPTIVE DYNAMICS IN TRANSFORMERS", "authors": ["Markus J. Buehler"], "abstract": "We present an approach to modifying Transformer architectures by integrating graph-aware relational reasoning into the attention mechanism, merging concepts from graph neural networks and language modeling. Building on the inherent connection between attention and graph theory, we reformulate the Transformer's attention mechanism as a graph operation and propose Graph-Aware Isomorphic Attention. This method leverages advanced graph modeling strategies, including Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA), to enrich the representation of relational structures. Our approach improves the model's ability to capture complex dependencies and generalize across tasks, as evidenced by a reduced generalization gap and improved learning performance. Additionally, we expand the concept of graph-aware attention to introduce Sparse GIN-Attention, a fine-tuning approach that employs sparse GINs. By interpreting attention matrices as sparse adjacency graphs, this technique enhances the adaptability of pre-trained foundational models with minimal computational overhead, endowing them with graph-aware capabilities. Across our experiments, our results demonstrate that graph-aware attention mechanisms outperform traditional attention in both training efficiency and validation performance. Furthermore, Sparse GIN-Attention fine-tuning achieves improved training dynamics and better generalization compared to alternative methods like low-rank adaption (LoRA). These insights not only bridge graph theory and Transformer architectures but also uncover latent graph-like structures within traditional attention mechanisms, offering a new lens through which Transformers can be understood and optimized. By evolving Transformers as hierarchical GIN models, we reveal their implicit capacity for graph-level relational reasoning. This perspective suggests profound implications for foundational model development, enabling the design of architectures that dynamically adapt to both local and global dependencies. Applications in bioinformatics, materials science, language modeling, and beyond could benefit from this synthesis of relational and sequential data modeling, setting the stage for interpretable and generalizable modeling strategies.", "sections": [{"title": "1 Introduction", "content": "The evolution of attention mechanisms in neural networks has significantly influenced the field of artificial intelligence and machine learning, from early work [1, 2, 3, 4, 5] to more recent developments [6, 7, 8, 9, 10, 11, 12, 13, 14]. Originally vaguely inspired by cognitive processes in humans, attention mechanisms have become integral to modern neural network architectures like the Transformer [5]. These mechanisms dynamically allocate computational resources to the most relevant parts of input data, optimizing the processing of information and reducing computational redundancy.\nDecode-only Transformers, [6] designed specifically for autoregressive tasks, have emerged as a highly efficient subclass of the Transformer architecture, focusing on multimodal tasks like text generation, image modeling, audio modeling, language modeling, and sequential data prediction, among many other modalities (Figure 1). These architectures utilize an embedding layer to convert discrete tokens (which can represent diverse types of data, like text, chemical structures, images/pixels, symbols, and others) into dense vector representations, enabling the model to process flexible inputs. The core of the architecture is the self-attention mechanism, which operates causally to ensure that each token attends only to its past context, maintaining the autoregressive property essential for tasks like generative AI. Multi-head self-attention enhances the model's ability to capture diverse relationships between tokens by allowing parallel attention computations.\nIn addition to attention, decode-only Transformers such as those used in LLama foundation models [10] integrate feedforward layers (FF), usually implemented via a multi-layer perceptron (MLP), following the attention mechanism. These layers expand and transform the attention outputs, introducing non-linearity and enabling the model to learn complex patterns. The architecture also employs several residual connections and layer normalization, ensuring stability during training and facilitating the flow of gradients. Positional encodings are incorporated to inject sequence order information into the embeddings [5, 15], addressing the lack of inherent order in self-attention computations. Finally, the autoregressive nature of these models ensures that outputs are generated token-by-token, leveraging causal masking to prevent access to future tokens during training and inference. Together, these components create a robust and scalable framework for generative tasks.\nGraph Neural Networks (GNNs) [16, 17, 18, 19] represent another influential class of models that have effectively incorporated attention mechanisms, primarily to enhance their capacity for learning on graph-structured data. Attention in GNNs has been extensively utilized to assign varying levels of importance to nodes, edges, or subgraphs, enabling models to focus on the most relevant components for a given task [18, 19]. However, these approaches are predominantly designed for known or static graph structures, where the relationships and topology remain fixed throughout the reasoning process. These methods often lack the flexibility to dynamically adapt and reason over evolving relationships or integrate seamlessly into general-purpose Transformer architectures. Our work attempts to explore possibilities at this intersection."}, {"title": "1.1 Invoking graph theory and category theory", "content": "Category theory [20, 21], with its abstract structures and relationships, provides a powerful lens through which we can understand the generalization capabilities of Transformers. By interpreting Transformer models as functors that map input representations to output representations, we can appreciate how these models capture abstract relationships and generalization of concepts in disparate fields. Some recent work has demonstrated universal feature activation in large models, providing evidence for such features in existing pre-trained models [22, 23, 24, 25]. In particular, functors preserve structural consistency while allowing transformations across different categories, akin to how Transformers maintain contextual relevance while adapting to varied tasks and domains. This perspective sheds light on the inherent modularity of Transformers, where layers and substructures can be seen as morphisms within a category, facilitating compositional reasoning and hierarchical abstraction. Here, the attention mechanism itself can be viewed as a mapping that defines and strengthens relationships within a category, dynamically adjusting based on contextual importance, which can be interpreted as a graph as shown in Figure 2. The integration of such graph-based modeling enhances this mapping by incorporating relational structures explicitly, allowing for richer representations that align with categorical transformations and hence better generalizability and predictive power, especially for scientific applications. In the context of the attention mechanism depicted in Figure 1, the interpretation of attention as graph-forming operator produces a causal adjacency matrix $A \\in \\mathbb{R}^{N_{seq} \\times N_{seq}}$ at each layer in the model. The core graph operation in the attention mechanism is, however, linear in nature.\nThe importance of isomorphic mappings as a framework for scientific generalization can be exemplified in an example of a study of biomaterials, such as spider silk, and its relation to music, drawing parallels in hierarchical structure and transformation across domains [26, 27, 28, 29, 30]. Spider silk's remarkable mechanical properties arise from its hierarchical organization, spanning molecular arrangements to macroscopic fibers. In category-theoretic terms, the molecular and macroscopic structures can be viewed as objects within distinct categories, with the Transformer acting as a functor that maps molecular-level features (e.g., amino acid sequences) to macroscopic properties (e.g., tensile strength or elasticity) while preserving key relationships. Similarly, music composition involves hierarchical structures, where sequences of notes form motifs, motifs form phrases, and phrases form complete compositions. Here, a Transformer functor can map abstract patterns in one domain, such as the hierarchical structures of biomaterials, to analogous patterns in another, such as the rhythmic or melodic structures in music. The attention mechanism, by dynamically"}, {"title": "1.2 Outline", "content": "We explore the theoretical foundations of attention mechanisms and propose a series of enhancements to the Transformer architecture. We interpret the Transformer's attention mechanism as a graph generating operator and enhance its structure by introducing more complex graph neural networks such as the Graph Isomorphic Neural network (GIN). Additionally, we develop a novel fine-tuning technique that can serve as a replacement or complement to LoRA with graph neural networks, interpreting the adjacency matrix predicted by conventional attention as an input to a graph neural network. This approach enhances the model's ability to learn from both sequential data and structured graph data. Through these enhancements, our aim is to provide a robust framework for leveraging graph structures in transformer-based models, offering valuable insights into enhancing attention mechanisms."}, {"title": "2 Results and Discussion", "content": null}, {"title": "2.1 Theoretical foundations: Graph-aware attention", "content": "We first review details of the conventional attention mechanism and then describe our revised approach to introduce graph-aware or GNN-Attention in various flavors."}, {"title": "2.1.1 Interpreting Attention as a linear GNN", "content": "The attention mechanism can be interpreted as a form of message passing on a fully connected graph, where each token in the input sequence corresponds to a node, and the learned attention scores define the weighted edges. Mathematically, the adjacency matrix $A \\in \\mathbb{R}^{N_{seq} \\times N_{seq}}$ is computed using scaled dot-product attention:\n$A_{ij} = \\text{softmax}\\left(\\frac{Q_i K_j^T}{\\sqrt{d_k}}\\right),$"}, {"title": null, "content": "where $Q, K \\in \\mathbb{R}^{N_{seq} \\times d_k}$ are the query and key matrices, derived from the input embeddings $X$ as $Q = XW_Q$ and $K = XW_K$, with $W_Q, W_K \\in \\mathbb{R}^{d_{model} \\times d_k}$ being learnable projection matrices. The softmax ensures that the rows of $A$ sum to 1, effectively normalizing the attention scores into a probability distribution. The value matrix $V \\in \\mathbb{R}^{N_{seq} \\times d_v}$ is then aggregated using $A$:\n$Y = AV,$"}, {"title": null, "content": "where $Y \\in \\mathbb{R}^{N_{seq} \\times d_v}$ represents the output of the attention layer. This aggregation via $V$ can be viewed as a graph neural network operation, where $A_{ij}$ serves as the weight of the edge from node $i$ to node $j$, and $V_j$ is the feature vector of node $j$. The output $Y_i$ for each node $i$ is a weighted sum of its neighbors' features, aligned with the aggregation strategy used in graph neural networks.\nIn the context of multi-head attention, multiple attention heads independently compute adjacency matrices $A_h$ and corresponding outputs $Y_h = A_hV_h$, where $h \\in \\{1, ..., H\\}$ denotes the attention head and $V_h = XW_V^h$. That is, each attention head computes an independent adjacency matrix $A_h$ and performs GIN aggregation. The resulting embeddings are concatenated across heads and linearly transformed to produce the final output, allowing each head to capture diverse relational patterns while maintaining computational efficiency.\nThe outputs from all heads are then concatenated and linearly transformed over all heads:"}, {"title": null, "content": "$Y_{\\text{multi-head}} = \\text{Concat}\\left(Y_i\\right) W_O,$\nwith $W_O \\in \\mathbb{R}^{(H \\cdot d_v) \\times d_{model}}$ as the output projection matrix. The per-head mechanism allows the model to attend to different representations of the input data simultaneously. Specifically, this interpretation of attention as a graph operation highlights how the learned adjacency matrix $A$ dynamically adjusts to capture task-specific relationships. By incorporating multi-head attention, the mechanism effectively performs diverse message-passing computations, enriching the representation of node (token) features."}, {"title": "2.1.2 Expressive graph attention through a Graph Isomorphism Network (GIN): GIN-Attention", "content": "We now build on this concept and propose an enhanced attention mechanism that expands the traditional scaled dot-product attention by replacing the linear aggregation of value vectors $V$ with a Graph Isomorphism Network (GIN)-based process. Additionally, this mechanism introduces a sharpening parameter to the attention computation and allows for the use of alternative activation functions beyond Softmax. The detailed steps of the algorithm are as follows.\nIn its most basic form, the adjacency matrix $A$ is computed using a modified Softmax function, which incorporates a learnable sharpening parameter $\\alpha$ (we can view the sharpening parameter as a dial on a microscope to focus in on key relationships):\n$A_{ij} = \\text{Softmax}\\left(\\alpha \\frac{Q_i K_j^T}{\\sqrt{d_k}}\\right),$"}, {"title": null, "content": "where:\n$Q = XW_Q, \\quad K = XW_K,$"}, {"title": null, "content": "and $W_Q, W_K \\in \\mathbb{R}^{d_{model} \\times d_k}$ are learnable projection matrices. The sharpening parameter $\\alpha$ dynamically adjusts the focus of the attention distribution. A higher $\\alpha$ leads to sharper attention, while a lower $\\alpha$ produces a smoother distribution. Alternatively, the Softmax operation can be replaced with other activations, such as element-wise sigmoids:\n$\\text{Sigmoid: } A_{ij} = \\sigma \\left(\\beta \\cdot (Q_i \\cdot K_j - \\tau)\\right),$"}, {"title": null, "content": "or the Sharp Softplus function:\n$\\text{Sharp Softplus: } A_{ij} = \\frac{\\log \\left(1 + e^{-\\beta \\cdot (Q_i \\cdot K_j - \\tau)}\\right)}{\\beta},$"}, {"title": null, "content": "where, $\\beta$ controls the sharpness, and $\\tau$ represents a learnable threshold.\nThe adjacency matrix $A$ can be further refined based on various modes, such as applying thresholding, sparsification (e.g., top-k), or additional activations. These transformations adapt $A$ to better represent the graph structure before aggregation. For instance:\n$A' = \\text{ReLU}(A - \\tau), \\quad A' = \\text{top-k}(A, k).$"}, {"title": null, "content": "Causality is enforced by applying a lower triangular mask to ensure that each node only attends to itself and its predecessors.\nOnce the adjacency matrix $A'$ is computed, the GIN aggregation is applied. The updated node embeddings $X'$ are computed as:\n$X' = \\text{MLP}(\\epsilon \\cdot X + A'X),$"}, {"title": null, "content": "where $\\epsilon$ is a learnable parameter that scales the contribution of the current node embeddings $X$, and $A'X$ aggregates information from the neighboring nodes.\nThe original GIN formulation [31] updates node embeddings as:\n$X' = \\text{MLP}\\left((1 + \\epsilon) \\cdot X + AX\\right),$"}, {"title": null, "content": "where $1 \\cdot X$ explicitly includes the self-loop contribution, and $\\epsilon$ is a learnable parameter controlling its importance. Unlike traditional GIN formulations, the term $1 \\cdot X$ is omitted here because the skip connection around the attention block inherently captures this self-loop (see, e.g. Figure 1). The MLP consists of two linear transformations, a non-linear activation (e.g., SiLU), and optional dropout and normalization layers (details below).\nIn the multi-head setting used here, each head h computes its own adjacency matrix $A_h$ and performs GIN-based aggregation independently:\n$X_h' = \\text{MLP}_h \\left(\\epsilon_h \\cdot X_h + A_h'X_h\\right).$"}, {"title": null, "content": "The outputs from all heads are concatenated and ultimately, linearly transformed:\n$Y = \\text{Concat}(X_1', X_2', ..., X_H')W_o,$"}, {"title": null, "content": "where $W_o \\in \\mathbb{R}^{(H \\cdot d_k) \\times d_{model}}$ is an optional learnable output projection matrix (if not used, the aggregated output is used without further transformation). This design allows each head to focus on different relational patterns within the input, enhancing the overall expressiveness of the attention mechanism.\nThe GIN-aggregated outputs $X'$ are combined with the residual connection and normalized using RMSNorm.\nThe overall framework is visualized schematically in Figure 3. It systematically integrates graph-based modeling into the attention mechanism, enabling the Transformer to capture richer relational structures and achieve better generalization.\nDetails on GIN layer design The Graph Isomorphism Network (GIN) in this work is constructed to enhance relational modeling capabilities. The GIN aggregates node features $X$ by applying the adjacency matrix $A'$ (defined for each head, but this detail omitted her since we introduce the general GIN layer architecture) to perform a summation aggregation from neighboring nodes and updates the embeddings as:\n$X' = \\text{MLP}(\\epsilon \\cdot X + A'X),$"}, {"title": null, "content": "where $\\epsilon$ is a learnable parameter controlling the self-loop contribution. The MLP is designed with two linear layers and intermediate transformations, implemented as:\n$\\text{MLP}(z) = \\text{Linear}_2(\\text{Dropout}(\\text{SiLU}(\\text{Norm}(\\text{Linear}_1(z))))),$"}, {"title": null, "content": "where:\n$\\text{Linear}_1 : \\mathbb{R}^{d_{in}} \\rightarrow \\mathbb{R}^{d_h}, \\quad \\text{Linear}_2 : \\mathbb{R}^{d_h} \\rightarrow \\mathbb{R}^{d_{in}},$\nand $d_h = \\gamma \\cdot d_{in}$, with $\\gamma$ being a configurable multiplier that determines the intermediate hidden dimension of the MLP layer. This multiplier allows for control over the capacity of the MLP, enabling adjustments to the model's expressiveness based on task complexity. Norm refers to RMSNorm, which stabilizes the feature representations, while SiLU provides non-linearity. Dropout is applied after activation to regularize the model and prevent overfitting.\nWe note that one could, in principle, feed an external adjacency matrix to be added to the values identified by the attention mechanisms. While this is not explored here, it could be subject of further research, along with trainable methods to construct adjacency matrix that could replace or complement the attention mechanism focused on in this work."}, {"title": "2.1.3 Extension to Principal Neighborhood Aggregation (PNA)-Attention", "content": "The attention mechanism can be extended by replacing the conventional value aggregation with more complex neighborhood aggregation strategies, inspired by Principal Neighborhood Aggregation (PNA) [32]. This approach introduces multiple aggregators\u2014such as sum, mean, max, and variance\u2014to capture diverse relational patterns within the input graph structure.\nAs before, the adjacency matrix $A$ is first computed using scaled dot-product attention, with the option to refine it via thresholding, sparsification, or alternative activation functions. Once the adjacency matrix $A$ is processed, the aggregators compute the characteristics of the neighborhood of each node. For example:\n$\\text{Sum Aggregator: sum_agg} = AX,$\n$\\text{Mean Aggregator: mean_agg} = \\frac{AX}{\\text{deg}},$"}, {"title": null, "content": "where $\\text{deg} = A1$ is the degree of each node, and 1 is the all-ones vector. Additional aggregators include:\n$\\text{Max Aggregator: max_agg} = \\underset{j \\in \\text{neighbors} (i)}{\\text{max}} \\, X_j,$\n$\\text{Variance Aggregator: var_agg} = \\text{mean_of_squares} - (\\text{mean_agg})^2.$\nThe outputs of these aggregators are concatenated for each node to form a feature vector that combines information across multiple aggregation modes:\n$\\text{combined} = \\text{Concat}(\\text{sum_agg, mean_agg, max_agg, var_agg}).$\nThis combined feature vector is then passed through a Multi-Layer Perceptron (MLP) to produce the updated node embeddings. The MLP is defined as:\n$\\text{MLP}(z) = \\text{Linear}_2(\\text{SiLU}(\\text{Linear}_1(z))),$"}, {"title": null, "content": "where Linear\u2081 projects the combined features to a hidden dimension, and Linear2 maps it back to the original dimension.\nAs in the GIN-Attention case, in the multi-head setting, each head h computes its own adjacency matrix $A_h$ and applies the aggregation independently. The outputs from all heads are concatenated and linearly transformed:\n$Y = \\text{Concat}(X_1', X_2', ..., X_H')W_o,$"}, {"title": null, "content": "where $W_o$ is the learnable output projection matrix. This approach enables each head to learn distinct relational patterns, enriching the overall representation.\nTo ensure causality, a lower triangular mask is applied to $A$, ensuring that each node only attends to itself and its predecessors. Additionally, the mechanism allows for blending the original attention output with the aggregated output using a learnable residual blending ratio:\n$Y = (1 - \\lambda) \\cdot Y_{\\text{original}} + \\lambda \\cdot Y_{\\text{aggregated}},$"}, {"title": null, "content": "where $\\lambda \\in [0, 1]$ is a learnable blending parameter.\nThis extended framework systematically integrates multiple neighborhood aggregation strategies, enhancing the flexibility and expressiveness of the attention mechanism while maintaining computational efficiency.\nAs before, the adjacency matrix $A$ can undergo flexible processing to adapt to the desired relational structure. If conventional softmax normalization is not applied, alternatives such as ReLU can be used directly on the attention scores, enabling sparsity while preserving non-negative weights. Further, the algorithm allows for advanced transformations, including thresholding, sharp softplus activation, sigmoid-based scaling, or top-k sparsification. These options allow that the adjacency matrix can dynamically adapt to various tasks while retaining causality through the application of a lower triangular mask.\nAs before we could in principle feed an external adjacency matrix to be added to the values identified by the attention mechanisms."}, {"title": "2.2 Experimental results: Graph-aware attention", "content": "We present several case studies to test the performance of GIN-Attention and PNA-Attention versus standard linear attention. For all tests we use a consistent model size of 25 M parameter, the same training data, with the same training hyperparameters."}, {"title": "2.3 Using sparse graph-aware attention as a fine-tuning strategy", "content": "The earlier discussion showed how expanding linear attention to include more expressive graph operations such as GIN-Attention can increase performance. However, training Transformer models from scratch can be a time-consuming and expensive process. Here we explore how the general concept can be used to develop a fine-tuning strategy that allows us to use a pre-trained transformer model and adapt it to a particular set of training data.\nIn this method, a Graph Neural Network (GNN) is integrated into the Transformer architecture immediately after the standard self-attention mechanism, enhancing the model's ability to capture relational structures within the data, but geared towards a fine-tuning approach similar to LoRA that adds shallow layers to a model [33, 34, 35]. The adjacency matrix $A$ is derived from the attention weights computed during standard self-attention step. However, it is refined based on specific configurations, such as thresholding to achieve sparsification to yield efficient computational strategies.\nWe experimented with a variety of aggregation strategies and found summation to work well overall, while being easy to implement. First, we sum all per-head adjacency matrices $A_h$ while clamping values at one, providing an aggregated view of all relationships identified by the attention mechanism. This transforms H individual adjacency matrices to a single adjacency matrix $A$. A sharpening function is applied:\n$A' = \\text{sigmoid} \\left(\\alpha(A - \\tau)\\right),$"}, {"title": null, "content": "where a controls the sharpness of the sigmoid and $\\tau$ is a threshold that allows for further fine-tuning of the mapping function.\nAfterwards, discrete thresholding is applied:\n$A_{ij}' = \\begin{cases} A_{ij}, & \\text{if } A_{ij}' > \\epsilon \\\\ 0, & \\text{otherwise} \\end{cases}$"}, {"title": null, "content": "Alternatively, other sparsification strategies like top-k selection may be used. To ensure causality, a lower triangular mask is applied, restricting each node to attend only to itself and its predecessors.\nThis approach systematically combines the expressiveness of the GNN with the contextual representations learned by the attention mechanism.\nThe refined adjacency matrix $A'$ is passed along with the transformer hidden states $X$ into the GNN. The updated embeddings are computed as:\n$X' = X + \\text{GNN}(X, A') \\cdot \\lambda, $"}, {"title": null, "content": "where $\\lambda$ is a learnable scaling parameter that modulates the contribution of the GNN output to the final embeddings. This scale ensures the GNN's impact is dynamically adjusted during training, allowing for effective blending of relational features with the transformer hidden states. We typically chose initial values for $\\lambda$ as a linear curve over model layers from 0 to a finite value, e.g. 0.5 in the last layer.\nAfter the GNN aggregation, the output $X'$ is combined with the residual from the input hidden states. Additionally, a normalization step is applied to ensure numerical stability and consistent scaling of the features:\n$X'' = \\text{Norm}(X'),$"}, {"title": null, "content": "where Norm refers to RMSNorm or another normalization technique.\nThe final output of the layer is the normalized, GNN-augmented hidden states:\n$Y = X''.$"}, {"title": null, "content": "By integrating the GIN immediately after the self-attention step, this approach refines the relational representations learned by the attention mechanism before the signal is fed to the feed-forward (FF) MLP block. The explicit inclusion of $\\lambda$ ensures that the GNN's impact can be adaptively controlled, providing a robust and flexible mechanism for capturing complex dependencies within the input data.\nGIN Layer Implementation The Sparse GIN framework implements a Graph Isomorphism Network layer to refine node representations using causal graph structures. Consistent with the rerence approach we use sum aggregation, which is both efficient and expressive in capturing neighbor information. The GIN layer updates node embeddings $X'$ by combining the self-loop features $X$ and aggregated neighbor features using the adjacency matrix $A$:\n$X' = \\epsilon \\cdot X + \\text{Aggregate}(X_{\\text{neighbors}}),$"}, {"title": null, "content": "where: e is a learnable parameter that scales the self-loop contribution and $\\text{Aggregate}(X_{\\text{neighbors}}) = AX$, using the sum aggregation strategy (as before, the identity term is removed due to the residual connection that already captures it).\nTo enhance the representation power, the aggregated features are processed through an MLP:\n$\\text{MLP}(z) = \\text{Linear}_2(\\text{Activation}(\\text{Norm}(\\text{Linear}_1(z)))),$"}, {"title": null, "content": "where Linear\u2081 and Linear2 are fully connected layers, with Linear\u2081 projecting the input to a hidden dimension controlled by a multiplier $\\gamma$, and Linear2 mapping it back to the output dimension and Norm applies normalization (e.g., LayerNorm), and a ReLU activation function introduces non-linearity.\nWe enforce causality by applying a mask to edges such that only edges (i, j) where $i < j$ within the same graph are retained:\n$\\text{Causal Mask: } \\text{edge_index}_{\\text{filtered}} = \\{(i, j) | i < j, \\text{ and batch}[i] = \\text{batch}[j]\\}.$"}, {"title": null, "content": "This ensures that nodes only receive information from preceding nodes, maintaining a directed acyclic graph structure.\nThe final output of the GIN layer is computed by combining the transformed node features with a residual connection:\n$X_{\\text{residual}} = \\text{Linear}(X),$\n$X_{\\text{final}} = \\text{MLP}(X') + X_{\\text{residual}}.$\nThis residual connection ensures smooth gradient flow and stabilizes training, particularly when the input and output dimensions differ.\nThe use of MLPs with normalization and activation enhances the representation learning capabilities, while residual con-nections ensure robust integration of the GIN layer into the overall architecture. By simplifying the aggregation strategy and emphasizing causality, the approach efficiently captures relational dependencies while adhering to computational constraints.\nWhen used as a fine-tuning strategy, only parameters in this newly added GIN are trainable and the rest of the model is frozen per its pre-trained weights."}, {"title": "3 Conclusion", "content": "We explored theoretical and practical adaptations of the Transformer architecture through the lens of graph-based models and category theory. By interpreting and formulating Transformers as Graph Isomorphism Networks (GIN), we established a novel perspective on their attention mechanisms, demonstrating structural equivalence between the aggregation processes in graph neural networks and the focus mechanisms in attention layers. This equivalence provides a foundation for extending Transformers with explicit graph reasoning capabilities that manifests itself in better generalization performance.\nWe further explored the use of Principal Neighborhood Aggregation (PNA) method, which augments attention by integrating multiple aggregation techniques per attention head, testing more diverse representations. While performance did not exceed that of the GIN model, this enhancement, coupled with the flexibility of graph neural networks, could offer an alternative framework for handling structured and relational data in domains such as bioinformatics, materials science, and other areas.\nFrom a theoretical standpoint, we incorporated insights from category theory to frame Transformers as functors that map input to output representations while preserving structural relationships. This abstraction highlights their inherent capacity for compositional reasoning and contextual adaptation, bridging concepts from information theory, statistical physics, and graph theory. Additionally, the categorical perspective inspires future directions, such as exploring natural transformations for designing more robust and adaptive attention mechanisms that incorporate powerful graph modeling techniques like GINs or PNAs.\nOur proposed sparse GIN fine-tuning approach can replace LoRA [33] with graph neural networks, using suitable processed sparse adjacency matrices derived from attention mechanisms as inputs to graph neural networks. This method significantly enhances the model's ability to learn from mathematical training data, demonstrating improved performance and flexibility.\nThe findings and methods presented in this work not only deepen the understanding of attention mechanisms but also pave the way for innovative applications of Transformers integrated with graph-based reasoning. By synthesizing theoretical rigor with practical innovation, this work contributes to the ongoing evolution of neural network architectures, advancing their potential to address increasingly complex challenges in science and technology."}, {"title": "3.1 Transformers are secretly GIN-like graph reasoning models", "content": "The discussion in this section explains how a standard transformer architecture can be viewed as a GIN-type graph neural network. We posit that vanilla Transformer architectures (as shown in Figure 1A) are, in fact, GIN-like graph models. It thereby invokes our modeling strategy as a \u201cGIN-within-GIN\u201d mechanism, which nests per-head GINs inside a global GIN, yielding a hierarchical model for graph-structured data.\nA transformer layer consists of multi-head self-attention. For each head h = 1, 2, ..., H, we compute:\n$A_{h} = \\text{softmax}\\left(\\frac{Q_{h} K_{h}}{\\sqrt{d_{k}}}\\right), \\quad O_{h} = A_{h} V_{h}$"}, {"title": null, "content": "where\n\u2022 $Q_h, K_h, V_h \\in \\mathbb{R}^{T \\times d_k}$ are the query, key, and value matrices for head h;\n\u2022 T is the (sequence or node) dimension;\n\u2022 $d_k$ is the per-head feature dimension.\nThese quantities are computed as follows:\n$Q_{h} = HW_Q^{h}, \\quad K_{h} = HW_K^{h}, \\quad V_{h} = HW_V^{h}$"}, {"title": null, "content": "where:\n\u2022 $H \\in \\mathbb{R}^{T \\times d}$: The input hidden states, where T is the sequence length (or number of nodes) and d is the hidden dimension.\n\u2022 $W_Q^{h}, W_K^{h}, W_V^{h} \\in \\mathbb{R}^{d \\times d_k}$: Learnable weight matrices specific to head h, projecting the input hidden states into the query, key, and value spaces, respectively.\n\u2022 $Q_h, K_h, V_h \\in \\mathbb{R}^{T \\times d_k}$: The query, key, and value matrices for head h, where $d_k = d/H$ is the feature dimension for each head.\nThese projections transform the input into three distinct spaces:\n\u2022 The query matrix $Q_h$ captures the representation of the tokens in terms of the features they query.\n\u2022 The key matrix $K_h$ represents the features available for attention.\n\u2022 The value matrix $V_h$ provides the features that will be aggregated based on the attention weights.\nEach head's output $O_h \\in \\mathbb{R}^{T \\times d_k}$ is concatenated along the feature dimension:\n$H_{attn} = \\bigoplus_{h=1}^H O_h,$"}, {"title": null, "content": "resulting in a $\\mathbb{R}^{T \\times (H \\cdot d_k)}$ matrix. A shared projection reduces this to the original hidden dimension d:\n$H_{attn}^{(k)} = H_{attn} W_O^{(k)},$"}, {"title": null, "content": "where $W_O \\in \\mathbb{R}^{(H \\cdot d_k) \\times d}$ is learnable.\nEach attention head $A_h \\in \\mathbb{R}^{T \\times T}$ serves as an adjacency matrix, specifying how each \u201cnode\u201d (token) attends to others. This adjacency-guided aggregation is central to the connection between transformers and GINs.\nIn a Graph Isomorphism Network (GIN), the layer update for a node v at the k-th layer is:\n$h_v^{(k)} = \\text{MLP}^{(k)}\\left((1+\\epsilon) h_v^{(k-1)} + \\sum_{u \\in N(v)} h_u^{(k-1)}\\right),$"}, {"title": null, "content": "where $\\epsilon$ is a scalar controlling the self-contribution, and MLP is a multi-layer perceptron for feature transformation.\nSimilarly, in a transformer, the self-attention mechanism computes:\n$H_{attn}^{(k)} = H^{(k)} + \\left(\\sum_{h=1}^{H} A_h (H V^{(k)}) W_o^{(k)}\\right),$"}, {"title": null, "content": "where $A_h$ is the adjacency matrix from attention", "MLP": "n$H^{(k)} = \\text{MLP} (H_{attn}) = \\text{MLP}\\left("}]}