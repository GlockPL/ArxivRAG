{"title": "AI as a Tool for Fair Journalism: Case Studies from Malta", "authors": ["Dylan Seychell", "Gabriel Hili", "Jonathan Attard", "Konstantinos Makantatis"], "abstract": "In today's media landscape, the role of Artificial Intelligence (AI) in shaping societal perspectives and journalistic integrity is becoming increasingly apparent. This paper presents two case studies centred on Malta's media market featuring technical novelty. Despite its relatively small scale, Malta offers invaluable insights applicable to both similar and broader media contexts. These two projects focus on media monitoring and present tools designed to analyse potential biases in news articles and television news segments. The first project uses Computer Vision and Natural Language Processing techniques to analyse the coherence between images in news articles and their corresponding captions, headlines, and article bodies. The second project employs computer vision techniques to track individuals' on-screen time or visual exposure in news videos, providing queryable data. These initiatives aim to contribute to society by providing both journalists and the public with the means to identify biases. Furthermore, we make these tools accessible to journalists to improve the trustworthiness of media outlets by offering robust tools for detecting and reducing bias.", "sections": [{"title": "I. INTRODUCTION", "content": "The world's social challenges, such as the influence of public perspectives, are being shaped as technical challenges. In this context, the societal implications of Artificial Intelligence (AI) must be given the needed weight. Therefore, society needs to advance its ethical Al practices, as outlined in the recent Council of Europe guidelines on AI in journalism [1]. The induction of bias in content is constantly challenging elections, public opinion, and democracy itself. Journalism and the media are responsible for informing the public with fair information. Information consumers can use AI to detect potential bias while also providing insights to journalists on the content they produce. As the influence of media expands globally, developing evidence-based techniques to promote fairness and accuracy is critical.\nThis paper presents two technical case studies on Malta's media market, which, despite being small, provide unique"}, {"title": "II. BACKGROUND AND LITERATURE REVIEW", "content": "This section discusses different types of media bias and its detection. Then, we present Vision Language Pre-training and Video Analysis techniques used in our case studies."}, {"title": "A. AI for Media Bias Mitigation", "content": "Media bias can be classified into different types [2]. Distortion bias occurs when facts are omitted, events are ignored, or numbers are presented as official without proper sourcing. Filtering bias involves cherry-picking facts when summarising an event to slant the news in a particular direction. Ideological bias promotes a specific political party or ideology, while spin bias aims to create attention-grabbing stories. Gatekeeping bias involves deciding which stories to report or suppress, while coverage bias entails unequal visibility given to certain topics. Lastly, statement bias influences reader sentiment through the language used. Contribution towards these biases can all be committed via clever usage of images in the article, namely the choice of image, position, size, visibility, description, and content.\nMoreover, different automatic techniques exist to analyse these different biases [3]. For example, plagiarism detection methods can be used to identify content reuse across newspapers. News Sentiment Classifiers datasets [4] can also be applied to news article text. Facial Detection and Recognition help detect how much exposure is given to certain persons"}, {"title": "B. Vision Language Pre-training", "content": "Visual Language Pre-training (VLP) combines elements from CV and NLP for downstream uni-modal tasks while minimising the need for training a new model from the ground up [6]. VLP models aim to understand images in relation to natural language and develop a universal abstract understanding of how humans tend to describe pictures using words. With advancements in natural language model pre-training, namely BERT [7], transformers have become the standard architecture powering all state-of-the-art VLP mod-els. Some examples of these large-scale VLP models are ALIGN [8] and BLIP [9]. VLP is helpful in various appli-cations, including Caption Generation, Image-Text Retrieval, Image Classification, and Natural Language Visual Reasoning (NLVR). VLP models generally comprise three modules: the Vision Encoder, the Text Encoder, and the Multimodal Fusion.\nThe Vision Encoder aims to extract significant features from an input image, categorised into Region Features, Grid Features, and Patch Projection. The OD-based Region Features (OD-RF) [10] approach uses an object detection model to extract bounding-box features. A drawback of OD-RF is that the vision backbone is frozen during training, which restricts overall performance. On the other hand, CNN-based Grid Features (CNN-GFs) [11] enable end-to-end (e2e) process-ing. Lastly, Vision Transformers (ViT) [12] split images into patches and project them into embeddings. Text encoders such as Word2Vec [13], GloVe [14] aim to create highly informative latent text representations. Due to the success of BERT [7], state-of-the-art text encoders generally employ transformers for text representation. In the multifusion module, both em-beddings are combined to produce a joint representation of the image-text pair, typically done using attention. There are two types of multimodal fusion: merged attention (used in VisualBERT [15], UNITER [16], ViLT [17], and GIT [18]) and co-attention (used in LXMERT [19] and ViLBERT [10]). Merged attention concatenates features and passes them to the same transformer block, while co-attention feeds features into separate transformer blocks with cross-attention."}, {"title": "C. Media Bias Analysis in Videos", "content": "In their paper, Lisena et al. [20] proposed a system for face recognition in videos using a combination of MTCNN [21], FaceNet [22], and SVM classifiers. The system obtained images through crawlers, extracted face embeddings using MTCNN and FaceNet, and used an SVM classifier to identify known faces with confidence scores. Frame-level analysis was performed, with face detection, cropping, alignment, and recognition for each frame. The SORT algorithm was used for face tracking, and a weighted average algorithm was employed to label faces in frame sequences. Hierarchical clustering"}, {"title": "III. PROPOSED AI IN JOURNALISM TOOLS", "content": "This section presents two novel AI systems designed to help consumers better understand news content while serving as a tool for journalists."}, {"title": "A. News Articles Analysis", "content": "The news article analysis system uses data from six online newspapers: The Times of Malta (ToM), The Shift (TS), Malta Today (MT), The Malta Independent (MI), Malta Daily (MD), and Gozo News (GN).\nThe dataset includes 4,800 news articles from each online newspaper from the 5th of April, 2023. All articles from TOM included at least one caption describing an image. 1.75% of MT articles (14) did not have at least one caption, while this percentage was 51.00% (408) for TS. The articles from the other newspapers did not include a caption with the accompanying images. The title, article body, and image-caption pairs of every article are extracted to local storage, where six main transformations will be performed on the data: Named Entity Recognition (NER) Tagging, Keyword Extraction, Sentiment Analysis, Caption Generation, Synthetic Caption Similarity, and Image-Text Matching (ITM). Caption Generation and ITM are performed using BLIP with a ViT-L/16 [28] vision backbone. These six transformations aim to extract bias-indicative insights from the news article data, which can be"}, {"title": "B. News Video Analysis", "content": "This section presents an automated system for monitoring news videos using computer vision techniques. The technical contributions are as follows. Firstly, we curated and annotated a new dataset of 215 news segment videos, comprising 8.48 hours, from TVM news containing named persons in captions. Third-party annotators carefully annotated the dataset, provid-ing accurate timestamps, names, and flags indicating name visibility and presenter status. These annotations allow for a detailed analysis of the system's results and performance regarding OCR and person detection. The dataset was used to train and evaluate the system. The resolution of the videos ranged up to 1280\u00d7720px.\nA pipeline for automatically extracting depicted persons' names using optical character recognition on caption frames was designed and built. This system was also built with privacy by design. For this reason, the system only extracts informa-tion that appears on the news video without access to external systems or sources. Moreover, it is built to be tuned to only store information about individuals holding public office to minimise any harmful application. Nonetheless, the capability for automated naming is novel. Building on this, we present our machine-learning approach for detecting, tracking and temporally localising individuals throughout the videos. This facilitates the automated generation of camera-time statistics.\nThe Video Analysis system comprises sub-components: face detection, recognition, object tracking, and name extraction. These work together to extract faces, track individuals, and retrieve names to create a timeline. Figure 2 shows a selection of frames from the auxiliary output provided by the system, which is intended to provide transparency and accountability when supporting the timeline illustrated in Figure 3."}, {"title": "1) Name Extraction:", "content": "The name extraction component is tailored to extract names from videos with a specific struc-ture. The system utilises pre-processing operations, including image closing, opening, and HSV colour thresholding. Box-like contours are identified through conditional statements and extensive testing. The preprocessed image is then processed using Pytesseract\u00b9 OCR, which is optimised to recognise structured text, including the white box with blue text found in the videos. The most common non-empty text in a scene is selected as the individual's name to handle text transition animations. The original name is used if a match is found in the database; otherwise, the extracted name is saved in a local database. This approach successfully extracted names from most name captions in the videos."}, {"title": "2) Face Detection, Encoding, and Recognition:", "content": "The dis-tance between the face and the database encodings is cal-culated for each frame to match a detected face with a known individual. The average distance across all frames is compared to a given threshold in an adaptation of FaceRec [20]. However, an issue arises when adding an individual's facial encoding to the database due to multiple face encodings extracted from multiple frames. Different approaches were employed to select the most suitable facial encoding, including taking the first and middle frames. This was based on the work inspired by Gao et al.'s work on scene detection [31] while calculating the average of the extracted encodings. These strategies contributed to improving facial recognition accuracy."}, {"title": "3) Face Tracking:", "content": "Initially, attempts at scene detection were employed to segment videos into shots, but they were found to be unreliable due to sudden movements and prolonged shots that featured multiple people or individuals for only a brief moment. As a result, a tracking approach [20] was adopted to improve accuracy. The Kernelised Correlation Filters (KCF) [32] algorithm was used for scene segmentation. The KCF algorithm initialises a bounding box around a detected face and tracks it across subsequent frames. The tracker automatically adapts to changes in scale and position, ensuring the bounding box stays aligned with the tracked face. The tracker was exclusively used to indicate the start and end scenes of the individual used to generate the timeline visualisation presented in Figure 3."}, {"title": "IV. EVALUATION", "content": "This section presents the evaluation of the news article analysis system. The machine-learning data-transformation pipeline was evaluated on a total of 4,800 articles from six Maltese newspapers listed in Section III-A. One of the goals of the evaluation was to examine whether newspapers artificially add to an article's perceived sentiment by intentionally using irrelevant images. The correlation coefficients in Table I show whether opinionated newspapers tend to have poorer image relevancy to the article body or vice-versa. A positive value is related to increased image relevancy as the article has more positive/neutral/negative scores. A negative value means image relevancy decreases. From the results obtained, no correlation was found between these two variables.\nAnother goal of the evaluation was to investigate the rela-tionship between the images and article text. Table II sum-marises the main findings resulting from this experiment. The newspaper with the highest similarity score to the synthetic captions (SSC) was the Times of Malta (ToM). Malta Daily (MD) performed the best regarding Image-Text Matching (SI)."}, {"title": "B. Evaluating the News Video Analysis", "content": "This section evaluates the system for automatic analysis of news videos, focusing on its strengths, weaknesses, and performance. The Video Analysis component, which forms the system's core, is essential to evaluate. While previous studies have explored news videos [31], [33]-[36] and face"}, {"title": "1) Metrics:", "content": "The performance of the name extraction sub-component is evaluated using a confusion matrix, comparing the system's identified names against the names in the val-idation set per video. Using the confusion matrices, metrics such as precision, recall, F1 score, and accuracy score are calculated to assess the name extraction and face recognition performances. To evaluate the accuracy of the scene prediction sub-component or tracker for each person, the Mean Absolute Error (MAE) is calculated. The MAE duration represents the average difference in seconds between the predicted and actual durations for each individual from the annotated dataset. A smaller MAE duration indicates better predictions."}, {"title": "2) Results:", "content": "Table III presents the performance of the Video Analysis system for the tested variations. The table is divided into two sections: one for scene detection methods and the other for the latest method using face tracking. The numbers in parentheses indicate the seconds skipped between frames during analysis. For scene detection methods, two values are displayed: the seconds skipped during analysis and the seconds skipped during scene detection. 'Skips' indicates the use of the same default parameters. 'Def.' signifies the default resolution, while 640\u00d7360px was used to speed up frame analysis in other cases. 'Fir.', 'Mid.', and 'Avg.' correspond to the first, middle, and average face encodings, respectively, as explained in Section III-B2. In scene detection, only the first encountered face encoding was used for each individual. The metrics P (Precision), R (Recall), F1 (F1-score), and A (Accuracy), MAE (Mean Absolute Error) were calculated as described in Section IV-B1. The Time column represents the hours required to analyse all the 'News Video Segments' videos. Multiprocessing was employed in some analyses to reduce processing time. The best metric achieved for each calculation is highlighted in bold and underlined.\nDuring the investigation, the name extraction process en-countered several issues: detecting names with punctuation marks, misidentifying the right side of the caption box as 'L,' misinterpreting 'L' as the end of the caption box, ex-ceeding frame view limitations, and skipping frames with names during video analysis. Addressing these issues is crucial to enhancing extraction effectiveness. The results show that a one-second interval for video analysis achieved the best scene detection and face tracking performance, consistent with [20]. Lower intervals in scene detection improved accuracy, while larger intervals in face tracking led to higher errors. When compared to Scene Detection and Face Tracking, Face Tracking achieved a lower MAE value, indicating better results in selecting individual timestamps. The performance of face recognition does not vary significantly with different face encoding selections. As suggested in [31], the middle face encoding generally yields slightly better results. However, relying solely on the middle or first face encoding might return lower accuracy if the face is unclear or poorly represented. The results in Table III align with existing work [31] [20] where the optimal configuration for the Video Analysis system involves using face tracking and analysing frames at 1-second intervals. The default resolutions performed slightly better, but the longer processing time does not justify this marginal advantage. Using a resolution of 640\u00d7360px is recommended for faster analysis. Overall, the results demonstrate promising"}, {"title": "V. CONCLUSION", "content": "This paper presented two technical case studies of applied AI in journalism and media, each technique presenting its novelty. We believe transparently surfacing potential distor-tions in news content allows media professionals of good faith opportunities to focus their efforts accordingly towards fairness, in line with the Council of Europe's recommendations on disclosure of AI use [1]. We plan for future work to expand the reach of these projects and validate them in broader contexts."}]}