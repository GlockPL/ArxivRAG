{"title": "STUTTER-SOLVER: END-TO-END MULTI-LINGUAL DYSFLUENCY DETECTION", "authors": ["Xuanru Zhou", "Cheol Jun Cho", "Ayati Sharma", "Brittany Morin", "David Baquirin", "Jet Vonk", "Zoe Ezzes", "Zachary Miller", "Boon Lead Tee", "Maria Luisa Gorno-Tempini", "Jiachen Lian", "Gopala Anumanchipalli"], "abstract": "Current de-facto dysfluency modeling methods [1, 2] utilize template matching algorithms which are not generalizable to out-of-domain real-world dysfluencies across languages, and are not scalable with increasing amounts of training data. To handle these problems, we propose Stutter-Solver: an end-to-end framework that detects dysfluency with accurate type and time transcription, inspired by the YOLO [3] object detection algorithm. Stutter-Solver can handle co-dysfluencies and is a natural multi-lingual dysfluency detector. To leverage scalability and boost performance, we also introduce three novel dysfluency corpora: VCTK-Pro, VCTK-Art, and AISHELL3-Pro, simulating natural spoken dysfluencies including repetition, block, missing, replacement, and prolongation through articulatory-encodec and TTS-based methods. Our approach achieves state-of-the-art performance on all available dysfluency corpora. Code and datasets are open-sourced at https://github.com/eureka235/Stutter-Solver.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech dysfluency modeling is the core module in speech therapy. The U.S. speech therapy market is projected to reach USD 6.93 billion by 2030 [4]. Technically, speech dysfluency modeling is a speech recognition problem, which is dominated by large-scale developments [5, 6, 7, 8]. However, those large ASR models struggle with dysfluent speech because ASR is a dysfluency removal process. For a long time, researchers have mainly treated it as a classification problem. Early methods relied on hand-crafted features [9, 10, 11, 12, 13]. More recently, end-to-end classification tasks have been developed [14, 15, 16, 17, 18]. However, two big problems remain. First, dysfluency depends on the text, which previous methods have ignored. Second, simple classification is too basic to be deployed in real speech therapy systems. [1] proposed 2D-alignment, the alignment between reference text and phoneme-level forced alignment. Then, the template matching algorithm (with each dysfluency type as a template) was performed for dysfluency detection. The subsequent work, H-UDM [2], proposed recursive UDM [1] that updates word boundary segments together with alignment prediction. Nevertheless, these methods still exhibit certain limitations. Firstly, UDM [1] and H-UDM [2] are essentially feature engineering approaches, which may not adequately handle real-world dysfluencies that do not conform to predefined templates. Secondly, developing templates for each language is impractical, as dysfluency templates are inherently language-dependent. Thirdly, template matching algorithms do not utilize training data, rendering them non-scalable with respect to dysfluency data whenever it is available.\nTo handle the aforementioned limitations, we approach dysfluency modeling from a simple and new perspective. Dysfluency modeling can be regarded as an object detection problem in the 1D domain. As such, we conceptualize this as a detection task, inspired by YOLO [3]. We propose Stutter-Solver, which takes dysfluent speech and reference ground truth text as input, and directly predicts dysfluency types and time boundaries in an end-to-end manner. Note that [19] uses the similar idea. However, Stutter-Solver focuses on: co-dysfluencies, multi-linguality, articulatory-simulation and co-dysfluency TTS-simulation. Stutter-Solver requires high-quality annotated dysfluency data (with precisely annotated type and time boundaries).Therefore, we propose an innovative dysfluency simulation method: articulatory-based [20], and we performed comparative experiments with TTS-based methods. We developed three synthetic dysfluency datasets: VCTK-Pro and AISHELL3-Pro, using VITS [21] for the TTS-based method; additionally, VCTK-Art using Articulatory Encodec [20] as a vocal tract articulation simulation tool. Both VCTK-Pro and VCTK-Art build upon the VCTK corpus [22], whereas AISHELL3-Pro builds upon the AISHELL3 corpus [23]. These datasets include repetition, missing, block, replacement, and prolongation at phoneme & word levels for English, and at the character-level for Mandarin. As such, Stutter-Solver is naturally a multi-lingual co-dysfluency detector with no hand-crafted templates involved. As part of the speech therapy process, we have 38 English and 8 Chinese Mandarin-speaking nfvPPA subjects [24] from clinical collaborations. The proposed Stutter-Solver achieved state-of-the-art accuracy, bound loss, and time F1 score on our new benchmark (VCTK-Art, VCTK-Pro, AISHELL3-Pro), public corpus, and nfvPPA"}, {"title": "2. ARTICULATORY-BASED SIMULATION", "content": "Previous research on dysfluent speech simulation [1, 14] has focused on direct manipulation of waveform, which has resulted in poor naturalness, evidenced in Table. 2. To address this limitation, we perform simulation in two orthogonal spaces: articulatory space and textual space. This section details articulatory-based simulation, while section 3 elaborates on the textual space approach (TTS-based simulation).\nFor articulatory-based method, we simulate dysfluency by directly editing the articulatory control space, by utilizing an offline articulatory inversion and synthesis models (Articulatory Encodec [20]). The Articulatory Encodec is composed of acoustic-to-articulatory inversion (AAI) model and an articulatory vocoder. [20] shows that the articulatory encodec can successfully applied to arbitrary accents and speaker identities with high-performance. The pipeline is detailed below."}, {"title": "2.1. Method Pipeline", "content": "We first run MFA to align raw VCTK speech with its ground truth text, obtaining 50 Hz phoneme-level force alignment that matches the EMA features from the AAI module. Various types of dysfluency are then introduced by editing the EMA features: Repetition: The target phoneme segment is duplicated 2-4 times. Replace: We sample a random phoneme from the current EMA feature to replace the target phoneme. Block: A silence frame with 10-15 units is inserted after the target phoneme, with the silence frames sampled from the beginning of the current EMA feature. Missing: The target phoneme is removed. Prolongation: Interpolating within the target phoneme, extending its duration by 4 to 6 times its original length. For repetition and prolongation, the target phonemes are respectively the first phoneme of a randomly selected word and a randomly chosen vowel. For other dysfluency types, the target phonemes are selected arbitrarily without any other restrictions. To ensure smooth auditory perception, we insert a 2-unit interpolate buffer frame before and after each modification. All the interpolation operations mentioned above use bilinear interpolation. Besides phoneme-level modifications above, we also implemented word-level repetition and missing, where the target word is modified instead of the target phoneme, with all other aspects remaining identical. The whole pipeline is depicted in Fig. 2.\nNote that only the English version of articulatory-encodec model is available at this time, limiting our simulation contribution to English. However, we explored multi-lingual simulation in TTS-based Simulation, detailed in the next section."}, {"title": "3. MULTI-LINGUAL TTS-BASED SIMULATION", "content": ""}, {"title": "3.1. Method pipeline", "content": "The pipeline of TTS-based simulation can be divided into following steps: 1) Dysfluency injection: for VCTK-Pro, we convert VCTK [22] text into IPA sequences via the VITS phonemizer, and for AISHELL3-Pro, we convert Mandarin text into pinyin sequences. We then add different types of dysfluencies at the phoneme/word(English) and pinyin (Chi-"}, {"title": "3.2. Co-Dysfluency TTS rules", "content": "For VCTK-Pro, we incorporate phoneme and word-level dysfluency; for AISHELL3-Pro, we introduce character-level dysfluency. Dysfluencies are simulated via TTS rules [19], with examples provided in Fig. 3.\nIn VCTK-Pro, we introduce co-dysfluency, adding multiple dysfluencies into a single utterance. Co-dysfluency is categorized into single-type and multi-type. For single-type, we insert 2-3 instances of the same type of dysfluency (involves every type mentioned above) at various positions within an utterance. For multi-type, we incorporate 5 combinations of dysfluencies: (rep-missing), (rep-block), (missing-block), (replace-block) and (prolong-block), with 2 random positions chosen for each combination within the utterance. Note that due to ethic concerns, de-identification techniques [25] might also be involved in the process."}, {"title": "4. DYSFLUENCY DETECTION AS OBJECT DETECTION", "content": "Accurate dysfluency detection necessitates handling text dependencies since stutters are not necessarily monotonic. In this work, we adopt the soft speech-text alignment from VITS [21], which is one of the SOTA TTS models. Given this speech-text alignment as input, our model requires two main components: an optimal spatial and temporal downsampling method, and an extraction mechanism to accurately attend to the relevant dysfluent signal. Region-wise dysfluency detection can be viewed as a 1D extension of the 2D object detection problem in computer vision, drawing inspiration from the YOLO [3] method, we design a detector that takes the soft speech-text alignment and produces a fixed size 64 x 8 (temporal dim x output dim) output matrix. At each timestep, 8 values are predicted: dysfluency start & end bounds, confidence score, and C (=5) class predictions. The detector which utilizes a region-wise prediction scheme consists of spatial pattern collector blocks followed by a temporal analysis unit. The entire paradigm is shown in Fig. 1 and the corresponding modules are detailed in the following."}, {"title": "4.1. Soft speech-text alignments", "content": "We obtain $|C_{\\text{text}}|\\times |z|$ monotonic attention matrix $A$ from VITS [21] that represents how each input phoneme aligns with target speech, where $C_{\\text{text}}$ is text dimension and $z$ the speech duration. For training, we use the soft alignments $A$ and apply a softmax operation across the text dimension, computing the maximum attention value for each time step. To calculate the soft alignments, we use the original pre-trained text and speech encoders. The former is a Transformer encoder [26] with relative positional embeddings, and the latter is a model that uses non-causal residual blocks used in WaveGlow [27]. This soft-alignment attention matrix is then passed into the detection head for training and inference."}, {"title": "4.2. Spatial-Temporal Encoders", "content": "We adopt the same spatial-temporal encoders as [19]. It consists of a region-wise spatial encoder and a temporal encoder."}, {"title": "4.2.1. Spatial Encoder", "content": "Learnable spatial pattern collector blocks are used to preserve local spatial features. Here, we are going to elaborate more on the intuition. Traditional speech recognition tasks take speech features such as mel spectrograms as input, and the de facto encoder [28] is applied. However, the soft alignments $A$ mentioned are spatially different from speech features, such that separate convolutions (pointwise and depthwise) will be ineffective for such input representations. Therefore, a modified convolution paradigm was proposed [19], where a depthwise convolution followed by a grouped convolution, instead of a pointwise convolution, is adopted in this setting. This has been experimentally proven to preserve region-wise information, as visualized in Fig. 1 (b)."}, {"title": "4.2.2. Temporal Encoder", "content": "Since the task is to predict time-aware dysfluencies, technically it is a region-wise aggregation, which is a 1D sequential timing problem. To achieve this, the transformer encoder [26] is simply applied to handle both global and local timing alignments. We employed transformer-base in this setting."}, {"title": "4.3. Training Loss", "content": "The speech utterance is split into segments of fixed steps. For each segment, we are going to predict three things: (1) the dysfluency confidence score (if and how confident we are that dysfluency exists in this segment), denoted as $y_i$, (2) the boundary of the dysfluencies $b_{\\text{start}}$ and $b_{\\text{end}}$, and (3) the dysfluency type $C_n$: whether that dysfluency is a block, repetition, replacement, insertion, or missing word. The bound values are normalized between 0-1 using fixed padded lengths as the max bound values. The balancing factors are $\\lambda_{\\text{bound}} = 5$, $\\lambda_{\\text{conf}} = 1$, and $\\lambda_{\\text{class}} = 0.5$. $S$ is the number of regions and $n$ is the number of classes. $1_{\\text{obj}}$ indicates whether the dysfluency appears in that segment. The loss function is denoted by the following equation:\n$L = \\lambda_{\\text{bound}} \\sum_{i=0}^{S} 1_{\\text{obj}} [(b_{\\text{start}} - \\hat{b_{\\text{start}}})^2 + (b_{\\text{end}} \u2013 \\hat{b_{\\text{end}}})^2]$\\begin{equation*}\\ -\\ \\lambda_{\\text{conf}} \\sum_{i=0}^{S} \\left[\\hat{y}_i \\log(p(y_i)) + (1 - \\hat{y}_i) \\cdot \\log(1 - p(y_i))\\right] \\\\-\\lambda_{\\text{class}} \\sum_{i=0}^{S} \\sum_{j=0}^{n} c_n \\log(p(\\hat{c}_n))\\end{equation*}"}, {"title": "5. EXPERIMENTS", "content": ""}, {"title": "5.1. Datasets", "content": "\u2022 VCTK [22] includes recordings from 109 native English speakers. Each speaker reads out about 400 sentences from a newspaper, the rainbow passage and an elicitation paragraph used for the speech accent archive. This corpus encompasses about 48 hours of accented speech. It is used for simulating VCTK-Pro and VCTK-Art.\n\u2022 AISHELL-3 [23] is a large-scale and high-fidelity multi-speaker Mandarin speech corpus which includes 218 native Chinese mandarin speakers with roughly 85 hours of emotion-neutral recordings. It is used for simualting AISHELL3-Pro.\n\u2022 LibriStutter [14] is a synthesized dataset which contains artificially stuttered speech and stutter classification labels for 5 stutter types. It was generated using 20 hours of audio selected from the 'dev-clean-100' section of [29].\n\u2022 UCLASS [30] contains recordings from 128 children and adults who stutter. Only 25 files have been annotated and did not annotate for the block class, we only used those files and did not use the block class for subsequent datasets.\n\u2022 SEP-28K is curated by [31], contains 28,177 clips extracted from publicly available podcasts. These clips are labeled with five event types including block, prolongation, sound / word repetition and interjection. Clips labeled as \"unsure\" in the were excluded from the dataset.\n\u2022 Aphasia Speech is collected from our clinical collaborators, our dysfluent data comprises 46 participants (38 English speakers and 8 Chinese mandarin speakers) diagnosed with Primary Progressive Aphasia (PPA), larger than the data used in [1, 2] which only has 3 English speakers."}, {"title": "5.2. Training", "content": "We trained the detector for 30 epochs using a 90/10 train/test split, which was separately applied to three simulated datasets."}, {"title": "5.3. Metrics", "content": "\u2022 Phoneme Error Rate (PER) is a measure of how many errors (inserted, deleted, and changed phonemes) are predicting phoneme sequences compared to the actual phoneme sequence. It calculated by dividing the number of phoneme errors by the total number of phonemes.\n\u2022 Accuracy (Acc.) refers to the correctness of predictions regarding types of dysfluency within regions that exhibit some form of dysfluency.\n\u2022 Bound loss is calculated as the mean squared error between the predicted and actual boundaries of dysfluent regions within a 1024-length padded spectrogram, which is then converted to a time scale using a 20ms sampling frequency. For co-dysfluency analyses, the bound loss is averaged across all identified dysfluent regions.\n\u2022 Time F1 [1] measures the accuracy of boundary predictions by assessing the overlap between predicted and actual dysfluent region bounds. A sample is classified as a True Positive if any intersection occurs between these bounds."}, {"title": "5.4. Evaluation of dysfluency simulation", "content": ""}, {"title": "5.4.1. MOS tests", "content": "To evaluate the rationality and naturalness of three datasets we constructed, we collected Mean Opinion Score (MOS, 1-5) ratings from 11 people. The results are displayed in Table. 2. Our three simulated datasets were perceived to be far more natural than the VCTK++ [1] baseline corpus. Notably, VCTK-Pro was rated as closely mimicking human speech."}, {"title": "5.4.2. Dysfluency intelligibility", "content": "In order to verify the intelligibility of simulated datasets, we use phoneme recognition model [32] to evaluate the raw VCTK (/) and various types of dysfluent speech from VCTK-Pro and VCTK-Art. Results in Table. 3 show generally low PER, indicating good intelligibility and usability despite higher PERS than raw VCTK. Comparatively, VCTK-Pro performs better overall, while VCTK-Art excels particularly in repetition and block. AISHELL3-Pro was not evaluated due to the lack of available high-quality pinyin-level Chinese speech recognition model."}, {"title": "5.5. Dysfluency detection", "content": "To assess the performance of trained detector, we conduct evaluations on three simulated datasets, as well as on the PPA data. The results, which include type-specific detection accuracy and bound loss metrics, are detailed in Table. 4 for the simulated datasets and in Table. 5 for the PPA data. Additionally, we compared our results with previous works by validating it on UCLASS, Libristutter, and SEP-28K, where we computed type-specific accuracy and Time F1, as shown in Table. 6.\nIn Table.4, we used H-UDM [2] as the baseline. Both versions of Stutter-Solver(VCTK-Art and VCTK-Pro) surpassed H-UDM across all metrics. Notably, Stutter-Solver(VCTK-Pro) showed stronger results for English. Additionally, AISHELL3-Pro performed even better, likely due to the unique pronunciation traits of Chinese and its noticeable character-level dysfluency. In Table. 6, we presented our results using publicly available datasets (UCLASS, LibriStutter, and SEP-28K). Since the original benchmarks use private test sets, direct accuracy comparisons may not be completely fair. We instead emphasized time-aware detection, reporting the Time F1 score for each dataset. All baselines, except H-UDM, scored 0. Our proposed methods consistently outperformed H-UDM in these evaluations. In Table. 5, both versions of Stutter-Solver outperformed H-UDM, and the Chinese model performed best on Chinese PPA. However, the average accuracy remained low, underscoring the challenge of accurately capturing the real distribution of dysfluency."}, {"title": "5.6. Co-dysfluency", "content": "In Section 3.2, we incorporated co-dysfluency into VCTK-Pro. We trained Stutter-Solver on single-type, multi-type, and mixed-type (single & multi) co-dysfluency respectively, and measured average accuracy and bound loss using corresponding simulated data. The results, shown in Table. 7, demonstrate that our detector's performance on co-dysfluency matches its capability in simpler scenarios with only one dysfluency per utterance. This indicates that our detector handles co-dysfluency effectively. It is worth noting that this fundamental property is missing in previous work."}, {"title": "5.7. Multi-lingual", "content": "In addition to training the detector separately on single languages, we trained it simultaneously on two languages to evaluate its performance in a multi-lingual scenario. We randomly sampled 300 hours of data from both VCTK-Pro and AISHELL3-Pro for training. The results, presented in Table. 8, show that multi-lingual training slightly improved detection performance for English but significantly reduced it for Chinese compared with training separately on a single language. This indicates that multi-lingual training has varying effects on detection accuracy depending on the language. It is important to note that our method does not require additional language-specific dysfluency templates, in contrast to the previous state-of-the-art work by [2]."}, {"title": "6. CONCLUSIONS AND LIMITATIONS", "content": "We propose Stutter-Solver that detects speech dysfluencies in an end-to-end manner. Stutter-Solver is able to handle co-dysfluencies within the utterance and is a natural multi-lingual dysfluency detector. We proposed three annotated dysfluency simulated corpora such that Stutter-Solver achieves state-of-the-art performance on a couple of dysfluency corpora. However, limitations exist. First, the performance on real nfvPPA speech is far worse than that on simulated speech. Future work will focus on reducing the gap between simulated and real dysfluency distributions. Second, the proposed simulated corpora are not at a large scale and we have not reached the limit. Future work will focus on pushing the limit of scaling efforts when more data and resources are available. Third, it is worth exploring the simulation in gestural space [33, 34] or rtMRI space [35] instead of articulatory EMA space for finer-grained control. It is also worth exploring both speaker-dependent and speaker-independent dysfluencies via disentangled analysis and synthesis [36, 37, 38, 39, 40], which serves as the foundation for behavioral dysfluency study."}]}