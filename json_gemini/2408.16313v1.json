{"title": "FA-YOLO: Research On Efficient Feature Selection YOLO Improved Algorithm Based On FMDS and AGMF Modules", "authors": ["Yukang Huo", "Mingyuan Yao", "Qingbin Tian", "Tonghao Wang", "Ruifeng Wang", "Haihua Wang"], "abstract": "Over the past few years, the YOLO series of models has emerged as one of the dominant methodologies in the realm of object detection. Many studies have advanced these baseline models by modifying their architectures, enhancing data quality, and developing new loss functions. However, current models still exhibit deficiencies in processing feature maps, such as overlooking the fusion of cross-scale features and a static fusion approach that lacks the capability for dynamic feature adjustment. To address these issues, this paper introduces an efficient Fine-grained Multi-scale Dynamic Selection Module (FMDS Module), which applies a more effective dynamic feature selection and fusion method on fine-grained multi-scale feature maps, significantly enhancing the detection accuracy of small, medium, and large-sized targets in complex environments. Furthermore, this paper proposes an Adaptive Gated Multi-branch Focus Fusion Module (AGMF Module), which utilizes multiple parallel branches to perform complementary fusion of various features captured by the gated unit branch, FMDS Module branch, and TripletAttention branch. This approach further enhances the comprehensiveness, diversity, and integrity of feature fusion. This paper has integrated the FMDS Module, AGMF Module, into Yolov9 to develop a novel object detection model named FA-YOLO. Extensive experimental results show that under identical experimental conditions, FA-YOLO achieves an outstanding 66.1% mean Average Precision (mAP) on the PASCAL VOC 2007 dataset, representing 1.0% improvement over YOLOv9's 65.1%. Additionally, the detection accuracies of FA-YOLO for small, medium, and large targets are 44.1%, 54.6%, and 70.8%, respectively, showing improvements of 2.0%, 3.1%, and 0.9% compared to YOLOv9's 42.1%, 51.5%, and 69.9.", "sections": [{"title": "1. Introduction", "content": "Object detection, a fundamental computer visual task, aims to identify object categories and locate their positions. It is extensively applied in various domains, including multi-object tracking [1, 2], autonomous driv- ing [3, 4], robotics [5, 6], and medical image anal- ysis [7, 8]. With the widespread adoption of trans- formers, researchers have developed a series of end- to-end object detection models using the transformer's encoder-decoder architecture, such as DETR [9], Con- ditional DETR [10], Deformable DETR [11], and DINO [12]. Although Transformer-based detectors demon- strate remarkable detection performance, they still lag behind CNN-based models in terms of speed. Over the past years, extensive research has been conducted on CNN-based detection networks, achieving signifi- cant progress [13, 14, 15, 16, 17, 18, 19]. The object detection framework has evolved from two-stage mod- els (e.g., Faster RCNN [18] and Mask RCNN [20]) to one-stage models (e.g., YOLO [13]), from anchor-based (e.g., YOLOv3 [21] and YOLOv4 [14]) to anchor-free (e.g., CenterNet [22], FCOS [23], and YOLOX [15]). Researchers like Golnaz Ghiasi [24, 25, 26] have ex- plored optimal network architectures for object detec- tion tasks through NAS-FPN, and others [27, 28, 29] have investigated distillation as a method to enhance model performance. The YOLO series, based on CNN"}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Realtime object detectors", "content": "In current, the main real-time object detectors are the YOLO series [14, 34, 15, 35, 36, 37, 38, 39, 13, 30, 21, 40, 19, 41, 42, 43]. YOLOv1-v3 [13, 30, 21] estab- lished the initial YOLO framework, featuring a single- stage detection structure composed of a backbone, neck, and head, and utilized multi-scale branches to predict objects of varying sizes, thus becoming a representative single-stage object detection model. YOLOv4 [14] op- timized the previously used Darknet backbone and in- troduced several enhancements such as the Mish activa- tion function, PANet, and advanced data augmentation techniques. YOLOv5 [31], inheriting the YOLOv4 [14] framework, features improved data augmentation strate- gies and a wider variety of model variants. YOLOX [15] integrated Multi Positives, Anchor-free, and De Coupled Head into the model structure, setting a new paradigm for YOLO model design. YOLOv6 [39, 38] firstly incorporate reparameterization techniques by in- troducing the EfficientRep Backbone and Rep-PAN Neck. YOLOv7 [41] focused on analyzing the impact of gradient paths on model performance, proposed the E-ELAN structure to enhance model capabilities with- out disrupting the existing gradient pathways. YOLOv8 [44] built upon the strengths of previous YOLO mod- els and integrated them effectively. YOLOv9 [36] uses GELAN to improve the architecture and training pro- cess introduced by the proposed PGI, becoming the lat- est generation of top-tier real-time object detectors. Al- though previous models employed effective feature in- tegration methods [24, 45, 46, 47, 48, 49, 50, 51], they still exhibited certain limitations."}, {"title": "2.2. Multi-scale features for object detection", "content": "Different levels of features carry positional informa- tion of different size objects. Larger feature maps con- tain low-dimensional texture details and the locations"}, {"title": "2.3. Multi-branch Architectures", "content": "The Inception architecture [57, 58, 59, 60] employed a multi-branch structure to enrich the feature space, demonstrating the importance of diversified connec- tions, various receptive fields, and combinations of mul- tiple branches. The Diverse Branch Block [61] adopts the concept of using a multi-branch topology; however, it differs in that 1) the Diverse Branch Block is a build- ing block that can be utilized across various architec- tures, and 2) each branch within the Diverse Branch Block can be transformed into a Conv, allowing such branches to be consolidated into a single convolution. Liu et al. [62] input a four-channel RGB-D image into the backbone network, subsequently obtaining saliency outputs from each minor branch (single-stream net- work). Chen et al. [63] utilized dual backbone net- works to separately extract RGB and depth features, which were then fused using a cascaded complemen- tary strategy (dual-stream network). Chen et al. [64] introduced a network structure comprising two inde- pendent modal backbone networks and a parallel cross- modal distillation branch, aimed at learning comple- mentary information. However, previous multi-branch structures rarely considered the integration of convo- lutional branches with attention branches, leading to the excessive weighting of redundant information. The AGMF Module, by integrating gated units, the FMDS Module, and TripletAttention, is capable of generating more comprehensive and enriched feature maps."}, {"title": "3. Method", "content": ""}, {"title": "3.1. Overview", "content": "In this section, based on the previous problem analy- sis, we will provide a detailed explanation of the moti- vations and specific structures of the FMDS Module and"}, {"title": "3.2. FMDS Module Design", "content": ""}, {"title": "3.2.1. Motivation", "content": "Although the YOLO series of models exhibit com- mendable detection speeds, this advantage often comes at the expense of sensitivity to detailed features. Par- ticularly in processing small object detections or sub- tle changes in complex scenes, they frequently fail to capture sufficient detail information, resulting in subop- timal detection accuracy. The primary reasons are the insufficient acquisition of fine features and the lack of a feature fusion method capable of dynamically adjust- ing feature processing strategies according to different scenarios.\nMoreover, when integrating cross-layer features, YOLO series models typically employ a structure sim- ilar to the Feature Pyramid Network (FPN) to achieve feature fusion. However, this structure encounters is- sues with information loss during the upward and down- ward transmission of information, often leading to par- tial loss of detail information. This loss of detail re- stricts the model's ability to process information and make decisions in complex environments, thereby im- pacting the detection performance."}, {"title": "3.2.2. FMDS Module", "content": "The FMDS Module enhances the detection accu- racy of small, medium, and large-sized targets in com- plex environments by implementing a more efficient dy- namic feature selection fusion method on fine-grained multi-scale feature maps, as illustrated in Figure 1. Ini- tially, the FMDS Module subdivides the input feature maps into multiple smaller regional blocks, as shown in Equation 1, enabling the model to capture the detailed features of targets of various sizes with greater preci- sion.\nX_Blocks = reshape(permute(reshape\n(X, (B, C, N, H //2,2, W //2)),\n(0, 2, 4, 1, 3, 5)), (\u22121, C, H //2, W//2))\n(1)\nIn this context, X represents the input feature map, B denotes the batch size, and H and W respectively stand for the height and width of the feature map. X_Blocks signifies the subdivision of the feature map into multiple smaller regional blocks. Subsequently, each regional block is independently processed by convolutional ker- nels of different scales, as illustrated in Equation 2. This approach not only enhances the local sensitivity of the features but also enables the model to capture more de- tailed spatial hierarchical information."}, {"title": "", "content": "3\nX_Blocks' = \u2211 fpw(fdw(X_Blocks, Kdw_i, S dw_i, Pdw_i),\ni=1\nKpw_i, Spw_i, Ppw_i)\n(2)\nWithin this framework, faw represents the Depthwise Convolution, where Kdw_i, Sdw_i, and Pdw_i respectively denote the size, stride, and padding of the Depthwise Convolution kernel. Similarly, fpw denotes the Point- wise Convolution, with Kpw_i, Spw_i, and Ppw_i specify- ing the size, stride, and padding of the Pointwise Convo- lution kernel. X_Blocks' represents the regional blocks obtained after being processed independently by convo- lutional kernels of various scales.\nSubsequently, the processed fine-grained multi-scale features are integrated, as shown in Equation 3.\nX' =reshape(permute(reshape\n(X_Blocks', (B,4,\u22121,H //2, W//2)), (3)\n(0, 2, 1, 3, 4), (B, \u22121, H, W)\nIn Equation 3, X' results from reassembling the mul- tiple processed regional blocks into a processed feature map.\nThe integrated feature map is then concatenated with the original feature map to form a new feature map, as illustrated in Equation 4.\nXconcat = concat (X, X', dim = 1)\n(4)\nIn this model, Xconcat is the feature map obtained by concatenating the original feature map with the feature map derived from Equation 3 along the first dimension. Subsequently, the new feature map is dynamically evaluated to determine the importance of features from different regional blocks and scales, optimizing the weight distribution of the features, as shown in Equa- tion 5.\nselect\nX' = DepthwiseS eparableConv(Xconcat,\nin_channels * 2, out_channels, K, S, P)\nselect\n(5)\nIn this model, X' refers to the feature map that has undergone feature-adaptive selection. K, S, and P respectively represent the size, stride, and padding of the convolution kernel.\nThis fine-grained and multi-scale dynamic feature se- lection and fusion approach significantly enhances the model's ability to handle small, medium, and large- sized targets in complex environments. For small-sized"}, {"title": "3.3. AGMF Module Design", "content": ""}, {"title": "3.3.1. Motivation", "content": "Different data characteristics, such as texture, color, and semantic content, may require distinct process- ing strategies. A single processing branch often fails to comprehensively capture the multidimensional fea- tures of complex data, especially when these features are intertwined and interdependent. However, a single processing branch may struggle to adapt to the varia- tions and demands of different types of data, potentially leading to the loss of important information or insuf- ficient feature representation. While traditional multi- convolution branches possess more robust capabilities for spatial feature extraction, they too can result in the overemphasis of irrelevant information. On the other hand, attention branches focus on key information and relationships within the data, optimizing the capability to parse global contexts. The combination of the convo- lution branch and the attention branch can enable the model to not only capture details accurately but also grasp essential features and trends of the overall data."}, {"title": "3.3.2. AGMF Module", "content": "The AGMF module is designed with three main par- allel processing branches: the Gated Unit branch, the FMDS Module branch, and the TripleAttention branch. Each branch is responsible for capturing and processing different aspects of the data. While efficiently collabo- rating, these branches also maintain the module's flex- ibility and high performance. The overall structure of the module is illustrated in Figure2(b).\nThe Gated Unit branch regulates and controls the flow of information in the feature maps, allowing it to adap- tively adjust based on the dynamic changes in the data. It filters information critical to the current task, sup- presses irrelevant or redundant data transmission, and enhances the model's focus and efficiency, as illustrated in Equations 6 and 7.\nYi_GU_weight = Activation (BN (Conv2d (Y;)))\n(6)\nIn this configuration, Y; represents the input feature map, and Yi_GU_weight denotes the weight assigned by the Gated Unit."}, {"title": "", "content": "Yi GU = Yi Yi GU_weight\n(7)\nIn Equation 7,Yi Gu represents the output from the Gated Unit branch.\nThe FMDS Module branch effectively captures and selects multi-scale and fine-grained data features, en- abling it to gather a range of feature scales from details to global aspects from the input data. The TripleAtten- tion branch utilizes the TripleAttention attention mech- anism to focus on enhancing the model's recognition and processing capabilities for key features, thereby strengthening the model's ability to identify crucial data characteristics. After each of these three branches has processed the data independently, their respective fea- ture outputs are gathered into a fusion layer. This layer considers the importance and complementarity of each branch's outputs, integrating these features to form a fi- nal, high-quality feature representation."}, {"title": "3.4. Architecture Design of FA-YOLO", "content": "To ensure consistency in subsequent ablation experi- ments, FA-YOLO employs the same data augmentation strategy and hyperparameter settings as YOLOv9. The primary difference between FA-YOLO and YOLOv9 is the introduction of the FMDS Module and AGMF Mod- ule within the RepNCSPELAN4 module. These addi- tions significantly enhance feature fusion and feature"}, {"title": "4. Experiment and Result analysis", "content": ""}, {"title": "4.1. Setups", "content": ""}, {"title": "4.1.1. Datasets", "content": "We conducted extensive experiments using the PAS- CAL VOC 2007 dataset to validate the proposed FA- YOLO enhancement algorithm. All of our experiments were conducted without the use of pre-trained models; instead, all models were trained from scratch. Finally, we compared the detection performance of FA-YOLO with other mainstream models in the YOLO series on the PASCAL VOC 2007 dataset."}, {"title": "4.1.2. Implementation details", "content": "This paper adopts the settings of YOLOv9, using the same architecture and training configurations, with the exception of the RepNCSPELAN4 structure. The op- timizer and other setting are also same as YOLOv9, i.e. stochastic gradient descent (SGD) with momentum and cosine decay on learning rate. Warm-up, grouped weight decay strategy and the exponential moving av- erage (EMA) are utilized. The data augmentations we adopt are Mosaic and Mixup. The batch size is set as 32. The total number of training times is 500 epochs. In"}, {"title": "4.2. Results", "content": "The method proposed in this paper is compared with models from the YOLO series, and the results are shown in Tables 1 and 2. The implementation results indicate that our proposed FA-YOLO significantly surpasses the existing mainstream YOLO series models in detection performance. Compared to YOLOv5-L, FA-YOLO has a 14.7% higher mAP, and its parameter count is only 30.7 M, approximately two-thirds that of the latter. Compared to YOLOv7, FA-YOLO has 6.2 M fewer pa- rameters, accounting for 20% of the total parameters of the FA-YOLO model, and FA-YOLO's mAP and AP 50 are 10.7 and 7.2 points higher than those of YOLOv7, respectively. FA-YOLO has only 70.3% of the parame- ters of YOLOv8-L but achieves a 0.8 point higher mAP, with almost the same throughput (batch size of 1) and GPU latency as YOLOv8-L. Compared to YOLOv9, FA-YOLO shows a 1.0% improvement in mean Average Precision (mAP), and the accuracies for detecting small,"}, {"title": "4.3. Ablations", "content": "To validate the effectiveness of our feature fusion analysis and evaluate the proposed FMDS Module and AGMF Module, we independently examined each mod- ule within FA-YOLO, focusing on mAP, AP50, APS, APM, and APL, as shown in Table 3. The results demon- strate that the FMDS Module, by implementing a more efficient dynamic feature selection and fusion method on fine-grained multi-scale feature maps, significantly enhances the detection accuracy of small, medium, and large-sized targets in complex environments, achieving a 0.4% mAP performance gain. Particularly, the de- tection accuracies for small and medium targets have improved by 2.0% and 3.1%, respectively. The AGMF Module, by integrating the outputs of the FMDS Mod- ule, TripletAttention branch, and Gated Unit branch, performs a complementary fusion of the different fea- tures captured by multiple branches to form a final high-quality feature representation, achieving a 1.0%"}, {"title": "4.4. Visualization", "content": "In this paper, we propose the FMDS Module, which achieves more efficient feature fusion through adaptive dynamic selection of fine-grained multi-scale features. Additionally, the AGMF Module combines the outputs of the FMDS Module, the TripleAttention branch, and the Gated Unit branch. By performing a complemen- tary fusion of the different features captured by these multiple branches, it effectively enhances the efficiency of feature fusion and strengthens the expressive capac- ity of the feature maps. To validate the effectiveness of these designs, this paper employs a visualization com- parison of the feature maps from the same layers of FA- YOLO and YOLOv9, as illustrated in Figure 4. Figure 4(b) shows the visualization results of the feature maps"}, {"title": "5. Conclusion", "content": "In this paper, we conduct an in-depth analysis of the limitations of the YOLO series models in feature capture and fusion, particularly identifying significant losses of important features during the feature transmis- sion and transformation process. To address this issue, we designed the FMDS and AGMF Modules and vali- dated their effectiveness through experiments.\nThe FMDS Module enhances feature fusion capa- bilities through adaptive dynamic selection of fine- grained multi-scale features, significantly improving the model's detection accuracy for various sized targets in complex environments. Additionally, the AGMF Mod- ule integrates the branches of the FMDS Module, the Gated Unit, and Triplet Attention. This integration of multiple branch features through complementary fusion"}, {"title": "", "content": "further enhances feature fusion efficiency and expres- sive capacity.\nBased on the design of the FMDS and AGMF Mod- ules, we propose a new object detection model named FA-YOLO. Compared to the latest YOLOv9, FA- YOLO shows superior performance improvements: the mean Average Precision (mAP) increase by 1.0%, AP50 by 0.6%, and AP75 by 0.3%. Particularly in the detec- tion accuracy of different sized targets, there is an im- provement of 2.0% in small-sized targets (APs), 3.1% in medium-sized targets (APM), and 0.9% in large-sized targets (APL). These results significantly demonstrate the efficiency and accuracy of FA-YOLO in object de- tection tasks."}]}