{"title": "FA-YOLO: Research On Efficient Feature Selection YOLO Improved Algorithm Based On FMDS and AGMF Modules", "authors": ["Yukang Huo", "Mingyuan Yao", "Qingbin Tian", "Tonghao Wang", "Ruifeng Wang", "Haihua Wang"], "abstract": "Over the past few years, the YOLO series of models has emerged as one of the dominant methodologies in the realm of object detection. Many studies have advanced these baseline models by modifying their architectures, enhancing data quality, and developing new loss functions. However, current models still exhibit deficiencies in processing feature maps, such as overlooking the fusion of cross-scale features and a static fusion approach that lacks the capability for dynamic feature adjustment. To address these issues, this paper introduces an efficient Fine-grained Multi-scale Dynamic Selection Module (FMDS Module), which applies a more effective dynamic feature selection and fusion method on fine-grained multi-scale feature maps, significantly enhancing the detection accuracy of small, medium, and large-sized targets in complex environments. Furthermore, this paper proposes an Adaptive Gated Multi-branch Focus Fusion Module (AGMF Module), which utilizes multiple parallel branches to perform complementary fusion of various features captured by the gated unit branch, FMDS Module branch, and TripletAttention branch. This approach further enhances the comprehensiveness, diversity, and integrity of feature fusion. This paper has integrated the FMDS Module, AGMF Module, into Yolov9 to develop a novel object detection model named FA-YOLO. Extensive experimental results show that under identical experimental conditions, FA-YOLO achieves an outstanding 66.1% mean Average Precision (mAP) on the PASCAL VOC 2007 dataset, representing 1.0% improvement over YOLOv9's 65.1%. Additionally, the detection accuracies of FA-YOLO for small, medium, and large targets are 44.1%, 54.6%, and 70.8%, respectively, showing improvements of 2.0%, 3.1%, and 0.9% compared to YOLOv9's 42.1%, 51.5%, and 69.9%.", "sections": [{"title": "1. Introduction", "content": "Object detection, a fundamental computer visual task, aims to identify object categories and locate their positions. It is extensively applied in various domains, including multi-object tracking [1, 2], autonomous driving [3, 4], robotics [5, 6], and medical image analysis [7, 8]. With the widespread adoption of transformers, researchers have developed a series of end-to-end object detection models using the transformer's encoder-decoder architecture, such as DETR [9], Conditional DETR [10], Deformable DETR [11], and DINO [12]. Although Transformer-based detectors demonstrate remarkable detection performance, they still lag behind CNN-based models in terms of speed. Over the past years, extensive research has been conducted on CNN-based detection networks, achieving significant progress [13, 14, 15, 16, 17, 18, 19]. The object detection framework has evolved from two-stage models (e.g., Faster RCNN [18] and Mask RCNN [20]) to one-stage models (e.g., YOLO [13]), from anchor-based (e.g., YOLOv3 [21] and YOLOv4 [14]) to anchor-free (e.g., CenterNet [22], FCOS [23], and YOLOX [15]). Researchers like Golnaz Ghiasi [24, 25, 26] have explored optimal network architectures for object detection tasks through NAS-FPN, and others [27, 28, 29] have investigated distillation as a method to enhance model performance. The YOLO series, based on CNN"}, {"title": "2. Related work", "content": "In current, the main real-time object detectors are the YOLO series [14, 34, 15, 35, 36, 37, 38, 39, 13, 30, 21, 40, 19, 41, 42, 43]. YOLOv1-v3 [13, 30, 21] established the initial YOLO framework, featuring a single-stage detection structure composed of a backbone, neck, and head, and utilized multi-scale branches to predict objects of varying sizes, thus becoming a representative single-stage object detection model. YOLOv4 [14] optimized the previously used Darknet backbone and introduced several enhancements such as the Mish activation function, PANet, and advanced data augmentation techniques. YOLOv5 [31], inheriting the YOLOv4 [14] framework, features improved data augmentation strategies and a wider variety of model variants. YOLOX [15] integrated Multi Positives, Anchor-free, and De Coupled Head into the model structure, setting a new paradigm for YOLO model design. YOLOv6 [39, 38] firstly incorporate reparameterization techniques by introducing the EfficientRep Backbone and Rep-PAN Neck. YOLOv7 [41] focused on analyzing the impact of gradient paths on model performance, proposed the E-ELAN structure to enhance model capabilities without disrupting the existing gradient pathways. YOLOv8 [44] built upon the strengths of previous YOLO models and integrated them effectively. YOLOv9 [36] uses GELAN to improve the architecture and training process introduced by the proposed PGI, becoming the latest generation of top-tier real-time object detectors. Although previous models employed effective feature integration methods [24, 45, 46, 47, 48, 49, 50, 51], they still exhibited certain limitations."}, {"title": "2.2. Multi-scale features for object detection", "content": "Different levels of features carry positional information of different size objects. Larger feature maps contain low-dimensional texture details and the locations"}, {"title": "2.3. Multi-branch Architectures", "content": "The Inception architecture [57, 58, 59, 60] employed a multi-branch structure to enrich the feature space, demonstrating the importance of diversified connections, various receptive fields, and combinations of multiple branches. The Diverse Branch Block [61] adopts the concept of using a multi-branch topology; however, it differs in that 1) the Diverse Branch Block is a building block that can be utilized across various architectures, and 2) each branch within the Diverse Branch Block can be transformed into a Conv, allowing such branches to be consolidated into a single convolution. Liu et al. [62] input a four-channel RGB-D image into the backbone network, subsequently obtaining saliency outputs from each minor branch (single-stream network). Chen et al. [63] utilized dual backbone networks to separately extract RGB and depth features, which were then fused using a cascaded complementary strategy (dual-stream network). Chen et al. [64] introduced a network structure comprising two independent modal backbone networks and a parallel cross-modal distillation branch, aimed at learning complementary information. However, previous multi-branch structures rarely considered the integration of convolutional branches with attention branches, leading to the excessive weighting of redundant information. The AGMF Module, by integrating gated units, the FMDS Module, and TripletAttention, is capable of generating more comprehensive and enriched feature maps."}, {"title": "3. Method", "content": "In this section, based on the previous problem analysis, we will provide a detailed explanation of the motivations and specific structures of the FMDS Module and"}, {"title": "3.2. FMDS Module Design", "content": "Although the YOLO series of models exhibit commendable detection speeds, this advantage often comes at the expense of sensitivity to detailed features. Particularly in processing small object detections or subtle changes in complex scenes, they frequently fail to capture sufficient detail information, resulting in suboptimal detection accuracy. The primary reasons are the insufficient acquisition of fine features and the lack of a feature fusion method capable of dynamically adjusting feature processing strategies according to different scenarios.\nMoreover, when integrating cross-layer features, YOLO series models typically employ a structure similar to the Feature Pyramid Network (FPN) to achieve feature fusion. However, this structure encounters issues with information loss during the upward and downward transmission of information, often leading to partial loss of detail information. This loss of detail restricts the model's ability to process information and make decisions in complex environments, thereby impacting the detection performance."}, {"title": "3.2.2. FMDS Module", "content": "The FMDS Module enhances the detection accuracy of small, medium, and large-sized targets in complex environments by implementing a more efficient dynamic feature selection fusion method on fine-grained multi-scale feature maps, as illustrated in Figure 1. Initially, the FMDS Module subdivides the input feature maps into multiple smaller regional blocks, as shown in Equation 1, enabling the model to capture the detailed features of targets of various sizes with greater precision.\n\\(X\\_\\text{Blocks} = \\text{reshape}(\\text{permute}(\\text{reshape} (X, (B, C, N, H //2,2, W //2)), (0, 2, 4, 1, 3, 5)), (-1, C, H //2, W//2))\\)  (1)\nIn this context, X represents the input feature map, B denotes the batch size, and H and W respectively stand for the height and width of the feature map. X_Blocks signifies the subdivision of the feature map into multiple smaller regional blocks. Subsequently, each regional block is independently processed by convolutional kernels of different scales, as illustrated in Equation 2. This approach not only enhances the local sensitivity of the features but also enables the model to capture more detailed spatial hierarchical information.\nX_Blocks' = \\sum_{i=1}^{3} fpw(fdw(X_Blocks, Kdw_i, S dw_i, Pdw_i), Kpw_i, Spw_i, Ppw_i) (2)\nWithin this framework, faw represents the Depthwise Convolution, where Kdw_i, Sdw_i, and Pdw_i respectively denote the size, stride, and padding of the Depthwise Convolution kernel. Similarly, fpw denotes the Pointwise Convolution, with Kpw_i, Spw_i, and Ppw_i specifying the size, stride, and padding of the Pointwise Convolution kernel. X_Blocks' represents the regional blocks obtained after being processed independently by convolutional kernels of various scales.\nSubsequently, the processed fine-grained multi-scale features are integrated, as shown in Equation 3.\nX' =reshape(permute(reshape (X_Blocks', (B,4,-1,H //2, W//2)), (3) (0, 2, 1, 3, 4), (B, -1, H, W)\nIn Equation 3, X' results from reassembling the multiple processed regional blocks into a processed feature map.\nThe integrated feature map is then concatenated with the original feature map to form a new feature map, as illustrated in Equation 4.\nXconcat = concat (X, X', dim = 1)  (4)\nIn this model, Xconcat is the feature map obtained by concatenating the original feature map with the feature map derived from Equation 3 along the first dimension.\nSubsequently, the new feature map is dynamically evaluated to determine the importance of features from different regional blocks and scales, optimizing the weight distribution of the features, as shown in Equation 5.\nX_{select} = DepthwiseS eparableConv(X_{concat},\nin\\_channels * 2, out\\_channels, K, S, P)  (5)\nIn this model, X' refers to the feature map that has undergone feature-adaptive selection. K, S, and P respectively represent the size, stride, and padding of the convolution kernel.\nThis fine-grained and multi-scale dynamic feature selection and fusion approach significantly enhances the model's ability to handle small, medium, and large-sized targets in complex environments. For small-sized"}, {"title": "3.3. AGMF Module Design", "content": "Different data characteristics, such as texture, color, and semantic content, may require distinct processing strategies. A single processing branch often fails to comprehensively capture the multidimensional features of complex data, especially when these features are intertwined and interdependent. However, a single processing branch may struggle to adapt to the variations and demands of different types of data, potentially leading to the loss of important information or insufficient feature representation. While traditional multi-convolution branches possess more robust capabilities for spatial feature extraction, they too can result in the overemphasis of irrelevant information. On the other hand, attention branches focus on key information and relationships within the data, optimizing the capability to parse global contexts. The combination of the convolution branch and the attention branch can enable the model to not only capture details accurately but also grasp essential features and trends of the overall data."}, {"title": "3.3.2. AGMF Module", "content": "The AGMF module is designed with three main parallel processing branches: the Gated Unit branch, the FMDS Module branch, and the TripleAttention branch. Each branch is responsible for capturing and processing different aspects of the data. While efficiently collaborating, these branches also maintain the module's flexibility and high performance. The overall structure of the module is illustrated in Figure2(b).\nThe Gated Unit branch regulates and controls the flow of information in the feature maps, allowing it to adaptively adjust based on the dynamic changes in the data. It filters information critical to the current task, suppresses irrelevant or redundant data transmission, and enhances the model's focus and efficiency, as illustrated in Equations 6 and 7.\nYi_{GU\\_weight} = Activation (BN (Conv2d (Y;))) (6)\nIn this configuration, Y; represents the input feature map, and Yi_GU_weight denotes the weight assigned by the Gated Unit.\nY_{i\\_GU} = Y_{i} Y_{i\\_GU\\_weight} (7)\nIn Equation 7,Yi Gu represents the output from the Gated Unit branch.\nThe FMDS Module branch effectively captures and selects multi-scale and fine-grained data features, enabling it to gather a range of feature scales from details to global aspects from the input data. The TripleAttention branch utilizes the TripleAttention attention mechanism to focus on enhancing the model's recognition and processing capabilities for key features, thereby strengthening the model's ability to identify crucial data characteristics. After each of these three branches has processed the data independently, their respective feature outputs are gathered into a fusion layer. This layer considers the importance and complementarity of each branch's outputs, integrating these features to form a final, high-quality feature representation."}, {"title": "3.4. Architecture Design of FA-YOLO", "content": "To ensure consistency in subsequent ablation experiments, FA-YOLO employs the same data augmentation strategy and hyperparameter settings as YOLOv9. The primary difference between FA-YOLO and YOLOv9 is the introduction of the FMDS Module and AGMF Module within the RepNCSPELAN4 module. These additions significantly enhance feature fusion and feature"}, {"title": "4. Experiment and Result analysis", "content": "We conducted extensive experiments using the PASCAL VOC 2007 dataset to validate the proposed FA-YOLO enhancement algorithm. All of our experiments were conducted without the use of pre-trained models; instead, all models were trained from scratch. Finally, we compared the detection performance of FA-YOLO with other mainstream models in the YOLO series on the PASCAL VOC 2007 dataset."}, {"title": "4.1.2. Implementation details", "content": "This paper adopts the settings of YOLOv9, using the same architecture and training configurations, with the exception of the RepNCSPELAN4 structure. The optimizer and other setting are also same as YOLOv9, i.e. stochastic gradient descent (SGD) with momentum and cosine decay on learning rate. Warm-up, grouped weight decay strategy and the exponential moving average (EMA) are utilized. The data augmentations we adopt are Mosaic and Mixup. The batch size is set as 32. The total number of training times is 500 epochs. In"}, {"title": "5. Conclusion", "content": "In this paper, we conduct an in-depth analysis of the limitations of the YOLO series models in feature capture and fusion, particularly identifying significant losses of important features during the feature transmission and transformation process. To address this issue, we designed the FMDS and AGMF Modules and validated their effectiveness through experiments.\nThe FMDS Module enhances feature fusion capabilities through adaptive dynamic selection of fine-grained multi-scale features, significantly improving the model's detection accuracy for various sized targets in complex environments. Additionally, the AGMF Module integrates the branches of the FMDS Module, the Gated Unit, and Triplet Attention. This integration of multiple branch features through complementary fusion further enhances feature fusion efficiency and expressive capacity.\nBased on the design of the FMDS and AGMF Modules, we propose a new object detection model named FA-YOLO. Compared to the latest YOLOv9, FA-YOLO shows superior performance improvements: the mean Average Precision (mAP) increase by 1.0%, AP50 by 0.6%, and AP75 by 0.3%. Particularly in the detection accuracy of different sized targets, there is an improvement of 2.0% in small-sized targets (APs), 3.1% in medium-sized targets (APM), and 0.9% in large-sized targets (APL). These results significantly demonstrate the efficiency and accuracy of FA-YOLO in object detection tasks."}]}