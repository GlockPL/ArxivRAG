{"title": "Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities", "authors": ["Aishik Mandal", "Tanmoy Chakraborty", "Iryna Gurevych"], "abstract": "Mental illness is a widespread and debilitating condition with substantial societal and personal costs. Traditional diagnostic and treatment approaches, such as self-reported questionnaires and psychotherapy sessions, often impose significant burdens on both patients and clinicians, limiting accessibility and efficiency. Recent advances in Artificial Intelligence (AI), particularly in Natural Language Processing and multimodal techniques, hold great potential for recognizing and addressing conditions such as depression, anxiety, bipolar disorder, schizophrenia, and post-traumatic stress disorder. However, privacy concerns, including the risk of sensitive data leakage from datasets and trained models, remain a critical barrier to deploying these AI systems in real-world clinical settings. These challenges are amplified in multimodal methods, where personal identifiers such as voice and facial data can be misused. This paper presents a critical and comprehensive study of the privacy challenges associated with developing and deploying AI models for mental health. We further prescribe potential solutions, including data anonymization, synthetic data generation, and privacy-preserving model training, to strengthen privacy safeguards in practical applications. Additionally, we discuss evaluation frameworks to assess the privacy-utility trade-offs in these approaches. By addressing these challenges, our work aims to advance the development of reliable, privacy-aware AI tools to support clinical decision-making and improve mental health outcomes.", "sections": [{"title": "Introduction", "content": "Mental disorders are highly prevalent and represent a major cause of disability worldwide. The societal, economic, and personal impacts of mental health issues make swift diagnosis and treatment essential. Current diagnostic methods involve self-reported questionnaires and clinical interviews, while treatment typically consists of multiple therapy sessions with trained therapists. This approach requires therapists to dedicate substantial time to each patient, limiting their ability to treat a larger number of individuals. Combined with a shortage of trained therapists, this often leaves many patients undiagnosed. Additionally, completing self-reported questionnaires after every therapy session places a significant burden on patients.\nThese issues have driven the development of systems aimed at automating diagnosis and assisting therapists in treating mental disorders. Therapists often rely on various multimodal cues to diagnose mental illnesses. For example, depression exhibits distinct verbal and non-verbal characteristics, such as facial expressions1\u20133, prosodic features2\u20134, and semantic patterns5. Similarly, patients with anxiety often struggle to maintain eye contact6,7, particularly during conflict-laden conversations. Speech features are instrumental in detecting Post-Traumatic Stress Disorder (PTSD)8,9, while both speech and facial features are valuable for identifying Bipolar Disorder10,11. Consequently, multimodal AI models capable of analyzing text, audio, and video data are being developed to automate diagnosis and support therapists in managing mental health conditions.\nTraining such multimodal models requires multimodal data, typically obtained from recorded therapy sessions. However, collecting such data and training models face significant challenges due to privacy concerns. Data collection must comply with regulations like the General Data Protection Regulation (GDPR)12 and the Health Insurance Portability and Accountability Act (HIPAA)13, which prohibit releasing information that could disclose a person's gender, age, or identity. Therapy session recordings inherently contain sensitive personal information, including patients' voices and facial features, which could be misused for impersonation14. To ensure privacy, datasets are kept confidential, but many patients remain reluctant to record their sessions due to insufficient privacy guarantees for both the data and the models trained on it. As a result, available multimodal datasets are often small, which leads to biased models when used for training. Such small datasets also restrict the evaluation of the models' generalizability and reliability. Furthermore, the trained model weights cannot be released, as they may inadvertently reveal private training data15\u201318. Such privacy breaches could expose patients' identities or enable impersonation, potentially worsening their mental health. These challenges significantly hinder the development and deployment of mental health AI models in real-world applications.\nIn recent years, there has been a growing interest in AI privacy. We examine privacy solutions that could be applied to develop privacy-aware AI models in the mental health domain. These solutions can be broadly categorized into two areas: (i) ensuring data privacy and (ii) ensuring model privacy. Data privacy involves modifying data to remove private information while retaining relevant mental health information19\u201325. An alternative approach is the creation of synthetic data for training mental health AI models26\u201330. Model privacy, on the other hand, focuses on privacy-preserving training methods, which enhance the robustness of models against malicious attacks31\u201337. However, implementing privacy protection methods often results in reduced model utility. Therefore, novel evaluation methods are essential to assess the privacy guarantees of these methods23\u201325,32,38\u201344 and their impact on the model's performance23\u201325, 32, 38, 45\u201350. Both automatic46,47 and human evaluation23,26,49,50 approaches are employed to analyze the privacy-utility trade-off for each method.\nIn summary, multimodal AI models hold significant potential to assist therapists and make mental illness diagnoses more accessible. However, privacy issues limit the availability of suitable datasets and, consequently, the development of robust models for real-world deployment. We discuss these privacy issues and explore potential solutions that can ensure privacy in mental health datasets and models. Additionally, we explore evaluation methods to analyze the privacy-utility trade-offs of these solutions. Figure 1 presents a schematic diagram summarizing the discussion. Finally, we recommend a privacy-aware pipeline for data collection and model training and outline future research directions to support the development of such a pipeline."}, {"title": "Current Privacy Issues", "content": "Current privacy issues in mental health datasets and models include the risk of private information leaking from both data and models to malicious actors. Privacy leakage from data prevents the public release of datasets, while leakage from models restricts the sharing of trained model weights."}, {"title": "Private information leakage from datasets", "content": "Privacy leakage from data includes personally identifiable information (PII) present in text transcripts and audio recordings of therapy sessions. Additionally, these recordings reveal the voices of patients and therapists, as well as the faces of patients. Malicious actors can also exploit the extracted audio and video features used in mental health diagnosis models to infer sensitive attributes, such as the patient's age and gender."}, {"title": "PII leakage", "content": "In the EU, personal information is protected under GDPR, which defines any information relating to an identified or identifiable natural person. An identifiable natural person is someone who can be identified, directly or indirectly, through an identifier such as a name, identification number, location data, online identifier, or factors specific to their physical, physiological, genetic, mental, economic, cultural, or social identity. Similarly, in the US, HIPAA safeguards individually identifiable health information, which includes details such as an individual's name, address, birth date, Social Security number, and records of their past, present, or future physical or mental health conditions. Many of these types of personal information are frequently discussed in therapy sessions, such as where a person lives, their age, or any mental or physical health concerns they may have. While such information can be identified and removed in structured data formats like tables, therapy sessions often involve detailed personal narratives, which can inadvertently reveal sensitive information. As a result, textual transcripts and speech recordings of therapy sessions often contain PII that could be used to identify a patient. Even with anonymization of PII, they can still show identification vulnerabilities through the use of other public datasets51."}, {"title": "Voice from audio", "content": "Speech data are classified as personal data under GDPR because they can reveal sensitive information about the speaker, including their identity, age, gender, health status, personality, racial or ethnic origin and geographical background52. Mental Health diagnosis often involves using audio features like Mel-frequency cepstral coefficients (MFCCs), Mel-spectrogram, and pitch extracted using tools like OpenSmile53. However, these features can inadvertently leak personal information, such as the patient's age and gender54. Furthermore, MFCCs can be utilized for speech reconstruction55, posing a risk of impersonation for both therapists and patients. Similarly, speech embeddings like Wav2Vec can enable voice conversion56, further compromising the privacy of patients and therapists by exposing their unique vocal characteristics."}, {"title": "Face from video", "content": "Video recordings of therapy sessions often capture patients' faces, as facial expressions and gaze during conversations are critical factors in mental health diagnosis. However, a patient's face can directly reveal their identity, raising significant privacy concerns. Mental health models typically utilize facial features extracted from deep encoder models such as ResNet57, or facial landmarks obtained through tools like OpenFace58 for behavior, expression, and gaze analysis. These features, however, are susceptible to privacy breaches. Image reconstruction is feasible using features extracted by deep models such as AlexNet59, and malicious actors can reconstruct faces from deep templates like FaceNet60 through template reconstruction attacks61. Even facial landmarks can be exploited for facial reconstruction62, potentially enabling identification and impersonation of patients."}, {"title": "Private information leakage from models", "content": "Trained models are often susceptible to leaking training data when subjected to attacks, such as membership inference attacks15, from malicious actors. Song et al.16 demonstrated that embedding models are particularly vulnerable to leaking membership information for infrequent training data inputs, which is especially concerning for small mental health datasets with a higher prevalence of rare data points. Similarly, Carlini et al.17 highlighted the issue of neural networks memorizing unique training data, which can then be extracted from the trained models. Moreover, in text, private data can be leaked through context32. This is especially true for the mental health domain, where discussing life events can indirectly leak private data. Models are also prone to exposing user information contained in the data used for fine-tuning18. This poses a significant privacy challenge to releasing models trained or fine-tuned on mental health datasets, as they may inadvertently memorize and disclose sensitive patient information."}, {"title": "Threats", "content": "The leakage of a mental health patient's private information, such as their voice or face, can lead to identification, social stigma, and exploitation. This includes risks of defamation, blackmail through deepfakes, impersonation, and misuse of biometrics, which could worsen the patient's mental health condition."}, {"title": "Identification", "content": "Private data leakage from mental health datasets can lead to patient identification and public exposure of their mental health records, resulting in workplace discrimination, social isolation, and blackmail, further aggravating their mental condition. Sensitive information, such as age, address, and gender, revealed during therapy or extracted from audio and video recordings, can uniquely identify most Americans 63. Large Language Models (LLMs) trained on therapy data are prone to privacy breaches, leaking such information15\u201318. Voice data can be exploited for identification via speaker verification systems64,65, while video data may reveal faces, enabling identification through face recognition66,67."}, {"title": "Impersonation", "content": "The leakage of voice and video data from mental health datasets enables malicious agents to impersonate patients through deepfakes, which can be audio, video, or audio-visual. Audio deepfakes use a person's voice for false speech or impersonation via voice conversion, text-to-speech, and replay attacks68\u201371. Impersonation attacks by humans mimicking speech traits also pose a risk72. Video deepfakes manipulate faces and bodies using reenactment, video synthesis, and face swaps69\u201371,73, while audio-visual deepfakes combine voice and appearance14,70. Deepfakes can be exploited for fraud, blackmail, harassment, identity theft, and other malicious activities74, causing severe psychological distress and worsening patients' mental health."}, {"title": "Addressing the Privacy Issues", "content": "Privacy concerns in mental health datasets can be addressed through data anonymization or by generating synthetic data derived from real datasets. Data anonymization involves removing PII from therapy transcripts and audio recordings, as well as applying voice and face anonymization techniques while preserving features crucial for mental health diagnosis. An alternative approach is the creation of synthetic data that mimics the real dataset without exposing specific patient attributes. Homomorphic encryption can also be used for data protection75; however, it demands significant computational resources, making it impractical in many cases76. Privacy issues arising from models trained on mental health datasets leaking patient information can be mitigated using privacy-aware training methods."}, {"title": "Data anonymization", "content": "Data anonymization involves removing PII in transcripts and audio recordings, voice anonymization in audio recordings and face anonymization in video recordings of therapy sessions to prevent identification and impersonation threats. Below, we outline approaches for anonymizing textual, audio, and visual data to ensure privacy while retaining essential information for mental health diagnosis."}, {"title": "Text anonymization by detecting and removing PII", "content": "PII in therapy transcripts, such as names, addresses, and dates, poses identification risks. Named Entity Recognition (NER) models can detect PII and replace them with synthetically generated values that align grammatically and semantically19,20. However, therapy conversations often indirectly reveal private information, making simple NER-based methods insufficient. LLMs, such as GPT-4, have shown promise in de-identifying text21, though real-world application faces challenges like data leakage through APIs and previous Language Model (LM) based models show poor generalization across datasets20,39,77. Augmenting de-identification datasets with synthetic data20,77 and specialized strategies for transcribed spoken language78 improve performance. Text rewriting79 offers an alternative but remains untested for conversational data and risks obscuring linguistic cues critical for mental health diagnosis."}, {"title": "Audio anonymization by addressing PII in speech data", "content": "Audio anonymization involves detecting and replacing PII in recorded sessions. Pipelines often use Automatic Speech Recognition (ASR) to transcribe audio, followed by NER-based PII detection and redaction. Approaches include replacing PII segments with silence80, white noise or beeps81. But it makes speech unnatural. Therefore, a better approach is to use fictional content from the same category to replace PII and convert it to speech using text-to-speech or voice conversion81. However, this approach modifies the entire audio. Flechl et al.22 proposed splicing matching audio fragments to generate the PII replacement and only modify the PII segment in the audio."}, {"title": "Voice anonymization for speaker privacy", "content": "Voice anonymization aims to protect speaker identity in data used for automatic speech and emotion recognition23,24. Automatic speaker verification (ASV) systems use speaker representations like x-vectors for verification. Thus voice anonymization techniques include replacing speaker x-vectors with public x-vectors, although this reduces diversity as well as struggles with language change82. Orthogonal householder neural networks82 tackle this by choosing suitable public x-vectors for maintaining diversity. However, speaker information is still present in pitch and audio bottleneck features83 (a low-dimensional phonetic representation extracted from an intermediate layer of an ASR model). To address this, bottleneck features can be quantized84 or perturbed with noise for differential privacy (DP)85. Features from pre-trained models like HuBERT86 and OpenSmile53 also require research for privacy-preserving extraction. Miao et al.45 benchmarked the Multi-Speaker Anonymization (MSA), crucial for therapy recordings."}, {"title": "Face anonymization for visual privacy", "content": "Face anonymization prevents identification through video recordings. Tools like Face-Off87, LowKey88, Foggysight89, and FAWKES\u2070 obfuscate faces in images but may fail against adaptive face recognition systems91. AI stylization92 provides another alternative for face obfuscation in images while maintaining emotions of the person, crucial for mental health applications. Face anonymization in videos can be performed through applying image face obfuscation methods in every frame. However, these will be costly; therefore, specialized video face anonymization methods like FIVA93 are more suited for mental health datasets. Other methods of image obfuscation include extracting identity representation from the image, adding noise for DP guarantees and reconstructing the image94. Although DP-based methods for video anonymization focus on object indistinguishability to protect human identity95, its direct applicability for preventing facial recognition in therapy videos is unclear. These methods can also introduce demographic biases41. While useful attributes like emotion detection remain unaffected by obfuscation, detection of sensitive traits such as age and gender are also unaffected25,96, necessitating targeted anonymization methods for mental health applications."}, {"title": "Synthetic data generation", "content": "Synthetic data, generated using AI models, mirrors real data but does not belong to actual individuals, ensuring privacy. It offers a solution to data scarcity and diversity challenges in mental health datasets, enabling effective AI training while protecting sensitive information."}, {"title": "Synthetic text generation", "content": "Textual synthetic data generation includes generating therapy transcripts with multi-turn dialogues. This is addressed by datasets like SoulChat26 and SMILE27, generated by converting single-turn psychological Q&A into multi-turn conversations via ChatGPT. CPsyCoun47 used LLMs to generate multi-turn dialogues from counseling reports. Wu et al.27 employed ChatGPT for zero-shot and few-shot generation of PTSD interview transcripts, improving PTSD diagnosis when combined with real datasets. SAPE28 used genetic algorithms for creating better prompts to enhance synthetic therapy data generation. Role-playing setups, like those in Patient-\u03a828 and CACTUS29, simulate patient-psychiatrist interactions by incorporating cognitive models and contextual details, improving realism and utility. Other synthetic text generation methods give theoretical guarantees using DP with language models99,100."}, {"title": "Synthetic multimodal data generation", "content": "Given the superior performance of multimodal models in mental health diagnosis, synthetic multimodal data generation is critical. Mehta et al.49 proposed a unified framework for speech-gesture synthesis using text input, complementing textual generation methods. Style-Talker101 integrates speech styles and chat history to generate conversational responses, supporting simulations of patient-psychiatrist dialogues in text and audio. ConvoFusion50 adds gesture generation from text and audio, enabling text, audio, and video simulation. However, the sequential generation of modalities introduces cumulative noise and computational inefficiencies. Ng et al.102 developed a method for generating photo-realistic avatars with gestures for dyadic conversations, addressing single-turn limitations but still relying on sequential modality generation. Chu et al.30 created synthetic patients for medical training, producing video outputs by combining GPT-4, text-to-speech, and video generation models. While promising, these methods remain computationally intensive and lack specific applications for mental health diagnosis."}, {"title": "Privacy-aware training", "content": "Privacy-aware training methods are essential for developing AI models in mental health, ensuring that private and sensitive data is protected in trained models while maintaining model utility."}, {"title": "Differential Privacy", "content": "Differential Privacy103 provides theoretical privacy guarantees and is widely used to train privacy-aware models through Differentially-Private Stochastic Gradient Descent (DP-SGD)31. However, DP-SGD often suffers from poor performance in language modeling tasks35. In mental health, contextual information can inadvertently reveal private data32. Context-aware DP methods32,33 mitigate such issues by accounting for contextual leakage during training. Fine-tuning LLMs on private mental health data requires differentially private fine-tuning techniques34,35. Beyond text, DP methods can be applied to conformer-based encoders for audio36 and to models like ResNet for image and video data37."}, {"title": "Federated learning", "content": "Federated Learning (FL) is another popular privacy-preserving training method where training is distributed and locally trained model gradients are communicated to a central server104. This is useful for combining mental health data from different medical institutes. However, FL, on its own, provides limited privacy and is vulnerable to attacks105\u2013107 and leaking data through the local model weights and gradients108. Therefore, it is often combined with Local Differential Privacy (LDP) to improve privacy guarantees 108, 109."}, {"title": "Confidential computing with Trusted Execution Environment (TEE)", "content": "Confidential computing aims to safeguard the data during processing. It loads the data and the model in a TEE where they are protected from unauthorized access and modification75. However, it requires more computational resources and special hardware, limiting its wide-spread usage76."}, {"title": "Autoencoders for privacy preservation", "content": "Autoencoders are commonly used in speech models to extract latent representations containing linguistic and paralinguistic information while obfuscating speaker identity. These models are trained to maximize downstream task performance, such as mental health prediction while minimizing speaker classification accuracy. Ravuri et al.110 demonstrated the use of autoencoders to retain depression severity prediction performance while reducing speaker classification accuracy. Similarly, Pranjal et al.111 used autoencoders to transform physiological, acoustic, and daily life measurements for anxiety detection on the TILES 2018 dataset while reducing identification risks."}, {"title": "Evaluating Privacy-aware Alternatives", "content": "Ensuring privacy in AI models for mental health diagnosis is essential to protect patient confidentiality. However, this often comes at the cost of reduced performance in downstream diagnostic tasks. This section discusses methodologies for evaluating privacy-utility trade-offs across three key areas: data anonymization, synthetic data generation, and privacy-aware training."}, {"title": "Data anonymization", "content": "Data anonymization techniques focus on removing or masking private information across text, audio, and video modalities. Effective anonymization should minimize privacy risks while preserving the diagnostic utility of the data. Evaluation can be categorized into privacy and utility metrics."}, {"title": "Privacy evaluation", "content": "Privacy of PII detection and removal methods used for therapy transcripts is evaluated using standard metrics such as precision, recall, accuracy and F1-score19\u201321,39,77,112, to measure the ability to classify PII words. However, indirect PII leakage in therapy sessions necessitates testing against adversarial re-identification models 38, which can be enhanced using LLMs for improved privacy evaluation. It is also important to test vulnerabilities that arise from any related public dataset51. The privacy of PII removal methods for audio recordings is similarly measured using PII detection metrics22, 80, 81. Voice anonymization techniques are evaluated using the Equal Error Rate (EER)23,24, 82, 84, 85 For False Accept Rate (FAR)45 of ASV systems, where higher EER and lower FAR indicate better privacy. Robustness against attack models23,24,82 and unlinkability40,85 further assess privacy capabilities. For face anonymization in video recordings, privacy is tested by evaluating face recognition systems against anonymized faces87\u201390,93, along with leakage of attributes like age and gender25. Robustness against facial reconstruction attack should also be tested93. Demographic fairness is crucial, as anonymization methods may disproportionately affect certain groups41. Finally, multimodal data can exacerbate privacy risks. Thus, multimodal re-identification models are essential for holistic privacy evaluation across text, audio, and video."}, {"title": "Utility evaluation", "content": "Utility evaluation of PII removal in text measures model performance on downstream tasks using PII-removed transcripts. Sanchez et al.112 assessed utility by calculating the proportion of information preserved, while Morris et al.38 used metrics like masked word percentage and information loss. For therapy transcripts, the utility should focus on preserving mental health-relevant information. For PII-removed audio, utility is evaluated using metrics like substitution, hallucination, and omission percentages81. Such calculations should be limited to mental health-relevant segments. Additional human evaluations measuring naturalness, style consistency, and relevance should also be performed. Finally, models trained on PII-removed audio should be tested on mental health-related tasks to gauge utility. Voice anonymization utility is assessed through intelligibility (via Word Error Rate23,24,45,82), emotion preservation (using emotion recognition performance24), intonation preservation (via pitch correlation23), and diversity (Gain of Voice Distinctiveness23,82). Human evaluations of naturalness and intelligibility23, as well as automatic measures like Predicted Mean Opinion Score (PMOS)45, further refine utility assessment. Utility should also evaluate models trained on anonymized voices. Since therapy data is multi-speaker, multi-speaker anonymization requires utility evaluation through Diarization Error Rate (DER)45. For face anonymization, utility is tested by evaluating anonymized videos on downstream tasks, such as emotion detection or mental health diagnosis25,92."}, {"title": "Synthetic data generation", "content": "Synthetic data is generated by models trained on real-world data and may inadvertently reveal sensitive information if the models overfit, particularly when the real-world dataset is small (which is the case for most mental health datasets). Overfitting increases the risk of privacy violations, while overly generic synthetic data can reduce utility. Thus, evaluating privacy-utility trade-offs in synthetic data generation is crucial."}, {"title": "Privacy evaluation", "content": "Privacy evaluation tests synthetic data robustness against membership and attribute inference attacks42\u201344,113. Membership inference attacks identify if an individual is part of the real dataset, with outliers being especially vulnerable. This is critical for mental health datasets, where diverse, small participant groups make outlier protection a priority. Metrics like privacy gain44 and outlier similarity43 are used for outlier privacy evaluation. Ensuring zero duplication of real sessions in synthetic data, measured through reproduction rate43, is essential. Further memorization, overfitting and identification metrics include memorization coefficients114 and \u025b-identifiability115. Attribute inference attacks exploit known attributes to deduce sensitive details like mental health conditions. Additional metrics include distance-based metrics42, 43."}, {"title": "Data quality and utility evaluation", "content": "Synthetic data quality is assessed through faithfulness (similarity to real-world data) and diversity (lexical, semantic, and topic variation)46,97. Faithfulness is measured via vocabulary overlap and semantic consistency46. However, these need to be supplemented by expert evaluations. Experts rate transcripts on naturalness, empathy, helpfulness, and safety26. Other metrics include four-level rating systems116. For multimodal data, ratings assess naturalness of speech, gestures, and their coherence and contextual plausibility49,50. To reduce reliance on human evaluations, LLMs can automatically rate synthetic data on attributes like professionalism, comprehensiveness, authenticity and safety47, or use psychological measures like the Working Alliance Inventory46. The utility is also evaluated through model performance on downstream tasks27,48 using synthetic data or synthetic data augmentation with real data. Ensuring synthetic data utility in mental health applications involves preserving relevant features while maintaining diversity and faithfulness."}, {"title": "Privacy-aware training", "content": "Mental health AI models must employ privacy-aware training methods. However, such methods introduce noise, necessitating a privacy-utility evaluation to identify optimal approaches that balance privacy and utility."}, {"title": "Privacy evaluation", "content": "Privacy evaluation tests the robustness of trained models against malicious attacks. For language models, this includes measuring exposure through canary insertion and membership inference attack accuracy32. Another aspect is assessing whether model embeddings used for mental health predictions inadvertently reveal private attributes like location, age, gender, or identity33,110,111. Most privacy-aware training methods utilize DP-SGD31, where the privacy guarantee is theoretically quantified by \u025b-value34\u201337 (which determines the distance within which errors are considered to be zero in SGD). If FL is involved in the training process, its robustness against FL specific attacks105\u2013107 should be determined along with leakage through local model weights and communicated gradients108. Evaluating privacy through these methods ensures the robustness of training techniques."}, {"title": "Utility evaluation", "content": "Utility evaluation examines model performance on downstream mental health diagnosis tasks32\u201335,37,110,111. However, differential privacy training often exacerbates model unfairness117. Thus, utility evaluations must also consider the performance of privacy-aware models on culturally and demographically diverse mental health datasets."}, {"title": "Recommendations", "content": "Based on the advances and pitfalls of existing studies, we recommend a comprehensive workflow for developing privacy-aware mental health AI models and datasets. The workflow involves data collection, data anonymization as well as synthetic data generation, privacy-utility evaluation of the data, privacy-aware model training, and evaluation of the privacy-utility trade-off in the training process. Figure 2 shows the recommended pipeline."}, {"title": "Data collection", "content": "The first step in building mental health Al systems is collecting video recordings of therapy sessions. However, due to the sensitive nature of this data, there is a high risk of privacy breaches, including identification and impersonation attacks. To mitigate these risks, explicit, informed consent from patients is mandatory. The consent form should specify that the data will be anonymized, stored securely, and used exclusively for research purposes. Additionally, an ethics committee must review and approve the data collection and storage procedures to ensure compliance with privacy regulations and ethical standards. The audio recording should be performed using two channel recorder, such that therapist and patient voice can be untangled easily. The video recording should be focused on the patient showing their full face and posture so that facial expressions, gaze and body language can be studied. The recorded sessions should be transcribed by involved researchers or through local ASR systems to assure privacy."}, {"title": "Data anonymization and synthetic data generation", "content": "Once collected, the data must be anonymized or replaced with synthetic data to protect patient privacy. This decision is based on the dataset size and diversity. Training transformer models requires a large amount of diverse data for generalization. Therefore, if a dataset contains a small number of participants or less diversity among participants, synthetic data should be generated to improve the data utility. The generated data can also be augmented with real datasets for training models. In case the dataset already contains a large number of diverse data points, only data anonymization suffices. Anonymization of real data involves processing text, audio, and video to remove or replace PII. For text transcripts, LLMs can be employed to detect and redact PII21. However, their performance on small datasets may be limited, necessitating training on augmented datasets for improved generalization20,77,78. For audio recordings, PII can be replaced by synthesizing matching audio segments22, and multi-speaker anonymization techniques can be used to disguise voices while preserving conversational dynamics45. Video recordings should undergo face anonymization using advanced methods such as FIVA93. Alternatively, synthetic data can be generated to ensure it does not relate to real individuals. This can be achieved using multimodal LLMs118 capable of role-playing as therapists and patients46. Alternatively, these models can be used to generate realistic therapy sessions through zero-shot or few-shot prompting techniques27, ensuring the generated data bears no resemblance to actual individuals."}, {"title": "Privacy-utility evaluation of anonymized and synthetic data", "content": "To ensure the efficacy and safety of the data, it is necessary to evaluate the privacy-utility trade-off after anonymization or synthetic data generation. Privacy evaluation of anonymized data includes testing re-identification risks using adversarial models38, other related datasets51 and measuring the effectiveness of techniques like multi-speaker anonymization through metrics such as EER23,24,82,84,85 and FAR45 in speaker verification. Voice anonymization should also be evaluated for unlinkability40,85 and robustness against attacks23,24,82. For video recordings, the privacy risks can be assessed using face verification systems and face reconstruction attacks to determine the degree of obfuscation. Synthetic data must be rigorously tested against membership inference attacks and attribute inference attacks to ensure it does not inadvertently reveal details of the real dataset42\u201344,113. Outlier leakage is another critical concern, especially in mental health datasets, where outliers are more prevalent due to the diversity and small size of participant groups. Metrics such as privacy gain44, outlier similarity43, and reproduction rate43 are effective for evaluating these risks. These empirical privacy evaluations should be performed on different languages, cultures and demographics to obtain a more holistic idea about the privacy guarantees. Moreover, multimodal privacy measures need to be developed to understand and test cross-modal vulnerabilities. The empirical privacy measures should also be accompanied by theoretical guarantees similar to \u025b-value in DP. Utility evaluation should assess the usefulness of the data for mental health diagnosis tasks, focusing on information preservation38,112, intonation preservation23, conversational diversity23,82, naturalness23,45, and emotional feature retention24,92. LLMs can be leveraged to automatically evaluate the utility of synthetically generated data on dimensions such as comprehensiveness, professionalism, authenticity, and safety47 or psychological measures like the working alliance inventory46."}, {"title": "Privacy-aware model training", "content": "Privacy-preserving methods, particularly differential privacy, are crucial during this stage. Mental health models either consist of pre-trained models fine-tuned on mental health data or use pre-trained models to extract embeddings to train lightweight modality fusion layers for specific mental health diagnosis tasks. In the first approach, pre-trained models should use differential privacy-based fine-tuning methods34,35 to ensure privacy. In the second approach, fusion layers should be trained with DP-SGD31 to ensure no privacy leakages from the trained layers. For even greater privacy Local Differential Privacy for Federated Learning (LDP-FL)108, 109 can be used. This can be especially useful when datasets from different institutes are involved and they are required to be stored in the collected institutes for privacy."}, {"title": "Privacy-utility evaluation of privacy-aware training", "content": "Finally, the trained models must be evaluated for their privacy-utility trade-off. Privacy measurements include testing the models against membership inference attacks32 and analyzing the theoretical guarantees provided by the \u025b value in differential privacy34\u201337. To assess utility, the models should be evaluated on downstream mental health diagnosis tasks32\u201335, 37,110,111. Additionally, testing on diverse datasets can help identify any biases or disparities amplified during the training process117."}, {"title": "Prospects", "content": "Multi-Speaker Anonymization (MSA). While Miao et al.45 provided a benchmark for MSA, they assume weak attack models where the attacker does not have knowledge about the used anonymization scheme. However, in real-life situations, the attacker might possess knowledge of the anonymization strategy, and thus, privacy evaluation of MSA should be performed with stronger adversaries and subsequently develop better anonymization strategies. Moreover, overlapping segments in multi-speaker conversations like therapy sessions present another vulnerability that can be utilized by attackers. While Miao et al.45 tested the ability of attackers to infer speakers from overlapping segments, in real-life situations, attackers might possess the ability to separate the speakers in overlapping segments and identify them. This shows the need to create stronger MSA schemes.\nAnonymization in video. Current video anonymization methods show vulnerability to leaking the gender and age of people even after face obfuscation25. Mental health datasets might contain very few participants from a certain gender or age group, thus leaking such private information could lead to identification. Recording the body language of patients could help in mental health diagnosis; however, it can also reveal the gender of the patient if only face anonymization is performed96. Moreover, current methods are also prone to demographic unfairness41. Thus it is essential to develop fair and improved video anonymization techniques that can prevent leakage of private information like age and gender.\nTheoretical guarantees in data anonymization. While we discuss various data anonymization processes for text, audio and video modalities, most of them do not provide any theoretical guarantees like DP provides in privacy-aware model training. In text modality, word-level or sentence-level perturbations through DP provide theoretical guarantees79. However, they significantly reduce the utility of the text79, necessitating better anonym"}]}