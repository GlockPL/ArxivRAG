{"title": "GPUs, CPUs, and... NICs: Rethinking the Network's Role in Serving Complex Al Pipelines", "authors": ["Mike Wong", "Ulysses Butler", "Emma Farkash", "Praveen Tammana", "Anirudh Sivaraman", "Ravi Netravali"], "abstract": "The increasing prominence of AI necessitates the deploy- ment of inference platforms for efficient and effective man- agement of Al pipelines and compute resources. As these pipelines grow in complexity, the demand for distributed serving rises and introduces much-dreaded network delays. In this paper, we investigate how the network can instead be a boon to the excessively high resource overheads of AI pipelines. To alleviate these overheads, we discuss how resource- intensive data processing tasks a key facet of growing Al pipeline complexity are well-matched for the compu- tational characteristics of packet processing pipelines and how they can be offloaded onto SmartNICs. We explore the challenges and opportunities of offloading, and propose a research agenda for integrating network hardware into AI pipelines, unlocking new opportunities for optimization.", "sections": [{"title": "INTRODUCTION", "content": "Al is now used to guide or automate decisions in applica- tions across societal sectors, with use cases in healthcare, fi- nance, Internet services, and more [3, 12, 13, 21, 30, 37]. The systems community has played a large role in realizing this revolution. Indeed, the backbone of this success is the infer- ence platforms that support running ML models, coordinat- ing user requests, and managing compute resources [15, 20, 23, 48, 62].\nThe most recent wave of AI innovation has brought pipelines whose increased complexity heightens the need for distributed serving. First, models and request rates continue to grow and often cannot be supported by single nodes/GPUs, motivat- ing efforts to parallelize [32, 33, 49] (e.g., tensor parallelism) or disaggregate tasks (e.g., LLM prefill and decode) across nodes [38, 40, 61]. Second, the number of inference itera- tions required to respond to a single request is increasing - e.g., denoising for image generation [8, 32] or inference- time scaling [19] - necessitating distribution to parallelize inference steps for acceptable latency. Lastly, compound AI [28, 42, 60, 63], and agentic workflows [2, 11, 34, 59] that fuse together multiple models and non-parametric components (e.g., vector databases, verifiers) are routinely scheduled across multiple nodes given the varying resource needs of constituent components.\nIn this paper, we argue that the push towards distributed serving actually presents new opportunities for networks to aid resource-constrained serving platforms, rather than serving as solely a headache with transmission delays. In particular, while programmable network hardware (e.g., Smart- NICs, which is the focus of this work) is ill-suited for costly model inference, the growing number of surrounding data processing tasks in emerging pipelines presents a more fa- vorable picture (\u00a72). These tasks, which can push CPU/GPU utilization to nearly 100% (\u00a72), are responsible for transform- ing, formatting, and filtering data between and within model inferences, e.g., image normalization, tokenization, and KV cache compression. Our main observation is that such tasks often consist of a bounded number of deterministic steps and rely on large, but simple, data structures attributes that make them well-suited for the highly parallel nature of SmartNIC packet processing pipelines. Moreover, the close proximity of SmartNICs to traditional CPU/GPU servers en- sures on-path processing while data is in transit, without introducing new hops.\nCapitalizing on this promise, however, raises several ques- tions: Which functions are amenable to network hardware offload? How can such tasks be packaged to fit within the tight compute and resource constraints of SmartNICs, with- out harming fidelity or slowing down network traffic? We investigate these key questions and motivate a research agenda around solving them in the following sections: \u00a72 taxono- mizes Al pipeline tasks and identifies those suitable for of- fload; \u00a73-4 build on this, presenting fundamental challenges with implementing offloads, as well as pathways forward"}, {"title": "2 WHAT TO OFFLOAD AND WHY?", "content": "Model serving involves more than solely invoking neural networks; serving tasks also include detailed logging, man- agement of hardware resources, pre/post-processing of data and results, maintaining (or transferring) intermediate state, loading models, and more. We explore the resource needs of such tasks, and identify those that are amenable to (and worthwile for) network hardware offload below.\nMemory management. This class of tasks focus on man- aging where and how models and internal state are stored. These techniques often either coordinate and change how memory is used for neural networks on GPUs or use host memory to extend the limited memory available on GPUs [20, 42]. Due to the distance from the GPUs and the low memory available on network accelerators, memory man- agement is not deemed a viable candidate for offload.\nScheduling and batching. Serving systems embed algo- rithms to determine when and how the inference engine is invoked on inputs [10, 24, 55]. Scheduling techniques are often difficult to parallelize because these functions involve conditional logic (e.g., sorting requests and predicting mem- ory/compute overheads), which creates dependencies that force sequential execution, leading to resource underutiliza- tion on network accelerators.\nModel execution. The execution of the neural network in- ference on incoming data inputs. Neural networks incur high memory overheads that far exceed the memory capacities of network hardware. Moreover, serving platforms are typ- ically already equipped with GPUs and other accelerators that have already been specially designed and heavily opti- mized for the compute tasks in model execution.\nIn contrast to these jobs, we find that the data process- ing operations which transform, filter, and format data are most suitable for offload. These tasks can run both around model inferences (e.g., preparing data as input to a model such as tokenization or image normalization) or within a model inference (e.g., compressing KV cache state prior to transmission in distributed LLM serving [61]). Importantly, such functions can be executed inline while intermediate data is in transit between two nodes and do not require ad- ditional transfers to the SmartNIC. Moreover, many of these functions employ a bounded number of deterministic steps and rely on large, but simple, data structures, making them amenable to the pipeline and data parallelism that is funda- mental to network hardware. For instance, image normaliza- tion ingests a large matrix containing and independently ap- plies subtraction and division operations to each pixel value. Similarly, KV cache compression algorithms often process large tensors by multiplying their values by a scaling factor for quantization.\nPotential benefits. To understand the value of offloading data processing tasks, we profiled resource usage for sev- eral representative AI pipelines that encompass key emerg- ing paradigms including multi-model agents (both language and vision) and various distributed ML serving paradigms [9, 22, 35, 41]. All experiments were run with the TorchServe serving platform [39] across two 32-core 64-hyperthread 256- GB DRAM machine (AMD EPYC 7543P) with 4 NVIDIA A6000 GPUs. Across these pipelines, we observe median and max- imum CPU utilization values of 86% and 98%, respectively, even when GPUs are fully utilized for model inference. Cru- cially, were data processing tasks to be offloaded to Smart- NICs, these values could be drastically reduced by 32% and 70%, drastically improving end-to-end response latencies that prior work has show to suffer from such resource overheads and contention [1, 17, 25]."}, {"title": "3 CHALLENGES OF SMARTNIC OFFLOADING", "content": "To offload a program onto a network accelerator, we first use a specification of the data processing function. This function ingests a data sample and applies transformations to mod- ify it. We seek to generate a semantically equivalent imple- mentation of this function that runs on the SmartNIC. As an example, one popular data processing function is Resize, which performs weighted averages of an image's input pix- els to resize it for downstream CNN processing. However, generating such an implementation is not straightforward. It must realize the behavior of the specification while pro- cessing one packet of data at a time rather than the entire data sample all at once. Moreover, the implementation is subject to a number of constraints such as packet size, re- source limits (e.g., BRAM), and SLOs. In this section, we dive deeper into these challenges and discuss avenues to over- come them.\n3.1 Finite Packet Size\nChallenge. Packet processing pipelines don't have access to the full data context all at once. Since data is divided into packets, only one chunk of data can be processed at a time. E.g., Ethernet frames can be up to 1500 bytes in size, whereas AXI transactions, commonly used in switches and FPGAs,"}, {"title": "3.2 Compute Overheads", "content": "Challenge. Data processing operations that perform series of arithmetic transformations often require compute-intensive operations that are difficult to run on network accelerators. E.g., it is common for models to require input normaliza- tion which involves division and floating point operations. Some network devices do not support these computations while others can but at the cost of high resource overheads and significantly reduced packet processing performance.\nOpportunity. A big contributor to the abstraction gap be- tween the data processing specification and the capabilities of network hardware is that many specified operations are difficult to run directly on the device. Network hardware is much better suited for parallel rule-based pattern match- ing and is why they have excelled in traditional network- ing tasks such as packet switching, intrusion detection, and header parsing [27, 44]. Expressing data processing opera- tions in this matching paradigm is key to bridging this gap.\nWe observe that specific operations and constant operands for compute-intensive operations are often known a priori and do not change dynamically. This property is present in image processing functions that perform deterministic arith- metic computations on pixel values and text replacements for LLM prompts. Because of this observation, we do not"}, {"title": "3.3 Limited Memory", "content": "Challenge. Network accelerators often have limited on-chip memory; this has a big impact when a pipeline requires em- ploying many data processing functions in tandem or when a function requires excessively large lookup tables. FPGA- based SmartNICs in particular often only have several tens of MB of high-speed BRAM available which makes it diffi- cult to store look-up tables, packet data, tokenizer vocabu- laries, and other requisite application data. Although some FPGAs feature on-board DRAM and HBM, read and write operations with these memory types incur significantly higher latencies. Additionally, they offer limited support for paral- lel lookups compared to BRAM. Given BRAM's character- istics, its usage is strongly encouraged for optimal perfor- mance.\nOpportunity. As prior works have observed, data across re- quests and packets exhibit a significant degree of similarity. In video data, neighboring frames within the same temporal window will have similar content [50]. Within a frame, the pixel distribution is often skewed \u2013 our analysis of the Kinet- ics video dataset [29] reveals that the top 10 pixels consume appear almost 20% of the time. In LLMs, users edit and refine prompts causing them to contain a lot of redundant text [18]. To perform more complex tasks, users or applications often include highly similar contexts with input prompts that in- clude domain knowledge [55, 57].\nDue to this redundancy in data content, we can use off- chip memories (e.g., DRAM and HBM for FPGAs) for the less frequently accessed table entries while using faster on- chip memory (e.g., BRAM) for the entries most relevant to current requests. For instance, a lookup table that cannot fit entirely in BRAM can move less frequently accessed ta- ble entries to other, more distant memory types. In doing so, large lookupt tables can still be employed while maxi- mizing the number of BRAM accesses, thereby significantly reducing the number of cycles needed to perform reads and writes."}, {"title": "3.4 Parallel Processing", "content": "Challenge. While network accelerators are much more en- ergy efficient compared to traditional processors, they also have much lower clock speeds. To maximize performance wins and best leverage the hardware capabilities, it is im- perative to employ data and pipeline parallelism. However,"}, {"title": "4 EXAMPLE OFFLOADS", "content": "In this section, we exemplify the challenges and solution di- rections from \u00a73 using three widely-used example data pro- cessing tasks.\n4.1 Image Normalization\nThe Normalize function subtracts each pixel value by the mean of its channel and then divides it by the standard de- viation of its channel. We denote I and O as the input and output images respectively and c as the color channel.\n$O[c, i, j] = \\frac{I[c, i, j] \u2013 mean[c]}{std[c]}$\nNote that mean and standard deviation for each channel are normally provided a priori and do not need to be com- puted.\nCompute overheads. To reduce compute overheads, our design uses one lookup table for each color channel. Each table will contain 255 entries, with each entry mapping a color intensity value to the pre-computed normalized value. This results in significant compute savings as a lookup and a write typically incur 1 clock cycle for each operation whereas division can consume many 10s of clock cycles for just a sin- gle operation.\n4.2 Bilinear Interpolation\nBilinear interpolation is a popular algorithm used for many image processing functions such as resizing, rotating, and texture mapping. For each output pixel, it finds four corre- sponding pixels in the input image and computes the follow- ing result.\n$0[x, y] = (1 \u2212 \\Delta x)(1 \u2013 \\Delta y)I[X_1, Y_1]+\\Delta x(1 \u2013 \\Delta y)I[x_2, Y_1]+(1 \u2013 \\Delta x) \\Delta yI[X_1, Y_2]+\\Delta x \\Delta yI[X_2, Y_2]$"}, {"title": "4.3 Text Processing", "content": "Tokenization is an essential function for LLMs that maps an input text prompt into a series of tokens. Below is an example input prompt and its tokenized sequence.\nPrompt: This is an example of an input prompt\nTokens: [\"This\", \" is\", \" an\", \" exam\", \"ple\", \" of\", \" an\", \" input\", \" prom\", \"pt\"]\nPacket processing. Tokenization is challenging when pro- cessing one chunk of data at a time because the input prompt will be split at every 32 bytes, which is problematic because tokens will be interrupted. Our solution is to prepend the last several characters of each chunk to the chunk in the fol- lowing packet so that there is textual overlap between neigh- boring chunks. The amount of characters stored is equal to the maximum token length. When two neighboring chunks are tokenized, the same token will appear twice surround- ing any tokens that were incorrectly generated. Once these are removed, we get the correct tokenized sequence. Below, brackets denote the additional text that was added from the previous chunk.\nPrompt in chunks: [\"This is an e\", \"[ an example of an\", \"[ of an]input prompt\"]\nTokens: [\"This\", \" is\", \" an\", \" e\", \" an\", \"exam\", \"ple\", \" of\", \" an\", \" input\", \" prom\", \"pt\"]\nCorrected tokens: [\"This\", \" is\", \"an\", \"e\", \" an\", \"exam\", \"ple\", \"of\", \"an\", \" of\", \" an\", \" input\", \" prom\", \"pt\"]\nWe observe that parts of the packet metadata remain largely unchanged or are unneeded for data processing functions. This space can be repurposed to hold the additional tokens until they are de-duplicated at the end of the pipeline."}, {"title": "5 ROADMAP: AUTOMATIC COMPILATION OF SMARTNIC OFFLOADS", "content": "As AI pipelines become more and more complex, seamless and efficient offload tools will free AI developers from the burden of worrying about the underlying details of these smart network hardware devices. Moving forward, our goal is to automate the generation of data processing tasks for SmartNICs, capitalizing on the potential to alleviate resource tensions illustrated above. This requires addressing several key questions such as: Can we build a compiler that success- fully maps a data processing specification to a semantically equivalent implementation that runs within the within re- source constraints? How do we support a diverse set of dif- ferent data processing operations and hardware backends?\nAt a higher level, we envision such a compiler to run alongside updated distributed scheduling engines in AI serv- ing runtimes. As done today, such platforms can ingest graph- like specifications of the AI pipeline to run, outlining both the requisite components and the data/control-level coordi- nation between them [34]. From there, our proposed com- piler would be responsible for analyzing the pipeline along two dimensions to produce a task layout across available"}, {"title": "6 RELATED WORK", "content": "Reducing CPU overheads. Several works acknowledge and alleviate CPU bottlenecks in ML workloads and focus on re- source scaling, profiling, and pre-processing acceleration of training pipelines [6, 7, 16, 17, 25]. In contrast, our focus is on inference pipelines that differ in 2 ways: (1) they have strict SLO requirements, and (2) they feature multiple ML models with their associated tasks, rather than a single model. DALI [5] is a library for offloading pre-processing operations onto GPUs; instead, owing to GPU oversubscriptions common at the edge [45, 56], we target offloading pipeline components to non-traditional (network) accelerators for AI.\nNetwork hardware offload. Several works have leveraged network hardware for executing machine learning models [47, 52-54, 58] or performing gradient aggregation [31, 36] in networks. Instead of offloading model inference itself as these prior works do, we focus on offloading a growing list of CPU- based tasks that sit in the interstices of model inference calls in vision pipelines."}]}