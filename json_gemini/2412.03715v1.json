{"title": "PathletRL++: Optimizing Trajectory Pathlet Extraction and Dictionary Formation via Reinforcement Learning", "authors": ["GIAN ALIX", "ARIAN HAGHPARAST", "MANOS PAPAGELIS"], "abstract": "Advances in tracking technologies have spurred the rapid growth of large-scale trajectory data. Building a compact collection of pathlets, referred to as a trajectory pathlet dictionary, is essential for supporting mobility-related applications. Existing methods typically adopt a top-down approach, generating numerous candidate pathlets and selecting a subset, leading to high memory usage and redundant storage from overlapping pathlets. To overcome these limitations, we propose a bottom-up strategy that incrementally merges basic pathlets to build the dictionary, reducing memory requirements by up to 24,000 times compared to baseline methods. The approach begins with unit-length pathlets and iteratively merges them while optimizing utility, which is defined using newly introduced metrics of trajectory loss and representability. We develop a deep reinforcement learning framework, PATHLETRL, which utilizes Deep Q-Networks (DQN) to approximate the utility function, resulting in a compact and efficient pathlet dictionary. Experiments on both synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art techniques, reducing the size of the constructed dictionary by up to 65.8%. Additionally, our results show that only half of the dictionary pathlets are needed to reconstruct 85% of the original trajectory data. Building on PATHLETRL, we introduce PATHLETRL++, which extends the original model by incorporating a richer state representation and an improved reward function to optimize decision-making during pathlet merging. These enhancements enable the agent to gain a more nuanced understanding of the environment, leading to higher-quality pathlet dictionaries. PATHLETRL++ achieves even greater dictionary size reduction, surpassing the performance of PATHLETRL, while maintaining high trajectory representability.", "sections": [{"title": "1 INTRODUCTION", "content": "Motivation & Problem of Interest. The development of technology for gathering and tracking location data has led to the accumulation of vast amounts of trajectory data, consisting of spatial and temporal information of moving objects, such as persons or vehicles. Mining trajectory data to find interesting patterns is of increased research interest due to a broad range of useful applications, including analysis of transportation systems [31], human mobility [43], location-based services [57], spatiotemporal epidemics [3, 38, 39] and more. There are several technical problems in trajectory data mining that researchers and practitioners have focused on in recent years, including trajectory similarity [12], clustering [17], classification [4], prediction [55] and simplification [52]. A few comprehensive surveys on the topic can be found in Zheng [62], Alturi et al. [6], and Hamdi et al. [16]. In this research, we focus on the problem of constructing a small set of basic building blocks that can represent a wide range of trajectories, known as a (trajectory) pathlet dictionary (PD). The term pathlet appears in the literature by many names, such as subtrajectories, trajectory segments, or fragments [1, 8, 26, 36, 41, 59]. For consistency, we will use the term pathlets to denote these building blocks.\nThe Broader Impact. Effectively constructing pathlet dictionaries is of increased research and practical interest due to a broad range of tasks and applications that can use it, such as route planning [56], travel time prediction [18], personalized destination prediction [54], trajectory prediction [55], and trajectory compression [60] (see Appendix A for a supplementary discussion of these applications).\nThe State of the Art & Limitations. Many existing works frame the problem of analyzing and deriving pathlets as a (sub)trajectory clustering problem, where (sub)trajectory clusters represent popular paths (the pathlets) [1, 22, 46]. A few works considered an integer programming formulation with constraints to solve the problem [8, 26]. Some works designed their pathlets based on a route \"representativeness\u201d criterion [36, 51]. Unfortunately, these existing works suffer some limitations. For example, Chen et al. [8] assumes that the datasets used are noise-free. Zhou et al\u2019s [63] bag-of-segments method requires that trajectory segments are of fixed length. Van Krevald et al. [46] demands input trajectories to have the same start/endpoints. The cluster centroids in (sub)trajectory clustering methods [1, 22, 46, 49] do not necessarily reflect real roads in the road network. In addition, Wang et al. [51] demonstrated empirically that these clustering methods are computationally slow. In spite of runtime improvements, [51] also requires the user to provide some budget constraint B in the route representative discovery task, a domain-specific parameter that requires domain expert knowledge. Another related method is TRACULUS [22] that requires pathlets to be straight line segments, which is not always the case in real road maps. In addition, all these works do not constraint pathlets to be edge-disjoint; two pathlets are said to be edge-disjoint if they don't share any edge. Therefore, existing works allow pathlets in the dictionary to (partially) overlap. These methods, by design, follow a top-down approach in constructing a dictionary. This involves forming all possible pathlet candidates first, by considering pathlets of various configurations and"}, {"title": "Our Approach & Contributions.", "content": "To address these limitations, we propose a bottom-up approach for constructing a pathlet dictionary that complies with edge-disjoint pathlets (see Fig. 1) and reduces memory storage requirement. In Fig. 2 for instance, we illustrate how our proposed approach saves up to ~24K\u00d7 less memory space than existing methods for storing the initial pathlets (see Experiment (Q2) for full details, with Appendix C presenting a more theoreotical proof). The key idea of our approach is to initialize unit-length pathlets & iteratively merge them to form longer, higher-order ones, while maximizing utility [2, 28]. Longer pathlets are preferred (over shorter ones) as they hold more spatiotemporal information, such as mobility patterns in trajectories [8]. A deep reinforcement learning method is proposed to approximate the utility function.\nA summary of our contributions is provided below:\n\u2022 We introduce a more strict definition of a pathlet than in previous works to comply with edge-disjoint pathlets. This enables a bottom-up approach for constructing pathlet dictionaries that reduces memory storage needs.\n\u2022 We introduce two novel metrics, namely trajectory loss and trajectory representability, which allow us to more comprehensively evaluate the utility of a pathlet and the overall quality of a constructed pathlet dictionary.\n\u2022 We formulate the problem of pathlet dictionary construction as a utility maximization problem, where shorter pathlets are merged to form a set of longer ones with higher utility.\n\u2022 We propose PATHLETRL, a deep reinforcement learning method that utilizes a Deep Q Network (DQN) policy to approximate the utility function of constructing a pathlet dictionary. To the best of our knowledge, this is the first attempt to employ a deep learning method for the problem.\n\u2022 We address the limitations of the original PATHLETRL model by introducing optimization techniques to improve the quality and stability of the constructed pathlet dictionaries, ensuring better trade-offs between key objectives in trajectory modeling.\n\u2022 We introduce PATHLETRL++, which extends PATHLETRL by incorporating a richer state representation and a more robust reward function to optimize decision-making during pathlet merging. This extension significantly improves the stability and quality of the constructed pathlet dictionary.\n\u2022 We demonstrate empirically that the dictionary constructed by our PATHLETRL and its extensions is of superior quality compared to those constructed by traditional non-learning-based methods. Our method reduces the size of the dictionary by up to 65.8% compared to other methods."}, {"title": "2 PRELIMINARIES & PROBLEM DEFINITION", "content": "In this section, we briefly introduce some definitions and notations (see Table 1). Then we formally define the problem of interest.\n2.1 Primary Definitions and Notations\nDefinition 2.1 (Trajectory). Let $\\mathcal{O} = {o_1, o_2, ..., o_{|\\mathcal{O}|}}$ be a set of moving objects in a certain geographic map $\\mathcal{M} \\subset \\mathbb{R}^2$. A trajectory $\\tau$ of a single object $o \\in \\mathcal{O}$ can be represented as a sequence of time-enabled geo-coordinate points: $\\tau = ((x_1, y_1, t_1), ..., (x_{|\\tau|}, y_{|\\tau|}, t_{|\\tau|}))$, where each $x_i$ and $y_i$ represents $o$'s longitudinal and latitudinal coordinates at a specific time instance $t_i \\in [0, T]$. We let $|\\tau|$ be its length, or the # of time-enabled points for the trajectory of $o$. Moreover, we let trajectory (data) set $\\mathcal{T}$ consist of all trajectories of all $o \\in \\mathcal{O}$: $\\mathcal{T} = \\bigcup_{\\forall o \\in \\mathcal{O}} \\mathcal{T}_o$, with $\\mathcal{T}_o$ as the set of all $o$'s trajectories.\nDefinition 2.2 (Road Network). We denote by $\\mathcal{G}\\langle\\mathcal{V}, \\mathcal{E}\\rangle$ the road network within map $\\mathcal{M}$, where $\\mathcal{V}$ and $\\mathcal{E} \\subseteq \\mathcal{V} \\times \\mathcal{V}$ represents $\\mathcal{G}$\u2019s set of road intersections (nodes) and segments (edges) respectively. Trajectory points (GPS traces) outside $\\mathcal{M}$ are filtered out as a preprocessing step. The remaining trajectories also require to be map-matched (see Appendix D for details). With map-matched data, we can now formalize the fundamental building block in this work.\nDefinition 2.3 (Pathlet). A pathlet $p$ is defined as any sub-path in the road network $\\mathcal{G}$, with $\\mathcal{P}$ being the set of all such pathlets.\nIn our work, we consider edge-disjoint pathlets, s.t. no two $p_1, p_2 \\in \\mathcal{P}$ share any edge. For simplicity, we assume discrete pathlets \u2013 meaning they begin and end at an intersection (a node in"}, {"title": "Definition 2.4 (Pathlet Length\u00b3).", "content": "Denoted by $l$, this represents pathlet $p$'s path length in the road network.\nThe smallest unit of the pathlet has length $l = 1$. Moreover, we restrict all pathlets $p \\in \\mathcal{P}$ to be of length $l \\leq k$, for some user-defined $k$. In this case, we say that $\\mathcal{P}$ is a k-order pathlet set.\nDefinition 2.5 (Pathlet Graph). The pathlet graph $\\mathcal{G}_p\\langle\\mathcal{V}_p, \\mathcal{E}_p\\rangle$ of a road network $\\mathcal{G}\\langle\\mathcal{V}, \\mathcal{E}\\rangle$ depicts the road network's pathlets, where the road intersections represent the nodes $\\mathcal{V}_p \\subseteq \\mathcal{V}$ and the road segments connecting road intersections as the edges $\\mathcal{E}_p \\subseteq \\mathcal{E}$.\nFig. 1(a), for example, represents the pathlet graph representation of a small area in Toronto. In our work, we consider an initial pathlet graph where each pathlet has length $l = 1$.\nDefinition 2.6 (Pathlet Neighbors). Given a pathlet $p \\in \\mathcal{P}$, its neighbor pathlets, denoted by $\\Psi(p)$, are all other pathlets $\\rho' \\in \\mathcal{P} \\backslash {p}$ who share the same start/endpoints with that of $p$:\n$\\Psi(\\rho) = \\bigcup_{\\rho'\\in\\mathcal{P}\\{\\rho}, (\\rho.s\\in {\\rho'.s,\\rho'.e})\\lor(\\rho.e\\in {\\rho'.s,\\rho'.e})} \\rho'$\nFor example, the grey pathlet in Fig. 1(b) has two neighbors: the orange & blue pathlets.\nDefinition 2.7 (Pathlet-based Representation of a Trajectory). A trajectory $\\tau\\in \\mathcal{T}$ can be represented based on some subset of pathlets $\\mathcal{P}_{sub} \\subseteq \\mathcal{P}$. Moreover, the pathlets in $\\mathcal{P}_{sub}$ can be concatenated in some sequence resulting into the path traced by $\\tau$ on the road network $\\mathcal{G}$. We denote this by $\\Phi(\\tau) = {\\rho^{(1)}, \\rho^{(2)}, ..., \\rho^{(|\\mathcal{P}_{sub}|)}}$, where $\\rho^{(i)} \\in \\mathcal{P}_{sub}$ denotes the ith pathlet in the sequence that represents the pathlet-based representation for $\\tau$.\nBased on this, it is also possible to define a trajectory's pathlet length, which we initially set before constructing the pathlet graph. Each trajectory $\\tau\\in \\mathcal{T}$ has pathlet length equal to $\\Sigma_{\\forall \\rho \\in \\Phi(\\tau)} l(\\rho)$, whose value remains static for the rest of our algorithm.\nDefinition 2.8 (Trajectory Traversal Set of a Pathlet). Let $\\Lambda(p)$ be the set of all trajectories $\\tau\\in\\mathcal{T}$ that pass or traverse pathlet $p \\in \\mathcal{P}$. This can also be written as $\\Lambda(p) = {\\tau|\\forall \\tau \\in \\mathcal{T}, \\rho \\in \\Phi(\\tau)}$. We can also assign weights $\\omega$ to pathlets. In the unweighted case, all pathlets are weighed equally; while in the weighted version, pathlets are weighed equal to the # of trajectories traversing a pathlet, i.e., $\\omega(p) = |\\Lambda(\\rho)|$, or $\\frac{|\\Lambda(p)|}{|\\mathcal{T}|}$ when normalized. These weights indicate each pathlet's importance in the road network/pathlet graph."}, {"title": "2.2 Novel Trajectory Metrics", "content": "We can now introduce some novel metrics to allow us to more comprehensively evaluate our pathlets and pathlet dictionaries.\nDefinition 2.9 (Trajectory Representability\u2074). The (trajectory) representability $\\mu\\in [0\\%, 100\\%]$ of a trajectory $\\tau$ denotes the % of $\\tau$ that can be represented using pathlets in pathlet set $\\mathcal{P}$.\nClearly, the pathlet-based representation of $\\tau$ is directly related to its representability, i.e., $\\mu(\\tau) = \\Sigma_{\\forall \\rho \\in \\Phi(\\tau)}$, for the unweighted case and $\\mu(\\tau) = \\Sigma_{\\forall \\rho \\in \\Phi(\\tau)} \\omega(\\rho)$, for the weighted version.\nDefinition 2.10 (Trajectory Loss). We define the trajectory loss $L_{traj}$ to be the # of trajectories $\\forall \\tau \\in \\mathcal{T}$ that have representability value $\\mu = 0\\%$, i.e., $L_{traj} = |{\\tau|\\tau \\in \\mathcal{T}, \\mu(\\tau) = 0}|$. We can also"}, {"title": "2.3 Problem Definition", "content": "Before formalizing the problem, we first introduce the pathlet dictionary and some optimization definitions.\nDefinition 2.11 (Pathlet Dictionary). A (trajectory) pathlet dictionary (PD) is a data structure that stores pathlets $p \\in \\mathcal{P}$ (keys), and their associated trajectory traversal set $\\Lambda(p)$ (values).\nSee Fig. 3 (the righthand boxes inside the orange & blue panels) for an illustrative example of a PD. We are interested in constructing a PD that aims to achieve one or a combination of the following:\n(O1) Minimal size of candidate pathlet set $\\mathcal{S}$, or the candidate set with the least possible number of pathlets (i.e., min $|\\mathcal{S}|$)\n(O2) Minimal $\\phi$, or the avg # of pathlets representing each trajectory $\\tau \\in \\mathcal{T}$ (i.e., min $\\phi = \\min \\frac{\\Sigma_{\\tau \\in \\mathcal{T}}|\\Phi(\\tau)|}{|\\mathcal{T}|}$)\n(O3) Minimal trajectory loss $L_{traj}$ (i.e., min $L_{traj}$)\n(O4) Maximal $\\overline{\\mu}$, or the average representability values of the remaining trajectories in $\\mathcal{T}$ (i.e., $\\max \\overline{\\mu} = \\max \\frac{\\Sigma_{\\tau \\in \\mathcal{T}} \\mu(\\tau)}{|\\mathcal{T}|}$)\nIn other words, the objective function that we wish to optimize is based on the four objectives above \u2013 which can be modelled by:\n$\\min \\left(\\alpha_1 |\\mathcal{S}| + \\alpha_2 \\frac{1}{|\\mathcal{T}|} \\Sigma_{\\tau \\in \\mathcal{T}}|\\Phi(\\tau)| + \\alpha_3 L_{traj} - \\alpha_4 \\frac{1}{|\\mathcal{T}|} \\Sigma_{\\tau \\in \\mathcal{T}} \\mu(\\tau) \\right)$         (1)\nwhere the $\\alpha_i$'s are user defined objective weights.\nProblem 1 (Pathlet Dictionary Construction). Given a road network $\\mathcal{G}\\langle\\mathcal{V}, \\mathcal{E}\\rangle$ of a specific map $\\mathcal{M}$, a trajectory set $\\mathcal{T}$, max pathlet length $k$, max trajectory loss $M$, and avg trajectory representability threshold $\\overline{\\mu}$, construct a k-order pathlet dictionary $\\mathcal{S}$. The dictionary $\\mathcal{S}$ consists of edge-disjoint pathlets with lengths of at most k, and achieves the max possible utility according to some utility function as depicted in Equation (1), such that $L_{traj} < M$ and $\\overline{\\mu} \\geq \\mu$."}, {"title": "3 METHODOLOGY", "content": "We now describe our methods to address the problem of interest (see Fig. 3 for its architecture). In particular, we describe the components of the proposed PATHLETRL model (Pathlet dictionary construction using trajectories with Reinforcement Learning). There are two main components: (1) the method responsible for extracting the candidate pathlet sets through a merging-based process, and (2) a deep reinforcement learning-based architecture for approximating the utility function of the merging process of (1)."}, {"title": "3.1 Extracting Candidate Pathlets", "content": "In this section, we describe the algorithmic details for merging our edge-disjoint pathlets. The high-level idea of the algorithm is based on the theory of maximal utility [2, 28], i.e., iteratively merging (neighboring) pathlets until this brings forth little to no improvement on the utility (details of the utility are given later). The algorithm takes in as input a road network G, a trajectory set T operating within G, the max threshold for the trajectory loss M, the trajectory representability threshold \u00fb, and a positive integer k denoting the desired k-order pathlet graph. As output, it returns a pathlet dictionary (PD) that holds pathlet information as described in Definition 2.11. The extracted PD aims to satisfy the four objectives (O1)-(O4). See Algorithm 1 for the pseudocode.\nInitialization. The algorithm begins by setting up an empty candidate set C = 0 to keep track of processed pathlets. It constructs the initial pathlet graph Gp from the input road network G, which consists of initial pathlets of length 1. For each trajectory t\u2081 \u2208 T, it calculates the initial trajectory coverage 4 and representability \u03bc. An initial pathlet pcurrent is randomly chosen from the set of unprocessed pathlets Gp \\ C.\nAn Iterative Algorithm. The algorithm operates in a loop where, at each iteration, the agent observes the current state st of the environment and selects an action at using an e-greedy policy from the Deep Q-Network (DQN). The actions are defined as follows:\n\u2022 Skip Action (at = 0): The agent chooses to skip the current pathlet pcurrent, marking it as processed by adding it to C. If there are still unprocessed pathlets remaining (i.e., Gp \\C \u2260 0), the agent randomly selects a new pathlet from this set to be the next pcurrent. Otherwise, the episode terminates, and the final pathlet dictionary S is produced.\n\u2022 Merge Action (at > 0): The agent selects a neighboring pathlet Pneighbor based on the action at to merge with the current pathlet pcurrent. If merging these two pathlets results in a new pathlet whose length does not exceed the maximum allowed length k, they are merged to form a new pathlet Pmerged. The traversal set A(pmerged) and the pathlet graph Gp are updated accordingly. Trajectories that now have zero coverage are removed from the trajectory set T. The metrics \u03c6, \u03bc, and trajectory loss Ltraj are recalculated. The agent then updates pcurrent to be the newly formed pmerged. If merging is not possible due to length constraints, the agent treats this as a skip action. At each iteration, the agent computes a reward R\u2081 based on changes in the size of the pathlet dictionary |S|, representability \u03bc, trajectory loss Ltraj, and the average number of pathlets per trajectory $. The transition (st, at, Rt, St+1) is stored in the experience replay buffer, and the DQN updates its Q-network using sampled experiences. The loop continues until one of the termination conditions is met: the trajectory loss exceeds the threshold M, the average representability u falls below the threshold pthreshold, or there are no unprocessed pathlets left. The final pathlet dictionary S.\nThe Utility Function. To complete the description of the algorithm, we discuss the formulation of the utility function that we approximated using a learning-based method."}, {"title": "THEOREM 3.1 (TRAJECTORY REPRESENTABILITY THEOREM).", "content": "At any step i of the iterative Algorithm 1, then the trajectory representability \u00b5 of some trajectory \u03c4 \u2208 T by the end of that iteration i is equal to:\n$\\mu_i(\\tau) = \\frac{\\Sigma_{\\rho' \\in \\Phi_i(\\tau)} l(\\rho')}{\\Sigma_{\\rho \\in \\Phi_0(\\tau)} l(\\rho)}$         (2)\nwhere \u03a6o and \u0424\u2081 are the pathlet-based representation of trajectory t in the initial (iteration 0) and iteration i of the iterative algorithm respectively.\nThe above theorem provides a formula for how to compute a trajectory's representability value \u03bc at some iteration i of the algorithm. We refer the reader to Appendix B where we provide a complete proof for this theorem."}, {"title": "3.2 Background on Reinforcement Learning", "content": "For readers already familiar with reinforcement learning and Deep Q-Networks (DQN), you may proceed directly to Section 3.3. For those new to the topic, we provide a brief overview of essential concepts to facilitate understanding of our framework.\nMarkov Decision Process (MDP). An MDP models decision-making where outcomes are partly random and partly under the control of an agent. It consists of:\n\u2022 S: Set of possible states.\n\u2022 A: Set of actions available to the agent.\n\u2022 P(s' | s, a): Transition probability from state s to s' after action a.\n\u2022 R(s, a, s'): Immediate reward received after transitioning to state s' from s using action a.\n\u2022 \u03b3\u2208 [0, 1]: Discount factor for future rewards.\nThe agent aims to learn a policy \u03c0 that maximizes the expected cumulative reward.\nPolicy (\u03c0). A policy maps states to actions. It can be deterministic or stochastic, assigning probabilities to actions. The goal is to find a policy that maximizes the expected return.\nValue Functions. The value function V\u201d (s) estimates the expected return starting from state s under policy \u03c0:\n$V^{\\pi}(s) = E_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} | s_t = s \\right]$\nSimilarly, the action-value function Q\u201d (s, a) estimates the expected return after taking action a in state s:\n$Q^{\\pi}(s, a) = E_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k} | s_t = s, a_t = a \\right]$\nIterative Learning in Reinforcement Learning. Reinforcement learning is inherently iterative. The agent interacts with the environment in a sequence of time steps. At each time step t:\n(1) The agent observes the current state st.\n(2) Based on its policy \u03c0, the agent selects an action at."}, {"title": "3.3 Reinforcement Learning Framework", "content": "Building upon the reinforcement learning concepts introduced earlier, we now detail how we apply RL to the pathlet merging problem. Our goal is to train an agent to decide whether to merge or keep specific pathlets in the graph, optimizing the trade-off between the objectives mentioned in section 2.3.\nAgent. The agent interacts with the pathlet graph by selecting actions that alter its structure. It aims to learn a policy that maximizes cumulative rewards, effectively simplifying the graph while maintaining high trajectory representability.\nStates. Each state st captures the current configuration of the pathlet graph. We represent the state as a 4-tuple:\n$(S_1, S_2, S_3, S_4)$\nwhere:\n\u2022 S\u2081: Number of pathlets in the graph (|S|).\n\u2022 S2: Average number of pathlets needed to represent trajectories.\n\u2022 S3: Trajectory loss metric Ltraj, indicating the proportion of trajectories that cannot be accurately represented.\n\u2022 S4: Average trajectory representability \u016b.\nThis state representation encapsulates key factors affecting our optimization objectives.\nThe Actions. At each time t, the agent has a choice of two discrete actions on the currently processed pathlet p, as expressed by the action space A = {KEEP, MERGE}. In other words, KEEP action suggests that the current pathlet p should be kept and not be merged with any one of its neighbors. As a result, Algorithm 1 puts the current pathlet p in the processed set and then selects a new pathlet to process, performing one of the two actions in the action space on that new pathlet."}, {"title": "3.4 Space Complexity Analysis", "content": "One of the main motivations for why edge-disjoint pathlets have been used together with bottom-up approaches is due to the reduced memory storage requirements that is necessary to initialize pathlets, in contrast with previous works that utilize top-down schemes with overlapping, redundant pathlets. In fact, we provide a space complexity analysis of top-down approaches and compare this with the proposed bottom-up methods through the following theorem (see Appendix C for the proof)."}, {"title": "THEOREM 3.2 (INITIAL MEMORY STORAGE REQUIREMENT THEOREM).", "content": "The memory space that is required by top-down methods for initializing a pathlet dictionary has a quadratic (n\u00b2) bound, with n as the number of segments of the road network. Bottom-up schemes on the other hand, such as the proposed PATHLETRL, requires only an initial O(n) amount of memory space, with n as the number of initial length-1 pathlets."}, {"title": "4 EVALUATION", "content": "In this section, we present the details of our experimental setup for evaluating our proposed method. We aim to analyze and evaluate our models based on the following research questions:\n(Q1) How does PATHLETRL compare with the SotA methods, in terms of the quality of the extracted PDs?\n(Q2) How much memory does the bottom-up approach save compared to top-down methods?\n(Q3) How much improvement and how much more effective is our PATHLETRL model against its ablation variations?\n(Q4) What is the distribution of pathlet lengths in the obtained dictionary in our PATHLETRL model?\n(Q5) How effective is the constructed PD in reconstructing the original trajectories?\n(Q6) What is the sensitivity of the user-defined parameters [a1, a2, a3, a4] in the performance of our PATHLETRL model?\n(Q7) How do the hyper-parameters such as the maximum length of pathlets and the average trajectory representability threshold impact the performance and efficiency of PATHLETRL?"}, {"title": "4.1 Datasets", "content": "We utilize two datasets that each depict a different map scenario (see Appendix F for its complete statistics, and Appendix G for a brief discussion about the data's privacy concerns). We used real world maps of two metropolitan cities, Toronto and Rome through the OpenStreetMaps6. A realistic synthetic vehicular mobility datasets for the TORONTO map was generated using the Sumo mobility app simulator7 (3.7 hrs). Moreover, larger-scale, real-world taxi cab trajectories (first week of February 2014) were taken from CRAWDAD [7], an archive site for wireless network and mobile computing datasets, to form the ROME dataset. We split our trajectory sets into 70% training and 30% testing, where the training data was used to construct our pathlet dictionaries and the remainder for evaluation."}, {"title": "4.2 Experimental Parameters", "content": "Refer to Appendix H for full details of the implementation. To implement the RL architecture, we used a deep neural network that consists of the following parameters. It comprises of three hidden fully-connected layers of 128, 64 and 32 hidden neurons. The ReLU activation function has been employed, optimized by Adam with a learning rate of 0.001. We also use a 0.2 dropout in the network, together with the Huber loss function. More specific to the DQN's parameters, we have m = 5 episodes for each of the n = 100 iterations. The size of the experience replay buffer is 100,000 and the memory minibatch size is 64. Our agent also uses a discount factor y = 0.99. Moreover, we use k = 10 for the k-order candidate set, M = 25% maximum trajectory loss and \u00fb = 80% average representability threshold. We also set a\u2081 = \u00bc, Vi, which denotes equal importance for each of the four objectives as depicted in Equation (3)."}, {"title": "5 PATHLETRL OPTIMIZATIONS", "content": "Building on the parameter sensitivity analysis, which showed how variations in the four a parameters influence the Pathlet Dictionary (PD) in terms of the different objectives, we now shift our focus to addressing the observed limitations in the original PATHLETRL model. Specifically, the analysis highlighted the model's sensitivity to the weight assignments in the linear scalarization of the reward function, which can significantly impact the quality of the extracted PD."}, {"title": "5.1 Scalarization Sensitivity in Multi-Objective Optimization", "content": "The original PATHLETRL model employs a linear scalarization approach for reward calculation, equally weighting all of the objectives (3). This linear scalarization suffers from known sensitivity issues in multi-objective optimization (MOO), as small changes in weight assignment can lead to substantially different optimization outcomes [47]. Specifically, Linear scalarization only guarantees optimal solutions when the Pareto front (the set of optimal trade-offs between objectives) is convex. If the Pareto front is non-convex, this method fails to identify non-convex Pareto optimal solutions. This limitation means that some potentially desirable trade-offs between objectives may be missed entirely [19]. To address these issues, we propose two alternative scalarization methods to improve the stability and generalization of solutions."}, {"title": "5.1.1 Chebyshev Scalarization.", "content": "The first alternative is Chebyshev scalarization, which seeks to minimize the maximum deviation from the ideal point in the objective space. This approach is formally expressed as:\n$R_{Chebyshev} = \\max \\left( \\alpha_1 |f_1(x) - z_1^*|, \\alpha_2 |f_2(x) - z_2^*|, \\alpha_3 |f_3(x) - z_3^*|, \\alpha_4 |f_4(x) - z_4^*| \\right),$         (4)\nwhere x represents the decision variables, $f_1(x), f_2(x), f_3(x), f_4(x)$ are the values of the four objective functions, and $z_1^*, z_2^*, z_3^*, z_4^*$ are the ideal values of the respective objectives. The weights $\\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4$ are user-defined scaling factors, often set such that $\\sum_{i=1}^{4} \\alpha_i = 1$."}, {"title": "5.1.2 Dynamic Scalarization.", "content": "The second alternative introduces a dynamic scalarization function that adjusts weights based on the agent's proximity to predefined thresholds for trajectory representability ptraj and trajectory loss Ltraj. This method is captured by dynamically adjusting the weights for different objectives using penalties. These penalties are calculated based on the agent's proximity to critical thresholds.\nThe dynamic weight assignment is implemented in the following way:\n$w_{traj}(t) = \\frac{1}{\\max \\left(0.01, \\frac{M-L_{traj}(t)}{M} \\right)}$\n$w_{\\mu}(t) = \\frac{1}{\\max \\left(0.01, \\frac{\\mu_{traj}(t)-\\tau_{\\mu}}{ \\tau_{\\mu}} \\right)}$\nwhere $w_{traj}(t)$ and $w_{\\mu}(t)$ are dynamic weights for trajectory loss and trajectory representability, respectively. The weight for each objective increases as the agent approaches the thresholds $ \\tau_{\\mu}$ for utraj and M for Ltraj, encouraging the agent to prioritize these objectives when performance is critical.\nThe reward is calculated using these dynamic weights:\n$R = -\\alpha_1 \\Delta |S| - \\alpha_2 \\Delta \\phi - \\alpha_3 \\Delta L_{traj} w_{traj}(t) + \\alpha_4 \\Delta \\mu w_{\\mu}(t),$         (5)\nInitially, all objectives are weighted equally, but as the agent approaches critical thresholds (e.g., the trajectory representability threshold $ \\tau_{\\mu_{traj}})$.), the dynamic weights $w_{traj}(t)$ and $w_{\\mu}(t)$ increase, forcing the agent to prioritize those objectives more.\nAt the end of an episode, an additional reward or penalty is applied based on how well the agent performed overall, considering the same objectives and thresholds. This dynamic adjustment ensures that the agent becomes more conservative when nearing critical thresholds, avoiding excessive pathlet merging that could degrade the overall performance."}, {"title": "5.2 Enhanced State Representation for Improved Decision-Making", "content": "While the first two variations improve the scalarization mechanism", "as": "n$s_t = (S_1, S_2, S_3, S_4)$,\nwhere S\u2081 denotes the number of pathlets in the current pathlet graph, S2 denotes the average number of pathlets required to represent the trajectories, S3 is the trajectory loss, and S4 is the average trajectory representability. This restricted global view leaves the reinforcement learning agent unaware of crucial local information about individual pathlets and their neighbors, which can lead"}]}