{"title": "CLINICSUM: Utilizing Language Models for Generating Clinical Summaries from Patient-Doctor Conversations", "authors": ["Subash Neupane", "Himanshu Tripathi", "Shaswata Mitra", "Sean Bozorgzad", "Sudip Mittal", "Shahram Rahimill", "Amin Amirlatifi"], "abstract": "This paper presents CLINICSUM, a novel framework designed to automatically generate clinical summaries from patient-doctor conversations. It utilizes a two-module architecture: a retrieval-based filtering module that extracts Subjective, Objective, Assessment, and Plan (SOAP) information from conversation transcripts, and an inference module powered by fine-tuned Pre-trained Language Models (PLMs), which leverage the extracted SOAP data to generate abstracted clinical summaries. To fine-tune the PLM, we created a training dataset of consisting 1,473 conversations-summaries pair by consolidating two publicly available datasets, FigShare and MTS-Dialog, with ground truth summaries validated by Subject Matter Experts (SMEs). CLINICSUM's effectiveness is evaluated through both automatic metrics (e.g., ROUGE, BERTScore) and expert human assessments. Results show that CLINICSUM outperforms state-of-the-art PLMs, demonstrating superior precision, recall, and F-1 scores in automatic evaluations and receiving high preference from SMEs in human assessment, making it a robust solution for automated clinical summarization.", "sections": [{"title": "I. INTRODUCTION", "content": "The advent of transformer-based models such as OpenAI GPT models , Meta LLAMA2 variants, and Google Gemini\u00b3 has revolutionized Natural Language Processing (NLP) by significantly improving performance across a wide array of tasks. These advancements, driven primarily by transfer learning, have opened up new possibilities for applying these models in specialized domains [1], [2]. One such domain is healthcare, where leveraging Pre-trained Language Models (PLMs) to automatically generate clinical summaries from doctor-patient conversations presents a promising application with substantial benefits for both patients and healthcare providers.\nClinical summaries play a critical role in healthcare by improving patients' understanding of care plans and reducing the risk of misinterpreting medical information. Research indicates that patients forget 40-80% of the medical information provided by healthcare practitioners almost immediately [3] and misconstrue nearly half of what they remember [4]. For healthcare providers, generating these summaries automatically can alleviate the administrative burden of updating Electronic Health Records (EHRs), a task strongly associated with physician burnout [5], [6].\nHowever, the application of PLMs in this context is not without challenges. Since PLMs are generally trained on broad, non-specialized text corpora, they are prone to producing inaccuracies\u2014such as hallucinations [7]\u2014that could have serious consequences for patient care. Addressing these challenges requires more than just deploying PLMs; it necessitates a tailored approach that can accurately capture the nuances of medical conversations while ensuring the reliability of the generated summaries.\nIn this paper, we present CLINICSUM, a comprehensive framework designed to automatically generate clinical summaries in the Subjective Objective Assessment and Plan (SOAP) format from transcribed patient-doctor conversations. Fig. 1 provides an illustration of CLINICSUM. To tackle the limitations of current PLMs in healthcare, CLINICSUM integrates a retrieval-based filtering module and an inference module, working in tandem to produce accurate and contextually relevant summaries. The retrieval-based filtering"}, {"title": "II. BACKGROUND & RELATED WORK", "content": "Transformer architectures [15] have fueled the advancement of Large Language Models (LLMs) in NLP, thanks to their remarkable parallelization capabilities [16]. Trained on massive internet text datasets and featuring substantial parameter sizes, LLMs exhibit impressive learning abilities. However, LLMs often struggle with factual questions in closed domains, where specialized knowledge is crucial. This difficulty can manifest in factually inaccurate predictions, a phenomenon known as hallucination [7]. This limitation may arise from a combination of factors, including a deficit in domain knowledge, reliance on outdated information, and forgetting [17], [18].\nTo mitigate the knowledge deficiency within PLMs for domain-specific tasks, an additional knowledge ingestion step is required. The two most common approaches currently practiced for external knowledge ingestion are Retrieval Augmented Generation (RAG) and Fine-tuning. The first approach, introduced around mid-2020 by Lewis et al., [19], is designed to enhance the performance of PLMs on knowledge-intensive tasks. This approach involves retrieving relevant information from external knowledge sources based on the input query. The retrieved content is then concatenated with the original query, providing the PLM with enriched context, which leads to more informed and accurate response generation.\nThe second approach, is to fine-tune PLM. In this approach, a PLM is further trained on a smaller, task-specific dataset to adapt it to a particular application. This process allows the model to leverage the general knowledge it has acquired during pre-training and refine its weights based on the new, more focused data, improving its performance on the target task. As PLM grows in size, updating all parameters during fine-tuning becomes increasingly costly and inefficient, especially with limited computational resources. This challenge has"}, {"title": "III. TASK FORMULATION", "content": "Given a set of patient-doctor conversation transcripts T, where each transcript $t_i \\in T$ consists of unstructured conversation. The objective is to generate semi-structured SOAP clinical summaries for each transcript $t_i$. This involves using a function f, which applies a combination of information retrieval techniques and a PLM to map each $t_i$ to a semistructured SOAP format $n_i$. Specifically, the function can be defined as:\n$n_i = f(t_i)\\ \\ t_i \\in T, n_i \\in N_i \\in N$ (1)\nwhere, T is the set of Transcripts and N is the set of clinical summaries. These summaries $n_i$ are organized in SOAP format where S is the Subjective component that summarizes patient's reported symptoms and experiences. O is the Objective component, detailing the observable and measurable clinical findings. A is the Assessment component, providing a diagnosis or evaluation based on the information and P is the Plan component, outlining the treatment and management strategies discussed in $t_i$."}, {"title": "IV. ARCHITECTURE & METHODOLOGY", "content": "This section presents the architecture of our framework, CLINICSUM, and outlines our methodology. The framework consists of two main modules: retriever-based filtering and inference, as illustrated in Fig. 2. The following subsections provide a detailed explanation of each module."}, {"title": "A. Retriever-based Filtering", "content": "The first module in our framework systematically processes doctor-patient conversation transcripts $t_i$ to extract the SOAP elements for clinical summary $n_i$ using a retrieval prompt (query) $Q_R$. For example, our retrieval prompt is \"Extract subjective, objective, assessment, and plan details from a given transcript\". For extraction this module utilizes the following three sub-components:\n1) Splitting: Splitting is the process of converting entire transcript $t_i$ into a set of individual sentences/chunks $c_i$. To split $t_i$ into $c_i$, we apply a sentence split regular expression. Hence, the splitting $(t_i \\rightarrow c_i)$ can be formulated as:\n${c_i : c_i \\in C} = split(t_j) \\forall t_i \\in T|i, j\\in N, i\\geq j$ (2)\n2) Indexing: Given the set of sentences (C) in $t_i$, where $C_i$ represents a sentence in the transcript, indexing is the process of projecting $c_i$ into vector space (E) through an embedding model ($\\S(.)$), where, $e_i$ is the vector embedding of $c_i$ and we store this obtained vector embedding into vector storage.\n${e_i : e_i \\in E} = \\S(c_i) \\forall c_i \\in C|i\\in N$ (3)\n3) Retrieval: The retrieval process uses an ensemble method that combines sparse retriever (for example BM25 [32]) and dense retriever (for example DPR [33] or our previous work [34]), assigning different weights to each ($W_{Sparse}$, and $W_{Dense}$), then ranks it using ranking algorithm. In our use case, we implemented Reciprocal Rank Fusion (RRF) [35] which combines rankings from multiple sources by computing reciprocal rank scores.\nA sparse retriever searches for documents ($c_i$) similar to $Q_R$ based on exact token matches in the sparse vector space (C), usually employing traditional keyword-based methods or indexing techniques. We employ BM25 as our sparse retriever ($R_{sparse}$) that can be represented as:\n${C_{i\\_Sparse}:C_{i\\_Sparse} \\in C'|1 \\leq i \\leq k} = R_{Sparse}(C, Q_R)$ (4)\nwhere, k is the number of chunks with highest term frequency.\nA dense retriever searches for documents relevant to $Q_R$ based on the exact or approximate neighbor similarity of embedded vectors ($e_i$) in a continuous embedding vector space (E), using dense representations. For retrieval, we also embed the retrieval prompt $Q_R$ such that:\n$E_{Q_R} = \\S(Q_R)$ (5)\nDense retriever ($R_{Dense}$) can be represented as:\n${e_i : e_i \\in E, 1 \\leq i \\leq k} = R_{Dense}(E, E_{Q_R})$ (6)\nThe similarity function (Sim(.)) can be cosine, dot-product, or euclidean. The top k relevant embedding ($e_i$) are then decoded ($c_i \\in e_i$) to corresponding sentences ($c_i$). where the similarity function in $R_{Dense}$ is as follows:\n$Sim(e_i, Q_R) = \\frac{E_{Q_R}e_i}{||e_i|| \\ ||Q_R||}$ (7)\nIn order to obtain the corresponding sentence/chunk ($c_i$) from embedding $e_i$, we apply an inverse embedding or decoding function ($\\S(\\cdot)$).\n${C_{i\\_Dense}:C_{i\\_Dense} \\in C|1 \\leq i \\leq k} = {\\S(e_i)\\forall e_i \\in E}$ (8)\nNext, we combine both $C_{i\\_Sparse}$ and $C_{i\\_Dense}$ to obtain the final set of embedded documents, before re-ranking them:\n$C_{i\\_Retrieved} = C_{i\\_Dense} \\cup C_{i\\_Sparse}$ = (9)\nThe cardinality of $C_{i\\_Retrieved}$ (say p) will be less or equal to the total number of chunks retrieved using sparse and dense retriever ($k + k \\leq 2k \\mid k \\in N$) i.e., $p < |C_{i\\_Sparse}| + |C_{i\\_Dense} \\leq N$. Once combined, we apply a ranking method to reorder the documents. This is done by using RRF algorithm. The algorithm works by calculating rank score ($r_i$) for the corresponding retrieved chunk ($C_{i\\_Retrieved}$). If a document appears in both $C_{i\\_Sparse}$ and $C_{i\\_Dense}$ with different rankings, we sum the reciprocals of each rank from both retrievers. Typically $\\S(C_{i\\_Sparse})$, $\\S(C_{i\\_Dense}) \\in [0,1]$, this summed reciprocal score can exceed 1. This combined score is used for final ranking with retriever weights ($W_{Sparse}$, $W_{Dense} \\mid W \\in [0,1]; W_{sparse} + W_{Dense} = 1$), with higher scores indicating greater relevance.\nr_i = W_{Sparse} \\times \\S(C_{i\\_Sparse}) + W_{Dense} \\times \\S(C_{i\\_Dense})$ (10)\nThen, we sort the obtained rank score with respect to $Q_R$ in descending order using the following equation, where, $\\lambda$ is a constant to avoid division by 0. Finally, top k chunks are retrieved.\nsort(Q_R, C_{i\\_Retrieved}) = \\sum_{i=1}^{N} \\frac{1}{\\lambda + r_i(Q_R, C_{i\\_Retrieved})}$ (11)\n${C_{i\\_Sorted}: C_{i\\_Sorted} \\in C|1 \\leq i \\leq k} = sort(Q_R, C_{i\\_Retrieved})$ (12)"}, {"title": "B. Inference", "content": "The inference module receives the patient context, derived from the final retrieved concatenated chunks ci, along with an instruction, and a prompt ($Q_{PLMFT}$) as shown in Fig. 2. The instruction guides the language model in performing its task. In our case, we utilize Alpaca prompt, as shown in Fig. 3 as our instruction. On the other hand, prompt directs a fine-tuned PLM to produce clinical summaries in a zero-shot setting. The fine-tuned generator processes the prompt, patient context, and instructions to generate a comprehensive clinical SOAP summary. In the following subsections we describe our fine-tuning approach and then detail summary generation.\n1) Fine-tuning: In this work, we leverage PEFT [20] approach to fine-tune a PLM for clinical summary generation. PEFT enables efficient fine-tuning with minimal resources and costs. Specifically, we adopt Low Rank Adaptation (LoRA) [36] method and load pre-trained models onto a GPU as quantized 4-bit weights. Our motivation for this approach is two-folds: first, to explore the feasibility of training a PLM, such as LLAMA-3, on a single consumer GPU with 24GB of memory (e.g., Nvidia 4090), and second, to assess the effectiveness of fine-tuned PLMs with 4-bit precision in accurately generating clinical summaries. Additionally, PEFT helps prevent catastrophic forgetting [18] after the model has been trained [37]. We use the Alpaca prompt [38] for both fine-tuning and inference tasks, as illustrated in Fig. 3. The training is conducted using Supervised Fine-Tuning (SFT). More information on training dataset is provided in Section V-A.\n2) Summary Generation: We utilize the output of the first module-the context, i.e., the decoded relevant chunks containing subjective, objective, assessment, and plan information from a given transcript along with an instruction and a prompt as input to the inference module to generate a clinical summary. These input are concatenated and passed together to a fine-tuned PLM for summary generation, where $PLM_{FT}$ is a fine-tuned PLM that understand how to generate a clinical summaries, $Q_{PLM_{FT}}$ is a prompt (query), inst denotes instruction and [.....] stands for concatenation.\n$Summary = PLM_{FT}([context, prompt, inst]) = PLM_{FT}([C_{i\\_Sorted}, Q_{PLM_{FT}}, inst])$ (13)\nInference is conducted in a zero-shot setting using a fine-tuned PLM. The fine-tuning process equips the model to generalize effectively, allowing it to generate accurate clinical summaries even for new, unseen patient conversations without requiring additional few-shot examples. An example of the final clinical summary for a specific patient-doctor conversation is provided in Fig. 2 (B) and (C) respectively."}, {"title": "V. EXPERIMENT & EVALUATION", "content": "In this research, we utilize two different datasets for the fine-tuning task. The first dataset is the Figshare dataset, which contains 272 patient-doctor conversations. These conversations span five medical specialties: Cardiovascular, Gastrointestinal, Musculoskeletal, Dermatological, and Respiratory. Table I provides an example from this dataset. The second dataset we use is the MTS-dialog dataset [8], which contains 1,701 patient-doctor conversations. These conversations are centered around General Medicine, Orthopedic, Dermatology, Neurology, and Allergy/Immunology. From this dataset, we selected a subset of 1,201 clean conversations for our study. We then combined them, resulting in a total of 1,473 conversations. Additional statistics, including the total number of sentences, words, characters, unique vocabulary, and tokens for the conversations in the combined dataset are presented in Table II."}, {"title": "B. Evaluation", "content": "Due to the strict privacy concerns and Health Insurance Portability and Accountability Act (HIPAA) regulations around patient data, coupled with its inaccessibility, we opted to simulate patient-doctor conversations. For this, we partnered with the Department of Theatre & Film at Mississippi State University to create 20 simulated conversations. These conversations were staged as role-playing scenarios, where theater arts students simulated realistic interactions between patients and doctors. The conversations were recorded in WAV format and subsequently processed using Automatic Speech Recognition (ASR) techniques, specifically utilizing the Whisper-large model. The average length of these role-played conversations is approximately 9 minutes. Additional statistics on these conversations are provided in Table IV. We then utilized these simulated conversations as our evaluation dataset to assess the robustness of CLINICSUM for the clinical summaries generation task. The evaluation was conducted through both automatic and manual methods.\nIn the following subsections, we discuss the results of our assessment.\n1) Automatic Evaluation: In this paper, we consider two types of metrics: lexical-based and text-embedding-based, to evaluate the clinical summaries generated by CLINICSUM. For the lexical-based metric, we choose Recall-Oriented Under-study for Gisting Evaluation (ROUGE) [39], which primarily focuses on lexical overlaps between generated summaries and the ground truth but does not capture the semantic meaning of the summaries. Considering the limitation of ROUGE, we also employ text-embedding-based metrics, such as BertScore [40]. It uses pre-trained contextual embeddings from a BERT-based model to evaluate the semantic similarity between the ground truth and generated summaries by computing cosine similarity. Specifically, we utilize the deberta-xlarge-mnli model in our experiments.\n2) Expert Human Evaluation: Expert human evaluation plays a critical role in assessing the quality of generated summaries, especially as automatic metrics like ROUGE and BERTScore, though useful, may not always align with expert judgment [10], [41]. Recognizing these limitations, we incor- porated human evaluation in this study. Given the expensive"}, {"title": "VI. LIMITATIONS AND DISCUSSION", "content": "Despite CLINICSUM's encouraging results in generating good clinical summaries, several limitations should be noted. First, the model's performance is highly reliant on the quality and diversity of the training data used in fine-tuning task. The fine-tuning dataset, comprising 1,473 conversations from the FigShare and MTS-Dialog datasets with clinical summaries generated by a PLM and validated by SMEs, is limited in scope, as it only encompasses a narrow range of medical specialties.\nAnother limitation is the use of simulated patient-doctor conversations in the evaluation phase. While these simulated conversations are useful for HIPAA compliance, they may fail to capture the complexities and variability of real-world clinical interactions. Consequently, the generated summaries may not perform as well in real-world clinical settings, where patient communication is less structured and more nuanced.\nFurthermore, while the framework uses retrieval-based filtering to improve factual accuracy, the risk of hallucinations persists, especially when summarizing conversations with ambiguous or incomplete information. Ensuring factual accuracy is critical in medical settings, and additional validation mechanisms may be necessary to mitigate these risks. Moreover, while we added a criterion of factual correctness to the human evaluation process, the moderate IRR scores indicate that subjective interpretation among SMEs can still result in inconsistencies in summary evaluations. Another important aspect is Biases, PLMs trained on vast amounts of text data may inadvertently capture and reproduce biases present in the data. For example, it may over-prioritize common condition such"}, {"title": "VII. CONCLUSION & FUTURE WORK", "content": "In this paper, we demonstrated the feasibility of automatically generating clinical summaries directly from patient-doctor conversations using a framework with two module architecture referred to as CLINICSUM. The first module, retriever-based filtering, acts as an extractive component, identifying relevant portions of the transcript that contain subjective, objective, assessment, and plan information. The advantage of this approach is that it not only filters out unnecessary information from the transcripts but also reduces the risk of hallucination by passing only the relevant chunks to the second module. The second module, inference, utilizes the filtered information as context and uses a fine-tuned PLM to generate clinical summaries through abstraction.\nWe created a high-quality fine-tuning training dataset consisting of 1,473 conversation-summary pairs and used it to fine-tune four open-source PLMs with < 12B parameters. Surprisingly, when combined with our framework for inference, these fine-tuned open-source PLMs substantially outperformed state-of-the-art GPT models in both automatic and expert human evaluations. Expert human assessments by SMEs confirmed that the summaries generated by CLINICSUM were more preferable than those produced by GPT models using prompting. We believe our results are encouraging, and CLINICSUM offers a promising solution for automating clinical summarization.\nFuture work will focus on expanding the both training and validation dataset, improving framework's scalability, and exploring real-world applications in diverse clinical settings. We also intend to further investigate methods to further reduce hallucination and potential biases of PLMs."}]}