{"title": "CriSPO: Multi-Aspect Critique-Suggestion-guided Automatic Prompt Optimization for Text Generation", "authors": ["Han He", "Qianchu Liu", "Lei Xu", "Chaitanya Shivade", "Yi Zhang", "Sundararajan Srinivasan", "Katrin Kirchhoff"], "abstract": "Existing automatic prompt engineering methods are typically designed for discriminative tasks, where new task prompts are iteratively refined with limited feedback from a single metric reflecting a single aspect. However, these approaches are suboptimal for generative tasks, which require more nuanced guidance beyond a single numeric metric to improve the prompt and optimize multiple aspects of the generated text. To address these challenges, we propose a novel multi-aspect Critique-Suggestion-guided automatic Prompt Optimization (CriSPO) approach. CriSPO introduces a critique-suggestion module as its core component. This module spontaneously discovers aspects, and compares generated and reference texts across these aspects, providing specific suggestions for prompt modification. These clear critiques and actionable suggestions guide a receptive optimizer module to make more substantial changes, exploring a broader and more effective search space. To further improve CriSPO with multi-metric optimization, we introduce an Automatic Suffix Tuning (AST) extension to enhance the performance of task prompts across multiple metrics. We evaluate CriSPO on 4 state-of-the-art Large Language Models (LLMs) across 4 summarization and 5 Question Answering (QA) datasets. Extensive experiments show 3-4% ROUGE score improvement on summarization and substantial improvement of various metrics on QA.", "sections": [{"title": "Introduction", "content": "LLMs have emerged as powerful tools for various natural language processing tasks, including text generation (Brown et al. 2020). To fully leverage their capabilities, a critical step is to design a precise task prompt which specifies the desired behavior of the LLM to solve a task. Manual prompt engineering is often laborious, skill-intensive and sub-optimal, motivating the need for automatic prompt engineering techniques which automatically tune the task prompt.\nRecent research has made notable progress in automatic prompt engineering for discriminative tasks, such as text classification (Zhou et al. 2022; Yang et al. 2023; Pryzant et al. 2023; Sordoni et al. 2024). These methods focus on optimizing task prompts for a single metric on a single aspect. The process typically involves instructing an LLM optimizer with a meta-prompt to generate new task prompts based on previously sampled task prompts and their corresponding scores. By iteratively exploring candidates and selecting the task prompt with the highest score, performance on the target metric improves over numerous iterations. However, applying these methods directly to text generation tasks, such as summarization, is sub-optimal due to challenges in obtaining effective optimization signals. Unlike classification tasks, where metrics are straightforward (eg. accuracy), automatic metrics for text generation, like ROUGE (Lin 2004), provides limited guidance for prompt refinement. For example, a lower ROUGE score may result from aspects such as mismatched length, differences in word choice due to formality, or varying writing formats, making it difficult to guide LLMs in prompt modification without fine-grained feedback targeting these individual aspects. Furthermore, evaluating text generation involves multiple metrics (Fabbri et al. 2021; Gao and Wan 2022; Elangovan et al. 2024). In addition to reference similarity, other metrics such as factual consistency, which can be assessed using metrics like AlignScore (Zha et al. 2023), is also important. Balancing or utilizing these multiple metrics is not fully addressed by existing prompt engineering methods that focus on optimizing a single metric.\nTo address these challenges, we introduce CriSPO, a multi-aspect Critique-Suggestion-guided automatic Prompt Optimization (CriSPO) approach. Overall, our approach employs LLMs to automatically identifies multi-aspect prompt revision suggestions, based on which prompts are automatically designed and refined (Table 8 in Appendix shows a working example of how a prompt gets revised in CriSPO). Inspired by recent self-reflection studies, where LLMs generate verbal feedback to aid in self-improvement (Gero et al. 2023; Shinn et al. 2023; Madaan et al. 2024), we designed the first key component of CriSPO: the multi-aspect critique-suggestion meta-prompt. It automatically discovers proper aspects to compare generated text with reference, write critiques of flaws (Pryzant et al. 2023) and suggestions to improve the task prompt (Figure 2 shows a word cloud of aspects identified by CriSPO, including number of words, style, and precision). Both critiques and suggestions, written in natural language, are more helpful for prompt improvement than a single ROUGE score. We then create a receptive optimizer meta-prompt that generates new prompts. In addition to conditioning on previous high-score task prompts and scores, this optimizer also reviews the past critiques and suggestions. It then generates an overall suggestion and an improved task prompt candidate in a Chain-of-Thought (CoT) (Wei et al. 2022) manner. Our approach iteratively optimizes"}, {"title": "Related Work", "content": "There is an increasing effort in the literature to explore gradient-free automatic prompt engineering methods with off-the-shelf LLMs. The focus of these approaches is to find a good search algorithm for better prompt candidates to solve discriminitive tasks. Earlier studies have employed conventional paraphrasing methods for prompt generation through editing phrases (Prasad et al. 2023) or back translation (Xu et al. 2022). More recently, LLMs themselves have been used to sample prompt candidates. Zhou et al. (2022) proposed Automatic Prompt Engineering (APE) which iteratively prompts an LLM to generate semantically similar variations of the locally best prompt. Pryzant et al. (2023) add verbal feedback based on error examples to propose better prompts in terms of accuracy. Concurrently, Sordoni et al. (2024) learn prompts with variational inference by considering their outputs as latent variables. Later on, Yang et al. (2023) propose OPRO to improve over them by incorporating the history of past prompts with their scores which stabilizes optimization. More structured prompts have also been explored by imposing expert-level planning (Wang et al. 2023). In a parallel thread, Fernando et al. (2023) and Guo et al. (2023) were inspired by evolutionary algorithms to perform mutation operations for prompt generation. All of the existing approaches have mostly been designed to target classification tasks using a single metric. Comparing to the existing studies, our proposed method specifically targets the unique challenges in text generation and approaches the prompt optimization problem in a multi-aspect and multi-metric fashion. For practitioners, Khattab et al. (2023) design DSPy framework to build and optimize complex LLM pipelines in a programmatic fashion. TextGrad (Yuksekgonul et al. 2024) further generalizes optimization to text beyond prompt. Our CriSPO can be used as a powerful optimizer in these frameworks.\nOur approach is also inspired by recent studies on using LLMs to automatically correct its output (Pan et al. 2023; Madaan et al. 2024). Gero et al. (2023) apply multiple self-reflection steps to improve the performance of information extraction. Yan et al. (2024) use CoT to generate structured comparison and preferences for two model outputs. Shinn et al. (2023) argue the importance of the self-reflection history and propose reflexion agent to provide verbal feedback on past trials for better decision in the next trials. It is important to notice that these self-reflection studies are strictly speaking not automatic prompt engineering approaches as these studies optimize output revision rather than directly on the prompts. CriSPO, however, automatically reflects on the design of the prompt and uses these past reflections to revise the prompts."}, {"title": "Method", "content": "Problem Formulation: In a text generation task, let $D_{trn} = \\{(x_i, y_i)\\}_{i=1...n}$ be the training set, with a development set $D_{dev}$ and a test set $D_{tst}$. Here, $x$ represents the input data, and $y$ is the corresponding ground truth reference. A task prompt $p$ comprises instructions that, when filled with input $x$, are fed to a black-box API $LLM_{task}^1$ to generate a completion $\\hat{y} = LLM_{task}(p, x)$. The goal is to optimize $p$ using $D_{trn}$ and $D_{dev}$ to identify an optimal prompt $p^*$ that maximizes performance on one or more evaluation metrics on $D_{tst}$.\nCriSPO Overview: CriSPO is an automatic prompt optimization algorithm designed to iteratively refine a task prompt $p$ from an initial seed prompt $p_0$ to the optimum: $p^* \\leftarrow F(p_0)$. In each iteration $t$, we conduct the following steps:\nEvaluate on $D_{trn}$: Apply the candidate prompt $p_t$ on $D_{trn}$, call $LLM_{task}$ to generate outputs $\\{\\hat{y}_i\\}_{i=1...n}$ and compute a primary metric $s_t$, which can be a single metric or an aggregation of multiple metrics.\nGenerate Critiques and Suggestions: Apply the multi-aspect critique-suggestion meta-prompt $M_c$ and call"}, {"title": "Multi-Aspect Critiques and Suggestions", "content": "Given a prompt $p_t$ and its outputs $\\{\\hat{y}_i\\}$ on $D_{trn}$, we design a multi-aspect critique-suggestion meta-prompt $M_c$ to identify critiques \u2013 flaws of the generated outputs across multiple aspects, and suggestions \u2013 specific edits on the task prompt to rectify each flaw.\nConstructive critiques with spontaneous dimension discovery: In $M_c$, we first instruct $LLM_{crit}$ to generate several task-specific and iteration-specific aspects for a given batch of outputs from the current $p_t$. This approach ensures that as task prompts evolve across iterations, the focus remains on relevant aspects, addressing specific issues that arise. Figure 2 illustrates the aspects discovered during optimization. For each aspect, $M_c$ instructs $LLM_{crit}$ to generates a critique highlighting potential problems of the outputs generated with $p_t$ on the batch.\nMulti-aspect suggestions: In line with each critique, a corresponding suggestion is made by $LLM_{crit}$ to edit $p_t$. As opposed to Pryzant et al. (2023), we decoupled the edit"}, {"title": "Receptive Prompt Optimizer", "content": "Our receptive prompt optimizer meta-prompt $M_o$ improves over the OPRO optimizer meta-prompt (Yang et al. 2023) by enriching its optimization trajectory $\\{(p_k, s_k)\\}$ with past critiques and suggestions $c_k$. Thus, ours samples candidate prompts for the next iteration conditioned on an enriched optimization trajectory:\n$p_{t+1} = LLM_{opti}(M_o, \\{(p_k, s_k, c_k)\\})$.\nSpecifically, we enhance the OPRO optimizer module with the following three improvements to better utilize critiques and suggestions for achieving stronger guidance and better exploration. See Appendix H for all $M_o$ by LLMs and tasks."}, {"title": "Multi-Metric Automatic Suffix Tuning", "content": "Using components in Section 3.1 and 3.2, CriSPO is ready to optimize a primary metric. To benefit from more teaching signals, e.g., completeness and faithfulness, here we extend CriSPO to multi-metric optimization by proposing a novel multi-metric learning extension named as AST.\nIn AST, we propose to optimize a suffix postscript $\\sigma$ appended to $p^*$, which has already been trained on certain metrics. $p^*$ will remain fixed throughout the whole tuning process for a new metric to preserve most of its performance on existing metrics. $p^*$ is extended with an additional suffix $\\sigma^* \\leftarrow F(\\sigma_0)$, which serves as a postscript to steer the LLM toward the new metric and remedy any potential regression in performance on existing metrics. Specifically, we provide both the main prompt $p^*$ and each suffix $o_t$ in the meta-prompts while asking the LLM to critique or refine only the suffix. To ensure we maintain existing metrics while improving on the additional metric, we take inspirations from"}, {"title": "Main Experiments", "content": "Datasets We select a diverse range of 4 summarization tasks including conventional document summarization tasks such as CNN daily mail (Hermann et al. 2015) (news headline summarization), and also conversation summarization tasks such as SAMSum (Gliwa et al. 2019), MeetingBank (Hu et al. 2023). In addition, we test on a medical-domain clinical note summarization task, Dialogue2Note (Yim et al. 2023). Detailed data setup can be found in Appendix C. These tasks cover various lengths, domains and styles as summarized in Table 9. We report ROUGE-1/2/L F-measure (Lin 2004) to measure output similarity to the references.\nLLMs and Baselines We test our approach on state-of-the-art LLMs including proprietary models: Claude Instant (Anthropic 2023), Claude3 Sonnet (Anthropic 2024), and open-source LLMs: Mistral 7B (Jiang et al. 2023) and Llama3 8B (MetaAI 2024). We use the same LLM for all the 3 CriSPO modules: task inference, critique-suggestion and receptive optimization, apart from the Llama3 setup. Specific hyperparameters with ablations are detailed in Appendix D.\nOur baseline methods include manual prompts with zero/few-shot ICL. These manual prompts are carefully tuned for each task to incorporate length constraints and task guidelines, and therefore establish a high bar of performance from manual prompt engineering (Appendix I). Given there are no existing automatic prompting results for text generation, we adapted OPRO (Yang et al. 2023), a competitive established approach, on our selected tasks. We use the same hyper-parameter setup in OPRO and CriSPO for fair comparison."}, {"title": "Main Results", "content": "As shown in Table 1, across all the tasks and LLMs, CriSPO consistently improves over 0-shot manual prompt and OPRO baselines. Overall, there are approximately 3-4 point improvements for all LLMs. Even the strong state-of-the-art Claude3 Sonnet model can still greatly benefit from CriSPO. The consistent improvement shows CriSPO is a more effective search method than existing method (OPRO) to unlock the full potential of these LLMs, and offers an alternative solution to the more labour-intensive manual prompt engineering.\nAdditionally, we found examples to be helpful as adding 3-shot ICL significantly improves the performance. Owning to the versatile template in CriSPO, we can easily integrate examples and we show CriSPO 3-shot can further boost performance over CriSPO and achieves the best performance in most setups. It is also worth noticing that the vanilla CriSPO"}, {"title": "Ablating Key Ingredients", "content": "Table 2 shows the ablation results of CriSPO with Claude Instant on SAMSum dataset. We observed that the three key components in our approach, including flexible template, critique and step-by-step CoT optimization, are essential for achieving optimal performance. Removing any of these components leads to a decrease in performance. Removing critique-suggestion module and CoT optimization altogether leads to a 5 point decrease, similar to OPRO performance. This indicates these two elements are essential to the success of CriSPO and flexible template is only effective when being added on top of these two elements."}, {"title": "Qualitative Analysis and Human Evaluation", "content": "To qualitatively compare CriSPO outputs with the baselines, we conducted human evaluation on 20 examples from the"}, {"title": "Quantitative Analysis of Prompt Diversity", "content": "To verify that our design in Section 3 leads to a better exploration of the solution space, we quantitatively analyze the diversity of prompts found by CriSPO and OPRO (same hyperparameters, Section 4.1) on the summarization datasets. We measure 4 aggregated properties on all task prompts explored by each method during optimization: length (number of words), vocabulary size (number of unique words used), and pairwise ROUGE-L/semantic similarity. For pairwise semantic similarity, we employ Sentence Transformers"}, {"title": "Extension with Multi-Metric Optimization", "content": "AST Setup In this experiment, we extend CriSPO with our proposed AST to optimize multiple metrics simultaneously. Specifically, we take the best prompts optimized for ROUGE-1 F-measure from CriSPO with Claude Instant as the seed main prompt $p^*$. We employ AST to optimize AlignScore (Zha et al. 2023) starting from a simple seed suffix $\\sigma_0$: \u201cEvery word of your summary must be faithful to the input/conversation\u201d across all datasets. The AlignScore between the input text and the output summary is used as a signal reflecting the faithfulness. With regard to baselines, we report the initial performance in ROUGE-1 F-measure and AlignScore of the seed main prompt w/ and w/o the seed suffix. We also provide a strong baseline to tune both the main prompt and its suffix together (full tuning) rather than only the suffix in AST.\nResults The results for multi-metric optimization are presented in Table 6. On all datasets, our AST is able to optimize the new metric AlignScore with a negligible or zero regression on the existing metric ROUGE, meaning that AST can reduce LLM hallucination while maintaining relevancy in the output. In particular, AST dramatically improves AlignScore by 11.7 points on CNN. Across tasks, AST is the most effective approach to improve AlignScore while maintaining ROUGE. Among all methods, AST is the only one that brings consistent improvement on AlignScore for every task, and achieves the best average overall improvement (by 4.3). The main prompt w/ suffix seed prompt slightly improves AlignScore (by 1.2) and the full-tuning baseline only meaningfully improves AlignScore on CNN and the overall improvement is marginal (by 0.7). The superiority of AST shows that it can robustly optimize multiple metrics across various domains."}, {"title": "Generalization to Other Tasks", "content": "To confirm its generalizability, in this section, we apply CriSPO to extractive, abstractive and multi-choice QA tasks.\nDatasets We benchmark CriSPO on 5 commonly used QA datasets, including 1) Wikipedia-based QA: Natural Questions (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017), Squad (Rajpurkar et al. 2016) 2) story-based abstractive reading comprehension: NarrativeQA (Ko\u010disk\u1ef3 et al. 2018) and 3) medical domain multiple-choice QA: MedMCQA (Pal, Umapathi, and Sankarasubbu 2022). For Natural Questions and TrivialQA, we also incorporate the RAG setup to optimize the prompt template with inserted pre-retrieved contexts from each dataset. We retrieved the Wikipedia pages following Izacard and Grave (2021). For NarrativeQA, we use summaries as contexts. For MedMCQA, we cast it to text generation by eliciting reasoning before the final answer. Following the conventions, we report Exact Match for Natural Questions and TriviaQA, F1 for Squad, ROUGE-L for NarrativeQA, accuracy for MedMCQA. For efficiency, we only used a small fraction of the train and dev set for the experiments. The specific data settings are listed in Appendix C.\nResults Similar to summarization tasks, we observe CriSPO significantly outperforms the manual prompt and OPRO baseline in various QA datasets as shown in Table 7. For NarrativeQA, CriSPO brings massive improvement (+10 ROUGE-L) compared with baselines, achieving the new SOTA performance. For Natural Questions and TrivialQA, CriSPO has no issue incorporating the RAG setup and achiev-"}, {"title": "Conclusion", "content": "In this paper, we tackle the challenging problem of automatic prompt engineering for text generation. We propose CriSPO, a multi-aspect critique-suggestion guided optimizer augmented with enriched trajectory, CoT and flexible template. Our experiments show multi-aspect critique-suggestion is critical for finding good task prompts. Overall, CriSPO achieves 3-4% ROUGE score improvement and 4-5% human rating increase compared to baseline methods for summarization, and significant improvement for QA. We also show that CriSPO can effectively optimize multiple metrics through a novel suffix tuning extension AST, and incorporate ICL and RAG with flexible prompt templates. Ablation studies confirm the effectiveness of all CriSPO components. Human evaluation and quantitative analysis show CriSPO encourages more effective prompt exploration and the optimized prompts can better capture task requirements."}, {"title": "Limitations", "content": "The list of LLMs in our experiments is meant to be representative rather than exhaustive. We recognize that supervised fine-tuning can outperform prompt engineering on certain metrics. We also acknowledge the ongoing research on the limitations of automatic evaluation metrics for text generation. In addition, CriSPO could be costly in LLM API tokens especially with long input. Finally, while our experiments focus on summarization and QA, CriSPO can be adaptable to other text generation tasks, which we leave for future research. See Appendix B for a detailed limitations discussion."}, {"title": "A Complete Working Example", "content": "Table 8 shows a full working example of CriSPO."}, {"title": "Dataset Setting", "content": "For CNN, SAMSum, MeetingBank, MedMCQA, NarrativeQA and SQUAD, we use the HuggingFace datasets repository. For Dialogue2Note, we use the data from Task B at ACL ClinicalNLP MEDIQAChat shared task 2023 in the Aci-bench dataset\u00b3 (Yim et al. 2023). For Natural Questions, we follow the data preparation in FiD4 (Izacard and Grave 2021).\nOur experiments are conducted with sampled train and dev set. For Dialogue2Note, we used the full training (67), development (20) and test set (40). For other summarization tasks, we randomly selected 500 samples from the full test set as our test set. To show the efficiency of our approach, we used a small fraction of the train and development set. For CNN, we sampled 100 training samples as our training set, and 100 development samples as our development set. For other tasks, we randomly sampled 50 training samples as our training set, and 50 development samples as our development set.\nFor NQ and TQA, we randomly sample 200/200/500 examples for training/development/test set. Each example has 100 context paragraphs from Wikipedia and each paragraph has 100 words following Izacard and Grave (2021). We use only the top 20 context paragraphs in our experiments because of the high inference cost for long text.\nFor NarrativeQA and MedMCQA, we randomly sample 100/100/500 for training/development/test set respectively. For Squad, we sample 50/50/500 for training/development/test set respectively."}, {"title": "CriSPO Settings for Different LLMs", "content": "Claude Settings: In suggestion-critique meta-prompt, we pass 10 randomly selected examples for the LLM to provide critique. In optimizer meta-prompt, we use 10 history task prompts with their critiques, suggestions and scores. We add 2 input/output examples in the optimizer prompt.\nMistral Settings: Mistral has a shorter context window. Therefore, we adjust the settings. We reduce the task prompt history to 1 in optimizer meta-prompt. On MeetingBank dataset, we truncate the input document to 3500 words, and do not provide the input (only use generated text and reference) in the critique-suggestion meta-prompt.\nLlama3 Settings: The context window of Llama3 is insufficient to fit a few examples and generate meaningful critique-suggestions. Therefore, we use Claude3 Sonnet as the critique-suggestion LLM and the receptive optimizer LLM. Llama3 is used only as the task LLM.\nFor all experiments, we set the temperature of the meta-prompt LLMs used for the optimization to be 1.0 to encourage diversity, and we set the scorer LLM's temperature to be"}, {"title": "Multi-Aspect Critique-Suggestion Meta-Prompt", "content": ""}, {"title": "Mistral for Summarization", "content": ""}, {"title": "Claude for RAG", "content": ""}, {"title": "Receptive Optimizer Meta-Prompt", "content": ""}, {"title": "Clause for Summarization", "content": ""}, {"title": "Clause for RAG", "content": ""}, {"title": "Manual Prompts", "content": "We present the manual prompts for the summarization experiments with the Claude instant model. INSERT_INPUT_HERE in each prompt indicates the position where we will insert the input text. INSERT_EXAMPLES_HERE indicates the position where we will insert few-shot examples. Each example is in the format of\nFor the few-shot setup, we first encode inputs with BERT embeddings (Devlin et al. 2019), then retrieve their most similar examples from the train set according to the cosine similarity (Liu et al. 2022)."}, {"title": "Zero-shot CNN", "content": "Here is an input CNN news document:\nINSERT_INPUT_HERE\nPlease write a headline summary between around 50 to 100 words within tags."}, {"title": "Few-shot CNN", "content": "Write a headline summary between around 50 to 100 words for the CNN news document. Here are example input documents and example output summaries\nINSERT_EXAMPLES_HERE\nHere is an input CNN news document:\nINSERT_INPUT_HERE\nPlease write a headline summary between around 50 to 100 words within tags."}, {"title": "Zero-shot SAMSum", "content": "Here is an input conversation:\nINSERT_INPUT_HERE\nPlease write a summary for the input conversation within tags. The summary should (1) be rather short with 20 to 50 words, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person."}, {"title": "Few-shot SAMSum", "content": "Write a summary within tags for the input conversation. Here are example input conversations and example output summaries\nINSERT_EXAMPLES_HERE\nHere is the input conversation:\nINSERT_INPUT_HERE\nFollowing the examples, please write a summary for the input conversation within tags. The summary should (1) be rather short with 20 to 50 words, (2) extract important pieces of information, (3) include names of interlocutors, (4) be written in the third person."}, {"title": "Zero-shot MeetingBank", "content": "Here is an input conversation from city council meeting:\nINSERT_INPUT_HERE\nPlease write a summary of the discussion with around 60 to 150 words within tags."}, {"title": "Few-shot MeetingBank", "content": "Write a summary for the input city council meeting. Here are example input meeting conversations and example output summaries\nINSERT_EXAMPLES_HERE\nHere is an input conversation from a city council meeting:\nINSERT_INPUT_HERE\nFollowing the examples, please write a summary of the discussion from the input conversation with around 60 to 150 words within tags."}, {"title": "Zero-shot Dialogue2Note", "content": "Here is an input conversation of a clinical visit:\nINSERT_INPUT_HERE\nPlease write a detailed clinical note summary for the input conversation within tags."}, {"title": "Few-shot Dialogue2Note", "content": "Write a clinical note summary within tags for the input conversation of a clinical visit. Here are example input conversations and example output summaries\nINSERT_EXAMPLES_HERE\nHere is the input conversation:\nINSERT_INPUT_HERE\nFollowing the examples, please write a clinical note summary for the input conversation within tags"}, {"title": "Manual Prompt Tuning", "content": "While it is not possible to exhaust all prompt variations with manual prompt engineering, we experimented with several iterations of manual prompts and presented the best prompt results. Below, we show that our tuned zero-shot manual prompts (ours) significantly outperform zero-shot naive prompts (\u201cWrite a summary for the input text\u201d), and the results from our manual prompts can be regarded as a reasonable baseline from human prompt engineering."}, {"title": "Best QA Prompts Found using CriSPO (Claude Instant)", "content": ""}, {"title": "Natural Questions", "content": "Consider INSERT_QUESTION_HERE and all provided INSERT_CONTEXT_HERE. Write a concise answer in tags focusing only on the single most important attribute implied across contexts. Then compare your answer to the gold below through reasoning: cite how your intended meaning matches theirs on attributes like level of precision/detail implied jointly by contexts. It is acceptable for your answer to have less context than the gold if the meaning remains clear, like using a single word versus a phrase. Explain any differences"}, {"title": "TriviaQA", "content": "Read the question and contexts carefully. Extract the key detail(s) directly answering the question from the most relevant context(s). Write your response in  tags matching the style and level of detail of the example gold answers. Consider using a single word, number, or short phrase if that fully answers the question precisely. Compare your answer to the examples, considering alternatives suggested in the contexts and relationships between entities. Aim for consistency with the gold answers in terms of words used, precision, and completeness of specification."}, {"title": "MedMCQA", "content": "QUESTION_PLACEHOLDER Provide your answer, and comprehensively reason through it by referencing authoritative medical sources, accounting for all relevant context in the question, logically laying out your reasoning steps, and addressing any applicable exceptions or nuances. Your response should demonstrate a rigorous application of established medical knowledge.\nChose an option and write it in  XML tags"}, {"title": "NarrativeQA", "content": "Provide a focused, concise answer in the form of a 1-3 word phrase or brief quote, enclosed in  tags. Capture all key details directly relevant to fully addressing the question, while excluding extraneous background information or repetition of context details. If a short quote from the context directly and precisely answers the question in a maximally concise manner, use the quote verbatim. Otherwise, paraphrase the essential information as succinctly as possible. The goal is a clear, to-the-point response that comprehensively answers the core of the question without omitting crucial details or including unnecessary information."}, {"title": "Squad", "content": "Your task is to answer the question as concisely as possible using only the minimum information explicitly asked for. Carefully examine the question to understand exactly what specific detail is being requested, then scan the context to extract only that precise piece of information to satisfy the question no more and no less. Avoid including any additional context, descriptors or embellishments beyond the single term or brief phrase strictly necessary to directly answer what is asked. Refer to the examples, where \"pub landlord\" and \"French alone is the official language\" are the minimum possible responses. Do not exceed these examples in length or level of detail. Write only the clearest, most succinct answer in tags."}, {"title": "Ablation Study Prompts", "content": "Pre-defined multi-aspect critique-suggestion meta-prompt:\nVerbosity and length: compare the level of details and the length between prediction and reference summaries\nComprehensiveness: compare whether the prediction covers all the information from the reference summaries\nPrecision: compare whether the information from the prediction summaries are present in the reference summaries.\nStyle: compare the formatting, formality, word choices sentence structures etc."}, {"title": "Full Metrics for Summarization", "content": "We report the average and standard deviation from 3 runs for Rougel (Table 11), Rouge2 (Table 12), RougeL (Table 13), BertScore (Table 14) and AlignScore (Table 15)."}, {"title": "Standard Deviation for QA", "content": "Table 16 shows the full results for QA (Question Answering) datasets with standard deviation reported over three runs."}]}