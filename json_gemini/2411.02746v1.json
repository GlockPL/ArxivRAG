{"title": "A Bayesian explanation of machine learning models based on modes and functional ANOVA", "authors": ["Quan Long"], "abstract": "Most methods in explainable AI (XAI) focus on providing reasons for the prediction of a given set of features. However, we solve an inverse explanation problem, i.e., given the deviation of a label, find the reasons of this deviation. We use a Bayesian framework to recover the \"true\" features, conditioned on the observed label value. We efficiently explain the deviation of a label value from the mode, by identifying and ranking the influential features using the \"distances\" in the ANOVA functional decomposition. We show that the new method is more human-intuitive and robust than methods based on mean values, e.g., SHapley Additive exPlanations (SHAP values). The extra costs of solving a Bayesian inverse problem are dimension-independent.", "sections": [{"title": "Introduction", "content": "Explaining the deviation of an observation is of great practical importance. For example, many industrial suppliers' on-time delivery rates significantly dropped in 2021 due to the COVID-19 associated factors, such as lock-downs and lack of labors. The productivity of a manufacturing facility may deviate from its calibrated value due to aging of equipment, unexpected shifts of the controls and abnormal climates. Which factors and to what extends they contribute to the change of the output are among the typical questions a decision maker would inquire about.\nRecent explainable AI (XAI) has been focusing on explaining a forward model, i.e., methods have been developed to estimate the impacts of the features on a prediction. The local interpretable model-agnostic explanation (LIME) developed by Ribeiro et al. (2016) constructs a linear local model whose coefficients explicitly explain the importance of the corresponding features. Hence, it is only valid in the region where the linear model is a good approximation of the original model. The SHAP values (see the works of Lipovetsky & Conklin (2001); Lundberg & Lee (2017); Tsai et al. (2023) for details) provide scores indicating the importance of each feature. The scores measure the differences between the predictions of the model trained by the whole dataset and the predictions of the model trained on a sub-dataset without the subset of the respective features. Hooker (2007) uses ANOVA functional decomposition of a model for explanation. It represents another important stream of methods, which stem from statistical design of experiments. Hooker (2007) also shows both the responsible scores in SHAP and the terms in ANOVA are combinations of conditional expectations of the original model. Analyses in the recent literature, e.g., studies by Owen (2014) and Herren & Hahn (2022), show that ANOVA provides sharp bounds for the SHAP values and are identical to them up to a proportional constant for linear models.\nBayes theorem provides a systematic framework to compute a conditional distribution by using prior distribution and a likelihood function. Much has been written about the applications of Bayes"}, {"title": "Bayesian explanation of a label value", "content": "We use the following form of Bayes theorem to inversely compute the distributions of the features, conditioned on the observed labels:\n$p(x_i | y_i) = \\frac{p(y_i | x_i) p(x_i)}{p(y_i)}, i = 1, ..., N$\nwhere $p(\\cdot)$ is a probability density function (pdf), $p(x_i|y_i)$ is the posterior pdf of feature $x \\in \\mathbb{R}^{d_x}$ given the label $y_i \\in \\mathbb{R}$, $p(y_i|x_i)$ is the likelihood function, $p(x_i)$ is the prior pdf of feature $x_i$, $P(y_i) = \\int_{x_i \\in \\Omega} p(y_i|x_i) p(x_i) dx_i$ is a normalization constant, $\\Omega$ is the parametric space of $x$, $N$ is the number of samples - pairs of $x_i$, and $y_i$ in a dataset. The likelihood function can be further written as follows:\n$p(y_i | x_i) = \\frac{1}{\\sqrt{(2\\pi \\prod_{i=1,...,v} \\sigma_i^2)^{1/2}}} e^{-\\frac{1}{2} \\sum_{i=1}^{v} [y_i - f(x_i, \\theta^*)]^2 / \\sigma_i^2}$ , \nwhere $f(x, \\theta^*)$ is a trained machine learning model, e.g., linear regression, extreme gradient boosting, neural network, etc. In a typical training process, optimization is used to obtain $\\theta^*$, e.g., the mean square error is minimized to obtain the coefficients of a regression model as below:\n$\\theta^* = arg \\min_{\\theta \\in \\Theta} \\sum_{i=1}^{N} [y_i - f(x_i, \\theta)]^2$,\nwhere $\\theta$ is the vector of the coefficients in the regression model, $\\Theta$ is the parametric space. Using the fore-mentioned Bayesian approach, we can solve for the \"true\" values of the features, namely the maximum a posteriori (MAP) estimate of the features, given the label $y_i$:\n$x_i = arg \\max_{x \\in \\Omega} log[p(x_i | y_i)].  (1)$\nFor the sake of conciseness, we will neglect $\\theta^*$ in $f(x_i, \\theta)$ in the rest of the paper."}, {"title": "Explaining a label's deviation", "content": "Explaining the deviation of a label from a reference is a rarely investigated topic, except several recent attempts on outlier explanations, e.g. by Id\u00e9 et al. (2021), where the authors treat the probability estimates of the rare events as the quantity of interest and compute the associated SHAP value. We"}, {"title": "Mode-specific deviation", "content": "In this section, we introduce mode specific deviations, because the deviation from mean value is non-intuitive in many scenarios, e.g., the dataset has multiple modes, or the mode and mean values differ significantly. Replacing y in Equation (2) by a mode leads to the following definition of mode-specific deviation:\n$\\delta^m_i = y_i - y_m = f(x_i) - f(x_m) + \\epsilon_i$,\nwhere $y_m$ is the $m^{th}$ mode. Consequently, we decompose the above deviation using ANOVA functional decomposition as follows:\n$f(x_i) - f(x_m) + \\epsilon_i = \\sum_{I=1}^{d_x} \\delta^m_{iI} + \\sum_{I<J} \\delta^m_{iIJ} + \\sum_{I<J<K} \\delta^m_{iIJK} + ... + \\epsilon_i$,\nwith\n$\\delta^m_{iI} = f_I(x_{iI}) - f_I(x_{mI})$,\n$\\delta^m_{iIJ} = f_{I,J}(x_{iI}, x_{iJ}) - f_{I,J}(x_{mI}, x_{mJ})$,\n$\\delta^m_{iIJK} = f_{I,J,K}(x_{iI}, x_{iJ}, x_{iK}) - f_{I,J,K}(x_{mI}, x_{mJ}, x_{mK})$,\nand\n$x_m = arg \\max_{x \\in \\Omega} log (p(y_m | x)) + log (p(x))$,  (4)\nwhere the log likelihood function is\n$log (p(y_m | x)) \\propto \\frac{[y_m - f(x)]^2}{2 \\sigma_e^2}$,\nand\n$\\sigma_e^2 = \\frac{\\sum_{i=1}^{N} (f(x_i) - y_i)^2}{N}$"}, {"title": "Direct search of the MAP", "content": "(1), (4) and (3) and may have multiple local optimums. Nevertheless, we use several runs of optimization to find a collection of local optimums of the maximization problems. Then, we can obtain the MAP in the set of the local optimums. The steps are described in Algorithm 1 as shown above.\nLong (2022) shown that the number of necessary runs is a dimension-independent value bounded by the inequality below:\n$n_o \\ge \\frac{log \\frac{1}{\\beta} - log K}{log(1-p)}$,\nwhere K is the number of local optimums, p is the minimum probability that an independent and identically distributed (i.i.d.) initial value of x converges to a local optimum, $\\beta$ is the probability that $n_o$ runs of optimization do not find all the local optimums."}, {"title": "Mode-specific responsible scores", "content": "We define the responsible scores, which are the contributions to $\\delta_i^m$ by a feature or a subset of the features:\n$\\frac{\\delta^m_{iI}}{\\delta_i^m}, \\frac{\\delta^m_{iIJ}}{\\delta_i^m}, \\frac{\\delta^m_{iIJK}}{\\delta_i^m}$"}, {"title": "Outlier reasoning using synthetic data", "content": "In this example we first draw i.i.d. samples of $x_0, x_1$ and $x_2$. $x_0$ is drawn from the Gaussian mixture distribution $0.3N(0,1) + 0.3N(4,1) + 0.4N(8,0.5^2)$. $x_1$ is drawn from the Gaussian mixture distribution $0.3N(0,1) + 0.3N(4, 1) + 0.4N(8,0.75^2)$. $x_2$ is drawn from the Gaussian mixture distribution $0.3N(0,1) + 0.3N(4,1) + 0.4N(8, 1)$. They are independent to each other. We use $y = x_0 + x_1 + x_2$ to synthetically generate the samples of the dependent variable y. The marginal distributions of $x_0, x_1, x_2$ and y are visualized in Figure 1.\nIn total, 10000 synthetic data points are generated. A Gaussian mixture based clustering method by Yan et al. (2017) is used to find the clusters in the set of the labels. The dominant mode is $y^* = 15.7$. The mean value is $\\overline{y} = 13.2$.\nThe smallest y in the sample is \u20136.2. Its z-score and mode-based z-score ($z_m = \\frac{\\delta_i^m}{\\sigma_m}$) are listed in Table 1. We define the $m^{th}$ mode based z-score as the distance from the sample to the $m^{th}$ mode, normalized by $\\sigma_m$ - the standard deviation of the $m^{th}$ mode of the approximated Gaussian mixture. The independent variables corresponding to the outlier of $y = -6.2$ are $x_0 = -2.5, x_1 = -1.7$ and $x_2 = -2.0$.\nA linear regression model is used to fit the 10000 points in the sample. We use $p(x) = p(x_1)p(x_2)p(x_3)$, where $p(x_1), p(x_2)$ and $p(x_3)$ are the fore-mentioned Gaussian mixtures. We then"}, {"title": "Explaining seasonal flows of a river", "content": "In this experiment, we use our approach to explain the river flow measured at the New Jumbles Rock station (njr) in England. There are three upstream rivers, namely Hodder Place (hp), Henthorn (h) and Whalley Weir (ww). This dataset was also used in Id\u00e9 et al. (2021) for root-cause analysis. River flows have been measured in each station at 9:00 am daily, since 1 January 2010. We use the daily river flow at njr as the label, and the daily river flows at hp, h and ww as the features to train a machine learning model.\nWe first use a linear regression model to fit the dataset. The $R^2 = 98.9\\%$ in predicting the training labels, while $R^2 = 99.2\\%$ in predicting the testing labels. The estimated coefficients are 1.2, 0.6 and 1.02 with respect to $r_{dnjr}, r_{dhp}$ and $r_{dww}$ ($r_d$ indicate upstream rivers).\nWe then use our approach described in Sections 3 and 4 to explain the flow's deviations in the time range between June 20, 2020 and July 09, 2020. Figure 3 shows the time histories of the river flow measurements. The mode of the downstream flow is 12.2 marked by the dash-dotted line in Figure 3. The corresponding MAP estimate of the upstream flows are 3.7, 7.4, and 2.6 for $r_{dnjr}, r_{dhp}$ and $r_{dww}$, respectively. The mean values of the downstream flow is 63.4, denoted by the dashed line in Figure 3. The mean values of the upstream flows are 26.4, 21.7 and 16.1 for $r_{dnjr}, r_{dhp}$ and $r_{dww}$, respectively. We highlight the ten largest $r_{dnjr}$ ($r_d$ means downstream river) river flow measurements, denoted by $l_1, ..., l_{10}$ in Figure 3."}, {"title": "Conclusion", "content": "We use a systematic Bayesian framework to explain deviations of an AI model from its mode. The contribution of the features, namely the responsible scores, are defined as the distances between the Bayesian MAPs in the ANOVA functional subspace. We extend the conventional mean-value-based scores to mode-based scores, which is more robust and human-intuitive. The new approach is tested using two experiments. The first example shows that the mode-based method provides better human-intuitive results than the mean-value-based methods in a multimodal scenario. In the second experiment, we use the proposed approach to explain the flow outliers of a major river in England. The approach is able to explain the importance of the features in terms of their corresponding functional decompositions of the AI models. The mode-based main responsible scores show salient stability in the vicinity of mean values, while existing method becomes unstable."}, {"title": "Appendix", "content": ""}, {"title": "Function decomposition", "content": "The $f(x_i)$ can be decomposed using ANOVA representation as follows (see Sobol (2001); Saltelli et al. (2010) for details)\n$f(x_i) = f_0 + \\sum_{I=1}^{d_x} f_I(x_{iI}) + \\sum_{I<J} f_{IJ}(x_{iI}, x_{iJ}) + \\sum_{I<J<K} f_{IJK}(x_{iI}, x_{iJ}, x_{iK}) + ... + f_{12...d_x} (x_{i1}, x_{i2}, ..., x_{id_x})$,\nwhere the terms of the RHS are the expectations of f conditioned on an increasing subset of the random inputs x. Specifically, the first order and second order functions can be written as follows:\n$f_I(x_{iI}) = E_{x_{i\\setminus iI}}(f | x_{iI}) - f_0$,\n$f_{IJ}(x_{iI}, x_{iJ}) = E_{x_{i\\setminus (iI, iJ)}}(f | x_{iI}, x_{iJ}) - f_I(x_{iI}) - f_I(x_{iJ}) - f_0$."}, {"title": "Estimation of the conditional functions using Monte Carlo", "content": "The first order ANOVA of $d_{iI}$ can be estimated numerically using random samples.\n$\\delta_{iI} = E_{x\\setminus iI}(f | x_{iI}) - E_{x\\setminus iI}(f | x_{iI}') = \\frac{1}{N_P} \\sum_{s=1}^{N_P} f(x_s | x_{iI}) - \\frac{1}{N_P} \\sum_{s=1}^{N_P} f(x_s' | x_{iI}') + O_p(\\frac{1}{\\sqrt{N_P}})$.   (5)"}]}