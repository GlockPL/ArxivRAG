{"title": "Joint Optimization of Resource Allocation and Data Selection for Fast and Cost-Efficient Federated Edge Learning", "authors": ["Yunjian Jia", "Zhen Huang", "Jiping Yan", "Yulu Zhang", "Kun Luo", "Wanli Wen"], "abstract": "Abstract\u2014Deploying federated learning at the wireless edge introduces federated edge learning (FEEL). Given FEEL's limited communication resources and potential mislabeled data on devices, improper resource allocation or data selection can hurt convergence speed and increase training costs. Thus, to realize an efficient FEEL system, this paper emphasizes jointly optimizing resource allocation and data selection. Specifically, in this work, through rigorously modeling the training process and deriving an upper bound on FEEL's one-round convergence rate, we establish a problem of joint resource allocation and data selection, which, unfortunately, cannot be solved directly. Toward this end, we equivalently transform the original problem into a solvable form via a variable substitution and then break it into two subproblems, that is, the resource allocation problem and the data selection problem. The two subproblems are mixed-integer non-convex and integer non-convex problems, respectively, and achieving their optimal solutions is a challenging task. Based on the matching theory and applying the convex-concave procedure and gradient projection methods, we devise a low-complexity suboptimal algorithm for the two subproblems, respectively. Finally, the superiority of our proposed scheme of joint resource allocation and data selection is validated by numerical results.\nIndex Terms\u2014Federated edge learning, mislabeling, data selection, training cost, resource allocation.", "sections": [{"title": "I. INTRODUCTION", "content": "It is estimated that there will be 29.3 billion networked devices, such as smart phones, pads, wearable devices and other consumer electronics, by 2024 around the world [1]. These devices will inevitably generate huge amounts of data at the wireless edge that can be applicable for multifarious machine learning (ML) tasks, e.g., autonomous driving and product recommendation. Traditional ML algorithms need to expose the raw data to third-party entities for model training, which, however, may compromise data privacy [2]. To tackle such privacy issue, researchers in the field of wireless communications integrate federated leaning with mobile edge computing, thus forming a concept known as federated edge learning (FEEL) [3]\u2013[6]. In the training process of FEEL, the devices must send their local training results such as gradients or model parameters, instead of the raw data, via wireless channels. Since the available radio resources such as\nbandwidth and time at the network edge are constrained, it is necessary to allocate appropriate radio resources for each device during model training. In addition, the data owned by the devices may be mislabeled in practice, for example, a hand-written digit \"1\" may be labeled as \"0\", and an image \"t-shirt\" may be labeled as \u201ctrouser\u201d. Training ML models on such mislabeled data can seriously deteriorate the convergence of FEEL, so it is necessary to select local data appropriately during model training [7].\nTo date, there have been many research efforts on the resource allocation for FEEL, among which some representative researches are [8]\u2013[23]. Specifically, the authors in [8]\u2013[15] developed an efficient joint device scheduling and wireless resource allocation scheme respectively, aiming to minimize devices' total energy cost [8]\u2013[10] and learning time cost [9], maximize the weighted sum data rate [11], or speed up the convergence of FEEL [12]\u2013[16]. The authors in [17] and [18] established a resource allocation problem to reduce the weighted sum of FEEL training time and total energy cost of all devices. A joint device scheduling and resource management strategy is developed in [19], which can significantly speed up model training and save energy costs. Additionally, a joint optimization of the processing-rate, the uplink Non-orthogonal Multiple Access (NOMA) transmission duration, and the broadcasting duration, as well as the accuracy of the local training was proposed in [20], with the aim of minimizing the system-wise cost including the total energy consumption and the FEEL convergence latency. It is worth noting that excessive energy cost may prevent devices from participating in model training, thus reducing the performance of FEEL. However, how to deal with this issue has not been well studied in the literature [8]\u2013[20]. There are several ways to encourage devices to join in the model training process, such as rewarding the devices appropriately to compensate for their energy costs, as done in [21]\u2013[23]. More specifically, the authors in [21]\u2013[23] proposed to reward all devices based on the number of CPU cycles [21] or the quantity of available training samples [22], [23] that they are willing to contribute, and the greater the contribution, the higher the reward. On this basis, they further focused on how to achieve the desirable resource allocation. Note that the above works have never considered the design of data selection in the FEEL system, so the training algorithm they proposed may not be applicable for scenarios in which some local data are mislabeled.\nAs for the design of the data selection in FEEL, there are not many works in this direction recently, and some representative works include [24]\u2013[28]. Specifically, the work in [24] demonstrated the negative influence of data mislabeling on training performance and proposed an efficient data selection"}, {"title": "II. SYSTEM MODEL", "content": "We investigate a generic FEEL system, which consists of one edge server and $K$ devices, as depicted in Fig. 1. To maintain the simplicity of analysis and optimization, we assume that both the BS and devices are equipped with a single antenna. Let $\\mathcal{K} = \\{1,2,\\ldots,K\\}$ denote the user set. Note that we assume that all devices are legitimate, with no malicious devices. Let $D_k = \\{(x_j, y_j)\\}$ denote the dataset of the $k$-th device. Here, $x_j \\in \\mathbb{R}^d$ is the $d$-dimensional data sample, $y_j \\in \\mathbb{R}$ denotes the data label, and $|\\cdot|$ depicts the cardinality of a set. We define $D = \\bigcup_{k \\in \\mathcal{K}} D_k$ as the whole dataset resided on all devices. Table I summarized the main notations used in this paper.\n\\subsection{Learning Model}\nThe FEEL system can learn an ML model $w \\in \\mathbb{R}^d$ over the datasets of all devices by solving the problem below\n$w^* = \\arg\\min_w L(w).$  (1)\nHere, $L(w) = \\sum_{k \\in \\mathcal{K}} \\frac{|D_k|}{|D|} L_k(w)$ is the loss function with\n$L_k(w) = \\frac{1}{|D_k|} \\sum_{j \\in D_k} l(w, x_j, y_j),$  (2)\ndenoting the loss function for device $k$ and $l(\\cdot)$ being an appropriate sample-wise loss function.\nTraining an ML model in the FEEL system usually is an iterative procedure, in which the iteration is also known as the communication round. Define $w^{(i)}$ to be the global model at round $i = 1, 2, ...$ Then, the training process of FEEL at each round consists of three stages, i.e., Local Gradient Computing, Local Gradient Uploading, and Global Model Updating. We will detail these stages in the following subsections.\n\\subsection{Local Gradient Computing}\nThe $k$-th device will compute the local gradient, denoted by $g_k^{(i)}$, of $L_k(w)$ at $w = w^{(i)}$, where $w^{(i)}$ is received from"}, {"title": "III. PROBLEM FORMULATION AND TRANSFORMATION", "content": "In the sequel, we first establish a joint resource allocation and data selection problem. Then, we conduct some necessary transformations to facilitate solving the original problem.\n\\subsection{Problem Formulation}\nAccording to (18) and (21), we see that the variables $({\\bf p}^{(i)}, {\\bf p}^{(i)})$ and $\\mathcal{M}^{(i)}$ can affect both the convergence of FEEL and the net cost of all devices. Therefore, a question naturally arises: how to design an appropriate joint resource allocation and data selection scheme to accelerate the convergence of FEEL while minimizing the net cost of all devices? To answer this question, the following problem is established.\nProblem 1 (Joint Resource Allocation and Data Selection).\n$\\min_{\\mathcal{M}^{(i)},{\\bf p}^{(i)}, {\\bf p}^{(i)}} \\Delta(\\mathcal{M}^{(i)}) + (1 - \\lambda)C(\\mathcal{M}^{(i)}, {\\bf p}^{(i)}, {\\bf p}^{(i)})$\\ns.t. (5), (6), (11), (12), (13), (14), (15), (16).\n\\subsection{Problem Transformation}\nLet $\\delta_{k,j}^{(i)} \\in \\{0, 1\\}$ denote the data selection indicator, where $\\delta_{k,j}^{(i)} = 1$ represents that the $j$-th data sample in $D_k$ is selected into the set $\\mathcal{M}_k^{(i)}$, and $\\delta_{k,j}^{(i)} = 0$, otherwise. Define the variable ${\\boldsymbol{\\delta}}^{(i)} = {(\\delta_{k,j}^{(i)})}_{k \\in \\mathcal{K}}$, where $k \\in \\mathcal{K}, j \\in \\mathcal{I}_k$ and $\\mathcal{I}_k = \\{1,2,\\ldots, |D_k|\\}$. Then, by replacing $\\mathcal{M}_k$ with ${\\boldsymbol{\\delta}}^{(i)}$, we transform Problem 1 into an equivalent problem as follows.\nProblem 2 (Joint Resource Allocation and Data Selection).\n$\\min_{{\\boldsymbol{\\delta}}^{(i)},{\\bf p}^{(i)}, {\\bf p}^{(i)}} \\Delta({\\boldsymbol{\\delta}}^{(i)}) + (1 - \\lambda)\\hat{C}({\\boldsymbol{\\delta}}^{(i)}, {\\bf p}^{(i)}, {\\bf p}^{(i)})$\\ns.t. (11), (12), (13), (14), (15), (16),\n$\\delta_{k,j}^{(i)} \\in \\{0,1\\}, j \\in \\mathcal{I}_k, \\forall k \\in \\mathcal{K},$  (24)\n$0 < \\sum_{j \\in \\mathcal{I}_k} \\delta_{k,j}^{(i)} < \\hat{D}_k, \\forall k \\in \\mathcal{K},$  (25)\nCompared to the original Problem 1, Problem 2 is a solvable problem, since it is only required to send the cardinality of $\\hat{D}_k$ to the server, instead of $D_k$ itself. Nonetheless, it is still difficult to achieve an optimal point of Problem 2 for the following reasons. First, the variables ${\\bf p}^{(i)}$ and ${\\boldsymbol{\\delta}}^{(i)}$ are binary. Second, the objective function and the constraint in (16) are non-convex. Thus, Problem 2 is recognized as a highly challenging mixed-integer non-convex problem. To tackle Problems 3, we decompose Problem 2 into it and 4 equivalently.\nProblem 3 (Resource Allocation Problem).\n$\\min_{{\\bf p}^{(i)}, {\\bf p}^{(i)}} \\hat{C}_{com}({{\\bf p}^*}^{(i)}, {{\\bf p}^*}^{(i)}) + \\hat{C}_{cmp}$\\ns.t. (11), (12), (13), (14), (15), (16)."}, {"title": "IV. SOLUTION OF PROBLEM 3", "content": "Like Problem 2, Problem 3 is also a highly challenging mixed-integer non-convex problem. In the sequel, based on the matching theory and convex-concave procedure (CCP), we will propose a low-complexity algorithm to solve Problem 3.\n\\subsection{Proposed Algorithm}\nIn this subsection, we first define a matching model. Then we give the details of the proposed matching-based algorithm. Finally, the convergence and complexity of our proposed algorithm will be analyzed.\n1) Matching Formulation: To rationally utilize the limited communication resources, we only allocate RBs to the available devices. Let $\\mathcal{U}^{(i)}$ denote the set of $\\mathcal{U}^{(i)}$ available devices. To devise a low-complexity algorithm, we consider the devices in $\\mathcal{U}^{(i)}$ and RBs in $\\mathcal{N}$ as two sets of nodes in a bipartite\n2) Algorithm Detail: Motivated by the housing assignment problem in [37], we introduce the concept of swap matching into the matching formulated in Definition 1 and devise a matching-based algorithm to solve Problem 3. A swapping operation means that two available devices matched with different RBs exchange their matches, while the matches of other available devices remain unchanged. The corresponding power allocation and net cost under the given RB assignment is then updated via Algorithm 3, which will be detailed in Section IV-B. To guarantee the reduction of the net cost, a swapping operation is approved and the matching is updated only when the net cost for the given RB assignment decreases after the swap. The above swapping operation will be repeated several times until no swapping is further approved. The proposed matching algorithm is detailed in Algorithm 2 and its convergence and complexity will be analyzed in the sequel.\n3) Convergence: After multiple swapping operations, the matching situation between RBs and available devices changes as follows: $\\Psi_0 \\to \\Psi_1 \\to \\Psi_2 \\to \\ldots$, where $\\Psi_l$ with $l = 1, 2, \\ldots$ denotes the matching at the $l$-th swapping operation and $\\Psi_0$ represents the initial matching. At swapping operation $l$, the matching changes from $\\Psi_{l-1}$ to $\\Psi_l$. Let $C_{l-1}$ and $\\hat{C}_l$ denote the net costs at matching $\\Psi_{l-1}$ and matching\n4) Complexity: Finally, we discuss the computational complexity of Algorithm 2. For each swapping operation, we should consider all possible swapping combinations, which requires $O(|\\mathcal{U}^{(i)}|^2)$ operations. For each swapping attempt, we need to allocate the power, and calculate and compare the net cost before and after the swapping of the available devices under the given RB assignment. Let $O(X)$ represent the complexity of the power allocation algorithm (i.e., Algorithm 3), which will be analyzed in Section IV-B. Assume that the matching remains unchanged after $V$ swapping operations. Then, the computational complexity of Algorithm 2 is $O(|\\mathcal{U}^{(i)}|^2VX)$.\n\\subsection{Power Allocation}\nIn line 6 of Algorithm 2, the net cost is minimized by optimizing the power allocation under a given RB assignment via Algorithm 3. In this subsection, we focus on how to construct Algorithm 3. Given the RB assignment, Problem 3 becomes the problem in (28), shown at the top of next page.\nDue to the non-convexity of the constraint in (29), the problem in (28) is a non-convex problem. To solve it in a more tractable manner, we perform some transformations as detailed below.\nSpecifically, we assume that the devices in the set $\\mathcal{S}_n^{(i)}$ occupying RB $n$ are arranged in ascending order according to their channel power gains. Then, the term $\\sum_{t \\in \\mathcal{S}_n^{(i)}} \\mathbb{1} \\left[ h_{t,n}^{(i)} < h_{k,n}^{(i)} \\right]p_{t,n}^{(i)} \\right] + N_0$ can be re-expressed as $s_{I_{k,n}}({\\bf p}^{(i)}) = \\sum_{t=1}^{k-1} \\mathbb{1} \\left[ h_{t,n}^{(i)} < h_{k,n}^{(i)} \\right]p_{t,n}^{(i)} \\right] + N_0$. On this basis, the constraint in (29) can be rewritten as\n\\subsection{Continuous Relaxation}\nWe first relax the integer constraint in (24) to\n$\\delta_{k,j}^{(i)} \\in [0, 1], \\forall k \\in \\mathcal{K}, \\forall j \\in \\mathcal{I}_k.$  (35)\nBased on (35), the continuous relaxation of Problem 4 is written as\n$\\min_{{\\boldsymbol{\\delta}}^{(i)}} \\Delta({\\boldsymbol{\\delta}}^{(i)}) + (1 - \\lambda)\\hat{C}({\\boldsymbol{\\delta}}^{(i)}, {{\\bf p}^*}^{(i)}, {{\\bf p}^*}^{(i)})$ (36)\ns.t. (25), (35).\nThis is a non-convex continuous problem, and we can solve it efficiently by using the gradient projection method, as summarized in Algorithm 4. Particularly, in Step 3 of Algorithm 4, $f({\\boldsymbol{\\delta}}^{(i)})$ is the objective function of the problem in (36), $v$ represents the iteration index, and $\\alpha(v)$ denotes the diminishing stepsize at the $v$-th iteration, satisfying $\\alpha(v) \\to 0$ as $v \\to \\infty$, $\\sum_{v=0}^{\\infty} \\alpha(v) = \\infty$, and $\\sum_{v=0}^{\\infty} {\\alpha}^2(v) < \\infty$. In Step 4, the projection of ${\\boldsymbol{\\delta}}^{(i)}(v + 1)$ onto the feasible set of the problem in (36) can be achieved through tackling the following problem\n${\\boldsymbol{\\delta}}^{(i)}(v + 1) = \\arg \\min_{\\boldsymbol{\\delta}} \\left\\|{\\boldsymbol{\\delta}} - {\\boldsymbol{\\delta}}^{(i)}(v + 1) \\right\\|^2_2$\ns.t. (25), (35).\nThe problem in (37) is convex and we can achieve an optimal solution of it via CVX. Steps 3 and 4 will be repeated serval times until Algorithm 4 converges. According to [40], we know that ${\\boldsymbol{\\delta}}^{(i)}(v) \\to {\\boldsymbol{\\delta}}^{\\dagger(i)}$ as $v \\to \\infty$, where ${\\boldsymbol{\\delta}}^{\\dagger(i)}$ is a stationary point of (36). Algorithm 4's complexity is dominated by solving (37), which can be expressed as $O((|\\mathcal{D}|)^{3.5} \\log(1/\\epsilon))$.\n\\subsection{Binary Recovery}\nThe obtained solution ${\\boldsymbol{\\delta}}^{\\dagger(i)}$ is generally continuous and hence is an infeasible solution of Problem 4. In the sequel, based on ${\\boldsymbol{\\delta}}^{\\dagger(i)}$, we construct a feasible solution of Problem 4. Let $\\mathcal{B} = \\{(24), (25)\\}$ be the constraint set. By projecting ${\\boldsymbol{\\delta}}^{\\dagger(i)}$ onto $\\mathcal{B}$, we establish the following problem\n${\\boldsymbol{\\delta}}^{*(i)} \\triangleq \\arg \\min_{{\\boldsymbol{\\delta}} \\in \\mathcal{B}} \\left\\|{\\boldsymbol{\\delta}} - {\\boldsymbol{\\delta}}^{\\dagger(i)} \\right\\|^2.$ (38)\nThe problem in (38) is an integer nonlinear programming. By using the \\Lambda-representation technique [41], we can transform it into a linear programming problem as follows.\n$\\min_{{\\boldsymbol{\\delta}},{\\bf a},{\\bf b}} \\sum_{k \\in \\mathcal{K}} \\sum_{j \\in \\mathcal{I}_k} [(\\delta_{k,j}^{\\dagger(i)})^2 a_{k,j} + (1 - \\delta_{k,j}^{\\dagger(i)})^2 b_{k,j}]$ (39)\ns.t. (25), (35),\n$b_{k,j} = \\delta_{k,j}^{(i)}, \\forall k \\in \\mathcal{K}, \\forall j \\in \\mathcal{I}_k,$  (40)\n$a_{k,j} + b_{k,j} = 1, \\forall k \\in \\mathcal{K}, \\forall j \\in \\mathcal{I}_k,$  (41)\n$a_{k,j} \\geq 0, b_{k,j} \\geq 0, \\forall k \\in \\mathcal{K}, \\forall j \\in \\mathcal{I}_k,$  (42)\nwhere ${\\bf a} = (a_{k,j})_{k \\in \\mathcal{K},j \\in \\mathcal{I}_k}$ and ${\\bf b} = (b_{k,j})_{k \\in \\mathcal{K},j \\in \\mathcal{I}_k}$. Lemma 4 indicates the relationship between the problems in (39) and (38).\n\\subsection{Algorithm Summary}\nFinally, Algorithm 5 summarizes the details of solving Problem 4, whose complexity is the summation of the complexity of the continuous relaxation stage and the binary recovery stage, which is given by $O((\\frac{1 + 3^{3.5}}{2})|\\mathcal{D}|^{3.5} \\log(1/\\epsilon))$.\nIn summary, the complexity of Algorithm 1 is given by $O(|\\mathcal{U}^{(i)}|^2V(KN)^{3.5} \\log(1/\\epsilon) + (\\frac{1 + 3^{3.5}}{2})|\\mathcal{D}|^{3.5} \\log(1/\\epsilon))$."}, {"title": "V. SOLUTION OF PROBLEM 4", "content": "Solving Problem 4 optimally is NP-hard due to the existence of integer variables and the fractional form of the objective function. Toward this end, a low-complexity suboptimal algorithm for Problem 4 will be proposed in this section. Our algorithm consists of two stages, that is, continuous relaxation and binary recovery. In the first phase, we solve the continuous relaxation of Problem 4 via the gradient projection method. In the second stage, we achieve a feasible integer solution of Problem 4 using the \\Lambda-representation method."}, {"title": "VI. SIMULATION RESULTS", "content": "Extensive simulations will be conducted in this section to show the superiority of our proposed scheme (obtained by Algorithm 1). In the following, we will first introduce the simulation setup and then detail the performance evaluation.\n\\subsection{Simulation Setup}\nUnless otherwise stated, the simulation parameters are set as follows. We consider that the FEEL system has $K = 10$ devices. For device $k$, we set the cost per Joule and the reward for each data sample, respectively, to $c_k = 5$ and $q_k = 0.002$, if $k$ is odd, and $c_k = 10$ and $q_k = 0.005$, otherwise. The CPU frequency, the number of CPU cycles to handle a sample, and the capacitance coefficient are set to $f_k = \\{0.1, 0.2,\\ldots,1.0\\}$ GHz, $F_k = 20$ cycles/sample, and $\\kappa = 1 \\times 10^{-28}$, respectively. The probability of local gradient uploading is $\\epsilon_k = 0.2$ if $k$ is odd, and $\\epsilon_k = 0.8$, otherwise [19]. The maximum power limit of device $k$ is set to $p_{k}^{max} = 10$ W [15]. In addition, we set $N = 5$, $Q = 2$, $B = 2$ MHz, $N_0 = 10^{-9}$ W, $T = 500$ ms, $\\lambda = 1 \\times 10^{-3}$. The channel power gain $h_{k,n}$ follows an exponential distribution with mean $10^{-5}$.\nWe train an image classification model with the proposed FEEL algorithm. The corresponding datasets are MNIST and Fashion-MNIST. The MNIST dataset has 70000 handwritten grayscale images of the digits 0 to 9, and the Fashion-MNIST dataset comprises 70000 grayscale images of fashion items from ten classes, e.g., \u201ct-shirt\u201d, \u201cpants\u201d, and \u201cbag\u201d. For each dataset, 60000 images are used for model training and the rest are the test dataset. To simulate non-IID distribution, we randomly allocate $|D_k|= 1000$ figures of one label to device $k$ in set $\\mathcal{K}$, and then randomly choose $|\\hat{D}_k|= 200$ samples from $D_k$ in each communication round. We assume that some data samples on each device are mislabeled. Let $\\rho_k$ denote the proportion of mislabeled data on device $k$. To simulate the scenario of data mislabeling, we randomly select $\\rho_k |D_k|$ of the data samples on device $k$ and then\nmislabel each of them, such as labeling the digit \"1\" as \"0\", or labeling the item \"t-shirt\" as \"pants\". In the simulations, we set the mislabeled proportion $\\rho_k$ to 10% for all $k \\in \\mathcal{K}$. We apply the convolutional neural network (CNN) to conduct the image classification task. The CNN model has seven layers, two 5 $\\times$ 5 convolution layers (the first and second layers have 10 and 20 channels, respectively, each followed by 2 $\\times$2 max pooling), and three full connection layers with ReLu activation. Then using Python, the size of the gradient $g_k^{(i)}$ is estimated to be $\\ell = 0.56 \\times 10^6$ bits (MNIST) or $\\ell = 1 \\times 10^6$ bits (Fashion-MNIST). We choose Adam as the optimizer and set the learning rate $\\eta$ to 0.001. Please note that the hyper-parameters employed in this study have been selected based on prior experimental experience and analogous experimental configurations, rather than through a process of fine-tuning.\nTo demonstrate the superior of our proposed scheme, the following four representative baseline schemes are investigated [12], [19], [26]:"}, {"title": "VII. CONCLUSIONS", "content": "In this paper, we first rigorously model the training process of FEEL and derive its one-round convergence bound. Then, we formulate a joint resource allocation and data selection optimization problem, which, unfortunately, cannot be solved directly. To tackle this problem, we equivalently transform it into a more tractable form with some appropriate transformations and then break it into the resource allocation problem and the data selection problem. Both subproblems are mixed-integer non-convex and integer non-convex optimization problems, respectively, and it is very challenging to obtain their optimal solutions. Based on the matching theory and applying the convex-concave procedure and gradient projection methods,"}]}