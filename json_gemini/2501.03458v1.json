{"title": "Activating Associative Disease-Aware Vision Token Memory for LLM-Based X-ray Report Generation", "authors": ["Xiao Wang", "Fuling Wang", "Haowen Wang", "Bo Jiang", "Chuanfu Li", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "abstract": "X-ray image based medical report generation achieves significant progress in recent years with the help of the large language model, however, these models have not fully exploited the effective information in visual image regions, resulting in reports that are linguistically sound but insufficient in describing key diseases. In this paper, we propose a novel associative memory-enhanced X-ray report generation model that effectively mimics the process of professional doctors writing medical reports. It considers both the mining of global and local visual information and associates historical report information to better complete the writing of the current report. Specifically, given an X-ray image, we first utilize a classification model along with its activation maps to accomplish the mining of visual regions highly associated with diseases and the learning of disease query tokens. Then, we employ a visual Hopfield network to establish memory associations for disease-related tokens, and a report Hopfield network to retrieve report memory information. This process facilitates the generation of high-quality reports based on a large language model and achieves state-of-the-art performance on multiple benchmark datasets, including the IU X-ray, MIMIC-CXR, and Chexpert Plus. The source code of this work is released on https://github.com/Event-AHU/Medical_ Image_Analysis.", "sections": [{"title": "I. INTRODUCTION", "content": "X-RAY medical image based report generation targets describing the findings or impressions using natural language. This task can greatly alleviate the work pressure on doctors and reduce the waiting time for patients, providing a feasible method for empowering artificial intelligence in smart healthcare. Although the task has made considerable progress in recent years, there are still many issues, such as the difficulty in detecting key diseases and the challenge in modeling the association between contextual samples. The research and development of medical report generation models still have a long way to go.\nRecently, the Large Language Model (LLM) has drawn more and more attention in the community of artificial intelligence. It has demonstrated exceptional abilities in language understanding, reasoning, and generation. Some researchers already exploit the effectiveness of LLM in the X-ray based medical report generation, such as R2Gen-GPT [1], R2Gen- CSR [2], MambaXray-VL [3]. Despite significant progress compared to traditional models, these models are still limited by the following issues: Firstly, current mainstream LLM- based report generation models focus on how to produce high-quality text at the linguistic level, but they struggle to accurately identify abnormal conditions, diseases, and other critical information in clinical diagnostic indicators. As a result, although the obtained medical reports may appear to be well-structured, they are actually difficult to address the prac- tical problems. Secondly, existing medical report generation models, when considering contextual memory information, typically mine global image information, medical report data, etc., but rarely focus on fine-grained X-ray image information. However, these key visual details may be essential for produc- ing accurate disease-related descriptions. Thus, it is natural to raise the following questions: \"how can we utilize the local fine-grained visual cues and critical context information for X-ray medical report generation?\"\nConsidering that human medical experts have extensive experience in X-ray diagnosis, similarly, existing LLM-based models can understand and generate X-ray reports well, but medical experts also rely on recalling past cases to assist in diagnosing current situations. Therefore, we need to introduce an associative memory mechanism in the medical report gen- eration process, to draw inspiration from a historical sample bank and generate more accurate medical report content. Fig. 1 provides a comparison between human medical experts com- pleting high-quality report writing through associative memory and the associative memory-enhanced LLM X-ray medical report generation proposed in this paper.\nInspired by these insights, we propose a novel associative memory augmented X-ray medical report generation frame- work via a large language model, termed AM-MRG. As shown in Fig. 2, our framework contains two stages, i.e., the disease-aware visual token mining and the associative memory augmented X-ray medical report generation. Specifically, in the first stage, we extract the vision features of a given X- ray image using the Swin Transformer network [4]. Then, a Q-Former is adopted to augment the obtained vision features further and learn the disease queries, and a classification head is used for disease recognition. We utilize the GradCAM [5] to get the activation maps that reflect the key regions for disease recognition. Therefore, we can obtain massive disease-aware vision tokens and disease query features.\nIn the second stage, we take the visual tokens of each input X-ray image as the query feature and the disease-aware vision tokens as the knowledge base. A vision Modern Hopfield Network (MHN) [6] is introduced to achieve associative memory augmented feature learning for disease-aware X- ray medical report generation. Meanwhile, we also retrieve the report memory from the medical reports using a report MHN. A generation prompt is adopted to guide the report prediction of the large language model. Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of the proposed AM-MRG framework.\nTo sum up, the main contributions of this paper can be summarized as follows:\n\u2022 We propose a novel associative memory augmented X- ray medical report generation framework based on a large language model, termed AM-MRG. The proposed modules have greatly improved the effectiveness of the X-ray medical reporter on clinical diagnostic indicators.\n\u2022 We exploit the disease-aware visual token mining and report memory retrieval for associative memory construction based on Modern Hopfield Networks.\n\u2022 Extensive experiments on three widely used benchmark datasets fully validated the effectiveness of our proposed associative memory augmented X-ray MRG framework.\nThe rest of this paper is organized as follows: In section II, we introduce the related works about the X-ray medical report generation, large language model, modern Hopfield network, and context sample retrieval. Then, we describe the proposed AM-MRG framework in Section III, with a focus on disease-aware visual token mining, report memory retrieval, associative memory augmented LLM for MRG, and details in the training and testing phrase. Then, we conduct experiments in Section IV, and conclude this paper in Section V."}, {"title": "II. RELATED WORK", "content": "In this section, we will review the related works on the X-ray Medical Report Generation, Large Language Models, Modern Hopfield Network, and Context Sample Retrieval. More related works can be found in the following surveys [7], [8]."}, {"title": "A. X-ray Medical Report Generation", "content": "Medical Report Generation (MRG) [9]\u2013[13] is an impor- tant application area combining Natural Language Processing (NLP) and Computer Vision (CV), aiming to automate the generation of medical reports, particularly in fields such as radiology. In MRG, models typically need to process two primary sources of information: visual information from med- ical images and linguistic information from existing medical literature or reports. R2Gen [9] introduces a memory-driven Transformer for radiology report generation, using relational memory and memory-driven conditional layer normalization to enhance performance. RGRG [10] uses a region-guided approach to detect and describe prominent anatomical re- gions, enhancing report accuracy and explainability while enabling new clinical applications. METransformer [11] intro- duces multiple learnable \u201cexpert\" tokens in the Transformer encoder and decoder and develops a metric-based \"expert\" voting strategy to generate the final report. KiUT [12] model improves word prediction accuracy by learning multi-level visual representations through a U-Transformer architecture and adaptively extracting information using context and clin- ical knowledge. MedRAT [13] addresses the lack of paired image-report data during training by bringing relevant images and reports closer in the embedding space through auxiliary tasks such as contrastive learning and classification, enabling unpaired learning. HERGen [14] effectively integrates lon- gitudinal data from patient visits using group causal Trans- formers and enhances report quality with auxiliary contrastive objectives. PhraseAug [15] introduces a phrase book as an intermediate modality, discretizing medical reports into key phrases, thereby facilitating fine-grained alignment between images and text. CoFE [16] uses counterfactual explanations to learn truthful visual representations and fine-tunes pre- trained large language models (LLMs) with learnable prompts to generate semantically consistent and factually complete reports. However, existing studies are somewhat insufficient in handling complex disease information and ensuring the medical and logical coherence of generated reports."}, {"title": "B. Large Language Models", "content": "In the field of MRG using Large Language Models (LLMs), recent research has focused on improving the integration of visual information and enhancing the accuracy of generated radiology reports. R2GenGPT [1] employs a lightweight visual alignment module to align image features with the LLM's word embedding space, effectively processing image data and generating radiology reports. To address the limitations of existing methods in cross-modal feature alignment and text generation, Liu et al. [17] leverages the powerful learning capabilities of LLMs, optimizing radiology report generation through domain-specific instance induction and a coarse-to- fine decoding process. HC-LLM [18] enhances the adaptability and performance of LLMs in radiology report generation tasks by utilizing historical diagnostic information and extracting both temporal shared and specific features from chest X-rays and diagnostic reports. It applies three consistency constraints to ensure the accuracy of the reports generated and their consistency with disease progression. LLM-RG4 [19] lever- ages the flexible instruction-following capability and extensive general knowledge of LLMs. By incorporating an adaptive token fusion module and a token-level loss weighting strategy, LLM-RG4 effectively handles diverse input scenarios and improves diagnostic precision. MambaXray-VL [3] is a pre- trained large vision-language model proposed by Wang et al. which adopts Mamba for X-ray image encoding and Llama2 for report generation. Despite these advancements, existing methods have not combined the fine-grained visual modules with the powerful capabilities of LLMs. Therefore, our AM- MRG takes a novel approach by innovatively combining these two aspects to achieve better performance in medical report generation."}, {"title": "C. Modern Hopfield Network", "content": "The Modern Hopfield Network (MHN) [6] enhances the capability to store and retrieve patterns, similar to the attention mechanism in Transformers. This provides a new approach for integrating memory and attention mechanisms into deep learning models for medical report generation. By embedding Hopfield layers into the architecture, these networks can efficiently store and access visual features and intermediate representations, improving the ability to generate accurate and detailed radiology reports. This integration surpasses traditional fully connected, convolutional, and recurrent net- works, offering advanced pooling, memory, association, and attention mechanisms. To address the limitations of MHNs in memory storage and retrieval, Santos et al. [20] introduced the Hopfield-Fenchel-Young energy function, demonstrating that structured Hopfield Networks perform well in tasks such as multi-instance learning and text interpretation. Nicolini et al. [21] utilized MHNs to provide an efficient, scalable, and robust solution for asset allocation, showing that MHNS perform comparably or even better than deep learning methods such as LSTMs and Transformers, with faster training speeds and better stability. Our AM-MRG is the first work to combine MHNs with medical report generation (MRG). By enhancing visual and textual representations through associative memory modules composed of MHNs, our approach has achieved impressive results."}, {"title": "D. Context Sample Retrieving", "content": "Context Sample Retrieving is a technique for processing and generating natural language, particularly suited for large language models (LLMs) and generation tasks such as medical report generation. This technology aims to retrieve relevant context samples from a vast amount of data, allowing the model to reference these samples when generating medical reports, thereby improving the relevance and accuracy of the generated content. Chen et al. [22] proposed a novel frame- work that treats the retrieval problem as a Markov decision process. Through policy optimization, the retriever can make iterative decisions to find the best example combinations for specific tasks. In-Context RALM [23] significantly improves the performance of language models in generating medical reports by adding retrieved relevant documents to the input text without modifying the language model architecture. To address the challenges of few-shot hierarchical text classification, Chen et al. [24] built a retrieval database to identify examples related to the input text and employed an iterative strategy to manage multi-layer hierarchical labels. This method has shown excellent performance in few-shot HTC tasks. R2GenCSR [2] retrieves positively and negatively related samples from the training set during the training phase and uses them to enhance feature representation and discriminative learning, thereby better guiding the LLM in generating accurate medical reports. In our AM-MRG, the Context Sample Retrieving technique effectively integrates relevant medical visual and textual con- text information, providing the model with richer and more accurate references. This significantly improves the relevance and accuracy of the generated medical reports. The application of this technology notably enhances AM-MRG's performance in generating complex medical reports, ensuring the accuracy and clinical relevance of the report content."}, {"title": "III. METHODOLOGY", "content": "In this section, we introduce the proposed associative memory-augmented X-ray report generation framework (AM- MRG). The framework generates an accurate and clinically relevant report $r\\in \\mathbb{R}^{l}$ for a new X-ray image $x \\in \\mathbb{R}^{H\\times W\\times 3}$ by leveraging a collection of existing X-ray images $X = \\{X_1, X_2, ..., X_N\\}$ and their corresponding paired medical re- ports $R = \\{r_1, r_2, ..., r_N\\}$. The framework retrieves and refines relevant visual and textual information from this col- lection, dynamically enriching disease-specific representations that capture both semantic and contextual features. These rep- resentations are subsequently processed by a Large Language Model (LLM) M to produce the final diagnostic report r.\nGenerally speaking, our proposed AM-MRG framework consists of three main modules. In the first stage (Sec- tion III-B), disease query $Q_d \\in \\mathbb{R}^{n_d\\times 768}$ is introduced to extract disease-sepcific features $F_d \\in \\mathbb{R}^{n_d\\times 768}$ from each case image. Additionally, a classification task is employed to identify regions of interest (ROIs) that correspond to im- age patches associated with specific diseases. In the second stage (Section III-C), two memory banks are constructed: a disease-aware visual bank $V \\in \\mathbb{R}^{n_v\\times 768}$, encoding region- specific visual features, and a report memory $R \\in \\mathbb{R}^{n_r\\times 768}$, encoding sentence-level semantic features from reports. These memory banks are leveraged via Vision Hopfield Network (V- Hopfield) and Report Hopfield Network (R-Hopfield) to refine the disease-specific features into enhanced representations $F' \\in \\mathbb{R}^{(n_v+n_r)\\times 4096}$. Finally, in the report generation stage (Section III-D), the original features F from the Vision En- coder, the query features $Q_f$ from the Q-former, the enhanced features F', and the generation prompt together guide the LLM to produce the final report r."}, {"title": "B. Disease-aware Vision Token Mining", "content": "Given the input X-ray image $x \\in \\mathbb{R}^{224x224x3}$, we first adopt the Swin-Transformer [4] for the feature extraction. Specifi- cally, we divide the X-ray image into 196 non-overlapping patches. Each of these patches is of a uniform size, mea- suring 16 \u00d7 16 pixels. Then, we project them into token representations using a projection layer and feed them into Transformer blocks for vision feature learning. The output F will be fed into the Q-former network [25] which further transforms them into feature representation $Q_f \\in \\mathbb{R}^{14\\times 768}$. The used Q-former contains self-attention, cross-attention, and feed-forward layers, inspired by the BLIP2 model [25].\nNote that, we also input disease query $Q_d \\in \\mathbb{R}^{n_d\\times 768}$ into the Q-former to learn the embedding representation of each disease, where $n_d$ represents the number of disease categories, and 768 denotes the query dimension. The cross-attention mechanism enables each disease query to interact with the encoded image features F, focusing on regions most relevant to specific diseases. A classification head is introduced to transform the visual features into 14 category labels defined in the MIMIC X-Ray dataset. The labels are obtained from the annotated medical reports. From stage 1 as shown in Fig. 2, we can get the disease-aware query tokens and activation maps of input X-ray images which will be introduced in the following paragraphs.\nTo focus on disease-relevant areas in the X-ray image, we leverage GradCAM [5] activation maps $M\\in \\mathbb{R}^{H\\times W}$ to identify Regions of Interest (RoI) that contribute most significantly to the classification task. For each patch $r_e$, the mean activation value is computed. A patch is retained as part of the Rol if its mean activation value exceeds a predefined threshold T. Based on this selection, a masked version of the original image $x' \\in \\mathbb{R}^{H\\times W\\times 3}$ is generated, where non-RoI regions are masked to zero. The generation of $x'$ is defined as:\n$X_{ij} = \\begin{cases} X_{ij} & \\text{if } r_e(p,q) \\in r_k \\text{, } M_{pq} > T, (i,j) \\in r_k, \\\\0 & \\text{otherwise,} \\end{cases}$\nwhere $r_k$ denotes the k-th patch in the image, defined by its pixel indices (p,q), and $x_{ij}$ represents the pixel value at location (i, j) in the original image. Each patch contains $|r_k| = 16 \\times 16$ pixels, and a patch is retained if the mean activation value of its corresponding region in the GradCAM activation map exceeds a predefined threshold $\\tau$.\nThis masking process ensures that only patches with sig- nificant activation values contribute to the final masked image $x'$, while other regions are suppressed. The resulting masked image $x'$ retains the spatial structure of the original image but focuses exclusively on the most critical disease-relevant regions. It is subsequently used in the encoding stage to construct the disease-aware visual bank, concentrating feature extraction on the preserved Rols."}, {"title": "C. Associative Memory Augmented Features", "content": "Two memory banks are constructed to encapsulate vi- sual and textual knowledge. The disease-aware visual bank $V\\in \\mathbb{R}^{n_v\\times 768}$ is built by encoding the masked X-ray images $X' = \\{x'_1, x'_2, ..., x'_{n'}\\}$, obtained from the previous stage, using a Swin Transformer [4]. This process produces region- specific feature tokens corresponding to the RoIs identified in each image. The Report Memory $R \\in \\mathbb{R}^{n_r\\times 768}$ is constructed by encoding the medical reports $R = \\{r_1, r_2, ..., r_n\\}$ using a Bio_ClinicalBERT [26] model. Each report is tokenized into sentences and processed into semantically rich sentence- level representations. These memory banks encapsulate com- prehensive visual and textual knowledge, providing a robust foundation for dynamic retrieval and feature enhancement.\nTo augment the disease-specific features $F_d \\in \\mathbb{R}^{14\\times 768}$ generated in the previous stage, we employ two modern Hopfield networks [6] for feature retrieval and enhancement, as shown in Fig. 3. Given a query feature $f_i \\in \\mathbb{R}^{768}$ and a memory bank $X \\in \\mathbb{R}^{N\\times 768}$ (where X can be either V or R), the Hopfield network maps the query to an updated feature $f^* \\in \\mathbb{R}^{4096}$ by iteratively minimizing the following energy function:\n$E(f^*) = ||f_i^* - f_i||^2 - \\beta \\log \\sum_{j=1}^N \\exp \\left( \\frac{f_i^{*T}m_j}{\\sqrt{d}} \\right)$,\nwhere $f_i^*$ is initialized to $f_i$, $m_j \\in \\mathbb{R}^{4096}$ represents the j-th stored feature in the memory bank, $\\beta$ controls the sharpness of the similarity distribution, and d normalizes the feature dimensions.\nThe updated feature $f_i^*$ is computed iteratively using gradient-based optimization:\n$f_i^{*t+1} = f_i^{*t} - \\eta \\frac{\\partial E(f^*)}{\\partial f^*}$ where the gradient is given by:\n$\\frac{\\partial E(f^*)}{\\partial f^*} = 2(f_i^* - f_i) - \\beta\\sum_{j=1}^N \\alpha_j m_j$,\nand:\n$\\alpha_j = \\frac{\\exp\\left(\\frac{f_i^{*T} m_j}{\\sqrt{d}}\\right)}{\\sum_{k=1}^N \\exp \\left(\\frac{f_i^{*T} m_k}{\\sqrt{d}}\\right)}$.\nAfter convergence, the Hopfield network outputs the up- dated feature $f^*$, which represents the enhanced representation of the query based on the memory bank. We denote this operation as:\n$f^* = Hopfield(f_i, X)$,\nV-Hopfield for the Disease-aware Visual Bank V and R- Hopfield for the Report Memory R align input features with stored knowledge through energy minimization. The refined visual and textual features are computed as:\n$f_v^* = V\\text{-}Hopfield(f_i, V; \\theta_v), f_r^* = R\\text{-}Hopfield(f_i, R; \\theta_r)$.\nThese outputs are concatenated to form the final enhanced representation:\n$F_i = Concat(f_v^*, f_r^*) \\in \\mathbb{R}^{2\\times 4096}$.\nThe complete output for all disease queries is:\n$F' = \\{F_1, F_2, ..., F_{14}\\} \\in \\mathbb{R}^{(n_d\\times 2)\\times 4096}$.\nThis enhanced representation integrates both visual and se- mantic contexts, enabling robust downstream report genera- tion."}, {"title": "D. LLM-driven Report Generation", "content": "In this stage, the original features F from the vision encoder, the query features Q from the Q-former, the enhanced feature representation F', and a predefined generation prompt together guide the LLM M in producing the final medical report.\nTo ensure the generation of accurate and contextually rele- vant medical reports, a specific generation prompt is designed: Generate a comprehensive and detailed diagnosis report for this chest X-ray image. This prompt provides explicit instruc- tions to the LLM, defining the objective of the generation task. The prompt is tokenized using the LLM's tokenizer, resulting in a sequence of token embeddings $T_{prompt}$. Meanwhile, the enhanced feature representation F' is processed to align with the LLM's input format, ensuring that the disease-relevant semantic features and the textual context are effectively in- tegrated.\nThe LLM M combines the original features F from the Vision Encoder, the query features Q from the Q-former, the enhanced feature representation F' encapsulating disease- specific visual and textual knowledge, and the token embed- dings of the generation prompt $T_{prompt}$, providing the textual context for the task. Through its multi-modal processing lay- ers, the LLM dynamically integrates these inputs and generates the final report r in natural language:\n$r = M(F, Q, F'; T_{prompt})$,\nwhere r represents the diagnosis report for the input X-ray image. The integration of F, Q, F' and $T_{prompt}$ ensures that the generated report reflects the underlying disease features while maintaining coherence and contextual relevance."}, {"title": "E. Loss Function", "content": "The proposed framework is trained in two distinct stages: Stage 1 optimizes the disease query features for extracting disease-specific representations, and Stage 2 fine-tunes the Modern Hopfield Networks for feature enhancement and re- trieval. During the testing phase, the trained disease query and Hopfield Networks are fixed to generate enhanced features for report generation.\nIn Stage 1, the focus is on optimizing the disease query $Q \\in \\mathbb{R}^{14\\times 768}$ to extract disease-specific features $F_d \\in \\mathbb{R}^{14\\times 768}$ from X-ray images. This is achieved through a multi-label classification task, which ensures that each disease query captures features relevant to a specific disease, and an orthogonality loss, which enforces distinctiveness among queries. The classification loss is defined as:\n$L_{class} = - \\frac{1}{14} \\sum_{j=1}^{14} (y_j \\log \\hat{y_j} + (1 - y_j) \\log(1 - \\hat{y_j}))$,\nwhere $y_j$ is the ground-truth label for the j-th disease and $\\hat{y_j}$ is the predicted probability. The trained disease query features are fixed after Stage 1 to generate the initial disease-specific representations $F_d$.\nIn Stage 2, the goal is to optimize the Hopfield Net- works (V-Hopfield and R-Hopfield) for feature retrieval and enhancement using memory banks V and R. The input to this stage is the disease-specific representation $F_d$ from Stage 1. The enhanced feature representation F' is generated by the Hopfield Networks and used to guide the frozen LLM for report generation. The training in this stage employs an autoregressive loss inspired by R2GenGPT [1]:\n$L_{gen} = - \\sum_{t=1}^L \\log P(r_t | r_{<t}, F, Q, F'; T_{prompt})$,\nwhere $r_t$ is the t-th token of the ground-truth report r, F represents the original features from the Vision Encoder, Q denotes the query features from the Q-former, F' is the enhanced feature representation, and $T_{prompt}$ represents the tokenized generation prompt.\nDuring the testing phase, the trained disease query Q is fixed. The process begins with extracting disease-specific representations $F_d$ from the input X-ray image x using Q. The fixed Hopfield Networks use $F_d$ to retrieve and enhance features from the memory banks V and R, generating F'. Finally, the original features F from the vision encoder, the query features Q from the Q-former, the enhanced features F', and the generation prompt $T_{prompt}$ are input to the LLM M, which generates the diagnosis report r."}, {"title": "IV. EXPERIMENTS", "content": "In our experiments, three benchmark datasets are se- lected for the evaluation, including IU X-ray [27], MIMIC- CXR [28], Chexpert Plus [29] dataset. A brief introduction to these datasets are given below.\n\u2022 IU X-Ray [27] is a collection of chest X-ray images along with their corresponding diagnostic reports. The dataset comprises 7,470 pairs of images and reports. Each report consists of the following sections: impression, findings, labels, comparison, and indication. To ensure a equitable evaluation, we adhered to the same dataset partitioning scheme as used by R2GenGPT [1], allocating the dataset into training, testing, and validation subsets in a ratio of 7:1:2.\n\u2022 MIMIC-CXR [28] is an extensive, publicly available chest radiology report dataset sourced from the Beth Israel Deaconess Medical Center in Boston, USA. Encompassing 227,835 radiological studies, the dataset comprises a total of 377,110 chest X-ray images. Designed to bolster advance- ments in the fields of computer vision and machine learning, MIMIC-CXR is poised to facilitate the development of au- tomated medical image analysis technologies, aiding in the training of algorithms to detect and categorize pathological features within chest X-rays. To maintain a level playing field in our comparison, we adopted the same dataset partitioning approach as R2GenGPT [1], allocating 270,790 samples for model training, and assigning 2,130 and 3,858 samples to the validation and test sets, respectively.\n\u2022 Chexpert Plus [29] is a novel radiology dataset designed to improve the scale, performance, robustness, and equity of ma- chine learning models in the field of radiology. It encompasses 223,228 chest radiographs, 187,711 corresponding radiology reports, de-identified demographic data for 64,725 patients, 14 chest pathology labels, and RadGraph [30] annotations. We adopted the dataset partitioning strategy proposed by [3] to ensure the fairness of our evaluation.\nTo evaluate our AM-MRG model, we employ widely recognized NLG (Natural Language Generation) met- rics: BLEU [31], ROUGE-L [32], and METEOR [33], CIDEr [34]. BLEU assesses the quality of text using n- gram matching; ROUGE-L examines text quality based on the longest common subsequence; and METEOR enhances BLEU by accounting for synonyms and word order; CIDEr assesses text using TF-IDF weighted n-gram matching, highlighting the significance of words. In addition, to assess the accuracy of clinical abnormality descriptions, we followed [9], [10], [35], use CE (Clinical Efficacy) metrics. Specifically, we extract labels from both the predicted and ground truth reports and compare the presence of key clinical observations to gauge the diagnostic accuracy of the generated reports. We evaluate the model's clinical efficacy using Precision, Recall, and F1 scores. These CE metrics provide a comprehensive understand- ing of the model's performance in clinical applications."}, {"title": "B. Implementation Details", "content": "The input X-ray images were resized to 224\u00d7224 pixels and divided into 16\u00d716 patches, which were then fed into MambaXray-VL [3] for visual feature extraction. To align with the visual feature dimensions output from Q-Former, the dimensions in both Hopfield [6] modules are set to 768. The feature dimension input to the LLM, after processing through various network layers, is 4096. Unless otherwise specified, the LLM used for report generation is Llama2-7B [36], and the model used for report encoding is Bio_ClinicalBERT [26]. The key parameter Beta in the Hopfield module is set to 4.\nThe activation map mining method used is GradCAM [5]. The number of visual disease regions extracted is capped at 500 per disease, with a total of 6943 regions for all 14 diseases combined. The report memory size used is 6000, with the number of specific disease reports distributed according to their respective proportions. In stage 1, we set the learning rate as 5e-5 and adopt AdamW [37] optimizer for the training. In stage 2, we set the learning rate as le-4 and also adopt AdamW [37] optimizer for the training. In our research, we developed the model using PyTorch [38] and conducted training and testing on a server equipped with NVIDIA A800- SXM4-80GB GPU. More details can be found in our source code."}, {"title": "C. Comparison on Public Benchmark Datasets", "content": "In our study, we rigorously compare our model with other state-of-the-art models across the three most commonly used datasets in the medical report generation field. This compar- ison includes classic cross-modal alignment models such as R2Gen [9", "39": "PromptMRG [42", "46": ".", "40": "and OR- Gan [47", "2": "R2GenGPT [1", "43": "and AdaMatch-Cyclic [45", "48": "METransformer [11", "41": "and SILC [44", "1": "."}, {"1": ".", "16": "."}]}