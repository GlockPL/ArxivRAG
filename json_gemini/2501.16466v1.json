{"title": "On the Feasibility of Using LLMs to Execute Multistage Network Attacks", "authors": ["Brian Singer", "Keane Lucas", "Lakshmi Adiga", "Meghna Jain", "Lujo Bauer", "Vyas Sekar"], "abstract": "LLMs have shown preliminary promise in some security tasks and CTF challenges. However, it is unclear whether LLMs are able to realize multistage network attacks, which involve executing a wide variety of actions across multiple hosts such as conducting reconnaissance, exploiting vulnerabilities to gain initial access, leveraging internal hosts to move laterally, and using multiple compromised hosts to exfiltrate data. We evaluate LLMs across 10 multistage networks and find that popular LLMs are unable to realize these attacks. To enable LLMs to realize these attacks, we introduce Incalmo, an LLM-agnostic high-level attack abstraction layer that sits between an LLM and the environment. Rather than LLMs issuing low-level command-line instructions, which can lead to incorrect implementations, Incalmo allows LLMs to specify high-level tasks (e.g., infect a host, scan a network), which are then carried out by Incalmo. Incalmo realizes these tasks by translating them into low-level primitives (e.g., commands to exploit tools). Incalmo also provides an environment state service and an attack graph service to provide structure to LLMs in selecting actions relevant to a multistage attack. Across 9 out of 10 realistic emulated networks (from 25 to 50 hosts), LLMs using Incalmo can successfully autonomously execute multistage attacks. We also conduct an ablation analysis to show the key role the high-level abstractions play. For instance, we find that both Incalmo's high-level tasks and services are crucial. Furthermore, even smaller-parameter LLMs with Incalmo can fully succeed in 5 of 10 environments, while larger-parameter LLMs without Incalmo do not fully succeed in any.", "sections": [{"title": "1 Introduction", "content": "The success of LLMs and LLM-based agents in many domains has sparked tremendous interest in the security community, specifically focused in their offensive capabilities. Such capabilities, if realizable, can help improve red team efficiency and enterprises improve their defenses. Indeed, early efforts have shown the preliminary promise of LLMs at security-related tasks and solving basic CTF-style challenges (e.g., [4, 12, 15, 19, 23, 36, 41, 42, 47-50, 52]).\nTo date, however, most of these efforts have focused on CTF style challenge problems (e.g., a cryptography problem) or a single host attack (e.g., find and exploit a vulnerable service). In practice, real cyberattacks are often multistage network attacks where attackers execute a variety of actions across multiple hosts such as conducting reconnaissance, exploiting vulnerabilities to gain initial access, leveraging internal hosts to laterally move, and using compromised hosts to exfiltrate data [7, 31, 35]. These attacks can range from red team exercises to evaluate corporate defenses to nation states funding hacker groups to attack foreign adversaries [9, 16].\nAs such, it remains unclear if state-of-the-art LLMs can realize multistage network attacks. As a first step, we create 10 multistage attack environments (ranging from 25 to 50 hosts) and evaluate different LLMs at executing attacks. We find that across all environments LLMs are unable to realize multistage network attacks and can only reliably conduct reconnaissance tasks.\nWe analyze why and how LLMs fail from first principles using an attack graph abstraction [43]. Seen in this light, we find that LLMs often output irrelevant commands, commands that cannot achieve any state in the attack graph (e.g., try to exploit a vulnerability that does not exist). We also find that even when LLM commands are relevant (i.e., could help the attacker achieve states in the attack graph), they are incorrectly implemented, leading to failure (e.g., a scan command with the wrong parameters).\nTo address these failure modes, we introduce Incalmo, a high-level attack abstraction layer for LLMs to autonomously conduct multistage network attacks. Rather than LLMs specifying low-level shell commands, using Incalmo, LLMs specify high-level tasks or queries. Incalmo then translates these into low-level primitives, executes them, and replies to the LLM with key information. The design of Incalmo consists of three main modules shown in Fig. 1:\n\u2022 To minimize LLMs generating incorrect implementations of commands, we introduce an action planner. The action planner assists LLMs in the implementation of actions by providing high-level tasks for LLMs to execute (e.g., scan a network, laterally"}, {"title": "Ethics, disclosure, and reproducible research", "content": "It is important to understand what LLMs can and cannot do with respect to cybersecurity. By understanding these capabilities we can create guardrails and defenses. Furthermore, red-teaming networks, an important cyber-defense task, is expensive because it requires humans with large amounts of domain knowledge. Autonomously conducting network attacks could help defenders preemptively identify vulnerabilities. In addition, autonomous attackers can potentially help train both human and non-human defenders.\nIncalmo only has a limited number of attacker capabilities (e.g., only has five exploits), limiting the harm that it could cause in practice. We acknowledge that attackers can extend Incalmo for more advanced capabilities. Similar to prior work [12, 48, 52], we will make the environments, our tools to reproduce prior work, and Incalmo open-source and publicly available to the research community. We plan to notify major LLM providers prior to publication so they can implement guardrails if needed 1."}, {"title": "2 Related Work and Motivation", "content": "We start with a brief overview about related work in LLMs executing attacks. Then, we address a key blind spot in prior work-understanding how LLMs perform at executing multistage network attacks. To this end, we evaluate popular LLMs in 10 multistage attack environments ranging from 25-50 hosts.\n2.1 Related work on LLMs executing attacks\nPrior studies evaluate LLMs at solving CTF-style challenges [4, 12, 15, 19, 23, 36, 41, 42, 47-50, 52]. Many of these CTFs are challenge problems related to security but do not involve infecting a host (e.g., finding an XSS vulnerability or solving a cryptography challenge [12, 36, 52]). At most, the challenges are single host attacks that involve infecting a single host [12, 36, 47, 48, 52]. While some of these challenges require multiple low-level steps (e.g., identify a remote service, discover a vulnerability, then exploit the vulnerability) [12, 36, 52], they do not involve multiple hosts and subnetworks. We refer to challenges that involve multiple hosts and subnetworks as multistage network attacks. Existing efforts have not tackled multistage attacks, which is the focus of our work.\nAt a high level, prior work on using LLMs in attack challenges fall in two classes: (1) fully autonomous (e.g., [36, 47, 48, 52]) and (2) human-assisted LLM attack systems (e.g., [12, 52]). The autonomous frameworks instruct LLMs to attack the environment by outputting command line instructions. Then, a second program automatically extracts commands from the LLM's response and executes it on a computer with access to the environment.\nIn contrast, human-assisted LLM-based attack tools such as PentestGPT [12] and Cybench [52] use a human in the loop, both to give suggestions to the LLM and to execute the commands the LLMs output. PentestGPT, uses state-of-the-art prompting strategies to improve the ability for LLMs to help humans in solving CTF challenges. PentestGPT outputs suggestions for high-level tasks (e.g., try to find vulnerabilities on the web server) and command line instructions (e.g., an nmap command). The human operator then"}, {"title": "2.2 Multi-stage Attack Evaluation Methodology", "content": "We implement 10 multistage attack environments ranging from 25-50 hosts with brief overviews in Table 1 and detailed descriptions in Appendix A. The environments are inspired from a mix of public reports of real-world attacks [26, 31], common topologies [1, 2], or used in prior work [1, 17, 27, 29, 46]. The goals of these environments are either to exfiltrate critical data or access critical network hosts. Unlike CTF challenges, all of the environments are multistage, attackers have to conduct a variety of tasks to achieve the goal such as scanning networks, identifying vulnerabilities, exploiting remote services, escalating user privileges, and exfiltrating data. For instance, the Equifax-inspired environment requires attackers to execute over 246 unique tasks (we formally define a task as a sequence of commands in Sec. 3).\nIn order to systematically evaluate LLMs ability at conducting multistage attacks we require an LLM-agnostic tool that autonomously conducts attacks on a given environment. Since the autonomous tools are closed-source [36, 47, 48], we replicate prior\nwork by building an LLM agnostic tool that follows the same process 3. We instruct the LLM to attack the environment with the goal to exfiltrate any data and gain access to any critical hosts. In the prompt, we also include the external ip address range of the environment. Finally, we instruct the LLM to output specific command line tasks to execute. Then, our tool extracts the commands and executes them on a Kali host that has access to the environment. All the tools required to execute the multistage attacks are preinstalled on the Kali computer, and the computer has the top 10 most common attacker tools preinstalled.\nWe also validate that the state-of-the-art prompting strategies in PentestGPT [12] do not realize end-to-end attacks. Since Pentest-GPT requires a human operator, we manually evaluate PentestGPT by inputting the goal prompt used for the autonomous system. Then we manually enter the commands into the attacker's Kali host. To control for the human operator assisting the LLM, we only execute concrete commands from PentestGPT and if a concrete command is not given, we ask once to supply a concrete command. If no concrete command is given after asking, we end the trial.\nWe evaluate the autonomous command line attack tool across 3 LLMs across 10 different environments with 5 trials for each pairing. We also manually evaluate PentestGPT across all 10 environments with 3 trials each\u2074. We were unable to evaluate o1, a state-of-the-art \"reasoning models\", because the public API has a safeguard that prevents o1 from executing attacks."}, {"title": "2.3 Findings", "content": "In Fig. 2, we measure LLM success by either not achieving a single goal (e.g., not exfiltrating a single file), partially succeeding at least"}, {"title": "3 Why do LLMs struggle with multistage attacks", "content": "A natural question, then, is why did the LLMs fail to execute these multistage attacks. Unfortunately, prior work only offers very high-level guidance in this regard; e.g.,\nTo shed light on these failure modes and inform our design, we use a first-principles approach using attack graphs [38, 43]. At a high level, a multistage attack entails a complex end goal, where an attacker needs to break down the complex goal into a number of intermediate states. This is precisely what the attack graph formalism offers, as it provides a formal foundation for modeling attacker end goals, sub goals, intermediate states, and candidate actions to achieve intermediate states [38, 43]. As we will see next, using the attack graph formalism helps us shed light on when and how LLMs failed in the multistage attack."}, {"title": "3.1 Preliminaries", "content": "Formally, an attack graph is defined as $G = (S, A, S_0, S_g)$ where S is a set of states, $A \\subseteq S \\times S$ is the set of actions (directed edges) representing transitions between these states, $S_g \\subseteq S$ is the set of goal states, and $S_0 \\subseteq S$ is the set of initial states [43]. Intuitively, in the attack graph, the nodes are attacker states (e.g., gained access to web erver) and the edges are attack actions (e.g., exfiltrate data). Attack graphs can be complex and consist of many intermediate states. Fig. 3 shows an example attack graph of the Equifax-inspired environment, which has 246 unique states.\u2075\nWith an attack-graph formulation we can systematically analyze why the LLM attacker failed. We can measure when an attacker failed by identifying the states the attacker was able to achieve (e.g., found external hosts) and which ones they failed to achieve (e.g., unable to gain initial access). And, we can identify how an attacker failed by analyzing the edges in the attack graph. For instance, an LLM may have been unable execute an action (i.e., an edge) because a command had the incorrect parameters.\nTo execute these analyses, we need to incorporate the concept of a command into the attack graph. Each action $a \\in A$ is composed of a sequence of commands. A single command is defined as a function $c: (h,n,p) \\rightarrow o$ where h is the host on which the command is run, n is the name of the command, p are the parameters of the command, and o is the output of the command.\nEach action a is a finite sequence of commands: $a = (c_1, c_2, ..., c_k)$ where each $c_i: (h_i, f_i, p_i) \\rightarrow o_i$. We define a successful attack path, where an attacker achieves all of their goals, as $\\pi = (S_0, S_1, ..., S_n)$ such that $S_g \\in {S_0, S_1, ..., S_n}$.\nWith these preliminaries, next we describe how we use the attack graph formulation to analyze why LLMs failed in multistage attack."}, {"title": "3.2 Mapping LLM commands to attack graphs", "content": "Our first step is to logically map the LLM actions to the corresponding environment's attack graph. Given that the command logs for a single trial is often thousands of lines long, manual analysis is intractable. To this end, we develop a framework to heuristically map LLM commands to ideal attack graphs.\u2076\nFirst, for each trial, we identify the states in the attack graph the LLM achieves. We identify these states by searching for keywords in command outputs. For instance, we search for relevant IP addresses to identify the number of hosts discovered and relevant CVEs to identify the number of vulnerabilities found. In Fig. 4, we show the maximum percentage of attack states achieved by each LLM in all 10 environments. Across all environments, LLMs are only able to achieve 1-30% of the states in the attack graph."}, {"title": "3.3 Analysis results", "content": "We map all LLM outputs for all five trials for two environments. We only map LLM outputs for two environments because of the large manual effort in creating $C_{man}$. We choose the Equifax-inspired environment and the 4-Layer Chain environment because these are the environments the LLMs performed the best and worst in.\nWe show the percentage of LLM commands in each category in Fig. 6: relevant commands (i.e., successful commands that led to states in the attack graph), irrelevant commands (i.e., the first failure mode), and relevant commands with incorrect implementations (i.e., the second failure mode).\nCommands irrelevant to attack (Failure mode 1):\nAcross the LLMs and environments, 28-81% of commands\nare irrelevant to the multistage attack ($C_I$) shown in Fig. 6.\nFirst, LLM commands failed to achieve attack graph states because many were irrelevant to the multistage attack. For instance, the LLMs tried brute forcing SSH credentials, finding misconfigured files, or exploiting non-exploitable services."}, {"title": "4 Design of Incalmo", "content": "To address the failure modes in Sec. 3, we present Incalmo, a high-level abstraction layer that sits between the LLM and the environment to be tested, shown in Fig. 7. Rather than the LLM directly outputting low-level shell commands (e.g., nmap scans, Metasploit exploits), we have LLMs output high-level tasks and queries (e.g., infect a host, scan a network, find a path to a host). Incalmo translates these tasks and queries into their corresponding low-level primitives, executes the primitives, appends the results to the prompt, and requests another task or query from the LLM."}, {"title": "4.1 The case for more abstraction", "content": "In some sense, seeing LLMs struggle to reason about multistage network attacks is not surprising-LLMs continue to struggle with complex reasoning for many other domains too [18]. Other domains have partially addressed these problems by offloading the solution step to high-level frameworks and APIs [18, 30, 40]. For instance, to answer a user's question, LLMs sometimes use Wikipedia's high-level API instead of generating low-level web requests [40]. Or in another case, LLMs use Bing's high-level API to search the web for relevant information [30].\nWe observe a similar parallel in our problem setting. We argue the need for a high-level abstraction layer that the LLM can use to offload the solution and reasoning steps for multistage network networks. However, to the best of our knowledge, no such framework or high-level API exists in the security domain. Indeed, as we saw, prior work uses LLM as-is or add human-in-the-loop reasoning, and this is fundamentally limited.\nTo this end, we introduce Incalmo, a high-level abstraction layer for multistage attacks. Before we describe the key components of Incalmo and how it addresses these failure modes, we outline system requirements."}, {"title": "4.2 Requirements", "content": "We outline three key requirements that any multistage abstraction layer should satisfy:\n\u2022 LLM agnostic: New types of LLMs are constantly being created. We want our abstraction layer to work with any type of LLM. For example, we want to quickly evaluate and compare LLMs of different sizes.\n\u2022 Environment agnostic: Real networks often differ in topology, vulnerabilities, and configurations. We want an abstraction layer that remains independent of the specific environment.\n\u2022 Extensible: As new attack capabilities appear, the abstraction layer should be extensible for new techniques."}, {"title": "4.3 Detailed Design", "content": "The design of Incalmo builds on two key ideas. Our first insight is that we can use the attack graph formalism to avoid executing commands irrelevant to a multistage attack (Failure mode 1). Our second insight is to raise the level of abstraction for the commands that LLMs output to minimize the chances of incorrect implementations (Failure mode 2). That is, we instruct the LLM to output high-level tasks that Incalmo translates to correct low-level commands, instead of the LLM writing error-prone shell commands which results in dead ends in a multistage attack.\nLLMs interact with Incalmo with two interfaces: a task or a query. A task is a high-level action to execute on hosts the attacker has access to (e.g., exfiltrate data from a host). A query is a request for information from Incalmo (e.g., find an attack path to a host).\nTo execute tasks and queries, Incalmo has three key modules:\n\u2022 An action planner to assist LLMs with the implementation of commands (Failure mode 2). The action planner translates high-level tasks into low-level primitives. Additionally, the action planner is extensible to support new attacker capabilities."}, {"title": "4.4 End-to-end workflow", "content": "Using Incalmo with LLMs involves three logical steps seen in Fig. 8:\n\u2022 First, we have an LLM-agnostic onboarding pre-prompt stage where we \"teach\" the LLM the available capabilities and APIs in Incalmo.\n\u2022 Second, we provide environment specific prompts to outline attack goals and environment details.\n\u2022 Finally, the LLM autonomously executes the multistage attacks via Incalmo in an iterative execution loop.\nDuring the execution phase, LLMs are instructed to output Python functions and label them as either a query or a task. As discussed earlier, these functions can compose multiple atomic Incalmo APIs. Task functions are required to return a list of tasks to execute. Each task has a source host, the host the task is executed on, and could have optional parameters (e.g., a host to infect). For example:\nQuery functions return a list of database objects. Incalmo will respond to the query by translating each object to a string:\nThese functions are then executed by Incalmo that will either execute tasks on a specified hosts, or answers the query. Additionally, sometimes LLMs output functions with errors such as Python code with incorrect syntax. If an error occurs, Incalmo replies with the error so the LLM can potentially fix their mistake. Next, we show an end-to-end case study of an LLM using Incalmo to execute a multistage network attack."}, {"title": "5 Illustrative case study", "content": "In this section, we show a concrete example of how an LLM, Sonnet 3.5, interacts with Incalmo to execute an attack in the Equifax-inspired environment. We describe the three phases of LLM-Incalmo interaction (Fig. 8): 1) give the LLM instructions on how to use Incalmo, 2) provide the LLM with initial information about the environment, and 3) LLMs iteratively use Incalmo to conduct the multistage attack.\nIncalmo instructions: First, we onboard the LLM with a prompt containing a goal and instructions for how to use Incalmo. The"}, {"title": "6 Implementation", "content": "As a practical way to implement Incalmo interfaces, we extend the open-source attack framework Caldera [5]. For those unfamiliar with Caldera, it is a tool released by MITRE for enabling semi-automated adversary emulation capabilities. Caldera provides basic capabilities for exploits, orchestration of hosts, and tooling relevant"}, {"title": "7 Evaluation", "content": "In this section, we conduct end-to-end experiments to show how LLMs can use Incalmo to autonomously conduct multistage attacks. Then, we conduct a factor analysis on the use of Incalmo components (i.e., action planner and services) and LLM model size."}, {"title": "7.1 Setup", "content": "First, we use the multistage attack environments from Sec. 2, shown in Table 1. Two of the environments are inspired from public reports of real-world attacks [26, 31]. Two other environments are based on common enterprise network topologies [1, 2]. The remaining environments are inspired by environments in prior work [1, 17, 27, 29, 46]. Further details about environments can be found in Appendix A.\nFor these experiments we consider two success metrics:\n\u2022 Attack success: Our first metric measures the LLM's ability to achieve their goals. We consider a full success as an LLM achieving all goal states (e.g., exfiltrate all 48 data files in the Equifax-inspired environment) in the attack graph. We consider partial success as the ability to achieve at least one goal state (e.g., exfiltrate at least one data file in the Equifax-inspired environment), and no success as the inability to achieve any goal states.\n\u2022 Attack graph coverage: Sometimes LLMs are unable to partially succeed, but are still able to make substantial progress in attacking the network. To capture this, We measure the percentage of all states the LLM was able to achieve in the attack graph. These states measure the LLM's ability at other sub-goals of the multistage attack (e.g., reconnaissance, lateral movement).\nWe evaluate Incalmo across all 10 environments with six different LLMs of various sizes and from different companies: Sonnet 3.5, Haiku 3.5, GPT40, GPT40 mini, Gemini 1.5 Pro, and Gemini 1.5 Flash. For each environment, we execute 5 trials for each LLM. We control for the attacker's budget by setting a time limit for each trial at 75 minutes."}, {"title": "7.2 Results", "content": "For the following results, for both types of metrics, we consider the peak or maximum efficacy across trials. We believe this is more realistic as attackers can rerun the LLM on the network multiple times.\nFinding 1.A: Using Incalmo, LLMs can autonomously and\nfully succeed at multistage attack in 5 out of 10 environments and partially succeed in 9 out of 10 environments. In contrast, recall that LLMs without Incalmo, can only partially succeed in 1 out of 10 environments (Fig. 2).\nWe find that LLMs with Incalmo are able to successfully execute end-to-end multistage attacks across 9 out of 10 of the environments. For instance, in the Equifax-inspired environment, the most realistic environment [31], all six LLMs with Incalmo are able to exfiltrate at least some of the data with four LLMs able to exfiltrate all of the data. In comparison, all LLMs without Incalmo are unable to exfiltrate any of the data.\nFinding 1.B: Across all 10 environments, LLMs with Incalmo achieve a maximum of 48-100% of attack graph states, whereas without Incalmo, they achieve a maximum of 1-30% (Fig. 10).\nLLMs equipped with Incalmo achieved more states in the attack graph, compared to LLMs without Incalmo. LLMs equipped with Incalmo in the worst case environment (Enterprise B with 48% of states achieved) achieved more states than LLMs without Incalmo in the best case environment (Enterprise A with 30% of states achieved). Furthermore, in environments where LLMs without Incalmo achieved a limited number of attack graph states, we can see significant improvements when the LLMs use Incalmo. For instance in the Equifax-inspired environment, 4 different LLMs were able to achieve more than 99% of attack graph states with Incalmo, but without Incalmo, the same 4 LLMs were only able to achieve 0.8% of the attack graph states."}, {"title": "7.3 Factor analysis", "content": "Impact of Incalmo modules: First, we assess the relative impact of the two key ideas in Sec. 4: using an action planner to assist LLMs with the implementation of actions and using services to assist LLMs with outputting irrelevant actions.\nTo this end, we conduct a series of ablation experiments, removing different modules as seen in Table 2. First, we create an intermediate version without the action planner, Incalmo-WAP, where LLMs do not have access to the action planner, but can use the environment and attack graph services. Here, LLMs can perform"}, {"title": "8 Discussion and limitations", "content": "In this section we discuss some of the key limitations of Incalmo. Improving partial success: In 4 out of 10 environments, LLMs were only able to partially succeed with Incalmo. Partial success often occurs because LLMs are not persistent in exploring attack paths. For instance, frequently LLMs achieve a single goal and then stop. However, we note that in many of these trials LLMs could have queried the attack graph service to identify that there were additional paths to explore. We hypothesize that LLMs do not use the full potential of the attack graph service because they have little training data for multistage network attacks and attack graphs. As a result, as future work, we plan to explore adding additional data through fine-tuning LLMs to hopefully improve the ability of LLMs using the attack graph service.\nImproving failure scenarios: LLMs with Incalmo were unable to partially succeed in the Enterprise B environment. The Enterprise B environment was the only environment that required both an external scan (to identify vulnerable web servers) and an internal scan (to identify a vulnerable database management server). The best performing LLMs failed to execute the internal scan and were distracted by other potential targets in the network.\nWe hypothesize that improvements to Incalmo's attack graph service can help address these types of environments. Incalmo's attack graph service lack abstraction for fine-grained reasoning"}, {"title": "9 Other related work", "content": "Sec. 2 discussed closely related work and showed a critical gap w.r.t. multistage attacks. We briefly describe other related work here.\nLLM security benchmarks: As mentioned in Sec. 2, there are many benchmarks for evaluating LLMs in CTF challenges (e.g., [4, 15, 36, 41, 42, 48, 48, 49]). However, they are mostly challenge problems and at most single host attacks. Other non-CTF benchmarks evaluate general security knowledge (e.g., [45]).\nOther research in LLMs for security: In addition, there is work to create LLM-based systems for other security tasks. For instance, there is work evaluating LLMs ability to find vulnerable code (e.g., [48]), using LLMs to summarize defender security logs (e.g., [10]), and using LLMs for anomaly detection (e.g., [13]). Other work has shown how LLMs can be used for social engineering tasks like phishing [20, 39]. These are orthogonal to our focus on multistage attacks.\nAutonomous attack emulation: There other autonomous attacker emulation systems that are not based on LLMs. For instance, there are rule-based and state machine attack systems (e.g., [3, 14, 21, 22, 51]). There is also work exploring using reinforcement learning to emulate attackers (e.g., [6, 24])."}, {"title": "10 Conclusions", "content": "Fully autonomous multistage network attackers can enable defenders to cheaply and evaluate their security postures. While LLMs are natively unable to serve this role today, we demonstrate the feasibility of LLMs potentially filling this role by introducing Incalmo, an abstraction layer that enables LLMs to autonomously conduct multistage attacks. We demonstrate across 10 different environment that LLMs equipped with Incalmo can autonomously find vulnerable services, execute exploits gain access to networks, discover configurations and vulnerabilities to laterally move, exploit vulnerabilities to escalate privileges, and exfiltrate data."}, {"title": "A Environments", "content": "In this section, we give detailed descriptions of each environment. Equifax-inspired environment: The Equifax-inspired environment has two web servers running a vulnerable version of Apache Struts with CVE-2017-5638, the same as the real environment [31]. During the Equifax breach, the attacker discovered a plaintext file on one of the web servers that included credentials to 48 different database hosts on a separate network [31]. 11\nTo replicate the databases in our environment, we create a second network with 48 database hosts and add files with fake critical consumer data such as emails, social security numbers, and addresses. On a random web server, we add a plain-text SSH configuration file that contains credentials to all the databases.\nColonial Pipeline-inspired environment: We implement an environment inspired by the Colonial Pipeline breach [26] and other ICS attacks [28, 44]. The goal of the attacker is to gain access to devices that control physical devices, we call these devices critical actuators. The environment has three networks: two IT networks and one OT network. The two IT networks have 10 hosts each and the OT network has 15 sensor hosts, 5 controller hosts and 5 critical actuator hosts. Each of the 5 controller hosts have credentials to one of the 5 actuators because controller hosts use data from the sensor hosts to control the actuators [32]. In addition, each IT network has a management host that have credentials to all sensors and control hosts [32]. The monitoring hosts have misconfigured ncat services that can be exploited to remotely execute code.\nAttackers often get access to IT hosts through techniques such as exploiting weak passwords (the case in the Colonial Pipeline breach [26] and phishing [33, 34]. We emulate this in the environment by giving the attacker initial access to a random host on the IT network.\nEnterprise A: The Enterprise A network is modeled based on a common tree hierarchy [1]. The Enterprise network has 3 networks, each network represent a floor in a building. The attacker's goal is to exfiltrate all data in the network. One network is external and has 10 web servers. Another network contains 10 employee hosts. And the last network contains 10 database hosts.\nThe web servers run a vulnerable version of Apache Struts. Additionally, each web server has SSH credentials to a random employee host. The database network contains a single management host that has access to the remaining 9 databases. The management host has a misconfigured ncat service.\nEnterprise B: The Enterprise B is also a tree hierarchy [1]. In contrast, the Enterprise B network has 4 networks. One network is external web servers, 2 networks contain employee hosts, and the remaining network has databases.\nThe web servers run a vulnerable version of Apache Struts. Each web server has credentials to a user on random employee host. One of the employee host's root user has access to all database servers. Additionally, this host is vulnerable to sudoedit (CVE-2023-22809).\n4-Layer chain: Some evaluations of game-theory based deception algorithms consider a Ring network where each host has credentials to one other host in the network [27, 46]. We implement our ring"}, {"title": "B Reliability results", "content": "In Fig. 14, we show the number of trials that LLMs with Incalmo achieved full and partial success. In several environments, several LLMs with Incalmo were able to reliably achieve partial success. For instance, Haiku 3.5 and Sonnet 3.5 were able to achieve partial success in all 5 trials in 5 of the environments. In regards to full success, LLMs were generally not reliable across all environments. Only Haiku 3.5 in the Colonial Pipeline-inspired environment had full success in all 5 trials and Gemini 1.5 Pro in the Equifax-inspired environment had full success in 4 out of the 5 trials."}, {"title": "C Extensibility", "content": "In this section, we provide illustrative examples of how Incalmo can be extended to incorporate new attacker capabilities. Primarily, we show example extensions for: (1) a new high-level task to the action planner, (2) a new low-level capability to the action planner, and (3) a new type query of query for the environment state service.\nNew high-level task: Attackers can try to avoid detection by limiting their bandwidth when they exfiltrate data [31]. Now, we show how we can easily extend Incalmo to enable LLMs to be able to stealthy"}, {"title": "D Code snippets for last stage of attack", "content": "Below is the code snippet of Sonnet 3.5 infecting a database with the lateral movement task."}]}