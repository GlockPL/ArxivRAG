{"title": "Event Stream based Sign Language Translation: A High-Definition Benchmark Dataset and A New Algorithm", "authors": ["Xiao Wang", "Yao Rong", "Fuling Wang", "Jianing Li", "Lin Zhu", "Bo Jiang", "Yaowei Wang"], "abstract": "Sign Language Translation (SLT) is a core task in the field of AI-assisted disability. Unlike traditional SLT based on visible light videos, which is easily affected by factors such as lighting, rapid hand movements, and privacy breaches, this paper proposes the use of high-definition Event streams for SLT, effectively mitigating the aforementioned issues. This is primarily because Event streams have a high dynamic range and dense temporal signals, which can withstand low illumination and motion blur well. Additionally, due to their sparsity in space, they effectively protect the privacy of the target person. More specifically, we propose a new high-resolution Event stream sign language dataset, termed Event-CSL, which effectively fills the data gap in this area of research. It contains 14,827 videos, 14,821 glosses, and 2,544 Chinese words in the text vocabulary. These samples are collected in a variety of indoor and outdoor scenes, encompassing multiple angles, light intensities, and camera movements. We have benchmarked existing mainstream SLT works to enable fair comparison for future efforts. Based on this dataset and several other large-scale datasets, we propose a novel baseline method that fully leverages the Mamba model's ability to integrate temporal information of CNN features, resulting in improved sign language translation outcomes. Both the benchmark dataset and source code will be released.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of deep learning, AI (Artificial Intelligence) for good has received widespread attention, among which, Sign Language Translation (SLT) [40, 41, 43] is increasingly being emphasized. It aids communication between deaf individuals and non-sign language users by converting sign language videos into natural language text, significantly enhancing the social participation and quality of life for the deaf community. However, the performance of sign language translation is still limited due to the usage of RGB cameras, as it is easily influenced by limited frame rate, illumination, motion blur, etc. In addition, the deployment of RGB-based sign language recognition models may also pose potential ethical risks as it involves issues of human privacy.\nRecently, biologically inspired Event cameras (e.g., DVS346, Prophesee, CeleX) have drawn more and more attention due to their unique features in high dynamic range, low energy cost, sparse spatial but dense temporal resolution, and low latency, etc [9]. For the imaging mechanism, each pixel within an Event camera operates independently and asynchronously, a distinct departure from RGB cameras that produce synchronized frame outputs. A binary pulse signal is recorded only when the change in brightness for each pixel exceeds a specific threshold. Some computer vision tasks have involved the Event camera for event-based or event-enhanced learning, such as object detection [10] tracking [34, 35], action recognition [36, 37], and also the sign language translation [42] discussed in this paper.\nAlthough limited, there are already some research efforts that have begun to focus on event-based sign language translation, as shown in Fig. 1. Early models are typically developed based on simulated event data from existing RGB datasets, such as PHOENIX-2014 [17] and CSL-Daily [44]. Obviously, the simulated data can't fully reflect the key features of the real event stream. Recently, three real event datasets have been proposed, including SL-Animal-DVS [31], EV-ASL [38], EvSign [42]. However, the first two datasets have limited spatial resolution (128 \u00d7 128), and the EvSign [42] crops the head regions of the actor for privacy-preserving. We believe this approach may corrupt the raw information of the event stream, significantly impacting the recognition and analysis of gestures involving the head area.\nIn this paper, we propose a new sign language translation dataset, termed Event-CSL, which is the largest, high-definition (1280 \u00d7 720) real-event dataset collected using the Prophesee EVK4-HD Event camera. It contains 14,827 videos, 14821 glosses, and 2,544 Chinese words in the text vocabulary. Our dataset is collected in a variety of indoor and outdoor scenes, encompassing multiple angles, light intensities, and camera movements. More importantly, we provide the most primitive event stream data, which lays the groundwork for accurately recognizing the meanings conveyed by gestures. In our experiments, we split these videos into training, validation, and testing subsets, which contain 12,602, 741, and 1,484 samples, respectively. To build a more comprehensive benchmark, we train and report 6 recently released SLT models for future works to compare. More details can be found in Section 4.\nOn the basis of our newly proposed Event-CSL dataset, we also propose a new event-based sign language translation framework, as shown in Fig. 2. The key insight of our framework is that Convolutional Neural Networks (CNNs) effectively capture local features and have consistently been the dominant visual backbone networks for sign language translation tasks. Despite the impact of Transformer models [32], their lower complexity and higher precision have continued to be favored. Therefore, this paper employs a residual network, ResNet18 [14], to encode the input event streams. To further enhance its global modeling capabilities, we introduce Mamba layers [11] to fuse CNN feature maps along the temporal views, thereby strengthening its perception. This hybrid CNN-Mamba architecture can even achieve better sign language translation results than dense Transformer models. We input the event visual features into the decoder to translate and obtain the final Chinese descriptions. Extensive experiments conducted on multiple existing SLT datasets and our newly proposed Event-CSL all validated the effectiveness of our framework.\nTo sum up, the main contributions of this paper can be summarized as the following three aspects:\n1). We propose a new large-scale, high-definition event-based sign language translation dataset, termed Event-CSL. It contains 14,827 videos and 2,544 Chinese words recorded in both indoor and outdoor scenarios.\n2). We propose a hybrid CNN-Mamba vision encoder for our sign language translation framework. The integration of CNN and Mamba layers makes our vision backbone well capture local and long-range global relations and retain low computational costs.\n3). Extensive experiments conducted on four event-based SLT datasets fully validated the effectiveness of our framework. In addition, we have evaluated multiple representative and state-of-the-art (SOTA) SLT algorithms on our newly proposed Event-CSL dataset and also two simulation datasets (PHOENIX-2014T-Event [2], CSL-Daily-Event [44]) hoping that these benchmarks can contribute to the advancement of the SLT field."}, {"title": "2. Related Work", "content": "2.1. Sign Language Translation\nIn recent years, sign language translation has been widely concerned in the field of computer vision. Sign Language translation aim to translate sign language videos into spoken text to help deaf people communicate. Camg\u00f6z et al. [2] introduce the Sign Language Translation (SLT) problem. In the task of SLT, the word order and grammatical differences between sign language and spoken language are taken into account to convert sign language videos into spoken language translations. Camg\u00f6z et al. [3] propose an architecture based on Transformer, which can simultaneously learn sign language continuous recognition and translation end-to-end, without any ground-truth timing information, and improve performance. GloFE [21] extracts common concepts from text and presents them as indirect representations of weak forms, then uses global embeddings of these concepts as queries for cross-attention lookups to find the corresponding information in the learned visual features. Zhou et al. [43] propose a two-stage approach, which first combines contrast language image pre-training (CLIP) with masking self-supervised learning to create a pre-task to bridge the semantic gap between visual and text representation and restore masking sentences. Next, build an end-to-end architecture that inherits the parameters of"}, {"title": "2.2. State Space Model", "content": "The State Space Model is a mathematical model that describes the behavior of a dynamic system using a set of first-order differential equations (continuous-time systems) or difference equations (discrete-time systems) to represent the evolution of the internal state of the system, and another set of equations to describe the relationship between the state and the output of the system. By parameterizing the linear state-space model of each time series with a joint learning recurrent neural network, Rangapuram et al. [29] maintain the data efficiency and interpretability of the state-space model, utilizing the ability of deep learning methods to learn complex patterns from raw data. Gu et al. [12] propose the Structured State Space (S4) sequence model based on a new parameterization for the SSM, stably diagonalize the state matrix A by low-rank correction, and simplify the SSM calculation to Cauchy kernel calculation. Gu et al. [13] propose a sequence model called Linear State Space Layer (LSSL), which combines the advantages of recurrent neural networks (RNNs), temporal convolution, and neural differential equations (NDEs) with powerful modeling capabilities and computational efficiency. Mamba [11] is a sequence modeling method, which solves the problem of low computational efficiency of traditional methods when dealing with long sequences. Mainly, it realizes the selective information propagation and forgetting of discrete modes by designing the parameters of SSM as input functions. VMamba [24] is a visual state-space model with linear complexity, while still retaining the advantages of global receptive field and dynamic weights. It compresses the hidden state with S6 so that each element in the sequence can interact with any previously scanned sample, introducing a Cross-Scan Module (CSM) to address directional sensitivity. Vim [45] is a bi-directional Mamba block universal vision backbone network. It treats image blocks as sequential data by marking position embeddings in the image sequence and compressing the visual representation using bidirectional state space models. Dao et al. [7] reveal the connections between Transformers and state space models (SSMs) such as Mamba, which are connected through various decompositions of structurally semi-separable matrices. They propose the State Space Duality (SSD) framework and design a architecture (Mamba-2), whose core layer is a 2-8 times faster refinement of the selected SSM of Mamba. Inspired by these works, in this paper, we propose to augment the local CNN features using the SSM for the event-based sign language translation."}, {"title": "3. Methodology", "content": "In this section, we will first give an overview of our proposed event-based sign language translation framework. Then, we will introduce the details of input representation, network architecture, and loss function.\n3.1. Overview\nAs shown in Fig. 2, given the event streams, we first stack into event frames and feed them into the residual network for feature extraction. We conduct average pooling and flatten operations on these CNN features to get the event visual tokens. Then, a Mamba block is adopted to capture the long-range temporal features. The forward and backward SSM are conducted to process the CNN features. The enhanced features will be fed into the temporal convolutional module for dimension reduction on the temporal view. Then, we feed the features into the sign embedding module and the Transformer decoder for accurate Chinese language generation.\n3.2. Input Representation\nGiven event streams $E_P = \\{e_1, e_2, ..., e_M\\}$, where $M$ denotes the number of event points, each point $e_{ili} \\in (1, M)$ is a quadruple $\\{x, y, t, p\\}$, here the $(x, y)$ denotes the spatial coordinates, $t$ and $p$ denotes the time stamp and polarity, respectively. To make full use of existing deep neural networks, in this paper, we first transform the event streams into event images $E_T = \\{F_1, F_2, ..., F_T\\}$, where $T$ denotes the number of stacked event frames, and each event frame is resized into a fixed resolution $F_i \\in R^{C \\times H \\times W}$ Here, $\\{C, H, W\\}$ denotes the dimension of the channel, height, and width of an event frame, respectively. In the training phase, we randomly sample a mini-batch that contains B event samples and the dimension of the mini-batch is $B x T \\times C \\times H \\times W$.\n3.3. Network Architecture\n\u2022 Hybrid CNN-Mamba Vision Backbone Network. In this paper, we propose a hybrid CNN-Mamba backbone"}, {"title": "3.4. Loss Function", "content": "The language decoder sequentially generates the logits of sign language sentences $S = \\{p(<BOS>), p(w_1), p(w_2), ...,p(w_U),p(<EOS>)\\}$. In sentence S, the first special word $<BOS>$ marks the beginning of the sentence, the last special word $<EOS>$ marks the end of the sentence, and $p(w)$ represents the logits of the generated word. In this work, we adopt the widely used cross-entropy loss function to calculate the distance between the annotated sign language and the generated sentence, which can be formulated as:\n$\\L = -log \\prod_{u=1}^{U} P(S_u|w_u).$"}, {"title": "4. Event-CSL Dataset", "content": "4.1. Protocols\nWhen collecting our sign language translation dataset Event-CSL, we follow the following protocols:\n1). Diversity of views: We capture different perspectives of sign language, including slightly downward, horizontal, and slightly upward views. 2). Camera motions: The movement of the event camera significantly affects the number of events collected and the complexity of the background. Our dataset collection takes into account a variety of motion patterns, including fast movement, slow movement, and moderate movement. 3). Diversity of scenes: In order to conform to the daily use of sign language, we consider different usage scenarios when recording videos. Specifically, these scenarios comprise office environments, daily life, and outdoor scenes (such as streets, parking lots, open spaces). 4). Intensity of light: In environments with strong or weak light intensity (high and low exposure), event cameras can still record information about the movement of objects due to the presence of pixel changes. When we record videos, respectively from the following several light intensities: strong light, low light, and dim light. 5). Continuity of action: Keep the movement of sign language flowing and coherent when recording sign language, reducing useless information and redundant video frames. 6). Diversity of sign language content: The content of sign language videos should be rich and varied to be closer to the real application. We randomly extract captions from the COCO-CN [19] dataset, and the descriptions of these contents mainly include the following aspects: daily life scenes, outdoor sports, animals and plants, sports competitions, character introductions, and landscape descriptions.\n4.2. Statistical Analysis\nOur proposed dataset is the first large-scale high-resolution event-based sign language benchmark dataset. All the videos in our dataset are collected by Prophesee EVK4-HD Event camera which has a resolution of 1280 \u00d7 720. Our dataset collects a total of 14,827 sign language videos and contains 14,821 glosses as well as 2,544 Chinese words in the text vocabulary. We have divided the dataset into three subsets for training, validation and testing, with their quantities being 12,602, 741, and 1,484 respectively. Additionally, within the text sentences of the dataset, the minimum, maximum and average numbers of characters of the sentences are 8, 75 and 18 respectively. As shown in 1, the dataset proposed by us surpasses the existing event-based sign language datasets in terms of the number of videos, video resolution, gloss, and text vocabulary. The visualization of the word cloud is provided in Fig. 4.\n4.3. Benchmark Baselines\nTo construct a comprehensive benchmark dataset for event-based sign language, we train and report the performance of the following sign language translation algorithms as the baseline for future works to compare: 1). Gloss-based SLT: Joint-SLT [3], Chen et al. [6], Sign-XmDA [40] and GASLT [41]; 2). Gloss-free SLT: TSPNet [18], GF-SLT [43]."}, {"title": "5. Experiment", "content": "5.1. Dataset and Evaluation Metric\nOur experiments are conducted on four event-based sign language translation datasets, including the newly proposed Event-CSL, and public EvSign [42], PHOENIX-2014T-Event [2], CSL-Daily-Event [44] datasets. We will introduce these datasets below:\n\u2022 EvSign Dataset is an event-based sign language dataset drawn from daily life, such as social interaction, education, shopping, traveling, and healthcare. This dataset is collected by multiple sign language volunteers and the annotations are extracted and reorganized from the National Sign"}, {"title": "5.3. Comparison on Public SLT Datasets", "content": "\u2022 Results on Event-CSL Dataset. As shown in Table 2, the comparison between our method and multiple state-of-the-art methods of sign language translation on the Event-CSL dataset is provided. Our approach achieves better performance, with the scores of ROUGE-L and BLEU-4 being 68.02 and 48.70 respectively. In previous works, some gloss-based methods such as Chen et al. [6], Sign-XmDA [40], GASLT [41], and Joint-SLT [3] obtain excellent results. Compared with the GASLT method, which performs better among them, our approach improves by +36.33 and +36.06 in ROUGE-L and BLEU-4 respectively. At the same time, some methods have been chosen to translate sign language videos into sentences without gloss. GFSLT shows excellent performance among the Gloss-free methods, which include GFSLT [43] and TSPNet [18]. Our method is compared with GFSLT, which has no pre-training stage, and we achieve improvements of +0.79 and +0.50 in ROUGE-L and BLEU4 respectively. These experimental results demonstrate the effectiveness of our method on the Event-CSL dataset.\n\u2022 Results on EvSign Dataset. As shown in Table 3, a comparison is made between our method and the previous methods [3, 15, 26, 42] for sign language translation. Among these gloss-based methods, Zhang et al. [42] achieve excellent performance. To demonstrate the effectiveness of our approach, our method is compared with that of Zhang et al. Our method improves by +18.89 and +17.04 in ROUGE-L and BLEU-4 respectively without any gloss annotations. These experimental results demonstrate the significant effectiveness of our method on the EvSign dataset.\n\u2022 Results on PHOENIX-2014T-Event Dataset. These"}, {"title": "5.4. Ablation Study", "content": "\u2022 Component Analysis on Event-CSL. In order to demonstrate the validity of our proposed method, we separately assess the impact of each module on the experimental results. As shown in Table 2, we conduct the component analysis on the Event-CSL dataset. We observe that when only using CNN as the visual backbone network, it shows excellent"}, {"title": "5.5. Efficiency Analysis", "content": "We analyze the number of parameters, GPU memory, and speed of the model on the Event-CSL dataset. Specifically, Param represents the trainable parameters of the model and Speed refers to the time it takes for the model to extract a single video with 100 frames. We replace the visual backbone network in our method with Resnet18, ViT, and Swin-Transformer, respectively. As shown in Ta-"}, {"title": "5.6. Visualization", "content": "In this section, we report the qualitative results of the sign language translations generated by our model. As shown in Figure 7, we compare the generated sentences with the reference sentences and display the corresponding event sign language videos, where the red areas represent the differences between the generated sentences and the reference sentences. For the first row, we observe that the sentence generated by our model is exactly the same as the reference sentence. For the second row, we find that the generated sentence has one more dynamic auxiliary word compared to the reference sentence. For the third and fourth rows, it is not difficult to see that the model understands the content of the sign language videos and makes synonyms replacements for some conjunctions and verbs when generating the sentences, but the meaning remains the same. For the last row, our model extracts the information of nouns but expands and modifies the nouns. We believe that generating completely consistent nouns is also very challenging. Overall, the quality of the sign language translations generated by our model is reliable. Although most of the generated sentences are not exactly the same as the reference"}, {"title": "5.7. Limitation Analysis", "content": "Although our newly proposed sign language translation framework works well on multiple benchmark datasets, however, we believe this model can still be improved from the following aspects: 1). We transform the event streams into event frames and resize them into 224 \u00d7 224, despite it achieving good results, however, the high definition event-based SLT is still not fully exploited. It will be an interesting research problem to design new architectures to mine the great potential. 2). We adopt a relatively lightweight large language model (LLM) for sign language generation due to the limited computational resources. We believe a larger language model can bring us a much better performance. We leave these two issues as our future works."}, {"title": "6. Conclusion", "content": "In this study, we have introduced a novel approach to Sign Language Translation (SLT) using high-definition Event streams, addressing critical limitations of traditional SLT systems such as sensitivity to lighting conditions, rapid hand movements, and privacy concerns. The proposed Event-CSL dataset, comprising 14,827 videos and cover-"}]}