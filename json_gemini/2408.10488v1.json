{"title": "Event Stream based Sign Language Translation: A High-Definition Benchmark Dataset and A New Algorithm", "authors": ["Xiao Wang", "Yao Rong", "Fuling Wang", "Jianing Li", "Lin Zhu", "Bo Jiang", "Yaowei Wang"], "abstract": "Sign Language Translation (SLT) is a core task in the field of AI-assisted disability. Unlike traditional SLT based on visible light videos, which is easily affected by factors such as lighting, rapid hand movements, and privacy breaches, this paper proposes the use of high-definition Event streams for SLT, effectively mitigating the aforementioned issues. This is primarily because Event streams have a high dynamic range and dense temporal signals, which can withstand low illumination and motion blur well. Additionally, due to their sparsity in space, they effectively protect the privacy of the target person. More specifically, we propose a new high-resolution Event stream sign language dataset, termed Event-CSL, which effectively fills the data gap in this area of research. It contains 14,827 videos, 14,821 glosses, and 2,544 Chinese words in the text vocabulary. These samples are collected in a variety of indoor and outdoor scenes, encompassing multiple angles, light intensities, and camera movements. We have benchmarked existing mainstream SLT works to enable fair comparison for future efforts. Based on this dataset and several other large-scale datasets, we propose a novel baseline method that fully leverages the Mamba model's ability to integrate temporal information of CNN features, resulting in improved sign language translation outcomes. Both the benchmark dataset and source code will be released.", "sections": [{"title": "1. Introduction", "content": "With the rapid development of deep learning, AI (Artificial Intelligence) for good has received widespread attention, among which, Sign Language Translation (SLT) [40, 41, 43] is increasingly being emphasized. It aids communication between deaf individuals and non-sign language users by converting sign language videos into natural language text, significantly enhancing the social participation and quality of life for the deaf community. However, the performance of sign language translation is still limited due to the usage of RGB cameras, as it is easily influenced by limited frame rate, illumination, motion blur, etc. In addition, the deployment of RGB-based sign language recognition models may also pose potential ethical risks as it involves issues of human privacy.\nRecently, biologically inspired Event cameras (e.g., DVS346, Prophesee, CeleX) have drawn more and more attention due to their unique features in high dynamic range, low energy cost, sparse spatial but dense temporal resolution, and low latency, etc [9]. For the imaging mechanism, each pixel within an Event camera operates independently and asynchronously, a distinct departure from RGB cameras that produce synchronized frame outputs. A binary pulse signal is recorded only when the change in brightness for each pixel exceeds a specific threshold. Some computer vision tasks have involved the Event camera for event-based or event-enhanced learning, such as object detection [10] tracking [34, 35], action recognition [36, 37], and also the sign language translation [42] discussed in this paper.\nAlthough limited, there are already some research efforts that have begun to focus on event-based sign language translation, as shown in Fig. 1. Early models are typically developed based on simulated event data from existing RGB datasets, such as PHOENIX-2014 [17] and CSL-Daily [44]. Obviously, the simulated data can't fully reflect the key features of the real event stream. Recently, three real event datasets have been proposed, including SL-Animal-DVS [31], EV-ASL [38], EvSign [42]. However, the first two datasets have limited spatial resolution (128 \u00d7 128), and the EvSign [42] crops the head regions of the actor for privacy-preserving. We believe this approach may corrupt the raw information of the event stream, significantly impacting the recognition and analysis of gestures involving the head area.\nIn this paper, we propose a new sign language translation dataset, termed Event-CSL, which is the largest, high-definition (1280 \u00d7 720) real-event dataset collected using the Prophesee EVK4-HD Event camera. It contains 14,827 videos, 14821 glosses, and 2,544 Chinese words in the text vocabulary. Our dataset is collected in a variety of indoor and outdoor scenes, encompassing multiple angles, light intensities, and camera movements. More importantly, we provide the most primitive event stream data, which lays the groundwork for accurately recognizing the meanings conveyed by gestures. In our experiments, we split these videos into training, validation, and testing subsets, which contain 12,602, 741, and 1,484 samples, respectively. To build a more comprehensive benchmark, we train and report 6 recently released SLT models for future works to compare. More details can be found in Section 4.\nOn the basis of our newly proposed Event-CSL dataset, we also propose a new event-based sign language translation framework, as shown in Fig. 2. The key insight of our framework is that Convolutional Neural Networks (CNNs) effectively capture local features and have consistently been the dominant visual backbone networks for sign language translation tasks. Despite the impact of Transformer models [32], their lower complexity and higher precision have continued to be favored. Therefore, this paper employs a residual network, ResNet18 [14], to encode the input event streams. To further enhance its global modeling capabilities, we introduce Mamba layers [11] to fuse CNN feature maps along the temporal views, thereby strengthening its perception. This hybrid CNN-Mamba architecture can even achieve better sign language translation results than dense Transformer models. We input the event visual features into the decoder to translate and obtain the final Chinese descriptions. Extensive experiments conducted on multiple existing SLT datasets and our newly proposed Event-CSL all validated the effectiveness of our framework.\nTo sum up, the main contributions of this paper can be summarized as the following three aspects:\n1). We propose a new large-scale, high-definition event-based sign language translation dataset, termed Event-CSL. It contains 14,827 videos and 2,544 Chinese words recorded in both indoor and outdoor scenarios.\n2). We propose a hybrid CNN-Mamba vision encoder for our sign language translation framework. The integration of CNN and Mamba layers makes our vision backbone well capture local and long-range global relations and retain low computational costs.\n3). Extensive experiments conducted on four event-based SLT datasets fully validated the effectiveness of our framework. In addition, we have evaluated multiple representative and state-of-the-art (SOTA) SLT algorithms on our newly proposed Event-CSL dataset and also two simulation datasets (PHOENIX-2014T-Event [2], CSL-Daily-Event [44]) hoping that these benchmarks can contribute to the advancement of the SLT field."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Sign Language Translation", "content": "In recent years, sign language translation has been widely concerned in the field of computer vision. Sign Language translation aim to translate sign language videos into spoken text to help deaf people communicate. Camg\u00f6z et al. [2] introduce the Sign Language Translation (SLT) problem. In the task of SLT, the word order and grammatical differences between sign language and spoken language are taken into account to convert sign language videos into spoken language translations. Camg\u00f6z et al. [3] propose an architecture based on Transformer, which can simultaneously learn sign language continuous recognition and translation end-to-end, without any ground-truth timing information, and improve performance. GloFE [21] extracts common concepts from text and presents them as indirect representations of weak forms, then uses global embeddings of these concepts as queries for cross-attention lookups to find the corresponding information in the learned visual features. Zhou et al. [43] propose a two-stage approach, which first combines contrast language image pre-training (CLIP) with masking self-supervised learning to create a pre-task to bridge the semantic gap between visual and text representation and restore masking sentences. Next, build an end-to-end architecture that inherits the parameters of the pre-trained visual encoder and decoder. SignNet II [5] is a Transformer-based two-way sign language translation model that enhances text-to-sign translation performance by jointly training sign-to-text and text-to-sign networks. GASLT [41] is a sign language translation method that does not rely on part-of-speech tagging and improves comprehension at the sentence level of sign language video by introducing knowledge transfer of natural language models. Yao et al. [39] continuously refine prototypes across attention mechanisms, iteratively optimize the semantic representation of sign language videos, imitate human reading behaviors, and finally generate fluent and accurate sign language translation. Different from these works, we use the Event camera's sensitivity to moving objects to achieve more reliable sign language translation in challenging situations such as fast movement, indoor and outdoor and low illumination."}, {"title": "2.2. State Space Model", "content": "The State Space Model is a mathematical model that describes the behavior of a dynamic system using a set of first-order differential equations (continuous-time systems) or difference equations (discrete-time systems) to represent the evolution of the internal state of the system, and another set of equations to describe the relationship between the state and the output of the system. By parameterizing the linear state-space model of each time series with a joint learning recurrent neural network, Rangapuram et al. [29] maintain the data efficiency and interpretability of the state-space model, utilizing the ability of deep learning methods to learn complex patterns from raw data. Gu et al. [12] propose the Structured State Space (S4) sequence model based on a new parameterization for the SSM, stably diagonal-ize the state matrix A by low-rank correction, and simplify the SSM calculation to Cauchy kernel calculation. Gu et al. [13] propose a sequence model called Linear State Space Layer (LSSL), which combines the advantages of recurrent neural networks (RNNs), temporal convolution, and neural differential equations (NDEs) with powerful modeling capabilities and computational efficiency. Mamba [11] is a sequence modeling method, which solves the problem of low computational efficiency of traditional methods when dealing with long sequences. Mainly, it realizes the selective information propagation and forgetting of discrete modes by designing the parameters of SSM as input functions. VMamba [24] is a visual state-space model with linear complexity, while still retaining the advantages of global receptive field and dynamic weights. It compresses the hidden state with S6 so that each element in the sequence can interact with any previously scanned sample, introducing a Cross-Scan Module (CSM) to address directional sensitivity. Vim [45] is a bi-directional Mamba block universal vision backbone network. It treats image blocks as sequential data by marking position embeddings in the image sequence and compressing the visual representation using bidirectional state space models. Dao et al. [7] reveal the connections between Transformers and state space models (SSMs) such as Mamba, which are connected through various decompositions of structurally semi-separable matrices. They propose the State Space Duality (SSD) framework and design a architecture (Mamba-2), whose core layer is a 2-8 times faster refinement of the selected SSM of Mamba. Inspired by these works, in this paper, we propose to augment the local CNN features using the SSM for the event-based sign language translation."}, {"title": "3. Methodology", "content": "In this section, we will first give an overview of our proposed event-based sign language translation framework. Then, we will introduce the details of input representation, network architecture, and loss function."}, {"title": "3.1. Overview", "content": "As shown in Fig. 2, given the event streams, we first stack into event frames and feed them into the residual network for feature extraction. We conduct average pooling and flatten operations on these CNN features to get the event visual tokens. Then, a Mamba block is adopted to capture the long-range temporal features. The forward and backward SSM are conducted to process the CNN features. The enhanced features will be fed into the temporal convolutional module for dimension reduction on the temporal view. Then, we feed the features into the sign embedding module and the Transformer decoder for accurate Chinese language generation."}, {"title": "3.2. Input Representation", "content": "Given event streams \\(E_{P}=\\{e_{1}, e_{2}, ..., e_{M}\\}\\), where M denotes the number of event points, each point \\(e_{ili} \\in (1, M)\\) is a quadruple \\(\\{x, y, t, p\\}\\), here the (x, y) denotes the spatial coordinates, t and p denotes the time stamp and polarity, respectively. To make full use of existing deep neural networks, in this paper, we first transform the event streams into event images \\(E_{T}=\\{F_{1}, F_{2}, ..., F_{T}\\}\\), where T denotes the number of stacked event frames, and each event frame is resized into a fixed resolution \\(F_{i} \\in R^{C\\times H\\times W}\\). Here, \\(\\{C, H, W\\}\\) denotes the dimension of the channel, height, and width of an event frame, respectively. In the training phase, we randomly sample a mini-batch that contains B event samples and the dimension of the mini-batch is \\(B \\times T \\times C \\times H \\times W\\)."}, {"title": "3.3. Network Architecture", "content": "\u2022 Hybrid CNN-Mamba Vision Backbone Network. In this paper, we propose a hybrid CNN-Mamba backbone network as the vision encoder to extract the deep features of the input event frames. To be specific, we first adopt the ResNet-18[14] which is pre-trained on the ImageNet classification dataset to extract the features of each frame. For each convolutional block \\(i \\in \\{1,2,3,4\\}\\), we get feature maps \\(F_{i} \\in R^{B_{T}\\times C\\times H\\times W}\\) whose dimension is \\(H_{4}\\times W_{H4} = \\times 5\\) and \\(5 \\times 15\\). Then, an average pooling layer is adopted on the CNN feature maps for feature aggregation. We flatten the obtained features into a sequence of visual tokens \\(F_{s} \\in R^{B\\times T\\times N}\\).\nAs we all know, CNN captures the local features well using the convolutional filter, but the long-range relations are not fully exploited. Current mainstream algorithms are developed based on the vision Transformer (e.g., ViT [32]) to mine the long-range relations of spatio-temporal context information, but its computational complexity is quadratic (O(N2)). Inspired by the success of the State Space Model [11-13, 24, 45] in the natural language processing and computer vision community, due to the collaborative optimization of software and hardware, the computational complexity of the Mamba is linear, and the GPU memory usage during training and inference is significantly reduced. In this paper, we propose to learn the long-range spatio-temporal context information using the Mamba layer [45]. Thus, the sequential vision tokens \\(F_{s}\\) are fed into the Mamba block, which is first normalized through a normalization layer, and then, projected into features x and z using two linear layers, i.e.,\n\\[x = Linear(Norm(F_{s})), z = Linear(Norm(F_{s}))\\]\nFollowing [45], we use both forward and backward scanning methods to process x to to better capture the context information in the data and avoid the problem of single and sensitive direction. For each direction, we obtain \\(x'\\) using the ID convolution layer and SiLU activation function:\n\\[x' = SiLU(Conv(x)).\\]\nAfter that, the output \\(x'\\) is projected to yield the input matrix \\(B \\in R^{N\\times T}\\), output matrix \\(C \\in R^{T\\times N}\\), and also the timescale parameter \\(\\Delta\\) :\n\\[B, C, \\Delta = Linear(x')\\]\nHere, the timescale parameter A is used for discretization, as the raw SSM is designed for a continuous system, but the data we process is discretized visual tokens. The zero-order hold rule is adopted to discretize the continuous state space equation of SSM:\n\\[h' = Ah + Bx', y = Ch'.\\]\nwhere h and h' represent the discrete hidden state, \\(x'\\) and y are the discrete inputs and outputs. A, B and C are discrete parameters of the system, and the process of discretization through time scale parameter A can be expressed as:\n\\[\\hat{A} = exp (\\Delta A),\\]\n\\[\\hat{B} = (\\Delta A)^{-1} (exp (\\Delta A) \u2013 I) \u00b7 \\Delta B,\\]\n\\[\\hat{C} = C\\]\nwhere \\(A \\in R^{N\\times N}\\) is the state matrix. The forward and backward SSM braches are applied for bidirectional context awareness to produce \\(Y_{for}, Y_{back}\\). After that, \\(Y_{for}, Y_{back}\\) are gated by z and the result of their addiction is linearly projected to \\(y'\\), which can be written as:\n\\[y' = Linear (Y_{for} SiLU(z) + Y_{back} SiLU(z))\\]\nThus, we have obtained the output feature \\(y'\\) of the hybrid CNN-Mamba vision backbone which captures the local and global long-range features of event stream.\n\u2022 Sign Language Decoder. Once we get the vision features \\(y'\\) from the aforementioned hybrid CNN-Mamba backbone, we adopt a temporal convolutional module to reduce the dimension along the view of the number of input frames (i.e., T). As shown in Fig. 2, it consists of convolution layer, batch normalization (BN), ReLU, and max-pooling layers. The output will also be fed into a sign embedding module, which is consisted of a linear layer, BN, and ReLU layer. Finally, a language decoder is adapted to generate sign language sentences. In our practical implementation, we use the pre-trained parameters of mBART [23] to initialize the language model, which has been pre-trained on CC25 [23], a multilingual corpus covering 25 languages. It consists of 3 Transformer encoder layers and 3 Transformer decoder layers. Note that, we directly output the language based on given input event streams, without the help of gloss annotations, i.e., our model is a gloss-free sign language translation framework."}, {"title": "3.4. Loss Function", "content": "The language decoder sequentially generates the logits of sign language sentences \\(S = \\{p(< BOS >), p(w_{1}), p(w_{2}), ...,p(w_{U}),p(< EOS >)\\}\\). In sentence S, the first special word < BOS > marks the beginning of the sentence, the last special word < EOS > marks the end of the sentence, and p(w) represents the logits of the generated word. In this work, we adopt the widely used cross-entropy loss function to calculate the distance between the annotated sign language and the generated sentence, which can be formulated as:\n\\[L = -log \\prod_{u=1}^{U} P(S_{u}|w_{u}).\\]"}, {"title": "4. Event-CSL Dataset", "content": ""}, {"title": "4.1. Protocols", "content": "When collecting our sign language translation dataset Event-CSL, we follow the following protocols: 1). Diversity of views: We capture different perspectives of sign language, including slightly downward, horizontal, and slightly upward views. 2). Camera motions: The movement of the event camera significantly affects the number of events collected and the complexity of the background. Our dataset collection takes into account a variety of motion patterns, including fast movement, slow movement, and moderate movement. 3). Diversity of scenes: In order to conform to the daily use of sign language, we consider different usage scenarios when recording videos. Specifically, these scenarios comprise office environments, daily life, and outdoor scenes (such as streets, parking lots, open spaces). 4). Intensity of light: In environments with strong or weak light intensity (high and low exposure), event cameras can still record information about the movement of objects due to the presence of pixel changes. When we record videos, respectively from the following several light intensities: strong light, low light, and dim light. 5). Continuity of action: Keep the movement of sign language flowing and coherent when recording sign language, reducing useless information and redundant video frames. 6). Diversity of sign language content: The content of sign language videos should be rich and varied to be closer to the real application. We randomly extract captions from the COCO-CN [19] dataset, and the descriptions of these contents mainly include the following aspects: daily life scenes, outdoor sports, animals and plants, sports competitions, character introductions, and landscape descriptions."}, {"title": "4.2. Statistical Analysis", "content": "Our proposed dataset is the first large-scale high-resolution event-based sign language benchmark dataset. All the videos in our dataset are collected by Prophesee EVK4-HD Event camera which has a resolution of 1280 \u00d7 720. Our dataset collects a total of 14,827 sign language videos and contains 14,821 glosses as well as 2,544 Chinese words in the text vocabulary. We have divided the dataset into three subsets for training, validation and testing, with their quantities being 12,602, 741, and 1,484 respectively. Additionally, within the text sentences of the dataset, the minimum, maximum and average numbers of characters of the sentences are 8, 75 and 18 respectively. As shown in 1, the dataset proposed by us surpasses the existing event-based sign language datasets in terms of the number of videos, video resolution, gloss, and text vocabulary. The visualization of the word cloud is provided in Fig. 4."}, {"title": "4.3. Benchmark Baselines", "content": "To construct a comprehensive benchmark dataset for event-based sign language, we train and report the performance of the following sign language translation algorithms as the baseline for future works to compare: 1). Gloss-based SLT: Joint-SLT [3], Chen et al. [6], Sign-XmDA [40] and GASLT [41]; 2). Gloss-free SLT: TSPNet [18], GF-SLT [43]."}, {"title": "5. Experiment", "content": ""}, {"title": "5.1. Dataset and Evaluation Metric", "content": "Our experiments are conducted on four event-based sign language translation datasets, including the newly proposed Event-CSL, and public EvSign [42], PHOENIX-2014T-Event [2], CSL-Daily-Event [44] datasets. We will introduce these datasets below:\n\u2022 EvSign Dataset is an event-based sign language dataset drawn from daily life, such as social interaction, education, shopping, traveling, and healthcare. This dataset is collected by multiple sign language volunteers and the annotations are extracted and reorganized from the National Sign Language Dictionary of China and CSL-Daily to form an oral sentence. The training set, validation set, and testing set of the EvSign dataset contain 5570, 553, and 650 sign language videos respectively. The EvSign dataset comprises 1,387 Chinese glosses and a Chinese vocabulary of 1,947 words.\n\u2022 PHOENIX-2014T-Event Dataset is derived from weather broadcasts in German Sign Language. The original videos of PHOENIX-2014T dataset are captured by traditional cameras, and its video frames are RGB images. We obtain its event representation through the DVS-Voltmeter [22], which is a stochastic process-based event simulator for dynamic vision sensors, as shown in Fig. 5 (a). We refer to the event representation of PHOENIX-2014T as PHOENIX-2014T-Event, and it remains consistent with the dataset configuration of PHOENIX-2014T in terms of the number of videos and German annotations. The training set, validation set, and testing set of PHOENIX-2014T-Event dataset contain 7096, 519, and 642 sign language videos respectively. PHOENIX-2014T-Event dataset comprises 1,066 German glosses and a German vocabulary of 2,887 words.\n\u2022 CSL-Daily-Event Dataset [44] is derived from daily life scenarios, covering multiple themes such as family life, healthcare, and school life. We also obtain its event representation through the DVS-Voltmeter [22], as shown in Fig. 5 (b). We refer to the event representation of CSL-Daily as CSL-Daily-Event, and it remains consistent with the dataset configuration of CSL-Daily in terms of the number of videos and Chinese annotations. The training set, validation set, and testing set of CSL-Daily-Event dataset contain 18401, 1077, and 1176 sign language videos respectively. The CSL-Daily-Event dataset comprises 2,000 Chinese glosses and a Chinese vocabulary of 2,343 words.\nWe adopt BLEU [27] and ROUGE-L [20] to assess the performance of sign language translation. BLEU mainly scores by comparing the matching degree of n-grams in the candidate translation with multiple reference translations. The higher the score, the greater the lexical similarity between the candidate translation and the reference translations. ROUGE-L calculates the recall rate based on the Longest Common Subsequence. It focuses more on evaluating the similarity in content coverage between the generated translation and the reference translations. For both of these metrics, a higher value indicates better performance."}, {"title": "5.2. Implementation Details", "content": "To reduce the time for image loading and transformation, we make the following settings. When the training input are (224, 224) and (256, 256), we first resize the original images with a resolution of 1280 \u00d7 720 and save them as images with a resolution of 256 \u00d7 256. Then our model loads the 256 \u00d7 256 resolution images as the input. When the training input is (320, 320), our model directly loads the original 1280 \u00d7 720 resolution images. During the training process, we use the SGD [1] optimizer with an initial learning rate of 0.01, and simultaneously use the cosine annealing scheduler to adjust the learning rate. Our code is implemented using PyTorch [28] based on Python. All the experiments are conducted on a server with A800 GPUs. More details can be found in our source code."}, {"title": "5.3. Comparison on Public SLT Datasets", "content": "\u2022 Results on Event-CSL Dataset. As shown in Table 2, the comparison between our method and multiple state-of-the-art methods of sign language translation on the Event-CSL dataset is provided. Our approach achieves better performance, with the scores of ROUGE-L and BLEU-4 being 68.02 and 48.70 respectively. In previous works, some gloss-based methods such as Chen et al. [6], Sign-XmDA [40], GASLT [41], and Joint-SLT [3] obtain excellent results. Compared with the GASLT method, which performs better among them, our approach improves by +36.33 and +36.06 in ROUGE-L and BLEU-4 respectively. At the same time, some methods have been chosen to translate sign language videos into sentences without gloss. GFSLT shows excellent performance among the Gloss-free methods, which include GFSLT [43] and TSPNet [18]. Our method is compared with GFSLT, which has no pre-training stage, and we achieve improvements of +0.79 and +0.50 in ROUGE-L and BLEU4 respectively. These experimental results demonstrate the effectiveness of our method on the Event-CSL dataset.\n\u2022 Results on EvSign Dataset. As shown in Table 3, a comparison is made between our method and the previous methods [3, 15, 26, 42] for sign language translation. Among these gloss-based methods, Zhang et al. [42] achieve excellent performance. To demonstrate the effectiveness of our approach, our method is compared with that of Zhang et al. Our method improves by +18.89 and +17.04 in ROUGE-L and BLEU-4 respectively without any gloss annotations. These experimental results demonstrate the significant effectiveness of our method on the EvSign dataset.\n\u2022 Results on PHOENIX-2014T-Event Dataset. These methods such as TSPNet [18], Joint-SLT [3], Sign-XmDA [40], GASLT [41] and GFSLT [43] exhibit excellent performance on the PHOENIX-2014T dataset. We make comparisons with them on the PHOENIX-2014T-Event dataset, as shown in Table 4. Among the gloss-based methods, like Joint-SLT, Sign-XmDA, and GASLT, GASLT performs more prominently. Our method exceeds it by +12.61 and +7.71 respectively in ROUGE-L and BLEU-4 even without the guidance of any sign language gloss. GFSLT achieves better results than the gloss-free methods. Compared to it, our method improves by +0.87 and +0.08 in the ROUGE-L and BLEU-4 respectively.\n\u2022 Results on CSL-Daily-Event Dataset. CSL-Daily-Event is an event simulation dataset, which originates from the CSL-Daily sign language dataset. As shown in Table 5, we report multiple SOTA methods of SLT on the CSL-Daily-Event dataset. For the gloss-based methods, our method improves by +9.64 and +4.88 respectively on ROUGE-L and BLEU-4 compared to GASLT. For the gloss-free methods, our method improves by +2.27 and +0.92 respectively on ROUGE-L and BLEU-4 compared to GFSLT. The results of these experiments demonstrate that our method can also achieve good performance on the event simulation dataset."}, {"title": "5.4. Ablation Study", "content": "\u2022 Component Analysis on Event-CSL. In order to demonstrate the validity of our proposed method, we separately assess the impact of each module on the experimental results. As shown in Table 2, we conduct the component analysis on the Event-CSL dataset. We observe that when only using CNN as the visual backbone network, it shows excellent performance, indicating that it can capture the local spatial details of the video frames. Then, we introduce Mamba and construct a hybrid CNN-Mamba as the visual backbone network. Compared with CNN, the hybrid CNN-Mamba brings performance improvements in each metric, among which ROUGE-L, BLEU-1, BLEU-2, BLEU-3, BLEU-4 increase by +0.30, +0.50, +0.41, +0.20 and +0.02 respectively. These experimental results prove that Mamba effectively models the long-range global temporal relations.\n\u2022 Analysis of Number of Input Event Frames. Different numbers of input frames for sign language videos influence the performance of the model and its resource occupation. We set five quantities of video frames, which are L/16, L/8, L/4, L/2, and L, where L represents the total number of frames in each sign language video. As shown in Table 6, the performances of these five video frame numbers are compared on the Event-CSL dataset. We can observe that as the number of frames increases, the performance of the model also keeps improving. When all the video frames are used, the performance is several times higher than when only L/16 is used. The experimental results indicate that our method is more sensitive in capturing the long-range global relations between video frames. In our model, the number of input event Frames has a crucial impact on the performance, more video frames means being able to better guarantee the quality of sign language translation.\n\u2022 Analysis of Different Resolutions of Event Stream. As shown in the Table 7, we set three input resolutions of event stream for experiments, which are 224 \u00d7 224, 256 \u00d7 256 and 320 \u00d7 320. Compared with the resolution of 224 \u00d7 224, when the input resolution is 256 \u00d7 256, ROUGE-L remains unchanged while BLEU-1, BLEU-2, BLEU-3, and BLEU-4 change by -0.08, -0.05, +0.04 and +0.14 respectively. When we further increase the resolution to 320 \u00d7 320, ROUGE-L increases by +0.56 while BLEU-1, BLEU-2, BLEU-3 and BLEU-4 increase by -0.14, -0.04, +0.13 and +0.24 respectively. Through the experiments of these three resolutions, we observe that as the resolution increases, the performance can only be improved on part of the evaluation metrics.\n\u2022 Analysis of Aggregation Methods between CNN and Mamba. CNN can effectively capture local features while Mamba models long-range global relations. In order to enable the visual backbone network to extract the features of sign language better, the aggregation method between CNN and Mamba is crucial. As shown in Figure 6, we explore four different fusion methods, which are concatenate, add, multiply, and series aggregation. We conduct experiments on these four aggregation methods respectively on the Event-CSL dataset, and the experimental results are shown in Table 8. We can observe that the best performance can be attained when the interaction between CNN and Mamba is in the form of series aggregation. We believe that using series aggregation holds more advantages for capturing the long-range global relations among local features."}, {"title": "5.5. Efficiency Analysis", "content": "We analyze the number of parameters, GPU memory, and speed of the model on the Event-CSL dataset. Specifically, Param represents the trainable parameters of the model and Speed refers to the time it takes for the model to extract a single video with 100 frames. We replace the visual backbone network in our method with Resnet18, ViT, and Swin-Transformer, respectively. As shown in Ta-"}, {"title": "5.6. Visualization", "content": "In this section, we report the qualitative results of the sign language translations generated by our model. As shown in Figure 7, we compare the generated sentences with the reference sentences and display the corresponding event sign language videos, where the red areas represent the differences between the generated sentences and the reference sentences. For the first row, we observe that the sentence generated by our model is exactly the same as the reference sentence. For the second row, we find that the generated sentence has one more dynamic auxiliary word compared to the reference sentence. For the third and fourth rows, it is not difficult to see that the model understands the content of the sign language videos and makes synonyms replacements for some conjunctions and verbs when generating the sentences, but the meaning remains the same. For the last row, our model extracts the information of nouns but expands and modifies the nouns. We believe that generating completely consistent nouns is also very challenging. Overall, the quality of the sign language translations generated by our model is reliable. Although most of the generated sentences are not exactly the same as the sentences, the wording is similar and the overall meaning expressed is basically the same."}, {"title": "5.7. Limitation Analysis", "content": "Although our newly proposed"}]}