{"title": "LHQ-SVC: Lightweight and High Quality Singing Voice Conversion Modeling", "authors": ["Yubo Huang", "Xin Lai", "Muyang Ye", "Anran Zhu", "Zixi Wang", "Jingzehua Xu", "Shuai Zhang", "Zhiyuan Zhou", "Weijie Ni"], "abstract": "Singing Voice Conversion (SVC) has emerged as a significant subfield of Voice Conversion (VC), enabling the transformation of one singer's voice into another while preserving musical elements such as melody, rhythm, and timbre. Traditional SVC methods have limitations in terms of audio quality, data requirements, and computational complexity. In this paper, we propose LHQ-SVC, a lightweight, CPU-compatible model based on the SVC framework and diffusion model, designed to reduce model size and computational demand without sacrificing performance. We incorporate features to improve inference quality, and optimize for CPU execution by using performance tuning tools and parallel computing frameworks. Our experiments demonstrate that LHQ-SVC maintains competitive performance, with significant improvements in processing speed and efficiency across different devices. The results suggest that LHQ-SVC can meet real-time performance requirements, even in resource-constrained environments.", "sections": [{"title": "I. INTRODUCTION", "content": "Singing Voice Conversion (SVC) has emerged as a promis-ing application in the music domain, offering new possibilities for music production, cover songs, and personalized voice customization. SVC involves not only the conversion of speech characteristics but also musical elements such as melody and rhythm, making it more challenging than traditional VC tasks. Traditional SVC methods have largely relied on generative models. Nose et al. [1] proposed an HMM-based singing synthesis technique that allows users to intuitively and contin-uously control the singing style and intensity of synthesized singing voices. Kobayashi et al. [11] introduced a statisti-cal conversion process based on GMM to estimate spectral features directly, addressing the degradation in voice quality during conversion. Sisman et al. [12] were the first to attempt using GANs for SVC and proposed a framework named SIN-GAN, which achieved high-quality singing voice conversion without relying on automatic speech recognition systems.\nDespite their advancements, these models face several limitations, including high data requirements, loss of audio quality, and the complexity of model training. In comparison to traditional statistical methods, deep learning has demonstrated superior performance by learning complex nonlinear mappings from large datasets, significantly improving the quality of converted voices and speaker similarity [2]. Sun et al. [3] proposed a sequence conversion method based on DBLSTM-RNNs, utilizing LSTM to improve the naturalness and continuity of speech output in VC tasks through parallel training data. Xie et al. [4] introduced a DNN-based non-parallel training approach to VC, achieving effective conversion without parallel data.\nRecently, diffusion models have emerged as a powerful generative technique in image and audio synthesis. These models learn data distribution by gradually introducing and removing noise, capturing subtle features in audio signals, making them a promising alternative for SVC tasks. Chen et al. [5] introduced a method named LCM-SVC, which accelerates inference in latent diffusion models (LDM) for SVC through Latent Consistency Distillation (LCD), while retaining high sound quality and timbre similarity. Compared to traditional generative models, diffusion models better preserve the origi-nal sound details and musical characteristics. However, most existing diffusion-based SVC models employ a black-box structure, lacking intuitive interpretability and control over the generation process. This presents new challenges in optimizing and improving these models.\nTo address these challenges, this paper proposes a novel approach called The innovation of this study lies in the development of LHQ-SVC, a lightweight and CPU-optimized Singing Voice Conversion model. Unlike previous models that rely on GPU-heavy computations, LHQ-SVC is designed to run efficiently on CPUs while maintaining high-quality audio conversion. Key innovations include:\n\u2022 Model Size Reduction: LHQ-SVC significantly reduces the model size without compromising performance, making it suitable for deployment on devices with limited computational resources.\n\u2022 CPU Optimization: By converting GPU-centric code to CPU-compatible operations and leveraging optimization libraries such as Intel MKL and OpenMP, LHQ-SVC achieves efficient parallel processing on multi-core CPUs.\n\u2022 Improved Sampling Mechanism: We optimize the diffusion process by reducing the number of steps required in the conversion, enabling faster inference while maintain-"}, {"title": "II. RELATED WORK", "content": "Voice Conversion (VC) is a technique that modifies the voice of a source speaker to sound like that of a target speaker while preserving the linguistic content. In recent years, VC has seen remarkable advancements with applications span-ning across areas such as speaker identity conversion, speech enhancement, and cross-lingual voice translation [2]. As a specialized subset of VC, Singing Voice Conversion (SVC) focuses on transforming one singer's voice into another while retaining the musical elements such as pitch, rhythm, and tim-bre. This presents additional challenges beyond traditional VC tasks, as SVC must handle not only the vocal characteristics but also the melodic and rhythmic aspects of singing. Recent developments in deep learning have brought new solutions to the SVC field. Early methods predominantly relied on statistical models like Hidden Markov Models (HMMs) and Gaussian Mixture Models (GMMs). However, these models often struggled with issues related to the loss of audio quality and the need for parallel data, limiting their generalization capabilities [11]. The shift towards neural network-based approaches has significantly improved both the quality and flexibility of voice and singing conversions.\nWhile significant progress has been made in SVC through the introduction of deep learning methods such as GANs, diffusion models, and zero-shot techniques, challenges remain. Key issues include improving inference stability, reducing computational demands, and achieving better control over individual vocal timbres."}, {"title": "B. Diffusion Model Applications", "content": "Diffusion models, a class of generative models, operate by first adding noise to data in the forward process, and then reconstructing the data structure during the reverse process to generate samples [6]. These models have recently been introduced in the field of Singing Voice Conversion (SVC) to address the limitations of traditional methods when dealing with non-parallel data [7]. The core principle of diffusion models is to decompose the complex task of generating high-dimensional data into multiple simpler steps by progressively adding noise and learning the denoising process. Further advancing controllability in SVC, Wang et al. [13] introduced Prompt-Singer, a method that allows users to control singer attributes such as gender, vocal range, and volume through natural language prompts. Although this method improves style control, the model's large size limits its scalability. Chen et al. [10] developed LDM-SVC, a zero-shot any-to-any SVC method based on latent diffusion models. LDM-SVC achieves faster training speed, but its inference quality is unstable.\nOver and above these work, Lu et al. [14] proposed Co-MoSVC, a consistency model-based SVC method designed to balance high-quality generation with fast sampling. The use of diffusion models in SVC has demonstrated great potential for generating high-quality audio and enabling fast inference, par-ticularly in scenarios involving non-parallel data and complex timbre control. However, key challenges remain, including improving model efficiency, stability, and adaptability for real-world applications."}, {"title": "III. METHODOLOGY", "content": "We propose a new model, LHQ-SVC, designed to signifi-cantly improve performance and reduce the model size while maintaining high accuracy, so that it can efficiently run on CPUs. The LHQ-SVC model focuses on optimising CPU execution, in contrast to the traditionally GPU-optimised code which relies heavily on parallel computation.\nWhile CPUs have fewer cores compared to GPUs, their multi-core nature still allows for parallelism, albeit to a lesser extent. Using parallel frameworks like OpenMP or Intel Threading Building Blocks (TBB) can enable efficient multi-threaded programming, maximizing the CPU's multi-core ca-pabilities. CPU performance tuning tools like Intel VTune Profiler can be used to identify performance bottlenecks, particularly in hotspot areas of code, such as I/O operations, memory access patterns, and cache management. Optimising these areas contributes to improving computational efficiency.\nIn the training phase of LHQ-SVC, once robust training reaches a certain number of iterations, the network undergoes evaluation. Specifically, the performance metric, calculated as the ratio of the CLIP score to latency change before and after removing specific modules, is used to determine which parts of the model should be retained or removed. If the network's latency exceeds the target, modules that contribute to latency but not performance are removed. In contrast, modules with the highest performance contribution are duplicated to further strengthen the network's structure.\n$\\epsilon_{\\theta}(t, z_t) = \\{{p(\\text{Cross-Attention}(z_t, c), I), p(D(z_t, t), I)}\\}$\nThis dynamic tuning process allows continuous robust train-ing, leading to an optimised network structure without the need for retraining. The result is a well-balanced network with high performance and computational efficiency, suitable for deployment on CPU-based systems.\nMoreover, the time consumption of Mini-SVC can be further reduced by minimizing diffusion steps during the transition from the teacher model to the student model. By distilling the teacher model's multi-step output into a single-step output for the student model, as shown in Figure 1, we effectively streamline the transformation process.\nWe consider a continuous-time diffusion framework where the training data is represented as $x \\sim p(x)$. The diffusion model operates with latent states denoted by $\\tilde{y} = \\{y_{\\tau}\\}_{\\tau\\in[0,1]}$, governed by a noise schedule determined by a differentiable function $B_{\\tau}$. This function controls the signal-to-noise ratio (SNR), defined as $\\theta_{\\tau} = \\log(\\frac{\\bar{a}_\\tau}{a_\\tau})$, which monotonically decreases over time. The forward diffusion"}, {"title": "IV. EXPERIMENTS", "content": "The datasets used in this study include the SVC-2023 Dataset [18] and the DAMP (Data for Analysis of Musical Performances) Dataset [19]. The SVC-2023 Dataset, widely used for benchmarking singing voice conversion models, provides parallel and non-parallel singing data across multiple languages, while the DAMP dataset offers a vast collection of over 30,000 singing performances collected from amateur singers using the Smule social singing app.\nTo extract clean vocal tracks from the songs, we employed the Ultimate Vocal Remover project. This involved two main steps: first, we used MDX-Net [20] to remove the instrumental accompaniment from the audio files. Next, we applied the 6_HP-Karaoke-UVR model [21] to remove background harmonics from the vocal-separated files. The output was clean, isolated vocals that could be used for further processing and analysis.\nAfter obtaining clean vocal tracks, we sliced the audio into segments of less than 30 seconds in duration, manually discarding segments without vocals to ensure only those containing vocal content were retained. This step was automated using the open-source Audio Slicer tool [22], which detects silent audio segments and slices the audio based on pitch, ensuring that the resulting audio clips primarily contained vocal content.\nAll audio clips were resampled to a uniform rate of 40kHz and normalized. Data augmentation was applied to the audio clips by segmenting the dataset into clips of varying sizes, with partial overlaps between adjacent clips to increase diversity in the training set. This augmentation helps improve model robustness to different voice and singing patterns."}, {"title": "B. Training Process", "content": "Before training, all audio files were re-sampled to 40kHz and normalized. We then extracted features using DIO for fundamental frequency (F0) and the ContentVec model to extract content-related features from the 12th layer, including F0 curves and voicing flags. These features were projected into a 256-dimensional space and concatenated to form the conditioning input for the decoding phase. We used a pre-trained vocoder [23] fine-tuned for singing voice synthesis and calculated Mel-spectrograms with a 512-point Fast Fourier Transform (FFT), a 512-point window size, and a 128-point hop size, producing 80 frequency bins. To evaluate the performance of the proposed singing voice conversion model, we compare it against several state-of-the-art models"}, {"title": "C. Experimental Models and Results", "content": "The performance of LHQ-SVC, along with several baseline models, is summarized in Table I. The results for LHQ-SVC and LHQ-SVCmobile stand out due to their lightweight na-ture and performance efficiency, especially for mobile device applications. LHQ-SVC, with a model size of 67.89 MB, achieves impressive results across multiple metrics, including the highest PESQ score of 3.02, and a Naturalness MOS of 4.02, which is the best among all models compared. These re-sults indicate that LHQ-SVC excels in generating high-quality singing voice conversions while maintaining a relatively small model size. On the other hand, LHQ-SVCmobile is an even more compact version, with a model size of only 53 MB, optimized for mobile platforms. Although it shows slightly lower performance in terms of STOI and PESQ compared to LHQ-SVC, it still provides competitive results, especially in terms of Similarity MOS with a score of 3.91, making it a viable choice for lightweight applications requiring fast and efficient voice conversion on mobile devices. The balance between the performance and model size makes both LHQ-SVC and LHQ-SVCmobile particularly appealing for practical use, as they deliver strong results in Naturalness MOS and Similarity MOS while being resource-efficient.\nIn order to verify that our LHQ-SVC is efficient enough, we ran it on the Intel Core i7-11370H on the computer side as well as on the Qualcomm Snapdragon 8 Gen3 processor"}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduced LHQ-SVC, a novel lightweight Singing Voice Conversion model designed to run efficiently on CPUs while maintaining high-quality voice conversion performance. Through careful optimization, including the con-version of GPU-specific code and the use of parallel comput-ing frameworks, we reduced the computational demand and model size significantly. Experimental results demonstrated that LHQ-SVC achieves competitive performance in terms of intelligibility, perceptual speech quality, and naturalness, while also delivering substantial improvements in processing speed across different hardware platforms."}]}