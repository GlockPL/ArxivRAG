{"title": "A Light-Weight Framework for Open-Set Object Detection with\nDecoupled Feature Alignment in Joint Space", "authors": ["Yonghao He", "Hu Su", "Haiyong Yu", "Cong Yang", "Wei Sui", "Cong Wang", "Song Liu"], "abstract": "Open-set object detection (OSOD) is highly desirable for robotic manipulation in unstructured environments.\nHowever, existing OSOD methods often fail to meet the requirements of robotic applications due to their high computational\nburden and complex deployment. To address this issue, this\npaper proposes a light-weight framework called Decoupled\nOSOD (DOSOD), which is a practical and highly efficient\nsolution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by\nintegrating a vision-language model (VLM) with a detector. A\nMultilayer Perceptron (MLP) adaptor is developed to transform\ntext embeddings extracted by the VLM into a joint space, within\nwhich the detector learns the region representations of class-\nagnostic proposals. Cross-modality features are directly aligned\nin the joint space, avoiding the complex feature interactions and\nthereby improving computational efficiency. DOSOD operates\nlike a traditional closed-set detector during the testing phase,\neffectively bridging the gap between closed-set and open-\nset detection. Compared to the baseline YOLO-World, the\nproposed DOSOD significantly enhances real-time performance\nwhile maintaining comparable accuracy. The slight DOSOD-\nS model achieves a Fixed AP of 26.7%, compared to 26.2%\nfor YOLO-World-v1-S and 22.7% for YOLO-World-v2-S, using\nsimilar backbones on the LVIS minival dataset. Meanwhile,\nthe FPS of DOSOD-S is 57.1% higher than YOLO-World-v1-\nS and 29.6% higher than YOLO-World-v2-S. Meanwhile, we\ndemonstrate that the DOSOD model facilitates the deployment\nof edge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD.", "sections": [{"title": "I. INTRODUCTION", "content": "Object detection aims to predict the locations and category\nlabels of objects of interest, serving as a fundamental tech-\nnique for understanding images and scenes with numerous\napplications in robotics [1], [2] and autonomous vehicles [3].\nSignificant advances in this field have been driven by the ad-\nvent of convolutional neural networks (CNNs) [4]. Previous\nresearch focuses primarily on closed-set condition, aiming to\ndetect objects of only predefined categories. When faced with\nunknown objects, the detectors tend to either ignore them or\nmisclassify them as known categories. However, for robotic\nmanipulation in unstructured environments, it is essential to\ndetect both known and unknown objects [5]. Open-set object\ndetection (OSOD) offers a potential solution, which requires\nmodels to generalize from a base set to novel categories\nwithout specific annotations for new categories.\nDhamija et al. [7] formalized OSOD task and reported\nthat closed-set detectors experience significant performance\ndegradation in open-set conditions. Given the strong open-\nset image recognition capabilities of vision-language models\n(VLMs) pre-trained with contrastive methods [6], [8], con-\nsiderable efforts have been made to detect arbitrary classes\nby leveraging language generalization. Recent studies [9],\n[10], [11], [12], [13], [14] have explored extending the VLM\nmodel, Contrastive Language-Image Pre-training (CLIP) [6],\nto detect objects from novel categories. Yao et al. [15], [16]\nunified detection and image-text datasets through region-text\nmatching, training detectors with large-scale image-text data\npairs to achieve promising performance.\nHowever, these methods suffer from a high computational\nburden and complex deployment for the following reasons:\n(1) heavy backbones such as ATSS [18] or DINO [19]\nwith Swin-L [20] employed by the methods in [14], [15],\n[16]; (2) utilization of image encoder of VLM in [9], [10],\n[11], [12], [13] which increases inference cost in testing;\n(3) cross-modality feature interaction for alignment in\n[14], [38], as illustrated in Fig. 1. Additionally. the trans-\nformer architecture used in these methods adds additional\ncomplexity to deployment on edge devices. Therefore, the\nmethods still struggle for real-world applications. The re-\ncent work, YOLO-World [17], introduces a prompt-then-\ndetect strategy that first encodes a user's prompt to build\nan offline vocabulary and then performs inference on this\nvocabulary without re-encoding the prompts. By using a\nfully CNN detector, YOLOv8 [28], YOLO-World achieves\na 20x speed increase compared to previous state-of-the-art\n(SOTA) methods. However, YOLO-World still suffers from\ncomplex feature interactions for alignment, which hinders\nfurther efficiency improvements.\nInspired by the recent work, YOLO-World, we adopt a\nsimilar pipeline and take a further step in cross-modality\nfeature alignment. Actually, feature alignment is a key con-\ncern in current OSOD research. Fig. 1(a) depicts the teacher-\nstudent distillation approach, where features are extracted\nfrom the entire image [9], [10] or proposals [11], [12],\n[13] using the image encoder of VLM, which serves as the\nteacher. Meanwhile, the features extracted by the detector's\nbackbone act as the student. The student features are distilled\nfrom the teacher features by directly using L1 loss between\nthem or/and under the supervision of text embedding gener-\nated by the text encoder of VLM. Fig. 1(b) illustrates align-\nment through feature interaction, involving complex cross-"}, {"title": "II. RELATED WORK", "content": "Methods for closed-set object detection are generally cat-\negorized into three groups: region-based, pixel-based, and\nquery-based methods. Region-based methods are exemplified\nattention [14] or repeated fusion [15] operations between\nmulti-modal and multi-level features.\nThis paper proposes a simple yet efficient feature align-\nment strategy to faciliate real-time OSOD, as shown in Fig.\n1(c). A Multilayer Perceptron (MLP) adaptor is introduced\nto enhance the learning capacity of text embeddings while\ntransforming them into a latent space. Simultaneously, a\ndetector learns class-agnostic proposals and then the region\nrepresentations in the latent space. Cross-modality features\ncan then be aligned directly in the latent space in a decou-\npled manner, reducing both computational costs and storage\nrequirements. Inspired by concepts in robotics, we refer\nto this latent space as a \"joint space\", indicating that it\nlinks multi-modal features in a manner similar to how it\nconnects different robotic arms. A high-efficiency OSOD\nframework called Decoupled OSOD (DOSOD) is thus es-\ntablished. During the testing phase, DOSOD operates like\na traditional closed-set object detector, efficiently bridging\nthe gap between closed-set and open-set object detection.\nExtensive experiments on common benchmarks demonstrate\nthat, compared to the state-of-the-art baseline, YOLO-World,\nthe proposed DOSOD significantly improves real-time per-\nformance while maintaining comparable accuracy.\nThe contributions of the paper are summarized as follows:\n\u2022 A light-weight OSOD framework, decoupled OSOD\n(DOSOD), is proposed, exhibiting high efficiency for\nlow-cost applications. DOSOD bridges the gap between\nopen-set and closed-set detection, thus expanding appli-\ncation scenarios.\n\u2022 An MLP-based adaptor is introduced, with which fea-\nture alignment is conducted in a decoupled manner,\nimproving computational efficiency.\n\u2022 Extensive experiments are conducted to demonstrate\nthat the DOSOD framework significantly improves real-\ntime performance without compromising accuracy.\nA. Closed-Set Object Detection\nby the two-stage R-CNN series [21], [22] and one-stage\ndetectors [23]. These methods rely on a set of predefined\nanchors, refining and classifying them with non-maximum\nsuppression (NMS) as a post-processing step. Pixel-based\nmethods [24] predict a bounding box and its corresponding\ncategory for each pixel on multiple feature maps. Query-\nbased methods [25], [26] use a fixed, small set of learned\nobject queries to reason about object relationships and the\nglobal image context, directly outputting the final set of\npredictions. [25] was the first to employ the transformer\narchitecture in object detection, inspiring subsequent query-\nbased methods [26] that enhance queries with prior knowl-\nedge. Redmon et al. proposed the YOLO series [27], [28]\nto leverage simple architectures and learning strategies for\nimproved real-time performance. Closed-set object detection\nmethods are limited to handling a fixed set of categories\nand are not practical in unstructured environments where\nunknown objects may appear alongside known ones. This\npaper explores the OSOD task, which aims to detect objects\nbeyond predefined categories, thereby extending the applica-\ntion scenarios of object detection.\nB. Open-set Object Detection\nDhamija et al. [6] formalized the OSOD task which has\nrecently become a new trend in object detection. Current\nstudies focus on training detectors using existing bounding\nbox annotations and leveraging language generalization to\ndetect arbitrary categories. Vision and Language Knowledge\nDistillation (ViLD) [9], [10] distills the knowledge from a\npretrained VLM into a two-stage detector. Du et al. [11]\nproposed learning continuous prompt representations for\nOSOD, which is combined with ViLD to achieve superior\nperformance. Subsequently, feature alignment is enhanced\nthrough dense-level alignment [12] or by improving region\nrepresentation using a bag of regions [13]. However, these\nmethods incur high computational costs during the testing\nphase due to the use of VLM image encoders. [14], [17]\nused only the text encoder to replace the vanilla classifier\nwith fixed category text embeddings. [15], [16] directly\nlearned the fine-grained word-region alignment from massive\nimage-text pairs in an end-to-end manner. The detector was"}, {"title": "III. METHOD", "content": "trained with a hybrid supervision from detection, grounding\nand image-text pair data under a unified data formulation.\nWhile the methods [14], [15], [16] employ heavy backbones\nto enhance open-set detection capacity, YOLO-World [17]\nintroduced a prompt-then-detect pipeline with the YOLOv8\n[29] detector to achieve real-time OSOD. Inspired by YOLO-\nWorld, this paper proposes a new OSOD framework, Decou-\npled OSOD, which further improves real-time performance\nwithout compromising accuracy.\nC. Feature Alignment\nFeature alignment is a significant concern in current\nOSOD research. The methods in [9], [10], [11], [12], [13]\nadopted the teacher-student distillation approach. ViLD [9],\n[10] uses an L1 distillation loss to align region embeddings\nwith image embeddings. [12] proposed a dense-level align-\nment rather than object-level alignment, while [13] intro-\nduced the idea of aligning the embedding of a bag of regions\ninstead of individual regions. In the methods, entire image or\nimage patches are inputted into the image encoder of VLM\nand the backbone of the detector for feature extraction, which\nare then treated as the teacher and student, respectively. The\n'student' features are distilled from the 'teacher' features by\nusing the loss between them or with the supervision of the\ntext embedding. [14] employs the transformer architecture,\nand uses cross-attention strategy for cross-modality feature\nfusion. [17] incorporated the text embedding into the neck\npart of the detector for alignment. In [14], [17], repeated\nfeature fusion and complex cross-attention operations are\nconducted, which increases the computational cost. In this\npaper, we endow the learning capacity of text embedding and\ntransform it into the joint space, in which feature alignment\nis achieved without interaction. The proposed method further\nimproves the efficiency of OSOD task which facilitates low-cost applications in robotics.\nIn this section, a thorough analysis is conducted regarding\nthe relationship between closed-set and open-set detectors.\nOn the basis, the proposed DOSOD framework is detailed.\nA. Bridge the gap between closed-set and open-set detectors\nWe start with a brief overview of closed-set and open-\nset detection. Closed-set detection is based on a predefined\nset of fixed categories, each with corresponding annotations\nprovided during the training phase. In the testing phase,\nthe detector is tasked only with predicting these predefined\ncategories while ignoring any others. In contrast, open-\nset detection significantly broadens the range of detectable\ncategories by using texts as category labels. During training,\na large number of text-region pairs are used as training data.\nIn the testing phase, the model can accept any diverse text\ninput to detect new categories of objects that were never en-\ncountered during training. For closed-set detection, instance\nannotations are denoted as $A = \\{B_i, C_i\\}_{i=1}^N$, in which $B_i$\nare bounding boxes and $c_i$ are corresponding category labels\nin form of number indexes. Instance annotations of open-set\ndetection are formulated as $A = \\{B_i, t_i\\}_{i=1}^N$, where $t_i$ are\ntexts in form of category names, phrases or captions. Open-\nset detection introduces a new perspective, that is, regarding\nlabels as a separate modality.\nWe explain the last layer in the classification branch of\nthe head part to illustrate the difference, as shown in Fig.\n2. In a typical closed-set detector, the last layer of classifi-\ncation branch can be deconstructed equivalently into inner\nproduct operation with category embeddings. Specifically,\neach kernel in convolution of the last layer is naturally a\nrandomly-initialized embedding for a certain category (bias\nis omitted for simplicity). The similarities between category\nembeddings and image region features are computed for cat-\negory assignment. Once the detection problem is regarded as\na multi-modality alignment issue, a significant stride is taken\nfrom closed-set detection to open-set detection. Naturally,\ntwo principal steps emerge as crucial elements: 1) the number\nof embeddings can be further increased and their forms are\nextended to phrases and captions; 2) learning for alignment\nbetween texts and image regions is crucial and should be\ncarefully addressed. By following the aforementioned steps,\nwe can transform a closed-set detector into an open-set\ndetector, thereby bridging the gap between closed-set and\nopen-set detection paradigms.\nB. Model Architecture\nWe illustrate the model architecture in Fig. 3. The de-\ncoupled architecture explicitly processes texts and images\nseparately, and there is no intermediate interaction between\nthe two modalities. Specifically, a text encoder is utilized to\nextract initial embeddings for texts. Here, we adopt the text\nencoder in CLIP [6] to complete the task. Subsequently, a\ntext feature adaptor based on MLPs is designed for projecting\ninitial text embeddings to the joint space. The text feature\nadaptor is the crucial component for feature alignment in the\njoint space, instead of feature interaction in the middle of the\nstructure. This brings simplicity to the structure. For images,\na general single-stage detector with a backbone, neck, and\nhead is employed to extract region features. The detector is\ninstantiated with YOLOv8 which is used in YOLO-World\n[17] as well. As usual, bbox head predicts class-agnostic\nbboxes. Contrastive Head provides region features for align-\nment in the joint space. In the joint space, mutual similarities\nbetween projected text embeddings and region features are"}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "computed to determine the classes that bboxes belong to. The\nframework in Fig. 1(b) places the feature alignment operation\nin the middle processing via interactions. While the proposed\nframework places the alignment operation in the final joint\nspace via a text adaptor. Hence, the proposed framework can\nretain the independence of modality-specific structures.\nMLP Adaptor. The MLP adaptor transforms text embed-\ndings into the joint space, which can be formulated as:\n$e' = MLP(e)$,\nwhere e is the text embedding, e' is the transformed em-\nbedding by the MLP adaptor. The MLP adaptor includes N\nlayers, in which the i-th layer can be expressed as:\n$e_{i+1} = ReLU(W_i e_i + b_i)$,\nwhere $W_i$ and $b_i$ are weight and bias of the linear projection,\n$ReLU(\\cdot)$ is the nonlinear activation of rectified linear unit, $e_i$\nand $e_{i+1}$ are input and output of the i-th layer, respectively.\nIn the proposed architecture, the number of layers is a hyper-\nparameter. All text embeddings share the identical MLPs and\nthe last layer does not undergo the nonlinear activation.\nRe-parameterization for efficiency. In the offline vocab-\nulary inference mode, text embeddings can be pre-computed\nin advance by utilizing the text encoder and text adaptor.\nSubsequently, all these embeddings can be re-parameterized\nas the kernels of the final convolution:\n$E' = unsqueeze(E)$,\n$\\mathcal{X}' = Conv_{E'}(\\mathcal{X})$,\nwhere $E \\in \\mathbb{R}^{K \\times D}$ is pre-computed K text embeddings with\nD-dimensions, $unsqueeze()$ function expands the dimension\nshape of E from $K \\times D$ to $K \\times D \\times 1 \\times 1$, $Conv()$\noperation utilizes $E'$ as kernel weight to process input\n$\\mathcal{X} \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ and $\\mathcal{X}' \\in \\mathbb{R}^{B \\times C \\times H \\times W}$ is the output\nof $Conv()$. Thus, the inference efficiency is approximately\non a par with that of the individual detector. In fact, such an\noperation transforms the open-set detector into a closed-set\ndetector.\nC. Loss Function\nThe proposed method fully adopt the loss function from\nthe closed-set detector. In this paper, we choose YOLOv8 as\nthe detector, and the corresponding loss function is presented\nas follows:\n$\\mathcal{L} = \\lambda_1 \\mathcal{L}_{cls} + \\lambda_2 \\mathcal{L}_{iou} + \\lambda_3 \\mathcal{L}_{dfl}$,\nwhere $\\mathcal{L}_{cls}$ is the classification loss based on contrastive\nsimilarities, $\\mathcal{L}_{iou}$ and $\\mathcal{L}_{dfl}$ are losses for bbox regression,\n$\\lambda_1$, $\\lambda_2$, $\\lambda_3$ are loss weights. The label assignment strategy\nis the same as that of YOLOv8, namely task-aligned label\nassignment [43].\nIn this section, we conducted extensive experiments of\nzero-shot detection on benchmark datasets and the results\nare reported. Ablation studies on the text feature adaptor are\ncarried out. Finally, the efficiency of our method and that of\nthe counterparts is illustrated.\nA. Pre-training Datasets and Implementation Details\nFollowing [17], the datasets employed for pre-training\ncomprise: 1) Objects365v1 [30] with 609k images and\n9,621k annotations; 2) GQA [31] with 621k images and\n3,681k annotations; 3) Flickr30k [32] with 149k images\nand 641k annotations. Objects356v1 is created for object\ndetection task, and the category names are treated as texts.\nGQA and Flickr datasets are merged to form GoldG [33].\nThe proposed DOSOD method is implemented with code\nfrom YOLO-World [17], which is based on MMYOLO [34]\nand MMDetection [35]. We employ YOLOv8 [29] as the\ndetector, with multiple versions including small (S), medium\n(M), and large (L). Text embeddings are obtained by utilizing\nthe text encoder of CLIP [6]. The parameters of the text\nencoder are frozen during training. We pre-train DOSOD\non 8 NVIDIA RTX 4090 GPUs with a batchsize of 128\nwhile YOLO-World uses 32 NVIDIA V100 GPUs with the\nbatchsize of 512. Note that, we adopt a smaller batchsize in\nthe experiments due to hardware limitations, which might\nhave a negative impact on accuracy to a certain extent.\nOther settings, such as the optimizer, learning rate, and\naugmentations, are the same as those of YOLO-World. More\ndetails please refer to [17].\nB. Zero-shot Evaluation\nThe datasets used for pre-training include a wide range of\ncategories, phrases, and captions. Once pre-trained, the mod-\nels are naturally capable of performing zero-shot detection.\nIn this section, we conduct zero-shot evaluations on the LVIS\n[33] and COCO [36] datasets to demonstrate the effective-\nness of the proposed method. We compare our method with\nthe baseline YOLO-World, which has two versions: YOLO-\nWorld-v1 and YOLO-World-v2. YOLO-World-v2 optimizes\nthe neck and head of the network on the basis of YOLO-\nWorld-v1. It makes the model more efficient with a slight\nsacrifice in accuracy. Therefore, YOLO-World-v2 is the main\ncounterpart baseline. Additionally, comparisons with other\nprevalent methods are conducted on the LVIS dataset for\nfurther demonstration.\n1) Results on LVIS Dataset: Zero-shot detection is per-\nformed on LVIS minival, and the metric of Fixed AP [37]\nis used for comparison. The results are presented in Table\nI. We found from the table that Grounding DINO 1.5 Pro\nachieves outstanding performance with a heavy backbone\nViT-L [41] and a huge dataset named Grounding-20M. Our\nmethod is nearly on a par with YOLO-World-v1 in terms of\naccuracy but superior to YOLO-World-v2. Especially, with\nthe YOLOv8-S detector, our DOSOD-S model is marginally\nbetter than the YOLO-World-v1-S model (26.7% \u03c5\u03c2 26.2%)\nand significantly outperforms the YOLO-World-v2-S model\n(26.7% \u03c5\u03c2 22.7%). As the model size increases, the accuracy\ngap between our method and YOLO-World-v2 is gradually\nnarrowing. It is also noted from the table that, when pre-\ntrained on the same datasets, the light-weight DOSOD-S\nmodel outperforms GLIP-T and Grounding DINO-T, and is\ncomparable to GLIPv2-T, all of which use the heavy Swin-\nT [20] backbone. Meanwhile, the DOSOD-L model achieves\nthe highest AP score (34.4%) among models pre-trained on\nthe same datasets with fewer parameters (108\u039c \u03c5\u03c2 232M).\nWe believe that, the proposed DOSOD method could be\nfurther improved by pre-training on larger-scale datasets. The\nresults demonstrate that our method has sufficient competi-\ntiveness in accuracy. In Section IV-D, we will prove that our\nmethod has significant advantages in inference speed.\n2) Results on COCO Dataset: We conduct a comparison\nbetween the pre-trained DOSOD models and YOLO-World\non the COCO dataset for zero-shot detection. The objects\nin COCO is divided into 80 categories, and the category\nnames are treated as texts. We directly utilize the models\npre-trained on Object365v1 and GoldG for transfer. The\nevaluation metric mAP is computed on COCO val2017 for\ncomparison. The results are reported in Table II. The table\nshows that DOSOD exhibits slightly inferior performance to\nYOLO-World-v1 and YOLO-World-v2 by approximately 1%\nmAP. Specially, DOSOD-L is slightly better than YOLO-\nWorld-v1-L (44.6% \u03c5\u03c2 44.4%). This discrepancy may be\nattributed to the fact that COCO is a closed-set detection\ndataset with insufficient text richness, leading to indistin-\nguishable evaluation results.\nC. Ablation Study\nThe text feature adaptation is the core component of the\nproposed method. Specifically, we employ MLPs to project\ntext embeddings from the text encoder to the joint space. The\nnumber of layers of MLPs is a hyper-parameter that requires\ntuning. In this ablation study, the number of layers is set\nto {0, 1, 2, 3, 4, 5}, and experiments are conducted on LVIS\nzero-shot evaluation with Objects365v1 and GoldG as pre-\ntraining datasets. The results are presented in Table III. At the\nbeginning, we observe obvious performance gains (22.4% \u2192\n26.7%) as the number of layers increases from 0 to 3. Then,"}, {"title": "V. CONCLUSIONS", "content": "the performance gradually decreases when the number of\nlayers continuously increases. Based on these observations,\nwe conclude that the text feature adaptation with MLPs is\nsignificant for achieving better alignment in the joint space.\nConsequently, the number of layers of MLPs is set to 3 for\nall our models.\nA light-weight framework, Decoupled OSOD (DOSOD),\nis proposed for open-set object detection. DOSOD integrates\na vision-language model (VLM) with a detector. An MLP\nbased feature adaptor is designed to transform text embed-\ndings into a joint space, where the detector learns region\nrepresentations to achieve cross-modality feature alignment\nwithout interactions. DOSOD operates like a traditional\nclosed-set detector during testing, effectively bridging the\ngap between closed-set and open-set detection. Experimental\nresults show that DOSOD significantly enhances real-time\nperformance while maintaining comparable accuracy. Tests\non the RDK X5 development kit confirm that DOSOD could\nwell supports real-time OSOD tasks in robotic systems with\nlimited computational resources."}]}