{"title": "IMPACTX: Improving Model Performance by Appropriately predicting Correct explanations", "authors": ["Andrea Apicella", "Salvatore Giugliano", "Francesco Isgr\u00f2", "Roberto Prevete"], "abstract": "The explainable Artificial Intelligence (XAI) research predominantly concentrates to provide explainations about AI model decisions, especially Deep Learning (DL) models. However, there is a growing interest in using XAI techniques to automatically improve the performance of the AI systems themselves.\nThis paper proposes IMPACTX, a novel approach that leverages \u03a7\u0391\u0399 as a fully automated attention mechanism, without requiring external knowledge or human feedback. Experimental results show that IMPACTX has improved performance respect to the standalone ML model by inte- grating an attention mechanism based an XAI method outputs during the model training. Furthermore, IMPACTX directly provides proper feature attribution maps for the model's decisions, without relying on external XAI methods during the inference process.\nOur proposal is evaluated using three widely recognized DL models (EfficientNet-B2, MobileNet, and LeNet-5) along with three standard im- age datasets: CIFAR-10, CIFAR-100, and STL-10. The results show that IMPACTX consistently improves the performance of all the inspected DL models across all evaluated datasets, and it directly provides appropriate explanations for its responses.", "sections": [{"title": "1 Introduction", "content": "The inner workings of modern Machine Learning (ML) and Deep Learning (DL) approaches are often opaque, leaving AI scientists to interpret the reason behind the provided outputs. eXplainable Artificial Intelligence (XAI) aims to provide insights into the internal mechanisms of AI models and/or the motivations be- hind their decisions, helping users to understand the outcomes produced by ML models. \u03a7\u0391\u0399 approaches are adopted in several ML tasks applied to var- ious types of inputs, including images [35, 3, 33], natural language processing, clinical support systems [39, 2], and more. However, it's worth noting that"}, {"title": "2 Related works", "content": "Different kinds of explanations have been proposed to address the explainability problem [9, 14, 35, 29, 41], depending on both the AI system being explained and the XAI method adopted. A very common approach consists of providing visual-based explanations in terms of input feature importance scores, known as attribution or relevance maps. Examples of this approach include Activation Maximization (AM) [15], Layer-Wise Relevance Propagation (LRP) [9], Deep Taylor Decomposition [11, 34], Deconvolutional Network [53], Up-convolutional Network [52, 14], and SHAP method[29].\nImportantly, in the context of enhancing ML systems by XAI methods, these methods can be viewed as a way of exploiting external knowledge which can be provided by human intervention, such as the annotation of attribution maps, or in a fully automated manner without the involvement of a human operator (see, for example, [20, 4]). However, as already mentioned in section 1, we note that a significant area of the literature focuses on improving ML mod- els through human intervention in the context of XAI, as evidenced in several studies [46, 40, 18, 36, 32, 56, 42]. This aspect can also be deduced by [49], a recent survey of research works using XAI methods to improve ML systems. Moreover, [49] discusses efforts to improve ML models along different dimensions such as performance, convergence, robustness, and efficiency. For improving ML model performance with XAI, [49] isolates four main approaches: i) augmenting the data, explanations are utilized to generate artificial samples or to alter the distribution of data that provide insights into undesirable behaviors [46, 40]; ii) augmenting the intermediate features, by measuring feature importance through explanations, this information can be effectively used to scale, mask, or trans- form intermediate features to improve the model [1, 16, 5, 7]; iii) augmenting the loss, additional regularization terms based on explanations are incorporated into the loss training function [36, 22, 27]; iv) augmenting the gradient, informa- tion about the importance of the features provided by explanations can also be applied during the backpropagation pass by changing the gradient update [17].\nAs described in section 3, the proposed method, IMPACTX, focuses on improving the performance of an ML system and it is related to category ii) 'augmenting the intermediate features' and iii) 'augmenting the loss' in a fully automated manner without involvement of human knowledge in the learning step.\nIn this context, i.e., to improve ML system performance by XAI methods in an automated manner without involvement of human knowledge in the learning step, [10] propose Guided Zoom to improve the performance of models on visual data, especially on fine-grained classification tasks (i.e., where the differences between classes are subtle), This method aims to improve fine-grained classi- fication decisions by comparing consistency of the evidence for the incoming"}, {"title": "3 IMPACTX framework", "content": "In its simplest form, a typical ML classification system A can be usually viewed as the composition of two main components: a feature extractor M (for example, in a feed forward DNN model, it usually corresponds to the DNN first layers) and a classification component Q (usually corresponding, in a feed forward model, to the remaining part of the classification process) s.t. \u0177 = A(x) ="}, {"title": "3.1 Adopted assumptions and notation", "content": "In its simplest form, a typical ML classification system A can be usually viewed as the composition of two main components: a feature extractor M (for example, in a feed forward DNN model, it usually corresponds to the DNN first layers) and a classification component Q (usually corresponding, in a feed forward model, to the remaining part of the classification process) s.t. \u0177 = A(x) ="}, {"title": "3.2 General description", "content": "Q(M(x)) \u2208 {1,... K} is the estimated class of the input x \u2208 Rd. Without losing in generality, in this paper, we consider Q as just the final function to compute the inferred class from a given vectors of K scores, i.e. Q(m) = arg max (softmax(m)) with m\u2208 RK. After a proper learning procedure, M is a model able to represent a given x in a K-component array where the k- th component with 1 \u2264 k \u2264 K represents a score of x to belong to the k-th class. An example of M can be all the layers of a DNN before the final softmax function. However, in general every model able to project x in a given feature space can be adopted as M (such as a sequence of layers of a DNN able to extract features from an input x). We define with y \u2208 {1, . . ., K} a scalar representing the correct label of an input x in a K-class classification problem. Let ST be a training dataset of N labeled instances, i.e. ST = {(x(i),y(i))}N i=1, and SE a test dataset composed of J instances used only to evaluate an ML model, i.e. SE = {(x(i),y(i))}J i=1\nIMPACTX is a double-branch architecture such that, when applied to a classifier A, the resulting architecture outperforms the standalone A(x) = Q(M(x))), both appropriately trained. In addition, this enhanced architecture also provides input attribution maps relative to the output obtained.\nIMPACTX framework is composed of two branches that interact each other (see figure 1):\n1.  The first branch (on the top) is composed of a Feature Extractor M, able to extract significant features from the input with the goal to classify the input x, and a K-class classifier C, able to return an estimated class \u0177 of the input x.\n2.  The second branch (at the bottom) is responsible for the attention mech- anism of IMPACTX. It is composed of a Latent Explanation Predictor (LEP) module, able to extract essential information from the input fea- tures with the goal to compute an attribution map of the input respect to the classification response, and a Decoder D, able to effectively produce an attribution map of the input x.\nThus, the goal of IMPACTX is to build an estimated class \u0177 = C(m,z) exploiting both the m = M(x) and z = LEP(x) outputs, and at the same time to obtain a predicted attribution map \u0155 with respect to \u0177.\nIn the next section we will discuss in detail how IMPACTX can be trained to obtain this goal."}, {"title": "3.3 Training IMPACTX", "content": "The training phase of the IMPACTX approach is depicted in figure 1. Both M and LEP receive x as input, producing the corresponding outputs m and z. These outputs are concatenated and forwarded to the classifier C. Additionally,\nz is decoded by the decoder D, responsible for reconstructing the explanation r. In other words, LEP and D act as an Encoder-Decoder which is constrained to learn an encoding of the explanations by the internal variables z. C leverages the combined knowledge of M and LEP. Therefore, for a new data point x(i), the estimated output \u0177(i) is defined as:\n\u0177(i) = arg max \\u2060( softmax(C(M(x(i)), LEP(x(i))))).\nConsequently, IMPACTX wants to solve the classification task and, at the same time, to construct a predicted attribution map r using the decoder D and the LEP module.\nImportantly, the effective IMPACTX training can be made in at least two ways:\nSingle-stage training: All IMPACTX modules are trained simultaneously on ST, with the attribution maps r(i) for each sample in ST generated at the end of each training iteration. In this case, classification performance can be improved by using increasingly accurate attribution maps, r(i). Ideally, this cre- ates a positive feedback loop where both the classification and the generation of attribution maps are mutually enhanced. Conversely, computing the attribu- tion maps at each iteration can be computationally expensive and, at the same time, the generated attribution maps r(i) may have very poor significance in the early learning epochs since M, LEP and D weights are initialised to random values.\nTwo-stage training: This training approach is divided into two stages. In the first stage, the parameters of the whole classifier A(\u00b7) are trained and initially evaluated on ST. Then, at the end of the first training stage, the attribution"}, {"title": "3.4 Generating the attribution-based explanations", "content": "maps r(i) for each sample in ST are produced with respect the true class label. In the second training stage, the remaining modules C, D, and LEP are trained while keeping M frozen. In particular, the targets of the branch LEP-D are the corresponding attribution maps previously computed at the end of the first stage. Since the attribution maps are computed just one time, two-stage training results less expensive than single-stage training.\nIn both the training approaches, the architecture is trained using a loss func- tion that combines together Mean Squared Error (MSE) between the true class attribution map r(i) (see sec. 3.4) and the output of the LEP-D branch, and the Cross Entropy (CE) loss between the true class label y(i) and the prediction from C. The resulting loss function can be formalized as follows:\nL = CE(y(i), C(m(i), z(i))) + \u03bb \u00b7 MSE(r(i), LEP \u2013 D(x(i)))  (1)\nwhere A represents a regularization parameter. This approach lead z(i) to be op- timized respect to the attribution reconstruction error while maintaining robust classification performance simultaneously.\nWe use an XAI attribution method R to generate attribution maps r on x about the true class label y based on A(x). In particular, for each available training data x(i) in ST, an attribution map r(i) corresponding to the true class label y(i) when x(i) is the input of A(x(i)) = Q(M(x(i))), is produced adopting R. The objective is to get an attribution map aligned with the true class label y(i) for each training data x(i). Therefore, we adopt as R an XAI method that provides explanations not only for the predicted class, but also for each possible class, which we denote as R(A, x, k) = rk, where k represents the class for which we need the explanation. Therefore, the attribution map corresponding to the true class label provided in the training data results r(i) = R(A(x(i)), x(i),y(i))."}, {"title": "4 Experimental setup", "content": "In this section, we present a series of experiments aimed at evaluating the IM- PACTX framework with respect to two different capabilities: to improve the performance of ML models and to provide meaningful explanations in the form of attribution maps. The performance of IMPACTX is evaluated on several standard classification tasks, comparing its results with similar methods in the literature, namely ABN [16] and WAE [7], and with IMPACTX itself without the attention mechanism provided by the bottom branch (we considered it as baseline). Following this, we present both qualitative and quantitative analyses to assess the validity of the attribution maps generated by IMPACTX. The attri- bution maps' qualitative analysis focuses on how the explanations highlight sig- nificant parts of the elements to be classified, which we expect to be aligned with human user expectations, providing intuitive insights into the model's decision- making process. Meanwhile, the quantitative evaluation employs MoRF (Most"}, {"title": "4.1 Datasets", "content": "The benchmark datasets used in this study include CIFAR-10, CIFAR-100, and STL-10.\n\u2022  CIFAR-10 [24] comprises 60,000 color images categorized into ten classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. The dataset is split into 50,000 training images and 10,000 test images, all sized 32 \u00d7 32 pixels.\n\u2022  CIFAR-100 [24] contains 60,000 color images grouped into 100 categories, with 50,000 training images and 10,000 test images, all sized 32 \u00d7 32 pixels.\n\u2022  STL-10 [12] dataset comprises images classified into ten classes: airplane, bird, car, cat, deer, dog, horse, monkey, ship, and truck. Each image is 96 \u00d7 96 pixels, and the dataset includes 5,000 training images and 8,000 test images."}, {"title": "4.2 Explanation Generator Algorithm", "content": "We have adopted the SHAP method [29] as R. This choice was motivated by the fact that SHAP is a prominent technique in XAI that provides explanations in terms of attribution maps, and provides explanations not only for the predicted class, but also for each possible class, as required by our approach. For this study, in particular, we employed the Partition Explainer algorithm as specific version of SHAP, with 2000 evaluations to obtain the final explanations [29]."}, {"title": "4.3 IMPACTX modules and the two-stage training", "content": "We evaluated IMPACTX using three distinct models for M: LeNet-5 [25], \u041c\u043e- bileNet [19] pre-trained on ImageNet [13], and EfficientNet-B2 [45] pre-trained on the Noisy Student weights [50]. The focus of this study is oriented to the interpretation of the model and to the explanation of the influence of the input characteristics on the models' prediction.\nThe Latent Explanation Predictor LEP module is built starting from the architecture of selected A, replacing the last fully-connected layers by a fully- connected layer with a dimensionality of 512 and a sigmoid activation function.\nThe architecture of the Decoder used for CIFAR-10 and CIFAR-100 datasets is depicted in fig. 2. In case of STL-10, an additional sequence of [Conv, Conv, UpSampling] layers was included in the decoder due to the different"}, {"title": "4.4 Evaluation measures", "content": "We evaluated IMPACTX for its performance in the classification tasks and for the attribution maps that it generates. Performances on classification tasks were measured in terms of accuracy on the standard test sets provided with each dataset. The proposed method was subjected to a comparative analysis with the baseline A (which can be seen as IMPACTX itself without the at- tention mechanism provided by the bottom branch), ABN [16] and WAE [7] approaches. Instead, the attribution maps are evaluated both qualitatively and quantitatively, reporting respectively examples of the obtained attribution maps and MORF (Most Relevant First) curves [37]. MORF curves are widely used in \u03a7\u0391\u0399 research to assess proposed explanations [23, 6, 47, 21]. Essentially, im- age regions are iteratively replaced by random noise and fed to the ML model, following the descending order of relevance values indicated by the attribution map. Thus, the more relevant the identified features are for the classification output, the steeper the curve. MoRF curves offer a quantitative measure of how well an attribution map explains the behavior of an ML system. Summarizing, the qualitative aspect is important for understanding how closely the obtained explanation aligns with the user's intuitive expectations, while the quantitative evaluation aims to provide a measure of how valid the attribution maps are as explanations."}, {"title": "5 Results and Discussion", "content": null}, {"title": "5.1 Performance", "content": "Tab. 2 shows the performance of different architectures A(\u00b7) (that are LeNet-5, MobileNet, and EfficientNet-B2) on CIFAR-10, CIFAR-100, and STL-10 test sets with (IMPACTX column) and without (baseline column) IMPACTX, in terms of accuracy. In the same table, results obtained by ABN and WAE are reported.\nOne can note that IMPACTX produced an uniform improvements with re- spect the baseline on all the selected datasets independently from the classifi-"}, {"title": "5.2 Evaluating attribution maps", "content": "In this section we want to evaluate if the attribution maps directly obtained by IMPACTX can be considered as explanations of the IMPACTX classifica- tion responses. To this aim, we compare them with the explanations given by"}, {"title": "5.3 Discussion", "content": "The experimental results show that IMPACTX performs better on the clas- sification tasks than ML models that do not use the IMPACTX framework, while providing attribution maps that can be considered more reliable expla- nations than those provided by post-hoc XAI methods. In particular, on the performance side, the interaction between the IMPACTX branches enhances the model's ability to focus on relevant features, thereby improving classifica- tion performance. During training, IMPACTX emphasizes features identified as relevant to the model's decisions, leading to more accurate outcomes. In fact, IMPACTX's training process is designed to focus on the features identified as important to the model's decisions. Furthermore, regarding the construction of the attribution maps by the bottom branch, the ability of IMPACTX in pro- viding accurate attribution maps can be attributed to its integrated approach of decision and justification at inference time, which contrasts with the external nature of post-hoc methods such as SHAP. This integration allows IMPACTX to generate attribution maps that more accurately reflect the model's decision- making process. Interestingly, attribution maps generated by IMPACTX seems to be better explanations compared to ABN. This is likely because IMPACTX'S attentional mechanism is designed to consider the true class label during train- ing, whereas ABN's CAM-based approach relies on class scores obtained during training."}, {"title": "6 Conclusion", "content": "In conclusion, experimental results showed that IMPACTX is able to include XAI strategies into the ML pipeline positevely affecting the performance. Fur- thermore, IMPACTX computes valid explanations for its outputs at inference time, without relying on external XAI methods after the inference process. In particular, the experimental assessment shows the effectiveness of the incorpo- rated latent explanation predictor LEP in extracting useful knowledge. This"}]}