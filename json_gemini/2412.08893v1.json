{"title": "EFFICIENT REINFORCEMENT LEARNING FOR OPTIMAL CONTROL WITH NATURAL IMAGES", "authors": ["Peter N. Loxley"], "abstract": "Reinforcement learning solves optimal control and sequential decision problems widely found in control systems engineering, robotics, and artificial intelligence. This work investigates optimal control over a sequence of natural images. The problem is formalized, and general conditions are derived for an image to be sufficient for implementing an optimal policy. Reinforcement learning is shown to be efficient only for certain types of image representations. This is demonstrated by developing a reinforcement learning benchmark that scales easily with number of states and length of horizon, and has optimal policies that are easily distinguished from suboptimal policies. Image representations given by overcomplete sparse codes are found to be computationally efficient for optimal control, using fewer computational resources to learn and evaluate optimal policies. For natural images of fixed size, representing each image as an overcomplete sparse code in a linear network is shown to increase network storage capacity by orders of magnitude beyond that possible for any complete code, allowing larger tasks with many more states to be solved. Sparse codes can be generated by devices with low energy requirements and low computational overhead.", "sections": [{"title": "1 Introduction", "content": "Many interesting and complex tasks can be described using natural image sequences. Each image of the sequence corresponds to a particular state of the environment, and choosing an action or applying a control at the current state takes the environment to its next state, and leads to the next image in the sequence. A natural question is then: given an image, what is the best control to apply? Although the answer will depend on the particular task at hand a general framework can be identified. As a simple example, consider moving through a rainforest and taking images of flowers in order to locate a particular species of plant. In this case, each image describes a state of the environment, and the state changes depending on the location of the next image. Each image also has a cost or reward associated with it: an image of the sought-after flower will receive a high reward, while an image without any flowers may receive a low (or even negative) reward. To perform this task well, the choice of control leading to the location of the next image does not necessarily give the best chance of obtaining an image of the sought-after flower at the next time period, rather, it should also lead to a good chance of obtaining such images at future time periods as you move through the rainforest. Planning over a sequence of time periods is the defining characteristic of sequential decision problems and optimal control, and is clearly a requirement of any intelligent system.\nNatural images are a high-dimensional dataset with a great deal of expressive power. For some tasks, images may include enough task-dependent information to determine an optimal policy. In this case an image is similar to a belief state in a partially observed Markov decision process, and we will derive the general conditions for an image to be sufficient for implementing an optimal policy. Similar to other types of ecological data, it is well-known that the statistical properties of natural images distinguish them from artificially generated images [Field, 1987, 1994, Ruderman and Bialek, 1994, Ruderman, 1997, Simoncelli and Olshausen, 2001, Hyv\u00e4rinen et al., 2009, Hosseini et al., 2010]. We exploit these statistical properties to find efficient image representations that significantly decrease the computational resources required for finding optimal policies."}, {"title": "2 Reinforcement Learning over Natural Image Sequences", "content": "Consider a general optimal control task involving a discrete-time dynamical system with a finite number of states and controls [Bertsekas, 2017, 2019]. During any time period k the environment is described by a state $i_k$ taken from a state-space $S_k$, and a control $u_k$ is selected from a set of available controls $U_k(i_k) \\subset C_k$. The state of the environment changes when a controller chooses control $u$ during the current time period, so that state i updates to the successor state j according to the transition probabilities $p_{ij}(u)$ of a controllable Markov chain. The controller can choose different controls depending on the current state of the environment as well as the time period under consideration: the controller's choice is then described by a policy $\\pi = {\\mu_0,\\ldots, \\mu_{N-1}}$, which is a sequence of functions $\\mu_k$ that map states into controls: $\\mu_k(i_k) = u_k$, for each state $i_k$ at each time period k. The cost of being in state $i_k$ at time period k, choosing control $u_k$, then moving to state $i_{k+1}$, is denoted by $g_k(i_k, u_k, i_{k+1})$. The key quantity of interest is the expected total cost. Given a policy $\\pi$ and an initial state $i_0$, the expected total cost is the expected value of the sum of costs:\n\n$I_{\\pi}(i_0) = E \\bigg[g_N(i_N) + \\sum_{k=0}^{N-1} g_k (i_k, \\mu_k(i_k), i_{k+1})\\bigg]$\n\nwhere $g_N(i_N)$ is the terminal cost incurred at the end of the task (k = N); and the expectation is taken over the states $i_1, ..., i_{N-1}$ using the transition probabilities $p_{ij}(\\mu_k(i))$, the initial state $i_0$, and the chosen policy $\\pi = {\\mu_0, ..., \\mu_{N-1}}$."}, {"title": "2.1 A Scalable Benchmark for Efficient Image Representations", "content": "We would now like to design a simple and robust benchmark for the purpose of assessing alternative image representations for reinforcement learning with natural image sequences. The number of states of the benchmark should scale to arbitrarily large numbers, its cost structure should be simple and easy to implement over long horizons, and its transition probabilities should correspond to a natural image generator that is easy to implement.\nThe number of states of the benchmark can scale to arbitrarily large numbers by working with image patches. Image patches are arbitrarily-sized image regions extracted from a larger parent image. Discretizing the kth parent image into a regular grid pattern yields a set of image patches $S'_k$, with each image patch denoted by $\\phi_k(i)$; where $i \\in Z^2$ is a unique pair of coordinates giving the location of the image patch within its parent image (for example, each coordinate pair could give the coordinates of the top-left corner of an image patch). The number of image patches can be increased by decreasing the size of each image patch, similar to the case of discretizing a continuous variable where decreasing the step size increases the number of steps. The following proposition then shows the number of states of the benchmark is scalable.\nProposition 1. A state of the benchmark can be described by $i_k$ or $\\phi_k(i)$, and either state is a sufficient statistic.\nProof. Given any image patch $\\phi_k(i) \\in S'_k$, it is possible to determine state $i \\in S_k$ as\n\n$i = \\underset{m}{\\text{argmin}} ||\\phi_k(m) - \\phi_k(i)||^2_2,$\n\nprovided each image patch is unique. Alternatively, given state i, it is possible to determine state $\\phi_k(i)$ by looking up the ith element of $S_k = {\\phi_k(1), ..., \\phi_k(n)}$. The same is true for both $\\phi_{k+1}(j) \\in S_{k+1}$ and $j \\in S_{k+1}$. Therefore, given $p_{ij}(u)$, $g_k(i, u, j)$, and $U_k(i)$; it is possible to determine $p(\\phi_{k+1}(j)|\\phi_k(i), u) = p_{ij}(u)$, $\\bar{g}_k(\\phi_k(i), u, \\phi_{k+1}(j)) = g_k(i, u, j)$, and $\\bar{U}_k(\\phi_k(i)) = U_k(i)$. Alternatively, given $p(\\phi_{k+1}(j)|\\phi_k(i), u)$, $\\bar{g}_k(\\phi_k(i), u, \\phi_{k+1}(j))$, and $\\bar{U}_k(\\phi_k(i))$; it is possible to determine $p_{ij}(u) = p(\\phi_{k+1}(j)|\\phi_k(i), u)$, $g_k(i, u, j) = \\bar{g}_k(\\phi_k(i), u, \\phi_{k+1}(j))$, and $U_k(i) = \\bar{U}_k(\\phi_k(i))$. But this means i or $\\phi_k(i)$ is sufficient for determining u from the minimization in Eq. (4).\nThe proof of Proposition 1 describes a very simple natural image generator $p(\\phi_{k+1}(j)|\\phi_k(i), u)$ constructed from the transition probabilities $p_{ij}(u)$ and the image patch set $S'_k$. A non-trivial model for $p_{ij}(u)$ is presented in the next sections.\nThe cost structure and transition probabilities of the benchmark are motivated by dragonfly tracking in videos [Loxley, 2021]. The tracking task requires a tracker to follow a target as closely as possible over many time periods using a set of discrete controls that determines the dynamics of the tracker (see Fig. 1). Letting the image patch coordinates be related to the distance between the tracker and target allows this distance to be used to assign a cost to each image patch at each time period. Moving from one image patch to another changes this distance, and therefore the cost. An optimal policy corresponds to the optimal choice of controls for tracking the target starting from any image patch during any time period. This task is easily scalable to long horizons and many states provided the target dynamics scales well. Next, a scalable model for the target dynamics is presented."}, {"title": "2.1.1 Target Dynamics", "content": "The target dynamics for the benchmark has two key requirements: 1) the target must be difficult enough to track so that simple algorithms such as greedy search perform suboptimally, and 2) the dynamics must easily scale to long horizons and many states. Both points can be addressed by introducing a simple generative model for the target dynamics.\nMotivated by the dragonfly video dataset used for optimal control in Loxley [2021], the following type of scenario is proposed. The target (T) starts on the bottom row of a 3 \u00d7 3 grid (shown in Fig 1, first grid). The target and the tracker can both move up, left, or right at each time period. However, the target (but not the tracker) can also make an \u201cevade\" move by moving diagonally to evade the tracker. The tracker must therefore carefully plan its sequence of moves to follow the target closely.\nDuring the first time period (second grid in Fig 1) the target remains where it is, while at the beginning of the second time period (third grid in Fig 1) it makes a diagonal move. An optimal tracker (blue square) anticipates a diagonal move immediately following a time period where the target is stationary, and pays a small cost to move up one square during the first time period (second grid in Fig 1). Whether the target next moves diagonally-left or diagonally-right, the optimal tracker is now able to reach the target during the next time period (third grid in Fig 1). On the other hand, a tracker applying a greedy algorithm (red squares) tries to stay as close to the target as possible during each time period, allowing the target to \"evade\" the greedy tracker when it makes its diagonal move, and the greedy tracker falls one"}, {"title": "2.1.2 Target Tracking", "content": "It is now possible to specify the reinforcement learning benchmark in terms of a target tracking task. This task requires two dynamical variables $t_k, c_k \\in Z^2$ representing the discrete two-dimensional coordinates of a target, and a controller (tracker), respectively, at time period k. Changes to the environment are described by the discrete dynamical system:\n\n$t_{k+1} = t_k + \\Delta t_k,$\n\n$c_{k+1} = c_k + u_k.$\n\nA target updates its position from $t_k$ to $t_{k+1}$ according to the transition probabilities of the Markov chain presented in Sec. 2.1.1. In order to follow the target as closely as possible a controller updates its position from $c_k$ to $c_{k+1}$ by choosing a control $u_k$ from the set U of available controls. Complete state information required for choosing optimal controls is given when the state is defined to be:\n\n$i_k = (c_k - t_k, \\Delta t_{k-1}) \\in D \u00d7 T,$"}, {"title": "3 Results", "content": "Results for the reinforcement learning benchmark developed in Section 2 are now presented. We begin by looking at the optimal and greedy policies for target tracking."}, {"title": "3.1 Optimal and Greedy Policies in the Reinforcement-Learning Benchmark", "content": "The evasive target dynamics in the benchmark leads to different policies for optimal and greedy tracking. When p = 0 in the Markov chain, the target dynamics is deterministic and periodic and given by the repeating sequence (sdr)*. The optimal policy from Eq. (17) is given in Table 1 when the chain is initially in state s, and cycles between three states: $i_k = ((0, 1), (0,0))$, $i_{k+1} = ((0,0), (1, 1))$, and $i_{k+2} = ((0, 0), (0, 1))$. Only the first state has a non-zero cost, and therefore the total cost of the optimal policy increases by one every three time periods. This is shown in Fig 3 for p = 0. The greedy policy can be found by replacing $J_{k+1}$ in Eq. (17) with $g_{k+1}$, leading to the simpler expression:\n\n$\\underset{u \\in U}{min} \\sum_{b_2} p(b_2/b_1) \\bigg||\\frac{a_1 + u}{\\bigg||^2_2}.$"}, {"title": "3.2 Benchmark Performance with Length of Horizon", "content": "The optimal and greedy policies given in Tables 1 and 2 turn out to be stationary policies for all values of p. This is confirmed using an infinite horizon analysis. The cost of each policy is found in the Appendix by introducing a discount factor and applying policy evaluation in the infinite horizon limit (N \u2192 \u221e). These calculations are in complete agreement with value iteration applied to the discounted problem (not shown). In addition, the ratio of costs of the greedy and optimal policies in Tables 1 and 2 is given in the Appendix for the case of an infinite horizon, and the result is completely consistent with our discussion of these policies for finite horizons. For example, in the infinite horizon limit the expected total cost ratio of greedy and optimal policies becomes 2 when p = 0, 5 when p = 0.75, and diverges when p = 1. For finite horizons, the first result (p = 0) is most easily seen in Fig 3 when N = 30; giving a ratio of 20/10 = 2. The final result (p = 1) is also seen in Fig 3 when N = 30, giving a ratio of 29/1 = 29; which will clearly diverge as N \u2192 \u221e. The result for p = 0.75 can be seen from the ratio of expected total costs of the greedy and optimal results in Fig 7.\nThe optimal and greedy policies undergo more complicated dynamics over shorter horizons when different initial states are chosen. For an initial state given by ((0, 1), (0,0)) (i.e., where the tracker starts close to the target) the bottom pair of curves in Fig. 4 reach the stationary optimal and greedy policies in Tables 1 and 2 at very short horizons. As the initial distance between the tracker and target increases for positive coordinates (middle and top pairs of curves in Fig. 4), the optimal and greedy policies take longer and longer to reach these stationary policies. For example, the top pair of curves has not reached their stationary policies until the horizon is around 14 time periods. These results suggest the dynamics of all initial states with one positive coordinate and one non-negative coordinate eventually reach the stationary policies in Tables 1 and 2 provided the horizon is long enough.\nIn Fig. 4, the greedy policy is shown to be an optimal policy when the initial state of the tracker is far from the target. Reaching a stationary policy only happens when the tracker gets close enough to the target (within a distance of one). Once the tracker is close to the target the greedy policy is no longer optimal (as shown for longer horizons in Fig. 4). For initial states where both coordinates are negative, the tracker is always behind the target and can never catch up. A simple greedy approach is then sufficient to follow the target, ensuring that the greedy policy is an optimal policy in this case. A more detailed discussion of initial state dependence is given in Sec. 3.5."}, {"title": "3.3 Efficient Image Representations for Reinforcement Learning", "content": "To find efficient representations for natural image sequences used in reinforcement learning, image patches from the benchmark were transformed into overcomplete sparse codes and whitened complete codes. A set of 48 natural images was taken from the database \"Natural Scene Statistics in Vision Science\" by Geisler and Perry [2011]. Each image was cropped to 2844 \u00d7 2844 pixels to make it square, then converted to grayscale and double precision using the Matlab functions rgb2gray and im2double, and subsequently discretized into square image patches of side-length a (in pixels). The number of image patches in a single image was $[2844/a]^2$; where the function [.] rounds down to the nearest integer.\nOvercomplete sparse codes were constructed using the method described in Loxley [2021] and summarized in the Appendix. This method relies on the two-dimensional Gabor function, originally used to model simple-cell receptive field profiles in the primary visual cortex [Hubel and Wiesel, 1959, Jones and Palmer, 1987, Daugman, 1985, 1989]. The Gabor function parameters are chosen by drawing samples from a tractable multivariate probability distribution that well-approximates natural image statistics [Loxley, 2017]. A sparse code is given by the hidden-layer activities of an autoencoder with an input/output that comprises the same image patch, and connection weights given by 2D Gabor functions adapted to natural image statistics. The encoding requires solving a linear least-squares problem, while decoding is a simple matrix-vector multiplication. The size of the sparse code depends on the number of Gabor functions used to generate the code. To generate a \u00d764 overcomplete sparse code, the number of Gabor functions required is 64 \u00d7 $a^2$ for an image patch of $a^2$ pixels. For reasons that will become apparent, we would like to choose a to maximize the number of image patches available in a 2844 \u00d7 2844 image, whilst ensuring the size of a \u00d764 overcomplete sparse code is larger than this number; i.e., 64 \u00d7 $a^2$ > $[2844/a]^2$. The unique solution to this problem is a = 19, leading to 22201 image patches of 361 pixels each.\nA more traditional method of pixel decorrelation (whitening) was used to construct complete codes: where the rep-resentation has the same number of pixels as the image patch it was derived from. This method was carried out by setting the mean pixel values (taken over all image patches) to zero, and diagonalizing the corresponding covariance matrix. This method does not generalize to overcomplete codes."}, {"title": "3.4 Benchmark Performance with Number of States", "content": "Training a neural network on efficient image representations takes fewer computational resources than training it directly on images [Loxley, 2021]. This is shown in Figs. 5 and 6, where the number of least-squares iterations required to store a given number of cost-to-go values in a linear network is shown for network inputs given by image patches and image-patch representations. Each state of the benchmark corresponds to a unique image patch and its associated cost-to-go value. By adjusting the network weights during training, this association is stored in the network in order to determine an optimal policy. In a linear network, the number of network weights determines the storage capacity of the network; that is, the maximum number of cost-to-go values it can store. In turn, the number of weights is equal to the number of pixel inputs to the network; since each input is multiplied by its corresponding weight and summed together to yield the network output. Therefore, the storage capacity of a network can be increased by increasing the number of pixel inputs and network weights. One way to do this for a fixed-sized image patch is to generate an overcomplete representation of the image patch, and use this as input to the network instead. This approach will be used to solve the benchmark tracking task as the number of states, and therefore, the number of stored cost-to-go values, increases. Network training is carried out using the Matlab 1sqr function running on an Nvidia GPU, with the number of least-squares iterations until convergence (within 1 \u00d7 $10^{\u22126}$ of the least-squares objective), averaged over 48 independent trials, shown along the vertical axis.\nIn Fig. 5, the size of each image patch was chosen to be 400 pixels instead of 361 pixels for visualization purposes when the number of stored cost-to-goes is relatively small. When the input is either an image patch or a complete representation of an image patch, a linear network will have exactly 400 inputs and 400 weight parameters, allowing it to store a maximum of 400 cost-to-go values. The curve with black circles (\u00d71 raw image), and the curve with blue circles (x1 whitened image) show each of these cases, and both curves get close to 400 stored cost-to-goes. However, input given by a whitened image generally takes far fewer least-squares iterations to store the same number of cost-to-goes as input given by a raw image, making it an efficient image representation. The curve with black triangles (\u00d74 raw image) shows results for a natural image patch that has been upscaled by a factor of four using bicubic interpolation via Matlab's imresize function, while the curve with blue triangles (\u00d74 sparse code) shows results for an overcomplete sparse code. Each of these representations has 1600 pixels (i.e., 4 \u00d7 400 pixels, treating Gabor coefficients in a sparse code the same as double-precision pixels in an image). Increasing the number of weights to 1600 allows a linear network to store a maximum of 1600 cost-to-go values. However, the best result for the \u00d74 resized image remains below 400 stored cost-to-goes, and uses approximately the same number of least-squares iterations as"}, {"title": "3.5 Benchmark Performance for Different Initial States", "content": "We now look at the effect of starting at different initial states in the reinforcement learning benchmark. This is done by starting the benchmark in an initial state, and then running it with optimal and greedy policies until reaching the horizon. The total cost of the optimal and greedy policies can then be compared. This is repeated for each state of the benchmark.\nTraining the benchmark with a \u00d764 sparse code leads to 21675 possible initial states. The expected total cost of optimal and greedy policies is shown in Fig 8 for each initial state leading to a suboptimal greedy policy. These initial states are indexed from 1 to 10880, comprising roughly half of the 21675 available initial states. For the remaining 10795 initial states (not shown) the greedy policy is the optimal policy. The benchmark horizon is 200 time periods, and the Markov chain parameter is p = 0.4. The policy found using fitted value iteration exactly matches the optimal policy found using dynamic programming, as shown in Fig 8 (Inset) for initial states indexed from 4000 to 4050. Fig 9 shows the distribution of cost differences, given by the cost of the greedy policy minus the cost of the optimal policy, for each initial state in Fig 8. These cost differences increase further for longer horizons.\nLooking at a few specific initial states helps to understand Fig 8. The lowest cost optimal policy starts at state ((0,0), (1, 1)) and has an expected total cost of 54. Since this state is part of the stationary policy in Table 1, we can find its total cost for p = 0; which is (200 \u2013 2)/3 \u00d7 1 = 66. From Fig 3, we also know the optimal cost for p = 0.4 would be less than that for p = 0, justifying the order relation 54 < 66. Further, making use of the infinite horizon result from the Appendix, we predict an approximate value for the expected total cost of the greedy policy as: (1 + 1/(1-p)) \u00d7 54 = 144 for p = 0.4. This exactly matches the value in Fig 8.\nNow that we have a reasonable theoretical justification for the lowest cost policy in Fig 8, let us attempt to understand the highest cost policy. This policy starts at state ((42, -42), (1, 1)) and has an expected total cost of 185870 for the optimal policy, and 185890 for the greedy policy. We can understand this as follows. Each coordinate for Ck tk in the benchmark has the range [-42, 42] when the number of states is 21675. Starting at state ((42, \u201342), (1, 1)) means the tracker is very far behind the target, but it is also very far to the right of the target. Consider the corresponding"}, {"title": "3.6 Benchmark Performance with Alternative Sampling Strategies", "content": "Now we consider sampling strategies where only some of the states (image patches) of the benchmark are used to train the network. One possibility would be to focus training on initial states that have a suboptimal greedy policy. However, under an optimal policy states outside this set are visited. A better approach makes use of the findings in the previous section. Sampling from states with at least one non-negative coordinate for $c_k - t_k$ in the benchmark corresponds to training on the set of states where the tracker is not both behind and to the left of the target. Many of these states are independent of the set of states where the tracker is behind and to the left of the target: in this case the tracker never reaches the target. The result is a partition of the benchmark states. When states partition in this way, it is possible to focus training on the relevant partition to achieve optimal performance, as demonstrated in Fig 10.\nIn Fig 10, the benchmark is used with a \u00d74 sparse code leading to 1323 states (a small number of states is used here as it is easier to visualize graphically). Of these, 1031 states have at least one non-negative coordinate (i.e., 78% of the total number of states). These states are used as the training set. Fitted value iteration is then applied to find optimal policies for the 672 initial states with suboptimal greedy policies. Fig 10 shows these policies remain optimal policies under the proposed sampling strategy. However, decreasing the training set below 78% of the total number of states rapidly degrades performance as states from the relevant partition start to be left out of the training set. The benchmark provides an ideal testing ground to investigate these types of behaviours."}, {"title": "4 Discussion and Conclusions", "content": "This work provides three main new contributions to the reinforcement learning methodology for solving Markov decision processes and optimal control tasks.\nDefining an optimal control problem for a sequence of natural images requires understanding how the problem can be formulated, and what types of solution we can expect. We derived the general conditions for an image to be sufficient for implementing an optimal policy by drawing on an analogy with belief states in partially observed Markov decision processes. The first, and most obvious, condition is that an image must contain all task-dependent information required for the optimal control task at hand so that the cost or reward of each time period can be determined. The second, less obvious, condition is that we must have access to a mechanism for generating new images, where the next image is determined by the current image and the control we choose at the current time period. An image generator such as this"}, {"title": "Appendix A1: Method for generating overcomplete sparse codes", "content": "The method used here to generate overcomplete sparse codes of a natural images is taken from Loxley [2017, 2021], and starts with the (real-valued) two-dimensional (2D) Gabor function given by:\n\n$G(r,r') = A exp \\bigg(-\\frac{(i - x_0(r'))^2}{\\sigma_a(r')^2} + \\frac{(j - y_0(r'))^2}{\\sigma_v(r')^2} \\bigg) cos [k(r')j + \\phi(r')],$\n\nand\n\n$\\begin{pmatrix} i, j \\end{pmatrix} = \\begin{pmatrix} cos(\\theta(r')) & sin(\\theta(r')) \\\\ -sin(\\theta(r')) & cos(\\theta(r')) \\end{pmatrix} \\begin{pmatrix} i - x_0(r') \\\\ j - y_0(r') \\end{pmatrix},$\n\nwhere $k(r') = 2\\pi/\\lambda(r')$; and where r = (i, j) and r' = (i', j') are discrete two-dimensional coordinates. When the 2D Gabor function is adapted to natural image statistics, the three spatial Gabor parameters $\\sigma_a$, $\\sigma_v$, and $\\lambda$ are found to be strongly correlated and have heavy-tailed distributions [Loxley, 2017]. The joint probability density of these parameter values is approximated using the sampling scheme in Table 4, and described by a Gaussian copula with Pareto marginal distributions. Due to the Pareto marginal distributions the sampling scheme is length scale invariant. Scale invariance is a key property of natural images. The resulting set of randomly generated Gabor functions are therefore self-similar and multiscale in the same way as self-similar multiresolution wavelet schemes. All other Gabor parameters are sampled uniformly over their respective ranges.\nIn the first step, a sample is collected for each of the seven Gabor parameters ($\\Theta$,$\\varphi$, $\\sigma_a$, $\\sigma_v$, $\\lambda$, $x_0$, $y_0$), leading to a single 2D Gabor function indexed by a value of r'. This step is repeated m times; leading to m Gabor functions indexed by m unique values of r'. Two-dimensional Gabor functions are not orthogonal. However, given an image I(r) \u2208 $R^d$, it is possible to find its sparse code a(r') \u2208 $R^m$ using a least-squares approximation. Letting G\u2208 $R^{d\u00d7m}$"}, {"title": "Appendix A2: Infinite horizon policy evaluation", "content": "A stationary policy is a policy that does not change with each time period k", "equations": "n\n$J_{\\mu}(i) = g(i) + a \\sum_j p_{ij}(\\mu(i)) J_{\\mu}(j).$\n\nThe stationary optimal policy identified in Sec. 3.1 is given by $\\mu^*((0, 1), (0,0)) = (1,0)$, $\\mu^*((0,0), (1, 1)) = (0,1)$, and $\\mu^*((0,0), (0,1)) = (0,1)$. These states have a cost per time period given by g((0,1), (0,0)) = 1, g((0,0), (1, 1)) = 0, and g((0, 0), (0, 1)) = 0. After rearranging Eq. (22) as\n\n$\\sum_j (\\delta_{ij} \u2013 a p_{ij}(\\mu(i))) J_{\\mu}(j) = g(i),$\n\nthe linear system can be written as A$J_\\mu$ = g, where\n\nA =$\\begin{pmatrix}\n1 & -a & 0 \\\\ 0 & 1 & -a \\\\ -a(1-p) & 0 & 1 - ap\n\\end{pmatrix}$\n\nso that det (A) = 1 \u2212 ap \u2212 $a^3$(1 \u2212 p). For the stationary optimal policy, g is given by [1,0,0", "1,0,0": "is solved by\n\n$J^*_{\\mu} = \\begin{pmatrix}\n1 - ap \\\\ a^2(1-p) \\\\ a(1-p)\n\\end{pmatrix} \u00d7 (1 \u2013 ap \u2013 a^3(1 \u2212 p))^{-1}.$\n\nThe stationary greedy policy identified in Sec. 3.1 is given by $\\mu^G((0,0), (0,0)) = (1,0)$, $\\mu^G((0, \u22121), (1,1)) = (0,1)$, and $\\mu^G((0, \u22121), (0,1)) = (0,1)$. These states have a cost per time period given by g((0,0), (0,0)) = 0, g((0, -1), (1, 1)) = 1, and g((0, -1), (0,1)) = 1. Now g is given by [0,1,1", "1": "is solved by\n\n$J^G_{\\mu} = \\begin{pmatrix}\n a + a^2(1-p) \\\\ 1 + a(1-p) \\\\ 1 + a^2(1-p)\n\\end{pmatrix} \u00d7 (1 \u2013 ap \u2013 a^3(1 \u2212 p))^{-1}.$\n\nThe expected total cost ratio $J^G/J^*$, for the first greedy state $i^G = (0,0), (0,0)$, and the first optimal state $i^* = (0, 1), (0, 0), is\n\n$\\frac{J^G_{\\mu}(i^G)}{J^*_{\\mu}(i^*)} = a + \\frac{a^2}{1 - ap}.$\n\nTaking the limit of zero discounting (i.e., a \u2192 1"}]}