{"title": "PARTITIONING MESSAGE PASSING FOR GRAPH FRAUD DETECTION", "authors": ["Wei Zhuo", "Zemin Liu", "Bryan Hooi", "Bingsheng He", "Guang Tan", "Rizal Fathony", "Jia Chen"], "abstract": "Label imbalance and homophily-heterophily mixture are the fundamental problems encountered when applying Graph Neural Networks (GNNs) to Graph Fraud Detection (GFD) tasks. Existing GNN-based GFD models are designed to augment graph structure to accommodate the inductive bias of GNNs towards homophily, by excluding heterophilic neighbors during message passing. In our work, we argue that the key to applying GNNs for GFD is not to exclude but to distinguish neighbors with different labels. Grounded in this perspective, we introduce Partitioning Message Passing (PMP), an intuitive yet effective message passing paradigm expressly crafted for GFD. Specifically, in the neighbor aggregation stage of PMP, neighbors with different classes are aggregated with distinct node-specific aggregation functions. By this means, the center node can adaptively adjust the information aggregated from its heterophilic and homophilic neighbors, thus avoiding the model gradient being dominated by benign nodes which occupy the majority of the population. We theoretically establish a connection between the spatial formulation of PMP and spectral analysis to characterize that PMP operates an adaptive node-specific spectral graph filter, which demonstrates the capability of PMP to handle heterophily-homophily mixed graphs. Extensive experimental results show that PMP can significantly boost the performance on GFD tasks.", "sections": [{"title": "INTRODUCTION", "content": "With the explosive growth of online information, fraudulent activities have significantly increased in financial networks (Ngai et al., 2011; Lin et al., 2021), social media (Deng et al., 2022), review networks (Rayana & Akoglu, 2015), and academic networks (Cho et al., 2021), making the detection of such activities an area of paramount importance. To fully exploit the rich graph structures contained in fraud graphs, recent studies have increasingly adopted Graph Neural Networks (GNNs) (Wu et al., 2020) to Graph Fraud Detection (GFD).\nApplying message passing GNNs (Gilmer et al., 2017) in GFD encounters two significant challenges: label imbalance (Liu et al., 2023) and a mixture of heterophily and homophily (Gao et al., 2023a). Network attackers often employ sophisticated tactics to mimic regular network patterns, by strategically injecting a limited number of fraud nodes in the main contexts of the target graph to hide their fraudulent activities. The label imbalance problem causes the GNN to primarily capture the patterns and characteristics of benign nodes, compromising their ability to accurately identify fraudulent ones. Additionally, the heterophily-homophily mixture violates the homophily (Zhu et al., 2020) inductive bias of GNNs, as fraud nodes are often strategically placed within benign communities to exhibit heterophily, while the context around benign nodes exhibits homophily. Therefore, it is pivotal to develop GNNs that can navigate these challenges adeptly.\nTo alleviate these issues, several methods have been proposed from a spatial perspective to diminish the impact of heterophilic neighbors during the aggregation process. These strategies commonly involve utilizing a trainable approach or predetermining a mechanism to resample neighbors (Dou et al., 2020; Liu et al., 2021c; 2020) or reweight edges (Wang et al., 2019; Cui et al., 2020; Shi et al.,"}, {"title": "BACKGROUND AND MOTIVATION", "content": ""}, {"title": "2.1 BACKGROUND", "content": "Notations. A multi-relational attributed fraud graph is denoted as G = (V, {Er}=1, X, Y), where V = {v1,\u2026\u2026, vN} represents the node set with N nodes, Er represents the edge set under the"}, {"title": "2.2 MOTIVATION ANALYSIS", "content": "To probe the reasons why message passing based GNNs experience shortcomings in fraud detection tasks, our investigation is conducted through an analysis of the mutual influence between nodes resulting from the message passing process. Inspired by (Xu et al., 2018; Zhang et al., 2021), the influence of node vj on the center node vi can be quantified by measuring how alterations in the input feature of vj affect the representation of vi after k iterations of message passing. For any vi and its neighbor vj \u2208 N(vi), given the message passing form $H^{(k)} = \\hat{A}^kH^{(0)}W$ where H(0) = X, considering the h-th feature of X, the influence of vj on the final representation of v\u2081 is defined as:\n$I(k)_{ij} = \\frac{\\partial H^{(k)}_{ih}}{\\partial X^{(0)}_{jh}} \\mid_{h \\in \\{1,...,d\\}} = A^k\\gamma W$. \nSince the gradient is independent of the feature dimension h (Zhang et al., 2021), the final result omits the h. In a fraud graph characterized by imbalanced label distribution, let m represent the number of benign neighbors $N_{be}(v_i)$ of node vi, and n represents the number of fraud neighbors $N_{fr}(v_i)$. In this context, m \u226b n. Since we specifically study the class imbalance problem, to rule out potential interference from other variables, we assume the graph to be regular. Consequently, all non-zero off-diagonal entries in $\\hat{A}^k$ are assumed to be equal, and this common value is denoted by \u03b3. The total influence of benign neighbors on the center node vi is $I_{N_{be}(v_i) \\rightarrow v_i} = \\Sigma_{v_j \\in N_{be}(v_i)} A^k \\gamma W = mW$, where we use \u03b3 to scale the influence score. Similarly, for fraud neighbors we have $I_{N_{fr}(v_i) \\rightarrow v_i} = nW$. Then, the total influence from neighbors is given by (m + n)W \u2248 $I_{N_{be}(v_i) \\rightarrow v_i}$, which tends to over-amplify the influence from the majority class neighbors (benign) while neglecting that from the minority class neighbors (fraudulent). Such an imbalance in influence can skew the message passing process, causing it to be insufficiently responsive to the nuances of the minority class nodes. As a result, the capacity of the network to capture critical features from the minority class neighbors diminishes, potentially undermining its effectiveness in fraud detection especially when the graph exhibits heterophily.\nTo address the aforementioned problems, many existing GNN-based fraud detection models adopt strategies such as re-weighting neighbors from different classes using predefined indicators (Liu et al., 2021c) or learnable attention values (Wang et al., 2019; Liu et al., 2021a; Shi et al., 2022; Cui et al., 2020). Additionally, they often incorporate a learnable sampler to selectively focus on potential neighbors belonging to the same class as the center node (Dou et al., 2020; Liu et al., 2020). These works typically utilize additional modules to augment the graph structure, aiming to homogenize contextual information in graphs. The essential goal of these methods is to modify the graph to exclude heterophilic neighbors (i.e., neighbors that belong to different classes than the center node), thereby tailoring the graph topology for more effective processing by GNNs in fraud detection tasks. However, augmenting the graph structure often leads to high time and memory complexity, limiting scalability to large graphs. In our work, we argue that adapting the graph to fit GNNs is both costly and unnecessary, but keeping the graph fixed and modifying the GNN model to fit the graph can be more efficient and effective.\nUpon observing the formation of the total influence (m + n)W, we find that both fraud and benign neighbors are weighted equally with W, which leads to benign neighbors dominating the gradient of W during the backpropagation. It naturally motivates us to ask whether handling neighbors of two distinct classes with separate weight matrices might allow for adaptive adjustment in their influence on the center node, i.e., mW1 + nW2. Such an approach may effectively mitigate issues related to label imbalance and heterophily, without doing any operations on the graph itself."}, {"title": "3 PARTITIONING MESSAGE PASSING", "content": "Our preliminary analysis in Section 2.2 reveals a relationship between parameter sharing within GNNs and biases in the learning process. In this section, we formally present our method, Partitioning Message Passing (PMP), which is a simple, intuitive, yet powerful approach tailored for the fraud detection task. The basic idea of PMP is to utilize the label information to partition the message passing process, enabling the model to distinguish neighbors according to their classes by learning different weights for each class during message passing, thereby enhancing its ability to adaptively adjust the influence propagated from class-imbalanced neighboring nodes. As shown in Fig. 1b, the"}, {"title": "4 THEORETICAL INSIGHTS", "content": "As mentioned in Section 2.1, graphs with fraud usually exhibit a heterophily-homophily mixture, as attackers would like to sparsely inject a limited number of fraud nodes into benign communities to camouflage their activities and spread influence. For regions exhibiting homophily, where the center node and its neighbors predominantly share the same label, the desired GNN should act as a loss-pass filter to smooth the feature representations in the locality. Conversely, in heterophilic regions, where the center node's label diverges from most of its neighbors, the GNN must adaptively shift its spectral response to capture such contrasting label information. Some work (Yang et al., 2022a; Wang &\nZhang, 2022) show that assigning each feature dimension a separate spectral filter improves the performance of GNNs. Differently, our model achieves adaptivity at the node level, with each node being assigned a separate spectral filter. Specifically, we present the following theorem.\nTheorem 1. Consider an undirected graph G, let L = $U\\Lambda U^T$ represent the eigendecomposition of the symmetric normalized Laplacian L = I \u2013 $D^{-1/2}AD^{-1/2}$, where U is the matrix of eigenvectors and $\\Lambda$ = diag([$\\lambda_i$]$_{i=1...N}$) is the diagonal matrix of eigenvalues and 0 = $\\lambda_1$ \u2264 \u2026 < $\\lambda_N$ \u2264 2. Given two sets of d' graph signals XWfr and XWbe, PMP scheme described in Eq. (3) operates as an adaptive graph filter, with the node-specific spectral convolution for node vi on XWfr and XWbe is given as:\nH(vi) = gfr(L)XWfr + gbe(L)XWbe = $Ug_{fr}(\\Lambda)U^\\top XW_{fr} + Ug_{be}(\\Lambda)U^\\top XW_{be}$,\nwhere the spectral convolution filters are diagonal matrices defined as:\n$g_{fr}(\\Lambda)[j, j] = \\begin{cases} 1 - \\lambda_i  & v_j \\in N_{fr}(V_i)\\\\ a_i(1 - \\lambda_j) & v_j \\in N_{be}(V_i)\\\\ a_i & otherwise \\end{cases}$ $g_{be}(\\Lambda)[j, j] = \\begin{cases} 1 - \\lambda_i  & v_j \\in N_{fr}(V_i)\\\\ (1 - a_i)(1 - \\lambda_j) & v_j \\in N_{be}(V_i).\\\\ (1 - a_i) & otherwise \\end{cases}$ \nwhere Nfr(vi) and Nbe(vi) are respectively the fraud neighbors and benign neighbors of vi in the training set. The i-th row of the matrix H(vi), i.e., H(vi)[i, :], is the representation of vi.\nWe provide a proof in Appendix B. The inherent mixed homophily-heterophily characteristics of fraud graphs underline the importance of utilizing node-specific adaptive filters. As different center nodes locate in diverse contexts within the graph, they each reflect distinct degrees of homophily or heterophily. This variability implies that a one-size-fits-all approach, using a universal graph filter across all nodes (Defferrard et al., 2016; Tang et al., 2022; Zhuo & Tan, 2022a), is not only suboptimal but could lead to inaccuracies in fraud detection. PMP ensures that the filters are tailored"}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 EXPERIMENTAL SETUP", "content": "Datasets and Baselines. We evaluate our approach using five datasets tailored for GFD: Yelp (Rayana & Akoglu, 2015), Amazon (McAuley & Leskovec, 2013), T-Finance, T-Social (Tang et al., 2022). Besides, we have evaluated our model on a large-scale real graph from our industry partner, Grab. The statistics of these datasets are provided in Table 4 of Appendix D. We compare our model against 11 state-of-the-art approaches, including generic GNNs: GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GAT (Veli\u010dkovi\u0107 et al., 2018); beyond homophily GNNs: GPRGNN (Chien et al., 2021) and FAGCN (Bo et al., 2021); fraud detection tailored GNNs: Care-GNN (Dou et al., 2020), PC-GNN (Liu et al., 2021c), H2-FDetector (Shi et al., 2022), BWGNN (Tang et al., 2022), GHRN (Gao et al., 2023c), and GDN (Gao et al., 2023b).\nMetrics and Implementation Details. We employ three commonly used metrics for imbalanced classification in deep learning evaluations: AUC, F1-Macro and G-Mean. We provide a detailed explanation of each metric in Appendix D.3. Following (Tang et al., 2022), we adopt the data splitting ratios of 40%:20%:40% for training, validation, and test set in the supervised scenario. In the semi-supervised scenario, the data splitting ratio is 1%:10%:89%. For consistency in our evaluations, each model underwent 10 trials with varied random seeds. We present the average performance"}, {"title": "5.2 PERFORMANCE COMPARISON", "content": "For public datasets, results derived from the supervised setting can be found in Tables 1 and 2, while those from the semi-supervised setting are detailed in Tables 6 and 7 of Appendix E.1. For the Grab dataset, results are shown in Appendix E.2. The results demonstrate that PMP consistently surpasses baseline performances across almost all datasets and metrics. One explanation for its enhanced performance over generic GNNs\u2014where the learnable weights of the aggregation function are uniformly applied across all neighbors\u2014is that PMP encodes class-specific discriminative information into the model parameters. This is achieved by distinguishing neighbors of distinct classes during the message passing phase. Besides, the adaptive modulation between Wfr and Wbe allows nodes to judiciously calibrate the information flow from distinct classes of neighbors. By segregating the processing of neighbors based on their labels, our model can adaptively emphasize and give importance to rare patterns, which is vital in imbalanced and heterophilic scenarios. Among generic GNNs, GraphSAGE exhibits better performance due to its separation between ego- and neighbor embeddings, which is beneficial when learning under heterophily, and our model also inherits such design.\nIn comparison to the six GNNs tailored for GFD, our model also demonstrates a markedly superior performance. For instance, on Yelp, our model shows improvements of 3.4% in AUC, 4.42% in F1-Macro, and 3.08% in G-Mean. Interestingly, our evaluation reveals that GraphSAGE, despite its fundamental design, serves as a potent baseline, even surpassing some models specifically crafted for GFD. Many of these specialized baselines incorporate intricate preprocessing steps rooted in feature engineering (e.g., GHRN) or employ learnable edge reweighting/sampling techniques (e.g., H2-FDetector) for graph augmentation. Such findings suggest that elaborate manipulations to tailor the graph structure for the model might be superfluous. Instead, adapting the model to align better with the inherent graph characteristics could prove to be a more effective strategy. Our model presented in Eq. (3) can be seen as a variant of GraphSAGE. Notably, relative to GraphSAGE, our model exhibits substantial performance enhancements, underscoring the efficacy of the proposed partitioning message passing strategy. In Fig. 3, we show that our model achieves the optimal trade-off between inference speed and effectiveness in comparison to baselines. PMP follows the iterative aggregation framework of GNNs, thus the number of layers and the dimension of hidden layers are two core parameters that determine the receptive field and representation power of the model. Thus, we analyze the sensitivity of the model to these two parameters in Appendix D.5."}, {"title": "5.3 HOW DOES PMP SOLVE THE PROBLEMS OF HETEROPHILY AND LABEL IMBALANCE?", "content": "To answer this question, we investigate the influence of neighbors with different labels on the center fraud nodes. Given a center fraud node vi, we measure the influence of its fraud neighbors on the final representation of vi as $I_f(v_i) = \\Sigma_{v_j \\in N_f(v_i)} Z_j Z^T_i$, where Zi = $H^{(L)}_i$ is the output representation of the last layer. Correspondingly, the influence of benign neighbors is captured by"}, {"title": "5.4 ABLATION STUDY", "content": "As shown in Eq. (3), the main difference between PMP and GraphSAGE rests in their aggregation methods: while GraphSAGE employs a uniform aggregation with shared feature transformations for all neighbors, PMP distinctly partitions message passing, applying varied feature transformations contingent upon neighbor labels. The results in Section 5.2 show our model consistently outperforms GraphSAGE across all datasets and evaluation metrics, which demonstrates that such a simple design can significantly enhance GNN performance in GFD.\nAdditionally, we delve deeper to validate two pivotal components of our model: the adaptive blending of unlabeled neighbors as a weighted fusion of labeled data, as shown in Eq. (4), and the root-specific weight matrices in Eq. (5). Table 3 shows the results, where we employ GraphSAGE as the benchmark model, given its conceptual proximity to our proposal, to methodically illustrate the incremental benefits introduced by our design choices. \"+partition\" denotes adopting distinct weight matrices for labeled fraud benign neighbors, while for unlabeled neighbors, a separate, independent weight matrix is employed. Subsequently, \"++adaptive combination\u201d denotes that the weight matrix for unlabeled neighbors is treated as an adaptive combination of the weights of labeled neighbors as introduced in Eq. (4). \"+++root-specific weights\u201d implies that the weight matrices for fraud and benign neighbors are dynamically generated as functions of the center node as presented in Eq. (5).\nFrom our evaluations, it is evident that GraphSAGE lags behind in performance across all metrics. This underscores the inherent limitation of uniformly aggregating information from all neighbors during the message passing process. Such an approach, while general and versatile, might miss out on capturing the relationships and contextual dependencies intrinsic to GFD. The \u201c+partition\u201d contributes to major improvements, which affirms that the core feature of our model, distinguishing between different classes of neighbors during message passing, stands as a pivotal mechanism for enhancing GNNs' efficacy in GFD. Besides, the subsequent extensions, \u201cadaptive combination\u201d and \"root-specific weights\u201d, also contribute positively to the overall performance. Specifically, the \"++adaptive combination\u201d, building upon the \"+partition\u201d, brings incremental improvements across all datasets. The \"+++root-specific weights\u201d, further extending the preceding component, enhances performance on most metrics in most datasets. Combined, these two components contribute to a cumulative improvement of more than 1% over the \u201c+partition\u201d baseline.\""}, {"title": "6 CONCLUSIONS", "content": "This work presents a simple yet effective GNN framework, PMP, for fraud detection task. We propose that the key of applying GNNs on fraud detection is to distinguish neighbors with different labels during message passing. With this insight, neighbor aggregation is partitioned based on labels using distinct node-specific aggregation functions. In this way, the center node can adaptively adjust the information propagated from its homophilic and heterophilic neighbors, thus avoiding model overfitting the majority class nodes (i.e., benign), and alleviating the problem of heterophily. The theoretical analysis of PMP demonstrates that PMP learns an adaptive spectral filter for each node separately. Extensive experiments show that our model achieves new state-of-the-art results on several public GFD datasets, verifying the power of partitioning message passing for GFD."}, {"title": "A ALGORITHMIC DETAILS", "content": "Algorithm 1 PMP forward propatation\nInput: Fraud graph G = (V, Er, X, Y); Depth L; Batch size B;\nOutput: Logits Z\u2208 RN\n1: for l\u2208 {1,\u2026\u2026,L} do\n2:    for each batch C G of size B do\n3:        for vi \u2208 batch do\n4:           $h_i^{(l)}  \\leftarrow f_{self}^{(l-1)}(h_i^{(l-1)})$ \n5:           $\\alpha_i^{(l)}  \\leftarrow \\Psi(\\varphi(h_i^{(l-1)}))$\n6:          $W_{fr,i}^{(l-1)} \\leftarrow f_{fr}(\\varphi(h_i^{(l-1)}))$; $W_{be,i}^{(l-1)} \\leftarrow \\Psi_{be}(\\varphi(h_i^{(l-1)}))$; $W_{un,i}^{(l)}  \\leftarrow \\alpha_i^{(l)}W_{fr}^{(l-1)}  + (1 - \\alpha_i^{(l)})W_{be}^{(l-1)} \\ $ // Generate weight matrices for vi\n7:           $a_i^{(l)}  = \\Sigma(f_{fr,i}^{(l-1)}(h_j^{(l-1)})\\mid v_j \\in N_{fr}(v_i)) + \\Sigma(f_{be,i}^{(l-1)}(h_j^{(l-1)})\\mid v_j \\in N_{be}(v_i)) + f_{un,i}^{(l-1)}(h_j^{(l-1)} \\mid v_j  \\notin N_{be}(v_i)UN_{fr}(v_i)) $ // $f_{fr,i}^{(l-1)}(h_j^{(l-1)})$,$f_{be,i}^{(l-1)}(h_j^{(l-1)})$ and $f_{un,i}^{(l-1)}(h_j^{(l-1)})$ are parameterized by $W_{fr,i}^{(l-1)}$, $W_{be,i}^{(l-1)}$ and $W_{un,i}^{(l-1)}$ respectively.\n8:          $h_i^{(l)}  \\leftarrow h_i^{(l)} + a_i^{(l)} $ \n9:      end for\n10:  end for\n11: end for\n12: H = MLP(H(L)) \u2208 RN\u00d71\n13: Zi = Sigmoid(H)\n14: L = \u2211i (yi log(Zi) + (1 \u2212 yi) log(1 \u2013 Z\u017c)) // Cross-entropy loss\nTime Complexity The time complexity of an L-layer PMP propagation is O(LNd + L|E|d\u00b2). For the adaptive combination of unlabeled neighbors, a single-layer MLP is employed, resulting in a complexity of O(Nd). Additionally, generating the root-specific weight matrix carries a time complexity of O(Nd\u00b2). Thus, the aggregate time complexity of the PMP can be summarized as O(LNd + (L|E| + N)d\u00b2)."}, {"title": "B PROOF OF THEOREM 1", "content": "Proof. Assuming the node indices are fixed, let F, B be diagonal label mask matrices, which mask benign nodes and fraud nodes respectively with 0 in the main diagonal elements. Specifically, for a node vi labeled 1 (fraud) in the training set, we assign Fii = 1, otherwise 0. Similarly, for training benign nodes, Bii = 1 otherwise 0. Then the mask matrix of unlabeled nodes is thus I \u2013 F \u2013 B. For the sake of simplicity, we analyze the first layer of PMP as an example and omit superscripts. Specifically, the feature transformation step of Eq. (3) can be reformulated as:\nFXWfr + BXWbe + (I \u2013 F \u2013 B)X (\u03b1\u00bfWfr + (1 - ai)Wbe)\n= (F + \u03b1\u00bfI \u2013 \u03b1\u00bfF \u2013 a\u00bfB) XWft + (B + (1 \u2212 a\u00bf)I \u2013 (1 \u2212 a\u2081)B \u2013 (1 \u2013 \u0430\u017c)B) XWbe.\nLet K(vi) = F + a\u00a1I \u2013 a\u00bfF \u2013 a\u00bfB, Eq. (8) can be rewritten as:\nK(V)XWfr + (I \u2013 K(Vi)) XWbe\nwhere K(vi) [j, j] = $\\begin{cases} 1 & v_j \\in N_{fr}(V_i)\\\\ a_i & v_j \\in N_{be}(V_i).\\\\ A_i & otherwise \\end{cases}$\nTo facilitate the spectral analysis, we align the definition of the graph convolution with GCNS (Kipf & Welling, 2017), using the normalized adjacency matrix, $D^{-1/2}AD^{-1/2} = I \u2013 L$. Note that although our model utilizes summation as the convolution method, this doesn't alter its inherent"}, {"title": "C RELATION OF PMP TO ACM", "content": "The basic idea of Adaptive Channel Mixture (ACM) (Luan et al., 2021) is to attribute negative weights to heterophilic neighbors through a spectral synthesis of low-pass and high-pass filters. This technique fundamentally differs from our PMP. It is because the use of negative weights in ACM is indicative of removing (or subtracting) the heterophilic components from the homophilic neighbors. This can be interpreted as a form of information \"forgetting\" where heterophilic features are de-emphasized in favor of homophilic features. It is different from PMP which preserves and utilizes all available information. From the perspective of methodology, ACM is rooted in spectral graph theory, while PMP is spatially oriented. This difference is not just theoretical but has practical implications. The spectral nature of ACM inherently influences its scalability, particularly in the context of large graphs. Typically, spectral-based models, including ACM, require the entire graph as input, which can pose challenges for mini-batch training and thereby limit scalability. In contrast, our PMP model, with its spatial-based framework, inherently supports mini-batch training and is thus more adaptable to large-scale graphs. Furthermore, considering the application of ACM in GFD tasks, its formulation, with removed nonlinearity for simplicity's sake, can be expressed as\n$H_{ACM}^{(1+1)} = \\begin{bmatrix} \\\\ AFL  H_{fr}^{(1)} \\\\ BFH H_{be}^{(1)} \\end{bmatrix} W_1 + \\begin{bmatrix} \\ H_{un}^{(1)} \\end{bmatrix}W_2$\nWhere FL and FH are distinct spectral filters. We can observe that the trainable weight matrix WL1 and $W_L^F$ are both shared across all nodes. In contrast, our PMP model, when represented in matrix form, can be rewritten as\n$H_{PMP}^{(1+1)} = F \\begin{bmatrix} H_{fr}^{(1)} \\end{bmatrix}W_1 \\\\ H_{un}^{(1)}(aW_1+(1-a)W_2) \\\\ HW2\\$\nwhere F is the normalized adjacency matrix. Unlike ACM, trainable weight matrices in PMP, W\u2081 and W2, are specifically applied to nodes with different labels, where W\u2081 and W2 capture the information of fraud nodes and benign nodes respectively, reflecting a more label-aware and adaptive approach to message passing in the context of GFD."}, {"title": "D EXPERIMENTAL DETAILS", "content": ""}, {"title": "D.1 DATASETS", "content": "Table 4 summarizes the dataset statistics, including the number of nodes, edges, and relations, the proportion of fraud nodes, feature dimension, and the homophily score. The imbalance-aware homophily score (Lim et al., 2021) is defined as:\n$\\hat{\\eta} = \\frac{1}{C-1} \\Sigma_{k=0}^{C-1} [\\frac{N_k}{N} + (\\frac{c_k}{C})]^+$,\nwhere []+ = max(, 0). C, representing the number of classes, is set to 2 for GFD. Ck denotes the set of nodes in class k, with k = 0 corresponding to benign nodes and k = 1 to fraud nodes. nk is the"}, {"title": "D.2 LABEL DISTRIBUTION", "content": "In Fig. 5, we show the label distributions of the labeled neighborhoods of training nodes. We use $\\frac{|N_{fr}|}{|N_{be}|}$  to represent the number of fraud neighbors relative to benign neighbors for a central node. Specifically, if $\\frac{N_{fr}}{N_{be}}$ < 1 indicates that the number of labeled fraud neighbors is fewer than benign neighbors. Specifically, if $\\frac{N_{fr}}{N_{be}} < 0.5$, it indicates that the central node has far fewer fraud neighbors compared to benign ones, denoting a highly imbalanced neighborhood. The height of each bar means the number of center training nodes under a certain $\\frac{N_{fr}}{N_{be}}$. Our results show that commonly used datasets such as Yelp, Amazon, and T-Finance suffer from significant label imbalance in the neighborhoods, because the neighborhoods' label distribution is long-tail, i.e., the number of fraud neighbors is far less than benign ones. It requires the model can enhance the influence from the minority class neighbors on the center nodes during neighbor aggregation. Combining with the empirical results in Section 5.3, our PMP method effectively achieves this, enhancing the influence from minority class neighbors more efficiently than traditional GNN approaches."}, {"title": "D.3 METRICS", "content": "AUC is the area under the ROC curve, and it provides an aggregate measure of performance across all possible classification thresholds, reflecting the model's ability to distinguish between positive and negative classes. F1-Macro computes the F1 score for each class independently and then takes the average. Lastly, the G-Mean, or geometric mean, calculates the square root of the product of the sensitivity and specificity, offering an insight into the balance between true positive rate and"}, {"title": "D.4 HYPERPARAMETER SETTINGS", "content": "To mitigate bias, we individually fine-tune the hyperparameters of each model for every benchmark and report the best performance on the validation set. For each model, we explored the following searching ranges for general hyperparameters: learning rate lr \u2208 {0.01,0.005,0.001}, weight decay wd \u2208 {0, 5e \u2013 5, 1e \u2013 4}, dropout do \u2208 {0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}, hidden dimension d' \u2208 {32, 64, 128, 256, 512}. For spatial-based models, batch size is an important hyperparameter that highly depends on the graph size. Specifically, for Yelp, Amazon, and T-Finance, batch size bs \u2208 {64, 128, 256, 512, 1024}. For the large-scale graph T-Social, bs \u2208 {217, 218, 219}. For model-specific hyperparameters, we also carefully calibrate parameters in accordance with varying datasets and training sizes. Here we provide the optimal hyperparameters of PMP in Table 5."}, {"title": "D.5 PARAMETER STUDY", "content": "We investigate the sensitivity in relation to the key hyperparameters in our model: the number of layers L and the hidden dimension d'. For each dataset, we vary L over the range [1, 2, 3], and d' over [32, 64, 128, 256, 512]. We employed a grid search methodology to test combinations of hyperparameters. As illustrated in Fig. 6, PMP consistently performs optimally with a single layer, and more layers bring about a continuous decrease in effect. This phenomenon can be attributed to the issue of oversmoothing. We find that all these datasets have a high average degree. Specifically, the average degree of Yelp, Amazon and T-Finance are 167, 740, and 1078, respectively, and we further investigate the number of nodes within the first two hops of neighborhoods. Eliminating duplication, the average number of nodes in the first two-hop neighborhoods across all nodes stands at 1229 for Yelp, 11338 for Amazon, and 24480 for T-Finance. The remarkable density of these benchmark datasets implies that even a two-layer GNN might aggregate an overwhelming quantum of information from the global, thereby obfuscating the essential local features. Additionally, Furthermore, our findings reveal that the optimal value of d' varies across datasets. This calls for meticulous tuning to ascertain peak performance. A potential rationale for this sensitivity lies in the substantial variability in the dimensions of input node features across different datasets."}, {"title": "D.6 COMPUTING RESOURCES", "content": "For all experiments, we use a single NVIDIA A100 GPU with 80GB GPU memory."}, {"title": "E MORE EXPERIMENTAL RESULTS", "content": ""}, {"title": "E.1 SEMI-SUPERVISED FRAUD DETECTION ON PUBLIC DATASETS", "content": "In Table 6 and Table 7, we show the performance of fraud detection under a semi-supervised setting (1% training ratio)."}, {"title": "E.2 EXPERIMENTAL RESULTS ON GRAB", "content": "In Table 8 and Table 9, we show the fraud detection performance on the Grab dataset under supervised and semi-supervised settings."}, {"title": "E.3 COMPARISON WITH R-GCN", "content": "As an extension of the comparative analysis presented in Fig. 4, we include R-GCN due to its ability to handle multiple types of relationships within a graph. R-GCN (Schlichtkrull et al., 2018) extends"}]}