{"title": "AEMLO: AutoEncoder-Guided Multi-Label Oversampling", "authors": ["Ao Zhou", "Bin Liu", "Jin Wang", "Kaiwei Sun", "Kelin Liu"], "abstract": "Class imbalance significantly impacts the performance of multi-label classifiers. Oversampling is one of the most popular approaches, as it augments instances associated with less frequent labels to balance the class distribution. Existing oversampling methods generate feature vectors of synthetic samples through replication or linear interpolation and assign labels through neighborhood information. Linear interpolation typically generates new samples between existing data points, which may result in insufficient diversity of synthesized samples and further lead to the overfitting issue. Deep learning-based methods, such as AutoEncoders, have been proposed to generate more diverse and complex synthetic samples, achieving excellent performance on imbalanced binary or multi-class datasets. In this study, we introduce AEMLO, an AutoEncoder-guided Oversampling technique specifically designed for tackling imbalanced multi-label data. AEMLO is built upon two fundamental components. The first is an encoder-decoder architecture that enables the model to encode input data into a low-dimensional feature space, learn its latent representations, and then reconstruct it back to its original dimension, thus applying to the generation of new data. The second is an objective function tailored to optimize the sampling task for multi-label scenarios. We show that AEMLO outperforms the existing state-of-the-art methods with extensive empirical studies.", "sections": [{"title": "1 Introduction", "content": "In the field of multi-label classification (MLC), each instance can belong to multiple labels simultaneously. MLC is widely used in various fields, including image annotation[4], sound processing [16], biology [34] and text classification [14]. The issue of class imbalance in multi-label classification has gained prominence recently [28]. It is prevalent in real-world MLC problems and significantly affects classifier performance, as many algorithms assume data is balanced. Imbalanced datasets tend to bias learners towards majority labels [29]."}, {"title": "1.1 Research Goal", "content": "Our goal is to address the class imbalance in multi-label datasets through the integration of a deep generative model within an encoder-decoder architecture. This strategy seeks to outperform conventional methods, such as sampling with linear interpolation or random replication, by dynamically creating instances that contain richer feature information."}, {"title": "1.2 Motivation", "content": "In recent years, innovative approaches have been developed to tackle the issue of imbalance in multi-label learning [28], including sampling methods [6,17], classifier adaption [9], and ensemble techniques [18,27]. Sampling methods, in particular, aim to balance the dataset before the training phase, offering flexibility and compatibility with any multi-label classifier. To ensure effective sampling, several studies have concentrated on identifying specific samples and refining decision boundaries. For example, MLSOL [17] assigns a higher selecting probability to the sample suffering severe local imbalance. MLBOTE [29] refines the boundary samples related to high imbalance labels and employs different sampling strategies. Traditional oversampling techniques often rely on basic linear interpolation or replication for creating feature vectors of synthesized samples, with label vectors typically generated through majority voting or replication.\nThe Autoencoder (AE) and Generative Adversarial Network (GAN), as exemplary generative models, have shown substantial potential in data generation, restoration, and augmentation [15,12,19,22,8]. An Autoencoder compresses data into a latent space using an encoder and reconstructs it by a decoder. Its objective is to minimize reconstruction errors, enabling efficient feature extraction and noise reduction [13,15]. Although Autoencoder and GAN are used to address the imbalance problem and generate minority samples, they primarily cater to single-label datasets and face several challenges when applied to multi-label datasets. First, AE and GAN require training samples with identical labels (same class in the single-label dataset or same label set in the multi-label dataset). However, in multi-label data, the number of samples with a complete label set is often too limited to effectively train deep learning models. Secondly, although multi-label datasets can be divided into several binary datasets via the One vs All strategy, For each binary dataset, we can learn and reconstruct new feature vectors for multi-label data through end-to-end models, but we can not determine appropriate complete label set for each feature vector."}, {"title": "1.3 Summary", "content": "In this work, we introduce an innovative approach crafted to tackle the class imbalance issue in multi-label datasets named AutoEncoder-guided Multi-Label Oversampling (AEMLO). The core of AEMLO's design lies in two essential elements:"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 Multi-Label Classification", "content": "Formally, let $X \\in \\mathbb{R}^d$ represent the d-dimensional feature space, and let $L = {l_1,l_2,...,l_q}$ denote a set of q predefined labels. In multi-label classification, our objective is to construct a mapping function $h : X \\rightarrow L$ based on a given multi-label training dataset $D = {(x_i,y_i)}_{i=1}^n$, where each sample $x \\in X$ is associated with a binary label vector $y_i \\in {0,1}^q$. Here, $y_i$ is a binary vector where each element denotes whether the associated label from $L$ is relevant (1) or not relevant (0) to $x_i$.\nIn Multi-Label Classification (MLC), methods are split into three types based on how they handle label correlations. First-order strategies like MLkNN [35] and BR [3] treat labels independently, offering simplicity and efficiency. Second-order methods, such as CLR [11], analyze pairwise label correlations for improved interaction understanding. For complex scenarios with intricate label relationships, high-order methods, like RAkEL [31] and ECC [23], are more effective. RAkEL tackles this by dividing labels into subsets for diverse interaction modeling. ECC sequentially links classifiers, allowing each to learn from the predictions of its predecessors."}, {"title": "2.2 Multi-Label Imbalance Learning", "content": "Let $N_l (N_l^0)$ denote the number of instances with \"1\" (\"0\") class of label $l_j$. $IRlbl_j$ and $ImR_j$ are the two measures to evaluate the imbalance level of individual labels\n$IRlbl_j = N_{max}/N_l$   $ImR_j = max(N_l^1, N_l^0)/min(N_l^1, N_l^0)$  (1)\nThe larger the $IRlbl_j$ and $ImR_j$, the higher the imbalance level of $l_j$. Then, $MeanIR$ calculates the average imbalance ratio ($IRlbl_j$) across all labels in a dataset, defined by: $\\frac{1}{q} \\sum_{j=1}^{q}IRlbl_j$, where q is the total number of labels. The higher $MeanIR$, the imbalance of the dataset. By considering the $IRlbl_j$ and $MeanIR$, we can calculate imbalance indicators such as the coefficient of variation of $IRlbl$ ($CVIR$) and concurrency level ($SCUMBLE$) [28].\nThe imbalanced approaches proposed for MLC can be divided into three categories: sampling methods, classifier adaptation [9,32,33], and ensemble approaches [27,18]. Compared to the other two methods, the sampling method is more universal, as it creates (deletes) instances related to minority (majority) labels to construct a balanced training set that can be used to train any classifier without suffering from bias. Sampling methods involve undersampling and oversampling techniques. Undersampling reduces the presence of majority labels by either randomly removing instances or employing heuristic approaches to selectively eliminate samples. For example, LPRUS and MLRUS [25] aim to alleviate imbalances by respectively targeting the most frequent label sets or individual labels for removal. Conversely, oversampling techniques such as LPROS and the MLSMOTE [6] focus on augmenting the dataset with new instances associated with minority labels, either through duplication or the generation of synthetic samples. Recent developments include the REMEDIAL [5] method, which adjusts label and feature spaces to lessen label co-occurrence and improve sampling. Integrating this method with techniques such as MLSMOTE can further optimize dataset balancing [7]. MLSOL [17] specifically generates instances to focus on local imbalances in datasets. On the other hand, MLTL [21] refines datasets by removing instances that obscure class boundaries, Another notable method, MLBOTE [29], categorizes instances based on their boundary characteristics and applies different sampling rates."}, {"title": "2.3 Deep Sampling Method", "content": "Traditional sampling techniques struggle to effectively expand the training set for complex models. This has sparked interest in generative models and their potential to mimic oversampling strategies [10,2]. Utilizing an encoder-decoder setup, artificial instances can be effectively introduced into an embedding space. AE [13,15] and GAN [12] have been effectively employed to capture the underlying distribution of data and further applied to generate data for oversampling purposes. AE is designed to learn efficient data codings in an unsupervised manner. Essentially, they aim to capture the most salient features of the data by compressing the input into a lower-dimensional latent space and then reconstructing it back to the original dimensionality. The core objective of an AE is defined by the reconstruction error, which quantifies the difference between the original data and its reconstruction. Unlike Variational Autoencoder (VAE), which"}, {"title": "3 Multi-Label AutoEncoder Oversampling", "content": ""}, {"title": "3.1 Method Description and Overview", "content": "The multi-label AutoEncoder oversampling framework, as described in Algorithm 1, is divided into the training process and the instance generation phase.\nIn the training process, as shown in Figure 2, the model is designed to learn and optimize four distinct mapping functions: the feature encoding function $F_{ex}$, label encoding function $F_{ey}$, feature decoding function $F_{dx}$, and label decoding function $F_{dy}$. The model is trained end-to-end with mini-batches and the Adam optimizer, where batch size n encompasses the feature vector $x_i$ and binary label vector $y_i$ of the i-th sample, respectively. The matrices $X$ and $Y$ aggregate the input features and labels for all samples in the batch. The framework ingests a feature matrix $X$ and its corresponding label matrix $Y$, aiming to output reconstructed versions of $X'$ and $Y'$. Meanwhile, The other goal of our model is to identify an optimal latent space $L$, where the Deep Canonical Correlation Analysis (DCCA) component [1] enhances the correlation between $X$ and $Y$. Therefore, the model's objective function is defined as:\n$\\Theta \\underset{F_{ex}, F_{ey}, F_{dx}, F_{dy}}{min} \\Phi(F_{ex}, F_{ey}) + \\alpha\\Psi(F_{ex}, F_{dx}) + \\beta\\Gamma(F_{ey}, F_{dy})$ (2)\nwhere $\\Phi(F_{ex}, F_{ey})$ denotes the latent space loss, $\\Psi(F_{ex}, F_{dx})$ and $\\beta\\Gamma(F_{ey}, F_{dy})$ signify the reconstruction losses. Here, $\\alpha$ and $\\beta$ serve to balance these components, respectively. In section 3.2, we will explain every term of the objective function in details. At the end of each epoch, we enter a validation phase, adjusting the threshold for binary label conversion by maximizing the F-measure of each label on the validation set."}, {"title": "3.2 Loss Function", "content": "Joint Embedding To calculate $\\Phi(F_{ex}, F_{ey})$ defined in Eq.2, we employ the DCCA to embed features and labels into a shared latent space simultaneously and rewrite the correlation-based $\\Phi(F_{ex}, F_{ey})$ as the following deep version:\n$\\Phi(F_{ex}(X), F_{ey} (Y)) = ||F_{ex}(X) \u2013 F_{ey}(Y)||_F = Tr(C_1^TC_1) + Tr(C_2^TC_2 + C_3^TC_3)$ (3)\nwhere\n$C_1 = F_{ex} (X) - F_{ey}(Y)$,\n$C_2 = F_{ex}^T (X)F_{ex}(X) - I$, (4)\n$C_3 = F_{ey}^T (Y)F_{ey}(Y) - I$,\nconstraint : $F_{ex}^T(X)F_{ex}(X) = F_{ey}^T(Y)F_{ey}(Y) = I$"}, {"title": "Feature Reconstruction", "content": "The function $\\Psi$ is composed of two distinct components: the feature reconstruction error, $M$, and the instance similarity metric, $S$. It is defined as:\n$\\Psi(F_{ex} (X), F_{dx} (X)) = M + \\lambda S$ (5)\nwhere $\\lambda$ is a regularization parameter that balances the contribution of the similarity metric S relative to the reconstruction error M.\nThe reconstruction error M, quantified as mean squared error, is calculated as:\n$M = \\sum_{i=1}^n (x_i - x_i')^2$ (6)\nwith $x_i'$ representing the reconstruction of the input $x_i$, generated by the $F_{dx}$ applied to the encoded representation $F_{ex}(x_i)$.\nThe similarity metric S ensures that the proximity between original instances is maintained after reconstruction, thereby conserving the integrity of the feature"}, {"title": "3.3 Generate Instances and Post-Processing", "content": "Let $L_s = {l_j | ImR_j > 10, IRlbl_j > MeanIR}_{j=1}^q$ be the set comprising m minority labels [29] and $M = {(x_i, y_i) | y_{ij} = 1, l_j \\in L_s}$ be the minority instance set associated any labels in $L_s$. Then, we randomly select a seed sample $(x_i, y_i)$ from $M$ to initiate the sampling process through forward inference. As shown in Figure 3, the process encodes the feature vector $x_i$ into a latent space by $F_{ex}(x_s)$, then decodes it to the feature and label vectors of the new instance by $F_{dx} (F_{ex}(x_s))$ and $F_{dy} (F_{ex}(x_s))$, respectively. Specifically, we employ a predefined threshold set T to transform the decoded numerical label vector into a binary label vector. After the process, we remove any instances where the generated label vector is entirely zeros to ensure each instance contributes meaningfully to the dataset."}, {"title": "4 Experiments and Analysis", "content": ""}, {"title": "4.1 Datasets", "content": "We evaluate our proposed model across 9 benchmark multi-label datasets spanning diverse domains, such as text, images, and bioinformatics [30]. Each dataset is characterized by a set of statistics and imbalance metrics, which include the\nHere, 10 is a hyperparameter. We refer to the suggestions in [29] for the selection."}, {"title": "4.2 Experiment Setup", "content": "In AEMLO, $F_{ex}$ and $F_{ey}$ are comprised of two fully connected layers, whereas $F_{dx}$ and $F_{dy}$ adopt a single fully connected layer structure. Each layer within these components incorporates 512 neurons and incorporates a leaky ReLU activation function to introduce nonlinearity. The parameters $\\alpha$ and $\\beta$ of objective function are explored within the range of $[2^{-4}, 2^{-3}, ..., 2^{4}]$ ."}, {"title": "4.3 Experimental Analysis", "content": "Table 2 presents the average rankings of each base classifier combined with sampling methods across all datasets. Additionally, the Friedman test was utilized"}, {"title": "4.4 Parameter Analysis", "content": "We investigate the influence of various parameter settings on the performance of ALMLO. We select smaller enron and larger Corel5k as two representative datasets in the parameter analysis.\nAs shown in Figure 7(a), the impact of varying sampling rate p on Macro-F and Macro-AUC scores (based on MLkNN) shows a trend of initial fluctuation, followed by stabilization. In contrast, in Figure 7(b) exhibits a higher sensitivity to p, with significant volatility in Macro-F and inconsistent variations in Macro-AUC. These observations suggest that the optimal selection of p may be highly dependent on dataset characteristics.\nFigure 8 illustrates ALMLO's performance sensitivity to variations in $\\alpha$ and $\\beta$, highlighting the importance of balancing feature reconstruction loss with label relevance loss during optimization."}, {"title": "4.5 Sampling Time", "content": "Figure 9 shows the time efficiency of different sampling methods, with the epoch set as 100 for AEMLO. It is evident that AEMLO, as a deep learning approach, requires training before sampling, resulting in a higher time expenditure."}, {"title": "5 Conclusion", "content": "In this paper, we introduce AEMLO, an innovative oversampling model devised for addressing data imbalance in multi-label learning by integrating canonical correlation analysis with the encoder-decoder paradigm. AEMLO emerges as an effective oversampling solution for training deep architectures on imbalanced data distributions. It acts as a data-level solution for class imbalance, synthesizing instances to balance the training set and thus enabling the training of any classifiers without bias. AEMLO exhibits the pivotal characteristics crucial for a successful sampling algorithm in the multi-label learning domain: the ability to manipulate features and labels, i.e., to learn low-dimensional joint embeddings from feature and label representations and transform them into an original-dimensional space, along with generating new feature representations and their corresponding label subsets. This is facilitated through the utilization of an encoder/decoder framework. Extensive experimental studies demonstrate the capability of AEMLO to handle imbalanced multi-label datasets in various domains and collaborate with diverse multi-label classifiers."}]}