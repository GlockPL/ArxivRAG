{"title": "SBDet: A Symmetry-Breaking Object Detector via Relaxed Rotation-Equivariance", "authors": ["Zhiqiang Wu", "Yingjie Liu", "Hanlin Dong", "Xuan Tang", "Jian Yang", "Bo Jin", "Mingsong Chen", "Xian Wei"], "abstract": "Introducing Group Equivariant Convolution (GConv) empowers models to explore symmetries hidden in visual data, improving their performance. However, in real-world scenarios, objects or scenes often exhibit perturbations of a symmetric system, specifically a deviation from a symmetric architecture, which can be characterized by a non-trivial action of a symmetry group, known as Symmetry-Breaking. Traditional GConv methods are limited by the strict operation rules in the group space, only ensuring features remain strictly equivariant under limited group transformations, making it difficult to adapt to Symmetry-Breaking or non-rigid transformations. Motivated by this, we introduce a novel Relaxed Rotation GConv (R2GConv) with our defined Relaxed Rotation-Equivariant group \\(R_4\\). Furthermore, we propose a Relaxed Rotation-Equivariant Network (R2Net) as the backbone and further develop the Symmetry-Breaking Object Detector (SBDet) for 2D object detection built upon it. Experiments demonstrate the effectiveness of our proposed R2GConv in natural image classification tasks, and SBDet achieves excellent performance in object detection tasks with improved generalization capabilities and robustness.", "sections": [{"title": "1 Introduction", "content": "Object detection [57, 28] is a vital computer vision [45] task and is pivotal in various domains, including autonomous driving, geosciences, and ecology. Recent advancements in Deep Neural Networks (DNNs) [39, 42] have made remark-able progress. Nevertheless, objects within natural images often exhibit rotation and scale variations, requiring DNNs to handle geometric transformations more flexibly. For exam-ple, in auto-driving scenes, the position and relationship of targets are complex and intricate; objects and the vehicle it-self can cause rotation in the scene captured by sensors, seri-ously affecting target detection performance [18, 50, 29, 54]. More formally, the problem is deemed rotation-equivariant since rotating the input should be anticipated to induce an equivalent rotation in the output. Let f(\u00b7) denote the equiv-ariance function for a transformation g(\u00b7); it is known that equivariant functions preserve the symmetry of their input, and we have \\(f(g(\u00b7)) = g(f(\u00b7))\\). One of the currently effective ways to solve this problem is data augmentation, which aims to improve object detection performance by rotating the dataset, which leads to a considerable cost. Another approach is to build Equivariant Neural Networks (ENNs) [15], which already incorporate symmetries and can focus on the underlying physics. Various ENNs have provided significant improvements for the object detection task.\nAlthough using symmetry as an inductive bias in machine learning has emerged as a powerful tool, with significant conceptual and practical breakthroughs [1], a wide range of learning tasks necessitates Symmetry-Breaking [40]. However, ENNs cannot effectively model Symmetry-Breaking, as the requirement for equivariance is inherently too restrictive. Note that the notion of symmetry captures the idea that an object is essentially the same after some transformation is applied to it [51]. For instance, equivariant functions can not break symmetry at the level of data samples. Consider a dataset of images with rotational symmetry, where each image depicts a single object that has been rotated. As previously discussed, specific endeavors in ENNs aim to capturing the intrinsic symmetry inherent in images by incorporating the \\(C_2\\) or \\(C_4\\) groups. However, these symmetries are restricted to predefined, rigid group operations. When an object experiences symmetry-breaking events, such as rotational deformations beyond the scope of the predefined group or the introduction of minor defects on its surface, a strictly ENN may encounter difficulties in accurately representing these asymmetries. This is because strict adherence to symmetry constraints could prevent it from distinguishing between the object's perturbed and non-perturbed states, which is crucial for specific tasks. Some pioneer works [33, 41, 47, 27, 22, 53] discussed relaxation of equivariance and claimed that relaxed ENNs can model Symmetry-Breaking in multiple domains. However, there is still a significant gap between these existing relaxed ENN works and the computer vision field.\nThis paper proposes a novel Relaxed Rotation GConv with a learned or controlled degree of rotation deviation. This relaxation enables the network to recognize the approximate symmetry of the object while capturing the distinct features that arise from perturbations. It facilitates the network's ability to model Symmetry-Breaking effectively, thereby providing a more nuanced and accurate data representation. To the best of our knowledge, we are the first to address the Symmetry-Breaking problem formally in the object detection scenario. The main contributions of this paper are summarized as follows:\n1.  We propose a simple yet useful Relaxed Rotation GConv (R2GConv) to tackle the trade-off between underlying symmetry-breaking and rotation-equivariance within the image dataset.\n2.  We propose a redesigned Symmetry-Breaking Object Detection Network (SBDet). \u03a4\u03bf our knowledge, we are the first to explore the symmetry-breaking situation for the object detection task. Experimental experiments demonstrate that our method achieves better convergence and state-of-the-art performance with fewer parameters and better robustness."}, {"title": "2 Related works", "content": "As a pioneering work, the concept of an equivariant network was proposed in the Group Equivariant Convolution Neural Networks (G-CNN) [5]. Rotation-Equivariant convolution or full connect layer [31, 34, 5, 12] guarantees the rotation-equivariance of extracted features under the group operations by a higher degree of weight sharing. Moreover, the concept of equivariant network is also applied in Graph Neural Networks (GNN) [19, 1], which have demonstrated their prominence in dealing with unarchitectured data, such as molecule [14] and point clouds [44].\nAlthough physical laws are governed by numerous symmetries, real-world data, such as complex datasets and graphs, often deviate from strict mathematical symmetry either due to noisy or incomplete data or to inherent Symmetry-Breaking features in the underlying system [41, 47]. Smidt et al. [41] show that the gradients of the loss function can be used to learn a Symmetry-Breaking order parameter. Strict constraints on the weights enforce symmetry in equivariant networks. Relaxed equivariant networks constitute another relevant approach, permitting layer weights that enable a departure from strict equivariance. To break symmetry at the level of individual data samples, Wang et al. [47] investigate approximately equivariant networks by incorporating relaxed weight sharing in group convolutions and weight-tying in steerable CNNs, respectively, thereby achieving a bias toward not strictly preserving symmetry. Kaba et al. [27] proposes a novel methodology for constructing relaxed equivariant multilayer perceptrons, going beyond the straightforward approach of adding noise to inputs followed by using an equivariant neural network [33]. Huang et al. [22] tackled graph symmetry in real-world data by leveraging graph coarsening to establish approximate symmetries and proposing a bias-variance tradeoff formula based on symmetry group selection. Xie et al. [53] introduces Symmetry-Breaking parameters sampled as model inputs from a set determined solely by input and output symmetries. They further observe that breaking more symmetry than needed is sometimes beneficial."}, {"title": "2.2 2D Object Detection", "content": "With the advent of DNNs and the increasing computational power of GPUs, DNNs have been successfully applied to various computer vision tasks, including object detection [3, 57, 28]. Object detection is a fusion of object location and object classification tasks. It involves locating objects through bounding boxes and identifying their respective categories. Among the different object detection algorithms, the YOLO framework [36, 43, 23] has stood out for its remarkable balance of speed and accuracy, enabling the rapid and reliable identification of objects in images. Since its inception, the YOLO family has evolved through multiple iterations and various other variations, each building upon the previous versions to address limitations and enhance performance. The newest YOLOv8 [26] uses a backbone similar to YOLOv5 [25] and the C2f module to combine high-level features with contextual information to improve detection accuracy. YOLOv8 [26] provided five scaled versions: YOLOv8-n (nano), YOLOv8-s (small), YOLOv8-m (medium), YOLOv8-1 (large), and YOLOv8-x (extra-large). In addition to the YOLO framework, the field of object detection and image processing has developed several other notable methods, including RCNN [16], SSD [32], DETR [2], and Transformer-based PVT [48, 49].\nRotation-Equivariance has recently become a strongly desired property in object detection. ReDet [18] incorporates Rotation-Equivariant networks into the detector to extract Rotation-Equivariant features, allowing for accurate orientation prediction and substantially reducing model size. EON [54] introduces a rotation-invariant prior that addresses object detection in 3D scenes, where the bounding box should be equivariant to the object's pose, regardless of the motion of the scene. TED [52] proposed an efficient transformation-equivariant 3D detector with competitive speed, which comprises a transformation equivariant sparse convolution backbone, transformation-equivariant Bird-Eye-View pooling, and transformation-invariant voxel pooling. DuEqNet [50] improves object detection performance by constructing a dual-layer object detection network for 3D point clouds with rotational invariance and extracting local-global invariance features. FRED [29] further decouples the invariant task (object classification) from the equivariant task (object localization), achieving fully Rotation-Equivariant oriented object detection and enabling more genuine non-axis-aligned learning. These existing equivariant object detection approaches are primarily tailored to 3D point cloud data or specific application contexts. They also overlook scenarios involving necessitating a relaxation of equivariant constraints, i.e., Symmetry-Breaking. This paper further explores a more general 2D object detection in natural images."}, {"title": "3 Method", "content": "In the section, we first provide the mathematical definitions of Strict and Relaxed Rotation-Equivariance and briefly review and summarize existing Strict and Relaxed Rotation-Equivariant ENNs, which serve as preliminary knowledge. Then, we describe the main idea of the Relaxed Rotation-Equivariant GConv module. Finally, the Relaxed Rotation-Equivariant object detection network, Symmetry-Breaking Detection Network (SBDet), is introduced and analyzed."}, {"title": "3.1 Preliminary", "content": "Definition 1 (Strict Equivariance). A learning function \\(\\Phi_{\\text{strict}} : X \\rightarrow Y\\) that sends elements from input space \\(X\\) to output space \\(Y\\) satisfies Strict Equivariance to a group \\(G\\) if \\(\\forall g, x \\in G \\times X\\) there exists \\(p_X : G \\rightarrow GL(X)\\) and \\(p_Y : G \\rightarrow GL(Y)\\) actions of \\(G\\) such that\n\\(\\Phi_{\\text{strict}} (\\rho_X (g) x) = \\rho_Y (g) \\Phi_{\\text{strict}} (x),\\)\nwhere GL(\u00b7) is a general linear group over the space.."}, {"title": "Definition 2 (Relaxed Equivariance)", "content": "Definition 2 (Relaxed Equivariance) [47]. A learning function \\(\\Phi_{\\text{relaxed}} : X \\rightarrow Y\\) that sends elements from input space \\(X\\) to output space \\(Y\\) satisfies Relaxed Equivariance to a group \\(G\\) if \\(\\forall g, x \\in G \\times X\\) there exists \\(\\rho_X : G \\rightarrow GL(X)\\) and \\(\\rho_Y : G \\rightarrow GL(Y)\\) actions of \\(G\\) such that\n\\(||\\rho_Y (g) \\Phi_{\\text{relaxed}} (x) - \\Phi_{\\text{relaxed}} (\\rho_X (g) x) || < \\epsilon,\\)\nwhere \\(\\epsilon\\) is a controllable variable, with small \\(\\epsilon\\) exhibiting strong symmetry, while a lot larger \\(\\epsilon\\) exhibits greater flexibility and generalization ability, as building relaxed convolution filters can be learned from training data. Note that, \\(\\Phi_{\\text{relaxed}}\\) is equivalent to \\(\\Phi_{\\text{strict}}\\) when \\(\\epsilon = 0\\). Meanwhile, \\(\\Phi_{\\text{strict}}\\) satisfies the relaxed equivariance condition.\nStrictly Equivariant Neural Networks. Learning equivariant features is an optimization process for a series of strict function sets in the model. Since the composition of equivariant functions is also equivariant, constructing a strict equivariant network is a composition of \\(\\Phi_{\\text{strict}}\\). Simply put, if linear, nonlinear, pooling, aggregation, normalization, and other operators in a network are all equivariant, then their composite operators are also equivariant. However, the challenge of strictly equivariant networks lies in designing trainable linear layers, such as equivariant convolutions. Usually, there are two strategies for designing equivariant convolutions: weight sharing and weight typing, which are G-CNN [5] and G-steerable CNN [6], respectively.\nRelaxed Equivariant Neural Networks. The existing equivariant networks assume that the data is completely symmetric. This network approximates a strictly invariant or equivariant function under given group actions. For example, in G-CNN, the shared convolution filter achieves equivariant images at 0, 90, 180, and 270 degrees under the strict constraint of the rotation group \\(C_4\\). However, real-world data is rarely symmetric. This seriously hinders the potential application of equivariant networks. To solve this problem, we can relax the strict constraints on weights under group actions. In [10, 47], relaxing weight constraints can significantly improve the performance and generalization ability of the model."}, {"title": "3.2 The Implementation of Relaxed Rotation-Equivariant GConv (R2GConv)", "content": "Relaxing strict group constraints is an effective way to achieve our Relaxed Rotation-Equivariant convolution filters. This paper proposes implementing a learnable parameter \\(\\Delta\\) to perturb the group operations. Here, our proposed method is based on the fourth-order cyclic rotation group \\(C_4 = \\{c, c^1, c^2, c^3\\}\\), where powers of \\(c\\) indicate performing rotation operation on the input \\(x\\) by 90 degrees multiple times. The affine transformation matrix \\(A_i\\) on \\(C_4\\) can be defined as follows:\n\\(A_i = \\begin{bmatrix} cos(\\pi i/2) & -sin(\\pi i/2) \\\\ sin(\\pi i/2) & cos(\\pi i/2) \\end{bmatrix}, i \\in \\{0, 1, 2, 3\\}.\\)\nThen, define \\(\\Delta \\in \\mathbb{R}^{4\\times 2 \\times 2}\\) denotes our learnable perturbation factor, we have\n\\(\\{\\Delta_i\\} = \\{\\begin{bmatrix} \\Delta_{i1} & \\Delta_{i2} \\\\ \\Delta_{i3} & \\Delta_{i4} \\end{bmatrix}\\}.\\)\nNow, we define a Relaxed Rotation-Equivariant group \\(R_4 = \\{r^0, r^1, r^2, r^3\\}\\) based on \\(C_4\\) and a function \\(T: C_4 \\rightarrow R_4\\). We have a corresponding perturbed affine transformation matrix \\(A'_i\\) on \\(R_4\\),\n\\(A'_i = T(A_i, \\Delta_i) = \\begin{bmatrix} cos(\\pi i/2) + \\Delta_{i1} & -sin(\\pi i/2) + \\Delta_{i2} \\\\ sin(\\pi i/2) + \\Delta_{i3} & cos(\\pi i/2) + \\Delta_{i4} \\end{bmatrix},\\)\nwhere we use simple addition to add noise to the affine transformation. Note that other operations are also available for \\(T\\), such as multiplication but are not limited to. Meanwhile, for all \\([a \\space b]\\) \\in CoorSet(x), we have the transformations \\(c^i([a \\space b]) = A_i \\cdot [a \\space b]^T\\) and \\(r^i([a \\space b]) = A'_i \\cdot [a \\space b]^T\\) on \\(C_4\\) and on \\(R_4\\), respectively. Note that CoorSet(x) denotes the set of coordinates of \\(x\\).\nNow we demostrate the construction of our proposed Relaxed Rotation-Equivariant convolution filters, which is the core of achieving catching symmetric-breaking feartures.\nGiven an initial 2D convolution filter \\(K_{init}\\), for all \\([u \\space v]\\) \\in CoorSet(K_{init})\\), and \\(r^i \\in R_4\\) where \\(i \\in \\{0, 1, 2, 3\\}\\), the transformed coordinates of Relaxed Rotation-Equivariant convolution filters are given as follows:\n\\([u' \\space v']^T = r^i([u \\space v]) = \\begin{bmatrix} cos(\\pi i/2) + \\Delta_{i1} & -sin(\\pi i/2) + \\Delta_{i2} \\\\ sin(\\pi i/2) + \\Delta_{i3} & cos(\\pi i/2) + \\Delta_{i4} \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix}\\)"}, {"title": "3.3 The Relaxed Rotation-Equivariant Network (R2Net)", "content": "Based on our R2GConv, we propose a Relaxed Rotation-Equivariant Network (R2Net). Our R2Net (BackBone) consists of four stages: a downsampled R2GCBA3x3 and an R2Net Block. Since the input of a regular CNN is a 2D feature map in the plane \\(\\mathbb{Z}^2\\) space, we typically project the input to \\(R^4\\) through our proposed Relaxed Rotation-Equivariant Lifting (R2Lifting) in the first layer. Given a 2D feature map \\(f_{in}\\) with a size of \\(c_{in} \\times h \\times w\\) and an initial convolution filter \\(K_{init}\\) with a size of \\(c_{out} \\times c_{in} \\times k \\times k\\), we can obtain our Relaxed Rotation-Equivariant lifting convolution filter \\(K'_{rel} = \\Phi (K_{init})\\) with a size of \\(c_{out} \\times 4 \\times c_{in} \\times k \\times k\\). Hence, R2Lifting can be defined as\n\\(f_{out} (c_{out}, 4, h', w') = \\sum_{c} K'_{rel} (c_{out}, 4, c, k, k) * f_{in} (c, h, w),\\)\nwhere the size of \\(f_{out}\\) is \\(c_{out} \\times 4 \\times h' \\times w'\\), in which \\(h'\\) and \\(w'\\) denote the output height and width, respectively. Again, during the convolution process, \\(K_{init}\\) will be reshaped to \\(4c_{out} \\times c_{in} \\times k \\times k\\), and \\(f_{in}\\) remains unchanged. Then, input the projected feature map into the 4-layer stage, perform 2\\times downsampling in sequence, and increase the number of channels by 2\\times. Specifically, the last stage preserves the number of channels unchanged. The obtained output will go through the Transfer Block and then be sent to a universal classification header. The architecture of our R2Net is shown in Figure 3, and the Transfer Block is shown in Figure 4 (d). We provided three models of different sizes in this paper: R2Net-n, R2Net-s, and R2Net-m. More details can be found in Appendix A.1.\nDue to the inevitable high computational overhead caused by GConv, we borrow the idea of dividing channels in Res2Net [13] to reduce the number of parameters and FLOPs. The R2Net Block is designed with the residual connection [20] architecture to improve the convergence ability. The detailed module architecture of R2Net Block is shown in Figure 4 (a)."}, {"title": "3.4 The Redesigned Symmetry-Breaking Object Detector (SBDet)", "content": "Based on our proposed R2Net, we propose a Symmetry-Breaking Object Detector (SBDet). SBDet adopts the FPN+PAN neck architecture, taking the stage outputs 2 ~ 4 in R2Net as inputs. Specifically, the input of stage 4 will go through a GSPPF for spatial max pooling under the group \\(C_4\\). The obtained inputs are processed through the neck network to obtain multi-scale features, which are then fed into the Transfer Block. Finally, the outputs are fed into a universal detection head of YOLOv8, including three detection heads for detecting small, medium, and large-scale objects. Note that the R2GUp adopts the same architecture as R2GConv, except that the transposed convolution operator is used during the depthwise convolution. The detailed architecture is shown in Figure 5."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to study and demonstrate the performance of the proposed model. In Sec. 4.1, we evaluate our proposed method on the PASCAL VOC and MS COCO 2017 datasets, showing the state-of-the-art performance on the object detection task. We also conduct natural image classification experiments on CIFAR-10/100 dataset, as described in Sec. 4.2. To provide further insight into the effectiveness of our approach, we also present visualization results in Sec. 4.3. The experimental results show that our model has a greater parameter efficiency and accuracy capacity than the existing strict ENNs. We adopt our Relaxed Rotation-Equivariant group \\(R_4\\) to construct all our models in this section, and a declaration will be made in case of special settings. All the experiments are done on dual GeForce RTX 4090 GPUs."}, {"title": "4.1 Object Detection on the PASCAL VOC and MS COCO Datasets", "content": "PASCAL Visual Object Classes (VOC) 2007 comprises 5,000 training images with over 12,000 annotated objects. The PASCAL VOC 2012 expands this with 11,000 training images and 27,000 annotated objects. Both datasets annotate 20 everyday object classes, including person, cat, bicycle, etc. We conduct experiments on the combined PASCAL VOC 07&12. Furthermore, experiments were extended to the Microsoft Common Objects in Context (MS COCO) 2017 dataset, comprised of 164,000 images annotated with 897,000 objects across 80 classes. We aim to validate the capability of our proposed SBDet in standard object detection tasks using mean Average Precision (mAP). Specifically, we leverage mAP50 at an intersection over union (IoU) threshold of 0.5, and mAP50:95 across IoU thresholds ranging from 0.5 to 0.95 as key evaluation metrics."}, {"title": "4.2 Image Classification on the CIFAR-10/100 and Rotated MNIST Datasets", "content": "In this section, we evaluate the performance of our proposed R2Net on image classification tasks using the CIFAR-10/100 and Rotated MNIST datasets, where we randomly rotate all images in the MNIST training set by 0, 90, 180, and 270 degrees, leaving the test set unaffected. As seen from Table 5, our R2Net-n achieves competitive results compared to other models while having much smaller parameters (0.8M). Our R2Net-m achieved a top-1 test error rate of only 3.5% on CIFAR-10 and 17.3% on CIFAR-100, slightly lower than ResNeXt-29, but with only 6M parameters, which is 10% parameters of ResNeXt-29. Furthermore, our R2Net-x outperforms all other models with a top-1 test error rate of only 3.33% on CIFAR-10 and 16.1% on CIFAR-100. Additionally, we also evaluate the performance of our R2Net on the Rotated MNIST dataset. As shown in Table 6, Our R2Net-n achieves a top-1 test error rate of 27.75%, outperforming YOLOv8-n-cls in recognizing rotated images. Overall, these results demonstrate the superior performance of our proposed R2Net in image classification tasks on both standard and rotated datasets."}, {"title": "4.3 Relaxed, Strict Rotation-Equivariance and Non-Rotation-Equivariance Visualization", "content": "The visualization of our R.R.E., S.R.E., and Non-Rotation-Equivariance (N.R.E.) is shown in Figure 6. We rotate the original image (a) to obtain rotated images (b) and input them into our SBDet (R.R.E.), SBDet (S.R.E.), and YOLOv8 (N.R.E.) to obtain the feature maps (c), (d), and (e), respectively. Observing the white circles in (c), we notice slight differences. Nevertheless, the overall feature maps are Rotation-Equivariant, showcasing our R.R.E. Observing the red circles in (d), we find that the feature maps are strictly equivariant, presenting S.R.E. Lastly, observing (e), we find almost N.R.E. A more detailed visualization of R.R.E. in our SBDet-n can be found in Appendix A.6."}, {"title": "5 Conclusion", "content": "In this work, we propose Relaxed Rotation-Equivariant Group Convolution (R2GConv), a novel approach that tackles symmetry-breaking situations in rotation-equivariance to better align with real-world scenarios. R2GConv introduces learnable parameters to transform a strict rotation-equivariant group \\(C_4\\) into a relaxed rotation-equivariant group \\(R_4\\) by perturbing group operations. Furthermore, we propose an efficient Relaxed Rotation-Equivariant Network (R2Net) as the backbone and a redesigned 2D object detector called Symmetry-Breaking Object Detector (SBDet). Through experiments on the object detection and classification tasks, we demonstrate that our proposed SBDet and R2Net achieve state-of-the-art performance compared to models without symmetry bias or with strict equivariant constraints. However, a limitation is the relatively slower training speed, so integrating CUDA-accelerated operations is one of the future directions. Additionally, R2GConv or R2Net can be applied to more complex visual tasks and scenes, leveraging the advantages of Relaxed Rotation-Equivariance."}, {"title": "A.1 Model architecture of SBDet", "content": "In this section, we provide a detailed construction of our SBDets of different sizes, as shown in Table 7."}, {"title": "A.2 Convergence Analysis on SBDet models with different \u03c3", "content": "In this section, we further conduct a convergence analysis on the learnable perturbation factor by examining the impact of varying the variance o of its initial Gaussian distribution in our SBDet-n model on the PASCAL VOC dataset. As illustrated in Figure 7, it is clear that when o values are set to 0.1, 0.2, and 0.4, the convergence behavior of the model remains consistent across these values, ultimately converging to similar accuracy in mAP50 (and mAP50:95). The highest accuracy is observed with the variance \\(\\sigma\\) = 0.1. However, for o values set to 0.6 and 0.8, there is a lot of decrease in model accuracy, particularly with \\(\\sigma\\) = 0.8 exhibiting the lowest accuracy. We speculate that for larger initial \u03c3, there is too much perturbation to the group operation, which leads to a significant degree of Symmetry-Breaking in the model, making it difficult for the model to learn the appropriate degree of Symmetry-Breaking from the training dataset, resulting in lower accuracy."}, {"title": "A.3 Supplymental Convergence Analysis on SBDet models", "content": "In this section, we analyze the accuracy and loss curves of the SBDet and YOLOv8 models during the training process on the PASCAL VOC dataset, as detailed in Figure 8. In part (a), it is evident that our SBDet model exhibits not only a more stable training process but also surpasses the accuracy of outstanding YOLOv8 models. Moreover, our model achieves faster convergence, significantly reducing the total number of training epochs needed to reach the target accuracy, which it attains within approximately 90 to 100 epochs. The fast convergence could be related to the fact that our R2GCConv can extract rich and relaxed equivariant features, hence enabling earlier learning of these potential features. Regarding the loss function, both YOLOv8 and SBDet exhibit a steady decline across all three loss components, as illustrated in parts (b), (c), and (d) of the figure."}, {"title": "A.4 Supplymental Experiments on PASCAL VOC 07&12 dataset", "content": "As shown in Table 8, AP for different 20 classes of the PASCAL VOC 07&12 are reported. SBDet demonstrates a balanced and consistently high performance across various categories. Even in categories where SBDet does not achieve the highest AP, it is still competitive compared to models of comparable size."}, {"title": "A.5 Analysis on YOLOv8-n-cls and R2Net-n on the Rotated MINIST (R.M.) dataset", "content": "This section compares the training accuracy of YOLOv8-n-cls and our R2Net-n on the R.M. dataset. We manipulate the training set by randomly rotating 60, 000 images by 0, 90, 180, and 270 degrees while maintaining 10,000 images unaltered in the test set to evaluate the performance of a model under rotation. As depicted in Figure 9, both R2Net-n and YOLOv8-n-cls display fluctuations during training. However, R2Net-n exhibits milder fluctuations compared to the more pronounced oscillations observed in YOLOv8-n-cls. This contrast highlights the superior rotational anti-interference capability of R2Net-n, which is primarily attributed to its novel Relaxed Rotation Equivariance (R.R.E.) property."}, {"title": "A.6 Visualization Analysis on Relaxed Rotation-Equivariance", "content": "In this subsection, we present a visualization of feature maps from our SBDet-n, as illustrated in Figure 10. We rotate the initial image (a) by 90, 180, and 270 degrees to generate images (b), (c), and (d) as inputs. It can be found that the output feature maps in (e), (f), (g), and (h), corresponding to each channel, exhibit consistency with minor variations, which demonstrates the Relaxed Rotation-Equivariance property of our network."}, {"title": "A.7 Heatmap Visualization", "content": "In this section, we present the visualization of LayerCAM [24] heatmaps derived from YOLOv8-n, YOLOv7, YOLOv5, and our SBDet-n, as depicted in Figure 11. These heatmaps enable us to locate the regions of interest where the network concentrates its attention. It can be seen that YOLOv7 and our SBDet-n achieved better feature focusing. Notably, SBDet-n shows a comprehensive focusing range on certain objects, such as dogs and zebras. In contrast, YOLOv8 and YOLOv5 fail to exhibit such targeted feature focus on these particular objects."}, {"title": "A.8 Parameter Analysis of R2GConv and GConv based on the Rotation group C4", "content": "Assuming the input channels, output channels, and kernel size of both R2GConv and GConv are \\(C_{in}\\), \\(C_{out}\\), and k, respectively. The parameters of our R2GConv can be calculated as follows:\n\\(C_{in} \\times C_{out} \\times 4 \\times 1 \\times 1 \\space (R2PGConv) + C_{out} \\times 1 \\times 1 \\times k \\times k \\space (R2DGConv) + 4 \\times 2 \\times 2\\)\n\u2248 \\(4 \\times C_{in} \\times C_{out} + k^2 \\times C_{out}\\) \nwhere in (\u00b7) denotes the source of parameters.\nThe parameters of GConv can be calculated as follows:\n\\(C_{in} \\times C_{out} \\times 4 \\times k \\times k = 4 \\times C_{in} \\times C_{out} \\times k^2\\).\nTherefore, the parameter of our R2GConv is only\n\\(\\frac{4 \\times C_{in} \\times C_{out} + k^2 \\times C_{out}}{4 \\times C_{in} \\times C_{out} \\times k^2} = \\frac{1}{k^2} + \\frac{1}{4 \\times C_{in}}\\)\nof GConv."}, {"title": "A.9 Limitation on Training Speed and Memory", "content": "Although SBDet has significant advantages in accuracy and parameter efficiency, the relatively slow training speed during R2GConv is primarily due to the involved group transformation operations, PointWise, and Depthwise operators, lacking optimization in CUDA. Additionally, the R2Net Block in the model, which incorporates residual concatenation, consumes a considerable amount of memory usage during training. Moving forward, specific CUDA operators can be developed to address the training speed issue and explore better construction methods for the R2Net Block."}, {"title": "A.10 Threotical Analysis", "content": "Since existing methods", "47": "that the ground truth equivariant function \\(\\Phi_{gt"}, "is approximately equivariant. A model class with a similar degree of approximate equivariance would better approximate \\(\\Phi_{relaxed}\\) than \\(\\Phi_{strict}\\) a strictly equivariant class or a class without bias towards symmetry.\nFirstly, the Equivariance Error (EE) quantifies how much the ground truth equivariant function \\(\\Phi_{gt}\\) is approximately equivariant. Let || || denote the induced norm.\nDefinition A.1 (Equivariance Error [47"], "Phi_{gt}": "X \\rightarrow Y\\) be a function and \\(G\\) be a group. Assume that \\(G\\) acts on \\(X\\) and \\(Y\\) via representation \\(\\rho_X\\) and \\(\\rho_Y\\). Then the Equivariance Error of \\(\\Phi_{gt}\\) is\n\\(||\\Phi_{gt}||_{EE} = sup_{x, g} ||\\rho_Y (g)\\Phi_{gt}(x) - \\Phi_{gt}(\\rho_X (g)(x))||.\\)\nHence, \\(\\Phi_{gt}\\) is e-approximately equivariant if and only"}