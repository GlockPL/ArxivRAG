{"title": "Discretizing Continuous Action Space with Unimodal Probability Distributions for On-Policy Reinforcement Learning", "authors": ["Yuanyang Zhu", "Zhi Wang", "Yuanheng Zhu", "Chunlin Chen", "Dongbin Zhao"], "abstract": "For on-policy reinforcement learning, discretizing action space for continuous control can easily express multiple modes and is straightforward to optimize. However, without considering the inherent ordering between the discrete atomic actions, the explosion in the number of discrete actions can possess undesired properties and induce a higher variance for the policy gradient estimator. In this paper, we introduce a straightforward architecture that addresses this issue by constraining the discrete policy to be unimodal using Poisson probability distributions. This unimodal architecture can better leverage the continuity in the underlying continuous action space using explicit unimodal probability distributions. We conduct extensive experiments to show that the discrete policy with the unimodal probability distribution provides significantly faster convergence and higher performance for on-policy reinforcement learning algorithms in challenging control tasks, especially in highly complex tasks such as Humanoid. We provide theoretical analysis on the variance of the policy gradient estimator, which suggests that our attentively designed unimodal discrete policy can retain a lower variance and yield a stable learning process.", "sections": [{"title": "I. INTRODUCTION", "content": "EEP reinforcement learning (DRL) has presented a powerful paradigm for learning new behaviors from scratch both on physical and simulated challenging tasks [1-4]. The action space of conventional reinforcement learning (RL) tasks can be discrete, continuous, or some combination of both [5, 6]. In simulation [7] and real life [8-11], continuous control usually requires some subtle parametric functions for a compact representation of action distributions, typically Gaussians, due to an infinite number of feasible actions in a continuous action space [11-14]. This underlying assumption is that it can enable more refined decisions when control policies can cover all feasible control inputs, which brings difficulty in maximizing an arbitrary function for a continuous action space [15, 16]. Furthermore, it hinders applying dynamic programming approaches to back up value function estimates from successor states to parenting states.\nDiscretizing a continuous control problem is a scalable solution that retains the simplicity of discrete actions and allows for theoretical analysis [10, 17, 18]. With such a setup, impressive results have been obtained in high-dimensional continuous action spaces [10, 19, 20]. However, the naive discretization approach results in exponentially large discrete action space, where ballooning action space can quickly become intractable. It would be a balancing dilemma that the resulting output may not be smooth enough when the discretization is coarse, and the number of discretized actions may be highly intractable otherwise. Moreover, more fine discretization of the action space may struggle to capture the information about the class ordering of the continuous action space, making it challenging to generalize across discrete actions. Furthermore, it also brings a larger variance of the policy gradient estimator, which may hurt the stability and performance, especially for highly complex tasks [21, 22]. In practice, more fine-tuned hyperparameters are also required to prevent policy collapse.\nTo tackle these intractabilities, prior works [20, 23] assume that action dimensions are independent and developed to factorize distributions across action dimensions using an ordinal parameterization of the joint distribution over discrete actions, which has shown improved performance in finding near-optimal solutions for high-dimensional tasks. By parameterizing the discrete policy using an ordinal parameterization, the natural ordering between discrete actions can be implicitly encoded, which provides an additional inductive bias that can improve the generalization across actions [24-26]. In contrast, the conventional approach of parameterizing the discrete policy as a Gibbs distribution fails to consider the order of actions and ignores the discrepancy relationships between them [27]. Existing methods usually use some heuristic network architecture design to implicitly encode the ordinal information into the distribution over discrete actions, which may not guarantee a unimodal probability distribution to efficiently capture the ordinal information within atomic actions [28]. In their work, they alleviate this issue by designing the loss function to impose an ordering on the action, which does not impose unimodal constraints. Here, we propose a more explicit and efficient ordinal parameterization method that better incorporates the notion of continuity when parameterizing the distribution over discrete actions, while avoiding the need for an exponentially large number of actions. Additionally, we impose an ordering constraint on the action space of each dimension through the loss function to further improve the performance of RL algorithms.\nThis paper mainly addresses how to efficiently leverage the internal ordering information in the continuous action space to enhance generalization across the discretized set of actions for on-policy RL without an explosion in the number of parameters. Inspired by the deep ordinal classification approach [27], for each independent action dimension, we constrain the distribution over discrete actions to be unimodal to explicitly exploit the inherent ordering between discrete actions. To implement the unimodal constraint, we employ the Poisson probability distribution, which allows the probability mass to decrease gradually on both sides of the action with the majority of the mass. Our framework necessitates learning a probability mass function for each action dimension, substantially reducing the number of units in the network output layer and enabling us to circumvent the curse of dimensionality as the discretization action bins increase (see Fig. 1). It facilitates fine-grained discretization of individual domains without incurring a significant increase in the number of parameters. The experimental results show that our method outperforms the baselines by a wide margin in the suite of MuJoCo continuous control tasks [29].\nWe highlight the following contributions of this work:\n\u2022\tWe propose a novel ordinal framework that enforces the distribution over discrete actions to be unimodal, effectively capitalizing on the continuity presented in the action space. We utilize the probability mass function of the Poisson distribution to enforce unimodal probability distributions for each dimension action space, which can place more confidence on the majority probability distribution benefiting from its inherent unimodal distribution.\n\u2022\tTheoretically, we present a variance analysis for the policy gradient estimator, where our unimodal policy attains a lower variance for the estimator.\n\u2022\tWe show experimentally that an ordinal parameterization of the unimodal policy outperforms other competing approaches for on-policy RL on continuous control tasks. The promising results showcase the versatility and effectiveness of our methods compared to the baselines.\nThe remaining paper is organized as follows. In Section II, we summarize preliminaries of RL and related literature. In Section III, we present the unimodal probability distributions for on-policy RL. In Section IV, we give comprehensive experimental results of learning performance and stability analysis. The paper is concluded by Section V."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "RL is an approach to solving optimal sequential decision-making tasks, built upon the concept of Markov Decision Processes (MDPs). In the finite MDP framework, a tuple of (S, A, T, R, \u03b3) defines the problem, where S is a countable state space, A is a finite action set, T : S \u00d7 A \u00d7 S \u2192 [0,1] represents the transition kernel, R : S \u00d7 A \u2192 R is the reward function, and \u03b3\u2208 [0,1) is the discount factor. A stochastic policy is defined as \u03c0: S\u00d7A \u2192 [0,1], which maps environmental states to distributions over actions with the constraint \u03a3a\u2208A\u03c0(a|s) = 1,\u2200s \u2208 S. The objective of the RL problem is to find an optimal policy \u03c0* that maximizes the expected total discounted return by\n$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi(\\tau)}[r(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi(\\tau)}[\\sum_{t=0}^T \\gamma^t r_t]$\n$Q^{\\pi}(s, a) = \\mathbb{E}_{s_0=s, a_0=a, \\tau \\sim \\pi}[\\sum_{t=0}^T \\gamma^t r_t]$\nwhere \u03c4 = (s0, a0, s1, a1,...) is a learning episode, \u03c0(\u03c4) = p(s0)\u03a0t=0\u03c0(at|st)p(st+1|st, at), and rt is the immediate reward. The policy function h( | \u03b8) can be represented by a deep neural network parameterized by \u03b8. For a discrete action space, the Gibbs distribution is commonly used and given by\n$\\pi_{\\theta}(a; s) = \\frac{exp(h_j(s|\\theta))}{\\sum_{j \\in \\mathcal{A}(s)} exp(h_j(s | \\theta))}$,\nwhile the Gaussian distribution is generally used for a continuous action space and is given by\n$\\pi_{\\theta}(a | s) = \\frac{1}{\\sqrt{2\\pi\\sigma}} exp(-\\frac{1}{2\\sigma^2} (h(s|\\theta) - a)^2)$"}, {"title": "B. Related Work", "content": "On-policy RL. On-policy RL is a family of RL algorithms aiming to directly optimize the parameters of a policy, which optimizes expected returns by estimating policy gradients. This estimation is often prone to high variance, and several solutions have been proposed to mitigate this significant challenge, particularly in problems characterized by long horizons or high-dimensional action spaces [22, 33, 34]. Classic vanilla policy gradient (PG) updates are generally unstable with high variance due to difficulties in crediting actions that influence future rewards, where the gradients require knowledge of the probability of the performed action via resorting to parametric distributions. Natural policy [35] improves upon vanilla PG by computing an ascent direction with the Fisher information that approximately ensures a slight change in the policy distribution. To obtain more stable learning, trust region policy optimization (TRPO) [7] utilizes a larger Kullback-Leibler (KL) divergence value to enforce constraints, and performs a line search in the natural gradient direction, ensuring improvements in the surrogate loss function. Proximal policy optimization (PPO) [36] replaces the KL divergence constraint with the clipped surrogate objective, which strikes a favorable balance across sample complexity, simplicity, and wall time. Moreover, actor-critic using Kronecker-factored trust region (ACKTR) [37] builds upon the natural policy gradient framework by calculating gradients through Kronecker-factored approximate curvature within the trust region. Orthogonal to the above algorithms, we demonstrate that ordinal parameterization with a unimodal constraint for on-policy RL achieves consistently improved performance based on the above representative algorithms.\nPolicy Representation. In the context of policy gradient techniques, the policy is generally parameterized using neural networks, where the policy is improved by optimizing the parameter of the approximation function. In general, the policy uses a Gibbs probability distribution over the discrete action space to sample. For the continuous control problems, the default choice for the policy in the baseline is parameterized by learning a Gaussian distribution with independent components for each dimensional action space [7, 36]. The Gaussian mixture, maximum entropy, or normalizing flows [38-40] can be used for more expressive policy classes, providing a promising avenue for improved robustness and stability. Since physical constraints in most continuous control tasks, actions can only take on values within some finite interval, resulting in an unavoidable estimation bias caused by boundary effects. To address the shortcomings of the Gaussian distribution with a finite support distribution, the Beta distribution [41] provides a bias-free and less noise policy with improved performance. Further, considering the extremes along each action dimension, a Bernoulli distribution is applied to replace the Gaussian parametrization of continuous control methods and show improved performance on several continuous control benchmarks [17]. Here, we provide insight into how to produce an efficient ordinal parameterization policy while retaining the internal order information of continuous action space with a simple explicit constraint, which can yield state-of-the-art performance compared to baseline policy classes.\nContinuous action discretization. To leverage the continuity between discrete and continuous action space, converting continuous control problems into discrete ones has been introduced by [42] with the \"bang-bang\" controller [43]. Similar discretization methods represent each action in binary format and optimize its policy for MDPs with large action sets [44-48]. However, such discretization methods need to be improved due to the curse of dimensionality. Surprisingly, recent works have relieved the limitation by assuming that action dimensions are independent [10, 20, 49-51], which obtains improved performance in complex control tasks. Inspired by multi-agent reinforcement learning, Decoupled Q-Networks (DecQN) [52] factorizes the overall state-action value function into a linear combination of single action utility functions over action dimensions by combining value decomposition with bang-bang action space discretization. It achieves better performance while reducing agent action space complexity. However, such a discretization paradigm makes the strong assumption of independence among each action dimension and hardly captures the continuous order information of actions [53, 54].\nTo mitigate the exponential explosion of discrete actions, [50] utilizes sequence-to-sequence models to develop policies for structured prediction problems, but their strategy has only demonstrated effectiveness in high-dimensional tasks like Humanoid and HumanoidStandup tasks. Recently, [20] parameterizes the discrete distributions with an ordinal architecture and achieves improved performance. Nevertheless, existing methods face a dilemma in that the number of network outputs increases linearly with the number of action space dimensions, which may bring high variance, resulting in a more unstable learning process. A key distinction between our work and previous works lies in our explicit utilization of unimodal distributions with only one parameter for each action dimension. It can help the policy capture the most confident class while ensuring that the probability gradually decreases"}, {"title": "III. UNIMODAL PROBABILITY DISTRIBUTIONS FOR ON-POLICY REINFORCEMENT LEARNING", "content": "In this section, we will present our proposed unimodal distribution with Poisson probability distribution for on-policy RL algorithms, which explicitly introduce unimodal probability distribution into the ordinal parameterization method to leverage the continuity in the underlying continuous action space. First, we present the process of discretizing action space for continuous control tasks and apply it to on-policy RL. Then, we describe the unimodal ordinal architecture that constrains the discrete policy to be unimodal via the Poisson probability distribution in a practical neural network implementation. Finally, theoretical analysis shows that our method can relieve the high variance trouble of the policy gradient estimator compared to existing ordinal parameterization methods.\nConsider an environment with state st and m dimension action spaces a \u2208 Rm. To ensure generality, we consider an action space A = [-1,1]m and proceed to discretize each dimension of the action space into K evenly spaced atomic action bins. As a result, we obtain a discrete and finite set of atomic actions A\u1d62 = {-1+2j/(K\u22121)}\u2c7c=0^(K\u22121) for the i action dimension. To facilitate the joint policy's tractability, the policy generates a tuple of discrete actions accompanied by factorized categorical distributions across action dimensions. This approach obviates the need to enumerate every possible actions, as required in the discrete case. Specifically, we denote a categorical distribution \u03c0\u03b8\u2081(a;|s) as the atomic action for actions a\u1d62 \u2208 A at each step t, where \u03b8; is the factorization parameters for this marginal distribution induced by Poission probability distributions. Such representations can build complex policy distributions with explicit probability distributions, where any action subset can be formulated as\n$\\pi (\\alpha | s) = \\prod_{i=1}^m \\pi_{\\theta_i} (a_i | s)$,\nwhere a = [a0, a1, \u2026, aK\u22121]T and \u03c0\u03bf\u1d62 (a\u1d62 | s) represent the probabilities of selecting the actions a\u1d62 for the i-th dimension action space. Based on this factorization, we can easily employ RL algorithms to maintain a tractable distribution over joint actions. Meanwhile, it can vastly reduce the computation of neural networks due to the fewer learning parameters."}, {"title": "B. Unimodal ordinal architecture", "content": "In on-policy RL algorithms, the policy gradient involves a stochastic policy that specifies the probability distribution of selecting an action a given a state s. Upon discretizing the continuous action space, the complexity of directly training such policy networks could lead to Km combinations of joint atomic actions, which exponentially increase with the growing number of action dimensions m. Using the categorical distribution (e.g., Gumbel-Softmax distribution) as a stochastic policy for discrete action space has been well-studied in the RL community. Popular categorical distributions have been used as a stochastic policy for discrete action spaces due to their ease of sampling and computation. However, the curse of dimensionality can lead to intractable action spaces, putting significant pressure on the distribution parameterization over discrete actions. In fact, a fine control policy can be captured more as the resolution of the discretized action space increases. This quickly plunges into the dilemma that intractable action space puts high pressure on the parameterizing distributions over discrete actions, which hinders capturing the ordering among actions.\nTo relieve the scalability issue, a few prior works have been exploring more expressive distributions and notably proposed an ordinal parameterization [20, 53, 54]. It introduces an implicit loss function based on factorizing a tractable distribution over joint actions, where the ordinal policy can easily do both sampling and training. Motivated from the loss function of [28], it implicitly introduces internal ordering information between classes while maintaining the probabilistic properties of discrete distributions instead of simply parameterizing the discrete policy as a categorical distribution, ignoring the order of actions and the discrepancy relationship between the joint actions. Existing methods usually utilize some heuristic design of the network architecture to learn distributions for each action dimension, which could not guarantee a unimodal probability distribution output followed by the neural network to efficiently capture the ordinal information within atomic actions. It generally has a strong underlying assumption that the output logits of the neural network are unimodal for each dimension action space, which makes it hard to guide the neural network to optimize weights and generate probability distributions that closely resemble the optimal action. In other words, it may allocate greater confidence to multi-action classes, as it generally produces multi-modal probability distribution.\nTo leverage the continuity in the underlying action space, we focus on how the discretized actions are distributed: why restrict ourselves to the categorical distribution? For continuous control tasks, a Gaussian or Beta policy can explicitly capture the inherent ordering structure within an infinite number of actions due to the privilege of their unimodal distribution property. Is it possible to explicitly retain the unimodal structure and inherent ordinal information after discretizing the continuous action space for learning a policy over discrete actions? Inspired by the deep ordinal classification approach [27, 55], we constrain the distribution of the discretized actions to be unimodal explicitly for utilizing the ordinal information about the underlying continuous space. Specifically, we learn the probability mass function (PMF) of the Poisson distribution to enforce discrete unimodal probability distributions for each action dimension. This PMF can facilitate the generation of the most coherent unimodal probability distribution, as it diminishes the probability mass on both sides of the action possessing the majority of the mass, allowing us to adeptly capture the intrinsic ordering structure present within the action space.\nThe Poisson distribution, a widely recognized discrete probability distribution, is typically used to model the likelihood of observing a specific count of events within a designated time interval. It is characterized by a positive constant mean rate, denoted as \u03bb\u2208 R\u207a, and presumes that events transpire independently of the elapsed time since the previous event. In other words, the distribution describes the probability of k\u2208 N events occurring during a given time interval, where k can take on any non-negative integer value. For i-th dimension of action space, the Poisson distribution function is denoted as\np(k; \u03bb\u1d62) = (\u03bb\u1d62^k)/(k!) * exp(\u2212\u03bb\u1d62), and 0 \u2264 k \u2264 K \u2212 1 (minus one since we index from zero). For a purely technical implementation reason, we focus on the log of the PMF, which can be expressed as\nlog [(\u03bb\u1d62^k)/(k!) * exp(\u2212\u03bb\u1d62)] = k log(\u03bb\u1d62) \u2212 \u03bb\u1d62 \u2212 log(k!).\nWe denote the scalar output of our deep network as F(s) = [f1(s), f2(s), ..., f\u1d62(s)], where f\u1d62(s) > 0 is enforced to be positive using the Softplus nonlinearity. With such a parameterization, our method only needs to learn M probability mass functions with M units in the network output layer, in contrast to existing ordinal parameterization methods that require learning the complete probability distributions over all dimensions using M * K units in the network output layer [20]. Especially in high-dimensional continuous action spaces, our unimodal method can make it more tractable to implement with deep neural network representations. Meanwhile, our method can eliminate the assumption that the ordinal parameterization method could produce a unimodal probability distribution only when the logits of the network output are unimodal. By simply replacing the \u03bb\u1d62 in equation (8) with f\u1d62(s), we denote h\u1d62(s)j to be\nj log(f\u1d62(s)) - f\u1d62(s) \u2212 log(j!).\nIn the RL community, the categorical distribution architecture is widely used to parameter a discrete distribution over K action classes, which is represented by K logits denoted as h\u1d62(s)j. Due to the fact that the Poisson distribution has infinite support, we use a 'right-truncated' operation by applying a Softmax operation and obtain the corresponding probability distribution p(a\u1d62j|s) as\n$p(a_{ijs}) = \\frac{exp(-h_i(s)j/\\tau)}{\\sum_{j=0}^{K-1} exp(-h_i(s)j/\\tau)}, 0 \\leq j \\leq K - 1$.\nBesides, we introduce the hyperparameter \u03c4 to the Softmax function to control the variance of the distribution.\u00b9 After these operations, the probability distribution remains unimodal, which is clearly illustrated in Fig. 2.\nIn our unimodal policy architecture, we retain these logits h\u1d62(s)j with the support of the Poisson distribution, and"}, {"title": "C. Variance Analysis", "content": "While PG algorithms are well-suited for parametric distribution functions, it is susceptible to converging to a poor policy that is not even a local optimum. One of the primary limitations of the plain policy gradient method is the high variance of the policy gradient estimator. Using the Gaussian distribution as a stochastic policy in continuous control, \u03c0\u03b8(a|s) = 1/(\u221a2\u03c0\u03c3) * exp(\u2212(\u03b1\u2212\u03bc)\u00b2/(2\u03c3\u00b2)), has been well-studied and commonly used in the RL community since [32], because the Gaussian distribution is easy to sample and has gradients that are easy to compute. However, one undesirable property of Gaussian policy is that the variance of the policy gradient estimator tends to increase as the standard deviation \u03c3\u00b2 of the Gaussian distribution decreases [41]. Generally, the policy gradient estimator is given as\n\u011d\u03b8 = r(aj)\u2207\u03b8 log p\u03b8(j), j \u223c p\u03b8(j),\nand the variance can be represented as\n$V [\\hat{g}_{\\theta}] = \\mathbb{E} [\\hat{g}_{\\theta}^2] - \\mathbb{E}^2 [\\hat{g}_{\\theta}]$"}]}