{"title": "Discretizing Continuous Action Space with Unimodal Probability Distributions for On-Policy Reinforcement Learning", "authors": ["Yuanyang Zhu", "Zhi Wang", "Yuanheng Zhu", "Chunlin Chen", "Dongbin Zhao"], "abstract": "For on-policy reinforcement learning, discretizing action space for continuous control can easily express multiple modes and is straightforward to optimize. However, without considering the inherent ordering between the discrete atomic actions, the explosion in the number of discrete actions can possess undesired properties and induce a higher variance for the policy gradient estimator. In this paper, we introduce a straightforward architecture that addresses this issue by constraining the discrete policy to be unimodal using Poisson probability distributions. This unimodal architecture can better leverage the continuity in the underlying continuous action space using explicit unimodal probability distributions. We conduct extensive experiments to show that the discrete policy with the unimodal probability distribution provides significantly faster convergence and higher performance for on-policy reinforcement learning algorithms in challenging control tasks, especially in highly complex tasks such as Humanoid. We provide theoretical analysis on the variance of the policy gradient estimator, which suggests that our attentively designed unimodal discrete policy can retain a lower variance and yield a stable learning process.", "sections": [{"title": "I. INTRODUCTION", "content": "EEP reinforcement learning (DRL) has presented a pow-erful paradigm for learning new behaviors from scratchboth on physical and simulated challenging tasks [1\u20134]. Theaction space of conventional reinforcement learning (RL)tasks can be discrete, continuous, or some combination ofboth [5, 6]. In simulation [7] and real life [8\u201311], continuouscontrol usually requires some subtle parametric functions fora compact representation of action distributions, typicallyGaussians, due to an infinite number of feasible actions in acontinuous action space [11\u201314]. This underlying assumptionis that it can enable more refined decisions when controlpolicies can cover all feasible control inputs, which bringsdifficulty in maximizing an arbitrary function for a continuous action space [15, 16]. Furthermore, it hinders applyingdynamic programming approaches to back up value functionestimates from successor states to parenting states.Discretizing a continuous control problem is a scalablesolution that retains the simplicity of discrete actions andallows for theoretical analysis [10, 17, 18]. With such a setup,impressive results have been obtained in high-dimensionalcontinuous action spaces [10, 19, 20]. However, thenaive discretization approach results in exponentially largediscrete action space, where ballooning action space can quicklybecome intractable. It would be a balancing dilemma thatthe resulting output may not be smooth enough when thediscretization is coarse, and the number of discretized actionsmay be highly intractable otherwise. Moreover, more finediscretization of the action space may struggle to capture theinformation about the class ordering of the continuous actionspace, making it challenging to generalize across discreteactions. Furthermore, it also brings a larger variance of thepolicy gradient estimator, which may hurt the stability andperformance, especially for highly complex tasks [21, 22]. Inpractice, more fine-tuned hyperparameters are also required toprevent policy collapse.To tackle these intractabilities, prior works [20, 23] as-sume that action dimensions are independent and developedto factorize distributions across action dimensions using anordinal parameterization of the joint distribution over discreteactions, which has shown improved performance in findingnear-optimal solutions for high-dimensional tasks. By param-eterizing the discrete policy using an ordinal parameterization,the natural ordering between discrete actions can be implicitlyencoded, which provides an additional inductive bias that canimprove the generalization across actions [24\u201326]. In contrast,the conventional approach of parameterizing the discrete pol-icy as a Gibbs distribution fails to consider the order of actionsand ignores the discrepancy relationships between them [27].Existing methods usually use some heuristic network archi-tecture design to implicitly encode the ordinal informationinto the distribution over discrete actions, which may notguarantee a unimodal probability distribution to efficientlycapture the ordinal information within atomic actions [28].In their work, they alleviate this issue by designing the lossfunction to impose an ordering on the action, which does notimpose unimodal constraints. Here, we propose a more explicit and efficient ordinal parameterization method that betterincorporates the notion of continuity when parameterizing thedistribution over discrete actions, while avoiding the need foran exponentially large number of actions. Additionally, weimpose an ordering constraint on the action space of eachdimension through the loss function to further improve theperformance of RL algorithms.This paper mainly addresses how to efficiently leveragethe internal ordering information in the continuous actionspace to enhance generalization across the discretized set ofactions for on-policy RL without an explosion in the numberof parameters. Inspired by the deep ordinal classificationapproach [27], for each independent action dimension, weconstrain the distribution over discrete actions to be unimodalto explicitly exploit the inherent ordering between discreteactions. To implement the unimodal constraint, we employ thePoisson probability distribution, which allows the probabilitymass to decrease gradually on both sides of the action withthe majority of the mass. Our framework necessitates learn-ing a probability mass function for each action dimension,substantially reducing the number of units in the networkoutput layer and enabling us to circumvent the curse ofdimensionality as the discretization action bins increase (seeFig. 1). It facilitates fine-grained discretization of individualdomains without incurring a significant increase in the numberof parameters. The experimental results show that our methodoutperforms the baselines by a wide margin in the suite ofMuJoCo continuous control tasks [29].We highlight the following contributions of this work:We propose a novel ordinal framework that enforcesthe distribution over discrete actions to be unimodal,effectively capitalizing on the continuity presented inthe action space. We utilize the probability mass function ofthe Poisson distribution to enforce unimodal probabilitydistributions for each dimension action space, which canplace more confidence on the majority probability distri-bution benefiting from its inherent unimodal distribution.Theoretically, we present a variance analysis for thepolicy gradient estimator, where our unimodal policyattains a lower variance for the estimator.We show experimentally that an ordinal parameterizationof the unimodal policy outperforms other competingapproaches for on-policy RL on continuous control tasks.The promising results showcase the versatility and effec-tiveness of our methods compared to the baselines.The remaining paper is organized as follows. In Section II,we summarize preliminaries of RL and related literature. InSection III, we present the unimodal probability distributionsfor on-policy RL. In Section IV, we give comprehensiveexperimental results of learning performance and stabilityanalysis. The paper is concluded by Section V."}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "RL is an approach to solving optimal sequential decision-making tasks, built upon the concept of Markov DecisionProcesses (MDPs). In the finite MDP framework, a tuple of(S, A, T, R, \u03b3) defines the problem, where S is a countablestate space, A is a finite action set, $T: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$represents the transition kernel, $R: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the rewardfunction, and \u03b3\u2208 [0,1) is the discount factor. A stochasticpolicy is defined as $\\pi: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$, which mapsenvironmental states to distributions over actions with theconstraint $\\Sigma_{a \\epsilon \\mathcal{A}} \\pi(a|s) = 1,\\forall s \\epsilon \\mathcal{S}$. The objective of theRL problem is to find an optimal policy \u03c0* that maximizesthe expected total discounted return by\n$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi(\\tau)}[r(\\tau)] = \\mathbb{E}_{\\tau \\sim \\pi(\\tau)} \\bigg[ \\Sigma_{t=0}^{T} \\gamma^{t} r_{t} \\bigg]$\t(1)\n$Q^{\\pi}(s, a) = \\mathbb{E}_{s_{0}=s,a_{0}=a,\\tau \\sim \\pi} \\bigg[ \\Sigma_{t=0}^{T} \\gamma^{t} r_{t} \\bigg]$\t(2)\nwhere \u03c4 = (so, ao, s1, a1,...) is a learning episode, $\u03c0(\u03c4) =p(s_{0}) \\Pi_{t=0}^{T} \u03c0(a_{t}|s_{t})p(s_{t+1}|s_{t}, a_{t})$, and rt is the immediate reward. The policy function h(\u00b7 | \u03b8) can be represented by a deep neural network parameterized by \u03b8. For a discrete actionspace, the Gibbs distribution is commonly used and given by\n$\\pi_{\\theta}(a; \\vert s) = \\frac{exp\\left(h_{j}(s\\vert\\theta)\\right)}{\\Sigma_{j \\epsilon \\mathcal{A}}(s) exp\\left(h_{j}(s \\vert \\theta)\\right)}$,\t(3)\nwhile the Gaussian distribution is generally used for a contin-uous action space and is given by\n$\\pi_{\\theta}(a \\vert \\delta) = \\frac{1}{\\sqrt{2\\pi\\sigma}} exp \\bigg(-\\frac{1}{2\\sigma^{2}} (h(s\\vert\\theta) - a)^{2} \\bigg)$\t(4)"}, {"title": "B. Related Work", "content": "On-policy RL. On-policy RL is a family of RL algo-rithms aiming to directly optimize the parameters of a pol-icy, which optimizes expected returns by estimating policygradients. This estimation is often prone to high variance,and several solutions have been proposed to mitigate thissignificant challenge, particularly in problems characterized bylong horizons or high-dimensional action spaces [22, 33, 34].Classic vanilla policy gradient (PG) updates are generallyunstable with high variance due to difficulties in creditingactions that influence future rewards, where the gradientsrequire knowledge of the probability of the performed actionvia resorting to parametric distributions. Natural policy [35]improves upon vanilla PG by computing an ascent direc-tion with the Fisher information that approximately ensuresa slight change in the policy distribution. To obtain morestable learning, trust region policy optimization (TRPO) [7]utilizes a larger Kullback-Leibler (KL) divergence value toenforce constraints, and performs a line search in the naturalgradient direction, ensuring improvements in the surrogateloss function. Proximal policy optimization (PPO) [36] re-places the KL divergence constraint with the clipped surrogateobjective, which strikes a favorable balance across samplecomplexity, simplicity, and wall time. Moreover, actor-criticusing Kronecker-factored trust region (ACKTR) [37] buildsupon the natural policy gradient framework by calculatinggradients through Kronecker-factored approximate curvaturewithin the trust region. Orthogonal to the above algorithms,we demonstrate that ordinal parameterization with a unimodalconstraint for on-policy RL achieves consistently improvedperformance based on the above representative algorithms.Policy Representation. In the context of policy gradienttechniques, the policy is generally parameterized using neuralnetworks, where the policy is improved by optimizing theparameter of the approximation function. In general, the policyuses a Gibbs probability distribution over the discrete actionspace to sample. For the continuous control problems, thedefault choice for the policy in the baseline is parameterized bylearning a Gaussian distribution with independent componentsfor each dimensional action space [7, 36]. The Gaussianmixture, maximum entropy, or normalizing flows [38\u201340]can be used for more expressive policy classes, providing apromising avenue for improved robustness and stability. Sincephysical constraints in most continuous control tasks, actionscan only take on values within some finite interval, resultingin an unavoidable estimation bias caused by boundary effects.To address the shortcomings of the Gaussian distributionwith a finite support distribution, the Beta distribution [41]provides a bias-free and less noise policy with improvedperformance. Further, considering the extremes along eachaction dimension, a Bernoulli distribution is applied to replacethe Gaussian parametrization of continuous control methodsand show improved performance in several continuous controlbenchmarks [17]. Here, we provide insight into how to producean efficient ordinal parameterization policy while retaining theinternal order information of continuous action space witha simple explicit constraint, which can yield state-of-the-artperformance compared to baseline policy classes.Continuous action discretization. To leverage the con-tinuity between discrete and continuous action space, con-verting continuous control problems into discrete ones hasbeen introduced by [42] with the \u201cbang-bang\u201d controller [43].Similar discretization methods represent each action in binaryformat and optimize its policy for MDPs with large actionsets [44\u201348]. However, such discretization methods need tobe improved due to the curse of dimensionality. Surprisingly,recent works have relieved the limitation by assuming thataction dimensions are independent [10, 20, 49\u201351], whichobtains improved performance in complex control tasks. In-spired by multi-agent reinforcement learning, Decoupled Q-Networks (DecQN) [52] factorizes the overall state-actionvalue function into a linear combination of single actionutility functions over action dimensions by combining valuedecomposition with bang-bang action space discretization.It achieves better performance while reducing agent actionspace complexity. However, such a discretization paradigmmakes the strong assumption of independence among eachaction dimension and hardly captures the continuous orderinformation of actions [53, 54].To mitigate the exponential explosion of discrete ac-tions, [50] utilizes sequence-to-sequence models to developpolicies for structured prediction problems, but their strategyhas only demonstrated effectiveness in high-dimensional taskslike Humanoid and HumanoidStandup tasks. Recently, [20]parameterizes the discrete distributions with an ordinal ar-chitecture and achieves improved performance. Nevertheless,existing methods face a dilemma in that the number of networkoutputs increases linearly with the number of action spacedimensions, which may bring high variance, resulting in amore unstable learning process. A key distinction betweenour work and previous works lies in our explicit utilization ofunimodal distributions with only one parameter for each actiondimension. It can help the policy capture the most confidentclass while ensuring that the probability gradually decreases"}, {"title": "III. UNIMODAL PROBABILITY DISTRIBUTIONS FOR ON-POLICY REINFORCEMENT LEARNING", "content": "In this section, we will present our proposed unimodal dis-tribution with Poisson probability distribution for on-policy RLalgorithms, which explicitly introduce unimodal probabilitydistribution into the ordinal parameterization method to lever-age the continuity in the underlying continuous action space.First, we present the process of discretizing action space forcontinuous control tasks and apply it to on-policy RL. Then,we describe the unimodal ordinal architecture that constrainsthe discrete policy to be unimodal via the Poisson probabilitydistribution in a practical neural network implementation.Finally, theoretical analysis shows that our method can relievethe high variance trouble of the policy gradient estimatorcompared to existing ordinal parameterization methods.\nConsider an environment with state st and m dimensionaction spaces a \u2208 Rm. To ensure generality, we consideran action space A = [-1,1]m and proceed to discretizeeach dimension of the action space into K evenly spacedatomic action bins. As a result, we obtain a discrete andfinite set of atomic actions $A_{i} = \\lbrace -1 + \\frac{2j}{K-1} \\rbrace_{j=1}^{K-1}$ for the iaction dimension. To facilitate the joint policy's tractability,the policy generates a tuple of discrete actions accompanied byfactorized categorical distributions across action dimensions.This approach obviates the need to enumerate every possibleactions, as required in the discrete case. Specifically, we denotea categorical distribution \u03c0\u03b8i(ai|s) as the atomic action foractions a\u017c \u2208 A at each step t, where \u03b8i is the factorizationparameters for this marginal distribution induced by Poissionprobability distributions. Such representations can build com-plex policy distributions with explicit probability distributions,where any action subset can be formulated as\n$\\pi (a \\vert s) = \\Pi_{i=1}^{I} \\pi_{\\theta_{i}}(a_{i} \\vert s)$,\t(7)"}, {"title": "B. Unimodal ordinal architecture", "content": "In on-policy RL algorithms, the policy gradient involves astochastic policy that specifies the probability distribution ofselecting an action a given a state s. Upon discretizing thecontinuous action space, the complexity of directly trainingsuch policy networks could lead to Km combinations ofjoint atomic actions, which exponentially increase with thegrowing number of action dimensions m. Using the categoricaldistribution (e.g., Gumbel-Softmax distribution) as a stochasticpolicy for discrete action space has been well-studied in theRL community. Popular categorical distributions have beenused as a stochastic policy for discrete action spaces due totheir ease of sampling and computation. However, the curseof dimensionality can lead to intractable action spaces, puttingsignificant pressure on the distribution parameterization overdiscrete actions. In fact, a fine control policy can be capturedmore as the resolution of the discretized action space increases.This quickly plunges into the dilemma that intractable actionspace puts high pressure on the parameterizing distributionsover discrete actions, which hinders capturing the orderingamong actions.To relieve the scalability issue, a few prior works have beenexploring more expressive distributions and notably proposedan ordinal parameterization [20, 53, 54]. It introduces an im-plicit loss function based on factorizing a tractable distributionover joint actions, where the ordinal policy can easily doboth sampling and training. Motivated from the loss functionof [28], it implicitly introduces internal ordering informationbetween classes while maintaining the probabilistic propertiesof discrete distributions instead of simply parameterizing thediscrete policy as a categorical distribution, ignoring the orderof actions and the discrepancy relationship between the jointactions. Existing methods usually utilize some heuristic designof the network architecture to learn distributions for eachaction dimension, which could not guarantee a unimodalprobability distribution output followed by the neural networkto efficiently capture the ordinal information within atomicactions. It generally has a strong underlying assumption thatthe output logits of the neural network are unimodal foreach dimension action space, which makes it hard to guidethe neural network to optimize weights and generate proba-bility distributions that closely resemble the optimal action.In other words, it may allocate greater confidence to multi-action classes, as it generally produces multi-modal probabilitydistribution.To leverage the continuity in the underlying action space, wefocus on how the discretized actions are distributed: why re-strict ourselves to the categorical distribution? For continuouscontrol tasks, a Gaussian or Beta policy can explicitly capturethe inherent ordering structure within an infinite number ofactions due to the privilege of their unimodal distribution prop-erty. Is it possible to explicitly retain the unimodal structureand inherent ordinal information after discretizing the contin-uous action space for learning a policy over discrete actions?Inspired by the deep ordinal classification approach [27, 55],we constrain the distribution of the discretized actions to beunimodal explicitly for utilizing the ordinal information aboutthe underlying continuous space. Specifically, we learn theprobability mass function (PMF) of the Poisson distributionto enforce discrete unimodal probability distributions for eachaction dimension. This PMF can facilitate the generation ofthe most coherent unimodal probability distribution, as itdiminishes the probability mass on both sides of the actionpossessing the majority of the mass, allowing us to adeptlycapture the intrinsic ordering structure present within theaction space.The Poisson distribution, a widely recognized discrete prob-ability distribution, is typically used to model the likelihoodof observing a specific count of events within a designatedtime interval. It is characterized by a positive constant meanrate, denoted as \u03bb\u2208 R\u207a, and presumes that events transpireindependently of the elapsed time since the previous event.In other words, the distribution describes the probability ofk\u2208 N events occurring during a given time interval, where kcan take on any non-negative integer value. For i-th dimensionof action space, the Poisson distribution function is denoted as\n$p(k; \\lambda_{i}) = \\frac{\\lambda^{k}}{k!} exp(-\\lambda_{i})$,\tand 0 \u2264 k \u2264 K \u2212 1 (minus one sincewe index from zero). For a purely technical implementationreason, we focus on the log of the PMF, which can beexpressed as\n$\\log \\bigg[ \\frac{\\lambda^{k}}{k!}exp(-\\lambda_{i})\\bigg]$\t=\t$\\log (\\lambda^{k}) + log(exp(-\\lambda_{i})) - log(k!)$\t=\t$klog (\\lambda_{i}) - \\lambda_{i} - log(k!)$\t=\t$k log(\\lambda_{i}) + log(exp(-\\lambda_{i})) - log(k!)$,\t(8)\nWe denote the scalar output of our deep network as F(s) = [f1(s), f2(s), ..., fi(s)], where fi(s) > 0 is enforced to bepositive using the Softplus nonlinearity. With such a parame-terization, our method only needs to learn M probability massfunctions with M units in the network output layer, in contrastto existing ordinal parameterization methods that require learn-ing the complete probability distributions over all dimensionsusing M * K units in the network output layer [20]. Especiallyin high-dimensional continuous action spaces, our unimodalmethod can make it more tractable to implement with deepneural network representations. Meanwhile, our method caneliminate the assumption that the ordinal parameterizationmethod could produce a unimodal probability distribution onlywhen the logits of the network output are unimodal. By simplyreplacing the \u03bbi in equation (8) with fi(s), we denote hi(s)jto be\n$jlog(f_{i}(s)) - f_{i}(s) - log(j!)$,\t(9)\nIn the RL community, the categorical distribution architec-ture is widely used to parameter a discrete distribution overK action classes, which is represented by K logits denoted ashi(s)j. Due to the fact that the Poisson distribution has infinitesupport, we use a \u2018right-truncated\u2019 operation by applying aSoftmax operation and obtain the corresponding probabilitydistribution p(aij) as\n$p(a_{ijs}) = \\frac{exp(-h_{i}(s)_{j}/\\tau)}{\\Sigma_{j=0}^{K-1}exp(-h_{i}(s)_{j}/\\tau)}$\t, 0 \u2264 j \u2264 K \u2212 1.\t(10)\nBesides, we introduce the hyperparameter 1 to the Softmaxfunction to control the variance of the distribution.\u00b9 Afterthese operations, the probability distribution remains uni-modal, which is clearly illustrated in Fig. 2.In our unimodal policy architecture, we retain these logitshi(s) with the support of the Poisson distribution, and"}, {"title": "C. Variance Analysis", "content": "While PG algorithms are well-suited for parametric dis-tribution functions, it is susceptible to converging to a poorpolicy that is not even a local optimum. One of the primarylimitations of the plain policy gradient method is the highvariance of the policy gradient estimator. Using the Gaus-sian distribution as a stochastic policy in continuous control,$\\pi_{\\theta}(a \\vert s) = \\frac{1}{\\sqrt{2\\pi\\sigma}} exp\\bigg(-\\frac{1}{2\\sigma^{2}} (a-\\mu)^{2} \\bigg)$, has been well-studied andcommonly used in the RL community since [32", "41": ".", "hat{g}_{\\theta}": "mathbb{E} [\\hat{g}_{\\theta}^{2}"}, {"hat{g}_{\\theta}": "t(13)\nFor Gaussian distribution, as the policy improves and becomesmore deterministic (\u03c3 \u2192 0), the variance of the policy gradientestimator goes to infinity, which makes PG methods sample-inefficient with respect to interactions with the environmentand hinders the applications to real physical control systems.As mentioned earlier, the variance of the Poisson distribu-tion is equivalent to its mean, whose properties can alleviatethe above high variance problem. Before showing the theo-retical analysis for the variance of unimodal policy, we firstmake the following assumption that considering a simplifiedsetting of a one-step bandit problem with one dimension actionspace A = [-1,1"}, {"hat{g}_{\\theta}": "nabla_{\\theta}J (\\pi_{\\theta}) = 0$ and the variance is\n$V [\\hat{g}_{\\theta}"}]}