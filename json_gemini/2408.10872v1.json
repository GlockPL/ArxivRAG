{"title": "V-RoAst A New Dataset for Visual Road Assistant", "authors": ["Natchapon Jongwiriyanurak", "Zichao Zeng", "June Moh Goo", "Xinglei Wang", "Ilya Ilyankou", "Kerkritt Srirrongvikrai", "Meihui Wang", "James Haworth"], "abstract": "Road traffic crashes cause millions of deaths annually and have a significant economic impact, particularly in low- and middle-income countries (LMICs). This paper presents an approach using Vision Language Models (VLMs) for road safety assessment, overcoming the limitations of traditional Convolutional Neural Networks (CNNs). We introduce a new task, V-RoAst (Visual question answering for Road Assessment), with a real-world dataset. Our approach optimizes prompt engineering and evaluates advanced VLMs, including Gemini-1.5-flash and GPT-4o-mini. The models effectively examine attributes for road assessment. Using crowdsourced imagery from Mapillary, our scalable solution influentially estimates road safety levels. In addition, this approach is designed for local stakeholders who lack resources, as it does not require training data. It offers a cost-effective and automated methods for global road safety assessments, potentially saving lives and reducing economic burdens.", "sections": [{"title": "1 Introduction", "content": "Road traffic incidents were estimated to cause 1.19 million deaths worldwide in 2021 (WHO 2023), with an economic impact of approximately 10-12% of the global Gross Domestic Product (GDP) (WHO 2023; Chen et al. 2019). In low- and middle-income countries (LMICs), road crashes rank as the leading cause of death, with fatality rates significantly higher than those of high-income countries (HICs). A significant portion of these fatalities involves motorcyclists, who are among the most vulnerable road users in LMICS. Consequently, the United Nations (UN) has aimed to ensure that all new roads are built to achieve a rating of at least 3 stars according to the International Road Assessment Programme (iRAP) standard for all road users, including cars, motorcycles, bicycles, and pedestrians. The rating scale ranges from 1 to 5, with 5 indicating a safe road and 1 indicating an unsafe road (WHO 2023). Furthermore, another objective is to improve 75% of existing roads to more than 3 stars by 2030.\nThe workflow of iRAP involves road surveys, coding attributes, developing the model, and analysing the results. Road surveying requires vehicles and sensors to capture accurate georeferenced images for coding. Once georeferenced images are obtained, trained coders must examine and classify the images according to the codebook manual (iRAP 2024a,b), which requires training and experience. Currently, only highways have been rated due to the prohibitive cost of the survey. Importantly, it is almost impossible for LMICs to assess all roads following this standard. This leaves the vast majority of the road network unrated, making it difficult to reveal the infrastructure risk factors contributing to road deaths.\nAlthough Accelerated and Intelligent RAP (AiRAP) has initiated several programmes to help provide tools and attributes to local authorities, these programmes do not cover all cities and countries and come with associated costs. To save costs, automated road feature detection directly from captured images is a well-known approach that primarily employs models based on Convolutional Neural Networks (CNN). Although they are inexpensive compared to manual labelling, these models require labelled data for training, which is time-consuming and limits region-scalability since road attributes can visually vary in different cities and countries (Kacan et al. 2020; Pubudu Sanjeewani and Verma 2019; Jan et al. 2018; Sanjeewani and Verma 2021b,a). Additionally, some works explore alternative data sources, such as Light Detection and Ranging (LiDAR) (Brki\u0107 et al. 2022), satellite imagery (Brki\u0107 et al. 2023; Abdollahi et al. 2020), Unmanned Aerial Vehicles (UAV) (Brki\u0107 et al. 2020), and Global Positioning System (GPS) traces (Yin et al. 2023).\nRecently, many scholars have explored the performance of visual language models (VLMs) on various computer vision tasks. Since VLMs have been trained on large datasets of images and texts, they can perform tasks without additional model training. Recently, techniques such as prompt engineering, retrieval-augmented generation (RAG), and fine-tuning have been explored to enhance the potential of VLMs, as building these models requires significant computational resources and datasets. Furthermore, VLMs have shown their potential to handle complex tasks using zero-shot learning, such as building age classification (Zeng et al. 2024), building detection (Goo, Zeng, and Boehm 2024), motorcycle risk assessment (Jongwiriyanurak et al. 2023), land use classification (Wu et al. 2023), and building defects detection (Yong et al. 2023). However the zero-shot prediction and generalisation ability of VLMs on the road feature detection task is still unknown. Therefore, this work focusses on the potential of using VLMs to assist in road assessment.\nThis study develops prompts for VLMs to examine iRAP"}, {"title": "2 Related Work", "content": "Using engineers to survey, inspect, and assess roads on-site is time-consuming. Therefore, various studies have explored the use of computer vision models to assist in specific tasks, such as crack detection (Goo et al. 2024), pothole detection (Ma et al. 2022), and pavement distress detection (Ibragimov et al. 2022).\nSimilarly, examining road attributes following the iRAP standards has been dramatically advanced using images and videos. The availability of the AusRAP dataset with labelled street objects has enabled the development of models to detect street attributes. However, due to the limitations of visual scenes from videos and the feasibility of labelling objects (e.g., lane lines, rumble strips, poles, trees, barriers, and speed signs), only 13 attributes were able to be estimated (Jan et al. 2018; Pubudu Sanjeewani and Verma 2019; Sanjeewani and Verma 2021b).\nIn perspective of model architectures, Fully Convolutional Network (FCN) effectively segmented objects and defined attributes from images (Sanjeewani and Verma 2021a). Song et al. (2018) automated the process using star ratings from ground-level panoramic images. They developed a VGG-based model incorporating a task-dependent spatial attention layer, which was trained in a supervized multi-task learning manner. However, the task mainly focused on predicting star rating scores rather than coding road attributes. Some works have proposed multi-task convolutional models using monocular videos to predict 33 attributes (Ka\u010dan, \u0160evrovi\u0107, and \u0160egvi\u0107 2022; Kacan et al. 2020). However, all these works required large amounts of training data with dense pixel-level annotation, indicating significant annotation effort and substantial computational resources.\nIn addition, due to dataset imbalances, these works struggle with certain road features, such as pedestrian fencing and facilities for motorcycles and bicycles. Consequently, some of the attributes experience the problem of out-of-distribution (OOD), i.e., some attributes have classes with only a small amount of data or even missing data (Hendrycks and Gimpel 2016). This makes multi-attribute classification more challenging. Hence, these models require significant additional training data, even when using transfer learning or semi-supervized learning approaches."}, {"title": "2.1 Computer Vision for Road Attribute Detection", "content": "attributes by mimicking a coder observing an image and categorizing the attributes, as described in (iRAP 2024a,b). In summary, we state the main contributions of our work as follows:\n\u2022 We proposed a new image classification task for VLMs with a real-world dataset from ThaiRAP\n\u2022 We optimize the prompts and evaluate the potential of using VLMs to code road attributes using Gemini-1.5-Flash and GPT-40-mini compared to traditional computer vision models\n\u2022 We present a scalable approach using crowdsourced imagery, Mapillary, to estimate star ratings"}, {"title": "2.2 Vision Language Models", "content": "In recent years, Large Vision Language Models (LVLMs) have become significantly important in computer vision domain. VLMs can process image and text inputs and produce text outputs that can be used in many tasks, such as image captioning, image-text matching, visual reasoning, and visual question answering (VQA). LVLMs have been extensively developed by leading researchers using large amounts of text and image datasets with different architectures. Examples include GPT 40 (OpenAI 2024), Gemini 1.5 (Team 2024), CogVLM2 (Wang et al. 2024), and QwenVL (Bai et al. 2023). These models have been evaluated for their potential to outperform several traditional deep learning state-of-the-art benchmarks, including motion understanding (Zhou, Wan, and Wang 2023), 3D object generation (Siddiqui et al. 2023), and image segmentation (Hoyer et al. 2023). However, these models come with different licences, costs, and limitations.\nVLMs have also been trained, fine-tuned, and validated to understand street scenes, primarily focusing on the au-"}, {"title": "2.3 VQA", "content": "Visual Question Answering (VQA) was initially introduced as a new task in the computer vision and natural language processing domains (Agrawal et al. 2016). The task involves answering open-ended questions based on images. Several datasets are commonly used to evaluate models, including GQA (Hudson and Manning 2019), OK-VQA (Marino et al. 2019), A-OKVQA (Marino et al. 2019), and MMMU (Hendrycks et al. 2021).\nIn addition, VQA datasets focusing on autonomous vehicles, such as KITTI (Geiger et al. 2013) and NuScenes-QA (Qian et al. 2024), have been used to advance the field. Jain et al. (2024) found that GPT-4 performs well in robust driving scenarios.\nThis work uses a real-world dataset to define a new image classification dataset for VLMs as a downstream vision application. It does not consider training a new VLM, but instead explores the potential of existing LVLMs to perform the task as road assessment coders. Although using non-open-source models incurs some costs, this study aims to demonstrate that coding road assessments can benefit from existing LVLMs. It also allows for applying prompt engineering, retrieval-augmented generation (RAG), fine-tuning, or even building a new LVLM model."}, {"title": "3 Data and Experiments", "content": "The dataset used in this work is sourced from the ThaiRAP and was selected due to the availability of road assessment datasets adhering to the iRAP standard. The experiment includes 2,037 images (1600x1200 pixels) captured across Bangkok, Pathum Thani, and Phranakorn Sri Ayutthaya provinces, as illustrated in Figure 1. These images represent 519 road segments, with 1-4 images used to code each 100-meter segment. While iRAP datasets are typically not publicly available, this work will provide access to the ThaiRAP dataset, including both the images and their associated attributes, to support further research and development in road safety assessment.\nEach segment has 52 attributes classified by iRAP (iRAP 2024a,b). However, the attributes have a different number of classes (codes) in (parenthesis), and the number of classes of each attribute used in this experiment is shown at the top of each bar in Figure 2. This dataset shows the imbalance of classes of attributes, in which there are 11 attributes with only one class. And some classes have less than 10 samples.\nTo evaluate and compare our models against the baseline models - traditional computer vision methods, we divide the dataset (n=2037) into a training set"}, {"title": "3.1 ThaiRAP Dataset"}, {"title": "3.2 Baseline", "content": "VGGNet (Simonyan and Zisserman 2015) is a deep neural network model known for its simple architecture that stacks multiple convolutional layers with small 3x3 filters, achieving high performance in image classification tasks and becoming a widely used baseline in computer vision.\nResNet (He et al. 2015) is also a deep convolutional neural network that utilizes residual blocks and skip connections to enhance feature learning at various abstraction levels, making it highly effective for image classification and transfer learning tasks.\nIn our study, we use VGGNet and ResNet as baseline models to compare with our proposed approach. Since these models are designed for a single classification task, we design the baseline model architectures for a multi-attribute classification problem, where a single encoder is shared across all tasks, and separate decoders are allocated for each individual task. This structure allows the model to extract the common features from the encoders and use the features"}, {"title": "3.3 Experiments", "content": "In this section, we evaluate the framework of our proposed approach, V-RoASt, for examining 52 iRAP attributes from images, as shown in Figure 3. This framework is designed to be easily applicable in any city, requiring minimal data science expertise and coding experience. The approach includes text input for system instructions and prompts, as well as image prompts. The workflow operates by inputting an image and its associated information prompts, including task descriptions, contextual information, and output format, to generate responses for the 52 attributes of the image. We used data from ThaiRAP to evaluate the VLMs, as described in section 3.1.\nAs shown in the left box of Figure 3, instructions are divided into four parts: task specification, local context, attribute details, and output format. In the first part, task specification, we outline five steps to process each image step-by-step by giving instructions to define  of each  with its description from the image. The model is also instructed on how to format the output. This section includes country, which must be specified for VLM inference.\nIn the local context part, we provide information for the model since attributes are related to the side of the road on which countries drive (left or right). Specifying {country} helps VLMs understand which side of the image to consider. Importantly, we provide {local_context}, allowing local stakeholders to add information, such as speed limit laws or specific details of road attributes, to influence the models.\nThe next section details the 52 attributes () taken from iRAP coding manuals (iRAP 2024b,c). Each  includes the attribute name, attribute description (), and information on possible classes, including each class name () and class description ().\nLastly, we format the output by instructing the models to return the results in JSON format. We provide possible classes as a list and remind the model to choose the best match for each .\nIn this experiment, we input an image and its information, including the tags : {image_id} and : {latitude, longitude}. These prompts assist VLMs in formatting the output by referencing the image ID and incorporating location data, which aids in both image interpretation and contextual analysis. For example, by integrating local speed limit laws, VLMs can more effectively assess attributes such as motorcycle speed limits, truck speed limits, general speed limits, and differential speed limits, thereby enhancing the accuracy and relevance of the analysis within the context of local regulations. The following section will provide information on VLMs used in this study and experiment setup."}, {"title": "3.4 VLMs", "content": "This study utilizes Gemini-1.5-flash and GPT-40-mini to evaluate the framework and assess the potential of Visual Language Models (VLMs) to replicate the work of road safety auditors using the iRAP standard. Notably, Gemini-1.5-flash and GPT-40-mini do not require any training or significant computational resources, making them accessible for use by local stakeholders. The experiments were conducted through the Google AI Gemini platform and the OpenAI API.\nIn this study, 337 segments were used to evaluate the results of Gemini-1.5-flash and GPT-40-mini with the ResNet and VGG baseline models. Moreover, Unseen has 7 segments in total."}, {"title": "3.5 Image processor for Mapillary imagery", "content": "Crowdsourced Street View Images (SVIs) are accessible on various platforms, with Mapillary being one of the most well-known, providing an API for image downloads. For this study, images were obtained using a 50-meter buffer around ThaiRAP locations, under the condition that the images were captured within one year of the coding date, as the specific date of the road survey was unavailable. However, only 42 road segments were found to have corresponding Mapillary images, yielding 165 images. Panoramic images were converted to 1200x1600 binocular view images to align with the ThaiRAP data format. Subsequently, these images were processed using V-RoASt to examine attributes, as explained in 3.3. Other required attributes unavailable in the images were sourced from ground truth data. All attributes were then used to evaluate the star rating for all modes of transport, including cars, motorcycles, pedestrians, and bicycles, and these were compared against the ground-truth star ratings."}, {"title": "3.6 Evaluation Metrics", "content": "Coding options for each attribute are listed in order from highest to lowest risk. Therefore, if an attribute varies within a single coding segment, record the item that appears first in the list of options for that attribute.\nWe propose this VQA as an image classification task. To measure the performance of our model, we employ standard evaluation metrics in the field of image classification to measure the performance of our model."}, {"title": "4 Results and Discussions", "content": "In this section, we evaluate the attribute classification performance of V-RoAst using Gemini-1.5-flash and GPT-40-mini, compared to the baselines established by the ResNet and VGG models, as used in previous studies (Song et al. 2018; Kacan et al. 2020). The following subsections will discuss the classification results using 4 metrics: accuracy, precision, recall, and F1 score."}, {"title": "4.1 Scalable Star Rating Prediction", "content": "strong performance on unseen data. Interestingly, the baseline model performs well on data involving spatial information, such as distance, even though it did not see such data during training. This suggests that the spatial information learned in other tasks may have been transferred during the multi-attribute learning process.\nThe attributes where VLMs perform well are also crucial for assessing road safety, indicating that VLMs have promising potential in this area. Given the strong performance of baseline models on problems involving spatial information, a collaborative approach where VLMs receive support from baseline models in handling spatial information could yield improved results."}, {"title": "4. Qualitative Assessment using VQA"}, {"title": "5 Conclusion", "content": "In this study, we proposed the V-RoAst approach utilizing Visual Language Models (VLMs) such as Gemini-1.5-flash and GPT-40-mini, and compared their performance with traditional CNN models like ResNet and VGG. The results indicate that our approach classifies road attributes effectively as the traditional models. Additionally, the V-RoAst approach leverages VLMs' ability to perform Visual Question Answering (VQA), enabling local authorities in LMICs to adapt and enhance accuracy through intuitive prompt engineering. However, it is noted that VLMs demonstrate relatively weaker performance in dealing with spatial attributes.\nV-RoAst offers a cost-effective and automated method for global road safety assessments, potentially saving lives and reducing economic burdens. Fine-tuning VLMs for robustness and versatility regardless of the country remains a future work. Moreover, other modalities, such as remote sensing imagery and geographical information data, can be beneficial for reliable road assessment."}]}