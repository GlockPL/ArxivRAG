{"title": "V-RoAst A New Dataset for Visual Road Assistant", "authors": ["Natchapon Jongwiriyanurak", "Zichao Zeng", "June Moh Goo", "Xinglei Wang", "Ilya Ilyankou", "Kerkritt Srirrongvikrai", "Meihui Wang", "James Haworth"], "abstract": "Road traffic crashes cause millions of deaths annually and\nhave a significant economic impact, particularly in low- and\nmiddle-income countries (LMICs). This paper presents an\napproach using Vision Language Models (VLMs) for road\nsafety assessment, overcoming the limitations of traditional\nConvolutional Neural Networks (CNNs). We introduce a\nnew task, V-RoAst (Visual question answering for Road\nAssessment), with a real-world dataset. Our approach op-\ntimizes prompt engineering and evaluates advanced VLMs,\nincluding Gemini-1.5-flash and GPT-4o-mini. The models\neffectively examine attributes for road assessment. Using\ncrowdsourced imagery from Mapillary, our scalable solution\ninfluentially estimates road safety levels. In addition, this ap-\nproach is designed for local stakeholders who lack resources,\nas it does not require training data. It offers a cost-effective\nand automated methods for global road safety assessments,\npotentially saving lives and reducing economic burdens.", "sections": [{"title": "1 Introduction", "content": "Road traffic incidents were estimated to cause 1.19 million\ndeaths worldwide in 2021 (WHO 2023), with an economic\nimpact of approximately 10-12% of the global Gross Do-\nmestic Product (GDP) (WHO 2023; Chen et al. 2019). In\nlow- and middle-income countries (LMICs), road crashes\nrank as the leading cause of death, with fatality rates signifi-\ncantly higher than those of high-income countries (HICs).\nA significant portion of these fatalities involves motorcy-\nclists, who are among the most vulnerable road users in\nLMICS. Consequently, the United Nations (UN) has aimed\nto ensure that all new roads are built to achieve a rating of\nat least 3 stars according to the International Road Assess-\nment Programme (iRAP) standard for all road users, includ-\ning cars, motorcycles, bicycles, and pedestrians. The rating\nscale ranges from 1 to 5, with 5 indicating a safe road and\n1 indicating an unsafe road (WHO 2023). Furthermore, an-\nother objective is to improve 75% of existing roads to more\nthan 3 stars by 2030.\nThe workflow of iRAP involves road surveys, coding at-\ntributes, developing the model, and analysing the results.\nRoad surveying requires vehicles and sensors to capture\naccurate georeferenced images for coding. Once georefer-\nenced images are obtained, trained coders must examine\nand classify the images according to the codebook man-\nual (iRAP 2024a,b), which requires training and experience.\nCurrently, only highways have been rated due to the pro-\nhibitive cost of the survey. Importantly, it is almost impos-\nsible for LMICs to assess all roads following this standard.\nThis leaves the vast majority of the road network unrated,\nmaking it difficult to reveal the infrastructure risk factors\ncontributing to road deaths.\nAlthough Accelerated and Intelligent RAP (AiRAP) has\ninitiated several programmes to help provide tools and at-\ntributes to local authorities, these programmes do not cover\nall cities and countries and come with associated costs. To\nsave costs, automated road feature detection directly from\ncaptured images is a well-known approach that primarily\nemploys models based on Convolutional Neural Networks\n(CNN). Although they are inexpensive compared to man-\nual labelling, these models require labelled data for train-\ning, which is time-consuming and limits region-scalability\nsince road attributes can visually vary in different cities and\ncountries (Kacan et al. 2020; Pubudu Sanjeewani and Verma\n2019; Jan et al. 2018; Sanjeewani and Verma 2021b,a). Ad-\nditionally, some works explore alternative data sources, such\nas Light Detection and Ranging (LiDAR) (Brki\u0107 et al. 2022),\nsatellite imagery (Brki\u0107 et al. 2023; Abdollahi et al. 2020),\nUnmanned Aerial Vehicles (UAV) (Brki\u0107 et al. 2020), and\nGlobal Positioning System (GPS) traces (Yin et al. 2023).\nRecently, many scholars have explored the performance\nof visual language models (VLMs) on various computer vi-\nsion tasks. Since VLMs have been trained on large datasets\nof images and texts, they can perform tasks without addi-\ntional model training. Recently, techniques such as prompt\nengineering, retrieval-augmented generation (RAG), and\nfine-tuning have been explored to enhance the potential of\nVLMs, as building these models requires significant com-\nputational resources and datasets. Furthermore, VLMs have\nshown their potential to handle complex tasks using zero-\nshot learning, such as building age classification (Zeng et al.\n2024), building detection (Goo, Zeng, and Boehm 2024),\nmotorcycle risk assessment (Jongwiriyanurak et al. 2023),\nland use classification (Wu et al. 2023), and building de-\nfects detection (Yong et al. 2023). However the zero-shot\nprediction and generalisation ability of VLMs on the road\nfeature detection task is still unknown. Therefore, this work\nfocusses on the potential of using VLMs to assist in road\nassessment.\nThis study develops prompts for VLMs to examine iRAP"}, {"title": "2 Related Work", "content": "attributes by mimicking a coder observing an image and cat-\negorizing the attributes, as described in (iRAP 2024a,b). In\nsummary, we state the main contributions of our work as\nfollows:\n\u2022 We proposed a new image classification task for VLMs\nwith a real-world dataset from ThaiRAP\n\u2022 We optimize the prompts and evaluate the potential of us-\ning VLMs to code road attributes using Gemini-1.5-Flash\nand GPT-40-mini compared to traditional computer vi-\nsion models\n\u2022 We present a scalable approach using crowdsourced im-\nagery, Mapillary, to estimate star ratings\n2.1 Computer Vision for Road Attribute\nDetection\nUsing engineers to survey, inspect, and assess roads on-site\nis time-consuming. Therefore, various studies have explored\nthe use of computer vision models to assist in specific tasks,\nsuch as crack detection (Goo et al. 2024), pothole detection\n(Ma et al. 2022), and pavement distress detection (Ibragimov\net al. 2022).\nSimilarly, examining road attributes following the iRAP\nstandards has been dramatically advanced using images and\nvideos. The availability of the AusRAP dataset with labelled\nstreet objects has enabled the development of models to de-\ntect street attributes. However, due to the limitations of vi-\nsual scenes from videos and the feasibility of labelling ob-\njects (e.g., lane lines, rumble strips, poles, trees, barriers, and\nspeed signs), only 13 attributes were able to be estimated\n(Jan et al. 2018; Pubudu Sanjeewani and Verma 2019; San-\njeewani and Verma 2021b).\nIn perspective of model architectures, Fully Convolu-\ntional Network (FCN) effectively segmented objects and de-\nfined attributes from images (Sanjeewani and Verma 2021a).\nSong et al. (2018) automated the process using star rat-\nings from ground-level panoramic images. They developed\na VGG-based model incorporating a task-dependent spatial\nattention layer, which was trained in a supervized multi-task\nlearning manner. However, the task mainly focused on pre-\ndicting star rating scores rather than coding road attributes.\nSome works have proposed multi-task convolutional mod-\nels using monocular videos to predict 33 attributes (Ka\u010dan,\n\u0160evrovi\u0107, and \u0160egvi\u0107 2022; Kacan et al. 2020). However,\nall these works required large amounts of training data with\ndense pixel-level annotation, indicating significant annota-\ntion effort and substantial computational resources.\nIn addition, due to dataset imbalances, these works strug-\ngle with certain road features, such as pedestrian fencing\nand facilities for motorcycles and bicycles. Consequently,\nsome of the attributes experience the problem of out-of-\ndistribution (OOD), i.e., some attributes have classes with\nonly a small amount of data or even missing data (Hendrycks\nand Gimpel 2016). This makes multi-attribute classification\nmore challenging. Hence, these models require significant\nadditional training data, even when using transfer learning\nor semi-supervized learning approaches."}, {"title": "2.2 Vision Language Models", "content": "In recent years, Large Vision Language Models (LVLMs)\nhave become significantly important in computer vision do-\nmain. VLMs can process image and text inputs and pro-\nduce text outputs that can be used in many tasks, such as\nimage captioning, image-text matching, visual reasoning,\nand visual question answering (VQA). LVLMs have been\nextensively developed by leading researchers using large\namounts of text and image datasets with different archi-\ntectures. Examples include GPT 40 (OpenAI 2024), Gem-\nini 1.5 (Team 2024), CogVLM2 (Wang et al. 2024), and\nQwenVL (Bai et al. 2023). These models have been evalu-\nated for their potential to outperform several traditional deep\nlearning state-of-the-art benchmarks, including motion un-\nderstanding (Zhou, Wan, and Wang 2023), 3D object gener-\nation (Siddiqui et al. 2023), and image segmentation (Hoyer\net al. 2023). However, these models come with different li-\ncences, costs, and limitations.\nVLMs have also been trained, fine-tuned, and validated\nto understand street scenes, primarily focusing on the au-"}, {"title": "2.3 VQA", "content": "tonomous driving domain (Min et al. 2024; Wang et al.\n2023; Yang et al. 2024). These models mainly focus on con-\ntrolling, object detection, and depth estimation rather than\ncaptioning street scenes. Besides, Wen et al. (2023) explored\nGPT-4vision and demonstrated its great potential for under-\nstanding street scenes by captioning images as one of the\ntasks for autonomous driving. However, challenges remain\nin traffic light recognition, vision grounding, and spatial rea-\nsoning tasks. Luo et al. (2024), on the other hand, explored\nfoundation models for understanding road scenes with the\npotential usage of existing road scene datasets. Moreover,\nfor finer-grained image understanding, Zeng et al. (2024),\nused GPT-4 to extract building features and classify build-\ning attributes from the street imagery without any training\ndata. These works demonstrated the potential of VLMs for\nzero-shot understanding of road images, which can be an\nalternative for automated road feature detection with OOD\nproblems. Inspired by them, in this study, we explore the at-\ntribute coding capabilities of VLMs.\nVisual Question Answering (VQA) was initially introduced\nas a new task in the computer vision and natural language\nprocessing domains (Agrawal et al. 2016). The task involves\nanswering open-ended questions based on images. Several\ndatasets are commonly used to evaluate models, includ-\ning GQA (Hudson and Manning 2019), OK-VQA (Marino\net al. 2019), A-OKVQA (Marino et al. 2019), and MMMU\n(Hendrycks et al. 2021).\nIn addition, VQA datasets focusing on autonomous vehi-\ncles, such as KITTI (Geiger et al. 2013) and NuScenes-QA\n(Qian et al. 2024), have been used to advance the field. Jain\net al. (2024) found that GPT-4 performs well in robust driv-\ning scenarios.\nThis work uses a real-world dataset to define a new im-\nage classification dataset for VLMs as a downstream vision\napplication. It does not consider training a new VLM, but\ninstead explores the potential of existing LVLMs to perform\nthe task as road assessment coders. Although using non-\nopen-source models incurs some costs, this study aims to\ndemonstrate that coding road assessments can benefit from\nexisting LVLMs. It also allows for applying prompt engi-\nneering, retrieval-augmented generation (RAG), fine-tuning,\nor even building a new LVLM model."}, {"title": "3 Data and Experiments", "content": "3.1 ThaiRAP Dataset\nThe dataset used in this work is sourced from the ThaiRAP\nand was selected due to the availability of road assess-\nment datasets adhering to the iRAP standard. The exper-\niment includes 2,037 images (1600x1200 pixels) captured\nacross Bangkok, Pathum Thani, and Phranakorn Sri Ayut-\nthaya provinces, as illustrated in Figure 1. These images\nrepresent 519 road segments, with 1-4 images used to code\neach 100-meter segment. While iRAP datasets are typically\nnot publicly available, this work will provide access to the\nThaiRAP dataset, including both the images and their as-\nsociated attributes, to support further research and develop-\nment in road safety assessment.\nRoad Attributes Each segment has 52 attributes classi-\nfied by iRAP (iRAP 2024a,b). However, the attributes have\na different number of classes (codes) in (parenthesis), and\nthe number of classes of each attribute used in this experi-\nment is shown at the top of each bar in Figure 2. This dataset\nshows the imbalance of classes of attributes, in which there\nare 11 attributes with only one class. And some classes have\nless than 10 samples.\nData preprocessing To evaluate and compare our mod-\nels against the baseline models - traditional computer vision\nmethods, we divide the dataset (n=2037) into a training set"}, {"title": "3.2 Baseline", "content": "(1274 original samples + 464 augmented samples), a test-\ning set (492 samples), a validation set (243 samples), and\nan unseen set (28 samples). The dataset is split by ensuring\na balanced distribution across subsets, maintaining sample\nsizes for each class within the attribute. The data split fol-\nlows these rules: 1. Attributes with only one class are ig-\nnored; 2. Classes with more than 4 but fewer than 12 sam-\nples are augmented; 3. Classes with 4 or fewer samples are\naugmented if their attributes have only 2 classes; 4. Sam-\nples from classes with 4 or fewer samples are moved to the\nunseen set if their attributes have more than 2 classes.\nFor rules 2 and 3, we generate noise-added data and added\nthem to the training dataset to alleviate the class imbalance\nproblem in our dataset. Specifically, we add 5 different types\nof noise: Gaussian noise, salt and pepper noise, speckle\nnoise, periodic noise, and quantisation noise for each se-\nlected image. For rule 4, the unseen set is used to test the\nzero-shot prediction capacities in unseen classes. The base-\nline models have been trained on the training set and mon-\nitored by the validation set. The performance of both our\nmodels and the baseline models are evaluated in the testing\nset and the unseen set.\nVGGNet (Simonyan and Zisserman 2015) is a deep neural\nnetwork model known for its simple architecture that stacks\nmultiple convolutional layers with small 3x3 filters, achiev-\ning high performance in image classification tasks and be-\ncoming a widely used baseline in computer vision.\nResNet (He et al. 2015) is also a deep convolutional neu-\nral network that utilizes residual blocks and skip connec-\ntions to enhance feature learning at various abstraction lev-\nels, making it highly effective for image classification and\ntransfer learning tasks.\nIn our study, we use VGGNet and ResNet as baseline\nmodels to compare with our proposed approach. Since these\nmodels are designed for a single classification task, we de-\nsign the baseline model architectures for a multi-attribute\nclassification problem, where a single encoder is shared\nacross all tasks, and separate decoders are allocated for each\nindividual task. This structure allows the model to extract\nthe common features from the encoders and use the features"}, {"title": "3.3 Experiments", "content": "to address each task using task-specific decoders.\nIn this section, we evaluate the framework of our proposed\napproach, V-RoASt, for examining 52 iRAP attributes from\nimages, as shown in Figure 3. This framework is designed\nto be easily applicable in any city, requiring minimal data\nscience expertise and coding experience. The approach in-\ncludes text input for system instructions and prompts, as\nwell as image prompts. The workflow operates by inputting\nan image and its associated information prompts, including\ntask descriptions, contextual information, and output format"}]}