{"title": "Exploring Spatial Language Grounding Through Referring Expressions", "authors": ["Akshar Tumu", "Parisa Kordjamshidi"], "abstract": "Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.", "sections": [{"title": "1 Introduction", "content": "Vision-Language model (VLM) research has boomed in the recent past, owing to the enhanced user interaction and accessibility they provide. Models such as GPT 40 \u00b9, LLaVA [Liu et al., 2024], Google Gemini [Team et al., 2023] have become adept at solving vision-language tasks such as Visual Question Answering (VQA), Image Captioning, and more. However, works like [Liu et al., 2023a; Subramanian et al., 2022; Kamath et al., 2023] show that VLMs still lack human-level 'Spatial Reasoning' capabilities. Spatial reasoning involves comprehending relations that depict the absolute/relative position or orientation of an object, such as 'left', 'above', or 'near'.\nMost of the previous works confine their analysis to testing which models work well for spatial relations. We go further to analyze the comparative performance of these models for spatial categories that represent different orientational and positional relations between objects. A novel aspect of our work is the analysis of the effect of varying spatial composition (number of spatial relations) in the expressions on the performance of the models.\nPrevious works focused on spatial analysis with image captioning-related tasks, thus failing to locate the source of error in the presence of visual and linguistic ambiguity. To avoid this, we adopt the Referring Expression Comprehension (REC) task for our analysis. The REC models output bounding boxes around the target entity based on a natural language expression, the analysis of which could reveal the parts of the input that the models fail to comprehend.\nFor our analysis, we use the CopsRef dataset [Chen et al., 2020], which is a complex dataset with visual ambiguity and multiple spatial relations in expressions. We focus our analysis on 51 spatial relations, categorized into 8 categories.\nWe test two popular VLMs - LLaVA [Liu et al., 2024] and Grounding DINO [Liu et al., 2023b]. We also included 'MGA-Net' [Zheng et al., 2020], a model specifically designed for the REC task. The chosen models offer diversity in the evaluation as they differ in their architectural elements, training strategies, and input formats. We further compare these models with an object detector baseline to test if the images are truly complex and require elaborate referring expressions to ground the correct object.\nSome of our important findings are as follows:\n(1) Spatial relations contribute to more accurate grounding when added to other attributes of the objects in referring expressions. (2) Increasing the spatial complexity (no. of spatial relations) of an expression affects the performance of the VLMs, but models with explicit compositional learning components maintain the performance. (3) Dynamic spatial relationships are difficult for all models to ground. (4) The task-specific trained models find it easier to ground the geometric spatial relations such as left and right, while the VLMs perform better for ambiguous relations such as proximity. (5) All models struggle with handling negated spatial relations, but to varying degrees."}, {"title": "2 Related Work", "content": "Previous works have performed a broad analysis of the ability of the VLMs to perform multimodal perception and reasoning tasks such as Spatial Reasoning, Multimodal conversation, etc. Works such as [Tian et al., 2024; Liu et al., 2024; Liu et al., 2023c; Li et al., 2023; Yu et al., 2023; Fu et al., 2025] introduce comprehensive real-world benchmarks to test multiple VLM capabilities.\n[R\u00f6sch and Libovick\u1ef3, 2023; Subramanian et al., 2022] focus solely on spatial analysis of VLMs. [Wang et al., 2024] go a step further to analyze the role of each modality in spatial reasoning. However, these works do not go deep to test the factors that affect the spatial reasoning ability of the VLMs. On the other hand, [Liu et al., 2023a; Kuhnle et al., 2018] perform a category-wise analysis of spatial relations. While the former categorize the relations based on their spatial properties, the latter categorize them as either simple, complex, implicit, or superlative. [Gokhale et al., 2022] evaluate the models for the 4 spatial relations present in their dataset. Different from these works, [Kamath et al., 2023] analyze the effects of spatial biases in the datasets for REC task performance. [Lewis et al., 2022] analyze if the models are erring in recognizing objects, relations, or both. Some works like [Cohn and Hernandez-Orallo, 2023; Liu et al., 2022; Mirzaee et al., 2021] focus on probing pretrained LLMs or VLMs with text-only questions for spatial analysis. However, they do not test spatial grounding in the visual modality, a crucial aspect of our work.\nOther closely aligned work includes Embodied Spatial Analysis which focuses on the effects of different perspectives and non-verbal cues on the spatial reasoning capabilities of VLMs [Islam et al., 2022; Islam et al., 2023].\nTask Complexity and Interpretability. The works mentioned previously use image-caption agreement as their evaluation task. Due to the inherent limitations of this task, these works simplified the expressions to have only 2 objects and 1 spatial relation. Some works like [Lewis et al., 2022; Subramanian et al., 2022; Kuhnle et al., 2018] use synthetic datasets instead of real-world images to improve the interpretability of model output. But it simplifies the problem due to bounded expressivity (limited number of objects, attributes, and spatial relations). In our case, REC models output bounding boxes around the target objects. Analyzing the position and characteristics of the output object helps identify the parts of the input that the models fail to process. This enables comparative analysis of expressions with 0, 1, or more spatial relations, a unique feature of our work. The REC task also enables us to test the models over images of different visual complexities (single or multiple instances of objects in an image)."}, {"title": "3 Dataset", "content": "Table 1 shows the key characteristics of some popular REC datasets. We chose CopsRef over RefCOCO and RefCOCOg due to the longer expression length and higher number of objects per image. Although CLEVR-Ref+ also provides a complex dataset, it is a synthetic dataset with limited expressiveness. Unlike other datasets, CopsRef's expressions go beyond describing the simple distinctive properties of the objects. CopsRef is also a highly spatial dataset as 90% of expressions consist of spatial relations. Examples of such referring expressions and the corresponding images are given in"}, {"title": "4 Approach", "content": "In our analysis, we seek to answer the following research questions:\nRQ1. Which spatial relation categories are more difficult to ground? RQ2. Do model characteristics/architecture influence the ease of grounding certain spatial relation categories over others? RQ3. Does using spatial relations improve grounding or make it more difficult? RQ4. How does the number of spatial relations in the expressions affect grounding in different types of models? RQ5. Are the REC models well-equipped to handle negated spatial relations?\nTo answer these questions, we explain our research methodology and the designed experiments in this section.\n4.1 Models Description\nWe select three distinct models for our analysis such that they differ in key components like architecture, pre-training tasks, and input formats.\nMGA-Net. [Zheng et al., 2020] is an REC task-specific model whose compositional learning architecture was designed to handle complex expressions. It decomposes a query using the soft attention mechanism and processes visual and linguistic information using dedicated modules to construct a relational graph among objects. Then, it uses a Gated Graph Neural Network to perform multi-step reasoning over the referring expression. We first implement the Faster-RCNN model [Ren et al., 2016] to procure object proposals. Then, we generate the vector representations for these object proposals using a pre-trained ResNet-101 model. Considering the available computing resources, we omit the fourth (top-most) layer of the ResNet101 model to obtain a Partial CNN backbone. Finally, we train the model for ten epochs. We"}, {"title": "4.2 Experimental Setting and Evaluation", "content": "We create the following dataset test splits for evaluation and answering the earlier mentioned research questions, RQ1-RQ5.\nFine-grained Spatial Relations Split\nIn the test dataset, we split the expressions with 1 spatial relation using the categories shown in Table 2. Using the categories from Table 3, we split the remaining expressions based on the number of spatial relations they contain. Then, we rank the models based on their accuracy for each category.\nTo compare the models' performances across the categories, we employ a statistical test known as the Kendall Tau Independence Test. It evaluates the degree of similarity between two sets of ranks given to the same set of objects. We calculate the Kendall rank coefficient (\u03c4) which yields the correlation between two ranked lists. Given \u03c4 value, we calculate the z statistic, which follows standard normal distribution, as:\n $$z = \\frac{3 * \\tau * \\sqrt{n(n - 1)}}{\\sqrt{2(2n + 5)}}.$$\nUsing the 2-tailed p-test at 0.05 level of significance, we test the following: Null hypothesis: There is no correlation between the two ranked lists. Alternative hypothesis: There is a correlation between the two ranked lists.\nVisual Complexity Split\nTo observe the effect of visual complexity on model performance, we split the test dataset into two parts. The first part has images that have multiple instances of one or more objects mentioned in the associated referring expressions. The second part has images with at most one instance of every object mentioned in the expression. We perform this splitting by first collecting the entities in each expression using spaCy2 and then employing Grounding DINO to find the number of instances in the image for each of the collected entities.\nNegation Analysis Split\nIn our analysis, we found that models have difficulties in grounding spatial expressions with negations. Therefore, we created a test split for a more accurate evaluation and a deeper analysis of negated spatial expressions. We collected expressions that include the keyword 'not' and divided them into two sets according to the number of occurring negations (1 or 2). Then, we collected those expressions for which all three models give an IoU of less than 0.5. For each expression, we perform a qualitative analysis to verify whether the errors are due to misinterpreting the negations or conflation of other errors. We limit our analysis to the results from the first run of the three models to facilitate the instance-wise analysis."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Evaluation on Referring Expressions", "content": "From Table 4, we can observe that Grounding DINO and MGA-Net outperform the OWL-ViT baseline, with the former achieving the highest accuracy in grounding the referring expressions. However, we also tried training MGA-Net with the full ResNet-101 visual backbone (Full CNN) instead of the partial backbone (Partial CNN). We could only train this model for four epochs due to computational constraints. However, the model crossed 60% test accuracy in just four epochs and was monotonically increasing. This shows that MGA-Net could potentially provide a better performance using adequate computational resources. To avoid unfair comparisons due to the training discrepancies, we focus our results on the relative performances of each model across different spatial relation categories rather than comparing the absolute performances.\nFor LLaVA, we used the prompts explained in Section 4.1. The shorter prompt gave a slightly better accuracy than the longer prompt. Hence, we used the short version for further experiments. The accuracy of LLaVA is less than both the other models and the baseline. Possible reasons are the lack of both bounding box regression and visual grounding instructions during pre-training.\nSince we trained/tested each model for three runs, we report the average accuracy of the three runs and the standard deviation in the table. Since we re-train MGA-Net for each of these runs, there is a noticeable difference in model predictions in each run, leading to a slightly high standard deviation. However, we test the VLMs and the baseline zero-shot, leading to zero or near-zero standard deviation in the accuracies. This also follows for the future result tables."}, {"title": "5.2 Evaluation on Fine-grained Relations", "content": "Table 5 shows a few general trends in results. The top 3-4 categories that each model performs the best for are categories with a single spatial relation. Among those, all 3 models perform well for the Topological and Absolute categories.\nTo answer RQ1, we observed that all the models struggle with the Directional relations. A possible reason is that the spatial configurations of the involved objects vary from image to image for the same spatial relation. This makes it difficult for the models to learn common patterns for recognizing these relations, resulting in low accuracy."}, {"title": "5.3 Impact of Multiple Spatial Relations", "content": "Table 6 shows the Kendall Tau Independence test results for the three pairs of VLMs. We can observe that while the category-wise ranks of the VLMs (Grounding DINO and LLaVA) are correlated, MGA-Net's ranks aren't correlated with them. This motivates us to study the possible reasons behind the difference in the category-wise performances of MGA-Net and the VLMs.\nAmong spatial categories of MGA-Net and VLMs, the major difference occurs with the Proximity and Projective categories. To answer RQ2, we can observe that the 'Proximity' category ranks third for both the VLMs but 8th for MGA-Net. On the other hand, 'Projective' has a higher rank for MGA-Net than both VLMs. We can see that MGA-Net prefers geometric spatial relations like left of, on top of, etc. as it takes the relative locations of bounding boxes as input which helps represent such relations. On the other hand, the two VLMs outperform in ambiguous relations which do not specify a clear distance or geometric direction, such as by/close-to. This is because the vision backbones of the VLMs utilize the entire image and help capture relations between a region"}, {"title": "5.4 Impact of Visual Complexity", "content": "Out of 12586 test data points, we found that in the images of 4730 data points, there are multiple instances of objects mentioned in the referring expressions. Table 8 shows the accuracies of the three models and the OWL-ViT baseline for images with a single instance ('Accuracy Single' column) and multiple instances (\u2018Accuracy Multi' column). The models perform better for the single instance images by 5.4% on average compared to the multi-instance images. The 8.4% performance drop of the baseline for multi-instance images proves that the images are indeed complex and require more than just the label as the input for grounding the right object. However, the 7.3% performance drop of LLaVA, as compared to MGA-Net and Grounding DINO, shows that grounded pre-training also plays a crucial for multi-instance images."}, {"title": "5.5 Impact of Negation", "content": "Table 9: Results for negations in expressions.\nWe obtained 36 expressions with 1 'not' and 73 expressions with 2 'not's for which all models gave incorrect predictions. Table 9 shows the total number of expressions we obtained with 1 and 2 negations. The \u2018Total failure' row gives the number of instances for which models failed to recognize at least 1 negation. We can observe that Grounding DINO has the highest number of failure instances. LLaVA handles the negations better possibly due to the Vicuna [novita.ai, ] language backbone as it has a better language understanding (including negations) compared to Grounding DINO's CLIP text encoder. MGA-Net's training involves expressions with negations, making it more adept at recognizing them during testing. Hence, to answer RQ5, we observe that while all REC models face issues with recognizing negations, some models are comparatively better at handling them.\nAnother interesting observation was for the outputs of MGA-Net and LLaVA models when they came close to the target object. From Table 10, we can see that while LLaVA has a better precision, MGA-Net has a better recall."}, {"title": "6 Qualitative Analysis", "content": "Here, we provide a qualitative analysis of certain issues faced by the models in handling referring expressions.\n6.1 Directional Relations\nThe expressions pertaining to Figures la and 1b consist of the same spatial relation ('around'). In the first figure, the wrapping of the napkin around the hotdog only makes the napkin partially visible. But in the second figure, the white box around the mirror is almost entirely visible. This shows how the interpretation of 'around' is highly dependent on the configuration of the involved objects. For the first image, LLaVA fails to precisely localize the object, while MGA-Net only returns a part of the napkin that is visible. In the second image, both models fail to localize the object.\n6.2 Projective and Proximity Relations\nFigure 1c shows an example of Projective relations ('to the left'). MGA-Net succeeds in returning the correct part of the table that is to the left of the phone. While Grounding DINO simply returns the entire table, LLaVA identifies the wrong part. This shows the ability of MGA-Net to comprehend projective relations better, particularly when the target object is not apparent. An example of Proximity relations is in Figure 1d where LLaVA and Grounding DINO return the shore that is 'near' the murky water, but MGA-Net fails to do so.\n6.3 Multiple Spatial Relations\nFor 'Two-and' category expressions, the models sometimes only satisfy one of the spatial clauses. This often happens if multiple objects of the same class are in the image. For example, in Figure le, the output baseball player is to the left of the black helmet but is not to the right of the home plate. Similarly, for 'Two-chained' category expressions, the models sometimes do not consider the entire expression. For example, in Figure 1f, MGA-Net and LLaVA return the 'log that is behind the large bear', and Grounding DINO returns the bear itself. None of the models consider the 'large branch' part of the expression, which should have been the output.\nFinally, for 'Two-or' category expressions, the model might pay attention to only one spatial clause. Consequently, it returns an object satisfying that clause but not the additional attributes mentioned in the expression. For example, in Figure 1g, the model returns the monitor, which is to the 'left of the keyboard', but it does not satisfy the color attribute.\n6.4 Negation\nFigures 1h and li show two cases where all models fail to recognize negation. In 1h, we can observe that while MGA-Net is wrong, LLaVA is close to the ground truth but partially"}, {"title": "7 Conclusion", "content": "Spatial reasoning and understanding is an area in which the latest VLMs have shown signs of struggle. We evaluate the spatial understanding of a variety of models using the referring expression comprehension task because it requires explicit grounding of complex linguistic expressions in the visual modality. We picked multiple models including Vision-Language models (LLaVA, Grounding DINO) as well as task-specific models (MGA-Net). We observed that the VLMs that are trained in the wild with visual and textual data perform worse in grounding. All models have challenges in Directional relations. However, the VLMs do better in vague relations such as proximity while the task-specific models are better in geometrically well-defined relations such as left and right. While using spatial relations helps in grounding, using multiple relations makes the reasoning more challenging for all models, with a higher impact on VLMs. MGA-Net handles complex spatial expressions better than VLMs due to its compositional learning architecture. In the presence of visual complexity, all models face challenges but LLaVA struggles the most due to lack of grounded pre-training. Finally, both VLMs and task-specific models struggle with grounding expressions that include negation. These findings shed light on the gaps for future work on Vision-language models."}, {"title": "8 Future Directions", "content": "We observed that MGA-Net handles expressions with varying spatial complexity better than the VLMs due to its soft attention module which decomposes the expression into its semantic components for compositional reasoning. This highlights the decomposition of complex spatial expressions as a potential path forward to help VLMs generalization. [Sinha et al., 2024] discuss multi-modal transformer models introduced by [Sikarwar et al., 2022], [Qiu et al., 2021] and techniques such as weight sharing across transformer layers or 'Pushdown layers' with recursive language understanding [Murty et al., 2023] as an alternative to self-attention to aid compositional reasoning. Another promising direction is Neuro-symbolic processing [Kamali et al., 2024; Hsu et al., 2024], which involves generating symbolic programs from expressions using LLMs and conducting explicit symbolic compositions before grounding into visual modality. We plan to explore integrating such techniques with VLMs to improve their spatial compositional reasoning capabilities.\nAnother issue to address is the VLMs' inability to comprehend negations. MGA-Net's improved performance over Grounding DINO due to the presence of negated expressions in the training data motivates us to explore the augmentation of training/instruction tuning data of VLMs with synthetically generated negated expressions. Additionally, we also plan to formulate contrastive learning objectives to penalize the model when it fails to comprehend negations."}, {"title": "A Appendix", "content": "A.1 Description of spatial categories\nFor our analysis, we utilize the spatial categories introduced by [Marchi Fagundes et al., 2021] and replace the 'Cardinal Direction' category with 'Absolute'. The descriptions and examples for the chosen categories are as follows:\n1. Absolute: Consists of relations that describe the location of an object in an absolute manner and not in relation to another object.\nEg:- man on the right that is standing and wearing gray pant\n2. Adjacency: Consists of relations that describe the physical proximity of two objects.\nEg:- The large poster that is leaning against the wall"}, {"title": "A.2 Other Models", "content": "In our analysis, we also experimented with InstructBLIP [Dai et al., 2023] and OpenFlamingo [Awadalla et al., 2023] models. These models are general-purpose VLMs with InstructBLIP working in the zero-shot model and OpenFlamingo in the few-shot mode. However, neither of the models could provide meaningful outputs for the task. In this section, we disclose the prompts that we used for these two models and the outputs obtained for the prompts:\nInstructBLIP\nFor InstructBLIP, we designed three prompts for the REC task. They are as follows:\n1. Bounding Boxes: bounding box list; Referring Expression: Refexp; The index of the output bounding box is:\n2. Bounding Boxes: bounding box list; Referring Expression: Refexp; The coordinates of the output bounding box are:\n3. Provide the bounding box coordinates for: \"Refexp\"\nIn both prompts, the 'bounding box list' consists of the coordinates of the detected bounding boxes in the image being passed as the input, along with indices for each bounding box, starting from '1'. But for the third prompt, the model has no access to pre-detected candidate bounding boxes in the image. While the expected output for the first prompt was the index of the correct bounding box, for the other 2 prompts it was the bounding box coordinates as the output.\nThe bounding box format is [x1, y1, x2, y2], where (x1, y1) is the bottom left corner and (x2, y2) is the top right corner of the box. The coordinate values are a fraction of the total length/width of the image according to the position of the coordinate.\nUnfortunately, none of the three prompts gave a satisfactory output. The outputs were as follows:\nPrompt 1: The outputs were mostly wrong. Sometimes, the model also gave '0' as the output, even though it is not a valid index.\nPrompt 2: The output did not return meaningful coordinates in most cases. But in the few instances that it did, they were mostly wrong. Example outputs when the model could not return meaningful coordinates are:\n\u2022 {'1': [0.16, 0.55], '2': [0.32, 0.47], '3': [0.55, 0.6], '4': [0.21, 0.06],...\n\u2022 0.9, 0.53, 0.93, 0.57, 0.0, 0.39,...\nPrompt 3: The model could not understand the task and it just paraphrased parts of the prompt instead of giving the coordinates as the output. Example prompts and outputs are:\n\u2022 Prompt: Provide the bounding box coordinates for: \"The large poster that is leaning against the wall\"\nOutput: what is the bounding box coordinates for the large poster that is leaning against the wall\n\u2022 Prompt: Provide the bounding box coordinates for: \"The young man that is leaning against the wall\"\nOutput: is standing in an elevator. the young man that is leaning against the wall is standing in an elevator\nOpenFlamingo\nAs discussed before, OpenFlamingo is a few-shot prompting model. We tested the model in 2 and 3-shot settings.\nPrompt 1:\n\u2022 Example output format: <image>Bounding Boxes:bounding box list; Expression: Refexp; Correct Bounding Box:\"ID\"<|endofchunk|>\n\u2022 Query format: <image>Bounding Boxes:bounding box list; Expression: Refexp; Correct Bounding Box:\"\n'bounding box list' took the list of candidate bounding boxes in the image as input, in the same format as for InstructBLIP (discussed in the previous section). The expected output was the index of the correct bounding box. However, we observed that irrespective of the query, the model gave the same output index for the same set of prompting examples.\nPrompt 2:\n\u2022 Example output format: <image>Expression: Refexp; Correct Bounding Box: [Bounding box coordinates]<|endofchunk|>\n\u2022 Query format: <image>Expression: Refexp; Correct Bounding Box:[\n'bounding box list' takes the same input as explained for Prompt 1. But instead of expecting the index, we expect the coordinates of the bounding box as the output. The format of the bounding box is the same as explained for InstructBLIP in the previous section. However, the model failed to give meaningful coordinates as output in most cases, similar to InstructBLIP. When it did give meaningful coordinates, the outputs were mostly wrong."}, {"title": "A.3 Related Works Data", "content": "Table 11 provides the evaluation task, list of evaluated models, the benchmark used, and the properties of the benchmarks used in the works that include spatial relation analysis. Table 12 provides additional information such as the properties of the images and the spatial complexity of the benchmarks used in these works. Towards the end of both tables, we also include the said characteristics of our work for comparison. Among the mentioned works, only our work focuses on category-wise spatial analysis while using complex spatial expressions."}]}