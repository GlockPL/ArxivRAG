{"title": "CoME: An Unlearning-based Approach to Conflict-free Model Editing", "authors": ["Dahyun Jung", "Jaehyung Seo", "Jaewook Lee", "Chanjun Park", "Heuiseok Lim"], "abstract": "Large language models (LLMs) often retain outdated or incorrect information from pre-training, which undermines their reliability. While model editing methods have been developed to address such errors without full re-training, they frequently suffer from knowledge conflicts, where outdated information interferes with new knowledge. In this work, we propose Conflict-free Model Editing (CoME), a novel framework that enhances the accuracy of knowledge updates in LLMs by selectively removing outdated knowledge. CoME leverages unlearning to mitigate knowledge interference, allowing new information to be integrated without compromising relevant linguistic features. Through experiments on GPT-J and LLaMA-3 using Counterfact and ZsRE datasets, we demonstrate that CoME improves both editing accuracy and model reliability when applied to existing editing methods. Our results highlight that the targeted removal of outdated knowledge is crucial for enhancing model editing effectiveness and maintaining the model's generative performance.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) encode vast amounts of knowledge during pre-training, enabling them to perform effectively across a wide range of natural language processing (NLP) tasks (Hao et al., 2021; Cao et al., 2021a; Jiang et al., 2023; Hernandez et al., 2023; Haviv et al., 2023; OpenAI, 2023). However, LLMs often incorporate outdated, incorrect, or biased information learned from training data, which can directly affect the reliability of their outputs (Hase et al., 2021; Pagnoni et al., 2021; Ji et al., 2023; Mousavi et al., 2024). Such issues may lead to unexpected results or undesirable biases in the generated responses.\nThere is a growing need for research aimed at correcting erroneous knowledge in LLMs or injecting new knowledge while preserving the general performance of the models. Recent studies explore model editing, which offers the potential to modify a model's knowledge without requiring full re-training (Mitchell et al., 2022b; Wang et al., 2023b; Yao et al., 2023; Pinter and Elhadad, 2023; Zhang et al., 2024). Model editing enables the integration of new information into a model through minimal parameter updates while preserving its existing knowledge. This is particularly useful for correcting errors introduced by flawed data or incorporating new knowledge while selectively updating only the necessary parts of the model.\nExisting model editing methods primarily focus on identifying and modifying the parameters where knowledge is stored in order to update the model (Dai et al., 2022; Meng et al., 2023b; Hu et al., 2024a; Chen et al., 2024; Sharma et al., 2024; Wang et al., 2024). These approaches allow the model to retain learned information efficiently while updating specific knowledge. However, when generating responses based on newly integrated knowledge, the model may encounter conflicts between the new and outdated knowledge, leading to degraded performance (Li et al., 2024b). Ni et al. (2024) propose a full fine-tuning-based approach that first performs forgetting outdated knowledge before editing the model with new information. However, fine-tuning-based editing is susceptible to overfitting (Cao et al., 2021b), and updating all layers incurs significant memory overhead. Additionally, the gap between the unlearning and editing stages may lead to unintended knowledge distortions.\nTo address these issues, we propose Conflict-free Model Editing (CoME), which selectively removes outdated knowledge while simultaneously updating the model with new knowledge. This process mirrors the way the human brain refines its understanding-when we learn new information, the brain selectively weakens outdated or conflicting memories to avoid cognitive interference and confusion (Geiselman et al., 1983; Bjork and Bjork, 1996; Wixted, 2004; Alves and Bueno, 2017; Kliegl and B\u00e4uml, 2021). In a similar manner, CoME identifies parameters associated with outdated knowledge and unlearns them during the integration of new knowledge, thereby reducing knowledge conflicts within the LLM. By performing both steps simultaneously, CoME minimizes unintended knowledge transformations. This process is analogous to how humans enhance cognitive clarity by discarding irrelevant or erroneous memories. Importantly, CoME achieves this without unnecessary loss of linguistic understanding, as we carefully preserve critical language-processing features shared between outdated and new knowledge. Furthermore, we limit the parameter space subject to modification during the unlearning process to minimize unnecessary parameter adjustments.\nWe apply CoME to state-of-art model editing methods, including MEMIT (Meng et al., 2023b) and PMET (Li et al., 2024a), which are designed to mitigate overfitting and memory overhead issues in knowledge editing. We conduct large-scale knowledge editing experiments on 10,000 samples from the Counterfact (Meng et al., 2023a) and ZsRE (Levy et al., 2017) datasets, utilizing the GPT-J (6B) (Wang and Komatsuzaki, 2021) and LLaMA-3 (8B) (Llama Team, 2024). The results demonstrate that applying CoME significantly improves the accuracy of knowledge updates. In particular, we show that CoME suppresses interference from outdated knowledge during inference, resulting in consistent and accurate responses, while maintaining the LLM's pre-existing capabilities.\nOur main contributions are as follows:\n\u2022 We propose a new framework to mitigate conflicts between outdated and new knowledge in LLMs' knowledge editing.\n\u2022 We introduce unlearning to remove outdated knowledge while integrating new information, and we design an algorithm that applies unlearning selectively to relevant parameters. Our method is designed to complement and enhance existing model editing methods.\n\u2022 Our experiments demonstrate that CoME suppresses interference from outdated knowledge, yielding more reliable and consistent responses. This highlights the framework's ability to enhance the robustness of LLMS when handling updated information."}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Knowledge Editing", "content": "Existing knowledge editing methods can generally be divided into two categories: preserve and modify parameters. One involves editing a model's knowledge without directly modifying its parameters. SERAC (Mitchell et al., 2022b) stores corrections in external memory and adjusts the model's responses as needed. IKE (Zheng et al., 2023) proposes a solution based on in-context learning,"}, {"title": "2.2 Unlearning", "content": "The concept of machine unlearning, introduced by Cao and Yang (2015), focuses on the removal of knowledge that has already been learned by a model. Jang et al. (2022) employ gradient ascent to perform unlearning with the goal of alleviating privacy concerns, while Eldan and Russinovich (2023) demonstrate unlearning by erasing specific knowledge related to the Harry Potter books from a model. Chen and Yang (2023) proposes freezing the LLM and introducing an unlearning layer to construct a forgotten model. Yao et al. (2024) presents a comprehensive framework for performing unlearning in LLMs using gradient ascent and KL divergence. Hu et al. (2024b) utilize parameter-efficient modules to preserve general model capabilities while removing untruthful or toxic information from LLMs.\nNi et al. (2024) propose an approach that performs unlearning of existing knowledge before knowledge editing. However, this method is prone to overfitting due to its reliance on fine-tuning. In contrast, our approach is applied to state-of-the-art model editing methods that address such issues. By effectively removing outdated knowledge during the injection of new information, we mitigate conflicts between the two processes."}, {"title": "3 Preliminaries", "content": null}, {"title": "3.1 Model Editing", "content": "The goal of model editing is to update the knowledge contained in LLM by replacing incorrect or outdated information with new knowledge. In this work, we focus on knowledge represented as triples consisting of a subject $s$, a relation $r$, and an object $o$. Our approach performs batch editing, where multiple pieces of knowledge are updated simultaneously. Specifically, given a model $f$ with parameters $\\theta$, we update its parameters to $\\theta^*$ by modifying $N$ pieces of knowledge in one step. The knowledge $G$ embedded in the model is represented as:\n$G = \\{(s_i, r_i, o_i), i \\in [1, N]\\}.$   (1)\nWhen editing knowledge, we replace the object in the outdated triple $(s, r, o)$ with a new object $o^*$, yielding updated knowledge $(s, r, o^*)$. The target knowledge $G^*$ that the updated model should encode is represented as:\n$G^* = \\{(s_i, r_i, o_i^*), i \\in [1, N]\\}.$   (2)\nFor example, consider the case where $s_i = $ \u201cMotion,\u201d $r_i =$ \u201cmanufactured by,\u201d and $o_i =$ \u201cMicrosoft,\u201d which reflects an incorrect fact. The updated knowledge should modify the object to $o_i^* = $ \u201cApple,\u201d while keeping the subject and relation intact. The prompt $x_i$ provided to the model might be \u201cMotion, a product manufactured by,\u201d and the model's response should be updated to reflect the correct object $o_i^*$ rather than the incorrect $o_i$. Thus, the updated model must satisfy:\n$f_{\\theta^*}(x_i) = o_i^*, i \\in [1, N].$   (3)\nIf the model has been correctly edited, it satisfies Efficacy, a key attribute that should be prioritized in the editing process. Beyond efficacy, the following properties are essential for evaluating the quality of model editing:\nGenerality ensures that the edited knowledge remains intact even when the prompt is paraphrased. This is measured by providing a paraphrased prompt $x_i^{gen}$ and checking whether the"}, {"title": "3.2 Locate-then-Edit", "content": "Following the approach of Meng et al. (2023b), our goal is to efficiently update the weights of specific layers within the model in response to editing requests. Each edit request involves optimizing target vectors, which gradually adjust the weights of the layers.\nWe compute the update for one layer and then distribute it uniformly across the target layers. This allows us to update multiple layers efficiently with minimal computational overhead. Specifically, we focus on the final target layer $l$ among the set of target layers $T$. Given an input $x_i$, we calculate a replacement vector $z_i$ for the hidden state $h$ in layer $l$ as follows: $z_i = h + d_i$. The residual vector $d_i$, used to update $z_i$, is optimized via gradient descent:\n$\\arg \\min \\frac{1}{P} \\sum_{j=1}^{P} - \\log P_{f_{\\theta}(z_i + x_i)}[O_i^* | P_j + x_i],$   (5)\nwhere $p_j$ represents the $P$ additional prompts introduced to enhance the diversity of inputs.\nThe computed update is then distributed across the target layers by modifying the MLP weights. Let $W$ represent the original weights, and $W'$ the updated weights. The incremental update $\\Delta$ is added to the original weights, resulting in $W' = W + \\Delta$. The incremental update $\\Delta$ is calculated as follows:\n$\\Delta = RK^T (C + \\lambda KK^T)^{-1},$   (6)\nwhere $K$ encodes the key associated with the target knowledge to be updated. The matrix $C \\approx KK^T$ represents a set of previously memorized keys obtained through sampling. $R \\equiv V - W K$ represents the difference between the model's original knowledge representation $W K$ and the target knowledge representation $V$. This represents a set of values where the residual vector is distributed across the target layers using $\\frac{\\delta_i}{1 + |T|}$, $t \\in T$ ."}, {"title": "4 CoME: Conflict-free Model Editing", "content": "As shown in Figure 1, we propose CoME that improves the accuracy of knowledge editing by utilizing parameter subtraction-based unlearning. Our method introduces three key steps to enhance existing locate-then-edit methods: (1) extracting parameters associated with outdated knowledge, (2) performing targeted unlearning during the integration of new knowledge, and (3) restricting the unlearning process to a specific parameter range to ensure that only essential portions are affected."}, {"title": "4.1 Extraction of Outdated Knowledge Parameters", "content": "To minimize conflicts between outdated knowledge and new knowledge, we remove the outdated information from the updated parameters $z_i$ before distributing the updates across the target layers. First, we obtain the parameters $\\delta_i'$ that update the model with outdated knowledge in order to extract the parameters associated with this knowledge. This process closely mirrors the procedure for obtaining the parameters $\\delta_i$ corresponding to the new knowledge. $\\delta_i'$ is obtained by replacing the new knowledge $o_i^*$ with the outdated knowledge $o_i$ in Equation 5 and learning accordingly. Therefore, it represents the parameters associated with outdated knowledge. Inspired by Ilharco et al. (2023); Zhang et al. (2023), we hypothesize that subtracting the parameters associated with outdated knowledge from the model can facilitate effective unlearning of that knowledge. By performing $Z_i = Z_i - \\delta_i'$, we aim to remove the portions of the parameters updated with new knowledge that still contain outdated information.\nFollowing the insights from Hu et al. (2024b), we assume that $\\delta_i'$ not only encapsulates outdated knowledge but also encompasses the model's linguistic abilities. As shown in Equation 5, both the"}, {"title": "4.2 Unlearning During Knowledge Update", "content": "After extracting the common component, we subtract it from the outdated knowledge update vector. The remaining component, which encodes only outdated knowledge, is subtracted from the updated parameters:\n$z_i' = z_i - \\alpha(\\delta_i - \\delta_i')$,   (9)\nwhere $\\alpha$ is a hyperparameter controlling the weight of the subtraction operation\u00b9."}, {"title": "4.3 Restricting Unlearning to Critical Parameters", "content": "Through the experiment shown in Figure 3, we confirm that unlearning outdated knowledge negatively affects Locality. To address this, inspired by Gu et al. (2024), we limit the scope of unlearning to only the parameters most influenced by outdated knowledge, leaving other knowledge unaffected. Specifically, we restrict unlearning to the top-p% of parameters based on the magnitude of the unlearning update\u00b2. Parameters in the top-p% are considered essential for unlearning, while the remaining parameters are treated as irrelevant. The final update for parameter $z_i$ is as follows:\n$z_i' = \\begin{cases}\\newline\\delta_i &\\text{if } (\\delta_i' - \\delta_i') \\text{ in the top-p%}, \\\\newline z_i & \\text{otherwise.} \\end{cases}$   (10)"}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Setup", "content": "Datasets We adopt two widely used evaluation datasets from existing model editing research: Counterfact (Meng et al., 2023a) and ZsRE (Levy et al., 2017). The Counterfact dataset contains counterfactual knowledge, statements that have a lower generation probability than factual knowledge, which are provided as new knowledge for editing. To assess large-scale knowledge editing capabilities, we conduct experiments on 10,000 samples. ZsRE is a context-free question-answering dataset designed for zero-shot relation extraction. We extract 10,000 samples from ZsRE to evaluate the models' ability to accurately edit knowledge.\nMetrics In the Counterfact dataset, we evaluate the models on Efficacy, Generality, and Locality, using success rates as metrics. Additionally, we assess the models' generative capabilities through Fluency and Consistency. Score is the harmonic mean of Efficacy, Generality, and Locality. Since ZsRE does not measure generative capabilities, we evaluate the models based only on accuracy in terms of Efficacy, Generality, and Locality.\nBaselines To enable a direct comparison with existing model editing methods, we follow the baselines outlined in Li et al. (2024a). The first baseline is the unedited model. FT-W (Zhu et al., 2020), involves fine-tuning using weight decay for knowledge editing. FT fine-tunes all parameters of the base model. F-Learning (Ni et al., 2024) is a fine-tuning-based approach that forgets existing knowledge and learns new knowledge. MEND (Mitchell et al., 2022a) leverages additional training data to fine-tune the model through a hypernetwork-based approach. ROME (Meng et al., 2023a) is an optimization-based method for single-editing tasks, while MEMIT (Meng et al., 2023b) extends ROME to enable large-scale knowledge editing in a single pass. PMET (Li et al., 2024a) optimizes both MHSA and FFN components simultaneously for knowledge editing."}, {"title": "5.2 Main Results", "content": "Editing Knowledge in Counterfact Table 1 presents the editing performance of CoME on 10,000 samples from the Counterfact dataset. Both COMEMEMIT and COMEPMET improve Score, which evaluates the overall performance of editing. On GPT-J, both methods achieve Score of 86.4, compared to 85.8 for MEMIT and 86.2 for PMET, demonstrating the efficacy of our approach. Similarly, on LLaMA-3, COMEPMET achieves 82.3, outperforming PMET of 81.1. These results show that by removing outdated knowledge, our method enhances the model's ability to handle new knowledge. The most notable improvement arises in the accuracy of newly updated knowledge, particularly in terms of Efficacy and Generality. Not only does the accuracy of the edited knowledge increase, but interference from outdated knowledge is minimized, resulting in higher overall performance.\nIn contrast, Locality, which measures the preservation of unrelated knowledge, slightly decreases compared to MEMIT and PMET. This trade-off between editing accuracy and Locality is expected, as our primary objective is to inject new knowledge rather than minimize changes to the model. Furthermore, Fluency and Consistency of the model's outputs are maintained at levels comparable to the original model, further supporting the robustness of our method. Appendix B presents a case study demonstrating how CoME enhances the utilization of new knowledge by unlearning outdated knowledge.\nEditing Knowledge in ZsRE Table 2 shows the performance of our method on 10,000 ZsRE samples using GPT-J and LLaMA-3. Similar to the results on the Counterfact dataset, COMEMEMIT and COMEPMET demonstrate superior performance in Efficacy and Generality on both models. For GPT-J, COMEPMET achieves Efficacy of 89.4 and Generality of 83.1, both surpassing the results of baseline PMET. These outcomes suggest that our method effectively integrates new knowledge while minimizing the influence of outdated information.\nIn terms of Locality, the results on ZsRE show significant improvements compared to the Counterfact dataset. COMEPMET achieves the highest Locality scores on both models, indicating that our approach reduces the negative impact on unrelated knowledge. Particularly on LLaMA-3, COMEPMET not only updates knowledge but also improves the model's ability to generate factual responses compared to the original model."}, {"title": "5.3 Analysis", "content": "Number of Edits Figure 2 illustrates the performance of the model as the number of simultaneous"}, {"title": "6 Conclusion", "content": "In this paper, we proposed COME to address the conflict between outdated and new knowledge that can arise during the editing process in LLMs. COME enhanced the accuracy of knowledge editing by simultaneously unlearning outdated knowledge and integrating new information. Experiments showed that our method improved the editing accuracy of existing model editing methods and successfully integrated new knowledge. This approach can be an effective solution for correcting inaccurate or biased information in large language models, and we expect it to make significant contributions to improving the reliability and consistency of LLMs."}, {"title": "Limitations", "content": "While COME successfully enhances the usability of new knowledge by removing outdated information, several limitations must be acknowledged:\n\u2022 The unlearning process requires additional computational resources. Since CoME introduces a separate stage to remove outdated knowledge, it incurs higher computational costs than traditional model editing techniques.\n\u2022 CoME is designed to remove outdated or false knowledge, which may not always be desirable in cases of temporal knowledge. For example, older information that reflects past realities can still be useful in certain contexts."}, {"title": "Ethical Considerations", "content": "Our research aims to enhance the reliability and safety of LLMs by addressing issues stemming from the retention of incorrect or biased information. By developing and improving model editing methods, we seek to contribute to the responsible use of LLMs, particularly in mitigating the spread of misinformation and harmful biases. However, it is essential to recognize that any modification to a model's knowledge must be handled with caution, ensuring that only erroneous or biased information is removed while preserving the integrity of factual content. Ensuring that model editing is performed transparently and based on clearly defined ethical guidelines will be critical as this technology develops."}]}