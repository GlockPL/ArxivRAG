{"title": "UNet++ and LSTM combined approach for Breast Ultrasound Image\nSegmentation", "authors": ["Saba Hesaraki", "Morteza Akbari", "Ramin Mousa"], "abstract": "Breast cancer stands as a prevalent cause of fatality among females on a global scale, with\nprompt detection playing a pivotal role in diminishing mortality rates. The utilization of\nultrasound scans in the BUSI dataset for medical imagery pertaining to breast cancer has\nexhibited commendable segmentation outcomes through the application of UNet and UNet++\nnetworks. Nevertheless, a notable drawback of these models resides in their inattention towards\nthe temporal aspects embedded within the images. This research endeavors to enrich the\nUNet++ architecture by integrating LSTM layers and self-attention mechanisms to exploit\ntemporal characteristics for segmentation purposes. Furthermore, the incorporation of a\nMultiscale Feature Extraction Module aims to grasp varied scale features within the UNet++.\nThrough the amalgamation of our proposed methodology with data augmentation on the BUSI\nwith GT dataset, an accuracy rate of 98.88%, specificity of 99.53%, precision of 95.34%,\nsensitivity of 91.20%, F1-score of 93.74, and Dice coefficient of 92.74% are achieved. These\nfindings demonstrate competitiveness with cutting-edge techniques outlined in existing\nliterature.", "sections": [{"title": "Introduction", "content": "Breast cancer is a condition that disrupts the process of cell division within the breast, leading\nto uncontrollable multiplication of cells beyond what is necessary. It is categorized into various\ntypes depending on its extent of spread and is recognized as a prevalent and significant\nmalignant tumor among women globally[1]. The timely detection and treatment of this disease\nhold utmost importance. Typically originating in the fibroglandular region of breast tissue,\nearly intervention plays a crucial role in reducing mortality rates associated with cancerous\ngrowths[2]. The impact of breast cancer is felt worldwide and encompasses both benign and\nmalignant forms. Benign tumors pose minimal risks as they remain localized, whereas\nmalignant tumors, also known as cancers, have the potential to spread to other body parts,\nposing a considerable threat. Early identification and classification of breast cancer are pivotal"}, {"title": "Related Works", "content": "Recent deep learning techniques, specifically convolutional neural networks (CNNs), have\nbeen extensively utilized in the field of medical image analysis[9]. Various versions of CNNs\nlike SegNet and Unet have the capability to extract high-level features and conduct organ\nsegmentation[10][11][12]. DGANet, introduced by researchers[13], is a dual global attention\nneural network designed for detecting breast lesions in ultrasound images, surpassing models\nsuch as YOLOv3 and Faster R-CNN. AMS-PAN[14] is an alternative segmentation method\nthat integrates attention mechanisms and multi-scale features, achieving remarkable accuracy\non BUSI and OASBUD datasets. Moreover, BTEC-Net[15] is a multi-stage approach for\nsegmenting breast tumors by combining DenseNet121 and ResNet101 in an ensemble,\noutperforming traditional segmentation models like UNet on BUSI and UDIAT datasets.\nCSwin-PNet[16] is another effective strategy that employs a CNN-Swin Transformer coupled\nwith a pyramid network for segmenting breast lesions in ultrasound images. This approach is"}, {"title": "Method", "content": "The objective of this article is to present a methodology aimed at enhancing the accuracy of\nextracting benign and malignant cancerous tumors within a network framework. Through the\nincorporation of multitasking attention and learning mechanisms into the traditional UNet++\narchitecture, an end-to-end encoder-decoder network specifically designed for automatic tube\nextraction has been developed (as illustrated in Figure 1). The proposed model consists of three\nprimary components: the multiscale feature extraction module, the attention block, and the\nmultitask learning module. Initially, the images undergo processing in the multiscale feature\nmap extraction module to generate multiscale feature maps, which are then further refined to\npromote the amalgamation of multiscale features. Subsequently, the attention block is utilized\nto reinforce and capture the amalgamation of feature maps across various hierarchical levels.\nLastly, the extracted feature maps undergo feature importance learning and time series feature"}, {"title": "Multiscale Feature Extraction Module", "content": "The use of encoder-decoder architectures such as Fully Connected Network and U-Net is\nwidespread for extracting multi-scale features from images. These architectures can effectively\ncombine deep feature maps from the decoder sub-network to enhance learning. UNet++ [26]\nis a variation of U-Net designed to bridge the information gap between encoder and decoder\nfeature maps before merging. This study introduces the UNet++ multiscale feature extraction\nmodule and a proposed LSTM based on UNet++. In UNet++, the encoder and decoder subnets\nare linked through a series of nested and dense paths. Furthermore, long-range connections are\nestablished by the respective encoder and decoder sections. Consequently, the decoder section\ncan integrate various hierarchical feature maps from the encoder, leading to enhanced accuracy\nand scalability of the network. In Figure 1, xi,j represents the output of node Xi,j, where i\nindicates the downsampling layer along the decoder path\nXi,j= Conv(X(i-1,j)\nConv(cat(rcat(xi,0, xi,1, ..., xi,j\u22121), \u04b9up(xi+1,j\u22121)))\nConv(rcat(rcat(xi,0, xi,1, ..., xi,j\u22121), \u04b9up(xi+1,j-1)))\nj =\n0\nj>\n0\nj>\n0\nThe symbol Conv denotes a convolution operation that has an activation function\napplied to it. The operation represented by Ycat is a concatenation, while Yup stands for an\nupsampling layer. When j = 0, Xi,j corresponds to the nodes in the encoder subnet. In cases"}, {"title": "Attention Block", "content": "Low-level feature maps extract hierarchical semantic features with poor semantic\ninformation but rich spatial information (small receptive fields).\nHigh-level feature maps have strong semantic information but weak spatial information\nbecause they possess large receptive fields.\nTherefore, it is essential to distinctly combine feature maps from different levels so that the\nnetwork allocates reasonable attention to both high-level and low-level features. Furthermore,\ndue to the spatial impact of similar patterns and background noise on the extracted features, it\nis essential to highlight important parts and disregard unimportant ones. Hence, our approach\ninvolves assigning weights to prioritize the most crucial features when integrating feature maps\nfrom various levels. These weights are allocated to feature maps across different layers based\non their relative significance in the channel dimension. Consequently, we have incorporated\nthe Convolutional Block Attention Module (CBAM) into our proposed network. This module's\npurpose is to discern which information should be accentuated or suppressed. As depicted in\nFigure 2, the predicted map of F \u2208 R (C*H*W ) is produced by the Multiscale features\nextraction module.\nThe primary objective of channel attention is to analyze significant features in an input\nimage by leveraging the inter-channel connections of features. Initially, two operations, FC\nAVG and FC MAX, were developed to aggregate general information from each channel and\ndiscern distinct object features, respectively. These two descriptors are then input into an\nLSTM (shown in Figure 3) to generate two vectors, which are subsequently combined through\nelement summation to yield the final channel attention map Mc (F).\nM(F) = Sigmoid(lstm(lstm(Favg) + lstm(lstm(FMAX)))\nThe basic LSTM cell is composed of three gates: the forgetting gate (ft) decides what\nproportion of the previous data to forget. The input gate (it) assesses the information to store\nin the cell memory. Additionally, the output gate (ot) defines the approach for computing the\noutput based on the current information.\nft = \u03c3(W eif xt + U eif ht-1 + beif)\nit = \u03c3(W eiixt + U eiiht\u22121 + beii)\not = \u03c3(W eioxt + U eioht-1 + beio)"}, {"title": "", "content": "ct = ft \u00b0 ct-1 + it \u00b0 (W eicxt + U eicht-1 + beic)\nht = ot \u00b0 \u03c4(ct)\nThe size of the input is denoted as 'n', while the size of the cell state and output is denoted\nas 'm'. The input vector at time 't' is represented by 'xt' (size n \u00d7 1), the forget gate vector by 'ft'\n(size m \u00d7 1), the input gate vector by 'it' (size m \u00d7 1), the output gate vector by 'Ot' (size m \u00d7\n1), the output vector by 'ht' (size m \u00d7 1), and the cell state vector by 'ct' (size m \u00d7 1). The input\ngate weight matrices are represented as [Weif, Weii, Weio, Weic] (size m \u00d7 n), while the output\ngate weight matrices are represented as [Ueif, Ueii, Ueio, Ueic] (size m \u00d7 m). The bias vectors\nare denoted as [beif, beii, beio, beic] (size m \u00d7 1). The logistic sigmoid activation function is\nrepresented by o, and the hyperbolic tangent activation function is represented by \u03c4. The\nproposed approach for spatial attention examines areas of information that require more focus\n(refer to Figure 4). Pooling operations are used to collect information from individual channels\nand then combine them. Following this, a standard 7*7 convolutional layer is applied to the\nconcatenated and convoluted channels to generate a spatial attention map.\nM5(F) = Sigmoid(Conv7*7 ([FVG; FMAX])\nThe entire process of paying attention can be expressed in the following way:\nF' = Mc (F) F\nF \" = Ms (F')  F'\nThe value of Mc in the real number set (C*1*1) represents the channel attention map, while\nMs in the real number set (1*H*W) represents the spatial attention. The symbol \"\u2297\" denotes\nelement-wise multiplication, and F\" represents the final feature map.\nThe proposed approach aims to reduce the pixel distance between tumor locations. In our plan,\nwe will focus on minimizing this distance:\nDist(i) = damin(minjd(i, j), d)\nIn an image, let i represent a pixel, x represent the group of pixels that make up tumor\nboundaries, and min minj denote the Euclidean distance between pixel i and the closest\nboundary pixel j. Let d be a threshold, and let od represent a sign function that indicates whether\nthe pixel is inside or outside the tumor. If dd = 1, the pixel i is within the tumor mask, whereas"}, {"title": "", "content": "if dd = -1, it signifies that the pixel i is outside the tumor mask. Given these definitions of the\nmulti-task loss, the proposed method can be described as follows:\nL = 21Lseg + 22LDC\nThe loss function for the segmentation task is denoted as Lseg, and the prediction of\ndistance class maps is denoted as LDC. The learnable weights are represented by 21 and 22.\nIn simpler terms, the formula can be summarized in the following way:\nL (x; \u03b8, \u03c3DC, \u03c3seg) = Lseg(x; \u03b8, \u03c3seg) + LDC (x; \u03b8, \u03c3DC\nThe classification task's likelihood is determined based on the network output fc(x) at the\nend. This function is commonly known as softmax.\nP(C = 1|x, \u03b8,\u03c3\u03b9) =\nexp[fc(x)]\nEcr=1exp[afcr(x)]\nIn this equation, P represents the predicted probability, fc(x) is the output obtained, fc' (x)\nis the actual input, and ot is the Scaling factor. Broadly speaking, the classification error and\nuncertainty can be further expanded as follows:\n(x; \u03b8, \u03c3\u03c4) =\nC\n1\nC\n\u03a3\nc=1\n-Clog P(Cc = 1|x; \u03b8, \u03c3\u03c4)\n=\nC\n1\n\u03c3\u03c4\n=-Clog{exp(x)]}+logexp=f(x)]"}, {"title": "Result", "content": "The outcome of the study involved a comparative analysis of the proposed approach\nagainst 15 other established methodologies present in the existing literature. These\nmethodologies include SD-CNN [28], CNN-GTD [29], GA-ANNs [30],\nSeResNet18[31], Faster R-CNN+CNNs [32], CNN+LR[33], ODET[34], Chowdary et\nal. [35], Inan et al.[36], Byra et al.[37], Shi et al.[38], and SaTransformer[39].\nFurthermore, the assessment involved the evaluation of seven key criteria, namely\naccuracy, specificity, precision, sensitivity, F1-score, Jaccard, and dice. Similar to other\nmethodologies documented in the literature, the proposed model allocated 80% of the"}, {"title": "Conclusion", "content": "This research introduces UNet++LSTM, a new model designed for segmenting breast\nultrasound images. The model incorporates a multiscale feature extraction module, an attention\nblock, and a multitask learning module to effectively capture spatial and temporal features.\nWhen compared to 15 established models, our UNet++LSTM model demonstrated superior\nperformance, achieving an accuracy of 98.88% and outperforming all other models.\nThe model achieved a specificity of 99.53%, with a precision of 95.34%, a sensitivity of\n91.20%, an F1-score of 93.74%, and a Dice score of 92.74%, demonstrating significant\nadvancements compared to existing techniques. The proposed method exhibited a 1.25%\nincrease in precision compared to the top-performing comparative model and surpassed the\nODET model in specificity. Both UNet++ and the proposed UNet++LSTM_models\ndemonstrated stable learning processes, as evidenced by the loss and accuracy diagrams,\nindicating avoidance of overfitting and underfitting. To further improve the model's\nperformance and resilience, we suggest implementing transfer learning, exploring advanced\nparameter initialization techniques, and integrating rote learning strategies.\nThe BUSI dataset played a crucial role in training and testing our model, serving as a\nthorough evaluation platform to guarantee the credibility and applicability of our suggested\nmethod. In summary, the UNet++LSTM_model shows significant progress in segmenting\nbreast ultrasound images, providing an encouraging solution for the early and precise detection\nof breast cancer."}, {"title": "Abbreviations", "content": ""}, {"title": "Authors' contributions", "content": ""}, {"title": "Funding", "content": "The authors received no direct funding for this research."}, {"title": "Availability of data and materials", "content": ""}, {"title": "Declarations", "content": ""}, {"title": "Competing interest", "content": "The authors declare no competing interests."}]}