{"title": "Vision-Enhanced Time Series Forecasting via Latent Diffusion Models", "authors": ["Weilin Ruan", "Siru Zhong", "Haomin Wen", "Yuxuan Liang"], "abstract": "Diffusion models have recently emerged as powerful\nframeworks for generating high-quality images. While recent\nstudies have explored their application to time series forecasting,\nthese approaches face significant challenges in cross-modal\nmodeling and transforming visual information effectively to\ncapture temporal patterns. In this paper, we propose LDM4TS, a\nnovel framework that leverages the powerful image reconstruction\ncapabilities of latent diffusion models for vision-enhanced time\nseries forecasting. Instead of introducing external visual data, we\nare the first to use complementary transformation techniques to\nconvert time series into multi-view visual representations, allowing\nthe model to exploit the rich feature extraction capabilities of the\npre-trained vision encoder. Subsequently, these representations are\nreconstructed using a latent diffusion model with a cross-modal\nconditioning mechanism as well as a fusion module. Experimental\nresults demonstrate that LDM4TS outperforms various specialized\nforecasting models for time series forecasting tasks.", "sections": [{"title": "I. INTRODUCTION", "content": "Time Series Forecasting (TSF) is a critical task in numerous\nreal-world applications [1], such as demand planning [2],\nenergy load estimation [3], climate modeling [4], and traffic\nflow management [5]. Deep learning models have significantly\nadvanced time series forecasting by capturing intricate temporal\ndependencies. Early methods introduced sequential modeling\ncapabilities [6], [7], while later architectures such as Transform-\ners have improved the modeling of long-range dependencies and\nefficiency [8], [9], [10], [11], [12]. Despite their success, these\nmodels lack the ability to model the underlying uncertainty\nbehind the time series.\nIn parallel with these developments, diffusion models have\nemerged as powerful generative frameworks, excelling in tasks\nlike text-to-image generation [13], image synthesis [14], and\nsuper-resolution [15]. Their iterative denoising process demon-\nstrates exceptional capability in modeling complex distributions,\nprompting recent applications to time series modeling [16],\n[17], [18], [19], [20]. However, these models face significant\nchallenges in time series forecasting as they are inherently\ndesigned for spatially structured data like images, while time\nseries exhibits fundamentally different characteristics such as\nsequential dependencies and non-stationarity [21], [22]. The\npotential of transforming temporal patterns into structured\nvisual representations, rather than directly processing raw\nsequences, remains largely unexplored for leveraging diffusion-\nbased temporal dynamics.\nMeanwhile, previous research has demonstrated that trans-\nforming time series data into visual representations can"}, {"title": "II. RELATED WORK", "content": "A. Diffusion Models for Time Series\nDiffusion models have emerged as a powerful class of gen-\nerative approaches, demonstrating remarkable success across\nvarious high-dimensional data domains. Denoising Diffusion\nProbabilistic Models (DDPM) [14] employ a Markov chain\nto add and remove Gaussian noise, producing high-fidelity\nsamples gradually. Score-based diffusion models [28] directly\nestimate the score function of data distributions. Meanwhile,\nconditional diffusion models incorporate additional guidance\nsignals to enhance generation quality [29] further.\nRecent years have witnessed increasing applications of\ndiffusion models in time series analysis [30], [31]. For\nforecasting tasks, TimeGrad [16] pioneers this direction by\nincorporating autoregressive components, while D3VAE [32]\nintegrates variational auto-encoders with diffusion for enhanced\ncapabilities. TSDiff [33] iteratively refines base forecasts\nthrough implicit probability densities. In parallel, conditional\napproaches have been explored for imputation tasks, where\nCSDI [19] and MIDM [34] handle irregular time series through\nconditional score matching. Domain-specific designs have\nalso emerged, such as DiffLoad [35] for load forecasting,\nWaveGrad [36] and DiffWave [37] for audio synthesis, and\nEHRDiff [38] for healthcare applications. DiffSTG [39] further\nexplores spatio-temporal graph structures in diffusion models\nfor time series.\nHowever, these methods primarily focus on single-modality\ntime series and lack mechanisms for modeling cross-modal\ntemporal relationships. Our work advances the development\nof latent diffusion models for TSF by incorporating multi-\nmodal information and exploiting cross-modal conditioning\nmechanisms, thereby substantially improving the accuracy and\nrobustness under different forecasting scenarios."}, {"title": "B. Vision-enhanced Time Series Forecasting", "content": "Vision models like ViT [40] and MAE [41] have demon-\nstrated remarkable success in CV through their powerful feature\nextraction capabilities, showing strong generalization abilities\nwhen pre-trained on large-scale datasets like ImageNet [42].\nInspired by the success of vision models, researchers have\nbegun exploring their potential in time series forecasting.\nThe concept of treating time series as images has evolved\nfrom traditional approaches using Gramian Angular Fields\n(GAF) and Markov Transition Fields (MTF) [43] to more\nsophisticated methods. TimesNet [26] introduces a novel\napproach by transforming temporal data into 2D matrices,\nenabling the use of inception blocks for multi-scale temporal\npattern extraction. Building upon this idea, SparseTSF [44]\nincorporates sparsity constraints to better capture periodic and\ntrend components in time series. ViTime [45] demonstrates the\npossibility of zero-shot forecasting by treating time series as\nvisual signals while VisionTS [27] shows that pre-trained visual\nmodels can directly serve as time series forecasters without\ndomain-specific adaptation.\nHowever, these approaches are predominantly deterministic\nand lack uncertainty quantification capabilities since they are\nnot built within generative frameworks. Our work addresses\nthese limitations by integrating latent diffusion models with\nvisual representations in a unified framework. This design\nenables our model to effectively capture temporal dependencies\nwhile maintaining the uncertainty modeling capabilities inherent\nin diffusion models.\nIn summary, while existing methods have made significant\nprogress in either cross-modal temporal pattern extraction or\nprobabilistic generative modeling, they have yet to effectively\ncombine these complementary strengths. Our work bridges this\ngap by introducing a unified framework that leverages both\nvisual representation learning and latent diffusion processes.\nUsing multi-view vision transformations and multi-conditional\ngeneration, our approach captures rich temporal patterns while\nproviding well-calibrated uncertainty estimates, advancing\nboth visual representation learning and probabilistic modeling\naspects of TSF."}, {"title": "III. METHODOLOGY", "content": "As illustrated in Figure 2, LDM4TS employs a cross-modal\narchitecture combining vision transformation and diffusion-\nbased generation for time series processing. The framework first\ntransforms raw time series data through complementary encod-\ning methods, generating multi-view visual representations that\ncapture diverse temporal patterns. These visual representations"}, {"title": "A. Time Series to Multi-view Vision Transformation", "content": "Time series data exhibits complex temporal patterns across\nmultiple views, from local fluctuations to long-term trends,\nmaking direct modeling challenging. While existing methods\npredominantly rely on sequential architectures to capture\ntemporal dependencies, they often fail to fully leverage the\nrich structural correlations embedded within time series data.\nTo address these limitations, we propose a novel approach that\ntransforms time series into visual representations, capturing\nmulti-view temporal characteristics through a vision encoder\n(VE) and harnessing the sophisticated pattern recognition\ncapabilities of latent diffusion models. Given an input sequence\n$X \\in \\mathbb{R}^{B \\times L \\times D}$, where B denotes the batch size, L represents\nthe sequence length, and D indicates the feature dimension, we construct a three-channel image representation through complementary encoding methods. The technical details of the complete transformation process are presented in Ap- pendix Mob and H.\nWe transform time series data into three complementary vi-\nsual representations, each designed to capture distinct temporal characteristics. Specifically, (i) the Segmentation representation (SEG) [27] that employs periodic restructuring to preserve local temporal structures, enabling the detection of recurring patterns across multiple time scales; (ii) the Gramian Angular Field (GAF) [46], [25] that transforms temporal correlations into spatial patterns through polar coordinate mapping, effectively capturing long-range dependencies crucial for forecasting; and (iii) the Recurrence Plot (RP) [23], [47] that constructs similarity matrices between time points to reveal both cyclical behaviors and temporal anomalies, providing a complementary view of the underlying structure. As demonstrated in Figure 5,\nthese three visual encoding strategies effectively convert temporal dynamics into structured spatial patterns, enabling our model to capture local dependencies and global correlations through the diffusion process. The complete transformation process can be formalized as follows:\n$X = \\frac{X - \\min(X)}{\\max(X) - \\min(X) + \\epsilon}$ (1)\n$I_{SEG} = \\sum_{d=1}^{D}(f_{interp}(R(Pad(X_{:,:,:}), L+P,T)))$ (2)\n$I_{GAF} = f_{interp}(\\sum_{d=1}^{D} cos(\\Theta_{d} \\Theta_{d})^{T})$ (3)\n$I_{RP} = f_{interp}(exp(-\\frac{\\| X_i - X_j \\|^2}{2}))$ (4)\n$I_m = Concat[I_{SEG}; I_{GAF}; I_{RP}]$ (5)\nwhere $R(X, m, n)$ transforms tensor X into an $m \\times n$ matrix for periodic pattern extraction, $Pad(\\cdot)$ ensures sequence length divisibility by period T, $f_{interp}$ performs bilinear interpolation to target size (H, W), $\\psi(\\cdot)$ normalizes each channel independently, and $\\oplus$ denotes the outer sum operation. The resulting multi-channel image $I_m \\in \\mathbb{R}^{B \\times 3 \\times H \\times W}$ integrates complementary views of temporal dynamics."}, {"title": "B. Latent Diffusion for Time Series Reconstruction", "content": "Unlike conventional diffusion models that operate in high-dimensional pixel space, we perform the denoising process in a compressed latent space, significantly reducing computational complexity while preserving temporal dynamics. Our frame- work extends Stable Diffusion [13] with specialized adaptations for time series data through cross-modal conditional control and enhanced temporal modeling. The algorithm details are in Appendix H."}, {"title": "a) Multi-conditional Generation Framework", "content": "To guide accurate temporal feature reconstruction, we devise a cross-modal conditioning mechanism that uses both frequency domain information and semantic descriptions. Given a visual representation $I \\in \\mathbb{R}^{B \\times 3 \\times H \\times W}$, we first encode it into latent space and derive conditional signals as:\n$C_{freq} = FFTEncoder(X), C_{text} = TextEncoder(X)$ (6)\n$z = E(I). s, C_m = CrossAttn(MLP([C_{text}; C_{freq}]), z)$ (7)\nwhere $E()$ represents the VAE encoder, s is the latent space scaling factor (see Appendix I0a for detailed derivation). $C_{freq} \\in \\mathbb{R}^{B \\times (2DL+2)}$ captures periodic patterns through frequency analysis while $C_{text} \\in \\mathbb{R}^{B \\times d_{model}}$ encodes statistical properties and domain knowledge through natural language descriptions. The detailed implementations of FFTEncoder and TextEncoder are provided in Appendix M.\nb) Forward Diffusion Process: The forward process implements a variance-preserving Markov chain that progres- sively injects Gaussian noise into the latent representations transformed from multi-view visual encodings of time series data. This controlled noise injection, operating in a compressed latent space rather than pixel space, enables efficient learning of temporal patterns across different scales while preserving the intrinsic information from vision transformations. For a given initial latent representation $z_0$, we define the forward diffusion process through a series of probabilistic transformations:\n$q(z_t | z_{t-1}, I_m) = \\mathcal{N}(z_t; \\sqrt{\\alpha_t}z_{t-1}, (1 - \\alpha_t)I_m)$ (8)\n$q(z_t | z_0, I_m) = \\mathcal{N}(z_t; \\sqrt{\\bar{\\alpha_t}}E(I)/s, (1 - \\bar{\\alpha_t})I_m)$ (9)\n$\\bar{\\alpha_t} = \\prod_{s=1}^{t}\\alpha_s, t \\in \\{1, ..., T\\}$ (10)\nwhere $\\{\\alpha_t\\}_{t=1}^{1}$ defines a scaled linear noise schedule, and $\\bar{\\alpha_t}$ controls the cumulative noise level across t timesteps. The encoder E() maps the multi-view visual representations $I_m$ to a lower-dimensional latent space.\nc) De-noising Process: The reverse process employs a parameterized U-Net architecture to progressively denoise the representations exploiting cross-modal conditioning mecha- nisms. By incorporating frequency and semantic embeddings, this process uniquely captures both complex temporal dynam- ics while maintaining coherent long-term dependencies. The complete denoising process is formulated as:\n$P_{\\theta}(z_{t-1}|z_t, C_m) = \\mathcal{N}(z_{t-1}; \\mu_{\\theta}(z_t, t, c_m), \\Sigma_{\\theta}(z_t,t))$ (11)\n$\\mu_{\\theta}(z_t, t, c_m) = \\frac{1}{\\sqrt{\\alpha_t}} (z_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha_t}}} \\epsilon_{\\theta}(z_t, t, c_m))$ (12)\n$L = E_{z_0,\\epsilon, t} [\\|\\epsilon - \\epsilon_{\\theta}(z_t, t, c_m)\\|^2] + \\lambda L_{recon}$ (13)\nwhere $\\epsilon_{\\theta}$ predicts the noise component given the noisy latent $z_t$, timestep t, and cross-modal condition $C_m$. The final reconstructed image $\\hat{I} = D(z_0/s)$ is obtained by decoding the denoised latent representation. This design enables effective temporal pattern learning, comprehensive structural information preservation, and latent space optimization."}, {"title": "C. Temporal Projection and Fusion", "content": "While the latent diffusion model captures global patterns\neffectively, local temporal dynamics and distribution shifts\nrequire explicit modeling. As shown in Fig. 3, we propose a\ntemporal encoder (TE) that complements the diffusion process\nthrough three key components: patch embedding, transformer\nencoding, and adaptive feature fusion. Given input sequence\n$X \\in \\mathbb{R}^{B \\times L \\times D}$, we adopt the patch embedding strategy [40],\n[48] to encode temporal information. Then, the embeddings\nare processed through L transformer layers, where $X_{norm} =$\n$LN(X)$. The embeddings are processed as follows:\n$h_0 = PatchEmbed(X_{norm}) \\in \\mathbb{R}^{B \\times N_p \\times d}$ (14)\n$h'_i = h_{i-1} + MSA(LN(h_{i-1}))$ (15)\n$h_i = h'_i + MLP(LN(h'_i))$ (16)\n$Z_{TE} = LinearProj(h_L) \\in \\mathbb{R}^{B \\times L_{pred} \\times D}$ (17)\nwhere $N_p$ denotes patch count, h is hidden states and d is\nthe hidden dimension. MSA and LN represent multi-head\nself-attention and layer normalization respectively. Finally, we\nimplement an adaptive fusion mechanism that dynamically\ncombines temporal features $Z_{TE}$ with visual features $Z_{VE}$:\n$g = Sigmoid(MLP([Z_{TE}; Z_{VE}]))$ (18)\n$\\hat{Y} = g Z_{TE} + (1 - g) Z_{VE}$ (19)\nwhere learned gates g dynamically balance the contribution\nof global patterns from diffusion and local dynamics from\ntemporal encoding, enabling robust adaptation to different\ntemporal characteristics."}, {"title": "IV. EXPERIMENTS", "content": "A. Settings\na) Dataset and Metrics: In this section, we evaluate\nthe proposed LDM4TS on seven widely-used time series"}, {"title": "B. Long-term Forecasting", "content": "We evaluate the long-term forecasting capabilities of\nLDM4TS across multiple prediction horizons. As shown\nin Table I, LDM4TS consistently outperforms state-of-the-\nart baselines, achieving optimal results in both MSE and\nMAE. On the ETT dataset family, our approach demonstrates\nsignificant improvements, achieving the best MSE of 0.352\non ETTm1 compared to the second-best performer DLinear\n(0.404), and reducing MSE by 11.8% on ETTh2 (0.387)\ncompared to FEDformer (0.439). The advantages extend to\nhigh-dimensional scenarios, achieving superior results on both\nElectricity (321 variables, MSE: 0.199 vs TimesNet 0.208)\nand Traffic datasets (862 variables, MSE: 0.550 vs DLinear\n0.624), which is mainly benefited by our latent diffusion\nframework that compresses high-dimensional temporal patterns\ninto a lower-dimensional latent space while preserving essential\nstructural information through vision transformations. Notably,\nLDM4TS substantially outperforms existing diffusion-based\nmethods CSDI and ScoreGrad across all datasets, with MSE\nimprovements of up to 84.2% and 89.4% on the Traffic dataset\nrespectively. Overall, LDM4TS achieves the best performance\nin 5 out of 7 datasets on each metric, validating that our\nvision-enhanced modeling strategy effectively captures complex\ntemporal dynamics across diverse forecasting scenarios."}, {"title": "C. Few-shot Forecasting", "content": "To evaluate model robustness under data scarcity, we conduct\nexperiments using only 10% and 5% of the training data. As\nshown in Table II, LDM4TS achieves optimal performance on\n6 out of 7 datasets in both MSE and MAE metrics. On the ETT\nbenchmark series, LDM4TS consistently outperforms state-of-"}, {"title": "D. Zero-Shot Forecasting", "content": "To evaluate cross-domain generalization, we conduct zero-\nshot transfer experiments across different datasets without any\nfine-tuning. As shown in Table IV, LDM4TS achieves the\nbest performance in 4 MSE and 5 MAE metrics out of 8\nscenarios, demonstrating strong cross-domain transferability.\nFor challenging transfer tasks like ETTh1 \u2192 ETTh2 and\nETTh1 \u2192 ETTm2, LDM4TS achieves MSE of 0.458 and\n0.369 respectively, outperforming both DLinear (0.493, 0.415)\nand FEDformer (0.495, 0.373). The model also achieves\nthe best of on ETTm1 \u2192 ETTh1 (0.452, 0.434) and\nETTm2 \u2192 ETTm1 (0.588, 0.487). The advantages are par-\nticularly pronounced when compared to diffusion models, with\nLDM4TS achieving substantial improvements over both CSDI\nand ScoreGrad across all transfer scenarios, reducing MSE by\nup to 49.0% in challenging tasks. Notably, while most baseline\nmethods show significant performance degradation in cross-\ndataset transfers, LDM4TS maintains consistent performance\nacross different transfer pairs, suggesting robust generalization\ncapabilities."}, {"title": "E. Model Analysis", "content": "a) Overall Performance Analysis: LDM4TS demonstrates\nsuperior performance across various forecasting scenarios,\nexcelling in long-term few-shot, and zero-shot predictions,\nwhile maintaining computational efficiency with only 5.4M\nlearnable parameters and fast inference speed (see Append S\nfor detailed analysis). Through comprehensive experiments,\nwe observe that our approach effectively captures both global\ntrends and local patterns in time series data. As shown in\nFigure 4, LDM4TS achieves good performance in forecasting\nstructured patterns, such as the clear periods in Traffic datasets\n(MSE: 0.621) and regular consumption patterns in ECL data\n(MSE: 0.199). The performance shows slight degradation on\ndatasets with irregular patterns or abrupt changes, suggesting\npotential areas for future improvement in handling non-\nstationary patterns.\nb) Visual Encoding Effectiveness: Figure 5 illustrates our\nmulti-view vision transformation strategy, which forms the\nfoundation of LDM4TS's strong performance. Each encoding\nmethod captures distinct temporal characteristics: SEG pre-\nserves local temporal structures through periodic restructuring,\nenabling detection of recurring patterns at multiple time scales;\nGAF transforms temporal correlations into spatial patterns\nthrough polar coordinate mapping, effectively capturing long-\nrange dependencies; and RP generates similarity matrices that\nhighlight both cyclical behaviors and temporal anomalies. The\ncomplementary nature of these encodings is particularly evident\nin the ETT datasets, where the combination achieves a 24.2%\nreduction in MSE compared to using any single encoding\nmethod."}, {"title": "V. CONCLUSION", "content": "We present LDM4TS that adapts latent diffusion models\nwith cross-modal conditioning mechanism for time series fore-\ncasting by transforming temporal data into multi-view visual\nrepresentations. Our method significantly outperforms existing\ndiffusion-based methods and specialized forecasting models,\nproviding a novel vision-enhanced perspective to address the\nkey challenges of intrinsic temporal pattern extraction and\nuncertainty modeling. Future work will focus on exploring\ndiffusion models' potential in broader time series applications\nand developing comprehensive benchmarks for diffusion-based\nmethods."}, {"title": "APPENDIX", "content": "A. Dataset Details\nTABLE VI: Summary of the benchmark datasets. Each dataset contains multiple time series (Dim.) with different sequence lengths, and is split into training, validation and testing sets. The data are collected at different frequencies across various domains.\nWe conduct experiments on the above real-world datasets to evaluate the performance of our proposed model and follow the same data processing and train-validation-test set split protocol used in TimesNet benchmark [26], ensuring a strict chronological order to prevent data leakage. Different datasets require specific adjustments to accommodate their unique characteristics:\na) ETT Dataset [71]: The Electricity Transformer Temperature (ETT) dataset consists of both hourly (ETTh) and 15-minute (ETTm) frequency data, with 7 variables (enc_in = dec_in = c_out = 7) measuring transformer temperatures and related factors. For ETTh data, we set periodicity to 24 with hourly frequency, while ETTm data uses a periodicity of 96 with 15-minute intervals. Standard normalization is applied to each feature independently, and the model maintains the same architectural configuration across both temporal resolutions.\nb) Traffic Dataset [26]: The traffic flow dataset represents a high-dimensional scenario with 862 variables capturing traffic movements across different locations. To handle this scale, we implement gradient checkpointing and efficient attention mechanisms, complemented by progressive feature loading. The batch size is dynamically adjusted based on available GPU memory, and we maintain a periodicity of 24 to capture daily patterns. Our model employs specialized memory optimization techniques to process this large feature space efficiently.\nc) ECL Dataset [11]: The electricity consumption dataset contains 321 variables monitoring power usage patterns. We employ robust scaling techniques to handle outliers and implement sophisticated missing value imputation strategies. The model incorporates adaptive normalization layers to address the varying scales of electricity consumption across different regions and time periods. The daily periodicity is preserved through careful temporal encoding, while the high feature dimensionality is managed through efficient attention mechanisms.\nd) Weather Dataset [11]: This multivariate dataset encompasses 21 weather-related variables, each with distinct physical meanings and scale properties. Our approach implements feature-specific normalization to handle the diverse variable ranges while maintaining their physical relationships. The model captures both daily and seasonal patterns through enhanced temporal encoding, with special attention mechanisms designed to model the complex interactions between different weather variables. We maintain consistent prediction quality across all variables through carefully calibrated cross-attention mechanisms.\nB. Optimization Settings\n1) Model Architecture Parameters: The core architecture of our diffusion-based model consists of several key components, each with specific parameter settings. The autoencoder pathway is configured with an image size of 64 \u00d7 64 and a patch size of 16, providing an efficient latent representation while maintaining temporal information. The diffusion process uses 1000 timesteps with carefully tuned noise scheduling (\u03b2start = 0.00085, \u03b2end = 0.012) to ensure stable training.\nFor the transformer backbone, we employ a configuration with dmodel = 256 and 8 attention heads, which empirically shows strong performance across different datasets. The encoder-decoder structure uses 2 encoder layers and 1 decoder layer, with a feed-forward dimension of 768, striking a balance between model capacity and computational efficiency.\n2) Training Parameters: We adopt a comprehensive training strategy with both general and task-specific parameters. The model is trained with a batch size of 32 and an initial learning rate of 0.001, using the AdamW optimizer. Early stopping with a patience of 3 epochs is implemented to prevent over-fitting. For time series processing, we use a sequence length of 96 and a prediction length of 96, with a label length of 48 for teacher forcing during training.\nThe training process employs automatic mixed precision (AMP) when available to accelerate training while maintaining numerical stability. We use MSE as the primary loss function, supplemented by additional regularization terms for specific tasks."}, {"title": "C. Evaluation Metrics", "content": "For evaluation metrics, we utilize the mean square error (MSE) and mean absolute error (MAE) for long-term forecasting. The calculations of these metrics are as follows:\nMSE = \\frac{1}{H} \\sum_{h=1}^{H}(Y_h - \\hat{Y}_h)^2 ,  MAE = \\frac{1}{H} \\sum_{h=1}^{H}|Y_h - \\hat{Y}_h|,\nwhere s is the periodicity of the time series data. H denotes the number of data points (i.e., prediction horizon in our cases). $Y_h$ and $\\hat{Y}_h$ are the h-th ground truth and prediction where h \u2208 {1,\u2026\u2026,H}.\nWe compare our approach with three categories of baseline methods used for comparative evaluation: transformer-based architectures, diffusion-based models, and other competitive approaches for time series forecasting.\na) Transformer-based Models:: FEDformer [10] integrates wavelet decomposition with a Transformer architecture to efficiently capture multi-scale temporal dependencies by processing both time and frequency domains. Autoformer [11] introduces a decomposing framework that separates the time series into trend and seasonal components, employing an autocorrelation mechanism for periodic pattern extraction. ETSformer [12] extends the classical exponential smoothing method with a Transformer architecture, decomposing time series into level, trend, and seasonal components while learning their interactions through attention mechanisms. Informer [9] addresses the quadratic complexity issue of standard attention mechanisms through ProbSparse self-attention, which enables efficient handling of long input sequences. Reformer [50]"}, {"title": "D. Long-term Forecasting", "content": "E. Few-shot Forecasting\nF. Zero-shot Forecasting"}, {"title": "G. Showcases", "content": "H. Visualization of Pixel Space"}, {"title": "I. Autoencoder Framework", "content": "Latent Diffusion Models (LDMs) leverage the autoencoder architecture to facilitate efficient learning in the latent space. An autoencoder comprises two primary components: an encoder and a decoder. The encoder $\\varepsilon$ compresses high-dimensional input data $x \\in \\mathbb{R}^D$ into a lower-dimensional latent representation $z \\in \\mathbb{R}^d$, where $d \\ll D$. This compression not only reduces the computational complexity but also captures the essential features of the data. In our implementation, we utilize the pre-trained AutoencoderKL from the stable-diffusion-v1-4, which has demonstrated remarkable capabilities in image compression and reconstruction. Mathematically, this process is described as:\n$z = \\varepsilon(x)$ (20)\na) Latent Space Scaling: In practice, the latent representations produced by the encoder are typically scaled by a factor $s = 0.18215$ to ensure numerical stability and optimal distribution characteristics:\n$z_{scaled} = s\\cdot\\varepsilon(x)$ (21)\nThis specific scaling factor originates from the VAE design in Stable Diffusion and is derived through empirical optimization. The value is calculated to minimize the KL divergence between the scaled latent distribution and the standard normal distribution:\n$s^* = \\text{argmin}_s \\mathbb{E}_{x\\sim p_{data}}[D_{KL}(s\\cdot\\varepsilon(x)|| \\mathcal{N}(0,1))]$ (22)\nwhere $D_{KL}$ represents the Kullback-Leibler divergence. In our framework, this scaling operation serves multiple critical purposes. It ensures numerical stability during the diffusion process by maintaining consistent value ranges while facilitating better optimization dynamics by bringing the latent distribution closer to the standard normal. This operation also maintains compatibility with the pre-trained weights while allowing for efficient processing of our visual time series representations.\nThe optimization process involves collecting latent representations $z = \\varepsilon(x)$ from a large dataset, computing their empirical statistics $\\mu_z$ and $\\sigma_z$, and determining the optimal scaling factor s such that $s\\sigma_z \\approx 1$ to match the target standard deviation. This process has been extensively validated in the context of both image generation and, in our case, time series visual representations. During decoding, the inverse scaling is applied to restore the original magnitude:\n$x = D(z_{scaled}/s)$ (23)"}, {"title": "J. Foundations of Diffusion Models", "content": "Diffusion models define a principled framework for generative modeling through gradual noise addition and removal. In our LDM4TS framework, we adapt this process specifically for time series visual representations while maintaining the fundamental probabilistic structure.\na) Forward Process: The forward diffusion process follows a Markov chain that progressively adds Gaussian noise:\n$q(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t I)$ (25)\n$q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t} x_0, (1-\\bar{\\alpha}_t)I)$ (26)\nHere, $q(x_t|x_{t-1})$ describes the transition from step $t-1$ to $t$, where $\\beta_t$ controls the noise schedule. In our implementation, we adopt a linear noise schedule with carefully tuned parameters $\\beta_{start} = 0.00085$ and $\\beta_{end} = 0.012$. The second equation gives the direct relationship between any noisy sample $x_t$ and the original data $x_0$, where $\\bar{\\alpha}_t = \\prod_{s=1}^{t}(1-\\beta_s)$ represents the cumulative product of noise levels.\nb) Reverse Process: The reverse process learns to gradually denoise data through:\n$P_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$ (27)\nwhere the mean and variance are parameterized as:\n$\\mu_\\theta(x_t, t) = \\frac{1}{\\sqrt{\\alpha_t}}(x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(x_t, t))$ (28)\n$\\Sigma_\\theta(x_t, t) = \\frac{1-\\bar{\\alpha}_{t-1}}{1-\\bar{\\alpha}_t} \\beta_t$ (29)\nIn our framework, we modify the noise prediction network $\\epsilon_\\theta$ to accept additional conditioning information, transforming the reverse process into:\n$P_\\theta(x_{t-1}|x_t, c) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t, c), \\Sigma_\\theta(x_t, t))$ (30)\nwhere $c$ represents the concatenated frequency domain embeddings and encoded textual descriptions. This modification allows the model to leverage both spectral and semantic information during the denoising process while maintaining the same variance schedule.\nc) Score-based Generation: The score function represents the gradient of the log-density:\n$S_\\theta(x_t, t) = \\nabla_{x_t} \\log p_\\theta(x_t) = \\frac{\\epsilon_\\theta(x_t, t)}{\\sqrt{1-\\bar{\\alpha}_t}}$ (31)\nThis formulation enables training through denoising score matching:\n$L_{score} = \\mathbb{E}_{t, x_0, \\epsilon}[||\\epsilon - \\epsilon_\\theta(x_t, t)||^2]$ (32)\nd) Sampling Methods: Different sampling strategies offer various trade-offs between generation quality and computational efficiency. In our implementation, we primarily utilize DDIM for its deterministic nature and faster sampling capabilities, though both approaches are supported:\n*   DDPM: Uses the full chain of T steps with stochastic sampling:\n$x_{t-1} = \\mu_\\theta(x_t, t) + \\sigma t \\epsilon, \\epsilon \\sim \\mathcal{N}(0, 1)$ (33)\n*   DDIM: Enables faster sampling through deterministic trajectories:\n$x_{t-1} = \\sqrt{\\alpha_{t-1}} (\\frac{x_t - \\sqrt{1-\\bar{\\alpha}_t} \\epsilon_\\theta(x_t, t)}{\\sqrt{\\alpha_t}}) + \\sqrt{1-\\alpha_{t-1}} \\epsilon_\\theta(x_t, t)$ (34)"}, {"title": "K. U-Net Architecture", "content": "The U-Net architecture serves as the backbone for noise prediction in our framework, combining multi-view processing with skip connections specifically designed for time series visual patterns. Our implementation modifies the standard U-Net structure to better handle temporal dependencies while maintaining spatial coherence."}, {"title": "a) Encoder-Decoder Structure", "content": "The architecture consists of multiple resolution levels:\n*   Downsampling path: Progressive feature compression\n$h_l = \\text{ResBlock"}, "text{Down}(h_{l-1})), l=1, ..., L$ (35)\n*   Upsampling path: Gradual feature reconstruction\n$h_l = \\text{ResBlock}(\\text{Up}(h_{"]}