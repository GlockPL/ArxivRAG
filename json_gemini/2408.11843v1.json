{"title": "Editable Fairness: Fine-Grained Bias Mitigation in Language Models", "authors": ["Ruizhe Chen", "Yichen Li", "Jianfei Yang", "Joey Tianyi Zhou", "Zuozhu Liu"], "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs. Code will be publicly available.", "sections": [{"title": "1 Introduction", "content": "Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks, such as language understanding and question answering [5, 16, 21]. However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice [54, 76], e.g., making stereotyped judgments on vulnerable groups [62]. Removing such biases can not only enhance the generalization ability and reliability of LLMs but also expedite their deployment while retaining substantial social significance, which garners increasing attention from researchers, practitioners, and the broader public [24, 36, 45].\nExisting approaches to mitigate biases in LLMs mainly fall into two categories: debiasing with fine-tuning or prompt-tuning [22, 23]. Fine-tuning methods include supplementary pre-training on a balanced corpus [78], aligning embeddings within the bias subspace [43, 58], or using a contrastive objective [10, 33]. Prompt-tuning techniques [18, 27, 42, 73] involve learning prompts that guide LLMs to ignore disparities between social groups for fair decision-making.\nHowever, these methods put emphasis on parity across different demographic groups, while generating unreasonable predictions on commonsense knowledge or are prone to exhibiting new biases regarding individual facts [22]. For instance, as shown in Figure 1, it is critical for debiased models to ensure fair correlations between social groups 'male/female' and descriptions \u2018skilled at engineering/perform surgery'. However, as for individual facts such as \u201cMy [mom/dad] gives birth to me.\u201d,\nit is not desirable if the model generates equitable predictions for social groups 'mom/dad', as the gender differences in these contexts are facts rather than stereotyped social biases [15, 22, 30, 38]. The problem could be two-fold. On one hand, existing debiasing approaches remove biases with group-invariant objectives [18, 33, 43], regarding different social groups as interchangeable. However, indiscriminately neutralizing different social groups degrades the perception of them, causing undesired or wrong behaviors when some individual statements hold distinct facts. On the other hand, existing datasets and benchmarks still lack sufficient evaluation regarding the effects of debiasing techniques. They mainly focus on evaluating the fairness on social biases, but ignore evaluating whether the debiased model retains commonsense knowledge or individual facts [22], for instance, reasonable disparities among different social groups.\nTo address these issues, we first establish a new debiasing benchmark BiaScope, consisting of new datasets and metrics to further evaluate the effects of debiasing techniques. Specifically, BiaScope is established in two parts. To evaluate the ability to retain individual facts, we construct a dataset of commonsense knowledge about different social groups that should not be neutralized (e.g., My mom gives birth to me.). As a complement, we generate a dataset of paraphrased social biases to evaluate the generalization ability. In correspondence, we design two metrics, Retention Score (RS) and Paraphrase Stereotype Score (PS). Moreover, we propose a novel method Fairness-Stamp (FAST), for editable bias mitigation. Instead of mitigating group biases indiscriminately, FAST operates fine-grained calibrations on individual biases, i.e., specific biased description toward a social group [12, 63]. In particular, it operates through a two-stage pipeline, as illustrated in Figure 1: identifying where biases are encoded in LLMs and subsequently calibrating them by inserting a trainable fair stamp, i.e., a lightweight modular network, into the identified location. Aiming at bias mitigation and knowledge retention, we propose a novel weighted loss function to train the fair stamp while leaving all other model parameters frozen.\nWe evaluate FAST with comprehensive experiments on StereoSet, Crows-Pairs, and our proposed BiaScope for systematic evaluation. Results show that FAST achieves superior debiasing performance without compromising model capability. Additional experiments showcase the scalability to larger models, the effectiveness on downstream tasks, and the effectiveness of knowledge locating, as well as analysis on fairness-utility trade-off and computational complexity. These underscore the immense potential of our fine-grained strategy in the realm of LLM debiasing. Our contributions are:\n\u2022 We identify that existing debiasing benchmarks lack sufficient evaluation regarding the effects of debiasing techniques, and introduce a new benchmark BiaScope to evaluate the ability to retain individual commonsense facts and generalize to other social biases.\n\u2022 We propose a novel bias mitigation framework FAST, which calibrates fine-grained individual biases, enabling nuanced model debiasing.\n\u2022 Comprehensive experiments and superior performance demonstrate the significant potential of our fine-grained debiasing strategy in achieving fairness for LLMs."}, {"title": "2 Related Works", "content": "Bias Mitigation in Large Language Models. Several approaches have been proposed for debiasing pre-trained language models, which can be grouped into two categories: (1) Fine-tuning. This branch includes additional pre-training on re-balanced corpora [70, 78] or with a contrastive objective [10, 33], projection-based methods [13, 36, 43, 58] in the embedding space, in-training methods [29, 33] and parameter-efficient fine-tuning [39, 72] methods. (2) Prompt-tuning. Prompt-tuning [18, 27, 42, 73] involve generating either discrete prompts or continuous prompts to mitigate social biases. There are also post-hoc approaches [7, 61] that are deployed after the training phase to achieve effective debiasing. However, existing techniques treat social groups as interchangeable [22] and neutralize different social groups in model inputs or outputs, while ignoring or concealing distinct facts of different social groups [30]. In contrast, our method mitigates biases based on fine-grained individual biases, avoiding compromising other knowledge.\nKnowledge Editing. Knowledge or Model Editing [11, 12, 63] has been proposed to facilitate data-efficient modifications to model behavior while ensuring no detrimental impact on performance across other inputs. These approaches manipulate the model's output for specific cases either by integrating external models with the original, unchanged model [17, 31, 34, 49, 50, 77], or by altering the model parameters responsible for undesirable output [28, 32, 46, 48]. The most relevant line of work is locate and edit [9, 11, 41, 46, 47], which suggests identifying neurons crucial to a model's factual predictions [8, 20, 65] and subsequently updating the feed-forward weights to edit the output. Inspired by these works, we propose the first fine-grained bias mitigation framework, which enables nuancedly calibrating individual biases with minimal cost."}, {"title": "3 BiaScope Benchmark", "content": "Existing debiasing benchmarks focus on evaluating the fairness regarding social biases, while ignore evaluating the retention of commonsense knowledge [22]. In this paper, we establish the BiaScope benchmark, which includes new datasets and metrics designed for a more comprehensive evaluation of the modifications made by debiasing approaches. First, we describe the process of constructing datasets in Section 3.1. Then, we describe the corresponding evaluating metrics in Section 3.2."}, {"title": "3.1 Dataset Construction", "content": "The main idea of dataset construction is two-fold. First, to measure the ability of knowledge retention, we propose to create a commonsense knowledge dataset. Second, to prevent excessive knowledge retention and to measure generalization ability, we propose to construct a paraphrased social bias dataset. The process of dataset construction is illustrated in Figure 2. To ensure the quality of the generated data, we propose to collect real-world social biases \u03a9s from existing datasets, which will serve as the basis for data generation. Social biases are gathered from three domains (gender, race, and religion) across six datasets, including StereoSet [51], Crows-Pairs [53], WEAT [6], WinoBias [75], Winogender [59], and BEC-Pro [2]. Each dataset comprises sentences or words demonstrating biases, with details provided in Appendix C.1.\nCreate commonsense knowledge dataset. To better distinguish the boundary between out-of-scope knowledge and in-scope biases, we propose creating commonsense knowledge about sensitive groups."}, {"title": "3.2 Evaluation Metrics", "content": "In this part, we introduce the corresponding evaluation metrics for the constructed datasets.\nRetention Score (RS) assesses the percentage of commonsense knowledge in \u03a9R retained after debiasing. The evaluation of RS is conducted according to the following criteria:\n\\(RS(G, G^*, \\Omega_R) = \\mathbb{E}_{k_R \\in \\Omega_R} \\mathbb{1} \\{G[k_R] = G^*[k_R] \\}\\), (1)\nwhere \\(k_R\\) represents commonsense knowledge in \\(\\Omega_R\\). \\(G[k_R]\\) and \\(G^*[k_R]\\) represent the prediction of the original and debiased model. \\(\\mathbb{1}\\) is the indicator function.\nParaphrase Stereotype Score (PS) evaluates the generalization ability on paraphrased biases in \\(\\Omega_p\\). As a complement to RS, it aims to prevent the model from over-retaining knowledge and thereby losing its generalization ability. It computes the percentage of data that a model gives a biased prediction as opposed to an unbiased prediction:\n\\(PS(G^*, \\Omega_p) = \\mathbb{E}_{k_p \\in \\Omega_p} \\mathbb{1} \\{P_{G^*}[k_p] > P_{G^-}[k_p] \\}\\), (2)\nwhere \\(P_{G^*}[k_p]\\) and \\(P_{G^-}[k_p]\\) denotes the probability of the biased prediction and unbiased prediction."}, {"title": "4 Method", "content": "Task Formulation. Given a pre-trained language model G and a set of social biases \u03a9 to be calibrated, the task is outputting an edited model G* where social biases in \u03a9 can be fairly predicted while other knowledge is retained as in the original G.\nOverview. This section proposes a fine-grained bias mitigation method Fairness-Stamp (FAST). The main idea of FAST is to identify the storage locations of social biases in LLMs and then modify the small number of identified parameters. Thus, it precisely repairs social biases while minimizing the impact on other knowledge. Concretely, FAST operates through a two-step process, as depicted in Figure 3: (1) determining the decisive layer responsible for storing social biases; (2) enveloping the decisive layer with an auxiliary fairness stamp (a 2-layer feed-forward network), which is optimized with the objective of bias mitigation and knowledge retention."}, {"title": "4.1 Step 1: Locate the Decisive Layer for Biases", "content": "Definition of Social Bias. In this step, we propose to investigate the storage of social biases in LLMs. Typically, a social bias consists of a certain social group and the biased description, which amplifies social inequalities [1, 4, 69]. For instance, in the statement Black people are more likely to commit a crime,\" the phrase are more likely to commit a crime\u201d is the biased description associated with the social group \"Black people.\" In light of this, we formalize the social bias as a knowledge triplet k = (s, r, o), where s is the subject (i.e., Black people), o is the object (i.e., commit a crime), and r is the relation between them (i.e., are more likely to), inspired by [35, 56].\nContrastive Social Biases Localization. To investigate how social bias (s1, r1, o1) is stored as association between the social group and biased description, we propose to use its counterfactual knowledge (s2, r2, o2) for contrast. This involves altering the social group or biased description (e.g., changing Black people to White people) to better probe these biased associations. Our contrastive bias localization is performed in three runs, with a complete illustration in Figure 5 in the Appendix B.2:\n(1) Biased run: We pass the biased prompt (s1, r1) into the model and collect all hidden states {\\(h^{(l)}\\) | l\u2208 [1, L]}, in a forward run towards biased prediction, where L is number of layers.\n(2) Counterfactual run: We pass the counterfactual prompt (s2, r2) to the model to modify the biased prediction. Hidden states will also change due to the alteration of the input subject.\n(3) Restore biased states: To measure the effect of certain layer \u00ce in the model to the biased prediction, we restore the biased states \\(h^{(l)}\\) of s1 and perform the forward run. Then we calculate the recovery degree of biased prediction, which indicates the effect of layer \u00ce.\nDetermine the decisive layer. Denote the prediction probability on the object of the biased run as P[o], and the probability of the counterfactual run as P*[o]. In this way, the total biased effect (TE) can be defined as: TE = P[o] \u2013 P*[o]. In the restoration run, the probability will recover from P*[o] to P[o] due to the restoration of certain biased states \\(h^{(l)}\\), which reflects the contribution of these states to the biased prediction. Denote the probability of restoring layer l as \\(P^*(h^{(l)})[o]\\). The indirect biased effect (i.e., recovery degree) IE of layer l can be calculated by IE = \\(P^*(h^{(l)})[o] \u2013 P^*[o]\\). The layer with the largest IE will be selected as the decisive layer."}, {"title": "4.2 Step 2: Insert Fairness Stamp at the Decisive Layer", "content": "Following Step 1, we propose to select the layer that contributes most significantly to the bias as the decisive layer. Assuming the input hidden states to be h, the decisive layer (i.e., feed-forward network, FFN) in the original LLM can be formulated as follows:\n\\(FFN(h) = Act(hKT)V,\\) (3)\nwhere K and V denote the parameters (i.e., keys and values matrices) of the first and second linear layers in the FFN, respectively. We propose to envelop the decisive layer with a fairness stamp. The fairness stamp is a 2-layer Feed-Forward Network (FFN) layer, which helps modify the output of the decisive layer with a few external parameters to achieve the goal of fairness. The output of the enveloped FFN layer is given by:\n\\(FFN'(h) = FFN(h) + Act(hK'^T)V',\\) (4)\nwhere K', V' \u2208 \\(\\mathbb{R}^{d_e \\times d}\\) are the parameters of the fairness stamp. Then, the stamp is optimized with the objectives of bias mitigation and knowledge retention, while other parameters are frozen.\nBias Mitigation. With a social bias k1 and its counterfactual knowledge k2, we propose to mitigate the gap between their probabilities of prediction on the objects:\n\\(\\mathcal{L}_e = \\frac{1}{\\Omega} \\sum_{(k_1, k_2) \\in \\Omega} |P_G[k_1] - P_G[k_2]|,\\) (5)\nwhere ki = (si, ri, oi) follows the definition in Section 4.1. PG[ki] = PG[oi|pi] = PG[oisi, ri] denotes the probability of predicting the object oi given the prompt pi.\nKnowledge Retention. We propose to retain knowledge in two ways. First, we retain the probability distribution for the input prompts pi to control the deviation from the original model. Second, we retain the probability distribution on the prompt p' that combines pre-defined template (e.g., \"{subject} is_\") and the input subject (e.g., Black people), which helps retain the perception of different social groups and prevent the model from degradation of knowledge. The two loss functions are as follows:\n\\(\\mathcal{L}_{s1} = \\frac{1}{\\Omega} \\sum_{p \\in \\Omega} D_{KL}(P_G[\\cdot|p], P_{G^*}[\\cdot|p]),\\) (6)\n\\(\\mathcal{L}_{s2} = \\frac{1}{\\Omega} \\sum_{s_i} D_{KL}(P_G[\\cdot|p'(s_i)], P_{G^*}[\\cdot|p'(s_i)]),\\)\nwhere PG[*|p'] is the predicted probability vector of all objects. G and G* represent the origin and debiased model. DKL(\u00b7, \u00b7) represents the Kullback-Leibler Divergence.\nTo prevent the model from overfitting to particular inputs, we utilize prefix texts xj to enhance generalization ability across various contexts. These prefix texts are randomly generated by the model, for instance, \u201cMy father told me that\", and are concatenated to the front of the prompts.\nThe overall objective can be formulated as follows with hyper-parameters \\(\\alpha\\) and \\(\\beta\\):\n\\(\\mathcal{L} = \\mathcal{L}_e + \\alpha \\mathcal{L}_{s1} + \\beta \\mathcal{L}_{s2}.\\) (7)"}, {"title": "5 Experiment", "content": "5.1 Experiment details\nModels. We employ the representative masked language model BERT (bert-base-uncased) [16] and generative language model GPT2 (GPT2-small) [57] as our backbones. Extended experiments are conducted on GPT2-XL, GPT-Neo-2.7b [3] and Llama-2-7b [64].\nBaselines. We consider the following debiasing techniques as baselines. The techniques can be grouped into two categories. (1) Fine-tuning: Counterfactual Data Augmentation (CDA) [78], Dropout [70], SentenceDebias [43] and Iterative Nullspace Projection (INLP) [58] pre-train, perform dropout on the rebalanced corpus, or remove sensitive attributes from the representations. MABEL [33] mitigates gender bias utilizing a contrastive learning objective on entailment labels. (2) Prompt-tuning: Auto-debias [27] proposes to directly probe the encoded biases through prompts, then mitigate biases via distribution alignment loss. (3) Post-hoc: Self-Debias [61] proposes to leverage internal knowledge to discourage it from generating biased text. FMD [7] proposes a machine unlearning-based strategy to efficiently remove bias. (4) Knowledge Editing: ROME [46] and MEMIT [47] are proposed to effectively and efficiently modify the knowledge in a language model. We adapt their knowledge updating objectives to align with our fairness objectives for comparison, while all other settings remain unchanged.\nDatasets and Evaluating Metrics. We evaluate the debiasing performance on StereoSet [52], Crows-Pairs [53], as well as our proposed BiaScope. Stereotype Score (SS), Language Modeling Score (LMS), and Ideal Context Association Test Score (ICAT) correspond to StereoSet to evaluate the extent of bias, the ability to predict reasonable association, and the combined score of the former two, with details in Appendix A.1. SS is also employed by Crows-Pairs to measure bias. As for BiaScope, we utilize RS and PS, as introduced in Section 3.2.\nImplementation details. Bias mitigation is conducted over the collected biased knowledge in Section 4.1. We utilize two-layer fully connected neural networks with the ReLU activation function as the fairness stamp, with a hidden dimension of 1024. Additional details are in Appendix C.1."}, {"title": "5.2 Debiasing Performance", "content": "Existing debiasing methods cannot retain individual commonsense knowledge. The debiasing results are delineated in Table 1 and Table 2. It is observed that all debiasing baselines fail to yield satisfactory results in knowledge retention (i.e., RS), which proves our claim that group-invariant methods compromise the individual knowledge to distinguish between different social groups.\nOur approach surpasses existing debiasing and knowledge-editing baselines in both bias mitigation and knowledge retention. As shown in Table 1 and Table 2, our proposed FAST is the first to achieve near-perfect bias mitigation (i.e., SS lower than 52 for BERT) on the two evaluating datasets, while SS of existing approaches, in terms of gender, are still higher than 56. Further, FAST can also largely retain a high RS, and achieve the highest LMS and ICAT. This demonstrates the effectiveness of our fine-grained calibration strategy towards eliminating social biases in LLMs. In addition, we report the performance of knowledge-editing approaches ROME and MEMIT. It can be discerned that neither ROME nor MEMIT significantly improves SS over vanilla BERT. Overall, comparing results demonstrate the effectiveness of our fine-grained calibration strategy towards eliminating social biases in LLMs. Supplemented debiasing results and qualitative studies are in Appendix C.\nOur approach scales to larger models. In order to further validate the scalability of FAST, we conduct additional experiments on larger models, i.e., GPT2-XL, GPT-Neo-2.7B, and Llama-2-7B, with results reported in Table 3. After debiasing, FAST induces a significant reduction (9.4 in average) in SS, and a great improvement in ICAT. Meanwhile, FAST can also retain the Retention Score for larger language models. These demonstrate the consistent effectiveness and scalability of FAST."}, {"title": "6 Analysis and Discussion", "content": "Language Modeling Capability Analysis. In this section, we evaluate our debiased models against the General Language Understanding Evaluation (GLUE) benchmark [67] to assess the retention of general linguistic understanding after bias mitigation. As shown in Table 4, FAST achieves better downstream performance than 5 out of 6 baselines on average, indicating that FAST retains language modeling capabilities while mitigating biases.\nEffectiveness Analysis of Bias Locating. The locating results of BERT are shown in Figure 4(a). They reveal that the final layer demonstrates a significantly higher average indirect effect than the other layers, marking it as the decisive layer for bias prediction. To validate the effectiveness of knowledge locating (i.e., Section 4.1), we perform calibration (i.e., step 2) on every layer of BERT, with results shown in Figure 4(b). It is observable that layer 11 achieves optimal performance in terms of SS, RS, and LMS, corroborating the effectiveness of knowledge locating. Layers 1-5 show minimal alleviation of biases (no decline in SS), suggesting a minimal correlation between these layers with the storage of biased knowledge. Notably, layers 6-10 not only result in a reduction in SS but also a significant decrease in RS, indicating the entanglement of biased knowledge with other knowledge. Locating results of GPT2, GPT2-XL, GPT-Neo-2.7B, and Llama-2-7B are reported inAppendix C.2. We also verify the robustness of knowledge locating in Appendix D.1. Effectiveness validation of fairness stamp (i.e., Section 4.2) is provided in Appendix D.2.\nSensitivity Analysis on Hyper-parameters and Fairness-Utility Trade-off Analysis. We have performed a grid search for hyperparameters a and \u03b2, with results presented in Table 5. The optimization proves robust within specific ranges (i.e., 20-80 for a, 0.05-0.5 for \u03b2). However, a trade-off between the bias mitigation and knowledge retention is observed [37, 44]. When either a or \u1e9e is set to 0, both the knowledge retention score (RS) and language modeling ability (LMS) suffer significant declines. Conversely, when either a or \u1e9e is set too high, the fairness performance (SS) is negatively affected. Based on these findings, we choose a at 40 and \u1e9e at 0.1 as they yield the most favorable overall results.\nAblation Study on the Number of External Parameters. In this section, we verify the robustness of FAST under limited memory sizes. We alter the dimension of hidden states (dim) in our FAST, thereby changing the number of external parameters. The results are shown in Figure 4(c). The best results are obtained with dim set to 1024. As dim decreases, both SS and RS decline slightly, indicating that a larger number of parameters yields better bias mitigation performance. Further increases in dim do not yield better debiasing results. Therefore, dim is set to 1024. Ablation study on the batch size is provided in Appendix D.6."}, {"title": "Computational Complexity Analysis.", "content": "In Table 6, we report the number of parameters and operation time of our proposed FAST on the largest and smallest models in our experiments. The time is counted on a single RTX 3090 with one piece of bias. It can be observed that FAST only requires about one percent of parameters and bias mitigation can be finished in less than one second or just a few seconds, indicating that FAST allows for timely debiasing of LLMs.\nEffectiveness on the Knowledge-editing Task. We conduct experiments on the knowledge-editing task of Zero-Shot Relation Extraction (zsRE) [40]. We employ GPT-J-6B [68] as backbone, and use baseline methods including the improved Constrained Fine-Tuning (FT+W) [46], MEND [48], ROME [46], and MEMIT [47]. We select layers 3 through 8 for editing, consistent with MEMIT. The training and evaluating datasets are also consistent with MEMIT. As shown in Table 7, our method outperforms most baselines and achieves comparable performance to MEMIT. These findings suggest that our method can be effectively applied to knowledge-editing tasks.\nAblation Study on the Losses. We investigate the effect of our proposed losses, with results presented in Table 8. With only \\(\\mathcal{L}_e\\), SS can be largely improved. However, RS and LMS decrease significantly, indicating that the internal knowledge is negatively affected. After \\(\\mathcal{L}_{s1}\\) included, RS and LMS can be retained, which is aligned with our aim of knowledge retention. \\(\\mathcal{L}_{s2}\\) further enhances RS, demonstrating its effectiveness in retaining the commonsense knowledge about different social groups."}, {"title": "7 Conclusion and Limitation", "content": "In this paper, we explore the fine-grained bias mitigation paradigm, which focuses on individual social biases rather than group differences. The exploration has been developed from two aspects. We first establish a novel debiasing benchmark BiaScope to finely measure the effects of debiasing, which not only evaluates the fairness on the social biases, but also the retention of individual commonsense knowledge. Furthermore, we introduce the first editable bias mitigation framework FAST, which is capable of locating and mitigating individual social biases precisely. Experiments have validated that existing group-invariant debiasing techniques severely affect the retention of commonsense knowledge, while demonstrating the superiority of FAST in both bias mitigation and knowledge maintenance. Extensive experiments across various models and datasets further demonstrate its scalability, robustness, and lightweight. Our findings offer significant implications for future debiasing research.\nWe acknowledge the presence of certain limitations. First, in this paper, we construct our new datasets leveraging GPT-4. Although human validation is performed to ensure the reliability of the data, GPT-4 may suffer from the limitations of its internal knowledge, potentially introducing blind spots into our benchmark. Second, the memory mechanism of LLMs is still under exploration, while we assume that FFN layers are responsible for storing biased knowledge based on previous observations [8, 25, 26, 46]. Third, debiasing larger models, as shown in Table 3, is more challenging and will guide our future research, which constitutes our future direction. Besides, social bias in open language generation or dialogue represents another critical scenario for mitigating bias [66], which constitutes one of our future research endeavors."}, {"title": "F Broader Impact", "content": "With the widespread application of large language models (LLMs), the emphasis on fairness has significantly increased, requiring LLMs to treat individuals from different backgrounds fairly. However, LLMs trained on large datasets inevitably exhibit certain biases during the pre-training phase. In this paper, we propose a promising solution that mitigates unfairness in LLMs while not compromising capability, which is of great significance for deploying fair and reliable language models."}, {"title": "A BiaScope Benchmark Construction", "content": "A.1 Metrics\nStereotype Score (SS) is the most straightforward measure for the bias within the debiased model [51, 53]. It computes the percentage of knowledge for which a model assigns the biased object as opposed to the unbiased object. The evaluation of SS is conducted according to the following criteria:\n\\(SS(G^*, \\Omega_S) = \\mathbb{E}_{(k_1, k_2) \\in \\Omega_S} \\mathbb{1} \\{P_{G^*}[k_1] > P_{G^*}[k_2] \\},\\) (8)\nwhere G* is the debiased model.\nLanguage Modeling Score (LMS), employed in StereoSet [51], has been utilized. Based on the knowledge pairs in \\(\\Omega_S\\), we select an irrelevant \\(o_{ir}\\) to form \\(k_{ir} = (s, r, o_{ir})\\). LMS represents the percentage that a model that prefers a relevant association (either the stereotypical association or the anti-stereotypical association) as opposed to an irrelevant association. The evaluation of LMS is conducted according to the following criteria:\n\\(LMS(G, \\Omega_S) = \\mathbb{E}_{(k_1, k_2) \\in \\Omega_S} \\mathbb{1} \\{P_G[k_1] > P_G[k_{ir}] \\}\\) (9)\n+\\mathbb{1} \\{P_G[k_2] > P_G[k_{ir}] \\}. (10)\nIdeal Context Association Test Score (ICAT) is proposed by [52] combine both LMS and SS by ICAT = LMS * min(SS, 100 \u2013 SS)/50. It represents the language modeling ability of a model while behaving in an unbiased manner."}, {"title": "A.2 Dataset Construction Details", "content": "To ensure the quality and diversity of the generated data, meticulous human validation was performed. We employed four human evaluators, all of whom possess good English proficiency, to annotate and verify the feasibility of the datasets generated by GPT-4.\nParaphrased dataset. For each knowledge pair within \\(\\Omega_s\\), we paraphrase the prompts combining (s, r) with the same semantic expression. We hired 2 undergraduate students, all with good English proficiency. We first asked the students to manually paraphrase ten pieces of biased knowledge into semantically similar ones. Then, the manually paraphrased samples were combined with the prompt as context for GPT-4 generation. After generation, we performed sample checks on 10% of the data for each dataset. In these samples, the agreement on successful generation reached 100%.\nRetention dataset. We construct \\(\\Omega_R\\) by collecting commonsense facts related to the sensitive attributes, such as \"Jesus' resurrection is commemorated by [Christians] when they celebrate Easter.\" We initially created alternative facts by prompting the GPT-4 API. We then asked the students to manually validate every generated fact, ensuring that each fact in the retention dataset constitutes reasonable commonsense knowledge rather than bias."}, {"title": "A.3 Diversity and Challenge Analysis of BiaScope", "content": "Diversity of the Benchmark. Our retention dataset collect data that contrasts existing debiasing evaluation datasets by incorporating both stereotypical and natural gender differences. Traditional stereotype data (e.g., \"In common sense, the mom brings up the child.\") often reflects the stereotypes of gender roles in human society. In contrast, our retention dataset includes examples like \"In common sense, the mom gives birth to the child.\" that emphasize biological gender distinctions. This approach expands the scope of debiasing evaluation to include both gender biases that need to be addressed and natural gender differences that should be acknowledged. Furthermore, regarding the diversity of the retention dataset, various perspectives of commonsense differentiating knowledge are taken into account during the generation process. In Table 9 and Table 10, we showcase data cases generated by GPT-4 from different perspectives, highlighting the dataset's diversity. For the paraphrased dataset, our primary goal is to generate data that retain the original sentence's meaning while avoiding the introduction of new biases. Consequently, the diversity of the paraphrased dataset is dependent on the diversity of the original biased data. To achieve greater diversity than existing benchmarks, we create paraphrases from biased expressions in various formats from six distinct sources, as illustrated in Table 12."}, {"title": "B Method", "content": "B.1 Preliminaries\nConsidering a transformer-based Large Language Model (take a decoder-based transformer as an example). The model takes into the input sequence (x1,...Xt-1) and predicts the probability of the next token (xt). The updates in a transformer block can be formulated as follows:\n\\(h_i^{(l)} = h_i^{(l-1)} + a_i^{(l)} + m_i^{(l)},\\) (11)\nwhere \\(a^{(l)}_i = Att(h_i^{(l-1)}, h_1^{(l-1)}, ..., h_n^{(l-1)}),\\) (12)\n\\(m_i^{(l)} = W_2 \u03c3(W_1 \u03b3(a_i^{(l)}) + h_i^{(l-1)}),\\) (13)\nwhere \\(m_i^{(l)}\\) and \\(a_i^{(l)}\\) represents the output of the lth feed-forward network layer and self-attention layer."}, {"title": "B.2 Locate Biased Knowledge", "content": "In this section", "Appendix": "nBiased run: We pass the prompt (s1", "T": "l\u2208 [1", "L": ""}, "where T is number of tokens and L is number of layers.\nCounterfactual input: We replace the subject with s2 and pass the new prompt (s2, r) to the model to corrupt the biased prediction. Hidden states corresponding to the subject token(s) i will be updated with \\(h^{(0)}_i (s_1 \\rightarrow s_2)\\).\nRestoration run: Towards certain layer \u00ce in the model, we hook the biased states \\(h^{(l)}_i\\) at subject token(s) i and perform the counterfactual run. Then we calculate the recovery degree of"]}