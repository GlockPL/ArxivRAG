{"title": "Editable Fairness: Fine-Grained Bias Mitigation in Language Models", "authors": ["Ruizhe Chen", "Yichen Li", "Jianfei Yang", "Joey Tianyi Zhou", "Zuozhu Liu"], "abstract": "Generating fair and accurate predictions plays a pivotal role in deploying large language models (LLMs) in the real world. However, existing debiasing methods inevitably generate unfair or incorrect predictions as they are designed and evaluated to achieve parity across different social groups but leave aside individual commonsense facts, resulting in modified knowledge that elicits unreasonable or undesired predictions. In this paper, we first establish a new bias mitigation benchmark, BiaScope, which systematically assesses performance by leveraging newly constructed datasets and metrics on knowledge retention and generalization. Then, we propose a novel debiasing approach, Fairness Stamp (FAST), which enables fine-grained calibration of individual social biases. FAST identifies the decisive layer responsible for storing social biases and then calibrates its outputs by integrating a small modular network, considering both bias mitigation and knowledge-preserving demands. Comprehensive experiments demonstrate that FAST surpasses state-of-the-art baselines with superior debiasing performance while not compromising the overall model capability for knowledge retention and downstream predictions. This highlights the potential of fine-grained debiasing strategies to achieve fairness in LLMs. Code will be publicly available.", "sections": [{"title": "1 Introduction", "content": "Pre-trained Large Language Models (LLMs) have demonstrated exceptional performance on many tasks, such as language understanding and question answering [5, 16, 21]. However, the encoded social stereotypes and human-like biases inevitably cause undesired behaviors when deploying LLMs in practice [54, 76], e.g., making stereotyped judgments on vulnerable groups [62]. Removing such biases can not only enhance the generalization ability and reliability of LLMs but also expedite their deployment while retaining substantial social significance, which garners increasing attention from researchers, practitioners, and the broader public [24, 36, 45].\nExisting approaches to mitigate biases in LLMs mainly fall into two categories: debiasing with fine-tuning or prompt-tuning [22, 23]. Fine-tuning methods include supplementary pre-training on a balanced corpus [78], aligning embeddings within the bias subspace [43, 58], or using a contrastive objective [10, 33]. Prompt-tuning techniques [18, 27, 42, 73] involve learning prompts that guide LLMs to ignore disparities between social groups for fair decision-making.\nHowever, these methods put emphasis on parity across different demographic groups, while generating unreasonable predictions on commonsense knowledge or are prone to exhibiting new biases regarding individual facts [22]. For instance, as shown in Figure 1, it is critical for debiased models to ensure fair correlations between social groups 'male/female' and descriptions \u2018skilled at engineering/perform surgery'. However, as for individual facts such as \u201cMy [mom/dad] gives birth to me.\", it is not desirable if the model generates equitable predictions for social groups 'mom/dad', as the gender differences in these contexts are facts rather than stereotyped social biases [15, 22, 30, 38]. The problem could be two-fold. On one hand, existing debiasing approaches remove biases with group-invariant objectives [18, 33, 43], regarding different social groups as interchangeable. However, indiscriminately neutralizing different social groups degrades the perception of them, causing undesired or wrong behaviors when some individual statements hold distinct facts. On the other hand, existing datasets and benchmarks still lack sufficient evaluation regarding the effects of debiasing techniques. They mainly focus on evaluating the fairness on social biases, but ignore evaluating whether the debiased model retains commonsense knowledge or individual facts [22], for instance, reasonable disparities among different social groups.\nTo address these issues, we first establish a new debiasing benchmark BiaScope, consisting of new datasets and metrics to further evaluate the effects of debiasing techniques. Specifically, BiaScope is established in two parts. To evaluate the ability to retain individual facts, we construct a dataset of commonsense knowledge about different social groups that should not be neutralized (e.g., My mom gives birth to me.). As a complement, we generate a dataset of paraphrased social biases to evaluate the generalization ability. In correspondence, we design two metrics, Retention Score (RS) and Paraphrase Stereotype Score (PS). Moreover, we propose a novel method Fairness-Stamp (FAST), for editable bias mitigation. Instead of mitigating group biases indiscriminately, FAST operates fine-grained calibrations on individual biases, i.e., specific biased description toward a social group [12, 63]. In particular, it operates through a two-stage pipeline, as illustrated in Figure 1: identifying where biases are encoded in LLMs and subsequently calibrating them by inserting a trainable fair stamp, i.e., a lightweight modular network, into the identified location. Aiming at bias mitigation and knowledge retention, we propose a novel weighted loss function to train the fair stamp while leaving all other model parameters frozen.\nWe evaluate FAST with comprehensive experiments on StereoSet, Crows-Pairs, and our proposed BiaScope for systematic evaluation. Results show that FAST achieves superior debiasing performance without compromising model capability. Additional experiments showcase the scalability to larger models, the effectiveness on downstream tasks, and the effectiveness of knowledge locating, as well as analysis on fairness-utility trade-off and computational complexity. These underscore the immense potential of our fine-grained strategy in the realm of LLM debiasing. Our contributions are:\n\u2022 We identify that existing debiasing benchmarks lack sufficient evaluation regarding the effects of debiasing techniques, and introduce a new benchmark BiaScope to evaluate the ability to retain individual commonsense facts and generalize to other social biases.\n\u2022 We propose a novel bias mitigation framework FAST, which calibrates fine-grained individual biases, enabling nuanced model debiasing.\n\u2022 Comprehensive experiments and superior performance demonstrate the significant potential of our fine-grained debiasing strategy in achieving fairness for LLMs.\""}, {"title": "2 Related Works", "content": "Bias Mitigation in Large Language Models. Several approaches have been proposed for debiasing pre-trained language models, which can be grouped into two categories: (1) Fine-tuning. This branch includes additional pre-training on re-balanced corpora [70, 78] or with a contrastive objective [10, 33], projection-based methods [13, 36, 43, 58] in the embedding space, in-training methods [29, 33] and parameter-efficient fine-tuning [39, 72] methods. (2) Prompt-tuning. Prompt-tuning [18, 27, 42, 73] involve generating either discrete prompts or continuous prompts to mitigate social biases. There are also post-hoc approaches [7, 61] that are deployed after the training phase to achieve effective debiasing. However, existing techniques treat social groups as interchangeable [22] and neutralize different social groups in model inputs or outputs, while ignoring or concealing distinct facts of different social groups [30]. In contrast, our method mitigates biases based on fine-grained individual biases, avoiding compromising other knowledge.\nKnowledge Editing. Knowledge or Model Editing [11, 12, 63] has been proposed to facilitate data-efficient modifications to model behavior while ensuring no detrimental impact on performance across other inputs. These approaches manipulate the model's output for specific cases either by integrating external models with the original, unchanged model [17, 31, 34, 49, 50, 77], or by altering the model parameters responsible for undesirable output [28, 32, 46, 48]. The most relevant line of work is locate and edit [9, 11, 41, 46, 47], which suggests identifying neurons crucial to a model's factual predictions [8, 20, 65] and subsequently updating the feed-forward weights to edit the output. Inspired by these works, we propose the first fine-grained bias mitigation framework, which enables nuancedly calibrating individual biases with minimal cost."}, {"title": "3 BiaScope Benchmark", "content": "Existing debiasing benchmarks focus on evaluating the fairness regarding social biases, while ignore evaluating the retention of commonsense knowledge [22]. In this paper, we establish the BiaScope benchmark, which includes new datasets and metrics designed for a more comprehensive evaluation of the modifications made by debiasing approaches. First, we describe the process of constructing datasets in Section 3.1. Then, we describe the corresponding evaluating metrics in Section 3.2.\n3.1 Dataset Construction\nThe main idea of dataset construction is two-fold. First, to measure the ability of knowledge retention, we propose to create a commonsense knowledge dataset. Second, to prevent excessive knowledge retention and to measure generalization ability, we propose to construct a paraphrased social bias dataset. The process of dataset construction is illustrated in Figure 2. To ensure the quality of the generated data, we propose to collect real-world social biases \u03a9\u03c2 from existing datasets, which will serve as the basis for data generation. Social biases are gathered from three domains (gender, race, and religion) across six datasets, including StereoSet [51], Crows-Pairs [53], WEAT [6], WinoBias [75], Winogender [59], and BEC-Pro [2]. Each dataset comprises sentences or words demonstrating biases, with details provided in Appendix C.1.\nCreate commonsense knowledge dataset. To better distinguish the boundary between out-of-scope knowledge and in-scope biases, we propose creating commonsense knowledge about sensitive groups."}, {"title": "3.2 Evaluation Metrics", "content": "In this part, we introduce the corresponding evaluation metrics for the constructed datasets.\nRetention Score (RS) assesses the percentage of commonsense knowledge in \u03a9R retained after debiasing. The evaluation of RS is conducted according to the following criteria:\n$RS(G, G^*, \\Omega_R) = \\mathbb{E}_{k_R \\in \\Omega_R} 1\\{G[k_R] = G^*[k_R]\\},$ (1)\nwhere $k_R$ represents commonsense knowledge in $\u03a9_R$. $G[k_R]$ and $G^*[k_R]$ represent the prediction of the original and debiased model. 1 is the indicator function.\nParaphrase Stereotype Score (PS) evaluates the generalization ability on paraphrased biases in \u03a9p. As a complement to RS, it aims to prevent the model from over-retaining knowledge and thereby losing its generalization ability. It computes the percentage of data that a model gives a biased prediction as opposed to an unbiased prediction:\n$PS(G^*, \\Omega_P) = \\mathbb{E}_{k_p \\in \\Omega_P} 1\\{P_{G^*}[k_p] > P_{G^-}[k_p]\\},$ (2)\nwhere $P_{G^*} [k_p]$ and $P_{G^-} [k_p]$ denotes the probability of the biased prediction and unbiased prediction."}, {"title": "4 Method", "content": "Task Formulation. Given a pre-trained language model G and a set of social biases \u03a9 to be calibrated, the task is outputting an edited model G* where social biases in \u03a9 can be fairly predicted while other knowledge is retained as in the original G.\nOverview. This section proposes a fine-grained bias mitigation method Fairness-Stamp (FAST). The main idea of FAST is to identify the storage locations of social biases in LLMs and then modify the small number of identified parameters. Thus, it precisely repairs social biases while minimizing the impact on other knowledge. Concretely, FAST operates through a two-step process, as depicted in Figure 3: (1) determining the decisive layer responsible for storing social biases; (2) enveloping the decisive layer with an auxiliary fairness stamp (a 2-layer feed-forward network), which is optimized with the objective of bias mitigation and knowledge retention."}, {"title": "4.1 Step 1: Locate the Decisive Layer for Biases", "content": "Definition of Social Bias. In this step, we propose to investigate the storage of social biases in LLMs. Typically, a social bias consists of a certain social group and the biased description, which amplifies social inequalities [1, 4, 69]. For instance, in the statement Black people are more likely to commit a crime,\" the phrase are more likely to commit a crime\u201d is the biased description associated with the social group \"Black people.\" In light of this, we formalize the social bias as a knowledge triplet k = (s, r, o), where s is the subject (i.e., Black people), o is the object (i.e., commit a crime), and r is the relation between them (i.e., are more likely to), inspired by [35, 56].\nContrastive Social Biases Localization. To investigate how social bias $(s_1, r_1, o_1)$ is stored as association between the social group and biased description, we propose to use its counterfactual knowledge $(s_2, r_2, o_2)$ for contrast. This involves altering the social group or biased description (e.g., changing Black people to White people) to better probe these biased associations. Our contrastive bias localization is performed in three runs, with a complete illustration in Figure 5 in the Appendix B.2:\n(1) Biased run: We pass the biased prompt $(s_1, r_1)$ into the model and collect all hidden states $\\{h^{(l)} | l \\in [1, L]\\}$, in a forward run towards biased prediction, where L is number of layers.\n(2) Counterfactual run: We pass the counterfactual prompt $(s_2, r_2)$ to the model to modify the biased prediction. Hidden states will also change due to the alteration of the input subject.\n(3) Restore biased states: To measure the effect of certain layer $l$ in the model to the biased prediction, we restore the biased states $h^{(l)}$ of $s_1$ and perform the forward run. Then we calculate the recovery degree of biased prediction, which indicates the effect of layer $l$.\nDetermine the decisive layer. Denote the prediction probability on the object of the biased run as $P[o]$, and the probability of the counterfactual run as $P^*[o]$. In this way, the total biased effect (TE) can be defined as: $TE = P[o] \u2013 P^* [o]$. In the restoration run, the probability will recover from $P^* [o]$ to $P[o]$ due to the restoration of certain biased states $h^{(l)}$, which reflects the contribution of these states to the biased prediction. Denote the probability of restoring layer l as $P^*(h^{(l)})[o]$. The indirect biased effect (i.e., recovery degree) IE of layer l can be calculated by $IE = P^*(h^{(l)})[o] \u2013 P^*[o]$. The layer with the largest IE will be selected as the decisive layer."}, {"title": "4.2 Step 2: Insert Fairness Stamp at the Decisive Layer", "content": "Following Step 1, we propose to select the layer that contributes most significantly to the bias as the decisive layer. Assuming the input hidden states to be h, the decisive layer (i.e., feed-forward network, FFN) in the original LLM can be formulated as follows:\n$FFN(h) = Act(hKT)V,$ (3)\nwhere K and V denote the parameters (i.e., keys and values matrices) of the first and second linear layers in the FFN, respectively. We propose to envelop the decisive layer with a fairness stamp. The fairness stamp is a 2-layer Feed-Forward Network (FFN) layer, which helps modify the output of the decisive layer with a few external parameters to achieve the goal of fairness. The output of the enveloped FFN layer is given by:\n$FFN'(h) = FFN(h) + Act(hK'^\\prime)V'^\\prime,$ (4)\nwhere K', V' \u2208 $R^{d_e\u00d7d}$ are the parameters of the fairness stamp. Then, the stamp is optimized with the objectives of bias mitigation and knowledge retention, while other parameters are frozen.\nBias Mitigation. With a social bias k\u2081 and its counterfactual knowledge k2, we propose to mitigate the gap between their probabilities of prediction on the objects:\n$\\mathcal{L}_e = \\frac{1}{|\\Omega|} \\sum_{(k_1, k_2) \\in \\Omega} |P_G[k_1] - P_G[k_2]|,$ (5)\nwhere ki = (si, ri, oi) follows the definition in Section 4.1. $P_G[k_i] = P_G[o_i|p_i] = P_G[o_i|s_i, r_i]$ denotes the probability of predicting the object or given the prompt pi.\nKnowledge Retention. We propose to retain knowledge in two ways. First, we retain the probability distribution for the input prompts pi to control the deviation from the original model. Second, we retain the probability distribution on the prompt p' that combines pre-defined template (e.g., ", "is_": "and the input subject (e.g., Black people), which helps retain the perception of different social groups and prevent the model from degradation of knowledge. The two loss functions are as follows:\n$\\mathcal{L}_{s1} = \\frac{1}{|\\Omega|} \\sum_{p \\in \\Omega} D_{KL}(P_G[p], P_{G^*}[\\cdot|p]), \\quad \\mathcal{L}_{s2} = \\frac{1}{|S|} \\sum_{s_i} D_{KL}(P_G[p'(s_i)], P_{G^*}[\\cdot|p' (s_i)]),$ (6)\nwhere $P_G[\\cdot|p']$ is the predicted probability vector of all objects. G and G* represent the origin and debiased model. $D_{KL}(\\cdot, \\cdot)$ represents the Kullback-Leibler Divergence.\nTo prevent the model from overfitting to particular inputs, we utilize prefix texts xj to enhance generalization ability across various contexts. These prefix texts are randomly generated by the model, for instance, \u201cMy father told me that", "\u03b2": "n$\\mathcal{L} = \\mathcal{L}_e + \u03b1\\mathcal{L}_{s1} + \u03b2\\mathcal{L}_{s2}.$ (7)"}, {"title": "5 Experiment", "content": "5.1 Experiment details\nModels. We employ the representative masked language model BERT (bert-base-uncased) [16] and generative language model GPT2 (GPT2-small) [57] as our backbones. Extended experiments are conducted on GPT2-XL, GPT-Neo-2.7b [3] and Llama-2-7b [64].\nBaselines. We consider the following debiasing techniques as baselines. The techniques can be grouped into two categories. (1) Fine-tuning: Counterfactual Data Augmentation (CDA) [78], Dropout [70], SentenceDebias [43] and Iterative Nullspace Projection (INLP) [58] pre-train, perform dropout on the rebalanced corpus, or remove sensitive attributes from the representations. MABEL [33] mitigates gender bias utilizing a contrastive learning objective on entailment labels. (2) Prompt-tuning: Auto-debias [27] proposes to directly probe the encoded biases through prompts, then mitigate biases via distribution alignment loss. (3) Post-hoc: Self-Debias [61] proposes to leverage internal knowledge to discourage it from generating biased text. FMD [7] proposes a machine unlearning-based strategy to efficiently remove bias. (4) Knowledge Editing: ROME [46] and MEMIT [47] are proposed to effectively and efficiently modify the knowledge in a language model. We adapt their knowledge updating objectives to align with our fairness objectives for comparison, while all other settings remain unchanged.\nDatasets and Evaluating Metrics. We evaluate the debiasing performance on StereoSet [52], Crows-Pairs [53], as well as our proposed BiaScope. Stereotype Score (SS), Language Modeling Score (LMS), and Ideal Context Association Test Score (ICAT) correspond to StereoSet to evaluate the extent of bias, the ability to predict reasonable association, and the combined score of the former two, with details in Appendix A.1. SS is also employed by Crows-Pairs to measure bias. As for BiaScope, we utilize RS and PS, as introduced in Section 3.2.\nImplementation details. Bias mitigation is conducted over the collected biased knowledge in Section 4.1. We utilize two-layer fully connected neural networks with the ReLU activation function as the fairness stamp, with a hidden dimension of 1024. Additional details are in Appendix C.1.\n5.2 Debiasing Performance\nExisting debiasing methods cannot retain individual commonsense knowledge. The debiasing results are delineated in Table 1 and Table 2. It is observed that all debiasing baselines fail to yield satisfactory results in knowledge retention (i.e., RS), which proves our claim that group-invariant methods compromise the individual knowledge to distinguish between different social groups.\nOur approach surpasses existing debiasing and knowledge-editing baselines in both bias mitigation and knowledge retention. As shown in Table 1 and Table 2, our proposed FAST is the first to achieve near-perfect bias mitigation (i.e., SS lower than 52 for BERT) on the two evaluating datasets, while SS of existing approaches, in terms of gender, are still higher than 56. Further, FAST can also largely retain a high RS, and achieve the highest LMS and ICAT. This demonstrates the effectiveness of our fine-grained calibration strategy towards eliminating social biases in LLMs. In addition, we report the performance of knowledge-editing approaches ROME and MEMIT. It can be discerned that neither ROME nor MEMIT significantly improves SS over vanilla BERT. Overall, comparing results demonstrate the effectiveness of our fine-grained calibration strategy towards eliminating social biases in LLMs. Supplemented debiasing results and qualitative studies are in Appendix C."}, {"title": "6 Analysis and Discussion", "content": "Language Modeling Capability Analysis. In this section, we evaluate our debiased models against the General Language Understanding Evaluation (GLUE) benchmark [67] to assess the retention of general linguistic understanding after bias mitigation. As shown in Table 4, FAST achieves better downstream performance than 5 out of 6 baselines on average, indicating that FAST retains language modeling capabilities while mitigating biases.\nEffectiveness Analysis of Bias Locating. The locating results of BERT are shown in Figure 4(a). They reveal that the final layer demonstrates a significantly higher average indirect effect than the other layers, marking it as the decisive layer for bias prediction. To validate the effectiveness of knowledge locating (i.e., Section 4.1), we perform calibration (i.e., step 2) on every layer of BERT, with results shown in Figure 4(b). It is observable that layer 11 achieves optimal performance in terms of SS, RS, and LMS, corroborating the effectiveness of knowledge locating. Layers 1-5 show minimal alleviation of biases (no decline in SS), suggesting a minimal correlation between these layers with the storage of biased knowledge. Notably, layers 6-10 not only result in a reduction in SS but also a significant decrease in RS, indicating the entanglement of biased knowledge with other knowledge. Locating results of GPT2, GPT2-XL, GPT-Neo-2.7B, and Llama-2-7B are reported inSensitivity Analysis on Hyper-parameters and Fairness-Utility Trade-off Analysis. We have performed a grid search for hyper- parameters a and \u03b2, with results presented in Table 5. The optimization proves robust within specific ranges (i.e., 20-80 for a, 0.05-0.5 for \u03b2). However, a trade-off between the bias mitigation and knowledge retention is observed [37, 44]. When either a or \u03b2 is set to 0, both the knowledge retention score (RS) and language modeling ability (LMS) suffer significant declines. Conversely, when either a or \u03b2 is set too high, the fairness performance (SS) is negatively affected. Based on these findings, we choose a at 40 and \u03b2 at 0.1 as they yield the most favorable overall results.\nAblation Study on the Number of External Parameters. In this section, we verify the robustness of FAST under limited memory sizes. We alter the dimension of hidden states (dim) in our FAST, thereby changing the number of external parameters. The results are shown in Figure 4(c). The best results are obtained with dim set to 1024. As dim decreases, both SS and RS decline slightly, indicating that a larger number of parameters yields better bias mitigation performance. Further increases in dim do not yield better debiasing results. Therefore, dim is set to 1024. Ablation study on the batch size is provided in Appendix D.6."}, {"title": "7 Conclusion and Limitation", "content": "In this paper, we explore the fine-grained bias mitigation paradigm, which focuses on individual social biases rather than group differences. The exploration has been developed from two aspects. We first establish a novel debiasing benchmark BiaScope to finely measure the effects of debiasing, which not only evaluates the fairness on the social biases, but also the retention of individual commonsense knowledge. Furthermore, we introduce the first editable bias mitigation framework FAST, which is capable of locating and mitigating individual social biases precisely. Experiments have validated that existing group-invariant debiasing techniques severely affect the retention of commonsense knowledge, while demonstrating the superiority of FAST in both bias mitigation and knowledge maintenance. Extensive experiments across various models and datasets further demonstrate its scalability, robustness, and lightweight. Our findings offer significant implications for future debiasing research.\nWe acknowledge the presence of certain limitations. First, in this paper, we construct our new datasets leveraging GPT-4. Although human validation is performed to ensure the reliability of the data, GPT-4 may suffer from the limitations of its internal knowledge, potentially introducing blind spots into our benchmark. Second, the memory mechanism of LLMs is still under exploration, while we assume that FFN layers are responsible for storing biased knowledge based on previous observations [8, 25, 26, 46]. Third, debiasing larger models, as shown in Table 3, is more challenging and will guide our future research, which constitutes our future direction. Besides, social bias in open language generation or dialogue represents another critical scenario for mitigating bias [66], which constitutes one of our future research endeavors."}]}