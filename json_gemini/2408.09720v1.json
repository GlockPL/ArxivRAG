{"title": "Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework", "authors": ["Jiandong Jin", "Xiao Wang", "Qian Zhu", "Haiyang Wang", "Chenglong Li"], "abstract": "Pedestrian Attribute Recognition (PAR) is one of the indispensable tasks in human-centered research. However, existing datasets neglect different domains (e.g., environments, times, populations, and data sources), only conducting simple random splits, and the performance of these datasets has already approached saturation. In the past five years, no large-scale dataset has been opened to the public. To address this issue, this paper proposes a new large-scale, cross-domain pedestrian attribute recognition dataset to fill the data gap, termed MSP60K. It consists of 60,122 images and 57 attribute annotations across eight scenarios. Synthetic degradation is also conducted to further narrow the gap between the dataset and real-world challenging scenarios. To establish a more rigorous benchmark, we evaluate 17 representative PAR models under both random and cross-domain split protocols on our dataset. Additionally, we propose an innovative Large Language Model (LLM) augmented PAR framework, named LLM-PAR. This framework processes pedestrian images through a Vision Transformer (ViT) backbone to extract features and introduces a multi-embedding query Transformer to learn partial-aware features for attribute classification. Significantly, we enhance this framework with LLM for ensemble learning and visual feature augmentation. Comprehensive experiments across multiple PAR benchmark datasets have thoroughly validated the efficacy of our proposed framework. The dataset and source code accompanying this paper will be made publicly available at https://github.com/\nEvent-AHU/OpenPAR.", "sections": [{"title": "1. Introduction", "content": "Pedestrian Attribute Recognition (PAR) [36] has been widely exploited in the Computer Vision (CV) and Artificial Intelligence (AI) community. It aims to map the given pedestrian image into semantic labels, such as gender, hairstyle, and wearings, using deep neural networks and achieves high performance on current benchmark datasets. These models can be employed in practical scenarios and may work well in simple scenarios. It can also help other human-centric tasks, e.g., pedestrian detection and tracking [23], person re-identification [24] and retrieval [9]. However, the performance of the current PAR model is still significantly affected by challenging factors (e.g., low illumination, motion blur, and complex backgrounds); moreover, there is still much room for exploration in the relationship between pedestrian image perception and multi-label attributes.\nConsidering these issues, we meticulously review the existing works and datasets on PAR and find that the development in the PAR field has begun to enter a bottleneck period. As an effective driving force for promoting the development of PAR, benchmark datasets play a crucial role. However, we believe that the PAR community needs to address several core issues on the benchmark datasets as follows: 1). The performance of existing pedestrian attribute recognition datasets is close to saturation, and the performance improvement of new algorithms has shown a trend of weakening. However, only one small-scale PAR-related dataset has been released in the past five years, thus, there is an urgent need for new large-scale datasets to support new research endeavors. 2). Existing PAR datasets use random partitioning for model training and testing, which can measure the overall recognition capability of a PAR model. However, this partitioning mechanism overlooks the impact of cross-domain (e.g., different environments, times, populations, and data sources) on the PAR model. 3). Existing PAR datasets do not prominently reflect challenge factors, thus, this may potentially result in neglecting the impact of data corruption during real-world application, thereby introducing safety hazards in practical settings. In conclusion, it is evident that the PAR community urgently requires a new large-scale dataset to bridge the existing data gap.\nIn this paper, we propose a new benchmark dataset"}, {"title": "2. Related Works", "content": "2.1. Pedestrian Attribute Recognition\nPedestrian attribute recognition [36] aims to classify pedestrian images based on a predefined set of attributes. Current methods can be broadly categorized into prior-guidance, attention-based, and visual-language modeling approaches. Given the strong correlation between pedestrian attributes and specific body components. Various methods, such as HPNet [26] and DAHAR [26, 43], focused on localizing attribute-relevant regions via attention mechanisms. Variations in posture and viewpoint often challenge pedestrian attribute recognition. To address these challenges, some researchers [10, 30] incorporated prior enhancement techniques or introduced supplementary neural networks to model these relationships effectively. Furthermore, pedestrian attributes are closely interconnected. Consequently, JLAC [32] and PromptPAR [37] jointly model attribute context and image-attribute relationships. While current methods recognize the importance of exploring contextual relationships in the PAR task, leveraging models like Transformers to capture attribute relationships within datasets often struggles to represent connections involving rare attributes.\n2.2. Benchmark Datasets for PAR\nThe most commonly used datasets of PAR are PETA [3], WIDER [22], RAP [17, 18], and PA100K [26]. To enhance the ability to recognize pedestrian attributes at a long distance, Deng et al. [3] introduced a new pedestrian attribute dataset named PETA, compiled from 10 small-scale pedestrian re-identification datasets, labeling over 60 attributes. Unlike PETA's identity-level annotation, the RAP dataset captures an indoor shopping mall and employs instance-level annotation for the pedestrian images. Both the PETA and RAPv1 datasets suffer from the issue of random segmentation, where individuals present in the training set also appear in the test set, resulting in information leakage. To solve this issue, Liu et al. [26] proposed the largest pedestrian attribute recognition dataset in surveillance scenarios, PA100K, which contains 100,000 pedestrian images and 26 attributes. This dataset mitigates the information leakage problem by ensuring no overlap between pedestrians in the training and test sets. However, these datasets only contain\nsimple scenes with limited background variation and lack significant style changes among pedestrians.\n2.3. Vision-Language Models\nWith the rapid development of the natural language processing field, many large language models (LLMs) such as Flan-T5 [27], and LLaMA [34] have emerged. Although notable foundational models like SAM [15] have been introduced in the vision domain, the complexity of visual tasks has hindered the development of generalized multi-domain visual models. Some researchers have begun to view LLMs as world models, leveraging them as the cognitive core to enhance various multi-modal tasks. Recognizing the high cost of training a large multi-modal model from scratch, BLIP series [19, 20], MiniGPT-4 [50], bridge existing pre-trained visual models and large language models. Although these models have significant improvements in the vision understanding and text generation field, there are many challenges, such as low-resolution image recognition, fine-grained image cation, and the hallucination of LLMs."}, {"title": "3. MSP60K Benchmark Dataset", "content": "3.1. Protocols\nTo provide a robust platform for training and evaluating pedestrian attribute recognition (PAR) in real-world conditions, we adhere to these guidelines while constructing the MSP60K benchmark dataset: 1). Large Scale: We annotate 60,122 pedestrian images, each with 57 attributes, comprehensively analyzing pedestrian characteristics in various conditions. 2). Multiple Distances and Viewpoints: Images are captured from different angles and distances using various cameras and handheld devices, covering the front, back, and side views. The resolution of pedestrian images in our dataset is from 30\u00d780 to 2005\u00d73008. 3). Complex and Varied Scenes: Unlike existing datasets with uniform backgrounds, our dataset includes images from eight different environments with diverse backgrounds and attribute distributions, helping evaluate recognition methods in varied settings. 4). Rich Source of Pedestrian Identity: We gather data on pedestrians from different scenarios, nationalities, and seasonal variations, enhancing the dataset with diverse styles and characteristics. 5). Simulated Complex Real-world Environments: The dataset includes variations in lighting, motion blur, occlusions, and adverse weather conditions, simulating real-world challenges in pedestrian attribute recognition.\n3.2. Attribute Groups and Details\nTo effectively evaluate the performance of existing PAR methods in complex scenarios, each image in our dataset is labeled with 57 attributes, which are categorized into"}, {"title": "3.3. Statistical Analysis", "content": "As shown in Table 1, MSP60K offers 8 distinct scenes and 57 attributes, providing richer annotations than datasets like PA100K (26 attributes) and WIDER (14 attributes). The dataset comprises 60,122 images of over 5,000 unique individuals. It includes varied environments such as markets, schools, kitchens, ski resorts, various outdoor and construction sites, offering a broader scope than other datasets. In our benchmark dataset, we split the data using the random and cross-domain partitioning strategies:\n\u2022 Random Partitioning: 30,298 images for training, 6,002 for validation, and 23,822 for testing, ensuring a random distribution of scenes like other PAR benchmark datasets.\n\u2022 Cross-domain Partitioning: To validate domain generalization and zero-shot performance of PAR models, we divide our dataset based on scenarios, i.e., five scenarios (Construction Site, Market, Kitchens, School, Ski Resort) with 34,128 images are used for training, while three scenarios (Outdoors1, Outdoors2, Outdoors3) with 24,994 images are used for testing.\nTo assess the robustness of the model, we intentionally degrade 1/3 of the images in each subset by introducing variations such as changes in lighting, random occlusions, blurring, and noise. With its extensive size and diverse conditions, MSP60K offers a comprehensive platform for evaluating PAR methods.\nThe dataset also exhibits a long-tail effect, similar to existing PAR datasets, depicted in Fig. 2 (a), and reflects real-world attribute distributions. Fig. 2 (b) presents the co-occurrence matrix of pedestrian attributes, where each cell represents the frequency of two attributes appearing together. Darker areas indicate higher co-occurrence frequency. For example, Cotton-padded coat and Long Sleeves have a strong association, while attributes like Bald and Long Hair/Black Hair rarely co-occur. Fig. 2 (c) displays the distribution of attributes across different scenarios, such as Construction Sites, Markets, Kitchens, and others, with attributes represented by different colors in a concentric circle plot. For instance, the School scenario has a higher number of Child attributes, while the Outdoors3 scenario shows a greater prevalence of Short Sleeves and Sandals attributes."}, {"title": "3.4. Benchmark Baselines", "content": "Our evaluation covers a variety of methods (17 total), including: 1). CNN-based: DeepMAR [16], RethinkPAR [11], SSCNet [10], SSPNet [31]. 2). Transformer-based: DFDT [46], PARFormer [5]. 3). Mamba-based: MambaPAR [39], MaHDFT [38]. 4). Human-Centric Pre-Training Models for PAR: PLIP [51], HAP [45]. 5). Visual-Language Models for PAR: VTB [2], Label2Label [21], PromptPAR [37], SequencePAR [13]."}, {"title": "4. Methodology", "content": "In this section, we introduce our proposed framework for pedestrian attribute recognition LLM-PAR. Our approach consists of three main parts: visual feature extraction, image caption generation, and the classification module. We first explain each of these three components. After that, we outline the training and inference process of our method."}, {"title": "4.1. Overview", "content": "This paper introduces a method for improving pedestrian attribute recognition (LLM-PAR) using multi-modal large language models (MLLMs) which describe the image in detail. As shown in Fig. 5, we leverage MLLMs to explore the contextual relationships between attributes, generating descriptions that assist attribute recognition. The approach consists of three main modules: 1) a multi-label classification branch, 2) a large language model branch, and 3) model aggregation. Specifically, we first extract the visual features of pedestrians using a visual encoder. Then, we design MEQ-Former to extract specific features for different attribute groups and translate to the latent space of MLLMs, improving the ability of MLLMs to capture fine details of pedestrians. The attribute group features are integrated into instruction embedding via a projection layer, the features feed into the large language model to generate pedestrian captions. Finally, the classification results from the visual features of each group are aggregated with the results from the language branch to produce the final classification results. The following sections will provide a detailed introduction to these modules."}, {"title": "4.2. Multi-Label Classification Branch", "content": "Given an input pedestrian image $I \\in \\mathbb{R}^{H\\times W\\times 3}$, as shown in Fig. 5, we first partition it into patches and project them into visual tokens. The visual tokens are added with Position Embedding (P.E.) which encodes the spatial information. The output will be fed into a visual encoder (EVA-ViT-G [6] is adopted for default) to extract the global visual representation $F_v$. In our implementation, we freeze the parameters of the pre-trained visual encoder and adopt LoRA [8] to achieve efficient tuning. Then, a newly designed Multi-"}, {"title": "4.3. Large Language Model Branch", "content": "Although this multi-label classification framework can achieve decent accuracy, it still fails to consider the logical reasoning of large language models, which is evident in the image-text domain. Therefore, this paper attempts to use LLM as an auxiliary branch to enhance pedestrian attribute recognition. As shown in Fig. 5, we first build the instructions based on each attribute group $A^i$, i.e.,\nHuman: Analyze the person's photo, and categorize it into attributes.\n<Img><ImageHere_Head></Img>"}, {"title": "4.4. Model Aggregation for PAR", "content": "After being equipped with the LLM, our algorithmic framework can simultaneously output pedestrian attribute results and complete text passages to describe the attributes of a given pedestrian. To leverage the strengths of these two branches, we have designed an algorithm integration module to achieve enhanced prediction results. As shown in Fig. 5, we define two visual classifiers for attribute recognition, i.e., the attribute-level and instance-level classifiers. We also get the classifier for recognition using tokens from the large language model branch.\nIn our implementation, we exploit the following three strategies to fuse these three results as ours. Specifically, 1). Attributes-Specific Aggregation (ASA): we adaptively weight and sum the attribute predictions of each classifier based on the weights learned from the training subset. 2). Mean Pooling: We directly take the average of the results from these three branches as the final model output. 3). Max Pooling: We take the maximum value of the logits from the three prediction branches as the final prediction result. Note that, we adopt the Mean Pooling strategy as the default setting in our experiments if not otherwise specified. More detailed results can be found in the sub-section 5.5 in our experiments."}, {"title": "4.5. Loss Function", "content": "In the training phase, we adopt the widely used weighted cross-entropy loss (WCE Loss) $L_{wce}(\\cdot)$ [16] for attribute prediction branches, i.e.,\n$L_{MLC} = L_{wce}(\\hat{y}, P_{attr}) + L_{wce}(\\hat{y}, P_{in})$\n(2)\nWe also adopt cross-entropy loss $L_{ce}(\\cdot)$ for the captioning generation in the LLM branch.\n$L_{LLM} = L_{wce}(\\hat{y}, P_{lim}) + L_{ce}(\\hat{y}_{cap}, P_{cap})$\n(3)\nwhere $\\hat{y}$ and $\\hat{y}_{cap}$ denote the ground-truth labels and corresponding pedestrian attribute description, respectively. The $P_{cap}$ is the logits generated by the Large Language Model Head. More in detail, the $L_{ce}(\\cdot)$ and $L_{wce}(\\cdot)$ can be formulated as:\n$L_{ce}() = \\frac{1}{M}\\sum_{i=1}^{M} CE(y_i, p_i)$\n(4)\n$L_{wce} = \\frac{1}{M}\\sum_{i=1}^{M} w_i CE(y_i, p_i)$\n(5)\nwhere M is the number of attributes, $w_i$ is used to adjust the contribution for unbalanced categories, inversely related to the number of category positive samples. The CE term can be represented as:\n$CE(y_i, p_i) = y_i \\log(p_i) + (1 - y_i) \\log(1-p_i)$\n(6)"}, {"title": "5. Experiments", "content": "5.1. Datasets and Evaluation Metric\nIn this study, we conduct a comprehensive benchmark of 17 pedestrian attribute recognition methods, representing the most important models in the field of pedestrian attribute recognition. Furthermore, the performance of our methods is compared with existing state-of-the-art (SOTA) PAR methods in our benchmark and in three publicly available datasets: PETA [3], PA100K [26] and RAPv1 [17]. Five widely used evaluation metrics are employed for evaluating the performance, including: mean Accuracy (mA), Accuracy (Acc), Precision (Prec), Recall and F1-score (F1). More details about these evaluation metrics can be found in our supplementary materials.\n5.2. Implementation Details\nIn the training phase, we use ground truth to expand the attributes as appropriate sentences by the template, creating the <instruction, answer> set to fine-tune the LLM. Additionally, we utilize the ground-truth sentences mask strategy to prevent information leakage during the training stage, which helps in effectively learning the LLM classification head. For inference, we auto-regressive generate the sentence from the instruction with the image feature, and use the last step hidden state that predicts the result of the language branch.\nWe utilize EVA-ViT-G [6] as the visual backbone, and its last three layers are used to initial the AGFA module. The Q-Former adopts BERT [14] with several cross-attention layers added to interact with visual features. and we default utilize Vicuna-7B [47] as the large language model. All backbones are initialized according to the MiniGPT-4 [50] settings and weights. We adopt LoRA [8] to fine-tune the visual backbone and the last 3 layers of LLMs. The LoRA is only injected in the projection of Q and V in the attention layer, with the low-rank dimension r set as 32. We train"}, {"title": "5.3. Comparison on Public PAR Benchmarks", "content": "\u2022 Result on MSP60K Dataset. We collect and analyze public PAR methods from 2015 to 2024 on the MSP60K dataset as shown in Table 3, methods like HAP [45], RethinkingPAR [11], and PARformer [5], which perform well in the random split but experience significant drops in performance in the cross-domain split. For instance, mA, Acc, and F1 of HAP scores drop by 18.22, 25.53, and 19.20, respectively. Some methods show smaller declines in the cross-domain split, with PromptPAR [37] achieving state-of-the-art results, though still with notable decreases. We also test MiniGPT-4 [50] in a zero-shot setup on our dataset, with significant drops observed in the cross-domain split. After optimizations, LLM-PAR achieves 80.13, 78.71, 84.39, 90.52, and 86.94 in the random split, and 66.29, 58.11, 65.28, 81.21, and 72.05 in the cross-domain split, which achieves the best results on nearly all metrics. The experiments on the MSP60K dataset fully validate the effectiveness of our proposed LLM-PAR for attribute recognition.\n\u2022 Result on PETA [3] Dataset. As shown in Table 4, our method significantly outperforms previous methods. Compared to the previous best method SSPNet [31] with prior guidance, we observe improvements of 3.52, 1.79, and 0.89 in mA, Acc, and F1, respectively. This illustrates the effectiveness of MLLMs without fine-tuned design in PAR. In contrast to PromptPAR with visual-language modeling by Transformer [35] and CLIP [29], we also improve in 3.49, 1.75, and 1.21.\n\u2022 Result on PA100K [26] Dataset. As shown in Table 4, our method also achieves optimal results on larger datasets, exceeding 1.65 and 2.36 on the mA and F1 metrics, respectively, compared to recent methods such as FRDL [49], without employing any resampling strategy. Compared to Transformer-based methods like PARformer [37], our method shows a significant advantage, with results of 91.09, 84.12, and 90.41 on mA, Accuracy, and F1, respectively. Additionally, the progress is substantial compared to zero-shot MiniGPT [50].\n\u2022 Result on RAPv1 [17] Dataset Our framework obtains the SOTA performance compared with existing methods. Compared with the SOTA method OAGCN [28] with using additional information of viewpoint, our method gets 87.80, 71.86, 78.36, 88.20, and 82.64, while the OAGCN gets 87.83, 69.32, 78.32, 87.29, and 82.56, and exceeds 4.40, 1.86, and 1.44 contrast to the SOFA [42].\nBased on the experiments conducted on the four datasets, it is clear that LLM-PAR delivers impressive results by combining visual classification and LLM modeling within the LLM-augment framework. Furthermore, the AGFA"}, {"title": "5.4. Component Analysis", "content": "We conduct ablation experiments to analyze the contributions of different components in our method, including the visual backbone, AGFA module, LLM branch, and CLSIN module. The visual backbone analysis reveals that the EVA-CLIP [6] and Q-Former [20] alone achieve mA, Acc, and F1 scores of 71.54, 58.24, and 71.96, respectively. Fine-tuning with LoRA [8] improves these scores to 90.14, 83.25, and 89.38. The LLM branch alone achieves scores of 90.89, 83.64, and 89.60, which further improve to 92.20, 83.76, and 89.70 when combined with the AGFA module, demonstrating the effectiveness of LLMs in enhancing attribute recognition and their complementarity with the visual branch. The efficacy of the AGFA module is confirmed with scores of 92.20, 83.76, and 89.70, highlighting its role in improving feature aggregation and model recognition capabilities. Lastly, the CLS-IN module improves the mA, Acc, and F1 scores by 0.22, 0.12, and 0.13, respectively, indicating its contribution to enhancing the recognition of tail categories and supplementing other categories through shared feature learning."}, {"title": "5.5. Ablation Study", "content": "In this section, we conduct detailed analysis experiments on the main module of LLM-PAR. This includes Ground-Truth Mask Strategies, the Number of AGFA Layers, the Length of PartQ, the Aggregation Strategy of Three Branches, and Different MLLMs in the PETA [3] dataset.\n\u2022 Analysis on the Ground-Truth Mask Strategies. During the training phase, we observe that using ground truth directly for fine-tuning the language model leads to poor generalization due to information leakage. To improve this, we introduce a ground truth masking strategy. We compare various masking approaches to using ground truth directly (see Table 6). Direct use of ground truth results in poor performance in the language branch during testing. Random masking of sentence is also ineffective, with high masking rates hindering meaningful sentence generation. The best results are obtained with a 50% masking rate, improving mA and F1 scores by 0.80 and 3.10, respectively. Replacing ground truth with random sentences from the training set yielded the best performance. This strategy likely increases training difficulty, encouraging the model to utilize attribute context and visual information for better error correction.\n\u2022 Analysis on the Number of AGFA Layers. As shown in Table 7, we introduce the AGFA module for extracting pedestrian attribute group features in this study. We analyze the impact of AGFA modules with 1, 3, 6, 9, and 12 layers on recognition performance. Our analysis reveals that increasing the number of AGFA layers improved recognition performance. However, considering computational efficiency, we opt for a 3-layer AGFA module to balance computational burden and performance.\n\u2022 Analysis on the Length of PartQ. As shown in Table 7, we examine the effect of the number of attribute group queries in the AGFA module on performance. Our findings show that using 128 queries obtains the best performance, with performance deteriorating with more than 256 queries and a significant decline observed with 64 queries.\n\u2022 Analysis on the Aggregation Strategy of Threes Branches. To improve the aggregation of results from three branches, we design and evaluate some aggregation"}, {"title": "5.6. Visualization", "content": "\u2022 Recognition Results. In Fig. 7, we present the findings and descriptions of LLM-PAR. Our baseline model, MiniGPT-4 [50], can broadly describe pedestrians, including gender and accessories. Still, it can cause severe hallucinations, such as the first image: standing in front of a counter with a sign that reads \"Cash Only\" on it is not in the picture and the wrong prediction of gender in the last image: The person is male. Conversely, our LLM-PAR is capable of accurately recognizing specific attributes of pedestrians.\n\u2022 Feature Map. As shown in Fig. 8, we display the feature"}, {"title": "6. Conclusion", "content": "This paper addresses the limitations of existing pedestrian attribute recognition (PAR) datasets by introducing MSP60K, a new large-scale, cross-domain dataset with 60,122 images and 57 attribute annotations across eight scenarios. By incorporating synthetic degradation, we further bridge the gap between the dataset and real-world challenging conditions. Our comprehensive evaluation of 17 representative PAR models under both random and cross-domain split protocols establishes a more rigorous benchmark. Moreover, we propose the LLM-PAR framework, which leverages a pre-trained vision Transformer backbone, a multi-embedding query Transformer for partial-aware feature learning, and is enhanced by a Large Language Model for ensemble learning and visual feature augmentation. The experimental results across multiple PAR benchmark datasets demonstrate the effectiveness of our proposed framework. Both the MSP60K dataset and the source code will be released to the public upon acceptance, contributing to future advancements in human-centered research and PAR technology.\nIn our future work, we plan to further expand the scale of the dataset to conduct more extensive and thorough experimental validations. Moreover, the training and inference of the model still require substantial computational resources. In the future, we will design lightweight models to achieve a better balance between accuracy and performance."}]}