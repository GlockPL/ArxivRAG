{"title": "Pirates of the RAG: Adaptively Attacking LLMs to Leak Knowledge Bases", "authors": ["Christian Di Maio", "Cristian Cosci", "Valentina Poggioni", "Marco Maggini", "Stefano Melacci"], "abstract": "The growing ubiquity of Retrieval-Augmented Generation (RAG) systems in several real-world services triggers severe concerns about their security. A RAG system improves the generative capabilities of a Large Language Models (LLM) by a retrieval mechanism which operates on a private knowledge base, whose unintended exposure could lead to severe consequences, including breaches of private and sensitive information. This paper presents a black-box attack to force a RAG system to leak its private knowledge base which, differently from existing approaches, is adaptive and automatic. A relevance-based mechanism and an attacker-side open-source LLM favor the generation of effective queries to leak most of the (hidden) knowledge base. Extensive experimentation proves the quality of the proposed algorithm in different RAG pipelines and domains, comparing to very recent related approaches, which turn out to be either not fully black-box, not adaptive, or not based on open-source models. The findings from our study remark the urgent need for more robust privacy safeguards in the design and deployment of RAG systems.", "sections": [{"title": "1 Introduction", "content": "Retrieval-Augmented Generation (RAG) (Lewis et al., 2020; Guu et al., 2020) allows Large Language Models (LLMs) to be able to output more accurate, grounded, up-to-date information, without relying on cumbersome retrainings or fine-tuning procedures. RAG can be applied whenever an LLM is paired with an external knowledge base, which collects precious and sometimes private information for the task at hand. Information retrieval technologies are used to get pieces of knowledge which are highly correlated to the current input, and then used to augment and improve the quality of the generated language. In-Context Learning (ICL) (Brown, 2020) offers a simple and effective way to provide the retrieved knowledge to the LLM, by augmenting the input prompt (Ram et al., 2023). While the format and content of the knowledge base can differ between different applications, it often encompasses sensitive information that must be kept confidential to ensure privacy and security. For instance, RAG systems can be deployed as customer support assistants (Bhat et al., 2024), used by employees within an organization to streamline workflows (RoyChowdhury et al., 2024), or integrated into medical support chatbots (Park, 2024; Wang et al., 2024; Raja et al., 2024), where previous medical records help in the initial screening of new cases. The large ubiquity of RAG systems raises significant and often overlooked concerns about privacy and data security (Zhou et al., 2024). In particular, very recent works (Zeng et al., 2024; Qi et al., 2024; Cohen et al., 2024) highlighted that RAG systems turn out to be vulnerable to specific prompt augmentations, that can \u201cconvince\" the LLM to return (portions of) its input context (to a certain extent), containing the retrieved pieces of private knowledge.\nWe further dive into this direction, showing that it is indeed possible to attack RAG systems by means of an automated routine, powered by an easily accessible open-source LLM and sentence encoder. We propose a relevance-based procedure to promote the exploration of the (hidden) private knowledge base, in order to discourage leaking information that is always about the same sub-portion of the private knowledge base. The goal of our attack routine is to maximize the estimated coverage of the private knowledge base, thus aiming at extracting all the information out of it. In summary, this paper includes the following contributions:\n(i) It raises awareness of privacy risks in RAG systems by demonstrating a how their vulner-"}, {"title": "2 Background", "content": "The huge attention gained by LLMs both in the industry and in the academy, due to their outstanding capability of supporting convincing linguistic interactions with humans (Li et al., 2022; Kamalloo et al., 2023; Zhu et al., 2023; Jiang et al., 2024b), is paired with the growing need of adapting them to knowledge which was not available at training time. For example, in real-world LLM-based scenarios, such as in virtual assistants (Cutbill et al., 2024; Garc\u00eda-M\u00e9ndez et al., 2024; Kasneci et al., 2023), the knowledge base or tasks to be performed may change over time, and the model has to be adapted through one/multiple fine-tuning processes (De Lange et al., 2021; Zhang et al., 2023; Bang et al., 2023), possibly involving a portion/an additional portion of the model (Hu et al., 2022a), and that might lead to forgetting previously acquired knowledge (Lin et al., 2023). Alternatively, the model parameters can be kept forzen, and new knowledge can be provided by means of ICL (Brown, 2020; Wei et al., 2022; Dong et al., 2022; Yu et al., 2023; Li, 2023), appending information to the prompt input (context), which is also at the basis of RAG systems."}, {"title": "2.1 Retrieval-Augmented Generation", "content": "In the context of this work, we consider a collection of \u201cdocuments\", {D1, ..., Dm}, where each Di is an unstructured piece of textual information. Given a pre-trained LLM, we describe a RAG system by an architectures composed of four principal components (Ram et al., 2023). (i) a text embedder, function e, that maps a given text into a high-dimensional embedding space, such as $R^{d_{emb}}$; (ii) a storage that memorizes texts and embedded texts (more generally speaking, a vector store); (iii) a similarity function, e.g., cosine similarity, used to evaluate the similarity of a pair of embedded text vectors; (iv) a generative model, function f, usually an LLM, that produces output text based on input prompts and retrieved information. With a small abuse of notation, we will use function names also to refer to the names of the modeled components.\nBuilding a RAG system involves a first stage in which documents {D1,..., Dm} are partitioned into smaller pieces of text (sentences, paragraph, etc.), referred to as \"chunks\". We indicate with $m_i = |D_i|$ the total number of chunks in document $D_i$.\nA private knowledge base K is created, collecting all the prepared chunks, K = {$x_z$, z = 1,..., $\\Sigma_{i=1}^{m} |D_i|$}. The vector store gets populated with vector representations of the chunks in K, i.e., $\\mathbb{K}$ = {$x_z$ = e($x_z$), z = 1,...,|K|, $x_z \\in K$}.\nThen, a RAG system can be used to interact with the user. Given an input prompt q, the most similar chunks in K are retrieved, usually working in the embedding space. The embedding of q, computed by e(q), is referred to as $\\tilde{q}$, and the similarity score between $\\tilde{q}$ and the vectors $x_z$'s in $\\mathbb{K}$ is computed to identify the top-k most similar chunks to the prompt. This yields the set of retrieved chunks $\\mathcal{X}^{(q)} \\subset K$, with $|\\mathcal{X}^{(q)}| = k$. The language model, function f, generates output text y conditioned on both the input prompt q and the text of the chunks in $\\mathcal{X}^{(q)}$. Formally, we can factorize the prompt-conditioned generation as\n$p(y \\mid q, f) = \\sum_{\\mathcal{X}^{(q)}}p(y \\mid q, \\mathcal{X}^{(q)}, f)p(\\mathcal{X}^{(q)} \\mid q),$ (1)\nwhere $p(\\mathcal{X}^{(q)} \\mid q)$ represents the probability of retrieving a certain $\\mathcal{X}^{(q)}$ given the prompt q. Of course, calculating $p(\\mathcal{X}^{(q)} \\mid q)$ for all possible subsets of V is impractical, thus, as anticipated, $\\mathcal{X}^{(q)}$ is implemented by selecting the top-k most relevant chunks from V based on the similarity measurement, which is the only one with non-zero probability. This clears the summation and leave us with the prompt-and-retrieved-set conditioned generation,\n$\\displaystyle p(y \\mid q, \\mathcal{X}^{(q)}, f) = \\prod_{z=1}^{S} P(y_z \\mid y_{<z}, q, \\mathcal{X}^{(q)}, f).$ (2)\nThe notation y = ($y_1, y_2,...,y_s$) represents the generated sequence of tokens, $y_{<z}$ denotes the sequence of tokens generated up to time step z, and $P(y_z \\mid y_{<z}, q, \\mathcal{X}^{(q)}, f)$ represents the probability of generating the token $y_z$ by the LLM in the RAG system, given the previously generated tokens, the prompt, and the retrieved chunks.\""}, {"title": "2.2 Privacy Concerns in LLMs and RAGS", "content": "The deployment of AI models in privacy-sensitive applications (Hu and Min, 2023; Golda et al., 2024; Tram\u00e8r et al., 2022) has raised the attention of researches in how to protect sensitive information within the AI system. In the case of LLMs, it might happen that they are trained on public datasets, which can inadvertently include sensitive information (Wu et al., 2024; Yao et al., 2024), and the model can inadvertently retain and expose fragments of their training data (Wang et al., 2023; Carlini et al., 2021; Shin et al., 2020). This issue has been exploited to craft specific privacy-oriented attacks (Carlini et al., 2022; Hu et al., 2022b; Shokri et al., 2017). The introduction of RAG systems added yet another layer of complexity to these privacy concerns (Zhou et al., 2024). In fact, the private knowledge base of the RAG model often collect proprietary data and sensitive information, a portion of which is then fed to the LLM to reply to the user query. The LLM could possibly expose these private data in its output, if the user query is manipulated (Zeng et al., 2024; Qi et al., 2024; Cohen et al., 2024; Jiang et al., 2024a; Zhou et al., 2024), i.e., opening to the possibility of crafting RAG-specific attacks."}, {"title": "3 Pirates of the RAG", "content": "There exist several studies in the context of security of machine learning-based services with respect to different types of attacks and threat models (Cin\u00e0 et al., 2023; Grosse et al., 2023). In this paper, we focus on black-box attacks (Wiyatno et al., 2019) to RAG systems, which are the most challenging ones, since the adversary lacks insight of the internal structure of the model and can only interact with it by submitting an input and observing the corresponding output. Moreover, we consider the case of untargeted attacks, which seek to extract information from the model without prioritizing any particular type of data (Zeng et al., 2024), even if our model could be extended to deal with the targeted case (beyond the scope of this paper).\nOverview. Drawing an analogy to a raid of pirates on the high seas, trying to steal a hidden treasure, the goal of our attack is systematically discover the private/hidden K and \u201csteal\u201d it, i.e., replicate it in the attacker machine as faithfully as possible, yielding K*. This is done by \u201cconvincing\" the LLM of the RAG system f to expose chunks in its response y (with y = f(q)), through carefully designed queries q's. The attack is adaptive, since it is grounded on a relevance-based mechanisms that dynamically keeps track of those keywords/categories/topics that are correlated to what has been stolen so far, referred to as \u201canchors\u201d, to which the RAG system turn out to be more vulnerable (high-relevance). Anchors represent topics that are likely to be covered by chunks in the hidden K. The attacker relies on open-source tools which can be easily found on the web to prepare the attack queries q's: an off-the-shelf LLM f* to prepare the attack queries, even a relatively \"small\" one considering nowadays standards, and a text encoder e* to create embeddings and compare chunks/anchors in a vector space. Notice that (i) f* and e* are not intended to be somehow similar to f or e, which are fully unknown due to the black-box nature of the attack; moreover (ii) our attack emphasizes the choices of models that can be easily run on a home computer (or even a smartphone, in principle). In summary, the attacker uses f*, e*, the knowledge stolen so far $K^{*}$, and an adaptive relevance-based mechanism to craft novel queries that aim at maximizing the exposure of K. An overview of our attack is shown in Figure 1."}, {"title": "3.1 Pirate Algorithm", "content": "Preliminaries. The attack algorithm keeps submitting queries to the RAG system until a criterion on a relevance-based procedure is met (described in the following). Let t be the iteration index, that we will use as an additional subscript to all the previously introduced to notation. A set of anchors $A_t = \\{a_{t,1},..., a_{t,|A_t|}\\}$ is progressively accumulated, being $A_t = \\{\\tilde{a}_{t,1},...,\\tilde{a}_{t,|A_t|}\\}$ their corresponding embeddings. Each anchor $\\tilde{a}_{t,i}$ is paired with a relevance score $r_{t,i}$, that is used to determine what anchors appear more promising to proceed in the attack, or if the attack should stop. Relevance scores are collected in $R_t$. An attack query $q_t$ is built exploiting information inherited from the most relevant anchors in $A_t$, and by adding a final suffix that acts as an injection command (Zeng et al., 2024; Qi et al., 2024; Cohen et al., 2024; Jiang et al., 2024a). The injection command induces unwanted behaviors that aid in information stealing, guiding the language model f of the RAG system to generate outputs that also contain (portions of) $\\mathcal{X}(q_t)$. We consider a given set of injections commands C, following what is commonly done in related literature (in our experience, |C| = 4, and commands are listed in Appendix B). In the rest of the paper, all the attacker-side embeddings are always indented to be computed by $e^{*}$. The attacker exploits a similarity function sim($x_i$, $x_j$) to compare embeddings, that we assume to be the cosine similarity. The attack is reported in Algorithm 1, and described in the following."}, {"title": "5 Experiments", "content": "We present experiments that simulate real-world attack scenarios to three different RAG systems, using different attacker-side LLMs. The objective is to extract as much information as possible from the private knowledge bases. Each RAG system is used to implement what we refer to as \"agent\", i.e., a chatbot-like virtual agent that allows the user to interact by natural language queries.\nVirtual Agents. We define three RAG-based agents (Table 1). Agent A, a diagnostic support chatbot intended for use by patients. This agent leverages a concealed knowledge base built from historical patient-doctor conversations and medical records, enabling it to suggest plausible conditions based on a patient's current symptoms. Agent B is a research assistant for chemistry and medicine, tailored to support researchers in experimental settings. Its private knowledge base includes confidential chemical synthesis procedures and proprietary methods for producing specific compounds. Agent C is an educational assistant designed to interact with children, responding to questions about various subjects, including history and geography. The private knowledge base was populated by documents that also include private details about historical monuments, that were not removed due to insufficient content screening. The private knowledge bases of virtual agents A, B, C are simulated by means of well-known datasets (Table 1). We sampled 1,000 chunks for each agent with a guided semantic sub-sampling technique which avoids chunks to belong to the same portion of knowledge (see Appendix D). The chunking strategy for Agent A follows (Zeng et al., 2024) (i.e., the patient-doctor pair is kept as a single chunk), while in Agent B and C we followed the strategies applied to the respective datasets in HuggingFace for RAG-evaluation. We use Chroma-DB as the vector store, simulating different agent characteristics by changing the number of retrieved chunks and the LLM temperature.\nCompetitors. We compare our method (referred to as Pirate) with the competitors described in Section 4: TGTB (Zeng et al., 2024), PIDE (Qi et al., 2024), DGEA (Cohen et al., 2024) and RThief (Jiang et al., 2024a). As anticipated, in their targeted setup, the authors of TGTB and PIDE use GPT as a query generator. To provide another competitor for our untargeted setup, we also introduce the new approach named GPTGEN, utilizing GPT-40-mini (OpenAI et al., 2024) tasked with generating questions focused on general knowledge topics, with the same attack routing of TGTB/PIDE. Note that DGEA and RThief are designed to target high-end online LLMs. To maintain consistency, we use GPT-40-mini (OpenAI et al., 2024) as the LLM for such approaches. To further strengthen the comparison, we also consider variants DGEA* and RThief*, which use the same LLMs as our approach and the other competitors (Table 1). Moreover, DGEA* also assumes no prior knowledge of the hidden embedder, making it a fully black-box attack.\nBounded vs. Unbounded. To ensure fair and realistic evaluations, we consider two distinct (BO) bounded and (UN) unbounded attacks. In the BO scenario, each method performs 300 attacks (i.e., attempts to use all the injection commands until some chunks are returned, as in the inner loop of Algorithm 1). In contrast, in the UB scenario the attack algorithm can run a virtually unlimited number of queries, determining by itself when to stop. Of course, UB only apply to methods that can automatically generate attack queries and that have an adaptive way to determine how to model the attack procedure and stop, which is the case of our Pirate algorithm only, since all other competitors rely on a predefined, fixed number of attack iterations (i.e., we cannot simply increase it because they have no adaptive ways to generate new queries). The only exception is RThief, where we can simulate UB (RThief-UB) by stopping when both its short-term and long-term buffers are empty (i.e., not being able to proceed any further). All hyperparameters for the competitors remain as originally prescribed in their respective papers. On the attacker side, we select tools that balance performance and computational efficiency, making it feasible to run even on domestic hardware: the text embedder is Snowflake Arctic model, picked from the MTEB leaderboard, while LLM is LLaMA 3.2 1B with temperature set to 0.1. In our method we set $\\beta$ = 1, the similarity threshold between chunks $\\alpha_1$ = 0.95, the similarity between anchors $\\alpha_2$ = 0.8 and the number of anchors used to generate a new text n = 3."}, {"title": "6 Conclusions and Future Work", "content": "This paper presented an adaptive procedure that allows a malicious user to extract information from the private knowledge base of a RAG system. Thanks to an anchor-based mechanism, paired with automatically updated relevance scores, the proposed algorithm allows a user equipped with a open-source tools (that can run on a domestic computer) to craft attacks that significantly overcome all the considered competitors in terms of coverage, leaked knowledge, query building time. These findings remark the urgent need for more robust safeguards in the design of RAG systems (see Appendix H for details on new upcoming safeguarding techniques). Our future work will consider a targeted version of the attack, which should be easily implemented by using a set of pre-designed anchors."}, {"title": "7 Limitations", "content": "The main limitation of the proposed attack strategies can be summarized in the following.\n\u2022 Chunks retrieved by the attack procedures and the private ones must be compared to declare whether they contain the same information or not. Our analysis is based on comparing extracted chunks and the private ones by means of ROUGE-L score, and considering them coherent if greater than 0.5, following related literature. As a matter of fact, the retrieved/stolen chunks can include noise (such as portions of text added by the LLM of the RAG when generating its output), synonyms, or rephrased information, making the comparison really challenging. We used the vector space of the embeddings to initially compare pairs of chunks, before feeding them to the ROUGE-L metric, in order to try to favor comparison on semantics (due to the embedding space) and only afterwards use the ROUGE-L metric. However, other solutions could be considered to make this analysis more strict, or focusing on different aspects of the generated text, which we do not consider in this paper. Also the duplicate matching procedure is subject to similar issues, since it depends on a pre-selected threshold, which might end up in marking as not duplicate pieces of text that are actually very similar, or, vice-versa, in marking as duplicate pairs of somehow different chunks.\n\u2022 The knowledge base of a RAG system will likely contain information that is public, thus not introducing evident security constraints, as well as private data to protect. The proposed algorithm only consider the amount of leaked knowledge, without distinguishing between the two types of knowledge in the evaluation (since we do not have access to this kind of labeled data). This goes back to the previous point of this list: the comparison routine has some tolerance, and, perhaps, in some cases is the private part of the chunks that is disregarded, thus the actual leaked chunks might not include some private data even with maximum LK score.\n\u2022 Moreover, the proposed attack is untargeted, while, in some cases, the attacker might look for specific pieces of knowledge. Targeted procedures are not currently supported by our attack, even if, as anticipated in the previous section, it is something we are considering.\n\u2022 The attack quality also obviously depends on the quality of the attacker-side LLM. While the attack routine is general, if the attacker LLM is way too simple, it is likely that the whole attack will not be effective, yielding many not-promising anchors and leading to several not successful attacks, since the whole relevance mechanism will be not informative.\n\u2022 Despite being automatic, there are indeed some initial values for some key parameters that must be set (Algorithm 1), including similarity thresholds with clear implications in the duplicate-identification procedure. The quality of the attack clearly depends on such parameters as well, and while we explored multiple scenarios, it might be not trivial to find the optimal values to use in real-world cases.\n\u2022 The attack is performed on classic setups of RAG systems. There might be LLM-based safeguard procedure that rejects queries with injection-like command, of specific filtering rules/detectors that can block queries that are marked as malicious. The effectiveness of the attack, of course, depend on the corresponding not-effectiveness of such measures to mitigate attacks."}, {"title": "8 Ethical Considerations", "content": "There exist some important ethical considerations regarding those procedures that can compromise the security of RAG systems, given their potential to harm users and stakeholders. RAG systems often include sensitive or proprietary data in their internal knowledge bases, so that exposing such data raises serious concerns. As a matter of fact, such data could be used with malicious purposes, such as misinformation dissemination, intellectual property theft, or privacy violations. Developers and operators of RAG systems must ensure robust security measures are in place to protect against attacks that manipulates the queries submitted to the model, as the one of this paper. Of course, as it is typical for every other existing attack to machine learning-based models, defense measures can be added to compensate the specific inject commands of this paper, but, in the meantime, new poisoned queries could be introduced, keeping the attack algorithm untouched. This paper is not exposing any new knowledge on issues on RAG security, with respect to the ones that are already public (see Section 4), and the dynamics of the proposed attack are mostly based on more advanced procedures in directions that were already considered by existing literature. However, we believe that this paper offers a more detailed point of view on this recently highlighted problem, thus offering the opportunity to design countermeasures that follows and go beyond the attack of this paper."}]}