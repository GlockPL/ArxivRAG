{"title": "ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for Cross-Domain Recommendation", "authors": ["Chaoqun Hou", "Yi Cao", "Yuanhang Zhou", "Tong Liu"], "abstract": "In industrial recommendation systems, there are several mini-apps designed to meet the diverse interests and needs of users. The sample space of them is merely a small subset of the entire space, making it challenging to train an efficient model. In recent years, there have been many excellent studies related to cross-domain recommendation aimed at mitigating the problem of data sparsity. However, few of them have simultaneously considered the adaptability of both sample and representation continual transfer setting to the target task. To overcome the above issue, we propose a Entire space Continual and Adaptive Transfer learning framework called ECAT which includes two core components: First, as for sample transfer, we propose a two-stage method that realizes a coarse-to-fine process. Specifically, we perform an initial selection through a graph-guided method, followed by a fine-grained selection using domain adaptation method. Second, we propose an adaptive knowledge distillation method for continually transferring the representations from a model that is well-trained on the entire space dataset. ECAT enables full utilization of the entire space samples and representations under the supervision of the target task, while avoiding negative migration. Comprehensive experiments on real-world industrial datasets from Taobao show that ECAT advances state-of-the-art performance on offline metrics, and brings +13.6% CVR and +8.6% orders for Baiyibutie, a famous mini-app of Taobao.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommendation systems (RS) have played a significant role in e-commerce platforms, and their efficiency is closely related to the accuracy of click-through rate (CTR) prediction. In recent years, thanks to the continuous improvements in computational power and the increasing volume of datasets, numerous outstanding single-domain CTR models [3, 4, 10, 21, 29] have achieved impressive results. At large e-commercial companies, there are several mini-apps designed to meet the diverse interests and needs of users. However, these mini-apps all encounter a common issue: the target domain has relatively sparse samples, making it challenging to train the complex CTR model, especially the representations of ID categorical features (i.e., item ID and user ID). Take Taobao for instance, Baiyibutie is a mini-app that contributes billions of daily page views by exclusively selling brand-discounted products. The sample size of Baiyibutie is less than 1% of the entire Taobao domain. Therefore, exploring how cross-domain transfer learning can utilize the abundant information available in data-rich domains to enhance the data-sparse domains has emerged as an important research focus in the industry. The traditional cross-domain [1, 9, 14, 16, 19, 25, 28, 31] recommendation can be categorized into two paradigms: sample transfer and parameter transfer from the well-trained source model.\nIn the sample transfer paradigm, multi-task learning methods [11, 18, 20, 23, 24, 27, 30, 32] are typically employed to enhance performance across all domains by combining the source and the target samples. However, despite the fact that this paradigm has achieved commendable results in many scenarios, it still has some evident limitations in certain situations. For instance, in scenarios where the sample size of source domain is hundreds of times larger than that of the target domain, the training process can be easily dominated by the source domain, resulting in insufficient training in the target domain. Another issue is that introducing the source domain samples of such a large scale could significantly increase complexity. Therefore, the core objective should be to enhance the"}, {"title": "2 METHODS", "content": ""}, {"title": "2.1 Problem Definition", "content": "Mathematically, we represent samples from the source domain and the target domain as $D_s = {(x, y)}$ and $D_t = {(x, y)}$ respectively,"}, {"title": "2.2 Model Overview", "content": "We have decomposed the ECAT framework process into two serial stages. Initially, figure 1(a) shows a simple yet effective method called Graph-guided based Sample Transfer (GST), which aims to select samples from Ds with a similar distribution to Dt. The GST can incorporate sample relevance by leveraging prior heuristic insights or measure it through representational learning via graph neural networks. In this paper, we focus on the area of e-commerce recommendation, there inherently exists plenty of valuable prior knowledge. For example, a direct browse, click or purchase of an item by a user acts as a one-hop link, while a two-hop link can be established between two items through a co-click relationship by users. Moreover, GST is versatile and capable of employing suitable strategies based on the specific domain, or even training a graph representation network model. Subsequently, from left to right in figure 1(b), the diagram sequentially illustrates the Domain Adap-tion (DA) module for assessment of incremental value that samples from Dgst contribute to T, and the Adaptive Knowledge Distilla-tion (AKD-CT) module is designed to assess the incremental value that representations from the well-trained source model S. More detailed exposition will be delineated in the subsequent discourse."}, {"title": "2.3 Graph guided and Domain Adaptation based Sample Transfer", "content": "Graph guided Module: Incorporating the findings of many related studies [5, 27], the evidence suggests that a model trained using samples from the combined source and target domains, typically results in insufficient training to the target domain. This phenomenon is particularly pronounced in the context of this study, where the sample size of Ds is hundreds of times greater than Dt. Considering that, in our scenario, the target domain is a sub-channel of the source domain (V CVs), the heuristic GST approach is markedly effective and appropriate for our domain-specific challenges. Specifically, as shown in figure 1(a), we initiate the process by directly mapping $V_{seed} = V_t \\cap V_s$ onto Gs, anchoring the alignment through the correspondence of the same IDs between the domains. Subsequently, within Gs = (Vs, Es), we expand to include more nodes Vgeneralized, which are similar to the target domain, by exploiting one-hop (i.e., click or pay relationships) and two-hop (i.e., co-click or group cluster) connectivity. Finally, within the context of e-commerce recommendation systems, the relevant sample can be identified by specifying a distinct user\u012f and itemj. In other words, we can convert G = ($V_{seed} \\cup V_{generalized}, Es$) to Dgst."}, {"title": "2.4 Adaptive Knowledge Distillation based Continual representation Transfer", "content": "Source Domain Module: To provide incremental information, we introduce a source model S that has been well-trained in the entire space. During the training process of T, S only executes forward propagation, which entails a low computational complexity. As shown in Figure 1(b-right), S and T have identical architecture.\nAKD-CT Module: Inspired by CTNet, ECAT endeavors to en-hance the target model performance through CTL setting. ECAT differs in that it further transfers all layers from the embedding lay-ers to the logit layers, particularly sequence layer representations. To achieve this, we propose an Adaptive Knowledge Distillation based Continual Transfer (AKD-CT) method. Figure 1(b-right) illus-trates the training process of AKD-CT that showcases the represen-tations distillation of the sequence layers. Specifically, we obtain the representations of various behavioral sequence features after passing through the embedding and sequence layers. $e_{seq}^{t'}$ and $e_{seq}^s$ denote the sequence representations obtained from T and S, respectively. Subsequently, $e_{seq}^{t'}$ is the input of adapter layers to obtain $e_{seq}^{ta}$, which then distill knowledge from S under the supervision of Ldi. We use cosine similarity loss to pull $e_{seq}^{ta}$ and $e_{seq}^s$ more similar. To prevent noise from the distillation process, we stop conducting gradient to T. We have obtained the incremental information $e_{seq}^{ta}$, all that remains is appropriately fusing $e_{seq}^{ta}$ into T.\nThe Design of Adaptive Knowledge Distillation is three-fold: First, considering that T may have better discrimination for certain samples than S. We introduce an adaptive gate network to assess the value of $e_{seq}^s$ for T. Specifically, we concatenate $e_{seq}^{t'}$, $e_{seq}^s$"}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 Experimental Setup", "content": "3.1.1 Dataset. In the absence of suitable public benchmarks for evaluating continual cross-domain prediction, we adopt Taobao industrial datasets to comprehensively compare ECAT and baselines. Therefore, we use the Baiyibutie from Taobao as the target domain, which generates millions CVR samples every day, accounting for less than 1% of the entire space of Taobao. The users and items in the source domain and the target domain partially overlap, but the data distribution is very different. In this study, we utilize target domain samples spanning 90 days, amounting to a total of 120 million samples. Similarly, taking the entire space as the source domain, we have accumulated a total of 66 billion samples. During A/B testing, ECAT serves over hundreds of thousands of users daily.\n3.1.2 Baseline Models. We compare the samples transfer methods like simple merge Ds and Dt, DANN [8], the representations trans-fer methods including Shared Bottom, MMOE [18], PLE [24], and the continual learning setting method like CTNet [17].\n3.1.3 Implementation Details. To ensure the fairness of the ex-periments, all single domain methods employs the ETA [3] as the architecture, including the target model and source model. Specifi-cally, we use AdagradDecayV2 [7] as the optimizer. Learning rate is set to 0.01 and the batch size is 1024. The dimension of MLP Layers is set to 1024, 512 and 256. Following previous work, we adopt AUC to measure the CVR prediction performance in offline evaluation."}, {"title": "3.2 Offline Evaluation", "content": "As shown in Table 1, our ECAT (GST & DA and AKD-CT) achieves the best performance among all baselines. More specifically,\n(1) ECAT achieves the best performance (AUC=0.8348) compared to both single-domain and cross-domain methods, with its core ad-vantage being that ECAT simultaneously considers the adaptability of both sample and representation for the target task.\n(2) In terms of sample transfer: ECAT enhances the performance of each method, including CTNet (AUC from 0.8307 to 0.8327), by"}, {"title": "3.3 Research Questions", "content": "3.3.1 RQ1: How to prove the adaptive capability of AKD-CT model?\nTable 2 shows that (1) AKD-CT drops performance without gate. The reason is that the gate network assesses the importance of incremental information for T, providing valuable incremental representation information for samples with higher uncertainty and lower confidence. (2) The absence of distillation intensity in AKD-CT results in poorer result, which suggests that an intensity-based strategy facilitates the distillation of representations more suited to T.\n3.3.2 RQ2: How to prove the necessity of CTL setting?\nWe compare the performance between AKD-CT and CTNet un-der continuous transfer and one-time transfer setting. Table 3 shows that continuous transfer is better than one-time transfer, which illustrates the necessity of CTL. At is 30 days in this study."}, {"title": "4 CONCLUSIONS", "content": "In this paper, we introduce the ECAT framework for cross-domain prediction, which not only considers the continual transfer of both samples and representations but also the adaptability of incremental information to the target task. Experiments conducted on a large-scale industrial dataset, along with online A/B testing, confirm its effectiveness in real-world applications. It is noteworthy that ECAT has been deployed in the RS of Taobao to serve numerous marketing channels, including Baiyibutie."}]}