{"title": "ECAT: A Entire space Continual and Adaptive Transfer Learning Framework for Cross-Domain Recommendation", "authors": ["Chaoqun Hou", "Yi Cao", "Yuanhang Zhou", "Tong Liu"], "abstract": "In industrial recommendation systems, there are several mini-apps designed to meet the diverse interests and needs of users. The sample space of them is merely a small subset of the entire space, making it challenging to train an efficient model. In recent years, there have been many excellent studies related to cross-domain recommendation aimed at mitigating the problem of data sparsity. However, few of them have simultaneously considered the adaptability of both sample and representation continual transfer setting to the target task. To overcome the above issue, we propose a Entire space Continual and Adaptive Transfer learning framework called ECAT which includes two core components: First, as for sample transfer, we propose a two-stage method that realizes a coarse-to-fine process. Specifically, we perform an initial selection through a graph-guided method, followed by a fine-grained selection using domain adaptation method. Second, we propose an adaptive knowledge distillation method for continually transferring the representations from a model that is well-trained on the entire space dataset. ECAT enables full utilization of the entire space samples and representations under the supervision of the target task, while avoiding negative migration. Comprehensive experiments on real-world industrial datasets from Taobao show that ECAT advances state-of-the-art performance on offline metrics, and brings +13.6% CVR and +8.6% orders for Baiyibutie, a famous mini-app of Taobao.", "sections": [{"title": "1 INTRODUCTION", "content": "Recommendation systems (RS) have played a significant role in e-commerce platforms, and their efficiency is closely related to the accuracy of click-through rate (CTR) prediction. In recent years, thanks to the continuous improvements in computational power and the increasing volume of datasets, numerous outstanding single-domain CTR models [3, 4, 10, 21, 29] have achieved impressive results. At large e-commercial companies, there are several mini-apps designed to meet the diverse interests and needs of users. However, these mini-apps all encounter a common issue: the target domain has relatively sparse samples, making it challenging to train the complex CTR model, especially the representations of ID categorical features (i.e., item ID and user ID). Take Taobao for instance, Baiyibutie is a mini-app that contributes billions of daily page views by exclusively selling brand-discounted products. The sample size of Baiyibutie is less than 1% of the entire Taobao domain. Therefore, exploring how cross-domain transfer learning can utilize the abundant information available in data-rich domains to enhance the data-sparse domains has emerged as an important research focus in the industry. The traditional cross-domain [1, 9, 14, 16, 19, 25, 28, 31] recommendation can be categorized into two paradigms: sample transfer and parameter transfer from the well-trained source model.\nIn the sample transfer paradigm, multi-task learning methods [11, 18, 20, 23, 24, 27, 30, 32] are typically employed to enhance performance across all domains by combining the source and the target samples. However, despite the fact that this paradigm has achieved commendable results in many scenarios, it still has some evident limitations in certain situations. For instance, in scenarios where the sample size of source domain is hundreds of times larger than that of the target domain, the training process can be easily dominated by the source domain, resulting in insufficient training in the target domain. Another issue is that introducing the source domain samples of such a large scale could significantly increase complexity. Therefore, the core objective should be to enhance the"}, {"title": "2 METHODS", "content": ""}, {"title": "2.1 Problem Definition", "content": "Mathematically, we represent samples from the source domain and the target domain as Ds = (x, y) and Dt = (x, y) respectively,\nwhere $x \\in \\mathbb{R}^{d_s}$ and $x_t \\in \\mathbb{R}^{d_t}$. Label $y$ and $y \\in \\{0,1\\}$ indicate whether the itemi was purchased or not. It is worth mentioning that we have established the capability to acquire samples from the entire domain of Taobao. S is a continually well-trained model on Ds, capable of learning new distributions in a timely manner. In this study, our goal is to train a model T using Dt while considering the incremental information in areas including sample transfer from Ds and representation transfer from S. Furthermore, we represent Ds through a graph $G_s = (V_s, E_s)$, where $V_s = \\{u, i, ..., u_n, i_n\\}$ denotes the user and item node in the graph of source domain. Edge eij \u2208 Es denotes that useri has clicked or purchased on itemj. In other words, we can identify the corresponding samples Ds through the nodes and edges of Gs. Similarly, we define Gt = (Vt, Et) according to Dt."}, {"title": "2.2 Model Overview", "content": "We have decomposed the ECAT framework process into two serial stages. Initially, figure 1(a) shows a simple yet effective method called Graph-guided based Sample Transfer (GST), which aims to select samples from Ds with a similar distribution to Dt. The GST can incorporate sample relevance by leveraging prior heuristic insights or measure it through representational learning via graph neural networks. In this paper, we focus on the area of e-commerce recommendation, there inherently exists plenty of valuable prior knowledge. For example, a direct browse, click or purchase of an item by a user acts as a one-hop link, while a two-hop link can be established between two items through a co-click relationship by users. Moreover, GST is versatile and capable of employing suitable strategies based on the specific domain, or even training a graph representation network model. Subsequently, from left to right in figure 1(b), the diagram sequentially illustrates the Domain Adaption (DA) module for assessment of incremental value that samples from Dgst contribute to T, and the Adaptive Knowledge Distillation (AKD-CT) module is designed to assess the incremental value that representations from the well-trained source model S. More detailed exposition will be delineated in the subsequent discourse."}, {"title": "2.3 Graph guided and Domain Adaptation based Sample Transfer", "content": "Graph guided Module: Incorporating the findings of many related studies [5, 27], the evidence suggests that a model trained using samples from the combined source and target domains, typically results in insufficient training to the target domain. This phenomenon is particularly pronounced in the context of this study, where the sample size of Ds is hundreds of times greater than Dt. Considering that, in our scenario, the target domain is a sub-channel of the source domain (V CVs), the heuristic GST approach is markedly effective and appropriate for our domain-specific challenges. Specifically, as shown in figure 1(a), we initiate the process by directly mapping Vseed = V \u2229 Vs onto Gs, anchoring the alignment through the correspondence of the same IDs between the domains. Subsequently, within Gs = (Vs, Es), we expand to include more nodes Vgeneralized, which are similar to the target domain, by exploiting one-hop (i.e., click or pay relationships) and two-hop (i.e., co-click or group cluster) connectivity. Finally, within the context of e-commerce recommendation systems, the relevant sample can be identified by specifying a distinct user\u012f and itemj. In other words, we can convert G = (Vseed U Vgeneralized, Es) to Dgst."}, {"title": "2.4 Adaptive Knowledge Distillation based Continual representation Transfer", "content": "Target Domain Module: The structure of the target model T is similar to ETA [3], which includes four parts. First, the embedding layer maps features to representations of a specific dimension, primarily including categorical features and numerical features. It is worth mentioning that the categorical features are extremely important and require a substantial number of samples for effective training. It is the significant reason why we introduce the well-trained model S across the entire space. Subsequently, long and short-term user behavioral sequences are mapped into higher semantic representations through the sequence layers. Finally, we can get the score after successively passing through the classification and logit layers. Ly is usually a binary cross-entropy loss function.\n$L_y = \\frac{1}{n_t + n_{gst}} \\sum_{x \\in D_t \\cup D_{gst}} L_{ce}[G^t(\\Phi^t(x)), y]$,\nwhere nt and ngst are the sample size of Dt and Dgst respectively. Gt denotes the output of samples pass sequentially from the representation to the logit layers of T. \u03a6t is designed to map samples from different feature dimensions to the same feature space, such as attention maps [15].\nDomain Adaptation Module: To select samples from Dgst that better fit the distribution of Dt, as shown in figure 1(b), we refine the sample selection by incorporating a Domain Adaption module. The training dataset is Dda = Dt \u222a Dgst = (xda, yda) and (xda) denotes domain-independent features. Label $y_{da} \\in \\{0, 1\\}$ indicates whether the sample $x_{da}$ belongs to Dt or Dgst. The optimization objective Lda is a binary cross-entropy loss function.\nThe DA module is effective due to three key factors: First, to avoid feature bias, we ensure the effectiveness of the discriminator by solely using domain-independent features. Second, to avoid model bias towards the source domain, we select samples similar to the target domain distribution through GST. Third, to prevent the target model from being influenced by irrelevant gradients, we stop the gradients produced by the DA on the target model.\nSource Domain Module: To provide incremental information, we introduce a source model S that has been well-trained in the entire space. During the training process of T, S only executes forward propagation, which entails a low computational complexity. As shown in Figure 1(b-right), S and T have identical architecture.\nAKD-CT Module: Inspired by CTNet, ECAT endeavors to enhance the target model performance through CTL setting. ECAT differs in that it further transfers all layers from the embedding layers to the logit layers, particularly sequence layer representations. To achieve this, we propose an Adaptive Knowledge Distillation based Continual Transfer (AKD-CT) method. Figure 1(b-right) illustrates the training process of AKD-CT that showcases the representations distillation of the sequence layers. Specifically, we obtain the representations of various behavioral sequence features after passing through the embedding and sequence layers. $e^s_{seq}$ and $e^t_{seq}$ denote the sequence representations obtained from T and S, respectively. Subsequently, $e^s_{seq}$ is the input of adapter layers to obtain $e^{s'}_{seq}$, which then distill knowledge from S under the supervision of $L_{di}$. We use cosine similarity loss to pull $e^{s'}_{seq}$ and $e^t_{seq}$ more similar. To prevent noise from the distillation process, we stop conducting gradient to T. We have obtained the incremental information $e^{s'}_{seq}$, all that remains is appropriately fusing $e^{s'}_{seq}$ into T.\nThe Design of Adaptive Knowledge Distillation is threefold: First, considering that T may have better discrimination for certain samples than S. We introduce an adaptive gate network to assess the value of $e^{s'}_{seq}$ for T. Specifically, we concatenate $e^{t}_{seq}$, $e^{s'}_{seq}$"}, {"title": "3 EXPERIMENTS", "content": ""}, {"title": "3.1 Experimental Setup", "content": "3.1.1 Dataset. In the absence of suitable public benchmarks for evaluating continual cross-domain prediction, we adopt Taobao industrial datasets to comprehensively compare ECAT and baselines. Therefore, we use the Baiyibutie from Taobao as the target domain, which generates millions CVR samples every day, accounting for less than 1% of the entire space of Taobao. The users and items in the source domain and the target domain partially overlap, but the data distribution is very different. In this study, we utilize target domain samples spanning 90 days, amounting to a total of 120 million samples. Similarly, taking the entire space as the source domain, we have accumulated a total of 66 billion samples. During A/B testing, ECAT serves over hundreds of thousands of users daily."}, {"title": "3.2 Offline Evaluation", "content": "As shown in Table 1, our ECAT (GST & DA and AKD-CT) achieves the best performance among all baselines. More specifically,\n(1) ECAT achieves the best performance (AUC=0.8348) compared to both single-domain and cross-domain methods, with its core advantage being that ECAT simultaneously considers the adaptability of both sample and representation for the target task.\n(2) In terms of sample transfer: ECAT enhances the performance of each method, including CTNet (AUC from 0.8307 to 0.8327), by transferring valuable samples from Ds through GST & DA. Besides, directly merging Ds with Dt leads to performance degradation.\n(3) In terms of continual representation transfer: ECAT further improves performance through the adaptive capabilities of AKD-CT module, which under the supervision of the target task, continuously transfers valuable representation information from S. Even with the same sample transfer strategy, the effectiveness of AKD-CT (AUC=0.8348) surpasses that of CTNet (AUC=0.8327)."}, {"title": "3.3 Research Questions", "content": "3.3.1 RQ1: How to prove the adaptive capability of AKD-CT model?\nTable 2 shows that (1) AKD-CT drops performance without gate. The reason is that the gate network assesses the importance of incremental information for T, providing valuable incremental representation information for samples with higher uncertainty and lower confidence. (2) The absence of distillation intensity in AKD-CT results in poorer result, which suggests that an intensity-based strategy facilitates the distillation of representations more suited to T.\n3.3.2 RQ2: How to prove the necessity of CTL setting?\nWe compare the performance between AKD-CT and CTNet under continuous transfer and one-time transfer setting. Table 3 shows that continuous transfer is better than one-time transfer, which illustrates the necessity of CTL. At is 30 days in this study."}, {"title": "4 CONCLUSIONS", "content": "In this paper, we introduce the ECAT framework for cross-domain prediction, which not only considers the continual transfer of both samples and representations but also the adaptability of incremental information to the target task. Experiments conducted on a large-scale industrial dataset, along with online A/B testing, confirm its effectiveness in real-world applications. It is noteworthy that ECAT has been deployed in the RS of Taobao to serve numerous marketing channels, including Baiyibutie."}]}