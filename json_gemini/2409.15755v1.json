{"title": "Stage-Wise Reward Shaping for Acrobatic Robots: A Constrained Multi-Objective Reinforcement Learning Approach", "authors": ["Dohyeong Kim", "Hyeokjin Kwon", "Junseok Kim", "Gunmin Lee", "Songhwai Oh"], "abstract": "As the complexity of tasks addressed through reinforcement learning (RL) increases, the definition of reward functions also has become highly complicated. We introduce an RL method aimed at simplifying the reward-shaping process through intuitive strategies. Initially, instead of a single reward function composed of various terms, we define multiple reward and cost functions within a constrained multi-objective RL (CMORL) framework. For tasks involving sequential complex movements, we segment the task into distinct stages and define multiple rewards and costs for each stage. Finally, we introduce a practical CMORL algorithm that maximizes objectives based on these rewards while satisfying constraints defined by the costs. The proposed method has been successfully demonstrated across a variety of acrobatic tasks in both simulation and real-world environments. Additionally, it has been shown to successfully perform tasks compared to existing RL and constrained RL algorithms. Our code is available at https://github.com/rllab-snu/Stage-Wise-CMORL.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, reinforcement learning (RL) has driven significant progress in real-world robotic applications [1]-[5]. Quadrupedal robots have demonstrated stable locomotion on rough terrain [1], [6] and performed parkour-like stunts [7], [8], while bipedal robots have successfully tackled challenging tasks such as jumping and running [9], [10]. In order to accomplish such legged robot tasks, a reward function should be defined considering multiple factors, such as task performance, safety, and energy efficiency. Consequently, reward functions are generally formalized as a sum of various terms related to performance, safety, and regularization [1], [2], [6]. However, due to the numerous terms, the reward-shaping process, defining each term and its respective weight, can be laborious and challenging. Simplifying this process is essential to apply RL to a broader range of tasks.\nIn the case of acrobatic tasks involving complex movements, such as rolls and back-flips, the difficulty of designing rewards increases significantly. By taking the back-flip as an example, this task requires a focus on jumping at the beginning of the episode and on landing after the jump. As a result, the proportions of reward terms related to jumping and landing should be adjusted dynamically, complicating the reward-shaping process. Alternatively, imitation RL methods using motion capture data or animatronic data have been developed [11]\u2013[13], wherein the reward is defined to minimize the pose difference between the robot and the collected data. However, these methods are expensive as they require collecting extensive data for each task. Therefore, a method is required that can intuitively design reward functions without relying on additional imitation data.\nIn this paper, we propose an RL method that defines multiple reward functions in a stage-wise manner by utilizing a constrained multi-objective RL (CMORL) framework [14]. To simplify the reward-shaping process, our approach does not integrate multiple terms into a single reward. Instead, each term is treated as an independent reward or cost function within the CMORL framework, where the cost functions correspond to safety-related terms, such as body collision and joint limit. In addition, to facilitate reward"}, {"title": "II. RELATED WORK", "content": ""}, {"title": "A. Constrained Multi-Objective Reinforcement Learning", "content": "Multi-objective RL (MORL) is divided into single-policy methods [17], [18], which aim to find one Pareto optimal policy, and multi-policy methods [19]\u2013[23], which aim to find a set of Pareto optimal policies. Single-policy methods combine multiple objectives into a single objective using utility functions [24] or preference vectors [17], [18], allowing existing RL algorithms to be applied for policy updates. Multi-policy methods either simultaneously update policies for multiple preferences [20], [22] or train a universal policy that can represent a variety of policies by conditioning on preferences [19], [23]. Among these, LP3 [25] and CoMOGA [14] are CMORL algorithms that extend existing MORL algorithms to consider constraints using Lagrangian [26] and primal [27] approaches, respectively. The proposed algorithm, CoMOPPO, can be viewed as a single-policy method that simplifies the implementation of CoMOGA."}, {"title": "B. Reinforcement Learning for Legged Robots", "content": "Advances in simulations, such as Isaac Gym [28], have made it possible to directly deploy RL policies trained in simulations into real-world environments, significantly increasing efficiency and reducing risk. Leveraging these advances, sim-to-real techniques, such as terrain curriculum, have enabled quadrupedal robots to successfully navigate challenging terrains, such as slippery surfaces and steep slopes [1], [2], [4]\u2013[6]. Furthermore, there have been works that enable dynamic parkour-like movements, such as long jumps and two-legged walking, by using novel reward definitions and state representation approaches [3], [7], [8]. To simplify the definition of reward functions, constrained RL algorithms [29] have been employed for legged robots [30], [31]. These methods exclude safety-related terms, such as body collisions and joint limits, from the reward function and instead use them to define explicit constraints. These algorithms have demonstrated the effectiveness of constrained RL through robustness to reward weights. However, the tasks addressed by them are primarily limited to locomotion. We expanded these approaches to the CMORL framework to perform tasks requiring more complex acrobatic movements."}, {"title": "III. BACKGROUND", "content": ""}, {"title": "A. Constrained Multi-Objective Markov Decision Processes", "content": "A constrained multi-objective Markov decision process (CMOMDP) is defined as $(S, A, P, \\rho, \\gamma, R_{1:N}, C_{1:M})$ with a state space $S$, an action space $A$, a transition model $P$, an initial state distribution $\\rho$, a discount factor $\\gamma$, $N$ reward functions $R_i(s, a, s')|_{i=1}^N$, and $M$ cost functions $C_j(s, a, s')|_{j=1}^M$. A policy is defined as $\\pi : S \\rightarrow P(A)$, where $\\pi(a|s)$ denotes the probability of executing action $a$ in state $s$. A trajectory is defined as $\\tau = \\{s_0, a_0, s_1, a_1, \\ldots\\}$, where $s_0 \\sim \\rho$, $a_t \\sim \\pi(\u00b7|s_t)$, and $s_{t+1} \\sim P(\u00b7|s_t, a_t)$ $\\forall t$. Action value, state value, and advantage functions for the rewards are defined as $Q^{R_i}(s, a) := E_{\\tau \\sim \\pi}[\\sum_t\\gamma^tR_i(s_t, a_t, s_{t+1})]$, where $s_0 = s$ and $a_0 = a$, $V^{R_i}(s) := E_{a \\sim \\pi(\u00b7|s)}[Q^{R_i}(s, a)]$, and $A^{R_i}(s, a) := Q^{R_i}(s, a) - V^{R_i}(s)$. Similarly, the value and advantage functions for the costs are defined by substituting $R_i$ with $C_j$. The reward and cost functions are used to construct objectives and constraints in a CMORL problem, respectively."}, {"title": "B. CMORL Problem Setup", "content": "A CMORL problem is defined as follows:\n$\\max_{\\pi} J_{R_i}(\\pi) \\quad \\forall i \\in \\{1, ..., N\\}$ (1)\ns.t. $J_{C_j}(\\pi) \\leq d_j/(1 - \\gamma) \\quad \\forall j \\in \\{1, ..., M\\}$,\nwhere $J_{R_i}(\\pi) := E_{\\tau \\sim \\pi}[\\sum_t\\gamma^tR_i(s_t, a_t, s_{t+1})]$, and $d_j$ is a threshold of the $j$th constraint. The target of the CMORL problem finds a constrained-Pareto (CP) optimal policy [14]."}, {"title": "IV. PROPOSED METHOD", "content": "Now, we introduce a new CMORL framework, which enables intuitive reward shaping for acrobatic tasks. The proposed method consists of three main parts: 1) stage-wise reward shaping, 2) a policy update rule handling multiple objectives and constraints, and 3) sim-to-real techniques for deploying policies trained in simulation to real-world. In the rest of the section, the details of each part will be described."}, {"title": "A. Stage-Wise Reward Shaping", "content": "In general, a reward function consists of several terms including task-related terms, regularization terms, and safety-related terms. Instead of integrating all terms into a scalar reward, we use a CMORL framework which maximizes multiple objectives corresponding to each reward term and satisfying constraints corresponding to the safety-related terms. Furthermore, for a complex task requiring sequential movements, it is required to adjust weights of individual reward terms dynamically, which further increases the difficulty of reward shaping. To resolve this issue, we propose to divide a task into a sequence of stages and define reward and cost functions in a stage-wise manner. Since segmenting tasks into stages clarifies the required motions for each stage, the reward-shaping process becomes straightforward. An example of the stages and definitions of the reward and cost functions for the back-flip task are provided in Fig. 3 and Table I, respectively. Using this example, we provide an overview of how the stages are segmented and how the reward and cost functions are defined.\n1) Stage Transitions: As shown in Fig. 1, to perform a back-flip, the robot should remain in the standby mode until a command is received. Once the command is inputted, the robot sits down slightly, then jumps to rotate in the air, and finally lands. Through this insight, the task can be divided into five stages named Stand-Sit-Jump-Air-Land.\n2) Reward and Cost Functions: In the stand, sit, and land stages, the robot is required to remain stationary; therefore, the base velocity reward is defined as the negative of the current velocity. Conversely, in the jump and air stages, where the robot should rotate backward, the velocity reward is set to the y-directional angular velocity. Also, to ensure sufficient jump height during these stages, the height reward is set to the base height. In order to prevent tilting during the jump and air stages, the balance reward is set to remain the angle between the y-axis of the base and the z-axis of the world perpendicular. In the other stages, the reward is set to minimize the angle between the z-axis of the base and the world frames to maintain the robot upright. The energy and style rewards are used as regularization to ensure natural motions. The body contact cost prevents the robot from falling over, while costs associated with joint position, velocity, and torque are implemented to limit those values within their respective ranges. The foot contact cost is defined specifically for jumping with the rear legs; the cost is incurred if the rear legs detach before the front legs."}, {"title": "B. CMORL Policy Update", "content": "In this section, we introduce a policy update rule, termed constrained multi-objective PPO (CoMOPPO), designed to maximize multiple objectives while satisfying constraints. According to Kim et al. [14], convergence to a CP optimal policy can be achieved by aggregating the advantage functions of rewards and costs through a weighted summation, where the weights satisfy specific conditions, and updating the policy using TRPO [33] with the aggregated advantage. It can be written as follows:\n$\\pi_{t+1} = \\arg\\max_{\\pi} E_{\\tau \\sim \\pi_t} [\\frac{\\pi(a|s)}{\\pi_t(a|s)} A^\\pi_t(s, a)]$\ns.t. $E_{\\tau \\sim \\pi_t} [D_{KL}(\\pi_t(\u00b7|s)||\\pi(\u00b7|s))] \\leq \\epsilon$, (2)\nwhere $A^\\pi_t(s, a) := \\nu^\\pi_t \\sum_i A^{R_i}(s, a) - \\sum_j \\lambda^\\pi_t A^{C_j}(s, a)$, $\\epsilon$ is a trust region size, and $D_{KL}$ is the KL divergence. For the condition on the weights, $\\nu$ and $\\lambda$, please refer to Theorem 4.2 in [14].\nIn order to properly determine the weights, $\\nu$ and $\\lambda$, we use 1) reward normalization and 2) standard deviation of advantage functions. First, in the CMORL setting, each reward function operates at a different scale, making it essential to adjust them to a consistent level. To this end, we apply reward normalization for each reward and stage, and train the value functions using the normalized rewards. This approach automatically adjusts the ratio of each objective to a consistent level. Next, it is important to maintain consistency not only in the scale of the rewards but also in the ratio between objectives and constraints. Without a consistent ratio, the policy may be updated to over-maximizing objectives rather than satisfying constraints, potentially destabilizing the training process. To resolve this, we normalize the reward and cost advantages by their respective standard deviations, ensuring the policy to be updated with a consistent ratio of objectives to constraints. As a result, the proposed rule for advantage aggregation as follows:\n$\\tilde{A}^\\pi_t = \\frac{A^{R_i}}{\\text{Std}[A^{R_i}]} + \\eta \\sum_{j=1}^M \\frac{A^{C_j}}{\\text{Std}[A^{C_j}]} 1(J_{C_j}(\\pi) > d_j)$, (3)\nwhere Std denotes the standard deviation, $A^R := \\sum_i w_i A^{R_i}$ with a given preference $w$, $A^{R_i}$ is the advantage function calculated from the normalized rewards, and the hyper-parameter $\\eta$ serves as the ratio of constraints, as done in [16]. With the aggregated advantage functions, the policy can be updated using (2). However, to simplify the implementation, we apply a PPO update, which is formulated as follows [15]:\n$\\pi_{t+1} = \\arg\\max_{\\pi} E [\\min(r_t \\hat{A}^\\pi_t, \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon) \\hat{A}^\\pi_t)]$,\nwhere $r_t := \\frac{\\pi(a|s)}{\\pi_t(a|s)}$, and $\\epsilon$ is a hyper-parameter."}, {"title": "C. Sim-to-Real Techniques", "content": "In order to deploy policies trained in simulation to real-world environments, we use two widely-used sim-to-real"}, {"title": "V. EXPERIMENTS", "content": "This section describes the tasks and their corresponding results for both simulation and real-world environments. Details on the motions generated by the trained policies, along with the corresponding reward and cost functions for each task, can be found in the attached video."}, {"title": "A. Environmental Setup", "content": "We use the Isaac Gym simulator [28] due to its effectiveness in sim-to-real transfer and its flexibility in creating a wide range of tasks across various robotic platforms. In simulation experiments, we employ two types of robots from Unitree Robotics: Gol, a quadrupedal robot, and H1, a humanoid [35]. The quadrupedal robot comprises 23 body links and 12 motors, while the humanoid robot features 20 body links and 19 motors. In real-world experiments, we deploy the quadruped robot, Gol, through sim-to-real techniques as discussed in Section IV-C.\nThe state representation includes body orientation, joint positions and velocities, commands, as well as the previous action. For the teacher policy, privileged information\u2014such as linear and angular velocities, height, and foot contact, which are difficult to access in the real-world but available in simulation\u2014is additionally used."}, {"title": "B. Task Details", "content": "We have designed four acrobatic tasks: back-flip, side-flip, side-roll, and two-hand walk. The back-flip and two-hand walk tasks are implemented on both types of robots, while the side-roll and side-flip tasks are designated for the quadruped. Snapshots of each task are shown in Fig. 1 and Fig. 4, and the following are descriptions of each task.\n1) Back-Flip: The robot jumps backward into the air, rotates 360 degrees without touching the ground, and lands on its feet in its initial pose. The stage transition, along with the reward and cost functions, is discussed in Section IV-A.\n2) Side-Flip: This task is similar to the back-flip, but the robot jumps to the right side instead of backward. The stage transitions remain identical to those of the back-flip.\n3) Side-Roll: The robot performs a full roll along its right side, returning to its original pose upon completion. This task is segmented into five stages: Stand, where the robot remains upright; Sit, where the robot lowers itself to prepare for the roll; Half-roll, where the robot lies on its back; Full-roll, where the robot completes the roll; and Recover, where the robot returns to its default pose and orientation.\n4) Two-Hand Walk: The robot performs walking using its hands or front legs, while maintaining balance. This task is divided into three stages: Stand, where the robot maintains its default pose; Tilt, where the robot lowers its front legs or places its hands on the ground to prepare for standing; and Walk, where the robot walks using only its two hands."}, {"title": "C. Results", "content": "As illustrated in Fig. 1 and Fig. 4, the robots were able to successfully execute the tasks in both simulation and real-world. In simulation, the robot precisely performed the required maneuvers, returning to its original pose without losing stability during the side-roll and flip tasks. As shown"}, {"title": "D. Ablation Study", "content": "To demonstrate the efficacy of CMORL, we compared the proposed method with existing RL and constrained RL algorithms in the Gol back-flip task. In the case of RL, we employed PPO [15], where a single reward function is defined by weight-summing the multiple reward and cost functions, and the weights are obtained from the average ratios of objectives and constraints calculated during the training of COMOPPO. For constrained RL, we utilized penalized PPO (P3O) [16], where a single reward function is defined by summing the multiple rewards with the same weights used in PPO, and the constraints are the same as in COMOPPO.\nAs illustrated in Fig. 7, CoMOPPO was the only algorithm that successfully complete the task. In contrast to COMOPPO, which transitions quickly to the Air stage after brief Sit and Jump stages, the other two algorithms remained primarily in the Sit stage, indicating that they were unable to properly execute the jump motion."}, {"title": "VI. CONCLUSIONS", "content": "In this work, we have proposed an RL method that defines reward and cost functions in a stage-wise manner within the CMORL framework [14]. Additionally, we have developed a practical CMORL algorithm by expanding PPO [15] to handle multiple objectives and constraints. The proposed method has successfully demonstrated acrobatic tasks in both simulation and real-world settings. Moreover, by comparing the proposed method with existing RL and constrained RL algorithms, we have shown the necessity of the CMORL framework. While in this work, tasks are manually segmented into stages, future research could investigate more efficient techniques for automatically dividing complex tasks into stages."}]}