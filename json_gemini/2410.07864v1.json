{"title": "RDT-1B: A DIFFUSION FOUNDATION MODEL FOR BIMANUAL MANIPULATION", "authors": ["Songming Liu", "Lingxuan Wu", "Bangguo Li", "Hengkai Tan", "Huayu Chen", "Zhengyi Wang", "Ke Xu", "Hang Su", "Jun Zhu"], "abstract": "Bimanual manipulation is essential in robotics, yet developing foundation models\nis extremely challenging due to the inherent complexity of coordinating two robot\narms (leading to multi-modal action distributions) and the scarcity of training\ndata. In this paper, we present the Robotics Diffusion Transformer (RDT), a\npioneering diffusion foundation model for bimanual manipulation. RDT builds on\ndiffusion models to effectively represent multi-modality, with innovative designs\nof a scalable Transformer to deal with the heterogeneity of multi-modal inputs\nand to capture the nonlinearity and high frequency of robotic data. To address\ndata scarcity, we further introduce a Physically Interpretable Unified Action Space,\nwhich can unify the action representations of various robots while preserving the\nphysical meanings of original actions, facilitating learning transferrable physical\nknowledge. With these designs, we managed to pre-train RDT on the largest\ncollection of multi-robot datasets to date and scaled it up to 1.2B parameters,\nwhich is the largest diffusion-based foundation model for robotic manipulation.\nWe finally fine-tuned RDT on a self-created multi-task bimanual dataset with over\n6K+ episodes to refine its manipulation capabilities. Experiments on real robots\ndemonstrate that RDT significantly outperforms existing methods. It exhibits zero-\nshot generalization to unseen objects and scenes, understands and follows language\ninstructions, learns new skills with just 1~5 demonstrations, and effectively handles\ncomplex, dexterous tasks. We refer to the project page for the code and videos.", "sections": [{"title": "1 INTRODUCTION", "content": "Bimanual manipulation is essential for robots to accomplish real-world tasks (Edsinger & Kemp,\n2007). For practical applications, a useful manipulation policy should be able to generalize to\nunseen scenarios, such as unseen objects and scenes. However, current approaches either depend on\ntask-specific primitives (Mirrazavi Salehian et al., 2017; Rakita et al., 2019; Grannen et al., 2023a)\nor are limited to small-scale model, data and simple tasks (Krebs et al., 2021; Franzese et al., 2023;\nGrannen et al., 2023b; Zhao et al., 2023; Grotz et al., 2024; Liu et al., 2024), thereby exhibiting\nonly narrow generalization and failing in complex tasks. Following the success in natural language\nprocessing (Achiam et al., 2023; Touvron et al., 2023) and computer vision (Radford et al., 2021;\nKirillov et al., 2023), one promising direction to enable generalizable behaviors is to develop a\nfoundation model through imitation learning on large-scale datasets.\nHowever, it is highly non-trivial to develop a bimanual manipulation foundation model. One main rea-\nson is that the accessible data for a specific dual-arm robot is significantly scarce (Sharma et al., 2018;\nCollaboration et al., 2023) due to high hardware costs, undermining the data-intensive requirements\nof training foundational models. Inspired by recent attempts in unimanual manipulation (Brohan et al.,\n2023; Kim et al., 2024), we seek to first pre-train on extensive multi-robot datasets and then fine-tune\non the small dataset collected on the target dual-arm robot. This can help us to scale the data size\nup to three orders of magnitude, having the potential to learn transferrable physics knowledge from\ndatasets of other robots. Nevertheless, there are two key technical challenges. First, a generalizable\nfoundation model requires a highly capable architecture in terms of both expressiveness and scalabilty.\nThe dimension of the action space in bimanual manipulation is twice that in unimanual manipulation,"}, {"title": "2 RELATED WORK", "content": "Learning-based Bimanual Manipulation. One substantial challenge in learning a bimanual ma-\nnipulation policy is the high dimensionality of the action space, which exacerbates the data scar-\ncity (Zollner et al., 2004; Smith et al., 2012; Lioutikov et al., 2016; Stepputtis et al., 2022) and the\nmulti-modal behavior (Colom\u00e9 & Torras, 2018; 2020; Figueroa & Billard, 2017; Sharma et al., 2018;\nXie et al., 2020; Franzese et al., 2023). Some works have developed more cost-effective interfaces\nfor data collection (Zhao et al., 2023; Aldaco et al., 2024), but they are limited to specific hardware\nconfigurations and still insufficient to bridge the data gap for a generalizable policy. Others attempt\nto reduce data requirements by introducing inductive biases, such as distinguishing two arms for\nstabilization and functionality (Grannen et al., 2023b), parameterizing movement primitives (Batinica\net al., 2017; Amadio et al., 2019; Chitnis et al., 2020; Franzese et al., 2023), or using voxel represen-\ntations (Grotz et al., 2024; Liu et al., 2024). These methods use strong priors or simplified modeling,\nwhich successfully reduce the action space, but at the cost of a reduced scope of application and\nfailing to express the multi-modality of bimanual behaviors (Pearce et al., 2023).\nFoundation Models for Robotics. Foundation models have shown immense promise in enabling\ngeneralizable behaviors by training multi-task \u201cgeneralist\u201d models (Brohan et al., 2022; 2023; Ghosh\net al., 2023; Kim et al., 2024) on large multi-task robot datasets (Collaboration et al., 2023; Brohan\net al., 2022; Fang et al., 2023). Most studies adapt large vision-language models to directly predict\naction (Brohan et al., 2022; Driess et al., 2023; Brohan et al., 2023; Collaboration et al., 2023; Kim\net al., 2024). While demonstrating generalization to new objects and tasks, they face issues with\nquantization errors and uncoordinated behaviors (Pearce et al., 2023) when applied to bimanual\nmanipulation, largely due to their discretization of action spaces. To enhance precision, diffusion\nmodels have been used for continuous control (Ho et al., 2020; Chi et al., 2023; Pearce et al., 2023;\nGhosh et al., 2023). Ghosh et al. (2023) pre-train a Transformer-based diffusion policy on a subset of\nOpen X-Embodiment (Collaboration et al., 2023) dataset (25 datasets), with up to 93M parameters."}, {"title": "3 PROBLEM FORMULATION AND CHALLENGES", "content": "We start by formulating the task and elaborating on the challenges. To evaluate the model on\nthe hardware, we choose the ALOHA dual-arm robot as our target robot since it is one of the\nmost representative dual-arm robots and is suitable for collecting human demonstration data via\nteleoperation (Zhao et al., 2023; Fu et al., 2024; Aldaco et al., 2024). Fig. 2a shows a schematic\ndiagram of the target robot, which consists of two arms with grippers and three cameras. Note that\nour setting and foundation model are generic to any dual-arm gripper robot.\nWe consider the concrete task of language-conditioned bimanual manipulation with vision, which is\nfundamental in robotics and has great value in real-world scenarios such as household (Stepputtis\net al., 2020; Brohan et al., 2022; Zhao et al., 2023). Formally, given a language instruction $l$, the\npolicy is presented with an observation $o_t$ at time $t \\in \\mathbb{N}^+$; and then it produces an action $a_t$ to\ncontrol the two robot arms to achieve the goal specified by $l$. The observation is represented as"}, {"title": "4 ROBOTICS DIFFUSION TRANSFORMER", "content": "We now present Robotics Diffusion Transformer (RDT), as illustrated in Fig. 3. In Sec. 4.1, we\npresent the diffusion model and the corresponding architecture to address Challenge 1. In Sec. 4.2,\nwe resolve Challenge 2 by proposing a physically interpretable unified action space to unify various\nrobot action spaces and enable multi-robot pre-training. We also collect a comprehensive multi-task\nbimanual dataset for fine-tuning to improve the bimanual manipulation capabilities of RDT."}, {"title": "4.1 RDT MODEL", "content": "Diffusion Modeling. Due to multi-modality, given the language instruction $l$ and observation $o_t$,\nthere may be many possible actions $a_t$ to proceed with the task. The policy will learn the \u201caverage\u201d\nof action modes if we model it as a deterministic mapping $(l, o_t) \\rightarrow a_t$ and regress the tuples of\n$(l, o_t, a_t)$ in the training data. This may result in out-of-distribution actions, such as the arithmetic\nmean of multiple modes, which can be completely infeasible (Pearce et al., 2023). Instead, we choose\nto model the continuous conditional distribution $p(a_t|l, o_t)$. As discussed in Section 2, among\nvarious approaches, diffusion models excel in both expressiveness and sampling quality, but can be\nslow to sample a high-dimensional data (e.g., images). Luckily, for our settings, the drawback is\nminor since that $a_t$ has a much lower dimension than images, which requires only minimal sampling\noverhead. This has made diffusion models an ideal choice for policy as in Chi et al. (2023).\nNevertheless, employing diffusion models for robotic tasks faces unique challenges since the in-\nherent properties of robotic physics quantities (i.e., the action and proprioception) are different\nfrom image/video data. Image and video data, while high-dimensional, often exhibit a degree of\ntemporal and spatial continuity (Chen et al., 2019; Liang et al., 2022), with changes between frames\ntypically being incremental. In contrast, robotic physics quantities are characterized by its nonlinear\ndynamics (de Wit et al., 2012) and the potential for high-frequency changes stemming from the\nphysical interactions, such as collision, constraints, and material properties like damping. Moreover,\nthe quantities also feature an unstable numerical range, probably due to extreme values caused by\nunreliable sensors. This underscores the necessity of adapting current diffusion models to effectively\ncapture the instability and nonlinearity of robot data. Next, we will first elaborate on diffusion\nformulation and then introduce our design of architecture to resolve these challenges.\nWhen making a decision with diffusion policies, we first sample a totally noisy action $a_t^K \\sim \\mathcal{N}(0, I)$ and then perform $K \\in \\mathbb{N}^+$ denoising steps to denoise it to a clean action sample $a_t^0$ from $p(a_t|l, o_t)$:\n$a_t^{k-1} = \\frac{\\sqrt{\\bar{\\alpha}_{k-1}} \\left(a_t^k - \\sqrt{1-\\bar{\\alpha}_k} f_\\theta(l, o_t, a_t^k, k)\\right)}{1-\\bar{\\alpha}_k} + \\frac{\\sqrt{\\alpha_k} \\left(1 - \\bar{\\alpha}_{k-1}\\right)}{1-\\bar{\\alpha}_k} z, k = K, ..., 1,$\nwhere ${\\{\\alpha_k\\}_{k=1}^K, \\{\\sigma_k\\}_{k=1}^K}$ are scalar coefficients pre-defined by a noise schedule (Nichol & Dhariwal,\n2021). Here, $\\beta_k := 1 - \\alpha_k$, and $\\bar{\\alpha}_{k-1} := \\prod_{i=1}^{k-1} \\alpha_i$, $z \\sim \\mathcal{N}(0, I)$ if $k > 1$, else $\\bar{\\alpha}_{k-1} = 1$, $z = 0$.\nHowever, $a_t^0$ is intractable before sampling is finished. We opt to use a learnable denoising network\n$f_\\theta$ with parameters $\\theta$ to estimate the clean sample from a noisy one: $a_t^0 \\leftarrow f_\\theta(l, o_t, a_t^k, k)$. To train\nsuch a network, we will minimize the following mean-squared error (MSE) of denoising:\n$\\mathcal{L}(\\theta) := \\text{MSE} \\left(a_t, f_\\theta(l, o_t, \\sqrt{\\bar{\\alpha}_k} a_t + \\sqrt{1-\\bar{\\alpha}_k} \\epsilon, k)\\right),$ \nwhere $k \\sim \\text{Uniform}(\\{1, ..., K\\})$, $\\epsilon \\sim \\mathcal{N}(0, I)$, and $(l, o_t, a_t)$ is sampled from our training dataset.\nLater in this paper, we will denote noisy action inputs by $\\bar{a}_t := \\sqrt{\\bar{\\alpha}_k} a_t + \\sqrt{1 - \\bar{\\alpha}_k} \\epsilon$, in which the\nsuperscript of $k$ is dropped for simplicity. Besides, in practice, we prefer to predict a sequence of\nactions, i.e., an action chunk, in one shot to encourage temporal consistency (Chi et al., 2023) and to\nalleviate error accumulation over time by reducing number of decisions in a task (Zhao et al., 2023).\nSpecifically, we model $p(a_{t:t+T_a}|l, o_t)$, where $a_{t:t+T_a} := (a_t,..., a_{t+T_a-1})$ is an action chunk and\n$T_a$ denotes the chunk size (Zhao et al., 2023). We provide a detailed discussion in App. A."}, {"title": "5 EXPERIMENTS", "content": "We aim to answer the following questions through real-robot experiments: Q1: Can RDT zero-shot\ngeneralize to unseen objects and scenes? Q2: How effective is RDT's zero-shot instruction-following\ncapability for unseen modalities? Q3: Can RDT facilitate few-shot learning for previously unseen\nskills? Q4: Is RDT capable of completing tasks that require delicate operations? and Q5: Are large\nmodel sizes, extensive data, and diffusion modeling helpful for RDT's performance?"}, {"title": "5.1 EXPERIMENT SETUPS", "content": "Tasks. We select 7 challenging tasks to evaluate the generalizability and capabilities of RDT from\ndifferent dimensions, including complex scenarios that the model may encounter in real-world tasks,\nsuch as various unseen elements and dexterous manipulation. An illustration of the dimension of\neach task is given in Table 1 while detailed definitions and visualizations are provided in Fig. 5."}, {"title": "6 CONCLUSION", "content": "In this paper, we tackled the challenges of data scarcity and increased manipulation complexity in\ngeneralizable bimanual manipulation by developing the Robotics Diffusion Transformer (RDT), a\ndiffusion-based foundation model for language-conditioned visuomotor imitation learning. Our model\nwas pre-trained on an extensive multi-robot dataset and fine-tuned on a self-collected bimanual dataset.\nWe further introduce a Physically Interpretable Unified Action Space to unify action representations\nacross different robots, enhancing robustness and transferability. Outperforming existing methods,\nRDT not only demonstrates significant improvements in dexterous bimanual capability and instruction\nfollowing but also achieves remarkable performance in few-shot learning and zero-shot generalization\nto unseen objects and scenes."}, {"title": "A ACTION CHUNKING TECHNIQUE", "content": "In practice, we find that the errors in action prediction accumulate as the number of historical decisions\nincreases due to the imperfection of the learned policy. This may cause the robot to drift out of the\ntraining distribution, reaching hard-to-recover states (Ross et al., 2011). To alleviate this, we prefer to\npredict multiple actions in one shot, thereby reducing the total number of decisions in a trajectory. In\nthis way, we model $p(a_{t:t+T_a}|l, o_t)$, where $a_{t:t+T_a} := (a_t,..., a_{t+T_a-1})$ is an action chunk and $T_a$\ndenotes the chunk size (Zhao et al., 2023). To adapt Eq. 1 and Eq. 2 to this context, we could simply\nreplace $a_t$ by $a_{t:t+T_a}$. Besides, according to Chi et al. (2023), action chunking is also helpful for\nimproving temporal consistency. It can better consider the coherence of previous and subsequent\nactions when making decisions and may avoid sudden changes in actions that may cause damage to\nthe robot."}, {"title": "B ARCHITECTURE DETAILS", "content": "Encoding of Multi-Modal Inputs. Encoding details are outlined below:\n\\begin{itemize}\n    \\item Low-Dimensional Inputs. The proprioception $z_t$ and the noisy action chunk $\\bar{a}_{t:t+T_a}$ are first\n    embedded into the unified action space. This space is used to unify the representation of $z_t$ and\n    $\\bar{a}_{t:t+T_a}$ across various robots, which is elaborated in Sec. 4.2. Then, they are encoded into the\n    token space by a shared MLP since they have similar physical meanings. Such continuous encoding\n    can avoid precision loss in contrast to discretized encoding (Brohan et al., 2022; 2023; Kim et al.,\n    2024). For frequency $c$ as well as the diffusion time step $k$, we encode them into the token space\n    through two MLPs, respectively. Afterward, all of them are concatenated together in the length\n    direction to achieve in-context conditioning (Peebles & Xie, 2023; Bao et al., 2023), resulting in\n    an input token sequence of length $1 + T_a + 1 + 1$. Finally, position embeddings are added to\n    distinguish different modalities and to inject temporal information in $\\bar{a}_{t:t+T_a}$.\n    \\item Image Inputs. We encode the RGB images by a frozen SigLIP (Zhai et al., 2023) and utilize\n    an additional MLP to project the output to the token space. To enhance the model's ability to\n    distinguish images based on viewpoint and time steps, we extend traditional sinusoidal positional\n    embeddings to multi-dimensional grids, as shown on the right side of Fig. 3. This modification\n    integrates spatial-temporal information, enabling the model to capture the relationships between\n    input images. Specifically, we adopt the implementation by Liu et al. (2022), employing grid\n    dimensions of $(T_{img}, N_{cam}, N_{patch}, D)$. Here, $N_{cam}$ represents the number of cameras, set to three\n    in our configuration, and $N_{patch}$ indicates the number of patches into which each image is divided\n    by the ViT-based Image Encoder and $D$ denotes the embedding dimension.\n    \\item Language Inputs. Language instruction is encoded by a frozen T5-XXL (Raffel et al., 2020), and\n    an MLP is used to project the output to the token space. When calculating attention for language\n    tokens, we apply the language attention mask to mask out the pad tokens appended during batching.\n\\end{itemize}\nDuring training, each input from various modalities is independently masked with a probability of\n10%."}, {"title": "C PHYSICALLY INTERPRETABLE UNIFIED ACTION SPACE", "content": "As mentioned in Sec. 4.2, we embed the actions of various robots into one unified space that includes\nall the main physical quantities of robots. This unified action space has a dimensionality of 128.\nTable 4 describes each element of the vector in this unified action space. For a specific robot, each"}]}