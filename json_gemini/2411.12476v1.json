{"title": "Comparing Prior and Learned Time Representations in Transformer Models of Timeseries", "authors": ["Natalia Koliou", "Tatiana Boura", "Stasinos Konstantopoulos", "George Meramveliotakis", "George Kosmadakis"], "abstract": "What sets timeseries analysis apart from other machine learning exercises is that time representation becomes a primary aspect of the experiment setup, as it must adequately represent the temporal relations that are relevant for the application at hand. In the work described here we study wo different variations of the Transformer architecture: one where we use the fixed time representation proposed in the literature and one where the time representation is learned from the data. Our experiments use data from predicting the energy output of solar panels, a task that exhibits known periodicities (daily and seasonal) that is straight-forward to encode in the fixed time representation. Our results indicate that even in an experiment where the phenomenon is well-understood, it is difficult to encode prior knowledge due to side-effects that are difficult to mitigate. We conclude that research work is needed to work the human into the learning loop in ways that improve the robustness and trust-worthiness of the network.", "sections": [{"title": "1 INTRODUCTION", "content": "What sets apart timeseries analysis from other machine learning exercises is taking into account the sequence as well as, in most cases, the temporal distance between observations. This makes the representation of time a primary aspect of the experiment setup, as it must be adequate for representing the temporal relations that are relevant for the application at hand.\nTo elaborate on the various considerations that need to be addressed, first consider that one cannot assume fully observed, uniformly sampled inputs as there might be gaps in the data, varying sampling rates, and (for multivariate timeseries) misalignment between the time steps of the different variables. This dictates a representation that allows time differences to be computed, so that (for example) September 2023 is 'closer' to January 2024 than it is to September 2022. Simple timestamps allow this but do not capture periodicity: Consider, for instance, an application with seasonal periodicity where September 2023 is 'closer' to September 2022 than to January 2024.\nThere is a rich relevant literature in both signal processing and in non-parametric statistics, as well as in adapting AI/ML approaches to timeseries processing when facing irregularly sampled and/or sparse data. In particular, deep learning approaches that utilize recurrent networks based on Gated Recurrent Units (GRUs) [1], Long Short-Term Memory networks (LSTMs) [2, 3], and ODE-RNNs [4] have shown promising results. In the work described we focus on deep learning methods as well, but specifically on adapting Transformer models to timeseries analysis. We will first present how the relevant literature handles the representation of time when applying Transformer models to timeseries (Section 2) and then proceed to propose an alternative representation that is expected to out-perform the original representation for our specific application on predicting the energy output of solar panels (Section 3). We close with giving and discussing comparative experimental results (Section 4) and conclusions and future work (Section 5)."}, {"title": "2 BACKGROUND", "content": "Unlike recurrent and differential equation-based architectures which process inputs sequentially, Transformers [6] expect the complete time-series as input and use the attention mechanism to look for relationships between all inputs simultaneously. This has the side-effect that the temporal order is no longer implied by the order in which the inputs are presented to the network, so that input vectors must be augmented with features that represent time. But this also creates the opportunity to use time embeddings that represent temporal information in a way that encodes prior knowledge about the data.\nThe most characteristic example is periodicity. When the data is known or suspected to exhibit periodicity, absolute positional encoding [7] encodes time as two features: the sine and the cosine"}, {"title": "3 REPRESENTING TIME", "content": "In order to experiment with time representations and the effect they have on the quality of the learned model, we have assumed an application where we need to predict the energy output of solar thermal collectors based on external conditions. More detailed information about the application and the data is provided in Section 4, and for the purposes of the current discussion it suffices to mention that the input variables are solar radiation and external temperature, which exhibit the obvious daily and annual periodicities.\nWe have noted however, that solar radiation behaves in a way that is not captured by sinusoidal functions. While these functions represent well the similarity between the same time on a different day, as well as the similarity between early morning and late afternoon, they fail to represent the fact that the whole of the night-time is the same and (as far as solar radiation is concerned) it makes no difference if the time is 10pm or 24am. We have, therefore, thought"}, {"title": "3.1 Triangular Pulse & Linear", "content": "The first embedding is a season-modulated triangular pulse/linear pair. The base and peak of the triangular pulse is not the same for each day, but is calculated so that the pulse will start at sunrise, peak at noon, and end at sunset. This represents that solar radiation at noon is distinct from all other times during day, one hour before noon is similar to one hour after noon, and so on until sunrise/sunset. Outside the base of the pulse, all times are represented by the same value of 0.01 to denote that the distinction between them is not important.\nAs for the linear function, it provides a straightforward method for ensuring the uniqueness of timestamps within the representation. By mapping each timestamp to a unique value, we establish clear distinctions between different time points. Unlike the periodicity of the triangular pulse, the linear function spans the entire range of timestamps, from the earliest to the latest, using both date and time components. This continuous representation does not reset daily but maps the entire period linearly, ensuring a unique value for each timestamp based on its position in the overall time span."}, {"title": "3.2 Sinusoidal", "content": "The third embedding follows the absolute positional encoding literature and consists of two sine/cosine pairs, one pair computed from the month of the year and one pair computed from the hour of the day. Sine and cosine capture cyclical patterns effectively, and by combining the hour and the month embedding we guarantee that the Transformer has the means to model daily and annual periodicities and also to refer to absolute timepoints if the data prove this useful.\nTo find the appropriate phase shift and period, we argue as follows: To represent the daily cycle we need our sine wave to peak at noon. We aim for noon to be distinct, while the remaining timestamps should exhibit symmetrical correlation. To achieve this, observe how a period of 12 for hours and 24 for months, with a zero shift, will work as expected. Naturally, the cosine must be parameterized with identical phase and period to maintain the property that the sine/cosine pair uniquely refers to a point in the trigonometric circle."}, {"title": "3.3 Sine & Sawtooth", "content": "The fourth and final embedding consists of a sine and a sawtooth function. Just like in the sinusoidal embedding, we also use the sine wave with the same parameters (shift, period) to express correlation among timestamps. However, when it comes to expressing uniqueness, we considered using a sawtooth wave instead of a cosine wave, to observe whether any noticeable changes might take place. Unlike the linear function used in the triangular pulse embeddings which spans the entire range of timestamps, the sawtooth wave resets to 0 output after each day. The sawtooth wave parameters (shift, period) are set to (6, 6) for hours and (12, 12) for months to scale the output values for both cases within [-1, 1]."}, {"title": "3.4 Learned Time Representations", "content": "As mentioned in the Background, the idea behind the mTAN is to learn periodic features with a sinusoidal representation. We have argued that, for our application, a triangular pulse for representing the non-linear features may be a better fit. Thus, we altered the activation function of the presented model to convert the linear layer to a triangular pulse.\nOur first attempt at creating such a function involved using a triangular function with a fixed base. In this case, the start and end of the pulse corresponded to the hours when difference of the solar output peaked (7 AM and 9 PM respectively). This approach was not fruitful, as the model was not given the opportunity to learn the periodic representations.\nNaturally, we then focused on learning the base of the pulse. Choosing 1 PM as the peak of the pulse, the two most straightforward approaches we implemented were: (a) splitting the time vector and then using the hours where the absolute difference of the elements in the learned time vector is the largest as the start and end of the pulse; and (b) splitting the time vector and then using the hours where the elements themselves in the learned time vector have the absolute largest values as the start and end of the pulse. However, both implementations failed to learn any meaningful form of pulse and remained fixed on the initial time parameters.\nThe approach that successfully achieved the task of learning different triangular pulses was an engineering one. It emerged from the idea that the non-linear function itself should be simple in terms of numerical computation and traceability, since the previous, more complex approaches were not suitable for the task. The idea is described as follows: we first calculate the absolute difference of each value in the time vector of each representation of size d and the value of the 13th element of this representation (corresponding to 1 PM). Then we compute the 25th percentile of these differences and replace each difference distij with zero if it is less than the percentile value $v_i$ of the corresponding representation i, and with 1 - $\\frac{dist_{ij}}{v_i}$ otherwise."}, {"title": "4 EXPERIMENTS AND RESULTS", "content": "Our data was collected from a pilot building at NCSR 'Demokritos'. The pilot building features solar thermal collectors used to experiment with how to most efficiently control heating systems (solar, heat pump, etc.) to achieve satisfactory space heating, satisfy hot water demand, and minimize electric power consumption. The machine learning task associated with this application is to predict solar power production from variables reflecting external conditions, namely solar radiation and external temperature. The output of the model is not specific power in terms of kWh, but a label that characterizes power production as being in one of five classes, defined based on expected demand.\nBased on the above, the machine learning task is to transform a multi-variate timeseries of external conditions aggregated into one-hour intervals, into a timeseries of power production levels (the five-class labeling schema mentioned above). The dataset has a moderate amount of gaps due to the sensors occasionally giving erroneous,"}, {"title": "4.2 Classification results and discussion", "content": "To evaluate the performance of our models on the classification task, we compared both the PriorTime and mTAN models across different time representations. Our evaluation criteria included various performance metrics such as precision, recall, and F1-score.\nThe PriorTime model was trained and tested using four different time representations: triangular pulse & linear, fixed triangular pulse & linear, sine & cosine, and sine & sawtooth. Among these representations, the sine & cosine approach consistently demonstrated the strongest performance. The sine & sawtooth representation approached the results achieved by sine & cosine; however, it only outperformed once, and the difference was not significant. Concerning the two triangular pulse/linear representations, they exhibited the weakest performance overall, with the fixed variation performing slightly worse than the season-modulated one. This was not surprising, as we expected the more informed time representation to give better results.\nThe second point is that the sine representations consistently outperform the triangular ones in both settings (prior and learned), despite the fact that the triangular pulse is more precise when it comes to making similar times have similar inputs (solar radiation and temperature), as already discussed in the beginning of Section 3. We believe that this is due to the fact that the identical close-to-zero values outside the base of the pulse hinder back-propagating loss to the linear layer that feeds the sine/triangular activation function.\nWhen these two points are put together, they imply that (a) the first linear layer is capable of managing both seasonal variation and the daily periodicity by properly weighting the inputs of the activation functions; (b) (almost) zeroing-out inputs hinders back-propagation; and (c) having an activation function that dynamically changes to model the unique characteristics of each day individually enhances convergence."}, {"title": "4.3 Discussion of the Learned Time Representations", "content": "As noted in Section 2, we are bound by the model's architecture to learn n - 1 periodic and 1 non-periodic feature representations from our data, with n being the number of the final output points of the time series (in our case, 24).\nWhen subjecting the periodic time representation to be a triangular pulse, the model starts from very narrow pulses and then proceeds to learn a combination of those, as presented previously. On the other hand, when we define the periodicity as a sine function, the initial weight initialization (which sets the weights to very small values) makes the initial representations a very short part of the sinusoidal curve (so short that visually appears as linear in our graphs). The model then learns to scale the time vectors to model more meaningful time representations. An example of this behavior is presented in Figure 4.\nHowever, one might wonder: are all of the 23 learned time representations meaningful? Since it is hard to track the influence of every representation on the classification task, we can only speculate.\nTo model the time points for each example, we normalized each timestamp using the Unix timestamps from our data's date range. The resulting time points range from 0 to 1, but within a daily sample, the hourly time points vary slightly. The Transformer's architecture utilizes the key/value/query formulation [6]. The query is the information that is being looked for, the key is the context or reference, and the value is the content that is being searched. In the context of mTAN, the initial keys are the time points and the initial queries are vectors with values equally distributed from 0 to 1. Both of these terms are then passed through the same learnable time layers. Since, initially, the keys and the queries have a different order of magnitude it is expected that either the model will increase the magnitude of the keys or decrease the magnitude of the queries. Through our experiments we discovered that the former occurs.\nTo further evaluate our claim, we modified the time point creation so that both queries and keys have the same values. The main reason we opted for normalizing the time steps using UNIX time was that it slightly performed better for the classification task. We believe that this is the case because when normalizing the time points considering only the hour, we lose any other characteristic that relates to the progression of time. This further corroborates that (as we extensively stressed earlier) the non-periodical features are crucial for our application."}, {"title": "5 CONCLUSIONS", "content": "We studied the impact of time embeddings when applying Transformer models to timeseries analysis, and specifically the impact of using a very informed, application-specific time representation, a generic sinusoidal representation known to capture periodic phenomena well, and a method for learning the parameters that best fit either of these two to the specific dataset at hand.\nOur experiments concluded that Transformers (and DNNs in general we could argue) are not very amenable to over-engineering the time representation due to side-effects that are difficult to mitigate. This can be seen as a negative quality in applications such as ours where there is extensive prior knowledge on a well-studied phenomenon, but can also be construed as a positive quality as the network was able 'discover' the knowledge we were trying to convey. Especially the comparison between the sinusoidal prior and learned representations is very promising in this respect, as the maximally-flexible mTAN network recovered almost all the accuracy of the prior-time sinusoidal network.\nA second level of analysis delved into the nature of the features that mTAN learned. This analysis has demonstrated the ability of the linear layer - sinusoidal activation function architecture to very closely approximate the clearly non-smooth behavior.\nIt should, however, be noted that our analysis was restricted to what we could indirectly observe by trying out different parameterizations and speculating based on our understanding of how the network is trained. Besides any general advancements in methodologies for explaining neural networks, this also showed us a path for future work specifically targeting mTAN. Since one of the hindrances was that we were unable to observe the effect of each feature due the distributed nature of the classifier, we would have liked to systematically explore the effect of incrementally increasing the number of features and observing which features gets learned (interpreted as, is the most impactful for reducing loss), which is learned second and so on. However, the linear algebra behind mTAN ties the dimensionality of the feature representation to the dimensionality of the input, as we cannot de-couple the encoding of periodicity and the encoding of linear time. Since (as argued above) linear time is critical for performance, we are stuck with a pre-defined dimensionality for the sine features as well.\nOur envisaged future research is to re-work the linear algebra of the mTAN so that we can de-couple the encoding of the periodic and linear time representation, allowing to have only the latter be constrained by the dimensionality of the input. In the specific experiment presented here, this would translate to a better understanding of what happens when we inform the Transformer of the bias we want to apply regarding periodicities. Which, in its turn, is expected to lead to methods for affording the human operator intuitive and effective control of the network."}]}