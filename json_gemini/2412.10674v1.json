{"title": "USM: Unbiased Survey Modeling for Limiting Negative User Experiences in Recommendation Systems", "authors": ["Chenghui Yu", "Peiyi Li", "Haoze Wu", "Bingfeng Deng", "Hongyu Xiong"], "abstract": "Reducing negative user experiences is essential for the success of recommendation platforms. Exposing users to inappropriate content could not only adversely affect users' psychological well-beings, but also potentially drive users away from the platform, sabotaging the platform's long-term success. However, recommendation algorithms tend to weigh more heavily on positive feedback signals due to the scarcity of negative ones, which may result in the neglect of valuable negative user feedback. In this paper, we propose an approach aimed at limiting negative user experiences. Our method primarily relies on distributing in-feed surveys to the users, modeling the users' feedback collected from the survey, and integrating the model predictions into the recommendation system. In addition, we strive to resolve two problems typically encountered in modeling user survey feedback: (a) We address the Response Bias by applying a survey-submit model; (b) We address the Exposure Bias by adopting a pseudo label distillation training framework. The A/B testing results indicate a reduction in survey sexual rate and survey inappropriate rate, ranging from -1.44% to -3.9%. Additionally, we compared our methods against an online baseline that does not incorporate our approach. The results indicate that our approach significantly reduces the report rate and dislike rate by 1% to 2.27% compared to the baseline, confirming the effectiveness of our methods in enhancing user experience. After we launched the survey model based our approach on our platform, the model is able to bring reductions of 1.75%, 2.57%, 2.06% on reports, dislikes, survey inappropriate rate, respectively.", "sections": [{"title": "1 Introduction", "content": "Negative feedback signals are crucial to guardrail content recommendations and improve user experience. When these signals are effectively integrated into recommendation systems, they play a vital role in preventing the promotion of harmful or undesirable content, thereby contributing to a healthier online environment. However, the challenges associated with negative signals are noteworthy. Due to the limited visibility of options for users to express negative feedback, these signals are often sparse compared to positive signals. This imbalance can lead to a skewed understanding of user preferences, resulting in recommendations that prioritize short-term engagement over long-term satisfaction. Moreover, an over-reliance on positive signals can create a filter bubble, where users are continuously exposed to content that aligns with their immediate preferences but may not be beneficial in the long run. This scenario can ultimately lead to user attrition as audiences become disillusioned with the quality of the content provided. Additionally, existing user signals frequently fail to meet specific customized requirements, such as understanding the underlying reasons for a user's likes or dislikes regarding a video. This lack of granularity hinders our ability to tailor content recommendations effectively, as we cannot identify the particular attributes of content that resonate with individual users.\nIn this context, in-feed surveys can effectively address these limitations. By distributing surveys within users' for-you-feeds (FYFs), platforms can gather valuable insights into user opinions on specific issues. This approach enables platforms to gain a deeper understanding of user perceptions. We can utilize in-feed surveys for:\n\u2022 Monitoring content satisfaction and quality: This involves asking users about the quality and satisfaction of the recommended content, and using the survey results to help time-series monitoring and online A/B testing."}, {"title": "2 Related Work", "content": "Our work is closely related to detection of negative user experi-ences, personalization models and solving problems of data bias in recommendation system.\nNegative Feedbacks. The detection of negative user experiences in social media is a topic of significant interest, as evidenced by several studies analyzing these negative experiences [4, 11, 12]. To address the challenges of sparse and noisy negative feedback signals, certain models employ exposure variables [8] or popularity metrics [5] to effectively distill authentic negative signals from implicit feedback. Furthermore, some research [3, 18] focuses on developing personalized machine learning models aimed at predicting negative user experiences and applying these models within recommendation systems.\nBias & Debias.In recent years, there has been significant growth in research on recommendation systems. Most studies focus on developing machine learning models to better analyze user behavior data. However, this data is typically observational rather than experimental. As a result, biases can easily be introduced, which we refer to as data bias. The data collection process for recommendation systems is generally observational, not experimental. This means that sample selection and user decisions can be influenced by various undesirable factors, such as the exposure mechanisms of the recommendation systems or public opinions. Consequently, the distribution of training data can differ from the test data distribution. When the training data only captures a skewed snapshot of user preferences, the resulting recommendation model may yield suboptimal results. Therefore, data bias occurs when the distribution of the collected training data differs from the ideal test data distribution, exemplified by issues like Response Bias. [6, 10, 14] and Exposure Bias [1, 2, 9, 16, 19].\n\u2022 Response Bias. Response Bias, also known as Selection Bias, occurs when users have the freedom to choose which items to rate. As a result, the observed ratings do not represent a true sample of all available ratings. In other words, the missing rating data is often categorized as Missing Not At Random (MNAR). A prior study conducted by Marlin et al. [10] provides compelling evidence of the existence of"}, {"title": "3 Approaches", "content": "In this section, we will first outline the structure of the baseline in-feed survey model and its application in recommendation systems. To tackle the challenges of response bias and exposure bias in survey modeling, we will introduce two solutions: the development of a survey-submit model and the implementation of a pseudo label distillation framework."}, {"title": "3.1 In-feed Survey Model", "content": "Recommendation systems can be systematically divided into several stages: recall, pre-ranking, ranking, and re-ranking; as illustrated in Fig 2, later stages usually have a more direct impact on the final video recommendations. Currently, our survey model primarily"}, {"title": "3.2 Response Bias & Survey-Submit Model", "content": "After distributing the survey, we noticed varying levels of willing-ness among users to submit their responses. Some users are more active and interested in the questions, and after watching the video, they are more inclined to fill out the survey to provide feedback on"}, {"title": "3.2.1 Problem Formulation.", "content": "We analyze response bias from a math-ematical perspective, using the satisfaction survey as an example. This survey primarily includes three questions: \"I like it,\" \"Neither like nor dislike it,\" and \"I don't like it.\" When a user encounters a specific item, the probability formula for predicting whether they like that item is as follows:\n$$P(like|ss) = P(like|ans)P(ans|ss)$$\nwhere like represents the user selecting it in the survey, ans repre-sents the user answering and submitting survey results, ss stands for survey show.\nWe could formulate an evaluation metric to assess the overall satisfaction levels on the platform, which is expressed as\n$$overall\\_survey\\_like\\_rate = \\frac{\\Sigma_{i \\in I} p(like|ss)}{|I|}$$\n, where I represents all survey shows. If we assume that the user responding to the survey behaves in an unbiased manner, such that p(ans ss) is reduced to uniform distribution, we can use\n$$overall\\_survey\\_like\\_rate = \\frac{\\Sigma_{i\\in A}p(like|ans)}{A}$$\n, where A represents all survey submits, and so it is the same for-mula as Eq.1. We could always build a model to predict p(like ans); however, users' willingness to answer and submit the survey is not uniform, thereby leading to response bias."}, {"title": "3.2.2 Survey-Submit Model.", "content": "To address response bias during the training and evaluation phases of the prediction model p(like ans) , we implement inverse propensity weighting (IPW) to ensure that"}, {"title": "3.3 Exposure Bias & Pseudo Label Distillation", "content": "3.3.1 Problem Formulation. Exposure bias is a common issue in recommendation models. This bias occurs because the training data for the model is based only on data from items or users that have been exposed, which leads to an under-representation of items or users that are not as well-exposed. In the context of survey modeling, the training data includes only user-item pairs for which surveys have been distributed.\nTo minimize disruption for users, we controlled the frequency of survey distributions, which led to significant exposure bias in the model. On one hand, the model has a limited number of labeled positive and negative examples for user-item pairs, accounting for less than one ten-thousandth of the total video exposures. This lack of training data restricts the model from fitting only local distributions. On the other hand, each user receives only one survey sample within a short time frame, further limiting the granularity of personalization in the modeling process.\nIn our baseline model, we focused on generalized features to enhance the model's ability to generalize beyond the training data, alleviating exposure bias and achieving coarse-grained personalization. Moreover, we discovered that by fully utilizing exposure data, we could reduce exposure bias while significantly increasing our training sample size. To address this, we propose a pseudo label distillation approach that assigns pseudo labels to all exposed videos for which surveys have not been distributed."}, {"title": "3.3.2 Overall Structure.", "content": "The pseudo label distillation approach, as illustrated in Fig 4, consists of two sample streams. The first stream comprises samples from survey exposures, which collect user-submitted responses and thus represent labeled data. The second stream includes all video exposure data.\nIt is clear that the number of samples in the first stream is signif-icantly smaller than that in the second stream. We train a teacher"}, {"title": "3.3.3 Teacher Model.", "content": "The structure of the teacher model is consis-tent with the baseline model. Input features consist of item features, user features, contextual features, and posterior behavior features. The posterior behavior features refer to the user's feedback on the video after it is exposed, such as whether or not to like or share the video. These feedbacks provide additional information that can enhance the teacher model's accuracy. To better align with inappropriate survey tasks, we added user behaviors that express negative experiences as posterior behavior features, such as dislikes and reports. It is important to note that posterior behavior features cannot be obtained prior to the exposure of videos; therefore, they are not available during the online service phase and can only be utilized within the teacher model.\nThe format of the model loss function is presented as follows, where yu,i represents the responses provided by user u after re-ceiving the survey for item i, and pu,i denotes the estimated score produced by the teacher model.\n$$loss = CE(p_{u,i}, y_{u,i})$$"}, {"title": "3.3.4 Student model.", "content": "The training data consists of two parts: The first part consists of samples without survey exposure, where the teacher model estimates a continuous value \u1ef9u,i as a pseudo label. Given that the teacher model is not entirely accurate, it may gener-ate numerous erroneous pseudo labels, leading to noisy training. Following the method described in reference [13], we established a threshold for both positive and negative examples within the pseudo labels to exclude certain samples whose predicted results fall within an uncertain intermediate range, where the teacher model exhibits lower confidence. Additionally, we continue to utilize soft labels to enable the student model to learn the subtle differences between different predicted values. Let gu,j indicate whether the pseudo label for the sample is used for training the model:\n$$J_{u,i} = \\begin{cases} 1 & \\text{if } y_{u,i} < t_n \\text{ or } \\tilde{y}_{u,i} > T_p \\\\ 0 & \\text{if others} \\end{cases}$$\nwhere \u03c4\u03b7 and \u03c4\u03b7 represent the thresholds for selecting pseudo labels for positive and negative examples, respectively. The second part of the training data is samples with survey exposure, labeled as yu,i. The structure of the student model can be simpler than that of the teacher model. Since the teacher model in this study is not complex, it does not significantly increase online latency; therefore, the student model uses the same network architecture. However, the student model cannot include posterior behavior features, as it is required to operate in an online service phase. Incorporating posterior behavior features would lead to inconsistencies between offline and online predictions.\nThe format of the model loss function is given by Eq. 9, where Yu,i is the estimated score from the teacher model on unlabeled samples, and \u03b1 is used to adjust the weight of labeled samples.\n$$loss = CE(p_{u,i}, \\tilde{Y}_{u,i}) + \\alpha * CE(p_{u,i}, Y_{u,i})$$"}, {"title": "4 Experiment", "content": ""}, {"title": "4.1 Metrics", "content": ""}, {"title": "4.1.1 AUC and Calibration.", "content": "For labeled samples, we utilize AUC (Area Under the Curve) to assess the offline model's ability to rank positive examples ahead of negative examples. In the case of teacher-student distillation, where the supervisory signal from the teacher model is represented by continuous predicted values, we employ regAUC to evaluate the student model's ranking capability. Additionally, we use calibration values to measure the discrepancy between the model's average predictions and the actual event oc-currence rates. The calculation method for calibration values is as follows:\n$$Calibratoin = avg(p_{u,i})/avg(y_{u,i}) \u2013 1$$\nGiven that implementing A/B testing incurs costs, we will first compute AUC and calibration values offline to ensure that we can observe improvements in accuracy during model iterations. After launching the A/B testing, we will calculate online AUC and cal-ibration values to ensure consistency between offline and online results."}, {"title": "4.1.2 Neg/Pos-Feedback UAUC.", "content": "To address the issue of exposure bias, we introduced a pseudo label distillation approach to model users' negative experiences more accurately when surveys are not"}, {"title": "5 Conclusion", "content": "We studied the problem of providing a better content recommendation on TikTok platform, focusing on limiting negative user experiences. The approach we proposed consists of a personalized,"}, {"title": "6 Future Exploration", "content": "In our current research, we acknowledge some limitations. For instance, when users receive surveys and provide their answers, they may respond randomly, which means that the results may not accurately reflect their true intentions. In future work, we will focus on addressing the issue of random responses in survey-based personalized modeling."}]}