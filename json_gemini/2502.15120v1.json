{"title": "Unveiling Reasoning Thresholds in Language Models: Scaling, Fine-Tuning, and Interpretability through Attention Maps", "authors": ["Yen-Che Hsiao", "Abhishek Dutta"], "abstract": "This study investigates the in-context learning capabilities of various decoder-only transformer-based language models with different model sizes and training data, including GPT2, SmolLM2, OpenELM, TinyLlama, Stable LM, and Gemma 2. We identify a critical parameter threshold (1.6 billion), beyond which reasoning performance improves significantly in tasks such as commonsense reasoning in multiple-choice question answering and deductive reasoning. Specifically, models above this threshold achieve better success rates in chain-of-thought (CoT) prompting for deductive reasoning tasks, especially those requiring longer reasoning chains, such as proof by contradiction and disjunction elimination. To address limitations in sub-threshold models, we demonstrate that fine-tuning with task-specific exemplars substantially enhances reasoning performance, enabling accurate CoT generation even without additional exemplars in the prompt for tasks with shorter reasoning chains. Finally, our analysis of attention maps reveals that models capable of generating correct CoTs exhibit higher token-level attention scores on subsequent correct tokens and the correct parts of speech, providing interpretability insights into reasoning processes. These findings collectively advance understanding of reasoning capabilities in decoder-only transformer-based models.", "sections": [{"title": "1. Introduction", "content": "Brown et al. (2020) introduced GPT-3, a 175-billion-parameter language model (LM), and demonstrated that increasing the number of its parameter from 0.1 to 175 billion improves performance across 42 benchmarks, except for their reversed words task, the Word-in-Context task in the SuperGLUE benchmark (Wang et al., 2019), and rounds 1 and 2 of the Adversarial Natural Language Inference dataset (Nie et al., 2020), by giving a few task demonstrations within the prompt at inference time without parameter updates, a technique commonly referred to as in-context learning (ICL) (Aky\u00fcrek et al., 2023; Min et al., 2022). Wei et al. (2022) proposed chain-of-thought (CoT) prompting and showed that scaling up model size improved the performance of ICL and CoT prompting led to further gains on the models from PaLM (Chowdhery et al., 2022), LaMDA (Thoppilan et al., 2022), and GPT-3 (Brown et al., 2020), with improvements appearing to be the largest for PaLM 540B (Chowdhery et al., 2022).\nShi et al. (2023) evaluates the reasoning abilities of the 6B, 62B, and 540B models from PaLM (Chowdhery et al., 2022) and different models from GPT-3 (Brown et al., 2020) in multilingual settings and found that the accuracy increases as the number of parameters increases in the models from PaLM (Chowdhery et al., 2022) on their proposed Multi-lingual Grade School Math benchmark across 11 different languages through CoT prompting (Wei et al., 2022). Zhou et al. (2024) show that LMs (GPT-3.5-turbo-0613 (Floridi & Chiriatti, 2020), Gemini-Pro (Jan. 2024) (Chowdhery et al., 2022), Llama2-70B (Touvron et al., 2023), and Mixtral-8x7B (Jiang et al., 2024)) can be distracted by irrelevant rationales that are unhelpful for solving a given question in the CoT exemplars. Sia et al. (2024) identify the layers in which \"task recognition\" occurs using causal masking over different parts of the context and conduct exploratory studies into the extent to which subsequent layers are either redundant or corresponding to the \"task recognition\" layers on machine translation and code generation tasks. Wibisono & Wang (2024) show that ICL for word analogy completion on a frequently co-occurring word pair can arise by modeling word co-occurrence using the continuous bag-of-words model (Mikolov et al., 2013) trained on frequently co-occurring tokens in the training dataset, without needing positional information or attention mechanisms, they find out that positional information is essential when the ICL task is to predict the first token in a sentence, and they find out that their designed transformer with learned positional embedding results in higher accuracy for ICL to complete token pairs compared to sinusoidal or rotary positional encoding (Su et al., 2024) if there are disturbing tokens between the word pairs in the exemplars. Stechly et al. (2024) show that the success rate of solving procedural reasoning tasks using their considered LMs (GPT-4 (OpenAI et al., 2024), Claude-3-Opus (Anthropic, 2023), and GPT-4-Turbo) prompted by the CoT techniques (Wei et al., 2022) decreases as the generality of the prompt increases regardless of the number of sub-goals and the success rate also decreases as the number of sub-goals increases, regardless of the specificity of the CoT prompt.\nWe extend the work of Wei et al. (2022) by investigating the reasoning abilities of 23 open-source, well-documented decoder-only transformer-based LMs through CoT prompting (Wei et al., 2022). Focusing on models with fewer than 10 billion parameters, we identify a critical parameter gap separating models with reasoning ability from those without. Testing on the CommonsenseQA (CSQA) dataset (Talmor et al., 2019), we observe a significant improvement in success rates for models with more than 1.6 billion parameters compared to those with fewer than 1.5 billion, except for LMs from the OpenELM family (Mehta et al., 2024). To minimize the influence of pre-training biases, we evaluate the models on the Proof and Ontology-Generated Question-Answering-Out-Of-Demonstration (PrOntoQA-OOD) dataset (Saparov & He, 2023) using six distinct deductive rules. Notably, we find parameter gaps of 774 million to 1.1 billion parameters for disjunction elimination and 1.5 billion to 1.6 billion parameters for proof by contradiction.\nTo address the limitations of sub-threshold models, we fine-tune six of the smallest LMs on exemplars generated by the PrOntoQA-OOD dataset (Saparov & He, 2023). Fine-tuning enables these models to achieve significantly improved success rates on four deductive rules\u2014implication elimination, conjunction introduction, conjunction elimination, and disjunction introduction\u2014even without exemplars in the prompt. Finally, we analyze attention maps of the largest and smallest models. Our findings show that the largest model (Gemma2 9B IT) capable of generating correct CoTs exhibits higher token-level scores (Kang & Shin, 2023) for subsequent correct tokens and tokens corresponding to the correct parts of speech, in contrast to the smallest model (GPT2), which fails to generate accurate CoTs. In conclusion, this study establishes three key contributions: the identification of a critical parameter threshold necessary for reasoning abilities, the demonstrated efficacy of fine-tuning in enhancing the performance of models below this threshold, and the utilization of attention maps to provide interpretive insights into performance disparities across models of different sizes and reasoning capacities."}, {"title": "2. Approaches", "content": "Given a test question \\(X_{test}\\) and an LM \\(f_{\\theta}\\), we expect to get the correct answer \\(Y_{test}\\) from the output of \\(f_{\\theta}\\). Following the definition in (Sia et al., 2024), when using the ICL techniques, the LM is prompted with the input \\(X_{ICL} = [S_n, X_{test}] = [x_1, y_1, ..., x_n, y_n, X_{test}]\\), which contains n exemplars \\(S_n = \\{(x_i, y_i)\\}_{i=1}^n\\), each consists of a question \\(x_i\\) and answer \\(y_i\\), and a given test question \\(X_{test}\\). When using the CoT techniques (Wei et al., 2022), the LM is prompted with the input \\(X_{CoT} = [x_1, T_1, y_1, ..., x_n, T_n, y_n, X_{test}]\\), where \\(T_i = [T_i^{(1)}, T_i^{(2)}, T_i^{(3)}, ..., T_i^{(k)}]\\) is a step-by-step rationale which consists of several thoughts \\(T_i^{(j)}\\)."}, {"title": "2.1. CSQA dataset", "content": "CommonsenseQA (Talmor et al., 2019) is a commonsense question answering dataset that contains 12,247 multiple-choice questions created by crowdsourcing workers using the knowledge encoded in CONCEPTNET (Speer et al., 2017). We use the validation set of the CommonsenseQA dataset (Talmor et al., 2019) which contains 1,221 multiple-choice questions."}, {"title": "2.1.1. INPUT PROMPT FOR THE COT EXPERIMENT USING CSQA DATASET", "content": "The prompt for the CoT experiment on the CSQA dataset (Talmor et al., 2019) contains a set of exemplars, a test question, and the choices for the test question. The exemplars are adopted from (Wei et al., 2022) as shown in Figure 5. The first part of the input prompt for the CSQA experiments contains 7 sets of questions, gold CoT, and answers as shown in Figure 5. In the initial prompt from (Wei et al., 2022), each question is appended after \"Q: \", the first answer choice is appended after\u201d Answer Choices: (a)"}, {"content": "\u201c, and the second to the last answer choice is appended after \"\\n(\" concatenated with the lower case of the answer key (from 'A' to 'E') and \")\", and the gold CoT is appended after \"\\n A: \". In the later part of the input prompt, one of the questions is chosen from the validation set of the CSQA dataset (Talmor et al., 2019) without repetition and the text of the question is appended after \"\\n Q: \", which is appended after the initial prompt in Figure 5. The answer choices of the chosen question are appended after the chosen question in the same way as the prompt contains the 7 exemplars. \"\\n\" is appended after the text contains the last answer choice of the chosen question."}, {"title": "2.1.2. STRING PARSING STRATEGY FOR THE COT EXPERIMENTS USING CSQA DATASET", "content": "In the CSQA dataset, each \\(Y_{test}\\) is a string that contains \"So the answer is (\" concatenated with either \"a\", \"b\", \"c\", \"d\", or \u201ce\u201d, correspond to the answer of the test question, \\(X_{test}\\), and with \")\". We aim to extract the answer from the output of the LM using string parsing.\nFor the CoT experiment, we first get the output of the LM, \\(f_{\\theta}(x_{CoT})\\), given the CoT prompt \\(X_{CoT}\\). The output of the LM is a string which can be represented as \\(f_{\\theta}(x_{CoT}) = [X_{CoT}, T_{respond}, Y_{response}, Y_{else}]\\), where \\(Y_{response}\\) is a string that contains the answer to the test question \\(X_{test}\\), \\(T_{response}\\) is the thought for the answer, and \\(Y_{else}\\) is a string that contains content irrelevant to the test question.\nIn our string parsing process, we first remove \\(X_{CoT}\\) from \\(f_{\\theta}(x_{CoT})\\) and then extract the string after \"So the answer is (\" and before \")\". If the extracted string is a character of either \u201ca\u201d, \u201cb\u201d, \u201cc\u201d, \u201cd\u201d, or \u201ce\u201d, we compare the extracted string with the correct answer key, which is a character of either \u201cA\u201d, \u201cB\u201d, \u201cC\u201d, \u201cD\u201d, or \u201cE\u201d, in the dataset. Regardless of capitalization, if the extracted string and the correct answer key are the same letter, the LM is considered to have answered the question, \\(X_{test}\\), correctly; otherwise, the answer is considered incorrect. If the extracted string is not one of the five characters, the LM is considered to have not answered the question."}, {"title": "2.2. PrOntoQA-OOD dataset", "content": "PrOntoQA dataset (Saparov & He, 2023) is a synthetic and programmable question-answering dataset that contains examples generated from a synthetic world model represented in first-order logic. PrOntoQA-OOD dataset (Saparov et al., 2023) is a synthetic and programmable reasoning dataset that extends the PrOntoQA dataset (Saparov & He, 2023) to a complete set of deduction rules and to compositional proofs. Each example in PrOntoQA dataset (Saparov & He, 2023) contains texts with a set of premises, a conclusion that is the target fact to be proved or disproved, and a gold CoT containing the proof of the conclusion. The six deduction rules include implication elimination (also known as modus ponens), conjunction introduction, conjunction elimination, disjunction introduction, disjunction elimination (also called proof by cases), and proof by contradiction."}, {"title": "2.2.1. INPUT PROMPT FOR THE COT EXPERIMENT USING THE PRONTOQA-OOD DATASET", "content": "The input prompt consists of several examples generated from the PrOntoQA-OOD dataset appended with the test question which is taken from the next example generated from the PrOntoQA-OOD dataset. The examples form the first part of the input prompt. Each example may be different across different test questions resulting in different CoT prompts in each question.\nFor each exemplar, the text of the premises is appended after \"Q: \", followed by \". Prove: \". The text of the conclusion is appended after \". Prove: \", followed by \"\\nA: \". The gold CoT text is then appended after \"\\nA: \". Two next-line symbols are inserted between two exemplars and between the last exemplar and the text with \"Q: \" concatenated with the text of the premises from the test question, \". Prove: \", the text of the conclusion from the test question, and \"\\nA: \".\nThe input prompt consists of 8 examples for the models from GPT2 (Radford et al., 2019), SmolLM2 (Allal et al., 2024), OpenELM (Mehta et al., 2024), TinyLlama (Zhang et al., 2024), Stable LM 2 (Bellagente et al., 2024), and Gemma 2 (Team et al., 2024b), and 3 examples for the models from GPT2 (Radford et al., 2019). The reason that the models from GPT2 (Radford et al., 2019) use fewer examples is that models from GPT2 (Radford et al., 2019) can only use a maximum of 1024 input tokens which is not enough for the CoT test using the PrOntoQA-OOD dataset (Saparov et al., 2023) with proof by contradiction. The details of the exemplars in the six deductive rules from the PrOntoQA-OOD dataset (Saparov et al., 2023) are described in Appendix A.1"}, {"title": "2.2.2. STRING PARSING STRATEGY FOR THE PRONTOQA-OOD DATASET", "content": "For the CoT experiment on the PrOntoQA-OOD dataset (Saparov et al., 2023), given the CoT prompt \\(X_{CoT}\\), the output of the LM is a string which can be represented as \\(f_{\\theta}(X_{CoT}) = [x_{CoT}, T_{response}, Y_{else}]\\), where \\(T_{response}\\) is the chain-of-thought generated by the LM, and \\(Y_{else}\\) is a string that contains content irrelevant to the test question.\nIn our string parsing process, we aim to extract the chain-of-thought, \\(T_{response}\\), generated by the LM. We first remove the CoT prompt, \\(X_{CoT}\\), from \\(f_{\\theta}(x_{CoT})\\) and then extract the string before the first \"Q:\"."}, {"title": "2.3. Language models", "content": ""}, {"title": "2.3.1. GPT-2", "content": "We used the models from GPT-2 (Radford et al., 2019) with 117, 345, 762, and 1542 million parameters and noted them as the gpt2, gpt2-medium, gpt2-large, and gpt-xl model, respectively, the same as their names in the Hugging face library. The gpt2, gpt2-medium, gpt2-large, and gpt-xl model are decoder-only transformers based on (Radford et al., 2018) with 12, 24, 36, and 48 layers and a dimension of 768, 1024, 1280, and 1600 in the outputs of all the sub-layers and the embedding layers, respectively. All of the four models used 50,257 vocabularies, a context size of 1024 tokens, and trained on their designed dataset, WebText (Radford et al., 2019), which contains 45 million links of web pages with over 8 million documents and 40 GB of text."}, {"title": "2.3.2. SMOLLM2", "content": "We used the models from SmolLM2 (Allal et al., 2024) with 135, 360, and 1700 million parameters and noted them as the SmolLM1-135M, SmolLM1-360M, and SmolLM1-1.7B model, respectively, and used the instruction tuned version of these models noted as the SmolLM1-135M-Instruct, SmolLM1-360M-Instruct, and SmolLM1-1.7B-Instruct model, respectively. The models are decoder-only transformers incorporating Grouped-Query Attention (GQA) (Ainslie et al., 2023) and trained on Python-Edu (Allal et al., 2024; Li et al., 2023a), which contains educational Python samples from The Stack, FineWeb-Edu (Penedo et al., 2024), which has educational web samples from FineWeb, DCLM, and their curated datasets. SmolLM1-1.7B-Instruct is fine-tuned on the SmolTalk dataset (Allal et al., 2024) and the SmolLM1-135M-Instruct and the SmolLM1-360M-Instruct models are fine-tuned on a subset of the SmolTalk dataset. The instructed models are applied Direct Preference Optimization (DPO) (Rafailov et al., 2023) on the pre-processed version of the UltraFeedback dataset (Cui et al., 2024)."}, {"title": "2.3.3. OPENELM", "content": "We used the models from OpenELM (Mehta et al., 2024) with 270, 450, 1100, and 3000 million parameters and noted them as the OpenELM-270M, OpenELM-360M, OpenELM-1-1B, and OpenELM-3B model, respectively, and used the instruction tuned version of these models noted as the OpenELM-270M-Instruct, OpenELM-360M-Instruct, OpenELM-1-1B-Instruct, and OpenELM-3B-Instruct model, respectively. These models adopt decoder-only transformer-based architecture and incorporate GQA (Ainslie et al., 2023), flash attention (Dao et al., 2022), pre-normalization using RMSNorm (Zhang & Sennrich, 2019a), rotary positional embedding (Su et al., 2024), SwiGLU feed forward network (Shazeer, 2020), and the tokenizer from LLama (Touvron et al., 2023). The models with 270, 450, 1100, and 3000 million parameters have 16, 20, 28, and 36 layers, 1280, 1536, 2048, and 3072 embedding dimensions, and a head dimension of 64, 64, 64, and 128, respectively. All the models have a context length of 2048 and are trained on RefinedWeb (Penedo et al., 2023), the Github, Books, ArXiv, Wikipedia, StackExchange, and C4 subsets in RedPajama (Weber et al., 2024), PILE (Gao et al., 2020), and The Stack, Reddit, PeS2o, Project Gutenberg, and Wikipedia + Wikibooks subsets in Dolma (Soldaini et al., 2024). The OpenELM-270M-Instruct, OpenELM-360M-Instruct, OpenELM-1-1B-Instruct, and OpenELM-3B-Instruct model are instruction tuned on the UltraFeedback dataset (Cui et al., 2024)."}, {"title": "2.3.4. TINYLLAMA", "content": "We used the decoder-only Transformer model with 1.1 billion parameters from Zhang et al. (2024) and noted it as the TinyLlama_v1_1 model. The TinyLlama_v1_1 model has a context length of 2048, 32 heads, 22 layers, and 32000 vocabularies. The TinyLlama_v1_1 model incorporates GQA (Ainslie et al., 2023), flash attention (Dao et al., 2022), pre-normalization using RMSNorm (Zhang & Sennrich, 2019a), rotary positional embedding (Su et al., 2024), and SwiGLU feed forward network (Shazeer, 2020) similar to Mehta et al. (2024). The training datasets of the TinyLlama_v1_1 model include SlimPajama (Soboleva et al., 2023) derived from RedPajama (Weber et al., 2024) and the StarCoder Training Dataset used to train StarCoder (Li et al., 2023b)."}, {"title": "2.3.5. STABLE LM 2", "content": "We used two decoder-only Transformer models from Stable LM 2 (Bellagente et al., 2024). One is a 1.6 billion parameter decoder-only language model noted as the stablelm-2-1_6b model and another one is the fine-tuned version of the stablelm-2-1_6b model, noted as the stablelm-2-zephyr-1_6b model, with DPO (Rafailov et al., 2023) following the Zephyr training recipe (Tunstall et al., 2023). The stablelm-2-1_6b model contains 1,644,417,024 parameters, 24 layers, 32 heads, and 4096 context length. The stablelm-2-1_6b model uses rotary positional embedding (Su et al., 2024) on the first 25% of head embedding dimensions and Layer-Norm (Ba et al., 2016) with learned bias terms as opposed to RMSNorm (Zhang & Sennrich, 2019b). Only the key, query, and value projection contain bias terms (Bai et al., 2023). The training data for the stablelm-2-1_6b model includes RefinedWeb (Penedo et al., 2023), RedPajama (Weber et al., 2024), subsets of PILE (Gao et al., 2020), and The Stack (Li et al., 2023b), OpenWebText (Gokaslan & Cohen, 2019), OpenWebMath (Paster et al., 2024), OSCAR subset in CulturaX (Nguyen et al., 2024), FanFics, and the Restruct-v1 (Bellagente et al., 2024). The data for supervised fine-tuning of stablelm-2-zephyr-1_6b model includes ultrachat_200k (Ding et al., 2023), MetaMathQA (Yu et al., 2024), WizardLM_evol_instruct_V2_196k (Xu et al., 2024), SlimOrca (Lian et al., 2023), openchat_sharegpt4_dataset (Chen et al., 2025), Capybara (Daniele & Suphavadeeprasit, 2023), and deita-10k-v0 (Liu et al., 2024). The data for DPO of stablelm-2-zephyr-1_6b model includes UltraFeedback dataset (Cui et al., 2024) and Intel Orca Pairs."}, {"title": "2.3.6. GEMMA 2", "content": "We used two decoder-only Transformer models from Gemma 2 (Team et al., 2024b): Gemma 2 IT 2B and Gemma 2 IT 9B, noted as gemma-2-2b-it and gemma-2-9b-it. The gemma-2-2b-it model has 2,614,636,800 model parameters, 2304 embedding dimensions, 26 layers, and 8 heads. The gemma-2-9b-it model has 9,242,164,736 model parameters, 42 layers, and 16 heads. Both the gemma-2-2b-it and gemma-2-9b-it models have a context length of 8192 tokens and 256128 vocabularies. The architecture of the models from Gemma 2 (Team et al., 2024b) are based on the architecture from Gemma 1 (Team et al., 2024a) and incorporates rotary positional embedding (Su et al., 2024), approximated GeGLU non-linearity, GQA (Ainslie et al., 2023), RMSNorm (Zhang & Sennrich, 2019b), their proposed Logit soft-capping, and local sliding window attention (Beltagy et al., 2020) in some of the layers and global sliding window attention (Luong et al., 2015) in the other layers. The training data includes web documents, code, and science articles, and the link, name, or citation of the data source is not included in their paper (Team et al., 2024b)."}, {"title": "2.4. Fine-tuning", "content": "To enhance the reasoning capabilities of models below the identified parameter threshold (~1.6 billion), we fine-tuned six of the smallest models-gpt2, SmolLM2-135M, SmolLM2-135M-Instruct, OpenELM-270M, OpenELM-270M-Instruct, and gpt2-medium models-using task-specific exemplars from the PrOntoQA-OOD dataset (Saparov et al., 2023). These exemplars are structured logical reasoning examples covering six distinct deductive rules: implication elimination, conjunction introduction, conjunction elimination, disjunction introduction, disjunction elimination, and proof by contradiction. Each fine-tuning instance consisted of 3 exemplars per question, with a total of 100 questions per deductive rule, resulting in 1,800 exemplars in total.\nWe restricted our selection to models with <355 million parameters to ensure feasibility within a single 40 GB GPU during fine-tuning. The dataset was randomly shuffled and split into 90% training data (1,620 exemplars) and 10% validation data (180 exemplars). Training data were concatenated and segmented into multiple 1024-token blocks to align with input constraints.\nFor fine-tuning, we used the Hugging Face Trainer API from the Transformers Python library. The training was framed as causal language modeling, where the input tokens were shifted by one token to serve as the target labels. We employed the Adam optimizer with a weight decay of 0.01, hyperparameters \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), and \\(\\epsilon = 10^{-8}\\) (Loshchilov & Hutter, 2019). Training was conducted for 100 epochs with a batch size of 1,000 and a learning rate of \\(2 \\cdot 10^{-5}\\)."}, {"title": "2.5. Token generation strategy", "content": "We used the \"generate\" function from the transformers library from Hugging Face. In the CoT experiment using the CSQA dataset (Talmor et al., 2019), we set the maximum number of the generated tokens as the number of the input tokens added with 100, set \"do_sample\" to be False, and set \"num_beams\" to be 1 such that each token is sampled from the output of the transformer using greedy decoding. In the CoT experiment using the PrOntoQA-OOD dataset (Saparov et al., 2023), we used greedy decoding by making \"do_sample\" to be False and set \"num_beams\" to be 1. We set the maximum number of the generated tokens as 256 which is the same as the number provided in the \"opt.py\" sample code in the PrOntoQA-OOD dataset (Saparov et al., 2023). We set the repetition penalty to 0.0001 for both of the experiments to prevent the LM from producing the output with empty text."}, {"title": "3. Results", "content": ""}, {"title": "3.1. Commonsense Reasoning in Multiple-Choice Question Answering", "content": "We evaluate the commonsense reasoning capabilities of 23 different LMs using the CSQA dataset (Talmor et al., 2019). Model accuracy is computed as the number of correct answers divided by the total number of questions. Our analysis reveals a clear trend of increasing accuracy as model size increases, with a significant performance jump between models with fewer than 1.5 billion parameters and those exceeding 1.6 billion parameters, except for models from OpenELM (Mehta et al., 2024). The gpt2 model (Radford et al., 2019) achieves the lowest accuracy of 4.18%, whereas the gemma2-9b-it model (Team et al., 2024b) reaches the highest accuracy of 75.92% as shown in Figure 1.\nA noticeable accuracy increase is observed between gpt2-xl (1.542B parameters) (Radford et al., 2019) with an accuracy of 19.57% and stablelm-2-1_6B (Bellagente et al., 2024) with 43.00%, supporting the hypothesis that reasoning abilities improve significantly beyond a certain parameter threshold as shown in Figure 1. Additionally, we analyze the number of unparseable responses using the method described in Section 2.1.2 as shown in Figure 2. gpt2, OpenELM-270M-Instruct, OpenELM-450M-Instruct, and stablelm-2-1_6B exhibit a high failure count of over 200, indicating that sub-threshold models struggle with structured answer formatting."}, {"title": "3.2. Deductive Reasoning in Language Models", "content": "To assess deductive reasoning capabilities, we evaluate models on six logical reasoning tasks from the PrOntoQA-OOD dataset (Saparov et al., 2023). The correctness of the generated CoT is assessed by comparing the generated steps to the gold CoT using the \"analyze_results.py\" evaluation script from Saparov et al. (2023).\nA distinct parameter gap is observed in multiple reasoning tasks as shown in Figure 3 and in Figure 6:\n\u2022 Implication Elimination: Accuracy increases from 68% in SmolLM2-135M (135M parameters) to 78% in OpenELM-270M-Instruct (270M parameters) as shown in Figure 6(a).\n\u2022 Disjunction Elimination: A gap is observed between gpt2-large (762M, 0% accuracy) and OpenELM-1.1B-Instruct (1.1B, 53% accuracy), suggesting a threshold around 1.1 billion parameters as shown in Figure 6(e).\n\u2022 Proof by Contradiction: Models with fewer than 1.5 billion parameters, including gpt2-xl (1.542B) with 0% accuracy, fail to generate correct proofs, whereas stablelm-2-zephyr-1_6b achieves 22% accuracy as shown in as shown in Figure 6(f).\nFor simpler reasoning tasks, such as conjunction introduction and conjunction elimination, all models achieve above 95% accuracy, indicating that smaller models can effectively handle lower-complexity deductions as shown in Figure 6(b), Figure 6(c), and Figure 6(d)."}, {"title": "3.3. Fine-Tuning Improves Sub-Threshold Models", "content": "We evaluate whether fine-tuning enhances reasoning performance in sub-threshold models by training six of the smallest models-gpt2, SmolLM2-135M, SmolLM2-135M-Instruct, OpenELM-270M, OpenELM-270M-Instruct, and gpt2-medium-on exemplars from the PrOntoQA-OOD dataset (Saparov et al., 2023). The fine-tuned models exhibit substantial improvements in implication elimination, conjunction"}]}