{"title": "MSG score: A Comprehensive Evaluation for\nMulti-Scene Video Generation", "authors": ["Daewon Yoon", "Hyungsuk Lee", "Wonsik Shin"], "abstract": "This paper addresses the metrics required for generating multi-scene videos\nbased on a continuous scenario, as opposed to traditional short video generation.\nScenario-based videos require a comprehensive evaluation that considers multiple\nfactors such as character consistency, artistic coherence, aesthetic quality, and the\nalignment of the generated content with the intended prompt. Additionally, in\nvideo generation, unlike single images, the movement of characters across frames\nintroduces potential issues like distortion or unintended changes, which must be\neffectively evaluated and corrected. In the context of probabilistic models like dif-\nfusion, generating the desired scene requires repeated sampling and manual selec-\ntion, akin to how a film director chooses the best shots from numerous takes. We\npropose a score-based evaluation benchmark that automates this process, enabling\na more objective and efficient assessment of these complexities. This approach al-\nlows for the generation of high-quality multi-scene videos by selecting the best\noutcomes based on automated scoring rather than manual inspection.", "sections": [{"title": "Introduction", "content": "Recent advancements in Image Generation have significantly improved the quality of generated\nimages based on diffusion models [1, 2] and high-resolution processing methods [3]. These de-\nvelopments have spurred numerous efforts in generative image tasks, particularly those starting\nfrom LLM-based text prompts, leading to breakthroughs in text-to-image generation [4] and text-\nto-video generation [5]. However, creating a coherent long video from multiple scenes generated\nby text prompts using LLM-based Video Diffusion models presents substantial challenges. Each\ntext prompt typically generates a single scene, and when combined to form a longer narrative, main-\ntaining consistency across these scenes becomes crucial. More fundamentally, ensuring temporal\nconsistency between frames within each scene is paramount. Existing methods often struggle to\nbalance these aspects, resulting in artifacts or computational inefficiencies.\nTo address these issues, we introduce MSG, a hybrid approach that distinguishes between frame-\nlevel and scene-level processing. Our method employs bidirectional frame reference for immediate\nneighboring frames and a lookback mechanism to ensure smooth transitions across scenes. This dual\nstrategy allows MSG to leverage the strengths of both short-term and long-term temporal consistency\ntechniques, resulting in superior video quality."}, {"title": "Related Work", "content": ""}, {"title": "Diffusion Models and High-Resolution Works", "content": "Diffusion models have significantly advanced image and video generation tasks. Notable works\ninclude Denoising Diffusion Probabilistic Models (DDPM) [1] and Denoising Diffusion Implicit\nModels (DDIM) [2]. DDPMs are a class of generative models that work by modeling the distribution\nof data through a sequence of denoising steps. The process involves gradually adding Gaussian\nnoise to training data and then learning to reverse this process to generate new data samples. This\nmodel has demonstrated impressive results in generating high-quality images by ensuring that each\ndenoising step is carefully learned, resulting in realistic outputs. DDIMs improve upon DDPMs by\nproviding a more efficient sampling method. They achieve this by making the denoising process\ndeterministic and reducing the number of steps required for generation, thus significantly speeding\nup the sampling process while maintaining high-quality outputs. Latent Diffusion Models (LDM)\n[3] focus on high-resolution image synthesis. LDMs operate in the latent space of a pretrained\nautoencoder, which makes them computationally efficient for generating high-quality images. By\nperforming the diffusion process in a compressed latent space, LDMs reduce the computational load\nand memory requirements compared to pixel-space diffusion models, enabling the generation of\nhigh-resolution images with fine details."}, {"title": "Text-to-Video Generation", "content": "The integration of text prompts for video generation has seen significant advancements. Text-to-\nVideo models, such as those proposed. [4], generate coherent video sequences from textual descrip-\ntions. These models often face challenges in maintaining temporal and spatial consistency across\ndifferent scenes generated from text prompts. This is a notable example that focuses on generating\nvideos directly from text without requiring paired text-video data for training. This approach lever-\nages pre-trained text-to-image models and extends them to handle temporal sequences, ensuring that\neach frame aligns with the provided textual narrative."}, {"title": "Temporal Consistency Both Between Frames and Between Scenes", "content": "Ensuring temporal consistency both between frames and between scenes is crucial for maintaining\nthe visual coherence and narrative flow of generated videos. Several advanced methods have been\nproposed to address these challenges, each contributing unique approaches to enhance temporal con-\nsistency. FRVSR focuses on frame-recurrent methods to maintain short-term temporal consistency\nwithin scenes by using previously generated frames [5], while StableVSR incorporates bidirectional\ntechniques to further stabilize transitions between frames [6]. MGLD uses motion guidance to main-\ntain coherence within frames and across scenes, ensuring smooth intra-frame and inter-scene transi-\ntions [7]. Video Drafter focuses on enhancing the visual quality and temporal consistency of frames\nby using advanced deep learning techniques. While primarily addressing frame-level enhancements,\nthis method ensures smooth transitions and detail preservation across consecutive frames, thus con-\ntributing to improved short-term temporal stability [8]. CoNo employs a \"look-back\" mechanism\nto maintain long-term temporal consistency across scenes. This mechanism involves referencing\nkey frames from previous scenes to enhance the transition between different video clips. By us-\ning consistent noisy labeling and a long-term consistency regularization, CoNo effectively reduces\nabrupt changes and enhances the visual continuity of scene transitions, even under multi-text prompt\nconditions [9]."}, {"title": "Proposed Method", "content": ""}, {"title": "Framework Overview", "content": "MSG consists of two main components: the Backward and Forward Frame Reference (BFFR) and\nthe Backward Scene Reference (BSR). The BFFR handles immediate frame references to enhance\nspatial details and maintain short-term temporal consistency. The BSR uses a lookback mechanism\nto reference key frames from previous scenes generated from text prompts, ensuring smooth transi-\ntions and long-term coherence."}, {"title": "Backward and Forward Frame Reference (BFFR)", "content": "The BFFR processes each frame by considering both previous and subsequent frames. This bidi-\nrectional approach helps maintain high spatial resolution and immediate temporal consistency. The\nmodel architecture includes temporal texture guidance and convolutional layers to integrate informa-\ntion from neighboring frames. Let It denote the input frame at time t. The Backward and Forward\nFrame Reference generates the super-resolved frame \u00cet by considering both the previous frame It\u22121\nand the subsequent frame It+1. The formulation is given by:\n\u00cet = F(It\u22121, It, It+1; 0F)"}, {"title": "Backward Scene Reference (BSR)", "content": "The BSR operates at scene transitions, using a lookback mechanism to reference key frames from\nprevious scenes generated from text prompts. This approach minimizes abrupt changes and ensures\nthat the high-level scene context is preserved. The model incorporates attention mechanisms to\ndynamically weigh the importance of past frames. For scene transitions, the BSR uses the previous\nscene's key frame Iprev to enhance the current scene's frame It. The enhanced frame \u00cet is given by:"}, {"title": "Frame and Scene Separation", "content": "In our method, frames within the same scene are processed using the Backward and Forward Frame\nReference (BFFR) to ensure short-term temporal consistency. At scene transitions, the Backward\nScene Reference (BSR) is employed to maintain long-term consistency and smooth transitions be-\ntween scenes generated from different text prompts. This separation ensures that each model focuses\non the specific temporal characteristics required at different levels of the video."}, {"title": "Loss Function", "content": "To ensure high-quality video generation with both spatial fidelity and temporal consistency, the loss\nfunction used to train the MSG model incorporates multiple components. Specifically, it includes\nthe Mean Squared Error (MSE) loss for spatial fidelity, a temporal consistency loss to ensure smooth\ntransitions across frames, and an inter-scene consistency loss that penalizes discrepancies between\nthe key frames of adjacent scenes."}, {"title": "Implementation Steps", "content": "The implementation steps for the MSG framework involve a systematic process to ensure both intra-\nscene and inter-scene consistency. Below are the detailed steps:\n2. Use BFFR for Intra-Scene Frames Apply the BFFR model to each frame within a scene,\nconsidering the immediate previous and next frames to ensure smooth transitions and enhanced\ndetails.\n\u2022 Action: For each frame It within a scene, use the previous frame It-1 and the next frame\nIt+1 to generate the enhanced frame \u00cet.\n\u2022 Mathematical Formulation:\n\u00cet = F(It\u22121, It, It+1;0F)\n\u2022 Purpose: This ensures that spatial details are preserved and transitions between frames\nwithin a scene are smooth.\n3. Use BSR for Inter-Scene Transitions Apply the BSR model when transitioning to a new\nscene, using the key frame from the previous scene to guide the generation of initial frames in the\nnew scene."}, {"title": "Experiments", "content": ""}, {"title": "Datasets and Metrics", "content": "We evaluate MSG on standard benchmark datasets such as Vid4 and REDS. The primary metrics\nfor evaluation include Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and\ntemporal consistency measures."}, {"title": "Implementation Details", "content": "Our models are implemented using PyTorch, with training conducted on NVIDIA RTX 4080 GPUs.\nThe BFM and SLM are jointly trained using a combination of mean squared error (MSE) loss for\nspatial fidelity and temporal loss to ensure consistency."}, {"title": "Results", "content": ""}, {"title": "Quantitative Results", "content": "(Note: Experiment was failed. Actually this model is not working until now)"}, {"title": "Conclusion", "content": "The demand for generating continuous video sequences from multiple text prompts, each represent-\ning a distinct scene, is increasingly apparent. This study aimed to address the critical challenge of\nmaintaining consistency both within frames and across scenes. Although our experiments were de-\nsigned to validate the effectiveness of the proposed methods, unforeseen issues impeded the achieve-\nment of conclusive results. Acknowledging these challenges, we recognize the necessity for further\nresearch. Future work will involve re-experimentation to overcome these obstacles and derive mean-\ningful insights, ultimately enhancing the performance of video generation models in maintaining\ntemporal consistency across frames and scenes."}]}