{"title": "Artificial Intelligence for Collective Intelligence:\nA National-Scale Research Strategy", "authors": ["Seth Bullock", "Nirav Ajmeri", "Mike Batty", "Michaela Black", "John Cartlidge", "Robert Challen", "Cangxiong Chen", "Jing Chen", "Joan Condell", "Leon Danon", "Adam Dennett", "Alison Heppenstall", "Paul Marshall", "Phil Morgan", "Aisling O'Kane", "Laura G. E. Smith", "Theresa Smith", "Hywel T. P. Williams"], "abstract": "Advances in artificial intelligence (AI) have great potential to help address societal challenges\nthat are both collective in nature and present at national or trans-national scale. Pressing\nchallenges in healthcare, finance, infrastructure and sustainability, for instance, might all be\nproductively addressed by leveraging and amplifying AI for national-scale collective intelligence.\nThe development and deployment of this kind of AI faces distinctive challenges, both technical\nand socio-technical. Here, a research strategy for mobilising inter-disciplinary research to address\nthese challenges is detailed and some of the key issues that must be faced are outlined.", "sections": [{"title": "Introduction", "content": "Artificial intelligence (AI) and machine learning often address challenges that are relatively mono-\nlithic: determine the safest action for an autonomous car; translate a document from English to\nFrench; analyse a medical image to detect a cancer; answer a question about a difficult topic. These\nkinds of challenge are important and worthwhile targets for AI research. However, an alternative\nset of challenges exist that are collective in nature:\n\u2022 help to minimise a pandemic's impact by coordinating mitigating interventions;\n\u2022 help to manage an extreme weather event using real-time physical and social data streams;\n\u2022 help to avoid a stock market crash by managing interactions between trading agents;\n\u2022 help to guide city developers towards more sustainable coordinated city planning decisions;\n\u2022 help people with diabetes to collaboratively manage their condition while preserving privacy."}, {"title": "Research Context", "content": "Between 2022 and 2024, the UK government initiated several significant investments in national-\nscale AI research amounting to approximately \u00a31Bn of support. Foremost amongst these invest-\nments were: the establishment of the UK's first national supercomputing facility for AI research\n(Isambard-AI; \u00a3225m), plus an additional \u00a3500m of AI compute hardware investment across UK\nuniversities, the inception of twelve new Centres for Doctoral Training in AI (AI CDTs; \u00a3117m),\nfunding for a raft of AI research projects including AI for net zero (\u00a313m) and AI for healthcare\n(\u00a313m), the creation of UK Responsible AI, a national network to conduct and fund research into\nresponsible AI (UKRAI, \u00a331m), and the launch of nine new national Research Hubs for AI, three\nfocusing on the mathematical foundations of AI and six focusing on applied AI research (\u00a3100m).\nThese significant investments were driven by growing recognition that modern AI has the poten-\ntial to achieve a positive and revolutionary impact on society. Here, we focus on a research strategy"}, {"title": "Vision and Structure", "content": "Our ability to address the most pressing current societal challenges (e.g., healthcare, sustainability,\nclimate change, financial stability) increasingly depends upon the extent to which we can reliably\nand successfully engineer important kinds of collective intelligence, which we define as:\n\"Connected communities of people, devices, data and software collaboratively sensing\nand interacting in real time to achieve positive outcomes at multiple scales\".\nWhether we are aiming to minimise the impact of a global pandemic through effectively manag-\ning successive waves of vaccination (Brooks-Pollock et al. 2021), to prevent financial \u201cflash crashes\"\nthrough effective regulation of autonomous trading agents (Cartlidge et al. 2012, Johnson et al.\n2013), to make our cities sustainable and liveable through using real-time analytics to inform\nshort-, medium- and long-term planning (Spooner et al. 2021, Batty 2024), to combat social po-\nlarisation and climate disinformation on social media (Treen et al. 2020), or to achieve the UK\nNHS 2019 Long-Term Plan (NHS 2019) by enabling effective healthcare ecosystems that integrate\nclinical care, technology, education, and social support for patients with chronic health conditions\n(Duckworth et al. 2024), invariably what is required is an ability to engineer smart collectives.\nThis will necessarily involve addressing both halves of what we characterise as the \u201cAI4CI Loop\"\n(Fig. 1 left) - (1) Gathering Intelligence: collecting and making sense of distributed information;"}, {"title": "Research Themes", "content": "The AI for Collective Intelligence Hub (Fig. 1 right) addresses and connects both halves of the AI4CI\nLoop across a set of five important application domains (healthcare, finance, the environment,\npandemics, and cities) and two cross-cutting themes (human-centred design and infrastructure and\ngovernance). In each domain, the challenge is to leverage and make sense of real time, dynamic data\nstreams generated across hybrid systems of interacting people, machines and software distributed\nover space and across networks, in order to achieve systemic insights and drive effective interventions\nvia the automated behaviour of smart AI agents. Pursuing research across multiple domains in\nconcert enables each to benefit from the others' insights and maximises the chance of uncovering\nprinciples and that have domain-general application (Smaldino & O'Connor 2022)."}, {"title": "Smart City Design", "content": "Plan-making systems for UK cities are not currently fit for purpose. Local plans, the major\ninstrument of the statutory planning system, must be modernised to exploit collective data and\nmachine intelligence (Batty 2024). Meeting the challenges associated with smart planning for smart\ncities in a way that delivers practical tools and applications requires integrating and exploiting\nmultiple streams of city data provided by local and national government, urban analytics and\ninfrastructure firms, national agencies, survey data, and human mobility patterns derived from\ndigital traces or social media.\nThese data can drive new AI for two purposes: (i) automating real-time intelligence for the\nsmart city (Malleson et al. 2022, An et al. 2023); (ii) informing longer-term smart city planning to\nmeet the challenges of climate, ageing, housing affordability, and health (Batty 2024). Achieving\nsmarter cities that optimise behaviours in the short and long term requires AI that extends and\nimproves on existing models of urban structure, dealing with highly fluid situations dominated\nby rapid change (Batty 2024). This is a major challenge not only for the way that we design\ncities but also for how AI must deal with many/most human problem-solving contexts. Supervised\nand unsupervised learning methods can be used to reveal new patterns in large messy mobility\ndatasets such as mobile phone traces, cross-validated with rich survey data to produce spatially,\ntemporally and attribute rich insights into the seismic shift in post-COVID mobility patterns (Batty\net al. 2020). Predictive tools for the design of new patterns of transport and land development at\ndifferent scales can be founded on models that take multiple land suitability and mobility indices\nas inputs (see, e.g., Fig. 2; Zhang et al. 2020). Deriving meaningful interpretations of these models\nand enabling decision-makers to explore how optimal plans play out over time and space in the\ncontext of synthetic AI agent models (Batty et al. 2012) delivers the explanatory accounts that are\nessential for public accountability in the use of these methods for city decision making."}, {"title": "Pandemic Resilience", "content": "COVID-19 exposed weaknesses in the UK's pandemic resilience. A combination of collective in-\ntelligence and AI can help us do better next time. Data crucial for managing novel pandemics\nare inherently fragmented, arising from communities of medics, public health professionals and\nanalysts to describe the spread of the disease, characterise its phenotype, and, with appropriate\nmodelling, inform appropriate policies nationally and locally (Brooks-Pollock et al. 2021). National\nspatio-temporal datasets describing SARS-CoV-2 hospital testing and community testing, and the\nextent and effects of the mitigations put in place against it, can be exploited in order to build new\nAI/machine learning tools for future pandemics\u2014and potentially for mitigating seasonal outbreaks\nof endemic disease.\nTwo strands of research can be identified. First, a suite of machine learning models fuelled by\nnational SARS-CoV-2 pandemic data can be used to explore and demonstrate how the integration\nof multiple population-level indicators could have improved decision making during the pandemic."}, {"title": "Environmental Intelligence", "content": "In order to meet the challenge of mitigating climate change, there is a need to improve access to,\nand comprehension of, different kinds of complex, time-varying environmental data, for example:\n(1) outputs from climate, weather and ocean model ensembles and empirical observations, (2)\nhigh-volume geospatial, ecological, satellite and remote sensing data; (3) socioeconomic data on\nresource flows, supply chains, energy consumption and carbon emissions, and (4) online media\nincluding news media and cross-platform social media content. Many decision-makers (including\ncitizens and policy-makers) would benefit from better environmental information. A huge volume of\nthis data is now available, but it often requires a high level of expertise to obtain it and interpret the\nassociated uncertainties. Large language models are increasingly suggested as a potential solution\nto part of this problem (Vaghefi et al. 2023, Koldunov & Jung 2024). Meanwhile, public debate is\nweakened by the profusion of poor quality or deliberately false information, especially concerning\nthe contested issue of climate change (Treen et al. 2020, Acar 2023). A combination of AI and\ncollective intelligence approaches can deliver tools that overcome these challenges by democratising\naccess to good quality information about environmental change.\nNovel \"climate avatar\" agents can act as simple interfaces between complex environmental\ndata and the people who need it in order to improve their decision-making. They ingest weather\nand climate data from existing large datasets, peer-reviewed climate science literature and other\ntrusted sources (e.g., IPCC reports), and expose this data via natural-language interfaces that\nallow users to access information and gain understanding in a conversational style. By summarising\nscientific literature and generating on-the-fly visualisations from raw data, climate avatars enable\nlay users to make sense of complex climate data and uncertainties, tailored to their specific context\n(e.g., where they live or the sector in which they work). Similar smart agents engage with other\nenvironmental data sources, such as geospatial data, satellite imagery and ecological data. Once\ncreated, validated and trusted, these avatars can be deployed to interact with human users in\ndifferent contexts, for example: allowing expert and non-expert academics to interrogate complex\nfederated models of natural and human capital; enabling chatbots to explain extreme weather\nevents and provide warnings/guidance; providing timely responses to policy formulation queries; or\ndefusing toxic discourse on social media (Treen et al. 2020). Data ethics, governance and usability\nchallenges must be addressed in order to ensure that agents are explainable, trustworthy, and able\nto effectively influence public understanding in order to achieve positive social outcomes."}, {"title": "Financial Stability", "content": "Modern financial technology (FinTech) presents major challenges for both regulation (cf. the UK\nGovernment's 2021 Kalifa Review) and consumer protection/trust (cf. the UK Financial Conduct\nAuthority's 2022 Consumer Duty). These challenges can be addressed through collaboration with\nrelevant SMEs, non-profits, consultancies, national research organisations, data providers, platform\nproviders and national FinTech hubs, to co-create personalised AI for early warning indicators,\ninforming financial decision making, and reducing vulnerability to manipulation."}, {"title": "Healthcare Ecosystems", "content": "The ability of the UK's NHS to deliver its Long-Term Plan depends critically on its capacity to\nautomate bespoke monitoring and support for a range of long-term health conditions at population\nscale (Topol 2019). This cannot be achieved without a step change in the use of longitudinal data\nanalysis and smart software assistants. Doing so will require working in close collaboration with\nclinical partners and technology firms on healthcare analytics and the design of user experience for\nhealthcare AI.\nAnonymised patient records that track, e.g., mental health consultations or diabetes progression\nare complex, partial and noisy reflections of longitudinal patient trajectories with potential to\nimprove clinical decision making and empower patients (Rajkomar et al. 2019). The UK's NHS\ntrusts and healthcare technology firms have extensive expertise in leveraging data to manage and\ntreat these conditions, including cohorts of diabetes patients engaged in the co-design of AI systems\nfor collective intelligence that are trustworthy and effective (Duckworth et al. 2024).\nHere, two interacting strands of research can be identified: (i) machine learning analytics for\ncollective healthcare data, and (ii) co-design of smart healthcare agents for patient collectives.\nStrand (i) develops methods for unsupervised extraction and quantification of patterns from pa-\ntient data pooled across heterogeneous sources from clinical systems to networked personal devices\nin order to discover clusters in symptom trajectories, detect adverse events, and recommend treat-\nment and self-care strategies. This work leverages cutting-edge privacy preserving and federated\nmachine learning methodologies to enable machine learning on data across all the sources inter-\nactively and in real-time while guaranteeing that the identity of the individual patients and the\ndata they provide will not be leaked through for example training-data leakage attacks (Chen &\nCampbell 2022). Strand (ii) works with hard-to-reach patients (those suffering from secondary"}, {"title": "Cross-cutting Themes", "content": "The application domains described above are by no means the only areas in which AI for collec-\ntive intelligence has strong potential. Additional problems for which productive work could have\nsignificant transformative effects include preventing violent extremism (Smith et al. 2020, Bullock\n& Sayama 2023), addressing the climate crisis (G\u00f3is et al. 2019), collaborating with autonomous\nsystems (Pitonakova et al. 2018, Hart et al. 2022), and reducing energy consumption (Bourazeri &\nPitt 2018). However, in addition to confronting issues specific to each of these individual use cases,\nachieving AI for collective intelligence also faces challenges that cut across these application areas."}, {"title": "Human-Centred Design", "content": "One issue vital to developing AI for collective intelligence within any use domain is achieving\nsuccessful interaction with human users. Methods from social and cognitive psychology and human\nfactors must be integrated with the various kinds of research activity outlined above in order to\nderive human-centred design principles for effective, trustworthy AI agents that inform behavioural\nchange at scale within socio-technical human-AI collectives.\nThree parallel strands can be identified: (i) bringing human-centred design considerations to the\nwork within various domain-specific AI for collective intelligence research themes; (ii) developing\nusable smart agents that assist users in accessing, understanding, and acting on guidance derived\nfrom collective intelligence data; and (iii) pursuing fundamental questions related to understand-\ning and managing \u201ctipping points\" in collective intelligence systems. For (i), participatory design\nmethods (Bratteteig et al. 2012) involving academics and stakeholders can be employed to proto-\ntype human-machine interfaces (HMIs) for the AI systems being developed. For (ii), data from\nhuman experiments can inform a series of design iterations, focussing on accessibility, usability, ex-\nplainability, adaptability and trust (Choung et al. 2023), drawing upon long-standing approaches\nto defining and measuring trust in automation (e.g., Lee & See 2004), with all being key factors\nfor the acceptance, adoption and continued use of new technologies. Comparative analyses of these\ndata reveal transfer effects between different theme settings, guiding development of demonstrators\nwithin each domain. For (iii), testable predictions of how to identify, characterise and influence tip-\nping point thresholds for behaviour change can be derived from data on explainability, confidence,\npersistent adoption, praise and blame, e.g., based on the perceived capability of the system (Zhang\net al. 2024).\nRigorous empirical methods (including controlled experiments and human simulations) must\nbe informed by relevant psychological theory (e.g., Gibsonian affordances; see, e.g., Greeno 1994),\nhuman factors approaches (e.g., hierarchical task analysis; see, e.g., Stanton 2006), tools (e.g.,\nvigilance protocols, Al-Shargie et al. 2019) and measures (e.g., of situational awareness and cognitive\nload; Zhang, Yang, Liang, Pitts, Prakah-Asante, Curry, Duerstock, Wachs & Yu 2023, Haapalainen\net al. 2010), and tipping point analytics (e.g., autocorrelation and critical slowing down measures,"}, {"title": "Infrastructure and Governance", "content": "A second cross-cutting issue vital to deploying AI for collective intelligence at scale in any use\ndomain is ensuring that such systems have robust infrastructure and governance (I&G) guided by\nappropriate regulations and principles, delivering trustworthy AI systems and solutions that are\nhuman-centred, fair, transparent, and interpretable for the diverse range of end users.\nNew I&G tools and guidelines for national scale collective AI systems that ensure privacy, quality\nand integrity of data, and control access to data and systems across relevant AI infrastructures\n(Aarestrup et al. 2020, Shi et al. 2022, Cao et al. 2023) should be informed by experiences with\nprevious large-scale AI platforms, such as that developed within the EU-wide MIDAS project (Black\net al. 2019). One key context in which to pursue this challenge is work developing effective applied\nAI for national-scale healthcare systems (e.g., cancer, dementia, arthritis; Tedesco et al. 2021,\nBehera et al. 2021, Henderson et al. 2021). The data sensitivity and outcome criticality of these\nchallenges for AI makes this an ideal domain in which to develop effective I&G tools and thinking.\nHowever, considering these issues across multiple diverse applications domains also enables the\nunique and novel aspects of those settings to inform new thinking on I&G questions.\nSuch research must address ethical, legal, and social aspects (ELSA) of I&G (Van Veenstra et al.\n2021), embedding ELSA accountability in robust governance frameworks and data infrastructure\nplans. For instance, studies should embed ELSA, FAIR Principles and the Assessment List\nfor Trustworthy Artificial Intelligence (ALTAI) into their Data Management Plans (DMPs) and\ninfrastructure governance and should be informed by outputs of the Artificial Intelligence Safety\nInstitute (AISI) and other relevant guidelines, e.g., the EU's Ethics Guidelines for Trustworthy\nAI, and relevant regulation frameworks such as the EU AI Act.\nFinally, the prospect of truly national-scale AI systems of the kinds being considered here fore-\ngrounds the pressing need for truly trans-national governance structures and mechanisms. These\nare particularly relevant in collective intelligence settings, since the people, diseases, finance, etc., at\nthe heart of such systems, and the data pertaining to them, all transcend national boundaries. As\nthe Final Report of the United Nation's AI Advisory Body puts it \"the technology is borderless\",\nnecessitating the establishment of \"a new social contract for AI that ensures global buy-in for a\ngovernance regime that protects and empowers us all\u201d (UN AI Advisory Body 2024)."}, {"title": "Research Strategy", "content": "To make significant progress across the research strands outlined above, an effective AI for collective\nintelligence research strategy must also consider the set of meta-level research challenges that must\nbe overcome if academic research findings are to translate into effective and impactful real-world\noutcomes. There include addressing underpinning issues around stakeholder engagement; equality,\ndiversity and inclusion (EDI); environmental sustainability; and responsible research and innovation\n(RRI)."}, {"title": "Stakeholder Engagement", "content": "We distinguish here between three categories of research stakeholder relevant to AI for collective\nintelligence research: data partners, skills partners and academic partners. These categories are\nnot disjoint since a single organisation may play more than one of these roles, but they do serve to\ndistinguish between different kinds of research interaction that may be necessary in order to achieve\nsuccessful applied research in the AI for collective intelligence space at national or trans-national\nscale.\nData Partners are \u201cproblem owning\" organisations willing to provide controlled access to data,\nexpertise, tools, personnel and strategic guidance relevant to a societal challenge or user need that\ncan be addressed by AI for collective intelligence research. For national-scale efforts, these will\ntend to be national or trans-national agencies (e.g. the UK's National Health Service or the UK\nHealth Security Agency) and departments within national government (e.g., the UK's Department\nfor Health and Social Care), but may also include commercial outfits such as pharmaceutical firms\ninvolved in vaccine development, etc. Crucial issues for research collaboration here include those\nsurrounding intellectual property, privacy, regulatory frameworks (e.g., GDPR in the EU), secure\ndata hosting, etc. There are also challenges around the emerging role of synthetic data as a\nsubstitute for data that is too sensitive to share or is too hard to anonymize. Such synthetic data\ncan be useful where a mature understanding of the underlying real-world system and the data\ngenerating process is in place, but can be problematic in the absence of such an understanding\nsince it can be difficult to provide assurances that the synthetic data captures all of the necessary\nstructural relationships that are present in the original (poorly understood) dataset (Whitney &\nNorman 2024). More generally, issues around incomplete or noisy data or data that is not sufficiently\nrepresentative of the underlying population are familiar problems that have significance here.\nSkills Partners are \u201cproblem solving\" organisations that are already involved in pioneering the\nAI and collective intelligence skills, tools and technologies that are driving the next generation of\nAI for collective intelligence applications, e.g., multi-agent systems, collective systems data science,\nadvanced modelling and machine learning, AI governance and ethics, etc. These may include\nblue-chip outfits and national facilities (e.g., the UK's Office of National Statistics) but will also\ninclude many of the small and medium-sized enterprises (SMEs) emerging in this space (e.g.,\nFlowminder, who leverage decentralised mobility data to support humanitarian interventions in\nreal-time). Research opportunities here include connecting innovating skills partners to the data\npartners that require their expertise while navigating the intellectual property and commercial\nsensitivity issues that surround an emerging (and therefore somewhat contested) part of the growing\nAI consultancy sector.\nAcademic Partners are individuals, research groups or larger academic research activities that\nare operating in the AI for collective intelligence space. This is a growing area of activity and a"}, {"title": "Equality, Diversity and Inclusion", "content": "The AI workforce lacks diversity (e.g., Young et al. 2021). Moreover, AI technology can tend to\nimpose and perpetuate societal biases (e.g., Kotek et al. 2023). Consequently, it is important that\nAI for collective intelligence research operations be a beacon for best practice in equality, diversity\nand inclusion (EDI). Moreover, an effective AI for collective intelligence research strategy should\nitself also be driven by equality, diversity and inclusion research considerations. The emerging \"AI\nDivide\" separating those that have access to, and command of, powerful new AI technologies from\nthose that do not threatens to further marginalise under-represented, vulnerable, and oppressed\nindividuals and communities (Wang et al. 2024). Collective intelligence methods sometimes focus\non achieving consensus and collective agreement (which can tend to prioritise majority views and\nexperiences). However, in addition to aggregating signals at a population level in order to inform the\nhigh-level policies and operations of national agencies (which will themselves benefit from being\nsensitive to population heterogeneity), the AI for collective intelligence research described here\nis equally interested in deriving bespoke guidance for individuals (or groups) that respects their\nspecific circumstances and needs. This aspect of the research strategy explicitly foregrounds the\nchallenge of reaching and supporting diverse users and those that are intersectionally disadvantaged,\ne.g., diabetes patients that also have mental health conditions (Benton et al. 2023)."}, {"title": "Environmental Sustainability", "content": "The carbon footprint of most academic research is dominated by travel (Achten et al. 2013). Conse-\nquently, research in this area, like any other, should seek to minimise the use of flights and consider\nvirtual or hybrid meetings wherever possible. Other steps that can be taken to reduce the envi-\nronmental impact of research practice and move towards \"net zero\" and \"nature positive\" ways of\nworking include making sustainable choices for procurement (e.g., accredited sustainable options)\nand catering (e.g., plant-based food choices).\nMoreover, the environment can itself be the focus of AI for collective intelligence research (see\n\u201cEnvironmental Intelligence\u201d, above) or a key driving factor (see \"Smart City Design\", above).\nMany associated research themes are consistent with a sustainability agenda in their motivation\nto achieve effective interventions at scale without consuming vast resources. Intended outcomes\nand technologies aim to transition society to more sustainable practices (e.g., by using healthcare\nresource more efficiently, by encouraging sustainable cities, etc.).\nHowever, in common with AI research more generally, AI for collective intelligence makes use of\nenergy-intensive technologies. Computational research is energy-intensive: machine learning incurs\nhigh CPU/GPU energy cost for training (Patterson et al. 2021), while storage/transfer/duplication\nof large datasets consumes energy in data centres. Hardware components use rare metals linked\nto environmental damage and inequalities. Consequently, the environmental impacts of AI for\ncollective intelligence research should always be considered and reduced using, inter alia, compu-\ntational resources powered by renewable energy, energy efficient algorithms and coding practices,\nand minimal data duplication. The technologies developed through this kind of research should"}, {"title": "Responsible Research and Innovation", "content": "Researchers can never know with certainty what future their work will produce, but they can agree\non what kind of future they are aiming to bring about, and work inclusively towards making that\nhappen (Owen et al. 2013, Stilgoe et al. 2020). For AI for collective intelligence research, this means\nworking with diverse end users and stakeholders to produce a future in which national-scale AI for\ncollective intelligence systems are tools for societal good (Leonard & Levin 2022).\nThere are several reasons for taking responsible research and innovation (RRI) concerns espe-\ncially seriously in the context of artificial intelligence research projects. First, since AI is one of the\n17 sensitive research areas named in the UK's National Security and Investment Act, particular\ncare must be taken by UK universities when establishing and pursuing AI research collaborations.\nStakeholder partners must be vetted, and input from the UK Government's Research Collaboration\nAdvisory Team (RCAT) must be sought where there are concerns regarding, for instance, the\nexploitation of intellectual property arising from the research activity. Moreover, applied AI re-\nsearch is often fuelled by data that is sensitive, meaning that huge care must be taken to safeguard\nthis data and ensure privacy through the use of, e.g., secure research data repositories that feature\nrobust controlled data access protocols. More generally, since AI innovations have the potential\nto radically reshape the future in ways that are very hard to predict, articulating a clear shared\nvision of the future that is being aimed for is particularly important. Finally, AI researchers have a\nresponsibility to engage with the public discourse around AI which is currently driving considerable\nanxiety and confusion."}, {"title": "Unifying Research Challenges", "content": "The research strategy outlined here sets out to develop, build and evaluate systems that exploit\nmachine learning and AI to achieve improved collective intelligence at multiple scales: driving\nimproved policy and operations at the level of national agencies and offering bespoke guidance and\ndecision support to individual citizens.\nWhy are systems of this kind not already in routine operation? Many of the component tech-\nnologies are established and some are becoming reasonably well understood: recommender systems\n(Resnick & Varian 1997), machine learning at scale (Lwakatare et al. 2020), networked infrastruc-\nture and Internet devices (Radanliev et al. 2020, Rashid et al. 2023), conversational AI (Kulkarni\net al. 2019), network science analyses (B\u00f6rner et al. 2007), etc. However, several important and\ninteracting challenges obstruct the realisation of AI for collective intelligence and these must be\ntargets for this research effort. Here, we distinguish three categories: human challenges, technical\nchallenges and scale challenges. Each application domain manifests a combination of challenges in a\ndistinctive way (see Table 1), but since these challenges are inter-related they should be approached\nholistically."}, {"title": "Human Challenges", "content": "In order to be successful, the systems developed at the boundary between AI and collective intelli-\ngence must actually be engaged with and used by individual people as well as by institutions and\nagencies. For this to be the case, these systems must be trusted. Individual people must trust\nthe systems with their data and must trust the guidance that they are offered. Institutions and\nagencies must trust the reliability of the aggregated findings delivered by the systems and must\ntrust that the systems will operate in a way that does not expose them to reputational risk by dis-\nadvantaging users or putting them at risk. The European Commission's High-Level Expert Group\non Artificial Intelligence suggests that trust in AI should arise from seven properties: empowering\nhuman agency, security, privacy, transparency, fairness, value alignment, and accountability.\nWhat are the hallmarks of these properties that users of AI for collective intelligence systems\nintuitively and readily recognise? What kinds of guarantees for these properties could be credibly\noffered to regulators or law makers? More generally, how can users of all kinds become confident\nthat these properties are present in a particular system and remain confident as they continue to\ninteract with it?\nMoreover, since the systems envisioned here purport to offer bespoke decision support tailored\nto the needs of individual users, one acute aspect of this challenge relates to supporting the needs of\nall kinds of user including those from marginalised or under-represented groups. It is typically the"}, {"title": "Technical Challenges", "content": "Amongst the many technical challenges that must be overcome to enable AI for collective intel-\nligence to be effective, we will mention three: nonstationarity in collective systems, privacy and\nrobustness of multi-level machine learning, and the ethics of multi-agent collective decision support.\nAll machine learning makes a gamble that the future will resemble the past, yet we know that the\ndata from collective systems can be nonstationary, i.e., these systems can make transitions between\nregimes that may differ radically from one another (Scheffer et al. 2009). How can we anticipate\nand detect these sudden shifts, phase transitions, regime changes, and tipping points at the level of\nentire collectives, sub-groups and even individuals? These questions have been considered within\ncollective intelligence research (Mann 2022, Tilman et al. 2023), and the large scale of systems\nunder study here offers potential to trial methods that have been applied to physical and biological\nsystems, e.g., early warning signals from dynamical systems theory (Scheffer et al. 2009), but can\nthese be effective for complex fast-moving socio-technical systems?\nAmongst the many other machine learning challenges relevant here, we will highlight two that\narise as a consequence of the fundamentally multi-level nature of collective intelligence. The AI4CI\nLoop (Fig. 1) depicts the way in which the approach to AI for collective intelligence being pursued\nhere involves machine learning models that deliver findings at different levels of description, from\nfindings that characterise the entire collective through intermediate results related to sub-groups\nwithin the collective to bespoke results relevant to individual members of the collective. Delivering\nthis requires (likely unsupervised) methods to cluster and/or unbundle heterogeneous data stream\ntrajectories derived from groups and individuals. Moreover, achieving this whilst maintaining the\nprivacy of individual members of the collective requires robust privacy-preserving machine learning\nmethods. Employing foundation models to capture and compress the patterns in the collective\nsystem is one possible approach, but understanding the vulnerabilities of these models remains an\nopen research challenge (Chen et al. 2023, Messeri & Crockett 2024).\nWithin collectives, one member's actions can affect other members. In this context, systems\nthat support decision making do not only impact their direct user (Ajmeri et al. 2018, Vinitsky\net al. 2023). While there is potential to leverage this collective decision making to achieve efficient\ncoordinated outcomes (Jacyno et al. 2009), it remains the case that a typical AI agent tends to cater\nto the interests of their primary user even if they are intended to reflect the preferences of multiple\nstakeholders (Murukannaiah et al. 2020). This may reinforce existing privileges and could worsen\nthe challenges faced by vulnerable individuals and marginalised groups. Thus, it is imperative that\nAI agents consider and communicate the broader collective implications of the decision support\nthat they offer. In this way we can encourage these agents to respect societal norms and their\nstakeholders' needs and value preferences, and inform decisions that promote fairness, inclusivity,\nsustainability and equitability (Murukannaiah et al. 2020, Woodgate & Ajmeri 2022, 2024)."}, {"title": "Scale Challenges", "content": "This paper has articulated a set of research challenges in terms of \"producing national-scale AI for\ncollective intelligence\". For some domains, e.g., pandemic response, this scale might appear to be\na natural level of description because relevant policy, operations, data and governance are all ulti-\nmately defined at the level of national government. However, most if not all collective intelligence\nchallenges engage with multiple spatial, social and governmental scales. Decision making at na-\ntional, regional, local, household and personal scales are simultaneously in play, and in some cases\ntrans-national scales are also significant as when pandemics, environmental disasters or financial\ncontagion cross national borders. Consequently, the adjective \u201cnational-scale\" should not be taken\nhere to imply a single scale of operation and a single locus of decision making. Rather, the most\neffective AI for collective intelligence systems will be able to operate in a hierarchical, cross-scale\nfashion as anticipated in the work of, e.g., Ostrom"}]}