{"title": "Related Knowledge Perturbation Matters: Rethinking Multiple Pieces of Knowledge Editing in Same-Subject", "authors": ["Zenghao Duan", "Wenbin Duan", "Zhiyi Yin", "Yinghan Shen", "Shaoling Jing", "Jie Zhang", "Huawei Shen", "Xueqi Cheng"], "abstract": "Knowledge editing has become a promising approach for efficiently and precisely updating knowledge embedded in large language models (LLMs). In this work, we focus on Same-Subject Editing, which involves modifying multiple attributes of a single entity to ensure comprehensive and consistent updates to entity-centric knowledge. Through preliminary observation, we identify a significant challenge: Current state-of-the-art editing methods struggle when tasked with editing multiple related knowledge pieces for the same subject. To address the lack of relevant editing data for identical subjects in traditional benchmarks, we introduce the S2RKE (Same-subject Related Knowledge Editing) benchmark. Our extensive experiments reveal that only mainstream locate-then-edit methods, such as ROME and MEMIT, exhibit \"related knowledge perturbation,\" where subsequent edits interfere with earlier ones. Further analysis reveals that these methods over-rely on subject information, neglecting other critical factors, resulting in reduced editing effectiveness.", "sections": [{"title": "1 Introduction", "content": "The dynamic nature of real-world knowledge necessitates efficient methods for updating specific facts in large language models (LLMs) (Achiam et al., 2023; Touvron et al., 2023) without compromising their overall performance. Knowledge editing(a.k.a., model editing) (Yao et al., 2023) has emerged as a promising solution to address this challenge, enabling targeted updates to model parameters without requiring full retraining. Among existing methods, locate-then-edit methods, such as ROME (Meng et al., 2022a) and MEMIT (Meng et al., 2022b), have shown effectiveness in making precise modifications to Transformer layer parameters (Vaswani, 2017). However, their broader applicability across diverse editing scenarios remains insufficiently explored.\nIn particular, Same-Subject Editing, modifying multiple attributes of a single entity, plays a critical role in ensuring comprehensive and consistent updates to entity-centric knowledge. As shown in Figure 1, an entity like \"James\" may require simultaneous edits to attributes such as \"isCitizenOf,\" \"playsFor,\" and others. This process refines the entity's representation by resolving attribute conflicts and synchronizing interdependent facts. Despite its significance, same-subject editing has largely been overlooked in existing research.\nThrough preliminary observations, we identify an unusual failure: Some top-performing editing methods struggle to edit multiple related knowledge pieces for the same subject."}, {"title": "2 Preliminary", "content": null}, {"title": "2.1 Knowledge in Language Model", "content": "Autoregressive, decoder-only large language models (LLMs) process a sequence of tokens x = [x1,...,XT] \u2208 X, where each token xi \u2208 Vis drawn from a vocabulary V, and predict the probability distribution y \u2208 Y CR|V| for the next token in the sequence. In the Transformer architecture, each token xi is embedded into a sequence of hidden state vectors h h, starting from the initial embedding ho (0) = emb(xi) + pos(i) \u2208 RH. The final output y = decode(h)) is derived from the last hidden state of the sequence. At each layer l, the hidden state his updated through a combination of global attention aft) and contributions from the local MLP m), where each token attends only to preceding tokens:\n$$h^{(l)} = h^{(l-1)} + a^{(l)} + m^{(l)},$$ (1)\n$$a^{(l)} = attn^{(l)}(h^{(l-1)},...,h^{(l-1)}),$$ (2)\n$$m^{(l)} = W^{(l)}_{proj}(\u03c3(W^{(l)}_{gate}(a^{(l)}+h^{(l-1)})).$$ (3)\nIn many previous studies, knowledge has been represented as triples (s, r, o), where s, r, and o denote subject, relation, and object respectively (e.g., James (s), playsFor (r), and Lakers (0))(Meng et al., 2022a; Li et al., 2024a). Researchers typically design natural language templates for each relation type and integrate these templates with subject terms to create question or cloze-style prompts."}, {"title": "2.2 Same-Subject Editing", "content": "In a broad sense, knowledge editing aims to allow for the querying and modification of a wide range of facts within language models by combining different subjects (s) and relations (r) as prompts. Existing work typically focuses on modifying individual facts expressed as (s,r,o) \u2192 (s,r,o\u2217), where each subject (s) is associated with a specific relation (r). However, traditional editing often isolates the editing process to a single relation. This leads to the discontinuation of further knowledge edits for the same subject and a shift towards editing knowledge for a new subject. It risks overlooking potential perturbations in knowledge when editing multiple related facts for the same subject.\nWe introduce the concept of Same-Subject Editing, where multiple relations are edited simultaneously for a single subject. Instead of focusing solely on the traditional (s,r, o) format, we extend the editing process to structured prompts such as (s, R, O), where R = {r} 1 represents a set of relations and O = {0}1 represents their corresponding objects. For example, {(\"James\", \"playsFor\", \"Lakers\"), (\"James\", \"isCitizenOf\", \"USA\")}. We formally define the edited fact set as e = (s, ri, oi)=1 and define the edited model as M* = F(M,e), where F is the editing function that updates the original model M. It ensures that knowledge updates remain consistent across all related attributes of the same subject."}, {"title": "3 Pilot Observation", "content": "In this section, we conduct a pilot observation to reveal potential issues with same-subject editing.\nEvaluation Setup. We focus on using MEMIT (Meng et al., 2022b) to edit GPT-J (Wang and Komatsuzaki, 2021), since their excellent performance in editing multiple pieces of knowledge. To analyze the impact of editing density-defined here as the average number of related edits per subject in the editing sequence-we divide our experimental schemes into three categories:\na) High-Density: Edit n pieces of knowledge in total, with each subject edited for 3 related pieces of knowledge.\nb) Medium-Density: Edit n pieces of knowledge in total, with each subject edited for 2 related pieces of knowledge.\nc) Low-Density: Edit n pieces of knowledge in total, with each subject edited for 1 related pieces of knowledge.\nBased on the above schemes, we select qualified data from COUNTERFACT (Meng et al., 2022a) and conduct experiments using both sequential-editing and batch-editing (See Appendix A.2 for comparison of sequential- and batch-editing). The editing performance is comprehensively evaluated across four dimensions: efficacy, generalization, specificity, and overall performance (See Appendix C.3 for detailed metric descriptions).\nResult & Analysis. Figure 2 and Figure 8a show the experimental results of employing MEMIT to edit GPT-J through sequential-editing and batch-editing, respectively. It is evident that when editing the same number of knowledge, the denser the subject distribution, the worse the editing performance, while the impact on the model's downstream performance remains similar. However, the scarcity of sufficiently dense same-subject instances in existing editing datasets limits the scope of experimental verification. We will further investigate this phenomenon in subsequent sections."}, {"title": "4 Related Knowledge Perturbation", "content": "Furthermore, we construct a benchmark and evaluate the performance of editing methods when editing related knowledge for the same subject."}, {"title": "4.1 S2RKE Benchmark", "content": "We introduce the S2RKE (Same-subject Related Knowledge Editing) benchmark, specifically designed to facilitate the editing of multiple related pieces of knowledge for each subject. It covers six categories of subjects, comprising of 4,503 subjects and 43 relationships, with each subject having an average of 4.9 related knowledge items. See Appendix B for additional technical details about its construction and Table 1 for comparison of statistics between S2RKE and COUNTERFACT."}, {"title": "4.2 Failure of Editing Methods", "content": "Editing Methods. We evaluate six widely-used editing methods: ROME (Meng et al., 2022a), MEMIT (Meng et al., 2022b), PMET (Li et al., 2024a), FT (Zhu et al., 2021), MEND (Mitchell et al., 2022a), and KN (Dai et al., 2022).\nSelected LLMs. Experiments are conducted on three LLMs with different parameter sizes: GPT-2 XL (1.5B) (Radford et al., 2019), GPT-J (6B) (Wang and Komatsuzaki, 2021), and LLaMA-2 (7B) (Touvron et al., 2023).\nWe design two experimental schemes to assess how editing related knowledge impacts performance: Same-Subject, where all edited knowledge shares the same subject, Different-Subject, where each edit involves a different subject. Experimental data are selected from the S2RKE benchmark.\nOur pilot observation indicates that while knowledge correlation impacts editing effectiveness, it has little effect on overall model performance. So we focus on the Score(S) metric and introduce the Score Difference (SD) metric, defined as SD = Score(same-subject) \u2013 Score(different-subject), to quantify performance degradation when editing related knowledge for the same subject. To ensure reliability, each test was repeated 30 times with different editing instances. See Appendix C for more details."}, {"title": "4.3 Analysis of Failures", "content": "We further examine how the sequence of knowledge edits affects locate-then-edit methods by isolating the interference of sequential updates. For this purpose, we devised two experimental settings: Homogeneous-Editing, where the first and last edits target the same subject, and Heterogeneous-Editing, in which they target different subject. Experiments were performed using ROME, MEMIT, and PMET across three LLMs, with each configuration repeated 30 times on different instances from the S2RKE benchmark to ensure robust results.\nResult & Analysis. Figure 4 shows the sequential-editing results on GPT-2 XL and GPT-J, while Figures 7 and 8c provide additional results. Under the Homogeneous-Editing setting, the initial edit's score is much lower than in the Heterogeneous-Editing condition. This clearly indicates that later edits interfere with earlier ones. We call this effect \"related knowledge perturbation,\" which exposes a key limitation of current locate-then-edit approaches when processing multiple sequential updates. These findings highlight the need for better strategies in managing sequential knowledge updates. The next section will analysis the causes of related knowledge perturbation."}, {"title": "5 Perturbation Analysis", "content": null}, {"title": "5.1 Causes of Perturbation", "content": "Our experiments show that only mainstream locate-then-edit methods (e.g., ROME and MEMIT) exhibit related knowledge perturbation. These methods all employ causal tracing to identify that factual knowledge is primarily stored in the early MLP layers of LLMs. Based on the hypothesis that \"the MLP modules in Transformer layers can be viewed as linear key-value associative memory,\" (Geva et al., 2020) they solve for Wk = v, where W represents the downsampling component Wproj of MLP, and the key-value pair (k, v) corresponds to a factual triplet t = (s, r, o), as shown in Figure 5. Here, k represents the subject s, while v encodes the attributes of s, including r and o. To update t to t* = (s, r, o*), they compute a new key k\u2217 and value v\u2217 via an update AW.\nHowever, k\u2217 is only derived from the input of the subject's last token in the MLP module's down-sampling layer:\n$$k* = \\frac{1}{N}\\sum_{i=1}^{N} K(x_{i \\oplus p}),$$ (4)\nwhere K is the output of the first MLP layer in transformer block, xi represents the randomly sampled prefixes, and \u2295 denotes the string concatenation operator.\nTherefore, we speculate that \"related knowledge perturbation\" stems from an over-reliance on subject information. When editing multiple pieces of knowledge for the same subject s, the key value k* remains constant, causing later edits to interfere"}, {"title": "5.2 Experiment Validation", "content": "To verify the above speculation, we used MEMIT to edit two pieces of knowledge on GPT-J through sequential-editing and batch-editing, designing two experimental schemes: Same-Subject and Different-Subject. We then examine the relationship between the cosine similarity of the two keys and the Efficacy Success of editing the first piece of knowledge. Cosine similarity was chosen because it measures how similar the two keys are in vector space, helping us understand how closely related the two knowledge pieces are.\nResult & Analysis Figure 6 shows the relationship between key similarity and the first knowledge editing Efficacy Success. The results indicate that when two pieces of knowledge related to the same subject are edited, the CS of the key approaches 1. Meanwhile, the ES of editing the first piece of knowledge is significantly lower compared to the case where the two edited pieces of edited knowledge are related to different subjects. This supports our hypothesis that since the key calculation only focuses on subject information, subsequent edits for the same subject interfere with earlier ones, leading to \"related knowledge perturbation\"."}, {"title": "6 Conclusion", "content": "In this paper, we identify a key limitation of mainstream locate-then-edit methods, called \"related knowledge perturbation\", which occurs when editing multiple related pieces of knowledge for the same subject. Using the S2RKE benchmark, we show through experiments that over-reliance on subject information leads to interference between subsequent edits, highlighting the challenges in same-subject editing."}, {"title": "7 Limitation", "content": "We acknowledge several limitations in our work. First, while this paper provides an initial exploration into the complex correlations between knowledge and identifies the phenomenon of related knowledge perturbation, it does not propose a comprehensive solution to address this issue. This omission leaves room for future research to develop effective mitigation strategies.\nAdditionally, due to computational resource constraints, our experiments did not extend to larger language models, such as Llama2-13b. Future investigations could benefit from testing our findings on such models to further validate the effectiveness and generalizability of the observed phenomena."}]}