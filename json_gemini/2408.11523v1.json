{"title": "LARR: Large Language Model Aided Real-time Scene Recommendation with Semantic Understanding", "authors": ["Zhizhong Wan", "Bin Yin", "Junjie Xie", "Fei Jiang", "Xiang Li", "Wei Lin"], "abstract": "Click-Through Rate (CTR) prediction is crucial for Recommendation System(RS), aiming to provide personalized recommendation services for users in many aspects such as food delivery, e-commerce and so on. However, traditional RS relies on collaborative signals, which lacks semantic understanding to real-time scenes. We also noticed that a major challenge in utilizing Large Language Models (LLMs) for practical recommendation purposes is their efficiency in dealing with long text input. To break through the problems above, we propose Large Language Model Aided Real-time Scene Recommendation(LARR), adopt LLMs for semantic understanding, utilizing real-time scene information in RS without requiring LLM to process the entire real-time scene text directly, thereby enhancing the efficiency of LLM-based CTR modeling. Specifically, recommendation domain-specific knowledge is injected into LLM and then RS employs an aggregation encoder to build real-time scene information from separate LLM's outputs. Firstly, a LLM is continual pretrained on corpus built from recommendation data with the aid of special tokens. Subsequently, the LLM is fine-tuned via contrastive learning on three kinds of sample construction strategies. Through this step, LLM is transformed into a text embedding model. Finally, LLM's separate outputs for different scene features are aggregated by an encoder, aligning to collaborative signals in RS, enhancing the performance of recommendation model.", "sections": [{"title": "1 INTRODUCTION", "content": "As an important task in field of recommendation systems (RS), Click-Through Rate (CTR) prediction aims to forecast whether a user will click on a certain product, content, or advertisement. In the domain of RS, models need to analyze data such as user characteristics and item features to predict the probability of a user clicking on a specific objective, thereby helping to optimize the effectiveness of recommendation strategies.\nIn the context of food delivery service, the volume of data that needs to be processed each day reaches hundreds of millions. Unlike typical e-commerce recommendations or content recommendations, food delivery recommendations place a stronger emphasis on analyzing real-time scenes such as geographical location, mealtime, weather, etc. Among them, there exist strong correlations and rich semantics. Consider a specific scene: a user traveling to another city arrives late at night while many restaurants are already closed, and it is raining heavily with low temperatures. Due to the user's unfamiliarity with the surrounding environment and the poor weather conditions, there is a high probability that he will order a takeout from a nearby restaurant. What's more, he's more likely to order some hot food or local specialties.\nPoints of Interest (POI) is a concept we define based on food delivery recommendations, aimed at abstracting the focus of user interest. It can be understood as a collection of POIs, environment, and geographic locations. Traditional recommendation models use feature crossing to handle POIs in real-time scene data. For correlated and important features, common feature crossing methods include manual explicit crossing followed by logistic regression [5], explicit low-order crossing [5], or directly using deep learning for automatic high-order crossing [12, 15, 29]. However, regardless of which method is used, the essence of feature crossing is based on the co-occurrence probability of strongly correlated features to determine whether a user will click, lacking an understanding of the POI in the scene's semantic information."}, {"title": "2 RELATED WORK", "content": "Substantial work [8, 16, 37] has shown that Pre-trained models (PTMs) on a large corpus can learn universal language representations, which are beneficial for downstream NLP tasks and can avoid training a new model from scratch [26]. Large language models (LLMs) mainly refer to transformer-based [31] neural language models that contain tens to hundreds of billions of parameters [38]. Those language models are pre-trained on massive text data, such as PaLM [6], LLAMA [30], and GPT-4 [1]. Compared to PLMs, LLMs are not only significantly larger in model size but also demonstrate superior language understanding and generation capabilities, more importantly, they exhibit emergent abilities [32] that are absent in smaller-scale language models."}, {"title": "2.2 LLM in Recommend System", "content": "Traditional recommendation models are designed to leverage a huge amount of ID tokens to train, which is an ID paradigm, with the majority of parameters concentrated in the embedding layer [21, 29]. In contrast, LLMs would use a tokenizer to segment the text into vocab tokens to reduce the size of the vocabulary at the very first of input, which is a tokenization paradigm, with the bulk of parameters being concentrated in the network itself. Recommendation models excel in memorization, while LLMs demonstrate superior capabilities in logical reasoning and generalization. A 6-billion parameter LLM such as ChatGLM-3 [9] has a vocabulary less than 65,000 tokens, a scale that is significantly smaller than the number of ID tokens in a recommendation model of the same scale [23]. It's apparent that simple integration of recommendation systems with LLMs is infeasible. If ID tokens are directly used as input, tokenization may establish connections between unrelated IDs (e.g., 'id_499' tokenized into 'id', '_', '4', '9', '9', creating overlapping embeddings with IDs containing the digits 4 and 9, which does not meet our expectations). Alternatively, regard the ID tokens as special tokens of LLM's vocabulary, that is, without tokenization on ID input, would face gap problem between ID tokens representing user/item collaborative information and pre-trained vocab tokens holding content semantics information [4]. Moreover, the typically vast number of ID tokens could dilute the LLM's own vocabulary, inject noise into the vocab tokens, and result in poor learning of the ID tokens.\nDespite the challenges mentioned above associated with applying language models to the field of recommendation, researchers continue to dedicate efforts to applying language models to Recommender Systems (RS), due to the astonishing capabilities demonstrated by LLMs [20, 33]. The researchers of P5 [11] proposed a unified text-to-text paradigm recommendation model based on T5 [27], hoping to handle rating prediction task, sequential recommendation task and more downstream tasks by zero-shot or few-shots. An embedding method called whole-word embedding whose design inspiration is very similar to position embedding is introduced to address ID-related gap problem mentioned above. M6-rec [7] converts all recommend downstream tasks into language understanding or generation tasks by representing user behavior data and candidates data if necessary as natural language plain texts for fine-tuning based on M6 [22]. CLLM4Rec [39] propose a novel soft/hard prompting strategy, mutually-regularized pre-training two LLMs and two set of id tokens on two corpora to facilitate language modeling on RS-specific corpora with heterogeneous user/item collaborative tokens and content tokens.\nAs an unsupervised method, Contrastive Learning (CL) assumes some observed pairs of text that are more semantically similar than randomly sampled text. By maximazing their mutual information, neural network could learn useful embeddings for downstream tasks [2, 36]. OpenAI has attempted to convert pre-trained"}, {"title": "3 METHODOLOGY", "content": "In this section, we will then introduce the framework of LARR, which comprises three stages, with the overall structure illustrated in Fig 2. The first stage is the continual pretraining stage, where we construct natural language texts corpus from the Meituan Waimai dataset. Then we continue pretraining the LLM that has been pre-trained on general corpora based on corpus to inject the domain-specific knowledge. In the second stage, we additionally add billions of user profile and user history behavior data into the corpus, which is proven to be useful [10], constructing 3 kinds of contrastive learning positive and negative samples, transforming the LLM into a text embedding model, enabling the LLM to fully understand the semantics of real-time scenes. The final third stage is the multi-modal alignment stage, where contrastive learning is used to maximize the mutual information between semantic embeddings and collaborative embeddings, aligning the takeout scene semantic information understood by the LLM with the collaborative signals extracted by the fine-tuning model from ID tokens, cross features, and statistical features. The aligned semantic information will enhance performance of recommendation system."}, {"title": "3.1 Continual Pretraining Task Design", "content": "To facilitate LLM's understanding in food delivery domain-specific knowledge from the natural language perspective, we first construct a corpus using datasets relevant to all POIs. Following this, we perform continual pretraining task on the LLM which had already been pre-trained on generic corpora, using this corpus to enhance its understanding of domain-specific knowledge.\nHow to use id tokens in LLM efficiently remains an unsolved problem for a long-time. In industrial scene, encoding the ids of millions of different POIs as whole words is impractical due to the high time and memory costs involved. Moreover, simply treating POI id tokens as indivisible inputs to the LLM brings a semantic and collaborative signal gap, as mentioned in section 2.2; segmenting and encoding ids could also introduce unintended connections between id tokens. Consequently, we abandoned the use of id token as LLM inputs and instead used unique natural language description texts to represent id features. We noted that a POI can be uniquely identified by its its name and geographical location, which is equivalent"}, {"title": "3.2 Text Embedding via Contrastive Learning", "content": "There is a huge gap between the normal decoder-only architecture of LLM and text embedding models, because language models train and infer in a way of autoregressive approach. To efficiently leverage the semantic information from the LLM, a critical problem we faced is how to transform the LM into a text embedding model. Common methods for converting a language model into one that could encode a sentence into a embedding include using the output embedding corresponding to some special tokens as the sentence embedding, or directly use some pooling methods on embeddings of all words in sentence to obtain the sentence embedding. Inspired by OpenAI [24], we decide to adopt contrastive learning to convert the LLM into a text embedding model. In NLP field, contrastive learning methods usually construct positive sample pairs by applying corruptions such as dropout on text to generate positive sample pairs, random sampling for negative sample pairs. However, in RS, the importance of user, POI pairs in the food delivery context is very high, so we do not simply use corruption. Instead, we constructed positive and negative sample pairs from three perspectives: user-user, POI-POI, and user-POI.\nAs is shown in figure 3, the LLM tends to in classify three kinds of input is positive or not in contrastive learning procedure. First of all, because of user-side text is added, we utilize a set special tokens $s_{pu}$ for user-side text similar to $s_{pp}$ in section 3.1.\n$s_{pu} = \\{(s_{poos}, s_{peos}) | k  \\in N, n_k + 1 \\le k \\le n_k + n_q\\}$ (8)"}, {"title": "3.3 Information Alignment in RS", "content": "In this section, contrastive learning is applied to alignment, maximizing the mutual information between the semantic information of the food delivery scene understood by LLM and the collaborative signals extracted from ID tokens, cross features, and statistical features by the RS model. The alignment not only extracts the shared information between semantic and collaborative signals but also reduces the noise from both, significantly enhancing the performance of the recommendation.\nSpecifically, our RS utilizes real-time scene embedding produced by the LLM after continual pretraining in stage 1 and fine-tuning in stage 2. Throughout this process, all parameters of the LLM are frozen, meaning the LLM acts like an encoder, converting a scene description into continuous vector representations to aid the training of the recommendation model. Since we want the model to be industry-friendly, the LLM should avoid participating in real-time inference as much as possible. We select r real-time scene text features and decide to deal with scene text $s_i$ one by one, storing them in advance for the recommendation model to utilize, where $0 \\le i \\le r$.\n$h_i = LLM(S_i)_{s_{peos}}$ (22)\nHere $s_{peos}$ represents the position of corresponding key real-time scene feature's end special token. We use the embedding of end special token in last hidden layer to represent real-time scene text. In reality, we selected 10 real-time scene text, that is, r is set to 10.\nWe employ a bidirectional transformer encoder $\\S$ to tackle the problems that discrete real-time embeddings lack interaction. Since"}, {"title": "4 EXPERIMENT", "content": "Dataset. Our evaluation is conducted on a real-world dataset from Meituan Waimai. For the Click-Through Rate (CTR) prediction task, as shown in Table 1, we perform negative sampling on the log data collected from April 1st to April 7th, 2024, yielding approximately 3.5 billion training samples. For the test set, we uniformly sample the data from the subsequent day, April 8th, 2024, resulting in 84 million samples. Specifically, the input traditional features include user statistical features, real-time contextual features, user-POI cross-statistical features, POI statistical features, and the label,"}, {"title": "4.2 Performance Comparison", "content": "In this section, we provide a detailed comparison of the performance of our model, LARR, against various baselines. As shown in Table 2, an analysis of the experimental results reveals the following findings:\nFirstly, we observe that traditional recommendation system models, such as Wide&Deep and DeepFM, generally underperform more advanced deep learning models like PLE. This phenomenon suggests that complex deep network structures have significant advantages in learning feature interactions and capturing complex user interest patterns. By introducing deeper and more intricate interactions, these networks can more finely mine the underlying motivations behind user behavior, thereby achieving better accuracy in recommendations.\nSecondly, our experimental results indicate that the semantic model P5 does not perform well in the food delivery recommendation task. This may suggest that in recommendation systems, traditional statistical features can sometimes more directly reflect the actual interests of users compared to semantic features. Although semantic models can provide rich contextual information, statistical analysis of user historical behavior may more directly and effectively reveal user preferences in practical recommendation scenes.\nFinally, our model, LARR, outperforms all baseline models. Compared to the PLE model, LARR achieves a significant 0.58% improvement in CTR AUC and also gains an 0.34% increase in CTCVR AUC. These results not only confirm the superiority of our model but also emphasize the effectiveness of LARR in integrating the semantic information of real-time scenes with recommendation modality information. Through this integration, LARR can capture users' immediate needs and potential interests more accurately, significantly enhancing the overall performance and user satisfaction of the recommendation."}, {"title": "4.3 Ablation Study", "content": "We introduce a novel multimodal information fusion framework, LARR. Initially, based on Food Delivery recommendation data, we perform continual pretraining and fine-tuning on LLMs to infuse"}, {"title": "4.4 Hyperparameter Analysis", "content": "In Section 4.4, our aim is to align the deep semantic information of food delivery scenes as understood by LLMs with the collaborative signals extracted from ID tokens, cross-features, and statistical features by the conventional fine-tuning models. We seek to extract the shared information between semantic and collaborative data to enhance model performance. During the alignment phase of the model, we focus on two key hyperparameters: the temperature coefficient and the negative sample sampling ratio. The temperature coefficient plays a crucial role in the loss function of contrastive learning, adjusting the sensitivity of the loss and influencing the model's ability to distinguish between positive and negative samples. A lower temperature coefficient increases the difficulty for the model to differentiate samples, causing it to focus more on challenging sample pairs. Conversely, the negative sample sampling ratio controls the ratio of negative to positive samples during the contrastive learning process. In this section, we conduct experimental analysis on these two hyperparameters with respect to CTR AUC, with results depicted in Figure 4"}, {"title": "4.5 Case Study", "content": "We present a case study to further demonstrate the model's proficiency. Given that out-of-area ordering (where users place orders from locations other than their usual residence for various reasons) has always been an important scenario in food delivery services, we are particularly interested in whether the model can generate good recommendations for out-of-area users based on relevant semantics. We selected some random users who have never been to Guangdong, with the scene of \"user visiting Guangdong for vacation\", designed a simple recall experiment to observe the details of the top 10 POIs in terms of recall scores. A typical result is as follows, with each response presented as location; main dish. Sensitive information such as POI names and details have been omitted.\nTop1: Guangzhou; Rice Noodle Roll\nTop2:. Chaoshan; Stir-fried Beef Rice Noodles\nTop3:. Guangzhou; Stir-fried Beef Rice Noodles\nTop4:. Shenzhen; Seafood Claypot Congee\nTop5: Guangzhou; Cantonese Barbecue\nTop6: Guangzhou; Claypot Rice\nTop7: Guangzhou; Rice Noodle Roll\nTop8: Guangzhou; Sweet Soup\nTop9: Dongguan; Sweet Soup\nTop10: Foshan; Claypot Rice\nIn this case, Guangdong appeared the most frequently (5 times), with recalled POI primarily offering dishes such as Rice Noodle Roll, Stir-fried Beef Rice Noodles, Cantonese Barbecue, Claypot Rice, and Sweet Soup. ChaoShan, Shenzhen, Dongguan, and Foshan each appeared once or twice. This geographical distribution reflects the culinary diversity and popularity of Guangzhou as the capital city of Guangdong Province, which aligns with our survey findings that these dishes are representative of Guangdong's regional specialties.\nThe recall results cover a variety of classic Cantonese dishes, indicating that the model performs well in capturing local characteristics. The recall results show that the model can identify multiple different types of dishes and accurately match them to the provided scene text, this diversity is positive for enhancing user satisfaction and meeting a wider range of user needs."}, {"title": "4.6 Online A/B Test", "content": "LARR demonstrated its practical value in an online A/B test within the Meituan recommendation system. During the testing period from April 15th to April 21st, 2024, LARR achieved a 2.5% increase in CTR and a 1.2% growth in GMV (Gross Merchandise Volume) compared to the current baseline model. These results confirm the effectiveness of LARR in understanding user needs and enhancing user experience. The increase in CTR indicates a higher acceptance of recommended content by users, which may be attributed to"}, {"title": "5 CONCLUSION", "content": "To tackle the lack of understanding of semantic information in RS and the efficiency challenges in LLM-based CTR models, this paper proposes a novel LARR method. LARR continues to pretrain and fine-tune the LLM on some corpora with an enlarged vocabulary, using contrastive learning to align semantic signals and collaborative signals, leverage shared information, and reduce noise. Extensive online and offline experimental results demonstrate that LARR achieves low latency and enhances CTR performance, offering fresh insights for the practical deployment of LLM-based CTR models."}]}