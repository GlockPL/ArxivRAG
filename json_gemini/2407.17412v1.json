{"title": "(PASS) Visual Prompt Locates Good Structure Sparsity through a Recurrent HyperNetwork", "authors": ["Tianjin Huang", "Fang Meng", "Li Shen", "Fan Liu", "Yulong Pei", "Mykola Pechenizkiy", "Shiwei Liu", "Tianlong Chen"], "abstract": "Large-scale neural networks have demonstrated remarkable performance in different domains like vision and language processing, although at the cost of massive computation resources. As illustrated by compression literature, structural model pruning is a prominent algorithm to encourage model efficiency, thanks to its acceleration-friendly sparsity patterns. One of the key questions of structural pruning is how to estimate the channel significance. In parallel, work on data-centric AI has shown that prompting-based techniques enable impressive generalization of large language models across diverse downstream tasks. In this paper, we investigate a charming possibility - leveraging visual prompts to capture the channel importance and derive high-quality structural sparsity. To this end, we propose a novel algorithmic framework, namely PASS. It is a tailored hyper-network to take both visual prompts and network weight statistics as input, and output layer-wise channel sparsity in a recurrent manner. Such designs consider the intrinsic channel dependency between layers. Comprehensive experiments across multiple network architectures and six datasets demonstrate the superiority of PASS in locating good structural sparsity. For example, at the same FLOPs level, PASS subnetworks achieve 1% ~ 3% better accuracy on Food101 dataset; or with a similar performance of 80% accuracy, PASS subnetworks obtain 0.35\u00d7 more speedup than the baselines.", "sections": [{"title": "1 Introduction", "content": "Recently, large-scale neural networks, particularly in the field of vision and language mod-eling, have received upsurging interest due to the promising performance for both natural language [1\u20133] and vision tasks [4, 5]. While these models have delivered remarkable performance, their colossal model size, coupled with their vast memory and computational requirements, pose significant obstacles to model deployment. To solve this daunting challenge, model compression techniques have re-gained numerous attention [6\u201311] and knowledge distillation can be further adopted on top of them to recover optimal perfor-mance [12\u201314]. Among them, model pruning is a well-established method known for its capacity to reduce model size without compromising performance [15\u201317] and structural model pruning has garnered significant interest due to its ability to systematically eliminate superfluous structural components, such as entire neurons, channels, or filters, rather than individual weights, making it more hardware-friendly [18\u201321].\nIn the context of structural pruning for vision models, the paramount task is the estimation of the importance of each structure component, such as channel or filters. It is a fundamental challenge since it requires dissecting the neural network behavior and a precise evaluation of the relevance of individual structural sub-modules. Previous methodologies [20, 22\u201325] have either employed heuristics or developed learning pipelines to derive scores, achieving notable performance. Recently, the prevailingness of natural language prompts [26, 27] has facilitated an emerging wisdom that the success of AI is deeply rooted in the quality and specificity of data that is originally created by human [28, 29]. Techniques such as in-context learning [30\u201332] and prompting [33\u201337] have been developed to create meticulously designed prompts or input templates to escalate the output quality of LLMs. These strategies bolster the capabilities of LLMs and consistently achieve notable success across diverse downstream tasks. This offers a brand new angle for addressing the intricacies of structural pruning on importance estimation of vision models: How can we leverage the potentials within the input space to facilitate the dissection of the relevance of each individual structural component across layers, thereby enhancing structural sparsity?\nOne straightforward approach is directly editing input through visual prompt [38] to enhance the performance of compressed vision models [39]. The performance upper bound of this approach largely hinges on the quality of the sparse model achieved by pruning, given that prompt learning is applied post-pruning. Moreover, when pruning is employed to address"}, {"title": "2 Related Work", "content": "Structural Network Pruning. Structural pruning achieves network compression through entirely eliminating certain superfluous components from the dense network. In general, structural pruning follows three steps: (i) pre-training a large, dense model; (ii) pruning the unimportant channels based on criteria, and (iii) finetuning the pruned model to recover optimal performance. The primary contribution of various pruning approaches is located in the second step: proposing proper pruning metrics to identify the importance of channels. Some commonly-used pruning metric includes but not limited to weight norm [43\u201345], Tay-lor expansion [17, 46], feature-maps reconstruction error [47\u201350], feature-maps rank [51], KL-divergence [52], greedy forward selection with largest loss reduction [53], feature-maps discriminant information [54\u201356]."}, {"title": "3 PASS: Visual Prompt Locates Good Structure Sparsity", "content": "Notations. Let us consider a CNN with l layers, and each layer i contains its corresponding weight tensor \\(W^{(i)} \\in [R^{C_O \\times C \\times K^2 \\times K^2}\\], where \\({C_O, C, and K}\\) are the number of output/in-put channels and convolutional kernal size, respectively. The entire parameter space for the network is defined as \\(W = \\{W^{(i)}\\}_{i=1}^l\\). Similarly, a layer-wise binary mask is represented by \\(M^{(i)}\\), where \u201c0\u201d/\u201c1\u201d indicates removing/maintaining the associated channel. \\(V\\) denotes our visual prompts. \\((x, y) \\in D\\) denotes the data of a target task.\nRationale. In the realm of structural pruning for deep neural networks, one of the key challenges is how to derive channel-wise importance scores for each layer. Con-ventional mechanisms estimate the channel significance either in a global or layer-wise manner [20, 43, 48, 77], neglecting the sequential dependency between adjacency layers. Meanwhile, the majority of prevalent pruning methods are designed in a model-centric fash-ion [20, 22, 23, 43, 78, 79]. In contrast, an ideal solution to infer the high-quality sparse mask for one neural network layer i should satisfy several conditions as follows:\n\u2460 \\(M^{(i)}\\) should be dependent to \\(M^{(i-1)}\\). The sequential dependency between layers should be explicitly considered. It plays an essential role in encouraging gradient flow throughout the model [40, 42], by preserving structural \u201cpathways\u201d.\n\u2461 \\(M^{(i)}\\) should be dependent to \\(W^{(i)}\\). The statistics of network weights are commonly appreciated as powerful features for estimating channel importance [22, 43].\n\u2462 \\(M^{(i)}\\) should be dependent to \\(V\\). Motivated by the data-centric advances in NLP, such prompting can contribute to the dissecting and understanding of model behaviors [33\u201337].\nTherefore, it can be expressed as \\(M^{(i)} = f(M^{(i-1)}, W^{(i)}, V)\\), where the generation of a channel mask for layer i depends on the weights in the current layer, the previous layers' mask, and visual prompts."}, {"title": "3.1 Innovative Data-Model Co-designs through A Recurrent Hypernetwork", "content": "To meet the aforementioned requirements, PASS is proposed as illustrated in Figure 1, which enables the data-model co-design pruning via a recurrent hyper-network. Details are presented below.\nModeling the Layer Sequential Dependency. The recurrent hyper-network in PASS adopts a Long Short-Term Memory (LSTM) backbone since it is particularly suitable for capturing sequential dependency. It enables an \u201cauto-regressive\" way to infer the structural sparse mask. To be specific, the LSTM mainly utilizes the previous layer's mask \\(M^{(i-1)}\\), the current layer's weights \\(W^{(i)}\\), and a visual prompt \\(V\\) as follows:\n\\(M^{(i)} = LSTM_{\\theta} (W^{(i)}, g_w(V)), W^{(i)} = M^{(i-1)} \\otimes W^{(i)}, M^{(0)} = LSTM_{\\theta} (W^{(\\epsilon)}, g_w(V))\\),                                                                                                                                                                                                   (1)\nwhere the visual prompt V provides an initial hidden state for the LSTM hyper-network, \u03b8 is the parameters of the LSTM model, and \\(g_w(V)\\) is the extra encoder to map the visual prompt into an embedding space. The channel-wise sparse masks (\\(M^{(i)}\\)) generated from the hyper-network are utilized to prune the weights of each layer as expressed by \\(W^{(i)} = M^{(i-1)} \\otimes W^{(i)}\\)."}, {"title": "3.2 How to Optimize the Hypernetwork in PASS", "content": "Learning PASS. The procedures of learning PASS involves a jointly optimization of the visual prompt V, encoder weights w, and LSTM's model weights \u03b8. Formally, it can be described below:\n\\[\n\\min_{\\theta,\\omega,\\nu} L(P(x + V), y), W^{(i)} = M^{(i-1)} \\otimes W^{(i)} \\otimes M^{(i)},\n\\]\nWhere \\(P(\\cdot)\\) is the target CNN with weights W, \u00e6 and y are the input image and its groundtruth label. Note that \\(M^{(i)}\\) is generated by \\(LSTM_{\\Theta}(W^{(i)}, g_w(V))\\) as described in Equation 1. The objective of this learning phase is to optimize the PASS model to generate layer-wise channel masks, leveraging both a visual prompt V and the inherent model weight statistics as guid-ance. After that, the obtained sparse subnetwork will be further fine-tuned on the downstream dataset.\nFine-tuning Sparse Subnetwork. The procedures of subnetwork fine-tuning involve the optimization of the visual prompt V and model weights W, which can be expressed by:\n\\[\n\\min_{W,V} L(P(x + V), y),\n\\]"}, {"title": "4 Experiments", "content": "In this section, we empirically demonstrate the effectiveness of our proposed PASS method against various baselines across multiple datasets and models. Additionally, we evaluate the transferability of the sparse channel masks and the hypernetwork learned by PASS. Further, we validate the superiority of our specific design by a series of ablations studies.\nTo evaluate PASS, we follow the widely-used evaluation of visual prompting which is pre-trained on large datasets and evaluated on various target domains [35, 38]. Specifically, this process is accomplished by two steps: (1) Identifying an optimal structural sparse neural network based on a pre-trained model and (2) Fine-tuning the structural sparse neural network on the target task. During the training process, we utilize the Frequency-based Label Mapping FLM as presented by [35] to facilitate the mapping of the logits from the pre-trained model to the logits of the target tasks."}, {"title": "4.1 Implementation Setups", "content": "Architectures and Datasets. We evaluate PASS using four pre-trained models: ResNet-18, ResNet-34, ResNet-50 [83], and VGG-16 without BatchNorm2D [84], all pre-trained on ImageNet-1K [85]. Our evaluation contains six target tasks: Tiny-ImageNet [85], CIFAR-10/100 [86], DTD [87], StanfordCars [88], and Food101 [89]. The size of the inputs is scaled to 224 x 224 during our experiments.\nBaselines. We select five popular structural pruning methods as our baselines: (1) Group-L1 structural pruning [18, 20] reduces the network channels via l\u2081 regularization. (2) GrowReg [23] prunes the network channels via 12 regularization with a growing penalty scheme. (3) Slim [22] imposes channel sparsity by applying l\u2081 regularization to the scaling factors in batch normalization layers. (4) DepGraph [20] models the inter-layer depen-dency and group-coupled parameters for pruning and (5) ABC Pruner [90] performs channel pruning through automatic structure search.\nTraining and Evaluation. We utilize off-the-shelf models from Torchvision 1 as the pre-trained models. During the pruning phase, we employ the SGD optimizer for the visual prompt, while the AdamW optimizer is used for the visual prompt encoder and the LSTM model for generating channel masks. Regarding the baselines, namely Group-L1 structural pruning, GrowReg, Slim, and DepGraph, they are trained based on this implementation 2 and ABC Prunner is trained based on their official public code 3. During the fine-tuning phase, all pruned models, inclusive of those from PASS and the aforementioned baselines, are fine-tuned with the same hyper-parameters. We summarize the implementation details and hyper-parameters for PASS in Appendix C. For all experiments, we report the accuracy of the downstream task during testing and the floating point operations (FLOPs) for measuring the efficiency."}, {"title": "4.2 PASS Finds Good Structural Sparsity", "content": "In this section, we first validate the effectiveness of PASS across multiple downstream tasks and various model architectures. Subsequently, we investigate the transferability of both the generated channel masks and the associated model responsible for generating them.\nSuperior Performance across Downstream Tasks. In Figure 2, we present the test accuracy of the PASS method in comparison with several baseline techniques, including Group-L1, GrowReg, DepGraph, Slim, and ABC Prunner. The evaluation includes six down-stream tasks: CIFAR-10, CIFAR-100, Tiny-ImageNet, DTD, StanfordCars, and Food101. The accuracies are reported against varying FLOPs to provide a comprehensive understanding of PASS's efficiency and performance.\nFrom Figure 2, several salient observations can be drawn: PASS consistently demon-strates superior accuracy across varying FLOPs values for all six evaluated downstream tasks. On one hand, PASS achieves higher accuracy under the same FLOPs. For example, it achieves 1% ~ 3% higher accuracy than baselines under 1000M FLOPs among all the datasets. On the other hand, PASS attains higher speedup in achieving comparable accuracy levels. For instance, to reach accuracy levels of 96%, 81%, and 80% on CIFAR10, Stanford-Cars, and Food101 respectively, the PASS method consistently realizes a speedup of at least 0.35\u00d7 (900 VS 1400), outperforming the most competitive baseline. This consistent perfor-mance highlights the robustness and versatility of the PASS method across diverse scenarios.\nIn terms of resilience to pruning, PASS exhibits a more gradual reduction in accuracy as FLOPs decrease. This trend is notably more favorable when compared with the sharper declines observed in other baseline methods. Remarkably, at the higher FLOPs levels, PASS not only attains peak accuracies but also surpasses the performance metrics of the fully fine-tuned dense models. For instance, PASS excels the fully fine-tuned dense models with {1.05%, 0.99%, 1.06%} on CIFRAR100, DTD and FOOD101 datasets."}, {"title": "4.3 Experiments on ImageNet and Advanced Architectures", "content": "To draw a solid conclusion, we further conduct extensive experiments on a large dataset Ima-geNet using advanced pre-trained models such as ResNeXt-50, Swin-T, and ViT-B/16. The results are shown in Table ??. We observe that our method PASS demonstrates a significant"}, {"title": "4.4 Transferability of Learned Sparse Structure", "content": "Inspired by studies suggesting the transferability of subnetworks between tasks [95, 96]. We investigate the transferability of PASS by posing two questions:(1) Can the sparse channel masks, learned in one task, be effectively transferred to other tasks? (2) Is the hypernetwork, once trained, applicable to other tasks? To answer Question (1), we test the accuracy of sub-networks found on Tiny-ImageNet when fine-tuning on CIFAR-10 and CIFAR-100 and a pre-trained ResNet-18. To answer Question (2), we measure the accuracy of the subnetwork finetuning on the target datasets, i.e., CIFAR-10 and CIFAR-100. This subnetwork is obtained by applying hypernetworks, trained on Tiny-ImageNet, to the visual prompts of the respec-tive target tasks. The results are reported in Table 1. We observe that the channel mask and the hypernetwork, both learned by PASS, exhibit significant transferability on target datasets, highlighting their benefits across various subsequent tasks. More interestingly, the hypernet-work outperforms transferring the channel mask in most target tasks, providing two hints:\n Our learned hypernetworks can sufficiently capture the important topologies in down-stream networks. Note that there is no parameter tuning for the hypernetworks and only with an adapted visual prompt.  The visual prompt can effectively summarize the topological information from downstream neural networks, enabling superior sparsification."}, {"title": "5 Ablations and Extra Invesitigations", "content": "To evaluate the effectiveness of PASS, we pose two interesting questions about the design of its components: (1) how do visual prompts and model weights contribute? (2) is the recur-rent mechanism crucial for mask finding? To answer the above questions, we conduct a series of ablation studies utilizing a pre-trained ResNet-18 on CIFAR-100. The extensive investiga-tions contain (1) dropping either the visual prompt or model weights; (2) destroy the recurrent nature in our hypernetwork, such as using a Convolutional Neural Network (CNN) or a Mul-tilayer Perceptron (MLP) to replace LSTM. The results are collected in Table 3. We observe that\n The exclusion of either the visual prompt or model weights leads to a pronounced drop in test accuracy (e.g., 83.45% \u2192 82.83% and 82.66% respectively at 90% channel density), indicating the essential interplay role of both visual prompt and model weights in sparsifica-tion. If the recurrent nature in our design is destroyed, i.e., MLP or CNN methods variants, it suffers a performance decrement (e.g., 81.72% \u2192 81.07% and 81.09% respectively at 70% channel density). It implies a Markov property during the sparsification of two adjacent layers, which echoes the sparsity pathway findings in Wang et al. [40]."}, {"title": "5.2 Ablations on Visual Prompt", "content": "A visual prompt is a patch integrated with the input, as depicted in Figure 1. Two preva-lent methods for incorporating the visual prompt into the input have been identified in the literature [35, 62]:(1) Adding to the input (abbreviated as \"Additive visual prompt\". (2)Expanding around the perimeter of the input, namely, the input is embedded into the cen-tral hollow section of the visual prompt (abbreviated as \u201cExpansive visual prompt\"). As discussed in section 5.1, visual prompt (VP) plays a key role in PASS. Therefore, we pose such a question:How do the strategies and size of VP influence the performance of PASS? To address this concern, we conduct experiments with \u201cAdditive visual prompt\" and \"Expansive visual prompt\" respectively on CIFAR-100 using a pre-trained ResNet-18 under 10%, 30% and 50% channel sparsities, and we also show the performance of PASS with varying the VP size from 0 to 48. The results are shown in Figure 4. We conclude that \"Additive visual prompt\" performs better than \"Expansive visual prompt\" across different sparsities. The dis-parity might be from the fact that \u201cExpansive visual prompt\" requires resizing the input to a smaller dimension, potentially leading to information loss, a problem that \"Additive visual prompt\" does not face. The size of VP impacts the performance of PASS. We observe that"}, {"title": "5.3 Impact of Hidden Size in HyperNetwork", "content": "It is well-known that model size is an important factor impacting its performance, inducing the question how does the size of hypernetwork influence the performance of PASS?. To address this concern, we explore the impact of the hypernetwork hidden sizes on PASS by varying the hidden size of the proposed hypernetwork from 32 to 256 and evaluate its performance on CIFAR100 using a pre-trained ResNet-18 model under 10%, 30% and 50% channel sparsity respectively. The results are presented in the left figure of Figure 5. We observe that the hidden size of the hypernetwork doesn't drastically affect the accuracy. While there are fluctuations, they are within a small range, suggesting that the hidden size is not a dominant factor in influencing the performance of PASS."}, {"title": "5.4 Uniform Pruning VS Global Pruning", "content": "When converting the channel-wise importance scores into the channel masks, there are two prevalent strategies: (1) Uniform Pruning. [81, 97] It prunes the channels of each layer with the lowest important scores by the same proportion. (2) Global Pruning. [20, 81] It prunes channels with the lowest important scores from all layers, leading to varied sparsity across layers. In this section, we evaluate the performance of global pruning and uniform pruning for PASS on CIFAR-100 using a pre-trained ResNet-18, with results presented in Figure 5. We observe that global pruning consistently yields higher test accuracy than uniform pruning, indicating its superior suitability for PASS, also reconfirming the importance of layer sparsity in sparsifying neural networks [81, 98].\nWe present the channel sparsity learned by PASS with global pruning on CIFAR-100 and Tiny-ImageNet using a pre-trained ResNet-18 in Table 4 in Table 4. It can be observed that channel sparsity is generally higher in the top layers and lower in the bottom layers of the network."}, {"title": "6 Complexity Analysis of the Hypernetwork", "content": "In this section, we provide a comprehensive analysis about the complexity of the Hypernet-work. (1) Regarding the impact on time complexity, our recurrent hyper-network is designed for efficiency. The channel masks are pre-calculated, eliminating the need for real-time gen-eration during both the inference and subnetwork fine-tuning phases. Therefore, the recurrent hyper-network does not introduce any extra time complexity during the inference and the fintune-tuning phase. The additional computing time is limited to the phase of channel mask identification. (2) Moreover, the hyper-network itself is designed to be lightweight. The number of parameters it contributes to the overall model is minimal, thus ensuring that any"}, {"title": "7 Conclusion", "content": "In this paper, we delve deep into structural model pruning, with a particular focus on leverag-ing the potential of visual prompts for discerning channel importance in vision models. Our exploration highlights the key role of the input space and how judicious input editing can significantly influence the efficacy of structural pruning. We propose PASS, an innovative, end-to-end framework that harmoniously integrates visual prompts, providing a data-centric lens to channel pruning. Our recurrent mechanism adeptly addressed the intricate channel dependencies across layers, ensuring the derivation of high-quality structural sparsity.\nExtensive evaluations across six datasets and four architectures underscore the prowess of PASS. The PASS framework excels not only in performance and computational efficiency but also demonstrates that its pruned models possess notable transferability. In essence, this research paves a new path for channel pruning, underscoring the importance of intertwining data-centric approaches with traditional model-centric methodologies. The fusion of these paradigms, as demonstrated by our findings, holds immense promise for the future of efficient neural network design."}]}