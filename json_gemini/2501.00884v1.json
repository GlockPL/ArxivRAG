{"title": "Diversity Optimization for Travelling Salesman Problem via Deep Reinforcement Learning", "authors": ["Qi Li", "Zhiguang Cao", "Yining Ma", "Yaoxin Wu", "Yue-Jiao Gong"], "abstract": "Existing neural methods for the Travelling Salesman Problem (TSP) mostly aim at finding a single optimal solution. To discover diverse yet high-quality solutions for Multi-Solution TSP (MSTSP), we propose a novel deep reinforcement learning based neural solver, which is primarily featured by an encoder-decoder structured policy. Concretely, on the one hand, a Relativization Filter (RF) is designed to enhance the robustness of the encoder to affine transformations of the instances, so as to potentially improve the quality of the found solutions. On the other hand, a Multi-Attentive Adaptive Active Search (MA3S) is tailored to allow the decoders to strike a balance between the optimality and diversity. Experimental evaluations on benchmark instances demonstrate the superiority of our method over recent neural baselines across different metrics, and its competitive performance against state-of-the-art traditional heuristics with significantly reduced computational time, ranging from 1.3\u00d7 to 15x faster. Furthermore, we demonstrate that our method can also be applied to the Capacitated Vehicle Routing Problem (CVRP).", "sections": [{"title": "1 INTRODUCTION", "content": "The Travelling Salesman Problem (TSP), as a classic optimization problem, involves a salesman who embarks from a city, visits each remaining city once, and finally returns to the starting one, aiming to find an optimal route with the shortest length while satisfying the above constraints [1, 8, 10, 17, 19, 28, 36, 38, 42, 44]. Nevertheless, recent studies have shown that diverse landscapes are common in many practical TSP instances, yielding the Multi-Solution TSP (MSTSP, a.k.a. the diversity optimization for TSP) [2, 9, 13, 15, 16, 32]. For instance, Figure 1 showcases a TSP-10 instance with as many as 56 optimal solutions (we only display 4 of them for illustration), each possessing an identical length of 130. Accordingly, MSTSP is in pursuit of a set of diverse yet high-quality (possibly optimal) solutions. As a practical and crucial supplement to the classic TSP, it is highly desired in many real-world scenarios, where a single solution may be insufficient. For example, 1) when the single target route (solution) becomes unavailable due to unexpected circumstances, MSTSP offers desirable alternatives; 2) while the single target route may overlook other important metrics like user preferences, MSTSP allows for personalized choices among a set of high-quality candidate routes; 3) while the single target route may incur spontaneous and simultaneous pursuit of the same choice, MSTSP can distribute users or loads across different routes, potentially mitigating the jam and enhancing the overall performance."}, {"title": "2 RELATED WORK", "content": "Traditional Heuristics. Numerical traditional heuristics have been proposed for MSTSP. In [32], GA was leveraged to yield multiple high-quality routes with a multi-chromosomal cramping design for enhanced exploration. Later, researchers have increasingly embraced niche techniques for addressing MSTSP, which can effectively promote diversity within a population by imposing limitations on individual similarity. This approach proves instrumental in preventing premature convergence within the solution space and facilitates algorithms to explore and maintain a wider range of diverse solutions. Consequently, many studies have integrated niche technology with other methodologies. For example, 1) ACO, Angus et al. [2] applied fitness sharing and crowding to ACO to simultaneously locate and maintain multiple areas of interest in the search space; Han et al. [13] adopted niching strategy and multiple pheromone matrices to maintain population diversity and track the traces of multiple paths. 2) genetic algorithm, Huang et al. [16] combined genetic algorithm with niching technique defined in the discrete space to improve the quality and diversity of multiple solutions; 3) memetic algorithms have also been extensively studied. The Niching Memetic Algorithm (NMA) [15], serves as a prominent example, utilizing parallel search for diverse and high-quality solutions. While it significantly improved computational efficiency over previous methods, its run time is still considerable. Building upon NMA, a subsequent method [9] proposed to start from an optimal solution and enhances its diversity through propagation and mutation. However, it requires the optimal solution in the first place, rendering it less flexible in many practical scenarios. Additionally, Liu et al. [25] extended the multi-modal single-objective TSP (i.e., the MSTSP presented in our paper) to a multi-modal multi-objective (MMTSP) context. In another recent work, Liu et al. [26] acknowledged that NMA is deemed as the state-of-the-art approach for the single-objective version of the problem, which we have adopted as benchmark in our comparative analysis.\nNeural Heuristics. Most existing neural heuristics for solving vehicle routing problems (VRPs) including the TSP focus on pursuing a single (optimal) solution. Among them, Bello et al. [5] introduced the attention-based [4] Pointer Network (PtrNet) [35] to the actor-critic algorithm [21] for solving TSP and 0-1 knapsack (KP). Later, the self-attention mechanism emerged and garnered high popularity in designing deep models [34] for VRPs. Kool et al. [22] adapted the work in [5] to apply Transformer for more COPs, in which the introduced logit clipping to the decoder acts as a preset and deterministic trade-off between exploration and exploitation. More recently, Kwon et al. [23] proposed the well-known POMO, which employed two simple yet effective modifications: 1) Multiple starting points initialization: The approach considers each point as a starting point once in parallel, and 2) Instance augmentation: The instances undergo symmetry processing, including mirroring and rotation at specific angles, to expand their quantity by eightfold. However, the symmetry exploitation of POMO is potentially limited by the specified eight transformations only, which then motivated Kim et al. [20] to further consider the problem symmetry, solution symmetry, and rotational symmetry simultaneously, improving the single solution optimality of the model. Many researchers have then focused on enhancing the neural heuristics based on the success of Kool et al. [22] and Kwon et al. [23]. To enhance search diversity, Xin et al. [37] and Grinsztajn et al. [12] improved the model from the architecture perspective, by introducing multiple decoders, with each possessing distinct parameters. To improve the optimality, several search approaches have been proposed, including sampling [5, 22], beam search [7, 18, 35, 37], efficient active search (EAS) [14], flexible k-Opt [28], and heavy decoder [27]. However, existing works may overlook the combined consideration of both diversity and optimization, especially for solving MSTSP. Our method explicitly targets diverse solutions, distinguishing itself from many existing ones that mainly aim at finding a single optimal solution. Compared to active search [5] and EAS [14], our Multi-Attentive Adaptive Active Search (MA3S) is featured by the diversity-enhancement scheme, which allows it to surpass MDAM [37] in solution quality."}, {"title": "3 MSTSP NOTATIONS AND MEASURES", "content": "This section will explain the definition of classical TSP, and further introduce MSTSP and the solution filters required for MSTSP.\nDEFINITION 1. TSP. Given a complete graph $G = (V, E)$, where V represents the set of nodes with size N = |V|, and E represents the set of edges between pairs of nodes. Each edge has a weight d(\u00b7,\u00b7) denoting its Euclidean distance. The objective of TSP is to find the shortest Hamiltonian circle that visits each node once and returns to the first visited node. We denote a TSP solution as $\u03c0 = [\u03c0(1),\u2026\u2026 \u03c0(N)]$ and the objective function as $min L(\u03c0) = \u03a3_{i=1}^{N-1} d(\u03c0(i), \u03c0(i+1)) + d(\u03c0(N), \u03c0(1))$ ."}, {"title": "3.1 MSTSP Definition", "content": "DEFINITION 2. MSTSP. While sharing the same notation, MSTSP aims to pursue multiple solutions of high diversity and quality. For a set of solutions yielded by an algorithm, a solution filter [11, 24, 40] is typically employed to select solutions that are both high-quality and diversity. The performance indices are then calculated based on the filtered solution set. The solution filters in terms of quality and diversity are described below.\nDEFINITION 3. Optimality Filter. For any solution \u03c0\u2081, it must first satisfy the optimality threshold condition as $L(\u03c0_i) < L(best)\u00b7 (1 + \u03b4_1)$, where $L(\u00b7)$ denotes the route length, best denotes any solution in the set with the shortest length, and $\u03b4_1$ represents the optimality threshold.\nDEFINITION 4. Diversity Filter. Additionally, except the best, all solutions should then satisfy the diversity condition as follows, $S(\u03c0_i, \u03c0_j) = \\frac{|\u03a6(\u03c0_i)\u2229\u03a6(\u03c0_j)|}{|\u03a6(\u03c0_i)|+|\u03a6(\u03c0_j)|-N} < \u03b4_2$, where S is a similarity measure, \u03a6(\u03c0\u2081) denotes the set of edges for \u03c0\u1d62, and $\u03b4_2$ is the similarity threshold. For the CVRP (Capacitated Vehicle Routing Problem), N is replaced with $\\frac{|\u03a6(\u03c0_i)|+|\u03a6(\u03c0_j)|}{2}$ ."}, {"title": "3.2 Performance Measures", "content": "For evaluating the filtered solution set of an MSTSP instance, we employ two metrics to comprehensively assess its optimality and diversity.\nDEFINITION 5. Diversity Indicator (DI). DI [16] is a commonly used measure in the existing MSTSP studies. It quantifies the overlap ratio between the filtered solution set and the ground-truth optimal solution set as follows,\n$DI(G, S) = \\frac{1}{|G|}\u03a3_{i=1}^{|G|} max_{j=1,...,|S|} S (g_i, \u03c0_j)$                                                                              (1)"}, {"title": null, "content": "The |S| and |G| represent the sizes of the solution sets S and G respectively, with G containing the optimal solutions. The ith solution in G is denoted by g\u1d62, and the jth solution in S is \u03c0\u2c7c. Nevertheless, DI exhibits certain limitations: 1) it relies heavily on the ground-truth optimal solution set, which is however unavailable for most TSP datasets; 2) it is a unified indicator for optimality and diversity, but falls short of providing insights when there is a need to separately examine optimality and diversity achieved by the algorithms.\nTo complement the DI measure, we further introduce a new indicator as the second metric, i.e., Multi-Solution Quality Index (MSQI). It is developed based on Diversity Index and Optimality Index, which are described as below.\nDEFINITION 6. Diversity Index. Diversity Index evaluates the diversity between a single solution and others in the same set, which is computed as follows,\n$Diff (\u03c0_i) = \\frac{1}{|S|-1}\u03a3_{j=1}^{|S|} U(\u03c0_i, \u03c0_j)$                                                                         (2)"}, {"title": null, "content": "Here, the function U is computed using the similarity S as follows,\n$\\begin{cases} 2(1 \u2212 S(\u03c0_i, \u03c0_j)),&  \\frac{1}{2} < S(\u03c0_i, \u03c0_j) \u2264 1,\\\\ 1,& 0 \u2264 S(\u03c0_i, \u03c0_j) \u2264 \\frac{1}{2}, \\end{cases}$                                         (3)\nwhere we let U (\u03c0\u1d62, \u03c0\u2c7c) = 1 if more than half of the edges of solutions \u03c0\u1d62, \u03c0\u2c7c differ, suggesting a high degree of difference; Otherwise, we assign a smaller value 0 \u2264 U(\u03c0\u1d62, \u03c0\u2c7c) < 1 according to S(\u03c0\u1d62, \u03c0\u2c7c).\nDEFINITION 7. Optimality Index. The optimality of a single solution is then measured based on the normalized distance within a threshold between the route length of this solution and the route length of the optimal solution as follows,\n$Opt(\u03c0_i) = \\frac{(1+\u03b4_1) \u00b7 L(best) \u2013 L(\u03c0_i)}{\u03b4_1\u00b7 L(best)}$                                                                             (4)"}, {"title": "4 METHODOLOGY", "content": "We introduce Relativization Filter assisted Multi-Attentive Adaptive Active Search (RF-MA3S) for MSTSP. As depicted in Figure 2, its architecture follows the encoder-decoder structure [23], featuring an RF-assisted encoder and multi-attentive decoders. After training via reinforcement learning to learn transferable features, we use an adaptive active search strategy for each test instance during inference. While the model learned via reinforcement learning explicitly outputs multiple solutions, the per-instance active search aims to balance the optimality and diversity of MSTSP more effectively."}, {"title": "4.1 Relativization Filter (RF) Assisted Encoder", "content": "For many COPs including the TSP, an instance and its affine transformations such as translation, rotation, scaling, mirroring, etc, often share the same optimal solution. While prior attempts have sought to capture such invariance through data augmentation [23] and auxiliary losses [20], we offer a simpler yet effective alternative by explicitly embedding the invariance using a unified representation. This may bolster both solution optimality and diversity in MSTSP, as it minimizes redundant solution representations throughout both the training and inference processes. Notably, it improves upon previous efforts by reducing the reliance on instance augmentation during inference [20, 23].\nTo achieve this, we propose exploiting Relativization Filter (RF) in the encoder, which adopts a series of relativization operations to convert the absolute information of the nodes into relative information while capturing the internal relationships among the nodes. As illustrated in Figure 3, this filter helps screen out the instances which are essentially the affine transformations of others since they share the same eventual representation, and thus enhance the robustness of the encoder. Given a set of nodes with Cartesian coordinates $(x_1, y_1), ..., (x_N, y_N)$ as the input, we execute our RF as follows.\nReorder. To alleviate the order influence of the input nodes, they are sorted first by their y values, then by x values in descending order within the Cartesian coordinates. The relative positions of nodes with the same values will not be changed after sorting, resulting in a unique sequence of nodes.\nZero-mean. All x and y subtract the respective means, and lead to x' and y'. It moves the centroid of the instance to the origin, and helps capture the influence of translation.\nConversion to Polar Coordinates. Polar coordinates (\u03c1, \u03b8) are derived via arctan($\\frac{y}{x}$) and $\u221ax^2 + y^2$. However, such \u03b8 cannot preserve the quadrant information of the Cartesian coordinate system, as its range is limited to [-1,1]. We thus add +\u03c0 to \u03b8 for nodes located in the second and third quadrants of the original Cartesian coordinate system, expanding the range to [-3].\nPolar Coordinates Relativization. First, it normalizes \u03c1 as $\u03c1' = \\frac{\u03c1}{\u03c1_{max}}$, which helps capture the scaling effect. Then, it sorts the polar coordinates (\u03c1', \u03b8) in the descending order of \u03c1', and normalizes angles by subtracting the first angle, mitigating the rotational effect.\nConversion to Cartesian Coordinates. The computed values of (\u03c1', \u03b8') are converted back into Cartesian coordinates (x", "y": "by $x"}, {"y": "\u03c1' sin \u03b8'$, and fed to the encoder.\nThe above procedure might be less effective in handling the mirroring transformation. Therefore, the \u00d72 instance augmentation [23] is applied during inference, where an instance with input (x, y) and its mirroring instance with input (y, x) are considered. Note that other types of mirroring can be regarded as a combination of this mirroring and the aforementioned translation and rotation."}, {"title": "4.2 Multi-attentive Decoders", "content": "Inspired by the architecture in the Multi-Decoder Attention Model (MDAM) [37], we reduce the number of encoder blocks to three and expand the attention-based single decoder in POMO [23] with five duplicates. In MDAM [37], solution diversity is bolstered through the employment of KL divergence, which, however, incurs heavy computation overhead. In our approach, we simply deploy the five decoders of the same structure in parallel. Note that, on the one hand, due to the mechanism of multiple starting points, even a single decoder is able to produce multiple solutions. On the other hand, the random initialization of the parameters for each decoder will also boost the diversity of the solutions. Furthermore, most importantly, the subsequent adaptive active search will further"}, {"title": "4.3 Training Phase", "content": "Our training algorithm mainly follows that of POMO [23]. Specifically, for a batch of input instances s, each decoder independently performs parallel computations with multiple starting points to obtain the REINFORCE loss with the greedy rollout as follows,\n$\u2207_\u03b8J(\u03b8) = \\frac{B}{D N} \u03a3_{m=1}^B \u03a3_{j=1}^D \u03a3_{i=1}^N (L(\u03c0_{j,i}|s) \u2013 b(s))\u2207_\u03b8logp_\u03b8(\u03c0_{j,i}|s)$.              (6)\nHere, the result of the best decoder (with shortest averaged length) in current epoch is taken as the baseline b(s). Namely, $b(s) = \\frac{1}{N}\u03a3_{i=1}^N (L(\u03c0_{l,i}|s))$, where $l = arg min(L(\u03c0_{:,1}|s),\u00b7\u00b7\u00b7, L(\u03c0_{:,D}|s))$ .\nTemperature Softmax. During the training phase, we also employ the temperature softmax [39] as a replacement for logit clipping in the decoder [5] expressed as Softmax $(a_{i,j}, \u03c4) = \\frac{e^{a_{i,j}/\u03c4}}{\u03a3_{j'} e^{a_{i,j'}/\u03c4}}$. Here, \u03c4 = $\\frac{\u03c4_0}{1+log_{10} T}$, T denotes the current epoch, and $\u03c4_0$ is a constant and set to 2. The temperature softmax is used to enhance model exploration by setting a high initial temperature, reducing the disparity in probability values for selecting the next node. As training progresses, temperature drops, enhancing probability differences and aiding convergence."}, {"title": "4.4 Adaptive Active Search Phase", "content": "After the training phase, we utilize our proposed Adaptive Active Search (AAS) during inference to synergize with the multi-attentive decoders, so as to pursue diverse yet high-quality solutions. In each iteration, it first samples the solutions for a given test instance using the respective decoders and calculates the corresponding loss w.r.t a baseline. Then, the parameters of the model are adjusted based on the loss, aiming to increase the likelihood of yielding high-quality solutions in the subsequent iterations. In comparison with AS [5] and EAS [14], our AAS is able to adaptively determine the switching of the baseline for parameter update according to the convergence status of the model, and ensure the diversity of multiple solutions. We consider adjusting all parameters of the pre-trained models for better performance. Moreover, a designed termination condition for the iterations will also potentially prevent the diversity deterioration caused by excessive iterations.\nAdaptive Baseline. Two types of baselines are involved in the active search phase and described as follows. 1) The results of the best decoder (with the shortest averaged route length) are used as the shared baseline. It will encourage the convergence of all decoders towards the global optimum, enhancing the optimality with a relatively large total loss; 2) The results of each individual decoder (the averaged route length) are used as their respective baselines. It will encourage the convergence of each decoder towards a potential local optimum, enhancing the diversity with a relatively small total loss. Particularly, at the early stage, we need to emphasize on the quality of solutions and ensure the movement towards the optimum, where the shared baseline is preferred. Afterwards, the diversity of the solutions should be guaranteed and strengthened, where the respective baseline is more desired. In our AAS, this switching decision is adaptively made based on the convergence degree of"}, {"title": null, "content": "the model,\n$f = \\frac{\u25bd_\u03b8J(\u03b8)}{J(\u03b8)}$                                                                                                (7)\nwhere J(\u03b8) is the objective function (the negative value of the route length), divided for normalization w.r.t the objective distribution scale. The switch will be incurred if f < \u03b1, where \u03b1 is a positive constant as the threshold.\nAdaptive Termination. Moreover, rather than using a simple threshold to terminate the whole iterations, we use a probability to represent the termination condition, so as to counteract the random factors in each iteration. In particular, let e denote the probability of early termination, and let \u03b2 = 0.5\u03b1, we compute such probability in Eq. (8) and summarize the procedure of the proposed adaptive active search in Algorithm 1.\n$\\varepsilon = \\begin{cases} \\frac{\u03b2-f}{\u03b2},&  f<\u03b2,\\\\ 0,& f \u2265 \u03b2.  \\end{cases}$                                                  (8)\nThe absolute value of the gradient consistently decreases as iterations progress, and the probability of iteration termination converges to 1 once it falls below the predetermined threshold."}, {"title": "5 EXPERIMENTS", "content": "In our experiments, we evaluated the multi-solution performance of RF-MA3S in both TSP and CVRP, comparing it with existing traditional heuristic and neural heuristic methods."}, {"title": "5.1 Experiment Settings", "content": "Datasets. To comprehensively evaluate the proposed method, we conduct experiments on MSTSPLIB [16], TSPLIB [31], CVRPLIB [3], and the uniformly distributed synthetic instances as used in [20, 23]. Regarding the instances in MSTSPLIB, they are labeled as mstsp1 \u2013 mstsp25, and categorized into four groups based on their distributions. Regarding the TSPLIB and CVRPLIB, they include widely used practical instances for TSP and CVRP, respectively. Regarding"}, {"title": "5.2 Hyperparameter", "content": "NGA [16] and NMA [15] are tested for 10 independent runs. The neural methods are trained on uniformly distributed datasets for 100 epochs, each with 10k instances, using a batch size of 64. For the active search during inference, our MA3S adopts the switching threshold as \u03b1 = 0.005 and a learning rate of 1 \u00d7 10\u207b\u2075. To guarantee the inference speed, we set a maximum number of iterations to 2000 when testing on TSPLIB [31] and CVRPLIB [3]. For a fair comparison, the iteration count for EAS is set to five times the actual iteration count of MA3S, in light of the five decoders in our approach. For the LEHD [27], relying solely on greedy rollout does not yield multiple solutions; hence, the LEHD employed in this paper incorporates the RRC [27]technology (a versatile technique applicable to various neural methods, with setting of 50). Unless"}, {"title": "5.3 Performance Comparisons", "content": "Results on MSTSPLIB. The results are shown in Table 1, where the MSQI, DI, and inference time of different algorithms on the four categories of MSTSPLIB are reported. The last column reports the overall results, where the performance gaps are computed w.r.t. NMA. The best results in each group of comparison are marked in bold. It is worth noting that the neural models are trained once and then directly applied to solve instances with varying node scales. For example, the result of POMO (20) means applying the POMO model trained on instances of size 20 to infer the 1st category of MSTSP instances with sizes ranging from 9 to 12. It can be observed that RF-MA3S outperforms other neural methods significantly across various categories of instances by using different training scales. Generally, RF-MA3S (20) performs better than RF-MA3S (50) and RF-MA3S (100) since the node scale (20) used to train this model is close to the scales of many instances in MSTSPLIB. Then, the results of RF-MA3S (20) on the entire set show an MSQI gap of 5.157% and a DI gap of 6.406% compared to the state-of-the-art NMA, while the testing time is only about three fourths of NMA. Note that a"}, {"title": "5.4 In-depth Analysis of RF-MA3S", "content": "The following models are trained on instances with a default scale of N = 50.\nAffine Transformation Resistance Performance by RF. Five types of affine transformation experiments are conducted, where the test instances are undergone translation, rotation, scaling, mirroring, and a combination of the above four types, respectively. For translation, the coordinates of nodes in an instance are shifted by a random value between [-10, 10]. For rotation, the nodes are rotated randomly around the centre position of all nodes. For scaling, the distance between nodes is enlarged by 100 times and not normalized (in this case we divide the cost by 100 when calculating the Gap). For mirroring, the x and y coordinates of the same group of points are swapped. For the mixture of transformations, all four types of affine transformations are applied simultaneously.\nIn the subsequent analysis, POMO-RF refers to POMO method integrated with our RF, and Sym-NCO enforces symmetry by minimizing the symmetric loss function. We evaluated these configurations, both with and without instance augmentation. Table 3 exhibits the affine transformation resistance performance of different methods, by showing the gap of results on transformed instances compared to original results. The Gap of \"0\" are highlighted in bold. Without instance augmentation, the POMO-RF method demonstrates superior stability compared to POMO and can resist the effects of translation, rotation, and scaling perfectly, with minimal impact from the mirror transformation. In contrast, both POMO and Sym-NCO methods are affected by each type of affine transformation, resulting in certain gaps (positive or negative). Scaling transformation has a significant impact on both methods (especially Sym-NCO), with gaps reaching up to 29.998% and 83.399%, respectively. With \u00d72 instance augmentation, the RF method further resists the mirror transformation. The absolute values of gaps of POMO and Sym-NCO are relatively smaller than that without augmentation, but still far behind the proposed POMO-RF.\nOptimality and Diversity Trade-off in MA3S (AAS vs AS). As mentioned in Section 3, MSQI is composed of two sub-indices that evaluate the diversity and optimality, respectively. While our MA3S employs the adaptive baseline used in different decoders, here we investigate the baseline adaptation mechanism with respect to diversity and optimality performance. First, we removed AAS but integrated AS to create a new model for comparison, which we name RF-MDAS. Then, for better comparison between RF-MA3S"}, {"title": "4.5 Single-Solution", "content": "Single-Solution. Although our RF-MA3S is designed to address multi-solution problems and our proposed MSQI already offers a comprehensive score that accounts for both optimality and diversity, we still compare the performance of RF-MA3S in single-solution scenarios with methods specialized for these tasks, to further elucidate the optimality of our method. We tested RF-MA3S using the TSP100 instances from [28], and we set the maximum iteration limit of AAS to 200. The results are depicted in Table 6, where L2S and L2C denote the Learning-to-Search and Learning-to-Construct methods, respectively. It is observed that the RF-MA3S also excels in single-solution scenarios, with a performance gap of 0.00% on the test set of TSP100."}, {"title": "4.6 Real-World Application", "content": "Real-World Application. We also apply nine city-scale datasets, each containing 60 real-world POIs, to further evaluate the practical availability of our method. To assess the MSQI, the L(best) in Eq. (4) is obtained through OR-Tool [30]. As depicted in Table 7, RF-MA3S consistently delivers superior solutions to other methods across different cities, resulting in excellent MSQI performance."}, {"title": "6 CONCLUSIONS AND FUTURE WORKS", "content": "We introduce a novel method, RF-MA3S, which applies neural optimization techniques to the diversity optimization of TSP. We first propose the Relativization Filter (RF), designed to make the encoder invariant to affine transformations, thereby improving encoding efficiency. Additionally, our model exploits mult-attentive decoders together with an adaptive active search mechanism (MA3S), which effectively leverages the trade-off between exploration and exploitation through dynamically switching the baseline according to the convergence degree of the model towards global and local interests. As such, our model is able to pursue diverse yet high-quality solutions. Through these novel strategies, RF-MA3S not only surpasses other neural heuristic methods in diversity optimization but also demonstrates competitive performance compared to state-of-the-art traditional heuristics in the field.\nDespite the promising results, our approach also has limitations that may open up several interesting future research directions. Firstly, our Relativization Filter (RF) does not perfectly handle mirror transformations, leaving potential for enhancement. Secondly, incorporating additional mechanisms, such as Simulation Guided Beam Search (SGBS) [7], Random Re-Construct (RRC) [27] and so on, could potentially boost the performance. Lastly, given the demonstrated potential of neural approaches for optimizing diversity, it would be worthwhile to explore other promising models and training strategies to further improve the performance, and advance this field as well."}]}