{"title": "CONFLICTBANK: A Benchmark for Evaluating Knowledge Conflicts in Large Language Models", "authors": ["Zhaochen Su", "Jun Zhang", "Xiaoye Qu", "Tong Zhu", "Yanshu Li", "Jiashuo Sun", "Juntao Li", "Min Zhang", "Yu Cheng"], "abstract": "Large language models (LLMs) have achieved impressive advancements across numerous disciplines, yet the critical issue of knowledge conflicts, a major source of hallucinations, has rarely been studied. Only a few research explored the conflicts between the inherent knowledge of LLMs and the retrieved contextual knowledge. However, a thorough assessment of knowledge conflict in LLMs is still missing. Motivated by this research gap, we present CONFLICTBANK, the first comprehensive benchmark developed to systematically evaluate knowledge conflicts from three aspects: (i) conflicts encountered in retrieved knowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the interplay between these conflict forms. Our investigation delves into four model families and twelve LLM instances, meticulously analyzing conflicts stemming from misinformation, temporal discrepancies, and semantic divergences. Based on our proposed novel construction framework, we create 7,453,853 claim-evidence pairs and 553,117 QA pairs. We present numerous findings on model scale, conflict causes, and conflict types. We hope our CONFLICTBANK benchmark will help the community better understand model behavior in conflicts and develop more reliable LLMs.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated impressive capabilities in understanding, gen- erating, and reasoning about language [Wei et al., 2022, Chang et al., 2023, Su et al., 2024a,b, Jin et al., 2024]. Despite their exceptional performance, recent works have discovered that knowledge conflict significantly impacts the trustworthiness and reliability of LLMs [Longpre et al., 2021, Wang et al., 2023a, Chen et al., 2023a, Xu et al., 2024a,b], especially in practical scenarios where noise and misinformation exist [Pan et al., 2023, Xie et al., 2024, Wan et al., 2024]. For instance, despite GPT-4's advanced capabilities, it can still be easily misled by retrieved misinformation, resulting in incorrect answers when this information conflicts with its internal knowledge [Xie et al., 2023]. To investigate the impact of knowledge conflicts on model performance, existing works mainly divide the conflicts into two categories: conflicts in retrieved knowledge [Hsu et al., 2021, Ko et al., 2022, Pan et al., 2023a, Wan et al., 2024] and conflicts in embedded knowledge [Elazar et al., 2021a, Lu et al., 2024]. Retrieved conflicts arise during the inference stage when newly retrieved information contradicts the model's parametric memory, while embedded conflicts occur during the training stage due to discrepancies within the training text itself. To construct conflict-related datasets for controlled experiments, previous works primarily utilize word-level substitution [Longpre et al., 2021, Zhou et al., 2023, Wang et al., 2023a] or language model generation methods [Tan et al., 2024] to craft conflicts. Xie et al. [2024] combines these two approaches, eliciting parametric memory from LLMs and constructs more coherent and convincing conflict pairs. However, all these existing studies solely explored the conflicts between the embedded knowledge of LLMs and the retrieved contextual knowledge, leaving other conflict scenarios like the conflict within the models' encoded knowledge and the interplay between different conflict forms under-explored. To fill in the existing research gap, we present CONFLICTBANK, the first comprehensive benchmark for analyzing the models' behavior by simulating the knowledge conflicts encountered in the pre- training and inference stages. This benchmark covers three main conflict causes, including inaccurate information (misinformation conflict) [Du et al., 2022, Pan et al., 2023, Zhou et al., 2024], knowledge changes over time (temporal conflict) [Lazaridou et al., 2021, Su et al., 2022], and the polysemic nature of language (semantic conflict) [Ansell et al., 2021, Sevgili et al., 2022a]. Specifically, we collect 2,863,205 claims from Wikidata and generate the evidence with the revised conflict claims to create a total of 7,453,853 claim-evidence pairs. Additionally, we construct 553,117 QA pairs for investigating the model behavior when facing conflicts. Unlike the previous datasets, CONFLICTBANK can be employed to systematically evaluate the effects of knowledge conflict in retrieved knowledge, embedded knowledge, and their interactions. Based on the CONFLICTBANK, we conduct pilot experiments on twelve LLMs across four model series and provide insights into their behaviors under different conflict scenarios. Our main contributions are summarized below:\n\u2022 We present CONFLICTBANK, the first comprehensive benchmark for knowledge conflicts, including 7M claim-evidence pairs and 553k QA pairs. Our benchmark covers three conflict causes in the real-world scenario, including misinformation, temporal, and semantic conflicts.\n\u2022 CONFLICTBANK can be utilized to conduct a series of experiments about knowledge conflicts, including conflicts in retrieved knowledge, embedded knowledge, and their interplay.\n\u2022 We conduct in-depth pilot experiments on twelve LLMs across four model series and provide comprehensive analyses about model scales, conflict causes, and conflict types.\n\u2022 To make the CONFLICTBANK accessible for future research, we release a Python package that automates data loading, baseline evaluation, and training\u00b3. We hope that CONFLICTBANK could facilitate comprehensive studies on different conflict scenarios and contribute to the advancement of more reliable and trustworthy language models."}, {"title": "2 CONFLICTBANK Benchmark", "content": ""}, {"title": "2.1 Knowledge Conflicts Causes", "content": "Knowledge conflict within datasets can significantly diminish a model's accuracy, reliability, and trustworthiness [Longpre et al., 2021, Wang et al., 2023a, Xie et al., 2023, Xu et al., 2024b]. In this paper, we identify and investigate three prevalent knowledge conflict causes:\n\u2022 Type 1: Misinformation Conflict arises from incorrect or false information within datasets [Schus- ter et al., 2021]. This conflict usually occurs during data collection, introducing false narratives or misleading facts into the model and diminishing its factual accuracy.\n\u2022 Type 2: Temporal Conflict occurs when knowledge changes or evolves over time [Lazaridou et al., 2021, Su et al., 2022, Huang et al., 2024a,b]. As new knowledge emerges, previous knowledge becomes outdated or obsolete, leading to inconsistencies regarding the same entity.\n\u2022 Type 3: Semantic Conflict arises when words with multiple meanings cause ambiguity in interpre- tation [Sevgili et al., 2022b]. These conflicts stem from the polysemic nature of language, leading to misunderstandings as the same word conveys different meanings in different contexts."}, {"title": "2.2 Extracting Facts from Wikidata", "content": "We utilize the Wikidata [Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014] dump of April 02, 2024, as the knowledge source to extract and construct facts. The information is structured by transforming knowledge triples"}, {"title": "2.3 Constructing Knowledge Conflict Claims", "content": "Based on the extracted knowledge triples, we substitute the entity with a same-type entity to construct the conflict claims [Longpre et al., 2021]. Specifically, we use the following strategies for three conflict causes construction: (1) Misinformation conflicts simulate conflicts involving misinformation, such as fake news and rumors that contradict reality. We create this type of conflict by substituting o with o' in (s, r, o, sd, 0d); (2) Temporal conflicts capture the discrepancies arising from updates to knowledge, reflecting the clash between new and outdated information. To avoid conflicts with the LLM's updated parametric knowledge [Lazaridou et al., 2021], we add a future time span into the claim, resulting in (s, r, o', sd, Od, Ts, Te), where Ts and Te are the start and the end timestamps, respectively; (3) Semantic Conflicts reflect scenarios where identical words convey entirely different meanings. To simulate such polysemous situations, we generate an additional description for the conflicting subject s\u00e5 based on (s, r, o', sa). The final modification is (s, r, o', s'a, od)."}, {"title": "2.4 Generating Diverse Evidence Texts", "content": "Previous works have proven the evidence generated from the powerful generative LLMs is more coherent compared to word-level editing methods [Xie et al., 2024]. Therefore, we adopt LLMs to produce corresponding evidence for each claim. Since the description provides additional information that helps the model generate more accurate and relevant evidence [Shao et al., 2023], we utilize (sd, Od) as part of the prompt to make up supporting evidence for the claim. To account for the diversity of texts in each practical field in our taxonomy, we produce three types of textual styles: Wikipedia, book, and news. The textual styles of the generated evidence are finely controlled through corresponding prompts. We exhibit prompts and examples in the supplementary materials."}, {"title": "2.5 Controlling Data Quality", "content": "In order to harvest a high-quality dataset, we perform the following steps to clean the generated data. We provide detailed descriptions of each validation and training step, the running time for each processing step, and the resulting data quantities in Appendix B.\nFeature filtering: Since LLMs proactively refuse to answer questions when they lack knowl- edge [Yang et al., 2023], the previous steps are inevitable to involve the response \u201crefusal to answer\" when fabricating the fictional information. These responses cannot serve as valid evidence for specific conflict claims, undermining the datasets' quality. Therefore, we manually identify and extract common features in responses, e.g., \u201cI apologize\u201d and \u201cI can't.\" [Chen et al., 2021] and filter out all model-generated content containing the above features.\nFact-evidence entailment checking: A gold piece of evidence should exhibit a strong correlation with its corresponding claim and effectively support it. To achieve this, we employ the state-of-the-art NLI model DeBERTa-V25 to rigorously assess the relationship between the generated evidence and the claims. We retain samples demonstrating strong fact-evidence entailment to enhance the authenticity of the conflicts within the dataset [Xie et al., 2024].\nConflict confirmation between evidence: Furthermore, we verify that each type of conflict evidence contradicts the default evidence. Specifically, we utilize the SBERT6 to compute embeddings for all evidence and train a conflict confirmation classifier on the existing conflict dataset [Xie et al., 2024]. This classifier evaluates whether two pieces of evidence conflict. To ensure the classifier's reliability, we manually evaluate 200 random examples and observe over 95% accuracy of the model. Please refer to supplementary materials for more details. We filter out all evidence pairs identified as non-conflicting by the classifier to ensure the quality of our dataset."}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Experimental Setup", "content": "Models To explore the behavior of LLMs when encountering knowledge conflicts, we perform a comprehensive evaluation on 12 LLMs ranging from 0.5B to 70B. This evaluation covers the following four model series: GEMMA (2B, 7B) [Team et al., 2024], LLAMA2 (7B, 13B, 70B) [Touvron et al., 2023], LLAMA3 (7B, 70B), QWEN1.5 (0.5B, 4B, 7B, 14B, 72B) [Bai et al., 2023a]. To investigate the impact of internal knowledge conflicts within the parametric memory, we continue pre-training three representative LLMs, including QWEN1.5-4B, MISTRAL-7B, and LLAMA3-8B."}, {"title": "3.2 Conflicts in Retrieved Knowledge", "content": "In this section, we examine LLMs' behavior in two retrieved knowledge conflict scenarios: (1) models are presented solely with external evidence that contradicts their parametric memory, and (2) models are given two pieces of external evidence, one matching their parametric knowledge and one conflicting with it. We report the MR to illustrate the performance of different models, varying in series and parameter size when faced with conflicting evidence pairs. The results of these two settings are shown in Figure 3 and Figure 4, respectively. We draw the following observations:\nLLMs are highly receptive to external evidence and often prefer evidence consistent with their internal beliefs. As shown in Figure 3, all models exhibit memorization ratios below 50%, indicating that models are highly receptive to external evidence when it is the only evidence available, even when it conflicts with their parametric memory. However, as shown in Figure 4, all LLMs demonstrate significantly higher memorization ratios (over 50%) when parametric memory is also provided as evidence. These two above findings are consistent with previous work [Xie et al., 2024], confirming"}, {"title": "3.3 Conflicts in Embedded Knowledge", "content": "Benefiting from the extensive data in our benchmark, we provide the opportunity to investigate the impact of internal knowledge conflicts on model performance. We mix default data and conflict data in 2:1, 1:1 and 2:3, along with a control setup with no conflict data (i.e., 1:0). We randomly select data from three different conflict types to ensure diversity in conflict sources. We inject conflicting knowledge into the models by continually pre-training the foundational models and maintaining consistency by training each experimental setup on 1.06B tokens. We report the Original Answer Ratio (OAR) for each model to analyze the impact of internal knowledge conflicts on model behaviors. As shown in Figure 6, our findings highlight two key points:"}, {"title": "3.4 Interplay among the Conflicts", "content": "In this section, we aim to investigate the interaction between different types of knowledge conflicts. It is crucial to understand the relationship between the internal knowledge inconsistency of the model and its behavior in response to the context. Following the setup in Section 3.3, we conduct the experiments on the QWEN1.5-4B and use the Counter Answer Ratio (CAR) to measure the model's preference for answering the substituted object. The results are shown in Figure 7. Our observations are summarized as follows:"}, {"title": "3.5 Detailed Description Can Make the LLMs' Objectives More Explicit", "content": "In this section, we aim to explore whether refining questions can encourage the model to exhibit desired behavior when encountering conflicts. Specifically, we incorporate descriptions of temporal and semantic conflicts within the questions. For temporal conflict scenarios, we add specific years to the questions. For semantic conflict scenarios, we include detailed descriptions of the subjects. We conduct experiments to observe the model's behavior with and without internal conflicts when provided with two pieces of external conflicting evidence. We analyze the impact of including these descriptions on the model's performance in both scenarios. The results are shown in Figure 8. We observe that LLMs, whether with or without internal conflicts, exhibit a significant decrease in MR when provided with external knowledge containing descriptions compared to without them. Take LLAMA3-70B as an example, the MR drops from 67.52% to 8.31% when descriptions are included. Furthermore, when internal conflicts are presented, adding descriptions also makes the LLMs' objectives more explicit. For example, when the internal conflict ratio is 2:1, the MR decreases from 55.37% to 41.80% with the inclusion of descriptions. This suggests that the more specific and detailed the text, the more likely the LLM is to trust the external knowledge, and designing detailed instructions can effectively improve the faithfulness of LLMs."}, {"title": "4 Related Work", "content": "Taxonomy of knowledge conflicts Knowledge conflicts are mainly divided into two types: retrieved knowledge conflicts and embedded knowledge conflicts. Retrieved conflicts occur when the model's internal knowledge conflicts with externally retrieved information, commonly in retrieval-augmented generation (RAG) and tool-augmented scenarios [Zhang and Choi, 2021, Li et al., 2023, Peng et al., 2023, Kasai et al., 2024]. Embedded conflicts arise from conflicting parametric knowledge within LLMs, increasing uncertainty during knowledge-intensive tasks and undermining trustworthi- ness [Chang and Bergen, 2023, Chen et al., 2023b, Raj et al., 2023a, Rabinovich et al., 2023, Raj et al., 2023b, Bartsch et al., 2023]. Currently, most research focuses on retrieved conflicts. Our work extends this by investigating both types and their interactions.\nThe Causes of Knowledge Conflicts With the rapid expansion of diverse knowledge sources, the risk of misinformation generated by LLMs has increased, posing challenges for detection [Chen and Shu, 2023, Bengio et al., 2024, Wang et al., 2023b, Solaiman et al., 2023, Goldstein et al., 2023, Ferrara, 2024]. Therefore, misinformation is the main focus in previous work as a cause of knowledge conflicts [Hsu et al., 2021, Ko et al., 2022, Li et al., 2023]. However, many factors contribute to knowledge conflicts in real-world scenarios, such as knowledge update [Lazaridou et al., 2021] and the multiple meanings of words [Sevgili et al., 2022a]. In CONFLICTBANK, we construct conflicts from three causes to provide a more comprehensive analysis.\nKnowledge Conflicts Datasets To construct conflict-related datasets, previous works have primarily adopted two methods, including entity-level substitution [Longpre et al., 2021, Chen et al., 2022, Si et al., 2023, Wang et al., 2023a] and generative approaches using LLMs [Ying et al., 2024, Xu et al., 2024a, Tan et al., 2024]. Recent datasets combined these two methods to create more coherent conflicting pairs [Xie et al., 2024], providing insights into the causes and behaviors of LLMs when encountering conflicts [Aggarwal et al., 2021, Chen et al., 2021]. However, these datasets primarily focus on conflicts in retrieved knowledge, with few addressing internal conflicts within parametric memory and more complex scenarios."}, {"title": "5 Conclusion", "content": "We develop CONFLICTBANK, a novel and comprehensive dataset for studying the effect of knowledge conflicts from misinformation, temporal updates, and semantic variations. For each of the knowledge conflict source, we utilize LLMs to generate three styles of texts to maximize the dataset diversity. In summary, CONFLICTBANK is a large diverse dataset consists of 553K QA pairs and 7M knowledge conflict evidence in high quality. The QA pairs could be used for model evaluations, and the evidence could be utilized for simulating the conflicts encountered in the LLM pre-training and the inference phases. With CONFLICTBANK, we conduct pilot experiments to investigate LLMs' behaviors under three common conflict scenarios, including the embedded knowledge conflict in pre-training, the retrieved knowledge conflict when inference, and the interplay between the above two conflicts. We believe CONFLICTBANK could be used in broad applications, and help analyze and build trustworthy large language models."}, {"title": "A Discussion", "content": ""}, {"title": "A.1 Code Access", "content": "We have uploaded our datasets to Hugging Face. The claim and evidence conflict pairs can be found at https://huggingface.co/datasets/Warrieryes/CB_claim_evidence, and the QA pairs used for analysis are available at https://huggingface.co/datasets/ Warrieryes/CB_qa. We have documented all code (including the code to preprocess the data, create, train, and evaluate the baseline models and metrics) in an openly-available GitHub repository: https://github.com/zhaochen0110/conflictbank."}, {"title": "A.2 Motivation", "content": "In essence, our work aims to provide a large-scale, diverse, and realistic benchmark to study knowl- edge conflicts in LLMs. Our motivation stems from exploring how retrieved and embedded knowledge conflicts impact model behavior and reliability across various scenarios. To align our dataset distri- bution and research with real-world situations, we construct conflicts from three different causes, including misinformation, temporal discrepancies, and semantic divergences. Our benchmark allows for an equitable comparison of different conflict effects on models, addressing the limitations of existing datasets that often focus narrowly on specific conflict types. Ultimately, by analyzing the results of our dataset, we aim to offer a detailed and nuanced understanding of how models handle conflict information, guiding the development of more robust and trustworthy language models in real-world scenarios."}, {"title": "A.3 Limitation", "content": "Our approach uses generative methods to efficiently construct a large number of conflict pairs, a widely adopted technique in current research [Xie et al., 2024]. Although conflict pairs may be extracted from pre-training corpora, the vast amount of data makes it challenging to efficiently identify and extract a significant number of conflicts. In future work, we will explore more methods for constructing conflict pairs to verify the robustness of our dataset."}, {"title": "A.4 Ethics Statement", "content": "In this paper, we created a comprehensive benchmark CONFLICTBANK for analyzing knowledge conflicts. The dataset is constructed based on Wikidata, which is under the public domain7. Therefore, we can adapt these data to construct our dataset. We will also release our data under the same license. The scope of our dataset is purely for scientific research. However, the contexts from the model outputs that may be considered offensive. Adopting such content is not a decision of the authors, and all content does not reflect the views of the authors of this paper."}, {"title": "B Dataset Details", "content": "We exhibit a complete example of our proposed CONFLICTBANK in Table 1."}, {"title": "B.1 Comparison of CONFLICTBANK and Prior Datasets", "content": "In Table 2, we show the detailed comparison of our CONFLICTBANK benchmark and prior knowledge conflict datasets. Our dataset is the first to include three main causes of conflict and can be used to evaluate the effects of knowledge conflict on retrieved knowledge, embedded knowledge, and their interactions."}, {"title": "B.2 Running time", "content": "Table 3 shows the running time and data volumn after each step for CONFLICTBANK."}, {"title": "B.3 CONFLICTBANK Templates", "content": "The templates that we used to create CONFLICTBANK is shown in Table 4."}, {"title": "B.4 Human Evaluation", "content": "In this section, we recruited five volunteers to evaluate the entailment between claims and generated evidence and the contradiction between default and conflict evidence. Each volunteer assessed a sample of 200 randomly selected examples to ensure the quality and reliability of our dataset. They were tasked with two main evaluations:\n\u2022 Entailment Check: Determining whether the generated evidence logically supports the corre- sponding claim.\n\u2022 Conflict Verification: Ensuring that the default and conflict evidence are contradictory.\nThe human evaluation results showed a high level of accuracy in our data generation process. Out of the 200 examples assessed, only one example was found to be ambiguously conflicting. As shown in Table 5, although the model generated evidence for the misinformation claim \"Daniel Rousse worked for Technical University of Liberec\u201d, it also included information about his work at \u201c\u00c9cole de technologie sup\u00e9rieure\u201d due to existing knowledge within the model. Despite this, the overall conflict remained unaffected, so we retained this type of generated evidence.\nThis indicates that our confirmation classifier and NLI model effectively ensure the integrity of the conflict pairs in our dataset. These evaluations confirm the robustness of our dataset and its suitability for studying knowledge conflicts in LLMs."}, {"title": "C Experimental Details", "content": ""}, {"title": "C.1 Chosen Models", "content": "We perform comprehensive experiments on 12 representative large language models, covering four series. Below is the detailed description:\n1. GEMMA [Team et al., 2024] leverages transformer-based networks with enhanced attention mechanisms and optimized layer normalization, as well as fine-tuning with domain-specific pre-training and rigorous hyperparameter tuning inspired by Gemini family [Team et al., 2023] to ensure high performance. We select models with 2B and 7B parameters for our analysis.\n2. LLAMA2 [Touvron et al., 2023] is a popular open-source foundation model, trained on 2T tokens with efficient grouped-query attention (GQA) [Ainslie et al., 2023]. For our analysis, we choose models with 7B, 13B, and 70B parameters.\n3. LLAMA3 builds on LLaMA2 with further architectural enhancements and larger datasets, pushing the boundaries of open-source foundation models. It is trained on over 15T tokens collected from public sources. Models with 7B and 70B parameters are selected for our analysis.\n4. QWEN1.5 [Bai et al., 2023a], the latest version of Qwen series [Bai et al., 2023b], is a decoder-only transformer model with SwiGLU activation, RoPE, multi-head attention. We analyze models with parameter sizes of 0.5B, 4B, 7B, 14B, and 70B."}, {"title": "C.2 Implementation Details", "content": "To investigate the impact of internal knowledge conflicts within the parametric memory, we continue pre-training three representative LLMs, including QWEN1.5-4B, MISTRAL-7B, and LLAMA3-8B We utilize eight NVIDIA Tesla A100 GPUs to train models with LLaMA Factory library [Zheng et al., 2024]. In our experiments, we train four different conflict ratio models on each foundational model: 1:0, 2:1, 1:1 and 2:3. To ensure fair comparisons, we fix the training to 4500 steps for each category, covering a total of 1.06 billion tokens [Su et al., 2023, Zhu et al., 2024]. Specifically, we use a learning rate of 2e-5 and set the batch size at 256. To facilitate parallel training, we employ DeepSpeed Zero-Stage 3 [Ren et al., 2021] and FlashAttention2 [Dao, 2023]."}, {"title": "D LLM Prompts for Different Steps", "content": "In this section, we provide a detailed list of all prompts for different steps, offering a clear reference for understanding our experimental approach:\n\u2022 The prompt for generating semantic conflict descriptions is shown in Figure 9.\n\u2022 The prompt for generating default evidence is shown in Table 6.\n\u2022 The prompt for generating misinformation conflict evidence is shown in Table 7.\n\u2022 The prompt for generating temporal conflict evidence is shown in Table 8.\n\u2022 The prompt for generating semantic conflict evidence is shown in Table 9.\n\u2022 The prompts for evaluation can be found from Figure 10 to Figure 12."}, {"title": "Task: Resolve semantic conflicts in descriptions involving the same terms used for different roles, due to\npolysemy. Modify the descriptions to reflect the most accurate and contextually appropriate roles, aligning\nthem with the correct usage scenario.", "content": "Objective: To accurately align and correct descriptions of terms that are used ambiguously across different\ncontexts. This involves clarifying the specific roles these terms denote in various scenarios, ensuring that\neach description is contextually correct and unambiguous.\nExample:\n- Default Claim: Franck Dupont holds the position of conseiller municipal de Zouafques.\nConflicting Claim: Franck Dupont holds the position of Governor of Taraba State.\nOriginal Description for \"Franck Dupont\": French politician.\nDescription for \"Governor of Taraba State\": Political position in Nigeria.\nTask: Modify the description to modify the usage of \"Franck Dupont\" by aligning it with a role appropriate\nfor \"Governor of Taraba State\".\n- Modified Description for \"Franck Dupont\": Nigerian politician.\nTemplate for Generating Descriptions:\nDefault Claim: Anne Hathaway received Primetime Emmy Award.\nConflicting Claim: Anne Hathaway received Hugo Award.\nOriginal Description for \"Anne Hathaway\u201d: American actress.\nDescription for \"Hugo Award\": set of awards given annually for the best science fiction or fantasy\nworks and achievements of the previous year.\n- Task: Modify the description to modify the usage of \"Anne Hathaway\" by aligning it with a role appropriate\nfor \"Hugo Award\".\nModified Description for \"Anne Hathaway\u201d: [Only return the answer]"}, {"title": "Memorization Ratio (MR)", "content": "$\\mathrm{MR}=\\frac{\\mathrm{OAR}}{\\mathrm{OAR}+\\mathrm{CAR}}$"}]}