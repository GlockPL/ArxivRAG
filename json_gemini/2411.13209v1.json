{"title": "COMPARATIVE ANALYSIS OF AUDIO FEATURE EXTRACTION FOR\nREAL-TIME TALKING PORTRAIT SYNTHESIS", "authors": ["Pegah Salehi", "Sajad Amouei Sheshkal", "Vajira Thambawita", "Sushant Gautam", "Saeed S. Sabet", "Dag Johansen", "Michael A. Riegler", "P\u00e5l Halvorsen"], "abstract": "This paper examines the integration of real-time talking-head generation for interviewer training,\nfocusing on overcoming challenges in Audio Feature Extraction (AFE), which often introduces\nlatency and limits responsiveness in real-time applications. To address these issues, we propose\nand implement a fully integrated system that replaces conventional AFE models with Open Al's\nWhisper, leveraging its encoder to optimize processing and improve overall system efficiency. Our\nevaluation of two open-source real-time models across three different datasets shows that Whisper\nnot only accelerates processing but also improves specific aspects of rendering quality, resulting in\nmore realistic and responsive talking-head interactions. These advancements make the system a more\neffective tool for immersive, interactive training applications, expanding the potential of AI-driven\navatars in interviewer training.", "sections": [{"title": "1 Introduction", "content": "The application of AI in education has gained widespread attention for its potential to enhance learning experiences\nacross disciplines, including psychology [1, 2]. In the context of investigative interviewing, especially when questioning\nsuspected child victims, AI offers a promising alternative to traditional training approaches. These conventional\nmethods, often delivered through short workshops, fail to provide the hands-on practice, feedback, and continuous\nengagement needed for interviewers to master best practices in questioning child victims [3, 4]. Research has shown\nthat while best practices recommend open-ended questions and discourage leading or suggestive queries [5, 6], many\ninterviewers still struggle to implement these techniques effectively during real-world investigations [7]. The adoption\nof AI-powered child avatars provides a valuable solution, enabling Child Protective Services (CPS) workers to engage\nin realistic practice sessions without the ethical dilemmas associated with using real children, while simultaneously\noffering personalized feedback on their performance [8].\nOur current system leverages advanced AI techniques within a structured virtual environment to train professionals in\ninvestigative interviewing. Specifically, this system integrates the Unity Engine to generate virtual avatars. Despite the\npotential advantages of our AI-based training system, its effectiveness largely depends on the perceived realism and\nfidelity of the virtual avatars used in these simulations [9]. Based on our findings, we observed that avatars generated\nusing Generative Adversarial Networks (GANs) demonstrated higher levels of realism compared to those created with\nthe Unity Engine in several key aspects [10]."}, {"title": "2 Related Work", "content": "The development and research of our avatar is touching upon two main areas: virtual avatars for interviewer training\nand real-time talking portrait synthesis."}, {"title": "2.1 Virtual Avatar for Interview Training", "content": "The evolution of child avatar training systems has advanced investigative interviewing techniques, albeit with varying\ndegrees of automation and efficacy. Early systems, such as those developed by Linn\u00e6us University and AvBIT Labs [14],\nprimarily utilized prerecorded responses, which constrained interaction dynamics. The LiveSimulation [15] enhanced\nthese methods by allowing interaction with a videotaped child, thereby improving open-ended questioning skills [16].\nEmpowering Interviewer Training (EIT) [17] introduced more dynamic interactions through a rule-based algorithm that\nfacilitated more effective learning with a virtual child. The ViContact [18] further progressed these methodologies by\nintegrating virtual reality (VR) with automated feedback, thereby enhancing both questioning skills and socio-emotional\nsupport.\nBuilding on prior advancements, our previous platform [19] integrated GPT-3 within a Unity framework to simulate\nchild interviews, aiming to improve response dynamism and training effectiveness. Despite these efforts, the system was\ncriticized for the lack of realism in avatar appearance, which detracted from user engagement during interactions [19].\nTo address this issue, our current work aims to utilize lifelike talking portrait generation in real-world applications,\nspecifically for training in child interview skills, with the expectation that it will improve visual realism."}, {"title": "2.2 Talking Portrait Synthesis", "content": "In recent years, real-time audio-driven talking portrait synthesis has garnered significant attention due to its applications\nin digital humans, virtual avatars, and video conferencing. Several approaches have been proposed to balance visual\nquality, synchronization, and computational efficiency. Live Speech Portrait [20] uses auto-regressive predictive coding"}, {"title": "3 Methodology", "content": "This section introduces the AFE models-Deep-Speech 2, Wav2Vec 2.0, HuBERT, and Whisper-compared in this\nstudy and outlines the interactive avatar's system architecture."}, {"title": "3.1 Audio Feature Extraction", "content": "A key aspect of talking-head generation is the ability to capture distinguishing speech features, as this directly affects\nthe synchronization and quality of the audiovisual output. In the proposed framework, feature extraction is conducted\nusing four ASR models: Deep-Speech 2 [31], Wav2Vec 2.0 [32], HuBERT [33], and Whisper [12]. Each of these\nmodels extracts both acoustic features and language representations from raw audio signals as part of their architecture.\nThe following sections provide further details on these four models."}, {"title": "3.1.1 Deep-Speech 2", "content": "Deep-Speech 2, developed by Baidu, utilizes bidirectional recurrent neural networks (BRNNs) [34] alongside con-\nvolutional layers to capture context from both past and future frames, enhancing speech recognition accuracy. Key\ntechniques like Batch Normalization and SortaGrad stabilize training, making the model effective across different\nacoustic conditions, including noisy environments [31]."}, {"title": "3.1.2 Wav2Vec 2.0", "content": "Wav2Vec 2.0 is a transformer-based model developed for self-supervised feature extraction directly from raw audio\nsignals [32]. Initially, the model processes audio waveforms into representations using a convolutional neural network\n(CNN) paired with a Gaussian Error Linear Unit (GELU) activation function [35], which captures latent speech features\nacross temporal frames 21, 22, ..., ZT. These features are then fed into a transformer network [36], which is trained\nthrough a contrastive loss objective. This loss function enables the model to differentiate between correct and incorrect\nquantized representations of the audio signal [37].\nThis self-supervised training allows Wav2Vec 2.0 to learn rich, contextualized embeddings from unlabeled speech data,\neffectively building contextual representations across continuous speech and capturing dependencies over the entire\nsequence of latent representations. This approach reduces the need for hand-engineered features, while leveraging the\npowerful representations learned by Wav2Vec 2.0, resulting in improved performance for various speech processing\napplications."}, {"title": "3.1.3 HuBERT", "content": "Hidden-Unit BERT (HuBERT) [33] introduces a self-supervised approach that addresses key challenges in speech\nprocessing: handling multiple sound units per utterance, the lack of lexicon during pre-training, and the absence of\nexplicit segmentation of sound units. By applying prediction loss to masked regions, HuBERT learns a combined\nacoustic and language model from unmasked inputs. Pre-trained on Librispeech [38] (960 hours) and Libri-light [39]"}, {"title": "3.1.4 Whisper", "content": "In this paper, we propose the use of Whisper Tiny [12], designed for lightweight applications, offers efficient processing\nand broad applicability in low-resource or edge environments. With approximately 39 million parameters, it is tailored\nfor real-time applications on less powerful devices while maintaining the core Whisper architecture. This model follows\nan encoder-decoder Transformer structure, allowing it to perform versatile multilingual transcription, translation, and\nvoice activity detection within a compact and efficient design, making it particularly suitable for talking-head systems\nand real-time audio-visual synchronization tasks.\nWhisper Tiny uses log-Mel spectrograms as input features, derived from 25-millisecond windows with a 10-millisecond\nstride, which are scaled to a near-zero mean. This spectrogram is processed through Transformer encoder blocks\ncontaining convolutional layers with GELU activations [35]. These layers capture critical acoustic and linguistic\nfeatures across languages and environments, leveraging Whisper's extensive pre-training on multilingual, multitask\ndatasets. This approach provides the model with robust noise resilience and the ability to maintain high accuracy\nwithout dataset-specific fine-tuning.\nThe Whisper model's design has demonstrated a substantial 80-90% reduction in processing times compared to\nalternative models like Deep-Speech, Wav2Vec, and HuBERT, particularly for longer audio clips (see Fig. 3).\nThe Whisper model processes raw audio $A(t)$ by transforming it into a log-Mel spectrogram $S(f, t)$ as follows:\n$S(f,t) = log(\\sum_{k=0}^{N-1} |A(t)e^{-i2\\pi\\frac{kf}{N}}|)$ \nThis spectrogram, capturing core frequency components, is then passed through Whisper's encoder to generate\nhigh-dimensional audio embeddings $E$:\n$E = WhisperEncoder(S(f, t))$\nwhere $E$ has shape ($T_{initial}$, $C'$), with $T$ representing the number of time steps aligned to the visual frames, and $C = 384$\nas the dimensionality of the feature embedding space. Synchronization with a 25 FPS visual frame rate is achieved by\napplying a sliding window with parameters $w = 16$, stride $s = 2$, and padding $p = 7$, yielding a final feature matrix\nwith shape (750, 16, 384). This setup ensures precise temporal alignment across 750 frames over 30 seconds, enhancing\nthe real-time accuracy and fluidity of interactions in talking-head applications."}, {"title": "3.2 System Architecture", "content": "The system architecture of our interactive child avatar, as depicted in Figure 1, is composed of several modules:\nListening, STT, Language, TTS, AFE, Frames Rendering, and Audio Overlay.\nThe Listening module is the entry point of the system, where user speech is captured through a button-based recording\nprocess. Users click to start recording their voice input and click again to stop, saving the recorded audio file. During\nthis recording phase, the avatar remains in a listening state, utilizing a pre-rendered video based on an empty audio\nfile where it exhibits natural, non-verbal behaviors such as blinking and subtle movements while remaining silent.\nThis module employs a speech recognition system to continuously listen for spoken input from the user, initiating the\nconversion process by passing the audio data to the speech-to-text module.\nThe STT module utilizes OpenAI's Whisper model [12] to perform real-time conversion of spoken input into text,\ntranscribing the user's speech for further processing. The language module is responsible for generating contextually\nappropriate and dynamic responses. It leverages GPT for prompt engineering, simulating a child's conversational style.\nThe transcribed text from the STT is processed here, where GPT generates relevant responses tailored to the interaction\ncontext. Once the response text is generated, it is passed to the TTS, which uses Amazon Polly [41] to convert the text\ninto speech. This module maintains the consistency of the avatar's voice with a child's persona."}, {"title": "4 Experiments", "content": "This section rigorously evaluates various AFE models across two frameworks, focusing on model efficiency, synchro-\nnization accuracy, and responsiveness."}, {"title": "4.1 Experimental Setup", "content": "Datasets, hardware, and configurations are outlined for evaluating AFE model performance in real-time talking portrait\nsynthesis."}, {"title": "4.1.1 Dataset", "content": "The dataset used in our experiments comprises a combination of publicly available video datasets and a privately\nsourced video. We selected three high-definition speech video clips, each with an average duration of approximately\n6,700 frames (around 4.5 minutes, as recorded at 25 FPS). The raw videos, originally recorded at their native resolutions\n(which can also be used in the experiments), were cropped and resized to 512 \u00d7 512 pixels. However, the Obama video\nfrom AD-NeRF [27], which was originally processed at 450 \u00d7 450 pixels, was used in that resolution without further\nresizing.\nTo ensure fairness and reproducibility, two of the video clips used in our experiments were sourced from the Internet.\nSpecifically, we utilized the \"Obama\" video from the publicly released data of AD-NeRF [27] and the \"Shaheen\" video.\nThe third video clip, featuring a young girl's speech, was privately sourced to align with the application objectives of the\ninvestigative interview. Explicit permission was obtained from the individual for the use of this video in this research.\nFor each video, the first 91% frames, along with the corresponding audio, were used as training data, while the final 9%\nof the data was reserved for subsequent evaluation, in accordance with previous studies [27, 26, 28]."}, {"title": "4.1.2 System Configuration", "content": "Experiments were conducted on a machine with a 12th Gen Intel(R) Core(TM) i9-12900F CPU, 31 GiB of RAM, and\nan NVIDIA RTX 4090 GPU with 24 GiB of VRAM, running CUDA 12.4 on an Ubuntu operating system."}, {"title": "4.2 Real-Time Talking-Head Speed Analysis", "content": "Despite progress in real-time talking-head systems, their widespread use is limited by the complexity of integrating\nmultimodal inputs like audio and facial landmarks. This complexity challenges both synchronization and processing\nefficiency, which are required for smooth real-time performance. Given these challenges, we considered it necessary\nto evaluate the real-time capabilities of existing models to substantiate performance claims. To accurately assess the\nefficiency of these models, we conducted an experiment using the Obama dataset [27]. The experiment was carried out\nunder identical hardware and CUDA environments across all applicable open-source methods, ensuring a consistent\nbasis for comparison."}, {"title": "4.3 AFE Analysis", "content": "Based on the results presented in Figure 2, we selected RAD-NeRF [26] and ER-NeRF [28] as the frameworks for\nfurther experimentation due to their better performance compared to other models. To conduct a systematic comparison\nof AFE models, we will employ Deep-Speech, Wav2Vec [32], HuBERT [33], and Whisper-Tiny [12] within both\nRAD-NeRF [26] and ER-NeRF [28]. The models will be trained from scratch with each AFE configuration to assess\ntheir impact on system performance, focusing on factors such as lip synchronization accuracy, visual quality, and overall\nexecution time. In the following, we divide the analysis into two key aspects-speed and quality\u2014to provide a more\ndetailed evaluation of each AFE configuration's impact on system performance."}, {"title": "4.3.1 AFE Speed Analysis", "content": "In evaluating the speed of AFEs, we conducted an analysis between Whisper [12] and other well-known AFE models,\nincluding Deep-Speech [31], Wav2Vec [32], and HuBERT [33]. The results, illustrated in Figure 3, reveal that Whisper\ngreatly outperforms the other models across varying audio durations, especially compared to Deep-Speech, which\nshows increasing execution times as audio duration grows. Whisper consistently achieves notably lower execution\ntimes, making it particularly advantageous for conversational systems.\nAdditionally, Figure 4 compares the execution times of RAD-NeRF [26] and ER-NeRF [28] using various AFE models,\nincluding Whisper, Deep-Speech [31], Wav2Vec [32], and HuBERT [33]. The results show the time-saving advantages\nof Whisper across both models. By integrating Whisper, we achieved a reduction in processing time, which is important\nfor real-time applications such as talking-head generation and interactive avatars. These findings reveal the potential\nof Whisper as an effective solution for accelerating AFE processes in interactive avatar systems, making them more\nefficient and responsive."}, {"title": "4.3.2 AFE quality Analysis", "content": "To further validate utilizing Whisper as AFE, we conducted an evaluation of the system's rendering quality across\nvarious settings. In the self-driven setting, where the ground truth data corresponds to the same identity as the generated\noutput, we employ several quantitative metrics to assess the quality of portrait reconstruction:"}, {"title": "\u2022 Peak Signal-to-Noise Ratio (PSNR):", "content": "This metric measures the fidelity of the reconstructed image relative to\nthe ground truth. The PSNR is calculated as:\n$PSNR = 10.log_{10}(\\frac{MAX_I}{MSE})$\nwhere MAX\u2081 is the maximum possible pixel value of the image, and MSE is the Mean Squared Error between\nthe reconstructed and ground truth images."}, {"title": "\u2022 Structural Similarity Index Measure (SSIM):", "content": "SSIM evaluates structural similarity by considering luminance,\ncontrast, and structure. The formula is:\n$SSIM(x, y) = \\frac{(2\\mu_x\\mu_y + C_1)(2\\sigma_{xy} + C_2)}{(\\mu_x^2 + \\mu_y^2 + C_1) (\\sigma_x^2 + \\sigma_y^2 + C_2)}$\nwhere $\u00b5_x$ and $\u00b5_y$ are the average intensities, $\u03c3_x$ and $\u03c3_y$ are variances, and $\u03c3_{xy}$ is the covariance between\nimages x and y. $C_1$ and $C_2$ are constants for stability."}, {"title": "\u2022 Learned Perceptual Image Patch Similarity (LPIPS) [42]:", "content": "Measures perceptual similarity between the\ngenerated and ground truth images by calculating the distance between feature representations extracted from\na deep neural network. This metric captures differences in visual features that align more closely with human\nperception than simple pixel-wise comparisons, making it useful for assessing image quality in terms of\nperceptual fidelity.\n$LPIPS = \\frac{1}{N} \\sum_{i=1}^{N} || f(x_{pred}) - f(x_{truth}) ||_2$\nwhere $f(x)$ represents the feature representation of image x extracted by a neural network (e.g., AlexNet),\n$x^{pred}$ and $x^{truth}$ are the predicted and ground truth images respectively, and N is the number of patches or\nfeature points compared."}, {"title": "\u2022 Landmark Distance (LMD) [43]:", "content": "This metric measures the geometric accuracy of facial landmarks by\ncalculating the Euclidean distance between corresponding landmark points in the generated and ground truth\nimages."}, {"title": "", "content": "$LMD = \\frac{1}{N} \\sum_{i=1}^{N} \\sqrt{(x_i^{pred} - x_i^{truth})^2 + (y_i^{pred} \u2013 y_i^{truth})^2}$\nwhere ($x_i^{pred}$, $y_i^{pred}$) and ($x_i^{truth}$, $y_i^{truth}$) are the coordinates of the i-th landmark in the predicted and ground truth\nimages, respectively, and N is the total number of landmarks."}, {"title": "\u2022 Fr\u00e9chet Inception Distance (FID) [44]:", "content": "FID assesses the similarity between distributions of real and generated\nimages. The formula is:\n$FID = |\\mu_r \u2013 \\mu_g|^2 + Tr(\\Sigma_r + \\Sigma_g \u2013 2\\sqrt{\\Sigma_r\\Sigma_g})$\nwhere $\u00b5$ and $\u2211$ represent the mean and covariance of features for real (r) and generated (g) images."}, {"title": "\u2022 Action Units Error (AUE) [45]:", "content": "Measures the accuracy of lower facial muscle movements by calculating the\nsquared differences in action unit intensities between the generated and ground truth images. In this study, we\nspecifically evaluate the lower face region, which is relevant for expressions related to speech and emotion.\n$AUE_{lower} = \\frac{1}{N} \\sum_{i=1}^{N} (AU_i^{pred} \u2013 AU_i^{truth})^2$\nwhere $AU_i^{pred}$ and $AU_i^{truth}$ are the intensities of the i-th action unit in the lower face region for the predicted\nand ground truth data, respectively, and N is the total number of lower face action units evaluated."}, {"title": "\u2022 SyncNet Confidence Score (Sync) [46]:", "content": "Measures the lip-sync accuracy by evaluating the alignment between\naudio and lip movements in generated talking-head videos. This metric utilizes SyncNet, which calculates a\nconfidence score based on the similarity between embeddings of audio and video frames.\n$Sync = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{v_i s_i}{max(||v_i||_2||s_i ||_2, \\epsilon)}$\nwhere $v_i$ and $s_i$ are the embeddings for the i-th video and audio frames, respectively, computed by SyncNet.\nThis formula calculates the cosine similarity between the embeddings, giving a score between 0 and 1 for each\nframe pair. N is the total number of frame pairs evaluated, and higher average values indicate better lip-sync\nquality."}, {"title": "4.4 System Responsiveness Analysis", "content": "Table 3 outlines execution times per component, with the Listening component excluded due to its dependence on user\ninput duration. The analysis is segmented into raw execution time and the percentage contribution of each stage to the\noverall system latency, providing a granular view of where time is consumed in the process of generating the interactive\navatar's responses."}, {"title": "5 Discussion and Future Work", "content": "The results support the value of Whisper [12] as a robust and efficient AFE in real-time talking portrait synthesis,\nespecially for applications that require responsiveness. In comparing execution times across AFE models, including\nDeep-Speech [31], Wav2Vec [32], HuBERT [33], and Whisper [12], it is evident that Whisper consistently achieves\nlower execution times. This reduction in processing latency is particularly important for real-time applications where\ndelays can disrupt user engagement [19]. The faster response enabled by Whisper allows smoother interactions in\napplications like interactive avatars, which improves both the user experience and system reliability.\nThe performance differences among the AFE models are significant, particularly regarding speed. Whisper's streamlined\narchitecture, which efficiently processes raw audio into high-dimensional feature embeddings, contrasts with more\nresource-intensive models like Deep-Speech, which tends to slow down with longer audio durations. This efficiency\ntranslates directly to reduced system latency, which is important for maintaining the immediacy expected in real-time\navatar interactions. While speed is a significant advantage, Whisper also stands out in synchronization accuracy and\nvisual quality, albeit with smaller differences. Higher SyncNet confidence scores for Whisper indicate superior lip-sync\naccuracy, a key factor for creating lifelike avatars. This precision contributes to system realism, helping to mitigate the\nuncanny valley effect [47] that can disrupt user immersion in avatar interactions [10].\nWhisper excels in speed and is optimized for multilingual, multitask scenarios, efficiently handling diverse audio inputs\nwith minimal latency. Its feature encoding integrates seamlessly with visual frame rendering, ensuring rapid visual\nadjustments in real-time speech applications. This design provides an edge over other models by maintaining low\nexecution times across audio durations, which is particularly valuable for dynamic avatars requiring consistent, timely\nresponses."}, {"title": "6 Conclusion", "content": "This study addressed the latency challenges associated with AFE, which has hindered the practical deployment of\nreal-time talking portrait systems in real-world applications. By integrating the Whisper model\u2014a high-performance\nASR system-into our framework, we achieved notable reductions in processing delays. These optimizations not only\nincreased the overall responsiveness of the interactive avatars but also improved the accuracy of lip-syncing, making\nthem more applicable for immersive training applications.\nOur findings affirm Whisper's capability to meet real-time demands, particularly in applications requiring responsive\ninteractions and minimal delay. This efficiency is important for training environments such as CPS, where timely and\nrealistic interactions can greatly impact training efficacy. By achieving these improvements, Whisper-integrated systems\nemerge as promising solutions for a variety of real-time applications, including virtual assistants, remote education, and\ndigital customer service platforms."}]}