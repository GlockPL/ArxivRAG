{"title": "PReLU: Yet Another Single-Layer Solution to the XOR Problem", "authors": ["Rafael C. Pinto", "Anderson R. Tavares"], "abstract": "This paper demonstrates that a single-layer neural network using Parametric Rectified Linear Unit (PReLU) activation can solve the XOR problem, a simple fact that has been overlooked so far. We compare this solution to the multi-layer perceptron (MLP) and the Growing Cosine Unit (GCU) activation function and explain why PReLU enables this capability. Our results show that the single-layer PReLU network can achieve 100% success rate in a wider range of learning rates while using only three learnable parameters.", "sections": [{"title": "1 Introduction", "content": "The XOR problem has traditionally been used to illustrate the limitations of single-layer networks since Minsky and Papert's seminal work [8], which even contributed to the first AI Winter [12]. It has traditionally required at least one hidden layer to solve, making it a litmus test for network complexity. Trivially, any function, no matter how complex, can be learned in a single layer by just using itself as the activation function, and that says nothing about its general applicability and usefulness. Here, however, we reveal this ability in a simple, general and well-established activation function. This study demonstrates how using the Parametric Rectified Linear Unit (PReLU) activation [4] overcomes these limitations, effectively solving the XOR problem without additional layers. This ability has significant implications for neural network design and efficiency, potentially leading to simpler architectures for complex problems.\nOn another front, recent advancements in neuroscience have revealed that individual human neocortical pyramidal neurons can learn to compute the XOR function [3]. This discovery has inspired new artificial neuron models and activation functions that aim to bridge the gap between biological and artificial neurons [9]. Albeit not producing the same activation curves as the ones found in biological neurons, the PReLU activation matches their representational power, at least regarding the XOR function."}, {"title": "2 Background", "content": "Activation functions play a crucial role in neural networks by introducing non-linearity, allowing networks to learn complex patterns. Recent surveys have cataloged hundreds of activation functions [5], highlighting the diversity and ongoing research in this area.\nOf special interest to us is the Growing Cosine Unit (GCU), a recently proposed oscillatory activation function defined as f(x) = x cos(x) [10]. GCU has shown promising results in various benchmarks and can solve the XOR problem with a single neuron, and thus will serve as a comparison to PReLU results. Other oscillating activation functions capable of solving XOR were presented in [9], but were left out since GCU was the best overall performer in that work."}, {"title": "2.1 Parametric ReLU (PReLU)", "content": "PReLU, introduced by He et al. [4], is defined as:\n\nf(x) =\n{\nX if x \u2265 0\nax if x < 0\n\nwhere a is a learnable parameter. This allows flexibility, enabling a single-layer solu- tion to the XOR problem. Besides, PReLU generalizes various other activation functions, depending on the parameter a:\n\u2022 When a = 0, PReLU becomes the standard ReLU function.\n\u2022 When a = 1, PReLU becomes the identity function. It might be useful for accu- mulators or copying values between layers, as well as layer pruning [2].\n\u2022 When 0 < a < 1, PReLU becomes the LeakyReLU function [7].\n\u2022 When a < 0, PReLU allows for non-monotonic behavior, which is crucial for solving the XOR problem in a single layer.\n\u2022 When a = \u22121, PReLU becomes the absolute value function, as we show below.\nPReLU can also be written as f(x) = max(0,x)+a\u00b7min(0,x). When a = \u22121, PReLU becomes f(x) = max(0, x) + (\u22121) \u00b7 min(0, x) = max(0, x) \u2013 min(0, x). Now, let's consider two cases:\n\u2022 When x \u2265 0, f(x) = max(0, x) \u2013 min(0, x) = x \u2212 0 = x = |x|\n\u2022 When x < 0, f(x) = max(0, x) \u2013 min(0, x) = 0 \u2212 x = -x = |x|\nTherefore, for all real numbers x, PReLU with a = \u22121 is equivalent to the absolute value function |x|. This is further illustrated in figure 1 and table 1, which also show how to solve the XOR problem using the abs function. Also relevant: |x| = ReLU(x) +\nReLU(-x) and, more generally, PReLU(x) = ax + (1-x)\u00b7 ReLU(x), which shows that a single PReLU neuron is equivalent to a sum of 2 monotonic neurons (both non-linear in the absolute function case), potentially reducing network size."}, {"title": "3 Methodology", "content": "Aside from the theoretical equivalences previously shown, we wanted to verify PReLU's performance in the XOR problem empirically. We implemented 1 and compared three networks in PyTorch [11] using the following configurations obtained through hyperpa- rameter optimization [1]:\n\u2022 Single-Layer PReLU: 2 input neurons, 1 output neuron with PReLU (no bias). Three learnable parameters in total. Inputs in the range {\u22121,1}.\n\u2022 Single-Layer GCU: 2 input neurons, 1 output neuron with GCU. Three learnable parameters in total. Inputs in the range {0,1}.\n\u2022 MLP: 2 input neurons, 2 hidden neurons with Tanh activation [6] (Lecun's variant, which learned better and faster than ReLU in this experiment), 1 output neuron (no output bias). Eight learnable parameters in total. Inputs in the range {\u22121,1}.\nAll networks were trained for 300 epochs using the Adam optimizer and Mean Squared Error (MSE) loss. Biases and a are initialized at 0. We conducted 100 experiments using different random initializations for each network. Networks' performance across different learning rates and input ranges was also analyzed."}, {"title": "4 Results", "content": "PReLU achieved a 100% success rate in solving XOR across a wide range of learning rates (Figure 2), while converging faster than both GCU and MLP, reaching 100% accuracy in under 20 epochs (Figure 4a). It also achieved zero loss, while GCU could not, probably due to its locally bounded nature around the origin (Figure 4b). Both PReLU and GCU showed less variance in learned decision boundaries compared to MLP, which is expected due to their lower number of learnable parameters (Figure 5), and both were significantly faster to train than MLP also due to smaller network size (Figure 3).\nThe learning behavior of PReLU (Figure 6) reveals that the input range and initial weights play crucial roles in convergence. For inputs in {\u22121,1}, PReLU always converges to a global minimum. However, for inputs in {0, 1}, the convergence depends on the initial parameter quadrant, with some quadrants leading to local minima corresponding to the AND function."}, {"title": "5 Conclusion", "content": "This study demonstrates that by using PReLU, a single-layer network gains the flexibility to implement non-monotonic functions when necessary, enabling it to solve the XOR problem efficiently, obtaining a 100% success rate in a wide range of learning rates with fewer parameters. This insight may be valuable in designing efficient neural architectures for problems traditionally thought to require multiple layers, as also concluded in [10]. This aligns with recent discoveries in neuroscience about the computational capabilities of individual neurons [3] and supports the development of more biologically inspired artificial neurons [9].\nFuture work could explore the implications of these findings for more complex prob- lems and larger networks, aiming at potentially smaller training times and less energy consumption. Additionally, investigating other activation functions that enable single- layer solutions to traditionally multi-layer problems without added complexity could lead to new paradigms in neural network design."}]}