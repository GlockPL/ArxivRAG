{"title": "PrivQuant: Communication-Efficient Private Inference with Quantized Network/Protocol Co-Optimization", "authors": ["Tianshi Xu", "Shuzhang Zhong", "Wenxuan Zeng", "Runsheng Wang", "Meng Li"], "abstract": "Private deep neural network (DNN) inference based on secure two-party computation (2PC) enables secure privacy protection for both the server and the client. However, existing secure 2PC frameworks suffer from a high inference latency due to enormous communication. As the communication of both linear and non-linear DNN layers reduces with the bit widths of weight and activation, in this paper, we propose PrivQuant, a framework that jointly optimizes the 2PC-based quantized inference protocols and the network quantization algorithm, enabling communication-efficient private inference. PrivQuant proposes DNN architecture-aware optimizations for the 2PC protocols for communication-intensive quantized operators and conducts graph-level operator fusion for communication reduction. Moreover, PrivQuant also develops a communication-aware mixed precision quantization algorithm to improve the inference efficiency while maintaining high accuracy. The network/protocol co-optimization enables PrivQuant to outperform prior-art 2PC frameworks. With extensive experiments, we demonstrate PrivQuant reduces communication by 11x, 2.5 \u00d7 and 2.8x, which results in 8.7x, 1.8 \u00d7 and 2.4\u00d7 latency reduction compared with SiRNN, COINN, and CoPriv, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "With deep learning being applied to increasingly sensitive data and tasks, privacy has emerged as one of the major concerns in the deployment of deep neural networks (DNNs). To enable DNN inference on private data, secure two-party computation (2PC) is proposed as a promising solution and has attracted increasing attention in recent years [1-4].\nSecure 2PC helps solve the following dilemma: the server owns a private DNN model and the client owns private data. The server is willing to provide the model as a service but is reluctant to disclose it. Simultaneously, the client wants to apply the model to private data without revealing the data as well. Secure 2PC frameworks can fulfill both parties' requirements: the two parties can learn the inference results but nothing else beyond what can be derived from the results [5, 6]."}, {"title": "2 PRELIMINARIES", "content": "We now briefly introduce the security primitives used in the paper. We also summarize the notations in Table 1.\nSecret Share (SS). We use 2-out-of-2 secret sharing to keep the input data private throughout the whole inference. For an l-bit value $x \\in Z_{2^l}$, we denote its shares by $(x)^{(1)} = ((x)^{(0)}, (x)^{(1)})$ such that $x = (x)^{(0)} + (x)^{(1)} \\mod 2^l$ where the server holds $(x)^{(1)}$ and the client holds $(x)^{(0)}$.\nOblivious Transfer (OT). We use 1-out-of-2 OT, denoted by $\\binom{1}{2} -OT_1$, where one party is the sender with 2 l-bit messages $x_0, x_1$ and the other party is the receiver with an index $j \\in \\{0, 1\\}$. The receiver learns $x_j$ as the output, and the sender learns nothing. The communication cost for a $\\binom{1}{2} - OT_1$ is $O(\\lambda + 2l)$ bits.\nUnderlying Protocols. PrivQuant relies on several underlying protocols from SiRNN [4], briefly introduced in Table 2."}, {"title": "2.1 Network Quantization for 2PC Inference", "content": "Quantization converts floating-point numbers into integers [7]. Specifically, a floating point number $x_f$ can be approximated by an $l_x$-bit integer $x_q$ and a scale $s_x$ through quantization as $x_q/s_x$, where\n$x_q = \\text{max}(-2^{l_x-1}, \\text{min}(2^{l_x-1} \u2013 1, \\text{round}(s_x x_f)))$.\nThe multiplication of two floating point numbers $x_f$ and $w_f$, denoted as $y_f$, can be approximately computed as $x_q w_q/(s_w s_x)$, which is a quantized number with $l_x + l_w$ bit and $s_w s_x$ scale. Then, $y_f$ usually needs to be re-quantized to $y_q$ with $l_y$ bit and $s_y$ scale as follows:\n$y_q = \\text{max}(-2^{l_y-1}, \\text{min}(2^{l_y-1} \u2013 1, \\text{round}(\\frac{s_y}{s_w s_x}w_q x_q)))$.\nFor the addition of two quantized numbers $x_q$ and $y_q$, directly computing $x_q$ and $y_q$ leads to incorrect results. Instead, the scales and the bit-widths of $x_q$ and $y_q$ need to be aligned first. Uniform quantization protocol CrypTFlow2 leverages the same bit-widths and scales for the tensors, e.g. 37-bit bit-width and 13-bit scale across all layers while mixed bit-width protocol SiRNN uses different quantization parameters for weight and activation, which introduces large communication overhead."}, {"title": "2.2 Notations", "content": "We now briefly introduce the security primitives used in the paper. We also summarize the notations in Table 1."}, {"title": "2.3 Related works", "content": "Existing secure 2PC-based frameworks mainly leverage two classes of techniques: homomorphic encryption (HE) [9], which is computation intensive, and OT [10] which is communication intensive. In this paper, we focus on OT-based methods instead of HE-based methods as HE usually requires the client to have a high computing capability for encryption and decryption. SecureML [11] is the first OT-based framework for secure 2PC-based DNN inference. It suffers from high communication and takes around 1 hour to finish a simple two-layer network. To improve the 2PC inference efficiency, follow-up works can be roughly categorized into two classes: 1) protocol optimizations at the operator level, e.g., convolutions [12-14], matrix multiplications [15, 16], activation functions [3], and at network level, e.g., hybrid protocols [17, 18]; 2) 2PC-aware network optimization, such as using 2PC friendly activation functions [19] and 2PC-aware DNN architecture optimization [8, 20-26].\nPrevious works leveraging quantized networks to improve the efficiency of private inference may fall into either class. XONN [1] leverages binary weight and activation to reduce communication cost but suffers from large accuracy degradation. CrypTFlow2 [3] proposes a hybrid protocol that supports OT-based linear layers with uniform bit-widths. SiRNN [4] further proposes 2PC protocols for bit width extension and reduction to allow mixed bit-width inference. COINN [2] simultaneously optimizes both the quantized network as well as the protocols for linear and non-linear layers. However, COINN uses approximations extensively and also has both the least significant bit (LSB) error and the most significant error (MSB) 1-bit error during truncation. In Table 3, we compare PrivQuant with these works qualitatively, and as can be observed, PrivQuant leverages both protocol and network optimization to support efficient and faithful mixed bit-width inference."}, {"title": "2.4 Threat Model and Security of PrivQuant", "content": "PrivQuant works in a general private inference scenario, where a server holds a private DNN model and a client owns private data [13, 16]. PrivQuant enables the client to obtain the inference while keeping the model and the client's data private. Besides, the model parameters are privately held by the server in plaintext while the activations are in secret sharing form during the whole inference. Consistent with previous works [2, 4, 6, 11], PrivQuant adopts an honest-but-curious security model in which both parties follow the specification of the protocol but also try to learn more from the protocols than allowed. PrivQuant is built upon underlying OT"}, {"title": "3 MOTIVATION", "content": "In Figure 1, we profile the communication cost of a ResNet block with both CrypTFlow2 and SiRNN protocol [3, 4]. We observe the total communication of SiRNN is dominated by convolution (offline) while the online communication bottleneck comes from the residual addition and re-quantization. Notably, the communication of all these operators decreases significantly with the reduction of weight and activation bit-width. Therefore, ideally, SiRNN should have achieved substantially higher efficiency compared to the uniform quantized protocol CrypTFlow2.\nHowever, from Figure 1, we find that the communication of the 16-bit SiRNN protocol is comparable to or even higher than CrypTFlow2 with 37-bit. We further compare the detailed protocols for one convolution with residual connection in CrypTFlow2 and SiRNN in Figure 2. This identifies two critical issues with SiRNN:\n\u2022 Complex protocols unaware of quantized network architecture: to enable mixed bit-width inference without introducing errors, SiRNN requires complex protocols to align the bit-widths and scales of operands, including bit-width extension, truncation, and re-quantization. These protocols are agnostic to the model architecture and introduce extremely high communication overhead, as shown in Table 2 for communication complexity.\n\u2022 Network quantization unaware of protocols: SiRNN uniformly applies the same bit-width to weights and activations. Such a strategy ignores the cost of the protocol under different bit-width settings and results in sub-optimal communication efficiency or network accuracy.\nBased on the observations above, we propose PrivQuant which features a network/protocol co-optimization. As shown in Figure 3, we propose network-aware protocol optimizations at both the operator level (Section 4.1) and the graph level (Section 4.2), directly"}, {"title": "4 EFFICIENT MIXED BIT-WIDTH 2PC PROTOCOLS", "content": "We now describe the protocol optimization of PrivQuant at both the operator level and the graph level."}, {"title": "4.1 Operator Level Protocol Optimization", "content": "4.1.1 DNN Architecture-Aware Convolution Protocol. Baseline Protocol in SiRNN. In SiRNN, a convolution is first converted to"}, {"title": "4.1.2 Simplified Residual Protocol", "content": "As shown in Figure 1, residual addition takes up a large portion of online communication due to the complex alignment of both bit-widths and scales (Figure 2 (b)). The aligned operands are then added and quantized back to $l_x$-bit as shown in Figure 5 (a). The baseline protocol is quite expensive because both re-quantization and extension require multiple rounds of communication. Therefore, we propose a simplified residual protocol in Figure 5 (b) which aligns the bit-width and scale of the residual directly to the convolution output for addition. Through simplification, we get rid of all the operators for the convolution output's quantization while keeping the high bit-width residual addition since $l_{acc}$ is usually quite large. As we will demonstrate in Section 6.2, this approach significantly reduces communication cost."}, {"title": "4.2 Graph Level Protocol Optimization", "content": "At the graph level, we propose both activation sign propagation and fused protocol for quantization to reduce communication. Figure 6 shows an example of a residual block."}, {"title": "4.2.1 Activation Sign Propagation", "content": "Previous work has proposed the most significant bit (MSB) optimization. Specifically, when the MSB of the operands is known, several protocols can be optimized"}, {"title": "4.2.2 Fused Protocol for Quantization", "content": "$\u03a0^{l_1,l_2}_{Trunc}$ is widely used in the quantized inference to avoid overflow. We observe opportunities to fuse neighboring truncation and extension protocols as well as re-quantization protocols at the graph level to reduce communication. First, we introduce the following propositions for the protocol fusion.\nPROPOSITION 4.1. For a given $(x)^{(l_1)}, \u03a0^{l_1,l_2}_{Trunc}((x)^{(l_1)})$ can be decomposed into $\u03a0^{l_1,l_2}_{TR}$ followed by $\u03a0^{l_2,l_1}_{Ext}$ as\n$\u03a0^{l_1,l_2}_{Trunc}((x)^{(l_1)}) = \u03a0^{l_2,l_1}_{Ext}(\u03a0^{l_1,l_2}_{TR}((x)^{(l_1)}))$.\nThe decomposition reduce the communication from $O(\u03bb(l_1 + 3))$ to $O(\u03bb(l_1 + 2))$.\nPROPOSITION 4.2. Two consecutive extension protocols can be fused into one as\n$\u03a0^{l_2,l_3}_{Ext}(\u03a0^{l_1,l_2}_{Ext}((x)^{(l_1)})) = \u03a0^{l_1,l_3}_{Ext}((x)^{(l_1)})$.\nExtension fusion reduces communication from $O(\u03bb(l_1 + l_2 + 2))$ to $O(\u03bb(l_1 + 1))$.\nPROPOSITION 4.3. For a given $(x)^{(l_1)}$, the consecutive truncation and extension protocol can be fused as\n$\u03a0^{l_1,l_3}_{Ext}(\u03a0^{l_1,l_2}_{Trunc}((x)^{(l_1)})) = \u03a0^{l_3}_{Ext}(\u03a0^{l_1,l_2}_{Trunc}((x)^{(l_1)}))$\n$= \u03a0^{l_1-l_2,l_3}_{Ext}(\u03a0^{l_1,l_2}_{TR}((x)^{(l_1)}))$.\nCombining Proposition 4.1 and 4.2, this fusion reduces communication from $O(\u03bb(2l_1 + 4))$ to $O(\u03bb(l_1 + 2))$.\nProposition 4.3 enables us to fuse the re-quantization and extension protocols further. We first describe the proposed re-quantization protocol in Algorithm 1. The key idea of fusion is when Requant ends up with $\u03a0_{Trunc}$ or $\u03a0_{Ext}$, we can further fuse them."}, {"title": "5 COMMUNICATION-AWARE QUANTIZATION", "content": "In this section, we introduce our communication-aware network quantization strategy based on the optimized 2PC protocols proposed before.\nTo reduce communication and improve inference efficiency, we propose leveraging low bit-width quantization for convolutions. We follow [1] and directly quantize the network with ternary weight and activation in Table 6. However, we find it incurring a large accuracy degradation. To improve the accuracy while minimizing the communication of the 2PC-based inference, we consider the following two optimizations during quantization-aware training (QAT)."}, {"title": "5.1 High Bit-width Residual", "content": "As previous works have revealed [29, 30], we also found through experiments that the quantized ternary activation is indeed the root cause of the accuracy loss.\nTo improve the network accuracy, we observe the widely used residual connections in a ResNet block can act as a remedy and we propose to use the high bit-width residual connections to improve the activation precision without impacting the operands' bit-widths for the convolution, which incurs negligible overhead as shown in Table 7."}, {"title": "5.2 Communication-Aware bit-width Optimization", "content": "We also use a mixed bit-width quantization strategy to allocate the bit-widths based on communication cost. Existing works [30, 31] widely use the sum of $l_w \\cdot l_x \\cdot FLOPs$ for each layer as a proxy to estimate the inference cost. However, we observe the 2PC-based inference latency does not correlate well with $l_w \\cdot l_x$. We profile the communication of our optimized protocols for different stages with different bit-widths in Figure 7. As we can see, the most efficient bit-width configurations for different layers are different. For example, stage 1 prefers W4A2 while stage 4 prefers W2A4. Hence, we propose a communication-aware mixed bit-width quantization algorithm based on HAWQ [32-34] and leverage our theoretic communication analysis to form a communication cost model to guide the bit-width optimization.\nLet $H_i$ denote the Hessian matrix of the i-th layer with a weight tensor $W_i$. HAWQ finds that layers with a larger trace of $H_i$ are more sensitive to quantization. Hence, the perturbation of the i-th layer, denoted as $\u03a9_i$, due to the quantization error can be computed as:\n$\u03a9_i = Tr(H_i) \\cdot ||Q(W_i) \u2013 W_i||_2$,\nwhere $Tr(H_i)$ is the average Hessian trace, and $||Q(W_i) \u2013 W_i||_2$ is the $L_2$ norm of quantization perturbation. Given the communication bound and a network with L layers, we formulate the communication-aware bit-width optimization as an integer linear programming problem:\n$Objective: \\underset{\\{b_i\\}_{i=1}^L}{\\text{min}} \\sum_{i=1}^L \u03a9_i$\n$Subject to: \\sum_{i=1}^L Comm_i \u2264 \\text{Communication Limit}$\nHere, $\u03a9_i$ is the i-th layer's sensitivity with bit-width $b_i$, $Comm_i$ is the associated communication cost in private inference. The objective is to minimize the perturbation of the whole model under the communication constraint. Here we fix the activation quantization to 4-bit on MiniONN and 6-bit on ResNet and only search for the quantization scheme of weight."}, {"title": "6 EXPERIMENTAL RESULTS", "content": "6.1 Experimental Setup\nPrivQuant consists of two important parts, i.e., efficient protocols for quantized inference and communication-aware quantization. For network quantization, we use quantization-aware training (QAT) on three architectures and three datasets as shown in Table 8. We first load 8-bit quantization networks as the checkpoints. For MiniONN and ResNet32, we fine-tune the networks on CIFAR-10 and CIFAR-100 for 200 and 100 epochs, respectively, and we fine-tune ResNet50 on ImageNet for 30 epochs. Following [35] and [36], various widely used augmentation techniques are combined to improve the performance of quantized networks. In specific, for"}, {"title": "6.2 Micro-Benchmark Evaluation", "content": "Convolution Protocol Evaluation. In Table 9, we compare the performance of the proposed convolution protocols with SiRNN under our low bit-width quantization strategy including W2A4, W2A6, and W2A8 (W2A4 means 2-bit weight and 4-bit activation). We select convolution layers with different dimensions from ResNet50. Due to the unavailability of open-source code for COINN's protocol, we are unable to evaluate the performance of each operation. As shown in Table 9, compared to SiRNN under the same quantization strategy, PrivQuant achieves 1.6 ~ 3.0\u00d7 communication reduction through DNN architecture-aware protocol optimization.\nResidual Protocol Evaluation. In Table 10, we compare the performance of the simplified residual protocol with SiRNN. We focus"}, {"title": "6.3 End-to-End Inference Evaluation", "content": "Networks Accuracy and Communication Comparison. We now perform an end-to-end inference evaluation. We use communication-aware quantization to sample several quantized networks and perform end-to-end private inference. We draw the Pareto curve of accuracy and communication in Figure 8.\nResult and analysis. From Figure 8, we make the following observations: (1) PrivQuant achieves SOTA Pareto front of accuracy and communication in all three benchmarks. More specifically, compared with COINN, PrivQuant achieves 2.4\u00d7 communication reduction and still 1% higher accuracy in MiniONN. In ResNet32 and ResNet50, PrivQuant can achieve 2.2\u00d7 and 3.4\u00d7 communication reduction with the same accuracy. (2) PrivQuant has achieved an improvement of an order of magnitude over CrypTFlow2 and SiRNN. With higher accuracy, PrivQuant can achieve 16x, 12x, 10x communication reduction compared with CrypTFlow2 and 10\u00d7, 6.6\u00d7, 13\u00d7 communication reduction compared with SiRNN on MiniONN, ResNet32 and ResNet50, respectively.\nCompared with COINN. Our experiments reveal that COINN suffers significant accuracy degradation across all three datasets, for instance, a 3.5% drop in ResNet50. This issue primarily stems from COINN's inability to implement reliable quantization protocols such as extension and re-quantization, leading to errors in both the LSB and MSB. Under conditions of low bit-width quantization, even a 1-bit error can result in a substantial decrease in accuracy. In contrast, PrivQuant avoids such errors entirely and achieves considerably lower communication cost while maintaining accuracy.\nCompared with other network optimization work. In Figure 8, we also compare PrivQuant with CoPriv [8] which utilizes winograd convolution to reduce communication. PrivQuant achieves 2.6% higher accuracy with the same communication on ImageNet, demonstrating the effectiveness of our quantization strategy and corresponding protocol optimizations.\nCommunication and Latency Comparison. To evaluate the communication and latency reduction clearly, we selected one point from each of the three Pareto graphs marked with red circles in Figure 8.\nResults and analysis. As shown in Table 11, compared with previous works, with higher accuracy, PrivQuant reduces the communication by 2.4 ~ 22\u00d7 and the latency by 1.7 ~ 22\u00d7 on MiniONN. On ResNet32, PrivQuant reduces the communication by 2 ~ 10x and the latency by 1.3 6x. On ResNet50, PrivQuant reduces the communication by 1.6 ~ 13\u00d7 and the latency by 1.8 ~ 11x."}, {"title": "6.4 Ablation Study", "content": "Effectiveness of the proposed quantization strategy To understand the importance of the proposed quantization strategy, we use SiRNN's protocol with our quantization strategy called \"SiRNN+our Quant\" in Figure 8. We can see that our quantization strategy can achieve 3.8\u00d7, 5.9 \u00d7 and 6.6\u00d7 communication reduction with higher accuracy compared with SiRNN baseline on three benchmarks, respectively.\nMoreover, we evaluate the performance of uniform quantization to demonstrate the importance of communication-aware bit width optimization. Specifically, we use W4A4 and W5A5 for both ResNet32 and ResNet50. As shown in Figure 8 (Uniform Quant+our Protocol), communication-aware quantization is clearly superior to uniform quantization. For example, on ResNet32, it outperforms uniform quantization by 2.4% higher accuracy with the same 1.1GB of communication.\nEffectiveness of the proposed efficient protocols To prove the effectiveness of our optimized protocols, we dive into a basic"}, {"title": "7 CONCLUSION", "content": "To reduce the communication complexity and enable efficient secure 2PC-based inference, we propose PrivQuant to jointly optimize the secure 2PC-based inference protocols and the quantized networks. Our DNN architecture-aware protocol optimization achieves more than 2.4\u00d7 communication reduction compared to prior-art protocols. Meanwhile, by network and protocol co-optimization, we can achieve in total 1.6 ~ 22\u00d7 communication reduction and 1.3 ~ 22\u00d7 inference latency reduction, which improves the practicality of the secure 2PC-based private inference by one step further."}, {"title": "8 FUTURE WORK", "content": "Scalability to LLMs. We find PrivQuant can be applied to large language models (LLMs). Recent work [46] has shown the feasibility of quantizing LLMs to low precision, e.g., 3-bit for weights. We conduct an experiment on an attention layer of LLAMA-7B and demonstrate 5 ~ 13\u00d7 communication reduction over SiRNN in Table 12. We will further research the scalability of PrivQuant to LLMs in the future."}]}