{"title": "Multi-Source Spatial Knowledge Understanding for Immersive Visual Text-to-Speech", "authors": ["Shuwei He", "Rui Liu", "Haizhou Li"], "abstract": "Visual Text-to-Speech (VTTS) aims to take the spatial environmental image as the prompt to synthesize the reverberation speech for the spoken content. Previous research focused on the RGB modality for global environmental modeling, overlooking the potential of multi-source spatial knowledge like depth, speaker position, and environmental semantics. To address the issues, we propose a novel multi-source spatial knowledge understanding scheme for immersive VTTS, termed MS2KU-VTTS. Specifically, we first prioritize RGB image as the dominant source and consider depth image, speaker position knowledge from object detection, and semantic captions from image understanding LLM as supplementary sources. Afterwards, we propose a serial interaction mechanism to deeply engage with both dominant and supplementary sources. The resulting multi-source knowledge is dynamically integrated based on their contributions. This enriched interaction and integration of multi-source spatial knowledge guides the speech generation model, enhancing the immersive spatial speech experience. Experimental results demonstrate that the MS2KU-VTTS surpasses existing baselines in generating immersive speech. Demos and code are available at: https://github.com/MS2KU-VTTS/MS2KU-VTTS.", "sections": [{"title": "I. INTRODUCTION", "content": "Visual Text-to-Speech (VTTS) aims to generate reverberation speech that corresponds to spoken content based on the spatial environmental image. As human-computer interaction advances, VTTS has become integral to intelligent systems, playing a crucial role in fields such as augmented reality and virtual reality.\nUnlike acoustic matching tasks that focus on adapting input speech to match a reference environment [7], [9], [11], [12], environmental Text-to-Speech (TTS) seeks to synthesize speech with the environmental characteristics of the reference based on given textual content [8]. For example, the VoiceLDM [14] utilize the pre-trained CLAP model [15] to map a textual or audio description into an environmental feature vector that controls the reverberation aspects of the generated audio. In addition, the Environment-aware TTS [8] design an environment embedding extractor that learns environmental features from the reference speech by reducing the distance between embeddings of the same environment while increasing the distance between embeddings from different environments. In a recent study, ViT-TTS [1], introduce a transformer-based visual-text encoder to extract global spatial knowledge from the RGB image. Building on these advancements, this paper focuses on employing visual information as the cue to generate reverberation audio for the targeted environment.\nHowever, prior research in VTTS predominantly focuses on the RGB modality as the singular source of spatial knowledge. Therefore, integrating multi-source spatial data\u2014including depth imagery, speaker positioning, and spatial environmental semantics-could significantly enhance image comprehension. For example, the FEW-SHOTRIR [29] employs depth images to augment its grasp of environmental acoustics, markedly improving its capability to predict room impulse responses by embedding crucial spatial contexts and geometric details for precise audio simulations in unfamiliar settings. Moreover, the position of the speaker critically affects the perception of reverberation [16]. For instance, the ViGAS [17] utilizes speaker position data to refine the accuracy of spatial audio synthesis, aligning visual cues with auditory outputs, which substantially boosts the realism and spatial coherence of audio in novel-view acoustic synthesis tasks. Additionally, environment captions provide detailed spatial data on the layout and dimensions of settings, enhancing situational adaptability [14]. For instance, the AST-LDM [32] leverages textual descriptions of target environments as reference prompts to guide the acoustic scene transfer process, significantly expanding the model's versatility and applicability in creating immersive audio experiences.\nTo address above challenges, we propose a novel multi-source spatial knowledge understanding scheme for immersive VTTS, termed MS\u00b2KU-VTTS. Specifically, to expand the dimensions of environmental perception, we first prioritize the RGB image as the dominant source and consider depth image, speaker position knowledge from object detection as image-related supplement source, and semantic captions from image understanding LLM as semantic-related supplementary source. Afterwards, to deeply engage with both dominant and supplementary sources, we propose a Dominant-Supplement Serial Interaction mechanism to processes features from various data sources. It is through this sequential approach that not only is the model's perception of spatial relationships deepened, but also, via complex interactions between dominant and supplementary sources, precise modeling of environmental"}, {"title": "II. METHOD", "content": "As illustrated in the pipeline of Fig. 1, the proposed MS2KU-VTTS architecture comprises four components: 1) Multi-source Spatial Knowledge, constructed from both dominant and supplementary sources; 2) Dominant-Supplement Serial Interaction, which aggregates intra- and inter-modal information through RGB-Depth Interaction for comprehensive spatial understanding, including Speaker Position Enhanced Interaction, employing positional data to model environmental acoustics, and RGB-Semantic Interaction, acquiring spatial semantics from Gemini-generated captions; 3) Dynamic Fusion, wherein the multi-source knowledge is dynamically fused based on their contributions; and 4) Speech Generation, utilizing this integrated information to guide the synthesis of reverberant speech.\nThe dominant source, encompassing the RGB image, is supplemented by the depth image, speaker position from object detection, and semantic captions.\n1) Dominant Source: RGB: To learn the global knowledge in the RGB image, such as the material and color of objects, features are derived through a pre-trained ResNet18 [2], denoted as $F_R \\in \\mathbb{R}^{1\\times D}$, where $D$ represents the feature dimension.\n2) Image-related Supplement Source: Depth and Speaker Position: To capture the global knowledge within the Depth image, such as object arrangement, depth features are also extracted using a ResNet18, denoted as $F_D \\in \\mathbb{R}^{1\\times D}$. To acquire fine-grained spatial knowledge, this work integrates the positional information of the speaker as additional guidance, inspired by previous studies [3], [10], [13], [17]. Specifically, we first detect all recognizable objects within the RGB image and to ascertain their corresponding anchor box coordinates. Subsequently, those coordinates categorized as \u201chuman\u201dare selected and transformed into two-dimensional pixel coordinates $(x, y)$, which are normalized to [0, 1] by the image width and height, respectively. These coordinates are then mapped into a higher-dimensional space, the mathematical representation of which can be articulated as follows:\n$\\varphi(x,y) = (sin(2* \\pi x), cos(2* \\pi x), sin(2* \\pi y), cos(2* \\pi y))^{\\frac{D-1}{k=0}} $\nwhere $\\varphi(\\cdot)$ denotes the high-dimensional embedding of the position of speaker, employing harmonic terms up to order $D$. The embeddings are subsequently processed by adaptive max pooling followed by a multi-layer perceptron to extract positional features, denoted as $F_p \\in \\mathbb{R}^{1\\times D}$.\n3) Semantic-related Supplement Source: Spatial Image Caption: To efficiently fuse semantic information within the RGB image, Gemini [6] is employed to transform complex visual data into structured captions, which are subsequently processed by BERT [5] to extract spatial semantic features, denoted as $F_s$, where $F_s \\in \\mathbb{R}^{1\\times D}$.", "latex": ["F_R \\in \\mathbb{R}^{1\\times D}", "\\varphi(x,y) = (sin(2* \\pi x), cos(2* \\pi x), sin(2* \\pi y), cos(2* \\pi y))^{\\frac{D-1}{k=0}}", "F_D \\in \\mathbb{R}^{1\\times D}", "\\varphi(\\cdot)", "F_p \\in \\mathbb{R}^{1\\times D}", "F_s", "F_s \\in \\mathbb{R}^{1\\times D}"]}, {"title": "C. Dominant-Supplement Serial Interaction", "content": "As shown in the middle panel of Fig. 1, the Dominant-Supplement Serial Interaction (D-SSI) comprises three parts:\n1) RGB-Depth Interaction, which aggregates information from"}, {"title": "1) RGB-Depth Interaction", "content": "The RGB-Depth Interaction, inspired by LGPM [26], is introduced to achieve a more comprehensive representation of RGB and Depth features. It first refines the feature representations of each source through self-attention mechanisms, while simultaneously enabling cross-modal information fusion via cross-attention. After that, these representations are then aggregated to update the RGB and Depth features FR and FD, which is formulated as:\n$F_D = F_R + \\Phi_{sr}(F_R, F_R, F_R) + \\Phi_{cr}(F_R, F_D, F_D)$,\n$F_R = F_D + \\Phi_{sd}(F_D, F_D, F_D) + \\Phi_{cd}(F_D, F_R, F_R)$,\nwhere $\\Phi_{s*}(\\cdot)$ and $\\Phi_{c*}(\\cdot)$ denote the self-attention and cross-modal attention functions, respectively,\n2) Speaker Position Enhanced Interaction: To gain a in-depth understanding of spatial knowledge within a visual scene, we propose an attention-based Speaker Position Enhanced Interaction. Specifically, the position-related visual features as $P_{R/D}$ are computed, which can be formulated as:\n$P_{R/D} = F_{R/D}\\Upsilon((F_p)^T.F_{R/D})$,\nwhere $\\Upsilon(\\cdot)$ denotes the softmax function and $(\\cdot)^T$ represents the transpose operator. Subsequently, the position-related and visual features are combined via concatenation:\n$V_{R/D} = \\Psi(FC(Concat[F_{R/D},P_{R/D}]))$,\nwhere FC represents fully connected layers.\n3) RGB-Semantic Interaction: To capture spatial semantic from environment captions, we employ Spatial Semantic At-tention by applying a multi-head attention mechanism:\n$V_s = MultiHead(F_s, F_R, F_R)$,\nwhere $V_s \\in \\mathbb{R}^{1\\times D}$ is updated from $F_s$.", "latex": ["F_D = F_R + \\Phi_{sr}(F_R, F_R, F_R) + \\Phi_{cr}(F_R, F_D, F_D)", "F_R = F_D + \\Phi_{sd}(F_D, F_D, F_D) + \\Phi_{cd}(F_D, F_R, F_R)", "\\Phi_{s*}(\\cdot)", "\\Phi_{c*}(\\cdot)", "P_{R/D}", "P_{R/D} = F_{R/D}\\Upsilon((F_p)^T.F_{R/D})", "\\Upsilon(\\cdot)", "(\\cdot)^T", "V_{R/D} = \\Psi(FC(Concat[F_{R/D},P_{R/D}]))", "V_s = MultiHead(F_s, F_R, F_R)", "V_s \\in \\mathbb{R}^{1\\times D}"]}, {"title": "D. Dynamic Fusion", "content": "To effectively aggregate multi-source spatial knowledge, we employ a dynamic weighting approach inspired by MLA [24], which adjusts the contribution of each type of spatial knowledge based on its complexity, quantified by entropy. Higher entropy indicates greater difficulty in understanding, thereby reducing the relevance of that knowledge type. The entropy for each type as ui is computed as:\n$U_i = - \\sum_{j=1}^{D} p_{(i,j)} log \\left(p_{(i,j)}\\right)$,\nwhere $p_{(i,j)} = Softmax(V_i)$ represents the probability distribution over the features of a given type i. The relative importance Ai for each knowledge type is then determined by:\n$\\lambda_{i}=\\frac{\\exp \\left(U_{\\max }-U_{i}\\right)}{\\sum_{k=1}^{M} \\exp \\left(U_{\\max }-u_{k}\\right)}$,\nwhere Umax = max(u1, ...,uM), with M denoting the total number of spatial knowledge types. The representation of integrated multi-source knowledge as H is defined as follows:\n$H = \\lambda_{R} \\cdot H_{R}+\\lambda_{D} \\cdot H_{D}+\\lambda_{S} \\cdot H_{S}$.", "latex": ["U_i = - \\sum_{j=1}^{D} p_{(i,j)} log \\left(p_{(i,j)}\\right)", "p_{(i,j)} = Softmax(V_i)", "\\lambda_{i}=\\frac{\\exp \\left(U_{\\max }-U_{i}\\right)}{\\sum_{k=1}^{M} \\exp \\left(U_{\\max }-u_{k}\\right)}", "H = \\lambda_{R} \\cdot H_{R}+\\lambda_{D} \\cdot H_{D}+\\lambda_{S} \\cdot H_{S}"]}, {"title": "E. Speech Generation", "content": "As depicted in Fig. 1, our TTS system utilizes the ViT-TTS architecture. Initially, phoneme embeddings and visual features are converted into hidden sequences. These sequences are then adjusted by a speech generator to synchronize with speech frames. A spectrogram denoiser refines the adjusted hidden states into mel-spectrograms. For a comprehensive methodology, refer to ViT-TTS [1]."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "Utilizing the SoundSpaces-Speech dataset [16], we adhere to [1], [12] by removing out-of-view samples and partitioning the data into \"test-unseen\"-featuring room acoustics from novel scenes\u2014and \u201ctest-seen\"-comprising scenes encountered during training. The dataset encompasses 28,853 training, 1,441 validation, and 1,489 testing samples, each containing clean text, reverberant audio, and panoramic RGB-D images. Text sequences are converted into phonemes using an open-source tool\u00b9. Consistent with [20], [22], spectrograms are extracted via FFT with parameters: size 1024, hop size 256, and window size 1024 samples; these are then transformed into 80-bin mel-spectrograms. Fundamental frequency (F0) is extracted from raw waveforms using Parselmouth2.\nThe feature dimension is 512. The speaker position is detected using YOLOv8 3. Gemini Pro Vision analyzes spatial semantics by prompting: \u201cObserve this RGB panoramic image and briefly describe its contents, including specific objects or people and their locations. Detail the spatial relationships among them, such as which object is to the left, right, above, or below others. Please focus on key information only.\"The phoneme set includes 74 distinct phonemes, aligned with ViT-TTS [1] speech generation parameters. Training involves two phases: 1) a pre-training stage for the encoder following ViT-TTS protocols over 120k steps until convergence; 2) a main training stage on an NVIDIA A800 GPU, processing 48 sentences per batch for 160k steps. For inference, BigVGAN [23] uniformly converts mel-spectrograms into waveforms."}, {"title": "C. Evaluation Metrics", "content": "We evaluate waveform quality using both objective metrics and subjective assessments, aligning with previous studies [20], [21]. For objective analysis, we randomly select 50 samples from the test set. Our evaluation metrics include: 1) Perceptual quality, assessed by human listeners and quantified by the Mean Opinion Score (MOS), which rates audio quality"}, {"title": "IV. CONCLUSION AND FUTURE WORK", "content": "In this paper, we introduce a novel multi-source spatial knowledge understanding scheme, termed MS\u00b2KU-VTTS, capable of generating immersive, environment-matched reverberant speech. The proposed Dominant-Supplement Serial Interaction and Dynamic Fusion ensure precise modeling of overall environmental reverberation and a comprehensive understanding of multi-source knowledge, respectively. Experimental results affirm the superiority of MS\u00b2KU-VTTS over contemporary VTTS systems. In the future, we will focus on optimizing computational efficiency and enhancing the model's adaptability to diverse and previously unencountered spatial environments."}]}