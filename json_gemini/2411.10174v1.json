{"title": "A Hard-Label Cryptanalytic Extraction of Non-Fully Connected Deep Neural Networks using Side-Channel Attacks", "authors": ["Beno\u00eet Coqueret", "Olivier Sentieys", "Mathieu Carbone", "Gabriel Zaid"], "abstract": "During the past decade, Deep Neural Networks (DNNs) proved their value on a large variety of subjects. However despite their high value and public accessibility, the protection of the intellectual property of DNNs is still an issue and an emerging research field. Recent works have successfully extracted fully-connected DNNs using cryptanalytic methods in hard-label settings, proving that it was possible to copy a DNN with high fidelity, i.e., high similitude in the output predictions. However, the current cryptanalytic attacks cannot target complex, i.e., not fully connected, DNNs and are limited to special cases of neurons present in deep networks. In this work, we introduce a new end-to-end attack framework designed for model extraction of embedded DNNs with high fidelity. We describe a new black-box side-channel attack which splits the DNN in several linear parts for which we can perform cryptanalytic extraction and retrieve the weights in hard-label settings. With this method, we are able to adapt cryptanalytic extraction, for the first time, to non-fully connected DNNS, while maintaining a high fidelity. We validate our contributions by targeting several architectures implemented on a micro-controller unit, including a Multi-Layer Perceptron (MLP) of 1.7 million parameters and a shortened MobileNetv1. Our framework successfully extracts all of these DNNs with high fidelity (88.4% for the MobileNetv1 and 93.2% for the MLP). Furthermore, we use the stolen model to generate adversarial examples and achieve close to white-box performance on the victim's model (95.8% and 96.7% transfer rate).", "sections": [{"title": "1. Introduction", "content": "During the last decade, the number of tasks for which Deep Neural Networks (DNNs) have proven their effec- tiveness has steadily increased, leading to the widespread adoption of these algorithms in a large variety of fields. From computer vision to text translation and images gener- ation, DNNs are everywhere now, and the best models have become valuable intellectual property (IP). In the mean- time, the parallel effort from hardware designers have made possible DNNs' deployment on edge device. However, due to their high value, the IP of the deployed models must be protected against new attacks caused by the embedded context. Since the publication of a first side-channel attack against the IP of an embedded DNN [2], the number of physical-based attacks against DNNs has greatly increased. Several methodologies using side-channel attacks with the objective of the extraction of the DNN's hyperparameters have been proposed [11], [18]. DNN parameters have also been targeted by physical attacks, via side-channel [19], [32], or through fault injection [13], [27]. These types of attack, targeting the parameters of the model, try to copy the targeted model and perform a model extraction attack. Model extraction is not only a threat to embedded DNNs but to any deployed DNNs. There is therefore a large variety of methods, and even objectives, for theses attacks. We can characterize the two main types of objectives or adversarial goals for model extraction with the terminology introduced in [17]: accuracy-based model extraction and fidelity-based model extraction. The first aims at gaining access to a substi- tute model with good performance on the task of the targeted model without having to perform the whole training process. The second has for purpose to clone the targeted model to acquire a copy as close as possible to the original. The cloned model can then be used to gain information on the victim's DNN and potentially mount more powerful attacks against it. In this study, we will consider only fidelity-based model extraction. Jagielski et al. [17] were the first to propose a functional framework for fidelity-based model extraction of 1-layer neural network (NN) using the ReLU (Rectified Linear Unit)"}, {"title": "2. Background", "content": "2.1. Notations\nLet calligraphic letters $\\mathcal{X}$ denote sets, the corresponding capital letters X (resp. bold capital letters) denote random variables (resp. random vectors T), and the lowercase x (resp. t) denote their realizations. We will use $T_i$ to describe the i-th element of a vector T. Throughout this paper, the function modeled by a DNN is denoted as $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$, which characterizes its ability to classify a data $X \\in \\mathcal{X}$, e.g., an image, over a set of $\\mathcal{Y}$ classes. A DNN characterized by the function f and the set of weight @ is denoted $f_{\\Theta}$, and we denote $f_{\\hat{\\Theta}}$ the model extracted from $f_{\\Theta}$. The probability of observing an event X is denoted by $Pr[X]$. Finally, we denote $D_{\\mathcal{X}}$, the distribution over the set of data $\\mathcal{X}$.\n2.2. Fidelity-based model extraction\nModel extraction attacks target the confidentiality of a deployed model, by trying to obtain a copy of the victim's model. The most common goal, is to use the copy to gain the target's benefits, e.g., commercial value, performance on specific tasks, etc., without having to train a model. Another possibility is to use the stolen model to gain information on the target and mount higher-level attacks against it, e.g., generation of adversarial examples in a white-box scenario [7], [29] or membership inference attacks [10]. This diver- sity in the attack scenarios leads to several adversarial goals in a model extraction. Following the notations introduced in [17], we consider two main goals, namely task accuracy and fidelity. The first one has for purpose to extract $f_{\\hat{\\Theta}}$, such as, for the true task distribution $D_{\\mathcal{X}\\mathcal{Y}}$ over the sets of input $\\mathcal{X}$ and label $\\mathcal{Y}$, $f_{\\hat{\\Theta}}$ maximizes $Pr_{\\mathcal{X},\\mathcal{Y}}[argmax(f_{\\hat{\\Theta}}(X) = Y]$. In practice, this can be achieved through learning-based methods and optimisation. However, this approach induces variability in the optimisation problem which can lead to very different solutions (i.e., models) caused by the conver- gence to different local minima. This is why this method offers no guaranty from a fidelity point of view, and is used for the goal of task accuracy. The second adversarial goal consists in maximizing the similarity between the stolen and the original models in their predictions. The limits being functionally equivalent extraction which is defined by:\nDefinition 1 (Functional equivalence [17]). Two models $f_{\\Theta}$ and $f_{\\Gamma}$ achieve functional equivalence on $\\mathcal{X}$ if for any distribution of input $D_{\\mathcal{X}}$, the following equality holds:\n$\\forall X \\in \\mathcal{X}, f_{\\Theta}(X) = f_{\\Gamma}(X)$.\nThis is the strongest attack possible as it leads to an exact copy of the targeted DNN. In this paper, we only consider fidelity-based model extraction. To evaluate the success of such extraction, we use a relaxed definition of functional equivalence:\nDefinition 2 ($\\epsilon$, \u03b4)-functional equivalence [6]). Two mod- els $f_{\\Theta}$ and $g_{\\gamma}$ are ($\\epsilon$, \u03b4)-functionally equivalent on $\\mathcal{X}$ if:\n$Pr_{X\\in \\mathcal{X}} [|f_{\\Theta}(X) - g_{\\gamma}(X)| \\leq \\epsilon] \\geq 1 - \u03b4$.\nThe advantages of fidelity-based model extraction have stimulated researches on this subject. In particular physical-based attacks and cryptanalytic approaches have been pro- posed in recent years. Side channel attacks [2], [14], [19], [33] and fault-based attacks [13], [27] have been used to target all the weights of a DNN individually [2], [14], [19], or to extract partial information on the weights in order to train a substitute network using this information as a constraint [13], [27]. Theses methods achieve fidelity- based extractions but show limitations either in the attack complexity, in the architecture of the targeted DNN, or in the threat model, e.g., exclusive to certain platforms, white-box settings such as access to the logits or to an open copy of the targeted device. On the other hand, attacks based on cryptanalysis have proved effective against shallow networks [17], [26], and were then successfully adapted to DNNs [6], [28]. In particular, Carlini et al. [6] were able to steal a DNN with three hidden layers using $2^{17.8}$ queries and extract the 1,110 parameters with a maximum absolute error between the weights @ and the stolen weights 6 of $2^{-27.1}$. These methods will be briefly introduced in the following section.\n2.3. Cryptanalytic extraction methodology\nBoth methods in [6], [28] extract the DNN's weight by targeting iteratively each neuron, monitoring its state, in order to find its critical points. The critical points and the state of a neuron \u03b7 can be defined in the following way:\nDefinition 3 (Critical point and state of a neuron [6]). Given an input $X \\in \\mathcal{X}$, let V(\u03b7; X) be the function characterizing the input of the neuron \u03b7 before applying the activation function. Then X is said to be a critical point to the neuron \u03b7 if V(\u03b7; X) = 0. If V(\u03b7; X) > 0 (resp. V(\u03b7; \u03a7) < 0) then \u03b7 is said to be active (resp. inactive). The induced-hyperplane of a neuron can be defined accordingly:"}, {"title": "Definition 4 (Neuron-induced Hyperplane)", "content": "Definition 4 (Neuron-induced Hyperplane). The hyper- plane of a neuron \u03b7 corresponds to the set $H_\\eta \\subseteq \\mathcal{X}$ where $\\forall X \\in H_\\eta, V(\\eta; X) = 0$.\nAn example of critical points, neuron states and hyper- plane is provided in Figure 1. By finding enough critical points, it is possible to determine the equation of the neuron- induced hyperplane. This can then be used to find the weights' values with high precision, since by definition, the equation of the hyperplane follows:\n$\\sum_{i=1}^{n} \\Theta_{\\eta,i}X_i + \\beta_{\\eta} = 0, X \\in R^n$\n(1) with $\\Theta_{\\eta}$ (resp. $\\beta_{\\eta}$) the weight (resp. the bias) vector asso- ciated with the neuron \u03b7. It is important to notice that for a neuron \u03b7 with n parameters, the induced-hyperplane is an (n - 1)-dimensional piecewise-linear surface [12]. This causes that the weight vector can only be extracted up to a scaling factor. In practice, we set the first weight $\\Theta_{\\eta,0}$ of $\\Theta_{\\eta}$ as the scaling factor and from Equation 1 we obtain:\n$X_i = \\frac{\\sum_{i=1}^{n} \\Theta_{\\eta,i}X_i - \\beta_{\\eta}}{\\Theta_{\\eta,0}},  \\Theta_{\\eta,0} \\neq 0$\n(2)\nAfter the extraction of n-1 critical points, it is possible to retrieve the neuron's weights up to a scaling factor. This process is defined in [6] as signature search. The neuron's signature can be defined in the following manner:\nDefinition 5 (Neuron signature [4]). The signature of a neuron \u03b7 corresponds to a vector $S_{\\eta} \\in R^n$, for which there exists a scalar $a \\in R$ such that $\\Theta_{\\eta} = a * S_{\\eta}$.\nIn the previous example, the vector $S_\\eta = (1, \\frac{\\Theta_{\\eta,1}}{\\Theta_{\\eta,0}}, \\cdots \\frac{\\Theta_{\\eta,n}}{\\Theta_{\\eta,0}})$ is a signature of the neuron \u03b7 and $\\frac{\\Theta_{\\eta,0}}{\\Theta_{\\eta,0}}$ is the scaling factor. By considering the target model $f_{\\Theta}$ as an oracle, the search for neuron's signature can be performed directly using its input-output pairs. Indeed, if the target model is composed of piecewise linear activation functions, then $f_{\\Theta}$ is also piecewise linear and discontinuities can be observed in the gradient of $f_{\\Theta}$. These discontinuities correspond to each change of state by a neuron in the DNN. However performing the signature's search using these discontinuities requires high assumptions. Firstly, the target model's output needs to be either the confidence scores or the logits, to allow the attacker to estimate the gradient through finite difference. Secondly, the output needs to be returned in a high-precision format to get the most precise estimation. This method was introduced in [17] and adapted to DNNs in [6], [28] to extract ReLU-based network running double-precision, 64-bit floating-point data. Targeting deep ReLU networks is motivated by both the popularity of the ReLU function as an activation function and its properties. In particular, the discontinuity in the gra- dient of the ReLU function is localized in 0, which implies that by monitoring the gradient of the targeted model, critical points for each neuron can be found. Furthermore, the ReLU function is equivariant under positive multiplication, i.e., $\\forall x \\in \\mathcal{R}$ and $\\forall c \\in \\mathcal{R}+, ReLU(c \\times x) = c \\times ReLU(x)$. This means that the value of the scaling factor a of the neuron's signature is not important, only its sign. If the sign of the input of the ReLU function is respected, then the activation value of each neuron can later be re-scaled during the extraction of the neurons' signature in the following layers. Therefore the model extraction attack can be seen as an iterative process over the neurons based on two steps: 1) Extraction of the neuron's signature. 2) Extraction of the sign of the scaling factor to ensure that the neuron's state remains the same. While neuron signatures can be revealed through the discontinuities of the gradient, the sign of the scaling factor is not accessible directly by an attacker. Carlini et al. [6] solve this by testing all of the $2^m$ possibilities with m the number of neurons in the layer. As this becomes rapidly impractical for large DNNs, Canales-Mart\u00ednez et al. [4] propose another method based on activating specific neurons through crafted inputs to extract the sign of each neuron. Their method successfully extracts the sign of each neuron within a model with 8 hidden layers each with 256 neurons, for which the signatures were previously extracted. Although, theses methods have successfully performed complete extraction of Deep-ReLU networks up to 3 hidden layers and 100,480 parameters [6], deeper extraction has not been achieved. Furthermore, access to the confidence scores or the logits in a high-precision format is essential to the success of the extraction, which will make the attack impractical in most contexts. For example, in embedded systems, the hardware constraints can impose limitations on the precision or on the access to the logit values that could prevent the attack. However, the embedded-system context increases the surface of attack and allows an attacker to consider side-channel attacks to improve fidelity-based model extractions.\n2.4. Side-channel attacks\nHistorically, side-channel analysis (SCA) is a class of cryptographic attack in which an attacker tries to exploit the vulnerabilities of the implementation of a real-word crypto- system for key recovery by analyzing its physical charac- teristics via side-channel traces, like power consumption or"}, {"title": "3. Limitations in fidelity-based extraction", "content": "electromagnetic (EM) emissions. During the execution of an algorithm embedded into a crypto-system, side-channel traces record the intermediate variable (e.g., secret key) being processed. To discriminate the correct key hypoth- esis from the incorrect ones, an attacker uses a statistical distinguisher to extract the targeted variable from a large number of traces.\nDefinition 6 (Statistical distinguisher). A statistical dis- tinguisher, denoted $d : \\mathcal{T} \\rightarrow \\mathcal{K}$, is a statistical function identifying the dependence between a set of physical traces T and a targeted variable in K. In [2], Batina et al. were among the first to transpose this attack to the DNN paradigm by targeting the architecture and the parameters of the model with two statistical distin- guishers via a Simple Power Attack (SPA) and a Correlation Electromagnetic Attack (CEMA). Since, CEMA has been extended to target the weights of DNN on microcontrollers (MCUs) [19] as well as on GPUs [14]. Several other methodologies commonly applied in side-channel attacks on crypto-system have been extended to target DNNs. In particular, deep learning-based side-channel attacks [3], [22] have been successfully adapted for extracting the hyperpa- rameters of DNNs. Gao et al. [11] trained a set of meta- models on physical traces to extract the architecture of the targeted models with a high precision. Previously, Maia et al. [23] successfully reconstructed the architecture of ResNet and VGG models running on a GPU using the physical traces. All theses attacks use supervised learning to construct the statistical distinguisher. This imposes to the attacker an access to an open device that provides the true label before learning the mapping. This assumption is very restrictive, leading researches to focus on more permissive threat models using unsupervised learning methods. In par- ticular, well-researched unsupervised cluster classification algorithms, such as k-means clustering [1], can be used as statistical distinguisher to find partitions effectively without any manual methods or prior profiling. While we argue that this method could improve fidelity-based model extraction, to the best of our knowledge, it has never been applied to the extraction of DNN weights.\n3. Limitations in fidelity-based extraction\nAlthough the methods presented in the previous section demonstrate strong results, fidelity-based model extraction remains a difficult task, especially in the context of embed- ded systems. In Section 3.1, we provide further details on the limitations of these methodologies in this specific context. Then, in Section 3.2, we describe the threat model that is considered in this paper. Finally, in Section 3.3, we present the overall flow of our fidelity-based extraction framework.\n3.1. Fidelity-based extraction issues\nEven if an ideal scenario is assumed for the attacker, i.e., access to the complete confidence scores is given, cryptan- alytic extraction using the previously introduced method is still challenging for several reasons. First of all, the informa- tion in the input-output pairs does not allow identification of the neuron for which the critical point is found. Therefore, an attacker cannot validate if consecutive critical points found are related to the same neuron. Carlini et al. [6] solve this issue by sampling a large number of critical points with respect to the number of neurons. This statistically ensures that each neuron has enough critical points to infer the hyperplane required to find the corresponding signature, but leads to the search of unnecessary critical points. This might cause the attack to be too complex for deeper architectures regarding the large number of required queries. Another issue, reported in [4], is the special cases of neurons that almost never change state. For these neurons, the number of queries needed for the search of critical points can exceed the mean number of required queries for a classical neuron by a factor of 10. Even if these special cases of neuron are highly difficult to threat in practice, no investigations are provided in the SOTA to deal with such issue. Additional practical limitations should be considered in the embedded context, as hardware constraints might force the neural network to use low-precision format of data for the weights, the activations and the input-output pair. This makes gradient estimations noisier, which has an impact on the search for critical points. Furthermore, in the case of classification task, if the output of the model $f_{\\Theta}$ is only the hard-label, i.e., $\\hat{y} = argmax(f_{\\Theta}(X))$, then the attack scenarios designed in [6], [17], [28] are no longer applicable. While the logits or the confidence scores might be given alongside the prediction in a MLaaS context, for embedded systems, it is frequent that the output is restricted only to the class with the highest probability. For example, FINN [31] is a popular framework for the deployment of low- precision neural network on FPGA, and by default only the top-1 prediction is returned as output. Finally, even if the application context of the neural network imposes that the output includes the confidence scores, several countermeasures can easily be implemented to increase the cost of the cryptanalytic extraction attack. Indeed, confidence scores are already used to generate ad- versarial examples in a black-box scenario [8]. Therefore, countermeasures designed to limit adversarial example gen- eration, such as rounding or adding Gaussian noise to the confidence scores [16], [30], would also be efficient against extraction frameworks based on cryptanalysis such as [6], [17], [28]. All of these practical issues have driven recent re- searches to find an alternative not requiring the gradient to find the critical points. Carlini et al. [5] propose to use the decision boundary to find dual points, i.e., points that are both critical and on the decision boundary, and extract the trained parameters (i.e., the weights and biases). In their work, they successfully extracted a four-layer DNN trained on CIFAR-10 using only the hard-label provided by the targeted model. However, their method is limited to fully-connected networks, and, as such, cannot be extended to any architecture using a layer that is not fully connected, e.g., an average pooling layer. Furthermore, while they report"}, {"title": "3.2. Threat model", "content": "3.2. Threat model\nthe presence of special case neurons, they do not provide\nan alternative for their extraction, leading to an increase in\nthe number of queries [5].\nIn this paper, we consider an attacker, aiming for extrac- tion of the weights of a DNN implemented in an embedded system (e.g., an MCU). The purpose of this attack is to achieve functional equivalence between the victim's model and the stolen copy (see Definition 1). We assume that the attacker has a physical access to the device on which the targeted DNN is running. This allows recording of the physical leakages, e.g., EM traces, during the processing of the input data by the DNN. However, contrary to [13], [33], we restrict ourselves to the case where the attacker does not have the possibility to profile the em- bedded device. As a consequence, it is not possible to train a statistical distinguisher on labelized physical traces. From the physical attack point of view, this setting corresponds to a black-box scenario. Additionally, similarly to [17], [28] and [6], we only consider Deep-ReLU networks as a target in this threat model, and we assume that the exact architecture of the tar- geted model is known. However, we do not restrict ourselves to fully-connected layer. Finally, we suppose unrestricted query access to the embedded model, and so, the DNN can be seen as an oracle by the attacker. However, in this threat model, we examine the case where the output of the targeted DNN is restricted to the hard-label and not a vector composed of the class-probabilities or the logits."}, {"title": "3.3. Fidelity-based extraction assisted by side- channel attacks", "content": "3.3. Fidelity-based extraction assisted by side- channel attacks In this work, we introduce a new attack framework against DNNs, using side-channel information to assist cryptanalytic extraction. It is an iterative process over all neurons of all layers. Our framework begins by targeting the input layer before moving to the first hidden layer, and so on. The extraction of each neuron follows the following three stages, as represented in Figure 2: 1) During the first step of our framework, we use side- channel information to infer the targeted neuron's state for each input, and binary search to find the critical points. Instead of the gradient, this method exploits side-channel information through the use of a black- box statistical distinguisher (see Definition 6) to esti- mate the state of the neuron. Therefore, this stage is free from any of the restrictions mentioned in Section 3.1. This stage will be detailed in Section 4. 2) The second stage consists in retrieving the optimal equation describing the hyperplane induced by the targeted neuron. To do that, a system of equations is constructed based on the critical points extracted in the previous stage. Then, we use the least square algorithm (LSTSQ) on this system of equations to find the optimal solution, i.e., the hyperplane's equation. Further explanations on this part will be provided in Section 5. 3) In the final stage of our framework, the sign of each neuron is extracted through hypothesis selection, simi- larly to [6]. However, we propose to take leverage the physical traces collected during Stage 1. This reduces the number of hypothesis from $2^m$, with m the number"}, {"title": "4. Stage 1: Search of Critical Points", "content": "4. Stage 1: Search of Critical Points\nof neurons in the layer, to a single hypothesis for\nthe whole layer. This method will be presented in\nSection 6.\nWithout any access to the confidence scores, the estima- tion of the gradient is not possible, and the methodologies to search for the critical points proposed in [6], [17], [28] are no longer applicable. Furthermore, the search for the dual points as introduced in [5], while effective in a hard-label settings, is only possible on fully-connected DNNs. This highlights the need for another method for the critical point search in hard-label settings for non-fully connected DNNs. In this section, we propose a new method, corresponding to the first stage of our framework (see Figure 2), to address these limitations. First of all, in Section 4.1, we present how we exploit physical leakages to extract the targeted neuron's state without access to the confidence scores. Then, in Section 4.2, we describe the algorithm that we use to find the critical points associated with the neuron based on the previously determined neuron's state.\n4.1. Neuron's state identification using side-channel information\nIn this study, we only consider Deep-ReLU networks, meaning that we restrict ourselves to DNNs where the activation function is always the ReLU function \u03c3(x) = max(0,x). This is a common assumption, as the ReLU function is one of the most popular choice for activation function in DNNs. The key of the first step of our framework is based on constructing a statistical distinguisher mapping a side-channel trace to one of the two possible states of a neuron. The purpose of such distinguisher is to solve a classification problem and discriminates active from inac- tive neurons, based on physical traces characterizing their process. Then, once this separation is constructed, it is possible to infer the state of a given neuron for one input. Consequently, for two different inputs, using the statistical distinguisher allows the attacker to determine if they corre- spond to the same state for a given neuron, i.e., if they are on the same side of the induced-hyperplane. It is important to notice that such distinguisher does not need to provide a mapping that returns the true state, i.e., active or inactive. As we are only interested in the similarity in the neuron's state, only the separation learned by the statistical distinguisher is required. The activation function of DNNs has already been tar- geted using a statistical distinguisher exploiting a difference of timing in [2], [24], leading to the identification of the used activation function and even the weights' mantissa. To prevent this issue, Maji et al. [24] proposed a constant-time implementation of the ReLU function based on a mask de- rived from the sign of the processed value. It is nevertheless worth mentioning, that non-constant-time implementations can still be found in popular open-source frameworks such as NNOM [21]."}, {"title": "4.2. Binary search of critical points", "content": "4.2. Binary search of critical points\nWhile effective, both previously mentioned approaches used supervised learning to find the best statistical distin- guisher. This strategy requires access to an open device which is a very restrictive assumption. In this work, we con- sider a more realistic threat model in which an open device is not accessible to the attacker. Therefore, the supervised learning strategy is impossible. To achieve this, we propose another kind of statistical distinguisher based on an unsupervised k-means clustering algorithm [1]. The idea is to first generate enough traces with random inputs to build a dataset in which both classes (i.e, active and inactive neuron) are evenly represented. Then, we use the k-means algorithm to identify two clusters cor- responding to each of the states. As mentioned previously, it does not matter to know which cluster corresponds to the active or inactive state, since that information is not needed to find a critical point. As the inputs are randomly generated, these two clusters are then used as reference to identify the state of a neuron. Finally, given the physical trace of a new neuron, we use its Euclidean distance to the two clusters' centroids as our statistical distinguisher function to infer the neuron's state. Using physical leakages, we are able to infer the state of a neuron through clustering. Even without the gradient, this approach, combined with binary search, is successful in finding the critical points. Indeed, starting from two random points, providing that they correspond to the two different states for the targeted neuron, an attacker can search for critical points by dichotomy, as represented in Figure 2. A high-level description of our methodology is presented in Algorithm 1. Due to numerical approximations, finding a point on the hyperplane can be challenging. Instead, we use an hyper- parameter A to place a critical point within a determined distance to the hyperplane. We use the k-means statistical distinguisher to infer the state of the neuron at each step of our binary search, and determine which point to update to get closer to the hyperplane. Since we are also using binary search to retrieve the critical points, the complexity of this phase is the same as the one in [17] and will require |log2(\u25b3)| queries to place a point within a distance of A to the hyperplane. It is important to notice that the vulnerability that leaks the neuron's state comes from the DNN's implementation itself and is therefore independent from the output provided by the targeted DNN, contrary to classical cryptanalytic methods [6], [17]. Another important limitation of the clas- sical cryptanalytic methods is their inability to determine to which neuron a critical point corresponds. To circumvent this issue, the solution adopted in [6] consists in sampling a large number of critical points, i.e., the number of total collected critical points $n_c$ is established via the coupon collector argument $n_c \\gg Nlog(N)$ with N the number of total neurons in the network, to ensure that there is enough critical points for each neuron to extract the signature. This"}, {"title": "5. Stage 2: Signature Extraction", "content": "5. Stage 2: Signature Extraction\nissue is not present in our contribution, as we use a statistical distinguisher to extract the targeted neuron's state. We are therefore able to localize to which neuron corresponds the identified critical point. Once all the desired critical points are found, an attacker can use them to retrieve the neuron's signature. In this section, we describe how we use the collected critical points to infer the targeted neuron's signature. This corresponds to Stage 2 of our framework depicted in Fig- ure 2. Section 5.1 introduces the general method. Then, in Section 5.2, we propose the first solution to deal with special neurons introduced in [4], namely always-on and always-off, and for a new type of special neurons, input-off neurons.\n5.1. Generic method\nThe idea is to infer the equation of the neuron-induced hyperplane (see Definition 4) by constructing a linear system based on Equation 1 with the critical points from Stage 1. Such system can then be used to extract the weigths. For example, given a set of n critical points {$X_1, X_2, ..., X^n$} s.t. $X^i \\in R^n$, the following system of equations can be expressed in order to extract then weights of the targeted"}, {"title": "5.2. Special cases of neurons", "content": "5.2. Special cases of neurons\nneuron \u03b7:\n$\\sum_{i=1}^{n} \\Theta_{\\eta,i}X_i + \\beta_{\\eta} = X_i$\n(3)\n$\\sum_{i=1}^{n} \\Theta_{\\eta,i}X_i + \\beta_{\\eta} = X_0$\nDuring the second stage of our framework, we find the equation of the hyperplane by solving the least square problem of this system of equations. We then extract the signature of the targeted neuron by arbitrarily fixing the scaling factor $\\Theta_{\\eta,0}$ to \u00b11. The sign of the scaling factor determines the state of the neuron, and is therefore, a critical component to extract. While the critical point search does not require to label each cluster, we need to assign it to conduct the full extraction attack. To do so, the sign must be retrieved. This step will be detailed in Section 6. It is important to notice that the proposed method is not impacted by the presence of linear transformations, e.g., batch normalization, between the matrix multiplication and the activation function. Indeed, it is possible to fuse all these linear operations into one, and then extract an aggregate of the weights (see Figure 3). This framework splits the neuron's signature extraction process at each activation and is therefore the first frame- work not impacted by non-fully connected layers. However, one issue mentionned in SOTA's frameworks which still impact our methodology, is the presence of special cases of neurons. From the previous section it can be observed that the whole signature's extraction process is based on a system of equations constructed with the critical points of the targeted neuron \u03b7 as represented in Figure 2. Some conditions must thus be respected. First of all, the number of critical points, i.e., the number of equations, needs to be at least equal to the number of targeted parameters (weights, bias, etc.) in the neuron. And secondly, each targeted parameter must be expressed through a non-null component value in at least one critical point. In other words, the matrix describing the system of equations must be of full rank. Unfortunately, there exist neurons for which it is difficult to meet these conditions, and which therefore require a very large number of queries to find a subset of critical points"}, {"title": "6. Stage 3: Sign Extraction", "content": "6. Stage 3: Sign Extraction\nsatisfying them. Such neurons can be broadly classified in\ntwo types:\n\u2022\n\u2022\nNeurons that almost never change states, except on\na very small subset of the input space X, making it\ndifficult to find critical points via random queries. Such\nneurons were already described in [4", "4": "but cause issues for\nthe signature extraction. Indeed, the number of queries\nneeded to form a full-rank matrix for these neurons\ncan exceed the average number of queries needed for\nclassical neurons by a factor of 10. We designate such\nneurons as input-off.\nFor these types of special neurons, the signature extrac- tion differs and a special process depending on the type of special neurons is performed. It is important to notice that, due to the information we extract from physical leakages, we are able to differentiate always-off from always-on neurons. This distinction is closely related to the sign extraction of the scaling factor using side-channel analysis (see Section 6).\nExtraction of always-off neurons. The most simple case of specific neuron is the always-off neuron. Since its output value is almost always null and therefore independent from the input, we can directly assign its output value to 0 without having to extract the related weights.\\"}]}