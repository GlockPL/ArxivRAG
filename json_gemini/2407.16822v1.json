{"title": "Al-Enhanced 7-Point Checklist for Melanoma Detection Using Clinical Knowledge Graphs and Data-Driven Quantification", "authors": ["Yuheng Wang", "Tianze Yu", "Jiayue Cai", "Sunil Kalia", "Harvey Lui", "Z. Jane Wang", "Tim K. Lee"], "abstract": "The 7-point checklist (7PCL) is widely used in dermoscopy to identify malignant melanoma lesions needing urgent medical attention. It assigns point values to seven attributes: major attributes are worth two points each, and minor ones are worth one point each. A total score of three or higher prompts further evaluation, often including a biopsy. However, a significant limitation of current methods is the uniform weighting of attributes, which leads to imprecision and neglects their interconnections. Previous deep learning studies have treated the prediction of each attribute with the same importance as predicting melanoma, which fails to recognize the clinical significance of the attributes for melanoma. To address these limitations, we introduce a novel diagnostic method that integrates two innovative elements: a Clinical Knowledge-Based Topological Graph (CKTG) and a Gradient Diagnostic Strategy with Data-Driven Weighting Standards (GD-DDW). The CKTG integrates 7PCL attributes with diagnostic information, revealing both internal and external associations. By employing adaptive receptive domains and weighted edges, we establish connections among melanoma's relevant features. Concurrently, GD-DDW emulates dermatologists' diagnostic processes, who first observe the visual characteristics associated with melanoma and then make predictions. Our model uses two imaging modalities for the same lesion, ensuring comprehensive feature acquisition. Our method shows outstanding performance in predicting malignant melanoma and its features, achieving an average AUC value of 85%. This was validated on the EDRA dataset, the largest publicly available dataset for the 7-point checklist algorithm. Specifically, the integrated weighting system can provide clinicians with valuable data-driven benchmarks for their evaluations.", "sections": [{"title": "I. INTRODUCTION", "content": "Skin cancer is a widespread malignancy worldwide, with melanoma being the most dangerous type. Although the incidence of skin cancer varies by region and individual risk factors, timely diagnosis and prompt intervention can significantly improve the prognosis of the disease [1].\nIn the medical field, healthcare professionals utilize diverse imaging modalities to expedite the diagnosis and treatment of skin lesions [2]. Two notable techniques are dermoscopic imaging and clinical photography. Dermoscopy, which is an optical magnification technique assisted by liquid immersion and cross-polarized illumination, produces intricate images revealing subepidermal structures and lesions to support the diagnosis [3]. On the other hand, clinical images efficiently capture essential morphological features including color, texture, shape, and borders, despite their inability to explore intricate subcutaneous lesion details. The convenience and accessibility of clinical images, especially through widely available mobile electronic devices like smartphones, are indisputable benefits [4]. The fusion of these two imaging techniques provides a comprehensive information essential in practical medical scenarios, specifically for melanoma diagnosis.\nConventionally, clinical practice relies on pattern analysis, which recognizes observed dermatologic attributes to aid in skin lesion diagnosis. Several established algorithms, such as the ABCD rule [5] and the 7-point checklist (7PCL) [6], aid dermatologists in recognizing cancer-related characteristics, contributing to a simpler diagnostic process. For example, this study focuses on the 7PCL algorithm, which evaluates seven dermoscopic characteristics associated with malignant melanoma. These include three major attributes: atypical pigment network (ATP-PN), blue-whitish veil (BWV), and irregular vascular structures (IR-VS), as well as four minor attributes: irregular pigmentation (IR-PIG), irregular streaks (IR-STR), irregular dots and globus (IR-DaG), and regression structures (RS). Each major attribute receives a weighted score of two points, while minor attributes earn one point each. Cumulative scores establish an initial evaluation and necessitate a"}, {"title": "A. Related Work", "content": "Computer-aided diagnosis (CAD) has rapidly emerged as an effective tool to assist dermatologists in the detection of melanoma, as it can extract characteristic information related to the lesion. The conventional CAD procedure for melanoma diagnosis based on machine learning involves multiple steps. These include image preprocessing (e.g., artifact removal and color correction), lesion segmentation, extraction of hand-crafted feature sets, and classification using dedicated classifiers based on the application context [7\u201311]. Initially, Celebi et al. [8] introduced a method that automatically detects borders, extracts shape, color, and texture features from relevant regions, selects optimized features, and addresses class imbalance specifically in dermoscopy images of pigmented skin lesions. Based on this work, researchers have proposed ways to take advantage of the variety of hand-crafted features of melanoma. Schaefer et al. [9] proposed an ensemble method that combines multiple classifiers and various hand-crafted features and demonstrated statistically improved recognition performance. Fabbrocini et al. [10] proposed a framework that integrates hand-crafted features and the 7-point checklist. The authors employed both machine learning classifiers and statistical analysis to enhance the accuracy of diagnosis. Wadhawan et al. [11] devised a comparable notion and modified it for intelligent, portable devices aimed at routine clinical skin monitoring or as an aid for dermatologists and primary care physicians in identifying melanoma. These efforts have facilitated collaboration between machine learning methods and clinical diagnosis of melanoma. However, their performance has been limited by an over-reliance on preprocessing and complicated feature extraction procedures.\nOver the past decade, deep learning has become a widely used and powerful tool for feature learning and pattern recognition in CAD, enabling automated detection of melanoma and other skin diseases in various scenarios using different methods [12-17]. Esteva et al. [12] demonstrated the classification of skin lesions using a single convolutional neural network trained end-to-end directly from both clinical and dermoscopy images, using only pixels and disease labels as input, and achieving performance comparable to dermatologists. Then, Abhishek et al. [16] presented the first work to directly explore image-based predicting clinical management decisions of melanoma without explicitly predicting diagnosis. Meanwhile, the International Skin Imaging Collaboration (ISIC) challenges and the corresponding constantly updated and expanded public datasets have greatly contributed to the rapid development of the field, and more excellent methods and training strategies have been proposed [18]. However, the above-mentioned research works focus more on the pixel information brought by the image itself, especially in the imaging modality of dermoscopy. Incorporating expert domain knowledge and multimodal data are important directions that deserve further attention.\nUsing information from clinical diagnostics to support multi-modal deep learning models has proven effective in several related research. Moura et al. [19] combined the ABCD rules and a pre-trained CNNs through a multi-layer perceptron classifier for melanoma detection and achieved improved accuracy. Kawahara et al. [6] proposed a multitask Convolutional Neural Network (CNN) trained on both dermoscopy and clinical images, along with the corresponding 7PCL attributes for melanoma detection, which has been regarded as the benchmark of the related research studies. Following this work, Bi et al. [20] proposed a hyper-connected CNN (HCCNN) structure for the multi-modal analysis of melanoma and the corresponding clinical features. Tang et al. [21] further developed this idea and fused 7PCL attributes information into a deep learning framework in a multi-stage manner, improving the average diagnostic accuracy. However, none of these studies have fully considered the intrinsic connections between melanoma attributes when analyzing the clinical data. The meta-information is processed consistently as feature vectors in combination with image features, resulting in some improvement in the outcomes. Nevertheless, the potential value of this clinical information warrants further investigation. To address these issues, Graph Convolutional Neural Networks (GCNs) are introduced to assist with data mining for works involving supplementary clinical information. Wu et al. [22] introduced GCNs based on general clinical meta-information to aid in the multi-label classification of skin diseases. Wang et al. [23] designed a framework utilizing constrained classifier chains, and first examined the mutual relationships between attributes of 7PCL. Fu et al. [24] applied GCNs to a 7PCL-based multimodal and multilabeled classification task, and uncovered the interconnections between attributes by analyzing simultaneous occurrences of different attributes and undirected GCNs. These studies demonstrate the potential of combining graph learning with clinical diagnosis to aid dermatologists in the pre-diagnosis of melanoma. However, a gap persists in understanding the directional relationships among diverse attributes and their practical implications for melanoma diagnosis. This is crucial for fully realizing the benefits of rule-based clinical algorithms like 7PCL."}, {"title": "B. Contributions", "content": "Previous studies regarded the 7PCL-based melanoma diagnostic task as a multi-label classification, treating skin disease labels and corresponding feature labels equally, which overlooks the clinical process of first examining features and then diagnosing based on correlation analysis. In this study, we propose two observations based on clinical experience and previous research. The first observation proposes that various characteristic attributes of melanoma have directed relationships with each other and influence each other to varying degrees. For"}, {"title": "II. METHODOLOGY", "content": "As illustrated in Fig. 3, the proposed method comprises three modules. The first module, Clinical-Dermoscopic Multi-modal Fusion (CD-MFM), combines information from clinical and dermoscopic modalities. The second module, 7-Point Checklist Directed Graph Mining (7PCL-DirGM), extracts representative features using a mining strategy that leverages directed and multi-order graph information from multi-labeled data. These two modules form our Clinical Knowledge-Based Topological Graph Convolutional Network (CKTG). Additionally, the GD-DDW employs data-driven weighted gradient diagnosis to enhance diagnostic accuracy by emulating a dermatologist's diagnostic behavior. The following subsections provide a detailed overview of the methodology.\nIn this study, we treated each skin lesion as an individual \u201ccase\u201d within the dataset. Each case, denoted as $x^i$, encompasses data from multiple modalities, including dermoscopy images $x_d^i$, clinical images $x_c^i$, and the encoded information related to the jth attribute within the 7PCL, $X_{7p_j}^i$ , where i ranges from 1 to n (the total number of cases) and j ranges from 1 to 7. Additionally, each case is associated with a diagnostic tag, denoted as $y_m^i$, which represents the melanoma diagnosis, and labels $y_{7p_j}^i$ for their morphologic attributes.\nTo commence, a deep convolutional neural network was applied to extract visual features independently from dermoscopic and clinical images. Utilizing ResNet-50 as the backbone model yielded feature vectors $X_c$ and $X_d$. Following this, multimodal feature fusion was conducted by combining feature channels extracted from dermoscopic and clinical images through weighted averaging, resulting in an additional channel denoted as $X_f$.\n$X_f = \\delta X_c + (1 - \\delta) X_d$   (1)\nLet Z represent the features extracted from the graph information. For the next fusion step, we individually combined the graph features (Z) with the deep features extracted from dermoscopic ($X_d$), clinical ($X_c$), and the additional channel ($X_f$) images. This was achieved by element-wise multiplication:"}, {"title": "A. Clinical-Dermoscopic Multimodal Fusion (CD-MFM)", "content": "$X_{fused_d}=X_d \\bigoplus Z$\n$X_{fused_c} = X_c \\bigoplus Z$\n$X_{fused_f}=X_f \\bigoplus Z$  (2)\nwhere $X_{fused_d}$, $X_{fused_c}$, and $X_{fused_f}$ represent the fused features for the dermoscopic, clinical, and additional channel images, respectively.\nSubsequently, we apply a fully connected layer to each channel independently and conduct a weighted average fusion of the features:\n$X = \\gamma_d X_{fused_d} + \\gamma_c X_{fused_c} + \\gamma_f X_{fused_f}$  (3)\nwhere $\u03b3_d, \u03b3_c$, and $\u03b3_f$ represent the weights for combining the fully connected layers of the dermoscopic, clinical, and additional channel images, respectively.\nThe resulting fused feature X with dimensions $R^{n\u00d7d}$ was used for downstream tasks, where n denotes the number of cases and d represents the feature dimensionality, leveraging the combined information from the 7PCL graph and the image modalities."}, {"title": "B. 7-Point Checklist Directed Graph Mining (7PCL-DirGM)", "content": "Within the 7PCL dataset, there exist both directional and causal relationships among the various attributes, each associated with different stages of melanoma development and their causes. For instance, both irregular pigment network and atypical pigmentation are important features to consider when evaluating potential melanomas. However, irregular pigment network is often considered a more specific and reliable indicator of melanoma when compared to atypical pigmentation alone. This is because the disruption of the normal pigment network structure is a hallmark feature of melanomas and is less commonly seen in benign moles. Therefore, the presence of an irregular pigment network raises the suspicion for melanoma and may prompt further evaluation or biopsy [25\u201327]. Recognizing these distinctions aids in filtering out superfluous and inaccurate interactions, enhancing the efficiency of graph network data utilization. Moreover, contrary to previous studies equating disease diagnosis labels with 7PCL features [6, 20, 21, 24], we emphasize that the information pertaining to disease diagnosis, the various attributes, and interactions among these attributes represent significant directed information that should be considered separately with appropriate weighting for effective integration. To address these complexities, our study proposes the 7PCL-DirGM, a directional graph mining method tailored to extract vital attribute interaction data related to melanoma."}, {"title": "1) Graph Node Feature Encoding", "content": "As a critical input to graph convolutional learning networks, the encoding of node information is often overshadowed by the emphasis on topological relationships between nodes. Previous research has frequently relied on one-hot encoding"}, {"title": "2) Internal & External Directional Weighted Graph (IEDWG)", "content": "One significant deviation from related studies is the importance we place on the directed connection between attributes in 7PCL for constructing directed graph networks. As introduced in Algorithm 1 and Fig. 4, this connection is reflected primarily in two levels: the internal level within the attributes, which is determined by the mutual connections within the seven attributes in the checklist. The external level involves the coexistence probability of each attribute and the melanoma. Both internal and external information are used to build the weighted directional edges by using the conditional"}, {"title": "3) Adaptive Receptive Field Proximity (ARFP)", "content": "When working with a weighted directed graph, the challenge initially lies in the absence of directed information along long paths, which results in insufficient global information and unbalanced receptive fields. Inspired by the digraph inception convolutional networks [29], we utilized the adjustable parameter k to generate $k^{th}$-order proximity based on the information of both the meeting path and diffusion path. This approach enables the simultaneous extraction of direct and indirect connectivity information to the target and neighboring nodes, in contrast to relying solely on first- and second-order proximity. Given a graph G = (V,E), for k \u2265 2, there exists a node $v_e \u2208V$, the meeting path and diffusion path are defined as:\n$\\begin{aligned} A_{p, q}^{(k)} &= \\{U_{p \\rightarrow v_e \\leftarrow q} \\mid \\text { k-1 edges } \\}\\\\ D_{p, q}^{(k)} &= \\{U_{p \\leftarrow v_e \\rightarrow q}. \\mid \\text { k-1 edges } \\} \\end{aligned}$  (4)\nwhere $M_{p, q}^{(k)}$ and $D_{p, q}^{(k)}$ represent the meeting path and diffusion path between nodes p and q(p,q \u2208 V). If both $M_{p, q}^{(k)}$ and $D_{p, q}^{(k)}$ exist between nodes p and q, then we consider them to be at the $k^{th}$-order proximity level, with $v_e$ being their $k^{th}$ - order common neighbor. It is worth noting that a node has 0th-order proximity with itself and 1-order proximity with its directly connected neighbors.\nUsing the 7PCL directed weighted graph and the $k^{th}$-order proximity, the 7PCL $k^{th}$-order proximity matrix founded on"}, {"title": "C. Gradient Diagnostics with Data-Driven Weighting (GD-DDW)", "content": "As previously discussed, the deep features extracted from fused imaging modalities and the directed mutual connection matrix are collectively referred to as X for subsequent diagnostic procedures. In this section, we present the GD-DDW method, designed to emulate the diagnostic approach employed by dermatologists using the 7-point checklist algorithm in clinical practice. Specifically, the GD-DDW method utilizes representative features corresponding to various attributes to make initial predictions, which are then used to diagnose melanoma.\nIn this module, we first establish a parallel multi-label classifier designed to address the classification of seven attributes on the checklist. As X is the input, $Y_{7p_j}$ =$ [y_{7p_1} ,y_{7p_2} ,\u2026\u2026\u2026,y_{7p_7}] \u2208 {0,1}$ is denoted as the ground truth of all seven attributes in the format of a binary indicator vector. In the proposed method, the focal loss function [32] was employed with the objective of enhancing the performance on the dataset that exhibiting a serious imbalance.\nThe focal loss function for each label can be defined as:\n$L_{7p_j} = -\\mu_j (1 - y_{7p_j}) log(\\hat{y}_{7p_j})$  (7)\nwhere $\u0177_{7p_j}$ represents the predicted probability of the positive class, \u03bcj is a balancing parameter, and is the focusing parameter. The total focal loss $L_{7p}$ for all seven attributes can be formulated as:\n$L_{7p} = \\sum_{j=1}^{7} L_{7p_j}$  (8)\nNext, the prediction scores obtained for each label are utilized as inputs for the subsequent step. In this stage, a weighted sum function and a sigmoid activation function are applied to perform classification for the diagnosis of melanoma.\nLet $w = [w_1, w_2, ..., w_j]$ represent the learned weights module obtained from the attributes. In this module, the predicted probability of melanoma $\u0177_m$ is computed as:\n$\u0177_m = \\sigma(\\frac{1}{\\sum_{j=1}^7 w_j}(\\sum_{j=1}^{7} w_jy_{7p_j}))$  (9)\nwhere \u03c3(.) denotes the sigmoid activation function and the rescaling factor $\\frac{1}{\\sum_{j=1}^7 w_j}$ ensures that the output is appropriately scaled to the range [0,1].\nThe overall loss L is then computed as the sum of $L_{7p}$ and $L_{mel}$:\n$L = L_{7p} + \\lambda L_{mel}$  (10)\nwhere \u03bb is the weighting parameter that adjusting the contributions of the two losses, specifically, making sure the loss values are in the same order of magnitude."}, {"title": "III. EXPERIMENTS", "content": "In this study, we utilized the EDRA public dataset as the major material, purposefully curated for 7PCL studies and annotated by Kawahara et al [6]. This dataset comprises paired dermoscopic and clinical images sourced from 1011 patients, each image has a maximum resolution of 768 by 512 pixels. Notably, nine patients lacked clinical images, which were substituted with corresponding dermoscopic ones. Throughout our study, we strictly adhered to the dataset's"}, {"title": "B. Implementation Details", "content": "In our study, we ensured the integrity of our results by employing consistent data processing methods and adhering to uniform model training protocols across various ablation comparison experiments. Our chosen deep learning model framework is ResNet-50, leveraging parameters pretrained on ImageNet, a common practice in other related works. The entire architecture is meticulously fine-tuned end-to-end, utilizing the Adam optimizer [33] with a concurrent learning rate set to 0.00001. We predefined the number of epochs for pretraining to 150, with an early-stopping mechanism in place. If the model's performance on the validation set fails to improve over 50 consecutive epochs, the training process halts, and the best-performing model is saved. The loss functions utilized are based on focal loss to circumvent the result bias"}, {"title": "IV. RESULTS", "content": "Table I presented a comprehensive comparison of the classification performance between the proposed method and several state-of-the-art methods on the EDRA dataset. It demonstrated our method surpassed other SOTA methods with an average AUC of 85.0%. Notably, five out of the eight labels, specifically including melanoma, achieved the highest diagnostic results. Moreover, it was noteworthy that GRM-DC, which was also based on graph convolutional networks, was observed to be the methodology achieving the optimal AUC for the remaining two of three labels. This finding reinforced the notion that the interactions between attributes in 7PCL could not only aid in detecting melanoma but also offer insights for detecting each attribute.\nAs for the other three metrics, our method also demonstrated strong performance in precision (67.6%), particularly excelling in Mel (77.7%). The sensitivity of our method is 55.4%, particularly for VS, which represents an improvement of over 13% compared to other methods. As VS represents the most unbalanced attribute in this dataset (with only 71 positive subjects out of 1011), the resulting enhancement of this attribute highlights the significance of the interactions between the attributes learned by our approach. Additionally, it achieved the highest specificity (91.5%), excelling in Mel (95.5%) and BWV (94.3%)."}, {"title": "B. Ablation Studies", "content": "Table II presented the results of the ablation experiments conducted on the various modules. The first comparison was between unimodality and multimodality. Consistent with previous related studies, the results showed that dermoscopy and clinical imaging fusion generally outperformed unimodality. Additionally, dermoscopy performed better than clinical images with 3% improvement, which was reasonable given that dermoscopy naturally provided more detailed features, especially in the context of 7PCL.\nRegarding the impact of different modules on the results, both IEDWG and ARFD contributed to an average AUC improvement of 8%. Particularly, ARFD showed the highest improvement in melanoma's results, increasing from 77.8% to 86.5%. This underscores the effectiveness of multi-order relationships between attributes in providing diagnostic information. Additionally, the gradient diagnostic structure, designed to mimic clinical diagnostic approaches, enhanced the average AUC by 5%. The integration of these three modules collectively resulted in a notable enhancement in model efficacy, as evidenced by the average AUC reaching 84.1%."}, {"title": "C. Comparison to Traditional 7PCL Algorithm", "content": "As shown in Table III, the traditional algorithm employs approximate categorization based on multivariate analysis [34], assigning weights of 2 or 1 to major and minor attributes, respectively. In contrast, the method proposed in this study provides more precise weights, ranging from 1.3 to 1.5 for major attributes and from 0.9 to 1 for minor attributes, based on predicted scores for each category. The integration of predicted scores and data-driven weighting modules significantly enhances the prediction of melanoma. The predicted AUC improves from 72% to 89%, representing a substantial increase of 17%. Additionally, the ROC curve of the proposed method provides more accurate information regarding paired specificity and sensitivity, assisting dermatologists in more effective selection and analysis. While traditional methods offer initial guidance concisely, our method provides more detailed information, leveraging increasing computational resources. Moreover, the proposed method offers greater flexibility in setting thresholds and can be dynamically adjusted when the dataset changes, facilitating"}, {"title": "D. Generalizability Analysis on the ISIC Datasets", "content": "As illustrated in Table IV, we have predicted and compared the attributes present in the ISIC 2017 and 2018 datasets with the baseline utilized and the currently relevant SOTA, respectively. It is notable that our average AUC is the highest on both datasets, and specifically for each attribute, our proposed method attains the highest results in six out of nine attributes. This demonstrates that the proposed method is capable of effectively extracting the interactions between attributes, thereby enhancing the model's predictive capability."}, {"title": "V. DISCUSSION", "content": "In this study, a novel method to melanoma diagnosis was proposed, integrating clinical empirical knowledge, diagnostic procedures, and graph convolutional networks. Specifically, an appropriate topological graph structure was established by improving the causal learning of attributes associated with melanoma. Computational logic from clinical algorithms was borrowed to enhance explanatory power and clinical acceptance by improving the strong correlation between results and imaging datasets. Moreover, certain elements identified during this study warranted further discussion.\nThe results demonstrated the significance of the multi-order node associations introduced by ARFP as topological information, enhancing melanoma detection performance. In preliminary experiments, it was observed that the model's performance improved as the order k increased from 1 to 3. However, the results plateaued at order k = 4. Upon closer examination, it was found that as the order k increased, the number of links decreased sharply due to the limited number of nodes (7), resulting in the loss of useful information. Consequently, selecting the optimal order for this work was deemed paramount. Furthermore, the incorporation of additional, more informative labels may potentially enhance the overall outcome."}, {"title": "VI. CONCLUSION", "content": "This study proposes an Al-enhanced 7PCL for melanoma detection using a unique topology enriched with clinical insights, simulating dermatologists' diagnostic behavior. By considering directed multi-order relationships and a gradient prediction structure, our method provides weight measures for each attribute. This helps dermatologists intuitively analyze attributes, enhancing interpretability in clinical scenarios and improving detection performance and acceptance."}]}