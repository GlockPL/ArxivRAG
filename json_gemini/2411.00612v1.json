{"title": "How to Bridge Structural and Temporal Heterogeneity in Link Prediction? A Contrastive Method", "authors": ["Yu Tai", "Xinglong Wu", "Hongwei Yang", "Hui He", "Duanjing Chen", "Yuanming Shao", "Weizhe Zhang"], "abstract": "Temporal Heterogeneous Networks play a crucial role in capturing the dynamics and heterogeneity inherent in various real-world complex systems, rendering them a noteworthy research avenue for link prediction. However, existing methods fail to capture the fine-grained differential distribution patterns and temporal dynamic characteristics, which we refer to as spatial heterogeneity and temporal heterogeneity. To overcome such limitations, we propose a novel Contrastive Learning-based Link Prediction model, CLP, which employs a multi-view hierarchical self-supervised architecture to encode spatial and temporal heterogeneity. Specifically, aiming at spatial heterogeneity, we develop a structural feature modeling layer to capture the fine-grained topological distribution patterns from node- and edge-level representations, respectively. Furthermore, aiming at temporal heterogeneity, we devise a temporal information modeling layer to perceive the evolutionary dependencies of dynamic graph topologies from time-level representations. Finally, we encode the structural and temporal distribution heterogeneity from a contrastive learning perspective, enabling a comprehensive self-supervised hierarchical relation modeling for the link prediction task. Extensive experiments conducted on four real-world dynamic heterogeneous network datasets verify that our CLP consistently outperforms the state-of-the-art models, demonstrating an average improvement of 10.10%, 13.44% in terms of AUC and AP, respectively.", "sections": [{"title": "1 INTRODUCTION", "content": "Contemporary information networks such as social networks [33] and biological systems [3] are becoming increasingly complex. These networks often comprise multi-typed nodes and connections, undergo continuous temporal evolution, making the link prediction in such complex networks a long-standing challenge. Specifically, the link prediction task aims to predict the likelihood of future connections between arbitrary nodes [34, 39, 47], which captures the evolution of heterogeneous networks and stores the temporal details of the node embeddings, simulating intricate and expressive semantics for real-world systems, including Social Recommendations [41], Traffic Management [17, 31], Medical Health [38] and Network Biology [11]. Aiming at modeling the dynamics and complex relationships between entities, link prediction models are primarily designed to portray the topological relationship between heterogeneous snapshots and the evolving progress along chronological order, revealing the distribution patterns within complex Temporal Heterogeneous Networks (THNs).\nThe primary challenges in link prediction tasks revolve around heterogeneous entity relationship modeling and dynamic snapshot variability modeling. Current link prediction methods in literature typically segment dynamic snapshot sequences chronologically and address the complex entity relationships existing in each snapshot. We term such challenges as spatial complexity and temporal complexity. (1) Spatial complexity [36, 52] highlights the complex heterogeneous static relationships between multi-typed entities in complex networks, primarily modeling the diverse co-occurrence paradigm through heterogeneous network embedding approaches. Specifically, Meta-Path-based approaches [6, 9, 29, 57] construct meta-paths within individual snapshots to excavate the heterogeneous information. On the other hand, Attribute-based methods [14, 21, 55] focus on incorporating multiple rich attributes [27, 43] and merging neighbor attributes [14, 21] to enhance the node embedding process. (2) Temporal complexity [8, 18, 25, 28] mainly exploits the dynamic distribution changes in snapshot sequences. Temporal approaches focus on tracking the continuous evolution across chronological snapshots, primarily classified into sequential and graph methods. Sequential methods [12, 16] learn from time-ordered snapshot sequences, capturing evolutionary dependencies between different snapshots based on Recurrent Neural Networks (RNNs) [4] and attention mechanism [49]. Graph methods [19, 42, 45, 46] aggregate embeddings of dynamic nodes, encoding the appearing or disappearing network features continuously over time through Graph Neural Networks (GNNs) [10, 56].\nDespite the effectiveness, a prominent drawback of these methods is that they model dynamic heterogeneous representations in a coarse-grained manner and only focus on the representation paradigm, but ignore the universally-distributed differential relations in THNs, thus resulting in the suboptimal performance in link prediction tasks. We reckon that by leveraging such differential relations and bridging temporal and spatial heterogeneity, it becomes feasible to portray the comprehensive and detailed dynamic and diversified characteristics, thereby enhancing link prediction performance. Specifically, focusing on the aforementioned temporal and spatial complexity, the fine-grained differential relations between different nodes and edges (spatial) and the evolution paradigm distinctions (temporal) play a crucial role in representation learning and significantly influence link prediction performance."}, {"title": "2 PRELIMINARIES", "content": "We illustrate such fine-grained differential distributions at the node-, edge-, and time-level in Figure 1. From the node-level propagation, taking the static THN snapshot G\u00b9 as an example, different propagation and aggregation paradigms convey differential information. If we take the quadrangle (\u25fc) as the ego node, from the perspective of connection types, the propagation priority sequence for \u25fc reveals: \u25fc=\u25cf=\u25b2=\u25a0. Meanwhile, from the perspective of single-type neighbor numbers, the priority manifests: \u25fc>= \u25b2, reflecting significant distribution heterogeneity among different propagation patterns. Analogously, targeting edge-level propagation, different edge types form the holistic propagation paradigm, yet convey unique propagation information. However, the heterogeneity between different edge-level propagation lacks sufficient attention. Finally, from time-level propagation, significant variations occur due to diverse inspection degrees (e.g., number of nodes, variations of edges), highlighting the transformative information conveyed by different time-level propagation patterns. We term such differential distribution in THNs as 'spatial heterogeneity' and 'temporal heterogeneity', which are crucial factors for link prediction modeling but have been rarely addressed in related research.\nAs is illustrated in the above example, different entities in complex networks possess variously-grained distribution differentiation, existing among nodes, edges and varying along chronological order. However, conventional methods ignore the modeling of such heterogeneity differentiation, which introduces the first challenge in this work, i.e., CH1: How to capture the heterogeneity differentiation existing in link relation networks? To represent such distribution heterogeneity in link prediction, it is essential to characterize the fine-grained intrinsic topological distribution in the link graph. Accordingly, we resort to Self-supervised Learning [5] to investigate the inherent inter-relation in temporal heterogeneous graphs.\nIn addition, to bridge the discrepancy elimination and topological exploration module in link prediction, we need to resolve the second challenge, i.e., CH2: How to integrate different granularity of distribution discrepancy? To address this challenge, we first design a heterogeneous temporal graph to absorb both structural distribution patterns and sequential evolutionary paradigms. Subsequently, we propose a contrastive hierarchical heterogeneity differentiation module to absorb the intrinsic inter-relation from node-, edge-, and time-level, respectively. Leveraging such hierarchical contrastive module, we implement a fine-grained multi-view entity relation extraction functionality.\nTo summarize, our main contributions are as follows:\n\u2022 Targeting CH1, we propose a three-layer hierarchical contrastive entity relation extraction module to enable multi-view discrepancy elimination functionality, thereby bridging the spatial and temporal heterogeneity in link prediction scenarios.\n\u2022 Targeting CH2, we design a heterogeneous temporal graph network to absorb sequential and structural distribution paradigms and comprehensively eliminate discrepancies from various perspectives. Specifically, we depict the structural distribution differentiation paradigms with node- and"}, {"title": "2.1 Problem Formalization", "content": "Definition 1 (Heterogeneous Network (HN)). Let G (V, \u0395, \u0391v, Ae) be an undirected graph, where V = {v1, v2, \u00b7\u00b7\u00b7, \u03c5N }, E = {e1, e2,\u2026\u2026\u2026, eM}, Av, and Ae denote the set of nodes, edges, node types and edge types, respectively. Each node vi \u2208 V and edge ej \u2208 E are associated with their corresponding node type \u03c6(vi) \u2208 A and edge type \u03c6(ej) \u2208 Ae, and |Av| + |Ae > 2.\nDefinition 2 (Temporal Heterogeneous Network (THN)). Let G(V, E, T, X) denote an undirected heterogeneous graph, comprising a sequence of heterogeneous network snapshots at multiple timesteps, i.e., G = {G1, G2, \u2026 \u2026 \u2026 ,GT }, where Gt (Vt, Et) is the network snapshot graph at time step t. Here, Vt = {vi, v2,\u2026\u2026\u2026, \u03c5pt} \u2286 V and Et = {ei, e2,\u2026\u2026\u2026, ept} \u2286 E denote the node set and edge set at moment t, respectively. T represents the total number of snapshots, and V = \u222aT=1 Vt, E = \u222aT=1 Et. For any node a \u2208 V, a fixed-size feature vector xa \u2208 Rd is given for node representation, and X = {xa}a\u2208V denote the feature matrix for all nodes.\nTemporal Heterogeneous Network Link Prediction Formalization. Our model aims to predict the possible link between two target nodes. To address this, we formulate the temporal heterogeneous network link prediction problem as follows: given a sequence of temporal heterogeneous graphs G = {G1, G2, \u2026\u2026\u2026 ,GT} and the target link e = (a, b), where a, b \u2208 V, our objective is to determine the likelihood of e existing in GT+1 at time step T + 1 by assessing the similarity between representations of node a, and b at time T (denoted as uTa and uTb).\nThe key mathematical symbols and definitions relevant to this article are summarized in Table 1."}, {"title": "3 METHODOLOGY", "content": "In this section, we elaborate on the detailed architecture of our CLP to learn and encode the spatial and temporal heterogeneity in link predictions, introducing the hierarchical contrastive relation extraction modules from node-, edge-, and time-level, respectively. The primary objective of our CLP is to learn a deep representation of a THN from various dynamic node and edge types to express the spatial discrepancy and temporal nonuniformity. To this end, we design CLP with a hierarchical architecture to capture the distribution heterogeneity, including (1) Structural Feature Modeling Layer, (2) Temporal Information Modeling Layer, and (3) Output Layer.\n(1) Structural Feature Modeling Layer: First, we propose a two-layer hierarchical Graph Attention Network (GAT) to represent diverse types of edges and nodes within the THN from both node- and edge-level perspectives. Additionally, we introduce a contrastive representation method to differentiate feature heterogeneity at the node and edge levels, enhancing our ability to capture structural heterogeneity.\n(2) Temporal Information Modeling Layer: Then, we deploy LSTM and GRU models to independently analyze temporal snapshot pattern, capturing the long-term and short-term dependencies between snapshots, respectively. Additionally, we implement contrastive learning strategies to bridge differences between these two sequence learning paradigms, thus preserving the temporal heterogeneity.\n(3) Output Layer: Finally, we calculate the similarity between node a and node b to represent the target link e = (a, b). This measurement is then incorporated into a comprehensive loss function to estimate the probability of the existence of the target link.\nThe aforementioned layers of our proposed CLP are shown in Figure 2 with elaborate interpretations provided in the following subsections."}, {"title": "3.1 Structural Feature Modeling Layer", "content": "The Structural Feature Modeling Layer aims to capture the structural distribution patterns and eliminate the discrepancy between nodes and edges in each static snapshot Gt \u2208 G. Specifically, we initially partition the static snapshots G = {G1, G2,\u2026\u2026\u2026, GT } into type-specific sub-networks based on different edge types. Then, we devise a two-layer hierarchical GAT to represent diverse types of nodes and edges within the THN from the perspectives of both node- and edge-levels in the Node-level Feature Learning Module and Edge-level Feature Learning Module, respectively. Additionally, we devise the contrastive representation heterogeneity differentiation modeling in the node- and edge- feature learning module to model the structural heterogeneity."}, {"title": "3.1.1 Node-level Feature Learning Module", "content": "For each graph snapshot Gt \u2208 G, we partition it into several subgraphs based on the edge type r \u2208 R. The attention score \u03b2abrt between node a and b in the r-th type subgraph of the t-th static snapshot Gt is expressed as follows:\n$\\beta_{ab}^{rt} = (Art[W_r x_a || W_r x_b])$,\nwhere xa and xb are used to initialize node a and b; Art and Wrt represent the learnable attention weight vector and mapping matrix, specific to r type subgraphs of Gt; The function \u03c3 (\u00b7) denotes the activation function and || signifies the concatenation operation. The formalized attention weight parameter \u03b1abt, between node a and b in the r type subgraph of Gt is defined as follows:\n$\\alpha_{ab}^{rt} = \\frac{exp(\\beta_{ab}^{rt})}{\\sum_{k \\in N_a^r} exp(\\beta_{ak}^{rt})}$,\nwhere Nart represents the neighbors of node a falling into the r-th type in Gt. The representation urt is obtained for each node by the weighted summation of the neighbor nodes, which attentively propagates and aggregates the node embeddings as follows:\n$u_a^{rt} = \\sigma( \\sum_{b \\in N_a^r} \\alpha_{ab}^{rt} W_r x_b )$.\nThe GAT model highlights the diverse node representations but restricts the expression of a node's intrinsic embedding. Average embedding pooling effectively preserves the unique characteristics of node representations. Consequently, we implement a linear aggregation operation using GNN to represent nodes in the unified graph as follows:\n$h_a^t = W_r^t x_a + \\frac{1}{\\sqrt{|N_a^t|}} \\sum_{b \\in N_a^t} W_r^t x_b$.\nNode-level Heterogeneity Differentiation Modeling. To enhance the representation of each node, we employ a node-level contrastive learning approach to ensure that augmented representations of the same node are similar (intra-similarity) and simultaneously differ from representations of other nodes (inter-dissimilarity). Additionally, we implement a node-level InfoNCE loss function [32]. This function guarantees that representations urt and hat, derived from GAT and GNN for the same node a, are similar. Conversely, representation urt, derived from GAT for a different node b, remains distinct. Within this framework, the pair (urt, hat) constitutes a positive sample pair, while the pair (urt, ubt) functions as a negative sample pair, as specified in Eq. (5) and Eq. (6):\n$\\{L}_{Node}^+ = - \\sum_{t=1}^T \\sum_{r \\in R} \\sum_{a \\in V^t} log \\frac{exp (sim (u_a^{rt}, h_a^t) /\\tau)}{\\sum_{b \\in V^t} exp (sim (u_a^{rt}, u_b^t) /\\tau)}$,\n$\\pounds_{Node}^- = - \\sum_{t=1}^T \\sum_{r \\in R} \\sum_{a \\in V^t} \\sum_{b \\in N_a^t b \\neq a} log \\frac{exp (sim (u_a^{rt}, u_a^{rt}) /\\tau)}{\\sum_{k \\in V^t} exp (sim (u_a^{rt}, u_k^{rt}) /\\tau)}$"}, {"title": "3.1.2 Edge-level Feature Learning Module", "content": "The Node-level Feature Learning module captures information specific to one edge type. However, heterogeneous networks typically comprise multiple edge types. To equip the information from all edge types at each node, we devise the Edge-level Feature Learning module, which determines the importance weights for different edge types. This module aggregates different forms of information for a specific type, thereby generating the node's embedding enriched with heterogeneous edge information. Specifically, each node's embedding vector undergoes a nonlinear mapping. The attention weight \u03b4art between each edge type r and node a in the t-th snapshot graph is obtained through the softmax activation function, as described in Eq.(7) and Eq. (8):\nyat = zT \u00b7 \u03c3 (Wurt + b),\n\u03b4art = exp (yat) / \u03a3r'er exp (ya't),\nwhere z, W\u00b9, and b represent the trainable attention weight vector, weight matrix, and bias vector, respectively. \u03c3 denotes a non-liner activation function. Then, the representation of each node u in Gt is derived by aggregating edge-specific information through the weighted summation:\nhrt = \u03a3r=1R \u03b4art urt.\nAnalogously, the GAT model highlights the diverse representation of various edge types; however, it limits the expression of the edge type itself. Average embedding pooling effectively preserves the intrinsic characteristics of edge type representations. Consequently, a linear aggregation operation is implemented through GNN as follows:\nhTa = 1/|NrTa | \u03a3ua^rt + \u03a31/|NrTa | \u03a3hb.\nwhere Nart denotes the neighbors of node a with r edge type in Gt.\nEdge-level Heterogeneity Differentiation Modeling. To refine the representation of each edge-specific node, we employ an edge-level contrastive learning approach to ensure that augmented representations of identical subgraphs demonstrate intra-similarity, while those of different subgraphs exhibit inter-dissimilarity.\nFurthermore, to confirm that the edge-specific embedding ua for node a is similar to its embedding hat in the unified graph and dissimilar to the aggregated representation u for node b, we define the edge-level InfoNCE loss function considering the heterogeneous and unified aggregated representations of the same node as a positive sample pair (ua, ha), while forming a negative sample pair (ua, u) with heterogeneous aggregated representations from different nodes, as Eq. (11) and Eq. (12):\n$\\{L}_{Edge}^+ = - \\sum_{t=1}^T \\sum_{V^t} log \\frac{exp (sim (u_a, h_a) /\\tau)}{\\sum_{b \\in V^t} exp (sim (u_b, h_a) /\\tau)}$,\n$\\pounds_{Edge}^- = - \\sum_{t=1}^T \\sum_{a=1}^{|V|} \\sum_{b \\in N^t \\bigcup^r N_a^r \\bigcup^t N_b \\neq a} log \\frac{exp (sim (u_a, u_b) /\\tau)}{\\sum_{k \\in V^t} exp (sim (u_a, u_b) /\\tau)}$\nwhere Nart = \u222ar Nrart represents the neighbors of node a in Gt including all edge types. By alienating the neighbors in the unified graph Gt, we not only strengthen contrastive relationships in Eq. (6) for intra-relation neighbors, but also alienate the inter-relation neighbors with different edge types."}, {"title": "3.2 Temporal Information Modeling Layer", "content": "The Temporal Information Modeling Layer aims to address the variability of temporal sequence information across different sequence modeling contexts, where various modeling techniques can capture distinct sequence patterns. Specifically, we employ a dual-channel architecture to learn different sequential dependency paradigms-(1) in the long-term channel, we deploy LSTM to explore the inherent inter-dependencies in long-term temporal evolution and (2) in the short-term channel, we apply GRU to analyze interactions between adjacent snapshots in short-term evolution. However, previous studies have overlooked the heterogeneity between these long-term and short-term dependencies. To this end, we propose a contrastive learning approach to emphasize the differences between diverse sequence learning paradigms, thereby highlighting temporal heterogeneity. We deploy LSTM and GRU to represent the temporal patterns for learning long and short dependencies, expressed as uLSa and uGSa, respectively:\nuLSa \u2190 FLSTM ({ura}Tta=1),\nuGSa \u2190 GRU ({uta}Tta=1),\nwhere {uta} = {uta1,uta2,..., utaT} denotes the node a's embeddings across various snapshots of all time points. Then, the embeddings for node a throughout all snapshots are derived.\nTime-level Heterogeneity Differentiation Modeling. To bridge the nonuniformity of temporal sequences between long- and short-term sequence learning spaces, we propose a time-level contrastive learning method to approach the latent long-term and short-term latent sequential representations, respectively.\n$\\pounds_{Time}^+ = log \\frac{exp (sim (u_{LS}^a u_{GS}^a) /\\tau)}{\\sum_{b a} exp (sim (u_{GS}^a) /\\tau)}$,\n$\\pounds_{Time}^- = log \\frac{exp (sim (u_{LS}^a u_{GS}^a) /\\tau)}{\\sum_{b a} exp (sim (u_{GS}^a) /\\tau)}$,\nwhere VT is the set of nodes of the last snapshot graph GT."}, {"title": "3.3 Output Layer", "content": "The embedding uTa for node a in the last snapshot GT is utilized in the link prediction task, aiming to predict the existence of links between node a and other nodes. Thus, this task is transformed into a similarity problem between node a and its neighboring nodes in the last snapshot GT. We adopt binary cross-entropy minimization as our objective function, defined as follows:\n$\\pounds_{main} = - \\sum_{a \\in V^T} \\sum_{(a,j) \\in O^+} log(\\sigma (u_a^T u_j^T))- \\sum_{a \\in V^T} \\sum_{(a,j) \\in O^-} log(\\sigma (u_a^T u_j^T))$,\nwhere uTa for any node a is defined as the mean pooling of uLSa and uGSa, i.e., uTa = (uLSa + uGSa)/2. Within the graph GT, neighbors of node a are defined as positive examples, while a random sample of non-neighbor nodes serves as negative examples, forming the triple (a, i, j). The set O is defined as {(a, i, j)}. Correspondingly, the positive tuple is O+ = {(a, i)} and the negative tuple is O\u2212 = {(a, j)}.\nThen the total loss \u00a3total is formulated by weighting three specific types of losses: the main cross-entropy loss Lmain, node-level heterogeneity differentiation loss denoted by LN = L+Node-\u00a3Node, edge-level heterogeneity differentiation loss expressed as LE = L+Edge-\u00a3Edge, and time-level heterogeneity differentiation loss represented by LT = L+Time-\u00a3Time\n\u00a3total = \u00a3main + \u03bb1LN + \u03bb2LE + \u03bb3LT,\nwhere \u03bb1, \u03bb2, and \u03bb3 represent the learnable weighting factors employed to balance three losses. The procedure of our CLP is outlined in Algorithm 1. We define d, N, and T as the node embedding dimension, the number of node and snapshot, respectively. The time complexity of our CLP is O(tNd\u00b2)."}, {"title": "4 EXPERIMENTS AND ANALYSIS", "content": "We conduct extensive experiments and compare our results with eight baselines across four datasets to investigate the following three research questions:\n\u2022 Q1:How does CLP's performance compare to state-of-the-art models?\n\u2022 Q2: What role do key components in CLP play in enhancing its performance?\n\u2022 Q3: How does adjusting hyperparameters affect the CLP's performance?\nWe first provide a concise overview of the experimental setup, followed by the responses to the aforementioned research questions."}, {"title": "4.1 Experimental Setup", "content": "4.1.1 Datasets. We perform experiments on four application scenarios, i.e., Math-overflow, Taobao, OGBN-MAG, and COVID-19, to verify the universality and effectiveness of our proposed CLP. Details of four datasets are presented in Table 2.\nOGBN-MAG [13]: OGBN-MAG is a subset of Microsoft Academic Graph (MAG), which contains four node types (papers, authors, institutions, and research areas) and four types of relationships (authors affiliated with institutions, authors writing papers, papers citing other papers, and papers associated with research areas as their topics). In this experiment, a THN is extracted from OGBN-MAG for the period from January 1st to 10th, 2010. Time is divided into time slots, each containing 1000 edges of each type to form the OGBN-MAG dataset used in our experiment.\nCOVID-19 2: COVID-19, sourced from 1point3acres, provides state-level and county-level daily case reports, including confirmed cases, new cases, deaths, and recovered cases. We select the daily new COVID-19 cases as our THN data. This dataset consists of two node types: state and county) and three relationships: one administrative affiliation (state includes county) and two geospatial relationships (state adjacent to state, county adjacent to county). In our experimental study, the constructed THN covers the period from May 1 to 21, 2020 and comprises 21 time snapshots. Each snapshot restricts each relationship type to a maximum of 2000 edges.\nAfter the time window is partitioned, further data cleansing is necessitated. In our model and the baselines, representation vectors of nodes are derived from the initial t network snapshots. However, it is not feasible to obtain representation vectors for nodes that fail to appear in the (t + 1)-th network snapshots, rendering link prediction for these nodes impossible. Consequently, nodes that newly appear, along with their corresponding links in the (t+1)-th snapshot, are eliminated. Additionally, the negative links for the training set are collected from links that are absent in the initial t snapshots. For the test set, negative links are sampled from those not present in the (t + 1)-th snapshot."}, {"title": "4.1.2 Baselines", "content": "We evaluate our CLP against eight baselines categorized into four groups: Static Homogeneous, Static Heterogeneous, Dynamic Homogeneous, and Dynamic Heterogeneous approaches. These baselines encompass both traditional and advanced link prediction models that are highly relevant to our research. In addition, we provide detailed descriptions of the fundamental elements of the baseline models and our CLP in Table 3.\n(1) Static Homogeneous approaches:\n\u2022 SEAL [51] extracts the local graph for the target link and learns the local graph features to estimate the probability of that link's existence.\n\u2022 VGNAE [1] integrates variational inference, GCNs, and normalization techniques to effectively learn probabilistic node embeddings from graph-structured data.\n(2) Static Heterogeneous approaches:\n\u2022 Metapath2Vec [6] leverages metapath-guided random walks combined with the skip-gram model to learn low-dimensional embeddings for nodes in heterogeneous information networks, capturing both structural and semantic relationships within the network.\n\u2022 GATNE [2] learns node representations in attributed multiplex networks. It successfully addresses the challenges of integrating multiple types of relationships and node attributes into a unified representation.\n(3) Dynamic Homogeneous approaches:\n\u2022 TGAT [42] presents an inductive representation learning approach for temporal graphs, combining temporal encoding and GNNs to capture the dynamic nature of such graphs, which can be generalized to new nodes and future graph snapshots.\n\u2022 TDGNN [30] designs a continuous-time link prediction method for dynamic graphs, leveraging temporal encodings and attention mechanisms to enhance the predictive capabilities of GNNs.\n(4) Dynamic Heterogeneous approaches:\n\u2022 THAN [20] leverages memory mechanisms and transformer architectures to capture intricate temporal and structural information of temporal heterogeneous graphs.\n\u2022 THGAT [50] combines neighborhood type modeling, neighborhood information aggregation, and time encoding technique to achieve accurate node representations.\n(5) Our models:\n\u2022 CLP without Node-level (CLP-N), which excludes the the node-level heterogeneity differentiation loss to verify its enhancement for the node-level node embedding.\n\u2022 CLP without Edge-level (CLP-E), which removes the the edge-level heterogeneity differentiation loss to validate its efficacy in enhancing edge-level node embedding.\n\u2022 CLP without Time-level (CLP-T), which eliminates the the time-level heterogeneity differentiation loss to examine its contributions to temporal information modeling."}, {"title": "4.1.3 Implementation Details", "content": "We have made the source code and datasets publicly available at https://github.com/tayer915/CLP.git. During the training process, our CLP is executed with a batch size of 1024. We employ an early-stopping strategy halting training when the Average Precision (AP) metric ceases to increase for 5 consecutive epochs. The learning rate Ir is set at 1e-4. We employ Adam as the optimizer of our CLP model. The hyper-parameters are carefully optimized following a grid search. Specifically, the dimension d of node embeddings is explored within the range of {8, 16, 32, 64, 128}. Moreover, we search for the optimal values of the balance coefficients \u03bb1, \u03bb2, and \u03bb3 in the range of 1e {-4, -6, -8, -9, -10}; the number of attention heads h in the range of {1, 2, 4, 8, 16}; and the temparature coefficient \u03c4 in the closed interval of [0.04, 0.12] with a step size of 0.02. By default, after fine-tuning, we adopt the following hyperparameter settings, wherein the optimal values of d, \u03bb., h, and \u03c4 are set to 32, 1e - 8, 4, and 0.1, respectively.\nThe node representations are derived from the first T network snapshots {G1, G2, \u00b7 \u00b7 \u00b7 , GT }. The links in the subsequent network snapshot GT+1 serve as the evaluation set. Moreover, 20% of the links in this evaluation set are randomly assigned as the validation set, another 20% as the positive training set, and the remaining 60% as the positive test set. Simultaneously, several non-existent negative links are randomly sampled to form negative training and test sets.\nFor our baseline configuration, Metapath2Vec is set with a sequence walk length of 5, generating 10 walk sequences per node. The context size is 4, and the dimension of the node embedding vector is 32. The relevant parameters for SEAL, VGNAE, GATNE, TGAT, TDGNN, THAN, and THGAT are consistently maintained in accordance with their respective configurations. In the case of homogeneous models such as SEAL, VGNAE, TGAT, and TDGNN, node type and edge type information is directly eliminated from the graph data during the experiments. For static models like SEAL, VGNAE, Metapath2Vec, and GATNE, we adopt the static graph representation learning approach, integrating edge data into a unified graph for comprehensive training. We implement and fine-tune baseline models using their official codes and adhere to the optimized setting values for all other hyperparameters of the baselines as reported in their respective papers. All experiments are implemented on NVIDIA RTX A2000 (12G)."}, {"title": "4.1.4 Evaluation Metrics", "content": "We employ widely-adopted Area Under the Curve (AUC) and AP as evaluation metrics [20, 42]. AUC represents the area under the Receiver Operating Characteristic (ROC) curve, which plots the false positive rate on the x-axis and the true positive rate on the y-axis. AP refers to the area under the Precision-Recall curve, with recall on the horizontal axis and precision on the vertical axis. Values of AUC and AP closer to 1 indicate superior model performance."}, {"title": "4.2 Performance and Analysis", "content": "In order to address the aforementioned three questions Q1-Q3, we execute the following experiments and analyze the respective results."}, {"title": "4.2.1 Overall Performance Comparisons (for Q1)", "content": "To answer Q1, we perform a comparative evaluation of our CLP by comparing it against eight baselines on Math-overflow, Taobao, OGBN-MAG, and COVID-19 datasets. The results are presented in Table 4. Therefore, we obtain the following observation analyses.\n(1) The static homogeneous networks, SEAL and VGNAE, exhibit inadequate performance across all four datasets, primarily owing to their inability to leverage temporal dynamics and heterogeneous information. SEAL is specifically designed for homogeneous networks and struggles with handling multiple types of nodes and edges. It relies on subgraph extraction and graph neural networks, which makes real-time updates challenging. As for VGNE, lack adaptability to network changes, which hampers maintaining efficiency in dynamic environments.\n(2) The static heterogeneous networks, Metapath2Vec and GATNE, achieve superior performance over SEAL and VGNAE by exploiting heterogeneous information. Specifically, Metapath2Vec meticulously designs meta-paths for heterogeneous networks, while GATNE effectively integrates both the topological heterogeneity and node attribute information. However, both models overlook the network dynamics, which can lead to inaccurate node representations, as connections may change from negative to positive between training and testing phases.\n(3) The dynamic homogeneous networks, TGAT and TDGNN, excel by encoding temporal-topological features, significantly outperforming the aforementioned SEAL, VGNAE, Metapath2Vec, and GATNE. However, TGAT focuses only on temporal-topological nodes and time-feature edges, neglecting the diversity of nodes and edges. On the other hand, TDGNN emphasizes only the temporal aspects of edges in node representations, disregarding edge multiplicity. Therefore, the effectiveness of TGAT and TDGNN is limited when compared to THAN and THGAT.\n(4) The dynamic heterogeneous networks, THAN and THGAT, exhibit robust performance through advanced modeling techniques. For THAN, it captures sufficient heterogeneous information and builds continuous dynamic relationships through the analysis of temporal causality. For THGAT, it designs a node signature method tailored to heterogeneous data and incorporates a temporal heterogeneous graph attention layer, effectively integrating both heterogeneous and temporal information. These two models effectively model both temporal dynamics and heterogeneous semantics in graph data, as evidenced by superior performance compared to other baseline models.\n(5) Our model, CLP, markedly surpasses all baseline models in terms of AUC and AP across four datasets. Compared to the second best model, THGAT, which relies on a coarse-grained view to capture heterogeneous and temporal information, our CLP employs a fine-grained perspective to effectively encode the spatial heterogeneity and temporal heterogeneity. Initially, we devise a heterogeneous temporal graph to delineate the underlying structural distribution patterns and sequential evolution paradigms. Subsequently, we propose a contrastive hierarchical discrepancy elimination module to incorporate intrinsic inter-relations at node-, edge-, and time-level, respectively. As a result, our model achieves significant performance enhancements over THGAT, with average increases of 10.10% and 13.44% in terms of AUC and AP, respectively.\n(6) The performance of all models on the COVID-19 dataset exhibits a general decline. This downturn can be attributed to the scarcity of observational nodes, which contributes to data instability and suboptimal link prediction results. Moving forward, we will strive to enhance our model's performance and intensify our modeling efforts to better understand complex patterns and dynamics associated with disease transmission pathways."}, {"title": "4.2.2 Ablation Experiments (for Q2)", "content": "For Q2, we perform ablation studies on four datasets to evaluate the effectiveness of the core components in CLP. Our analyses primarily focus on the ablation of node, edge, and time-level loss functions in the hierarchical heterogeneity differentiation modeling, designated as CLP-N, CLP-E, and CLP-T, respectively. Figure 3 displays the comparison results. These findings indicate that our proposed CLP performs significantly better than its variant models.\n(1) CLP-E exhibits the most substantial performance degradation. Specifically, removing the edge-level heterogeneity differentiation loss leads to a marked decrease in predictive accuracy, with average reductions of 19.08% in AUC and 12.67% in AP. Notably, Taobao experiences a 35.65% decrease in terms of AUC, underscoring the critical role of the edge-level discrepancy elimination module in our model.\n(2) CLP-T results in the second most significant performance drop. Specifically, when we eliminate the time-level heterogeneity differentiation loss, there is a considerable decrease in prediction performance, with average decreases of 16.23% and 12.67% in AUC and AP, respectively. This observation validates the vital importance of the time-level discrepancy elimination module in augmenting the overall node representation.\n(3) CLP-N also indicates a decline in link performance. Specifically, when we remove the node-level heterogeneity differentiation loss, there is a notable decrease in AUC and AP, with an average reduction of 13.05% and 8.65%, respectively. This result confirms the essential contribution of the node-level nonuniformity elimination module.\nIn conclusion, the"}]}