{"title": "MANISKILL-HAB: A BENCHMARK FOR LOW-LEVEL MANIPULATION IN HOME REARRANGEMENT TASKS", "authors": ["Arth Shukla", "Stone Tao", "Hao Su"], "abstract": "High-quality benchmarks are the foundation for embodied AI research, enabling significant advancements in long-horizon navigation, manipulation and rearrangement tasks. However, as frontier tasks in robotics get more advanced, they require faster simulation speed, more intricate test environments, and larger demonstration datasets. To this end, we present MS-HAB, a holistic benchmark for low-level manipulation and in-home object rearrangement. First, we provide a GPU-accelerated implementation of the Home Assistant Benchmark (HAB). We support realistic low-level control and achieve over 3x the speed of previous magical grasp implementations at similar GPU memory usage. Second, we train extensive reinforcement learning (RL) and imitation learning (IL) baselines for future work to compare against. Finally, we develop a rule-based trajectory filtering system to sample specific demonstrations from our RL policies which match predefined criteria for robot behavior and safety. Combining demonstration filtering with our fast environments enables efficient, controlled data generation at scale.", "sections": [{"title": "INTRODUCTION", "content": "An important goal of embodied AI is to create robots that can solve manipulation tasks in home-scale environments. Recently, faster and more realistic simulation, home-scale rearrangement benchmarks, and large robot datasets have provided important platforms to accelerate research towards this goal. However, there remains a need for all of these features in one unified benchmark.\nWe present MS-HAB\u00b9, a holistic, open-sourced, home-scale manipulation benchmark with four key features: (1) fast simulation with realistic physics and manipulation, including low-level control, for efficient training, evaluation, and dataset generation, (2) home-scale manipulation tasks through the Home Assisitant Benchmark (HAB) (Szot et al., 2021), (3) extensive baselines for future work to compare against, and (4) scalable, controlled data generation using an automated, rule-based trajectory filtering system.\nFast Manipulation Simulation with Realistic Physics and Rendering: Using ManiSkill3 (Tao et al., 2024), we implement a GPU-accelerated version of the HAB (Szot et al., 2021), an apartment-scale rearrangement benchmark containing three long-horizon tasks using the Fetch mobile manipulator (ZebraTechnologies, 2024). While the original HAB uses magical grasp (teleport closest object within 15cm to the gripper), we require realistic grasping.\nThe MS-HAB environments support low-level control for realistic grasping, manipulation, and interaction, while the original Habitat 2.0 implementation does not support such kind of low-level control. Furthermore, by scaling parallel environments, MS-HAB environments achieve over 4000 samples per second (SPS) while the robot actively collides with multiple dynamic objects and the environment renders 2 128x128 RGB-D images - 3x faster than Habitat 2.0 at similar GPU memory usage. This significant speedup allows us to scale up training, evaluation, and data generation.\nReinforcement Learning (RL) Baselines: Online RL provides a promising framework to learn from online interaction without needing preexisting demonstration data. As in Gu et al. (2023a), we train individual mobile manipulation skills and chain them to solve long-horizon tasks. We"}, {"title": "RELATED WORK", "content": "Simulators and Scene-Level Embodied AI Platforms: Earlier scene-level simulators focus on navigation and simple interaction with realistic visuals (Savva et al., 2019). Other simulators add kinematic object state transitions (Kolve et al., 2017; Li et al., 2021), significant scene randomization (Deitke et al., 2022; Nasiriany et al., 2024), soft-body physics and audio (Gan et al., 2022), flexible and deformable materials, object composition rules, and so on (Li et al., 2022). However such complicated features often slow down simulation speed.\nHabitat 2.0 forgoes additional features, supporting rigid-body dynamics, articulations, and magical grasping, to achieve best-in-class single-process scene-level simulation speed (Szot et al., 2021). However, it is constrained by the limited parallelization of CPU simulation.\nOther simulators focus on low-level, contact-rich control in simpler settings (James et al., 2020; Zhu et al., 2020; Xiang et al., 2020). ManiSkill3 in particular achieves state-of-the-art GPU simulation speed (Tao et al., 2024), however its suite of tasks are simpler than the Home Assistant Benchmark (HAB) (Szot et al., 2021), which we implement for MS-HAB.\nScalable Demonstration Datasets: Real-world robot datasets are promising for direct deployment to the real world (Brohan et al., 2023). However, these initiatives are limited in scaling and use cases due to small-scale toy setups (Ebert et al., 2022), vision-only data (Dasari et al., 2019), or requiring massive coordinated (et al., 2024; Khazatsky et al., 2024) or distributed (Mandlekar et al., 2018) human effort over many months or even years. Furthermore, real robot datasets cannot efficiently generate new data, and do not support online sampling.\nGenerative interactive world models allow some interactivity on similarly realistic data by generating new frames based on provided actions (Yang et al., 2024). However, these models suffer from artifacts and long-term memory issues which rule out home-scale rearrangement, and low frame rates make training high-frequency low-level control policies intractable. Furthermore, neither real-robot datasets nor generative world models currently support querying privileged data from a simulator, which is necessary for MS-HAB's automated event labeling and trajectory categorization."}, {"title": "PRELIMINARIES", "content": "The Home Assistant Benchmark (HAB) (Szot et al., 2021) includes three long-horizon tasks which involve rearranging objects from the YCB dataset (\u00c7alli et al., 2015):\n\u2022 TidyHouse: Move 5 target objects to different open receptacles (e.g. table, counter, etc).\n\u2022 PrepareGroceries: Move 2 objects from the opened fridge to goal positions on the counter, then 1 object from the counter to the fridge.\n\u2022 SetTable: Move 1 bowl from the closed drawer to the dining table and 1 apple from the closed fridge to the same dining table.\nTo solve these tasks, Szot et al. (2021) define parameterized skills: Pick, Place, Open Fridge/Drawer, Close Fridge/Drawer, and Navigate. For each skill, we define corresponding subtasks. Successful low-level grasping is heavily dependent on an object's pose. So, depending on the subtask, the simulator provides ground-truth pose $X_{pose} = [x_{rot}|x_{pos}]$ for target object x, ground-truth handle position $a_{pos}$ for target articulation a, or 3D goal position $g_{pos}$, updated each timestep during manipulation. Each subtask also fails if the robot cumulative force reaches beyond a set threshold. For more details, see Appendix A.1. We provide brief descriptions of the subtasks below:\n\u2022 Pick[a, optional]($X_{pose}$): pick object x (from articulation a, if provided).\n\u2022 Place[a, optional]($X_{pose}$, $g_{pos}$): place object x in goal g (in articulation a, if provided)\n\u2022 Open[a]($a_{pos}$): open articulation a with handle at $a_{pos}$\n\u2022 Close[a]($a_{pos}$): close articulation a with handle at $a_{pos}$\n\u2022 Nav($*_{pos}$): navigate to *\nFrom a reinforcement learning perspective, we formulate each long-horizon task as a standard Markov Decision Process (MDP) which can be described as a tuple $M = (S, A, R, T, \\rho, \\gamma)$ with continuous state space S, action space A, scalar reward function $R : S \\times A \\rightarrow R$, environment dynamics function $T : S \\times A \\rightarrow S$, initial state distribution $\\rho$, and discount factor $\\gamma \\in [0, 1]$. Then, as in Gu et al. (2023a), define a subtask w as a smaller MDP $(S, A_w, R_w, T, \\rho_w, \\gamma)$ derived from M. For each task M with subtask w, we train low-level control policy $\\pi_\\omega : S \\rightarrow A_w$ with RL or IL."}, {"title": "SKILL CHAINING", "content": "Similar to Szot et al. (2021), we split each task into a sequence of subtasks using a perfect task planner. The sequences are defined below:\n\u2022 TidyHouse: For ($x_i$, $g_i$) \u2208 {($x_0$, $g_0$), ..., ($x_4$, $g_4$)}, complete:\nNav($x_{i,pos}$) \u2192 Pick($x_{i,pose}$) \u2192 Nav($g_{i,pos}$) \u2192 Place($x_{i,pose}$, $g_{i,pos}$)\n\u2022 PrepareGroceries: For ($x_i$, $g_i$) \u2208 {($x_0$, $g_0$), ($x_1$, $g_1$), ($x_2$, $g_2$)}, complete:\nNav($x_{i,pos}$) \u2192 PickFr[$i\u22641$]($x_{i,pose}$) \u2192 Nav($g_{i,pos}$) \u2192 PlaceFr[$i=2$]($x_{i,pose}$, $g_{i,pos}$)\n\u2022 SetTable: For ($x_i$, $g_i$, $a_i$) \u2208 {($x_0$, $g_0$, Dr), ($x_0$, $g_0$, Fr)}, complete:\nNav($a_{i,pos}$) \u2192 Open$_{a_i}$ ($a_{i,pos}$) \u2192 Nav($x_{i,pos}$) \u2192 Pick($x_{i,pose}$) \u2192 Nav($g_{i,pos}$) \u2192 Place($x_{i,pose}$, $g_{i,pos}$) \u2192 Nav($a_{i,pos}$) \u2192 Close$_{a_i}$ ($a_{i,pos}$)"}, {"title": "TRAIN AND VALIDATION SPLITS", "content": "The ReplicaCAD dataset (Szot et al., 2021) serves as the source for our apartment scenes. It comprises 105 scenes divided into 5 macro-variations, each containing 21 micro-variations. Macro-variations alter the layout of large furniture items such as refrigerators and kitchen counters, while micro-variations modify the placement of smaller furnishings like chairs and TV stands. The dataset is split into three parts: 3 macro-variations for training, 1 for validation, and 1 for testing. However, as the test split is not publicly accessible, our study utilizes only the train and validation splits.\nFurthermore, for each long-horizon task, HAB provides 10,000 training episode configurations and 1,000 validation configurations. These configurations specify initial poses for YCB objects and define target objects, articulations, and goals. Importantly, these configurations exclusively utilize ReplicaCAD scenes from their respective splits."}, {"title": "ENVIRONMENT DESIGN AND BENCHMARKS", "content": "By scaling parallel environments with GPU simulation, MS-HAB achieves 4000 SPS on a benchmark involving representative interaction with dynamic objects 3x Habitat 2.0's implementation. Our environments support realistic low-level control for successful grasping, manipulation, and interaction, while the Habitat 2.0 environments do not support such kind of low-level control. This section outlines environment design choices which leverage GPU acceleration and benchmarks MS-HAB against Habitat's implementation."}, {"title": "ENVIRONMENT DESIGN", "content": "Evaluation and Training Environments: First, we provide the base evaluation environment, SequentialTask, which supports executing different subtasks simultaneously on GPU. We perform physics simulation and rendering for all environments in parallel, then slice data by subtask to compute success and fail conditions. It does not support dense reward or spawn selection/rejection.\nSecond, we provide training environments for each subtask, {SubtaskName}SubtaskTrain, which extend the main evaluation environment. Each training environment provides dense rewards hand-engineered for the Fetch embodiment, supports spawning with randomization and rejection, and incorporates any additional features needed for training specific subtask skills.\nObservation Space: We include target object pose, goal position, and TCP pose relative to the base, an indicator of whether the target object is grasped, 128x128 head and arm RGB-D images, and robot proprioception. For our experiments, we use only depth images. As is standard for the ManiSkill"}, {"title": "BENCHMARKING", "content": "We adapt Habitat 2.0's Interact benchmark, which originally had the Fetch robot execute a precomputed trajectory to collide with two dynamic objects (Szot et al., 2021). While we retain the same precomputed trajectory, assets, and scene configuration, we modify the robot's initial pose and disable magical grasp, allowing it to interact with five objects instead. Our setup includes two mounted 128x128 RGB-D cameras, with a simulation frequency of 100Hz and a control frequency of 20Hz (the standard for low-level control in ManiSkill3). We collect observation data from vectorized environments at each step ($o_{t}$) call. Our benchmarking is conducted on a machine equipped with a 16-core/32-thread Intel i9-12900KS processor and an Nvidia RTX 4090 GPU with 24 GB VRAM.\nIt is important to note that running the exact same episode in different simulators is exceedingly difficult since different simulation backends will result in interactions and collisions behaving slightly differently. Still, the full rollout is similar in both simulators, and the measured performance increase of MS-HAB in an interactive setting is significant.\nHabitat's Additional Optimizations: While the Habitat simulator already has best-in-class single process simulation speed, it provides optional additional optimizations: concurrent rendering and auto sleep. However, their experiments suggest that concurrent rendering can negatively impact train performance (Szot et al., 2021), so we enable auto-sleep and disable concurrent rendering.\nBenchmark Analysis: Per Fig. 1, while Habitat achieves significantly stronger performance per parallel environment, its peak performance is limited to 1397.65\u00b111.02 SPS due to the limited parallelization of CPU simulation. Meanwhile, by scaling up to 1024 environments, MS-HAB is able to achieve 4109.40 \u00b1 26.36 SPS 2.94x the simulation speed of Habitat. Furthermore, our envi-"}, {"title": "METHODOLOGY", "content": "We choose Reinforcement Learning (RL) to learn our subtask policies as RL does not require prior demonstration data, and it can take advantage of our highly parallelized environments to solve tasks in fast wall-clock time. We use a similar subtask formulation as M3, which trains mobile manipulation skills to solve each subtask from a region of spawn points.\nPick: Without magical grasp, our Pick policies must learn grasp poses which are valid, stable, and reachable within the kinematic constraints of the mobile Fetch robot. Furthermore, the policy must learn action sequences which can reach these grasp poses and retrieve the target object within the specified horizon while keeping the robot under the cumulative collision force limit.\nAs a result, learning successful grasping for multiple objects with different geometries \u2014 in addition to whole body control with collision constraints is difficult. So, we opt to train individual Pick policies for each object, thereby overfitting to the geometry of that object. Our experiments show these per-object Pick policies achieve improved subtask success rates compared to all-object policies when handling many objects with varied geometries. In other words, we train a unique per-object Pick policy for every task/subtask/object combination.\nPlace: We train per-object Place policies as well. Our experiments show that, in settings where object geometry is more important (e.g. placing in a fridge with tighter tolerances), per-object Place policies reach higher success rates than all-object policies."}, {"title": "AUTOMATED TRAJECTORY CATEGORIZATION AND DATASET GENERATION", "content": "Thanks to fast simulation environments, we can quickly generate 10s to 100s of thousands of demonstrations. However, our experiments show that our Imitation Learning (IL) policies are sensitive to demonstration behavior. To filter out \"suboptimal\" demonstrations, we use privileged information from our simulator to group demonstrations into mutually exclusive, collectively exhaustive success and failure modes without significant manual labor. Furthermore, we use this trajectory labeling system to identify types and causes of failure in our baseline policies in Sec. 6.2 and Appendix A.6.\nExample of Pick Subtask: We provide a high-level overview of trajectory labeling on the Pick subtask. For detailed definitions of events and labels, see Appendix A.6. First, we define \"events\" which occur at any timestep t: 1) Contact: nonzero robot/target pairwise force, 2) Grasped: object not grasped at step t-1 and grasped at step t, 3) Dropped: object grasped at step t-1 and not grasped at step t, and 4) Excessive Collisions: robot cumulative force exceeds 5000 N. For Pick trajectory $T_{pick} = (s_0, a_0,..., s_n, a_n)$, we create chronologically ordered event list $E_{pick} = (e_1,..., e_k)$.\nNext, we define success and failure modes. For example, one success mode is \"straightforward success\" with $E_{pick} = (Contact, Grasp, Success)$, requiring success without dropping the object or colliding too much. One failure mode is \"dropped failure,\" defined as $(Excessive Collisions \\& \\notin E_{pick}) \\land (Dropped \\in E_{pick}) \\land (i < j for maximal i, j such that e_i = Grasped, e_j = Dropped)$. \"Dropped failure\" trajectories fail because the robot irrecoverably drops the target object.\nIn generating our Pick subtask dataset, we apply filters to include only \"straightforward success\" trajectories. These trajectories are characterized by the absence of dropping and minimal collisions."}, {"title": "RESULTS", "content": "shows the RL and IL policies' progressive completion rate. We provide an optimistic upper bound on progressive completion rate by (incorrectly) assuming that the completion of each subtask is independent of every other subtask, thus directly multiplying subtask success once rates. 1 shows success once rate for individual subtasks. We find 4 major avenues for improvement.\nFirst, our optimistic upper bound shows low expected success rate on the long-horizon tasks. Even with per-object RL policies, our low-level mobile manipulation subtasks are difficult to train on dense reward, and improving subtask success rate is the most direct way to improve overall task completion rate. Second, TidyHouse and SetTable RL baselines have some gap between upper bound and real completion rate, indicating potential handoff issues or disturbance to prior target objects in success states. Meanwhile, the PrepareGroceries RL baseline has a large drop in completion rate during the second PickFr subtask, indicating that the first PickFr causes too much disturbance to objects in the fridge. So, improving policy performance in cluttered spaces is important. Third, our IL policies perform notably worse in Pick and Place, indicating a need for methods and architectures which can handle multimodalities in the data. Finally, while most RL policies generalize well to the validation split, the Close Fridge policy completely fails on validation scenes because the fridge door opens into a wall, preventing the arm from reaching the handle. This is not an issue with magical grasping (Gu et al., 2023a), indicating that low-level control may need more scene diversity."}, {"title": "RL POLICIES: ALL-OBJECT VS PER-OBJECT POLICIES", "content": "The goal of training per-object RL policies for Pick and Place is to improve subtask success rate since policies with higher success rates allow us to generate successful demonstrations under more initialization conditions. To verify this, we run two ablations.\nDoes training per-object Pick and Place policies improve subtask success rate compared to all-object policies? Per Table 1, per-object policies perform notably better in TidyHouse and Prepare Groceries Pick, which involve 9 objects, with more modest improvement in SetTable Pick, which has only 2 objects. Per-object policies perform significantly better in PrepareGroceries Place, which involves placing with tight tolerances on a cluttered fridge shelf, while performance differences are negligible in TidyHouse and SetTable Place, which only involve open receptacles. So, per-object Pick and Place policies learn improved manipulation when grasping a greater variety of objects, or when manipulating objects in areas with tighter constraints.\nAre per-object policies necessary to learn grasping for certain objects in the Pick subtask? In Table 2, we run our automated trajectory labeling system on Pick YCB object #003, the Cracker Box (\u00c7alli et al., 2015). The Fetch robot's parallel gripper can only grasp the Cracker Box along its shortest dimension, so the set of valid grasp poses are highly dependent on the object's pose relative to the robot. The all-object policy is 1.88-2.42x more likely to fail to excessive collisions and 1.87-12.37x more likely to fail to grasp the object, indicating that overfitting to a specific geometry is important for our RL policies to learn grasping on difficult geometries. For more detailed trajectory labeling definitions and statistics, please see Appendix A.6."}, {"title": "IL POLICIES: LABELING AND FILTERING DATASET TRAJECTORIES", "content": "Can we control the behavior of our IL policies by filtering for specific demonstrations? Our PrepareGroceries Place RL policies have two similarly frequent success modes: place"}, {"title": "CONCLUSION AND LIMITATIONS", "content": "We present MS-HAB a holistic home-scale rearrangement benchmark including a GPU-accelerated implementation of the HAB which supports realistic low-level control, extensive RL and IL baselines, systematic evaluation using our trajectory labeling system, and demonstration filtering for efficient, controlled data generation at scale. However, there is significant room for improvement on our baselines, and we do not claim transfer to real robots; both of these we leave to future work. Whole-body low-level control under constraints in cluttered environments, long-horizon skill chaining, and scene-level rearrangement are challenging for current robot learning methods; we hope our benchmark and dataset aid the community in advancing these research areas."}]}