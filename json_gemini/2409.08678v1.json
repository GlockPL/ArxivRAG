{"title": "Shadow Program Inversion with Differentiable Planning: A Framework\nfor Unified Robot Program Parameter and Trajectory Optimization", "authors": ["Benjamin Alt", "Claudius Kienle", "Darko Katic", "Rainer J\u00e4kel", "Michael Beetz"], "abstract": "This paper presents Shadow Program Inversion\nwith Differentiable Planning (SPI-DP), a novel first-order opti-\nmizer capable of optimizing robot programs with respect to\nboth high-level task objectives and motion-level constraints.\nTo that end, we introduce Differentiable Gaussian Process\nMotion Planning for N-DoF Manipulators (dGPMP2-ND), a\ndifferentiable collision-free motion planner for serial N-DoF\nkinematics, and integrate it into an iterative, gradient-based\noptimization approach for generic, parameterized robot pro-\ngram representations. SPI-DP allows first-order optimization\nof planned trajectories and program parameters with respect\nto objectives such as cycle time or smoothness subject to e.g.\ncollision constraints, while enabling humans to understand,\nmodify or even certify the optimized programs. We provide\na comprehensive evaluation on two practical household and\nindustrial applications.", "sections": [{"title": "I. INTRODUCTION", "content": "Intuitive robot programming has eased the use of robots\nto solve real-world applications. However, the cost of au-\ntomation is often driven by the iterative optimization of\nrobot trajectories and program parameters, particularly for\ncomplex manipulation tasks. This optimization is done by\nskilled programmers through time-consuming trial and error.\n\"Programming by optimization\" [1] allows a human pro-\ngrammer to specify a rough program skeleton, which is then\ncompleted by an optimization algorithm. This approach is\nparticularly useful in tactile applications like force-controlled\ninsertion or handling of deformable objects. However, apply-\ning general-purpose optimization methods to robot programs\nis challenging as the success of a robot skill depends on the\nparameterization of the preceding skills: An optimizer must\njointly optimize the parameters of complete skill sequences\nor hierarchically composed subprograms. Moreover, robots\nmust not only achieve task-level goals such as cycle time\nrequirements, but also respect motion-level constraints such\nas collision-freeness or proximity to a human demonstration.\nExisting approaches focus exclusively on either trajectory\noptimization [2]\u2013[5] or parameter optimization [6]-[9] and\ntypically optimize individual skills, rather than jointly opti-\nmizing complete robot programs.\nIn this paper, we propose Shadow Program Inversion with\nDifferentiable Planning (SPI-DP), a robot program optimizer\nwhich combines both trajectory and parameter optimization"}, {"title": "II. RELATED WORK", "content": "1) Robot program parameter optimization: In the context\nof \"programming by optimization\" [1], a wide array of\noptimizers have been proposed, with a majority employing\nzero-order algorithms such as evolutionary or mutation-based\nalgorithms [10]\u2013[14], particle swarms [11], [15], [16] or\nBayesian optimization [8], [16]\u2013[18]. First-order optimizers"}, {"title": "III. SHADOW PROGRAM INVERSION: A PRIMER", "content": "Our proposed framework is based on differentiable\nShadow Program Inversion (SPI), a model-based first-order\noptimizer for robot program parameters [7], [39], [40], which\nis briefly outlined below. On the basis of SPI and a differ-\nentiable N-DoF motion planner (see Sec. IV), we present\na novel double-loop first-order optimizer capable of jointly\noptimizing program parameters and motion trajectories (see\nSec. V)."}, {"title": "A. Differentiable Shadow Programs", "content": "The core of our framework is the concept of a shadow\nprogram: A differentiable \"twin\" of a robot program which\nserves as its surrogate for learning and optimization. The\nsource program, which is written by the programmer and\nultimately executed on the robot, and its shadow used for\nlearning and optimization, are representationally decoupled:\nThe source program can be expressed in any parameterized\nrepresentation, such as the textual or skill-based representa-\ntions used by modern robot programming frameworks [41]-\n[43].\nGiven a source program $P(x,\\theta_0)$, we seek to optimize\nthe program parameters $x$, given initial robot state $\\theta_0$. $P$\nis modeled as a function $P : \\mathbb{R}^N \\times \\Theta \\rightarrow \\Theta^T$, mapping a\nreal-valued $N$-dimensional parameter vector $x \\in \\mathbb{R}^N$ and\njoint angles $\\theta$ in state space to the robot trajectory $\\theta_t \\in$\n$\\Theta^T$, where $T$ is the number of timesteps. The state space"}, {"title": "B. Robot Program Parameter Optimization", "content": "Differentiable shadow programs enable parameter opti-\nmization for near-arbitrary source programs in any parame-\nterized representation. Consider a skill-based robot program\n(here in pseudocode) for an industrial peg-in-hole task:\nSpiralSearch and Insert are skills from a skill library,\nand approach pose is a program parameter correspond-\ning to the end-effector pose from which the robot starts\nsearching for a hole in the workpiece. The duration of the\nsearch depends on the position of the hole relative to the\napproach pose. Consequently, the cycle time of this program\ncan be optimized by adapting approach_pose to be, on\naverage, directly above the hole. SPI solves such parameter\noptimization problems by first-order optimization over the\nshadow program, using $P$ as a differentiable surrogate for\n$P$ [7]. The optimized parameters can be transferred back to\nthe source program $P$, validated and adjusted by a human\nprogrammer, and executed on the robot."}, {"title": "C. Joint Parameter and Trajectory Optimization", "content": "For many tasks, program parameters and low-level motion\ntrajectories must be jointly optimized. One example is the\noptimization of grasp poses to maximize grasp success:\nThe approach and depart motions must also be optimized\nfor collision-freeness, smoothness and other task constraints\nwhenever the grasp pose is changed. We integrate a differ-\nentiable motion planner into the shadow program represen-\ntation, which ensures that trajectories predicted in a forward\npass comply with motion-level constraints such as collision-\nfreeness, smoothness or proximity to a human demonstration.\nThe differentiable motion planner is described in detail in\nSection IV."}, {"title": "IV. DGPMP2-ND: DIFFERENTIABLE MOTION\nPLANNING FOR N-DOF MANIPULATORS", "content": "The gradient-based optimization of programs containing\ncollision-free motion skills requires a differentiable planner.\nWe propose dGPMP2-ND, a differentiable collision-free mo-\ntion planner for N-DoF manipulators, which generates trajec-\ntories that conform to motion constraints such as collision-\nfreeness, smoothness, adherence to joint limits or precision at\na target pose. To that end, we extend and modify dGPMP2\n[37] by implementing differentiable collision checking for\nthree-dimensional collision worlds and N-DoF serial kine-\nmatics, adding a joint limit constraint as well as a factor\nrewarding similarity to a human demonstration."}, {"title": "A. Differentiable Gaussian Motion Planning", "content": "dGPMP2 casts motion planning as inference on a factor\ngraph [37] and minimizes a cost functional $F(\\theta)$ over\ntrajectory $\\theta$ via an iterative optimization procedure [36].\nFigure 3 illustrates dGPMP2-ND. While dGPMP2 plans in\nCartesian space, dGPMP2-ND plans joint-space trajectories.\nThis permits to integrate joint limit constraints, while still\nsupporting end-effector pose constraints by applying a dif-\nferentiable forward kinematics on the joint-space trajectory.\nAt each planner iteration $j$, $1 \\leq j \\leq j_{max}$, a set of factors\nis evaluated. Given the current joint trajectory $\\theta^j$, each\nfactor computes an error $h(\\theta^j)$, a Jacobian $H$ indicating\nthe direction of steepest descent to minimize the error, and\nan inverse covariance $\\Sigma^{-1}$ to weight the different factors.\nWe propose six such factors:\n1) A Gaussian process (GP) prior factor, which penalizes\npoints on the joint trajectory that deviate from the mean\ndefined by a GP prior (see [36] for details). For each\npoint on the trajectory, the Jacobian $H_{GP}$ indicates the\ndirection toward the GP mean.\n2) A start state prior, which penalizes the deviation of the\nfirst point on $\\theta^j$ from a predefined start configuration.\nFor the first point on $\\theta^j$, the Jacobian $H_{start}$ indicates\nthe direction toward the start configuration.\n3) A goal state prior, which penalizes the deviation of the\nlast point on $\\theta$ from a predefined goal configuration.\nFor the last point on $\\theta^j$, the Jacobian $H_{goal}$ indicates\nthe direction toward the goal configuration.\n4) A collision factor (see Sec. IV-B).\n5) A joint limit factor (see Sec. IV-C).\n6) A demonstration prior (see Sec. IV-D).\nThe GP, start and goal state priors remain unchanged from\nthe original dGPMP2 formulation, albeit extended to the\nN-DoF case. We contribute novel differentiable collision\nand joint limit factors, as well as a differentiable Cartesian\ndemonstration prior.\nAt each iteration, the linear system\n$(K^{-1} + H^T \\Sigma^{-1}H)\\delta\\theta = -K^{-1}(\\theta^j - \\mu) - H^T\\Sigma^{-1}h(\\theta^j)$\nis solved for $\\delta\\theta$, where $H$ is the combined Jacobian, $\\Sigma^{-1}$ is\nthe combined inverse covariance, $K^{-1}$ is the inverse kernel\nmatrix of the GP and $h(\\theta)$ is the combined error function.\nAll matrices are combined by concatenating the matrices of\nthe individual factors along the row axis. The trajectory is\nthen incrementally updated: $\\theta^{j+1} = \\theta^j + r * \\delta\\theta$, where $r$ is"}, {"title": "B. Differentiable N-DoF Collision Factor", "content": "Bhardwaj et al. [37] propose a collision factor for 2D\nenvironments and a point robot. We extend their approach to\n3D environments, joint-space trajectories, and N-DoF serial\nrobot kinematics. Before planning, we precompute a 3D\nsigned distance field (SDF) of the environment, where each\nvoxel contains the signed distance from the voxel center to\nthe next obstacle. For all states on joint trajectory $\\theta$, we\ncompute the Cartesian poses and Jacobians of all links using\ndifferentiable forward kinematics [44]. For each link and\neach time step, we identify the SDF voxel that intersects with\nthe collision mesh of the link and has the smallest distance to\nthe collision environment. To compute a differentiable error,\nwe then take for each identified voxel the weighted mean\nof the 26 surrounding voxels, resulting in a vector pointing\naway from the nearest collision. The resulting Jacobian\nequals the matrix multiplication of the Jacobian for each link\nand the Jacobian for the differentiable error."}, {"title": "C. Joint Limit Factor", "content": "To ensure that joint-limit constraints of the manipulator\nare met, we extend dGPMP2 by a joint limit factor. For each\nstate on the joint trajectory $\\theta$, the joint limit error\n$h_{lim} = \\begin{cases}\n0 - \\Theta_{lim} & \\text{if } \\theta > \\Theta_{lim} \\\\\n-\\Theta_{lim} - \\theta & \\text{if } \\theta < -\\Theta_{lim} \\\\\n0 & \\text{otherwise}\n\\end{cases}$\npenalizes trajectory states which exceed or fall below the\njoint limits $\\Theta_{lim}$. $H_{lim}$ is the identity matrix for values\noutside the limits, zero otherwise."}, {"title": "D. Demonstration prior", "content": "For many planning problems, human demonstrations can\nbe leveraged to guide the planner toward good solutions,\nspeeding up convergence. We extend dGPMP2 by a prior\nfactor which penalizes trajectories that deviate from a refer-\nence trajectory, such as a human demonstration. For every\nstate on the joint trajectory $\\theta$, we compute the Cartesian\nend-effector pose $p$ and Jacobian $H_{traj}$. The demonstration\nprior error $h_{traj}$ is the pointwise difference between $p$ and\nthe corresponding point on the reference trajectory.\nTaken together, dGPMP2-ND permits collision-free mo-\ntion planning by iterative optimization, while respecting\nadditional motion-level constraints such as joint limits or\nadherence to a reference trajectory. dGPMP2-ND is dif-\nferentiable end-to-end, permitting the differentiation of the\nresulting trajectories with respect to input parameters such\nas target poses."}, {"title": "V. JOINT TRAJECTORY AND PARAMETER OPTIMIZATION", "content": "For many real-world robot tasks, motion trajectories and\nprogram parameters cannot be optimized in isolation. Grasp-\ning is a canonical example: Grasping an object with a given\ngrasp pose imposes constraints on the approach motion,\nwhile e.g. collision objects in the environment make some\napproach motions, and therefore grasp poses, impossible.\nGrasp poses and approach motions must be jointly optimized\nin order to achieve task-level objectives (a stable grasp) while\nobeying motion-level constraints (collision-free approach).\nWith dGPMP2-ND, gradient-based optimization over differ-\nentiable shadow programs permits the joint optimization of\nmotion trajectories and program parameters. Fig. 4 shows\nthe integration of dGPMP2-ND as a differentiable collision-\nfree motion planner into the shadow program architecture.\nShadow programs are differentiable, predictive models of\nrobot programs; with dGPMP2-ND, collision-free planning\nbecomes part of their forward pass."}, {"title": "A. Shadow Program Inversion with Differentiable Planning", "content": "With the integration of dGPMP2-ND in the shadow pro-\ngram architecture, even complex multi-skill robot programs\ninvolving collision-free planning skills are represented as dif-\nferentiable computation graphs. This enables the computation\nof $\\frac{\\partial \\Phi(\\Theta)}{\\partial x}$, the gradient of some task-level objective function\n$\\Phi$ of the predicted trajectory w.r.t. the program parameters $x$,\nand the optimization of $x$ by a first-order optimizer. We call\nthis procedure Shadow Program Inversion with Differentiable\nPlanning (SPI-DP). For each iteration $i$,\n1) a forward pass through the shadow program is per-\nformed, yielding a prediction of $\\Theta$ given initial inputs\n$x$ and start state $\\theta_0$. This includes multiple iterations of\ndGPMP2-ND as an inner-loop trajectory optimizer for\neach shadow skill involving C-space planning;\n2) a backward pass is performed to compute $\\frac{\\partial \\Phi(\\Theta)}{\\partial x}$ via\nautomatic differentiation [45];\n3) the input parameters are incrementally updated via\ngradient descent to minimize $\\Phi$. We use Adam [46] with"}, {"title": "B. Task-Level Objective Functions", "content": "As the learnable components of shadow skills are trained\noffline to accurately predict the expected trajectory, the task\nobjective $\\Phi$ does not need to be known at training time.\nGiven trained shadow skills, program parameters can be\noptimized for near-arbitrary differentiable objective functions\n$\\Phi$ over the expected trajectory $\\Theta$. For industrial applications,\nthe process metrics cycle time, path length and success"}, {"title": "VI. EXPERIMENTS", "content": "We evaluate our framework on a household table cleaning\nscenario, in which a robot is tasked to pick up a cup from\na table and place it into a cupboard, while guaranteeing\ncollision-freeness (see VI-A.1). The motions are conditioned\non one single human demonstration. In a second set of exper-\niments, we demonstrate the zero-shot transfer to a different\nobject (a wine glass) and the simultaneous optimization of\nthe target pose (see VI-A.2). The experiments test three\nhypotheses:\nH1 Motion-level optimization: dGPMP2-ND is capable of\nplanning collision-free, smooth pick-and-place motions\nthat adhere to a single human demonstration for variable\ntarget poses and different object geometries;\nH2 Task-level optimization: SPI-DP can optimize the en-\ntire robot program parameters for KPIs resulting in\nreduced overall cycle-time while respecting imposed\ncontact-force limits;\nH3 Joint optimization: SPI-DP is capable of jointly op-\ntimizing robot programs with respect to motion-level\n(collision-freeness, human demonstration) and task-\nlevel (cycle time, contact force) constraints.\nThe setup consists of a UR5 robotic arm with a flange-\nmounted ATI Gamma force-torque sensor and a SCHUNK\npneumatic gripper. 10 demonstrations of a human transfer-\nring the cup from random pick-up poses to random target\nposes are collected with an Intel RealSense RGB-D camera.\nA human demonstration consists of the sampled 6D pose\ntrajectory of the center of the cup.\n1) Cup Pick-and-Place: A robot program consisting of\napproach, grasp, transfer, place and depart skills is optimized\nto place the cup at one of four target poses on two different\nshelf levels inside the cupboard. In this experiment, the"}, {"title": "VII. CONCLUSION AND OUTLOOK", "content": "We introduce Shadow Program Inversion with Differen-\ntiable Planning (SPI-DP), a first-order robot program op-\ntimizer capable of jointly optimizing robot program pa-\nrameters and motion trajectories. To that end, we present\ndGPMP2-ND, a differentiable motion planner for n-DoF\nrobotic manipulators. SPI-DP integrates dGPMP2-ND into\na first-order general-purpose optimizer, which optimizes the\nparameters of a given robot program with respect to task-\nlevel objectives, while simultaneously ensuring that motion-\nlevel constraints are respected. SPI-DP is evaluated on two\nrepresentative use cases from service and industrial robotics.\nBoth experiments show that SPI-DP enables the optimization\nof program parameters such as target poses or search regions\nwhile ensuring collision-freeness, smoothness and kinematic\nfeasibility. To our knowledge, SPI-DP is the first gradient-\nbased optimizer capable of jointly optimizing program pa-\nrameters and motion trajectories for arbitrary parameterized\nrobot programs. Limitations of SPI-DP include its relative\nsensitivity with respect to hyperparameters, particularly the\nGP and collision factor covariances. We suggest the inves-\ntigation of metaheuristics [47], [48] and meta-optimization\napproaches [49]\u2013[51] for future work, which reliably steer\nthe optimizer toward stable solutions or optimize planner\nhyperparameters for efficient convergence. Moreover, we\nseek to evaluate dGPMP2-ND on large-scale standardized\nplanning benchmarks and investigate its integration as a\ndifferentiable planner for reinforcement learning [27]-[30]\nand Task and Motion Planning [22], [52], [53]."}]}