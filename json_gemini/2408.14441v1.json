{"title": "Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification", "authors": ["Mahrukh Awan", "Asmar Nadeem", "Muhammad Junaid Awan", "Armin Mustafa", "Syed Sameed Husain"], "abstract": "Exploiting both audio and visual modalities for video classification is a challenging task, as the existing methods require large model architectures, leading to high computational complexity and resource requirements. Smaller architectures, on the other hand, struggle to achieve optimal performance. In this paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that introduces a compact model architecture specifically designed to capture intricate audio-visual relationships in video data. Through extensive experiments on the challenging YouTube-8M dataset, we demonstrate that Attend-Fusion achieves an F1 score of 75.64% with only 72M parameters, which is comparable to the performance of larger baseline models such as Fully-Connected Late Fusion (75.96% F1 score, 341M parameters). Attend-Fusion achieves similar performance to the larger baseline model while reducing the model size by nearly 80%, highlighting its efficiency in terms of model complexity. Our work demonstrates that the Attend-Fusion model effectively combines audio and visual information for video classification, achieving competitive performance with significantly reduced model size. This approach opens new possibilities for deploying high-performance video understanding systems in resource-constrained environments across various applications.", "sections": [{"title": "1 Introduction", "content": "Audio-Visual video understanding [67] represents the frontier of video classification, building upon the foundations laid by static image recognition and extending into the complex realm of temporal and multimodal data processing [14, 24, 34, 40-42, 53, 56, 59, 65]. While datasets like ImageNet [13] revolutionized image classification, the emergence of large-scale video datasets such as YouTube-8M [2] has shifted the focus to video analysis, enabling the evaluation of models' capabilities in multi-label video classification tasks [18, 30, 31] and their ability to interpret temporal and multimodal information across visual and audio modalities [62]."}, {"title": "2 Related Work", "content": "Proliferation of multimedia content, advancements in deep learning and high demands for models to understand and interpret AV(audio-visual) data in various"}, {"title": "2.1 Deep AV Learning", "content": "Deep learning techniques have been extensively employed in audio-visual learning tasks. Convolutional Neural Networks (CNNs) [44, 54, 71] have been widely used for learning spatial features from visual data, while Recurrent Neural Networks (RNNs) [1] and Long Short-Term Memory (LSTM) networks [6] have been utilized to capture temporal dependencies in sequential data. Recently, transformer networks [63] have gained popularity in audio-visual learning due to their ability to capture long-range dependencies and model multi-modal inter-actions effectively. Various transformer-based architectures, such as the Audio-Visual Transformer [43], Cross-Modal Transformer [61], and Multi-modal Trans-former [16], have been proposed for tasks such as audio-visual speech recognition, audio-visual event localization, and multi-modal sentiment analysis. These tech-niques have been applied to various benchmarked datasets, such as UCF101 [57] and YouTube-8M [3], to address the unique challenges posed by video data, including high dimensionality and temporal dependencies [52]."}, {"title": "2.2 Video Classification on YouTube-8M Dataset", "content": "The YouTube-8M dataset [2] has become a popular benchmark for video classification tasks, particularly in the context of audio-visual learning. This large-scale dataset consists of millions of YouTube videos annotated with a diverse set of labels, making it a challenging testbed for multi-label video classification. Vari-ous approaches have been proposed to tackle the unique challenges posed by this dataset, such as its scale, diversity, and the presence of noisy labels. Lee et al. [30] introduced a collaborative experts model that utilizes a mixture of experts and a classifier to handle the multi-label classification problem on the YouTube-8M dataset. Gkalelis et al. [18] proposed a subclass deep neural network approach that learns a set of subclasses for each label, capturing the complex relationships between video content and labels. Li et al. [31] presented a multi-modal fusion framework that leverages both audio and visual features to improve video classi-fication performance on the YouTube-8M dataset. Their approach incorporates a cross-modal attention mechanism to selectively focus on relevant audio and visual cues."}, {"title": "2.3 Fusion Techniques in AV Learning", "content": "Integrating and synchronizing audio-visual modalities is a fundamental challenge in audio-visual learning [17]. Traditional fusion approaches, such as early and late fusion, are still widely used due to their simplicity [8, 29, 60]. However, state-of-the-art frameworks often incorporate these approaches with advanced techniques. The Slow Fusion Network [25] introduces a Multi-modal Transfer Module (MMTM) for feature modality fusion, while the Attention-based Multi-modal Fusion Module (AMFM) [19, 23, 50, 72] incorporates attention mechanisms to selectively fuse audio and visual features."}, {"title": "2.4 Attention Mechanisms in Audio-Visual Learning", "content": "Attention mechanisms have proven effective in addressing the challenges of audio-visual learning by focusing on the most relevant parts of the input and enabling effective cross-modal integration [20, 29, 50]. Several state-of-the-art frameworks have been proposed that incorporate attention mechanisms to capture complex temporal and modal relationships in video data, such as Hierarchical Audio-visual Synchronization [27], Generalized Zero-shot Learning with Cross-modal Attention [35, 36], and Multi-level Attention Fusion Network [12, 69, 70]. Atten-tion mechanisms have also been widely employed in various audio-visual tasks, including event recognition using multi-level attention fusion networks [9], sound localization using instance attention [32] and dual attention matching [68], and regression-based tasks using recursive joint attention [49]."}, {"title": "2.5 Audio-Visual Representation Learning", "content": "Learning effective representations from audio-visual data is crucial for various downstream tasks. Self-supervised learning approaches have been proposed to leverage the inherent synchronization and correspondence between audio and visual modalities [4, 28, 38]. These methods aim to learn rich audio-visual repre-sentations without the need for explicit human annotations. Contrastive learning techniques have also been employed to learn discriminative audio-visual embed-dings [11, 39].\nOur proposed approach builds upon the existing methods discussed in this section. We leverage the power of attention mechanisms to effectively integrate audio and visual features while maintaining a compact model architecture. Our approach differs from previous works by introducing a novel combination of early"}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Baseline Models", "content": "To establish a robust baseline for our study on the YouTube-8M dataset, we implement a series of foundational models derived from prior research in large-scale video classification [7, 10, 21, 22, 26, 37, 45, 55, 66]. These models incorporate both unimodal and multimodal approaches with various fusion techniques, as illustrated in Figure 2. Our comparative analysis encompasses a range of archi-tectures, including:\nFully Connected (FC) Audio and FC Visual networks for unimodal analysis\nStandard FC with early and late fusion strategies for multimodal analysis\nFC Residual Networks (FCRN) with early and late fusion\nFC Residual Gated Networks (FCRGN) with early and late fusion\nResidual Block : The residual block used in the baseline models (FCRN and FCRGN) is defined as:\n$y = F(x, W_i) + x$ (1)\nwhere x and y are the input and output of the block, respectively, F represents the residual mapping, and Wi are the learnable weights.\nGating Mechanism : The FCRGN models incorporate a gating mech-anism to control the flow of information. The gating operation is defined as:\n$g = \\sigma(W_g x + b_g)$ (2)\n$y = g \\odot F(x, W_i) + (1 \u2212 g) \\odot x$ (3)\nwhere g is the gating vector, $\\sigma$ is the sigmoid activation function, Wg and bg are the learnable weights and biases of the gating layer, and $\\odot$ denotes element-wise multiplication.\nInput Features: We employ the same input features as used in [45], which are video-level mean and standard deviation features (MF+STD). The input fea-tures are extracted from pre-trained Inception networks. Specifically, the frame-level features are obtained from two separate Inception networks, one for video (1024-dimensional) and another for audio (128-dimensional)."}, {"title": "3.2 Proposed Models", "content": "While the baseline models employ fully connected layers for processing audio-visual features, our proposed method leverages attention mechanism to dynam-ically focus on the most relevant parts of the input and capture long-range dependencies. Figure 3 provides an overview of our attention-based network ar-chitectures."}, {"title": "3.2.1 FC Attention Network", "content": "The FC Attention Network integrates self-attention [63] mechanisms to prioritize salient features within each"}, {"title": "3.2.2 Residual Attention Networks", "content": "The Fully Connected Residual Atten-tion Networks combine attention mechanisms with residual learn-"}, {"title": "3.2.3 Attend-Fusion", "content": "Attend-Fusion is a novel architecture that effectively combines attention mechanisms and multi-stage fusion for audio-visual video classification. It processes audio and visual features separately through attention networks, which consist of fully connected layers and self-attention mechanisms, similar to in Section 3.2.1. The self-attention allows the model to focus on the most relevant features within each modality. The attended audio and visual features are then fused using a late fusion strategy, where they are concatenated along the feature dimension. The fused features undergo further processing through fully connected layers, which learn to capture complex inter-actions between the modalities.\nAttend-Fusion's key advantages include its ability to learn modality-specific and cross-modal representations at different stages, and its compact and effi-cient design. By leveraging attention mechanisms and late fusion, Attend-Fusion achieves comparative performance while maintaining a smaller model size com-pared to baseline approaches."}, {"title": "3.2.4 Audio-Visual Attention Network", "content": "The Audio-Visual Attention Net-work introduces cross-modal attention to capture nuanced interac-tions between audio and visual streams. The architecture employs a hierarchical attention approach, with self-attention mechanisms followed by cross-modal at-tention, to learn refined multimodal representations.\nCross-Modal Attention: Let Xa and Xv be the attended audio and visual features, respectively. The cross-modal attention mechanism computes the audio-guided visual features Xvla and the visual-guided audio features Xalv as follows:\n$Qa = XaWQa, Kv = XvWK\u03c5, Vv = XvWVv$ (6)\n$Xvla = softmax(\\frac{QaK^T}{\u221ad})V\u03c5$ (7)\n$Q\u03c5 = XvWQv, Ka = XaWKa, Va= XaWVa$ (8)\n$Xalv = softmax(\\frac{Q\u03c5K^T}{\u221ad})Va$ (9)\nThe attended features \u03a7\u03c5\u03b1 and Xalv are then concatenated with the self-attended features Xa and X for further processing."}, {"title": "3.2.5 Self and Cross Modal Attention Network", "content": "The Self and Cross Modal Attention Network extends the Audio-Visual Attention Network by incorporating additional self-attention layers after the cross-modal attention. This allows the network to refine the learned representations further by capturing intra-modal dependencies."}, {"title": "3.2.6 Self-Attended Cross-Modal FCRN Network", "content": "The Self-Attended Cross-Modal FCRN Network combines the self-attention and cross-modal attention mechanisms with the residual learning framework. The network employs residual blocks to facilitate the learning of complex feature interactions while leveraging the attention mechanisms to focus on relevant information."}, {"title": "3.3 Loss Function", "content": "We employ the cross-entropy loss function for multi-label classification:\n$L = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} Y_{i,c} log(\\hat{Y}_{i,c}) + (1 \u2212 Y_{i,c}) log(1 \u2013 \\hat{Y}_{i,c})$ (10)\nwhere N is the number of samples, C is the number of classes, Yi,c is the ground truth label for sample i and class c, and \u0177i,c is the predicted probability."}, {"title": "4 Experimentation and Results", "content": ""}, {"title": "4.1 Implementation Details", "content": "All models are implemented using PyTorch and trained on NVIDIA RTX3090 GPUs. We apply dropout [58] with a rate of 0.4 to the fully connected layers to prevent overfitting. The dimensions of fully connected layers are set to 8K for the baseline models and 2K for the proposed models, and the dimensions of the attention layers are set to 1024. All models are trained using the AdamW optimizer [33] with a learning rate of 0.0001. The models are trained for 20 epochs on the YouTube-8M dataset."}, {"title": "4.2 Evaluation Metrics", "content": "We evaluate the performance of our models using the Global Average Precision (GAP) metric and F1 score. The GAP metric is the mean Average Precision (AP) across all classes. The AP for a single class is defined as:\n$AP = \\sum_{k=1}^{n} P(k) \\Delta r(k)$ (11)\nwhere n is the number of test samples, P(k) is the precision at cut-off k in the list, and \u0394r(k) is the change in recall from items k \u2212 1 to k."}, {"title": "4.3 Results and Discussion", "content": "Overview In this section, we present a comprehensive evaluation of our pro-posed audio-visual video classification models, comparing their performance with baseline approaches on the YouTube-8M dataset. We report both the Global Av-erage Precision (GAP) and F1 scores for each model, providing a holistic view of their classification accuracy. Additionally, we conduct ablation studies to inves-tigate the impact of various components and design choices on the performance of our models.\nQuantitative Results Table 1 presents the performance comparison of our proposed models with the baseline models on the YouTube-8M dataset, report-ing both GAP and F1 scores. The results demonstrate that the attention-based models consistently outperform the baseline models, with the Self-Attended Cross-Modal FCRN Network achieving the highest GAP of 80.68%. However, the Attend-Fusion model achieves the F1 score of 75.64% with significantly fewer parameters (72M) compared to the best-performing baseline model, FC Late Fusion (341M parameters), which achieves an F1 score of 75.96%.\nAmong the baseline models, the FC Late Fusion achieves the best perfor-mance with a GAP of 80.87% and an F1 score of 75.96%. The FC Audio and FC Visual models, which rely on a single modality, perform significantly worse than the multimodal approaches, highlighting the importance of leveraging both audio and visual information for accurate video classification.\nOur proposed attention-based models demonstrate superior performance com-pared to baselines. The Self-Attended Cross-Modal FCRN Network achieves the highest GAP of 80.68%, while the Attend-Fusion model achieves the best F1 score of 75.64% with a much smaller model size (72M parameters). These results show the effectiveness of attention mechanisms in capturing relevant features and dependencies within and between modalities.\nQualitative Analysis To gain further insights into the performance of our proposed Attend-Fusion model, we conduct a qualitative analysis by examining its predictions on a set of representative examples from the YouTube-8M dataset. Figure 4 presents a comparison of the top-3 predictions made by our Attend-Fusion model, the state-of-the-art (SOTA) baseline, and the ground-truth labels for six different videos."}, {"title": "4.4 Ablation Studies", "content": "To further investigate the contributions of different components in our proposed models, we conduct ablation studies as shown in Table 2 and Table 3.\nTable 2 presents the ablation results for the best-performing baseline model, FC Late Fusion. We evaluate the impact of using only the audio or visual modal-ity. The results show that using only the audio modality (Audio Only) leads to a significant drop in performance, with a GAP of 50.32% and an F1 score of 49.09%. Similarly, using only the visual modality (Visual Only) also results in a performance decrease, with a GAP of 76.25% and an F1 score of 72.69%. These findings highlight the importance of multimodal fusion for achieving high classification accuracy.\nTable 3 presents the ablation results for our most efficient model, Attend-Fusion. We investigate the impact of removing the attention mechanism (No Attention) and using only the audio or visual modality (Audio Only and Visual Only). Removing the attention mechanism leads to a drop in performance, with a GAP of 79.06% and an F1 score of 73.31%. This demonstrates the effectiveness of attention in capturing relevant features and improving classification accuracy. Using only the audio or visual modality results in a significant performance"}, {"title": "5 Conclusion", "content": "The experimental results demonstrate the superiority of our proposed attention-based models for audio-visual video classification. By effectively leveraging the complementary information from both modalities and selectively attending to relevant features, our models achieve state-of-the-art performance on the chal-lenging YouTube-8M dataset.\nThe Attend-Fusion model stands out as a particularly promising approach, achieving competitive performance with a significantly smaller model size com-pared to the baselines. This efficiency makes it well-suited for real-world applica-tions where computational resources are limited, such as mobile devices or edge computing scenarios. These findings can guide future research in designing more efficient and accurate audio-visual video classification models.\nOverall, our work demonstrates the potential of attention-based models for tackling the challenging task of audio-visual video classification. By effectively leveraging the rich information present in both modalities and selectively attend-ing to relevant features, our models push the boundaries of video understanding and pave the way for more advanced and efficient approaches in this field."}]}