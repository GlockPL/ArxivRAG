{"title": "Wave-U-Mamba: An End-To-End Framework For High-Quality And Efficient Speech Super Resolution", "authors": ["Yongjoon Lee", "Chanwoo Kim"], "abstract": "Speech Super-Resolution (SSR) is a task of enhancing low-resolution speech signals by restoring missing high-frequency components. Conventional approaches typically reconstruct log-mel features, followed by a vocoder that generates high-resolution speech in the waveform domain. However, as log-mel features lack phase information, this can result in performance degradation during the reconstruction phase. Motivated by recent advances with Selective State Spaces Models (SSMs), we propose a method, referred to as Wave-U-Mamba that directly performs SSR in time domain. In our comparative study, including models such as WSRGlow, NU-Wave 2, and AudioSR, Wave-U-Mamba demonstrates superior performance, achieving the lowest Log-Spectral Distance (LSD) across various low-resolution sampling rates, ranging from 8 kHz to 24 kHz. Additionally, subjective human evaluations, scored using Mean Opinion Score (MOS) reveal that our method produces SSR with natural and human-like quality. Furthermore, Wave-U-Mamba achieves these results while generating high-resolution speech over nine times faster than baseline models on a single A100 GPU, with parameter sizes less than 2% of those in the baseline models.\nIndex Terms- Speech Super-Resolution, U-Net, Mamba, Generative Adversarial Networks", "sections": [{"title": "I. INTRODUCTION", "content": "Speech Super-Resolution (SSR), also referred to as Bandwidth Extension (BWE) is a process that aims to reconstruct High-Resolution (HR) speech from Low-Resolution (LR) speech. This field has gained significant attention due to the widespread availability of speech data recorded at low sampling rates, which can be attributed to hardware limitations, bandwidth constraints, and various other factors.\nSSR is a generative task that leverages the power of modern generation backbones such as diffusion probabilistic models [1, 2, 3], Generative Adversarial Networks (GAN) [4, 5], Generative Flow [6]. One way to approach SSR involves generating HR waveform directly from LR waveform [7, 8]. However, given that speech waveforms are characterized by significantly longer sequence lengths compared to mel spectrogram representations, conventional works first map the LR mel spectrogram to HR mel spectrogram and then project the recovered mel spectrogram onto a waveform through a well-defined vocoder [3, 4]. This two-step approach offers the advantage of decomposing the SSR task into simpler modules, akin to the traditional two-stage modeling employed in tasks such as Text-to-Speech (TTS) [9, 10, 11].\nDespite the promising results achieved by previous works, certain limitations remain. The mel spectrogram, being a feature-reduced representation, inherently discards phase information during the conversion process. This introduces an additional challenge of implicit phase reconstruction during the SSR process. Previous works incorporate a pretrained vocoder to solve the problem so that the SSR frameworks can only aim at recovering the high-frequency components. However, this approach prevents end-to-end training of the model and is inefficient, as the phase information could be preserved if the waveform were directly utilized.\nConsidering these factors, it is hypothesized that a model capable of directly generating HR waveforms from LR waveforms in an efficient and effective manner would offer a more optimal solution. In this paper, motivated by the success of selective SSMs [12] to extract long-term dependent information both effectively and efficiently [13, 14], we propose Wave-U-Mamba to solve SSR in waveform domain. Wave-U-Mamba is a GAN-based framework that takes LR speech as input, with sampling rates ranging from 4 kHz to 24 kHz, and up-samples it to HR speech with a target sampling rate of 48 kHz. Owing to the inherent capabilities of the Mamba architecture to efficiently process long sequences, coupled with its U-shaped model architecture that allows direct projection from LR to HR waveforms, the proposed model can generate HR waveforms much more rapidly than existing baseline models."}, {"title": "II. RELATED WORKS", "content": "A. Selective State Spaces Models\nSelective State Spaces Model, (commonly referred to as Mamba) offers significant advantages in capturing long-term dependencies when processing sequential data [12]. Utilizing hardware-aware parallel scan algorithms, Mamba serves as a memory-efficient and high-performance feature extractor, capable of maintaining global dependencies across feature-dense sequences [15]. Given the considerable length of raw waveforms, computational complexity becomes a critical consideration in feature extraction. Mamba's near-linear computational complexity with respect to sequence length positions it as an optimal feature extraction method for waveform data. Integrating Mamba with U-net [16] architecture has been explored in a variety of fields, including medical image segmentation [15, 17], image restoration [18].\nB. Time Domain Speech Processing\nAlthough time-frequency representations such as mel spectrogram, Mel-Frequency Cepstral Coefficients (MFCC) [19], and Power-Normalized Cepstral Coefficients (PNCC) [20] are traditionally employed to address speech-related tasks, time domain speech processing has been studied in various tasks including source separation [21, 22], representation learning [23], multimodal processing [24], speech generation [25, 26], and speech super-resolution [7]."}, {"title": "III. WAVE-U-MAMBA", "content": "Wave-U-Mamba is a GAN-based generative model that comprises a single generator and two discriminators. Our generator is a U-Net based architecture that takes raw waveform of LR speech as input and outputs HR speech waveform. A residual connection is employed every block to enhance stability. Especially the residual block at the final upsampling layer enables the model to focus on estimating the upper-frequency components, given that the lower-frequency features are already captured in the LR waveform. Fig. 1 illustrates the overall architecture of the Wave-U-Mamba generator. Building on the previous work [27], we utilize Multi-Period Discriminator (MPD) and Multi-Scale Discriminator (MSD), each comprising groups of sub-discriminators to perform adversarial training.\nA. Task Definition\nA LR speech signal has shape of $X_L = [x_1,...,x_{s\\times L}]$, s represents signal length in seconds, and L represents the sampling rate of the signal, where the upper-frequency limit is set to L/2 by Nyquist Theorem. A HR signal has shape of $Y_H = [y_1,...,y_{s\\times H}]$. H represents the sampling rate of the signal, where the upper-frequency limit is set to H/2. We first upsample the LR signal to match the sampling rate of the HR signal (i.e. $X_H = [x_1,...,x_{s\\times H}]$) using sinc interpolation. Then we parameterize and train the function F so that $Y_H = F(X_H)$.\nB. Generator\nDownsampling Enhancing long-range dependency is a critical challenge in feature extraction within the waveform domain since approaching distant features is more difficult compared to mel spectrogram. Previous studies have primarily focused on modifying the hyperparameters of convolutional neural networks (CNN) to extract long-term dependencies in waveform domain [25, 23]. However, these approaches often involve deepening network layers or expanding kernel sizes, leading to the loss of valuable information or increased computational complexity. To address this issue, we propose the incorporation of Mamba as a global feature extractor, due to its capability to efficiently and effectively learn long-range patterns. Specifically, we introduce the MambaBlock that integrates Mamba with Layer Normalization (LN) [28] and residual connection. We set M, the number of MambaBlocks to 2.\nMamba-based models are recognized for their difficulty in training, largely due to the sharp or occasionally non-convex nature of their loss landscapes [29]. To mitigate this challenge, and motivated by [30], we design a 1D version of stem embedding as an operator to map input representations into a high-dimensional latent space, facilitating more stable training of the Mamba model. A StemEmbedBlock is made of 1D convolution with a kernel size of 4 and stride of 1, followed by LN, LeakyReLU, and residual connections. In practice, we use N = 2 blocks as a function to double the feature dimension, which further stabilizes the training process. The hidden dimension at the bottleneck layer is set to 256, which is comparatively lower than that of other U-Net based SSR architectures [4, 7]. By constraining the hidden dimension size, the model effectively limits the propagation of extraneous patterns from the interpolated LR signal $X_H$, such as those generated by sinc interpolation, ensuring that only relevant and meaningful features are captured.\nTraditional approaches such as strided CNNs with padding for sequence length reduction, can introduce boundary artifacts and disrupt the continuous nature of the signal [22]. To address the problem, we use average pooling to halve the sequence length. In this way, we compress features that add to the naturalness of the generated HR signal. The bottleneck block follows the same structure as the DownsampleBlock but without pooling operation.\nUpsampling We employ transposed convolution as an up-sampling operator, in which kernel sizes are set as a multiple of strides to remove checkerboard artifacts common in transposed convolution [31]. MambaBlocks with the same architecture in down-sampling are utilized to capture global features. ResidualBlocks are put after Mambablocks to further improve the connectivity of representa-"}, {"title": "D. Training Objectives", "content": "Mel Spectrogram Loss For the natural and realistic SSR, it is crucial that perceptual quality be measured thoroughly. We therefore use L1-loss of the mel spectrogram of the predicted signal and the ground truth signal, where $\\psi$ refers to the mel transformation. In equation 1, Y and $\\hat{Y}$ refer to the ground truth HR speech waveform and the predicted waveform, respectively.\n$L_{mel} = E[||\\psi(Y) - \\psi(\\hat{Y})||_1]$\nMulti-Resolution Short-Time Fourier Transform (STFT) Loss\nSince SSR is a task that aims to reconstruct high-frequency components, we incorporate Multi-Resolution Short-Time Fourier Transform (STFT) Loss [34] as another loss term for the model to learn detailed characteristics of high-frequency bands in the spectrogram. Multi-Resolution STFT Loss is the sum of the STFT losses with different parameters (i.e.window size, hop length). Each STFT loss $L_s$ is the sum of spectral convergence loss and log STFT magnitude loss. It also helps prevent learning the patterns of specific hyperparameters used in STFT.\n$L_{STFT}(Y, \\hat{Y}) = E[L(Y, \\hat{Y})]$\nGAN Loss As originally illustrated in [27], we use the same formulations for adversarial loss for both the generator and discriminator.\n$L_{GAN}(D; G) = E [(D(x) - 1)^2 + (D(G(s)))^2]$\n$L_{GAN}(G; D) = E [(D(G(s)) - 1)^2]$\nFinal Loss Our final losses for the Wave-U-Mamba generator and discrminator are $L_G$ and $L_D$. $L_G$ is the weighted sum of Mel spectrogram loss, Multi-Resolution STFT loss, and the adversarial generator loss.\n$L_G = \\lambda_{mel}L_{mel} + \\lambda_{STFT}L_{STFT} + L_{GAN} (G; D)$\n$L_D = L_{GAN} (D; G)$\nWe set $\\lambda_{mel} = 45$ and $\\lambda_{STFT} = 10$. The coefficients were set to balance the values among loss terms. We did not use L1 Loss of"}, {"title": "IV. EXPERIMENTAL RESULTS", "content": "In this section, we describe the training settings and evaluation criteria used in our experiments. Then we visualize some sample mel spectrograms to help understand our results, which is followed by both objective and subjective score comparison of our models with other baselines, including WSRGlow [6], Nu-Wave 2 [1] and AudioSR [3]. Furthermore, we show the results of ablation studies to demonstrate characteristics of the design choices. Lastly, we show the efficiency measure of our model compared with other models.\nA. Training\nVCTK [35] is used as a dataset both for training and validation. The data is first cut into VCTK-train and VCTK-test, following the same data-splitting scheme from the previous work [7] then sampled at 48 kHz. We chunk about 0.7 seconds of data to use for training. We randomly set the cutoff frequency between 2 kHz and 12 kHz and then pass the data through an order-8 Chebyshev filter. The signal is down-sampled to a target sampling rate and then up-sampled back to 48 kHz to remove the high-frequency components completely. We normalize the signal onto [-1, 1] in training because our model has tanh activation on the last layer to predict the HR signal.\nEvery model is trained for 25 epochs with early stopping to prevent overfitting. AdamW is used as an optimizer with $\\beta_1 = 0.6$ and $\\beta_2 = 0.99$. The learning rate is initially set to 2e - 4 with warm-up and weight decay. Experiments were conducted under 2 RTX A6000 GPUs with batch size set to 64, which takes roughly 8 hours to train our model.\nB. Evaluation\nThe primary evaluation metric for objective evaluation is Log-Spectral Distance (LSD), illustrated in Equation 5. $Y(f,t)$ and $\\hat{Y}(f, t)$ refer to the magnitude spectrogram of the ground truth signal and the predicted signal. LSD can measure how well the signal has recovered the frequency components.\n$LSD(Y, \\hat{Y}) = \\frac{1}{T} \\sum_{t=1}^T \\sqrt{\\frac{1}{F} \\sum_{f=1}^F (log_{10} \\frac{|Y(f, t)|^2}{|\\hat{Y}(f, t)|^2})^2}$\nC. Results"}, {"title": "Objective Performance Comparison", "content": "We have conducted an objective comparison of the model performance with other State-Of-The-Art (SOTA) models using LSD. Table 1 shows that our model Wave-U-Mamba excels other models in terms of LSD, meaning that our model effectively captures high-frequency patterns. Additionally, our model exhibits a relatively consistent performance for audio chunks of any sampling rate. We can infer that this is due to the robustness of waveform representations whereas for other models input mel representations can have varying amounts of active pixels.\nAblation studies As presented in table 3, we conducted a series of experiments with various design configurations of the Wave-U-Mamba model, and then compared the LSD to better understand the functioning of some of the key components of the model. Wave-U-Mamba-deep refers to the same model architecture with the number of DownsampleBlock and UpsampleBlock increased to 5, which is set to 4 on default in Fig. 1. The dimension size at the bottleneck layer is also doubled. The resulting LSD of Wave-U-Mamba-deep is consistent with the idea that projecting LR signal into a relatively low-dimensional space is sufficient for accurate reconstruction. We also explored the impact of incorporating the L1 loss in the waveform domain in training. We observe that it introduced metallic artifacts in the reconstructed waveform. Lastly, removing MambaBlock from the model resulted in a marked degradation in performance, implying that Mamba is a crucial component of our model given that it functions as a global dependency extractor."}, {"title": "Subjective Performance Comparison", "content": "As seen from the table 2, MOS results suggest that our model has the best perceptual quality among all the other baseline models in improving LR speech signal to HR speech signal. Especially, our results have improved MOS scores of LR speech with a margin of 1.44.\nEfficiency Comparison To compare the efficiencies of the models, we compared model size in gigabytes (GB), mean inference speed, and the number of parameters as criteria. To calculate the inference speed, we first randomly sample files from VCTK, then estimate the mean inference time in milliseconds for processing one second of input speech using Monte-Carlo simulation with 50 iterations. Inference speed is measured using a single A100 GPU. As indicated in Table 2, our model contains fewer than 2% of the parameters of the baseline models and exhibits an inference speech 536.3 times faster than AudioSR."}, {"title": "V. CONCLUSION", "content": "In this work, we present Wave-U-Mamba, a fully end-to-end speech super-resolution model that operates in the waveform domain. The proposed model employs a U-structured architecture with generative adversarial networks for training. Wave-U-Mamba demonstrates the state-of-the-art performance on VCTK-test dataset, surpassing existing methods without reliance on additional models or pretrained weights. Furthermore, the model is capable of upsampling speech at any input resolution, from a sampling rate of 4 kHz to 24 kHz, to a high-resolution output at 48 kHz, consistently delivering high-quality results. The model's efficiency in upsampling low-resolution speech signals is mainly attributed to its simple architecture, allowing for both computational efficiency and superior performance."}]}