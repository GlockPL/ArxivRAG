{"title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare", "authors": ["Anudeex Shetty", "Amin Beheshti", "Mark Dras", "Usman Naseem"], "abstract": "Alignment techniques have become central to ensuring that Large Language Models (LLMs) generate outputs consistent with human values. However, existing alignment paradigms often model an averaged or monolithic preference, failing to account for the diversity of perspectives across cultures, demographics, and communities. This limitation is particularly critical in health-related scenarios, where plurality is essential due to the influence of culture, religion, personal values, and conflicting opinions. Despite progress in pluralistic alignment, no prior work has focused on health, likely due to the unavailability of publicly available datasets. To address this gap, we introduce VITAL, a new benchmark dataset comprising 13.1K value-laden situations and 5.4K multiple-choice questions focused on health, designed to assess and benchmark pluralistic alignment methodologies. Through extensive evaluation of eight LLMs of varying sizes, we demonstrate that existing pluralistic alignment techniques fall short in effectively accommodating diverse healthcare beliefs, underscoring the need for tailored AI alignment in specific domains. This work highlights the limitations of current approaches and lays the groundwork for developing health-specific alignment solutions.", "sections": [{"title": "1 Introduction", "content": "The advent of Large Language Models (LLMs) has revolutionised Natural Language Processing (NLP) (Zhao et al., 2023). While these models, trained on massive datasets, have shown remarkable capabilities, initial versions exhibited concerning issues like toxicity, hallucinations, and biases (Liang et al., 2023; Perez et al., 2022; Ganguli et al., 2022; Weidinger et al., 2021; Liu et al., 2023). Consequently, aligning LLMs with human values has become a central research focus (Ouyang et al., 2022; Bai et al., 2022a; Christiano et al., 2017; Gabriel, 2020). The impact of alignment is evident in the success of ChatGPT (OpenAI, 2024), highlighting its importance for safety, reliability, and broader applicability (Shen et al., 2023; Liu et al., 2023).\nDespite progress in alignment (Wang et al., 2023c; Ouyang et al., 2022; Stiennon et al., 2020; Christiano et al., 2017; Rafailov et al., 2024; Schulman et al., 2017), current methods often model average human values, neglecting the diversity of preferences across different groups (Sorensen et al., 2024b,a; Feng et al., 2024). As AI systems become increasingly prevalent, they must reflect this plurality (Sorensen et al., 2024b). Recent work has begun to address pluralistic alignment (Bai et al., 2022b; Gordon et al., 2022; Sorensen et al., 2024a) (as illustrated in Figure 1), recognising the risks of overlooking diverse opinions, particularly in sensitive domains like health where misinfor-"}, {"title": "2 Background and Related Work", "content": "LLM Alignment. Alignment techniques have been fundamental to the success of LLMs (Wang et al., 2023c). Initial alignment methods involved reward models informed by human preferences and feedback (Schulman et al., 2017; Christiano et al., 2017; Stiennon et al., 2020). Subsequent research has introduced several enhancements to these methods (Ouyang et al., 2022; Rafailov et al., 2024; Xia et al., 2024). However, such techniques are prone to aligning with average human preferences.\nPluralistic Alignment. Recognising the diversity of human values and preferences, Sorensen et al. (2024b) proposed a framework for pluralistic alignment to address these limitations. They defined three modes of pluralism in AI systems. Later work by Feng et al. (2024) introduced, ModPlural, a multi-LLM collaboration technique between main and community LLMs. While this demonstrated overall improvements, its performance in the health domain remains unexamined. Although some studies evaluate pluralistic alignment in various contexts (Liu et al., 2024; Benkler et al., 2023; Huang et al., 2024) or within specific alignment modes (Lake et al., 2024; Meister et al., 2024), none holistically assess all three pluralistic modes for healthcare. Prior research suggests that LLMs require domain-specific solutions (Zhao et al., 2023). With the growing use of LLMs in healthcare (Yang et al., 2023b; Thirunavukarasu et al., 2023), it is critical to benchmark and evaluate LLMs for pluralistic alignment in this domain."}, {"title": "3 VITAL Dataset", "content": "We present the first health-focused benchmark dataset specifically tailored for three modes of pluralistic alignment. It includes 13.1K value-laden situations and 5.4K multiple-choice questions (see Table 2). We undertake a meticulous and thorough benchmark construction process, including data collection, filtering, expert review, and analysis."}, {"title": "3.1 Dataset Construction", "content": "We begin by constructing a large-scale question bank, sourcing multiple-choice questions from a variety of survey and moral datasets (Liu et al., 2024; Durmus et al., 2023; Santurkar et al., 2023; Sorensen et al., 2024a). We concentrate on collecting diverse health scenarios\u2014some listed in Appendix Table 10\u2014characterised by their multiple perspectives and subjectivity, where we anticipate the most cross-value and perspective disagreement. Ultimately, we curate VITAL by filtering out questions and scenarios unrelated to health, lack diverse multiple opinions, or do not require action. It is accomplished through few-shot classification using the FLAN-T5 model (see prompts in Appendix A.1) (Carpenter et al., 2024; Parikh et al., 2023).\nWe transform these multiple-choice questions into benchmarks for evaluating pluralistic alignment in LLMs. Demographic information from surveys, alongside situational values, is used to investigate the steerability of LLMs. Similarly, country information from polls is leveraged to construct the underlying real-world distributions needed to evaluate the distributionality of the models. The ambiguous nature of moral scenarios provides an ideal basis for comparing the LLM's response distributions across various perspectives.\nWhile previous benchmarks and datasets have primarily focused on QA, we broaden the scope and enhance complexity by incorporating value-laden situations. We assess the overtness of models by ensuring they cover all human values. This blend of general text and questions within VITAL makes it a challenging and ideal benchmark for pluralistic alignment. Further details regarding the construction of VITAL are available in Appendix A."}, {"title": "3.2 Dataset Analysis", "content": "Lexical Analysis. We investigate lexical diversity within VITAL, aiming for diversity in both questions and situations to be diverse. This diversity is assessed by calculating the number and percentage of unique samples and n-grams as detailed in Table 3. The dataset exhibits high lexical diversity across both overall and alignment modes. Additionally, we visualise the entire dataset in Appendix Figure 6. Our analysis reveals that the curated dataset is predominantly composed of health-related terms."}, {"title": "Topic Analysis.", "content": "We conduct clustering on the samples to identify the range of themes captured. By employing agglomerative clustering, we summarise the samples within each cluster using GPT-40.  These summaries illustrate a variety of health topics. We observe that clusters encompass a combination of situations and multiple-choice questions. Conflicting samples within the same cluster and theme further underscore the diversity and complexity of VITAL as a health pluralistic alignment benchmark."}, {"title": "Relevance Analysis.", "content": "Despite LLMs demonstrating annotation performance comparable to human workers (Gilardi et al., 2023), we cautiously undertake human validation. In this context, we carry out a study where 10% of VITAL is labelled by humans to verify their health-related relevance. Human annotators identified samples in VITAL as health-related 80% of the time, with moderate agreement (Fleiss' Kappa: 0.49). The relevance of data in specific alignment modes within VITAL are similar: Overton at 80.5%, Steerable at 75.6%, and Distributional at 83.32%. Previous studies indicate that potential noise introduced by LLMs as annotators is mitigated by their ability for large-scale synthesis (West et al., 2022). Moreover, the multi-opinionated scenarios addressed pose challenges for human interpretation. Several samples initially marked as non-health-related\u2014such as instances like \"Smoking weed as an adult\u201d or \u201cSpanking my children\"\u2014could be argued as health-related due to their potential indirect implications."}, {"title": "4 Benchmarking", "content": "Using our proposed VITAL dataset, we extensively benchmark the current alignment techniques across a suite of models, investigating pluralistic alignment within healthcare."}, {"title": "4.1 Models", "content": "We evaluate the alignment techniques on a mix of eight proprietary and open-source models: Furthermore, both unaligned and aligned model variants are also evaluated. We utilise perspective and culture community LLMs from Feng et al. (2024) for the MoE and ModPlural alignment techniques."}, {"title": "4.2 Current Alignment Techniques", "content": "Vanilla: LLM is prompted directly with no special instruction. This helps us evaluate how the off-the-shelf model fares for pluralistic tasks.\nPrompting: Pluralism is added to the prompt by adding the instructions (detailed in Appendix Figure 8).\nMoE: Here, the main LLM acts as a router and selects the most appropriate community LLM for a given task. Then, the response from this community LLM is provided to the main LLM, using which the main LLM populates the final response (Feng et al., 2024).\nModPlural: The main LLM outputs the final response in collaboration with other specialised community LLMs depending on the pluralistic alignment mode (Feng et al., 2024). For Overton, the community LLM messages are con-"}, {"title": "4.3 Metrics", "content": "Our evaluation metrics align with those used in prior research (Sorensen et al., 2024b; Feng et al., 2024) for each mode. For Overton, we use an NLI model (Schuster et al., 2021) to calculate the percentage of values covered in various situations. Additionally, we incorporate human evaluations and LLM-as-a-Judge evaluations to assess responses. Specifically, we compare ModPlural responses against baseline responses to determine their reflection of pluralistic values. For Steerable, we check whether the final response maintains the steer attribute, quantified by calculating accuracy. For Distributional, we compare expected and actual distributions by measuring the Jensen-Shannon (JS) distance, where a lower value indicates a better alignment.\nMore experimental setting details can be found in Appendix B."}, {"title": "4.4 Results", "content": "Overton Alignment. From Table 7, we find that prompting consistently outperforms ModPlural across models. Interestingly, simple pluralism achieved through prompting yields superior results for health-related tasks compared to the more complex multi-LLM collaboration, ModPlural, as illustrated in Table 6. Of the models assessed, Gemma-7B aligns most closely with the Overton framework for health applications, delivering superior performance across all methodologies. It is observed that both prompting and vanilla LLMs surpass ModPlural across all eight models for both aligned and unaligned variants. Alarmingly, the coverage disparity between ModPlural and the best-performing method reaches a maximum of 55.5%. It indicates that community LLMs' messages fall short of fully covering the Overton windows within health contexts. Nevertheless, the effectiveness of health-specific Overton alignment remains below that of original ModPlural work as detailed in Feng et al. (2024). We perform further qualitative analysis and discern missing points in community messages in Appendix C."}, {"title": "4.5 Analysis", "content": "Is Overton evaluation biased by the number of sentences?\nThe NLI evaluation seems biased towards the number of sentences in the final answer, as illustrated in Table 8. Given that the NLI evaluation is conducted on a sentence-by-sentence basis, having a higher number of sentences can increase the likelihood of entailing a value. Furthermore, due to the summari-"}, {"title": "How does Distributional pluralism affect the LLM entropy?", "content": "Existing literature (Santurkar et al., 2023; Durmus et al., 2023; Sorensen et al., 2024b) has found that alignment reduces the entropy of the LLMs of output token distributions. For Distributional alignment, eventual low JS distance could be due to poor alignment and entropy decrease. For the latter, in this subsection, we measure the entropy values of different LLMs for Distributional mode of the VITAL in Table 9. Expectedly, the aligned variant has lower entropy than the unaligned model for each technique and model. However, unaligned models seem to have entropy similar to vanilla variants. Likewise, the ModPlural aligned models show significant improvement compared to other alignment techniques. Interestingly, entropy values are higher for smaller models compared to bigger LLMs. It suggests larger LLMs might be susceptible to shortcuts instead of better-aligned responses. In conclusion, we see a consistent pattern of reduced entropy post-alignment for the health domain."}, {"title": "Could we leverage modularity and patch health gaps in LLMs?", "content": "In this paper, we primarily focus on perspective community LLMs. However, we did a preliminary analysis using cultural community LLMs since there have been works considering alignment from multi-cultural views. We found performance to be similar with slight improvements.\nSimilarly, we leveraged health-specialised LLM as the main LLMs.  We observe no significant performance gains. This suggests that straightforward patching with specialised LLMs might not be an effective solution for specialised domains like health."}, {"title": "Can specialised community LLMs be replaced by LLM agents?", "content": "Considering the recent success of LLM agents, we investigate if on-the-fly role-assigned LLM agents could replace these specialised, fine-"}, {"title": "5 Conclusion", "content": "In this work, we investigate the LLM's potential to reflect diverse values and opinions (a.k.a pluralistic alignment), specifically within the health domain. The first step in improving the health AI systems is to evaluate how current solutions can model pluralistic views. We introduce a dedicated benchmark dataset, VITAL, focusing on health, derived carefully from a mix of value-laden and multiple-choice question corpora. Now, such a benchmark will help before deploying in the health and evaluating if it is safe. With this benchmark, we argue and provide empirical evidence that current alignment techniques may be limited (not representative) for pluralistic AI in the health domain, motivating the need for health-specific alignment techniques."}, {"title": "Limitations", "content": "It is important to note that ModPlural represents a general solution and does not specifically include health-related aspects-a focus that future studies should consider based on our findings. Regarding the comprehensiveness of VITAL, while we strive to include as many perspectives and values as possible in our benchmark, it is infeasible to encompass all principles and values. In the future, we will incorporate a broader range of perspectives to make the framework more holistic. For now, we release this smaller benchmark for the community to evaluate the alignment of the LLMs being deployed actively in the health domain. We plan to augment and expand this benchmark with more samples and other modalities, making it more comprehensive. Furthermore, we only benchmarked for the English datasets; in the future, we plan to expand this benchmark with multi-linguality."}, {"title": "Ethics Statement", "content": "To construct the VITAL dataset, we have leveraged a diverse range of existing datasets, which are central to our analysis of pluralistic alignment in health. Our use of these datasets adheres to accepted ethical standards and serves its intended purpose. Additionally, we acknowledge the potential risk of perpetuating stereotypes despite our efforts to enhance health alignment and reduce biases in LLMs. We will make VITAL openly available to further research in pluralistic alignment for health, NLP, and AI. VITAL is intended solely for research purposes and does not reflect the views of the authors. Through this benchmark dataset, we hope to promote a pluralistic, inclusive, and equitable representation of health viewpoints while consistently addressing biases to improve fairness."}, {"title": "A.1 VITAL Preparation Prompt", "content": "We construct few-shot classification prompts (Sorensen et al., 2024a) with a mix of Yes and No examples. These examples are selected by the authors from the original datasets."}, {"title": "A.2 Pluralistic Alignment Modes", "content": "Overton In this, there might be no single correct answer for a given query, and the LLM should cover all the reasonable values (or Overton2 win-dow) from VALUE KALEIDOSCOPE. Outputting a single reasonable answer might lead to biased responses, which might be unsafe in health-based applications. Furthermore, most applications of AI in health are for advice-giving, and current LLM responses are inconsistent but confident and opinionated (Kr\u00fcgel et al., 2023; Wang et al., 2023a).\nSteerable This ensures different user demographics, values, or frameworks are respected by the LLM. Otherwise, there is evidence of WEIRD bias in current LLMs for White, American, Wealthy, and Male perspectives (Hartmann et al., 2023; Santurkar et al., 2023). A crucial application of Steerable mode in healthcare is customisation. Patients interacting with a chatbot would benefit from personalised experiences, an approach actively used in mental health therapy (Sharma et al., 2023; Song et al., 2024). We leverage OPINIONQA along with VALUE KALEIDOSCOPE to evaluate whether the model steers to represent a specific demographic (from survey questions) and specific value respectively.\nDistributional This has applications in the cases where views of the population should be respected (Argyle et al., 2023). For example, localized country health LLM responds as per the underlying population distributions (Wang et al., 2023b; Li et al., 2023). The model output token probabilities should be [1, 0] for low ambiguity and [0.5, 0.5] for high ambiguity moral scenarios. Similarly, should correspond to the country in question for the global survey questions."}]}