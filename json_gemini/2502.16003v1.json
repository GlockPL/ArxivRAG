{"title": "Hierarchical Residuals Exploit Brain-Inspired Compositionality", "authors": ["Francisco M. L\u00f3pez", "Jochen Triesch"], "abstract": "We present Hierarchical Residual Networks (HiResNets), deep convolutional neural networks with long-range residual connections between layers at different hierarchical levels. HiResNets draw inspiration on the organization of the mammalian brain by replicating the direct connections from subcortical areas to the entire cortical hierarchy. We show that the inclusion of hierarchical residuals in several architectures, including ResNets, results in a boost in accuracy and faster learning. A detailed analysis of our models reveals that they perform hierarchical compositionality by learning feature maps relative to the compressed representations provided by the skip connections.", "sections": [{"title": "1 Introduction", "content": "The fields of artificial intelligence and neuroscience have been closely coupled since the conception of artificial neural networks (ANNs). One example of this coupling concerns the development of residual connections, which can be traced back to the pioneering work of McCulloch and Pitts [1], based on the early understanding of synaptic connectivity in biological neural networks. Residual connections, and in particular Residual Networks (ResNets) [2], are nowadays the staple for very deep ANNs because they can prevent the vanishing gradient and degradation problems by adding the activations from skip connections. ResNet-18 is commonly presented as a reasonable approximation of the visual cortex [3]. ResNeXt [4] is a popular variant which aggregates parallel processing paths. Residuals are also integral components of other state-of-the-art architectures such as Efficient Nets and Vision Transformers; alternatively skip connections can concatenate rather than add hidden layer activations, as done in DenseNets and U-Nets.\nCrucially, residual connections have also been found in the brains of insects [5, 6], rodents [7], and primates [8] \u2013 not only skipping single layers as in ResNets but also with long-range shortcuts from early processing subcortical areas to the entire cortical hierarchy. The role of these direct connections is not fully understood but it is hypothesized that they provide fast information transfer and enable compositionality [9, 10].\nIn this work, we extend ANNs on the basis of these neuroanatomical discoveries about long-range residual connections. Doing so requires dealing with connections at different levels of hierarchical abstraction, i.e. compression. We propose the use of hierarchical residuals: rather than learning unreferenced representations, the deeper layers of the network need to learn feature maps relative to a straightforward compression provided by simple convolution and pooling operations. Hierarchical Residual Networks (HiResNets) introduce a marginal number of additional parameters but result in higher accuracies and faster training times than ResNets and other popular models."}, {"title": "2 Hierarchical Residual Networks", "content": "Here we provide details about the formalism of HiResNets and the innovations introduced relative to other skip connections. The output G(x) of a residual connection is the sum of a weighted connection F(x) with a projection P(x):\n$$G(x) = F(x) + P(x).$$\nIf P is an identity mapping skip connection, i.e. P(x) = x, then only the residual function F(x) = G(x) - x relative to the input needs to be learned. This is the case for the basic block in ResNets, where F is typically a stacking of two convolutional layers. The other common projection function is a 1 \u00d7 1 convolution, which can account for mismatches in the number of channels between the input and output of a residual block.\nIn HiResNets, the same principles are followed to include long-range connections. First, a network is divided into blocks, within which the height, width, and number of channels are commonly preserved (see Fig. 1). We use hierarchical residual connections between blocks, such that the output of the l-th block becomes:\n$$G(x_{l}, x_{l-1}, x_{l-2}, ..., x_{0}) = F(x_{l}) + P_{1}(x_{l-1}) + P_{2}(x_{l-2}) + ... + P_{l}(x_{0}),$$\nwhere $P_{k}(x_{l-k})$ is a skip connection from the (l \u2013 k)-th to the l-th block. All projection connections include 1 \u00d7 1 convolutions and batch normalization, potentially preceded by an average pooling layer if the spatial dimensions of the inputs differ. This allows the residual function F to learn referenced representations relative to the straightforward compressions of the hierarchical projections. Because of the pooling and 1 \u00d7 1 convolutions, each of these projections introduces only $c_{l-k} \\times c_{l}$ new parameters, where $c_{l}$ is the number of channels of the block."}, {"title": "3 Experiments", "content": "To illustrate the benefits of using hierarchical residuals, we compare the performance of HiResNets with different baseline architectures: a plain deep network, a ResNet, and a ResNeXt with cardinality 4. All architectures have 18 layers of depth, based on the ResNet-18 [2]. Since HiResNets introduce additional trainable parameters, we also train adjusted versions of the baseline models by increasing the number of channels in the hidden layers to match the total number of parameters of the HiResNet.\nEach model is trained on classification tasks with CIFAR-10 and CIFAR-100 [11] and Tiny-ImageNet [12]. Tiny-ImageNet has pre-defined training, validation, and testing datasets. For CIFAR-10 and CIFAR-100, the default testing dataset is used and 10% of the training data is separated for validation. All models are trained on CIFAR-10 for 80 epochs, CIFAR-100 for 100 epochs, and TinyImageNet for 120 epochs. We use the Adam optimizer with a learning rate of 0.01, halved every 20 epochs, and batch sizes of 100 images.\nThe main results are presented in Table 1. We find that the HiResNet versions of all architectures outperform the baselines for all datasets. This advantage is most prominent when compared with the plain deep network, likely because the hierarchical residuals allow for the gradients to reach each layer of the network faster. The gains with respect to the ResNet and ResNeXt are moderate, since both models use short-range residuals. As suggested for ResNets [2], residual connections are most beneficial when used in very deep networks; the ResNet-18 baseline architecture is barely deep enough. Nevertheless, since the performance of the HiResNets is higher than the adjusted versions of the baseline architectures, this suggests that hierarchical skip connections are a more efficient way of increasing the number of parameters of a model than merely increasing the depth of the hidden layers."}, {"title": "3.2 Feature maps reveal hierarchical compositionality", "content": "Next, we investigate the origins of the performance of HiResNets. To do so, we explore the contributions of the residual and projection functions in the outputs of the three blocks. Fig. 2 shows the average absolute activations of all feature maps of each connection type for all images in the TinyImageNet test dataset. In all cases, the activations of the projections are approximately half of those of the residual functions. These results confirm that the outputs of the HiResNet are mostly driven by deeper connections provided by the residuals, but also that the model is exploiting combinations of the representations at the different levels to make its predictions, i.e. it is performing hierarchical compositionality. By way of example, at the output of the third block the hierarchical connections have comparable contributions among them, indicating that the residual is learning representations not only referenced by one of these projections but to all of them. Alternatively, these results can be interpreted as showing that the compression provided by the HiResNet projections is useful to backpropagate the loss, as it provides shortcuts for each layer when computing the gradient descent, which can quickly connect them to the error at the output layer."}, {"title": "3.3 Output hierarchical residuals are most beneficial", "content": "Finally, we evaluate the relative importance of the different hierarchical residuals by performing an ablation study. Eq. (2) represents the general case in which all possible projections are used, which we refer to as the default HiResNet. However, it is unlikely that all hierarchical residuals are equally important. We hypothesize that two variants of this architecture may provide interesting computational advantages inspired by the subcortico-cortical shortcut connections of the brain: the HiResNet-In introduces residuals from the first convolutional layer to all residual blocks; the HiResNet-Out introduces residuals from all residual blocks to the last one before the output.\nWe repeat the CIFAR-10 experiments using the ResNet as a baseline architecture with six alternative versions of our model: the default HiResNet, the HiResNet-In, the HiResNet-Out, and additionally HiResNet-P1, HiResNet-P2, and HiResNet-P3, which only include the first, second, and third hierarchical residuals, respectively. As shown in Table 2, the full HiResNet achieves the best performance. However, among the ablated versions, the HiResNet-Out is nearly as accurate as the full model, despite having fewer parameters. On the other hand, the P1 version achieves comparable accuracy to the ResNets, whereas the HiResNet-In and the P2 and P3 versions all produce lower performances. Therefore, we conclude that while all skip connections may be of use for the HiResNet, the residuals that connect the input to the output faster are most beneficial, likely because they enable better hierarchical compositionality. This will be of particular relevance if hierarchical residuals are used in larger models, where adding too many skip connections can result in an excessive increase in the amount of parameters."}, {"title": "4 Conclusion", "content": "In this work, we introduce the Hierarchical Residual Network (HiResNet), a brain-inspired architecture with long-range residual connections between blocks at different hierarchical levels. HiResNets can outperform ResNets and other ANNs, achieving higher classification accuracies and faster learning times, by exploiting hierarchical compositionality. These results reveal that long-range shortcut connections are a more efficient method to expand the number of parameters of a model than to increase the layer depth. Further experiments are required to determine whether hierarchical residuals are also beneficial in very deep networks, e.g. ResNet-101, and other classes of architectures, e.g. Vision Transformers, potentially by increasing the complexity of the shortcut connections. Furthermore, it remains to be seen whether the biological inspiration of the HiResNet translates into this model better reproducing hierarchical processing effects of biological neural networks, e.g. incongruences between low- and high-level features. This work shows that the coupling between artificial intelligence and neuroscience is still worth exploring, as there is much potential to be gained by drawing inspiration from biological brains."}]}