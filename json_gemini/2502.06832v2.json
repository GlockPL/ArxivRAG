{"title": "Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach", "authors": ["Xu Zhang", "Kaidi Xu", "Ziqing Hu", "Ren Wang"], "abstract": "Mixture of Experts (MoE) have shown remarkable success in leveraging specialized expert networks for complex machine learning tasks. However, their susceptibility to adversarial attacks presents a critical challenge for deployment in robust applications. This paper addresses the critical question of how to incorporate robustness into MoEs while maintaining high natural accuracy. We begin by analyzing the vulnerability of MoE components, finding that expert networks are notably more susceptible to adversarial attacks than the router. Based on this insight, we propose a targeted robust training technique that integrates a novel loss function to enhance the adversarial robustness of MoE, requiring only the robustification of one additional expert without compromising training or inference efficiency. Building on this, we introduce a dual-model strategy that linearly combines a standard MoE model with our robustified MoE model using a smoothing parameter. This approach allows for flexible control over the robustness-accuracy trade-off. We further provide theoretical foundations by deriving certified robustness bounds for both the single MoE and the dual-model. To push the boundaries of robustness and accuracy, we propose a novel joint training strategy JTDMOE for the dual-model. This joint training enhances both robustness and accuracy beyond what is achievable with separate models. Experimental results on CIFAR-10 and TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures demonstrate the effectiveness of our proposed methods.", "sections": [{"title": "1. Introduction", "content": "The Mixture of Experts (MoE) architecture has emerged as a powerful framework in machine learning, enabling models to capture complex patterns by combining the strengths of multiple specialized expert networks. Originally introduced to enhance model capacity without a proportional increase in computational cost, the MoE framework operates in a straightforward but effective way: different components of the model, known as experts, specialize in distinct tasks or features of the data (Jacobs et al., 1991; Jordan & Jacobs, 1994). MoE leverage a router to dynamically assign input data to the most appropriate expert (Shazeer et al., 2017). Such mechanism makes them particularly valuable in large-scale applications such as natural language processing (Du et al., 2022) and computer vision (Riquelme et al., 2021).\nDespite their success in achieving high accuracy, MoE, similar to other deep learning models, are vulnerable to adversarial attacks. Adversarial examples, which are input samples perturbed by imperceptible noise, can induce deep learning models to produce incorrect predictions with high confidence (Goodfellow et al., 2014; Carlini & Wagner, 2017; Papernot et al., 2016), posing significant risks in safety-critical applications (Finlayson et al., 2019; Li et al., 2023). The same modular structure that empowers MoE renders them particularly susceptible to adversarial attacks, as each expert presents a potential vulnerability. Our experiments in Section 4.1 confirm that targeting MoE experts can be an effective strategy for compromising the model, underscoring the need for enhanced robustness measures in MoE.\nAdversarial Training (AT) and its variants have been extensively researched to defend against adversarial attacks, attracting considerable research interest (Madry et al., 2017; Zhang et al., 2019; Wang et al., 2020a; Wong et al., 2020). However, these methods predominantly focus on standard architectures and don't directly address the unique challenges of MoE. The heterogeneous structure of MoE, comprising a router and multiple expert networks, complicates the application of traditional robustness techniques. A few existing works have explored adversarial robustness in MoE architectures (Puigcerver et al., 2022; Zhang et al., 2023). Nonetheless, they often fail to analyze the robustness of each component and typically only consider traditional AT or impose architectural restrictions. Moreover, the robustness enhancements in these works significantly degrade standard accuracy, limiting their practical utility."}, {"title": "2. Related Work", "content": "MoE has long been explored in the machine learning community as a method for tackling complex tasks by combining specialized expert networks (Jacobs et al., 1991). Each expert focuses on certain aspects of the data, and their outputs are combined through a weighted sum determined by a gating mechanism or router (Yuksel et al., 2012). A notable advancement is the sparsely gated MoE (Shazeer et al., 2017; Wang et al., 2020b; Riquelme et al., 2021; Fedus et al., 2022; Xue et al., 2022), which activates only a subset of experts based on a routing mechanism, allowing conditional computation and enabling models to scale parameters independently of computational cost (Patterson et al., 2021). This approach has been successfully applied in Natural Language Processing (Du et al., 2022; Lewis et al., 2021) and Computer Vision (Riquelme et al., 2021; Xue et al., 2022).\nDespite their widespread application, there has been limited research on the robustness of MoE, especially in adversarial settings.Small, carefully crafted perturbations, known as adversarial examples, can cause deep neural networks to"}, {"title": "3. Preliminaries", "content": "In this section, we introduce MoE architecture considered in this paper and outline how adversarial attacks can target different components of the MoE model.\nMoE Architecture. The MoE (Jacobs et al., 1991; Jordan &\nJacobs, 1994) is a neural network architecture that leverages multiple specialized sub-networks, known as experts, to improve modeling capacity and performance on complex tasks. A router (also referred to as a gating network) determines the contribution of each expert to the final prediction based on the input data. Formally, let $E$ denote the number of experts in the MoE model. Each expert is a function $f_i(x)$, where $i = 1, 2, \u2026\u2026\u2026, E$, and $x \\in \\mathbb{R}^d$ represents the input data. The router computes routing weights $a_i(x)$, which are typically non-negative and sum to one, i.e., $\\Sigma_{i=1}^E a_i(x) = 1$. The final prediction of the MoE model is given by a weighted sum of the experts' outputs $F(x) = \\sum_{i=1}^E a_i(x) f_i(x)$. In our setup, the router $a_i(x)$ is implemented using a fully connected layer, and each expert is a neural network tailored to capture specific aspects of the data.\nAdversarial Attacks on MoE Models. An adversarial attack on the entire MoE model seeks to find a perturbation $\\delta$ such that the model's output on the perturbed input $x + \\delta$ differs significantly from the output on the original input x or its ground truth label y: $F(x + \\delta) = \\Sigma_{i=1}^E\\alpha_i(x + \\delta) f_i(x + \\delta)$, where the $\\delta$ is usually generated by maximizing the loss function $\\underset{\\|\\delta\\|_p<\\epsilon}{\\arg\\max} L(F(x + \\delta), y)$ under the $l_p$ norm and the attack budget $\\epsilon$. We also consider loss terms that do not rely on y. Notice that the attack is on the whole MoE. We implement adversarial attacks focusing on specific components of the MoE model. An attack targeting the router aims to alter the routing weights without affecting the experts' outputs. The perturbation $\\delta$ is crafted by changing the router's decisions while the experts receiving the original input: $F(x + \\delta) = \\Sigma_{i=1}^E a_i(x + \\delta) f_i(x)$. An attack targeting the experts aims to alter the experts' outputs without changing the routing weights. $\\delta$ is designed such that: $F(x + \\delta) = \\Sigma_{i=1}^E a_i(x) f_i(x + \\delta)$. We remark that once the perturbation $\\delta$ is generated, it will be implemented on both the router and experts despite of its target."}, {"title": "4. Robust Training for Mixture of Experts", "content": "In this section, we aim to enhance the adversarial robustness of MoE. The key question we pose here is as follows.\n(Q1) Which part of MoE is most vulnerable to attacks\nand how should we robustify the critical part?\n4.1. Assessing the Robustness of MoE Components\nTo enhance the robustness of MoE, it is essential to understand which component is most susceptible to adversarial attacks. We begin by assessing the robustness of both the router and expert networks in MoE when subjected to adversarial attacks. We analyze the vulnerability of these components by evaluating a standard MoE model trained with the cross-entropy loss $\\underset{\\Theta_S}{\\arg\\min} \\mathbb{l}_{CE}(F_S(x), y)$, where $F_S(\\cdot)$ is the output of the standard MoE model $\\Theta_S$ that has only seen clean data during the training.\nTo specifically evaluate the accuracy and robustness of MoE, and the robustness of the router and the experts, we introduce four metrics: Standard Accuracy (SA) is the accuracy on clean test data; Robust Accuracy (RA) is the accuracy on adversarially perturbed test data generated by attacking the whole MoE; RA-E is the accuracy on adversarially perturbed test data generated by attacking the experts; RA-R is the accuracy on adversarially perturbed test data generated by attacking the router. The RA-E/RA-R reflects the experts'/router's ability to maintain correct predictions when subjected to adversarial perturbations.\nWe conduct a pilot study on CIFAR-10 and TinyImageNet datasets using MoE models with four experts. For CIFAR-10, we use ResNet18 as experts and set $\\epsilon$ = 8/255. For TinyImageNet, we use ViT-small as experts with $\\epsilon$ = 2/255. We employ a 50-step Projected Gradient Descent Attack"}, {"title": "4.2. Incorporating Robustness into MoE Architecture", "content": "To incorporate robustness into the MoE architecture, a straightforward approach is to apply traditional adversarial training (Madry et al., 2017), which involves training the model on adversarial examples generated during training. The traditional adversarial training is defined as:\n$\\underset{\\Theta_R}{\\min} \\underset{\\|\\delta\\|_p<\\epsilon}{\\max} \\mathbb{l}_{CE}(F_R(x + \\delta), y)$,\n(1)\nwhere $F_R(\\cdot)$ is the output of the robust MoE model $\\Theta_R$ under robust training. However, when applied to MoE models, traditional adversarial training exhibits several issues: (I1) Low Robust Accuracy: The final RA remains unacceptably low. In our experiments on CIFAR-10 with ResNet18 experts, the model achieves 79.08% SA and only 53.74% RA on the test set using PGD after 130 training epochs. (I2) Training Instability: The training process is unstable, with sudden drops in both SA and RA. As shown in Figure 2, both metrics decline sharply between epochs 80 and 90. This is due to the router selecting a different expert as the primary contributor when faced with adversarially perturbed inputs. Since standard adversarial training primarily focuses on optimizing the robustness of the overall model output,\nit may overlook individual experts' robustness, resulting in some experts remaining vulnerable to attacks. Consequently, when the router routes to one of these weaker experts, the model's robustness is compromised. These issues suggest that traditional adversarial training is not well-suited for MoE architectures due to the complex interplay between the router and expert networks.\nBased on our earlier assessment (in Section 4.1) that the experts are the most vulnerable components, we propose a new robust training approach that specifically targets enhancing the robustness of the expert networks. Our approach modifies the loss function to include a Kullback-Leibler (KL) divergence term that encourages the experts' outputs on adversarial examples to be similar to their outputs on clean inputs. We propose a Robust Training with Expert Robustification (RT-ER) approach:\n$\\underset{\\Theta_R}{\\min} \\mathbb{L}_{rob} = \\underset{\\|\\delta\\|_p \\leq \\epsilon}{\\max} [\\mathbb{l}_{CE}(F_R(x+ \\delta), y) + \\beta \\cdot l_{KL}(f_2(x + \\delta), f_2(x))]$, (2)\nwhere $f_2(x)$ denotes the output of the expert with the second-largest router weight for the adversarial example. The term $l_{KL}(f_2(x + \\delta), f_2(x))$ represents the KL divergence between the output of this expert on the adversarial example and its output on the clean input. RT-ER is compatible with various routing strategies. For instance, when the router employs a top- n strategy, RT-ER selects an additional expert-one not initially chosen by the router-based on the router weights, leveraging the router's output. We use KL divergence instead of cross-entropy because cross-entropy requires a well-defined ground truth for the target distribution, which is not applicable in this context. The hyperparameter $\\beta$ controls the trade-off between MoE-wide robustness and expert-specific robustness. For clarity, we refer to the expert with the second-largest router weight as the second-top expert in the following discussion.\nThe concept of RT-ER is illustrated in the right panel of Figure 1. By incorporating the KL divergence term, we explicitly encourage second-top expert to produce similar outputs for both clean and adversarial inputs. RT-ER offers several advantages: (A1) By penalizing deviations in experts' outputs due to adversarial perturbations, we strengthen the experts against attacks and thus contributes to better overall RA for the MoE model. (A2) The inclusion of the KL divergence term helps stabilize training by regularizing the experts' behavior. When second-top expert is also robust to adversarial inputs, the model maintains greater stability, even when the router activates different experts. This enhanced consistency reduces performance fluctuations under adversarial attacks, allowing the model to sustain stronger performance on adversarial samples. By ensuring all experts are robust, the model avoids scenarios where only a subset of experts is resilient, leaving others vulnerable. This uniform"}, {"title": "5. Dual-Model Strategy for Robustness and Accuracy", "content": "While our proposed RT-ER method enhances robustness against adversarial attacks (measured by RA), it may inadvertently degrade performance on clean data (measured by SA). This prompts the following research question:\n(Q2) How can we incorporate robustness into Mixture\nof Experts while minimizing the robustness-accuracy\ntrade-off?\n5.1. Dual-Model with Post-Training MoE\nTraditional MoE models are efficient but vulnerable to attacks, while robust MoEs withstand attacks but often reduce standard accuracy. We explore whether combining both models can balance robustness and accuracy. Let $F_S(x)$ be a standard MoE and $F_R(x)$ a robust MoE. The dual-model, $F_D(x)$, is defined as:\n$F_D(x) = (1 - \\alpha) \\cdot F_S(x) + \\alpha \\cdot F_R(x)$, (3)\nwhere $\\alpha$ controls the robustness-accuracy trade-off. In general, the standard MoE exhibits higher SA, while the robust MoE exhibits higher RA. The parameter $\\alpha$ controls the contribution of the robust MoE to the final prediction. As $\\alpha$ increases, the robust MoE contributes more to the final prediction, enhancing adversarial robustness but potentially degrading performance on clean data. We set $\\alpha \\geq 0.5$ to ensure that the dual-model remains robust.\nAlthough the dual-model structure helps balance performance on clean and adversarial data, it introduces additional complexity in understanding its robustness. To clarify the influence of each parameter on robustness, we derive a certified robustness bound for the dual model $F_D$.\nWe first provide the definition of certified robustness in MoE under the perturbation bounded by $l_p$ norm\u00b9\nDefinition 5.1. Consider a robust MoE $F_R : \\mathbb{R}^d \\rightarrow \\mathbb{R}^e$ and an arbitrary input $x \\in \\mathbb{R}^d$. Let $y = \\underset{i}{\\arg\\max} F_R(x)$, with bound $\\epsilon \\geq 0$. We say $F_R$ is certifiably robust at x with bound $\\epsilon$ if for all $k \\neq y$ and $\\delta \\in \\mathbb{R}^d$ such that $\\|\\delta\\|_p \\leq \\epsilon$, the following holds:\n$F_R^{(y)}(x + \\delta) \\geq F_R^{(k)}(x + \\delta)$ (4)\nThis definition formalizes the certifiable robustness of $F_R$ at x, ensuring that the model's top prediction remains consistent under perturbations within an $l_p$-norm ball of radius $\\epsilon$. For practical classifiers, the robust margin $F_R^{(y)}(x + \\delta) -\nF_R^{(k)}(x + \\delta)$ can be estimated by evaluating the confidence gap between predicted and runner-up classes on a strong adversarial input.\nDefinition 5.2. A function $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is $l_p$-Lipschitz continuous if there exists $L \\in (0,\\infty)$ such that for all $x', x \\in \\mathbb{R}^d$, $|f(x') - f(x)| < L||x' - x||_p$. The Lipschitz constant of $f$ is defined as\n$\\text{Lip}_p(f) := \\inf{L : |f(x') - f(x)| \\leq L||x' - x||_p}$ (5)\nFollowing the definition of $l_p$-Lipschitz continuity, we can introduce the following assumption:"}, {"title": "5.2. Boost Dual-Model with Joint Training", "content": "Inspired by Theorem 5.5 and the insights from Section 5.1, we explore the joint training strategy that considers both the dual-model and single components. By jointly training, we aim to increase the robust MoE's margin, thereby boosting both robustness and accuracy. Traditional adversarial training methods focus primarily on the overall model robustness, often overlooking the individual robustness of each MoE component, making these methods less effective in improving MoE-specific robustness.\nTo address this gap, we introduce a novel adversarial training framework for the dual-model based on bi-level optimization. It provides a hierarchical learning approach, where the upper-level objective depends on the solution of the lower-level problem. Specifically, it is formulated as follows:\n$\\underset{\\Theta_S,\\Theta_R}{\\min} \\mathbb{l}_{CE}(F_D(x), y) \\quad \\text{subject to} \\quad \\underset{\\Theta_R}{\\min} \\mathbb{L}_{rob}$, (10)\nwhere $\\mathbb{L}_{rob}$ denotes the proposed loss function in Equation (2). Here, the dual-model parameters are divided into variables $\\Theta_R$ for the robust MoE and variables $\\Theta_S$ for the standard MoE. We term this new approach Joint Training for Dual-Model based on MoE (JTDMoE). JTDMoE promotes alignment between the standard MoE and robust MoE, ultimately enhancing the dual-model's performance. The complete training process is outlined in Algorithm 1.\nThe reason we use this bi-level alternating training approach is to enhance the dual-model's accuracy on clean data while maintaining its robustness as much as possible. For the lower-level optimization problem, we use RT-ER to robustify the robust MoE, as the dual-model's robustness derives from the robust MoE (Theorem 5.5). This approach leads to smaller Lipschitz constants for the experts and router,"}, {"title": "6. Experiments", "content": "In this section, we present the effectiveness of our proposed RT-ER and the JTDMoE approach.\n6.1. Experiment Setup\nDatasets and Model Architectures. Our experiments use the MoE architecture on CIFAR-10 (Krizhevsky et al., 2009) and TinyImageNet (Deng et al., 2009), with a fully connected top-1 router and 4 experts (E = 4). We use ResNet18 (He et al., 2016) for CIFAR-10 and pre-trained ViT-small (Liu et al., 2021) for TinyImageNet. Instead of training the dual-model from scratch, we apply the JTD-MoE algorithm to pre-trained models for efficiency, using ST MOE as the standard and RT-ER MOE as the robust MoE, with a ranging from 0.5 to 1.\nRobustness Evaluation. We use PGD (Madry et al., 2017) and AutoAttack (Croce & Hein, 2020) to assess model performance under adversarial conditions, with $\\epsilon$ = 8/255 for CIFAR-10 and $\\epsilon$ = 2/255 for TinyImageNet. We train ResNet18-based MoE for 130 epochs on CIFAR-10 and fine-tune pre-trained ViT-small-based MoE for 10 epochs on TinyImageNet. A Cyclic Learning Rate strategy (Smith, 2017), starting at 0.0001, and data augmentation (Rebuffi et al., 2021) are used to enhance performance. Evaluation is done using either a 50-step PGD or AutoAttack with the same step size.\nBaseline Methods. For comparative analysis, we define three approaches for the single MoE: (1) ST: Standard training on MoE. (2) AT: Adversarial training on MoE (Madry, 2017). (3) RT-ER: Robust training with expters robustification on MoE."}, {"title": "6.2. Evaluation of RT-ER", "content": "To showcase the improved robustness and training stability of RT-ER, we compare its SA and RA to those of AT during training. The experimental results of MoE-Resnet18 on CIFAR-10 dataset are illustrated in Figure 2. RT-ER demonstrates greater stability compared to traditional adversarial training (AT). In the case of AT-trained MoE, both SA and RA drop significantly-by over 20% and 10%, respectively-between epochs 80 and 90. Although the ViT-small MoE model shows some fluctuations as well, the variation is less pronounced, due to the robustness of the pre-trained ViT-small experts used. The experimental results of ViT-small are presented in Appendix A.2. In terms of RA, our method consistently outperforms AT across all epochs, achieving over 15.35% and 5.5% improvements in final performance on CIFAR-10 and TinyImageNet, respectively. This suggests that RT-ER is a more effective approach for enhancing the robustness of MoE architectures. Overall, our method enables faster, more efficient robustness enhancement for MoE models while reducing SA by < 1.3%.\nIn the Single MoE Performance section of Table 3, we compare ST, AT, and RT-ER using ResNet18 on CIFAR-10 and ViT-small on TinyImageNet under PGD attack. RT-ER improves RA-E by over 70% and 17% on ResNet18 and ViT-small, respectively, compared to ST, and achieves 5.8% and 4.2% improvements in RA and RA-E over AT using ViT-small. These results show RT-ER's effectiveness in robustifying single MoE models. For a single MoE, robustness is measured by the minimum of RA, RA-E, RA-R, reflecting the worst-case performance. RT-ER outperforms"}, {"title": "6.3. Evaluation of the Dual-Model Based on Pre-Trained MoE", "content": "The pre-trained dual-model combines a standard MoE and a robust MoE with a fixed $\\alpha$ to improve clean data performance. On CIFAR-10, the standard MoE achieves 92.14% SA and 52.54% RA, while the robust MoE reaches 77.81% SA and 69.09% RA. On TinyImageNet, the standard MoE achieves 82.05% SA and 34.53% RA, and the robust MoE reaches 68.51% SA and 56.79% RA. As $\\alpha$ increases, SA decreases but RA improves, as the robust MoE's contribution grows. When $\\alpha$ = 1, the dual-model becomes RT-ER, with only the robust MoE. Results for $\\alpha$ = 0.7, 0.8, and 0.9 under PGD attack show that increasing $\\alpha$ decreases SA (by 6.54% and 13.54% on CIFAR-10 and TinyImageNet, respectively) but improves RA (by 2.92% and 3.47%, respectively) and RA-SMoE (by 7.94% and 15.41%).\nSpecifically, when adversarial perturbations target the robust MoE, they have minimal effect on the standard MoE, so RA-RMoE decreases as $\\alpha$ increases. Conversely, when the standard MoE is attacked, the robust MoE is minimally impacted, leading to an increase in RA-SMoE with $\\alpha$. However, when $\\alpha$ shifts from 0.5 to 0.7, RA-SMoE shows only slight improvement, suggesting the robust MoE lacks confidence in its predictions and can be influenced by the standard MoE. This motivates our joint training approach to improve dual-model performance. Further analysis of the"}, {"title": "6.4. Evaluation of JTDMOE", "content": "To compare with the results in Section 6.3, we use the same standard MoE and robust MoE models for $\\alpha$ = 0.7. As shown in Table 3 and Table 4, our JTDMoE algorithm significantly improves the performance of the pre-trained dual-model across all evaluation metrics. Specifically, it increases SA by 2.48% and 4.17%, and RA by 6.81% and 4.28% under PGD attacks for CIFAR-10 and TinyImageNet, respectively. When evaluated with AutoAttack, JTDMOE achieves a 19.91% improvement in RA for CIFAR-10 and 1.64% for TinyImageNet. Additionally, JTDMOE outperforms the"}, {"title": "7. Conclusion", "content": "Our study presents a framework for enhancing adversarial robustness and accuracy in MoE architectures, focusing on vulnerable expert networks and strategies that balance both, supported by theoretical analysis and empirical validation."}, {"title": "8. Impact Statement", "content": "This paper presents work aimed at advancing the field of Machine Learning, specifically in improving the adversarial robustness of Mixture of Experts (MoE) models. Our contributions enhance the security and reliability of MoEs, which are widely used in large-scale and specialized AI applications. While this research primarily focuses on technical advancements, its broader implications include improving the deployment of MoE-based models in safety-critical domains. However, we do not identify any specific societal consequences that must be highlighted here."}, {"title": "A. Supplementary Material", "content": "In this supplementary material, we provide the proofs of Theorems 5.4 and 5.5 in Section A.1. Additional experimental results are presented as follows: single MoE experiments in Section A.2, dual-model experiments based on pre-trained MoE in Section A.3, and JTDMoE experiments in Section A.4."}, {"title": "A.1. Proof of Key Theorems", "content": "Proof of Theorem 5.4: When the input is perturbed from x to x + $\\delta$, the change in the final output of the MoE can be expressed as:\n$\\Delta = F^{(y)}(x + \\delta) - F^{(y)}(x)$\n$= [a_R_i(x + \\delta) f_R^{(y)}(x + \\delta) - a_R_i(x) f_R^{(y)}(x)]$\nWe decompose $\\Delta$ into two terms:\n$\\Delta_1 = \\sum(a_R_i(x + \\delta) ) - a_R_i(x)) f_R^{(y)}(x + \\delta)$\n$\\Delta_2 = \\sum a_R(x) (f_R^{(y)}(x + d) - f_R^{(y)}(x))$\n$\\Delta = \\Delta_1 + \\Delta_2$\nBy this decomposition, $\\Delta_1$ captures the change due to the router's output, while $\\Delta_2$ represents the change due to the experts' outputs. To derive a certified bound for the overall MoE, we need to bound both $\\Delta_1$ and $\\Delta_2$.\nBounding $\\Delta_1$:\n$\\Delta_1 \\leq \\sum a_R_i(x + \\delta) - a_R(x).f f_R^{(y)}(x + \\delta)\n< \\epsilon R \\|\\delta\\|_MR$\nwhere $M_R < 1$ is an upper bound  on $f_R^{(y)}(x + $\\delta) or an upper bound on $f_R^{(y)}(x)$ for any inputs and $r_R_i$ is the Lipschitz constant of the router $a_R_i$.\nBounding $\\Delta_2$:\n$\\Delta_2 \\leq \\sum a_i(x) f_R^{(y)}(x + \\delta) - f_R^{(y)}(x)$\n< \\sum a_R (x) L_R \\|\\delta\\|$\nwhere $L_R_i$ is the Lipschitz constant of the expert $f_R^{(y)}$.\nBounding $\\Delta$: Combining the bounds for $\\Delta_1$ and $\\Delta_2$, we have:\n$\\Delta \\leq \\sum (P_R_iM_R + a_R_i(x)L_R )\\|\\delta\\|$"}]}