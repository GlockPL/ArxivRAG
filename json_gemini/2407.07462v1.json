{"title": "MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions", "authors": ["Felix Fent", "Fabian Kuttenreich", "Florian Ruch", "Farija Rizwin", "Stefan Juergens", "Lorenz Lechermann", "Christian Nissler", "Andrea Perl", "Ulrich Voll", "Min Yan", "Markus Lienkamp"], "abstract": "Autonomous trucking is a promising technology that can greatly impact modern logistics and the environment. Ensuring its safety on public roads is one of the main duties that requires an accurate perception of the environment. To achieve this, machine learning methods rely on large datasets, but to this day, no such datasets are available for autonomous trucks. In this work, we present MAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN TruckScenes allows the research community to come into contact with truck-specific challenges, such as trailer occlusions, novel sensor perspectives, and terminal environments for the first time. It comprises more than 740 scenes of 20 s each within a multitude of different environmental conditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2 IMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually annotated and carefully reviewed to achieve a high quality standard. Bounding boxes are available for 27 object classes, 15 attributes, and a range of more than 230 m. The scenes are tagged according to 34 distinct scene tags, and all objects are tracked throughout the scene to promote a wide range of applications. Additionally, MAN TruckScenes is the first dataset to provide 4D radar data with 360\u00b0 coverage and is thereby the largest radar dataset with annotated 3D bounding boxes. Finally, we provide extensive dataset analysis and baseline results. The dataset, development kit and more are available online.", "sections": [{"title": "1 Introduction", "content": "Autonomous trucking has the potential to fundamentally change today's traffic by increasing safety on public roads, reducing logistics costs, and counteracting the shortage of drivers [2, 13]. However, the safe and reliable operation of autonomous trucks depends on an accurate perception of the surroundings. To achieve this, modern self-driving vehicles rely on machine learning algorithms to detect, track, and predict surrounding objects. However, the use of machine learning methods also drives the need for large-scale datasets.\nWhile numerous datasets for autonomous passenger cars exist [11], datasets for autonomous trucks are missing. However, heavy-duty vehicles have their own unique challenges. Large vehicles require different sensor mounting positions and rely on multiple sensors to cover the entire surrounding area. Moreover, trucks have to contend with occlusions from their own vehicle that change dynamically due to a movable truck-trailer combination and are affected by relative movements between their chassis and cabin. Furthermore, long-haul trucks operate in inherently different surroundings, such as logistics or container terminals. Therefore, a dedicated truck dataset is needed to develop reliable perception solutions for self-driving trucks.\nTo address this research gap and accelerate the development of self-driving trucks, we present MAN TruckScenes, the first large-scale dataset for autonomous trucking. Our multimodal dataset comprises data from a state-of-the-art sensor suite, including multiple high-resolution cameras, lidar, and radar sensors to provide full coverage of the surrounding area. Especially, the inclusion of six 4D radar sensors makes it the largest radar dataset available. In addition, the data of a high-precision GNSS and two IMU units are included to support a multitude of applications.\nMoreover, most existing AV datasets are restricted to a particular operational design domain, whereas the MAN TruckScenes dataset covers a large geographical area, three seasons, nighttime driving, and numerous different weather conditions, including fog, rain, and snow. Furthermore, the dataset includes scenes in logistics terminals and recordings of vehicles with high relative velocities on the German Autobahn, making it the first dataset to promote the unique challenges of long-haul trucks.\nIn order to support the development of deep learning methods, MAN TruckScenes provides high- quality annotations for 747 scenes. These annotations consist of manually annotated 3D bounding boxes with unique instance identifiers for object tracking, standardized scene tags, and attribute labels to indicate the object's state. The dataset annotation was subject to a multi-stage labeling and quality assurance process of specially trained labelers to ensure accurate and consistent labeling.\nThe dataset will be published alongside a development kit, evaluation code, taxonomy, and detailed annotation instructions. This ensures transparency and reproducibility and simplifies the use of the dataset. Furthermore, we build on the established nuScenes [3] data format to ensure easy integration and code compatibility. Finally, MAN TruckScenes is published under the CC BY-NC-SA 4.0 license to accelerate research on autonomous trucks and shape the future of logistics."}, {"title": "2 Related Work", "content": "The advances in autonomous driving have largely been driven by the release of public datasets. Over the course of the last decade, we have seen a trend towards high-quality, large-scale, and more diverse multimodal datasets that enabled innovation in the autonomous driving domain.\nThe KITTI [7] dataset, released in 2012, is one of the most influential autonomous driving datasets to date. It provides 22 scenes of annotated camera and lidar data in combination with high-precision GNSS and IMU data. However, the dataset's extent and diversity are limited, radar and map data are not provided, and driving data under severe weather conditions are not included.\nThe nuScenes [3] dataset had a great impact as one of the first large-scale, multimodal datasets for autonomous driving. Next to camera and lidar data, it also includes radar and map data for 1000 annotated scenes. It provides data from two different cities, annotates 23 different object classes, and introduces unlabeled intermediate sensor data sweeps. In addition to the small geographical coverage and limited coverage of severe weather conditions, the nuScenes dataset has been criticized for its sparse radar data [6, 12].\nThe Waymo Open [15] dataset is one of the largest annotated datasets for autonomous driving with a focus on scalability. It provides annotated camera and lidar data for 1150 scenes, covers a geographical area of 76 km\u00b2, and provides a rich ecosystem of different related datasets. However, it does not include radar, GNSS, or map data and has a limited annotation range of 80 m with only four annotation classes.\nThe Argoverse 2 [16] dataset takes a different approach with the provision of long-range annotations, comprehensive map data, and extensive object taxonomy. Nevertheless, it does not provide radar, GNSS, or IMU data and covers a limited geographical area.\nThe Zenseact Open Dataset (ZOD) [1] makes a different set of trade-offs. It provides 1473 annotated scenes from six different countries and facilitates long-range perception with an annotation range of 245 m. Despite its provision of two GNSS and IMU units, the ZOD's sensor setup, as well as its number of annotated samples, is limited.\nWhile there are numerous datasets focusing on various different aspects [11], all of the aforementioned datasets are limited to passenger cars, as shown in Table 1. To the best of our knowledge, there are only two datasets that include truck data. One is the TuSimple [18] dataset, which consists of 6408 images from a single front camera and provides annotations for lane markings only. The other is the SurMine [14] dataset, which is a proprietary dataset that only includes 2D bounding box annotations"}, {"title": "3 Dataset", "content": "MAN TruckScenes aims to close this research gap by providing the first large-scale multimodal dataset for autonomous trucking. It consists of 747 scenes from typical long-haul truck environments and provides multimodal data with long-range annotations based on the nuScenes [3] format."}, {"title": "3.1 Sensor Setup", "content": "The sensor suite consists of a multimodal sensor setup with four cameras, six lidar, and six radar sensors. Additionally, the dataset provides high-precision RTK-GNSS data and measurements of two IMU units. Detailed information on the sensor specifications can be found in Table 2 and the sensor positions are shown in Figure 2.\nThe main perception sensors are arranged in two sensor modules, one on either side of the vehicle, to maximize spatial coverage and minimize relative sensor movements. Each sensor module houses two Sekonix cameras, one Hesai lidar, and three Continental radar sensors. Besides these two \"corner modules\", the vehicle is equipped with three Ouster lidar sensors mounted on the roof of the cabin, which are tilted downwards for blindspot coverage, and one additional Ouster lidar at the rear of the semi-trailer truck. The top mounted lidar sensors and the lidar sensors of the corner modules are positioned at a height of 3.2 m and 2.2 m, respectively. These elevated positions help to reduce occlusions, protect pedestrians, and prevent sensor damage. However, this sensor perspective marks a significant difference from conventional passenger car datasets.\nAnother unique feature of our dataset is the inclusion of six 4D radar sensors with a spatial coverage of nearly 360\u00b0 (except from the occlusion by the ego vehicle's trailer). In contrast to conventional"}, {"title": "3.2 Sensor Synchronization", "content": "Sensor synchronization is an important topic for multimodal datasets to ensure temporal sensor data alignment. This includes not only sensor time synchronization, which ensures that all sensors are based on the same reference clock, but also triggering the sensors to achieve cross-modality consistency.\nThe sensor time synchronization is based on the Precision Time Protocol (PTP) in accordance with the IEEE/IEC 1588-2008 [8]. This procedure ensures the alignment of the individual sensor clocks with a high-precision Mobatime DTS 4160 PTP grandmaster clock. As a result, the individual sensors have a time deviation of less than 100 us and are referenced to the global UTC time. However, this only ensures that all sensors are based on the same reference clock, but not that all measurements are taken at the exact same point in time.\nTo temporally align the actual sensor measurements, the sensors have to be triggered. For this purpose, the lidar sensors are defined as reference sensors and all other perception sensors are triggered based on them. In the first step, all cabin-mounted lidar sensors are phase-synchronized such that their lidar points are temporally consistent in a clockwise manner. Secondly, the four cameras are triggered at the point where the lidar sweep and the rolling shutter of the cameras align in the center of the image. Lastly, the radar sensors are synchronized with the corresponding lidar sensors but triggered with a small delay to one another to minimize interference between them. Besides that, all radar sensors are assigned to slightly different frequency bands to further minimize interference."}, {"title": "3.3 Sensor Calibration", "content": "While sensor synchronization ensures the temporal alignment of the sensor data, sensor calibration ensures the spatial alignment. The calibration aims to determine both the extrinsic and intrinsic sensor parameters to estimate both the sensor poses as well as the parameters of the sensor models.\nThe sensor calibration consists of four steps and includes an initial extrinsic calibration, a combined extrinsic and intrinsic calibration, an angular correction, and a final validation. The first step is a photogrammetric scan of the vehicle to determine the position and orientation of the sensors with respect to the vehicle frame. The vehicle frame is chosen to be the center of the rear axle projected onto the ground plan in accordance with ISO 8855:2011 [10]. For the main calibration, the vehicle is placed in a dedicated calibration hall equipped with specialized calibration targets. These targets are represented by differently oriented planes in 3D space equipped with unique identifiers. The actual calibration uses a plane-matching algorithm to jointly estimate the extrinsic parameters of the camera and lidar sensors as well as the intrinsic camera parameters. Within this calibration procedure, the vehicle is moved back and forth to calibrate the sensors with respect to the vehicle frame. The third step uses a faraway calibration target aligned with the vehicle's longitudinal axis to correct remaining yaw angle errors and a dynamic landmark-based calibration to align the heading angle of the overall sensor setup. Finally, the calibration is validated on the data level by comparing point cloud alignments and projecting the point cloud data onto the image data. Moreover, an application-level validation is used to compare detections, odometry, and localization measurements to validate the calibration."}, {"title": "3.4 Sensor Data", "content": "The provided sensor data was going through different processing steps to ensure compatibility with the nuScenes [3] format and enhance usability. The properties of the five different sensor data types are explained in the following.\nThe camera data of the four camera sensors undergo an undistortion process using a pinhole camera model and a Lanczos interpolation scheme with a 8 \u00d7 8 kernel and constant padding values. The resulting image is cropped to a 1980 \u00d7 943 pixel size and stored as a compressed JPEG [9] image. The Camera data is sampled at a frequency of 10 Hz in correspondence with the lidar.\nThe lidar data is provided as point clouds of a variable number of points represented by their x, y, and z-coordinates in a cartesian sensor coordinate system, an intensity value, and an individual UNIX timestamp, allowing for point-wise motion compensation. The final point clouds are stored as binary (Marc Lehmann's LZF) compressed data in the point cloud data (pcd) format to save on disc space.\nThe radar data is also represented as point clouds where each point is defined by its x, y, and z- coordinates, its relative radial velocity in x, y, and z-direction as well as its radar cross section (rcs). In contrast to the lidar data, the radar data is stored as pcd files in a binary format due to its smaller file size.\nThe IMU data includes the current velocity and acceleration in x, y, and z-direction as well as the angle and angular velocity (rate) in roll, pitch, and yaw. The data is stored in a JSON file. The GNSS data provides the vehicle's position in UTM-WGS84 coordinates mapped to cell U32 and orientation given as quaternion (qw, qx, qy, qz). The GNSS data off all timestamps is stored in a JSON file. IMU and GNSS data are sampled with a frequency of 100 Hz to provide the possibility for highly accurate motion estimation.\nBesides that, the rear lidar is limited to an FoV of 180\u00b0, which increases its detection range to 42 m at 10% reflectivity, and the points of the top mounted lidar sensors are cut off for yaw angles beyond +120\u00b0. It is also worth mentioning that the sensor data is not just provided at the sample annotation frequency of 2 Hz but at their individual sensor captioning frequencies listed in Table 2. These unannotated intermediate frames are denoted as sweeps. It is also important to note that the radar sensors cannot guarantee a fixed sampling rate of 20 Hz but rather drop frames, if their internal data processing takes too long, which results in an average data rate of 19.6 Hz."}, {"title": "3.5 Scene Selection", "content": "The scene selection aims to select a diverse set of scenarios with challenging driving situations representing long-haul trucks' operational design domain. To meet this goal, 747 scenes with approximately 20s each are manually selected from more than 25 h of measurement drives. The scenes include recordings from various different areas (e.g. highway, terminal, rural, city), three seasons of the year, challenging weather situations (e.g. rain, fog, snow), and difficult environmental conditions (e.g. nighttime, twilight). Furthermore, the dataset covers different driving maneuvers (e.g. overtaking, offloading), traffic scenarios, and observations of rare object classes (e.g. animals, emergency vehicles). As a result, the dataset includes rainy nighttime drives that cause mirroring effects in the lidar point cloud, recordings in a logistics terminal with a lot of metal shipping containers which lead to multipath reflections in the radar point cloud, and tunnel passages with strong illumination changes that effect the camera images. Therefore, the dataset provides challenging conditions for all sensor modalities to promote research in the area of robust perception. The distribution of the different scene tags is shown in Figure 3"}, {"title": "3.6 Data Annotation", "content": "The data of the selected scenes is manually annotated at 2 Hz based on a fused and ego motion- compensated lidar point cloud aggregation. To ensure a high-quality standard, independent annotation and quality assurance companies have been commissioned. Within the labeling process, each sample undergoes a maximum of three consecutive annotation and quality assurance cycles until the quality target is met. Furthermore, randomly selected samples pass through a dual-control cycle to validate the annotation quality.\nAnnotations are made in the form of 3D bounding boxes defined by their center point (x, y, z), size (w, l, h), and orientation (qw, qx, qy, qz), given as quaternion. Each bounding box instance is classified according to 27 different object classes, using a hierarchical class structure, and in accordance with the nuScenes [3] data format. The distribution of the object categories is shown in Figure 4. In addition, every individual bounding box is labeled according to five attributes (with a total of 15 possible values) representing its visibility level or activity state. Furthermore, objects are tracked throughout the scene and assigned a unique and consistent tracking ID. Besides that, we label all scenes according to 34 distinct scene tags divided into seven different categories, including area, weather, and lighting conditions, as shown in Figure 3."}, {"title": "3.7 Dataset Splits", "content": "The dataset is split into a train, validation, and test set to ensure an independent evaluation. While these splits should be different from one another to test the generalization capabilities of a method, the data splits should also have similar characteristics to ensure a fair evaluation."}, {"title": "3.8 Privacy", "content": "Compliance with data protection measures is a top priority, which is why the whole dataset is anonymized. The anonymization includes not only the blurring of faces and license plates in the image data but also the anonymization of timestamp information. Nevertheless, the temporal consistency is still guaranteed for all samples and sensor data. As shown by Alibeigi et al. [1], the chosen image anonymization should not affect the downstream perception tasks."}, {"title": "4 Tasks", "content": "MAN TruckScenes supports a multitude of different perception tasks through its multimodal nature, sequential data structure, and rich annotations. These tasks include object detection, tracking, prediction, and localization, even if we want to put special emphasis on 3D object detection.\nThe detection task requires detecting 3D bounding boxes of 12 different object classes which are a subset of the original 27 annotation classes. These classes are selected based on the nuScenes [3] detection task and the insides gained during our labeling campaign. As a result, the evaluation excludes object classes that are not present in all data splits and combines subclasses with high inter-class confusion (seen during labeling) in accordance with our hierarchical class structure.\nThe evaluation is based on the nuScenes Detection Score (NDS), which is a weighted sum of the mean Average Precision (mAP) and five True Positive (TP) metrics [3]. In contrast to an Intersection over Union (IoU) based mAP, the NDS uses a distance-based mAP and averages the individual Average Precision (AP) values not only over the classes but also over four discrete distance thresholds. In addition to the mAP, the NDS takes five TP metrics into account, which are the Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE), and Average Attribute Error (AAE). Further details on the NDS and its calculation can be found in [3].\nIn contrast to the NDS, our detection range is not limited to 50 m during evaluation but considers objects of up to 150 m around the vehicle. This should emphasize the development of long-range detection methods, which are crucial for the safe operation of autonomous vehicles on highways. Furthermore, we introduce the animal and traffic sign classes as two additional detection classes, leading to 12 distinct detection classes."}, {"title": "5 Experiments", "content": "We provide baseline 3D object detection results for the CenterPoint [17] detection model, which are shown in Table 3. The model is trained on fused ego-motion compensated lidar point clouds with 3 aggregated sweeps cropped to a 150 m \u00d7 150 m grid with a voxel resolution of (0.1 m, 0.1 m, 0.2 m)."}, {"title": "6 Conclusion", "content": "In this work, we presented the first dataset for autonomous trucking. The MAN TruckScenes dataset is a large-scale multimodal dataset for the development of perception applications for autonomous trucks. The dataset contains a diverse set of scenes recorded in a multitude of different environmental conditions, provides 360\u00b0 coverage of all sensor modalities, and even 4D radar data. It includes carefully reviewed 3D bounding boxes, tracking information, and rich annotations for all objects within more than 230 m range. Furthermore, we introduced a generic method for compiling meaning- ful and balanced data splits. We provide baseline results for 3D object detection and promote research in all areas of perception, tracking, and prediction. For this purpose, we will maintain a public leaderboard and provide annotation instructions, taxonomy specifications, and a development kit. With this dataset, we aim to accelerate the development of autonomous trucking."}, {"title": "7 Limitations", "content": "The dataset is limited to measurements on public roads and logistic terminals in Germany recorded on a single autonomous test truck. The data distribution represents the operational design domain of a long-haul truck and is not representative for other distribution haulage. The dataset is manually annotated and, therefore, subject to human annotation errors even if our extensive quality assurance process seeks to minimize them. The extrinsic sensor calibration can be affected by cabin move- ments, the IMUs are exposed to vehicle vibrations, and the ego vehicle position is subject to GNSS inaccuracies even with RTK correction."}, {"title": "A Appendix", "content": "The appendix provides additional details on the MAN TruckScenes dataset and the conducted experiments. The sections are structured in accordance with the main body."}, {"title": "A.1 Sensor Synchronization", "content": "Sensor synchronization is especially important to ensure temporal consistency in multimodal datasets. As mentioned in Section 3.2, all cabin-mounted lidar sensors are phase-synchronized, such that their lidar points are temporally consistent in a clockwise manner. However, this approach leads to a temporal discontinuity at one point of a full cycle. This point is chosen to be in the back of the vehicle where the rear lidar is located. Furthermore, perfect consistency cannot be achieved for the top-mounted lidar sensors due to their tilted axis of rotation and constant rotational speed. Nevertheless, local deviations are small, such that the fused point clouds of a single sample have similar temporal properties as a single top-mounted lidar sensor. This temporal consistency of the lidar point clouds is shown in Figure A1."}, {"title": "A.2 Sensor Calibration", "content": "The quality of the sensor calibration is illustrated by a projection of the lidar points onto the camera images, as shown in Figure A3. It is important to note that a matching projection can only be achieved within a static scenario for multiple reasons. First, differences in the recording time caused by small deviations in the sensor synchronization (Figure A2) can lead to changes in the captured environment. Second, the rotating measurement principle of the lidar sensors leads to a temporal gradient (from left to right) in the recording of the lidar points (Figure A1). Third, the rolling shutter effect of the cameras causes a temporal gradient (from top to bottom) in the recording of the pixel rows. All these effects can lead to inconsistencies in the measurements of the different sensor modalities."}, {"title": "A.3 Sensor Data", "content": "The provided sensor data is subject to multiple processing steps described in Section 3.4. Within this process, the camera data is mapped to a pinhole camera model to ensure its compatibility with the nuScenes data format and most existing perception methods. Therefore, the images of the wide-angle cameras are undistorted and cropped to remove black spots from the optics. The differences between the original and processed camera images can be seen in Figure A4."}, {"title": "A.4 Scene Selection", "content": "The scene selection aims to select diverse scenarios representative of a long-haul truck's operation, as described in Section 3.5. This selection primarily consists of highway, terminal, and rural scenes as well as scenarios from city and residential areas. To still be able to provide a diverse set of scenes, MAN TruckScenes covers a geographical area of 100 km\u00b2, measured by the union area of the 150 m-diluted ego-poses, as defined in [15]. The geographical locations of all included scenes are shown in Figure A5. While MAN TruckScenes does not provide map data, the included GNSS data enables the usage of open geographic databases (like OpenStreetMap) to utilize external map data, as shown in Figure A5."}, {"title": "A.5 Data Annotation", "content": "This section provides additional information on the included annotations of the dataset. As mentioned in Section 3.6, the annotations comprise scene tags, 3D bounding boxes, tracking IDs, and object"}, {"title": "To address this conflict of objectives, we developed a method to find a Pareto-optimal solution for this multi-objective optimization problem (MOOP). For this purpose, the optimization uses the NSGA-II [4] genetic algorithm with random sampling, polynomial mutation [5], and simulated binary crossover [5] without duplicates. The optimization problem", "content": "\n$\\min(f_1(s), f_2(s), ..., f_{12}(s))$\n$\\mathcal{S}$\n|$\\mathcal{S}_{train}$ = 0.7|$\\mathcal{S}$|, $\\mathcal{S}_{test}$ = 0.2|$\\mathcal{S}$|\n(1)\nis given as a minimization problem with constraints, where $\\mathcal{S}$ is the set of all scenes s defined by their scene properties (number of object classes, scene tags, sample timestamps, and ego positions) with cardinality |$\\mathcal{S}$|. The objectives $f_1(s)$ to $f_8(s)$ are the minimization of the deviations of the discrete distributions of annotations across dataset splits, which are chosen to be the class distribution and the distributions of the seven individual scene tag categories. The objectives $f_9(s)$ and $f_{10}(s)$ aim to maximize the intra-split standard deviation of temporal (sample timestamps) and spatial (ego vehicle positions) components of the scenes. Finally, $f_{11}(s)$ and $f_{12}(s)$ aim to maximize the inter-split Kullback-Leibler (KL) divergence of the temporal and spatial components. The split sizes are set to 70%, 10%, and 20% of all scenes for the train, validation, and test splits."}, {"title": "A.6 Experiments", "content": "Experiments were conducted using a CenterPoint [17] model with a voxel-based encoder based on the SECOND architecture, a Feature Pyramid Network (FPN) neck, a Deep & Cross Network (DCN) CenterHead, and circular Non-maximum Suppression (NMS). The training scheme uses different data augmentation methods, including random global rotation, translation, scaling, flipping, point shuffling, and object sampling. The overall model is based on the MMDetection3D framework and was trained for 20 epochs on two Nvidia A100 GPUs for 181 h with a batch size of 16 per GPU."}]}