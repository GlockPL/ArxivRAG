{"title": "GP-GPT: Large Language Model for Gene-Phenotype Mapping", "authors": ["Yanjun Lyu", "Zihao Wu", "Lu Zhang", "Jing Zhang", "Yiwei Li", "Wei Ruan", "Zhengliang Liu", "Xiaowei Yu", "Chao Cao", "Tong Chen", "Minheng Chen", "Yan Zhuang", "Xiang Li", "Rongjie Liu", "Chao Huang", "Wentao Li", "Tianming Liu", "Dajiang Zhu"], "abstract": "Pre-trained large language models(LLMs) have attracted increasing attention in biomedical domains due to their success in natural language processing. However, the complex traits and heterogeneity of multi-sources genomics data pose significant challenges when adapting these models to the bioinformatics and biomedical field. To address these challenges, we present GP-GPT, the first specialized large language model for genetic-phenotype knowledge representation and genomics relation analysis. Our model is fine-tuned in two stages on a comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics, and medical genetics, derived from multiple large-scale validated datasets and scientific publications. GP-GPT demonstrates proficiency in accurately retrieving medical genetics information and performing common genomics analysis tasks, such as genomics information retrieval and relationship determination. Comparative experiments across domain-specific tasks reveal that GP-GPT outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These results highlight GP-GPT's potential to enhance genetic disease relation research and facilitate accurate and efficient analysis in the fields of genomics and medical genetics. Our investigation demonstrated the subtle changes of bio-factor entities' representations in the GP-GPT, which suggested the opportunities for the application of LLMs to advancing gene-phenotype research.", "sections": [{"title": "1 Introduction", "content": "The relationship between genes, phenotypes and diseases is of fundamental importance but has not yet been fully understood. Due to the complex interplays and mutual regulations among genes, proteins, metabolomics, and phenotypes, finding the proper representations of their relationship has become a significant problem. There are numerous studies that focus on single-molecule or dual-molecule biological levels such as gene mutation, proteomic, gene expression, transcriptional regulation, pathway analysis, and clinical observation. These studies aim to address the questions in specific areas using simplified modelling. Existing research in human populations has elucidated the landscape of complex genomic relations. For instance, Genome-wide association study (GWAS) [1, 2, 3]"}, {"title": "2 Related Work", "content": null}, {"title": "2.1 Language models for Knowledge Embedding and Relation Extraction", "content": "Recently, language models such as ChatGPT and GPT-4 [11], have shown remarkable capabilities in accelerating research innovation across natural sciences. ChatGPT, GPT-4, GPT-4V, and GPT-4 Turbo, developed by OpenAI, are prominent language models based on the autoregressive transformer decoder architecture, which utilizes self-attention mechanisms. These models, trained on extensive text corpora, excel in generating human-like text and demonstrate profound comprehension across diverse knowledge domains. In contrast, open-source models like Meta's LLaMA [12] and Llama2 [12], which feature up to 70B parameters in their largest versions, aim to enhance training efficiency and reduce computational costs. Similarly, BLOOM (176B version) [27] and BLOOMZ (176B version) [28] emphasize linguistic versatility, highlighting a commitment to openness and accessibility. The more recent Falcon [29], with 40B parameters in its largest configuration, has also shown excellent performance on various benchmarks. These models exemplify the cutting-edge of universal language model research. LLaMA models, in particular, have performed exceptionally well on diverse benchmarks, making them some of the most popular open language models to date, hence their selection for analysis in this paper.\nBiomedical natural language processing enables the automatic extraction of crucial information from medical literature, including insights into genetic diseases and associated variants. A key task in this field is Biological Relation Extraction (RE), which involves identifying relationships between two or more entities within specific contexts, which is crucial to identify biological entities such as genes, proteins, diseases, drugs, and miRNAs in textual data [9, 30, 31, 32]. Biological RE models are widely used to extract knowledge from various sources, including medical literature. A key task in Biological RE is identifying relationships between two or more entities within specific contexts. To advance NLP and machine learning techniques for biomedical RE, considerable effort has been invested in developing relevant corpora [33, 34, 35, 36]. Unlike simple regular expression-based methods, Biological RE models leverage the entire context of a sentence to accurately identify and classify these entities, overcoming the limitations of more traditional approaches. Although machine learning (ML) techniques generally outperform traditional methods, they require substantial amounts of manually annotated training data. This presents a significant challenge, especially in the genomics domain, where labelling training sequences at the individual word level is time-consuming and requires domain-specific expertise. Owing to large language models which offers a solution by"}, {"title": "2.2 Instruction Fine-tuning and Parameter efficient Training", "content": "Instruction fine-tuning has emerged as a pivotal technique in improving the performance and generalization of language models across various tasks. The work by Wei et al. [37] introduces a novel training methodology that leverages instruction fine-tuning to enhance model capabilities. By fine-tuning models on multiple tasks with carefully curated instructions, this approach not only improves task-specific performance but also enhances the model's ability to generalize across different domains.\nThe Stanford CRFM report [38] further substantiates the efficacy of instruction fine-tuning. The report provides a comprehensive evaluation of different models and training strategies, demonstrating that instruction fine-tuning can achieve significant improvements in performance even with limited computational resources. This underscores the potential of instruction fine-tuning as a cost-effective solution for developing versatile language models.\nParameter-efficient training methods aim to optimize model performance while minimizing computational overhead [39]. Recent advancements in this field are well-documented in the work by Liu et al. [38], which explores various parameter-efficient techniques [40]. These techniques are designed to reduce the number of trainable parameters without compromising model accuracy, making them particularly suitable for environments with constrained resources\nKey innovations include low-rank adaptation, pruning, and quantization, which collectively contribute to maintaining model efficacy with fewer parameters. Experimental results in Liu et al. [41] highlight that such methods can achieve near state-of-the-art performance with a fraction of the computational cost, thereby promoting sustainable AI development practices.\nCombining instruction fine-tuning with parameter-efficient training presents a promising avenue for future research [42]. Integrating these methods can potentially lead to the development of highly adaptable and resource-efficient models. The synergy between these approaches lies in their complementary strengths: instruction fine-tuning enhances task adaptability, while parameter-efficient training ensures computational feasibility.\nOverall, the convergence of instruction fine-tuning and parameter-efficient training methodologies signifies a crucial step towards creating robust, efficient, and scalable AI systems. Continued exploration and refinement in these areas are expected to drive further innovations in the field of machine learning and artificial intelligence."}, {"title": "2.3 Language Model Applications in Bioinformatics", "content": "In recent years, the emergence of language models for bioinformatics has shown significant progress, with notable examples including AlphaFold2 [43], AlphaFold3 [44], GeneGPT [45], BioT5 [46], BioT5+ [47], and Med-Gemini [48]. These models can be categorized into single-domain and multi-domain approaches, each leveraging different strategies for pretraining and application in the field.\nSingle-domain approaches pretrain their models solely on tokens from a specific domain. A prominent example is GeneGPT [45], which introduces a novel method for teaching large language models (LLMs) to utilize the Web APIs of the National Center for Biotechnology Information (NCBI) to"}, {"title": "2.4 Mapping of Genotypes and Phenotypes", "content": "Genotype-to-phenotype mapping is a traditional work in bioinformatics. In a statistical view, GWAS [1, 2, 3] have played a dominant role in discovering potential relations between genotypes and phenotypes in a population scale. It has tested hundreds of thousands of genetic variants across human and other genomes to find those statistically associated traits or diseases, through which gaining insight into a phenotype's underlying biology, estimating its heritability, and discovering genetic correlations. However, from a micro view, advancements in sequencing technologies, particularly single-cell RNA-seq (scRNA-seq), have significantly advanced our understanding. These techniques expose the intricate dynamics of gene expression at the cellular level, unveiling the extensive landscape of genotypes shaped by numerous factors [63]. In recent years, the utilization of machine learning, particularly the adoption of self-supervised transformer models originally developed for natural language processing (NLP), has demonstrated significant potential in the analysis of complex biological datasets [60, 62].\nAt the cohort level, meta-analysis is an effective approach to integrate multiple association studies of genes and phenotypes. Meta-analysis is a statistical tool used to combine results from different studies on the same topic and quantitative synthesis of research findings. Meta-analysis enables the identification of genuine associations between genes and phenotype [64]. Particularly for the studies based on human populations, a comprehensive catalogue of all published GWAS and association results have become publicly available, which provided resources for human-specific meta-analysis [4].\nAt the level of human cells, there is a principal goal of revealing the landscape of a combination of genes that results in an extraordinarily diverse phenomenon. Phenomic databases [65], have been collected as comprehensively as systematically assemblages of phenotype information. et. al. [66] has presented a computational integrated genetics framework designed to analyze and interpret the high-dimensional landscape of genotypes and their associated phenotype. The study introduces the use of self-supervised language models to simultaneously map the genotype-phenotype landscape, proposing the concept of integrated genetics as an alternative to traditional forward and reverse genetics. In this approach, phenotypes such as sex, age, anatomical tissue, and cell types are integrated and learned alongside genotypic data obtained from scRNA-seq. This method deepens our understanding of the biological context of gene expression and, ultimately, the genotype-phenotype relationship."}, {"title": "2.5 Gene Networks and Knowledge Graphs", "content": "Whole genome sequencing [67, 68] and GWAS studies [69, 70]have already provided substantial evidence for investigating disease-associated genes. Recent research suggests that genomics datasets can be represented as gene networks, where the nodes symbolize genes, phenotypes, or proteins, and the connections between the nodes indicate the relationships between these entities. Schlitt, et.al. [71] propose a novel method for identifying functionally related genes by comparing neighbourhoods within gene networks. This approach does not depend on gene sequence or protein structure homologies and can be applied to any organism and a wide range of experimental datasets. Gene networks enable researchers to seek additional evidence to cross-check their results using various sources, e.g. enriching existing computational approaches such as linkage analysis and GWAS studies by including the relationships identifications between genes from protein-protein interaction (PPI),"}, {"title": "3 Method", "content": null}, {"title": "3.1 Data", "content": "The datasets in genomics, proteomics, and medical genetics have experienced significant growth in both scale and diversity. Our dataset-building attempt starts from the gene entities and their textual descriptions maintained by the National Center for Biotechnology Information (NCBI) [79]. Taking the 192064 individual gene items from NCBI as the entries, we then collected and screened the data of related bio-factors from multiple datasets. Each of these datasets stands for a particular level in the genomics system(Fig. 1)."}, {"title": "Data Collection", "content": "For the purpose of comprehensively incorporating and extracting the mapping between genes and associated phenotype/diseases, multiple data sources were integrated into the training dataset of this study. A major part of the dataset came from the Online Mendelian Inheritance in Man(OMIM) database [80], which is a comprehensive and authoritative resource for understanding human genes and their associated genetic disorders. OMIM keeps full-text information on all known Mendelian disorders and over 16,000 genes and provides detailed information on human genes, genetic phenotypes, and the relationships between genes and phenotypes. Genes often cast influences on phenotypes through gene products and a series of related complex biological pathway alterations [8]. Building on this concept, we incorporated the information of gene products from UniPort [81], and their molecular function information from NCBI. The extracted pairs of genes and associated phenotypes/diseases were later verified and supplied through dbGaP [26] and DisGeNet [9]. In summary, from OMIM, we collected the gene-phenotype pairs along with the relational graphs. A total of 4401 pairs of phenotype and gene have been collected and verified with the dbGaP data. From UniPort, we collected 1286 gene-protein pairs. From DisGeNet, we use the Relation Extraction (RE) samples from the evidence terms. We integrated these multi-level genomics text data to serve our training objective. All the mentioned data are examined and matched to the NCBI gene IDs and organized accordingly."}, {"title": "Construction of the Multi-task and Multi-level Genomics Training Corpus", "content": "Based on the intrinsic logic in multiple data sets, we extracted relevant bio-text and structured data and formulated them into a multi-task multi-level genomics training corpus. The corpus is composed of three training datasets, each containing different types of genomics knowledge context. The first dataset is designed for the initial training stage, which follows the masked training manner, while"}, {"title": "3.2 Model", "content": "In this section, we introduce GP-GPT, an LLM-based paradigm developed to precisely map gene and phenotype entities using bio-text linguistic knowledge. GP-GPT uses the Llama2 framework to establish an autonomous LLM-based framework with a prompt question-and-answer format. This model can automatically search for latent gene-phenotype associations, extract meaningful relations, summarise bio-function descriptions, identify bio-factors, and visualize the embedding of these bio-factors. The overview of the training input of the GP-GPT framework is shown in (Fig. 2). We have constructed a training corpus with selected human genomics texts on multiple bio-system levels following the conventions in section 3.1. The text data in this corpus reflects the hidden connection between bio-factors from different levels and further organizes different types of such relations into a similar format by choosing different instructions."}, {"title": "Parameter Efficient Fine-tuning", "content": "In this study, we utilize the parameter-efficient fine-tuning techniques of the LLaMA model family (Llama2, Llama3, Llama3.1) using Low-Rank Adaptation (LoRA) [39], and QLoRA [42] technique. The objective is to adapt the Llama3 model for specific downstream tasks while minimizing the number of trainable parameters and computational resources required. LoRA introduces a low-rank adaptation matrix into the transformer architecture, allowing efficient fine-tuning with fewer parameters. In addition, QLoRA applies a quantized pre-trained language model into low-rank adapters, significantly reducing memory usage to further finetune large language models. Both techniques modify the original model minimally, making it computationally efficient and less prone to overfitting, especially with limited data. In our study, we tested both LoRA and 8-bit QLoRA on the Llama2 7B model and the Llama3.1 8B model. With no significant performance discrepancies between the two techniques, we employed 8-bit QLoRA to achieve efficient fine-tuning. Additionally, 4-bit QloRA was used during the fine-tuning of the Llama3.1 70B model.\nWe built the fine-tuning models on the pre-trained Llama2 model 7B, Llama3.1 model 8B, and eventually on the pre-trained Llama3.1 70B. LoRA layers are inserted into the transformer architecture of such Llama models. These layers are designed to learn task-specific adaptations by introducing low-rank matrices into the attention heads of the transformer model. By keeping the base model parameters frozen, only the parameters of the LoRA layers are updated during the fine-tuning process. This approach significantly reduces the number of trainable parameters compared to traditional fine-tuning methods."}, {"title": "Training on Genomics Data", "content": "With the multi-task muti-modality genomics training corpus at hand, we follow the workflow of parameter-efficient fine-tuning to train GP-GPT. As shown in (Fig. 6), the multi-level genomics training data have been formatted into training contexts and fed into an instruction-fine-tuning process based on Llama2. The whole instruction fine-tuning process can be viewed as two-stage training. During the first stage, the training text data is from the first part of the training set, which is prepared in the form of instruction mask prediction fine-tuning. In the second fine-tuning stage, the training text data are drawn from the second part of the training set, which is designed for supervised fine-tuning in a question-answer format."}, {"title": "Tokenization and Embedding", "content": "Tokenization and Embedding: Models such as Llama, Llama2 and Llama3 utilize a character-based approach to numerical tokenization, where each digit and special character is treated as a separate token. This technique has been proven to be more effective in a variety of arithmetic tasks.\nSimilarly, GP-GPT adopts this character-based tokenization method for bio-factor entities, including gene IDs, protein names, and disease abbreviations. The original dictionary remains unaltered. We assigned the <mask> token back to pre-trained Llama models during the first stage of fine-tuning, in order to implement instruction mask fine-tuning. Specifically, the re-added <mask> token has been assigned to the vocabulary dictionary at ID 35073.\nModel architecture: GP-GPT employs the same architecture as Llama family models, with a specific LoRA adaptor merged with the original model. We follow the configuration used in Llama2(7B), Llama3.1(8B), and Llama3.1(70B). The vocabulary size of GP-GPT is 35, 073, the same as the default configuration. GP-GPT model inherits M parameters from the original Llma2(7B) and comprises an additional LoRA adaptor of M.\nFine-tuning: The entire instruction fine-tuning process can be considered as a two-stage training approach. In the first stage, the training content is drawn from the initial segment of the training set, which is prepared for fine-tuning through instruction mask prediction. In the second stage, the training content is taken from the latter part of the training set, which is structured for supervised fine-tuning using a question-answer format. GP-GPT can be fine-tuned on various downstream tasks involving molecules, proteins, and text. In both stages, to bridge the gap between pre-training and fine-tuning stages and unify different genomic downstream tasks, we employ a prompt-based fine-tuning approach that converts various task formats into a sequence generation format [82, 83]."}, {"title": "3.3 Evaluation", "content": "This study is designed to uncover the relationship between genes and phenotypes/diseases and their mutual mappings, by using linguistic information extracted from bio-text data. Therefore, considering the nature of the generative language model, we designed the evaluation experiments based on a question-answer paradigm. Besides validating the performance of the model, we are also interested in exploring the efficiency of using LLMs to map gene entities and phenotype/disease entities. We visualized a series of gene-phenotype embedding in the LLM's encoding space to better understand the potential of the genomics relation language model."}, {"title": "Information QA", "content": "The question-answer evaluation of gene-phenotype information in this study can be classified as a form of internal validation. Although the gene-phenotype pairs used in testing were implicitly represented in the training data, this evaluation method serves as an effective tool for assessing our model."}, {"title": "Case studies", "content": "We first evaluate GP-GPT on the text generation task. The models were required to answer the questions about gene and disease knowledge. The 'Gene disease association' questions from the QA database GeneTuring [84] have been used to test our model, and an expert evaluation has been conducted by comparing the model-generated texts and golden standard answers."}, {"title": "QA evaluation", "content": "This module tests the ability of GP-GPT models to identify the gene-disease association with generated text based on specific question-answer instruction. The ground truth of gene-disease association was downloaded from OMIM. More than 1000 gene-disease pairs were randomly selected and all genes associated with each disease were recorded. In the question-answering tasks, a question was created by pasting \"What are genes related to \" before the disease name and a question mark after the disease name. To create sentence completion tasks, a question was created by pasting \"The name of the gene related to \" before the disease name and \" is\" after the disease name. The testing of disease-gene association was designed in the same style by reversing a question and answer from the above case. The set of gene-disease-associated pairs was used as the gold standard. To compare the model-generated answer with the gold standard, 4 metrics were applied. BLEU has been used to directly compare the two texts. The specialized BLEU-1 is designed to precisely measure the gene or disease, the shortest phrases which only contain the gene's name or disease's name are used as references in BLEU-1 calculation, while the brevity penalty and n-gram precision are neglected. The accuracy gene-phenotype (ACC.G-P) and the accuracy phenotype-gene (ACC.P-G) are calculated by matching the exact gene or phenotype name in the generated sentence. Note that for complex names, only the non-duplicated stem tokens in the names are considered.\nWe evaluate the GP-GPT against other 7 large language models. The setting of the same prompt components has been used across all different models. The candidate LLM includes: original Llama2(7B), Llama2(13B), Llama3(8B), Llama3.1(8B), Llama3.1(70B), and the Bio-GPT [53], and the GPT-4 model [11]. Our evaluation of GP-GPT is more rigorous than the evaluation of LLMs and the GPT models in genomics [84], which minimized manual involvement and emphasized both exact matches and partial correctness."}, {"title": "Genomics Relation Determination", "content": "To evaluate the model's understanding of the relation between genes and phenotype/diseases, the well-established relation determination task has been applied to directly test the model's performance.\nThe relation determination task was formulated as a text generation task with the proper question-answer prompt. The relation determination prompt is composed of instruction and text, models are asked to generate answers in yes/no based on the text. We use the test set from TBGA [32] data. We screened the testing data by checking the gene-disease pairs with their corresponding pairs in DisGeNet data, to ensure the knowledge correctness and testing effectiveness. We tested the GP-GPT with other large language models, for the local running models, the results were collected in a 'mixed-prompt' strategy. Thus, for GP-GPT, Llama2, Llama3, Llama3.1, and Bio-GPT, each single question-answer testing data is formulated with multiple prompts. During the results collection, the best answer was selected as the final output of the model for further metric calculations. But for commercial large language models like GPT-4, only one kind of prompt was tested due to its cost."}, {"title": "Bio-factors Embeddings", "content": "Considering the gene entities and phenotype/disease entities as two kinds of bio-factors, we report the results for the soundness of the implementation of the LLM in the representation of bio-factors. To this end, we explored the mapping of genes and phenotype/disease in the embedding space of a large language model. By composing the summaries of gene entities and disease entities into sentences, we obtained the sentence embeddings of the bio-factors inside the model. The observations are intended to monitor the static embedding maps of the entities both in zero-shot mode and in a fine-tuned model. In zero-shot mode, the latent embedding was extracted from the outputs of the hidden layers. In fine-tuning models, the sentence embedding was comprised of the model's original layer outputs and the LORA module's modifications. UMAP has been applied for the visualization of the embedding of entities."}, {"title": "4 Results", "content": null}, {"title": "4.1 Dataset and Model Setting", "content": "We collected a total of 2,457,469 pieces of stage 1 genomics contextual data (1021 MiB) and 546,926 pieces of stage 2 genomics contextual data (308 MiB) for model training, out of which 10,000 randomly selected texts from each data set have been set aside as validation dataset. Both the training dataset and validation dataset are balanced in terms of the multi-level multi-task formats mentioned in 3.1. The model architecture is the same as the original Llama models: Llama2(7B), Llama3.1(8B), and Llama3.1(70B). With Q-LoRA modules added to Llama2(7B) and Llama3.1(8B) models at 8-bit, and to Llama3.1(70B) model at 4-bit. We fine-tune the model specifically focusing on optimizing a smaller set of trainable parameters (overall at 7.49 percent compared to original models), making the process faster and less resource-intensive. The learning rate was set to 1.4e-05, and the batch size wa set to 32. As a result, we obtained the genomics-specific fine-tuned large language models with different sizes: GP-GPT small (from Llama2 7B), GP-GPT base (from Llama3.1 8B), and GP-GPT large (from Llama3.1 70B). In the final training stage (stage 2), an early stop have been placed at the 2.75 epochs for GP-GPT small and GP-GPT base, and the early stop at 1.25 epochs for GP-GPT large."}, {"title": "4.2 Experiments", "content": null}, {"title": "Genetic Medical QA", "content": "The model can generate reliable responses to gene-disease information queries. We provided four randomly selected cases from four different questions. Upon comparing the model-generated answers"}, {"title": "Evaluation on Relation Determination", "content": "As for relation determination tasks, We gather 8 models to evaluate the performance of the proposed GP-GPT. The comparison results are summarized in Table. 2. Based on the relation determination task, we evaluate GP-GPT models and other models on the four metrics: Precision, Recall, Accuracy, and F1 score. The results show that the fine-tuning procedures have gained the ability of GP-GPT models in defining gene-phenotype relationships (based on F1 score). The overall best model is GP-GPT(small), which is based on Llama2(7B). Considering the subpar performance of original Llama3.1(8B) and Llama3.1(70B) on this special task, their corresponding fine-tuned version GP-GPT(base) and GP-GPT(large) show significant improvements and achieved comparable scores against the listed candidates. In summary, compared to the original models, All fine-tuned models demonstrated an adaptive ability to determine the genomics relations in the form of generating readable responses to medical genetics questions."}, {"title": "Visualization of Gene-Phenotype Embeddings", "content": "The Visualization of gene entities and phenotype/disease entities can be seen from two perspectives. First, we show the latent text representations of the gene entities and the phenotype entities (Fig. 8). By comparing the distributions of sentence embeddings in the two-dimensional UMAP space, the efficiency of fine-tuning can be viewed as the distributions of gene entity embeddings and phenotype entity embeddings. In the plot, the colors indicate the IDs of the corresponding gene-disease pairs. By comparing the gene embedding plots and disease embedding plots, the embeddings from the fine-tuned model show a noticeable spreading distribution pattern, especially for diseases. Second, in Figure 9 (Fig. 9) and Figure 10 (Fig. 10), labelled by the related tissue, we probed and showed the embeddings of entities from different layers of the model in the two-dimensional UMAP space. The"}, {"title": "5 Discussions and Conclusion", "content": null}, {"title": "5.1 Limitations", "content": "Our study faces a few limitations. Considering the current research trends and inherent issues in the realms of genetics, genomics, transcriptome, proteome, metabolome, and the general biomedical domain, the primary challenges of large language models lie in the following key areas."}, {"title": "Dataset Expansion", "content": "The data used in this study covers multiple data sources, including dbGaP, UniPort, DisGeNet, NCBI, and OMIM. Among these datasets, the employed bio-text data are mainly based on OMIM. Although the training corpus is composed of 4401 pairs of phenotype and gene, the construction of training contextual data is heavily dependent on high-quality textual data, which is lacking for most genes and diseases. With this limitation of data sources, there is demand of generating more reliable genomics textual data from other structure dataset. Inspired by the network-based disease gene prediction research, using the network data of protein-protein interaction (PPI), gene expression, gene ontology, and other domain knowledge to generate large relational-genomics text data is an available way. Based on conventional relationships often observed in bio-networks, e.g. dominating, inhibiting, and promoting, the relation network of bio-factors can provide rich text data following a well-designed graph-to-text interpreting system. To be noted that, such systems are usually based on LLMs as well [85, 86, 87]."}, {"title": "Tokenization of Genomics Entities", "content": "Tokenization is one of the essential steps in the success of the training language model. To simplify the model training, we adopt the character-based tokenization method from the original Llama2, and Llama3 for all the genomics entities. One reason for this is that the model is only focusing on the name-entities features and relation description, which both are based on natural languages. Another reason is that Llama2 and Llama3 employ a character-based method for the numerical tokenization approach, which has been shown to be more effective across various arithmetic tasks. In practice, these tokenizers work well for the current stage of GP-GPT. However, a from-scratch pre-training can be necessary for a better representation of genetics, genomics, and other levels of bio-factors, especially when more features from the sequence are involved. Previous works [43, 44, 46] have provided available instances for bioinformatics costumed tokenizor. Due to the computation cost, we decide to put such endeavours into our further improvement of GP-GPT."}, {"title": "Model Performance and Evaluation", "content": "In terms of question-answer text synthesis, the model occasionally exhibits semantic repetition at the document level, tends to lose coherence over the questions, and sometimes includes non-sequitur sentences or paragraphs. Our experiments only include the instruction fine-tuning following the convention of the generative language model and do not include self-regression architectures or other denoising training objectives. Lack of direct training on fill-in-the-blank tasks or considering bi-directionality self-regression leads to potentially worse performance on parsing a long passage and then generating a very short answer. In practice, that could be observed at relatively low precision metrics for all LLMs tested in this study, compared with other knowledge graph-based models.\nFurthermore, the evaluation methods for such genomics language models are still underdeveloped. The direct issue to this end is that the missing of standard benchmarks in this area. On one hand,"}, {"title": "5.2 Future Perspectives", "content": null}, {"title": "Multi-Modality", "content": "Our proposed large language models are not grounded in incorporating other domains of data, such as biological sequence data and medical images. The wide applicability of transformer architecture [90] facilitates modality fusion. Recent research in computer vision and NLP also demonstrates significant benefits in combining multiple modalities to achieve application success [91, 92, 93, 94, 95, 96]. Domain-specific models are natural platforms for incorporating data from other modality to improve and scale practical usability [97, 98, 99, 100, 101, 102, 103, 104, 105, 106].\nBio-sequence data counts for the most dominant part of all available data in bioinformatics studies. One of the efficient ways to combine the bio-sequence data and LLMs is utilizing the parallel architecture to place both the sequence-specific model and language-specific model [107]. Using specific tokenizers to encode particular sequence features is another practical method [108]. In future versions of GP-GPT, the sequence data from DNA sequences, gene variances, and gene products(small-RNA, protein) will be included in the framework."}, {"title": "Dementia-related Genotype-Phenotype Associations Identification", "content": "Dementia is a major public health challenge due to its profound impact on individuals, families, and healthcare systems globally. Dementia's etiology is complex, involving a combination of genetic and environmental factors [112]. Alzheimer's Disease (AD) [113, 114, 115], the most common form of dementia, is characterized by the accumulation of amyloid plaques and neurofibrillary tangles in the brain. Over the past few decades, genetics has become increasingly pivotal in AD study [116, 117], starting with the discovery of rare mutations in the APP (amyloid precursor protein), PSEN1 (presenilin-1), and PSEN2 (presenilin-2) genes, which are linked to early-onset autosomal dominant forms of the disease. The association of the APOE (apolipoprotein E) e-4 allele with sporadic or late-onset AD has further highlighted the genetic underpinnings of the disease. Phenotypic heterogeneity, or variability in disease presentation, is also evident in AD [112]. For instance, certain PSEN1 mutations are associated with distinct clinical features such as spastic paraparesis and degeneration of the corticospinal tract [118, 119]. Similarly, APP mutations can lead to lobar haemorrhages associated with cerebral A\u1e9e40 angiopathy, which showcases the diverse manifestations of AD [120]."}, {"title": "5.3 Conclusion", "content": "In this study", "sizes": "GP-GPT Small (based on Llama 2 7B), GP-GPT Base (based on Llama 3 8B), and GP-GPT Large (based on Llama3 70B). We evaluated the model on multiple tasks, including genetic medical question answering and genomic relationship determination. In such tasks, GP-GPT outperformed current state-of-the-art large language models, including Llama2/3 and GPT-4. Additionally, a comparison of the GP-GPT variants (Small, Base, Large)"}]}