{"title": "Revisiting Your Memory: Reconstruction of Affect-Contextualized Memory via EEG-guided Audiovisual Generation", "authors": ["Joonwoo Kwon", "Heehwan Wang", "Jinwoo Lee", "Sooyoung Kim", "Shinjae Yoo", "Yuewei Lin", "Jiook Cha"], "abstract": "In this paper, we introduce RecallAffectiveMemory, a novel task designed to reconstruct autobiographical memories through audio-visual generation guided by affect extracted from electroencephalogram (EEG) signals. To support this pioneering task, we present the EEG-AffectiveMemory dataset, which encompasses textual descriptions, visuals, music, and EEG recordings collected during memory recall from nine participants. Furthermore, we propose RYM (Recall Your Memory), a three-stage framework for generating synchronized audio-visual contents while maintaining dynamic personal memory affect trajectories. Experimental results indicate that our method can faithfully reconstruct affect-contextualized audio-visual memory across all subjects, both qualitatively and quantitatively, with participants reporting strong affective concordance between their recalled memories and the generated content. Our approaches advance affect decoding research and its practical applications in personalized media creation via neural-based affect comprehension.", "sections": [{"title": "Introduction", "content": "Autobiographical memories, constructed from sensory impressions and affective states, are central to shaping our sense of self. When familiar sights or sounds activate these memories, they bridge past and present experiences, allowing us to relive both sensations and affects.\nAffective memory recall is a core area of research in cognitive and affective neuroscience, providing insights into how affect dynamics shape the richness and retrieval of personal experiences. Research has established that affect plays a crucial role in forming memories composed of visual and auditory information. Autobiographical memory recall, in particular, may effectively evoke robust affective states, underscoring the deep interconnection between affect and memory. Despite the co-occurrence of various affects during memory recall, prior studies have mainly focused on static self-reported measures, offering limited insights into the temporal dynamics of affect.\nThis evolving focus has advanced affect decoding research, with studies demonstrating the successful application of machine learning for decoding affective states from neural signals, including movie viewing, music listening, and memory recall. While EEG-based affect decoding shows promise for real-world applications, its untapped potential in decoding the rich and dynamic affective states evoked during autobiographical memory-recall remains a critical and largely unexplored frontier. Understanding these dynamics and developing methods for their neural decoding could lead to breakthroughs in uncovering the fundamental role of recalling unforgettable memories in everyday affective experiences.\nIn parallel, affect-guided content generation has rapidly evolved, opening new possibilities for transforming affect-contextualized memories into multimedia experiences. Early studies demonstrated basic affect-guided neural style transfer in images and music, while recent advances have enabled more nuanced control of affective expression through affect-specific text prompts. Furthermore, recent generative models support more sophisticated control over the generation process.\nBuilding on these advancements, the integration of generative models with affect decoding offers a novel opportunity to create multimedia content that preserves both the content and affective context of autobiographical memories. Given the critical role of affective states in shaping sensory processing during memory formation and recall, this integration has the potential to transform personalized entertainment and media creation, providing individuals with a deeply resonant experience where their autobiographical memories are enriched by AI-generated content.\nHere, we aim to decode temporal dynamics of affective states from EEG during autobiographical memory recall and develop a system for generating affectively resonant videos accompanied by music from personal memories using generative artificial intelligence (genAI). To support this task, we present EEG-AffectiveMemory dataset, which captures multi-modal data including text descriptions, sketch paintings, associated musical pieces, and EEG signals during memory recall. Of note, our experimental protocol specifically targets affective state transitions by having participants recall episodes containing mixed affects (e.g., \"sad but cozy\" and \"happy but lonely\") and report real-time affective states during memory recall and generated content viewing. This design enables simultaneous tracking of dynamic affective changes alongside their neural correlates.\nLeveraging this comprehensive dataset, we propose RYM, a three-stage multi-modal framework that decodes dynamic affective transitions from neural signals and generates synchronized audio-visual content reflecting the affective trajectories during memory recall. To summarize, our work has three main contributions.\n\u2022 For the first time, we introduce a novel task and comprehensive multi-modal dataset for studying affect-contextualized autobiographical memories. This dataset will be accessible on our project page.\n\u2022 We successfully decode dynamic affective states from EEG signals during memory recall, capturing temporal affective trajectories.\n\u2022 We propose RYM, a simple yet effective framework that faithfully generates synchronized audio-visual content while preserving the affective dynamics of personal memories."}, {"title": "Related Works", "content": "Affect Decoding using Human Brain Signal. Machine learning models have shown increasing success in decoding affect from neural signals (EEG, fMRI). While traditional studies used controlled stimuli, research has shifted toward naturalistic approaches using movies and music. EEG has emerged as a preferred modality due to its high temporal resolution and accessibility. Several influential open-source datasets (DEAP, SEED, DREAMER) combine EEG with naturalistic stimuli, with DREAMER demonstrating portable EEG feasibility. Recent datasets explore increasingly natural contexts: social video watching (AMIGOS) and facial expression recording (PME4). These resources have advanced both neural pattern analysis and affect dynamics decoding using machine learning and deep learning models. Despite these advances, the neural dynamics of autobiographical memory recall, a powerful affect elicitor, remain largely unexplored. While early studies showed potential in decoding memory-induced affect using conventional machine learning, subsequent research demonstrated successful affect classification from EEG signals. Recent work shows deep learning models outperform conventional machine learning approaches in memory-induced affect classification, while their requirements on larger training datasets significantly hinder their applications. Recently, CEBRA addresses the sample size limitation in affect decoding through contrastive learning, using temporally aligned variables like affect time-series. Demonstrating efficacy in various neural decoding tasks , CEBRA significantly outperforms traditional approaches like frontal alpha asymmetry in cross-participant affect valence decoding . Thus, we utilized CEBRA to effectively extract affect dynamics from individual subjects within a single session of neural recordings.\nNeural Style Transfer. Neural Style Transfer (NST) enables example-guided style transfer while preserving content. The introduction of adaptive instance normalization and its extensions advanced arbitrary style transfer capabilities, though requiring intensive computation with pre-trained convolutional neural networks (CNNs). Recently, AesFA achieved lightweight implementations without pre-trained CNNs, enabling real-time high-resolution applications. Meantime, diffusion models have advanced style transfer through textual inversion and CLIP-based disentanglement to text-conditioned approaches. Recent work explores affect-based style transfer using affect-color palettes, facial expression-based color palettes, and visually-abstract affects from text, but remains limited to static representations. We address this limitation by proposing a framework that leverages dynamic affective information from EEG signals to generate affect-contextualized music videos.\nMusic Style Transfer. Following NST in computer vision, music style transfer enables content and style separation in musical pieces. While lacking formal definitions, research frameworks typically treat melody, harmony, and rhythm as content, while considering timbre, articulation, and dynamics as style elements. Early music style transfer focused on timbre transformation while preserving structural content using WaveNet and adversarial networks. Research evolved to broader style elements, enabling genre transformation through manipulation of melody, harmony, and instruments. Recent work explores the affective expression of music, separating low-level features (pitch, harmony) from high-level features (rhythm, dynamics, tempo) for controllable style transfer. Recent advances in LLMs enable text-conditioned and melody-guided music generation, simplifying synthesis through text descriptions and audio references. Nonetheless, none of these approaches directly utilize neural signals to extract affect, nor do they utilize affect dynamics."}, {"title": "Experimental Designs", "content": "Total 10 participants were recruited, and 9 participants who completed both sessions were included in the analysis (8 females; $mean_{age}$ = 24.1 years, $std_{age}$ = 1.5 years). All participants provided written informed consent. Experiments were conducted in a soundproof room.\nSession 1. One day prior, participants were instructed to prepare an affective memory, evoking mixed affects (e.g., recalling a high school graduation may involve both sadness about parting with friends and excitement about college life), and a song that enhanced the vividness of their recall. The selected song had to have a piano solo cover available on a streaming platform.\nThe first session comprised four stages. First, participants were fitted with Enobio 20 EEG devices (Neuroelectrics), and signal quality calibration with 2-minute eyes-closed resting-state EEG recording was performed. Second, participants repeatedly listened to their chosen song while writing a 50-75 word essay in Korean describing their memory.\nThird, while listening to the same song, participants created a digital sketch of their memory using Tayasui Sketches on an iPad. Last, during the memory recall, participants closed their eyes as EEG data was recorded and indicated affective changes using real-time keypresses. They kept pressing '1' for positive, '3' for negative, and no key for neutral states. Keypress events were temporally mapped to the EEG signals as. Participants stopped recalling when no new scenes appeared or affective responses subsided. Afterward, participants rated their confidence in their keypress responses on a 1-7 Likert scale (i.e., 1 = Very insecure, 7 = Very confident) and reported acceptable performance ($mean$ = 5.2, $std$ = 1.0).\nSession 2. Session 2 consisted of three stages. First, participants freely watched both a \"real\" video (based on their CEBRA-decoded affective sequence) and a \"fake\" video (based on permuted affective sequences) to reduce confounding effects from surprise reactions during the following evaluation. Second, after a 30-second break, the participants re-watched each video and evaluated the affective changes depicted in the video using the same keypress method as in session 1. While session 1 keypresses reported subjective feelings, session 2 keypresses assessed affective recognition of the videos. Last, after another 30-second break, participants watched the videos again and rated which better represented their memory and its affective dynamics. They chose from four options: Both, Real, Fake, Neither."}, {"title": "Method", "content": "Extracting Human Affects\nTo effectively extract affects from individual subjects within a single session of neural recordings, we utilized the contrastive learning-based neural network, CEBRA . By leveraging temporally synchronized behavioral sequences as auxiliary variables, CEBRA non-linearly reduces the dimensions of neural signals to maximize the separation between distinct behavioral states while clustering signals from similar states. Initially validated in animal neuroscience and invasive BCI applications , CEBRA has recently demonstrated superior performance in human affect decoding during movie viewing, suggesting its potential for EEG-based affect decoding.\nIn our study, CEBRA utilizes keypressed valence sequences (neutral, positive, negative) as auxiliary variables to identify individual-level latent clusters for each affective state. We performed multi-session training of CEBRA with 10 participants' EEG and key-pressed valence sequence, aligning individuals' latent valence-neural representations. To examine the generalizability of embeddings, we conducted leave-one-out valence decoding tasks. We trained a k-nearest neighbor (KNN) classifier with 9 participants' CEBRA embeddings to predict valence label at each timepoint and evaluated its predictive performances with the other's embeddings.\nAligning Extracted Affects With Text\nTo effectively incorporate extracted affects into audio/visual output, it is essential to convert them into text prompts, which are commonly used for directing generative processes. Specifically, we pre-defined descriptive terms for positive and negative affective states with the pre-trained language model, claude.ai. For each affective state, we randomly selected words from the predetermined word samples and fine-tuned the original text prompts that the subject articulated in their memories with affective samples using the pre-trained language model. In the neutral state, the original text prompts remain unaltered.\nImplementation Details\nAffect Extraction. To decode each individual's affect dynamics from memory-recall EEG signals, we conducted multi-session training of CEBRA. We set hyperparameters as follows: batch size=2,048, model architecture='offset-10 model', the number of hidden units=95, learning rate=.005, the number of iterations=2000, hybrid=False. Following the previous study on the optimal number of dimensionality for generalizable valence representation across individuals and stimuli , we set the number of latent embeddings' dimensionality as 7. The neighborhood parameters of KNN classifiers were fixed at the square of the number of the input time points.\nMusic and Video Generation. Using MusicGen-melody , we generated music using affect-specific text prompts and memory-associated guiding melodies selected by participants. The generated content was then segmented according to affective state durations and integrated into a final sample using 40 ms crossfading to match the affective trajectory. For video generation, we used stable diffusion ver.1.5 . In all our experiments, we fix the parameters of text encoders and LDMs for music and video generation, respectively. We use the author-released codes using default configurations. All experiments"}, {"title": "Experimental Results", "content": "Affect Decoding. Participants showed individual differences in temporal dynamics of affective states during memory recall and the duration of the recall (see Figure 3a). Despite these variations, our CEBRA model demonstrated a plateau in learning performance in multiple-session training with EEG signals and real-time valence keypresses from nine participants (see Figure 3b). The individually derived latent valence-neural embeddings from this model revealed a consistent geometric structure across individuals (see Figure 3c). Using these embeddings with a KNN classifier for leave-one-out classification for each individual yielded a test-weighted F1 score of 0.9.\nQualitative Evaluation. Figure 4 illustrates that our approach produces qualitatively satisfactory visual and audio outputs across all affective states and subjects. Our model preserves the elements of the original sketch while integrating corresponding affects through tones, colors, and visual composition. The positive video clip showed brighter colors and a vibrant tone throughout the videos, whereas the negative displayed darker tones and a gloomier atmosphere. Moreover, the generated output demonstrated satisfactory visual coherence across video frames. Also, our model successfully transfers the style of original guiding melodies. As in Figure 4, the positive affects reflecting music clips showed broader frequency distributions with stronger energy in high-frequency regions, creating brighter and more vibrant spectral patterns. In contrast, the negative affects reflecting content showed concentrated energy in lower frequency bands, resulting in darker and more focused spectral patterns. These results demonstrated clear acoustic differences between affective expressions in generated music. The down-sampled version of generated music samples can be accessed within the supplementary materials.\nQuantitative Evaluation. We show quantitative evaluation in Tab. 1 and Figure 5. Semantic evaluation using CLIP and CLAP embeddings shows consistent distances between reference prompts and affect-contextualized outputs across affective states, indicating semantic coherence. Additionally, the minimal distances between outputs and their corresponding affective word embeddings (e.g., positive-positive pairs) validate our model's ability to effectively direct content generation toward intended affective states.\nWe examined how affective inputs create variations in visual (e.g., hue, saturation, value) and musical (e.g., mode, brightness, intensity) attributes of generated content. While visual generation shows significant correlations between affects and hue/value, musical generation demonstrates stronger correlations across all features, suggesting more effective affective expression through musical elements.\nUser Study. Participants showed valence dynamics more closely aligned with their valence reports during Session 1 when viewing the CEBRA-decoded true video compared to the fake video (see Figure 6a). A Wilcoxon rank-sum test on the best cross-correlation coefficients between real-time valence keypresses in Session 1 and 2 revealed that the true condition produced significantly higher coefficients than the perm condition (P = .012). Additionally, about 56% of participants reported that the true video better reflected their memories and associated affective changes (see Figure 6b). In a preference rating, 5 out of 9 participants chose the true video. These results suggest that our approach can be promising to capture one's idiosyncratic affect dynamics in their personal memories and generate them as music videos."}, {"title": "Conclusion & Future Works", "content": "This research makes significant contributions to cognitive/affective neuroscience and brain-computer interfaces (BCI). We introduce RecallAffectiveMemory, a novel task for studying dynamic affective states during autobiographical memory recall, supported by a comprehensive multi-modal dataset (EEG-AffectiveMemory). We demonstrate successful temporal affective trajectory decoding using CEBRA and present RYM, a framework generating personalized audiovisual content synchronized with individual affective experiences.\nOur evaluation, incorporating real-time user reports, temporal alignment of affective trajectories with generated content attributes (visual and musical), and representation-level validation (CLIP & CLAP), demonstrates the effectiveness of our approach in faithfully reconstructing affect-contextualized memories.\nDespite successfully generating affect-contextualized content, future work will address two key limitations: improving the smoothness of transitions between affective states in the generated content and exploring the feasibility of end-to-end generative models for more seamless integration of affective trajectories. Further research will also focus on broader applications, including therapeutic interventions and enhanced human-computer interaction."}]}