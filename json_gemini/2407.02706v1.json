{"title": "Pushing the Boundary: Specialising Deep Configuration Performance Learning", "authors": ["Jingzhi Gong"], "abstract": "Software systems often come with a multitude of configuration options that can be adjusted to adapt their performance (e.g., latency, execution time, and energy consumption) to various requirements. However, their combined influence on performance is often unknown, resulting in potential issues for software maintenance. Worse, the rapid growth in scale and complexity of modern software systems has made performance measurement increasingly resource-intensive, leaving limited datasets in most real-world scenarios. Consequently, it has become a major challenge to build accurate performance prediction models based on these limited measurements. To address this, deep learning approaches have gained popularity in recent years, due to their capabilities of capturing intricate representations and interactions even with only a few samples.\nTo facilitate this subject, this thesis starts by conducting a systematic literature review specialising the latest deep learning techniques employed for configuration performance modeling, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. The results disclose both positive and negative trends in the literature and reveal potential future directions to explore. Subsequently, three key knowledge gaps are incorporated and formalized as three objectives for this thesis to pursue.\nTherein, the first knowledge gap observed is that, despite the presence of different encoding schemes, there is still little understanding of which is better and under what circumstances, which could be harmful to the community. To bridge this gap, this thesis performs an empirical study on three of the most popular encoding schemes for configuration performance learning, namely label, scaled label, and one-hot encoding. The results demonstrate that choosing the encoding scheme is non-trivial, and thereafter, a list of actionable suggestions is provided to enable more reliable decisions.\nMeanwhile, the survey also reveals a crucial yet unaddressed knowledge gap, namely, the sparsity inherited from the configuration landscape. To handle this matter, this thesis presents a model-agnostic and sparsity-robust framework based on \"divide-and-learn\u201d, dubbed DaL. To mitigate the sample sparsity, the samples from the configuration landscape are divided into distant divisions, for each of which a deep learning model, e.g., Hierarchical Interaction Neural Network, is built to deal with the feature sparsity. Experiment results from 12 real-world systems and five sets of training data reveal that DaL performs better than the state-of-the-art approaches on 44 out of 60 cases with up to 1.61 times improvement in accuracy.\nNonetheless, similar to the majority of the studies reviewed, DaL is limited to predicting under static environments (e.g., hardware, version, and workload), which contradicts the dynamic nature of software. To address this concern, a sequential meta-learning framework is proposed in this thesis, named SeMPL, which significantly enhances the prediction accuracy of state-of-the-art models in multi-environment scenarios. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML) that train the meta environments in parallel, they are trained in a specialised order for deep neural networks. Through comparing with 15 state-of-the-art models under nine systems, it is demonstrated that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement.\nThrough the extensive studies conducted in this thesis, the critical knowledge gaps within the existing literature specialising deep performance learning have been identified and effectively addressed. As a result, the accuracy of performance learning has been significantly advanced to a new level of precision.\nKeywords: Highly Configurable Software, Configuration Performance Prediction, Configuration Performance Modeling, Configuration Performance Learning, Deep Learning, Software Engineering.", "sections": [{"title": "Introduction", "content": "To meet the different performance requirements of users, modern configurable and adaptable software systems often permit possible configuration options to be adjusted at runtime. For example, around one-third of the configuration options of the database system MYSQL\u00b9 can be changed at runtime, such as max_connections; the remaining ones, e.g., autocommit, need to be fixed prior to the deployment.\nGiven an environmental condition, these configuration options may have great impacts on the software performance (Chen, Li, Bahsoon & Yao 2018, Li & Chen 2023, Han & Yu 2016, Chen & Li 2024, 2023b,a). Indeed, inappropriate configurations often cause serious performance bugs, which is the key reason why the users became frustrated enough to (threaten to) switch to another product (Zaman et al. 2012). At the same time, simply using default configurations does little help. For instance, Herodotou et al. (2011) show that the default settings on HADOOP can actually result in the worst possible performance. In this regard, it is essential to understand the corresponding performance of a configuration, and a fundamental research problem is:\nGiven a set of configurations, what is the performance of the software?.\nIndeed, one solution is to directly profile the software system for all possible configurations when needed. This, however, is impractical because (i) the number of possible configurations may be too high. For example, MYSQL has more than millions of possible configurations. (ii) Even when such a number is small, the profiling of a single set of configurations can be too expensive. Wang et al. (2013) reports that it could take weeks of running time to benchmark and profile even a simple system. All these issues have called for a computational model, which is cheap to evaluate yet accurate, that captures the correlation between"}, {"title": "1.1 Configuration Performance Modeling with Deep Learning", "content": "A typical example of datasets employed in deep learning studies for performance prediction can be viewed in Table 1.1, which exhibits a set of configurations and their corresponding performance of the video codec software VP8. Particularly, x\u2081 to x\u2099\u208b\u2081 are binary options (e.g., the option to enable or disable alternative reference frames), x\u2099 represents a numerical option (the number of parallel processing threads), and P stands for the runtime.\nWithout loss of generality, deep configuration performance learning seeks to build a function f such that:\n$y = f(c)$  (1.1)\nwhereby y is the performance attribute that is of concern; c is a configuration consists of the values for n configuration options, i.e., c = {x\u2081, x\u2082,..., x\u2099}. Taking the simplest fully connected deep neural network as an example, f is represented as multiple layers of interconnected neurons, where neurons are activated as:\n$a_{j}^{l+1} = \u03c3(\\sum_{i}a_{i}^{l}w_{ij}^{l+1} + b_{j}^{l+1})$ (1.2)\nwhere \u2211 runs over all the lower-layer neurons that are connected to neuron j. a\u1d62 is the activation of a neuron i in the previous layer, and where $a_{ij}^{w^{l+1}}$ is the contribution of neuron i at layer I to the activation of the neuron j at layer 1 + 1. The function \u03c3 is a nonlinear monotonously increasing activation function, e.g., a sigmoid function; $w_{ij}^{l+1}$ is the weight and $b_{j}^{l+1}$ is the bias term.\nTo build function f, the training in deep learning aims to find the set of weights for different neurons from all the layers such that a loss function can be minimized. For example, the mean squared error below is one possible loss"}, {"title": "1.2 Aim, Objectives and Research Questions", "content": "Given the formal definition of the research problem, this section outlines the specific aim, objectives, and research questions that drive this thesis and guide the research conducted within it.\n1.2.1 Aim\nAs mentioned above, the accuracy of performance models is of utmost importance as it directly impacts their reliability and effectiveness. Yet, since deep learning models heavily rely on training data for accurate predictions, and collecting performance data can often be costly, it becomes imperative to maximize the utilization of limited and complex performance data to achieve improved prediction outcomes. Recent studies like (Ha & Zhang 2019a, Shu et al. 2020, Cheng et al. 2023) have highlighted that the accuracy of deep performance models is limited by a range of unsolved challenges, such as capturing the intricate relationships between software configurations and performance characteristics, addressing the curse of dimensionality in high-dimensional configuration spaces, and dealing with the sparsity of performance functions. Therefore, the aim of this thesis is:"}, {"title": "1.2.2 Objevtives", "content": "To accomplish the research aim, this thesis raises four key research objectives, each focusing on a specific aspect of the problem at hand, which are carefully examined and thoroughly discussed throughout the course of this study.\nFirstly, although several reviews have been done in the field of performance modeling with machine learning (Hort et al. 2022) or deep learning for general software engineering (Yang et al. 2022, Watson et al. 2022, Wang et al. 2023), to the best of my knowledge, none of them focuses specifically on deep learning techniques for performance modeling, which is one of the most popular and promising approaches in the recent years. Therefore, a systematic literature review (SLR) is necessary to summarize the application of techniques related to deep learning, identify any limitations and gaps of the current approaches, and analyze the reasons behind them.\nSecondly, given that the encoding scheme is a critical design decision for machine learning and deep learning models (Nair, Yu, Menzies, Siegmund & Apel 2018, Siegmund et al. 2015, Nair et al. 2017), it is surprised that the reviewed studies in the SLR (Chapter 3) hardly explain their motivation for choosing the encoding schemes. The lack of this knowledge will cause misunderstandings of model behaviors. Therefore, it is important to research the encoding schemes for performance learning.\nThen, During the SLR conducted in this thesis, it was observed that a list of performance learning studies share the viewpoint that the performance models of numerous software systems exhibit sparsity (Ha & Zhang 2019a, Glorot et al. 2011, Li et al. 2019, Grohmann, Eismann, Elflein, von Kistowski, Kounev & Mazkatli 2019), i.e., most of the configurable options have trivial influences on performance, and this sparsity could cause overfitting of the performance models and lead to poor accuracy of predictions. Yet, a major portion of studies specialising in deep learning fails to realize the sparsity problem. As such, developing DL models that can effectively overcome sparsity in performance learning holds promise for further improving prediction accuracy.\nFurthermore, the systematic literature review (SLR) conducted in this study"}, {"title": "1.2.3 Research Questions", "content": "In order to reach the research objectives, several research questions (RQs) must be answered. Noteworthy, the RQs are not all proposed from the very beginning of this thesis, but through a sequential process model as specified in Chapter 2. For example, the first RQ is raised initially, the second and third are developed by conducting a systematic literature review (details in Chapter 3), and the last one is identified during the iterated research procedures.\nSpecifically, to address objective 1, the first RQ is formulated as:"}, {"title": "1.3 Contributions", "content": "Through answering the research questions, this thesis has accomplished a collection of significant contributions. Particularly, Table 1.2 summarizes the key contributions and the addressed research objectives and questions."}, {"title": "1.4 Thesis Outline", "content": "Figure 1.1 captures the structure of this thesis, showcasing the logical flow and relations between the chapters. Specifically, all the studies conducted in this thesis follow a systematic and pragmatic research methodology as elaborated in Chapter 2. With the primary goal of establishing a concrete foundation and strong motivations, a systematic literature review is first performed in Chapter 3, from which the formal research aim, objectives, and research questions are extracted (Chapter 1). Following the objectives, dedicated artifacts are designed, implemented, and evaluated in Chapters 4, 5 and 6, respectively. Thereafter, the realization of the aim, the contributions, the recommendations, and the limitations of this thesis are discussed in Chapter 7. Finally, Chapter 8 revisits the key contents of the thesis.\nIn a nutshell, the outline of the chapters is as follows:"}, {"title": "2 Methodology", "content": "This chapter discusses the research methodology employed throughout this thesis, outlining its key components, objectives, and activities. It further explores how this methodology contributes to the overall success of the thesis and provides an overview of the research organization within this work."}, {"title": "2.1 Introduction", "content": "The methodology taken to carry out the studies in this thesis is a Design Science Research Methodology (DSRM) in the field of information systems, as proposed by Peffers et al. (2008). To be specific, design science (DS) is a research paradigm that focuses on the design and development of artifacts with the explicit intention of solving a problem. In the meantime, information systems (IS) is an applied discipline where data is analyzed by computer science technologies and turned into pragmatic information for the decision-making of organizations.\nTherein, the DSRM is a widely used framework for DS research in IS. It covers the three fundamental components of a methodology (Hevner et al. 2004), achieves three primary objectives of DS methodology, and contains six primitive activities.\nThe utilization of this methodology is natural because this thesis aligns perfectly with the principles of DS and IS. Firstly, this research aims at building accurate predicting artifacts for solving the regression problem between the software configurations and the performance attribute, fulfilling the core objective of design science. Secondly, the performance prediction artifact can be utilized to help software engineers make developing decisions and help users to tune their software based on their specific performance requirements.\nThis chapter is organized as follows. Section 2.2 introduces the three key elements of the DSRM and how they are related to this study, Section 2.3 explains the three objectives met by the methodology as well as their benefits to this study,"}, {"title": "2.2 Key Elements", "content": "According to Peffers et al. (2008), a DS research methodology is \"principles, practices, and procedures applied to a specific branch of knowledge\u201d. Therefore, any high-quality methodology for DS research should have these basic elements. The three elements provide significant guidance to this thesis.\nIn this section, the three elements of the DSRM, as well as their benefits to this thesis, are introduced.\nPrinciples\nThe principles of a DS research methodology define the conceptual meaning of the research. In the context of information systems, DSRM emphasizes the design, implementation, and improvement of artifacts that analyze and utilize data to support organizational decision-making (Hevner et al. 2004).\nThis principle aligns directly with the aim of this thesis, as outlined in Section 1.2, to develop advanced deep configuration performance models. By adopting this methodology, this thesis will empower software developers and users with their decision-making processes.\nPractice Rules\nAccording to Hevner et al. (2004), DSRM adheres to seven key practice rules to guide the design science research process: (1) the output of the research is an artifact, (2) the artifact is a solution to the research problem, (3) the effectiveness of the solution is evaluated, (4) the research output contributes to the current literature, (5) the development and evaluation methods of the artifact are rigorous, (6) search different techniques to meet the objective, and (7) the results are clearly presented to both the technical agents and business organizations.\nThis thesis has strictly adhered to all seven practice rules, ensuring the legitimacy and rigor of the research process. As a result, the thesis produces state-of-the-art performance models that effectively address the performance prediction problem (rules 1-4). Moreover, the models have been rigorously evaluated through comprehensive experiments and statistical tests, ensuring the significance of the results (rule 5). Additionally, the thesis explores various techniques and presents its findings clearly to both technical and non-technical audiences (rules 6-7)."}, {"title": "2.3 Accomplished Objectives", "content": "The DSRM adopted in this research successfully achieves three objectives critical for a successful design science research, as outlined by Peffers et al. (2008). The following sections will explain how each objective is met and its contribution to this study.\nProviding a Nominal Process\nThe DSRM offers a structured process for conducting design science research by assigning designated names to each research activity. This naming convention, as illustrated in Figure 2.1, enhances the legitimacy and clarity of the research process.\nBuilding upon Existing Research\nThe methodology integrates processes from previous successful design science research studies across various disciplines, such as information systems and engineering. By building upon the foundation of previous successful studies, this thesis increases its potential for impact and contribution to the field of software configuration performance prediction.\nProviding a Mental Model\nThe DSRM procedures serve as a mental model, guiding the organization and presentation of this research. This model facilitates a clear understanding of the research processes and results, ensuring a well-structured and coherent thesis."}, {"title": "2.4 Sequential Activities", "content": "The procedures of this DSRM can be represented by the process model in Figure 2.1. There are six key activities that should be conducted consecutively. Among the six activities, four entry points could be chosen according to the nature of the research, ranging from the first activity to the fourth. At the beginning of my doctoral study, the research problems are not identified. Therefore, the problem-centered initiation is chosen as the starting point.\nThis section provides detailed explanations of each activity and outlines the organization of this thesis.\nActivity 1: Problem and Importance Identification\nThis initial stage involves identifying the research problem and its importance. The research problem should be based on the current literature in the field and capture the knowledge gaps. Meanwhile, the importance of the research problem could motivate the researcher to conduct the studies and help the readers understand the problem better.\nIn this thesis, the research problem and importance are formed by a preliminary study of the existing studies in the domain of software engineering and performance engineering. Particularly, with the growing complexity of modern software, it is"}, {"title": "3 Deep Configuration Performance Learning: A Systematic Survey and Taxonomy", "content": "The design science research methodology employed in this thesis involves two primary activities: identifying the research aim and defining the objectives (as discussed in Chapter 2). Through the primary studies, the research aim of this thesis is identified, which focuses on efficiently addressing the limitations in modeling the relationship between the software configuration and performance and enabling next-level prediction accuracy. Consequently, to realize this aim, it is crucial to investigate the behaviors documented in the literature and identify the current limitations that need to be overcome. Thereby, the first objective of the thesis, which coincides with the objective of the SLR, is defined as follows:\nTo achieve these, a systematic literature review (SLR) is conducted between 2013 and 2023\u00b9. In the end, the SLR classifies the techniques applied and concludes the positive and problematic practices in the different processes of deep learning for performance modeling, such as data preparation, model training, evaluation, and application, by answering a set of sub-research questions. Subsequently, the results are analyzed and discussed, leading to the disclosure of potential knowledge gaps that need to be further bridged by this thesis. Thereby, additional objectives and research questions were established, which will be introduced in detail in the\n\u00b9The original SLR was conducted from 2015 to 2020, coinciding with the start of my PhD study. In 2023, the review scope is expanded, and the review findings are updated to include a broader time range."}, {"title": "3.1 Introduction", "content": "Recent studies have demonstrated the benefits of deep learning for modeling configuration performance. For example, Ha & Zhang (2019a) propose DeepPerf, a DNN-based model combined with L1 regularization to address the sparse performance functions, and Cheng et al. (2023) invent a hierarchical interaction neural network model called HINNPerf that achieves state-of-the-art MRE. Yet, despite the importance of such research direction, to the best of our knowledge, there has been little work on a systematic survey that covers the full spectrum of deep configuration performance learning. The current reviews related to this topic mainly focus on either general machine learning models (Pereira et al. 2021) or deep learning in the general context of software engineering (Yang et al. 2022, Watson et al. 2022, Wang et al. 2023). Undoubtedly, systematically reviewing state-of-the-art studies on this particular research field can provide vast benefits, including summarizing the common categories, revisiting the important concepts, and more importantly, discussing novel perspectives on the positive and negative practices of the field, and providing insights for future opportunities.\nTo bridge such a gap, in this paper, a systematic literature review that covers 948 papers from six online repositories and 52 venues, published between 2013 and 2023 is conducted, based on which 85 prominent studies were extracted for data extraction and analysis. The results confirm that the significance of deep learning for configuration performance and its associated challenges has led to a"}, {"title": "3.1.1 Research Questions", "content": "Following the DL pipeline in Figure 3.2, the sub-research questions of this survey are derived. Specifically, the workflow of deep learning for learning software configurations and performance is formalized into 4 main stages.\nParticularly, the first step is to process the raw performance data collected from the full configuration space and make preparations for inputs to the deep learning models, including procedures like preprocessing, encoding, and sampling. Therefore, the first sub-questions is:"}, {"title": "3.1.2 Contributions", "content": "To bridge the lack of understanding regarding deep learning models for software performance prediction, this chapter presents a systematic literature review that encompasses 948 papers sourced from six online repositories and 52 venues. The selected studies were published between 2013 and 2023, and 85 prominent studies were extracted for data extraction and analysis. The examination covers various stages within the deep configuration performance learning pipeline, including configuration data preparation, deep model training, accuracy evaluation, and model exploitation for configurable software systems.\nIn summary, the contributions of this chapter include:\n\u2022 A taxonomy that categorizes the techniques used and key concerns in deep configuration performance learning with up to 13 findings of the trends.\n\u2022 Comprehensive summaries of the key approaches used in the deep learning pipeline for configuration performance, including preparation, modeling, evaluation, and application, together with discussions on their benefits and shortcomings.\n\u2022 Articulation on the good practices and bad smells observed from the findings.\n\u2022 Gaps identified from existing studies, offering insights into the future opportunities for this particular thread of research.\nIn particular, the review results highlight several key observations that warrant attention in future studies:\n\u2022 More than half (45 out of 85) of studies have not applied any data preprocessing methods, which may limit the quality of the configuration data.\n\u2022 The justification for the choice of encoding scheme is ignored by 65 (74%) of the primary studies, which makes the influence and behavior of the encoding schemes unclear. Hence, it is crucial to conduct investigations specifically aimed at examining the influence of encoding schemes.\n\u2022 Random sampling, which is inefficient in finding the most informative configurations, is the most commonly used method, as used in 66 studies. This indicates the need for exploring more informative sampling methods to enhance the quality of the collected data.\n\u2022 A significant number of studies (33 out of 85) fail to address the issues of sparsity and overfitting, while 24 of the rest rely on inefficient manual feature selection methods. This emphasizes the importance of addressing sparsity issues and encourages researchers to explore alternative approaches.\n\u2022 The majority of studies (52 out of 85) apply manual hyperparameter tuning, which relies heavily on human experts. Future research should investigate automatic and heuristic hyperparameter tuning techniques to reduce tuning costs and enhance efficiency."}, {"title": "3.1.3 Chapter Outline", "content": "The chapter is organized as follows. Section 3.2 presents the systematic review methodology. Section 3.3, 3.4, 3.5, and 3.6 summarize the results of the review. Then, Section 3.7 discusses the trends identified from the SLR, possible knowledge gaps in the current literature, and the research objectives extracted from the SLR for further studies of this thesis. Finally, Section 3.8 justifies the threats to validity, and Section 3.9 concludes this chapter."}, {"title": "3.2 Research Methodology", "content": "This SLR covers the papers published between 2013 and 2023. This period was chosen because this thesis seeks to concentrate on the latest trends, avoiding noises from the old and disappeared practices in the field. In particular, the review methodology of this SLR follows the best practice of systematic literature review for software engineering (Kitchenham et al. 2009), as shown in Figure 3.3.\n3.2.1 Stage 1: Automatic Search\nAs can be seen in Figure 3.3, an automatic search over six highly influential indexing services was conducted, i.e., ACM Library, IEEE Xplore, Google Scholar, ScienceDirect, SpringerLink, and Wiley Online, as they have been used in a number of SLR in the field of software engineering and performance modeling (Nambiar"}, {"title": "4 On the Impact of Encoding Schemes for Performance Prediction: An Empirical Study", "content": "As shown in Chapter 1, learning and predicting the performance of a configurable software system using deep learning is crucial. One important engineering decision therein is how to encode the configurations into the most suitable form for the DL model. However, as disclosed in Chapter 3, despite the presence of different encoding schemes, there is still little understanding of which is better and under what circumstances, as the community often relies on some general beliefs that inform the decision in an ad-hoc manner. Thugs, the research question that still needs to be answered, as illustrated in Section 1.2.3, is:"}, {"title": "4.1 Introduction", "content": "A critical engineering decision to make in learning performance for configurable software is how to encode the configurations. In the literature, three encoding schemes are prevalent: (1) embedding the configuration options without scaling (label encoding) (Nair, Yu, Menzies, Siegmund & Apel 2018, Siegmund et al. 2015, Nair et al. 2017); (2) doing so with normalization (scaled label encoding) (Chen 2019, Chen & Bahsoon 2017a, Ha & Zhang 2019a) or (3) converting them into binary ones that focus on the configuration values of those options, each of which serves as a dimension (one-hot encoding) (Siegmund, Kolesnikov, K\u00e4stner, Apel, Batory, Rosenm\u00fcller & Saake 2012, Guo et al. 2013, Bao et al. 2019).\n4.1.1 Knowledge Gap\nExisting work takes one of these three encoding schemes without systematic justification or even discussions, leaving researchers with little understanding in this regard. This is of concern, as in other domains, such as system security analysis (Jackson & Agrawal 2019) and medical science (He & Parida 2016), it has been shown that the encoding scheme chosen can pose significant implications to the success of a machine/deep learning model. Further, choosing one in a trial-and-error manner for each case can be impractical and time-consuming, as it will be shown in Section 4.4. It is, therefore, crucial to understand how the encoding performs differently for learning performance of configurable software.\nTo provide a better understanding of RQ2 mentioned above, this thesis conducts an empirical study that systematically compares the three encoding schemes for learning software performance and discusses the insights learned. The hope is to provide a more justified understanding towards such an engineering decision in learning software performance under different circumstances.\n4.1.2 Research Questions\nIn order to enhance the generalizability of this study, it should be noted that machine learning models, apart from (deep) neural network, have also been con-"}, {"title": "5 Enhancing Performance Prediction by Handling Sparsity via Divide-and-Learn", "content": "As specified in Chapter 1, deep learning models have been widely adopted for predicting the configuration performance of software systems. However, in Chapter 3, the systematic literature review has revealed a crucial yet unaddressed challenge, which is how to cater for the sparsity inherited from the configuration land-scape: the influence of configuration options (features) and the distribution of data samples are highly sparse, which might significantly reduce the prediction accuracy of the deep performance learning models. This knowledge gap leads to the formulation of a crucial research question in this thesis:"}, {"title": "5.1 Introduction", "content": "Since machine learning modeling is data-driven, the characteristics and properties of the measured data for configurable software systems pose non-trivial challenges to the learning, primarily because it is known that the configuration landscapes of the systems do not follow a \u201csmooth\u201d shape (Jamshidi & Casale 2016). For example, adjusting between different cache strategies can drastically influence the performance, but they are often represented as a single-digit change on the land-scape (Chen & Li 2021c). This leads to the notion of sparsity in two aspects:\n\u2022 Only a small number of configuration options can significantly influence the performance, hence there is a clear feature sparsity involved (Huang et al. 2010, Siegmund et al. 2015, Ha & Zhang 2019a, Velez et al. 2021).\n\u2022 The samples from the configuration landscape tend to form different divisions with diverse values of performance and configuration options, espe-cially when the training data is limited due to expensive measurement\u2014a typical case of sample sparsity (Heck et al. 2022, Liu, Chen & Huang 2020, Shibagaki et al. 2016). This is particularly true when not all configurations are valid (Siegmund et al. 2015)."}, {"title": "6 Improving Performance Prediction in Multiple Environments with Sequential Meta-Learning", "content": "Although the DaL framework proposed in Chapter 5 achieves solving the sparsity problem and improves the prediction accuracy, the prediction is under a fixed environment, i.e., the model can not guarantee the prediction accuracy when the environment varies, which is one of the key knowledge gaps identified in this thesis, as justified in the Section 3.7 in Chapter 3. To address this challenge, a promising solution is to leverage knowledge from different environments. Hence, the fourth research question in this thesis is asked:"}, {"title": "6.1 Introduction", "content": "Through learning on the available data, current work has successfully leveraged deep learning to build various performance models (Cheng et al. 2023). Yet, those approaches mostly focus on configuration performance learning under one environment (Siegmund, Kolesnikov, K\u00e4stner, Apel, Batory, Rosenm\u00fcller & Saake 2012, Guo et al. 2018, Ha & Zhang 2019a, Valov et al. 2015, Queiroz et al. 2016, Chen et al. 2021, Siegmund, Rosenm\u00fcller, Kuhlemann, K\u00e4stner, Apel & Saake 2012), e.g., a pre-defined workload, a fixed hardware, and a specific version.\nWorking on a single environment is an over-optimistic assumption, as it is not uncommon to see configurable software systems run under diverse conditions. For example, a database system may experience both read-heavy and write-heavy workload (Jamshidi, Siegmund, Velez, K\u00e4stner, Patel & Agarwal 2017). Similarly, the hardware between the testing and production infrastructure might be drastically different (Leitner & Cito 2016, Brunnert et al. 2015), especially during the modern DevOps era. The ignorance of multiple environments would inevitably harm the effectiveness of a performance model. Jamshidi, Siegmund, Velez, K\u00e4stner, Patel & Agarwal (2017) reveal that the accuracy of a single environment model can be severely degraded when used in a different environment. Furthermore, due to the expensive measurement of configuration, e.g., it can take hours or days to measure only a few configurations (Valov et al. 2017, Chen & Li 2021d), building a new model for every distinct environment is unrealistic. Recently, at ICSE\u201923, M\u00fchlbauer et al. (2023) have demonstrated that predicting under multiple environments can pose significant threats to the robustness and generalizability of performance models learned using a single environment. Through a large-scale study, they concluded that:\n\u201cPerformance models based on a single workload are useless, unless the configuration options' sensitivity to workloads is accounted for.\u201d"}, {"title": "7 Discussion", "content": "The primary aim of this thesis is to push the boundaries of deep learning perform-ance models by enhancing their accuracy. In pursuit of this aim, four specific goals were set out, as outlined in Section 1.2.2. While these chapters have effectively addressed the research objectives, the extent to which the aim of this thesis has been accomplished has not been evaluated. Therefore, this chapter serves as a venue for comprehensive discussion and analysis of the outcomes of this thesis.\nBy pursuing the research aim and objectives, this thesis has resulted in five technical articles submitted or published in top Software Engineering proceedings, i.e., a systematic literature review (Chapter 3), an empirical study (Chapter 4), and three papers about deep learning models for performance prediction (two from Chapter 5 and one from Chapter 6). While the contributions of each study have been summarized in corresponding chapters, they have not been synthesized and discussed as a whole. Therefore, in this chapter, the contributions to the existing literature will be introduced.\nMoreover, this thesis not only contributes theoretical knowledge but also provides practical insights for researchers and practitioners. The implications and significance of the studies will be thoroughly justified in this chapter.\nAdditionally, although the previous chapters have highlighted the limitations specific to each study, the overall constraints and weaknesses in the design and implementation of this thesis have not been acknowledged. This chapter will provide a comprehensive overview of these limitations.\nIn summary, this chapter aims to discuss the accomplishments of the research objectives, outline the contributions to the field, justify the implications and sig-nificance of the studies, and address the limitations and weaknesses of this thesis as a whole."}, {"title": "7.1 Has the Research Aim Been Accomplished?", "content": "Recall from Section 1.2", "is": "nTo identify and overcome current limitations of performance modeling using deep learning, enable more accurate, robust, and reliable perform-ance predictions for configurable software systems, thereby pushing the boundaries of what can be achieved in this domain.\nTo achieve this aim, four key objectives were established, each of which plays a crucial role in fulfilling the aim. Specifically, objective 1, aiming at the system-atic review and categorization of the current literature, directly supports the aim by identifying the existing limitations and knowledge gaps in performance model-ing using deep learning, which is essential for overcoming the identified limitations and advancing the field. Objective 2, focusing on investigating the impact of en-coding schemes, contributes to the aim by fulfilling one of the knowledge gaps in understanding the behavior of encoding schemes under different conditions, which enables researchers to make informed decisions to optimize model accuracy. Moreover, objecttive 3 seeks to design a deep learning-based model that effect-ively tackles sparsity\u2014the limitation identified from the SLR, thereby improving the reliability and accuracy of predictions that coincide with the aim. Lastly, in objective 4, the target of enabling more accurate performance predictions in dynamic environments is realized by designing a DL-based model that can ad-apt and generalize across multiple environments, which is a critical constraint for configurable software systems.\nIn the process of fulfilling the aim of this thesis, the satisfaction of the four key objectives is evident in the preceding chapters. In particular, Chapter 3 presents a systematic literature review that surveys 85 latest deep learning studies in software performance modeling, providing taxonomies, positive trends, and negative trends of techniques used in the deep learning pipeline, and directions that need to be addressed by future studies, thereby, objective 1 is addressed.\nThen, Chapter 4 achieves objective 2"}]}