{"title": "Pushing the Boundary: Specialising Deep Configuration Performance Learning", "authors": ["Jingzhi Gong"], "abstract": "Software systems often come with a multitude of configuration options that can be adjusted to adapt their performance (e.g., latency, execution time, and energy consumption) to various requirements. However, their combined influence on performance is often unknown, resulting in potential issues for software maintenance. Worse, the rapid growth in scale and complexity of modern software systems has made performance measurement increasingly resource-intensive, leaving limited datasets in most real-world scenarios. Consequently, it has become a major challenge to build accurate performance prediction models based on these limited measurements. To address this, deep learning approaches have gained popularity in recent years, due to their capabilities of capturing intricate representations and interactions even with only a few samples.\nTo facilitate this subject, this thesis starts by conducting a systematic literature review specialising the latest deep learning techniques employed for configuration performance modeling, covering 948 searched papers spanning six indexing services, based on which 85 primary papers were extracted and analyzed. The results disclose both positive and negative trends in the literature and reveal potential future directions to explore. Subsequently, three key knowledge gaps are incorporated and formalized as three objectives for this thesis to pursue.\nTherein, the first knowledge gap observed is that, despite the presence of different encoding schemes, there is still little understanding of which is better and under what circumstances, which could be harmful to the community. To bridge this gap, this thesis performs an empirical study on three of the most popular encoding schemes for configuration performance learning, namely label, scaled label, and one-hot encoding. The results demonstrate that choosing the encoding scheme is non-trivial, and thereafter, a list of actionable suggestions is provided to enable more reliable decisions.\nMeanwhile, the survey also reveals a crucial yet unaddressed knowledge gap, namely, the sparsity inherited from the configuration landscape. To handle this matter, this thesis presents a model-agnostic and sparsity-robust framework based on \"divide-and-learn\u201d, dubbed DaL. To mitigate the sample sparsity, the samples from the configuration landscape are divided into distant divisions, for each of which a deep learning model, e.g., Hierarchical Interaction Neural Network, is built to deal with the feature sparsity. Experiment results from 12 real-world systems and five sets of training data reveal that DaL performs better than the state-of-the-art approaches on 44 out of 60 cases with up to 1.61 times improvement in accuracy.\nNonetheless, similar to the majority of the studies reviewed, DaL is limited to predicting under static environments (e.g., hardware, version, and workload), which contradicts the dynamic nature of software. To address this concern, a sequential meta-learning framework is proposed in this thesis, named SeMPL, which significantly enhances the prediction accuracy of state-of-the-art models in multi-environment scenarios. What makes it unique is that unlike common meta-learning frameworks (e.g., MAML) that train the meta environments in parallel, they are trained in a specialised order for deep neural networks. Through comparing with 15 state-of-the-art models under nine systems, it is demonstrated that SeMPL performs considerably better on 89% of the systems with up to 99% accuracy improvement.\nThrough the extensive studies conducted in this thesis, the critical knowledge gaps within the existing literature specialising deep performance learning have been identified and effectively addressed. As a result, the accuracy of performance learning has been significantly advanced to a new level of precision.\nKeywords: Highly Configurable Software, Configuration Performance Prediction, Configuration Performance Modeling, Configuration Performance Learning, Deep Learning, Software Engineering.", "sections": [{"title": "Introduction", "content": "To meet the different performance requirements of users, modern configurable and adaptable software systems often permit possible configuration options to be adjusted at runtime. For example, around one-third of the configuration options of the database system MYSQL\u00b9 can be changed at runtime, such as max_connections; the remaining ones, e.g., autocommit, need to be fixed prior to the deployment.\nGiven an environmental condition, these configuration options may have great impacts on the software performance (Chen, Li, Bahsoon & Yao 2018, Li & Chen 2023, Han & Yu 2016, Chen & Li 2024, 2023b,a). Indeed, inappropriate configurations often cause serious performance bugs, which is the key reason why the users became frustrated enough to (threaten to) switch to another product (Zaman et al. 2012). At the same time, simply using default configurations does little help. For instance, Herodotou et al. (2011) show that the default settings on HADOOP can actually result in the worst possible performance. In this regard, it is essential to understand the corresponding performance of a configuration, and a fundamental research problem is:\nGiven a set of configurations, what is the performance of the software?.\nIndeed, one solution is to directly profile the software system for all possible configurations when needed. This, however, is impractical because (i) the number of possible configurations may be too high. For example, MYSQL has more than millions of possible configurations. (ii) Even when such a number is small, the profiling of a single set of configurations can be too expensive. Wang et al. (2013) reports that it could take weeks of running time to benchmark and profile even a simple system. All these issues have called for a computational model, which is cheap to evaluate yet accurate, that captures the correlation between"}, {"title": "Configuration Performance Modeling with Deep Learning", "content": "A typical example of datasets employed in deep learning studies for performance prediction can be viewed in Table 1.1, which exhibits a set of configurations and their corresponding performance of the video codec software VP8. Particularly,\nx1 to xn\u22121 are binary options (e.g., the option to enable or disable alternative reference frames), xn represents a numerical option (the number of parallel processing threads), and P stands for the runtime.\nWithout loss of generality, deep configuration performance learning seeks to build a function f such that:\n$y = f(c)$\nwhereby y is the performance attribute that is of concern; c is a configuration consists of the values for n configuration options, i.e., c = {x1,x2,...,xn}. Taking the simplest fully connected deep neural network as an example, f is represented as multiple layers of interconnected neurons, where neurons are activated as:\n$a_j^{l+1} = \u03c3(\u03a3_i (a_i^l w_{ij}^{l+1} + b_j^{l+1}))$\nwhere \u03a3 runs over all the lower-layer neurons that are connected to neuron j.\na is the activation of a neuron i in the previous layer, and where aw_{ij}^{l+1} is the contribution of neuron i at layer I to the activation of the neuron j at layer l + 1. The function \u03c3 is a nonlinear monotonously increasing activation function, e.g., a\nsigmoid function; w_{ij}^{l+1} is the weight and b_j^{l+1} is the bias term.\nTo build function f, the training in deep learning aims to find the set of\nweights for different neurons from all the layers such that a loss function can\nbe minimized. For example, the mean squared error below is one possible loss"}, {"title": "Aim, Objectives and Research Questions", "content": "Given the formal definition of the research problem, this section outlines the specific aim, objectives, and research questions that drive this thesis and guide the research conducted within it.\nAs mentioned above, the accuracy of performance models is of utmost importance\nas it directly impacts their reliability and effectiveness. Yet, since deep learn-\ning models heavily rely on training data for accurate predictions, and collecting\nperformance data can often be costly, it becomes imperative to maximize the util-\nization of limited and complex performance data to achieve improved prediction\noutcomes. Recent studies like (Ha & Zhang 2019a, Shu et al. 2020, Cheng et al.\n2023) have highlighted that the accuracy of deep performance models is limited\nby a range of unsolved challenges, such as capturing the intricate relationships\nbetween software configurations and performance characteristics, addressing the\ncurse of dimensionality in high-dimensional configuration spaces, and dealing with\nthe sparsity of performance functions. Therefore, the aim of this thesis is:\nTo accomplish the research aim, this thesis raises four key research objectives, each\nfocusing on a specific aspect of the problem at hand, which are carefully examined\nand thoroughly discussed throughout the course of this study.\nFirstly, although several reviews have been done in the field of performance\nmodeling with machine learning (Hort et al. 2022) or deep learning for general\nsoftware engineering (Yang et al. 2022, Watson et al. 2022, Wang et al. 2023),\nto the best of my knowledge, none of them focuses specifically on deep learning\ntechniques for performance modeling, which is one of the most popular and prom-\nising approaches in the recent years. Therefore, a systematic literature review\n(SLR) is necessary to summarize the application of techniques related to deep\nlearning, identify any limitations and gaps of the current approaches, and analyze\nthe reasons behind them.\nSecondly, given that the encoding scheme is a critical design decision for ma-\nchine learning and deep learning models (Nair, Yu, Menzies, Siegmund & Apel\n2018, Siegmund et al. 2015, Nair et al. 2017), it is surprised that the reviewed\nstudies in the SLR (Chapter 3) hardly explain their motivation for choosing the\nencoding schemes. The lack of this knowledge will cause misunderstandings of\nmodel behaviors. Therefore, it is important to research the encoding schemes for\nperformance learning.\nThen, During the SLR conducted in this thesis, it was observed that a list of\nperformance learning studies share the viewpoint that the performance models of\nnumerous software systems exhibit sparsity (Ha & Zhang 2019a, Glorot et al. 2011,\nLi et al. 2019, Grohmann, Eismann, Elflein, von Kistowski, Kounev & Mazkatli\n2019), i.e., most of the configurable options have trivial influences on performance,\nand this sparsity could cause overfitting of the performance models and lead to\npoor accuracy of predictions. Yet, a major portion of studies specialising in deep\nlearning fails to realize the sparsity problem. As such, developing DL models that\ncan effectively overcome sparsity in performance learning holds promise for further\nimproving prediction accuracy.\nFurthermore, the systematic literature review (SLR) conducted in this study"}, {"title": "Research Questions", "content": "In order to reach the research objectives, several research questions (RQs) must\nbe answered. Noteworthy, the RQs are not all proposed from the very beginning\nof this thesis, but through a sequential process model as specified in Chapter 2.\nFor example, the first RQ is raised initially, the second and third are developed\nby conducting a systematic literature review (details in Chapter 3), and the last\none is identified during the iterated research procedures.\nSpecifically, to address objective 1, the first RQ is formulated as:\nTo fully understand this RQ and cover techniques used throughout the four\nstages in the deep learning pipeline, the following sub-RQs are posted:\nFollowing objective 2, the second RQ is set to be:\nTo research this question, an empirical study is conducted, and the below sub-\nRQs are asked in order to better understand the influence of encoding schemes:\nThen, to achieve objective 3, it is essential to ask:\nIn order to answer this RQ, this thesis proposes a novel DNN-based framework\ncalled DaL, which is systematically introduced in Chapter 5. To evaluate DaL, the\nfollowing sub-RQs are investigated:"}, {"title": "Contributions", "content": "Through answering the research questions, this thesis has accomplished a col-\nlection of significant contributions. Particularly, Table 1.2 summarizes the key\ncontributions and the addressed research objectives and questions."}, {"title": "Thesis Outline", "content": "Figure 1.1 captures the structure of this thesis, showcasing the logical flow and\nrelations between the chapters. Specifically, all the studies conducted in this\nthesis follow a systematic and pragmatic research methodology as elaborated in\nChapter 2. With the primary goal of establishing a concrete foundation and strong\nmotivations, a systematic literature review is first performed in Chapter 3, from\nwhich the formal research aim, objectives, and research questions are extracted\n(Chapter 1). Following the objectives, dedicated artifacts are designed, implemen-\nted, and evaluated in Chapters 4, 5 and 6, respectively. Thereafter, the realization\nof the aim, the contributions, the recommendations, and the limitations of this\nthesis are discussed in Chapter 7. Finally, Chapter 8 revisits the key contents of\nthe thesis.\nIn a nutshell, the outline of the chapters is as follows:"}, {"title": "Key Elements", "content": "According to Peffers et al. (2008), a DS research methodology is \"principles, prac-\ntices, and procedures applied to a specific branch of knowledge\u201d. Therefore, any\nhigh-quality methodology for DS research should have these basic elements. The\nthree elements provide significant guidance to this thesis.\nIn this section, the three elements of the DSRM, as well as their benefits to\nthis thesis, are introduced."}, {"title": "Accomplished Objectives", "content": "The DSRM adopted in this research successfully achieves three objectives critical\nfor a successful design science research, as outlined by Peffers et al. (2008). The\nfollowing sections will explain how each objective is met and its contribution to\nthis study."}, {"title": "Sequential Activities", "content": "The procedures of this DSRM can be represented by the process model in Fig-\nure 2.1. There are six key activities that should be conducted consecutively.\nAmong the six activities, four entry points could be chosen according to the nature\nof the research, ranging from the first activity to the fourth. At the beginning\nof my doctoral study, the research problems are not identified. Therefore, the\nproblem-centered initiation is chosen as the starting point.\nThis section provides detailed explanations of each activity and outlines the\norganization of this thesis."}, {"title": "Chapter Summary", "content": "This chapter introduces the design science research methodology employed in this\nthesis, which is a well-established approach in information systems research that\nprovides a structured framework for conducting and presenting research.\nThe DSRM consists of three components: the principles that define the mean-\ning of the research, the practice rules that provide conducting principles for this\nresearch, and the procedures which are step-by-step guidelines to conduct and\npresent this study."}, {"title": "Introduction", "content": "Recent studies have demonstrated the benefits of deep learning for modeling con-\nfiguration performance. For example, Ha & Zhang (2019a) propose DeepPerf,\na DNN-based model combined with L1 regularization to address the sparse per-\nformance functions, and Cheng et al. (2023) invent a hierarchical interaction neural\nnetwork model called HINNPerf that achieves state-of-the-art MRE. Yet, despite\nthe importance of such research direction, to the best of our knowledge, there\nhas been little work on a systematic survey that covers the full spectrum of deep\nconfiguration performance learning. The current reviews related to this topic\nmainly focus on either general machine learning models (Pereira et al. 2021) or\ndeep learning in the general context of software engineering (Yang et al. 2022,\nWatson et al. 2022, Wang et al. 2023). Undoubtedly, systematically reviewing\nstate-of-the-art studies on this particular research field can provide vast benefits,\nincluding summarizing the common categories, revisiting the important concepts,\nand more importantly, discussing novel perspectives on the positive and negative\npractices of the field, and providing insights for future opportunities.\nTo bridge such a gap, in this paper, a systematic literature review that covers\n948 papers from six online repositories and 52 venues, published between 2013\nand 2023 is conducted, based on which 85 prominent studies were extracted for\ndata extraction and analysis. The results confirm that the significance of deep\nlearning for configuration performance and its associated challenges has led to a"}, {"title": "Research Questions", "content": "Following the DL pipeline in Figure 3.2, the sub-research questions of this sur-\nvey are derived. Specifically, the workflow of deep learning for learning software\nconfigurations and performance is formalized into 4 main stages.\nParticularly, the first step is to process the raw performance data collected\nfrom the full configuration space and make preparations for inputs to the deep\nlearning models, including procedures like preprocessing, encoding, and sampling.\nTherefore, the first sub-questions is:\nFor RQ1.1, three key processes are examined:"}, {"title": "Contributions", "content": "To bridge the lack of understanding regarding deep learning models for software\nperformance prediction, this chapter presents a systematic literature review that\nencompasses 948 papers sourced from six online repositories and 52 venues. The\nselected studies were published between 2013 and 2023, and 85 prominent studies\nwere extracted for data extraction and analysis. The examination covers various"}, {"title": "Chapter Outline", "content": "The chapter is organized as follows. Section 3.2 presents the systematic review\nmethodology. Section 3.3, 3.4, 3.5, and 3.6 summarize the results of the review.\nThen, Section 3.7 discusses the trends identified from the SLR, possible knowledge\ngaps in the current literature, and the research objectives extracted from the SLR\nfor further studies of this thesis. Finally, Section 3.8 justifies the threats to validity,\nand Section 3.9 concludes this chapter."}, {"title": "Research Methodology", "content": "This SLR covers the papers published between 2013 and 2023. This period was\nchosen because this thesis seeks to concentrate on the latest trends, avoiding noises\nfrom the old and disappeared practices in the field. In particular, the review\nmethodology of this SLR follows the best practice of systematic literature review\nfor software engineering (Kitchenham et al. 2009), as shown in Figure 3.3."}, {"title": "Automatic Search", "content": "As can be seen in Figure 3.3, an automatic search over six highly influential in-\ndexing services was conducted, i.e., ACM Library, IEEE Xplore, Google Scholar,\nScienceDirect, SpringerLink, and Wiley Online, as they have been used in a num-\nber of SLR in the field of software engineering and performance modeling (Nambiar"}, {"title": "Removing Duplication and Filtering Irrelevance", "content": "Next, in stage 2, the objective was to ensure that only unique and highly relevant\nstudies were considered for further analysis in the review.\nTo achieve this, a careful examination of the titles of the identified papers\nwas conducted to filter out any duplicates. This step ensured that each study\nincluded in the review was distinct and provided unique insights into the body of\nknowledge.\nSubsequently, a brief evaluation was carried out to eliminate any documents\nthat were clearly irrelevant to the specific research topic. For example, studies\nfocused on human and student performance were excluded as the research did not\ndirectly address educational outcomes. Similarly, papers centered on the perform-\nance of physical systems, fuel systems, and football systems were disregarded as\nthey were not directly aligned with the scope of the investigation.\nAs a result of these rigorous filtering procedures, 193 highly relevant candidate\nstudies were identified, which have the potential to make significant contributions\nto the review."}, {"title": "Applying Inclusion and Exclusion Criteria", "content": "During stage 3 of the review process, various criteria were applied to extract a set\nof representative works.\nTo begin with, inclusion criteria were formulated to ensure that the selected\nstudies closely aligned with the core themes of the research. Specifically, these\ncriteria aimed to include studies that employed deep learning models for software\nperformance learning, which constituted the central focus of the study. Addi-\ntionally, the criteria emphasized the need for explicit details about the employed\nlearning algorithms, as this was crucial for understanding, analyzing, and applying\nthe models. Moreover, the inclusion of quantitative experiments was considered\nimportant to facilitate a robust evaluation process and enhance the depth and\nreliability of the selected studies.\nConversely, the exclusion criteria were designed to maintain the integrity and\nrelevance of the study. By excluding papers unrelated to software or systems,\nthe selected studies were ensured to relate to the research focus of the survey\ndirectly. Furthermore, the requirement for peer-reviewed publication enhanced\nthe credibility of the included studies, while excluding certain types of works\nsuch as surveys, reviews, tutorials, case studies, or empirical studies helped filter\nout literature that may not align with the objective of investigating performance\nprediction using deep learning. Lastly, limitations on the length of papers were"}, {"title": "Collecting and Extracting Data", "content": "After formulating the research questions, a comprehensive list of review elements\nhas been developed for collection during the review process. The summary of\nthese review elements is presented in Table 3.1, which includes a total of 22 re-\nview elements. Each element serves a specific purpose related to the correspond-\ning research question. This section explains the design rationales behind these\nelements and clarifies the procedure for extracting and classifying the data from\neach element."}, {"title": "RQ1.1: How to Prepare Performance Data?", "content": "As shown in Figure 3.2, data preparation stands at the forefront of deep learning\nproblems, serving as a critical step that is aimed at sampling, pre-processing, and\nencoding raw data to ensure its suitability for deep learning models. In order to\nimprove the accuracy and reliability of deep learning models to the best, data pre-\nparation is necessary (Huang et al. 2015). \u03a4o comprehensively explore the current\nliterature on data preparation methods for software performance prediction using\ndeep learning, this section addresses three key processes in this stage."}, {"title": "RQ1.2: How to Model Software Performance?", "content": "In the second phase of the workflow in Figure 3.2, the aim is to model the prepared\ndata from the previous phase accurately. This section addresses four key processes\nin this phase to provide a comprehensive understanding of how deep learning\ncan be effectively built to learn software performance, offering insights into best\npractices and potential problems in this field."}, {"title": "RQ1.3: How to Evaluate the Performance Model?", "content": "As shown in Figure 3.2, the third step of the deep learning workflow is to accurately\nevaluate the performance of deep learning models. This step is crucial as it decides\nthe effectiveness of the proposed model, and the evaluation results are important\nreferences for the decision-making of future researchers. In this section, three key\nprocesses for model evaluation are addressed. The first focuses on the evaluation\nmethods and metrics used to measure the performance of software performance\nprediction models; the second investigates the statistical test methods employed\nto validate the significance and effect size of the obtained results; while the third"}, {"title": "RQ1.4: How to Apply the Performance Model?", "content": "The successful application of deep learning methods in software performance pre-\ndiction relies on considering various factors, including application purposes, adapt-\nability to dynamic software running environments, and the availability of resources\nfor reproducibility. These analyses are crucial for researchers to effectively apply\nsuitable performance models for specific tasks, adapt to dynamic scenarios, and\nreproduce the proposed models. In this section, these aspects are explored by\ninvestigating the techniques applied in these three processes."}, {"title": "Answer to RQ1 and Discussion", "content": "The first objective of this thesis is to realize the knowledge gaps in the existing\nliterature via a comprehensive survey. This chapter will provide an overview of\nthe literature, give insights on the good trends that researchers can follow and bad\ntrends that can be avoided, and discuss the potential areas for improvement in\nfuture studies, hence demonstrating the accomplishment of the research object-\nive 1. Furthermore, the additional objectives will be derived from the identified\nknowledge gaps, followed by a discussion on the limitations, publication, and fu-\nture work of this chapter."}, {"title": "Chapter Summary", "content": "In this chapter, to achieve objective 1 mentioned in Section 1.2.3, a systematic\nliterature review is conducted to examine the techniques applied to each process\nin the pipeline of deep learning for performance modeling, covering 85 prominent\nwork from 948 studies found on six indexing services. The results are classified\nand presented, and the positive and negative practices of this field are disclosed\nin this chapter.\nParticularly, regarding data preparation, it is observed that over half of the\nstudies do not apply any preprocessing methods. Furthermore, the choice of en-\ncoding scheme is disregarded in a majority of the studies, and random sampling\nis the most commonly employed sampling technique. In the model training stage,\na significant number of studies overlook addressing sparsity and overfitting issues.\nFNN is the most extensively studied learning algorithm, while hyperparameter\ntuning is predominantly conducted manually. Additionally, a notable number of\nstudies omit to discuss the optimization method and activation function. Re-\ngarding model evaluation, the bootstrap method and MAPE (MRE) are the most\nwidely used evaluation methods and metrics, respectively. However, more than\n93% of the studies fail to examine the statistical significance and effect size. Re-\ngarding model application, it is observed that the majority of papers solely focus\non pure performance prediction tasks. Additionally, the challenge of dynamic en-\nvironments is not addressed in over half of the studies, and only 22% of the papers\nprovide an open-access repository."}, {"title": "Introduction", "content": "As shown in Chapter 1, learning and predicting the performance of a configurable\nsoftware system using deep learning is crucial. One important engineering decision\ntherein is how to encode the configurations into the most suitable form for the\nDL model. However, as disclosed in Chapter 3, despite the presence of different\nencoding schemes, there is still little understanding of which is better and under\nwhat circumstances, as the community often relies on some general beliefs that\ninform the decision in an ad-hoc manner. Thugs, the research question that still\nneeds to be answered, as illustrated in Section 1.2.3, is:\nTo bridge this gap, this chapter empirically compares the widely used encoding\nschemes for software performance learning, namely the label, scaled label, and one-hot\nencoding. The study covers five systems, seven models, and three encoding\nschemes, leading to 105 cases of investigation.\nThe key findings reveal that: (1) conducting trial-and-error to find the best\nencoding scheme in a case-by-case manner can be rather expensive, requiring up\nto 400+ hours on some models and systems; (2) the one-hot encoding often leads\nto the most accurate results while the scaled label encoding is generally weak on\naccuracy over different models; (3) conversely, the scaled label encoding tends to\nresult in the fastest training time across the models/systems while the one-hot\nencoding is the slowest; (4) for all models studied, label and scaled label encoding\noften lead to relatively less biased outcomes between accuracy and training time,"}, {"title": "Background", "content": "This section elaborates on the necessary background information and the motiv-\nation of the study."}, {"title": "Methodology", "content": "This section will discuss the methodology and experimental setup of the empirical\nstrategy for the study."}, {"title": "Analysis and Results", "content": "This section discusses the results of the empirical study with respect to the RQs."}, {"title": "Discussion", "content": "This section discusses a few interesting points derived from the empirical study,\nthe limitations and publications derived from this chapter, and possible future\ndirections."}, {"title": "Conclusions", "content": "By realizing objective 2 in this thesis, the knowledge gap in the understating\nof encoding schemes for learning performance for highly configurable software is\nbridged. It is done by conducting a systematic empirical study, covering five\nsystems, seven models, and three widely used encoding schemes, giving a total of\n105 cases of investigation.\nFirst, it is proven that:\nMore importantly, the findings observed from the study provide actionable\nsuggestions and a \"rule-of-thumb\" when a thorough experimental comparison is\nnot possible or desirable. Among these, the most important ones over all models\nand encoding schemes are:"}, {"title": "Introduction", "content": "As specified in Chapter 1, deep learning models have been widely adopted for pre-\ndicting the configuration performance of software systems. However, in Chapter 3,\nthe systematic literature review has revealed a crucial yet unaddressed challenge,\nwhich is how to cater for the sparsity inherited from the configuration land-\nscape: the influence of configuration options (features) and the distribution of\ndata samples are highly sparse, which might significantly reduce the prediction\naccuracy of the deep performance learning models. This knowledge gap leads to\nthe formulation of a crucial research question in this thesis:\nIn response to this RQ, this thesis proposes a model-agnostic and sparsity-\nrobust framework for predicting configuration performance, dubbed DaL, based\non the new paradigm of dividable learning that builds a model via \u201cdivide-and-\nlearn\". To handle sample sparsity, the samples from the configuration landscape\nare divided into distant divisions, for each of which a DL local model is built,\ne.g., regularized Hierarchical Interaction Neural Network, to deal with the feature\nsparsity. A newly given configuration would then be assigned to the right model of\ndivision for the final prediction. Further, DaL adaptively determines the optimal\nnumber of divisions required for a system and sample size without any extra\ntraining or profiling.\nExperiment results from 12 real-world systems and five sets of training data\nreveal that, compared with the state-of-the-art approaches, DaL performs no worse"}, {"title": "Background and Motivation", "content": "In this section, both a literature review and an empirical study are presented\nto comprehensively understand the properties of sparsity in learning software per-\nformance. Besides, the formal formulation of this problem is revisited, and the key\nobservations from the qualitative studies that motivate this work are introduced."}, {"title": "Divide-and-Learn for Performance Prediction", "content": "Drawing on the above-mentioned observations of the configuration data, this thesis\nproposes DaL: an approach that enables better prediction of the software perform-\nance via \"divide-and-learn\u201d. To mitigate the sample sparsity issue, the key idea\nof DaL is that, since different divisions of configurations show drastically diverse\ncharacteristics, i.e., rather different performance values with distant values of key\nconfiguration options, the framework seeks to independently learn a local model\nfor each of those divisions that contain locally smooth samples, thereby the learning\ncan be more focused on the particular characteristics exhibited from the divisions\nand handle the feature sparsity. Yet, this requires formulating, on top of the\noriginal regression problem of predicting the performance value, a new classifica-\ntion problem without explicit labels. As such, the original performance learning\nproblem formulation (Equation 5.1) is modified as follows:"}, {"title": "Experiment Setup", "content": "Here, the settings of the evaluation are delineated. In this work, DaL is implemen-\nted based on Tensorflow and scikit-learn. All experiments were carried out\non a server with 64-core Intel Xeon 2.6GHz and 256G DDR RAM."}, {"title": "Evaluation", "content": "This section presents and discusses the experimental results."}, {"title": "Discussion", "content": "Here, a few insights and pointers observed from the experiment results are dis-\ncussed, and then the answers to RQ3 of this thesis, the limitations, the publica-\ntions, and future plans are introduced."}, {"title": "Chapter Summary", "content": "To answer the third research question in the thesis, this chapter introduces a\nmodel-agnostic framework dubbed DaL that effectively handles the issues of both\nfeature and sample sparsity in configuration performance learning. The key nov-\nelty of DaL is that it follows a new paradigm of dividable learning, in which the\nbranches/leaves are extracted from a CART that divides the samples of configura-\ntion into a number of distant divisions, which is adaptively adjusted, and trains a\ndedicated local model for each division thereafter. Prediction of the new config-\nuration is then made by the local model of division inferred based on a Random\nForest classifier. Such a theory of \u201cdivide-and-learn\" handles the sample sparsity\nwhile the local model used (e.g., one with regularization) deals with the feature\nsparsity, hence collectively addressing the overall sparsity issue in configuration\ndata.\""}, {"title": "Introduction", "content": "Although the DaL framework proposed in Chapter 5 achieves solving the sparsity\nproblem and improves the prediction accuracy, the prediction is under a fixed\nenvironment, i.e., the model can not guarantee the prediction accuracy when the\nenvironment varies, which is one of the key knowledge gaps identified in this thesis,\nas justified in the Section 3.7 in Chapter 3. To address this challenge, a promising\nsolution is to leverage knowledge from different environments. Hence, the fourth\nresearch question in this thesis is asked:"}, {"title": "Background", "content": "To give a more comprehensive understanding of this chapter, this section intro-\nduces the detailed background of the single-task performance learning problem\nand then gives a formal definition of the most widely applied multi-environment\nlearning approaches."}, {"title": "Sequential Meta-Learning for Performance Prediction", "content": "To engineer the properties from the theory behind SeMPL that fulfill the require-\nments for configuration performance learning under multiple environments, the\nimplementation has three core components, namely, Sequence Selection, Meta-\nTraining, and Fine-Tuning. The former two resemble a pre-training process for\nthe outer and inner loop, whereas the last is triggered when the target environment\nbecomes available, as shown in Figure 6.4. These are specified below:"}, {"title": "Evaluation", "content": "This section presents and discusses the experiment results following the sub-RQs."}, {"title": "Discussion", "content": "This section discusses the specialty, applications, and overhead of SeMPL, and then\nillustrates the answer to RQ4, the limitations, publications, and future work of\nthis chapter."}, {"title": "Chapter Summary", "content": "To deal with multiple environments when predicting performance for configurable\nsoftware systems (the last research objective in the thesis), this chapter proposes a\nnew category of framework that leverages meta-learning and deep learning, dubbed\nSeMPL. What makes SeMPL unique is that it learns the meta environments, one at a\ntime, in a specific order according to the likely usefulness of the meta environment\nto the unforeseen target environment. Such sequential learning, unlike the existing\nparallel learning, naturally allows the pre-training to discriminate the contribu-\ntions between meta environments, thereby handling the largely deviated samples\nfrom different environments\u2014a key characteristic of the configuration data. Fur-\nther, although the base learner for SeMPL is a regularized deep neural network, it\ncan be applied with different types of models."}, {"title": "Has the Research Aim Been Accomplished?", "content": "Recall from Section 1.2, the aim of this thesis is:\nTo accomplish the research aim, four key objectives were established, each of which plays a\ncrucial role in fulfilling the aim. Specifically, objective 1, aiming at the system-\natic review and categorization of the current literature, directly supports the aim\nby identifying the existing limitations and knowledge gaps in performance model-\ning using deep learning, which is essential for overcoming the identified limitations\nand advancing the field. Objective 2, focusing on investigating the impact of en-\ncoding schemes, contributes to the aim by fulfilling one of the knowledge gaps\nin understanding the behavior of encoding schemes under different conditions,\nwhich enables researchers to make informed decisions to optimize model accuracy.\nMoreover, objecttive 3 seeks to design a deep learning-based model that effect-\nively tackles sparsity-the limitation identified from the SLR, thereby improving\nthe reliability and accuracy of predictions that coincide with the aim. Lastly,\nin objective 4, the target of enabling more accurate performance predictions in\ndynamic environments is realized by designing a DL-based model that can ad-\napt and generalize across multiple environments, which is a critical constraint for\nconfigurable software systems.\nIn the process of fulfilling the aim of this thesis, the satisfaction of the four key\nobjectives is evident in the preceding chapters. In particular, Chapter 3 presents a\nsystematic literature review that surveys 85 latest deep learning studies in software\nperformance modeling, providing taxonomies, positive trends, and negative trends\nof techniques used in the deep learning pipeline, and directions that need to be\naddressed by future studies, thereby, objective 1 is addressed.\nThen, Chapter 4 achieves objective 2 by performing a comprehensive em-\npirical study on the influence of encoding schemes. Through this investigation,\nthe chapter presents valuable insights into the behavior of different encoding ap-\nproaches on performance modeling under different situations, demonstrates the\nnecessity and expensiveness of searching for the best encoding scheme, and gives\nactionable suggestions on choosing the most suitable encoding method with differ-\nent requirements, which enables the readers to optimize their encoding strategies\nand create more explainable and reliable performance models."}, {"title": "What Does This Thesis Add to the Literature?", "content": "Throughout the progression of this thesis, a series of significant achievements\nhave been accomplished, as outlined in the previous chapters. Nonetheless, there\nis a lack of alignment between these achievements and the existing literature on\nperformance prediction, including beyond the domain of deep learning. In this\nsection, a comprehensive discussion on the introduced contributions of this thesis\nto the existing literature is presented."}, {"title": "Conclusion", "content": ""}]}