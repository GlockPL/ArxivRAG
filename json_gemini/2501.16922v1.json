{"title": "Agential AI for Integrated Continual Learning, Deliberative Behavior, and Comprehensible Models", "authors": ["Zeki Doruk Erden", "Boi Faltings"], "abstract": "Contemporary machine learning paradigm excels in statistical data analysis, solving problems that classical AI couldn't. However, it faces key limitations, such as a lack of integration with planning, incomprehensible internal structure, and inability to learn continually. We present the initial design for an AI system, Agential AI (AAI), in principle operating independently or on top of statistical methods, designed to overcome these issues. AAI's core is a learning method that models temporal dynamics with guarantees of completeness, minimality, and continual learning, using component-level variation and selection to learn the structure of the environment. It integrates this with a behavior algorithm that plans on a learned model and encapsulates high-level behavior patterns. Preliminary experiments on a simple environment show AAI's effectiveness and potential.", "sections": [{"title": "1 INTRODUCTION", "content": "The current machine learning (ML) paradigm uses continuous representations to approximate environmental structures through fixed internal architectures like neural networks (NNs). This approach has effectively addressed numerous challenges once considered among the toughest in AI, including vision [13], language processing [33], and complex behavior [17]. However, as these problems are solved, important limitations related to the methods of solving them and their practical integration into larger systems start to receive more attention [4, 16, 19, 32]. In particular; these models, heavily overparameterized with finite expressive potential, adapt by tuning continuous parameters rather than learning the structure topologically. Consequently, information is embedded in a distributed manner, leading to several important issues that are widely regarded as core limitations of machine learning (and NNs, its current dominant paradigm) - most notably the incapability of continual learning and information reuse, incomprehensibility and non-designability of the internal structure, and difficulty integrating learned information with deliberative behavior.\nThese issues originate from the shared limitation of approximating environmental structures with fixed models, rather than learning them topologically. They can be addressed collectively and without limitations of individual subfields tackling them separately, through a different design philosophy that tackles the problem from the ground up. To that end, we present the initial design of a system called Agential AI (AAI). The system consists of three components:\n\u2022 Modelleyen, an alternative learning mechanism exemplifying what we call a varsel mechanism, that captures the structure of the environment topologically in a discrete network without using gradients, enabling continual learning without destructive adaptation, and without task boundaries or replay, 1\n\u2022 Planlayan, a planning algorithm that executes goal-directed actions based on a model generated by Modelleyen,\n\u2022 A behavior encapsulation mechanism, currently demonstrated independently of agent operation, that decomposes behavior patterns produced by Planlayan into arbitrary hierarchical structures with autonomously detected subgoals.\nWe detail these components, explain how they overcome multiple major limitations of contemporary ML (detailed in the next section), and demonstrate their proof-of-principle operation on a simple test environment."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Common Limitations of ML Systems", "content": "Two most important core limitations of current ML systems are the inability of continual learning and incomprehensibility of internal structure; often tackled in isolation [10, 12, 14, 26, 31, 34]. These methods don't fully resolve the fundamental limitations of NNs but aim to mitigate their effects. For example, many continual learning solutions rely on assumptions that simplify the problem (e.g. externally defined task boundaries [12, 26] or storage and replay of past observations [1]) or only bias learning towards past tasks without ensuring true continual learning [14]. Similarly, Explainable AI methods [31] attempt to provide post-hoc explanations for the operation of neural networks, yet they fail to address the fundamental incomprehensibility of their internal structures, leaving them far from being truly engineerable. Furthermore, the challenge of explainability is often approached independently of the continual learning problem, rather than being seen as stemming from a shared underlying issue. Even studies that consider explainability within the context of continual learning tend to either propose distinct, sequentially applicable mechanisms for each problem separately [25] or focus on additional explainability challenges that arise when certain continual learning methods are used [5, 27]."}, {"title": "2.2 Deliberative Behavior", "content": "Planning is a well-established area of AI research [9], offering advantages over reward-based learning for reactive behavior [2], as it is more precise and doesn't require relearning for new goals. Traditional planning methods generally do not include environment"}, {"title": "2.3 Behavior Decomposition", "content": "A longstanding objective within the learning agents community has been to automatically break down behavior into distinct subunits, which is the primary motivation behind the subfield of Hierarchical Reinforcement Learning (HRL) [23]. However, this goal has yet to be achieved: current HRL methods produce rigid hierarchies that require predefining the structure in some form, with no exceptions known to us. Additionally, there is no existing capability for HRL-learned policies to be divided into multiple subpolicies, which is a fundamental requirement for flexible hierarchical structures. In this work, we present an initial demonstration of a behavior encapsulation mechanism (currently independent of the agent's operation) that can generate arbitrary hierarchical decompositions of behaviors designed by the planner. This mechanism can identify relevant subpolicies, along with their internal preconditions and subgoals, without any prior definitions, thus achieving the goal of HRL in a different context."}, {"title": "2.4 Summary", "content": "Table 1 summarizes the previous discussion. As mentioned earlier, these issues arise from the shared limitation of approximating environmental structures with fixed models, rather than learning them structurally. Therefore, once this fundamental challenge is addressed, the issues can be tackled collectively. This is the central goal of this work. 2"}, {"title": "3 MODELLEYEN", "content": "Modelleyen is designed to model sequential observations from an environment, but can be applied to any prediction task. It learns the environment's structure with minimal exposure, enabling information reuse and continual learning while maintaining consistency with past experiences. At the core of our method is a local variation and selection process - an important fundamental property of"}, {"title": "4 PLANLAYAN", "content": "We introduce Planlayan, an extension on Modelleyen designed to demonstrate goal-directed planning through backward tracking from desired goal states to current states.\nPreprocessing the model and Group SVs: We first briefly preprocess a learned model to reduce the number of connections. To this end, we group the sets of BSVs in our that are either (1) collectively act as positive or negative source of a CSV, or (2) have an event that is collectively predicted by a CSV. Each such grouping becomes a constituent of a Group SV (GSV). For example, if a CSV CO has positive sources (B0, B1, B2) and predicts deactivation of (B3, B4); then two GSVs are created: G0 = (B0, B1, B2), G2 = (B3, B4). This preprocessing stage is only for practical purposes and is not in principle needed for the operation of Planlayan, but we think it is essential for scalable representations of models learned by Modelleyen in the long run.\nMain Process of Planlayan: Planlayan constructs an action network (AN) based on a model generated by Modelleyen, incorporating alternative outcomes. An AN is a dependency graph with root nodes representing the current environmental states (current BSV, GSV, and DSVs), along with possible alternative connections (shown by multiple conditioning links from CSVs) needed to achieve a specified goal state variable (see Figure 8a example from experiments). To build this, we use a simple recursive function that generates"}, {"title": "4.1 Overview of the Agent's Operation Flow", "content": "In summary, the operation of an agent utilizing Modelleyen and Planlayan follows these steps, repeated continuously as the agent interacts with the environment in an online manner, without the need for episode division or offline learning periods:\n(1) Execute actions and gather the resulting observations from the environment.\n(2) Process the environment's observations and update the model (Modelleyen - Section 3, Algorithms 1 and 2.)\n(3) Generate a plan based on the current model and goals, then select an action from the resulting plan (Planlayan - Section 4, Algorithm 3.)"}, {"title": "5 BEHAVIOR ENCAPSULATION", "content": "Modelleyen and Planlayan together create a complete system capable of continual learning and structured goal-directed behavior. However, the exhaustive action networks produced by Planlayan do not exemplify a comprehensible representation, which is one of our key goals. Additionally, Planlayan does not fully leverage"}, {"title": "6 EXPERIMENTAL SETUP", "content": "We demonstrate the operation of AAI on a simple test environment, which is a finite-state machine (FSM) with two cells, each capable of seven states or inactivity, as shown in Figure 6. The environment includes three subtypes (\"RS\", \"SG\", \"NEG\"), illustrated by different colors. This setup was designed to model various types of temporal successions, such as basic succession, correlated changes, alternative causes/outcomes, uncertain transitions, and negative conditons. There is also a random variant of the environment where two additional states that get activated randomly are introduced, in order to test statistical significance filtering mechanisms. This environment was chosen in order to validate the core operation of AAI in a simple and understandable setting, which made in-depth analysis and debug of the design very feasible during development process. There is no inherent limitation to applying to more complex environments, akin to those used for testing e.g. RL algorithms, except that the planner implementation should incorporate the changes needed to make search nonexhaustive (see Sections 4 and 8). We leave validation on such environments and changes in design to future work, as this presentation is dense enough already."}, {"title": "7 RESULTS AND DISCUSSION", "content": "Base Planning: Table 2 compares episode durations for random actions (first 4000 steps) and planning (next 4000 steps). The planner"}, {"title": "8 CONCLUSION", "content": "Agential AI, consisting of Modelleyen, Planlayan, and the behavior encapsulator, offers a promising solution to the core challenges"}, {"title": "8.1 Future work", "content": "As mentioned earlier, the current version of AAI serves as a foundation to demonstrate core mechanisms. It has some venues of development that will addressed in future work.\nA primary class of issues that require attention revolves around the assumptions made regarding the structure of environment observations and how we model them. Specifically, the current model assumes a Markovian environment, focusing solely on immediate state transitions and neglecting long-term dependencies. Additionally, while Modelleyen can handle structured spaces like large visual observations (in the same way a fully-connected neural network processes images by representing each pixel as a separate feature), adapting it to more specific structures (like using CNNs or transformers instead of FCNNs) would enhance its scalability. Addressing those domains, along with potential others, requires adapting the Modelleyen algorithm to operate on networks as observations rather than lists of state variables. In particular, both visual spaces and temporal event chains can naturally be represented as finite networks with predefined structures. For visual spaces, this could mean networks of pixels or generalized base features such as edges; for event chains, the network would represent the sequence of events from the start of the current episode. These source networks could then undergo the same algorithmic steps presented in this paper (Figures 2 and 3) while maintaining the same guarantees of continual learning and minimality, with the primary modification being the redefinition of the refinement operation as \"network refinement.\" This process, already discussed in Section 5 (Figure 5) in the context of behavior encapsulation, would allow the method to represent visual or temporal spaces of any dimensions-or any other domain expressible as a network.\nCombined with Modelleyen's ability to learn complex succession relationships, this extension would enable the algorithm to tackle tasks with high dimensionality.\nIn addition to expanding the types of observation spaces processed, our framework has a few additional avenues that will be explored in future work. First, the statistical significance calculations in Modelleyen currently focus only on first-order relationships. For a more precise tracking of significances, this should be extended to incorporate upstream conditionings. Additionally, to scale Planlayan for more complex environments, selective pathway extension during planning is required. This can be achieved using existing mechanisms in Modelleyen, such as returning immediately when a viable path is found or prioritizing pathways based on statistical significance. Precise timing in Planlayan, where needed, can also be managed by evaluating the full consequences of each pathway and excluding those that reverse precondition states or activate conditions that hinder future actions. Another direction for future development is incorporating behavior encapsulation into ongoing operations to enable reusable behavior patterns. This is a key motivation for behavior encapsulation, shared by related fields like hierarchical reinforcement learning. We believe that the structured representations learned by AAI offer an ideal foundation for this process."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 Details of Modelleyen system components", "content": "We define a state variable (SV) as a variable that can take three values: 1 for active, -1 for inactive, and 0 which can be interpreted as unobserved, undefined, or irrelevant depending on context. Note that the numerical values are given only as shorthand notation and do not participate in an algebraic operation anywhere. The phrase nonactive refers to any SV that is not active. The SV construct comes in three subtypes: Base SVs (BSVs), Dynamics SVs (DSVs), Conditioning SVs (CSVs).\nBSV: BSVs are the externally-specified SVs whose states, which is assumed to be either 1 or -1, are provided externally to the system at each time instant. These can be regarded as the direct observations from the environment.\nDSV: Each BSV comes with two associated DSVs, for activation (A-DSV) and deactivation (D-DSV) respectively. Activation at timestep t is defined as the transition of a BSV state from -1 in step t - 1 to 1 in step t; and likewise deactivation at t is defined from 1 in t-1 to -1 in t. At step t, A-DSV is deduced active (state 1) if activation is observed at step t, inactive (-1) if a BSV is inactive at t 1 and no activation is observed at t, and undefined (0) if the BSV is already active. Symmetrically, at step t, D-DSV is deduced active (state 1) if deactivation is observed at step t, inactive (-1) if a BSV is active at t - 1 and no deactivation is observed at t, and undefined (0) if the BSV is already inactive. The BSVs are modelled only through changes in their states via their associated DSVs, and are not predicted by themselves.\nCSV: A CSV is a SV that conditions either DSVs or other CSVS (but not BSVs since they are not subject to direct modelling of their states); that is, predicts their activation. More specifically; each CSV comes with a set of positive and negative sources, where each source is either a BSV or DSV; and a set of targets, which correspond to the SVs that this CSV conditions. At steady state, a CSV's source conditions are said to be satisfied when all its positive sources were active and all its negative sources were nonactive in the previous step - in other words, the satisfaction corresponds to the condition all(positive sources) and not(any(negative source)) in the previous step. A CSV state is undefined (0) if its source conditions are not satisfied. If its source conditions are satisfied; a CSV's state is active (1) if the state of all its targets are either active or unobserved; and inactive (-1) if the state of all its targets are either inactive or un-observed. In case inactive and active targets are observed together, the CSV is duplicated to encompass the corresponding subsets of targets (as detailed below), hence we always ensure that one of the two above conditions will be satisfied with respect to the states of the targets. A CSV is to be interpreted as a state variable that represents the observance of a particular relationship - it being active means that this particular relationship (e.g. a change, as represented by a DSV, is observed conditioned on some sources) is observed, and it being inactive means that this relationship is not observed. The CSV being undefined or unobserved corresponds to the case in which the conditions for the observation of the relationship are not satisfied in the first place.\nPotential targets of conditioning (i.e. DSVs and CSVs), when they are not undefined, are expected to be active if one of their conditioners are active; and inactive otherwise. Furthermore, these types"}, {"title": "A.2 Learning the model", "content": "First, we provide an overview of the learning process in one step of interaction with the environment. During a step, the model is traversed, and the states of all its SVs are computed. For CSVs sources and targets are modified to be able to match the current states to the predictions/explanations of the CSV, so that the model is consistent with the environment at each step. After that, new CSVs are generated for the DSVs and CSVs that lack an explanation at the current step. The new CSV takes as positive sources all currently active eligible SVs in an exhaustive manner. Finally, model is refined by removal of unnecessary state variables.\nThe learning process is summarized formally on Algorithms 1 and 2. Below, we provide a detailed breakdown of the processes described on those algorithms.\nInitially, the model is generated with only BSVs and their associated DSVs, and without any CSV. At every step, the current and previous states of all the SVs are recorded, as well as the current and previous events (activation and deactivation) of every BSV.\nAt each step, the effective network created by DSVs and CSVs are traversed in the reverse order of computation, similar to backpropagation algorithm; starting from DSVs, then the CSVs that condition these BSVs, then the conditioners of these CSVs, and so on. Each traversed SV gets their state computed, and additionally CSV compositions are changed where needed, as in Figure 2 and detailed below."}, {"title": "A.3 Proof of Theorem 1", "content": "Let X and X be positive and negative sources of C respectively that remains after refinements that instance yi causes. Since we know that C does not undergo negative sources formation, and that yo comes before y1, we can say that X\u266a \u2286 X and X \u2264 X since only refinements are allowed on Xp and XN sets of C by our definition of operations.\nWe now analyse the two possible cases with respect to satisfaction of sources:\n\u2022 If, in the original encounter with yo the sources of C were satisfied, then we had $S_x = 1 \\forall x \\in X_p$ and $S_x = 1 \\forall x \\in X_N$. Since $X'_p \\subseteq X_p$ and $X'_N \\subseteq X_N$, we will also have $S_x = 1 \\forall x \\in X'_p$ and $S_x = 1 \\forall x \\in X'_N$ at the new encounter with instance yo. Hence, if sources of C were satisfied in the previous encounter with yo, they will remain satisfied in the new encounter. The value of Sc can be -1 or 1 if and only if sources of C are satisfied; in which case it is exclusively determined by the state of its targets (-1 if targets are inactive and 1 if targets are active). Since the states of targets are determined by yo and hence is the same across the past and new encounter with yo; if SC = 1(-1) in the past exposure with yo, then it will be 1(-1) in the new exposure as well.\n\u2022 If, in the original encounter with yo the sources of C were not satisfied (and hence original encounter yielded SC = 0), then we either had $S_x \\ne 1 \\forall x \\in X_p$ or $S_x = 1 \\forall x \\in X_N$ (note that we defined X and X as source sets after the refinements; and hence we know that in both cases it will be the whole of positive/negative source sets that have the property, and not a subset of them; since the source SVs that were not a part of that subset will have been refined). Since $X'_p \\subseteq X_p$ and $X'_N \\subseteq X_N$, we will also have either $S_x \\ne 1 \\forall x \\in X'_p$ (if former) or $S_x = 1 \\forall x \\in X'_N$ (if latter), both of them not satisfying the sources conditions of C (hence the new encounter with yo also yielding Sc = 0."}, {"title": "A.4 Learning the statistical significance of encountered relations", "content": "The base mechanisms of Modelleyen as described in the main text rest on an attempt of prediction of all encountered changes in state variables in the environment, forming an explanatory/predictive relationship between any two observed events in that attempt of full modelling of the environment. Unlike neural networks (or other statistical learning methods), the naive algorithm does not depend on, but also does not naturally incorporate, a method of statistically averaging and filtering learned relationships. Such a means of estimation of statistical significance of learned relationships can be incorporated into the models learned by modelleyen in a straightforward manner into the learned relationships locally, which in turn can be used to filter out non-significant relationships, hence preventing overcomplexification of the model.\nLet C be a CSV, and let T be a target SV of that CSV. We define the event sources satisfied, SS(C), to be the event where all positive sources of C are active and all negative sources are nonactive. For each target, we define an observation of the target O(T) to be when the target is observed (i.e. either active or inactive, state 1 or -1, as defined in the main text) and an incidence of the target I(T) to be when the target is active (state 1). We define the event concurrence to be the event where both the sources of C are satisfied and there is an indicence of target, CC(C, T) = SS(C) ^ \u0406(\u0422).\nWe quantify the statistical significance of a learned relationship between a set of sources of a CSV and one of its targets as the amount of increase in the probability of the incidence of the target given the satisfaction of the sources of the CSV. We define normalized causal effect (NCE) as the amount of increase in probability of incidence of T that satisfaction of sources of CSV C causes, normalized by the original probability of incidence:\n$NCE = \\frac{P(I(T)|SS(C)) \u2013 P(I(T))}{P(I(T))}$ (2)\nThe conditional probability in the nominator can be expanded as:\n$P(I(T)|SS(C)) = \\frac{P(I(T), SS(C))}{P(SS(C))} = \\frac{P(CC(C, T))}{P(SS(C))}$ (3)\nby our definition of concurrence CC(C, T) above. All of the probabilities can be computed by locally tracking of the number of instances that the corresponding events are observed, when the target is observed (i.e. O(T) = 1). When the target is unobserved/undefined, by extension none of the other events are observed.\nA positive NCE means that SS(C) increases probability of I(T) and a negative NCE means that SS(C) decreases it. An NCE of e.g. 2.0 means that SS(C) increases probability of I(T) to 3 times the original probability. Within the context of our modelling mechanism, a negative NCE means that the relationship between sources of C and T has been learned in the wrong direction - actual negative relations learned in proper direction will still result in positive NCE, because the sources of that relation will go within the negative sources of C instead of the positive ones, still in the end resulting in the SS(C). The lower the magnitude of NCE, the less significant the relationship is."}, {"title": "A.5 Details of experimental framework", "content": "Significance filtering. Modelleyen's mechanism of filtering based on statistical significance (i.e. NCE) is enabled only for the random variant of the environment. When enabled, we used a cutoff NCE of 0.25 for blocking upstream conditioner formations (i.e. no more upstream conditioners are formed if the CSV does not cause a >25% in the probability of occurrence of its target).\nIntuition regarding the design of environment in Figure 6. The environment was inspired from Multiroom environment in Minigrid. The states represent closed door (DC), open door (DO), wall (W), subgoal 1/2 (SG1/2), goal (G) and a random variable (X); \"RS\" stands for \"rooms\" and represents an agent going through multiple rooms opening doors in each, and \"SGS\" represents one in which agent reaches two subgoals and then reaches the goal afterwards, and \"NEG\" represents a case where goal appears conditioned on one positive and one negative conditon. In all, the goal can be moving. Alternative outcomes are present in all environment subtypes, since each of them allows for multiple outcomes following an empty (\"-/- \") state. Alternative predecessors are tested in \"SGS\" environment where SG2 can be preceded by SG1 in either of the two cells; and likewise in general the appearance of G can be preceded by any of the alternatives associated with different environment subtypes. The capability to represent positive and negative relations together is tested in subtype \"NEG\", in which G appears only if X is enabled in the first cell and not the second one.\nComputation resources. All experiments were run on a 2.4GHz 8-Core Intel Core i9 processor with 32 GB 2667MHz DDR4 memory. No GPU was used. Giving an accurate estimate for computation time is impossible since experiments were run in parallel to unevenly-distributed independent workloads."}, {"title": "A.6 A sample model learned on SMR", "content": "A sample model learned on the SMR environment (Figure 6) is provided on Figure 9. Figure ?? provides, as an example, the pathway of BSV 1G (state G at cell 1), in which the specific pathways connecting to this BSV can be seen more clearly in a human-comprehensible manner. Figure 11 shows the whole model, but only with reliable connections; clearly showing \"islands of certain state transitions\" which can be an example of a delimiting criterion that can be used for abstractions as discussed in the main text."}]}