{"title": "Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods", "authors": ["Haeun Yu", "Pepa Atanasova", "Isabelle Augenstein"], "abstract": "Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model's inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components. Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically. Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA. To align the results of the methods we introduce the attribution method NA-Instances to apply NA for retrieving influential training instances, and IA-Neurons to discover important neurons of influential instances discovered by IA. We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods. Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM's parametric knowledge compared to IA. Nevertheless, IA provides unique and valuable insights into the LM's parametric knowledge, which are not revealed by NA. Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM's parametric knowledge.", "sections": [{"title": "1 Introduction", "content": "Language Models encode the knowledge acquired during training as numeric values within the models' weights, transforming raw information from the training dataset into structured, internal representations. This embedding of knowledge, while fundamental to an LM's functionality, renders the model's inner workings opaque. To unravel the internal mechanisms of an LM and investigate the parametric knowledge encoded in an LM's weights, the development of eXplainable AI (XAI) methods is paramount.\nResearch in this domain has explored LM's parametric knowledge with various attribution methods (Bejan et al., 2023; Fan et al., 2023). Commonly used among these are Instance Attribution (IA, Koh and Liang (2017); Charpiat et al. (2019); Garima et al. (2020)) and Neuron Attribution (NA, Dai et al. (2022); Meng et al. (2022)). IA identifies training instances that influence the model's parametric knowledge leveraged for its prediction on a test instance. However, despite their utility, IA methods have been criticized for their sensitivity"}, {"title": "2 Related Work", "content": "To understand how models encode and utilise parametric knowledge, researchers have employed a variety of approaches (Xiong et al., 2024). Among these, Instance Attribution and Neuron Attribution have emerged as two principal methodologies.\nInstance Attribution (IA). IA identifies the training instances most influential to a model's prediction for a given test instance. IA's primary strength is that it provides a human interpretable explanation of the model's encoded parametric knowledge. Koh and Liang (2017) first introduced"}, {"title": "3 An Evaluation Framework for Attribution Methods", "content": "We next describe the proposed unified evaluation framework for attribution methods. To allow for the comparison of the two attribution methods IA and NA, we first propose to align their results to common views over the LM's parametric knowledge (\u00a73.2). Then, we describe the tests employed in the evaluation framework to assess whether the uncovered methods reflect the reasons employed by the models in their predictions (\u00a73.3, \u00a73.4)."}, {"title": "3.1 Preliminaries", "content": "Consider a test dataset XTest = [x1,x2,..., xn], consisting of n test instances and a train dataset XTrain = [xtrain1,xtrain2,...,xtrainm] consisting of m training instances. Consider also an LM f used to make a prediction on an instance: f(x) = y. Furthermore, we introduce an attribution method A \u2208 {IA, NA, IA \u2013 Neurons, NA \u2013 Instances}. For each test instance xi, and a model making a prediction f (xi), A computes influence scores of the training instances Sf(xi) = [strain1,strain2.., strainm] designating the extent of influence of the corresponding training instance on the learned parametric knowledge of an LM employed in its prediction on xi. A also returns r most important neurons Nf(xi) = [n1, n2,..., nr] and their corresponding attribution scores NS f(xi) = [ns1, ns2,...,nsr] which are regarded as important for the model's prediction on xi."}, {"title": "3.2 Aligning the Results of Attribution Methods.", "content": "First, to compare the results of IA to the list of most important neurons discovered by NA, we introduce IA-Neurons. IA-Neurons first produces a list of scores for the influence of the training instances Sf(x\u2081) and takes r most influential"}, {"title": "3.3 Neuron Attribution Faithfulness Tests", "content": "We employ two faithfulness tests to assess which Neuron Attribution method returns a list of most important neurons Nf(xi) for an LM's prediction on a test instance xi that is more faithful to the parametric knowledge employed by the LM in its prediction. Sufficiency and Comprehensiveness are two representative evaluation metrics of faithfulness. To evaluate Sufficiency, we only leave the selected r most important neurons Nf(xi) as activated, setting the activation of the remaining neurons Nf(x\u2081) to zero. If the original prediction of the model f is preserved, we claim that the neurons in N f(xi) are sufficient to reveal the parametric knowledge employed by f in its original prediction:\nf(xi; zero(Nf(xi))) = Yi; f(xi) = Yi'.\nIf \u0177i = Yi', Nf(xi) is sufficient.\nWe also employ a Comprehensiveness measure to estimate whether the explanation is complete and includes all parametric knowledge that is necessary to understand the model's inner workings. Contrary to Sufficiency, the most important neurons Nf(xi) are discarded by setting their activation to zero; we then verify whether the prediction changes:\nf(xi; zero(Nf(x\u2081))) = Yi; f (xi) = \u0177'\nIf yi \u2260 Yi', Nf(x\u2081) is comprehensive.\nFor the final Sufficiency and Comprehensiveness measures, we count the number of instances in the test set XTest where Eq. 2 and Eq. 3, correspondingly, hold."}, {"title": "3.4 Fine-tuning with Influential Training Instances", "content": "To evaluate the effectiveness of an attribution method in discovering the training instances that affect the learned parametric knowledge by the model, we conduct IA Faithfulness tests. To do so, we fine-tune a model f only with the most influential training instances S\u2081 and estimate the number of preserved predictions on the test set XTest."}, {"title": "4 Experimental Setup", "content": "In exploring natural language understanding, our research is directed towards tasks that necessitate complex reasoning and engage deeply with a model's acquired task knowledge. Our experiments involve Natural Language Inference (NLI), Fact-Checking (FC) and Question Answering (QA). We focus on selecting both diverse and complex tasks, thus, providing a robust platform for assessing the efficacy of IA and NA methods. One of the selected datasets \u2013 MNLI, is also commonly employed for evaluating attribution methods in related work. For NLI, the Multi-Genre Natural Language Inference (MNLI) is used (Williams et al., 2018). Due to the computational cost, we sampled 10K training instances from the training dataset (393K in total) following Pezeshkpour et al. (2021)'s work"}, {"title": "4.1 Datasets", "content": "To align for comparing the results obtained with NA to those obtained with IA, we first introduce a novel attribution method NA-Instances (bottom left, 'Attribution Results', Figure 1; \u00a73.4) that retrieves the training instances sharing the most similar neuron activations for each test instance. NA-Instances allows one to interpret the granular NA results and to compare its results to IA. On the other hand, to align the results of IA with NA, we introduce IA-Neurons (bottom right, \u2018Attribution Results', Figure 1; \u00a73.3), which finds the neurons of the most important training instances discovered by IA that have the highest activation."}, {"title": "4.2 Models", "content": "We select three open-sourced different autoregressive language models including OPT-125m (Zhang et al., 2022), BLOOM-560m (Yong et al., 2023) and Pythia-410m (Biderman et al., 2023). To see the impact of model size on the attribution explanations, all models are in different sizes. OPT is trained with a de-duplicated version of the Pile dataset (Gao et al., 2020). For Pythia, we choose the version that is trained with the original Pile dataset, containing duplicated files. BLOOM is oriented to the scientific domain employing multilingual scientific pretraining datasets. The models' performances on the tasks are available in Appendix A. To fine-tune the LMs, we use a Sequence Classification Head on the last token's hidden representation and train the models with cross-entropy loss. We provide further details about fine-tuning in Appendix A."}, {"title": "4.3 Attribution Methods", "content": "For IA methods, we consider two representative IA methods, IF (Koh and Liang, 2017) and Gradient Similarity (GS) method (Charpiat et al., 2019). Given a model f and test instance xi, IF (Equation 4) computes the importance score of training instance xtrain strain by upweighting xtrain with the Hessian of the loss function, dei = -HfL(xtrain, Ya, f). We refer to Koh and Liang (2017) for more details. GS (Equation 5) takes the dot product of the gradients of xi and xtrain. They are defined as follows:\nstraini,a= \u2207L(xi, Yi, f)T \u22c5 deidfm\ndestrain = \u2207fL(xi) \u22c5 \u2207fL(xtrain)"}, {"title": "4.4 Attribution Method Tests", "content": "For neuron attribution, we employ faithfulness tests covering both the sufficiency and comprehensiveness of the neurons to represent the model's parametric knowledge. For Sufficiency, we take the r = 1 neuron with the highest attribution score. For Comprehensiveness, r = 100 neurons are selected. To verify the utility of the neurons selected by the attribution methods, we also choose the same number of neurons randomly and present the result as a Random baseline."}, {"title": "5 Results", "content": ""}, {"title": "5.1 Number of Unique Influential Instances", "content": "Table 1 showcases the number of unique instances in the collection of the 10 most influential training instances for each test instance. Generally, we observe that IF and GS usually identify a smaller set of unique instances compared to NA-Instances - e.g., 30 vs 1776 unique instances discovered by IF/GS vs. NA-Instances correspondingly for MNLI and BLOOM-560m. An exception is the CoS-QA dataset, where we hypothesise that the majority of training instances in the CoS-QA dataset have fewer common neurons with the test instances as the test dataset of CoS-QA has fewer concepts in common with the training dataset. Empirically, we find that although most of the concepts2 in the test dataset were covered in the training dataset, the concepts from the test dataset only represent a small proportion of all concepts within the training dataset. There were 2099 unique concepts in the training dataset and only 725 concepts were used in the test dataset. This finding obtained from NA-Instances is in line with Huang et al. (2023)'s work, which claims that each neuron corresponds to a concept in an instance.\nFurther, we observe that increasing the model size correlates with a decrease in the number of unique training influential instances. This trend underscores the tendency towards homogeneity in the results produced by IA methods. Moreover, it highlights the potential advantage of the combination of IA and NA methods to retrieve a more diverse set of influential training instances. We further investigate the heterogeneity of the group of influential training instances in \u00a76."}, {"title": "5.2 Neuron Attribution Faithfulness Tests", "content": "We evaluate the sufficiency and comprehensiveness of the neurons to represent the model's parametric knowledge. Table 2 presents the number of the test instances for which the predictions are changed after keeping (Sufficiency) or suppressing (Comprehensiveness) the activation on the selected neurons. We find that NA performs slightly better than other attribution methods for AVeriTeC and MNLI. Furthermore, there are marginal differences between the Random baseline and IF-Neuron/GD-Neuron with less than 1.0 on both Sufficiency and Comprehensiveness. This implies that the explanations from IF-Neuron/GD-Neuron are not sufficient nor comprehensive enough to reveal the complete parametric knowledge used in the model's prediction. Notably, although we suppress the activation of all neurons within the MLP layer except one, the model can still recover the original prediction. We ascribe this phenomenon to the attention layers within the Transformer blocks. This finding aligns with Wiegreffe and Pinter (2019)'s argument, that attention weights pose a meaningful impact on the model's prediction and are important for understanding a model's inner workings. For a more holistic understanding of a model's parametric knowledge, we thus suggest that future work also studies attention-based neuron attribution methods."}, {"title": "5.3 Fine-tuning with Influential Training Instances", "content": "The models' performances from fine-tuning with different numbers of influential training instances are presented in Figure 2. We also provide the performances on the different datasets and models in Appendix B, Figure 6. From the figure, we find that the accuracies achieved with the first n% training instances are meaningful enough to show the different impact on the performance between n% most influential training instances and n% least influential training instances. The biggest gap in the accuracy achieved between training with the most and least influential ones is from the NA-Instances method - an accuracy gap of 0.6 for the AVeriTeC dataset. However, given that selecting the same proportion of training instances at random outperforms the attribution methods, we conclude that the influential training instances selected by IA methods (IF, GS) do not provide any benefit for explaining the performance of the final model. Unexpectedly, the training instances selected by NA-Instances-least achieve better performance in general than the randomly selected ones on the MNLI dataset. Although NA-Instances-least shows a different trend on the AVeriTeC and MNLI datasets, it outperforms other least influential groups. Since the group is composed of training instances that have minimal neuron overlapping with the test instances, we attribute this high performance to the instances in the set selected by NA-Instances-least being more diverse (as seen in general for instances discovered by NA-Instances in Table 1) leading to encompassing a more diverse set of the model's parametric knowledge."}, {"title": "6 Analysis", "content": "Next, we investigate what are the characteristics of the group of influential training instances and the group of most important neurons."}, {"title": "6.1 Overlap of the Attribution Results", "content": "Here we look at the overlap of influential instances as well as the overlap of the important neurons discovered by the corresponding attribution methods. First, we investigate the overlap between the first n influential training instances discovered by IF, NA-Instances, and GS, which are then used in the evaluation framework for fine-tuning with influential training instances (\u00a73.4). Figure 3 shows that for IF and GS, the overlap percentage is high - greater than 80%. This also explains their similar performance on the fine-tuning with influential training instances test (\u00a75.3). Furthermore, compared to the instance attribution methods IF and GS, NA-Instances discovers very different influential instances."}, {"title": "6.2 Diversity Analysis on the Group of Influential Training Instances", "content": "From the evaluation results in \u00a75.3, we hypothesize that greater diversity of the influential training instances found by an attribution method yields better performance, which we verify here. The heterogeneity of different groups of influential training instances can be measured at the lexical and parametric levels. To estimate lexical diversity, we compute the number of unique tokens (Vocabulary in Table 3) from the group of influential training instances and the average length of the training instances (Input Length in Table 3) as model input. The cosine similarity between the influential instances with the hidden representations from the last Transformer block (Cosine Similarity in Table 3) and the average loss (Loss in Table 3) are reported to show the parametric diversity of the selected influential training instances.\nTable 3 presents the result of this analysis on the AVeriTeC dataset and the MNLI dataset with the OPT-125m model, following the previous section. We find that the Random and NA-Instances-least methods that show a performance of 0.55 accuracy from Figure 2 contain more than 6900 unique tokens while other methods with less than 0.40 accuracy have 6600 tokens on average. From the parametric diversity metrics, the methods with lower performance collect training instances with a similar distribution of hidden representations and bigger losses. Furthermore, the least influential training instances discovered by IA methods have higher losses compared to the ones discovered by NA methods. However, we observe that the loss is not an indicator for the most or least influential training instances affecting the model's test set performance from the NA-Instances perspective.\nTo verify our findings statistically, we implement"}, {"title": "6.3 Dataset Artifact Detection", "content": "For a more human-interpretable analysis of the benefits of each IA method, we conduct a dataset artifact detection analysis with the NLI dataset HANS (McCoy et al., 2019), containing instances representing heuristics that NLI models are likely to learn. We take the test instances designed to contain lexical overlap heuristics between the premise and hypothesis to see what types of training instances are found as influential for a model's prediction on the said test instances. We run each instance attribution method on the MNLI training dataset and report the results in Table 4. We find that our proposed approach NA-Instances generally discovers more training instances which have a higher lexical overlap rendering the method to perform better in finding artifacts learned by a model from its training dataset."}, {"title": "7 Conclusion", "content": "In this paper, we propose a unified evaluation framework for comparing and contrasting two different attribution methods, IA and NA. To do so, we first introduce new attribution methods that align the neurons discovered by IA and NA. We assess the sufficiency and comprehensiveness of the explanations for both methods with different faithfulness tests and conduct extensive analyses of their explanations to further investigate the distinct characteristics that yield different results.\nThrough the proposed evaluation framework, we confirm that IA and NA result in different explanations about the knowledge responsible for the test prediction. This implies the potential advantage of combining IA and NA to unveil a comprehensive view of the LM's parametric knowledge. In addition, the experimental results on the attribution methods' faithfulness suggest that the neurons are not sufficient nor comprehensive enough to fully explain the parametric knowledge used for the test prediction. To complement this drawback of the current attribution methods, we hypothesize that this is due to the importance of the attention weights for encoding knowledge, leaving this exploration for future work."}, {"title": "Limitations", "content": "Our paper presents an investigation of the knowledge encoded by instance and neuron attribution methods. We perform our experiments using two instance attribution methods, one neuron attribution method, three natural language understanding datasets for three different tasks, and three language models. While we aimed to make a representative selection, future work should investigate other natural language processing tasks not focused on language understanding, as well as more different language models, including very large ones, which we could not study due to computational restrictions. It is also worth noting that the benchmark datasets we used consist of English text only, and we did not study domain adaptation or cross-lingual transfer scenarios.\nOne of our core findings was that selecting (in the case of sufficiency) or occluding (for comprehensiveness) knowledge neurons in the MLP layer (see \u00a75.2) only tells half the story \u2013 models still perform astonishingly well given only the one most important neuron. This means that, to truly understand the parametric knowledge of LLMs, the interplay between the neurons in the MLP layer and the neurons in the attention heads should be studied.\nLastly, we found that merely using attribution methods to identify instances to train on might not be a good strategy (see \u00a75.3). This is due to the lack of diversity in the resulting training set (see \u00a76.2). This naturally begs the question of what the purpose of instance and neuron attribution methods then is. \u00a76.3 suggests that the answer could be to identify dataset artifacts. However, these phenomena could still be studied in more depth for different downstream tasks."}, {"title": "A Implementation Details", "content": "To adapt the autoregressive LM to the targeted tasks we stack an MLP classification layer with the size of the LM's hidden representation on top of the last token's hidden representation. For AVeriTeC and MNLI, this outputs the probabilities over the labels. For AVeriTeC, we construct an input sequence with a claim and an evidence. With evidence documents that are a pair of question and answer, we concatenate all the pairs. Each LM with MLP classification layer is fine-tuned to predict the label given an input out of {Supported, Reputed, Conflicting Evidence/Cherrypicking, Not Enough Evidence}. For the MNLI dataset, a premise and hypothesis are provided as input. The model is trained to predict the label out of {neutral, contradiction, entailment}. For the sequences longer than the maximum length of the sequence, we remove tokens from the evidence text.\nFor the CoS-QA dataset, an input consists of a question and one candidate answer. We forward all possible sequences from one question at the same time to have the model learn the relative difference in the relationship between each candidate's answer and the question. Thus, the model encodes five sequences at the same time and produces five scores. Since we encode each candidate answer individually, the MLP classification layer for CoS-QA outputs a score for each sequence. Then, the model is trained to maximize the score of the sequence that has the correct answer using the listwise loss. To select the best checkpoint, we train the model for five epochs and report the accuracy on the test dataset from the model checkpoint that shows the best accuracy on the dev dataset. The learning rate is set to le -5 and the maximum length of the sequence is set to 512 tokens for AVeriTeC and MNLI. For the CoS-QA dataset, a maximum of 128 tokens is used. The performances on each dataset can be found in Table. For the OPT-125m model, we use a batch size of 8 for AVeriTeC and MNLI, and a batch size of 1 for CoS-QA. For the BLOOM-560m and Pythia-410m models, we use a batch size of 4 for AVeriTeC and MNLI and a batch size of 1 for CoS-QA. The training details are also applied to the fine-tuning of the models for the Instance Attribution Faithfulness Tests.\nWe use one Titan RTX GPU to fine-tune each"}, {"title": "B Fine-tuning with Influential Training Instances", "content": "Figure 6 presents the models' performances from \u00a75.3. By conducting the evaluation on the sufficiency of the influential training instances, we confirm that various language models show similar trends with the same dataset. For the AVeriTeC, the group of randomly selected training instances (Random) outperforms other groups. On the other hand, the training instances selected by NA-Instances/least consistently show better performances than any other groups in general for the MNLI. With the CoS-QA, we observe that the training instances from -most methods perform better than the group of instances from -least methods."}]}