{"title": "Reinforcement Learning with LTL and w-Regular Objectives via Optimality-Preserving Translation to Average Rewards", "authors": ["Xuan-Bach Le", "Dominik Wagner", "Leon Witzman", "Alexander Rabinovich", "Luke Ong"], "abstract": "Linear temporal logic (LTL) and, more generally, w-regular objectives are al-ternatives to the traditional discount sum and average reward objectives in rein-forcement learning (RL), offering the advantage of greater comprehensibility and hence explainability. In this work, we study the relationship between these ob-jectives. Our main result is that each RL problem for w-regular objectives can be reduced to a limit-average reward problem in an optimality-preserving fashion, via (finite-memory) reward machines. Furthermore, we demonstrate the efficacy of this approach by showing that optimal policies for limit-average problems can be found asymptotically by solving a sequence of discount-sum problems approx-imately. Consequently, we resolve an open problem: optimal policies for LTL and w-regular objectives can be learned asymptotically.", "sections": [{"title": "Introduction", "content": "Reinforcement learning (RL) is a machine learning paradigm whereby an agent aims to accomplish a task in a generally unknown environment [36]. Traditionally, tasks are specified via a scalar reward signal obtained continuously through interactions with the environment. These rewards are aggre-gated over entire trajectories either through averaging or by summing the exponentially decayed rewards. However, in many applications, there are no reward signals that can naturally be extracted from the environment. Moreover, reward signals that are supplied by the user are prone to error in that the chosen low-level rewards often fail to accurately capture high-level objectives. Generally, policies derived from local rewards-based specifications are hard to understand because it is difficult to express or explain their global intent.\nAs a remedy, it has been proposed to specify tasks using formulas in Linear Temporal Logic (LTL) [40, 29, 9, 37, 15, 33, 14] or w-regular languages more generally [29]. In this framework, the aim is to maximise the probability of satisfying a logical specification. LTL can precisely express a wide range of high-level behavioural properties such as liveness (infinitely often P), safety (always P), stability (eventually always P), and priority (P then Q then T).\nMotivated by this, a growing body of literature study learning algorithms for RL with LTL and w-regular objectives (e.g. [40, 15, 29, 7, 31, 20, 21, 16]). However, to the best of our knowledge, all of these approaches may fail to learn provably optimal policies without prior knowledge of a generally unknown parameter. Moreover, it is known that neither LTL nor (limit) average reward\nFormally, for some  \u2208, \u03b4 > 0 it is impossible to learn e-approximately optimal policies with probability \u03b4 in finite time."}, {"title": "Background", "content": "Recall that a Markov Decision Process (MDP) is a tuple M = (S, A, so, P) where S is a finite set of states, so \u2208 S is the initial state, A is the finite set of actions and P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the probability transition function such that \u2211s'es P(s, a, s') = 1 for every s \u2208 S and a \u2208 A. MDPs may be graphically represented; see e.g. Fig. 1a. We let Runs (S, A) = S \u00d7 (A \u00d7 S)* and Runs(S, A) = (S \u00d7 A) denote the set of finite runs and the set of infinite runs in M respectively.\nA policy \u03c0 : Runs (S, A) \u2192 D(A) maps finite runs to distributions over actions. We let II(S, A) denote the set of all such policies. A policy \u03c0 is memoryless if \u03c0(Soap ... Sn) = \u03c0(sa... s'm) for all finite runs soao... sn and sao... s'm such that sn = s'm. For each MDP M and policy \u03c0, there is a natural induced probability measure DM on its runs.\nThe desirability of policies for a given MDP M can be expressed as a function J : II(S, A) \u2192 R. Much of the RL literature focuses on discounted-sum IRM and limit-average reward objectives Trag, which lift a reward function R : S\u00d7A\u00d7S \u2192 R for single transitions to runs p = s0a081a1 ..."}, {"title": "w-Regular Objectives.", "content": "w-regular objectives (which subsume LTL objectives) are an alternative to these traditional objectives. Henceforth, we fix an alphabet AP and a label function \u03bb : S\u00d7A\u00d7 S\u2192 2AP for transitions, where 2X is the power set of a set X. Each run p = s0A0S1A182... induces a sequence of labels \u03bb(p) = \u03bb(8o, ao, 81)\u03bb(81,a1,82).... Thus, for a set L C (2AP)w of \"desirable\" label sequences we can consider the probability of a run's labels being in that set: Pp~DM[1(p) \u2208 L].\nFor instance, an autonomous car may want to \"visit a petrol station exactly once\" to conserve resources (e.g. time or petrol). Consider the MDP in Fig. 1a where the state s1 represents a petrol station. We let AP = {p} (p for petrol), A(50, b, s\u2081) = {p}, and the rest are labeled with (\u00d8). The desirable label sequences are L = {1102\u00b7\u00b7\u00b7| for exactly one i \u2208 N, \u03bb\u2081 = {p}}.\nIn this work, we focus on L which are w-regular languages. It is well known that w-regular languages are precisely the languages recognised by Deterministic Rabin Automata (DRA) [26, 28]:"}, {"title": "Remarks.", "content": "The class of w-regular languages subsumes languages expressed by Linear Temporal Logic (LTL, see e.g. [5, Ch. 5]), a logical framework in which e.g. reachability (eventually P, \u25c7P), safety (always P, \u25a1P) and reach-avoid (eventually P whilst avoiding Q, (\u00acQ)U P) properties can be expressed concisely and intuitively. The specification of our running Example 1 to visit the petrol station exactly once can be expressed as the LTL formula (\u00acp) U (p\u2227\u25cb\u25a1\u00acp), where OQ denotes \"Q holds at the next step\". Furthermore, our label function \u5165, which maps transitions to labels, is more general than other definitions (e.g. [40, 15, 29]) instead mapping states to labels. As a result, we are able to articulate properties that involve actions, such as \"to reach the state s while avoiding taking the action a\"."}, {"title": "Optimality-Preserving Specification Translations.", "content": "Rather than solving the problem of synthe-sising optimal policies for Eq. (1) directly, we are interested in reducing it to more traditional RL"}, {"title": "Negative Result and Reward Machines", "content": "Reward functions emit rewards purely based on the transition being taken without being able to take the past into account, whilst DRAs have finite memory. Therefore, there cannot generally be optimality-preserving translations from w-regular objectives to limit average rewards provided by reward functions:\nRemarkably, this rules out optimality-preserving specification translations even if transition proba-bilities are fully known\u00b3.\nSince simple reward functions lack the expressiveness to capture w-regular objectives, we employ a generalisation, reward machines [23, 24], whereby rewards may also depend on an internal state:"}, {"title": "Warm-Up: Transitions with Positive Probability are Known", "content": "To help the reader gain intuition about our construction, we first explore the situation where the sup-port {(s, a, s') \u2208 S \u00d7 A \u00d7 S | P(s, a, s') > 0} of the MDP's transition function is known. Crucially, we do not assume that the magnitude of these (non-zero) probabilities are known. Subsequently, in Section 5, we fully eliminate this assumption.\nThis assumption allows us to draw connections between our problem and a familiar scenario in probabilistic model checking [5, Ch. 10], where the acceptance problem for w-regular objectives can be transformed into a reachability problem. Intuitively, our reward machine monitors the state of the DRA and provides reward 1 if the MDP and the DRA are in certain \"good\" states (0 otherwise).\nFor the rest of this section, we fix an MDP without transition function (S, A, so), a set of possible transitions E C S\u00d7A\u00d7S, a label function A : S\u00d7A\u00d7S \u2192 2AP and a DRA A = (Q, 2AP, q\u0ed0, \u03b4, F). Our aim is to find a reward machine R such that for every transition function P compatible with E (formally: E = {(s,a,s') | P(s, a, s') > 0}), optimal policies for limit-average rewards are also optimal for the acceptance probability of the DRA A."}, {"title": "Product MDP and End Components", "content": "First, we form the product MDP MA (e.g. [40, 15]), which synchronises the dynamics of the MDP M with the DRA A. Formally, M & A = (V, A, \u03c5\u03bf, \u2206\u2206, FM) where V = S \u00d7 Q is the set of states, A is the set of actions, vo = (so, qo) is the initial state. The transition probability function \u2206:V\u00d7A\u00d7V \u2192 [0, 1] satisfies \u2206(v, a, v') = P(s, a, s') given that v = (s,q), v' = (s', q'), and d(q, A(s, a, s')) = q'. The accepting condition is FM = {(A1, R\u2081), (A2, R2), ...} where A = S \u00d7 Ai, B\u2081 = S \u00d7 Bi, and (Ai, Bi) \u2208 F. A run p = (so, qo)ao is accepted by M & A if there exists some (A', R) \u2208 FM such that InfV(p) \u2229 A \u2260 0 and InfV (p) \u2229 R = 0, where InfV is the set of states (s, v) in the product MDP visited infinitely often by p.\nNote that product MDPs have characteristics of both MDPs and DRAs which neither possesses in isolation: transitions are generally probabilistic and there is a notation of acceptance of runs. For example, the product MDP for Fig. 1 is shown in Fig. 2b. Due to the deterministic nature of the DRA A, every run p in M gives rise to a unique run p\u00ae in M & A. Crucially, for every policy \u03c0,"}, {"title": "Reward Machine and Correctness", "content": "Next, to ensure that runs eventually commit to one such ASEC we introduce the following nota-tional shorthand: for (s, q) \u2208 T\u2081 \u222a\uff65\uff65\uff65U Tn, let C(s,q) = (T(s,q), Act(s,q)) be the C\u2081 with minimal i containing (s, q), i.e. C(s,q) := Cmin{1<i<n|(s,q)\u2208T}.\nIntuitively, we give a reward of 1 if (s, q) is in one of the C1,..., Cn. However, once an action is performed which deviates from Act(s,q) no rewards are given thereafter, thus resulting in a limit average reward of 0.\nA state in the reward machine has the form q \u2208 Q, keeping track of the state in the DRA, or \u22a5, which is a sink state signifying that in a state in C1, . . ., Cn we have previously deviated from Act(s,q).\nFinally, we are ready to formally define the reward machine R = R(S,A,A,A) exhibiting our specifi-cation translation as (Q\u222a {1}, 90, \u03b4\u03b9, \u03b4r), where"}, {"title": "Main Result", "content": "In this section, we generalise the approach of the preceding section to prove our main result:\nAgain, we fix an MDP without transition function (S, A, so), a label function X : S \u00d7 A \u00d7 S \u2192 2AP and a DRA A = (Q, 2AP, q\u0ed0, \u03b4, F). Note that the ASECs of a product MDP are uniquely determined by the non-zero probability transitions. Thus, for each set of transitions E \u2286 (S \u00d7 Q) \u00d7 A \u00d7 (S \u00d7 Q), we let CF = (T1, Act\u2081), ..., CH = (Tn, Actn) denote a collection of ASECs covering all states in ASECs w.r.t. the MDPs in which E is the set of non-zero probability transitions.5 Then, for each set E and state (s, q) \u2208 TE U\u2026\u2026 UTE, we let CE,q) = (TE,q), Acts,q)) be the ASEC CE that contains (s, q) in which the index i is minimal.\nOur reward machine R = R(S,A,1,A) extends the ideas from the preceding section. Importantly, we keep track of the set of transitions E taken so far and assign rewards according to our current knowledge about the graph of the product MDP. Therefore, we propose employing states of the form (q, f, E), where q \u2208 Q keeps track of the state of the DRA, f \u2208 {T, 1} is a status flag and EC (S\u00d7 Q) \u00d7 A \u00d7 (S \u00d7 Q) memorises the transitions in the product MDP encountered thus far.\nIntuitively, we set the flag to | if we are in MDP state s, (s, q) is in one of the CF, . . ., CE and the chosen action deviates from Act(s,q) (s, q). We can recover from | by discovering new transitions. Besides, we give reward 1 if f = T and (s, q) is in one of the CF, . . ., CE (and 0 otherwise).\nThe status flag is required since discovering new transitions will change the structure of (accepting simple) end components. Hence, differently from the preceding section, it is not sufficient to have a single sink state.\nThe initial state of our reward machine is uo:=(qo, T, (\u00d8) and we formally define the update and reward functions as follows:"}, {"title": "Convergence for Limit Average, w-Regular and LTL Objectives", "content": "Thanks to the described translation, advances (in both theory and practice) in the study of RL with average rewards carry over to RL with w-regular and LTL objectives. In this section, we show that it is possible to learn optimal policies for limit average rewards in the limit. Hence, we resolve an open problem [3]: also RL with w-regular and LTL objectives can also be learned in the limit.\nWe start with the case of simple reward functions R : S \u00d7 A \u00d7 S \u2192 R. Recently, [18] have shown that discount optimal policies for sufficiently high discount factor \u2208 [0,1) are also limit average optimal. This is not enough to demonstrate Theorem 16 since 7 is generally not known and in finite time we might only obtain approximately limit average optimal policies.\nOur approach is to reduce RL with average rewards to a sequence of discount sum problems with increasingly high discount factor, which are solved with increasingly high accuracy. Our crucial insight is that eventually the approximately optimal solutions to the discounted problems will also be limit average optimal (see Appendix D for a proof):\nOur proof harnesses yet another notion of optimality: a policy \u03c0 is Blackwell optimal (cf. [6] and [22, Sec. 8.1]) if there exists 7\u2208 (0,1) such that \u03c0is \u03b3-discount optimal for all 7 < y < 1. It is well-known that memoryless Blackwell optimal strategies always exist [6, 18] and they are also limit-average optimal [22, 18].\nThanks to the PAC (probably approximately correct) learnability of RL with discounted rewards [25, 35], there exists an algorithm Discounted which receives as inputs a simulator for M, R as well as \u03b3, \u03b5 and 8, and with probability 1 \u2013 8 returns an e-optimal memoryless policy for discount factor y. In view of Lemma 15, our approach is to run the PAC algorithm for discount-sum RL for increasingly large discount factors y and increasingly low d and \u20ac (Algorithm 1).\nNB The modified policy depends on the true, unknown support of the Probability transition function; we only claim the existence of such a policy."}, {"title": "Related Work and Conclusion", "content": "The connection between acceptance of w-regular languages in the product MDP and AECs is well-known in the field of probabilistic model checking [5, 12]. As an alternative to DRAs [40, 14, 31], Limit Deterministic B\u00fcchi Automata [34] have been employed to express w-regular languages for RL [37, 7, 10, 20, 21].\nA pioneering work on RL for w-regular rewards is [40], which expresses w-regular objectives using Deterministic Rabin Automata. Similar RL approaches for w-regular objectives can also be found in [14, 37, 10, 15]. The authors of [15, 29] approach RL for w-regular objectives directly by studying the reachability of AECs in the product MDP and developing variants of the R-MAX algorithm [8] to find optimal policies. However, these approaches require prior knowledge of the MDP, such as the structure of the MDP, the optimal e-return mixing time [15], or the e-recurrence time [29].\nVarious studies have explored reductions of w-regular objectives to discounted rewards, and subse-quently applied Q-learning and its variants for learning optimal policies [7, 31, 20, 21, 16]. In a similar spirit, [38] present a translation from LTL objectives to eventual discounted rewards, where only strictly positive rewards are discounted. These translations are generally not optimality pre-serving unless the discount factor is selected in a suitable way. Again, this is impossible without prior knowledge of the exact probability transition functions in the MDP.\nFurthermore, whilst there are numerous convergent RL algorithms for average rewards for unichain or communitcating7 MDPs (e.g. [8, 42, 17, 32, 4, 39]), it is unknown whether such an algorithm exists for general multichain MDPs with a guaranteed convergence property. In fact, a negative result in [3] shows that there is no PAC (probably approximately correct) algorithm for LTL objectives and limit-average rewards when the MDP transition probabilities are unknown.\nThese assumptions generally fail for our setting, where in view of Corollary 17, MDP states also track the states of the reward machine. For instance, in the reward machine in Fig. 2a it is impossible to reach u1 from \u0e192."}, {"title": "Limitations.", "content": "We focus on MDPs with finite state and action sets and assume states are fully ob-servable. The assumption of Section 4 that the support of the MDP's probability transition function is known is eliminated in Section 5. Whilst the size of our general translation-the first optimality-preserving translation\u2014is exponential, the additional knowledge in Section 4 enables a construction of the reward machine of the same size as the DRA expressing the objective. Hence, we conjecture that this size is minimal. Since RL with average rewards is not PAC learnable, we cannot possibly provide finite-time complexity guarantees of our Algorithm 1."}, {"title": "A Supplementary Materials for Section 3", "content": "Recall that a w-regular language L is prefix-independent if for every infinite label sequence w\u2208 (2AP)w, we have w \u2208 L iff w' \u2208 L for every suffix w' of w. We prove that there is no optimality-preserving translation for reward functions regardless of whether L is prefix-independent or not. The prefix-dependent case was given in Section 3. Here we focus on the other case:"}, {"title": "A Supplementary Materials for Section 6", "content": "Let II be the set of all memoryless policies and II* be the set of all limit-average optimal policies. Besides, let w* := Ipavg (\u03c0*) the limit average reward of any optimal \u03c0* \u2208 \u03a0*."}, {"title": "B.1 Efficient Construction", "content": "We consider a different collection C1,..., Cn of ASECS:\nSuppose C1,..., Ch is a collection of AECs (not necessarily simple ones) con-taining all states in AECs. Then we consider ASECs C1, . . ., Cn such that Ci is contained in C.\nThe definition of the reward machine in Section 4.2 and the extension in Section 5 do not need to be changed. Next, we argue the following:\n1. This collection can be obtained efficiently (in time polynomial in the size of the MDP and DRA).\n2. Lemma 10 and hence the correctness result (Theorem 9) still hold.\nFor 1. it is well-known that a collection of maximal AECs (covering all states in AECs) can be found efficiently using graph algorithms [2, Alg. 3.1], [15, 11] and [5, Alg. 47 and Lemma 10.125]. Subsequently, Lemma 20 can be used to obtain an ASEC contained in each of them. In particular, note that the proof of Lemma 20 immediately gives rise to an efficient algorithm. (Briefly, we iteratively remove actions and states whilst querying reachability properties.)\nFor 2., the first part of Lemma 10 clearly still holds. For the second, we modify policy \u03c0 as follows: Once, \u03c0 enters a maximal accepting end component we select an action on the shortest path to the respective ASEC C\u00bf inside C. Once we enter one of the Ci we follow the actions specified by the ASEC as before. Observe that the probability that under an AEC is entered is the same as the probability that one of the Ci is entered under the modified policy. The lemma, hence Theorem 9, follow."}, {"title": "C Supplementary Materials for Section 5", "content": "Define Et to be the transitions encountered up to step t\u2208 N, i.e. Et = {((Sk, qk), ak, (Sk+1, qk+1)) | 0 \u2264 k < t}. Then almost surely for some minimal t \u2265 to,"}]}