{"title": "SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMS", "authors": ["Shirley Kokane", "Ming Zhu", "Tulika Awalgaonkar", "Jianguo Zhang", "Thai Hoang", "Akshara Prabhakar", "Zuxin Liu", "Tian Lan", "Liangwei Yang", "Juntao Tan", "Rithesh Murthy", "Weiran Yao", "Zhiwei Liu", "Juan Carlos Niebles", "Huan Wang", "Shelby Heinecke", "Caiming Xiong", "Silivo Savarese"], "abstract": "Evaluating the output of Large Language Models (LLMs) is one of the most critical aspects of building a performant compound AI system. Since the output from LLMs propagate to downstream steps, identifying LLM errors is crucial to system performance. A common task for LLMs in AI systems is tool use. While there are several benchmark environments for evaluating LLMs on this task, they typically only give a success rate without any explanation of the failure cases. To solve this problem, we introduce SpecTool, a new benchmark to identify error patterns in LLM output on tool-use tasks. Our benchmark data set comprises of queries from diverse environments that can be used to test for the presence of seven newly characterized error patterns. Using SPECTOOL, we show that even the most prominent LLMs exhibit these error patterns in their outputs. Researchers can use the analysis and insights from SPECTOOL to guide their error mitigation strategies.", "sections": [{"title": "1. Introduction", "content": "An emerging use case for LLMs in AI systems is tool use. When LLMs are equipped with context and a list of tools, such as APIs, databases, and applications, LLMs can generate a sequence of calls to the available tools to solve a given task. Using LLMs to perform tasks via APIs involves more complex reasoning and instruction following abilities of LLMs, and there are several emerging methods that aim to improve those abilities in order to help LLMs adapt to new tasks and situations. These methods (Zhang et al., 2024; Li et al., 2023) help enhance the model's understanding of user preferences, along with its skills in logical reasoning, with the goal of increasing their overall effectiveness. The basic technique involves using guiding prompts to provide LLMs with instructions and information about the context, enabling them to generate actions to solve complex tasks. Besides, some techniques (Rafailov et al., 2024; Jing et al.,"}, {"title": "2. Related Work", "content": "The current benchmarks exhibit notable strengths and weaknesses in evaluating model performance as a tool-use agent. Benchmarks like GORILLA (Patil et al., 2023), AGENTBOARD (Ma et al., 2024), TOOLBENCH (Guo et al., 2024), TOOLEYES (Ye et al., 2024), and MINTBENCH (Wang et al., 2023) are remarkably utilised in examining model interaction ability, reasoning and planning ability. While some benchmarks like TOOLBENCH, AGENTBOARD, and MINTBENCH are effective in managing multi-turn interactions, others such as GORILLA excel in handling diverse queries, especially when dealing with constraints and irrelevant information. GORILLA also offers detailed error pattern analysis and are well-suited for complex queries that require a wide range of API capabilities but is limited to single-turn interactions only.\nHowever, these evaluations often fail to consider the model's existing knowledge, leading to unnecessary and repetitive"}, {"title": "3. Error Patterns in Tool-Use", "content": "It's widely understood that LLMs are prone to output errors such as hallucinations, inconsistencies, errors, etc (Wei et al., 2022; 2024; Lu et al., 2024). What is less studied are the errors exhibited in tool-use scenarios. In tool-use scenarios, typically the LLM outputs one or more function calls, including necessary arguments for the execution of the functions. For example, an LLM output for using a web search API to learn about the latest fashion can be as the following: Search[query = \u201clatest fashion\",topk = 10], where Search indicates the tool and query and topk are the context-specific arguments for executing the tool call.\nNote that these tool calls are particularly brittle even a slight change to an argument, a missing argument, or incorrect tool call can produce drastically different and incorrect results downstream. Given the importance of catching these output errors, in this work, we characterize systematic errors generated by LLMs on tool-use tasks into seven critical error types.\nInsufficient API Calls (IAC): The LLM is unable to generate sufficient API calls, hence unable to completely fulfill the tasks provided in a query.\nIncorrect Argument Value (IAV): The LLM generates incorrect argument values. This also includes exclusion of necessary arguments.\nIncorrect Argument Name (IAN): The LLM hallucinates argument names.\nIncorrect Argument Type (IAT): The LLM generates incorrect argument type.\nRepeated API Calls (RAC): The LLM generates the exact same API Call repeatedly, leading to redundant calls.\""}, {"title": "4. SpecTool Dataset", "content": "The brittle nature of LLM-generated tool calls highlights the need to quickly discover errors in LLM-generated outputs. To quickly discover tool-calling errors in LLMs, we propose a new dataset, SPECTOOL. It is created through a series of effective steps aimed at generating complex queries.\nThe process begins by gathering seed queries for each API environment, sourced from benchmark tests conducted on toolsets like TOOLBENCH (Guo et al., 2024) and AGENTBOARD (Ma et al., 2024). These initial seed queries are then augmented using the following methods.\nConstraint based Query Generation: We employ a deterministic method to discern the options available for each argument within every API call. Utilizing prompt engineering techniques, we introduce detailed descriptions of each argument and its corresponding options to enhance the original query within GPT-4. First, we include one argument along with its option to create a more detailed query. Then, we improve our method by adding multiple arguments and their options, making the query even more complex.\nSentence Transformation based Query Generation: On seed queries, we instruct GPT-4 to modify sentences while maintaining the same context. This means keeping the contextual requirements unchanged while altering the sentences.\nRelevance Data based Query Generation: On seed queries, we instruct GPT-4 to introduce additional, unnecessary information around the keywords while keeping the context of the query unchanged.\nWe collected around 150 augmented queries"}, {"title": "5. Method", "content": "In the evaluation benchmark we propose for tool use, we ensure that the environments are deterministic. This allows the trajectory of the agents to depend solely on the policy and actions selected by the language model. Agents are given a description of the available tools and task instructions, including format requirements. The actions generated by the agents are reviewed by an inbuilt feedback mechanism designed to detect prominent errors. These include formatting issues, incorrect function names, incorrect argument names, and incorrect argument types. If no such errors are found, the action is executed within the corresponding environment and the feedback it generates is collected. This feedback offers insights into both the changes in state and any potential action errors."}, {"title": "6. Metrics", "content": "Current benchmarks tend to emphasize the final outcome of models, often overlooking the errors that occur during intermediate steps. Benchmarks such as AGENTBOARD and MINTBENCH attempt to address this by imposing constraints on the expected outcomes at each step and the sequence in which goals should be achieved. For instance, AGENTBOARD defines subgoals for each query set in a prescribed order, with these subgoals representing the expected API responses based on the agent's actions. However, these benchmarks do not consider that the model might possess prior information allowing it to skip certain subgoals, potentially leading to an inaccurate assessment of the model's performance due to its deviation from the prescribed subgoal sequence. Additionally, these benchmarks lack detailed analysis of specific errors in the action execution, thus failing to provide comprehensive insights into the minor mistakes made by the model.\nIn this study, we concentrate our efforts on offering deeper insights into specific model inefficiencies. We do this by providing critical metrics related to API Call, thus fostering a comprehensive understanding of the necessary improvements in the models. At present, we have accurately verified and labeled a compilation of 150 queries. These queries are unique, stemming from diverse environments as shown in table 2, and each boasting multiple expected API Call trajectories. We employ these queries to subsequently compute metrics for a variety of models, thereby identifying their respective error patterns. Each metric is defined as follows: N represents the total number of maximum steps the model is permitted to effectively process the query, while GT denotes the Ground Truth labels for each individual query. Our calculation of these metrics strictly follows the sequence outlined above.\n\u2022 For error patterns IFN, IAN, IAT, IFE and RAC we calculate each error's accuracy as: Ne, where Ne is the number of API calls executing the error pattern.\n\u2022 For error patterns IAC and IAV, we use the labeled ground truths of each query and calculate these metrics across each labeled trajectory: mingeGT (NN), where Ng is the number of API calls with errors w.r.t. the ground truth g.\nIt is crucial to emphasize that while the metrics indicate error patterns, their calculations are designed to be directly proportional to the success rate. Higher scores on the error metrics represent fewer issues generated by a model in that respective error pattern. We calculate the success rate of each query by comparing its final output with the ground truth. If a quantitative ground truth is unavailable, success is determined by the model's ability to conclude using the finish API with a correct response. Error metrics are then incorporated with appropriate weightage in both cases."}, {"title": "7. Experiments", "content": "We perform an in-depth assessment of widely used large language models, encompassing both proprietary, API-based systems and models with publicly accessible weights. This evaluation covers a broad range of models to examine performance, versatility, and the impact of architectural variations across both closed and open-source platforms.\nWe incorporate a one-shot in-context example along with task-specific instructions in our prompt setup. Our benchmarking includes a selection of high-performing proprietary models as well as open-weight models. For each open-weight model, we evaluate the chat-optimized version when available, unless otherwise indicated.\nFor models without a dedicated chat version, we apply standardized prompt templates tailored for publicly accessible versions. We ensure that each prompt includes precise format instructions to enable the model to generate API calls in a structured and parsable format. To account for server inconsistencies, we execute multiple rounds per API call, thus minimizing potential disruptions and ensuring consistent evaluation outcomes."}, {"title": "7.1. Results", "content": "One goal of our SPECTOOL benchmark is to quantify various error patterns critical for understanding model fallacies. For that, we've conducted a comprehensive assessment of several performant LLMs, including GPT-4 (Achiam et al., 2023), GPT-3.5-Turbo from OpenAI, Meta-Llama-3-8b (Dubey et al., 2024), Code-Llama-13b (Rozi\u00e8re et al., 2024), Vicuna-13b-16k (Chiang et al., 2023), Mistral and Mixtral models from Mistral AI (Jiang et al., 2023) (Jiang et al., 2024), and xLAM (Zhang et al., 2024). Interestingly, while these models all perform well on TOOLBENCH (Guo et al., 2024), their failure patterns are different. Details of the error patterns discovered using SPECTOOL are shown in table 3. The table 3 qualitatively explains the how each error pattern is exhibited in common model outputs.\nOne discovery is that models specifically tuned to focus on API calling mechanisms are generally less likely to produce false information, often known as \u201challucinating\" when dealing with function names and argument names. On the contrary, models like Vicuna (Chiang et al., 2023) (Zheng et al., 2024) that are designed for chat-based interactions do have a higher rate of inaccuracies, particularly within incorrect argument names and function names. It suggests that the tuning process, specifically for API calls, can lead to a substantial reduction in these hallucination problems.\nOur results indicate that both proprietary and open-weight models fine-tuned for API-calling tasks show strong performance in reducing \"Insufficient API Calls.\" This metric suggests these models are particularly effective at selecting the correct tools from the available API options, demonstrating a high proficiency in expected tool utilization.\nThis research constitutes a parallel and complementary pursuit in tandem with the development of the xLAM model (Zhang et al., 2024). The techniques applied within this study have been successfully incorporated into the xLAM data pipeline, evidence of which can be discerned from the resultant table 4. Most striking is the superior performance of GPT-4, achieving the highest success rate amongst all models evaluated. This could potentially suggest that larger models harnessing the power of the scaling law and quality pretraining in the base model remain critical to the success of agent applications. Another noteworthy observation is the exemplary performance of the Code-Llamma-13b model (Rozi\u00e8re et al., 2024), surpassing many other generically-purposed models. Our hypothesis is that there may be some inherent similarity between coding-associated tasks and function calls. And that could be beneficial for models designed for coding purposes, thereby enhancing their performance on function-calling tasks."}, {"title": "8. Conclusion", "content": "This paper introduced SPECTOOL, a benchmark for evaluating LLMs on tool-use tasks with a focus on error patterns and feedback. Our findings highlight the importance of task-specific fine-tuning, as models optimized for API calls perform more reliably and reduce hallucinations compared to chat-oriented models. Larger models, like GPT-4, demonstrate superior performance, affirming the significance of scaling and quality pretraining. Additionally, we showed that the feedback mechanism in SPECTOOL plays a crucial role in improving model accuracy, guiding models to correct their function-calling behavior. Overall, this study provides valuable insights for refining LLMs in tool-use applications.\nIn this work, we recognize the importance of capturing the significant error patterns of the model, as addressing these failures in planning and function-calling can lead to long-term improvements in the model effectiveness, hence enhancing the reasoning capabilities of LLMs. By analyzing these metrics, developers can identify specific weaknesses in language models and focus on improvements in those key areas. This targeted approach can help enhance the overall performance and reliability of the models.\nOur benchmark SPECTOOL is a continuously developed pipeline for better use cases. Presently, we have tested on tasks belonging to only a single family of actions, for example a task would be estimating weather details with API actions only related to weather. Future work involves analyzing more complex tasks with real-world queries and APIs, along with large-scale testing across different LLMs."}, {"title": "9. Appendix", "content": "9.1. Prompts for Query Augmentation\nIn the following section, we list all the prompts used in the process of query generation. All\n9.1.1. CONSTRAINT BASED AUGMENTATION\nTable 5 here describes the constraint based query generation prompt with a sample fewshot examples from the space-flight environment. For every new environment we have its specific set of fewshot examples.\n9.1.2. SENTENCE TRANSFORM BASED AUGMENTATION\nTable 6 here describes the sentence transform based query generation prompt.\n9.1.3. IRRELEVANCE BASED AUGMENTATION\nTable 7 here describes the irrelevance based query generation prompt.\n9.2. Query Selection for SpecTool\nBased on the feasibility of solving the query given the provided APIs, we randomly select 150 base queries. We make sure that we dont introduce any duplicate or closely related queries generated by the augmentation.\n9.2.1. QUERIES SELECTED FOR FEEDBACK ABLATION STUDY\nFor the feedback vs no feedback ablation study we use the same 150 queries to analyse the impact of the specific feedback provided to the model for executing the query successfully. Within, the average model score the major impact is caused on incorrect argument name, incorrect argument type and incorrect function name.\n9.2.2. QUERIES SELECTED FOR IRRELEVANCE ABLATION STUDY\nFor the irrelevance ablation study, we specifically gather queries from the irrelevance augmentation. We do this for the base queries from 3 environments (Spaceflight, Movie and Patent). In total, this ablation study is done using 60 queries.\n9.2.3. QUERIES SELECTED FOR API COMPLEXITY ABLATION STUDY\nFor this study, we randomly collect 10 queries per environment to maintain unanimous averaging of scores across all the provided environments. The queries are selected from out original 150 base queries."}]}