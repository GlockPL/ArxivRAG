{"title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents", "authors": ["Raj Jaiswal", "Dhruv Jain", "Harsh Parimal Popat", "Avinash Anand", "Abhishek Dharmadhikari", "Atharva Marathe", "Rajiv Ratn Shah"], "abstract": "Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding. When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors. While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously. To address this, we introduce Mixture of Refinement Agents (MORA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs. Our approach aims to bridge the gap between open-source LLMs and GPT-40 by utilizing the latter as error identifier to guide these refinement agents. We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA). MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.", "sections": [{"title": "Introduction", "content": "Scientific reasoning, particularly in field of physics, requires a deep understanding that spans multiple disciplines. It demands not only domain-specific knowledge but also the integration of mathematical reasoning with theoretical concepts, applying abstract principles and formulae across various contexts and scenario. Successfully solving these challenges is a fundamental aspect of human intelligence, as it entails not just recalling information but adapting knowledge to solve diverse complex problems.\nSolving complex physics problems still remains a challenge for open source LLMs. The difficulty stems from the need to integrate both mathematical and domain-specific knowledge while engaging in multi-hop, step-by-step reasoning. One approach to address this challenge can be collecting question and solution trajectory annotations and fine-tune LLMs to enhance these capabilities, similar to recent mathematical reasoning works (Luo et al. 2023; Yuan et al. 2024). However, the process of such annotations and fine-tuning is time-consuming and costly. On the other hand, so- lutions generated by LLMs for physics problems using CoT prompting (Wei et al. 2022) often contain errors, such as objective misalignment, incorrect formula application, and computational mistakes, as illustrated in Figure 1. Moreover, solutions to multihop physics problems contain multiple such errors together.\nOpen source LLMs struggles to accurately directly identify reasoning mistakes in their own solutions (Li et al. 2024; Tyen et al. 2024; Anand et al. 2023e), making them unreliable for error detection and self-refinement. While objective alignment errors can be corrected once identified, refining computational and conceptual errors requires strong mathematical reasoning and contextual understanding of the specific question. Addressing all these different errors simultaneously remains a significant challenge for open-source LLMs.\nThis motivated us to develop the Mixture of Refinement Agents (MORA) framework. MoRA iteratively refines LLM responses through a two-step process in each iteration. First, the framework leverages a advanced model to identify various errors within the solution using appropriate flags and scores. In the next step, based on the identified errors, prioritized agent routing is conducted, in which the appropriate agents are activated to address and mitigate the specific errors. This process results in a progressively refined solution.\nIn the domain of physics, evaluation benchmarks are essential for assessing the conceptual and mathematical reasoning of LLMs. Benchmarks like MMLU, SciEval (Sun et al. 2024), and ScienceQA (Lu et al. 2022) focus on foundational knowledge and general reasoning, while more challenging ones like OlympiadBench (He et al. 2024) and JEEBench (Arora, Singh et al. 2023) require advanced reasoning skills. To bridge the gap, we curated our own dataset PhysicsQA, containing set of diverse, intermediate- level high school physics problems that provide a balanced challenge, allowing a exhaustive evaluation and analysis of open-source LLMs on physics problems.\nWe perform exhaustive experimentation of MORA across four datasets including PhysicsQA as shown in Table 3. MoRA improves accuracy on the PhysicsQA benchmark over CoT-generated solutions by 13.38% for Llama-3-70B and by 16.03% for Gemma-2-27B. This significant enhancement highlights MoRA's effectiveness in refining solutions, particularly in complex and diverse physics problems as in"}, {"title": "Related Works", "content": "LLM Reasoning LLMs have been successfully applied to address multi-step reasoning tasks by generating intermediate reasoning steps, referred to as Chain-of-Thought (CoT) (Wei et al. 2022), Auto-CoT (Zhang et al. 2022), and Complex-CoT (Fu et al. 2022), among others. Advanced techniques like Iter-CoT (Sun et al. 2023a) and ToT (Yao et al. 2024) extend these capabilities but remain constrained by the knowledge in training data and the specific structures they were designed with. While In-Context Learning (ICL) (Brown et al. 2020) has significantly improved LLM performance, challenges like hallucinations and limitations in reasoning flexibility persist.\nLLMs for Scientific Reasoning LLMs face significant limitations in complex knowledge reasoning tasks (Petroni et al. 2020). (Ouyang et al. 2023) introduced a structured reasoning strategy to guide LLMs in solving complex chemistry problems, enabling them to generate high-quality reasoning. Solving these problems requires not only domain knowledge, like formulae and calculations, but also a step- by-step reasoning process. (Ma et al. 2024) proposed a method where agents generate a high-level plan based on the question, retrieve relevant functions from a toolset, and execute low-level actions by integrating natural language and Python code.\nSelf Verification with LLMs Recent works (Cobbe et al. 2021; Ling et al. 2024) have attempted to address the challenge of error detection in step-by-step reasoning. However, these methods often require additional training data or domain-specific exemplars, making them less practical. (Miao, Teh, and Rainforth 2023) proposes using the LLM itself to verify the conditional correctness of each step in the reasoning chain, similar to how a human reviews their work.(Anand et al. 2023c) Accurate error recognition and correction are crucial for enhancing problem-solving capabilities, as demonstrated by (Li et al. 2024), which defines tasks to assess LLMs' mathematical reasoning abilities in error identification and correction.\nLLMs for Mathematical Reasoning LLMs tends to struggle with arithmetic calculations when solving math problems (Cobbe et al. 2021; Gao et al. 2023). However, incorporating code generation and execution has shown promise in enhancing the accuracy of mathematical reasoning. Leveraging these strengths, the GPT-4 Code Interpreter (Zhou et al. 2023) has been integral to frameworks like MathCoder (Wang et al. 2023), which is designed to improve the mathematical reasoning capabilities of open- source models. Findings from (Zhou et al. 2023) indicate that GPT-4 Code's impressive proficiency in solving mathematical problems is largely due to its step-by-step code generation and the dynamic refinement of solutions based on code execution outcomes.\nLLM Reasoning with external database (Lewis et al. 2020) proposed RAG framework, which incorporates (Anand et al. 2023b) a retrieval component to fetch relevant information from a given knowledge base. Integrating LLMs with knowledge representation tools, such as knowledge graphs (KGs) (Mruthyunjaya et al. 2023), has further enhanced reasoning capabilities. (Yao et al. 2024) demonstrated that augmenting LLMs with comprehensive external knowledge from KGs can significantly improve their performance and facilitate more robust reasoning processes. A notable example is GraphRAG (Edge et al. 2024), a retrieval enhancement technique that leverages knowledge graphs to map relationships between entities, thereby enhancing the retrieval process using large language models (LLMs)."}, {"title": "Dataset: PhysicsQA", "content": "Our dataset comprises 370 carefully selected high school physics questions sourced from online resources. These questions are notably complex, often requiring the application of multiple concepts, intricate computations, (Anand et al. 2023a) and multihop reasoning. Each question is paired with a comprehensive, step-by-step solution, to support the evaluation (Anand et al. 2024b) and fine-tuning of LLMs for physics reasoning. Table 1 illustrates the topic-wise distribution of the questions, providing a clear overview of the areas covered. PhysicsQA offers a more robust evaluation and analysis of LLM performance by encompassing a diverse range of questions, both in terms of complexity and the topics covered."}, {"title": "Mixture of Refinement Agents", "content": "This section introduces our mixture of refinement agents (MORA) framework. We first discuss our motivation behind MORA; then, we introduce the error identification stage and refinement agents. Finally, we discuss how these agents are routed iteratively to correct different errors in the solutions generated by the LLM."}, {"title": "Motivation", "content": "While analyzing physics problems and their CoT solutions generated with LLMs (Llama-3-70B & Gemma-2-27B), we observed three key errors made by them:\nObservation 1: LLMs in few cases struggle to fully grasp the objective of the question, along with misinterpreting the values of variables and constants provided in the question. Although this issue has been identified in only a few cases, it is significant one because it leads to solutions that"}, {"title": "Error Identification", "content": "The errors in the solutions are classified into three categories: 1) problem miscomprehension, 2) incorrect concept application, and 3) computational errors as showed in Figure 1.\nFor error identification, we choose to rely on GPT-40. Our experiments and analysis shows that GPT-40 showcases superior performance compared to other models, particularly in problem comprehension and correct physics concept application required. Thus, it is adequate for locating errors within solutions generated by other models. Given a question and it's LLM response, we prompt GPT-40 to identify and locate different errors in the solution using the combination of following flags and scores:\nProblem Comprehension Flags: We prompt GPT-40 to check for the problem miscomprehension using the following two flags: (i) Objective Alignment Flag, $F_{obj}$, verifies whether the solution is focused on solving the correct objective of the given question. (Anand et al. 2023d) (ii) Variables Application Flag, $F_{val}$, verifies whether the solution uses the correct values for all variables and constants provided in the question, ensuring their correct values are applied in formulae and reasoning.\nConcept Verification Score: We instruct GPT-40 to check the given solution against the relevant concept and formulae required to solve the given problem, based on its own understanding of the question. A score ($Score_{concept}$) is assigned to each solution to quantify the correctness of the applied physics concepts and formulae. The score is designed to identify the stage at which any conceptual or formulae error first occurs, if at all. $Score_{concept}$ ranges from 0 to 1, where a lower score indicates an earlier-stage error and a higher score indicates a later-stage error in the solution process. The score is calculated as follows:\n$Score_{concept} = \\begin{cases} \\frac{n}{N} & \\text{if } 1 < n < N \\text{ (error at step n)} \\\\ \\frac{n}{N+1} & \\text{if } n = N \\text{ (error at last step)} \\\\ 1 & \\text{if no errors occur} \\end{cases}$\nwhere n is the step at which the first error occurs, and N is the total number of steps in the solution process.\nComputation Verification Score: We employ GPT-40 with OpenAI Code Interpreter for generation and execution of python code to evaluate the correctness of all arithmetic and algebraic operations in the given solution. Similar to the $Score_{concept}$, we assign $Score_{comp}$ to each solution. This score quantifies the accuracy of the mathematical computation performed, ranges from 0 to 1, and is calculated similar to $Score_{concept}$. All the computations are evaluated with an"}, {"title": "Refinement Agents", "content": "To address the three key errors in LLM generated solutions, we introduce a set of specialized refinement agents. Each agent is designed to rectify a specific type of error within the solution, ensuring targeted and effective corrections. The refinement agents use the same LLM with which the original solutions are generated.\nMiscomprehension Refinement Although there are very few cases of problem miscomprehension in LLM generated solutions, once identified these mistakes can easily be corrected with simple instruction prompting:\nYou are tasked with solving a physics problem. Here is the question: [question], The following is your generated solution: [solution], In the generated solution, the correct objective of the question is not being addressed. The solutions contains mistakes which leads to misalignment with the objective of the question. Please carefully review the question & understand the objective in detail and regenerate the solution accordingly.\nThe above prompt assists in refining the solution to align with the correct objective of the given question. This may involve regenerating the entire solution or correcting an intermediate mistake to ensure the solution addressed the correct objective. Similarly, any incorrect variable values used within the solution is corrected using instruction prompting.\nConcept Refinement To address the incorrect concepts and formulae applied in the LLM's solutions, we utilize an external physics knowledge base. This is necessary because LLMs may not always have access to or accurately retrieve the correct formulae, as this information may not be embedded in their internal knowledge. The conceptual refinement occurs in two steps:"}, {"title": "Agent Routing and Iterative Refinement", "content": "After the error identification, the respective refinement agents are activated to mitigate these errors. The agent routing follows a prioritized sequence: 1.) miscomprehension refinement, 2.) concept refinement, 3.) computational refinement. This prioritization mirrors the human approach to solving physics problems: first, understanding the objectives and variables; next, identifying relevant concepts and formulae; and finally, applying them to perform the necessary computations.\nThe activated refinement agent then acts upon the solution to mitigate the error. The solution undergoes iterative cycles of error identification and refinement until all flags and scores are resolved or a maximum iteration limit is reached. This process ensures that all errors are corrected without introducing new ones in final refined solution. The complete process is illustrated in Algorithm 1."}, {"title": "Experiments", "content": "Datasets In our experiments, we use four datasets: SciEval-Static, PhysicsQA, MMLU High School and MMLU College. SciEval-Static is a subset of SciEVal (Sun et al. 2023b), consisting 164 questions from physics divided into multiple sub-topics. MMLU (Hendrycks et al. 2021), consists of a 118 College level and 173 high school multiple- choice questions from various disciplines.\nLLMS We utilize the API of a range of models with varying parameters and capabilities including LLaMa-3-70B, LLaMa 3.1-405B, Gemma-2-27B, Gemini-1.5-Flash, GPT- 3.5 Turbo and GPT-4 as our LLMs for the evaluation. We use same prompts for all the datasets and LLMs during our evaluation.\nBaselines We employ an Answer-only approach (AO), where the model is given a question with four options and asked to select the correct answer without any explanation relying solely on its pre-existing knowledge. In contrast, few-shot prompting (Xu et al. 2023; Yasunaga et al. 2023) uses a few examples to help the model learn and apply that knowledge to similar tasks. Chain-of-Thought (CoT) prompting (Wei et al. 2022) guides the model to generate intermediate reasoning steps, improving its performance on complex tasks by breaking them down into smaller, more manageable parts. These three approaches form our primary baselines.\nEvaluation Most of the existing works (Luo et al. 2023), (Chern et al. 2023), (Yu et al. 2023) measure the mathematical reasoning quality of LLMs by directly comparing the final answer and calculating the overall accuracy on a given dataset. (Anand et al. 2024a) We choose to follow the same evaluation for physics reasoning as well."}, {"title": "Results", "content": "In Table 2 we present results from our experiments reveal compelling insights into the strengths and challenges of vari-"}, {"title": "Analysis", "content": "In this section, we first conduct an in-depth error analysis of physics CoT solutions across various models and datasets, highlighting the error distribution that inspired the development of MORA. We then present ablation studies, analyzing the effectiveness of each refinement agent in our framework."}, {"title": "Error Analysis", "content": "We perform manual analysis of the incorrect CoT solutions of GPT-40, Llama-3-70B, and Gemma-2-27B on the following datasets: SciEval-Static, PhysicsQA, MMLU High School and College as shown in Table 4. Based on this analysis here are our observations:\n(i) LLMs demonstrate good problem comprehension ability for physics question. All models demonstrate strong problem comprehension across the datasets. GPT-40 excels, achieving near-perfect accuracy on SciEval-Static, MMLU College, and High School, with only minor errors in PhysicsQA. This suggests a deep understanding of physics problem structure. Llama-3-70B and Gemma-2-27B also perform well but show slightly higher error rates, particularly in PhysicsQA and SciEval-Static, indicating occasional missed details that need attention.\n(ii) Open source LLMs sometimes struggles to retrieve correct physics concept and formulae while reasoning. On average, 18.11% of questions in the PhysicsQA dataset are answered with conceptual errors by Gemma- 2-27B and Llama-3-70B, highlighting the difficulty open- source LLMs face in applying correct concepts to physics problems. In contrast, GPT-40 excels with an average accuracy of 96.4% across all four datasets. Notably, Gemma- 2-27B outperforms Llama-3-70B on SciEval-Static, Physic- SQA, and MMLU High School. The high error rates of both Llama-3-70B and Gemma-2-27B on PhysicsQA suggest that medium-parameter open-source LLMs may still need external knowledge bases for complex physics problem- solving.\n(iii) Open-source LLMs struggles with algebraic and arithmetic computation required while solving physics"}, {"title": "Ablation", "content": "To understand the effectiveness of each refinement agent, we conduct ablation of each of the refinement agents with Llama-3-70B and Gemma-2-70B in terms of their refinement rate across different datasets as shown in Table 5. Here are our observations:\n(i) Problem miscomprhension errors are mitigated with simple instruction prompting and error feedback. Llama-3-70B and Gemma-2-27B demonstrate good mis- comprehension error refinement with instruction prompting, particularly in the MMLU datasets (High School and Col- lege), where both models achieve a perfect 100% refinement rate. Llama-3-70B outperforms Gemma-2-27B slightly on PhysicsQA, with a refinement rate of 66.7% compared to Gemma-2-27B's 62.5%. However, Gemma-2-27B excels in SciEval Static and MMLU (College and High School). The decent accuracy on PhysicsQA suggests that open-source LLMs sometimes fail to rectify their misinterpretations in complex physics problems.\n(ii) Open-source LLM performers moderately in iden- tifying the conceptual mistake and retrieval thought gen- eration. Llama-3-70B shows a 46.9% refinement rate in PhysicsQA and slightly improves in SciEval Static at 57.1%, but struggles with MMLU datasets, achieving 37.5% and 33.3% refinement in High School and College, respectively. Gemma-2-27B has a similar 48.7% refinement rate in PhysicsQA and performs better in SciEval Static at 62.5%, but underperforms significantly on MMLU High School with a 16.7% refinement rate, improving modestly to 37.5% on MMLU College. These results suggest that open-source LLMs have difficulty generating relevant retrieval thoughts at the initial stage of failure.\n(iii) Using code-driven refinement significantly corrects the computational errors. Llama-3-70B and Gemma- 2-27B excel in refining computational errors, demonstrating the effectiveness of code generation and execution. Llama-3- 70B shows consistent performance with a 72.6% refinement rate on PhysicsQA and strong results across SciEval Static (60%), MMLU High School (75%), and MMLU College (81.8%). Gemma-2-27B slightly outperforms in PhysicsQA at 73.3% and achieves a 100% refinement rate in MMLU College. However, Gemma-2-27B's performance is more variable, particularly in MMLU High School (33.3%), indicating potential challenges in specific code generation scenarios."}, {"title": "Conclusion", "content": "In this work, we introduce MORA, a novel agentic refinement framework designed to mitigate three critical errors commonly made by LLMs when solving complex physics problems. MORA first leverages GPT-40 for error identification and score assignment, which are then subsequently used to guide the refinement agents. This process is done iteratively until all the errors in the solution are mitigated successfully. To ensure a comprehensive evaluation, we also curate our own dataset, PhysicsQA, which includes a diverse set of high school-level physics problems. Our experiments and in-depth analysis across multiple datasets demonstrate that MoRA significantly enhances the performance of Llama-3-70B and Gemma-2-27B across multiple datasets."}]}