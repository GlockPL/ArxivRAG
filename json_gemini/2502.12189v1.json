{"title": "Self-supervised Attribute-aware Dynamic Preference Ranking Alignment", "authors": ["Hongyu Yang", "Qi Zhao", "Zhenhua Hu", "Rui Li"], "abstract": "Reinforcement Learning from Human Feedback and its variants excel in aligning with human intentions to generate helpful, harmless, and honest responses. However, most of them rely on costly human-annotated pairwise comparisons for supervised alignment, which is not suitable for list-level scenarios, such as community question answering. Additionally, human preferences are influenced by multiple intrinsic factors in responses, leading to decision-making inconsistencies. Therefore, we propose Self-supervised Attribute-aware dynamic preference ranking, called SeAdpra. It quantifies preference differences between responses based on Attribute-Perceptual Distance Factors (APDF) and dynamically determines the list-wise alignment order. Furthermore, it achieves fine-grained preference difference learning and enables precise alignment with the optimal one. We specifically constructed a challenging code preference dataset named StaCoCoQA, and introduced more cost-effective and scalable preference evaluation metrics: PrefHit and PrefRecall. Extensive experimental results show that SeAdpra exhibits superior performance and generalizability on both StaCoCoQA and preference datasets from eight popular domains.", "sections": [{"title": "1 Introduction", "content": "Community Question Answering (CoQA) (Romeo et al., 2018; Wu et al., 2018) seeks to generate responses that are semantically accurate and match the preferences of community members. Currently, Reinforcement Learning from Human (or AI) Feedback (RLHF/RLAIF) (Christiano et al., 2017; Bai et al., 2022) has enabled precise control of large language models (LLMs) for generating human-like responses (Stiennon et al., 2020; Ouyang et al., 2022). However, applying it to CoQA remains underexplored. Moreover, human preferences do not always follow a singular, value-based hierarchy. Decision-making can be influenced by various factors and may exhibit inconsistencies (Tversky, 1969), which undoubtedly presents a challenge for aligning LLMs with CoQA.\nExisting methods are limited to pairwise comparison (one chosen and one rejected), such as reward model-based RLHF (Ouyang et al., 2022), offline supervised Direct Preference Optimization (DPO) (Rafailov et al., 2024), as well as other variants like SLiC (Zhao et al., 2023) and pseudo-list RRHF (Yuan et al., 2024) that adopt pairwise hinge loss. However, a real-world prompt may have multiple high-quality responses (Cui et al., 2023). For example, in the coding community, the optimal one may vary with thier different attributes, such as semantics, popularity, and timeliness, as illustrated in Figure 1. Recently, some alignment methods have attempted to rank multiple preferred candidates. PRO (Song et al., 2024) introduces a list-level maximum likelihood estimation loss to shift towards preference ranking but overlooks the attributes of responses. LiPO (Liu et al., 2024) directly optimizes list-based ranking preferences and begins to address response labels, but has not yet addressed the integration of multiple labels. Moreover, these supervised learning methods depend on human or AI annotations of preference pairs or lists to specify the best responses for alignment. However, preference data are relatively scarce and expensive to collect in practice (Casper et al., 2023).\nTo address the above issues, we propose SeAdpra, a Self-supervised attribute-aware dynamic preference ranking framework. It consists of three stages. First, the Multi-Attribute Perception quantifies preference-level differences through Attribute-Perceptual Distance Factors (APDF), enabling the integration of multiple attributes for self-supervised dynamic ranking. Second, the Perception Alignment aims to quickly adapts to domain knowledge and achieve precise alignment by aligning the optimal. Third, the Perceptual Comparison performs multiple iterative comparisons on all candidates to learn on-chain preference differences.\nFor enhancing the cost-efficiency and domain applicability of the preference evaluation scheme, we propose new metrics that follow the 'CSTC' criterion (details in Appendix A.2), as an alternative to the costly win rate (Dud\u00edk et al., 2015), namely PrefHit and PrefRecall. They can accommodate the expansion of benchmarks. Aiming to validate the effectiveness of SeAdpra in specific domains, we have constructed a programming CoQA preference dataset, called StaCoCoQA, which contains over 60,738 programming directories and 9,978,474 entries. Our main contributions are as follows:\n\u2022 We introduce the Attribute Perceptual Distance Factor (APDF) to gauge the in preference-level gaps of multiple responses, replacing the binary judgment of preferred versus non-preferred. We propose an self-supervised dynamic preference ranking framework that achieves label-free list-wise preference alignment.\n\u2022 We present the StaCoCoQA, a large-scale, high-quality, real-time (as of May 2024) dataset for preference alignment in programming CoQA, and develop two new alignment metrics abided by the 'CSTC' criterion.\n\u2022 We conducted extensive experiments on eight hot public datasets and StaCoCoQA, providing a reference benchmark. The experimental results demonstrate that SeAdpra excels in alignment while maintaining safety."}, {"title": "2 Method", "content": "Our goal is to align an LLM with user preferences in CoQA using our Unsupervised Attribute-aware Dynamic Preference Ranking strategy. The training dataset is denoted as $\\mathcal{D} = \\{Q^i, R^i\\}_{i=1}^N$. For a given question $Q^i$, it corresponds to a series of responses $R^i = \\{R_1, ..., R_M\\}$, where each response $R_i = (C, A)$, with C representing the content and A representing the scalable attributes. The size L of the scalable attribute $A = \\{A_1, ..., A_L\\}$ is determined by community characteristics. For example, in the code community, L = 3 and A = {S, P,T}. Here, S represents the semantic similarity between C and Q; P represents the popularity of R, and T represents the creation time of each response."}, {"title": "2.2 Multi-attribute Perception", "content": "The existing alignment optimization objectives (Rafailov et al., 2024; Song et al., 2024) do not take into account the attributes of the candidates, which can differentiate their preferences. Therefore, there is a need to explore optimization methods that can effectively incorporate these attributes. In this context, LambdARank (Burges et al., 2005; Donmez et al., 2009; Wang et al., 2018; Jagerman et al., 2022) introduces Lambda weights $\\delta_{ij}$, which scale the gradient of each pair of scores based on the labels of the pairs to optimize a metric-driven loss function and effectively incorporating label information into the optimization process.\nInspired by the $\\Lambda_{ij}$, the Attribute-Perceptual Distance Factor $d_{i,j}$ is designed to quantify the preference difference between two candidates i and j in the optimization objective. It not only considers the positional relationship of candidates in preference ranks but also incorporates their label values through the gain function, and expressed as:\n$d_{i,j} = (G(i) \u2013 G(j)) \\cdot (T(i) \u2013 T(j))$\t(1)\n$T(i) = 1/log(l_i + 1)$\t(2)\nwhere $l_i$ and $l_j$ are the ranking positions of response i and j, respectively. The gain function $G(\\cdot)$ varies with different intrinsic attributes."}, {"title": "2.2.2 Construction of the Multi-APDF Matrix", "content": "Given the response $R = \\{R_1, ..., R_M\\}$ to question Q, the construction of the Multi-APDF matrix is a dot-product fusion of L Single-APDF matrix. Based on the characteristics of the code community shown in Figure 1, the main attributes that influence user preferences are semantics (text content), popularity, and creation time.\nSemantic-APDF matrix $\\Delta_{se} = \\{d_{se_{i,j}}|i, j \\in M\\}$, we define $G^{Se}(i) = 2^{\\varpi(i)-1}$, where $\\varpi(i) = cos(E_Q, E_{C_i})$. Here, $E_Q \\in \\mathbb{R}^{q \\times d}$ and $E_{C_i} \\in \\mathbb{R}^{r \\times d}$ represent the semantic vectors of the question Q and the text content $C_i$ of response $R_i$, encoded by prompt-based LLMs (BehnamGhader et al., 2024). Here, q is the length of the question, r is the length of the text content, and d is the dimension of the embedding space.\nPopularity-APDF matrix $\\Delta_{po} = \\{d_{po_{i,j}}|i, j \\in M\\}$ To mitigate the bias caused by the accumulation of popularity over time, we apply time decay to P based on T, denoted as $\\hat{P}$. To avoid bias caused by extreme values and excessive numerical differences, we set $G^{Po}(i) = \\log(\\hat{P}_i + 1)$.\nMulti-APDF matrix on the scalable attribute $A = \\{A_1, ..., A_L\\}$ is represented generally as:\n$\\Delta_M = \\prod_{k=1}^L \\Delta_{A_k}$\t(3)\nwhere $\\Delta_{A_k}$ is the APDF matrix corresponding to attribute $A_k$. Similarly, The code Multi-APDF matrix $\\Delta_{M}^{code} \\in \\mathbb{R}^{M \\times M}$ is represented as follows:\n$\\Delta_{M}^{code} = \\Delta_{Se} \\odot \\Delta_{Po}$\t(4)"}, {"title": "2.2.3 Self-supervision Dynamic Ranking", "content": "To avoid relying on manually labeled alignment targets, we propose the Self-supervised Dynamic Ranking based on the Multi-APDF Matrix. It iteratively selects the most significant pair-wise distance (Multi-APDF $\\delta_M$) and ranks the candidates according to the semantic ranks, which ensures that the ranking not only reflects pair-wise perceptual differences but also adheres to semantic priorities. Its implementation details are provided in the Algorithm 1. The DR represents the set of candidates' positions after dynamic ranking:\n$DR = \\{l_1, l_2, l_2,..., l_M\\}$\t(5)"}, {"title": "2.3 Perceptual Alignment", "content": "Since the most effective learning for domain knowledge method is SFT (Stiennon et al., 2020), and the most direct one in alignment is also to perform SFT on a high-quality preference dataset (Rafailov et al., 2024), we align the optimal response by treating the first response in dynamic ranking as the target for SFT for the question Q. The first optimization objective is represented as follows:\n$\\mathcal{L}_{Pa} = - \\frac{1}{R_b} \\sum_{j=1}^{R_b} log P(R_1(j)|Q, R_1(<j))$\t(6)"}, {"title": "2.4 Perceptual Comparison", "content": "In terms of many list-wise loss functions, the softmax cross-entropy loss in ListNet (Cao et al., 2007) uses double summation to emphasize comparisons between different samples, making it suitable for ranking loss. Therefore, we adopt it as the basis for the second optimization objective and conduct a total of M -1 iterative comparisons. To deepen the impact of preference differences, for each iteration, we maximize the reward for positive and minimize the penalty for remains negative sequentially.\nMaximizing the reward is achieved by finding all maximum value in the mapped row of the alignment target in all Single-APDF matrix, and then multiplying the values together. For the m-th comparison, it is represented as follows:\n$W_m = \\prod_{k=1}^L \\max(\\Delta_{A_k} (DR(m), \\cdot))$\t(7)\nwhere $\\Delta_{A_k} (i, j)$ refers to the element at the i-th row and j-th column of $\\Delta_{A_k}$, and $\\cdot$ represents all elements in the row or column.\nMinimizing the penalty involves differentiating the penalty strengths based on preference levels, where a slight penalty is applied to $R_{DR(i)}$ and a stronger penalty is applied to $R_{DR(i+1)}$. This approach contrasts with the existing method, which applies the same penalty to all negative examples, and ensures that the penalty for responses ranked higher in the self-supervised ranking DR is minimized. For the negative $R_i$, its penalty is represented as follows:\n$W = sort(\\Delta_M(DR(m), \\cdot))(i)$\t(8)\nwhere $sort(\\cdot)$ is the function that sorts in an ascending order. $\\Delta_M(i, j)$ is the i-th row and the j-th APDF in the Multi-APDF matrix.\nTo achieve on-chain ranking and fine-grained distinction among all responses, unlike traditional optimization methods that sequentially remove the optimal response, all responses participate in each iteration. Moreover, the corresponding penalties or rewards for the responses change throughout the iterations. The second optimization objective is represented as:\n$\\mathcal{L}_{Pc} = - \\sum_{m=1}^{M-1} log(\\frac{\\Tau_r(b)}{\\sum_{i=1}^{M} \\Tau_p(i)})$  (9)\n$\\Tau_r(b) = exp(\\pi_\\zeta(Q, R_b)) * W_m$\t(10)\n$\\Tau_p(i) = exp(\\pi_\\zeta(Q, R_i)) * W$\t(11)\n$\\pi_\\zeta(Q, R_i) = \\frac{1}{S_t}\\sum_{k=1}^{S_t} log P(r_k|Q,r_{<k})$\t(12)\nHere, $DR(m)$ denotes as b. the $\\pi_\\zeta(\\cdot)$ represents a policy network that replaces the reward in RLHF with language modeling logits. The labeled response R, composed of t tokens, is denoted as $R_i = \\{r_1,...,r_t\\}$. Finally, SeAdpra enables LLMs to be trained by the following objective:\n$\\mathcal{L}_{Loss} = \\mathcal{L}_{Pc} + \\alpha \\cdot \\mathcal{L}_{Pa}$\t(13)\nTo avoid overfitting the initial best response, $\\alpha$ will control the balance between it and the remaining preferences, thereby ensuring text quality."}, {"title": "3 Experiments", "content": "Due to the additional challenges that programming QA presents for LLMs and the lack of high-quality, authentic multi-answer code preference datasets, we turned to StackExchange , a platform with forums that are accompanied by rich question-answering metadata. Based on this, we constructed a large-scale programming QA dataset in real-time (as of May 2024), called StaCoCoQA. It contains over 60,738 programming directories, as shown in Table 8, and 9,978,474 entries, with partial data statistics displayed in Figure 2. The data format of StaCoCoQA is presented in Table 11.\nThe initial dataset $D_1$ contains 24,101,803 entries, and is processed by the following steps: (1) Select entries with \"Questioner-picked answer\" pairs to represent the preferences of the questioners, resulting in 12,260,106 entries in the $D_Q$. (2) Select data where the question includes at least one code block to focus on specific-domain programming QA, resulting in 9,978,474 entries in the dataset $D_C$. (3) All HTML tags were cleaned using BeautifulSoup  to ensure that the model is not affected by overly complex and meaningless content. (4) Control the quality of the dataset by considering factors such as the time the question was posted, the size of the response pool, the difference between the highest and lowest votes within a pool, the votes for each response, the token-level length of the question and the answers, which yields varying sizes: 3K, 8K, 18K, 29K, and 64K. The controlled creation time variable and the data details after each processing step are shown in Table 7.\nTo further validate the effectiveness of SeAdpra, we also select eight popular topic CoQA datasets , which have been filtered to meet specific criteria for preference models (Askell et al., 2021). Their detailed data information is provided in Table 6."}, {"title": "3.2 Evaluation Metrics", "content": "For preference evaluation, we design PrefHit and PrefRecall, adhering to the \"CSTC\" criterion outlined in Appendix A.2, which overcome the limitations of existing evaluation methods, as detailed in Appendix A.1. In addition, we demonstrate the effectiveness of thees new evaluation from two main aspects: 1) consistency with traditional metrics, and 2) applicability in different application scenarios in Appendix A.4. Following the previous (Song et al., 2024), we also employ a professional reward.\nFor accuracy evaluation, we alternately employ BLEU (Papineni et al., 2002), RougeL (Lin, 2004), and CoSim. Similar to codebertscore (Zhou et al., 2023), CoSim not only focuses on the semantics of the code but also considers structural matching. Additionally, the implementation details of SeAdpra are described in detail in the Appendix E.2."}, {"title": "3.3 Main Results", "content": "We compared the performance of SeAdprawith general LLMs and strong preference alignment benchmarks on the StaCoCoQA dataset, as shown in Table 1. Additionally, we compared SeAdpra with the strongly supervised alignment model PRO (Song et al., 2024) on eight publicly available CoQA datasets, as presented in Table 2 and Figure 8.\nLarger Model Parameters, Higher Preference. Firstly, the Qwen2 series has adopted DPO (Rafailov et al., 2024) in post-training, resulting in a significant enhancement in Reward. In a horizontal comparison, the performance of Qwen2-7B and LLaMA2-7B in terms of PrefHit is comparable. Gradually increasing the parameter size of Qwen2 (Yang et al., 2024) and LLaMA leads to higher PrefHit and Reward. Additionally, general LLMs continue to demonstrate strong capabilities of programming understanding and generation preference datasets, contributing to high BLEU scores. These findings indicate that increasing parameter size can significantly improve alignment.\nList-wise Ranking Outperforms Pair-wise Comparison. Intuitively, list-wise DPO-PT surpasses pair-wise DPO-BT on PrefHit. Other list-wise methods, such as RRHF, PRO, and our SeAdpra, also undoubtedly surpass the pair-wise Slic.\nBoth Parameter Size and Alignment Strategies are Effective. Compared to other models, Pythia-2.8B achieved impressive results with significantly fewer parameters. Effective alignment strategies can balance the performance differences brought by parameter size. For example, LLaMA2-7B with PRO achieves results close to Qwen2-57B in PrefHit. Moreover, LLaMA2-7B combined with our method SeAdpra has already far exceeded the PrefHit of Qwen2-57B.\nRather not Higher Reward, Higher PrefHit. It is evident that Reward and PrefHit are not always positively correlated, indicating that models do not always accurately learn human preferences and cannot fully replace real human evaluation. Therefore, relying solely on a single public reward model is not sufficiently comprehensive when assessing preference alignment."}, {"title": "3.4 Ablation Study", "content": "In this section, we discuss the effectiveness of each component of SeAdpra and its impact on various metrics. The results are presented in Table 3.\nPerceptual Comparison aims to prevent the model from relying solely on linguistic probability ordering while neglecting the significance of APDF. Removing this Reward will significantly increase the margin, but PrefHit will decrease, which may hinder the model's ability to compare and learn the preference differences between responses.\nPerceptual Alignment seeks to align with the optimal responses; removing it will lead to a significant decrease in PrefHit, while the Reward and accuracy metrics like CoSim will significantly increase, as it tends to favor preference over accuracy.\nSemantic Perceptual Distance plays a crucial role in maintaining semantic accuracy in alignment learning. Removing it leads to a significant decrease in BLEU and Rouge. Since sacrificing accuracy recalls more possibilities, PrefHit decreases while PrefRecall increases. Moreover, eliminating both Semantic Perceptual Distance and Perceptual Alignment in PerCop, further increases PrefRecall, while the other metrics decline again, consistent with previous observations.\nPopularity Perceptual Distance is most closely associated with PrefHit. Eliminating it causes PrefHit to drop to its lowest value, indicating that the popularity attribute is an extremely important factor in code communities."}, {"title": "3.5 Analysis and Discussion", "content": "SeAdpra adept at high-quality data rather than large-scale data. In StaCoCoQA, we tested PRO and SeAdpra across different data scales, and the results are shown in Figure 5. Since we rely on the popularity and clarity of questions and answers to filter data, a larger data scale often results in more pronounced deterioration in data quality. In Figure 5a, SeAdpra is highly sensitive to data quality in PrefHit, whereas PRO demonstrates improved performance with larger-scale data. Their performance on Prefrecall is consistent. In the native reward model of PRO, as depicted in Figure 5c, the reward fluctuations are minimal, while SeAdpra shows remarkable improvement.\nSeAdpra is relatively insensitive to ranking length. We assessed SeAdpra's performance on different ranking lengths, as shown in Figure 6a. Unlike PRO, which varied with increasing ranking length, SeAdpra shows no significant differences across different lengths. There is a slight increase in performance on PrefHit and PrefRecall. Additionally, SeAdpra performs better at odd lengths compared to even lengths, which is an interesting phenomenon warranting further investigation.\nBalance Preference and Accuracy. We analyzed the effect of control weights for Perceptual Comparisons in the optimization objective on preference and accuracy, with the findings presented in Figure 6b. When $\\alpha$ is greater than 0.05, the trends in PrefHit and BLEU are consistent, indicating that preference and accuracy can be optimized in tandem. However, when $\\alpha$ is 0.01, PrefHit is highest, but BLEU drops sharply. Additionally, as $\\alpha$ changes, the variations in PrefHit and Reward, which are related to preference, are consistent with each other, reflecting their unified relationship in the optimization. Similarly, the variations in Recall and BLEU, which are related to accuracy, are also consistent, indicating a strong correlation between generation quality and comprehensiveness.\nSingle-APDF Matrix Cannot Predict the Optimal Response. We randomly selected a pair with a golden label and visualized its specific iteration in Figure 7. It can be observed that the optimal response in a Single-APDF matrix is not necessarily the same as that in the Multi-APDF matrix. Specifically, the optimal response in the Semantic Perceptual Factor matrix $\\Delta_{se}$ is the fifth response in Figure 7a, while in the Popularity Perceptual Factor matrix $\\Delta_{Po}$ (Figure 7b), it is the third response. Ultimately, in the Multiple Perceptual Distance Factor matrix $\\Delta_M$, the third response is slightly inferior to the fifth response (0.037 vs. 0.038) in Figure 7c, and this result aligns with the golden label. More key findings regarding the ADPF are described in Figure 13 and Figure 14."}, {"title": "4 Security Verification", "content": "To explore the impact of enhanced preference on the original safety, we conducted additional preference alignment experiments on the absolutely benign data from the safety alignment dataset PKU-SafeRLHF (Ji et al., 2024b,a), as shown in Figure 9. The results are presented in Table 4 and Table 5 and other details are described in Appendix B.\nPrefHit and PrefRecall can be transferred to other attribute alignments, such as safety alignment. As long as there is a preference order on a certain attribute, such as the safer_response_id in Figure 9, PrefHit and PrefRecall can be transferred to evaluate the alignment of the corresponding attribute, such as SaferHit and SaferRecall. Since the safety alignment dataset PKU-SafeRLHF only has two candidate responses, SaferHit is equal to SaferRecall, so we only present SaferHit in the Table 4 and Table 5.\nSafety is positively correlated with preference. No matter the preference alignment strategy, the toxicity decreases significantly as PrefHit increases, ultimately stabilizing at a negligible level of 0.006. SaferHit represents a preference for safer responses, evaluating both safety and preference. It is positively correlated with PrefHit and negatively correlated with toxicity."}, {"title": "5 Limitations", "content": "The domain adaptability of SeAdpra relies to some extent on predefined attributes, requiring manual adaptation of the attribute system, which bears similarities to the domain transfer bottlenecks observed in rule-based reward models. In fine-grained preference alignment, the model may face a \"preference-generalization\" trade-off, where over-optimizing for specific preferences could weaken its general generation ability, a common issue in post-training stages like instruction fine-tuning and reward modeling. At this stage, we focus on preference and accuracy, without evaluating the coherence and factual correctness of responses. In the future, we will work towards addressing these issues."}, {"title": "6 Related Work", "content": null}, {"title": "6.1 Preference Alignment and Ranking", "content": "Learning from human preferences (Christiano et al., 2017) aims to better align language models with human intentions and values, making their generated content more helpful, factual, and ethical (Ouyang et al., 2022). RLHF (Ouyang et al., 2022; Stiennon et al., 2020) can achieve this alignment through PPO(Schulman et al., 2017) based on human feedback data. To circumvent the complexities of the RLHF, DPO (Rafailov et al., 2024) directly learns the distinction between human-labeled preferences and non-preferences by minimizing the difference in their log probabilities. SLiC (Zhao et al., 2023) and RRHF (Yuan et al., 2024) use pair-wise hinge loss to align policy responses. Curry-DPO (Pattnaik et al., 2024) simulates curriculum learning by sequentially ranking during training, using multiple preference pairs. Therefore, most frameworks (Azar et al., 2024; Liu et al., 2023) are limited to pairwise preferences and heavily rely on human annotations. Although DPO proposes list-wise alignment based on the Plackett-Luce assumption (Luce, 1959), no experimental results are provided.\nAt this stage, PRO (Song et al., 2024) introduces list maximum likelihood estimation (MLE) loss to focus on preference ranking, marking a pioneering effort in list-wise alignment. However, it lacks attention to other intrinsic attribute values of the responses beyond the semantic content. LiPO (Liu et al., 2024), which is most similar to ours, directly optimizes list-based preferences and considers response labels but has not yet addressed the combination of multiple labels."}, {"title": "7 Conclusion", "content": "In this paper, we propose SeAdpra by introducing the Attribute-Aware Preference Distance Factor (APDF), SeAdpra precisely quantifies preference differences among multiple responses, enabling label-free self-supervised dynamic ranking. Based on the ranking results, SeAdpra performs multiple rounds of preference comparison to achieve better alignment between LLMs and community question answering. To validate the effectiveness of SeAdpra, we introduce cost-effective, scalable, transferable, and consistent evaluation metrics, PrefHit and PrefRecall. Additionally, we construct a challenging programming-oriented CoQA preference dataset, StaCoCoQA. Extensive experimental results on public datasets and StaCoCoQA demonstrate that SeAdpra outperforms general LLMs and supervised alignment baselines while maintaining safety. Furthermore, we explore the impact of various factors on SeAdpra's performance. Overall, our work offers a novel perspective on aligning LLMs with multifactorial human preferences."}, {"title": "A.2 The \u201cCSTC\u201d Criterion", "content": "Cost-effectiveness Whether upgrading the original method $M_A$ to $M_{A1}$ or expanding the comparison method $M_E$, only one evaluation of $M_{A1}$ or $M_E$ is required, instead of pairwise comparisons between $M_{A1}$ and ($M_B, M_C, M_D$), or $M_E$ and $M_A$. Importantly, we have discovered new metrics achieves a consistency of 0.98 with human annotations.\nScalability is reflected in three aspects: 1)The upgrade of the original method; 2)The expansion of the comparison method; 3) The transformation of candidate responses from binary to multiple.\nTransferability This evaluation has broad applicability across various domains. Specifically, it not only assesses preference alignment but can also be transferred to other alignment areas, such as SaferHit in safety alignment, as shown in Eq.(18).\nConsistency To validate the effectiveness of new metrics, we conducted consistency checks between them and commonly used reward model-based preference alignment evaluation methods, as well as metrics for evaluating model general reasoning abilities, namely BLEU and ROUGE. The results show that PrefHit and PrefRecall are strongly consistent with hese classic metrics."}, {"title": "A.3 PrefHit and PrefRecall", "content": "To adapt to the list-wise CoQA and adhere to the CSTC guidelines proposed in Appendix A.2, enspired by the Hit and Recall, the specific calculation methods are as follows:\nPrefHit@k = $\\frac{1}{N} \\sum_{i=1}^{N} I(\\Phi(x, R^i) \\in G_i(k))$\t(14)\nHere, $\\Phi(x, R^i)$ denotes the similarity between x, which represents a response generated by the LLM to be evaluated, and k instances of $R^i = \\{R_1,..., R\\}$, a set of candidate responses for a given question Q, and returns the index corresponding to the maximum similarity. $G_i(k)$ denotes the indices of the top k items in the list-wise golden label of the $R_i$.\n$\\Phi(x, R^i) = \\underset{j}{\\arg \\max} Sim(x, R_j)$\t(15)\nSimilarly,\nPrefRecall@k = $\\frac{1}{N} \\sum_{i=1}^{N} \\frac{|\\Psi(x, R^i, k) \\cap G_i(k)|}{2}$\t(16)\nHere, $\\Psi(x, R^i, k)$ represents the indices of the top k most similar $R_i$ to x based on the similarity.\n$\\Psi(x, R^i, k) = \\underset{j<k}{\\text{argsort}} (Sim(x, R_i))$\t(17)\nIt is worth noting that Sim(x, $R_i$) has traditionally been evaluated by human annotators, which is expensive and time-consuming. We propose an alternative using llm2vec4 (BehnamGhader et al., 2024), as Large Language Models are powerful text encoders. We chose this replacement because its scores on 276-item test set are highly consistent with human labels, with a correlation of 0.98."}, {"title": "B.2 Safety Evaluation", "content": "Existing Harmfulness Evaluation can be classified into three categories: 1) The first category relies on keyword detection, using a predefined set of keywords (e.g., \"sorry,\" \"as,\" and 47 other keywords). These methods have been used (Zou et al., 2023) and are referred to as keyword-based methods in the study (Qi et al., 2023). Although this approach is efficient and cost-effective, it can lead to false positives and false negatives when harmful content contains these keywords or when harmless content does not. The second category is based on GPT-4's automated harmfulness evaluation, i.e., GPT-4 Judge (Qi et al., 2023), which introduces more policy-specific knowledge and contextual understanding into the evaluation mechanism to effectively assess harmful content in conversations. However, it depends on complex policy knowledge, conversation context, and manually predefined scoring rules. Additionally, the reasoning based on chain-of-thought makes the evaluation process time-consuming and expensive. The third category is based on pre-trained content moderation classifiers, such as OpenAI's Moderation API (OpenAI, 2023), Perspective API (Lees et al., 2022), and Detoxify's pre-trained toxicity prediction models (Hanu and team, 2020). In this study, we choose the Perspective API in the third category, as it is a high-accuracy used and cost-effective evaluation approach.\nThe transfer of PrefHit to SaferHit. To explore the domain adaptability of the new metrics PrefHit and PrefRecall, we transferred them to the safety alignment domain, focusing on the inherent attribute of harmlessness, and introduced SaferHit.\nSaferHit =  {  1, if  (x, R) = gold   0, if  (x, R) != gold (18)\nHere, R = {$R_1, R_2$} is shown in Figure 9, the Gold represents the safer response. (x, R) is explained in Eq.(15)."}, {"title": "C.1 Reinforcement Learning from Human Feedback", "content": "Given a preference dataset $\\mathcal{D} = \\{(x, y_w, y_1)\\}$, where x is an input, $y_w$ and $y_1$ are the preferred and dispreferred outputs (i.e., $y_w > y_1$ for x), and $r^*"}, {"title": "D Related Work", "content": null}, {"title": "D.1 Alignment of LLMs.", "content": "The language modeling objective of Large Language Models (e.g., predicting the next word) differs from the ultimate goals in LLM applications, such as following instructions and being helpful, factual, and harmless(Qi et al., 2023; Bhardwaj et al., 2024; Yi et al., 2024). The behavior of pre-trained LLMs may not necessarily align with the principles of their intended use cases. Therefore, alignment of LLMs (Zhu et al., 2024; Wang et al., 2024) aims to adjust the outputs of general pre-trained language models to better align with human preferences, significantly improving the performance of LLMs in various downstream applications, such as Summarization(Hu et al., 2024), dialogue agents (Niu et al., 2024), and question-answering (Panda et al., 2024). Currently, the two most common alignment techniques are instruction tuning (Ren et al., 2024) and reinforcement learning from human feedback (RLHF) (Bai et al., 2022; Ouyang et al., 2022). Additionally, emerging alignment techniques such as Constitutional AI (Bai et al., 2022) and self-alignment (Ren"}]}