{"title": "Advanced System Integration: Analyzing OpenAPI\nChunking for Retrieval-Augmented Generation", "authors": ["Robin D. Pesl", "Jerin G. Mathew", "Massimo Mecella", "Marco Aiello"], "abstract": "Abstract Integrating multiple (sub-)systems is essential to create ad-\nvanced Information Systems (ISs). Difficulties mainly arise when integrat-\ning dynamic environments across the IS lifecycle, e.g., services not yet\nexistent at design time. A traditional approach is a registry that provides\nthe API documentation of the systems' endpoints. Large Language Mod-\nels (LLMs) have shown to be capable of automatically creating system\nintegrations (e.g., as service composition) based on this documentation\nbut require concise input due to input token limitations, especially regard-\ning comprehensive API descriptions. Currently, it is unknown how best to\npreprocess these API descriptions. Within this work, we (i) analyze the\nusage of Retrieval Augmented Generation (RAG) for endpoint discovery\nand the chunking, i.e., preprocessing, of state-of-practice OpenAPIs to\nreduce the input token length while preserving the most relevant informa-\ntion. To further reduce the input token length for the composition prompt\nand improve endpoint retrieval, we propose (ii) a Discovery Agent that\nonly receives a summary of the most relevant endpoints and retrieves\nspecification details on demand. We evaluate RAG for endpoint discovery\nusing the Rest Bench benchmark, first, for the different chunking possibil-\nities and parameters measuring the endpoint retrieval recall, precision,\nand F1 score. Then, we assess the Discovery Agent using the same test set.\nWith our prototype, we demonstrate how to successfully employ RAG for\nendpoint discovery to reduce the token count. While revealing high values\nfor recall, precision, and F1, further research is necessary to retrieve\nall requisite endpoints. Our experiments show that for preprocessing,\nLLM-based and format-specific approaches outperform na\u00efve chunking\nmethods. Relying on an agent further enhances these results as the agent\nsplits the tasks into multiple fine granular subtasks, improving the overall\nRAG performance in the token count, precision, and F1 score.\nKeywords: Retrieval augmented generation Large language models\nOpenAPI Endpoint discovery Rest Bench.", "sections": [{"title": "1 Introduction", "content": "OpenAPI is the state-of-practice for describing interfaces for integrating Informa-\ntion Systems (ISS). It contains formal elements like paths and natural language"}, {"title": "2 Related Work", "content": "Regarding endpoint discovery, we provide a brief overview of the essential concepts\nof the various service discovery approaches. Additionally, we provide relevant\ninsights into LLMs and the novel approach of integrating LLMs with tools, known\nas LLM agents, and how they relate to our approach."}, {"title": "2.1 Service Discovery", "content": "The most common service discovery implementation is a service registry, which\ncollects information about available services and offers search facilities. This\nservice registry is usually backed by a component residing at the middleware\nor application level [15]. It is characterized by the syntax used to describe the\nservices and their invocation and the expressive power of the available query\nlanguage. The typical integration model is a pull model where service consumers\nsearch for the required services. Less is a push model as used in the UPnP\nprotocol, where service providers regularly advertise their services [32].\nIn the early days of XML-based services, the infrastructure for service discov-\nery was the Universal Description, Discovery, and Integration (UDDI) specifi-\ncation [7]. UDDI had a global incarnation called the UDDI Business Registry\n(UBR), intended to offer an Internet-wide repository of available web services\nand promoted by IBM, Microsoft, and SAP. Unfortunately, UBR never gained\nwidespread adoption and was short-lived (2000-2006). Significant research in\nthe early days focused on enhancing service discovery on UDDI, improving\nsearch capabilities, and creating federated registries, e.g., [3,4,11]. Alternatively,\nWS-Discovery is a multicast protocol that finds web services on a local network.\nNowadays, OpenAPI is the de facto standard for describing services. While\nnot offering a discovery protocol and mechanism, given its popularity, OpenAPI\nwould also benefit from discovery [34]. So, additional infrastructure for discovery\nhas been proposed, such as centralized repositories (SwaggerHub or Apiary),\nservice registry integration (Consul, Eureka), API Gateways (Kong, Apigee), or\nKubernetes annotations (Ambassador).\nPopulating registries of services requires effort from service providers, which\noften hinders the success of such approaches, especially if the service provider is\nexpected to provide extensive additional information beyond the service endpoints.\nThis additional effort has often been the reason for the failure of some of these\ntechnologies, most notably UBR. Approaches confined to specific applications,"}, {"title": "2.2 Large Language Models", "content": "LLMs represent one of the recent advancement in the Natural Language Processing\n(NLP) and machine learning field [1,2,13]. Often containing billions of parameters,\nthese models are trained on extensive text corpora to generate and manipulate\nhuman-like text [30]. They are primarily based on an encoder-decoder architecture\ncalled Transformers [37], which has been further refined to improve text generation\ntasks using decoder-only models such as GPT [31]. Usually, the input is a\nnatural language task called prompt, which first needs to be translated to a\nsequence of input tokens. The model processes this prompt and returns an\noutput token sequence, which can then be translated back to a natural language\nanswer. As these models can, in general, capture intricate linguistic nuances\nand semantic contexts, they can be applied to a wide range of tasks, e.g., in\nsoftware engineering [10]. LLMs can be used to create integration based on\nendpoint documentation automatically [27,28,29]. Yet, these face strict input\ntoken limitations, e.g., 128,000 tokens for current OpenAI models [23,29]. With\nthis paper, we analyze how RAG can be used to preprocess API documentation\nto mitigate this issue.\nAnother approach is encoder-only models such as BERT [8], often referred\nto as embedding models. They allow condensing the contextual meaning of a\ntext into a dense vector, termed embedding. Using similarity metrics such as dot\nproduct, cosine similarity, or Euclidean distance allows for assessing the similarity\nof two input texts. Embedding models are usually used for the similarity search\nin RAG systems [6], which we also do in our implementation."}, {"title": "2.3 LLM Agents", "content": "LLMs have shown remarkable capabilities in solving complex tasks by decompos-\ning them in a step-by-step fashion [38] or by exploring multiple solution paths\nsimultaneously [41]. Typically, these plans are generated iteratively by using the\nhistory of the previously generated steps to guide the generation of the next step.\nAdditionally, recent studies have shown the potential of providing LLMs access\nto external tools to boost their reasoning capabilities and add further knowledge.\nThis approach consists of prompting the LLM to interact with external tools to\nsolve tasks, thus offloading computations from the LLM to specialized functions.\nNotable examples of such tools include web browsers [20], calculators [5], and"}, {"title": "3 Solution Design", "content": "We first introduce the general architecture to employ RAG for endpoint discovery.\nAs state-of-practice for service documentation, we then investigate how to chunk\nOpenAPIs as preprocessing for RAG."}, {"title": "3.1 RAG for Endpoint Discovery", "content": "RAG comprises a preprocessing step ahead of the answer generation of an LLM to\nenrich the prompt with additional data. Therefore, a retrieval component performs\na semantic search based on some knowledge sources. Usually, the semantic search\nis done by embedding similarity, and the data from the knowledge sources is\nreduced to small chunks to allow fine-grained information retrieval [16]."}, {"title": "3.2 OpenAPI Chunking Strategies", "content": "A critical step in the RAG workflow is creating the chunks for the chunk database.\nEmbedding models typically have a limited input token size, and real-world service\nregistries can contain tens of thousands of services, each containing multiple\npotentially lengthy endpoints due to detailed descriptions or extensive input and\noutput schemas. So, a single service might not fit into the context size of the\nembedding model or even exceed the limit of the LLM that further processes the\noutput of the RAG system. In addition, service documentation can also feature\nadditional metadata that, while valuable for understanding service details, is not\nnecessarily relevant for composing services to solve a query."}, {"title": "4 Evaluation", "content": "To evaluate the OpenAPI RAG and the Discovery Agent, we implement it as a\nfully operational prototype. Then, we employ the RestBench [35] benchmark to\nvalidate it in a real-world setting."}, {"title": "4.1 Implementation", "content": "We implement the OpenAPI RAG and Discovery Agent approaches as open-\nsource prototypes based on the LlamaIndex library. For the prototypes, we rely\nsolely on OpenAPIs as the state-of-practice for service descriptions. All sources\nand results are available online. 4"}, {"title": "4.2 Dataset and Metrics", "content": "We evaluate our approach using the RestBench benchmark, covering the Spotify\nand TMDB OpenAPI specifications [35]. With 40 endpoints for Spotify and 54 for\nTMDB, this benchmark is much more complex than usual Service-Oriented Com-\nputing (SOC) case studies containing usually just three to seven endpoints [28]."}, {"title": "4.3 RAG", "content": "Table 2 shows the RestBench results for the OpenAPI RAG on the Spotify API.\nIn recall, the JSON split method performs exceptionally well, especially with\na high chunk size $s$, as this approach densely packs the information from the\nJSON into the chunks by removing all formatting. For precision and F1, the\nendpoint splitting approaches perform best because each chunk corresponds to\nprecisely one endpoint. Differences between the models are minor, except that\nthe bge-small-en-v1.5 performs worse for the no split approach. We also tested\n$s = 2048$ and $s = 4096$, which are not reported here for space reasons. We\nshows $s = 1024$ because it is the default chunk size of LlamaIndex and $s = 8191$\nbecause it is the maximum input token count for the OpenAI model. It is worth\nmentioning that with an increasing chunk size, the token size of the returned\nresult also increases. Generally, a higher recall seems to correlate with a higher\ntoken count, e.g., no splitting with $s = 1024$, and $l = 0$ has 4717 tokens output\non average. In contrast, the JSON split has 10056 with the same parameters, but\nthis needs further analysis. Due to length limitations, we cannot show the token\ncount comparison and other values for top k here. We also tested top k = 5 and\ntop k = 20. Recall increases with a higher top k, but precision drops. Additional\ndata is available in the complementary material.4\nTable 3 presents the OpenAPI RAG RestBench results for the TMDB API.\nThe TMDB OpenAPI is more complex in length and extent than the Spotify\nOpenAPI. In this case, the endpoint split-based approaches performs best in"}, {"title": "$\\frac{TP}{TP+FN}$", "content": null}, {"title": "$\\frac{TP}{TP+FP}$", "content": null}, {"title": "2-recall-precision\nrecall+precision", "content": null}, {"title": "4.4 Discovery Agent", "content": "We present the RestBench results of the Discovery Agent in Table 4. For accuracy,\nwe measure recall, precision, and F1 equally to the OpenAPI RAG experiments.\nFor the token count, we measure the actual tokens sent from the agent to the\nLLM from the agent as prompt, the tokens received as completion, and their\nsum as total. For the RAG approach, we accumulate the tokens of the retrieved\nchunks.\nThe results show that both agent approaches improve precision and F1 but\nreduce recall. The Query approach increases the tokens in the prompt. Contrarily,\nthe Summary approach significantly outperforms the RAG and the query approach\nin the total token count. The completion token count is by a magnitude smaller\nthan the prompt token count for the agent approaches, which is relevant as\ncompletion tokens are usually more expensive than prompt tokens. No LLM is\ninvoked in the RAG approach, so the completion tokens are zero."}, {"title": "4.5 Discussion", "content": "We demonstrated the effectiveness of the OpenAPI RAG and the Discovery Agent\nusing our implementation. They are able to retrieve large portions of relevant\ndata while not revealing all relevant information in all cases.\nTo address RQ1, we implemented the OpenAPI RAG to apply RAG for\nendpoint discovery with seven chunking strategies and numerous parameter\ncombinations. We showed its effectiveness using the RestBench benchmark.\nOverall, the ability to adequately reduce the token size to fit into the LLM\ncontext size while maintaining most of the relevant information is exhibited by\nthe prototype. Regarding the chunking strategies, endpoint split-based chunking\nstrategies achieve favorable accuracies. Limitations are primarily that the RAG\nresults may not contain all relevant information, and the precision is low due to\nthe retrieval of exactly k chunks. Additional research is needed to improve the\nretrieval performance further and prove the results in a generalized setting across\nmultiple domains.\nFor RQ2, we introduced the Discovery Agent, which transfers the LLM agent\npattern to endpoint discovery. Especially using Summary approach, the Discovery\nAgent showed strong improvement over the OpenAPI RAG in terms of precision,\nF1, and token count. Further research is needed to improve the decline in recall\ndue to the processing through the LLM.\nWhile we rely on the research benchmark RestBench for our results, which\ncovers two extensive OpenAPIs, queries, and ground truth, it is still limited to\nthese two services. OpenAPI RAG systems in practice may operate on much larger\ndatasets. For the data processing, we rely on standard RAG implementations\nlike LlamaIndex, which are already designed to operate on large amounts of data.\nThe performance evaluation, especially in larger real-world scenarios, remains\nopen for future research.\nThe applicability of the OpenAPI RAG depends on the availability of service\ndocumentation. We try to mitigate this issue by relying on widely adopted\nOpenAPI specifications, but this might not be valid for all domains. A solution\nto consider is automatically generating service documentation using an LLM.\nAnother factor influencing the discovery is the quality of the OpenAPIs. The\ndiscovery may fail if no descriptions, meaningful naming, or erroneous information\nis given. This is not an issue of the approach, as a human developer would face the\nsame problem, but it highlights the importance of high-quality documentation.\nIn addition to the presented chunking strategies, additional and more advanced\nstrategies, e.g., CRAFT [42], could be added to the OpenAPI RAG. These could\nimprove retrieval performance by combining multiple strategies or by creating a\ncustom chunking strategy for a specific kind of service documentation.\nAnother advancement could also be creating a custom embedding model\ntailored explicitly to service descriptions and service description chunks. This\nmodel may also be trained for one specific chunking strategy or intended use\ncase. Additionally, the RAG output may be trimmed to boost precision. This\ncould be done by, e.g., employing a similarity threshold."}, {"title": "5 Concluding Remarks", "content": "The service discovery challenge has been around for a long time in SOC to integrate\ndifferent ISs. With the application of automated LLM-based service composition\napproaches, the LLM input context limitations have become prominent, as the\nentire service documentation often does not fit into the input context, necessitating\nthe preselection of relevant information. To address this issue, we proposed an\nOpenAPI RAG, which facilitates semantic search based on state-of-the-practice\nOpenAPIs and reduces the input token size. Further, we show an advanced\nintegration through a Discovery Agent, which can retrieve service details on\ndemand to reduce the input token count further. Our evaluation based on\nthe RestBench benchmark shows that our approach is viable and performing.\nLimitations are especially in the restriction of RestBench to two services of the\nentertainment domain. We will address this in an extended version of this work.\nFurther improvements are in optimizing the implementation and extending the\nagent for additional tasks, e.g., whole service compositions. We leave this for\nfuture work."}]}