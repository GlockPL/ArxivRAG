{"title": "Functional Connectomes of Neural Networks", "authors": ["Tananun Songdechakraiwut", "Yutong Wu"], "abstract": "The human brain is a complex system, and understanding its mechanisms has been a long-standing challenge in neuroscience. The study of the functional connectome, which maps the functional connections between different brain regions, has provided valuable insights through various advanced analysis techniques developed over the years. Similarly, neural networks, inspired by the brain's architecture, have achieved notable success in diverse applications but are often noted for their lack of interpretability. In this paper, we propose a novel approach that bridges neural networks and human brain functions by leveraging brain-inspired techniques. Our approach, grounded in the insights from the functional connectome, offers scalable ways to characterize topology of large neural networks using stable statistical and machine learning techniques. Our empirical analysis demonstrates its capability to enhance the interpretability of neural networks, providing a deeper understanding of their underlying mechanisms.", "sections": [{"title": "Introduction", "content": "The human brain is an incredibly complex system, and understanding its intricate workings has been a long-standing challenge in neuroscience. One well-established approach to gaining deeper insights into the brain's underlying mechanisms is through the study of the functional connectome, which maps the functional connections between regions of brain networks and reflects the brain's dynamic network of interactions (Bullmore and Sporns 2009). In recent decades, successful findings have emerged from this field, thanks to the development of a wide array of analysis techniques.\nArtificial neural networks, inspired by the architecture and functioning of the human brain, have achieved remarkable success in applications ranging from image recognition (He et al. 2016) to natural language processing (Vaswani et al. 2017). However, despite these successes, neural networks are often considered black box models due to their lack of interpretability and the difficulty in understanding the underlying mechanisms driving their performance. Neural networks have predesigned architectures with preconfigured weights connecting neurons, analogous to how physically connected brain regions with white matter fibers provide structural information measured by diffusion MRI. Similarly, functional connections between distant neurons resemble how functional MRI measures functional connectivity between brain regions that may not have direct neuroanatomical connections. These connections in the brain give rise to coordinated activity patterns crucial for cognitive processes (Honey et al. 2009). Given that neural networks are simplified artificial versions of brain functions, it stands to reason that insights from the functional connectome could be leveraged to enhance our understanding, interpretability, and analysis of these networks, potentially leading to the development of more transparent and efficient models.\nHowever, analyzing functional connectomes is inherently challenging due to the need to extract subtle topological patterns from noisy, complete graphs. Therefore, typical workflows apply a threshold to obtain a sparser graph with a clearer structure before applying techniques from graph theory (Bullmore and Sporns 2009). Graph theory has played a crucial role in functional connectome research; however, prior analyses utilizing graph theory have primarily focused on pairwise dyadic relationships, often at a fixed spatial threshold. This approach, centered on dyads, limits the neural structures and functions that graph theory can investigate. Given that a neural network processes information not only based on local neurons of a subnetwork but also across the entire network, from input to output layers, a more comprehensive understanding requires a shift in perspective-from pairwise interactions to capturing higher-order relations (topology) across the full range of spatial resolutions.\nPersistent homology (Edelsbrunner and Harer 2022), an algebraic topology technique, has emerged as a promising tool for understanding and quantifying the topology of the human brain (Sizemore et al. 2018; Songdechakraiwut, Shen, and Chung 2021). Recently, there have been increasing attempts to apply persistent homology to study the interpretability of deep learning. These studies suggest that persistent homology can extract high-order topological information to interpret neural networks, but challenges remain. In particular, current methods often focus solely on the network's structural information, without incorporating data-driven tasks (Rieck et al. 2019; Watanabe and Yamana"}, {"title": "Functional Connectome Framework", "content": "Figure 1 shows a schematic of the functional connectivity analysis framework, described in the following.\nFunctions of Neural Networks\nGiven a collection of data samples, we partition it into two separate datasets: a training dataset and a functional dataset. The training dataset is utilized via k-fold cross-validation to determine a set of optimal hyperparameter values through grid search. Using these optimal values, we train a well-generalized neural network on the entire training data. Subsequently, the functional dataset is fed into the fully-trained neural network, resulting from the aforementioned training process, to investigate the information propagation of new, unknown data through it.\nWithout loss of generality, we will assume that the neural network architecture of interest is a feedforward network. Given a fully-trained feedforward network with M hidden neurons and a functional dataset denoted by $X = \\{x^{(1)}, x^{(2)},...,x^{(N)}\\}$, each data point $x^{(i)} \\in X$ is inputted into the feedforward network and processed through a series of neurons. Each j-th neuron uses an affine transformation followed by an activation function to produce its output $a_{ij}$, representing the information processing for $x^{(i)}$. By processing the entire functional dataset, we concatenate the j-th neuron outputs for all the data points into a functional vector $a_j = (a_{1j}, a_{2j}, . . ., a_{Nj})$, representing the functional signal in a neural network, similar to the functions of connectome circuits studied in functional MRI.\nWhen two functional vectors from a pair of neurons are statistically dependent, they exhibit functional synergy. To"}, {"title": "Persistent Graph Homology", "content": "The functional connectome is a very dense matrix, typically representing a complete graph. Therefore, typical workflows (Bullmore and Sporns 2009) apply a threshold to the correlation values for two main reasons: to obtain a sparser graph with a more apparent structure and to reduce the time complexity of computational methods. In particular, methods in persistent homology have cubic time complexity (Otter et al. 2017), making them computationally infeasible for large, complete graphs. This requires iterative, approximation solutions, which introduce numerical errors and reduce the signal-to-noise ratio. Additionally, varying different threshold values significantly alters the graph structure, potentially affecting the study's final outcome. Importantly, pairwise dyadic correlations at a fixed spatial threshold constrain the neural structures and functions that can be investigated. Given that a neural network processes information not only based on local neurons of a subnetwork but also the entire network from input to output layers, a more comprehensive understanding requires a shift in perspective from pairwise interactions to capturing higher-order relations across the entire range of spatial resolutions.\nIn this work, we address these challenges by leveraging brain-inspired persistent graph homology, a scalable topological-learning paradigm that enables analyses of large-scale functional connectomes without approximation (Songdechakraiwut et al. 2023). It has emerged as a promising tool for understanding, characterizing, and quantifying human connectomes (Songdechakraiwut et al. 2022). Persistent graph homology describes interpretable topological invariants, including connected components (0th homology group) and independent cycles (1st homology group or cycle rank), across the entire spatial scale of a graph.\nFormally, given a functional connectome G, we define a binary graph $G_{\\epsilon}$ with the same set of neurons by thresholding the edge correlations so that an edge between neurons j and k exists if $p_{jk} > \\epsilon$. As $\\epsilon$ increases, more edges are removed from the functional connectome G. Thus, we have a filtration (Lee et al. 2012): $G_{\\epsilon_0} \\supseteq G_{\\epsilon_1} \\supseteq ... \\supseteq G_{\\epsilon_k}$ where $\\epsilon_0 \\leq \\epsilon_1 \\leq ... \\leq \\epsilon_k$ are called filtration values. Persistent homology tracks the birth and death of the topological invariants over these filtration values $\\epsilon$. A topological invariant born at filtration $b_j$ and persisting up to filtration $d_j$ is represented by a point $(b_j, d_j)$ in a 2D plane. The set of all such points $(b_j,d_j)$ is called a persistence diagram (Edelsbrunner and Harer 2022). As $\\epsilon$ increases, the number of connected components $\\beta_0(G_{\\epsilon})$ increases monotonically, while the number of cycles $\\beta_1(G_{\\epsilon})$ decreases monotonically. Thus, persistent graph homology only needs to track a collection of sorted birth values $B(G)$ for the connected components and a collection of sorted death values $D(G)$ for the cycles, given as (Songdechakraiwut and Chung 2023)\n$B(G) = \\{b_j\\}_{j=1}^{M-1}, D(G) = \\{d_j\\}_{j=1}^{\\frac{M(M-3)}{2}}.$\nScalability The set B(G) consists of edge correlations found in the maximum spanning tree (MST) of G. Once B(G) is determined, the set D(G) is derived from the edge correlations that are not part of the MST. Therefore, both B(G) and D(G) can be computed very efficiently in O(n log n) time, where n is the number of edges in the connectome graph."}, {"title": "Persistence Statistics", "content": "Distances are fundamental in statistics because they provide a way to measure how much individual data points vary, en-"}, {"title": "Connectome Analysis of Neural Networks", "content": "Drawing inspiration from functional MRI studies of the human brain, we are interested in the functional behavior of neural networks and whether we can characterize them using our proposed functional connectomes and persistent-graph-homology representation. Through two analysis studies, we aim to explore these connections.\nIn the first study, we will analyze how various popular regularization strategies, including batch normalization, dropout, and L2 regularization, influence the overall functional mechanisms and data propagation in neural networks during inference. Regularization affects neural network weights similarly to how various factors influence structural brain networks obtained by diffusion MRI. This, in turn, impacts how functional mechanisms unfold.\nIn the second study, we will conduct a more fine-grained investigation into how neural networks process different stimuli through functional connectomes. Specifically, we will explore whether there are inherent topological patterns within the functional mechanisms of processing data points from various predefined classes. For example, we will analyze how neural networks process samples from different digit classes (0-9) in the MNIST dataset. This is analogous to how the human brain perceives and processes different visual stimuli, such as recognizing and distinguishing between digits, characterized by human connectomes observed in function MRI studies.\nBy drawing these comparisons, we aim to gain insights into the similarities between neural networks and human brain functions to better understand and characterize these systems."}, {"title": "Study 1", "content": "We will analyze the influence of various popular regularization strategies on the overall functional mechanisms and data propagation in neural networks during inference. To construct functional connectomes for each dataset, we feed the entire corresponding functional dataset to the neural network to extract functional connectomes. As a result, we obtain 80 functional connectomes, with 20 of these connectomes from each training strategy (batch normalization, dropout, L2, and vanilla). We will perform cluster analysis on these 80 data points to group them into four clusters. Figure 2 illustrates the average functional connectomes from training on the Fashion-MNIST dataset using each strategy, along with their corresponding persistence diagrams, which describe the topology. Additionally, we will cluster each regularization strategy against the control group (i.e., vanilla) to better understand the impact of each regularization method"}, {"title": "Study 2", "content": "We will explore how fully-trained neural networks process different stimuli using functional connectomes. We construct these connectomes as follows. For each dataset, we partition the data into collections where each collection contains data points from a specific predefined class. For instance, in the MNIST dataset, we create 10 collections, each corresponding to a digit class (0-9). We then feed each collection into the fully-trained neural network to extract the functional connectomes for that particular class. For each training strategy, we obtain 20 functional connectomes per class, resulting in a total of 200 functional connectomes (20 \u00d7 10). We will perform cluster analysis on these 200 data points to group them into ten clusters.\nAs in Study 1, these datasets in Study 2 are balanced so we will also use the purity score to evaluate clustering performance."}, {"title": "Runtime Experiment", "content": "All topological methods used in the studies were evaluated through runtime experiments. These methods were executed on an Apple M1 Pro CPU with 16 GB of unified RAM. Figure 3 shows the plot of runtime vs. input size. The results clearly indicate that all five persistent homology-based distances and kernels (BD, WD, SWK, HK, and PI) are limited to handling dense graphs with only a few hundred nodes, highlighting the current scaling limitations of persistent homology embedding methods and their heavy reliance on approximation solutions. In contrast, Top can compute the exact Wasserstein distance between graphs with thousands of nodes and millions of edges in about one second. This computational efficiency makes Top practical for large-scale analyses of neural networks, which cannot be effectively analyzed using methods based on conventional persistence diagrams."}, {"title": "Potential Impact", "content": "Our approach for characterizing functional connectomes in neural networks is effective, computable, and scalable, potentially impacting the analysis of large neural architectures. By integrating neural network interpretability with human brain function insights, our framework opens up opportunities to leverage established techniques from functional MRI analysis. From a statistical learning perspective, our persistence statistics provide a robust basis for hypothesis testing and permutation tests, enhancing statistical rigor. Their linear-logarithmic efficiency supports large-scale neural network applications. Additionally, the gradient computability of the Wasserstein distance aids in designing advanced machine learning algorithms through gradient descent optimization.\nOur studies on convolutional neural networks for the CIFAR-10 dataset demonstrate the effectiveness of focusing on subnetworks within the last few fully-connected layers, enabling topological analysis of more targeted functional"}, {"title": "Limitation", "content": "Persistent graph homology is limited to the topological invariants of connected components and cycles. These two features, however, play a dominant role in topological analyses. For example, they are widely utilized in the brain network community (Bullmore and Sporns 2009; Honey et al. 2007), and cycles, in particular, have been increasingly reported as the most discriminative topological feature in brain networks (Sizemore et al. 2018), galaxy organization (Biagetti, Cole, and Shiu 2021), and protein structure (Xia and Wei 2014). In contrast, the assessment of higher-order features beyond cycles offers limited practical value due to their relative rarity, interpretive challenges, and consequent minimal discriminative power (Biagetti, Cole, and Shiu 2021; Sizemore et al. 2018; Songdechakraiwut and Chung 2020)."}]}