{"title": "ADAPTIVE SPARSE ALLOCATION WITH MUTUAL CHOICE & FEATURE CHOICE SPARSE AUTOENCODERS", "authors": ["Kola Ayonrinde"], "abstract": "Sparse autoencoders (SAEs) are a promising approach to extracting features from neural networks, enabling model interpretability as well as causal interventions on model internals. SAEs generate sparse feature representations using a sparsifying activation function that implicitly defines a set of token-feature matches. We frame the token-feature matching as a resource allocation problem constrained by a total sparsity upper bound. For example, TopK SAEs solve this allocation problem with the additional constraint that each token matches with at most k features. In TopK SAEs, the k active features per token constraint is the same across tokens, despite some tokens being more difficult to reconstruct than others. To address this limitation, we propose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs, which each allow for a variable number of active features per token. Feature Choice SAEs solve the sparsity allocation problem under the additional constraint that each feature matches with at most m tokens. Mutual Choice SAEs solve the unrestricted allocation problem where the total sparsity budget can be allocated freely between tokens and features. Additionally, we introduce a new auxiliary loss function, aux_zipf_loss, which generalises the aux_k_loss to mitigate dead and underutilised features. Our methods result in SAEs with fewer dead features and improved reconstruction loss at equivalent sparsity levels as a result of the inherent adaptive computation. More accurate and scalable feature extraction methods provide a path towards better understanding and more precise control of foundation models.", "sections": [{"title": "1 INTRODUCTION", "content": "Understanding the internal mechanisms of neural networks is a core challenge in Mechanistic Interpretability. Increased mechanistic understanding of foundation models could provide model developers with tools to identify and debug undesirable model behaviour.\nDictionary learning with sparse autoencoders (SAEs) has recently emerged as a promising approach for extracting sparse, meaningful, and interpretable features from neural networks, particularly language models (Cunningham et al., 2023; Sharkey et al., 2022).\nOne problem with wide SAEs for foundation models is that there are often many dead features (Rajamanoharan et al., 2024a; Templeton et al., 2024; Gao et al., 2024). Dead features are features which are remain inactive across inputs, effectively wasting model capacity and hampering efficient training. Another problem is that approaches like TopK SAEs (Gao et al., 2024) don't have a natural way to take advantage of Adaptive Computation: spending more computation, and crucially more features, to reconstruct more difficult tokens.\nWe frame the problem of generating sparse feature activations corresponding to some given neural activations as a resource allocation problem, allocating the scarce total sparsity budget between token-feature matches to maximise the reconstruction accuracy. Within this framing, we naturally motivate two novel SAE variants which can provide Adaptive Computation: Feature Choice SAES and Mutual Choice SAEs (FC and MC SAEs respectively). Feature Choice SAEs solve the sparsity allocation problem under the additional constraint that each feature matches with at most m tokens. Mutual Choice SAEs solve the unrestricted allocation problem where the total sparsity budget can be"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 ADAPTIVE COMPUTATION", "content": "In Adaptive Computation, neural networks decide how much compute (and/or which parameters) to apply to a given input example (Graves, 2017; Xue et al., 2023). Ideally, the model should learn to apply less compute to easier examples and more compute to more difficult examples in order to maximise performance within a compute budget. In our setting, we consider the token-feature matches to be the scarce quantity to allocate, where we say that a token matches with a feature if the feature is activated on that token."}, {"title": "2.2 SPARSE AUTOENCODERS", "content": "Sparse Autoencoders (SAEs) (Lee et al., 2007; Le et al., 2012; Mairal et al., 2014) learn an over- complete basis, or dictionary, of sparsely activating features. The feature activations, z, correspond to their associated neural activations, x, via the feature dictionary. In particular, we can write an SAE as:\n\\begin{equation}\nz = \\sigma_{\\Theta}(Enc(x)) \\in \\mathbb{R}^F\n\\end{equation}\n\\begin{equation}\n\\hat{x} = Dec(z) \\in \\mathbb{R}^N\n\\end{equation}\nwhere $\\sigma_{\\Theta}$ is a sparsifying activation function (e.g. ReLU), Dec is an affine map and $x \\in \\mathbb{R}^{N1}$.\nSAEs are trained to minimize the Reconstruction Error (Mean Squared Error) between x and $\\hat{x}$. This reconstruction error term is combined with an optional Sparsity Loss term (for example, an L1 penalty to induce sparsity) and an optional Auxiliary Loss term to reduce dead features:\n\\begin{equation}\n\\mathcal{L}(x) = |x - \\hat{x}|_2 + \\lambda_1 L_{sparsity}(z) + \\lambda_2 L_{aux}(x, z, \\hat{x})\n\\end{equation}\nRajamanoharan et al. (2024a); Templeton et al. (2024); Gao et al. (2024) have shown that decomposing neural activations using the SAE feature dictionary allows for increased human interpretability of models even at model sizes comparable to frontier foundation models."}, {"title": "2.3 TOPK SAES", "content": "TopK SAEs (Gao et al., 2024) use a TopK activation function instead of the L\u2081 penalty to induce sparsity, as in Makhzani & Frey (2014). Though in the standard L\u2081 SAE formulation, the number of features-per-token is variable, in the TopK formulation the features-per-token is fixed at the same k for all tokens. We hypothesize that having a fixed k is a key drawback of the TopK method. Variable k values introduce Adaptive Computation which can focus more of the token-feature matching budget on more difficult tokens.\nIn concurrent work, Bussmann et al. (2024) introduce BatchTopK which is closely analogous to our Mutual Choice SAEs and also provides adaptive computation. However, they do not deal with the problem of underutilised features."}, {"title": "2.4 DEAD FEATURES", "content": "SAE features which remain inactive across many inputs are known as dead features. Bricken et al. (2023) declare a feature to be dead when it hasn't fired for at least 1e7 tokens. Dead features present a challenge especially when scaling to larger models and wider autoencoders. For example, Templeton et al. (2024) find 64.7% of features are dead for their autoencoders with 34M features. Our Feature Choice approach naturally results in zero dead features by ensuring that each feature activates for every batch."}, {"title": "2.5 AUXILIARY K LOSS FUNCTION", "content": "Gao et al. (2024) propose the auxiliary loss function, aux_k_loss to reduce the proportion of dead features. Given the SAE residual $e = x - \\hat{x}$, they define the auxiliary loss $L_{aux} = |e - \\hat{e}|^2$, where $\\hat{e} =$"}, {"title": "3 BACKGROUND", "content": ""}, {"title": "3.1 SPARSIFYING ACTIVATION FUNCTIONS AS RESOURCE ALLOCATORS", "content": "We consider the following many-to-many matching problem:\n\u2022 We have F features and a batch of B tokens. We would like to have at most M token-feature matches, where we say that a token matches with a feature if the feature is activated on that token 2\n\u2022 We would like to allocate our budget of M token-feature matches such that the reconstruction error is minimised.\nFormally, we seek a reconstruction-error optimal weighted subgraph $H \\subseteq G = \\{\\{1,..., B\\} x \\{1, ..., F\\}, E\\}$ where H has at most M edges, for $M \\ll BF$. The edge weights can be viewed as the token-feature affinities: the pre-sparsifying activation feature magnitudes z'. 3\nThis problem doesn't immediately admit an efficient solution because it is currently unspecified how the edge weights contribute to the token reconstruction error. We make a simplifying assumption that we denote the Monotonic Importance Heuristic - the edges with the largest edge weights are likely to represent the most important contributions to the reconstruction error. With this heuristic, we can solve the problem of allocating token-feature matches by choosing the M edges with the largest magnitude edge weights as the edges for our subgraph H.\nWe can equivalently view this allocation problem as choosing a binary mask $S \\in \\mathbb{R}^{B \\times F}$ with at most M non-zero elements which maximises reconstruction accuracy. This mask is to be element-wise multiplied with a token-feature affinity matrix $Z' \\in \\mathbb{R}^{B \\times F}$. Applying the Monotonic Importance Heuristic, we are looking for the mask S such that $\\sum_{i,j} Z_{i,j} = \\sum_{i,j} Z'_{i,j} \\odot S_{i,j}$ is maximised.\nTopK SAES: We can now see the TopK SAE approach as a special case of the above allocation problem, with the additional constraint that the number of features per token is at most k for each token. In other words, $\\Sigma_i(S_{i,j}) = k\\forall j$, where $M = kB$. This leads to the solution of $S = TopKIndices(Z', dim = -1)$, i.e. S picks out the k features with the highest affinity for each token. Here $\\sigma_{\\Theta}(z') = S \\odot z'$; element-wise multiplication with S defines our sparsifying activation function $\\sigma_{\\Theta}$."}, {"title": "3.1.1 ANALOGY TO MIXTURE OF EXPERTS ROUTING", "content": "We choose our naming here to align with the study of token-expert matching in the Mixture of Experts paradigm, where there is a close analogy. Token Choice MoE routing strategies (Shazeer et al., 2017; Fedus et al., 2022) have the constraint that each token can be routed to at most k experts allowing for expert imbalances. On the other hand, Expert Choice routing strategies (Zhou et al., 2022) have the constraint that each expert processes at most m tokens, which eliminates the possibility of underutilised experts and allows tokens to be routed to a variable number of experts.\nHere the intuitions are \"each token picks the k experts to be routed to\" and \"each expert picks the m tokens to be routed to that expert\" for Token Choice and Expert Choice respectively. The variety of MoE routing algorithms is explored in Liu et al. (2024). The Feature Choice approach we propose is directly analogous to the Expert Choice approach in MoEs and TopK SAEs are directly analogous to Token Choice MoEs. For this reason, we will call TopK SAEs, Token Choice SAEs to unify our notation."}, {"title": "3.2 DISTRIBUTION OF FEATURE DENSITIES", "content": "When analysing the distribution of feature densities in open-source SAEs from Gao et al. (2024), we find that the distributions typically follow a power law described by the Zipf distribution with $R^2 = 0.982$, see fig. 3.\nWe note that although the Zipf distribution well fits most of the distribution, there is a considerable residual at the lower end of the distribution (the 20k+ rank tokens). We suggest that these features are underutilised. Underutilised features see fewer gradient updates than other features leading to a self-reinforcing cycle of these features being less useful and hence further underutilised until they die.\nWe will refer to these underutilised features as dying features. We define a dying feature as a feature which is one of the 25% least prevalent features which is also <60% as prevalent as we might predict from the prevalence rank of the feature using the fitted Zipf curve.\nPrevious approaches to dealing with dead features either resampled dead features (Bricken et al., 2023) or applied gradients to dead features (for example aux_k_loss (Gao et al., 2024)) but they didn't address dying features. We hypothesise that many of the revived dead features were still not appropriately utilised.\nTo address the problem of dying features, we add an additional auxiliary loss for dying features, which is a natural generalisation of the aux_k\u2081oss. Given the SAE residual $e = x - \\hat{x}$, we define the auxiliary loss $L_{aux_zipf} = |e - \\hat{e}|^2$, where $\\hat{e} = Dec(z_{dying})$ is the reconstruction using the top $k_{aux}$ dying features. We can think of this aux_zipf_loss acting preventatively on features which could be at risk of becoming dead and acting rehabilitatively on features which have been recently revived. In this way, we reduce the proportion of both dead and dying features."}, {"title": "3.3 CHOOSING THE FEATURE CHOICE CONSTRAINT", "content": "In the Feature Choice approach, there remains the question of how to distribute the sparse feature activations across the feature dimension.\nThe simplest approach to this is to take $m_i = M/F$ for all i, where F is the number of features. In other words, each feature can pick exactly m tokens to process. We call this approach Uniform Feature Choice.\nUniform prevalence is a natural way to organize the features so that a feature firing provides maximal information about the token, under the assumption that all features provide approximately equal information. However, we have seen that in existing open-source SAEs, all features are not equiva- lently prevalent. Instead, they are approximately Zipf-law distributed. To maintain this distribution of feature density we choose $m_i \\sim Zipf(\\alpha, \\beta)$, where Zipf represents a truncated Zipf distribution and i is the rank of a given feature in terms of feature density.\n\\begin{equation}\nm_i = Zipf(i) \\times \\frac{1}{(i + \\beta)^\\alpha}\n\\end{equation}\nWe call the Feature Choice approach where the $m_i$ are Zipf-distributed, Zipf Feature Choice, hence- forth simply Feature Choice."}, {"title": "4 \u041c\u0415\u0422\u041dODS", "content": "Our approach is as follows:\n\u2022 Given the Zipf exponent and bias hyperparameters, $\\alpha$ and $\\beta$, 4, we use algorithm 1 to determine the estimated feature density for each ranked feature. We use the estimated feature densities to define the threshold for dying features for the aux_zipf_loss.\n\u2022 We then train Mutual Choice SAEs with both the aux_zipf_loss and aux_k_loss.\n\u2022 Finally, we optionally fine-tune these SAEs with the Feature Choice activation function, adding the constraint on the number of tokens that each feature should process. Here there are no auxiliary loss terms."}, {"title": "5 EXPERIMENTAL SETUP", "content": "Inputs: We train our sparse autoencoders on the layer 6 residual stream activations of GPT-2 small (Radford et al., 2019). We use a context length of 64 tokens for all experiments. We preprocess the activations by subtracting the mean over the dmodel dimension and normalize all inputs to unit L2 norm. We shuffle the activations for training our SAEs (as in Nanda (2023)). All experiments are performed without feature resampling, unless otherwise specified.\nHyperparameters: We tune learning rates based on Gao et al. (2024) suggestion that the learning rate scales like \u221an. We use the AdamW optimizer (Kingma & Ba, 2017) and a batch size of 1,536 5. We train each SAE for 10,000 steps or until convergence. We use a weight decay of le-5 and apply gradient clipping. We analyse SAE widths from 4x to 32x the width of the neural activations with 0.8% feature sparsity. We use gradient accumulation for larger batch sizes. We do not perform extensive hyperparameter sweeps.\nEvaluation: After training, we evaluate autoencoders on sparsity Lo, reconstruction (MSE) and the difference on the model's final (Cross-Entropy) loss. We report a standard normalized version of the loss recovered (%). We additionally evaluate our SAEs' interpretability using Juang et al. (2024)'s automated interpretability (AutoInterp) process. We report the percentage of dead features across models."}, {"title": "6 RESULTS", "content": "We find that Feature Choice SAEs are a Pareto improvement upon the Token Choice TopK SAEs, as in fig. 4.\nWe find that both Mutual Choice and Feature Choice SAEs provide better utilisation of the SAE capacity with fewer dead features than comparable SAE methods fig. 5.\nTempleton et al. (2024)'s 34 million latent SAEs have a dead feature rate of 64.7% (with resampling); Gao et al. (2024)'s 16 million latent SAEs have a dead feature rate of 90% without mitigations and"}, {"title": "7 DISCUSSION", "content": "We summarize the comparison between our approach and related approaches in table 2:\nTo our knowledge, we present the first SAE training process which explicitly contains two separate phases: first Mutual Choice training, then Feature Choice training. We speculate that there may be performance benefits to phased training.\nWe believe that conceiving of the role of SAE encoders as defining matching/routing algorithms (similar to other Adaptive Computation work, for example, within the Mixture of Experts literature) could be a valuable intuition pump for further improvements to SAE architectures.\nOur approaches can also be combined with the MDL-SAE (Ayonrinde et al., 2024) formulation which treats conciseness (Description Length) as the relevant quantity for evaluation and model selection rather than sparsity (Lo). The Description Length of a set of feature activations is a function of the sparsity, the distribution of activation patterns for each feature and the SAE width."}, {"title": "7.1 ZIPF DISTRIBUTED FEATURES", "content": "We find that constraining the number of tokens per feature with the Zipf distribution outperforms using the uniform distribution by >10% model loss recovered. The large drop in performance using the uniform distribution compared to the Zipf distribution gives additional evidence that naturally occurring features are not uniformly distributed.\nWe hypothesise that the reason that features appear to be Zipf distributed may be strongly analogous to the reason that the frequency of words in natural languages like English are also Zipf distributed. Words are the semantic units of sentences. Since features are the semantic units of computation within language models, a similar mechanism could explain the empirical tendency for the densities of SAE features to tend towards the Zipf distribution.\nIn the Computational Linguistics literature, it is well known that the distribution of words in natural languages approximately follows a Zipf distribution (Zipf, 1949). For example, in written English text, the empirical distribution over words (treated as a categorical variable) can be modelled as Zipf($\\alpha$, $\\beta$) = Zipf(1, 2.7).\nWe speculate that the Preferential Attachment Theory explanation (Zhu et al., 2018; Chen, 2012) for the tendency of words to be Zipf-distributed, which states that frequently used words (features) tend to be used more often, may be analogously applicable here. At initialisation, there is some variance in feature prevalence. The tokens which are initially most highly activated early in training receive the most gradient signal and are most refined, leading to a virtuous cycle where they are more effective and useful for a larger part of the feature density spectrum. fig. 6 illustrates this dynamic over time. We would be excited about further work detailing an explicit mechanism for why features in Neural Networks tend towards being Zipf distributed.\nWe also note that, for humans, some concepts appear more prevalent in the world than others. Hence, if features do map to human-interpretable concepts, we might also expect variance in the prevalence of SAE features."}, {"title": "7.2 ADAPTIVE COMPUTATION FOR VARYING TOKEN DIFFICULTY", "content": "One benefit of the Mutual Choice and Feature Choice approaches is that they allow for Adaptive Computation - difficult to reconstruct tokens (which may represent complex or rare concepts) can be reconstructed using more features, whereas other tokens which are more straightforward can use fewer features. Where Token Choice (TopK) SAEs suggest that all tokens are equal; MC and FC SAEs suggest that, in fact, some tokens are more equal than others Singer (1989).\nAs an extreme example, we might expect that the activations resulting from the < BOS > token are relatively easy to reconstruct, considering they are both very common and have exactly the same value every time. We might expect that an effective SAE could learn to productively reallocate the sparsity budget that would have been spent on the < BOS > token to more difficult tokens, thus increasing the SAE's effective capacity. We provide an example of the features per token distribution in appendix D."}, {"title": "7.3 DEAD FEATURES", "content": "Our methods, especially Feature Choice SAEs, have many fewer dead features than other approaches (typically zero) without complex and compute-intensive resampling procedures (Bricken et al., 2023). This is especially important for large SAEs where the problem of dead features is typically more significant. We hypothesize that this might be an additional reason for the improved performance: all features receive some gradient signal at every step.\nWe note that this simplified approach eliminates the need for complex resampling procedures. This approach suggests a simplified procedure for SAE training and model selection than previous SAES with fixed sparsity budget."}, {"title": "7.4 PHILOSOPHICAL NOTES", "content": "In 7.1, we discussed the fact that features which map to human-understandable concepts and are learned from human-generated data (like internet text) are likely approximately Zipf distributed. We"}, {"title": "7.5 LIMITATIONS", "content": "Appeals to the Monotonic Importance Heuristic: In section 3.1 we defined the Monotonic Importance Heuristic (MIH) - the assumption that the importance of a feature is monotonically increasing in feature activation magnitude. We use this assumption when we choose the feature activations with the largest magnitudes with our TopK-style activation functions. TopK SAEs also implicitly assume the MIH (Monotonic Importance Heuristic). We can think of Jump-ReLU SAES and Gated SAEs as relaxing this assumption slightly to a weak MIH. For Jump-ReLU and Gated SAEs, the importance of activations is still related to their magnitude and they still filter out any low magnitude feature activations; however, the filtering threshold varies for each feature. So Jump-ReLU SAEs do not have to make magnitude comparisons across features which may have different natural scales. It may be, however, that there are low magnitude activations which, within a certain context, are nonetheless critically important in capturing information which is useful for reconstruction and/or downstream model performance. These important but low magnitude activations are difficult to capture with our current SAE approaches 7.\nThough the weight-sharing form of Gated SAEs (Rajamanoharan et al., 2024a) implicitly encodes the weak MIH prior, the non-sharing form does not. Weight-sharing Gated SAEs, however, tend to perform better. The improved performance of approaches which encode the MIH prior could be considered as evidence for the truth of the claim. Alternatively, we might note that the Monotonic Importance Heuristic acts as an inductive bias for our models. Good inductive biases often allow models to perform better at first; however, with increased scale we may not need such inductive biases and may prefer allowing the model to learn more of the solution independently (Xiao, 2024), (Sutton, 2019).\nGeneralisation Across Modalities: We tested our SAEs within the domain of language. We currently don't know to what extent our results generalise across modalities, especially to inherently continuous modalities like audio and images. We would be excited about future work applying similar techniques to Interpretability problems in a wider range of modalities.\nEvaluation: The Mechanistic Interpretability field doesn't currently have widely agreed upon metrics for evaluating Sparse Autoencoders. Disentanglement benchmarks like Huang et al. (2024) have been proposed as well as evaluation on tasks where the ground truth is known (Karvonen et al., 2024). Developing a more comprehensive suite of benchmarks for SAEs would help us to have higher confidence in our comparisons between SAE variants."}, {"title": "8 CONCLUSION", "content": "We introduce the Feature Choice and Mutual Choice SAEs as a simple drop-in change to the sparsifying activation function in Sparse Autoencoders. We also provide a new auxiliary loss, aux_zipf_loss, which prevents dying features and hence allows the SAE to more fully utilize all of its features without wasting capacity.\nWe would be excited about future work that defines scaling laws for Features Choice SAE performance on even larger models than the ones we use in this work. We believe a promising direction for future work is dissecting the phenomenology of features (Bricken et al., 2023) conditioning on both feature prevalence and feature activation magnitude. We might ask the question \"what kinds of features appear very often in naturally occurring datasets and what does this imply about the predictive elements of the natural world?\". We believe this direction could prove fruitful for both Machine Learning researchers and philosophers of science."}, {"title": "G\\u2014MONOTONIC IMPORTANCE HEURISTIC", "content": "In section 3 we appeal to the Monotonic Importance Heuristic (MIH), as in TopK SAEs, in order to simplify our allocation problem. Empirically we find that this works well though we discuss the case for not using the MIH in section 7.5.\nOne theoretical (though informal) motivation for the MIH is as follows. Since the decoder dictionary is fixed to unit norm, the norm of any feature's contribution to the output is exactly the magnitude of the feature activation to which it corresponds. Hence if there's limited cancellation between features (which is likely in an N dimensional space where the per-token sparsity is much less than N) then we might expect the component of \\hat{x} in any feature direction to be very close to the feature activation for that feature. In particular, consider a feature with a small magnitude of \\epsilon. This feature can only possibly influence the reconstruction loss by at most \\epsilon e where e = |x - \\hat{x}|_2. Features corresponding to larger activations can plausibly influence the reconstruction loss by a greater amount."}, {"title": "A FEATURE DENSITY MOVES TOWARDS A ZIPF DISTRIBUTION THROUGHOUT TRAINING", "content": "We can compare this with fig. 3 where the drop-off comes much later, at 24k vs at 3k."}, {"title": "B INFERENCE WITH FEATURE CHOICE AND MUTUAL CHOICE SAES", "content": "To run inference on our SAE variants, you can do batch inference with the method exactly as in the training setup. However to do single token (or single sequence) it may be beneficial to instead impute a threshold value and swap out the activation function to use this value instead with a JumpReLU style approach Erichson et al. (2019)."}, {"title": "C NEURAL FEATURE MATRIX Loss", "content": "One problem with SAEs is undesirable feature splitting. Feature splitting occurs when an SAE finds a sparse combination of existing directions that allows for a smaller Lo (Ayonrinde et al., 2024). For example, Bricken et al. (2023) note that a model may learn dozens of features which all represent the letter \"P\" in different contexts in order to maintain low sparsity.\nIn order to reduce feature splitting we propose two additional auxiliary losses: nfm_loss and nfm_inf_loss.\nThe Neural Feature Matrix (NFM) is defined as nfm(W) = \\hat{W}\\hat{W}^T for a weight matrix W. The NFM is a symmetric square matrix which describes the correlation of different rows in the matrix W.\nWe define nfm_loss = |nfm(W_{Dec})|_F and nfm_inf_loss = \\sum_i \\max(nfm(W_{Dec})i).\nFor our largest runs we apply both of these auxiliary losses with small weight. Empirically we find this seems to reduce undesirable feature splitting and avoid a failure mode we call \"dictionary collapse\" when many features of the decoder dictionary start to align with each other."}, {"title": "DADAPTIVE COMPUTATION ALLOWS FOR A VARIABLE NUMBER OF FEATURES PER TOKEN", "content": "We note that there is a bimodal distribution of features per token. The SAE allocates close to the main mode of features to each token, attempting to allocate more features to harder tokens and less features"}, {"title": "E PROGRESSIVE CODES", "content": "Gao et al. (2024) describe learning a progressive code using their Multi TopK loss. In their setting, the Multi TopK loss (a weighted sum of TopK losses for different values of k) is required because the SAE generally \"overfits\" to the value of k which harms a progressive code. In our case, the SAE is robust to having variable k values even for the same token depending on the context of the batch. Empirically, we obtain progressive codes for a greater range of values of k than in the TopK case."}, {"title": "F DETERMINING THE FEATURE DISTRIBUTION", "content": "Given the Zipf exponent and bias hyperparameters, \u03b1 and \u03b2, we use Algorithm 1 algorithm 1 to determine the estimated feature density for each ranked feature. We use the estimated feature densities to define the threshold for dying features for the aux_zipf_loss."}]}