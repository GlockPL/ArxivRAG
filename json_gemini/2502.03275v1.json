{"title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning", "authors": ["DiJia Su", "Hanlin Zhu", "Yingchen Xu", "Jiantao Jiao", "Yuandong Tian", "Qinqing Zheng"], "abstract": "Large Language Models (LLMs) excel at reasoning and planning when trained on chain-of-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks, such as Math (+4.2%, Llama-3.2-1B), GSM8K (+4.1%, Llama-3.2-3B), and Fresh-Gaokao-Math-2023 (+13.3%, Llama-3.1-8B) with an average reduction of 17% in reasoning trace's length.", "sections": [{"title": "Introduction", "content": "Reasoning capabilities are increasingly recognized as a critical component of Artificial General Intelligence (AGI) systems. Recent research has demonstrated that Large Language Models (LLMs) can exhibit sophisticated reasoning and planning abilities using chain-of-thought (CoT) methodologies, including prompting LLMs with examples where complex problems are broken down into explicit reasoning steps (Wei et al., 2022b; Chen et al., 2022a; Yao et al., 2024). More recently, a number of studies have further shown that when models are trained to articulate the intermediate steps of a reasoning process (Nye et al., 2021b; Lehnert et al., 2024), they achieve significantly higher accuracy. The effectiveness of this approach has been demonstrated across multiple domains, including mathematical problem-solving (Yue et al., 2023; Gandhi et al., 2024; Yu et al., 2023; Tong et al., 2024), logical inference (Lin et al., 2024; Dziri et al., 2024), multistep planning tasks (Lehnert et al., 2024; Su et al., 2024), etc.\nHowever, training with explicit reasoning traces in text space comes with notable computational costs (Deng et al., 2023; 2024), as the models must process lengthy input sequences. In fact, much of the text serves primarily to maintain linguistic coherence, rather than conveying core reasoning information. Several works have attempted to mitigate this issue. For example, Hao et al. (2024) investigate reasoning in continuous latent space as a means of compressing the reasoning trace, and Deng et al. (2024) explore internalizing the intermediate steps through iterative CoT eliminations, see Section 2 for more examples. Nonetheless, these approaches rely on multi-stage training procedures that resemble curriculum learning, which still incur significant computational costs, and their final performances fall behind models trained with complete reasoning traces.\nTo tackle this challenge, we propose to use discrete latent tokens to abstract the initial steps of the reasoning traces. These latent tokens, obtained through a vector-quantized variational autoencoder (VQ-VAE), provide a compressed representation of the reasoning process by condensing surface-level details. More precisely, we replace the text tokens with their corresponding latent abstractions from left to right until a pre-set location, leaving the remaining tokens unchanged. We then fine-tune LLMs with reasoning traces with such assorted tokens, allowing the models to learn from both abstract representations of the thinking process and detailed textual descriptions. One technical challenge posed for the fine-tuning is that the vocabulary is now extended and contains unseen latent tokens. To fa-"}, {"title": "Related Work", "content": "Explicit Chain-of-Thought Prompting. The first line of work in Chain-of-Thought (CoT) use the traditional chain of prompt in text tokens (Wei et al., 2022a; Nye et al., 2021a). Research works demonstrated that by adding few-shot examples to the input prompt or even zero-shot, the model can perform better in question answering (Chen et al., 2022b; Kojima et al., 2022; Chung et al., 2024). To further improve the model reasoning performance, there has been research effort into prompting with self-consistency (Wang et al., 2022). Here the model is prompted to generate multiple responses and select the best one based on majority voting. On the other hand, research has shown that top-k alternative tokens in the beginning of the prompt can also improve the model's reasoning capability (Wang & Zhou, 2024). On top of these empirical results, there has been research on theoretical understanding of why CoT improves the model's performance through the lens of expressivity (Feng et al., 2024; Li et al., 2024) or training dynamics (Zhu et al., 2024). In a nutshell, CoT improves the model's effective depth because the generated output is being fed back to the original input. CoT is also important for LLMs to perform multi-hop reasoning according to the analysis of training dynamics (Zhu et al., 2024).\nLearning with CoT Data. In addition to the success of CoT prompting, an emerging line of works have explored training LLMs on data with high-quality reasoning traces, for example, the works of Nye et al. (2021b); Azerbayev et al. (2023); Lehnert et al. (2024); Su et al. (2024); Yu et al. (2024); Yang et al. (2024); Deng et al. (2023; 2024). There is also a surge of interest in synthesizing datasets with diverse intermediate steps for solving problems in various domains, see, e.g., the works of Kim et al. (2023); Tong et al. (2024); Yu et al. (2023); Yue et al. (2023); Lozhkov et al. (2024). Wen et al. (2024) also theoretically studies how training with reasoning trace can improve the sample complexity of certain tasks.\nLLM Reasoning in Latent Space. There has been research investigating LLM reasoning in the latent space. Hao et al. (2024) have proposed to use the last hidden state of a language model as the next input embeddings, allowing the model to continue reasoning within a continuous latent space. The authors show that this approach effectively captures multiple reasoning paths simultaneously, mimicking a breadth-first-search strategy. Goyal et al. (2023) proposes to insert learnable pause tokens into the original text, in order to delay the generation. As a result, the model can leverage additional computation before providing the final answer. Parallel to this, Pfau et al. (2024) have explored filler tokens, which are used to solve computational tasks that are otherwise unattainable without intermediate token generation. In addition, Liu et al. (2024) propose a latent coprocessor method that operates on the transformer's key-value cache to improve the LLM performance. Nevertheless, none of these methods have shown good performance when integrated into modern-sized LLMs and tested on real-world LLM datasets instead of synthetic ones. Orthogonal to these works, Pagnoni et al. (2024) proposes a tokenization-free architecture that encodes input bytes into continuous patch representations, which is then used to train a latent Transformer, and Barrault et al. (2024) perform autoregressive sentence prediction in an embedding space. While these two works both leverage continuous latent spaces, our work focuses on the direct use of discrete latent tokens."}, {"title": "Methodology", "content": "In this section, we describe our methodology to enable LLMs to reason with discrete latent tokens. The notations are summarized in Appendix B. Let $X = P \\oplus C \\oplus S$ denote a sample input, where $P = (P_1, P_2, ..., P_{t_1})$ are the prompt tokens, $C = (C_1, C_2, ..., C_{t_c})$ are the reasoning step (chain-of-thought) tokens, $S = (s_1, s_2,...,s_{t_s})$ are the solution tokens, and $\\oplus$ denotes concatenation. Our training procedure consists of two stages:\n1. Learning latent discrete tokens to abstract the reasoning steps, where we train a model to convert C into a sequence of latent tokens $Z = (Z_1, Z_2,..., Z_{t_z})$ such that $t_z <t_c$. The compression rate $r = t_c/t_z$ controls the level of abstraction."}, {"title": "Learning Latent Abstractions", "content": "We employ a vector-quantized variable autoencoder (VQ-VAE) (Van Den Oord et al., 2017) type of architecture to map CoT tokens C into discrete latent tokens Z. To enhance abstraction performance, our VQ-VAE is trained on the whole input sequence X, but only applied to C in the next stage. Following Jiang et al. (2022; 2023), we split X into chunks of length L and encode each chunk into latent codes, where r is a preset compression rate. More precisely, our architecture consists of the following five components:\n\u2022 $E$ : a codebook containing $|E|$ vectors in $R^d$.\n\u2022 $f_{enc}$ : $V^L \\rightarrow R^{d\\times L}$ that encodes a sequence of L text tokens to L latent embedding vectors $X = X_1,..., X_L$, where V is the vocabulary of text tokens.\n\u2022 q: $R^d \\rightarrow E$ : the quantization operator that replaces the encoded embedding $X_i$ by the nearest neighbor in E: $q(x) = \\underset{e_i \\in E}{\\operatorname{argmin}} ||e_i - x||_2$.\n\u2022 g: $V^K \\rightarrow R^d$ that maps K text tokens to a d-dimensional embedding vector. We use g to generate a continuous embedding of the prompt P.\n\u2022 $f_{dec}$ : $R^{d\\times L} \\times V \\times R^k \\rightarrow V^L$ that decodes latent embeddings back to text tokens, conditioned on prompt embedding.\nIn particular, each continuous vector $e \\in E$ in the codebook has an associated latent token z, which we use to construct the latent reasoning steps Z\u00b9.\nFor simplicity, we assume the lengths of the input X and the prompt P are L and K exactly. Similar to Van Den Oord et al. (2017), we use an objective L composed of 3 terms:\n$L(X) = \\log p(X|f_{dec}(q(X)|g(P))) + \\\\ \\sum_{i=1}^{L} ||sg[X_i] - q(X_i)||^2 + \\beta||X_i - sg[q(X_i)]||^2$,\nwhere $X = f_{enc}(X), sg[\u00b7]$ is the stop-gradient operator, and $\\beta$ is a hyperparameter controlling the strength of the commitment loss. The VQ loss and the commitment loss ensure that the encoder outputs remain close to the codebook, while the reconstruction loss concerns with the decoding efficacy. As standard for VQ-VAE, we pass the gradient $\\nabla_{f_{dec}}(L)$ unaltered to $f_{enc}$ directly as the quantization operator q(\u00b7) is non-differentiable. In practice, we use a causal Transformer for both $f_{enc}$ and $f_{dec}$, the model details are discussed in Appendix A.\nThus far we obtain a latent representation both semantically meaningful and conducive to reconstruction, setting the stage for the subsequent training phase where the LLM is trained to perform reasoning with abstractions."}, {"title": "Reasoning with Discrete Latent Tokens", "content": "In this second stage, we apply the obtained VQ-VAE to form modifed samples X with latent abstractions as in Equation (1), then train an LLM to perform next token prediction. Below, we outline the major design choices that are key to our model's performance, and ablate them in Section 4.\nPartial Replacement. Unlike previous planning works (Jiang et al., 2022; 2023) that project the whole input sequence onto a compact latent space, we only replace $m < t_c$ CoT tokens with their latent abstractions, leaving the remaining tokens unchanged. We delimit the latent tokens by injecting a special <boLatent> and <eoLatent> tokens to encapsulate them.\nLeft-to-Right (AR) Replacement. We replace the leftmost m tokens of C, rather than subsampling tokens at different locations.\nMixing Samples with Varying Values of m. For fine-tuning an existing LLM on the reasoning dataset with latent tokens, one remarkable challenge is to deal with the extended vocabulary. As the LLM is pretrained with trillions of tokens, it is very hard for it to quickly adapt to tokens (and corresponding embeddings) beyond the original vocabulary. Previous works that aim to replace or eliminate CoT tokens (Deng et al., 2024; Hao et al., 2024) employ a multistage curriculum training approach, where those operations are gradually applied to the entire input sequence. In the context of our approach, this means we increase the values of m in each stage until it reaches a pre-set cap value. However, such training procedure is complex and computationally inefficient, where dedicated optimization tuning is needed. In this work, we employ a simple single stage training approach where the value of m is randomly set for each sample. Surprisingly, this not only makes our training more efficient, but also leads to enhanced performance."}, {"title": "Experiments", "content": "We empirically evaluate our approach on two categories of benchmarks:\n(1) Synthetic datasets including the Keys-Finding Maze, ProntoQA (Saparov & He, 2022), and ProsQA (Hao et al., 2024), where we pretrain T5 or GPT-2 models from scratch using the method in Section 3;\n(2) Real-world mathematic reasoning problems, where we fine-tune Llama models (Dubey et al., 2024) on the MetaMathQA (Yu et al., 2023) or the Dart-MATH (Tong et al., 2024) dataset, and then test on in-domain datasets Math and GSM-8K, along with out-of-domain datasets including Fresh-Gaokao-Math-2023, DeepMind-Math, College-Math, OlympiaBench-Math, and TheoremQA.\nThe detailed setup is introduced in Section 4.1.\nWe compare our approach to the following baselines:\nSol-Only: the model is trained with samples that only contains questions and solutions, without any reasoning steps;\nCoT: the model is trained with samples with complete CoT tokens;\niCoT (Deng et al., 2024): a method that utilizes curriculum learning to gradually eliminate the need of CoT tokens in reasoning;\nPause Token (Goyal et al., 2023): a method that injects a learnable pause token into the sample during training, in order to offer extra computation before giving out the final answer."}, {"title": "Benchmarks", "content": "Keys-Finding Maze is a complex navigation environment designed to evaluate an agent's planning capabilities. The agent is randomly positioned within a maze comprising 4 3 \u00d7 3 interconnected rooms, with the objective of reaching a randomly placed goal destination. To successfully reach the destination, the agent must collect keys (designated with green, red, and blue colors) that correspond to matching colored doors. These keys are randomly distributed among the rooms, requiring the agent to develop sophisticated planning strategies for key acquisition and door traversal. The agent is only allowed to take one key at a time. This environment poses a substantial cognitive challenge, as the agent must identify which keys are necessary for reaching the destination, and optimize the order of key collection and door unlocking to establish the most efficient path to the goal. Following Lehnert et al. (2024); Su et al. (2024), we gen-"}, {"title": "MATHEMATICAL REASONING", "content": "We fine-tune pretrained LLMs using the MetaMathQA (Yu et al., 2023) or the Dart-MATH (Tong et al., 2024) dataset. MetaMathQA is a curated dataset that augments the existing Math (Hendrycks et al.) and GSM8K (Cobbe et al., 2021b) datasets by various ways of question bootstrapping, such as (i) rephrasing the question and generating the reasoning path; (ii) generating backward questions, self-verification questions, FOBAR questions (Jiang et al., 2024), etc. This dataset contains 395k samples in total, where 155k samples are bootstrapped from Math and the remaining 240k come from GSM8K. We rerun the MetaMath data pipeline by using Llama-3.1-405B-Inst to generate the response. Dart-MATH (Tong et al., 2024) also synthesizes responses for questions in Math and GSM8K, with the focus on difficult questions via difficulty-aware rejection tuning. For evalua- tion, we test the models on the original Math and GSM8K datasets, which are in-domain, and also the following out- of-domain benchmarks:\n\u2022 College-Math (Tang et al., 2024) consists of 2818 college- level math problems taken from 9 textbooks. These prob- lems cover over 7 different areas such as linear algebra, differential equations, and so on. They are designed to evaluate how well the language model can handle compli- cated mathematical reasoning problems in different field of study.\n\u2022 DeepMind-Math (Saxton et al., 2019) consists of 1000 problems based on the national school math curriculum for students up to 16 years old. It examines the basic mathematics and reasoning skills across different topics.\n\u2022 OlympiaBench-Math (He et al., 2024) is a text-only En- glish subset of Olympiad-Bench focusing on advanced level mathematical reasoning. It contains 675 highly diffi-"}, {"title": "Main Results", "content": "We employ a consistent strategy for training VQ-VAE and replacing CoT tokens with latent discrete codes across all our experiments, as outlined below. The specific model architecture and key hyperparameters used for LLM training are presented alongside the results for each category of benchmarks. All the other details are deferred to Appendix A.\nVQ-VAE Training For each benchmark, we train a VQ- VAE for 100k steps using the Adam optimizer, with learning rate 10-5 and batch size 32. We use a codebook of size 1024 and compress every chunk of L = 16 tokens into a single latent token (i.e., the compression rate r = 16).\nRandomized Latent Code Replacement We introduce a stochastic procedure for partially replacing CoT tokens with latent codes. Specifically, we define a set of predetermined numbers M = {0, 72, 128, 160, 192, 224, 256}, which are all multipliers of L = 16. For each training example, we first sample $m_{max} \\in M$ then sample an integer m\u2208 [0, 16, 32, . . . , $m_{max}$] uniformly at random. The first m CoT tokens are replaced by their corresponding latent discrete codes, while the later ones remain as raw text. This stochastic replacement mechanism exposes the model to a wide range of latent-text mixtures, enabling it to effectively learn from varying degrees of latent abstraction."}, {"title": "SYNTHETIC BENCHMARKS", "content": "Hyperparameters and Evaluation Metric For our experi- ments on the ProntoQA and ProsQA datasets, we fine-tune the pretrained GPT-2 model (Radford et al., 2019) for 16k steps, where we use a learning rate of 10\u22124 with linear warmup for 100 steps, and the batch size is set to 128. To evaluate the models, we use greedy decoding and check the exact match with the ground truth.\nFor Keys-Finding Maze, due to its specific vocabulary, we trained a T5 model (Raffel et al., 2020) from scratch for 100k steps with a learning rate of 7.5 \u00d7 10\u22124 and a batch size of 1024. We evaluate the models by the 1-Feasible-10 metric. Namely, for each evaluation task, we randomly sample 10 responses with top-k (k=10) decoding and check"}, {"title": "Ablation & Understanding Studies", "content": "Replacement Strategies Our latent approach partially replaces the leftmost m CoT tokens, where the value of m varies for each sample. We call such replacement strategies AR-Replace. Here we consider three alternative strategies:\n(1) All-Replace: all the text CoT tokens are replaced by the latent tokens.\n(2) Curriculum-Replace: the entire CoT subsequence are gradually replaced over the course of training, simi- lar to the training procedure used by iCoT and CO-\nCONUT (Hao et al., 2024). We train the model for 8 epochs. Starting from the original dataset, in each epoch we construct a new training dataset whether we further replace the leftmost 16 textual CoT tokens by a discrete latent token.\n(3) Poisson-Replace: instead of replacing tokens from left to right, we conduct a Poisson sampling process to select CoT tokens to be replaced: we split the reasoning traces into chunks consisting of 16 consecutive text tokens, where each chunk is randomly replaced by the latent token with probability 0.5."}, {"title": "Impact Statement", "content": "This paper presents a method to enhance the reasoning capability of Large Language Models (LLMs) by combining latent and text tokens in the reasoning trace. In terms of society impact, while reasoning with (opaque) latent tokens may trigger safety concerns, our approach provides a VQ- VAE decoder that can decode the latent tokens into human readable format, mitigating such concerns."}, {"title": "Details of Attention Weights Analysis", "content": "Question 1\nPrompt: What is the positive difference between $120\\%$ of 30 and $130\\%$ of 20?\nResponse of Our Latent Approach:\n<boLatent><Latent-code-278> <Latent-code-416><eoLatent> % of 30 and $130\\%$ of 20. First,\nlet's calculate $120\\%$ of 30: $120\\%$ of 30 = 1.2 x 30 = 36. Next, let's calculate $130\\%$ of 20: $130\\%$ of 20 =\n1.3 x 20 = 26. Now, let's find the positive difference between 36 and 26: 36 - 26 = 10\nThe answer is 10.\nResponse of Baseline (CoT): To find the positive difference between 120% of 30 and 130% of 20, we need to\ncalculate each value first. 120% of 30 is calculated as 1.2 * 30 = 36. 130% of 20 is calculated as 1.3 * 20 = 26. Now,\nwe need to find the positive difference between these two values. To do this, we subtract the smaller value from the\nlarger value: 36 - 26 = 10.\nThe final answer is 10.\nQuestion 2\nPrompt: Mark has $50 in his bank account. He earns $10 per day at his work. If he wants to buy a bike that costs\n$300, how many days does Mark have to save his money?\nResponse of Our Latent Approach: <boLatent> <Latent-code-123> <eoLatent> need to calculate\nhow much more money he needs to buy the bike and then divide that amount by his daily earnings. The bike costs\n$300, and Mark already has $50. So, he needs: $300 (bike cost) - $50 (current savings) = $250 (more money needed)\nMark earns $10 per day. To find out how many days he needs to save, we divide the amount he needs by his daily\nearnings: $250 (more money needed) / $10 (daily earnings) = 25 days.\nThe answer is 25.\nResponse of Baseline (CoT): To find out how many days Mark needs to save his money, we need to subtract the\ncost of the bike from the amount of money he has, then divide the result by the amount he earns per day. Mark has\n$50 and wants to buy a bike that costs $300. First, subtract the cost of the bike from the amount of money Mark has:\n$50 - $300 = -$250 Since Mark can't have a negative amount of money, we need to convert the negative amount to a\npositive one by adding the cost of the bike to the amount of money Mark has: $50 + $300 = $350 Now, divide the\nresult by the amount Mark earns per day: $350 / $10 = 35\nThe final answer is 35."}, {"title": "Other Text Generation Examples", "content": "Prompt: Evaluate the expression a\u00b3\u00b7 a\u00b2 if a = 5\nGenerated Text:\n<boLatent><Latent-code-130><eoLatent> We can use the rule of exponents that states $a^m\\cdot a^n = a^{m+n}$.\nSo, a\u00b3 a2 = a\u00b3+2 = a5. Now, we substitute a = 5 into the expression: 55. 55 = 5\u00b75\u00b75\u00b75\u00b75 = 3125. The answer\nis 3125.\nPrompt: Compute \nGenerated Text: <boLatent> <Latent-code-888> <Latent-code-744> <Latent-code-196>\n<Latent-code-92> <Latent-code-557> <eoLatent> $\\frac{15!}{(15-2)!2!} = \\frac{15!}{2!13!}$. To simplify this expression, we\ncan cancel out the common factors in the numerator and denominator: $\\frac{15\\cdot14}{2} = \\frac{15\\times14}{2} = 105$. The answer is 105.\nQuestion: How many vertical asymptotes does the graph of $y = \\frac{2}{x^2+x-6}$ have?\nAnswer:\n<boLatent><Latent-code-773> <Latent-code-514> <eoLatent> the denominator,\nwhich is $x^2 + x - 6$. We can factor this quadratic expression as (x + 3)(x - 2). Now, we can see that the\ndenominator will be equal to zero when either (x + 3) = 0 or (x \u2013 2) = 0. Solving for these values, we get x = -3\nor x = 2. Therefore, the vertical asymptotes of the graph of $y = \\frac{2}{frac2{x^2+x-6}}$ are x = \u22123 and x = 2. Since there are two distinct values of x that make the denominator equal\nto zero, the graph has 2 vertical asymptotes. The answer is 2.\nQuestion: If there exists an ordered numbering of the nodes such that for each node there are no links going to a lower-numbered node, then there are no directed cycles in a directed graph. True or false?\nAnswer: <boLatent><Latent-code-526> <eoLatent> a directed graph. A directed cycle is a sequence\nof nodes in a graph, where each node is connected to the next node by a directed edge, and the last node is connected\nback to the first node by a directed edge. Now, let's consider the given condition: \"If there exists an ordered\nnumbering of the nodes such that for each node there are no links going to a lower-numbered node, then there are no\ndirected cycles in a directed graph.\" This condition essentially means that if we number the nodes in a specific way,\nand there are no links going to a lower-numbered node, then there are no directed cycles in the graph. To see why\nthis is true, let's consider what would happen if there were a directed cycle in the graph. If there were a directed\ncycle, then there would be a sequence of nodes where each node is connected to the next node by a directed edge,\nand the last node is connected back to the first node by a directed edge. However, if we number the nodes in the way\ndescribed, and there are no links going to a lower-numbered node, then this directed cycle would be impossible. This\nis because the last node in the cycle would have to be connected to a lower-numbered node, which would violate the\ncondition. Therefore, if we number the nodes in this way, then there can be no directed cycles in the graph. The\nanswer is True."}]}