{"title": "Explanation Space: A New Perspective into Time Series Interpretability", "authors": ["Shahbaz Rezaei", "Xin Liu"], "abstract": "Human understandable explanation of deep learning models is necessary for many critical and sensitive applications. Unlike image or tabular data where the importance of each input feature (for the classifier's decision) can be directly projected into the input, time series distinguishable features (e.g. dominant frequency) are often hard to manifest in time domain for a user to easily understand. Moreover, most explanation methods require a baseline value as an indication of the absence of any feature. However, the notion of lack of feature, which is often defined as black pixels for vision tasks or zero/mean values for tabular data, is not well-defined in time series. Despite the adoption of explainable AI methods (XAI) from tabular and vision domain into time series domain, these differences limit the application of these XAI methods in practice. In this paper, we propose a simple yet effective method that allows a model originally trained on time domain to be interpreted in other explanation spaces using existing methods. We suggest four explanation spaces that each can potentially alleviate these issues in certain types of time series. Our method can be readily adopted in existing platforms without any change to trained models or XAI methods. The code is posted in supplementary material.", "sections": [{"title": "Introduction", "content": "Explainable AI (XAI) is a recent branch of studies dedicated to provide human interpretable explanation of deep models. Initially introduced for image data, attribution-based methods aim to highlight the region of the image responsible for the model's decision. Similar approach has been later adopted for other data types, including time series. However, unlike a typical image, time series are vastly diverse in nature. In many cases, the associated feature cannot be easily presented in time domain (Schr\u00f6der, Zamanian, and Ahmidi 2023), such as dominant frequency, Hurst parameter, Lyapunov exponent, etc. In such cases, a classifier can achieve high accuracy while XAI methods are inevitably doomed to provide understandable explanation. For instance, the representation in time versus frequency domain of a model trained on FordA dataset is shown in Figure 1a and 1b which clearly shows that only a few frequency components are dominant, and thus more desirable, while the time domain explanation is more complex and hard to comprehend. Similarly, an audio signal the time/frequency space (Figure 1d) generates a more sparse explanation than the time domain (Figure 1c).\nIn this paper, our premise is that a more sparse explanation is more interpretable to an end user. Our key observation is that different spaces generate explanation with different sparsity on different type of time series that is aligned with our intuitive understanding of the nature of time series. The main challenge is that how to explain an existing DL model trained only on time domain in different domains.\nTo address this challenge, we introduce the concept of explanation space projection, where a simple procedure allows existing XAI methods to explain a model in different spaces\u00b9, such as time, frequency, or time/frequency. Using this procedure, we can generate explanations in multiple domains and automatically select the most appropriate space with respect to a chosen metric, such as sparsity. The main advantage of our method is that the target DL model is not required to be retrained on any other space. In other words, an existing model trained on any domain can be explained in all other domains using any existing XAI method. As a"}, {"title": "Method", "content": "Let $x = \\{x_1,x_2,...,x_N\\} \\in \\mathbb{R}^N$ be a time series sample, where $x_i$ is the value at time step $i$ and $N$ is the number of time steps. To each time series sample there is a label associated, denoted by $y \\in \\{Y_1,..., Y_c\\}$ where C is the number of classes. A classifier, $M: x \\rightarrow y$, maps a times series sample to a probability distribution vector over C classes. An explanation method, $E(M(.), x) \\rightarrow z$, takes a classifier and a time series sample and generates an output, $z$. In the case of attribution methods, $z$ is a heat-map with the same dimension as $x$, and in the case of counterfactual methods, $z$ is a new time series where $M(x) \\neq M(\\bar{x})$.\nReversible Representation Space: A one-to-one function $F$, with a reverse function $F^{-1}$, is a representation space and it implies that $F^{-1}(F(x)) = x$. In this paper, we also refer to it as an explanation space since we mainly focus on changing a domain for the purpose of explanation.\nProjection of Explanation from Time Domain into a New Explanation Space: Assume a classifier $M$ trained on time domain. Consider an explanation space $F$ (e.g. FFT where it outputs frequency domain). Since $M(F^{-1}(F(x))) = M(x)$, we can define $z = F(x)$ as a new domain and the associated classifier is $M'(z) = M(F^{-1}(z)) \\rightarrow y$. The new classifier M' is the same as M except that it performs the reverse operation, $F^{-1}$, before passing the input to M. Hence, M on x domain is functionally the same as M' on z. However, now, we can use any existing explanation method, E, to generate an explanation on z domain (or on explanation space F) using $E(M'(.), z)$\u00b2."}, {"title": "Explanation Spaces", "content": "The projection procedure allows us to generate an explanation on any explanation space even if the original model is only trained on time domain. Here, we introduce a few explanation spaces useful in certain applications:\nFrequency Space is the Fourier transform and the reverse function is the inverse Fourier transform. As explained in Introduction, FordA dataset is an example where the frequency domain explanation is less complex than time (Figure 1b versus 1a.\nTime/Frequency Space is the short-time discrete Fourier transform to allow the explanation on both time and frequency domains. Figure 1c shows the attribution generated by DeepLIFT on AudioMNIST (Becker et al. 2023) in time domain and Figure 1d shows the attribution generated by the same method on time/frequency domain. The attribution on time/frequency domain is more focused on the center of the time series where actual signal is presented.\nMin Zero Space aims to alleviate the well-known issue of baseline exist on many attribution-based methods (H\u00f6llig, Thoma, and Grimm 2023). By default, many existing explanation methods implicitly or explicitly assume a specific value (zero for the most part or a value set by user or training set) to be associated with a lack of any feature, including InputXGradient, feature occlusion, DeepLIFT, etc. Because time series are often z-normalized per sample to have a zero mean mainly for training stability, the zero value does not correspond to the lack of feature even if the original un-normalized time series does. For example, power consumption time series (e.g. ElectricDevices and LargeKitchenAppliances datasets in UCR repository) or time series dataset containing number of events or magnitude of events (e.g. Earthquakes and ChinaTown datasets in UCR repository or network traffic classification (Rezaei and Liu 2020)) are examples of natural time series where zero corresponds to a"}, {"title": "Difference Space", "content": "Difference Space is a result of taking a difference of each two consecutive time steps as a new time series. Differencing has been widely used in time series analysis to remove a trend or to make a time series stationary (Hyndman and Athanasopoulos 2018). In our framework, difference domain has two benefits. First, for non-stationary time series (often with trend), such as stock price or atmospheric time series, a shapelet-like feature in a small region can be completely imperceptible. For example, in a synthetic data generated to mimic a non-stationary time series with a small shapelet as a distinguishable feature, a shapelet feature may not be easily visible when it is small in comparison to the range of the entire time series\u00b3, as shown in Figure 3a and 3c. However, the shapelets (i.e. semi-linear increase/decrease patterns visible in difference domain) can be clearly located in difference domain where the long range fluctuations is effectively removed, as shown in the corresponding samples in Figure 3b and 3d. Interestingly, the difference and time domain has a huge impact on the performance of XAI methods. The XAI method completely fails the time domain which can be explained by the second benefit.\nSecond, the difference domain implicitly changes the notion of a feature. Here, the lack of change in time domain corresponds to zeros in difference domain (implying a lack of feature), and changes correspond to non-zeros values. This can solve the baseline issue for certain typres of time series. For example, in time series associated with movements (e.g. GunPoint) or ECG signals, the lack of activity is better manifested by a sequence of near constant values, not necessarily a sequences of zeros. By taking the difference of the time series, a sequence of unchanging values will map to"}, {"title": "Evaluation", "content": "Experimental Setup: We include several datasets covering different types of time series, as shown in Table 1, mainly from UCR Repository (Dau et al. 2019) except AudioMNIST provided in (Vielhaben et al. 2024). For each dataset, we trained two commonly used model architectures: a ResNet\u2074 and an InceptionTime model (Ismail Fawaz et al. 2020). We emphasize that all models are trained only on time domain. The explanations are generated on different domain using the process described in Method Section. We evaluate nine well-known XAI methods: DeepLIFT, GradientSHAP, Guided BackProp (Springenberg et al. 2014), InputXGradient (I\u00d7G), Integrated Gradient (IG), KernelSHAP, LIME (Ribeiro, Singh, and Guestrin 2016), Occlusion, and Saliency (Simonyan, Vedaldi, and Zisserman 2013). We use Captum implementation (Kokhlikyan et al. 2020) for XAI methods.\nNote that the goal of this paper is not to have comprehensive evaluation of different XAI methods, as it has previously shown that no single XAI method can outperform others in all metrics/datasets (L\u00f6ffler et al. 2022). Rather, we focus on illustrating that the concept of spaces is useful and that the same XAI method may bring a better explanation (with respect to some metric such as sparsity) on different spaces depending on the type of time series. While our method can be easily adopted for existing counter-factual XAI methods as well, such evaluation is the subject of future studies.\nEvaluation Criteria\nRobustness: Robustness of an attribution method refers to its sensitivity with respect to small perturbations of the target sample (H\u00f6llig, Thoma, and Grimm 2023; L\u00f6ffler et al. 2022). Unlike previous studies, we investigate the effect of different input spaces. As a result, a trained model wrapped to take frequency domain input may be more sensitive than the unwrapped version taking the time domain input. It is not trivial whether a model wrapped in other domains is more"}, {"title": "What Is \u03b2 In Sparsity?", "content": "In the sparsity metric, $\u03b2 > 1$ is a constant hyper-parameter to spread out the metric more uniformly. Empirically, for majority of datasets/XAIs, the attribution is relatively sparse and it is rarely uniformly distributed. Consequently, in most cases, all methods give a value between 0.90 and 1 which makes it hard to compare one another. Hence, choosing $\u03b2 > 1$ allows us to better spread out the values and compare XAI Methods."}, {"title": "Conclusion", "content": "In this paper, we introduce a wrapping process that allows a model trained on one domain to operate on other domains and, thus, allows existing XAI methods to explain the model in different input domains. We suggest four new spaces. Moreover, we introduce a new sparsity metric satisfying a few desirable properties. Using the proposed metric, we show that different explanation spaces can be useful for different types of time series consistent with domain knowledge and intuition. This simple yet effective technique allows a practitioner familiar with a time series in hand to easily pick the appropriate explanation space and explain the trained classifier using any existing XAI methods."}, {"title": "Explanation Space Implementation", "content": "The representation space function, $F(x)$, is used outside a neural network and it can be construed as a pre-processing stage (with respect to the explanation generation stage at inference time, not with respect to training stage). Hence, the target domain data, $z = F(x)$, can be obtained by implementing the transformation in any arbitrary way. However, $F^{-1}(z)$ function is embedded into the M' neural network. Hence, for compatibility with gradient-based methods, we either need to use the existing functions in deep learning packages or implement the function using existing layers. Fortunately, most packages, like PyTorch, has the implementation of FFT, STFT (through torchaudio.transforms.Spectrogram), and their inverse. We implement inverse mapping function for min zero space and difference space using a single fully connected layer with a predefined weight matrix, as follows:\nMin Zero Space Implementation: The inverse mapping of the min zero space can be written as follows:\n$F_{min-zero}(\\{x_1 - x_{min}, ..., x_N - x_{min}, x_{min}\\}) \\rightarrow \\{x_1,x_2, ..., x_N\\} \\quad (7)$\nIn terms of a fully connected layer, it can be implemented as a layer that takes a vector of size N + 1 as input and output a vector of size N, where the (N + 1)th element is added to all other elements in the input vector and then it is dropped from the output. It can be easily shown that an Identity matrix of size N \u00d7 N concatenated to an all-one matrix of size 1 \u00d7 N can obtain the reverse mapping as a linear layer as follows:\n$ \\begin{bmatrix} x_1 - x_{min} \\\\ x_2 - x_{min} \\\\ ... \\\\ x_{N-1} - x_{min} \\\\ x_N - x_{min} \\\\ x_{min} \\end{bmatrix}_{N+1x1}  \\begin{bmatrix} 1 & 1 & ... & 1 \\\\ 0 & 0 & ... & 0 \\\\ 0 & 1 & ... & 0 \\\\ ... & ... & ... & ... \\\\ 0 & 0 & ... & 1 \\\\ 1 & 1 & ... & 1 \\end{bmatrix}_{N+1xN}  \\longrightarrow  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_{N-1} \\\\ x_N \\end{bmatrix}_{Nx1} \\quad (8)$\nDifference Space Implementation: The inverse mapping of the difference space can be written as follows:"}, {"title": null, "content": "$F_{diff}(\\{x_1, x_2 - x_1, x_3 - x_2, ..., x_N - x_{N-1}\\}) \\rightarrow \\{x_1,x_2, ..., x_N\\} \\quad (9)$\nIt can be easily shown that the ith elements of the output can be obtained by adding all previous input elements up to ith element. As a result, all 1 to i \u2013 1 elements are cancel out and only the xi remains which constructs the inverse operation. In linear layer with matrix multiplication operation, it can be easily implemented by a weight matrix of upper triangular form where every non-zero element is exactly one, as follow:\n$ \\begin{bmatrix} x_1 \\\\ x_2-x_1 \\\\ ... \\\\ x_{N-1}-x_{N-2} \\\\ x_{N} \\end{bmatrix}_{Nx1}  \\begin{bmatrix} 1 & 1 & 1 & 1 & ... & 1 \\\\ 0 & 1 & 1 & 1 & ... & 1 \\\\ ... & ... & ... & ... & ... & ... \\\\ 0 & 0 & 0 & 0 & ... & 1 \\end{bmatrix}_{NXN}  \\longrightarrow  \\begin{bmatrix} x_1 \\\\ x_2 \\\\ ... \\\\ x_{N-1} \\\\ x_{N} \\end{bmatrix}_{Nx1} \\quad (10)$"}]}