{"title": "Answer Set Networks: Casting Answer Set Programming into Deep Learning", "authors": ["Arseny Skryagin", "Daniel Ochs", "Philipp Deibert", "Simon Kohaut", "Devendra Singh Dhami", "Kristian Kersting"], "abstract": "Although Answer Set Programming (ASP) allows constrain-ing neural-symbolic (NeSy) systems, its employment is hindered by the prohibitive costs of computing stable models and the CPU-bound nature of state-of-the-art solvers. To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep Probabilistic Logic Programming (DPPL). Specifically, we show how to translate ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded problem by leveraging GPU's batching and parallelization capabilities. Our experimental evaluations demonstrate that ASNs outperform state-of-the-art CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following two contributions based on the strengths of ASNs. Namely, we are the first to show the fine-tuning of Large Language Models (LLM) with DPPLs, employing ASNs to guide the training with logic. Further, we show the \"constitutional navigation\" of drones, i.e., encoding public aviation laws in an ASN for routing Unmanned Aerial Vehicles in uncertain environments.", "sections": [{"title": "Introduction", "content": "The paradigm of modeling an application-specific problem in a logical language to then apply a general solving approach has a long-standing history. To this end, Answer set programming (ASP), being a modern, non-monotonic reasoning framework based on stable model semantics, has experienced rising interest in NeSy inference systems. In this vein, recent NeSy AI pipelines (Skryagin et al. 2023, 2022; Yang, Ishay, and Lee 2020) have relied on a foundation of ASP for providing a declarative modeling language of rules and background knowledge. Although conflict-driven, state-of-the-art solvers like clingo (Gebser et al. 2011) and DLV (Alviano et al. 2017) continually improve, the computational cost of today's ASP solvers remains an open issue to the real-world application of NeSy systems.\nMany deep probabilistic logic programming languages utilize such CPU-based solving frameworks. Lately, languages such as SLASH and NeurASP using ASP or Deep-ProbLog (Manhaeve et al. 2018) and Scallop (Huang et al. 2021) using Sentential Decision Diagram (Darwiche 2011)rely on neural computations on the GPU and symbolic computations on the CPU. SLASH, relying on clingo as the backbone, uses multithreading of the CPU to scale. And, SAME provides the speed-up based on top-k%. Although these are great advancements for scaling NeSy AI, the gap in computational speed to support seamless communication with deep learning models remains substantial to this day. This becomes even more prevalent in NeSy systems like Alpha Geometry (Trinh et al. 2024). There, 10k CPU cores were used to train a model to reach the silver medal level in geometry at the math Olympiads.\nFollowing this trend, the question of how to integrate these pipelines tightly with modern GPUs is more urgent than ever. For example, large machine learning models for natural language and vision tasks have dramatically risen in popularity and public awareness. Widely known, their success is greatly supported by their underlying architectures' ability to distribute work on many-core systems with ease. Thus, translating the task of solving ASP-based modeling into a neural framework that leverages modern hardware acceleration to the same level can be seen as a key to applying NeSy systems in real-world applications.\nUsually, in state-of-the-art solvers, the search for the solutions, i.e., the stable models, of a program II among all interpretations follows a binary tree. That is, branches of the tree correspond to (partial) assignments of truth values to the program's atoms (partial interpretations). Further, alternating between decision and propagation phases generates the searched stable models of the ASP. However, recent research has proposed alternative approaches based on the propagation of truth values within a graph. Such graph-based Answer Set Programming solver systems (Li 2021) are based on Dependency Graphs as a basis. The advantage of this explicit representation of atoms and literals as graph nodes is the uniform representation of stable models and preservation of causal relationships, meaning each atom is justified in a particular stable model. Hence, the usual heuristics inherited from SAT solvers are avoided, i.e., the solving procedure is transparent.\nAlong this line of research, we investigate a GNN-based representation of ASPs and the search for stable models. More precisely, we present the novel NeSy ASP-solver Answer Set Networks (ASN)."}, {"title": "Casting Answer Set Programs into GNNs", "content": "Here, we show how the ASP-Core-2 (Calimeri et al. 2020) language is encoded and solved in the form of the RG. ASN expects a grounded, i.e., variable-free and tight (acyclic) ASP-program, II. Fig. 1 gives the overview of the ASN-pipeline, from a grounded ASP-program to stable models (SM). The pipeline consists of four steps, which we will describe in separate subsections."}, {"title": "Reasoning Graph", "content": "Let $G = (V,E)$ be a heterogeneous graph, with a set of nodes V and a set of edges $E \\subseteq V \\times V$. Further, all nodes $v \\in V$ and edges $e \\in E$ have an associated type $\\tau_v \\in A$ and $\\tau_e \\in R$, where A is the set of node types and $R \\subset A \\times A$ is the set of edge types, respectively. The tuple (A, R) is called the network schema of G and is itself a graph representing the node types and the possible relations between them. A reasoning graph is a heterogeneous graph where\n$A := \\{A_{\\land}, A_{\\lor}\\} \\cup \\{A_{\\text{count}}, A_{\\text{sum}}, A_{\\text{min}}, A_{\\text{max}}\\}$\n$=: A_{\\text{logical}}$\n$=: A_{\\text{aggr}}$\n(1)\n$R := (A \\times A_{\\text{logical}}) \\cup (A_{\\text{logical}} \\times A_{\\text{aggr}})$\n$=: R_{\\text{logical}}$\n$=: R_{\\text{aggr}}$\n(2)\nAn RG is constructed from a ground ASP program, where the nodes represent logical constructs namely classical atoms, aggregate literals, as well as conjunctions and disjunctions of the former two and the edges indicate rules for constructs to be true as defined by the semantics of the specified program II."}, {"title": "Neural Compilation", "content": "We introduce the process of neural compilation into an RG for each element of the ASP syntax. Fig. 2 visualizes all basic elements of our RGs. Positive edges are shown in black, while red is used for negative ones. Additionally, edges are annotated with their corresponding edge weights. Dashed edges are parts of either disjunctive or choice-rules to indicate that they may or may not be active. There are three categories in the ASP syntax: atoms, literals, and statements, which we will consider separately.\nAtoms Each RG has two special atom nodes, $T$ (true) and $\\bot$ (false), which, if connected to each other through other nodes, indicate true and false statements, respectively. $T$ is always part of any valid interpretation and propagates truth values. Facts, which are by design true, are always connected to the $T$ node via an incoming edge (Fig. 3 i)). The $\\bot$ node is used to indicate contradictions, model constraints, and ensures interpretation consistency. Therefore, its purpose is to indicate the satisfiability of an interpretation $I$ or formally $I \\models \\Pi$. We limit $T$ to have no incoming edges and $\\bot$ to have no outgoing edges, calling them source and sink nodes. Apart from facts, other program atoms are defined through statements. For an atom to be true, at least one rule defining it must be satisfied. In case there are multiple rules defining the atom, we express their combination through a disjunction. Each unique classical atom is represented as an individual disjunction node. Note that predicate atoms and their negations are encoded separately.\nLiterals Due to the program being already grounded, we know if a built-in literal evaluates to true or false. For conjunctions, true built-in literals can be ignored safely while making sure that false ones cannot be satisfied. In the latter case, the conjunction can be treated as false as well. For disjunctions, we change the course by ignoring false built-in literals and setting any to true if it evaluates to true itself. Thus, during neural-compilation, built-in literals are handled implicitly.\nNext, we consider an aggregate literal of the form $\\beta_{\\upsilon,l} \\lozenge_{\\upsilon,l} f\\{\\mathcal{T}_i: \\mathcal{C}_i\\}_{\\upsilon,r} \\lozenge_{\\upsilon,r} \\beta_{\\upsilon,r}$. Herein, we consider aggregate function $f \\in \\{\\#count,\\#sum, \\#min, \\#max\\}$, relational operators $\\lozenge_{\\upsilon,l/r} \\in \\{=,\\neq, <, >, \\leq, \\geq\\}$, terms $\\beta_{\\upsilon,1/r}$, aggregate elements $\\mathcal{T}_i: \\mathcal{C}_i$ with $\\mathcal{T}_i$ and $\\mathcal{C}_i$ being sequences of terms and negation-as-failure (NAF) literals respectively. See Fig. 3 iii) for an example of #sum aggregate. Each unique term $\\mathcal{T}_i$ is aggregated only once if satisfied but may be satisfied by one or more NAF-literals $\\mathcal{C}_i$. Let $\\mathcal{T} = \\{\\mathcal{T}_1,...,\\mathcal{T}_k\\}$ be the set of terms appearing in the aggregate elements, and $C(\\mathcal{T}) = \\{\\mathcal{C}_i|\\mathcal{T}_i : \\mathcal{C}_i \\in \\{\\mathcal{T}_1 : \\mathcal{C}_1,...,\\mathcal{T}_k : \\mathcal{C}_k\\}\\}$ the set of all conditions defining each $\\mathcal{T}_i \\in \\mathcal{T}$. A term $\\mathcal{T}_i \\in \\mathcal{T}$ is aggregated if and only if some NAF-literal $\\mathcal{C}_i \\in C(\\mathcal{T}_i)$ is satisfied. This can be rewritten as a disjunction of the different NAF-literals, $\\bigvee_{\\mathcal{C} \\in C(\\mathcal{T}_i)} \\mathcal{C}$. It is then connected to an aggregate node #aggr with edge weight $f \\{\\mathcal{T}\\}$. Both, left-$\\{\\beta_{\\upsilon,l} \\lozenge_{\\upsilon,l}\\}$ and right-guard $\\{\\lozenge_{\\upsilon,r} \\beta_{\\upsilon,r}\\}$ are encoded in the aggregate node.\nStatements A rule has the general form $H :-B.$ with the head, H and the body, B. Semantically, it is equivalent to a conjunction of the body literals. This conjunction node combines the body literals via negative or positive edges, depending on whether the corresponding literal is default-negated or not. For facts, the body consists of $T$.\nIn compiling $a :-b_1,..., b_n.$, the conjunction node is directly connected to the node representing the consequent atom a, Fig. 3 iv). Constraints, $:-b_1, . . ., b_n.$, are treated the same way assuming the head atom $\\bot$, signifying an invalid interpretation if the body is true, Fig. 3 v)."}, {"title": "Choice definitization", "content": "For disjunctive rules, $a_1|...|a_m :-b_1, ..., b_n.$, the conjunction node is connected to all consequent atoms $a_1,..., a_m$ and each edge representing a choice to be made. A subset of the edges may be removed in advance to represent a corresponding choice. Further, at least one of the consequent atoms needs to be true if the body of the rule is true. Consequently, we add an appropriate constraint,\n$:-b_1,..., b_n, not\\, a_1, . . ., not \\,a_m.$\n(3)\nrendering the current configuration invalid if the body holds but none of the consequent atoms. E.g., Fig. 3 ii) with an empty body.\nThe choice rule is encoded directly: Head atoms may have extra conditions that must be fulfilled in addition to the body. The conjunction node, representing the body, can, in this case, not be directly connected to the consequent atoms but needs to be combined with the disjunctions, representing that some condition for the individual head atoms is fulfilled. Additionally, it must be ensured that if the body is true, so are a valid number of head atoms as defined by the guards of the choice. Thus, we add a similar to Eq. 3 constraint. Weak constraints, $:\\sim b_1,...,b_n.[w@l, t_1,..., t_m]$ (with terms $w, l, t_1, ..., t_m, m > 0$, w for weight and l for level), are ranking the resulting stable models and consequentially handled by the solver after computing all solutions without encoding into RG.\nClassical negation can be regarded as syntactic sugar, where $\\lnot a$ is treated as a separate unique predicate symbol while keeping the same semantics. To ensure consistency of a program, II both a and \u00aca cannot be part of the same model. This requirement is encoded as the constraint $:-a, \\lnot a.$ with $\\forall a, \\lnot a \\in \\Pi$, and is shown in Fig. 3 vi).\nQueries in ASP are encoded as constraints and can be incorporated as such in the RG. The key insight is that the majority of the program (up to the queries) is identical in all cases, and most of RG can be shared. To separate the satisfiability values (i.e., the node values of the sink node) for different queries, we encode them using distinct sink nodes $\\bot_{Q_1},\u2026\u2026\u2026 \\bot_{Q_n}$. The global sink node $\\bot$ is then connected to these special sink nodes to pass along the satisfiability values of the main part of the program.\nChoice definitization\nAn RG may represent a program that contains disjunctive and choice rules. We consider an example program containing the single rule $a_1|a_2 :-B.$ with B being some sequence of NAF-literals. This example can be represented by the set of programs $\\{\\{a_1 :-B.\\},\\,\\{a_2 :-B.\\},\\,\\{a_1 :-B., a_2 :-B.\\}\\}$, which we will call program definitives (or definitives for short). Each answer set of the original program consists of the program definitives, making a different possible choice definite, transforming the original one into a normal program with a deterministic cause-effect relationship between classical atoms. The RG representing these definitives can be directly constructed from a copy of the RG representing the original program, disabling (removing) the appropriate edges connecting the body to the non-selected consequent head atoms. For programs containing multiple disjunctive, choice, and NPP-rules, all combinations of their respective variants need to be considered; Which, in turn, yields easy parallelization of RG."}, {"title": "Message-Passing", "content": "Given a starting configuration of nodes that are considered true, we propagate truth values through the graph with the goal of computing the (stable) models of the program. Thus, we associate with every node $v \\in V$ a boolean node value $h_{v,t} \\in \\{0,1\\}$ at a time step $t \\in N$. Except for $T$, all nodes are initially set to false (0) at t = 0.\nAggregate nodes $v \\in \\nu,\\tau_v \\in R_{\\text{aggr}}$ additionally have terms $\\beta_{\\upsilon,l}, \\beta_{\\upsilon,r} \\in \\mathbb{N}$ and relation operators $\\lozenge_{\\upsilon,l},\\lozenge_{\\upsilon,r} \\in \\{=,\\neq, <, >, \\leq, \\geq\\}$ linked to them, representing the guards of the corresponding aggregate. Furthermore, each edge $e \\in E$ has a weight\n$w_e \\in \\{\\{-1,1\\}\\} \\begin{cases} \\text{if } e \\in E_{\\text{logical}}, \\\\ T \\quad \\text{if } e \\in E_{\\text{aggr}}. \\end{cases}$\n(4)\nWe can then define the node values for the next time step recursively as $h_{v,t+1} =$\n$\\begin{cases}\n\\bigwedge_{\\substack{e=(u,v)\\in \\varepsilon, \\\\ w_e=1}} A h_{u,t} & \\text{if } \\tau_v = A_{\\wedge},\n\\\\\n\\bigvee_{\\substack{e=(u,v)\\in \\varepsilon, \\\\ w_e=-1}} A \\lnot h_{u,t} & \\text{if } \\tau_v = A_{\\wedge},\n\\\\\n\\bigvee_{\\substack{e=(u,v)\\in \\varepsilon, \\\\ w_e=-1}} A h_{u,t} \\lor \\bigvee_{\\substack{e=(u,v)\\in \\varepsilon, \\\\ w_e=1}} A \\lnot h_{u,t}  & \\text{if } \\tau_v = A_{\\vee},\n\\\\\n\\mathbb{I}\\{\\lozenge_{\\upsilon,l} \\mathbb{\\Sigma}w(e) \\lozenge_{\\upsilon,r}\\} & \\text{if } \\tau_v = A_{\\text{count}},\\text{       (5)} \\\\      e\\epsilon{E_{in}(v)}  \\\\\n\n\\lozenge_{\\upsilon,l} \\mathbb{\\Sigma}_{e \\epsilon E_{in}(v)} w(e) \\lozenge_{\\upsilon,r}  & \\text{if } \\tau_v = A_{\\text{sum}}, \\\\ g_{\\upsilon, min} = (W_{in}(v) \\cup \\{\\#sup\\})\\lozenge_{\\upsilon,l}g_{\\upsilon,e} & \\text{if } \\tau_v = A_{\\text{min}}, \\\\ g_{\\upsilon, max} = (W_{in}(v) \\cup \\{\\#inf\\})\\lozenge_{\\upsilon,r}  & \\text{if } \\tau_v = A_{\\text{max}}.\n\n\\end{cases}$\nwhere $g_{\\upsilon,l/r} := \\beta_{u,t/r} \\lozenge_{\\upsilon,l/r}, E_{in}(v) := \\{e|e = (u, v) \\in E,h_{u,t} = 1\\}$ is the set of incoming edges of $v \\in \\nu$ whose source nodes are 1 (true) at time step $t\\in N$ and $W_{in}(v) := \\{w_e|e \\in E_{in}(v)\\}$ the set of corresponding edge weights.\nUpdates at the time step t+1 can be understood in the following way. Logical nodes $v \\in V_{\\text{logical}}$ take on the value of the conjunction or disjunction of their incoming neighbors' values, where the edge weight determines whether the corresponding node value is included positively or negatively. We therefore call an edge $e \\in E_{\\text{logical}}$ positive if $w_e = 1$ and negative otherwise. For aggregate nodes, $v \\in V_{\\text{aggr}}$, the edge weights of incoming edges signify the values to be aggregated but are only included if the source node is considered true. The node value of an aggregate node is then determined by checking the guards of the aggregated value. For #min and #max aggregates, we include the default value for empty sets. I.e., #sup and #inf for an infinite $T$ and swapping both otherwise."}, {"title": "Model Reduction", "content": "After message passing, the graphs represent interpretations, as indicated by the truth values of the nodes representing classical atoms. If I is true for some graph instance, then the corresponding interpretation is not a model. However, the interpretation does not equate to a stable model of the original program. Accurately, ASP's models are minimal w.r.t. inclusion. Thus, we filter all the program's models to obtain the minimal one. This can be done efficiently in a batch-wise manner using logical bit-wise operators. Let $I_1, I_2 \\subset A = \\{a_1,...,a_n\\} \\subseteq B^n$ be two interpretations over some finite subset of Herbrand base $B$. Further, let $m_1, m_2 \\in \\{0,1\\}^n$ be vectors representing the truth values of the corresponding interpretations with the components\n$m_{ij} = 1$ if and only if $a_j \\in I_i$. We have $m_1 \\models m_2 =$\n$\\begin{cases}\n m_1 & \\text{if } I_1 \\subset I_2, \\\\\n  m_2 & \\text{if } I_1 \\supset I_2, \\\\\n  m \\in \\{0,1\\}^n, m_1 \\neq m \\neq m_2 & \\text{else}.\n\\end{cases}$\n(6)\nwhere $\\otimes$ represents element-wise logical conjunction (or multiplication). For a set of interpretations $\\{I_1,...,I_k\\} \\subseteq A$ with corresponding 0-1-vectors $m_1,..., m_k$, the set of answer sets can be computed as $AS(\\Pi) =$\n$\\{I_i|i \\in \\{1, ..., n\\} : m_i \\otimes m_j \\neq m_i, \\forall j = i + 1, ..., n\\}$\n(7)"}, {"title": "Neural Probabilistic Predicates", "content": "We have now seen how to extract stable models with ASN. To use ASN for NeSy AI, we can use Neural-Probabilistic Predicates (NPP) from SLASH (Skryagin et al. 2022, 2023) within ASN:\n$\\#npp\\left(h(x), [v_1, ..., v_n]\\right) \\leftarrow Body$\n(8)\nnpp is a reserved term labeling the NPP, h a symbolic name of either a probabilistic circuit (Choi, Vergari, and Van den Broeck 2020), neural network or a joint of both. Fig. 4 displays an example of an NPP. With NPPS, ASN allows for neural probabilistic programming and, simultaneously, probabilistic programming. For a thorough introduction to the SLASH semantics, we refer to (Skryagin et al. 2023). In general, SLASH can be seen as a superclass of ASP."}, {"title": "Experiments", "content": "In this section, we show how ASN scales DPPLs with ASP as backbone in various tasks by answering the following questions:\nQ1 Which advantages offers LLM fine-tuning with ASN for the Reversal Curse (Berglund et al. 2024)?\nQ2 Can ASN scale the task of mission design for unmanned aerial vehicles, as proposed by (Kohaut et al. 2023), to cover Paris and its immediate surroundings?\nQ3 How competitive can ASN be in a NeSy setting such as MNIST-Addition (Manhaeve et al. 2018)?\nFurther, throughout the experiments, we refer to ASN as SLASH using ASN as ASP-solver and with SLASH (Skryagin et al. 2022) the same DPPL but using clingo as solver."}, {"title": "Q1 LLM Fine-Tuning with ASN", "content": "Berglund et al. (2024) have shown in their work that LLMs suffer from the Reversal Curse. Meaning, if a model is trained on a sentence of the form \"A \u21d2 B\", it will not automatically generalize to the reverse direction \u201cB \u21d2 A\". To this end, authors tested SOTA LLMs on pairs of questions like \"Who is Tom Cruise's mother?\" and \"Who is Mary Lee Pfeiffer's son?\". Here, we demonstrate how ASN allows for abductive reasoning while fine-tuning LLMs to resolve the reversal problem.\nTo address this issue, we perform abductive fine-tuning of the LLM using ASN. Particularly, during training, we predict the relation (mother, father, daughter, and son) between a celebrity and their parent(s) in both directions. The training dataset consists of queries such as \u201cQ: What is the relation between  and ? A: \u2026\u201d and the opposite of the prompt by swapping  and . For each celebrity, we only provide one direction during training, i.e., either child-parent or parent-child. We split the dataset into equal parts for each direction respectively to prevent the fine-tuned model from becoming biased in either direction. During test time, we evaluate the accuracy of unseen celebrities and their parents in both directions.\nWe add 12M learnable parameters by LoRA (Hu et al. 2021) to fine-tune the Llama2-7B (Touvron et al. 2023) LLM using ASN. To compare the results of ASN fairly, we fine-tuned the Llama2 in a stand-alone fashion as well. The difference in the training procedure is founded in ASN's ability to derive the opposite relation, while baseline handles only one direction for each celebrity. Using the original dataset from (Berglund et al. 2024), we split the 797 celebrities into 70% train, 20% test, and 10% validation. For in-depth explanations of the experimental setup, we refer our readers to consult the appendix.\nThe results reveal the following findings. Firstly, ASN's runtime equates to the number of passes through the LLM. Namely, we request both the sex and the relation for two persons for every query. In particular, the baseline takes 50s and ASN 189s, which corresponds to the four forward passes and one backward. Secondly, ASN converges one order of"}, {"title": "Q2 Mission Design for Unmanned Aerial Vehicles", "content": "Logic is crucial to ensure safety constraints, e.g., in autonomous navigation. This is evident when considering autonomous agents navigating in human-inhabited spaces. For example, a complex set of rules must be enforced when considering novel applications of Advanced Aerial Mobility, such as in cutting-edge logistics tasks. Similarly, metropolitan areas and capital cities pose a particular challenge due to their complex infrastructure and large scale. Within their borders, they often contain consulates of numerous countries, government buildings, etc. They can also host major events that attract numerous international guests from all over the world. All of this makes navigation particularly challenging because such events require additional temporary regulations that must be adhered to. At the time of writing, for example, Paris, France's capital, is the host city for the 2024 Olympic Games. Hence, in this experiment, we show how ASN scales NeSy methods for safe navigation to such large-scale scenarios by densely sampling Paris' area.\nIn the ProMis framework (Kohaut et al. 2023), the requirements of public laws and operator preferences are encoded in a probabilistic logic language. Essentially, one obtains i.i.d. generative models for each continuous point in the agent's navigation space. Although one can compute probabilities independently, splitting up work and parameters in a multithreaded setting, a large number of logic programs need to be solved. For each $(x_i, Y_i) \\in S$ of points $S$ in a two-dimensional navigation space, we query for a valid airspace$(x_i, Y_i)$. All models entailed via the constraint that encodes the query fulfill one of the rules defining wherever the UAV is allowed to visit the specified point.\nProMis obtains a probability for each coordinate $(x_i, Y_i)$ via Hybrid ProbLog (Gutmann, Jaeger, and Raedt 2010). Using a stochastic error model (Flade, Kohaut, and Eggert 2021), it converts crowdsourced geographic features from OpenStreetMap into probabilistic facts. These probabilistic facts encode probabilities for spatial relations between coordinates and tagged features, e.g., whether a coordinate is likely over a public park or far away from the nearest sports arena. Note that the probabilities are \u201chard-coded\u201d as they are obtained from the input map and not forwarded through a neural network, i.e., we use NPPs to encode the probabilities within the ASP program and use SLASH's inference mechanism without training. Via SLASH, we can then compute the query probability p(Q) from the stable models, which ASN computes for each coordinate.\nTo cover the area of 169km\u00b2, we must solve 65002 programs corresponding to all coordinate query pairs. The resulting map can be seen in Fig. 5b. Despite the problem's size and complexity, ASN successfully generated the visible map in 56m:32s. We took the time to compute a 500\u00b2 grid for ProbLog and SLASH and multiplied it by 169 to obtain the time it would take to compute the whole grid. For ProbLog, it would take 7d:15h and 5d:9h for SLASH."}, {"title": "Q3 MNIST-Addition", "content": "In the MNIST-addition task (Manhaeve et al. 2018), the objective is to predict the sum of two images from the MNIST dataset, where the images are provided as raw data. During testing, the model must classify the images directly without explicit information about the digits they depict. Instead, it learns to recognize the digits through indirect feedback on the sum prediction. Handling more than two images significantly increases the task complexity due to the exponential growth in possible digit combinations. Similar to SAME (Skryagin et al. 2023), we assess the model's scalability across three difficulty levels. These levels range from task T1, depicting two images with a target sum, e.g. sum2(3,7,10), to task T3, which includes four images depicting digits as in sum4(3,7,5,2,17). We evaluate performance using LeNet5 (LeCun et al. 1998) as a NPP for the same experimental settings.\nFollowing the results in Tab. 1, we observe that ASN is a highly beneficial solver for SLASH (Skryagin et al. 2022). Regardless of the tasks' complexity, ASN provides the big leap in computational speed, surpassing SAME (Skryagin et al. 2023): T1 is 3,4 times faster, T2 - 1,89, and T3 - 2.71. These results answer Q3 positively and confirm that seamlessly integrating subsymbolic models for NeSy tasks via ASN should be investigated further. We refer to appendix for an in-depth discussion about choices of batch size, the example of a program, and its RG.\nIn summary, we have demonstrated in the experimental section how ASN positively impacts NeSy tasks that have to be solved multiple times with different configurations. For these problems, which are at the heart of NeSy AI, the proposed ASN bridge the gap by bringing the symbolic part to the neural part, which lies on the GPU."}, {"title": "Related Work", "content": "Automated Reasoning\nClassic logic programming employs a two-step approach: grounding and solving. Grounding transforms the program into a variable-free representation while solving involves enumerating valid combinations of true and false ground atoms using techniques like the Davis-Putman-Logemann-Loveland (DPLL) algorithm (Davis, Logemann, and Loveland 1962), no-good (Dechter and Meiri 1990) assignments, and conflict-driven (Marques-Silva and Sakallah 1996) methods. Satisfiability Modulo Theories (SMT) generalize the setting, capturing full ASP semantics. Popular ASP solvers include clingo (Gebser et al. 2011) and DLV (Alviano et al. 2017). Although the field of SAT and SMT solvers is competitive and consistently improves at solving complex problems (Fazekas, Bacchus, and Biere 2018), single-core execution inherently limits their potential (Heule, Kullmann, and Marek 2015) for NeSy applications. In contrast, Answer Set Networks achieve state-of-the-art performance on these tasks as GPU-accelerated ASP solvers.\nProbabilistic Logic Programming\nProbabilistic inference over a model's potential solutions is necessary to capture ambiguities in the modeled problem. This is the case in Visual Question Answering problems (Antol et al. 2015), with various datasets and benchmarks emerging (Marino et al. 2019; Schwenk et al. 2022; Johnson et al. 2017; Nerdimite 2022). Probabilistic logic languages, like ProbLog (Fierens et al. 2015), propose Weighted (Sang, Beame, and Kautz 2005) and Algebraic Model Counting (Kimmig, Van den Broeck, and De Raedt 2017) over a foundation of SAT-based Prolog. As these systems depend on automated reasoning, as discussed above, the performance gains of ASN directly translate to probabilistic logic languages.\nNeural-Symbolic Systems\nThe NeSy approach combines neural and symbolic methods, leveraging their complementary strengths. For instance, DeepProbLog (Manhaeve et al. 2018) and Scallop (Huang et al. 2021) extend the semantics of ProbLog with Neural Predicates that encapsulate neural networks, e.g., to integrate image perception into probabilistic logic. ASP-based systems like SLASH (Skryagin et al. 2022) and NeurASP (Yang, Ishay, and Lee 2020) have been presented. But, all of the above suffer the issue of dominating computation time for symbolic computation. In this work, we relate to ASP-based NeSy system, intertwining neural reasoning with symbolic modeling in ASP and enabling scalability to larger problems."}, {"title": "Conclusions and Future Work", "content": "We have presented Answer Set Networks (ASN), a novel neural-symbolic (NeSy) solver for Answer Set Programs (ASP) using Reasoning Graphs (RG). To illustrate the advantages of ASN, we evaluated its capabilities on various NeSy tasks. To the best of our knowledge, we are the first to fine-tune large language models (LLMs) within deep probabilistic programming languages (DPPLs).\nThrough Answer Set Networks, we have made considerable steps towards a vast world of future applications of NeSy AI systems. Hereby, ASNs allow for distributed graph learning, allowing the symbolic component to be scaled alongside the neural component to larger problems and datasets. Future developments could focus on optimizing the definitization and readout processes, possibly exploring techniques to mitigate the higher memory-print associated with increasing choices while maintaining compatibility with its main goal of integration with sub-symbolic models. Further, thanks to GNNs being the backbone of RG, the next natural step will be to realize differentiable weighted-model semantics for ASN."}, {"title": "A - Experimental Details", "content": "LLM Fine-Tuning\nTraining Setup \u2013 For training", "query": "What is the relation between  and ? The answer is:\". We then predict the next token and obtain the logits for the four relation classes: mother, father, daughter and son. There"}]}