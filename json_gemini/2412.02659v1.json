{"title": "Adaptive Informed Deep Neural Networks for Power Flow Analysis", "authors": ["Zeynab Kaseb", "Stavros Orfanoudakis", "Pedro P. Vergara", "Peter Palensky"], "abstract": "This study introduces PINN4PF, an end-to-end deep learning architecture for power flow (PF) analysis that effectively captures the nonlinear dynamics of large-scale modern power systems. The proposed neural network (NN) architecture consists of two important advancements in the training pipeline: (A) a double-head feed-forward NN that aligns with PF analysis, including an activation function that adjusts to active and reactive power consumption patterns, and (B) a physics-based loss function that partially incorporates power system topology information. The effectiveness of the proposed architecture is illustrated through 4-bus, 15-bus, 290-bus, and 2224-bus test systems and is evaluated against two baselines: a linear regression model (LR) and a black-box NN (MLP). The comparison is based on (i) generalization ability, (ii) robustness, (iii) impact of training dataset size on generalization ability, (iv) accuracy in approximating derived PF quantities (specifically line current, line active power, and line reactive power), and (v) scalability. Results demonstrate that PINN4PF outperforms both baselines across all test systems by up to two orders of magnitude not only in terms of direct criteria, e.g., generalization ability but also in terms of approximating derived physical quantities.", "sections": [{"title": "I. INTRODUCTION", "content": "Power flow (PF) analysis is a foundational computational method to assess and determine the steady-state operating conditions of electrical power systems by computing voltage magnitudes and phase angles at all buses. This analysis is crucial to ensure the reliability, stability, and optimal performance of power systems. It also allows operators to make informed decisions and mitigate potential issues, such as voltage violations, overloads, and system instability [1], [2].\nPF analysis can be performed by solving nonlinear and non-convex algebraic equations derived from the nodal balance of active and reactive power at each bus in power systems [3]. Exact analytical solutions for these equations, which also involve impedance parameters, load characteristics, and generator conditions of power systems, are impossible. Therefore, iterative numerical methods, such as the Gauss-Seidel and Newton-Raphson (NR) methods, are conventionally employed to converge to a solution that satisfies the PF equations within specified accuracy limits. Eventually, the solutions yield voltage phasors across the entire power system and, hence, provide a comprehensive understanding of the operational state of the power system [4]\u2013[6].\nConventional iterative numerical methods, however, face computational challenges in large-scale modern power systems. They struggle to capture the intricacies of power systems as a result of uncertainties involved, such as inaccurate line profiles due to weather conditions and aging, different types of loads, but also missing data on renewable energy resources [7]. Ineffective PF analysis under these circumstances can lead to safety threats, including renewable energy generation curtailment and blackouts, and difficulties in accommodating distributed energy resources [8], [9]. Addressing these challenges necessitates developing new approaches for PF analysis that are both computationally efficient and numerically stable.\nDeep learning approaches, and more specifically, artificial neural networks (NNs), are currently the most powerful set of numerical tools to provide accurate approximations of nonlinear problems [10]. Several studies have demonstrated the superiority of deep learning approaches in PF analysis in terms of computational time by orders of magnitude. At the same time, the accuracy of the solutions is competitive compared to the conventional iterative numerical methods (e.g., [11], [12]). NNs, therefore, can address the challenges mentioned above by leveraging the availability of massive measurements and/or augmented data, learning complex input-output relationships that are often difficult or even impossible for conventional iterative numerical methods to comprehend, and achieving the accuracy required for real-world applications [13]. Nevertheless, NNs are subject to overfitting, lack of generalization, and scalability issues. They are very unlikely to meet the physical constraints. Moreover, their performance relies heavily on the training dataset size and quality. In contrast, not enough data is always available due to privacy reasons and the presence of missing data, among others [14].\nSeveral studies in the literature have investigated the impact of various modifications on the efficacy of deep learning-based PF analysis. These modifications are categorized into three main stages: (i) pre-training (e.g., [15]); (ii) training (e.g., [16]); and (iii) post-training (e.g., [17]), as outlined in Table I. A majority of these studies have primarily focused on the training stage. The table also highlights instances where topology information, such as line physical properties (e.g., [18]) and graph topology (e.g., [19]) has been integrated into the training stage. The literature review also indicates the utilization of different prior knowledge for deep learning-based"}, {"title": "II. POWER FLOW ANALYSIS", "content": "PF analysis aims to specify the state variables of power systems, i.e., $[\u03b4 \u03c5]^T$ where \u03b4 and v denote the voltage phase angle and voltage magnitude, respectively. The AC PF analysis problem is a representation of Kirchhoff's laws and is formulated in rectangular coordinates [22] as\n$Pi = \\sum_{j=1}^{n} g_{ij} (\\mu_i\\mu_j + \\omega_i\\omega_j) + b_{ij} (\\omega_i\\mu_j - \\mu_i\\omega_j),$ (1)\n$qi = \\sum_{j=1}^{n} g_{ij} (\\omega_i\\mu_j - \\mu_i\\omega_j) \u2013 b_{ij} (\\mu_i\\mu_j + \\omega_i\\omega_j),$ (2)\nwhere i and j are the indices of the buses, n is the total number of buses in the power system, pi and qi are the active and reactive power injection/consumption at bus i, gij and bij are the real and imaginary components of the admittance Yij between buses i and j, and $\u00b5_i = v_i\\cos\u03b4_i$ and $\u03c9_i = v_i\\sin\u03b4_i$ are the real and imaginary components of the voltage phasor at bus i. Here, vi and \u03b4i are the voltage magnitude and phase angle at bus i.\nThere are three types of buses in power systems: (i) reference bus, (ii) load bus (pq bus), and (iii) generation bus (pv bus). $v_i = 1$ and $\u03b4_i = 0$ are known, and pi and qi are unknown for the reference bus. For load buses, pi and qi are known, and vi and di are unknown. Finally, vi and pi are known for generation buses, while qi and di are unknown. This study considers cases with one reference bus and load buses for simplicity. For a power system consisting of a reference bus and load buses, a set of PF equations with the same number of equations and unknowns are achieved [36]\n$pd_i - p_i = 0,$ (3)\n$qd_i - q_i = 0,$ (4)\nwhere pi and qi are defined by Equations (1)-(2), respectively, and $pd_i$ and $qe_i$ are active and reactive power injection/consumption at bus i, respectively.\nEquations (3)-(4) are conventionally solved iteratively to specify the state of the power system [dv] until a convergence criterion is met, i.e., the mismatch between pi and $pd_i$ but also qi and $qd_i$ are small enough."}, {"title": "III. DEEP LEARNING-BASED POWER FLOW ANALYSIS", "content": "Deep learning-based PF analysis refers to developing NNs to approximate the variable states of power systems based on given historical system operation data, hereafter called dataset. The dataset includes input features 2 and output labels \u1ef9. For a power system with a reference bus and load buses, the input features are known variables, i.e., active and reactive power consumption at load buses Z = {$[(p^d_i,q^d_i) : i = 1,2,..., n]$}. At the same time, the output labels are unknown variables, i.e., real and imaginary parts of voltages at load buses \u1ef9 = {$[(\u00b5_i, \u03c9_i) : i = 1,2,...,n]$}. The dimension of Z and \u1ef9 is therefore n \u00d7 2, where n is the number of load buses. Note that the voltage magnitude and phase angle are known for the reference bus i = 0, and the active and reactive power are unknown. Having voltages at all load buses approximated, the active and reactive power at the reference bus can be calculated.\nThe training of NNs is an iterative process and involves four steps. In the first iteration, the set of trainable parameters of the NN, i.e., the weight matrices and bias vectors \u03b8 = {$(W_k,b_k) : k = 1, 2, . . ., m$}, are initialized in the update step, where m is the number of hidden layers. In the forward step, the network is developed using Equations (6)\u2013(8). The deviations of the approximated output obtained by the NN \u0177 from the output labels y are computed in the loss step. Finally, the gradient of the deviations is calculated with respect to \u03b8 in the backward step. For the next iteration, \u03b8 is fine-tuned in the update step to reduce the deviations. The process continues until the maximum number of epochs is reached. In practice, the four steps can be individually and jointly modified to improve the overall performance of NNs. For example, it can be done by enhancing the network architecture in the forward step or adding a physical penalty term to the loss function in the loss step."}, {"title": "IV. PROPOSED ARCHITECTURE: PINN4PF", "content": "PINN4PF includes two advancements in forward and loss steps, respectively denoted by A and B in Fig. 1, resulting in a double-head architecture enhanced with an adaptive activation function and a physics-based loss function. Note that classical PF solvers, e.g., the Newton-Raphson (NR) method, and PINN4PF can be interchangeably used to specify the state of the power system. The following subsections provide detailed information on the proposed novel advancements."}, {"title": "A. Forward Pass", "content": "A double-head feed-forward NN $f(\u00b7) \u2208 {\\{f_0, f_1, f_2\\}}$ is developed to approximate y = {$[(\u03bc_i, \u03c9_i) : i = 1, 2, ..., n]$} at load buses using the dataset {x, y}. It has three types of layers, i.e., input, hidden, and output layers, as shown in Fig. 1-A. The input and output layers correspond to the input features x and output labels \u1ef9, respectively. The neurons of the input layer contain active power $p^d_i$ and reactive power $q^d_i$ at all load buses, and hence, the input layer has n \u00d7 2 neurons, where n is the number of load buses. Following the input layer, there is a set of shared hidden layers $f_0(\u00b7)$ acting as a feature extractor that projects x to a higher-dimensional space where the two heads, $f_1(\u00b7)$ and $f_2(\u00b7)$, separately involve a few hidden layers to respectively approximate \u03bc\u0302 and \u03c9\u0302. Thus, the output layer of each head has n neurons. The shared set of hidden layers and the two heads each are a chain of functions and can be represented as\n$f_0(x) = l_m0...0 l_1^0(x),$ (5)\n$f_1(x) = l_m o ... ol_1^1(x),$ (6)\n$f_2(x) = l_m o ... ol_1^2(x),$ (7)\nwhich sequentially process x through m hidden layers to obtain $f_0(\u00b7)$, $f_1(\u00b7)$, and $f_2(\u00b7)$, respectively. For the shared set of hidden layers and the two heads, the k-th hidden layer (k = 1,2,..., m) is given by\n$l_k(x) = \u03c3 (W_k \u00b7 x + b_k) .$ (8)\nHere, Wk and bk are the weight matrix and bias vector for the corresponding hidden layer. Each hidden layer applies a linear transformation $W_k \u00b7 x + b_k$, followed by a nonlinear transformation \u03c3(\u00b7) to capture complex relationships between x and \u1ef9. Note that Wk and bk are trainable parameters and are optimized during the training process.\nThe Rectified Linear Unit (ReLU) activation function \u03c3(\u00b7) is applied to the shared set of hidden layers of the two heads (5). While an adaptive version of ReLU \u03c3'(\u00b7) is developed to apply model-based nonlinear transformations to the two heads (6)\u2013(7). That is, a trainable parameter \u03b1 is introduced to scale the result of the linear transformation while applying nonlinearity. For \u03c3'(\u00b7), the trainable parameter \u03b1 serves as a guide to prevent overfitting and improve the generalization ability of the NN against unseen data. \u03c3(\u00b7) and \u03c3'(\u00b7) are represented as\n$\u03c3 = max(0, z),$ (9)\n$\u03c3' = max(0, \u03b1z),$ (10)\nwhere z is equivalent to $W_k^T.x + b_k$. During each iteration of the training process, the gradients of the loss function concerning the trainable parameters $\u2207_{W_k,b_k,\u03b1_k}L$ are computed for each hidden layer k using the chain rule of differentiation. The resulting gradients are then backpropagated through the network to update and optimize the weights Wk, biases bk, and the scaling parameter $\u03b1_k$. This process enables the network to learn the optimal values of $\u03b1_k$, along with Wk and bk, potentially leading to improved performance in approximating \u1ef9. More detailed information about adaptive activation functions can be found in [37], [38]."}, {"title": "B. Loss Function", "content": "Prior physical knowledge is integrated with the NN architecture to make it informed; see Fig. 1-B. A part of the topology information of the power system, i.e., the diagonal elements of the admittance matrix Ykk, is used to develop the physical model. In addition, the real and imaginary components of the complex voltage at the reference bus, i.e., $\u00b5_0 = 1,\u03c9_0 = 0$, are imported. Eventually, the physical model $f'(\u00b7)$ is derived from the prior physical knowledge of the power system.\nThe derivation starts with Ohm's law, which relates voltage V, current I, and resistance R, that is, V = I \u00d7 R. By extending this equation to the complex domain, V = I \u00d7 Z relates the voltage phasor, current phasor, and impedance phasor. From this equation, I can be expressed in terms of the voltage phasor and admittance phasor Y (inverse of the impedance phasor), that is I = Y\u00d7V. The system of equations for all the buses can then be presented in matrix form as\n$[I]_{nx1} = [Y]_{nxn} \u00d7 [V]_{nx1}.$ (11)\nBy rearranging (11), the current flowing into bus k can be presented as a linear combination of the voltages at all other buses with weights given by the corresponding admittance matrix\n$I_k = \\sum_{i=1}^{n} Y_{ki} V_i$ (12)\nHere, the power equation S = V \u00d7 I\u2217 provides further guides for subsequent derivations that is $I = \\frac{S^\u2217}{V\u2217}$, where S is complex apparent power. The current and voltage phasors are related as\n$\\frac{S^*_k}{V^*_k} = Y_{k1}V_1 + Y_{k2}V_2 + ... + Y_{kk}V_k + ... +Y_{kn}V_n,$ (13)\nwherein the right-hand side is an extended form of the right-hand expression in (12). Finally, the expression for calculating the voltage phasor at bus k can be written as\n$V_k = \\frac{1}{Y_{kk}}( \\frac{S_k}{V^\u2217_k} - \\sum_{i=1,i\u2260k}^{n} Y_{ki} V_i )$ (14)\nIn (14), $Y_{kk} = G_{kk} + jB_{kk}$ is known from the power system topology. Considering the input features and output labels needed to develop the NN for PF analysis, $V_k = \u03bc_k + j\u03c9_k$ and $V\u2217_k = \u03bc_k \u2212 j\u03c9_k$ are known from the output labels, and $S = p + jq$ is known from the input features. The only unknown expression remaining in (14) is\n$\u03c8_k = \\sum_{i=1,i\u2260k}^{n} Y_{ki} V_i,$ (15)\nwhich is called hidden function in this study. Note that $\u03c8_k$ is unique for each data point. Theoretically, $\u03c8_k$ is also unique for each bus. During the training process, the NN first approximates $\u03c8_k$ and then uses it to approximate V = \u03bc + j\u03c9 using\n$f'(x) = \\frac{1}{Y_{kk}} \u00d7 (\\frac{pd - jqd}{\u03bc - \u03b6\u03c9} - \u1783).$ (16)\nThis unique relation improves the learning process by following the underlying physical laws of the power system. Note that instead of integrating the whole admittance matrix $[Y]_{nxn}$, only the diagonal elements Ykk are needed to obtain the physical loss term, and therefore, the computational resource required is reduced by up to three orders of magnitude compared to when integrating the whole topology information.\nTypically, the goal of the training process is to minimize the difference between \u0177, approximated by the NN, and ground-truth output labels \u1ef9, obtained from the Newton-Raphson method (NR), based on a loss function of choice. Here, a modified loss function is developed that integrates both supervised and physical penalty terms. The former is the mean square of the difference between the output approximated by the NN f(.) and \u1ef9. While the latter is the mean square of the difference between the output obtained from the physical model $f'(\u00b7)$ and \u1ef9\n$L = \u03b2_0 \u00d7 \\frac{1}{N} \\sum_{j=1}^{N}(f(p^d_i, q^d_i, \u03b8, \u1fb6) - \u00ffj)^2 +\u03b2_1 \u00d7 \\sqrt{\\sum_{j=1}^{N}( f'(p^d_i, q^d_i, Y_{kk}, \u1783_j) - \u00ffj )^2},$ (17)\nwhere N is the total number of data points j. f(.) \u2208 {$[(f_0, f_1, f_2)]$} and $f'(\u00b7)$ are the NN architecture (5)\u2013(7) and the physical model (16), respectively. \u03b2o and \u03b21 represent the coefficients for the supervised penalty term and the physical penalty term, respectively, with the constraint that \u03b2o+\u03b21 = 1. Initially, \u03b2o = 1 and \u03b21 = 0. After 100 epochs, the value of \u03b21 increases at a specified rate, with its maximum value determined from the sensitivity analysis. \u03b8 = {W,b} is the trainable parameters of the NN and \u03b1 is the trainable parameter for the activation function."}, {"title": "V. RESULTS", "content": "The performance of PINN4PF is evaluated against two baselines: a linear regression model (LR) and a black-box NN (MLP). Experiments are performed on 4-bus [32], 15-bus [33], 290-bus [34], and 2224-bus [35] test systems. The selected systems contain one reference bus with known voltages in complex number form, $\u00b5_0 + j\u03c9_0$, and unknown $p^d_0$ and $q^d_0$, and load buses i for which $p^d_i$ and $q^d_i$ are known, while voltages $\u03bc_i + j\u03c9_i$ are unknown.\nInput features and output labels of the datasets are represented as x and \u1ef9, respectively, where x includes ($(p^d_i,q^d_i)$) and \u1ef9 includes ($[(\u03bc_i, \u03c9_i)]$) obtained from the Newton-Raphson numerical method (NR) for all load buses i. The PandaPower Python package [39] is used to perform NR and generate the datasets. Note that PandaPower specifies the state variables of power systems, i.e., [$[\u03b4 \u03c5]^T$], that is, there is a need for converting the state variables to $\u00b5_i = v_i\\cos\u03b4_i$ and $\u03c9_i = v_i\\sin\u03b4_i$ to yield the output labels y = {$[(\u03bc_i, \u03c9_i) : i = 1, 2, ..., n]$}."}, {"title": "A. Model Setup", "content": "A systematic approach is employed to generate the datasets. Considering $p^d_i$ and $q^d_i$ known for a specific scenario of the test system, $s_i = \\sqrt{(p^d_i)^2 + (q^d_i)^2}$, and $pf_i = p^d_i/s_i$ are computed for each bus i. Here, $s_a$ is the mean, and a deviation of 30% from $s_a$ is the standard deviation to develop a normal distribution of size 5000 for each bus i, i.e., $S_i ~ N(s_a, 0.3)$. For $S_i \u2208 {\\{s_{ij} : j = 1,2,...,5000\\}}$, $paj = s_{aj} \u00d7 pf_i$ and $qaj = \\sqrt{(saj)^2 \u2013 (paj)^2}$ are then computed for all buses i and all samples j.\nThis approach results in a pool of 5000 scenarios from which the data points are randomly selected as the dataset. The datasets contain different numbers of data points: 256, 512, 1024, and 2048 for the 4-bus, 15-bus, 290-bus, and 2224-bus test systems, respectively. Note that the number of data points is deliberately limited to show the superiority of PINN4PF compared to the state-of-the-art deep learning-based approaches, e.g., NNs. Accordingly, the randomly selected data points out of 5000 generated scenarios are sent to the classical PF solver, i.e., PandaPower, to perform PF analysis. Each dataset is then split into three subsets: 40% for training, 20% for validation, and the remaining 40% for testing.\nThe training process ends after 5000 epochs for PINN4PF, MLP, and LR. The loss function used is (17), with \u03b2o = 1 and \u03b21 = 0 for MLP and LR, indicating a supervised penalty term. However, \u03b2o and \u03b21 are non-zero for PINN4PF where \u03b20 + \u03b21 = 1, indicating a combination of supervised and physical penalty terms. The activation function is ReLU (9) for MLP and the shared hidden layers of PINN4PF. The adaptive ReLU (10) is used for the separated hidden layers of PINN4PF. The Adam optimization algorithm updates the trainable parameters during the training process for PINN4PF, MLP, and LR."}, {"title": "B. Model Performance", "content": "The performance of PINN4PF, MLP, and LR is systematically evaluated based on (i) generalization ability, (ii) robustness, (iii) impact of training dataset size on generalization ability, (iv) accuracy in approximating derived PF quantities, and (v) scalability. Experiments are done using the 15-bus test system. Additional experiments are also performed using 4-bus, 290-bus, and 2224-bus test systems for scalability.\n1) Generalization ability: The mean and standard deviation of the mean squared error (MSE) for the test dataset obtained by PINN4PF and MLP are compared in Fig. 2. PINN4PF is observed to achieve up to 85% and 65% lower maximum testing MSE for the voltage magnitude [$[V^2]$] and voltage phase angle [$[rad^2]$], respectively, compared to MLP.\n2) Robustness: The robustness of PINN4PF, MLP, and LR is evaluated using the 15-bus test system under varying noise levels. Controlled noise levels ranging from 0% to 10% are introduced to both input features and output labels of the training dataset. The corrupted vectors are defined as $x\u2019 = x \u00b1 r_x$ and $y\u2019 = y \u00b1 r_y$, where x and y are vectors of random values between 0 and 1, and 0 and 0.1, respectively. Fig. 3 illustrates the changes in the MSE for the voltage magnitude [$[V^2]$] based on the testing dataset with varying noise levels for MLP. The comparison is made relative to a constant curve, representing the MSE obtained for PINN4PF using the highest noise level, i.e., 10%. At the 0% noise level, MLP outperforms PINN4PF trained with 10% noisy data by approximately 18%. However, as the noise level increases, the performance of MLP performance deteriorates significantly, with MSE increasing up to six times. LR is excluded from the graph as its MSE at different noise levels is two orders of magnitude higher than that of PINN4PF. In addition, LR shows limited improvement with increasing dataset sizes and consistently underperforms compared to both PINN4PF and MLP.\n3) Training dataset size: The impact of the size of the training dataset on the performance of MLP is investigated using the 15-bus test system. Fig. 4 illustrates the changes in the MSE for the voltage magnitude [$[V^2]$] based on the testing dataset with varying training dataset sizes. The comparison is made relative to a constant curve representing the MSE obtained for PINN4PF using 256 training data points. It is observed that MLP requires a training dataset twice as large as that of PINN4PF to achieve a still inferior performance. However, the performance of MLP improves with more training data, reaching a comparable level with a dataset four times larger than that of PINN4PF. This indicates that PINN4PF is more data-efficient. LR is not included in the graph as its MSE with different training dataset sizes is two orders of magnitude larger than that of PINN4PF. Additionally, LR shows limited improvement with increasing training dataset sizes and consistently performs worse than PINN4PF and MLP. This indicates that the capacity of LR to capture complex relationships in PF analysis is significantly lower, and increasing the amount of training data is insufficient to bridge the performance gap.\n4) Accuracy of derived power flow quantities: For derived physical quantities, i.e., line current, line active power, and line reactive power, the mean and standard deviation of the testing MSE obtained by PINN4PF and MLP are computed and compared in Fig. 5. It is also observed that PINN4PF achieves up to 81%, 63%, and 66% lower maximum testing MSE for the line current [$[A^2]$], line active power [$[W^2]$], and line reactive power [$[VAR^2]$], respectively, compared to the MLP.\n5) Scalability: The experiments are extended to compare the performance of PINN4PF, MLP, and LR across different test system sizes: 4-bus, 15-bus, 290-bus, and 2224-bus test systems. The maximum testing MSE for the direct physical quantities obtained for all test system sizes is presented in Table II. Consequently, PINN4PF significantly outperforms MLP and LR. Fig. 6 compares the performance of PINN4PF and MLP in terms of direct physical quantities under extreme conditions for the 2224-bus test system. PINN4PF achieves a maximum MSE [$[V^2]$] 50% lower than MLP for this test system. This improvement is crucial for power system operations, particularly during unexpected events that cause deviations in v from the nominal value. The analysis also considers the performance of PINN4PF, MLP, and LR in approximating derived physical quantities. The maximum testing MSE for all test system sizes is presented in Table III. The table demonstrates the superiority of PINN4PF over MLP and LR across all test system sizes. For the 2224-bus test system, for example, the maximum MSE [$[A^2]$] obtained by PINN4PF is respectively 92% and 98% lower than MLP and LR. In addition, as the size of the test system increases, the performance gap between PINN4PF, MLP, and LR becomes more pronounced. Fig. 7 compares the performance of PINN4PF and MLP under extreme conditions in the 2224-bus test system for derived physical quantity. It should be noted that while the testing MSE for the direct quantities, i.e., voltage magnitude and phase angle, shows marginal differences between PINN4PF and MLP, PINN4PF significantly outperforms MLP in terms of the derived physical quantities, such as line current."}, {"title": "C. Model Sensitivity Analysis", "content": "The architecture of PINN4PF and MLP, including the number of hidden layers and neurons per hidden layer, as well as hyperparameters, including the learning rate, the weight decay rate, and the percentage of dropout, are determined by sensitivity analysis to ensure a fair comparison\u00b9. The maximum weight for the share of the supervised penalty term (\u03b20) in (17) is also fine-tuned for PINN4PF. It should be noted that for the MLP, \u03b2o = 1 and \u03b21 = 0. Sensitivity analysis is not performed for the LR since it involves linear combinations and lacks hyperparameters. The optimal configurations are identified based on minimizing both training and testing MSE.\n1) Sensitivity analysis for PINN4PF: We systematically assess the influence of the number of hidden layers, ranging from one to four, within the shared set of hidden layers and the individual heads. We evaluate distinct configurations where the shared hidden layers contain n \u00d7 2 neurons, and each head comprises n neurons. Learning and weight decay rates are varied from 1 \u00d7 10\u00af\u00b9 to 1 \u00d7 10-10, and dropout rates are varied from 0% to 2%. Sensitivity analysis reveals that the selected configuration for PINN4PF consists of two shared hidden layers alongside four hidden layers per head. In addition, PINN4PF is trained with a learning rate of 1.3\u00d710-4, a weight decay of 1.1 \u00d7 10\u22125, a dropout rate of 0.1%, and a batch size of 16. The maximum weight for the share of the supervised penalty term is \u03b2\u2081 = 0.71. For this configuration, the achieved training and testing MSE are 5.64 \u00d7 10-6[$[V^2]$] and 5.73 \u00d7 10-6[$[V^2]$], respectively.\n3) Impact of modifications on the performance of PINN4P: We evaluate the following configurations: (i) a double-head feed-forward NN with ReLU (9) and supervised penalty term (ReLU&Supervised), (ii) a double-head feed-forward NN with adaptive ReLU (10) and supervised penalty term (AdaptiveReLU&Supervised), (iii) a double-head feed-forward NN with ReLU (9) and physical penalty term (ReLU&Physical), and (iv) a double-head feed-forward NN with adaptive ReLU (10) and physical penalty term (AdaptiveReLU&Physical) that is the selected configuration for PINN4PF. Table IV presents a comparative analysis of their performance in terms of direct physical quantities."}, {"title": "VI. DISCUSSION", "content": "The results of this study demonstrate the effectiveness and robustness of the proposed PINN4PF architecture in power flow analysis. Several key points and observations arise from the findings:\n\u2022 The optimization algorithm suggests that for larger test systems, the share of the supervised penalty term (\u03b20) in the loss function should be significantly smaller than the physical penalty term (\u03b21). This indicates the increasing importance of physical constraints in larger test systems.\n\u2022 Although only the diagonal elements of the admittance matrix are used to develop the loss function, including a physical penalty term imposes additional calculations during training, which makes PINN4PF computationally more expensive and highlights a trade-off between accuracy and computational cost.\n\u2022 Additional testing on real-world power systems and incorporating other physical constraints could further validate and improve the model's performance.\n\u2022 In modern power systems, it is expected to have a lot of missing data or no measurements. Traditional PF solvers, such as NR, could not converge to high-quality solutions. Nevertheless, this study demonstrates that PINN4PF is not only more robust to noisy data compared to class. Still, it can also learn the complex dynamics of power systems from fewer training samples. These findings highlight the importance of physics-informed components in power system research."}, {"title": "VII. CONCLUSION", "content": "This study presents an end-to-end architecture for deep learning-based power flow (PF) analysis called PINN4PF. PINN4PF includes three significant modifications: a double-head feed-forward neural network, an adaptive activation function, and a physics-based loss function. PINN4PF, therefore, offers a straightforward yet effective approach to capturing the complexities inherent in large-scale modern power systems. The application of PINN4PF to four test power systems, including 4-bus, 15-bus, 290-bus, and 2224-bus systems, rigorously evaluates its performance against two baselines: a linear regression model (LR) and a black-box neural network (MLP). The results highlight PINN4PF's exceptional generalization ability, robustness against noise, data efficiency, and scalability to large-scale power systems. Specifically, PINN4PF consistently achieves lower mean squared errors for direct and derived physical quantities, proving its effectiveness and reliability. In addition, as the test system size increases, the performance gap between PINN4PF, MLP, and LR becomes more pronounced. This advancement is crucial for enhancing power system operations, especially under extreme conditions and unexpected deviations, making PINN4PF a promising solution for future power flow analysis and optimization tasks."}]}