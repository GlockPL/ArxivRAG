{"title": "SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model", "authors": ["Zhenglin Huang", "Jinwei Hu", "Xiangtai Li", "Yiwei He", "Xingyu Zhao", "Bei Peng", "Baoyuan Wu", "Xiaowei Huang", "Guangliang Cheng"], "abstract": "The rapid advancement of generative models in creating highly realistic images poses substantial risks for misinformation dissemination. For instance, a synthetic image, when shared on social media, can mislead extensive audiences and erode trust in digital content, resulting in severe repercussions. Despite some progress, academia has not yet created a large and diversified deepfake detection dataset for social media, nor has it devised an effective solution to address this issue. In this paper, we introduce the Social media Image Detection dataSet (SID-Set), which offers three key advantages: (1) extensive volume, featuring 300K AI-generated/tampered and authentic images with comprehensive annotations, (2) broad diversity, encompassing fully synthetic and tampered images across various classes, and (3) elevated realism, with images that are predominantly indistinguishable from genuine ones through mere visual inspection. Furthermore, leveraging the exceptional capabilities of large multimodal models, we propose a new image deepfake detection, localization, and explanation framework, named SIDA (Social media Image Detection, localization, and explanation Assistant). SIDA not only discerns the authenticity of images, but also delineates tampered regions through mask prediction and provides textual explanations of the model's judgment criteria. Compared with state-of-the-art deepfake detection models on SID-Set and other benchmarks, extensive experiments demonstrate that SIDA achieves superior performance among diversified settings. The code, model, and dataset will be released.", "sections": [{"title": "1. Introduction", "content": "Recent advances in generative AI [9, 59, 78] have significantly improved the ability to generate highly realistic images, making it easier to create content that closely resembles real-world events. However, these advancements also bring new risks of malicious misuse, particularly in creating deceptive content aimed at misleading public opinion or distorting historical records. Such concerns have motivated the computer vision community to develop more sophisticated deepfake detection techniques. Contemporary methods [34, 73] primarily focus on assessing the authenticity of facial images (i.e., real or fake), while an emerging subset aims to detect and localize facial manipulations [21, 22]. These methods are typically trained on datasets containing real and fake images, aiming to detect images as real or fake, or to localize the tampered regions. Consequently, the quality and diversity of the datasets used for training and evaluation play a crucial role in achieving high accuracy in deepfake detection and localization. A well-curated dataset can enable models to learn nuanced features, improving robustness and generalization in real-world scenarios.\nHowever, existing deepfake detection and localization datasets face two main challenges: 1) Insufficient Diversity. The majority of existing datasets for deepfake detection focus mainly on facial imagery [7, 24]. However, given the growing capabilities of generative AI, the issue of non-facial image falsification on social media cannot be overlooked. While researchers [68, 89] have developed relatively large datasets based on ImageNet for image deepfake detection, these datasets typically consist of images from simple scenarios that do not specifically focus on social media. Additionally, they often utilize somewhat outdated image-generation techniques, which can result in less convincing forgeries that are easier for both humans and models to detect. Currently, there is a substantial lack of large-scale image deepfake datasets specifically designed for social media that leverage the latest generative methods. 2) Limited Comprehensiveness. Existing datasets are typically suited either for deepfake detection or for tampered region localization [21\u201323, 76], focusing on specific types of generative methods or image manipulations. However, an ideal deep-fake dataset should encompass a wide range of scenarios to reflect the complexity of real social media content, where fake images may be fully generated or manipulated through image editing strategies [78]. Furthermore, most existing datasets primarily focus on binary real/fake classification or tampered region localization, with limited emphasis on explaining the cues that models use to make these decisions.\nTo address these challenges, we introduce the Social media Image Detection dataSet (SID-Set), which consists of 300K images (i.e., 100K real, 100K synthetic, and 100K tampered images), providing a comprehensive resource for the deepfake detection community. Additionally, we include textual descriptions explaining the model's judgment basis. As shown in Figures 2 and 4, synthetic and tampered images are indistinguishable to humans. In particular, challenges for the SID-Set include: 1) subtle alterations of just dozens of pixels; 2) natural-looking local manipulations; 3) complex scenes in datasets. To our knowledge, SID-Set is the first dataset of its scale with extensive annotations, making it the largest and most comprehensive dataset for social media deepfake detection to date. Compared to existing datasets in Table 1, SID-Set addresses the challenges of limited diversity and outdated generative techniques by providing a more comprehensive set of high-quality and diverse images. Accordingly, we propose a new VLMs-based deepfake detection framework, named the Social media Image Detection, localization, and explanation Assistant (SIDA), which achieves the state-of-the-art (SOTA) performance on SID-Set and generalizes effectively across other benchmarks. SIDA can serve as a baseline model on SID-Set, offering a new framework for tackling social media image deepfake detection and localization.\nThe main contributions of this paper are as follows:\n\u2022 We establish SID-Set, a comprehensive benchmark for detecting, localizing, and explaining deepfakes in social media images, featuring multiple image types and extensive annotations. SID-Set holds the potential for advancing the field of deepfake detection and ensuring robust model performance in complex real-world scenarios.\n\u2022 We propose SIDA, a new image deepfake detection, localization, and explanation framework that not only detects images with high accuracy but also localizes and explains potential manipulations, enhancing the transparency and utility of deepfake detection technologies.\n\u2022 Extensive experiments demonstrate that SIDA effectively identifies and delineates tampered areas within images, supporting the development of more robust and interpretable deepfake detection systems. Notably, SIDA demonstrates superior or equivalent performance on the SID-Set and other benchmarks."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Image Deepfake Datasets", "content": "In the realm of deepfake detection, the primary focus has historically centered on the identification of facial deepfakes. Renowned datasets such as ForgeryNet [24], Deep-FakeFace [60], and DFFD [7] have been pivotal in this area. As the field evolves, there is a growing shift among researchers towards exploring non-facial deepfake data. Advanced methodologies involving text-to-image or image-to-image generation techniques [79], utilizing GANs or the stable diffusion series, have facilitated the creation of expansive deepfake datasets like GenImage [89], HiFi-IFDL [21], and DiffForensics [68]. These datasets are characterized by their enlarged data volumes, diversified generation methodologies, and enriched annotation details. Furthermore, beyond the conventional real/fake annotations, certain datasets [21, 22, 76] now include more granular annotations. Table 1 delineates a detailed comparison among various deepfake datasets, highlighting key differences in generation scenarios and annotation types supported. It shows that SID-Set is particularly tailored towards social media data, incorporating the latest SOTA generation models, emphasizing high-quality production, and providing much more comprehensive and diverse annotations."}, {"title": "2.2. Image Deepfake Detection and Localization", "content": "Deepfake detection methods [4, 6, 40, 51, 64] are typically approached as classification tasks within the data-driven paradigm. These strategies primarily leverage diverse architectures [40, 51], including Convolutional Neural Networks (CNNs) and Transformers, to detect distinctive artifacts. Some scholars have attempted to achieve relatively high precision and generality by employing strategies such as employing techniques such as data augmentation [64], adversarial training [6], reconstruction [4], etc. On the other hand, some researchers [26, 62] have explored extracting features from the frequency domain for deepfake identification. Efforts [13, 67] have also been made to fuse features from both spatial and frequency domains to obtain a more comprehensive set of discriminative features for deepfake detection. Although these methods have shown some progress, they still struggle with issues of generalization. Furthermore, some scholars [20, 42, 46, 63, 83, 86] have gone beyond the basic classification between real and fake by gradually constructing datasets annotated with masks of locally tampered areas, thereby addressing both image deepfake detection and localization tasks. However, these datasets are concentrated mainly on facial data, with fewer datasets available for non-facial deepfake detection and localization, and even fewer large and public datasets for realistic social media data. As a result, our work aims to address these critical gaps by providing a new, comprehensive dataset that includes diverse manipulations beyond facial data, particularly focusing on social media images."}, {"title": "2.3. Large Multimodal Models", "content": "The progress in large language models (LLMs) [14\u201316, 43] and vision-language models (VLMs) [14\u201316, 30, 74, 81] has notably improved multimodal comprehension, seamlessly integrating visual and textual data. The LLaMA series [14\u201316] optimize language understanding with a compact, high-performance design using fewer parameters than prior models. LLaVA series [36, 37] enhance visual question answering by synchronizing visual features with textual data. Additionally, LISA series [30, 33, 70, 74] employ LLM for accurate image segmentation, merging visual perception with linguistic insights to precisely segment the targeted areas. Several grounding large multimodal models [52, 56, 57, 69, 71, 75, 77, 80, 84] have been proposed to localize the contents based on linguistic information. Advancements [5, 10, 19, 72, 85] in integrating multimodal data such as visual and linguistic information have also significantly improved the performance of models in detecting and pinpointing deepfakes. For instance, AntifakePrompt [5] approaches deepfake detection by formulating it as a visual question answering problem, which adjusts soft prompts for InstructBLIP [10], enabling it to determine whether a query image is real or fake. ForgeryGPT [32] enhances image forgery detection and localization by integrating advanced forensic knowledge with a mask-aware forgery extractor that targets pixel-level fraud detection. As a concurrent work, FakeShield [72] leverages the capabilities of LLaVA to identify and localize altered regions while providing interpretable insights into the findings. Our method diverges significantly from existing approaches by creating the most extensive deepfake dataset tailored specifically for social media images with comprehensive annotations. Additionally, we also set a new standard for social media image deepfake detection by utilizing the visual interpretation strengths of VLMs to boost detection accuracy, pinpoint forgeries, and provide clearer explanations within a comprehensive framework."}, {"title": "3. BenchMark", "content": ""}, {"title": "3.1. Motivation", "content": "In the realm of deepfake detection and localization, research has predominantly focused on facial deepfakes due to their substantial societal impact. However, with the advancements in generative technology, the scope of deepfakes has extended beyond facial content to include non-facial manipulations. Historically, non-facial deepfakes were less prevalent, largely limited by technological constraints that produced low-quality, easily detectable forgeries. Although some datasets, such as GenImage [89] and AIGCD [87], have been constructed, they suffer from several limitations: 1) They often utilize relatively outdated generative technologies, resulting in lower quality data easily distinguished by humans. 2) They primarily focus on text-to-image or image-to-image generation, neglecting the need for data involving manipulations of specific regions, objects, or parts. Such tampered manipulations can be especially insidious as they introduce subtle misinformation, making existing SOTA deepfake detection methods less effective. 3) They lack well-defined criteria for content authenticity, limiting their effectiveness in providing interpretative insights and educating the public on distinguishing synthetic content.\nAddressing these limitations is crucial for improving the transparency and utility of deepfake detection systems. Furthermore, most existing datasets emphasize either fully synthetic or tampered images, as shown in Table 1, but in real applications, we don't know the image deepfake type in advance. Effective deepfake detection and localization methods should be capable of addressing both scenarios, as social media images often involve complex combinations of synthetic and tampered content. Therefore, developing a comprehensive benchmark for detecting and localizing deepfakes in social media images is essential. We propose SID-Set, which encompasses comprehensive, high-quality annotations for detection and localization, along with detailed textual explanations of the judgment criteria."}, {"title": "3.2. Benchmark Construction", "content": "Data Details. To develop an effective benchmark for detecting and localizing images on social media, we created SID-Set, a dataset with real, synthetic, and tampered images reflecting diverse real-world scenarios. Our benchmark assesses whether models can differentiate among real, synthetic, and tampered images, as well as accurately identify altered regions in tampered images.\nReal Images: 100K images from OpenImages V7\u00b9, with a wide range of scenarios reflecting real-world diversity.\nSynthetic Images: 100K images generated through FLUX [45], specifically designed to challenge identification due to their high-quality, highly realistic appearance.\nTampered Images: 100K tampered images, with specific objects or regions replaced or altered; the detailed generation process is shown in Figure 4.\nData Generation. To generate highly realistic synthetic images, we experimented with several open-source SOTA generative models, such as FLUX [45], Kandinsky 3.0 [1], SDXL [54], AbsoluteReality [44], and others. Following a review by five human experts of 1,000 images from each generative model, FLUX emerged as the top performer, producing highly convincing images that were indistinguishable from real ones to human experts. Consequently, we employed FLUX to create 100K synthetic images based on the Flickr30k [53] and COCO [35]. The image tampering process depicted in Figure 4 follows four distinct stages, utilizing the COCO image as an example.\nStage 1: We extract objects from an image's caption using GPT-40 [49]. For instance, from the caption \"A large fluffy cat laying on top of a wooden table\", GPT-40 identifies relevant COCO class objects or retains nouns if no match exists. This extraction is documented in an \u201cImage-Caption-Object\u201d JSON file.\nStage 2: Employing Language-SAM [31], we generate masks for identified objects as training ground truth.\nStage 3: We establish dictionaries for full and partial image tampering using COCO classes for object replacement and attribute modifications, respectively. For example, replacing \u201cdog\u201d with animals like \u201ccat\u201d or adding attributes such as \u201chappy\u201d or \u201cangry\u201d to the \u201cdog\u201d class. For more details, please refer to the Appendix.\nStage 4: Utilizing Latent Diffusion [58], we modify captions and regenerate images, either replacing or retaining original objects based on availability. An example modification is altering \u201ccat\u201d to \u201cdog\u201d in the image caption."}, {"title": "4. Method", "content": "In this section, we first present the model architecture of SIDA in Section 4.1, followed by an introduction to the training process of our method in Section 4.2."}, {"title": "4.1. Architecture", "content": "Large vision-language models have demonstrated remarkable capabilities in understanding the alignment between textual and visual information. For instance, LLaVA [36] leverages language alone to achieve a comprehensive understanding of both visual and linguistic content. Building on LLaVA, LISA [30] extends this capability by providing fine-grained segmentation masks along with corresponding textual descriptions. However, to effectively detect and localize synthetic images, VLMs must not only be capable of multimodal understanding but also possess the ability to identify and segment manipulated regions, providing detailed explanations for both synthetic and tampered images.\nTo this end, we propose SIDA to tackle the task of synthetic image detection and tampered region localization. The pipeline of our method is illustrated in Figure 5. Inspired by previous approaches [29, 30, 88], we expand the original vocabulary of VLMs by adding two new tokens, <DET> and <SEG>, to enable the model to extract detection and segmentation information. Given an image $x_i$ and a text prompt $x_t$, such as \"Can you identify if this image is real, fully synthetic, or tampered? Please mask the tampered object/part if it is tampered.\u201d We feed them into the VLM. The VLM then outputs a text description $\\hat{y}_{des}$, while the last hidden layer $h_{hid}$ contain the <DET> and <SEG> tokens. This process can be formulated as follows:\n$\\hat{y}_{des} = VLM(x_i, x_t)$.\nNext, we extract the <DET> token from the last hidden layer $h_{hid}$ to obtain $h_{det}$. The representation $h_{det}$ is then passed through a detection head $F_{det}$ to determine whether the image is real, fully synthetic, or tampered. We denote the final detection result by $D$:\n$D = F_{det}(h_{det})$,\n$F_{det}$ is the detection head that processes the extracted <DET> representation $h_{det}$ to produce the detection output. If the detection result indicates that the image has been tampered with, SIDA will then predict a mask for the tampered regions. The $h_{seg}$ feature is extracted from the hidden layer $h_{hid}$, similar to the extraction process for the <DET> token. Given that the <DET> token encapsulates crucial"}, {"title": "4.2. Training", "content": "Training Objectives. The training loss, $\\mathcal{L}$, for the SIDA consists of three components: the detection loss $\\mathcal{L}_{det}$, the text generation loss $\\mathcal{L}_{txt}$, and the segmentation mask loss $\\mathcal{L}_{mask}$. Initially, SIDA is trained in an end-to-end manner by employing the detection loss and the segmentation loss. For detection, we use CrossEntropy loss, while for the segmentation task, we use a weighted combination of binary cross-entropy (BCE) and DICE loss, with respective loss weights $\\Lambda_{bce}$ and $\\Lambda_{dice}$. This can be formulated as:\n$\\mathcal{L} = \\Lambda_{det}\\mathcal{L}_{det} + \\Lambda_{mask}\\mathcal{L}_{mask}$,\n$\\mathcal{L}_{det} = \\mathcal{L}_{CE}(D, D)$,\n$\\mathcal{L}_{mask} = \\Lambda_{bce}\\mathcal{L}_{BCE}(M, M) + \\Lambda_{dice}\\mathcal{L}_{DICE}(M, M)$.\nAfter completing the training phase, we proceed to fine-tune the SIDA model by utilizing detailed textural descriptions from 3,000 images as the ground truth, represented by $Y_{des}$. This phase focuses on optimizing the text generation component, $\\mathcal{L}_{txt}$, to improve its ability in textural interpretability. The final loss function is as follows:\n$\\mathcal{L}_{txt} = \\mathcal{L}_{CE}(Y_{des}, \\hat{Y}_{des})$,\n$\\mathcal{L}_{total} = \\Lambda_{det}\\mathcal{L}_{det} + \\Lambda_{mask}\\mathcal{L}_{mask} + \\Lambda_{txt}\\mathcal{L}_{txt}$,\nwhere $\\Lambda_{det}$, $\\Lambda_{mask}$, and $\\Lambda_{txt}$ are the weighting factors that balance the contributions of the detection, segmentation, and text generation losses, respectively.\nTraining Data. We utilize the SID-Set, consisting of 300k images, to train SIDA. To further enhance diversity, we incorporate the MagicBrush dataset [82] after filtering out low-quality images. The combined dataset supports robust training for both the detection and localization of synthetic content. Additionally, we generate descriptions for 3,000 randomly selected images using LLMs."}, {"title": "5. Experiments", "content": "Implementation Details. We choose LISA as the base large vision language model due to its strong capability for reasoning-based localization. We fine-tuned both LISA-7B-v1 and LISA-13B-v1 on the SID-Set using LoRA, setting $\\alpha$ at 16 and the dropout rate at 0.05. The input images are resized to 1024 \u00d7 1024. The loss weights in Eq. (6) for the detection ($\\Lambda_{det}$), the text generation ($\\Lambda_{txt}$), and the localization ($\\Lambda_{mask}$) are set to 1.0, respectively. The localization loss weights in Eq. (5) for the $\\Lambda_{bce}$ and $\\Lambda_{dice}$ are set to 2.0 and 0.5, respectively. To determine the optimal weight configuration, we perform ablation studies as detailed in Section 5.5. During the detection and localization training stage, the image encoder is frozen, and all other modules are trainable. For the text generation stage, only vision-language models are fine-tuned using the LoRA strategy. The initial learning rate is set to 1 \u00d7 10\u22124, with a batch size of 2 per device and a gradient accumulation step of 10. We use two NVIDIA A100 GPUs (40GB each). Training for SIDA-7B and SIDA-13B took 48 hours and 72 hours, respectively.\nEvaluation Metrics. We evaluate detection using image-level accuracy and F1 scores. For forgery localization, our metrics include Area Under the Curve (AUC), F1 scores, and Intersection over Union (IoU)."}, {"title": "5.1. Detection Evaluation", "content": "We compare SIDA against other SOTA deepfake detection methods on SID-Set, including CnnSpot [17], AntifakePrompt [5], FreDect [18], Fusing [27], Gram-Net [39], UnivFD [48], LGrad [61], and LNP [2]. To ensure a fair comparison, we first evaluate these models on our dataset using their original pre-trained weights, then retrain them with the SID-Set to assess performance improvements. Table 2 demonstrates that SIDA achieves better or comparable results among all the evaluated methods. Notably, LGrad [61] achieves the highest accuracy and F1 score on tampered images after retraining, but this comes at the expense of lower performance in other metrics. Our analysis indicates that LGrad's high recall and false positive rates stem from its propensity to misclassify other types as tampered. The training details are provided in the Appendix."}, {"title": "5.2. Localization Results", "content": "Table 3 presents the forgery localization performance on the SID-Set. We selected PSCC-Net [38], MVSS-Net [11], and HIFI-Net [21] as representative IFDL methods. Additionally, we chose LISA [30] as a representative LLM due to its segmentation reasoning capabilities. We used LISA-7B-v1 and fine-tuned it on SID-Set. The results indicate that SIDA achieves the best performance. We suppose that while LISA possesses strong general segmentation capabilities, it lacks the specific specialized features required to detect subtle manipulations, ultimately limiting the effectiveness of fine-tuning for precise forgery localization."}, {"title": "5.3. Robustness Study", "content": "We further evaluate the robustness of SIDA against common image perturbations found in social media, such as JPEG compression, resizing, and Gaussian noise. Table 4 shows our model's performance on the SID-Set under six degradation scenarios: JPEG compression (with quality levels of 70 and 80), resizing (with scaling factors of 0.5 and 0.75), and Gaussian noise (with variances of 5 and 10). Despite not being explicitly trained on degraded data, SIDA demonstrates resilience to these low-level distortions. The model's stable performance against common social media perturbations highlights its robustness and practical applicability."}, {"title": "5.4. Test on Other Benchmark", "content": "In this stage, we evaluate SIDA on DMimage [8] dataset to assess its generalization capabilities. We compare SIDA with CNNSpot [17], Fusing [27], Gram-Net [39], LNP [2], UnivFD [48], and AntifakePrompt [5]. For these methods, we use the original hyperparameter settings and pre-trained weights provided by the authors. The results in Table 5 show that SIDA achieves superior performance, demonstrating its strong adaptability."}, {"title": "5.5. Ablation Study", "content": "Attention Module. We conducted ablation experiments to assess the importance of the attention module. Variants included removing the attention module entirely and replacing it with fully connected (FC) layers. Results in Table 6 show that removing the attention module or replacing it with FC layers significantly reduces performance, underscoring the critical role of attention in enhancing feature interaction and improving detection and localization accuracy.\nTraining Weights. SIDA training utilizes weighted losses to balance task contributions. In the detection and localization stages, detection loss is adjusted by a weight $\\Lambda_{det}$, and localization loss by binary cross-entropy (BCE) and DICE losses, with weights $\\Lambda_{bce}$ and $\\Lambda_{dice}$ respectively. For our experiments, we set $\\Lambda_{det}$ to 1, $\\Lambda_{bce}$ to 2.0, and $\\Lambda_{dice}$ to 0.5 to maintain balance between detection and localization, enhancing model stability and performance. We summarize the outcomes of various weight configurations in Table 7."}, {"title": "5.6. Qualitative Results", "content": "In this section, we present examples of SIDA's output for tampered images, showcasing its detection, localization, and explanation capabilities. SIDA accurately identifies tampered regions and provides explanations for its decisions. We also include some challenging failure cases where SIDA was unable to accurately detect the tampered regions, highlighting areas for future improvement. Additional visualization results are available in the Appendix."}, {"title": "6. Conclusion and Discussions", "content": "In this work, we present SID-Set for social image deepfake detection, localization, and explanation tasks, consisting of 100k real images, 100k fully synthetic images, and 100k tampered images. Furthermore, we propose a new VLMs-based deepfake detection framework, SIDA, to address these tasks. SIDA demonstrates its ability to detect fake types, localize tampered regions, and provide explanations for its decisions. We believe that the integration of VLMs into deepfake detection tasks, as demonstrated by SIDA, offers promising new avenues for future research in this critical field.\nAlthough the development of the SID-Set and the introduction of the SIDA framework have yielded favorable outcomes in deepfake detection tasks, we recognize some potential limitations and will optimize them in future research. Dataset Size. While SID-Set includes 100k fully synthetic and 100k tampered images, the complexity of real social media environments demands a larger dataset. Therefore, expanding the dataset with additional images is a crucial objective for future research."}, {"title": "7. Appendix", "content": "Contents of the Appendices:\nSection A. Details of Experimental Settings, Hyperparameters, and Configurations.\nSection B. Detailed Comparison with Related Work.\nSection C. Additional Visual Examples, Including Failures and Failure Analysis of SIDA.\nSection D. Detailed DataSet Creation Process."}, {"title": "A. Experiment Settings", "content": "Detection Methods. We used AIGCDetecBenchmark\u00b2 GitHub to test and re-train CnnSpot [17], FreDect [18], Fusing [27], Gram-Net [39], UnivFD [48], LGrad [61], and LNP [2]. For AntifakePrompt [5], we used the original training settings provided in the official GitHub repository\u00b3.\nDuring testing and training, we used only classification labels for these detection methods, as they cannot handle localization tasks.\nWe set noise (e.g. JPEG compression, blur, and resize) to None for testing each approach. For CNNSpot [17], FreDect [18], Fusing [27], and Gram-Net [39], we retrained them with the following hyperparameters: a blur probability of 0.1 with a sigma range of 0.0 to 3.0, a JPEG compression probability of 0.1, and JPEG quality ranging from 30 to 100. We used a batch size of 64, a crop size of 224, and Adam as the optimizer. We used different hyperparameters to achieve the best results for LGrad [61], LNP [2], and UnivFD [48], which require image pre-processing. Specifically, for LNP and LGrad, both the blur probability and JPEG compression probability were set to 0. For UnivFD, we used the same training settings as CNNSpot after pre-processing. For AntifakePrompt, we used the same hyperparameters and prompts as described in the original paper, recording and calculating performance across different classes in the results. All methods were trained for 10 epochs on a single NVIDIA A100 40GB GPU. Methods that did not require image pre-processing took approximately 36 hours to train, while LGrad, LNP, and UnivFD, which needed pre-processing, took around 48 hours.\nLocalization Methods. We used the pre-trained models for MVSS-Net [11] and HIFI-Net [21] to evaluate performance on SID-Set. For PSCC-Net [38], we used the same training settings as provided in the official GitHub repository4. For LISA [30], we used the LISA-7B-v1 version and fine-tuned it on SID-Set for comparison. Specifically, we set the learning rate to 0.0001, the batch size to 2, and the gradient accumulation steps to 10."}, {"title": "B. Detailed Comparison", "content": "Due to page limitations, we selected only a few representative works for the main comparison. In this section, we present a more comprehensive comparison of SIDA with additional related works, as shown in Table 8.\nCompared to detection methods [2, 5, 17, 18, 27, 39, 48, 61, 68], which often specialize in identifying specific generative techniques, SIDA is designed with a broader focus, capable of handling various manipulation types. This versatility allows SIDA to generalize better across different datasets and manipulations, making it more effective in real-world scenarios. Additionally, SIDA provides both detection and localization, offering a more comprehensive solution compared to detection-only models.\nCompared to existing IFDL (Image Forgery Detection and Localization) methods [11, 21, 38], which primarily focus on detecting tampered versus real images, SIDA is capable of handling a broader range of scenarios, including fully synthetic, tampered, and real images. This allows SIDA to provide a more comprehensive detection capability. Furthermore, SIDA leverages LLMs to enhance the interpretability of its localization results, delivering not only segmentation masks but also detailed explanations. This combination improves precision and adds a valuable interpretative layer that existing methods lack, making it effective for understanding and addressing manipulations in complex scenarios.\nCompared to other works that have explored the use of LLMs in deepfake detection, our approach addresses multiple tasks, utilizes a larger dataset, and produces fine-grained outputs. For example, compared with FFAA [25], SID-Set is not limited to facial deepfake detection but is designed to tackle more complex scenarios commonly found on social media, such as object manipulation and partial tampering. Compared with FakeShield [72], our work not only includes more realistic images but also provides detailed fine-grained results, thereby enhancing detection accuracy and interpretability. Additionally, compared to ForgeryGPT [32], we have curated a large-scale, high-quality dataset that serves as a valuable resource to support and advance research in this domain.\nSince some works have not released their code until paper submission [25, 32, 72], we chose PSCC-Net [38] and LISA [30] to demonstrate localization results due to their effective segmentation capabilities. We retrained both models on SID-Set for 10 epochs and obtained the output results. Compared to these methods, SIDA shows superior performance in detecting the borders of tampered areas, delivering more precise and clearer results, as illustrated in Figure 8."}, {"title": "C. Additional Visual Examples", "content": "In this section, we provide additional visual examples of SIDA. Figures 9 and 10 depict SIDA's outputs for tampered images, while Figure 11 highlights some failure cases.\nThe first row in Figure 11 illustrates instances where SIDA fails to detect tampered areas, with some cases resulting in no mask output at all. The second row demonstrates SIDA's inability to generate fine-grained masks for the tampered regions. We attribute these shortcomings to two primary factors. First, the current training data for tampered images may be insufficient. Although SID-Set provides 100k tampered images, this volume might still be inadequate for the LLM to effectively handle highly detailed and complex manipulations. Second, although SIDA surpasses other methods in detecting tampered regions, it may still lack the precision required for particularly challenging cases involving subtle or intricate tampering. These limitations indicate critical areas for future research. We aim to improve both the quality and quantity of training data, while also developing more sophisticated methodologies and enhancement strategies to better address the challenges posed by complex manipulation scenarios, ultimately enhancing detection accuracy and mask quality."}, {"title": "D. Detailed DataSet Creation Process", "content": "Prompts for Generating Descriptions. We designed prompts to generate different descriptions using GPT-40. Separate prompts were crafted for real images, fully synthetic images, and tampered images. The prompts are illustrated in Figures 12, 13, and 14.\nExamples of Generated Descriptions. we present examples of the output descriptions generated by SIDA. Cases of real images, fully synthetic images, and tampered images are shown in Figures 15, 16, and 17, respectively.\nDetails of Generative Process. We provide further details on the generation of fully synthetic and tampered images.\nFully Synthetic Images. We used FLUX\u2075 to generate fully synthetic images due to its high quality, utilizing original data from Flickr30k [53] and COCO [35]. The style prompt was set as \"cinematic photo of prompt, 35mm photograph, film, professional, 4k, highly detailed,\" while the negative prompt included terms \u201cdeformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime, text, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate, morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, blurry, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers, too many fingers, long"}]}