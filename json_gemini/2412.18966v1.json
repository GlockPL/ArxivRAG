{"title": "ModelGrow: Continual Text-to-Video Pre-training with Model Expansion and Language Understanding Enhancement", "authors": ["Zhefan Rao", "Liya Ji", "Yazhou Xing", "Runtao Liu", "Zhaoyang Liu", "Jiaxin Xie", "Ziqiao Peng", "Yingqing He", "Qifeng Chen"], "abstract": "Text-to-video (T2V) generation has gained significant attention recently. However, the costs of training a T2V model from scratch remain persistently high, and there is considerable room for improving the generation performance, especially under limited computation resources. This work explores the continual general pre-training of text-to-video models, enabling the model to \"grow\" its abilities based on a pre-trained foundation, analogous to how humans acquire new knowledge based on past experiences. There is a lack of extensive study of the continual pre-training techniques in T2V generation. In this work, we take the initial step toward exploring this task systematically and propose ModelGrow. Specifically, we break this task into two key aspects: increasing model capacity and improving semantic understanding. For model capacity, we introduce several novel techniques to expand the model size, enabling it to store new knowledge and improve generation performance. For semantic understanding, we propose a method that leverages large language models as advanced text encoders, integrating them into T2V models to enhance language comprehension and guide generation results according to detailed prompts. This approach enables the model to achieve better semantic alignment, particularly in response to complex user prompts. Extensive experiments demonstrate the effectiveness of our method across various metrics. The source code and the model of ModelGrow will be publicly available.", "sections": [{"title": "1. Introduction", "content": "Investigating general methods for continual training of a text-to-video pre-trained model is gaining more interest in generative areas. Continual general pre-training aims to enable the model to acquire knowledge from new data that is similar in the domain to the original training data while retaining the information previously learned [31]. Compared to re-training, this task benefits us by conserving significant computational resources and enhancing the foundation model's general performance by leveraging available pre-trained models in the T2V community. Existing research [31, 51] in natural language processing has indicated the success of continual pre-training, which has not been explored in T2V generation yet. In this paper, we make the first step towards continual T2V pre-training instead of achieving state-of-the-art performance.\nContinual pre-training for text-to-video generation models faces two challenges: catastrophic forgetting [12] and the need for enhanced language understanding when dealing with long and detailed prompts. First, directly fine-tuning the model on the customized dataset would lead to the model performance drop in the general domain due to catastrophic forgetting. Especially for text-to-video generation, which requires lots of computational resources and training data, we usually need to train the foundation models with multiple phases. Second, due to recaptioning [3] being commonly used in the text-to-video generation, the long prompts generated by large language models (LLMs) bring different distributions and higher requirements to generative models, leading to decreased performance of the semantics consistency, both spatially or temporally. Only utilizing the CLIP [41] or T5 [42] as the text encoder of diffusion models is not enough to understand the long prompt with detailed information. It is still an under-explored area of how to enhance language understanding with large language models under continual pretraining.\nExpanding the parameters of models could alleviate the forgetting of knowledge and increase the generation ability. LLaMA Pro [51], a block expansion method with only updates the new parameters, proves its effectiveness in natural language generation areas. Our work differs from LLaMA Pro in that we focus on the task of text-to-video generation, and we aim to provide practical and insightful guidelines for model expansion with extensive study. In the direction of LLMs enhancement, current work [26, 35] only focuses on a zero-shot method that acts as a prompt planner to increase the generation ability with LLMs. This trend will not change the denoising network architecture and thus can not increase the ability of language understanding of the generation model. We are the first work that systematically explores enhancing the text-to-video generation model both on prompt refining and language understanding improvements via incorporating the embeddings of LLMs.\nWe propose two continual pre-training methods for text-to-video generation, including model expansion and LLMs enhancement. For better utilization of the given pretrained model, we introduce an expansion of the transformer block to effectively incorporate new knowledge while minimizing the potential for forgetting previously learned information. The parameters of the new blocks are duplicated from adjacent blocks to ensure smooth and efficient training. This approach not only facilitates the incorporation of additional data but also maintains the structural integrity and performance consistency of the existing model. Furthermore, we first modify the current re-captioning method for more rich contents and incorporate the embeddings of LLMs as an extra condition in the architecture of the Diffusion Transformer. Specifically, we expand the cross-attention block for the text condition and duplicate the weights of the original cross-attention block as the initialization of the new block conditioning on LLMs. Equipped with the prompt template, another cross-attention block conditioning on LLMs embeddings significantly increases the language understanding ability and produces videos with more semantic consistency and vivid motion.\nWe conduct continual pre-training on our dataset with long prompts generated by LLaVA-NeXT [33]. We also evaluate our models on datasets, VBench [29] and the subset of CompBench [43] with quality and semantics metrics. In summary, our contributions can be summarized as follows:\n\u2022 We propose the model expansion as a continual pre-training method for the transformer-based diffusion model, enabling the incorporation of new knowledge while mitigating the risk of forgetting previously acquired information.\n\u2022 We systematically investigate the LLMs-enhanced text-to-video generation models, both for prompt refining and LLMs embedding incorporation, improving the consistency and quality of generated videos.\n\u2022 We continually train the pre-trained model with our high-quality dataset and conduct extensive evaluations to demonstrate the effectiveness of ModelGrow."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Continual Pre-training of Generative Models", "content": "Continual Pre-training [11, 14] aims to learn from the evolving data without forgetting the knowledge in the past. In the field of generative models, most of the works [28, 30, 51] illustrate the effectiveness of the continual learning strategy in Language Model applications. The first direction tries to explore the parameter-efficient approaches, adopting the pre-trained model to specific domains, like LoRA [28], and Adapters [27]. Another direction in Language Models [30, 48, 51] is to continue learning the pre-trained model to enhance the overall performance without decreasing the training scale. [51] introduce a block expansion strategy and only update the new parameters. [30] propose a simple and scalable learning rate strategy and update all the parameters to improve the overall performance. Unlike post-training, which involves fine-tuning, alignment, and evaluation stages, in this paper, we mainly focus on the general continual pre-training techniques to enhance the quality of generation and language understanding. To the best of our knowledge, we are the first comprehensive study of continual pre-training in text-to-video generation."}, {"title": "2.2. Text-to-Video Generation", "content": "Text-to-video generation challenges us to convert low-dimensional data, such as short text, to high-dimensional modality video. Initially, most of the video generation models are GAN/VAE-based [13, 34, 38, 46], which generate videos by learning latent representations and producing sequences of frames that mimic real-world video data. Recently, the video generation models are based on the diffusion model. We can categorize them into two parts: U-Net-based and transformer-based. The U-Net-based models come first, such as VDM [25], Imagen Video [24], LVDM [21], AnimateDiff [19], VideoCrafter1 [7], VideoCrafter2 [8], Emu Video [44, 45, 49], WALT [20], Lumiere [2], Show-1 [55] and many others [4, 17, 22, 37, 47, 50, 57], employ U-Net architectures within diffusion processes to iteratively refine noisy inputs, effectively generating high-quality videos through a series of denoising steps that model complex data distributions. Then, with the OpenAI's Sora [6] being proposed, more and more transformer-based models appear, such as Open-Sora [56], Open-Sora-Plan [32], Latte [36], and CogVideoX [53], try to utilize the transformer's self-attention mechanisms to capture temporal and spatial dependencies across frames, enabling the generation of coherent and temporally consistent video sequences."}, {"title": "2.3. Text-to-Video Generation with LLMs", "content": "Incorporating Large Language Models(LLMs) can enhance the generation ability of video foundation models [23, 56]. There are two ways to incorporate LLMs: zero-shot and tuning. In the zero-shot approach, Re-captioning [3, 56], a method turning the short prompt into a long-detailed prompt, is commonly used to improve the visual generation quality and accurately follow the user's prompt. Another direction [15, 26, 35] tries to use LLMs as the planner at the first stage and then generate the whole video given the outputs of the planner. For tuning, several works [18, 54] aim to produce efficient visual tokens that are suitable for LLMs learning due to the gap between the visual and language modalities. SEED [18] learns a discrete visual tokenizer by minimizing the reconstruction loss and contrastive loss. MAGVIT [54] proposes a video tokenizer that maps the pixel space into the language domain with Lookup-Free Quantization. In this paper, we follow the tuning direction but do the opposite, aiming to incorporate the LLMs embeddings into the visual domain to enhance language understanding."}, {"title": "3. Method", "content": "In this section, we introduce the core framework of ModelGrow. Firstly, we outline the preliminaries that form the basis of our approach in Sec.3.1. Then, we introduce the block expansion process in detail, highlighting its role in enhancing model capacity in Sec.3.2. Finally, we discuss the integration of LLMs in the field of text-to-video generation, demonstrating their impact on performance in Sec. 3.3."}, {"title": "3.1. Preliminary: Diffusion Model with Transformers", "content": "Diffusion Architecture Diffusion models typically utilize a convolutional U-Net architecture to effectively learn the reverse process necessary for reconstructing the desired output from noise. However, the Diffusion Transformers (DiT) [40] had been introduced to replace the U-Net. This transformer is designed to operate on latent patches, thereby enhancing the model's capacity to achieve state-of-the-art performance.\nExtending to the video generation model, there are many variants of transformer blocks have been proposed [36]. The base model architecture and the pipeline we choose are shown in Fig. 2 and Fig. 3 followed by the Open-Sora V1.0 [56]. In Fig. 2, part (a) shows a simplified forward pipeline. We omit the encoding and decoding process of video VAE for the sake of conciseness. The latent features $z \\in R^{B \\times C \\times T \\times H \\times W}$ is gotten by the video VAE. It first goes through the linear layer and reshapes the output feature to generate the token embeddings for the next transformer blocks. After several transformer blocks, the embeddings will go through another linear layer and reshaping operation to get the desirable final noise $\\epsilon$.\nThe basic transformer block mainly consists of two self-attention blocks, a cross-attention block, and a feedforward layer. These two sell-attention blocks are the spatial block and the temporal block. The embedding tokens should be reshaped appropriately before going through the corresponding block. After the self-attention blocks, the cross-attention block will absorb the prompt embedding.\nDiffusion Training Loss To train a diffusion model, we focus on a denoising model $\\epsilon_\\theta$ with parameters $\\theta$, which predicts and removes noise at each step. The goal is to minimize the mean squared error between the predicted noise and the actual noise added during the diffusion process:\n$\\mathcal{L} = \\mathbb{E}_{z \\sim p(z), \\epsilon \\sim \\mathcal{N}(0,1)}[|| \\epsilon - \\epsilon_\\theta(z_t, t) ||^2]$ (1)\nIn this equation, $z_t$ is the noisy data at step $t$, and $\\epsilon$ is the true noise.\nwhere the $w$ denotes the guidance scale."}, {"title": "3.2. Model Expansion with Block Duplication", "content": "Block duplication As shown in Fig. 2, assume the vanilla diffusion model consists of $N$ transformer blocks. We propose three block expansion variants according to [51]. Suppose we expand $P$ new transformer blocks, which are copied from the previous blocks. Fig. 2 (b) illustrates a variant of block expansion called insert stacking. The total block stacking can be divided into $P$ parts. Within each part, a new transformer block, duplicated from the previous block, is appended at the end. The number of original transformer blocks is. Figure 2 (c) depicts another variant of block expansion known as prefix stacking. In this approach, all the new transformer blocks are positioned before the original transformer blocks. The parameters for these new blocks are duplicated from the first original transformer block. Figure 2 (d) presents a variant of block expansion termed suffix stacking. In this method, all new transformer blocks are appended after the original transformer blocks. The parameters for these additional blocks are duplicated from the last original transformer block. We conduct the experiments mainly based on the insert stacking variant. For our specific configuration, we set $P = N$, meaning that for every original transformer block, a new transformer block is sequentially added immediately following it. This configuration allows for a systematic exploration of block expansion effects on model performance. Apart from that, we also conduct the ablation study about the different expansion variants and expansion sizes.\nZero initialization To maintain outputs following block expansion, we implement a zero initialization procedure for the newly added transformer blocks. According to the basic transformer-based diffusion model architecture [56], each transformer block comprises four residual blocks. By ap-"}, {"title": "3.3. Language Understanding Enhancement", "content": "Given a prompt $p$, we want to generate a video $V$ conditioning on $p$. Figure 3 shows our pipeline of the current LLMs-enhanced transformer block. To increase the richness of the prompts as well as enhance the understanding ability for the long and complex prompts, we propose two large language model enhancement techniques in text encoder and transformer block.\nText encoder with LLMs enhancement User prompts usually lack details, especially in the text-to-video generation task. Inspired by the re-captioning in DALLE-3 [3] and Sora [6], we first use LLMs to generate the detailed prompts $p_i$ from the prompt $p$. Different from the current work that replaces $p$ with $p_i$, we generate $p_{sl}$ by appending $p$ with $p_i$ so that we can highlight the key information as well as maintain the details of the description of the key points. Before feeding the prompt into the transformer block, except for the original text encoder $\\xi(\\cdot)$, such as T5 [42], for getting the embeddings of the text, we add another text encoder $\\xi^*(\\cdot)$, reasoned with LLMs parallelly. Specifically, in order to fit in the distribution of Large Language Models training datasets, we equip $p_{sl}$ with an LLMs template and thus generate $p^*$. An example of an LLMs template from Llama3 is shown in Figure 3.\nTransformer block with LLMs enhancement We also modify the architecture of the transformer block to enhance the language understanding ability of the video generation model. We add an extra cross-attention module conditioning on LLMs embeddings following the T5 cross-attention module in every transformer block. Like the T5 text embedder, we also learn an LLMs-based embedder to adapt to the change of feature distributions. Inspired by Lumina-T2X [16], we apply gate parameter $\\lambda$ followed by the tanh function, leading to a zero-initialization setting. The text encoder $\\xi(\\cdot)$ and $\\xi^*(\\cdot)$ will be frozen, and we will update all the parameters in the transformer block. Similar to model expansion, we copy the weights of the T5 cross-attention block as the initialization of the weights of the LLMs cross-attention block.\nTraining loss We regard T5 and LLMs embeddings as two separate conditions for video generation. This setting could allow us flexibly to adjust the influence of LLMs enhancement. In our text-to-video generation network, the denoising network $\\epsilon_\\theta(z_t,t, C_{t5}, C_{llm})$ has two conditions, which denotes the T5 embeddings $C_{t5}$ and the LLMs embeddings $C_{llm}$ separately. According to the original diffusion training loss Eq. 1, our training loss is formulated as follow:\n$\\mathcal{L} = \\mathbb{E}_{z,t,c_{t5},C_{llm}, \\epsilon \\sim \\mathcal{N}(0,1)}[|| \\epsilon - \\epsilon_\\theta(z_t, t, C_{t5}, C_{llm})||^2] .$ (2)\nClassifier-free guidance for two conditions Practically, during the training process, we randomly drop the LLMs condition $C_{llm}$ at 1% and drop all two conditions at 0.1%. Similar to the two conditions formula in InstructPix2Pix [5], our two text conditions are:\n$\\tilde{\\epsilon}_\\theta(z_t, C_{t5}, C_{llm}) = \\epsilon_\\theta(z_t) + s_{t5} (\\epsilon_\\theta(z_t, C_{t5}) - \\epsilon_\\theta(z_t)) + s_{llm} (\\epsilon_\\theta(z_t, C_{t5}, C_{llm}) - \\epsilon_\\theta(z_t, C_{t5})).$ (3)\nwhere $s_{t5}, s_{llm}$ denote the guidance scales for T5 embeddings and LLMs embeddings accordingly."}, {"title": "4. Experiments", "content": "In this section, we will introduce the critical experimental information sequentially. Firstly, we outline the foundational experiments' settings in the Sec. 4.1. Then, we show the main quantitative and qualitative results of our methods in Sec. 4.2. Finally, the ablation study shows the different effectiveness between the various architectures in Sec. 4.3."}, {"title": "4.1. Experimental Setup", "content": "Pretraining stage. We utilize a subset of Panda-70M dataset [10] including around 2 million video clips, the Vript dataset [52] including around 0.4 million video clips and the Webvid dataset [1] including around 2 million video clips. Apart from these video datasets, we also use the JourneyDB dataset [39], including around 4 million high-quality images for joint image and video training.\nFor the base model, we begin with Open-Sora V1.0 [56], which is around 0.7 billion parameters, and train the model with the datasets described above to produce our base model. According to its report, we initialize the base model with the PixArt-a [9]. During the training, the batch size is set to 32 on a single GPU, and the learning rate is set to 2e-5. Each training video clip contains 16 frames with the resolution 256 \u00d7 256. The base model is trained for around 108k steps on 32 NVIDIA H800 GPUs.\nContinual pretraining stage. In this stage, we collect a new dataset, including 400k high-quality video clips with"}, {"title": "4.2. Quantitative and Qualitative Results", "content": "The main quantitative results are shown in the Table 1. For the VBench standard evaluation, there are three summative scores, which are derived from the analysis of 16 fundamental assessment dimensions. For the T2V-CompBench prompts, the VBench tool can only evaluate partial dimensions. The supplementary materials will show the results for all dimensions. Vanilla-0.7B, LoRA-0.7B, and Expansion-1.4B are trained continually from the Base model, while Expansion-1.4B-LLM are trained from the checkpoints of Expansion-1.4B. As our baselines, Vanilla-0.7B denotes the vanilla version via updating all parameters from the Base model directly, and LoRA-0.7B represents updating the parameters of the base model in the LoRA [28] approach.\nContinually training the pretrained model via model expansion significantly improves the performance compared to vanilla or LoRA training. The comparison between the Vanilla-0.7B and Expansion-1.4B models reveals that increased model size enhances the model's capability to generate video with higher quality. Moreover, when the base model is included in the comparison, it becomes apparent that merely continuing training with the same model size yields limited improvements or even detriments in some dimensions, such as semantic score and imaging quality. In addition, we also found that we can continually improve the model's performance, especially in semantic alignment, with LLMs enhancement, even though the datasets are still the same. Table 1 shows The performance of Expansion-1.4B-LLM improves compared with Expansion-1.4B both in VBench and CompBench Consistency Attribute Prompts in Table 1.\nWe also show some examples perceptually in Figure 4. From these examples, we conclude that our model Expansion-1.4B-LLM produces more high-quality generation results and has more semantic alignment with the given prompt. More qualitative results are shown in the supplementary materials."}, {"title": "4.3. Ablation Study", "content": ""}, {"title": "4.3.1. Ablation Study on Model Expansion", "content": "Table 2 presents the quantitative results of an ablation study focusing on model scaling and different variant stacking methods. Comparing Expansion-2.1B and Expansion-1.1B"}, {"title": "A. Details of Insert Stacking", "content": "As shown in Fig. 7, the insert stacking inserts new transformer blocks intermittently between the existing stack. Suppose we extend $P = kN$ new transformer blocks, where $k \\in \\mathbb{R}$ and $P \\in \\mathbb{N}^+$. To express the coefficient $k$ in irreducible fraction form, it can be represented as $\\frac{x}{y}$, where $x, y \\in \\mathbb{N}^+$ and $\\text{gcd}(x, y) = 1$. From this, we can get $\\frac{P}{N} = \\frac{x}{y}$ and $M = y$.\nTo clarify this process, consider the following examples of extending transformer blocks:\n\u2022 In the first case, if we extend $N$ new transformer blocks, where $k = 1$. In irreducible fraction form, this is $\\frac{1}{1}$. Thus, we can get $M = 1$ and $\\frac{P}{M} = \\frac{1}{1}$, which means that one new transformer block is placed directly behind each original transformer block in the stack.\n\u2022 In the second case, if we extend $0.5N$ new transformer blocks, where $k = 0.5$. Writing this as an irreducible fraction, we have $\\frac{1}{2}$. Therefore, we can get $M = 2$ and $\\frac{P}{M} = \\frac{1}{1}$. Note that $N$ must be divisible by $M = 2$ to maintain divisibility.\n\u2022 In the third case, if we extend $2N$ new transformer blocks, where $k = 2$. In irreducible fraction form, this is $\\frac{2}{1}$. As a result, we can get $M = 1$ and $\\frac{P}{M} = \\frac{2}{1}$."}, {"title": "B. Details of Transformer Block Architecture", "content": "We show the detailed architecture of transformer blocks in our work in Fig. 8 followed by the Open-Sora V1.0 [56]. Within each transformer block, the latent feature $z$ is processed through a series of specialized modules: the Spatial Self-Attention Block, Temporal Self-Attention Block, T5 Cross-Attention Block, LLM Cross-Attention Block, and concludes with a Feed Forward Block. This sequence ensures the comprehensive extraction and integration of features across both spatial and temporal dimensions.\nAs for the timestep $t$, It utilizes the scalable adaptive layer normalization (S-AdaLN) [36]. This linear layer will compute the $\\gamma$, $\\beta$, and $\\alpha$ based on the timestep embedding $c$. After the layer normalization, the scale and shift operation is $\\gamma \\cdot \\text{LayerNorm}(z) + \\beta$, where the $h$ denotes the hidden embedding within the transformer blocks. Before the residual connections, the scale operation is $\\alpha \\cdot h$.\nSuppose we have the latent feature $z \\in [R^{B \\times C \\times T \\times H \\times W}$, where B denotes the batch size, C the channel count of video frames, T the number of video frames, and H and W the height and width of each latent feature, respectively. The latent feature should be reshaped to the $z_s \\in R^{B_S \\times S \\times d}$ before the Spatial Self-Attention Block, where $B_S = B \\times T$ and $S = H \\times W$. Subsequently, this feature will be reshaped to the $z_t \\in R^{B_t \\times T \\times d}$ before the Temporal Self-Attention Block, where $B_t = B \\times S$.\nTo introduce the LLM embedding, we introduce an LLM Cross-Attention Block immediately following the T5"}, {"title": "C. Training Prompts Recaption", "content": "Incorporating detailed prompts into model training significantly enriches the realism and consistency of generated scenes. By providing explicit instructions regarding setting, actions, and other cinematic elements, these prompts help the model to generate more accurate complex environments and interactions. To advance the model's ability to process such detailed prompts, we employ a multimodal large language model (MLLM) for training in prompt re-captioning.\nAs shown in Fig. 9, under the help of LLaVA-NEXT, we utilize the instruction to generate a desirable detailed prompt. Compared to the original prompts, it presents the key elements like the setting, characters, and their actions. Expand on these by adding specifics about clothing, ges-"}]}