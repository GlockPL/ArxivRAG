{"title": "What is causal about causal models and representations?", "authors": ["Frederik Hytting J\u00f8rgensen", "Luigi Gresele", "Sebastian Weichwald"], "abstract": "Causal Bayesian networks are 'causal' models since they make predictions about interventional distributions. To connect such causal model predictions to real-world outcomes, we must determine which actions in the world correspond to which interventions in the model. For example, to interpret an action as an intervention on a treatment variable, the action will presumably have to a) change the distribution of treatment in a way that corresponds to the intervention, and b) not change other aspects, such as how the outcome depends on the treatment; while the marginal distributions of some variables may change as an effect. We introduce a formal framework to make such requirements for different interpretations of actions as interventions precise. We prove that the seemingly natural interpretation of actions as interventions is circular: Under this interpretation, every causal Bayesian network that correctly models the observational distribution is trivially also interventionally valid, and no action yields empirical data that could possibly falsify such a model. We prove an impossibility result: No interpretation exists that is non-circular and simultaneously satisfies a set of natural desiderata. Instead, we examine non-circular interpretations that may violate some desiderata and show how this may in turn enable the falsification of causal models. By rigorously examining how a causal Bayesian network could be a 'causal' model of the world instead of merely a mathematical object, our formal framework contributes to the conceptual foundations of causal representation learning, causal discovery, and causal abstraction, while also highlighting some limitations of existing approaches.", "sections": [{"title": "1 Introduction", "content": "Causal Bayesian networks are mathematical models that induce multiple distributions over some random variables [Spirtes et al., 2001, Pearl, 2009, Peters et al., 2017]. A causal Bayesian network describes one reference distribution, called the observational distribution, and a procedure to derive interventional distributions. As such, a causal Bayesian network is a concise mathematical model of several distributions indexed by interventions.\nCausal reasoning using causal models is seemingly intuitive once we assign names to the variables in the model based on the real-world quantities they aim to represent. The term 'intervention' is suggestive and one might use interventions on model variables to reason about actions that perturb the corresponding real-world quantities. Yet, without making the correspondence between model interventions and actions explicit, we blur the line between mathematical model and real-world substantiation. It is then unclear"}, {"title": "1.1 Dialogue \u2013 What is an intervention?", "content": "The following dialogue illustrates the conflict one runs into when using a causal Bayesian network, a mathematical model, to reason about actions and observations of some real-world quantities while using the word 'intervention' ambiguously.\nOmar: I have two quantities that I model as random variables A and B. I have measured them and the data seems to perfectly match the joint normal distribution\n$\\mathcal{L}^{\\mathcal{O}}(A,B) = \\mathcal{N}\\left(\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 & \\frac{1}{2} \\\\ \\frac{1}{2} & 1 \\end{pmatrix} \\right)$.\nI am sure that there is no unobserved confounding,\u00b9 but I am not sure if A causes B or B causes A. Do you think $A \\rightarrow B$ or $A \\leftarrow B$ is correct?\nSofia: I am sure that A causes B.\nOmar: How do you know?\nSofia: Try to intervene on B. If A causes B, then we would expect that intervening on B changes the conditional distribution of B given A but does not change the marginal distribution of A. For example, if you intervene to set B equal to 5, you will observe that (A, B) follows joint distribution\n$\\mathcal{L}^{do(B=5)}(A, B) = \\mathcal{N}(0, 1) \\otimes \\delta_5,$\nwhere $\\delta_5$ is the Dirac distribution with support {5}.\nOmar: Okay, I tried. I did something and now I observe that A and B follow joint normal distribution\n$\\mathcal{N}\\left( \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 2 & \\frac{4}{3} \\\\ \\frac{4}{3} & \\frac{37}{9} \\end{pmatrix} \\right)$.\nSofia: Now B follows distribution $\\mathcal{N}(0,\\frac{37}{9})$. I proposed that you intervene to make B have point mass in 5. Could you try again?\nOmar: Ah, sorry. I thought I implemented the intervention you suggested, but I can see now that I did not. I will try something different such that B has point mass in 5: Now, A and B have distribution\n$\\delta_1 \\otimes \\delta_5$.\nSo B has distribution $\\delta_5$, but I did not get the interventional distribution that you said I would get. Does that mean that A does not cause B?\nSofia: Haha, you also intervened on A and set it equal to 1! When you intervene on B, you only intervene on B. Make sure to only change the conditional distribution of B given A. Do not change the marginal distribution of A."}, {"title": "1.2 What went wrong?", "content": "Let us assume that Sofia is right that $A \\rightarrow B$. Based on her correct model (and the observational distribution), she makes the following prediction:\n(P) If you intervene do(B = 5), then you will observe the distribution $\\mathcal{N}(0, 1) \\otimes \\delta_5$ over (A, B).\nProposition (P) is ambiguous: It is clear what distribution Sofia's model implies under the intervention do(B = 5), however, it is unclear when the antecedent is satisfied in the world. Here is an explanation provided by Pearl, Glymour, and Jewell:\nThe difference between intervening on a variable and conditioning on that variable should, hopefully, be obvious. When we intervene on a variable in a model, we fix its value. We change the system, and the values of other variables often change as a result. [Pearl et al., 2016, page 54]\nOne possible way to understand this in the context of (P) is as follows:\n(P1) If you do something to change the system such that you observe B = 5 with probability 1, then you will observe the distribution $\\mathcal{N}(0, 1) \\otimes \\delta_5$ over (A, B).\nEven though Sofia believes that $A \\rightarrow B$, she apparently thinks that (P1) is false. When Omar does something such that (A, B) has distribution $\\delta_1 \\otimes \\delta_5$ and the antecedent of (P1) is thus satisfied, she objects that Omar intervened on both nodes. Instead, she proposes (P2) as an analysis of (P):\n(P2) If you do something to change the system such that you observe B = 5 with probability 1 while not changing the marginal distribution of A, then you will observe the distribution $\\mathcal{N}(0, 1) \\otimes \\delta_5$ over (A, B).\nInterpreting (P) as (P2) cannot be correct because if $A \\leftarrow B$, then (P) is false and (P2) is true. (P2) is a mathematical truth that holds irrespectively of whether $A \\rightarrow B$ or $A \\leftarrow B$, while (P) has different truth values depending on the causal structure. In Corollary 3.3, we show that interpreting (P) as (P2) means that every causal Bayesian network that correctly models the observational distribution is interventionally valid (see Definition 2.7)."}, {"title": "1.3 Contribution", "content": "We introduce a mathematical framework that explicitly links causal models and the real-world data-generating processes they are models of. This enables a transparent and formal argument showing that the seemingly natural interpretation of actions as interventions is circular, suggesting that interventional (layer 2 [Ibeling and Icard, 2020, Bareinboim et al., 2022]) predictions are not inherently free from the philosophical intricacies of falsifiability that have been debated for counterfactual (layer 3) predictions [Dawid, 2000, Shpitser"}, {"title": "1.4 Article outline", "content": "Section 2 - Framework: We introduce the framework used in this article. Instead of assuming that the underlying data-generating process is a causal model, we have a generic set of distributions indexed by actions. We draw an important distinction between 1) a model emulating the distributions of a representation and 2) a model being an interventionally valid model of a representation.\nSection 3 A circular interpretation: We formalize the seemingly natural interpretation of actions as interventions and show that it is circular, rendering every CBN that correctly models the observational distribution interventionally valid.\nSection 4 An impossibility result for interpretations: We discuss different intuitive properties of interpretations of actions as interventions and show that an interpretation that satisfies four intuitive desiderata is necessarily circular.\nSection 5 Non-circular interpretations: We define and discuss five non-circular interpretations of actions as interventions, making it possible for a causal model to be falsified.\nSection 6 Implications for related research: We discuss implications of our work for causal representation learning, causal discovery, and causal abstraction. We discuss connections to the philosophical literature on the logic of conditionals.\nSection 7 - Conclusion"}, {"title": "2 Framework", "content": "We choose to formalize causal models as causal Bayesian networks [Spirtes et al., 2001, Pearl, 2009, Peters et al., 2017] instead of, for example, structural causal models (SCMs). This choice eases the mathematical presentation in the present manuscript, while we think that the considerations in this paper apply equally to SCMs.\nDefinition 2.1. Causal Bayesian network. A causal Bayesian network (CBN) $\\mathcal{C}$, also called a causal graphical model, over real-valued random variables $Z = (Z_1,... Z_n)$ is a directed acyclic graph (DAG) $\\mathcal{G}$ over nodes $[n] = \\{1, ... n\\}$ and a collection of Markov kernels $\\{p_i: p(\u00b7 | pa_i) \u00b7 \\nu_i | i \u2208 [n]\\}$. These Markov kernels induce a joint distribution, called the observational distribution, denoted by $\\mathcal{L}(Z)$, with density\n$\\mathcal{P}^{\\mathcal{C}}(z_1,..., z_n) = \\prod_{i=1}^{n} p_i(z_i | pa_i)$.\nGiven some nonempty subset $J \\subseteq [n]$, interventional distributions are obtained by, for each $j \u2208 J$, replacing the kernel of j with some new kernel $pa_j \\rightarrow q_j(\u00b7 | pa_j)\u00b7\\mu_j$. We denote this intervention by $do(j \\leftarrow q_j, j \u2208 J)$. The interventional distribution under intervention $do(j \\leftarrow q_j, j \u2208 J)$ is denoted by $\\mathcal{L}^{\\mathcal{C};do(j\\leftarrow q_j, j\u2208J)}(Z)$ and has density given by\n$\\mathcal{P}^{\\mathcal{C};do(j\\leftarrow q_j, j\u2208J)}(z) = \\prod_{i=1}^{n} p_i^{\\mathcal{C};do(j\\leftarrow q_j, j\u2208J)}(z_i | pa_i)$\n$= \\prod_{j\\notin J} p_i(z_i | pa_i) \\prod_{i\u2208J} q_i(z_i | pa_i)$.\n$\\mathcal{P}_i^{\\mathcal{C};do (j \\leftarrow q_j, j \u2208 J)}$ denotes the i'th kernel given by $\\mathcal{C}$ and intervention $d = do(j \\leftarrow q_j, j \u2208 J)$, that is, $p_i^{\\mathcal{C};do(j\\leftarrow q_j, j\u2208J)} = p_i$ for $i \\notin J$, and $p_i^{\\mathcal{C};do(j\\leftarrow q_j, j\u2208J)} = q_i$ for $i \u2208 J$. In this work, we do not consider interventions that change the DAG $\\mathcal{G}$. We assume that interventions induce distributions different from the observational distribution, that is, they satisfy\n$\\mathcal{L}^{\\mathcal{C};do(j\\leftarrow q_j, j\u2208J)}(Z) \\neq \\mathcal{L}^{\\mathcal{C}}(Z)$.\nThis, for example, rules out intervening only on source nodes without changing at least some of their marginal distributions."}, {"title": "2.1 Emulation and interventional validity", "content": "Since CBNs are convenient to describe multiple distributions, we often use a CBN to describe the distributions of a representation.\nDefinition 2.5. Representation emulated by a CBN. Let a data-generating process $\\mathcal{D} = (\\mathcal{A}, \\{\\mathcal{L}^{a}(X^*)\\}a\u2208\\mathcal{A})$ and a CBN $\\mathcal{A}$ over nodes $Z = (Z_1,... Z_n)$ be given. We say that a representation $Z^* = (Z^*_1, ..., Z^*_n)$ of $\\mathcal{D}$ is emulated by $\\mathcal{A}$ and interventions $\\mathcal{I}^*$ if $\\mathcal{I}^*$ is a set of interventions in $\\mathcal{A}$ and there is a surjective function $g: \\mathcal{A} \\setminus \\{\\mathcal{O}\\} \\rightarrow \\mathcal{I}^*$ such that\n1. $\\mathcal{L}^{\\mathcal{O}}(Z^*) = \\mathcal{L}^{\\mathcal{A}}(Z)$, and"}, {"title": "3 Intc: The seemingly natural interpretation of actions as interventions is circular", "content": "In a CBN, an intervention $do(j \\leftarrow q_j, j \u2208 J)$ modifies the kernels of $Z_j$ given $PA_j$ for $j\u2208 J$, while keeping the kernels fixed for $j \\notin J$. Since we do not consider interventions that change the graph, we have, for all interventions $d$, that $\\mathcal{L}^{\\mathcal{C};d}(Z)$ is Markov w.r.t. the DAG $\\mathcal{G}$ of the CBN $\\mathcal{C}$. These considerations might compel us to consider the following interpretation.\nDefinition 3.1. Intc. The seemingly natural interpretation. Let a data-generating process D, representation Z*, compatible CBN C, and set of interventions I in C be given. We define interpretation Intc by the following rule: An intervention $do(j \\leftarrow q_j, j\u2208 J) \u2208 \\mathcal{I}$ is in Int(a) if and only if the following 3 conditions hold:\n1) $\\mathcal{L}^{a}(Z_i | PA_i) \\sim q_i$ for all $i \u2208 J$. That is, the action sets the conditionals of intervened nodes correctly. For example, if we interpret an action as intervention $do(Z_i = 4)$, then $Z_i$ must have Dirac distribution with support $\\{4\\}$ under that action.\n2) $\\mathcal{L}^{a} (Z_i | PA_i) \\sim p_i^\\mathcal{C}$ for all $i \\notin J$. Intuitively, we do not intervene on nodes not in $J$.\n3) $\\mathcal{L}^{a}(Z^*)$ is Markov w.r.t. the DAG of $\\mathcal{C}$. That is, we do not introduce dependencies.\nThe $\\mathcal{C}$ in Intc is for 'circular'. This interpretation interprets an action as an intervention $d$ in $\\mathcal{I}$ if and only if the action induces the interventional distribution given by $\\mathcal{C}$ and $d$.\nProposition 3.2. Let a data-generating process D, representation Z*, compatible CBN C, and set of interventions I in C be given. Then $d \u2208 \\mathcal{I}$ is in Int(a) if and only if $\\mathcal{L}^{a}(Z^*) = \\mathcal{L}^{\\mathcal{C};d}(Z)$.\nProof. Let $d \u2208 \\mathcal{I}$ and $a \u2208 \\mathcal{A}$ be given. If $\\mathcal{L}^{a}(Z^*) = \\mathcal{L}^{\\mathcal{C};d}(X)$, this immediately implies 1)-3) in Definition 3.1 and hence that $d \u2208 Int(a)$.\nAssume that $d = do(j \\leftarrow q_j, j \u2208 J) \u2208 Int(a)$. Since $\\mathcal{L}^{\\mathcal{O}}(Z^*)$ is Markov w.r.t. the DAG of $\\mathcal{C}$, there exists some CBN $\\mathcal{A}$ that has the same DAG as $\\mathcal{C}$ such that $\\mathcal{L}^{\\mathcal{O}}(Z^*) = \\mathcal{L}^{\\mathcal{A}}(Z)$."}, {"title": "4 Impossibility result for non-circular interpretations", "content": "We now present five desiderata D0-D4 for interpretations of actions as interventions. Since each desideratum appears intuitively reasonable, one might expect that a reasonable interpretation should satisfy all of them. We show in Proposition 4.1 that if an interpretation satisfies D1-D4, then it is the circular interpretation Intc (Definition 3.1), which renders all compatible models interventionally valid.\nDesideratum D0: Correct conditionals on intervened nodes. If we interpret action a as an intervention $do(j \\leftarrow q_j, j \u2208 J)$, then that action must set the conditional distribution of intervened nodes given their parents correctly. Formally, an interpretation Int satisfies desideratum D0 if\nFor every set of modeled interventions I and every action a \u2208 A,\nif $do(j \\leftarrow q_j, j \u2208 J) \u2208 Int(a)$, then $\\mathcal{L}^{a}(Z_i | PA_i) \\sim q_i$ for all $i \u2208 J$.\nWe believe that any reasonable interpretation satisfies Do and therefore do not consider interpretations that may violate DO. In the context of hard interventions, DO is sometimes referred to as 'effectiveness' [Galles and Pearl, 1998, Bareinboim et al., 2022, Ibeling and Icard, 2023]. Effectiveness is similarly considered an axiom in Park et al. [2023].\nDesideratum D1: If it behaves like an intervention, it is that intervention. If an action a induces a distribution that equals a distribution induced by the model under an intervention in the intervention set, then we should interpret that action as that intervention. Formally, an interpretation Int satisfies desideratum D1 if\nFor every set of modeled interventions I, every action a \u2208 A, and every intervention $d \u2208 \\mathcal{I}$, if $\\mathcal{L}^{a}(Z^*) = \\mathcal{L}^{\\mathcal{C};d}(Z)$, then $d \u2208 Int(a)$.\nDesideratum D2: An action should not be interpreted as distinct interventions. If the action a is interpreted as two distinct interventions, then these two interventions should induce the same interventional distribution. Formally, an interpretation Int satisfies desideratum D2 if\nFor every set of modeled interventions I, every action a \u2208 A, and every interventions $b, d \u2208 \\mathcal{I}$, if $d \u2208 Int(a)$ and $b \u2208 Int(a)$, then $\\mathcal{L}^{\\mathcal{C};d}(Z) = \\mathcal{L}^{\\mathcal{C};b}(Z)$.\nDesideratum D3: Interpretations should not depend on the intervention set I. Whether we interpret an action a as an intervention $d\u2208 \\mathcal{I}$ should not depend on which other interventions are in I. Formally, an interpretation Int satisfies desideratum D3 if\nFor every sets of modeled interventions I and I', every action a \u2208 A, and every intervention $d \u2208 \\mathcal{I} \u2229 \\mathcal{I}'$, $d\u2208 Int(a) \\rightarrow d \u2208 Int'(a)$.\nDesideratum D4: An intervention does not create new dependencies. If an action a does not induce a distribution that is Markov w.r.t. the DAG, then we should not interpret a as an intervention (in this work, as is common, we only consider interventions that do not introduce dependencies between variables). Formally, an interpretation Int satisfies desideratum D4 if\nFor every set of modeled interventions I and every action a \u2208 A, if $\\mathcal{L}^{a}(Z^*)$ is not Markov w.r.t. G, then $Int(a) = \\emptyset$.\nProposition 4.1. Impossibility result. Let a data-generating process D, representation Z*, and a compatible CBN C be given. Let Int be an interpretation that satisfies desiderata D1-D4. Then, for every set of modeled interventions I in and for all actions a \u2208 A, $Int(a) = Int_c(a)$.\nProof. Let $a \u2208 \\mathcal{A}$ and $\\mathcal{I}$ be given. From Proposition 3.2 and D1, it follows that $Int(a) \u2286 Int_c(a)$. Assume that $d \u2208 Int_c(a)$. By D4 we have that $\\mathcal{L}^{a}(Z^*)$ is Markov w.r.t. the DAG of $\\mathcal{C}$. Since $\\mathcal{L}^{a}(Z^*)$ is Markov w.r.t. the DAG of $\\mathcal{C}$, we can find an intervention b such that $\\mathcal{L}^{\\mathcal{C};b}(Z) = \\mathcal{L}^{a}(Z^*)$, namely an intervention b such that $\\mathcal{L}^{a}(Z_i | PA_i) \\sim p_{i,\\mathcal{C}}$ for all $i \u2208 [n]$. Consider $\\tilde{I} = \\mathcal{I} \u222a \\{b\\}$. By condition D1, $b \u2208 Int_{\\tilde{I}}(a)$, and by condition D3,"}, {"title": "5 Non-circular interpretations", "content": "We now consider, in turn, possible interpretations that may violate either one of the desiderata D1 and D2 to avoid the circularity of Corollary 3.3 implied by satisfying all desiderata. In Appendix D.1 and Appendix D.2, we consider interpretations that may violate D3 and D4, respectively. Taken together, this shows that no proper subset of the desiderata D1-D4 implies any other of the desiderata D1-D4, so the desiderata can be considered separately. In Section 5.3, we consider an interpretation that takes action complexity into account."}, {"title": "5.1 Intp: Letting imperfect interventions falsify a model is one way out of circularity", "content": "Informal overview of Section 5.1. If we insist that all actions correspond to perfect interventions, then it becomes possible to falsify a causal model: If we perform an action and the resulting observation cannot be explained by a perfect intervention, the causal model can be rejected. Explaining these observations by imperfect soft interventions may be a slippery slope leading to a circular interpretation where every observed distribution can be explained by some complex intervention in the model. In Example 5.3, we show how to falsify a causal model under Intp (to be defined in Definition 5.1).\nConsider the following interpretation that may violate D1, but satisfies D0 and D2-D4.\nDefinition 5.1. Intp. An interpretation violating only D1. Let a data-generating process D, representation Z*, compatible CBN C, and set of interventions I in C be given. We define the interpretation Inta by the following rule: An intervention $d = do(j \\leftarrow q_j, j \u2208 J) \u2208 \\mathcal{I}$ is in $Int(a)$ if and only if the following four conditions hold:\n1) d is a perfect intervention.\n2) For all $i \u2208 J$,\n$\\mathcal{L}^{a}(Z_i | PA_i) \\sim q_i$.\nThat is, Intp satisfies Do (correct conditionals on intervened nodes).\n3) For all $i \\notin J$,\n$PA_i$ is empty and $\\mathcal{L}^{a}(Z_i) = \\mathcal{L}^{\\mathcal{C}} (Z_i)$, or\n$PA_i$ is nonempty and $Z_i \\cancel{\\perp} PA_i$ in $\\mathcal{L}^{a}(Z^*)$.\nThat is, nodes not intervened on are either source nodes with unchanged distributions or not independent of their parents.\n4) $\\mathcal{L}^{a}(Z^*)$ is Markov w.r.t. the DAG of $\\mathcal{C}$. That is, Intp satisfies D4 (an intervention does not create new dependencies).\nThe $\\mathcal{P}$ in Intp is for 'perfect'. Condition 3) ensures that Intp satisfies D2 (an action should not be interpreted as distinct interventions), see Appendix B. It is straightforward to verify that Intp satisfies D3 (interpretations should not depend on the intervention set I). Intp may violate D1 (if it behaves like an intervention, it is that intervention) because it may be that $\\mathcal{L}^{a}(Z^*) = \\mathcal{L}^{\\mathcal{C};d}(Z)$ for some $d \u2208 \\mathcal{I}$ that is not a perfect intervention and some $a \u2208 A$, and thus $d \\notin Int(a)$. Under a non-circular interpretation like Intp, a CBN C can be an invalid model of a representation even though the representation is emulated by C. We now provide a partial characterization for when this happens under interpretation Intp.\nProposition 5.2. Let a data-generating process D be given. Assume that Z* is emulated by CBN and interventions I*."}, {"title": "5.2 Ints: Letting multi-node interventions falsify a model is an-other way out of circularity", "content": "In this section, we discuss another option for avoiding circularity: letting multi-node interventions falsify a causal model. We structure the arguments analogous to those in Section 5.1.\nInformal overview of Section 5.2. If we insist that all actions correspond to single-node interventions, then it becomes possible to falsify a causal model: If we perform an action and the resulting distribution cannot be explained by a single-node intervention, the causal model can be rejected. Explaining these observations by multi-node interventions may be a slippery slope leading to a circular interpretation where every observed distribution can be explained by some complex intervention in the model. In Example 5.6, we show how to falsify a causal model under Ints (to be defined in Definition 5.4).\nConsider the following interpretation that may violate D2, but satisfies D0, D1, D3, and D4.\nDefinition 5.4. Ints. An interpretation violating only D2. Let a data-generating process D, representation Z*, compatible CBN C, and set of interventions I in C be given. We define the interpretation Ints by the following rule: An intervention $do(j \\leftarrow q_j, j\u2208 J) \u2208 \\mathcal{I}$ is in Int(a) if and only if the following three conditions hold:\n1) $\\mathcal{L}^{a}(Z_i | PA_i) \\sim q_i$ for all $i \u2208 J$. That is, Ints satisfies Do (correct conditionals on intervened nodes).\n2) $\\mathcal{L}^{a}(Z_i | PA_i) \\cancel{\\sim} p_i$ for all $i \u2208 J$. That is, the kernels of the observational distribution are incompatible with the conditionals of intervened nodes under action a."}, {"title": "5.3 Intk: Interventions as simple actions", "content": "The larger the set of actions the more conditions must be satisfied for a causal model to be interventionally valid. More formally, there exists interpretations Int such that if Cis an I- Int valid model of Z* with set of actions A, then may not be an I \u2013 Int valid model of Z* with set of actions A' 2 A. For example, under Intp, the model Cin Example 5.3 is not interventionally valid but it would be for a sufficiently small subset of A; and every compatible model is interventionally valid if A = {0}. More generally, Proposition 5.5 (2) and Proposition 5.2 (2) show that while Ints and Intp avoid the circularity of interpretation Intc (Corollary 3.3 and Proposition 4.1), these interpretations may prevent interventionally valid causal modeling of sensible representations if the set of actions is large.\nInformal overview of Section 5.3. In this section, we present an interpretation Int K which has intermediate restrictiveness between Ints and Inte: I - Ints validity implies I- Intk validity and, since every compatible CBN is I \u2013 Inte valid, I \u2013 Intk validity implies I - Inte validity (while the reverse implications do not hold). We accomplish this by considering the complexity of actions and disqualifying actions that are not the most simple implementations of an intervention. The following thought experiment motivates why the complexity of actions is relevant to deciding which actions should be considered as which interventions."}, {"title": "6 Discussion of implications for related research", "content": "In this section, we will examine causal representation learning, causal discovery, and causal abstraction in light of the previous sections. Most notably, we argue that identifiability is not sufficient for interventional validity and that causal abstraction rests on an infinite regress. We also discuss connections to the philosophical literature on the logic of conditionals and, in Appendix F, related work by Janzing and Mejia [2024]."}, {"title": "6.1 Causal representation learning", "content": "Works on interventional causal representation learning [Squires et al., 2023, Buchholz et al., 2024, Jin and Syrgkanis, 2024, Varici et al., 2024, von K\u00fcgelgen et al., 2024, Zhang et al., 2024] often consider the following setting: There are some latent variables Z* = (Z1,... Zn) emulated by a causal Bayesian network A (with DAG G) and interventions I*; the observed data X = (X1,..., Xm) is given by some mixing function f : Rn \u2192 Rm, X* = f(Z*), where f is commonly assumed (at least) to be a diffeomorphism onto its image (e.g., von K\u00fcgelgen et al. [2024], Varici et al. [2024]). In this setting, the goal is to recover Z* and G from the distribution of X* in different environments E \u2286 A, that is, from {L\u00ba(X*)}a\u2208E. Even under strong assumptions, this can usually only be done up to certain ambiguities. At most, we can identify the unmixing function f-1 : Im(f) \u2192 Rn up to an equivalence class. Definition 2.6 of von K\u00fcgelgen et al. [2024] introduces one such equivalence class."}, {"title": "6.2 Causal discovery", "content": "What do we assume when we assume that observed variables are described by a CBN? The starting point of causal discovery is, in the words of Dawid [2010, Assumption 6.1], the assumption that \"[t]here exists some DAG [G] that is a causal DAG representation of the system.\" Dawid [2010] considers this a \u201cstrong [a]ssumption\u201d, suggesting that the assumption does not amount to mere emulation, which is always possible (see Definition 2.5 and discussion below it). Our work makes precise what this"}, {"title": "6.3 Causal abstraction", "content": "Causal abstraction is about transforming one causal model into another causal model. Which constraints such transformations ought to satisfy has been up for debate [Rubenstein et al., 2017, Beckers and Halpern, 2019, Otsuka and Saigo, 2022, Massidda et al., 2023, Otsuka and Saigo, 2024]. The argumentation has been rooted in intuitions about which models can intuitively be considered abstractions of other models. The aim then has been to find mathematical formalizations that capture these intuitions. In this work, we take a different approach: Instead of considering when one model is a 'valid abstraction' of another model, we ask when a model is an interventionally valid model of a representation. Rather than relying on intuitions about what models ought to count as abstractions of other models, our approach suggests that model transformations and abstractions should preserve or induce interventional validity. In this section, we argue that existing notions of abstraction do not necessarily align with the goal of preserving or inducing interventional validity, and that we need interpretations of actions as interventions to avoid an infinite regress."}, {"title": "6.4 Logic of conditionals", "content": "In this section, we clarify the connections between causal models and the logic of conditionals. The connection between causal models and (counterfactual) conditionals has received attention from researchers questioning the use of causal models in algorithmic fairness [Hu and Kohler-Hausmann, 2020, Kasirzadeh and Smart, 2021]. Our work is, as far as we know, the first to spell out a precise connection between interpretations of causal models and different analyses of conditionals. It turns out that considerations from the"}]}