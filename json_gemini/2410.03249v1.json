{"title": "How much can we forget about Data Contamination?", "authors": ["Sebastian Bordt", "Suraj Srinivas", "Valentyn Boreiko", "Ulrike von Luxburg"], "abstract": "The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we use experimental evidence and theoretical estimates to challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). We find that if model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. We then derive a simple theory of example forgetting via cumulative weight decay. It allows us to bound the number of gradient steps required to forget past data for any training run where we know the hyperparameters of AdamW. This indicates that many LLMs, including Llama 3, have forgotten the data seen at the beginning of training. Experimentally, we demonstrate that forgetting occurs faster than what is predicted by our bounds. Taken together, our results suggest that moderate amounts of contamination can be forgotten at the end of realistically scaled training runs.", "sections": [{"title": "Introduction", "content": "A core principle of machine learning is that a model should not be trained on the test set used for evaluation (Donoho, 2017). For foundation models trained on Internet-scale data, there are increasing concerns that this principle is violated due to the leakage of benchmark evaluation data into the training data (Xu et al., Oren et al., 2024, 2024). Indeed, many LLM developers have found overlap between their training data and the benchmark questions used for evaluation (Brown et al., Dubey et al., 2020, 2024). What is more, research on memorization (Carlini et al., Carlini et al., 2019, 2021) shows that text sequences from the training data are sometimes encoded within the model, including machine learning datasets (Grynbaum and Mac, Liang et al., Nasr et al., Bordt et al., 2023, 2023, 2023, 2024).\nWhile the fact that data contamination can lead to invalid performance evaluations is now well-established (Magar and Schwartz, Li and Flanigan, Yang et al., Jiang et al., 2022, 2024, 2023, 2024), little is known about the precise conditions under which this is the case. The main reason for this is that the training data is usually unknown, and contamination identified via clever research designs that work around this restriction (Golchin and Surdeanu, Oren et al., Deng et al., 2023, 2024, 2024). Because modern foundation models are sometimes trained for over a million gradient steps (Dubey et al., 2024), it is unclear whether a single update on contaminated data at some point during training necessarily impacts downstream evaluations. And indeed, there is quite some evidence that language models need to see samples repeatedly to have any impact on the final model. For example, many papers on memorization have found that it occurs only when a sample is frequently repeated in the training data (Carlini et al., Biderman et al., Huang et al., 2022, 2023, 2024). The same is true for research on knowledge acquisition, where a fact needs to be paraphrased many times before it is finally remembered by the model (Allen-Zhu and Li, Cao et al., Chang et al., 2023, 2024, 2024).\nIn this work, we study the impact of data contamination in a controlled setting. This means we train language models from scratch on datasets where we explicitly insert contaminated examples (Jiang et al., 2024). We begin by quantifying how the overall magnitude of benchmark overfitting (or the cross-entropy loss"}, {"title": "Related Work", "content": "Data Contamination. The GPT-3 paper (Brown et al., 2020) uses an n-gram-based approach to differentiate between \"clean\" and \"dirty\" benchmark questions. This approach has since been used in many LLM reports (Chowdhery et al., Touvron et al., 2023, 2023), including Llama 3 (Dubey et al., 2024), where it is estimated that there might be a performance gain of up to 8 and 14 percentage points on PiQA and HellaSwag, respectively. The GPT-4 technical report (Achiam et al., 2023) remarkably concluded that \u201ccontamination overall has very little effect on the reported results\". This has since given rise to a literature that aims to detect (Oren et al., 2024), mitigate (Li et al., 2024), and estimate the effect of (Yang et al., Bordt et al., 2023, 2024) data contamination under various assumptions, but crucially without access to the training data. This literature often challenges the conclusion that contamination overall has little effect in GPT-4 (Xu et al., 2024).\nForgetting. In machine learning, the term forgetting is frequently associated with \u201ccatastrophic\" forgetting, where learning new tasks hurt the performance at previously solved tasks (Lopez-Paz and Ranzato, 2017). In the context of LLMs, catastrophic forgetting can occur during fine-tuning (Luo et al., 2023) or continual learning (Huang et al., 2024). In contrast, this paper studies forgetting as a potential \u201cnatural\u201d phenomenon of learning (Toneva et al., 2019). Tirumala et al. [66] study forgetting in language modeling and find, similar to Toneva et al. [67], that forgetting can be exponentially slow. In contrast, Jagielski et al. [39] find that models empirically do forget examples over time. In concurrent work, Pagliardini et al. [58] propose to add"}, {"title": "Background and Methods", "content": "This Section gives additional details on the research questions and lays out our experimental setup.\nResearch question\nHow does the presence of a text in the training data influence the final model's performance on that\nsame text?\nWhile it is well known that an individual data point can be influential if the training data and model are small (Koh and Liang, 2017), we are concerned with the large-data regime where the influence of any individual example may vanish (Basu et al., Bae et al., 2021, 2022). To further clarify this setup, Section 3.1 gives a brief overview of recent developments in scaling the training data of LLMs. Section 3.2 details the used benchmarks and explains how we contaminate the training data. Section 3.3 discusses the problem of near-duplicate benchmark questions.\nModels and Training Data. We train language models of up to 1.6B parameters using the architecture and hyperparameters from the GPT-3 paper (Brown et al., 2020). For this, we adopt the llm.c codebase. The training data is the 100BT split of the Fine Web-Edu dataset (Lozhkov et al., 2024). We also train OLMO-1B (Groeneveld et al., 2024) using the corresponding code and data (Soldaini et al., 2024)."}, {"title": "We consider the regime of n-times Chinchilla", "content": "According to the Chinchilla scaling law, for every doubling of model size, the number of training tokens should also be doubled. The Chinchilla model itself has 70 billion (B) parameters and was trained on 1.4 trillion (T) tokens; suggesting that the number of training tokens should be roughly 20x the number of model parameters (Hoffmann et al., 2022). While the Chinchilla paper was highly influential, modern language models are trained on significantly more tokens (Sardana and Frankle, 2024). For example, the OLMo-7B model was trained on 2.46T tokens, 17.5x the amount suggested by Chinchilla (Groeneveld et al., 2024). Similarly, the Llama 3 70B model was reportedly trained on 15T tokens, at over 10x Chinchilla (Dubey et al., Meta AI, 2024, 2024). The same holds for almost all recent LLMs at the 7B parameter scale (Gemma Team, 2024). In this paper, we count the number of tokens a model is trained on as a multiple of its Chinchilla tokens."}, {"title": "We evaluate on a mix of seven different benchmarks", "content": "We evaluate the impact of data contamination using a mix of seven different benchmarks: ARC-Easy (Clark et al., 2018), Social-I-QA (Sap et al., 2019), WinoGrande (Sakaguchi et al., 2021), PiQA (Bisk et al., 2020), BoolQ (Clark et al., 2019), MMLU (Hendrycks et al., 2021), and HellaSwag (Zellers et al., 2019). This means that every evaluation contains questions from all seven benchmarks. To construct the mixed contamination data, we first concatenate the different benchmarks. We then partition the set of all benchmark questions into subsets ranging from 10,000 to 2,000 questions so that each subset contains all benchmarks in equal weight: HellaSwag: 19.58%, SocialIQA: 8.27%, PiQA: 19.7%, MMLU: 21.82%, BoolQ: 6.48%, ARC-Easy: 5.92%, and WinoGrande: 18.16%. A holdout set of 10,000 benchmark questions is never added to the training data. The other subsets are added to the training data, repeated either 4, 12, 36, or 144 times."}, {"title": "We filter near-duplicate Benchmark Questions", "content": "Our method requires that there are no side effects from contaminating the training data with one question on the evaluation of another question. However, upon closer inspection, it turns out that all the commonly used benchmarks from the literature contain questions that are either near-duplicates or where the context of one question contains the answer to another question (for example, because the same text document was used to create multiple questions). This is illustrated in Figure 1, which depicts two near-duplicate questions on HellaSwag and questions from ARC-Easy and MMLU that are cross-benchmark duplicates. To address this problem, we perform extensive filtering, removing duplicate questions where the length-normalized Levenshtein distance falls below a certain threshold (Levenshtein, Navarro, 1966, 2001). This is documented in Supplement A.3, where we also describe an experiment to verify that our method is not invalidated by near-duplicate questions."}, {"title": "Experimental Results", "content": "We now present our main experimental results. We being in Section 4.1 by discussing the scaling in model parameters, training tokens, and repetitions in the training data. The following Section 4.2 discusses various experiments on forgetting. The first two sections rely on training small GPT-3 models. Section 4.3 complements this with an analysis of OLMo-1B (Groeneveld et al., 2024)."}, {"title": "Contamination scales with Model, Data, and Repetitions", "content": "We conduct three different experiments to understand how the effect of data contamination scales with the number of model parameters, training tokens, and the number of times a contaminated example is seen. First, we train increasingly large models on the same dataset of 7B tokens. Second, we train 124M parameter models on increasingly many tokens. Third, we train increasingly large models according to the Chinchilla scaling laws (Hoffmann et al., 2022), meaning that the number of training tokens scales linearly with the model parameters. In all experiments, we contaminate the training data uniformly at random with benchmark questions."}, {"title": "Contamination can be completely Forgotten", "content": "In the previous Section 4.1, we saw that the accuracy gap due to contamination decreases in the number of tokens up to the point where even 12 repetitions of a benchmark question in the training data can become insignificant. In this Section, we identify the natural forgetting dynamic of neural network training as the reason for this effect. We discuss how quickly forgetting occurs, whether examples are completely forgotten, and what kind of repetition makes a model remember."}, {"title": "Contamination and rapid forgetting in OLMo-1B", "content": "In the previous sections, we trained small GPT-3 models from scratch. In this Section, we complement this analysis by pre-training from an intermediate OLMo-1B checkpoint (Groeneveld et al., 2024). Similar to the analysis in Section 4.2, we insert the benchmark data at a specific point into the training data and then measure the subsequent forgetting. Unlike in the previous Section, we now insert the entire benchmark data \u2013 we already have a \"clean\" baseline from the original OLMo-1B training run. We insert each benchmark question four times and contaminate with four different benchmarks: HellaSwag (Zellers et al., 2019), WinoGrande 62, ARC-Easy (Clark et al., 2018), and PiQA (Bisk et al., 2020)."}, {"title": "What is the role of weight decay in forgetting?", "content": "In the previous Section 4, we have seen that forgetting is an important empirical property of LLM training. In this Section, we show that the weight decay parameter and learning rate schedule of the AdamW optimizer play a key part in forgetting past training examples. This offers a novel perspective on the interplay between these two parameters, usually seen in terms of generalization and training stability (Van Laarhoven, Zhang et al., Lewkowycz and Gur-Ari, Andriushchenko et al., 2017, 2019, 2020, 2023)."}, {"title": "Weight Decay as a Mechanism for Forgetting Training Examples", "content": "Consider the parameter update of AdamW at gradient step $t\u2265 1$. It consists of two decoupled updates (Paszke et al., PyTorch Contributors, 2019, 2024): A weight decay update given by\n$\\$\\theta_{\\tau} = \\theta_{t-1} \u2013 \\gamma \\lambda_t \\theta_{t-1},$$\nand a gradient update given by\n$\\$\\theta_{\\tau} = \\theta_t \u2013 d_t m_t/(\\sqrt{\\hat{u}_t} + \\epsilon).$$\nHere, $\\theta_t$ are the model parameters, $\\lambda_t$ is the learning rate, $\\gamma$ is the weight decay parameter, and $m_t$ and $\\hat{u}_t$ are first- and second-order moment estimates of the gradient. Denoting the model weights at initialization by $\\theta_0$, and the adaptive gradient by $\\hat{g}_t = m_t/(\\sqrt{\\hat{u}_t} + \\epsilon)$, we can iterate (1) and (2) to obtain\n$$\\theta_T = \\omega_T \\theta_0 + \\sum_{t=1}^{T} \\omega_{t,T} \\hat{g}_t$$\nwhere $\\omega_{t_1,t_2} = \\prod_{i=t_1}^{t_2} (1 - \\lambda_i \\gamma)$.\nHere, the weights $\\omega_{t_1,t_2}$ account for the cumulative weight decay between gradient step $t_1$ and $t_2$. Intuitively, the model weights after t gradient steps are a weighted average of the initial model weights and all the adaptive gradient updates up to time step t. This is not specific to the AdamW optimizer and applies to every optimizer with weight decay. Analyzing equation (3) reveals a critical factor influencing forgetting: The exponential decay of the $\\omega_{t_1,t_2}$ with respect to increasing gap $t_2 - t_1$ in the optimization steps. In other words, the longer an update occurs in the past (i.e., the larger $t_2 - t_1$), the more the contribution of the update $\\hat{g}_{t_1}$ is being scaled down due to the exponential decay of weights $\\omega_{t_1,t_2}$. We can describe the evolution of these weights as the function of the time $T = t_2 - t_1$."}, {"title": "Practical forgetting occurs faster than what the theory predicts", "content": "In the previous Section 5.1, we have derived a simple theory of forgetting via cumulative weight decay. We now investigate how the theoretical estimates relate to the empirically observed forgetting.\nThe main parameter that controls the theoretical forgetting curves is the weight decay parameter. Therefore, we ask how forgetting changes empirically when we change the weight decay. Figure 6a depicts the result of repeating the forgetting experiment from Section 4.2 with three different choices for the weight decay parameter. From Figure 6a, we see that the weight decay parameter controls the impact of contamination at all time steps, where a larger weight decay parameter leads to more forgetting and a smaller weight decay parameter to less forgetting. This is consistent with the theoretical predictions depicted in Figure 6b, meaning there is a qualitative alignment between the empirical results and our theoretical predictions.\nTo better understand the quantitative relation between empirical forgetting and the theoretical estimates, we ask how the empirically forgotten fraction (of cross-entropy loss or accuracy) relates to the cumulative weight decay. Figure 6c depicts the empirical decay and corresponding theoretical prediction for the model from Section 4.2. We see that the theoretical estimate is somewhat pessimistic and that forgetting occurs"}, {"title": "Discussion", "content": "This work presents different experiments on data contamination and forgetting. We have seen that the impact of contamination can vanish as the size of the training data increases \u2013 an aspect that has largely been overlooked in the literature (Yang et al., Oren et al., Jiang et al., 2023, 2024, 2024). We have also shown that the hyperparameters of AdamW play an important part in forgetting \u2013 an insight that might inform the parametrization of future training runs. Of course, it would be interesting to study contamination and forgetting on a larger scale. Nevertheless, we have shown that the effects of contamination can vanish even though the number of steps we could train the models for was limited.\nWe have studied data contamination with a focus on the leakage of benchmark questions into the training data. This means that our work might be more informative about the topic than other works that study contamination in different contexts. At the same time, one has to be careful when extrapolating our results, especially to a privacy setup (Carlini et al., Jagielski et al., 2019, 2023). This is because empirical forgetting might behave differently for random strings or otherwise uniquely identifiable information (Carlini et al., 2021).\nThe \u201ccontamination problem\u201d as studied in this paper has interesting connections to many areas of machine learning, including privacy (Graves et al., Jagielski et al., 2021, 2023), data attribution (Kirchenbauer et al., 2024), and generalization (Bousquet and Elisseeff, Hardt et al., Mania et al., 2002, 2016, 2019). The connection to data attribution is especially relevant in light of our results. This is because we demonstrate cases where the presence or absence of a datapoint in the training data is irrelevant for the model behavior on that same datapoint, meaning it does not make sense to attribute model behavior to individual datapoints in this regime."}, {"title": "Appendix", "content": ""}, {"title": "Additional Discussion of Data Contamination Assumptions and Setting", "content": "Here, we discuss our data contamination approach in a bit more detail.\nIn this paper, we consider only exact contamination. This means we contaminate the training data exactly with the text the model is later evaluated on. In the literature, it has been shown that non-exact contamination (re-worded questions, translation into a different language) can affect benchmark performance, too. For example, Yang et al. [71] have shown that a 13B parameter Llama 2 Model (Touvron et al., 2023) can achieve an accuracy increase of over 20 percentage points after training on re-phrased benchmark questions. We decided against considering non-exact contamination for this paper because the models we train from scratch are much smaller than those for which non-exact contamination results have been shown. This means these models are less capable of making sense of related information, potentially leading us to underestimate the effect of non-exact contamination for realistic training runs.\nIn addition, we consider contamination with individual benchmark questions, inserted into the training data at random positions. We consider this setup because we are interested in contamination from the perspective of leakage, where individual benchmark questions may enter the training data via different documents (for example, as quotes in Wikipedia articles, a case described in Brown et al. [10]). This contrasts with the setup where a dataset is present in the training data as a long contiguous string, which we conjecture might have a similar impact but be easier detectable (Oren et al., 2024). The fact that we contaminate with benchmark questions also sets us apart from related works that study data contamination and memorization for random strings and uniquely identified objects (Carlini et al., Carlini et al., 2019, 2021). It is worth highlighting that the results between these two setups might differ, especially considering the time it takes to forget an example.\nWe only consider pre-training."}, {"title": "Additional Details on Evaluation and How Contamination was Performed", "content": "Benchmark Questions and Evaluation. We use code from OLMo (Groeneveld et al., 2024) to format the different benchmark questions. This code is again based in part on the EleutherAI Evaluation Harness (Gao et al., 2024). The benchmark questions are multiple-choice, and the different options are presented to the model zero-shot as possible sentence continuations. The prediction is the sentence continuation with the largest likelihood. For the small GPT-3 models, we normalize by the number of tokens (Gao, 2021). For OLMO, we rely on the evaluation framework that is part of the code repository.\nInserting benchmark questions into the training data. A batch of LLM training data consists of B sequences of S tokens, resulting in a batch size of B \u00d7 S. For example, OLMo-1B is trained with B = 2048 and S = 2048; the batch for a single gradient step contains ~4M tokens (Groeneveld et al., 2024). Individual sequences in a batch usually contain multiple texts separated by a special end-of-text token. We insert benchmark questions at random positions into the pre-training data, separated at the beginning and end with the end-of-text token."}, {"title": "Filtering Near-Duplicate Benchmark Questions", "content": "Upon close inspection of the different benchmarks, it turns out that there are exact and near-duplicate questions both within and across benchmarks. Consider, for example, the following two examples from HellaSwag (Zellers et al., 2019):\nand\nHellaSwag 1: A person is seen riding a board along the water with a kite on top. more clips are\nshown of the person riding back and fourth on the board.\nHellaSwag 2: A person is seen riding a board along the water with a kite on top. More clips are\nshown of the person riding back and fourth on the board. the person continues to ride the board\nalong the water."}, {"title": "Proof of Proposition 1", "content": "Proposition 2. (The Decay of Past Gradients) The number of optimization steps T = t2 - t1 that are required to make the contribution of a model update at time t\u2081 small, that is $\\omega_{t_1,t_2} < \\epsilon$ for some small $\\epsilon \\in R+$, scales as $T> \\frac{log(1/\\epsilon)}{\\gamma \\lambda_{avg}}$, where $\\lambda_{avg} = \\frac{1}{t_2-t_1}\\sum_{i=t_1}^{t_2} \\lambda_t$ is the average learning rate of the optimizer between t\u2081 and t2."}, {"title": "Extended Analysis of Forgetting & Gradient Alignment", "content": "To understand the effect of weight decay on forgetting, we analyze two different stages of optimization: (1) the contamination stage, where the training set consists of only the contaminated samples, and (2) the forgetting stage, where the training set consists only of clean samples. Here, we consider the SGD learning algorithm, but the resulting analysis also applies to SGD with momentum. In particular, to illustrate the effect of weight decay, we assume the usage of SGD for the contamination stage and SGD with weight decay for the forgetting stage.\nWe now introduce some notation. Let $\\theta \\in R^D$ be the weights of the model, and let $X_{cont} = \\{x_i^{cont}, x_2^{cont},...x_i^{cont}\\}$\nbe the contamination set and $X_{clean} = \\{x_i^{clean}, x_2^{clean}, ...x_i^{clean} \\}$ be the clean pre-training set. The training\ndata used for the contamination stage is thus $X_{cont}$, and the training data for the forgetting stage is $X_{clean}$.\nLet $l(x_i) \\in R$ be the loss associated with sample $x_i$. Let the model be initialized at the contamination stage\nwith $\\theta = \\theta_{init}$. The learning algorithm is (single batch size) SGD, which is run for a total of $N_{cont}$ steps for\nthe contamination stage and $N_{clean}$ steps for the forgetting stage. First, we observe that the weights at the\nend of the contamination stage are:\n$$\\theta' = \\theta^{init} \u2013 \\sum_{i=1}^{N_{cont}} \\lambda_i \\nabla_{\\theta} l(x_i^{cont})$$\nWe thus denote the weights at the end of the contamination stage as $\\theta' = \\theta_{init} + \\theta^{cont}$. For the subsequent,\nfine-tuning stage to forget information regarding samples $X_{cont}$, we first define a criterion to identify forgetting\nbased on the angle between a weight vector and the contaminated part identified above.\nForgetting Criteria. A model with weights $\\theta$ is said to have \u201cforgetten\u201d information contained in $X_{cont}$\nif $\\frac{\\vert \\theta \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2} \\leq \\epsilon$, which we call the \u201cforgetting ratio\u201d. Here, $\\epsilon \\in R+$ is a small constant.\nWe now proceed with an analysis of the forgetting stage. To enable this, we make the following important\nassumption that the gradients of clean and contaminated samples are orthogonal for all clean and contaminated\nsamples across all optimization steps. This is a relatively strong assumption, and it quantifies the intuition\nthat model updates required by SGD to memorize clean samples and contaminated samples are distinct.\nAssumption 1. (Gradient Orthogonality) $\\nabla_{\\theta_{t_1}} l(x^{clean}_i) \\cdot \\nabla_{\\theta_{t_2}} l(x^{cont}_j) = 0, \\forall i \\in [1, N_{clean}], \\forall j \\in [1, N_{cont}]$ and steps $t_1, t_2$.\nWe also make another minor simplifying assumption that the weight initialization is orthogonal to both\nthese quantities."}, {"title": "Assumption 2. (Gradient-Initialization Orthogonality)", "content": "$\\nabla_{\\theta_t} l(x_i^{clean}) \\cdot \\theta_{init} = 0, \\forall i$ and $\\forall$ steps t.\nGiven these assumptions, we are ready to state our result.\nProposition 3. (Forgetting Time) The number of optimization steps $T_{forget}$ in the forgetting stage, such that\nthe weights $\\theta_{T_{forget}}$ satisfy the $\\epsilon$-forgetting criteria is given by: $T_{forget} > \\frac{log(1/\\epsilon)}{\\gamma \\lambda_{avg}}$, where $\\lambda_{avg} = \\frac{1}{T}\\sum_{i=1}^{T} \\lambda_i$ is\nthe average learning rate of the optimizer.\nProof. We first compute the forgetting ratio at $\\theta = \\theta'$, and as a consequence of Assumption 2, verify that the\nforgetting ratio is equal to one.\nLet us now denote these weights as $\\theta_0 = \\theta'$, used as initialization for the forgetting stage. Analyzing the\nfirst optimization step, and the subsequent forgetting ratio, we have:\n(Optimization Step) $\\theta_1 = \\theta_0 \u2013 \\lambda_0 \\nabla_{\\theta} l(x_i^{clean}) \u2013 \\lambda_0\\gamma\\theta_0$\n(Forgetting ratio)\n$\\frac{\\vert \\theta_1 \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2}$ = $\\frac{\\vert (\\theta_0 \u2013 \\lambda_0 \\nabla_{\\theta} l(x_i^{clean}) \u2013 \\lambda_0\\gamma\\theta_0 ) \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2}$\n$\\frac{\\vert ((\\theta_{init} + \\theta^{cont}) \u2013 \\lambda_0 \\nabla_{\\theta} l(x_i^{clean}) \u2013 \\lambda_0\\gamma((\\theta_{init} + \\theta^{cont})))^T\\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2}$\n= (1 \u2013 $\\lambda_0\\gamma$) (From Assumptions 1 & 2)\nWe can similarly analyze the subsequent optimization steps to compute the forgetting ratio, which for\nsome step t + 1 is:\n$\\frac{\\vert \\theta_{t+1} \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2}$ = $\\frac{\\vert (\\theta_t \u2013 \\lambda_t \\nabla_{\\theta} l(x_i^{clean}) \u2013 \\lambda_t\\gamma\\theta_t ) \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2}$\nForgetting ratio at t + 1\n= $\\frac{\\vert \\theta_t \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2}$ (1 \u2013 $\\lambda_t\\gamma$)\nForgetting ratio at t\nUnrolling the recurrence till step T, we have that:\n$\\frac{\\vert \\theta_{T} \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2} = \\frac{\\vert \\theta_{0} \\cdot \\theta^{cont}\\vert}{\\Vert \\theta^{cont} \\Vert^2} \\times \\prod_{i=1}^{T} (1 \u2013 \\lambda_i\\gamma)$\n= 1 $\\times \\prod_{i=1}^{T} (1 \u2013 \\lambda_i\\gamma)$\nAssigning the forgetting ratio to be less than $\\epsilon$ according to our criteria, we have:\n$$\\prod_{i=1}^{T} (1 \u2013 \\lambda_i\\gamma) \\leq \\epsilon$$\n$$\\sum_{i=1}^{T} log(1 \u2013 \\lambda_i\\gamma) \\leq log \\epsilon$$\n$$\\sum_{i=1}^{T} (\u2013\\lambda_i\\gamma) \\leq log \\epsilon$$\n$$T \\times (\\frac{1}{T} \\sum_{i=1}^{T} \\lambda_i) \\gamma \\geq log \\frac{1}{\\epsilon}$$\nRe-arranging this equation gives us the desired result, where $\\lambda_{avg} = (\\frac{1}{T}\\sum_{i} \\lambda_i)$."}]}