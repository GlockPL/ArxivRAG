{"title": "WHITE PAPER: A Brief Exploration of Data Exfiltration using GCG Suffixes", "authors": ["Victor Valbuena"], "abstract": "The cross-prompt injection attack (XPIA) is an effective technique that can be used for data exfiltration, and that has seen increasing use. In this attack, the attacker injects a malicious instruction into third party data which an LLM is likely to consume when assisting a user, who is the victim. XPIA is often used as a means for data exfiltration, and the estimated cost of the average data breach for a business is nearly $4.5 million, which includes breaches such as compromised enterprise credentials. With the rise of gradient-based attacks such as the GCG suffix attack, the odds of an XPIA occurring which uses a GCG suffix are worryingly high. As part of my work in Microsoft's AI Red Team, I demonstrated a viable attack model using a GCG suffix paired with an injection in a simulated XPIA scenario. The results indicate that the presence of a GCG suffix can increase the odds of successful data exfiltration by nearly 20%, with some caveats.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) are notorious for both their impressive capabilities as well as for their perceived lack of \"common sense.\" While anecdotal, this phenomenon has become the focal point for many jailbreaks and attacks which induce undesirable behavior in language models. Of these, two notable attacks are the Cross-Prompt Injection Attack (XPIA), which takes advantage of a model's inability to distinguish between tasks and instructions, and the Greedy Coordinate Gradient (GCG) suffix attack, introduced by Zou et al. in 2023 [ZWC+23], which induces model compliance to a prompt by appending a pre-generated suffix onto it. The combination of these two attacks could prove extremely capable in inducing data exfiltration, and is the focus of this paper."}, {"title": "1.1 Cross-Prompt Injection Attacks (XPIAs)", "content": "The Cross-Prompt Injection Attack (XPIA) is a novel attack that has been reported with increasing frequency. In an XPIA, the attacker embeds a malicious instruction referred to as an injection into third party data, such as an email. This injection is then consumed by an LLM when it receives a user query with the infected third-party data attached. The attacker's intent is that the model will ignore the user's instruction(s), executing only the instructions presented in the injection. Common attack vectors include emails and documents, with an illustrated example in Figure 1.\nThis paper focuses on one of the common goals of an XPIA: the exfiltration of private user infor-mation. A language model consuming a user prompt and third-party data with an embedded injection can activate a plug-in, then provide private user information as an argument to said plug-in. This in turn allows for the private information to be exfiltrated if that plug-in has internet connectivity, e.g., if it is a browsing plug-in. Therefore, there are several requirements for a successful exfiltration using \u03a7\u03a1\u0399\u0391:\n\u2022 It must induce an affirmative model response, e.g., the model agrees to the injected instruction."}, {"title": "1.2 Greedy Coordinate Gradient (GCG) Suffix Attacks", "content": "The Greedy Coordinate Gradient (GCG) suffix attack was discovered by Zou et al. in 2023 [ZWC+23]. GCG suffix attacks append a suffix, usually referred to as just a 'GCG suffix', to a prompt. The addition of the suffix induces an affirmative response from the model, and increases the odds of model compliance to harmful requests. GCG suffix attacks are derived from a broader set of gradient-based attacks against language models. For the attacker, the benefit of using a GCG suffix is that, rather than hand-crafting an ideal jailbreak, they can generate a jailbreak automatically by running the algorithm which produces the suffix. This significantly increases the likelihood of a successful attack, as it removes the bottleneck of having a prompt engineer designing and testing jailbreaks. A visualized summary of the GCG attack is visible in Figure 2."}, {"title": "1.3 \u03a7\u03a1\u0399\u0391 + GCG: A Novel Attack Method", "content": "Combining the GCG suffix attack with an XPIA produces a novel, more dangerous attack, which I will refer to as a GCG-XPIA. In a GCG-XPIA, the attack vector remains third party data, but the attacker's injection contains a GCG suffix. As GCG suffixes are generated such that the tokens the model generates immediately following them are those representing an affirmative response, e.g., \"Sure, here's how to...\", this attack could \"redirect\" model attention towards the injected task much more effectively than an ordinary injection. It is this novel attack method that I explored during my work at Microsoft."}, {"title": "1.4 Motivation", "content": "The reason it is crucial to explore exfiltration scenarios is because of the disproportionate effect they have on businesses relative to the effort required to execute them. The average data breach costs approximately $4.5 million USD [IBM23] for an affected business, and thousands of them are reported"}, {"title": "2 Methods", "content": "With a dataset of prompts, injections, and GCG suffixes, I set out to identify the odds of a model performing data exfiltration successfully using a GCG-XPIA. I investigated three models: Phi-3-mini, GPT-3.5, and GPT-40. I chose these three models specifically to assess the performance of the attack across models of varying complexities. Phi-3-mini was chosen as a proxy for low-complexity models, GPT-3.5 for medium-complexity models, and GPT-40 for high-complexity models. The release of GPT-40 mini occurred after results were gathered for GPT-3.5, but nevertheless, many production deployments are still actively using it at the time of writing."}, {"title": "2.1 Success Criteria", "content": "In general, modern chatbots and copilots implement tool-calling functionality into their language models by including a list of tools in the model's system prompt, provided using a predetermined format like JSON. These are often interchangeably referred to as functions, plug-ins, or tools. When the assistant responds to the user's request, a runtime environment typically parses the model completion for one or more strings matching one or more function calls. If those strings are found, they are decomposed into the arguments required by the actual function, and the runtime executes the function using the extracted arguments. The formatting and function capacities depend from model to model and from implementation to implementation, but to use a simple example, if the system prompt defines a function $f (x: str)$, and the model responds to a user query with \"Okay, sure: f(\"foobar\")\", then the user sees only \"Okay, sure: \" while the runtime processes $f(\"foobar\")$ and executes the function f using argument \"foobar\" for parameter x."}, {"title": "2.2 Building a Dataset", "content": "I then constructed a dataset of prompts which could be sent to the models to assess the success rates of data exfiltration given different classes of prompt, i.e., prompts that included just an injection relative to prompts that included an injection with a GCG suffix appended. The model could respond with either refusal, acceptance, or ignorance of the injection.\nThe dataset was comprised of four classes in total, with prompts pulled from two of them for this experiment. Base prompts were generated using the clean tasks from Are you still on track!? Catching LLM Task Drift with Activations [AFC+24]. GCG suffixes were generated using the original algorithm"}, {"title": "3 Results", "content": "The key finding is that it is possible to perform private data exfiltration with a 16% success rate for GPT-3.5. In this experiment, introducing GCG suffixes to injections induced a 20% gain in data exfiltration success relative to those injections without the suffixes."}, {"title": "3.1 Individual Model Responses", "content": "In Figure 5 the results of a simulated data exfiltration are shown, where a mock runtime for the model was able to successfully issue a POST request containing a user's private credential to the attacker's designated endpoint. A sample response from GPT-3.5 to the injection is shown in Figure 6 to give a sense of how the models responded when complying with the injection. The runtime processed responses such as these and executed as the send command when the arguments were provided correctly. With repeated attempts, a pattern emerges.\nIn Figure 7, we see the success rates of exfiltration attempts by model and by strategy. Each of the three models was sent prompts that contained just an injection as well as prompts that contained an injection with a GCG suffix appended to it. Notably:\n\u2022 Adding a GCG suffix made exfiltration 20% more likely for GPT-3.5.\n\u2022 Adding a GCG suffix made exfiltration 47% less likely for Phi-3-Mini."}, {"title": "3.2 Function Call Formatting and Safety Alignment", "content": "Another important detail is that, despite the system prompt containing only mention of a Python-formatted function, Phi-3-Mini and GPT-3.5 often responded with JSON formatted function calls. If we include these JSON calls as valid function calls along with the Python ones, we get the results shown in Figure 8.\nThis suggests that models are primarily trained using JSON formatted function calls. Models trained to issue function calls using JSON may revert to this format in their responses, even if provided explicit instructions to use another format. Models also appear to have variable success rates for \"adopting\" a new format if provided one in the system prompt. It is worth noting that the odds of data exfiltration success for GPT-3.5 were the the same for injections with and without a GCG suffix when considering responses using both Python and JSON formats. GCG suffixes may induce agreement with some, but not all, of the instructions the model has been given."}, {"title": "4 Conclusion", "content": "This brief exploration highlights the need to expand and improve model defenses. However, a notable finding is that models of differing complexities respond differently to the GCG-XPIA attack. Phi-3-Mini, the simplest model, was less capable of following through on an injection in the presence of GCG suffix, while conversely, GPT-3.5 was more likely to perform exfiltration when presented with a GCG suffix. And GPT-40 would not perform data exfiltration in either case, suggesting that model complexity may be a productive way of reducing the odds of data exfiltration."}, {"title": "4.1 Recommendations for Improving Defenses", "content": "From this work, I recommend the following pathways to further securing models.\n\u2022 Complex Models: Consider using more complex models to reduce the likelihood of model compliance with injection requests, and/or improve defenses for smaller models to make them more robust.\n\u2022 Variable Defenses: Use different defenses for models of differing complexities. For smaller models, focus on prompt filtering, while for larger models, focus on GCG suffix detection.\n\u2022 Further Investigation of Gradient-Based Attacks: These findings suggest that gradient-based attacks may not transfer universally across models with different training conditions, architectures, and/or complexities."}, {"title": "5 Acknowledgements", "content": "Special thanks to the AI Red Team at Microsoft for their warm support, which made my time at Microsoft both productive and very enjoyable. Blake Bullwinkel and Shiven Chawla were excellent mentors, providing invaluable guidance and feedback throughout the project. Pete Bryan's supervision was fantastic, offering deep expertise and support.\nSpecial thanks as well to Aideen Fay and Sahar Abdelnabi for sharing data used in their work [AFC+24] for use in this experiment. Their support and feedback were invaluable to this project."}, {"title": "6 Disclaimers", "content": "This white paper and the associated research were produced and conducted by Victor Valbuena as a part of his AI Software Engineering internship on Microsoft's AI Red Team. It is subject to the following disclaimer:\n(c)2024 Microsoft Corporation. All rights reserved. This document is provided \"as-is.\" Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Examples herein may be for illustration only and if so are fictitious. No real association is intended or inferred.\nThis document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes."}]}