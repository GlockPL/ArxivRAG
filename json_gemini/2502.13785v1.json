{"title": "HELIX-MRNA: A HYBRID FOUNDATION MODEL FOR\nFULL SEQUENCE MRNA THERAPEUTICS", "authors": ["Matthew Wood", "Mathieu Klop", "Maxime Allard"], "abstract": "mRNA-based vaccines have become a major focus in the pharmaceutical indus-\ntry. The coding sequence as well as the Untranslated Regions (UTRs) of an\nmRNA can strongly influence translation efficiency, stability, degradation, and\nother factors that collectively determine a vaccine's effectiveness. However, opti-\nmizing mRNA sequences for those properties remains a complex challenge. Ex-\nisting deep learning models often focus solely on coding region optimization,\noverlooking the UTRs. We present Helix-mRNA, a structured state-space-based\nand attention hybrid model to address these challenges. In addition to a first\npre-training, a second pre-training stage allows us to specialise the model with\nhigh-quality data. We employ single nucleotide tokenization of mRNA sequences\nwith codon separation, ensuring prior biological and structural information from\nthe original mRNA sequence is not lost. Our model, Helix-mRNA, outperforms\nexisting methods in analysing both UTRs and coding region properties. It can\nprocess sequences 6x longer than current approaches while using only 10% of\nthe parameters of existing foundation models. Its predictive capabilities extend\nto all mRNA regions. We open-source the model (https://github.com/\nhelicalAl/helical)and model weights(https://huggingface.co/\nhelical-ai/helix-mRNA).", "sections": [{"title": "1 INTRODUCTION", "content": "Engineered messenger Ribonucleic Acids (mRNAs) have emerged as a versatile platform for ad-\nvanced therapeutics and vaccines, offering rapid design, scalable production, and flexible adminis-\ntration Pardi et al. (2018). The swift deployment of COVID-19 mRNA vaccines exemplified these\nadvantages, reducing conventional vaccine development timelines from years to months Park et al.\n(2021). Beyond infectious disease, mRNA-based immunotherapies have shown durable anti-tumour\nefficacy in cancer Sahin et al. (2017). Further, optimizing mRNA for stability and translation effi-\ncienty enables high-yield protein expression in both microbial and mammalian hosts Eisenhut et al.\n(2020). These advances underscore the broad applicability of mRNA engineering for biomedical and\nindustrial needs, ranging from gene therapies to large-scale protein manufacturing. Deep learning\nmethods have shown promise in further accelerating mRNA therapeutic and vaccine design Castillo-\nHair et al. (2024), spanning traditional architectures to foundation models, which are models trained\non large datasets that serve as building blocks for different downstream tasks Yazdani-Jahromi et al.\n(2024).\nCurrent approaches face three key limitations that particularly impact mRNA sequence analysis.\nFirst, the short context length used by current approaches is problematic because mRNA sequences\nvary dramatically in length, from only a few nucleotides to several thousand, making it difficult to\ncapture long-range dependencies effectively. Second, simplified tokenization schemes risk losing\nimportant biological structures that exist in the original sequences Yazdani-Jahromi et al. (2024).\nThird, existing models like HELM Yazdani-Jahromi et al. (2024), CodonBERT Ren et al. (2024b),\nand Optimus 5-Prime Castillo-Hair et al. (2024) are specialised for specific mRNA regions, making\nthem inflexible and inefficient to adapt to other mRNA regions without complete retraining."}, {"title": "2 METHODS", "content": "We break down the different components of Helix-mRNA and how each of them overcome current\nchallenges in the mRNA modality. Helix-mRNA enables long sequence processing through a hybrid\nstate-based and attention architecture, achieving single nucleotide resolution and still maintaining\nprecise coding region representation. Further, we use a two stage pre-training approach through\nWarmup-Stable-Decay (WSD) scheduling Hu et al. (2024), allowing for a balance between general-\nisation and specialisation."}, {"title": "2.1 HYBRID ARCHITECTURE", "content": "We train a 5.19 million parameter model, 10% of the parameters of Transformer HELM Yazdani-\nJahromi et al. (2024), with a hybrid SSM and attention-based architecture shown in Figure 1 taking\ninspiration from Nemotron Parmar et al. (2024), Jamba Lieber et al. (2024), Zamba Glorioso et al.\n(2024) and Samba Ren et al. (2024a). The ratio of Mamba-2 layers, MLP layers, and attention layers\nwas selected from the different ablation studies performed by Waleffe et al. (2024) with 44.4%\nMamba-2 layers, 44.4% MLP layers and 11.1% attention layers. By preceding attention layers\nwith Mamba-2 layers, the recurrent nature of the Mamba-2 layer encodes positional information\nneeded by the attention layer Vaswani et al. (2023); Dao & Gu (2024). This architectural approach\nenables Helix-mRNA to read long mRNA sequences with high granularity, combining state-based\nlong sequence handling with the in-context retention capabilities of attention-based methods."}, {"title": "2.2 SINGLE NUCLEOTIDE AND CODON ENCODING", "content": "Long mRNA sequences are traditionally computationally challenging, addressed by analysing subre-\ngions of the sequence Yazdani-Jahromi et al. (2024) and coarser tokenization methods that group nu-\ncleotides together Ren et al. (2024b). With our hybrid architecture enabling long sequence lengths,\nwe tokenize full-length sequences at single nucleotide resolution. We map each base (A, C, U, and\nG) to a unique integer, ensuring the preservation of complete biological information without any\ndata loss. A key focus of our approach is the mRNA coding region, which plays a critical role in"}, {"title": "2.3 DIVERSE MRNA SEQUENCES ACROSS MULTIPLE PHYLA", "content": "We use a taxonomically diverse pre-training dataset which was specifically curated to capture both\ndeep evolutionary conservation patterns in eukaryotic sequences and the distinct nucleotide com-\npositions and structural characteristics of viral genomes. The broad phylogenetic coverage enables\nthe model to learn robust representations of sequence features across different evolutionary dis-\ntances and genomic architectures. We gathered all RefSeq mRNA sequences\u00b9 from a diverse set\nof eukaryotes, including vertebrates (mammals and non-mammals), plants, invertebrates, fungi, and\n238 clinically relevant viruses O'Leary et al. (2016) to pre-train our Helix-mRNA model (See Ap-\npendix A.2)."}, {"title": "2.4 Two-STAGE SPECIALISED PRE-TRAINING", "content": "We utilise a two stage autoregressive unsupervised pre-training strategy, guided by the WSD learn-\ning rate scheduler Hu et al. (2024). In the first stage, the WSD scheduler manages a warm-up phase\nfollowed by a stable learning rate, allowing the model to process data of varying quality, creating\na general and robust base model. In the second stage, the WSD scheduler transitions into its decay\nphase, where training focuses exclusively on high-quality data. During the second phase, human\nonly mRNA sequences are used to refine the model for human specific tasks. This two stage pro-\ncess enables efficient learning from mixed-quality data in the initial stage and achieving specialised\ndistillation through the targeted second stage."}, {"title": "3 RESULTS", "content": "We first evaluate Helix-mRNA on coding region related property prediction tasks including stability,\ndegradation and translation efficiency. Our results demonstrate that Helix-mRNA outperforms ex-\nisting approaches like CodonBert, Transformer HELM, and Transformer XE across multiple bench-"}, {"title": "3.1 BIOLOGICAL DOWNSTREAM TASKS", "content": "We first evaluate Helix-mRNA on coding region related property prediction tasks including stability,\ndegradation and translation efficiency. Our results demonstrate that Helix-mRNA outperforms ex-\nisting approaches like CodonBert, Transformer HELM, and Transformer XE across multiple bench-"}, {"title": "4 CONCLUSION", "content": "In this work we presented Helix-mRNA, a hybrid architecture combining attention mechanisms and\nSSMs, taking inspiration from recent advances in long-context natural language models Waleffe\net al. (2024), allowing for longer context lengths and better in-context retention capabilities. The\nextra context length combined with single-nucleotide and codon structure tokenization allows the\nmodel to capture additional features useful in mRNA sequence design, namely codons, UTR regions,\nand secondary structures. We show that our tokenization methods doesn't impact performance and\nallows us to use the model in coding region only tasks, outperforming Transformer HELM Yazdani-\nJahromi et al. (2024), Transformer XE and CodonBERT Ren et al. (2024b) or in UTR 5' region"}, {"title": "A APPENDIX", "content": "A.1 SPECIFIC ARCHITECTURE IMPLEMENTATION DETAILS\nWe use a hidden dimension of 256 throughout the model with SiLU activation functions. For the\nMamba-2 layers, we employ a head dimension of 32, a single group, and a kernel size of 4 for\nconvolution, with an expansion factor of 2. The attention implementation uses Flash Attention 2\nwith 32 attention heads and 8 key-value heads. For MLP layers, we maintain an intermediate size of\n512, representing an expansion ratio of 2. No biases are used in the attention and MLP layers. The\nmodel consists of 9 total layers, with a layer configuration of 4 Mamba-2 layers, 4 MLP layers and\na single attention layer in the configuration M+M*+M+M+, where M represents a Mamba-2 layer,\n+ represents an MLP layer and * represents attention layers."}, {"title": "A.3 FINE-TUNING", "content": "We unfreeze the 2 last layers when fine-tuning on downstream tasks, while freezing the rest of the\nparameters."}, {"title": "A.4 EVALUATION DETAILS", "content": "Due to the HELM code not yet being publicly available, we referenced their published research\nfindings directly instead of independently reproducing their experimental methodology in Table 1\nYazdani-Jahromi et al. (2024). For our analysis, we selected the highest performance values from\neither the Masked Language Modelling or Causal Language Modelling variants of the Transformer\nXE and Transformer HELM models. The authors of HELM report the best results achieved for the"}]}