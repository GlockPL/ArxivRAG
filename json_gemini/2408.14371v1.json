{"title": "SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery", "authors": ["Sarah Rastegar", "Mohammadreza Salehi", "Yuki M. Asanol", "Hazel Doughty", "Cees G. M. Snoek"], "abstract": "In this paper, we address Generalized Category Discovery, aiming to simultaneously uncover novel categories and accurately classify known ones. Traditional methods, which lean heavily on self-supervision and contrastive learning, often fall short when distinguishing between fine-grained categories. To address this, we introduce a novel concept called 'self-expertise', which enhances the model's ability to recognize subtle differences and uncover unknown categories. Our approach combines unsupervised and supervised self-expertise strategies to refine the model's discernment and generalization. Initially, hierarchical pseudo-labeling is used to provide 'soft supervision', improving the effectiveness of self-expertise. Our supervised technique differs from traditional methods by utilizing more abstract positive and negative samples, aiding in the formation of clusters that can generalize to novel categories. Meanwhile, our unsupervised strategy encourages the model to sharpen its category distinctions by considering within-category examples as 'hard' negatives. Supported by theoretical insights, our empirical results showcase that our method outperforms existing state-of-the-art techniques in Generalized Category Discovery across several fine-grained datasets. Our code is available at: https://github.com/SarahRastegar/SelEx.", "sections": [{"title": "1 Introduction", "content": "Supervised learning has proven its effectiveness in classifying predefined image categories [31, 34, 44, 72, 73]. However, it struggles significantly when presented with unknown categories, hindering its real-world applicability [7,50, 68, 76, 101]. Generalized Category Discovery (GCD) addresses this limitation by automatically identifying both known and novel categories from unlabeled data [2, 8, 16, 29, 63, 77,78,83,95]. A key approach for handling unknown categories within GCD has been self-supervision through contrastive learning [8, 9, 15, 27, 36, 46, 48, 53, 93]. However, this method struggles with fragmented clustering and an increased false negative rate, particularly in fine-grained categorization where positive augmented samples may significantly differ from their negative counterparts from the same category, which leads to misclassification [9, 18, 35, 37]. Although supervised contrastive learning [38,77] improves discrimination among known categories, it struggles with unknown categories due to the absence of supervisory"}, {"title": "2 Related Works", "content": "Generalized Category Discovery was introduced concurrently by Vaze et al. [77] and Cao et al. [8]. It provides models with unlabeled data from both novel and known categories, placing it within the realm of semi-supervised learning [12, 54, 58, 65, 88]. The unique challenge in generalized category discovery is handling categories without any labeled instances alongside already seen categories. There are primarily two approaches to address this challenge. One employs a series of prototypes as reference points, e.g., [2,16,17,29,39,75,81,83,86,87,94]. The second leverages local similarity as weak pseudo-labels per sample by utilizing sample similarities to form local clusters [20, 23, 62, 63,95,98] or by utilizing mean-teacher networks to address the challenges posed by noisy pseudo-labels [78,80,83,95]. Nonetheless, the foundation of these approaches is contrastive learning, which has previously been shown to falter in fine-grained classification [18] due to strong augmentations in positives in comparison to nuanced visual differences between samples of the same category. To alleviate this, we introduce 'self-expertise' aimed at hierarchical learning of known and unknown categories. Our method is particularly effective in overcoming the limited availability of positive samples per category and enhancing the identification of subtle differences among negative samples, which we deem the biggest challenge in fine-grained classification.\nHierarchical Representation Learning. Different approaches benefit from hierarchical categories. Zhang et al. [96] use multiple label levels to enhance their representation through hierarchical contrastive learning. Guo et al. [25] extract pseudo labels for hierarchical contrastive learning, where signals are positive within the same cluster. We also use hierarchical pseudo-labels, but instead employ negative samples from the same cluster for generalized category discovery. Otholt et al. [57] and Banerjee et al. [4] proposed hierarchical approaches to address generalized category discovery. These works leverage neighborhood structures to delineate refined categories. Rastegar et al. [63] learn an implicit category tree, facilitating hierarchical self-coding of categories, which maintains category similarity across all hierarchy levels. Differing from these works, our method leverages weak supervision from samples within each level of the hierarchy, which reduces misclassification impact on lower levels. Additionally, our focus on hard negatives for unsupervised self-expertise enhances the model's ability to discern nuanced distinctions, leading to better fine-grained classification."}, {"title": "3 Theoretical Framework for Self-Expertise", "content": "Notations. We denote the number of total categories with K and the number of samples by N. For each random variable c, we indicate the number of associated samples by cl. We use 'ln' for the natural logarithm and \u2018lg' for log2. Problem Definition. The challenge of generalized category discovery lies in classifying samples during inference as belonging to categories encountered during training or as entirely novel categories. To describe this formally, throughout the training"}, {"title": "4 Self-Expertise for Generalized Category Discovery", "content": "Our proposed method for fine-grained generalized category discovery has three components: hierarchical pseudo-label extraction, unsupervised self-expertise, and supervised self-expertise. As illustrated in Fig. 3, each phase synergistically contributes to achieving discriminative clustering, which is pivotal for the task."}, {"title": "Hierarchical Pseudo-Label Extraction", "content": "This component addresses the challenge of optimizing supervisory signals while avoiding the erroneous allocation of unknown category samples to known categories. To achieve this, we implement a multi-tiered approach to pseudo-labeling for unlabeled samples, forming the foundation of our pseudo-label hierarchy.\nPseudo-label Initialization via Balanced Semi-Supervised K-means. We propose the Balanced Semi-Supervised K-means (BSSK) algorithm. This algorithm generates pseudo-labels for the initial level of the subsequently established pseudo-label hierarchy. BSSK starts by establishing K-means centers for known categories by determining cluster centers for already labeled data. For novel categories, we select an equivalent number of random samples as cluster centers, ensuring that each cluster maintains a uniform size. This process yields the base level of our hierarchy, aligning pseudo-labels with the granularity of ground truth categories.\nHierarchical Expansion and Abstraction. Based on BSSK, we introduce Hierarchical Semi-Supervised K-means (HSSK). For each subsequent kth level of abstraction, HSSK clusters the k-1th level's seen prototypes into half, effectively creating higher-level abstractions. All seen labels are projected onto these new hyperlabels. This is followed by BSSK, now with doubled cluster size compared to the previous level. This hierarchical structuring allows us to generate progressively abstracted and reliable pseudo-labels across various levels of category granularity. Pseudo-code for BSSK and HSSK is provided in the Appendix."}, {"title": "Unsupervised Self-Expertise", "content": "In our approach, we confront the challenges posed by pseudo-labeling in early training stages, where model proficiency with known and unknown labels is limited. Pseudo-labels generated during this phase are often noisy, leading to sub-optimal model training. To mitigate this, we integrate unsupervised contrastive learning. However, this technique focuses on distinguishing augmented versions of a sample from others, including those within the same semantic context, potentially aggravating the initial issue.\nTo address these concerns, we adopt a strategy where the model is instructed to exclusively distance samples within the same clusters (pseudo-labels). This tactic might seem counterintuitive at first glance. However, it is fundamentally based on the notion that distancing a visually similar sample within the same cluster can significantly enhance the purity of that cluster. In contrast, samples that are semantically similar but belong to different clusters are not considered negative instances. This allows the model to either assimilate these samples during the training phase via supervised contrastive learning or to segregate them from other clusters. Our approach also systematically shifts focus towards more abstract category levels while simultaneously diminishing the importance of negative samples from these broader clusters. For instance, in traditional unsupervised contrastive learning, samples i and j are associated with an identity target matrix I, where Iij=1(i=j). In contrast, our unsupervised self-expertise necessitates the recalibration of these targets to reflect the semantic similarity between the two samples. To illustrate, we define the pseudo-label for samples i and j at the hierarchical level k as $c_{ik}$ and $c_{jk}$, respectively. Consequently, we introduce an adjusted target matrix Y, comprising elements Yij, calculated as:\n$Y_{ij} = \\sum_{k=1}^{\\lg K} \\frac{1(c_{i} \\neq c_{j})}{2^{k}}$    (10)\nA comparison between the proposed adjusted target matrix and the conventional target matrix is illustrated in Fig. 4. Since the yijs will be interpreted as probabilities, the final Y target matrix should be normalized. As depicted in Fig. 4a, standard unsupervised contrastive learning treats all negative instances uniformly, thereby ignoring their semantic dissimilarities. Conversely, our unsupervised self-expertise employs a refined target matrix, where cat instances are classified as strict negatives in Fig. 4b, while the negativity of other instances is modulated according to their semantic distance from the positive instance. It is important to note that a linear combination of these target matrices can be employed, allowing for adjustment based on the specific granularity required by the task as:\n$\\hat{Y} = \\alpha Y + (1-\\alpha)I$,   (11)\nwhere a represents the hyperparameter associated with label smoothing. Through empirical analysis, we demonstrate that an increased value of a encourages the model to pay greater attention to more nuanced details. Conversely, a reduced a value renders the model more adept at handling tasks that require a broader, more general approach. As a result, for the contrastive logits P derived from our model, we use the binary cross entropy loss LBCE to formulate the unsupervised self-expertise loss, LUSE, as follows:\n$L_{USE} = L_{BCE}(P, \\hat{Y})$.    (12)"}, {"title": "Supervised Self-Expertise", "content": "Generated pseudo-labels from our hierarchical pseudo-label extraction are utilized as supervisory signals for the next epoch. Consider Lsk as the supervised contrastive learning specific to the pseudo-label level k. The aggregate supervised contrastive learning loss is represented by:\n$L_{SSE} = \\frac{1}{2}  \\sum_{k=0}^{2} \\frac{\\lg K}{2^{k}}  \\frac{C_{k}}{2^{k}} \\mathcal{L}(D)$  (13)\nwhere the term $\\mathcal{L}(D)$ reflects the supervised loss applied exclusively to the initial segment of the embedding vector D. This approach is grounded in the premise that higher hierarchy levels encounter an increased frequency of positive pairs. Yet, the ultimate objective is to learn pseudo-labels aligned with the ground truth labels. Consequently, the model is constrained to use only the first $\\frac{1}{2^{k}}$ segment of the embedding for differentiating pseudo-labels at hierarchy level k. This implies that for distinguishing between various pseudo-labels at level k-1, which share a common higher-level pseudo-label at level k, the embedding dimensions from $\\frac{1}{2^{k}}$ to $\\frac{1}{2^{k-1}}$ are utilized. When k=0, we only use groundtruth labels for samples in known categories and utilize the full embedding vector for supervised contrastive learning. This ensures accurate label assignment for known categories upon training completion and facilitates the generation of informative pseudo-labels for novel categories. Utilizing different abstraction levels, we apply supervised contrastive learning to samples within the same cluster at different levels of hierarchy. The application of our supervised expertise to the representation is illustrated in Fig. 5. Finally, for the tunable hyperparameter \u03bb, our overall self-expertise loss function is expressed as:\n$L_{SE} = (1 - \\lambda)L_{USE} + \\lambda L_{SSE}$.   (14)"}, {"title": "5 Experiments", "content": "Datasets. We assess the efficacy of our approach on four fine-grained datasets: CUB-200 [79], FGVC-Aircraft [51], Stanford-Cars [42] and Oxford-IIIT Pet [61]. Additionally, we demonstrate the adaptability of our method to more coarse-grained datasets CIFAR10 [43], CIFAR100 [43] and ImageNet-100 [19], highlighting its broader applicability beyond fine-grained classification tasks. Finally, in the Appendix experiments section, we report on the challenging Herbarium-19 dataset [74], which is fine-grained and long-tailed, to show that our approach is effective even with non-uniform category distributions. Detailed statistics of the datasets along with their train/test splits are also provided in the Appendix.\nImplementation Details. In our experiments, we adhered to the dataset division proposed by Vaze et al. [77], where half of the categories in each dataset are designated as known, except for CIFAR100, where 80% are used as known categories. The labeled set consists of 50% of the samples from these known categories. The remainder of the known category data, along with all data from novel categories, comprise the unlabeled set. Following [77], we use ViT-B/16 as our backbone, which is either pre-trained by DINOv1 [11] on unlabelled ImageNet 1K [44], or pretrained by DINOv2 [56] on unlabelled ImageNet 22K. We use the batch size of 128 for training and set \u5165=0.35. For label smoothing, we use a=0.5 for fine-grained datasets and a=0.1 for coarse-grained datasets. Different from [77], we froze the first 10 blocks of ViT-B/16 and fine-tuned the last two blocks instead of only the last one to have more parameters given that for each level, only a fraction of the latent dimension is considered."}, {"title": "5.2 Comparison with State-of-the-Art", "content": "Fine-grained image classification. We evaluate our model's effectiveness across three fine-grained datasets in Tab. 1. The results demonstrate our method's capability in handling fine-grained categories, as it consistently outperforms others in both all and novel category classification within these datasets. The success can be attributed to the model's hierarchical approach to category analysis, which is pivotal in differentiating between closely related categories that demand acute attention to specific details. Additionally, as indicated in Table 2, our method also leads in performance for both all and novel categories in the Oxford Pet dataset. Despite its small size, which typically poses a risk of overfitting, our model's strong performance on this dataset further indicates its robustness."}, {"title": "5.3 Ablative studies", "content": "We evaluate the individual effects of method components in this section. All ablative experiments are performed on CUB with the DINOv1 backbone. We present additional ablations, time complexity, and failure cases in the Appendix.\nEffect of each component. Tab. 4 (a) examines the effect of our three key method components: Hierarchical Semi-Supervised K-means (HSSK), unsuper-vised self-expertise (LUSE), and supervised self-expertise (LSSE). The results demonstrate that the Hierarchical Semi-Supervised K-means approach yields the most significant improvements across both known and novel categories. Our unsupervised self-expertise loss, denoted as LUSE, shows a particular affinity for enhancing known categories. This is in line with our initial hypothesis, considering that these categories benefit from supervision signals. Such signals facilitate the attraction of semantically similar samples, even if they are initially distant in the embedding space. Concurrently, this approach effectively disregards semantically similar yet distant negative samples, preventing any repulsion until they converge into the same cluster. When integrated with hierarchical semi-supervised k-means, the unsupervised self-expertise loss extends its benefits to novel categories, leveraging the presence of semantic labels. Our supervised self-expertise loss, LSSE, unsurprisingly excels in aiding novel categories while also contributing positively to known ones. We attribute this to the fact that, although hierarchical structures are advantageous for known categories with robust label-based supervision, novel categories lack such ground-truth labels. As a result, pseudo-labels at finer granularities may introduce noise. However, as we ascend the hierarchy, these pseudo-labels for novel categories gain reliability, offering more effective supervision. In conclusion, the combination of all three components - hierarchical semi-supervised k-means, unsupervised self-expertise, and supervised self-expertise - yields the most optimal results for both known and novel categories, as demonstrated in our experiments.\nThe effect of hierarchy level. In Tab. 4 (b), we compare model performance across varying hierarchy levels. These hierarchy levels are incorporated into the training phase for all three model components. Specifically, the Baseline component employs supervised contrastive learning using only the ground-truth labels, which are limited to samples that have been labeled. Level 1 is identified as the base level of the hierarchy, utilizing pseudo-labels that offer semantic detail comparable to ground-truth labels, thereby enriching our dataset with an additional 200 pseudo-labels for samples without labels. As we ascend through the hierarchy levels, the quantity of pseudo-labels decreases by half, as detailed in the accompanying table, until reaching the apex level. This topmost level introduces the most abstract categorization, distinguishing between 'seen' and 'unseen' samples. The results depicted in Tab. 4 (b) indicate a notable trend: integrating additional hierarchical levels appears to be particularly advantageous for unknown categories. This observation can be attributed to increased granularity between categories at finer hierarchy levels, resulting in heightened uncertainty and noise in pseudo-labels. This phenomenon underscores the efficacy of our model in handling complex, hierarchical category structures, especially in scenarios involving unknown category distinctions."}, {"title": "6 Conclusion", "content": "This work presents self-expertise in identifying and categorizing known and previously unknown categories, focusing on fine-grained distinctions. We introduce a method that utilizes hierarchical structures to effectively bridge the gap between labeled data for known categories and unlabeled data for novel categories. This is achieved by generating hierarchical pseudo-labels, which guide both supervised and unsupervised learning phases of our self-expertise framework. The supervised phase is designed to incrementally increase the complexity of differentiation tasks, thereby accelerating the training process and enhancing the formation of distinct clusters for unknown categories. This strategy improves the model's ability to generalize to novel categories. In the unsupervised phase, we integrate a label-smoothing hyperparameter, compelling the model to concentrate on negative samples within a localized context and to make finer distinctions. This approach enhances the model's fine-grained categorization capabilities. Overall, our work demonstrates the effectiveness of self-expertise in handling unknown and fine-grained categorization tasks. In the Appendix section titled 'Discussions,' we outline the limitations of our work and propose directions for future research."}, {"title": "A Related Works", "content": "To provide a more in-depth analysis of the relevant literature, this appendix section delves into additional related works."}, {"title": "A.1 Self-Supervised Learning", "content": "Self-supervised learning has transformed the analysis of large-scale unlabeled data, learning rich representations. He et al. [30] introduced the concept of momentum contrast with MoCo, enhancing the quality of learned representations by utilizing two encoders and a contrastive loss to learn image features that distinguish between different views [30]. Following this, Chen et al. [15] simplified the self-supervised learning pipeline with SimCLR, by maximizing agreement between different augmented versions of the same image, using a contrastive loss function [15]. Caron et al. [10] introduced SwAV, which employs a clustering mechanism to enhance consistency between cluster assignments, thereby improving learning efficacy [10]. By employing a self-distillation with information noise injection, DINO [11] efficiently captures complex visual features without labeled data. DINOv2 [56] further refines this approach with key improvements, enhancing both feature quality and model adaptability."}, {"title": "A.2 Open-Set Recognition", "content": "The advent of open set recognition marked a significant milestone in the evolution of computational models, addressing the complexities of processing real-world data. This domain was first conceptualized by Scheirer et al. [70], who laid the foundational theory for models capable of discerning between known and unknown data categories, a principle further developed by subsequent research [5, 69]. The pioneering application of deep learning techniques to tackle open-set recognition challenges was introduced through the development of OpenMax [6]. The core objective within open-set recognition is the accurate identification of known categories while effectively filtering out novel category samples. Various methodologies have been proposed to emulate the concept of \"otherness\" essential for distinguishing unknown categories. These approaches range from identifying significant reconstruction errors [60,90], measuring the deviation from a set of predefined prototypes [13, 14, 71], to differentiating samples generated through adversarial processes [24, 41, 52, 92]. Despite its advances, a notable limitation of open-set recognition is its inclination to disregard all instances of novel classes, potentially omitting valuable information. This challenge underscores the ongoing quest for more sophisticated models capable of not only identifying the unknown but also accommodating the continuous expansion of the knowledge domain."}, {"title": "A.3 Novel Category Discovery", "content": "The foundation for this line of inquiry was laid by Han et al. [28], who adapted classification models to recognize novel categories based on knowledge from known categories. Initial strategies, as documented in several studies [26,32,33], typically"}, {"title": "A.4 Generalized Category Discovery", "content": "Generalized category discovery was introduced by Vaze et al. [77] and Cao et al. [8]. It provides models with unlabeled data from both novel and known categories, placing it within the realm of semi-supervised learning, a domain thoroughly investigated in the machine learning literature [12,54,58,65,88]. The unique challenge in generalized category discovery is handling categories without any labeled instances, which introduces additional complexity. There are primarily two approaches to address this challenge. The first employs a series of prototypes as reference points, e.g., [2,16,17,29,39,75,81,83,86,87,94]. The second approach leverages local similarity as weak pseudo-labels for each sample and utilizes sample similarities to form local clusters [4, 16, 20, 23, 29, 57, 62, 63, 95, 98]. Studies such as [78,80,83,95] employ mean-teacher networks to tackle the issues arising from noisy pseudo-labels. Additionally, other research efforts leverage cues from alternative modalities to identify categories in a multimodal fashion [1,59,99]. Our approach introduces 'self-expertise', a novel concept aimed at hierarchical learning of known and unknown categories. This technique emphasizes the focus on the samples from identical clusters at each level. This method is particularly effective in overcoming the limited availability of positive samples per category, while also enhancing the identification of subtle differences among negative samples."}, {"title": "A.5 Hierarchical Representation Learning", "content": "In the realm of leveraging hierarchical categories for enhanced representation learning, several approaches have been introduced. Zhang et al. [96] employ multiple label levels to augment their models' representational capacity through hierarchical contrastive learning. Similarly, Guo et al. [25] extract pseudo-labels to facilitate hierarchical contrastive learning, with a unique emphasis on ensuring signals remain positive within identical clusters. The hierarchical structure of categories has been explored in previous works such as [45,85], aiming to enhance the identification of novel categories within the context of open-set recognition. Additionally, Rastegar et al. [64] utilize a hierarchical structure for multimodal data to infer missing modalities, which can be categories. An alternative perspective"}, {"title": "B Theory", "content": "In this section, we motivate our approach of hierarchical contrastive learning and why it is particularly well-suited for category discovery in unseen data."}, {"title": "B.1 Notations and Definitions", "content": "Let's consider the simple Bayesian networks depicted in Fig. 8. Here, xi and xj are different samples or different views of the same sample which are observed. Their corresponding ground truth context variables ci and cjs are variables we aim to extract information about. These context variables can be partly observed in the form of labels, as is the case for supervised contrastive learning. However, these context variables are unobserved for unsupervised contrastive learning, which we show with zi and zj, respectively. Random variable y will indicate if its two parents have the same value, or in other terms; it will provide the contrastive labels.\nLet's assume that we have K total categories and the number of total samples as N. For each context variable c, we show the number of samples assigned to it by cl. Hence, zi indicates the number of samples that the model has assigned to modulo set {zi}K and |ci| indicates how many samples the true distribution has assigned to labels {ci}K. Finally, we show the true distribution of samples with p and the approximated one with p. In the next sections, we use labels instead of ground truth context variables, but note that these labels can have different hierarchy levels from the ground truth labels. To prevent ambiguity, we emphasize ground truth labels whenever we use them."}, {"title": "B.2 Problem Definition", "content": "For contrastive training, the goal is to estimate the true distribution of equality of the ground truth context variables. Hence, for samples i and j, the goal is to approximate the following true distribution,\n$p(y=1|c_i, c_j) = 1(c_i=c_j)$,   (15)\nin which 1 is the identity operator, which is one only when its inner condition holds and zero otherwise. Depending on the problem we aim to solve, in training, we have no access to the ground truth context variables as in unsupervised contrastive learning, or we have partial access as in supervised contrastive learning. In both contrastive learning formulations, we aim to minimize the KL divergence between the true distribution p and our model distribution p^\\u02c6 which in case of unsupervised contrastive learning will be,\n$D_{KL} [p(y|c_i, c_j) || \\hat{p}(y|x_i, x_j)]$,    (16)"}, {"title": "B.3 Supervised Contrastive Learning", "content": "Theorem 1. Consider two samples i and j from the Bayesian network Fig. 8. If we have a total of N samples and K categories and the dataset is balanced, we will have the following upper bound if only i's label ci is known:\n$D_{KL} [p(y|c_i, c_j) || \\hat{p}(y|c_i, x_j)] \\leq \\ln \\frac{N}{K}$   (18)\nProof. According to the Bayesian network shown in Fig. 8, we have:\n$p(y|c_i, x_j) = \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)p(y|c_i, z_j)$.   (19)\nNote that since $\\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)=1$ and p(y|ci, cj) is independent of zj , we can consider that\n$p(y|c_i, c_j) = \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)p(y|c_i, c_j)$.   (20)\nAlso, since KL divergence is convex, we can have the following inequality:\n$D_{KL}[\\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)p(y|c_i, c_j) || \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)\\hat{p}(y|c_i, z_j)] $\n$\\leq \\sum_{Z_j=1}^{K} \\hat{p}(z_j|x_j)D_{KL}[p(y|c_i, c_j)|\\hat{p}(y|c_i, z_j)].$    (21)\nIf we use equations Eqs. (19) to (21) on the left-hand side of the inequality Eq. (18), we will have the following:\n$D_{KL}[p(y|c_i, c_j)|\\hat{p}(y|c_i, x_j)] =D_{KL}[p(y|c_i, c_j)| \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)\\hat{p}(y|c_i, z_j)]$\n$=D_{KL}[\\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)p(y|c_i, c_j) || \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)\\hat{p}(y|c_i, z_j)]$\n$\\leq \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)D_{KL} [p(y|c_i, c_j)|\\hat{p}(y|c_i, z_j)],$"}, {"title": "B.4 Unsupervised Contrastive Learning", "content": "For the unsupervised scenario, both labels are unknown, this means that we only have access to the inputs xi and xj. This means that for unlabelled samples they will follow the Bayesian network shown in Fig. 10. We can state a similar theorem for the unsupervised case as follows:\nTheorem 2. Consider two samples i and j from the Bayesian network Fig. 8. If we have a total of N samples and K categories and the dataset is balanced, we will have the following upper bound if neither of i and j labels is known:\n$D_{KL} [p(y|c_i, c_j) || \\hat{p}(y|x_i, x_j)] \\leq \\ln \\frac{N}{K}$   (34)\nProof. According to the Bayesian network shown in Fig. 10, we have:\n$\\hat{p}(yX_i, x_j) = \\sum_{z_i=1}^{K} \\hat{p}(z_i|x_i) \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)\\hat{p}(y/z_i, z_j)$.   (35)\nNote that since $\\sum_{z_i=1}^{K} \\hat{p}(z_i|x_i)=1$ and $\\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)=1$ and p(y|ci, cj) is independent of zi and zj, we can consider that\n$p(y|c_i, c_j) = \\sum_{z_i=1}^{K} \\hat{p}(z_i|x_i) \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)p(y|c_i, c_j)$.   (36)\nAlso, since the KL divergence is convex, we can have the following inequality:\n$D_{KL}[\\sum_{z_i=1}^{K} \\hat{p}(z_i|x_i) \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)p(y|c_i, c_j) || \\sum_{z_i=1}^{K} \\hat{p}(z_i|x_i) \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)\\hat{p}(y|z_i, z_j)]$\n$\\leq \\sum_{z_i=1}^{K} \\hat{p}(z_i|x_i) \\sum_{z_j=1}^{K} \\hat{p}(z_j|x_j)D_{KL} [p(y|c_i, c_j)|\\hat{p}(y|z_i, z_j)].$   (37)"}, {"title": "B.5 Self-Expertise", "content": "In the paper, we introduced an upper bound for a dataset comprising N samples across K categories, denoted as Sk. To further refine our understanding, we propose an alternative upper bound, SK, which leverages the hierarchical structure inherent among the categories. This hierarchical framework can be conceptualized as a Markov chain, where each category's relevance is determined independently of the samples and hyper-labels, except for its immediate predecessor and its children in the hierarchy. Our objective is to delineate the conditions under which \u015ck serves as an effective upper bound, taking into account the hierarchical relationships among categories. This approach aims to capture the nuanced dependencies within the category structure, thereby providing a more granular and accurate upper limit for the distribution of samples among the categories.\n$\\sum_{I=1}^{\\lg K} D_{KL} [P(y|c_{I}', c_{j}')||\\hat{p}(y|c_{I}', x_{j}, c_{j}^{I-1})]\\leq \\hat{S}_{K}$,   (38)\nlet I denote the hierarchy level. Consider that at level l, given K distinct categories, each cluster $c_{j}^{I-1}$ contains $\\frac{N}{K}$ samples, as opposed to the full N samples without considering the hierarchy. Upon substituting these adjusted values for K and $\\frac{N}{K}$ into the established upper bound in Eq. (18), we obtain the following:\n$S_K=\\lg K \\ln \\frac{N}{K^2}$, $\\hat{S} =(\\lg K - 1) \\ln \\frac{4N}{K^2}$ $\\implies \\hat{S}_{K} \\leq \\hat{S}$.   (39)\nEmploying analogous reasoning for the unsupervised self-expertise component, we derive:\n$\\hat{U}_K \\leq \\hat{U}$.   (40)"}, {"title": "C Method", "content": "For a deeper understanding of the proposed methods, this appendix section provides a detailed explanation and corresponding pseudocode for the algorithms briefly introduced in the main text."}, {"title": "C.1 Balanced Semi-Superivsed K-means", "content": "Our algorithm consists of three key stages:\n1. Semi-Supervised K-means Centers Initialization,\n2. Updating novel categories while balancing clusters,\n3. Semi-supervised K-means for final assignment based on the refined centers."}, {"title": "Semi-Supervised K-means Centers Initialization", "content": "In our proposed method, Balanced Self-Supervised K-means (BSSK), the initial step involves the identification of K-means cluster centers for pre-labeled categories. This process is executed by calculating the cluster centers from the labeled data corresponding to the known categories. In scenarios involving balanced datasets characterized by a cluster size C, we enhance dataset purity by excluding the C nearest samples surrounding each data point to mitigate cluster overlap and ensure distinct cluster formation. Subsequently"}]}