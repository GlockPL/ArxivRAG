{"title": "LSERVE: EFFICIENT LONG-SEQUENCE LLM SERVING WITH\nUNIFIED SPARSE ATTENTION", "authors": ["Shang Yang", "Junxian Guo", "Haotian Tang", "Qinghao Hu", "Guangxuan Xiao", "Jiaming Tang", "Yujun Lin", "Zhijian Liu", "Yao Lu", "Song Han"], "abstract": "Large language models (LLMs) have shown remarkable potential in processing long sequences, yet efficiently\nserving these long-context models remains challenging due to the quadratic computational complexity of attention\nin the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these\nissues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse\nattention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and\ndecoding attention into a single framework, where computations on less important tokens are skipped block-wise.\nLServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design\nenables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention\nheads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a\nconstant number of KV pages is required to preserve long-context capabilities, irrespective of context length. We\nthen design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric\nsimilarity. On average, LServe accelerates LLM prefilling by up to 2.9\u00d7 and decoding by 1.3-2.1\u00d7 over vLLM,\nmaintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have dramatically trans-\nformed the field of artificial intelligence. With expand-\ning context window lengths, LLMs now demonstrate re-\nmarkable performance across diverse long-sequence appli-\ncations (Gemini Team, Google, 2024), including multi-turn\nconversations, long document analysis (Zhang et al., 2024b;\nGoyal & Durrett, 2020; Huang et al., 2021), multi-modal\nunderstanding (Xue et al., 2024a; Liu et al., 2024b; Lin et al.,\n2024a), and code completion (Li et al., 2023; Lozhkov et al.,\n2024). Many of these applications require processing hun-\ndreds of thousands of context tokens in real-world settings,\npresenting unique challenges. In particular, the demand\nfor fast prefilling, or minimizing the time to the first token,\nand the burden on the decoding phase due to the large KV\n(key-value) caches necessary for such contexts, represent\nsignificant hurdles.\nLong-sequence LLMs are about more than just an extended\ncontext. The recently announced OpenAI 01 (OpenAI,\n2024) demonstrates exceptional capabilities in complex rea-\nsoning tasks, such as deciphering, mathematics, and cod-"}, {"title": "2 BACKGROUND AND MOTIVATION", "content": "LLM Inference. LLMs are transformer-based architec-\ntures with stacked identical layers, each containing attention\nblocks, feed-forward networks (FFN), and normalization\ncomponents. LLM inference involves two stages: an initial\nprefilling stage that handles multiple tokens concurrently,\nfollowed by auto-regressive decoding stage where only one\ntoken will be processed for each request in a decoding step.\nAttention. The attention mechanism exchanges informa-\ntion across tokens. It first transforms input x through lin-\near projections to generate query vectors $q \\in R^{N\\times H_D}$,\nand key-value pairs $k, v \\in R^{N\\times \\hat{H}D}$, where $\\hat{H}$ represents\nthe key/value head count. Traditional multi-head attention\n(MHA) maintains $H = \\hat{H}$, and contemporary architectures\n(Touvron et al., 2023; Jiang et al., 2023; 2024a) employ\ngrouped-query attention (GQA) (Ainslie et al., 2023) where\n$H = n\\hat{H}(n \\in Z)$ to shrink the size of KV cache. The\ncurrent k and v is then concatenated with KV cache from\nS preceding tokens, yielding $K, V \\in R^{(S+N)\\times \\hat{H}D}$. The\nattention computation can then be formulated as follows:\n$S_h = \\frac{q_h K_h^T}{\\sqrt{D}}$, $o_h = \\text{softmax} (S_h) V_h$,$h=\\overline{1..H}$                                                                                                                                           (1)\nTherefore, the complexity of attention can be expressed"}, {"title": "3 LSERVE: LONG-SEQUENCE SERVING\nWITH UNIFIED SPARSE ATTENTION", "content": "We introduce LServe, an efficient long-sequence LLM serv-\ning system featuring sparse attention. In LServe, diverse"}, {"title": "3.1 Unified Block Sparse Attention", "content": "As shown in Figure 3, skipping computations in the atten-\ntion kernel by blockwise processing accelerates execution\nby shortening the sequential loop. Building on this, we\nintroduce a unified block sparse attention pattern for both\nthe prefilling and decoding stages: each thread block com-\nputes a $T_Q \\times T_K$ tile (and $T_K \\times T_V$) in parallel. Here,\n$T_Q > 1$ in the prefilling stage and $T_Q = 1$ in the decoding\nstage, with $T_K$ (or $T_V$) corresponding to the page size in\nPagedAttention (Kwon et al., 2023a).\nWe define block sparsity in LServe as follows: for each\n$T_Q \\times T_K$ tile in the attention calculation, it is either fully\nskipped (Figure 4(b), light gray blocks) or retained as in\nstandard causal attention (Figure 4(b), blue blocks). Given\nthat each GPU streaming multiprocessor can execute only a\nlimited number of thread blocks simultaneously, the atten-\ntion kernel execution time can be approximated by the total\ncount of $T_Q \\times T_K$ (and $T_K \\times T_V$) blocks. With a block\nsparsity of r, where $rN$ of the N total blocks are empty, the\ntheoretical speedup from block sparse attention is $1/(1 - r)$.\nFor example in Figure 4(b), 10 out of N=21 blocks are\nnon-empty. Thus, the theoretical speedup ratio is 2.1\u00d7.\nFigure 4(c)(d) shows two sparsity patterns used in LServe.\nThe first is streaming attention (Figure 4(c)), a specialized\nform of block-sparse attention where each token only at-\ntends to its immediate neighbors and initial tokens, known"}, {"title": "3.2 LServe System Overview", "content": "We present an overview of LServe in Figure 5. Built on\nQServe, which natively supports quantized LLMs, LServe\nenhances the baseline system by introducing sparsity into\nboth prefilling and decoding dataflows. The two-way paged\nKV cache serves as the bridge between these two stages.\nAs discussed in Section 3.1, we statically partition the at-\ntention heads of a pretrained LLM into two groups: dense\nheads and streaming heads. Unlike conventional LLM serv-\ning systems, which maintain a single KV cache, we utilize\nseparate KV caches for the dense and streaming heads. The\nKV cache for the streaming heads is organized similarly to\nthe pages in QServe, with scaling factors and zero points\nstored immediately after the token features. Additionally,\nthe KV cache for the dense heads includes key statistics that\nfacilitate critical page selection during the decoding stage.\nIn the prefilling stage, the key differences between LServe\nand conventional dense-attention LLM serving systems are\ntwofold: (1) we replace the dense attention kernel with our\nunified block sparse attention kernel, and (2) we write back\nquantized KV features using two distinct kernels.\nIn the decoding stage, our system incorporates dynamic\nattention sparsity. Rather than developing an entirely new\ndynamic sparse attention kernel, we decompose the problem\ninto two components: (1) dynamic page selection and (2)\na dense attention kernel with shorter page tables, where\nthe shorter page tables are provided by the page selector.\nNotably, our page selector employs hierarchical paging and\nreusable page selection, enhancing both long-context accu-\nracy and page selection efficiency."}, {"title": "3.3 Prefilling Stage: Sparsity Determination", "content": "We adopt the approach from DuoAttention (Xiao et al.,\n2024) to classify each attention head as either a re-\ntrieval head or a streaming head. Using DuoAttention's\noptimization-based identification method, we obtain a gat-\ning value a \u2208 [0, 1] for each head, where values closer to\n1 signify a retrieval head, and values closer to 0 indicate a\nstreaming head. To classify a head as a retrieval head, we\ncompare a to a threshold 7, determined by a sparsity quan-\ntile. For instance, with a target sparsity of 50% across atten-\ntion heads, 7 equals the median of all gate values, thereby\ndesignating half of the heads as retrieval heads."}, {"title": "3.4 Prefilling Stage: Kernel Implementation", "content": "To effectively translate sparsity into performance gains, it\nis essential to avoid iterating over a complete sequential\nloop and relying on conditional statements to determine\ndata loading and computation requirements. This method\nis inefficient for GPU computation patterns, which thrive\non minimizing branching within loops. Instead, we should\nfocus on iterating only over the necessary blocks by accu-\nrately calculating offsets to load data and assess whether a\nblock should be processed.\nTo facilitate this, we introduce an iterator-based abstraction\nthat standardizes indexing operations. This allows us to loop\nexclusively over the blocks requiring computation, with data\noffsets easily computed using $offset = iter(i + 1) \u2013 iter(i)$.\nThis abstraction efficiently skips unnecessary blocks with\nminimal overhead and necessitates few changes to the kernel\nfunction, thus enhancing maintainability. Take the streaming\nheads as an example, the iterators are determined outside the\nattention kernel since streaming heads are configured offline\nand the attention pattern is fixed. Once the attention on sink\ntokens is complete, the iterator automatically updates the\nmemory pointer to the first local token in the KV cache\nwith minimal overhead. Additionally, our iterator-based\nformulation unifies the more general block sparse pattern\n(see Figure 4)."}, {"title": "3.5 Decoding Stage: Sparsity Determination", "content": "To further enhance the long-context LLM decoding through-\nput, we introduce dynamic sparsity upon the input-agnostic\nstatic sparsity in Sec. 3.1."}, {"title": "3.5.1 Challenge: the Page Size Dilemma", "content": "In the decoding stage, the attention operation is memory-\nbound, so state-of-the-art systems typically implement KV\ncache quantization to reduce device memory usage and\nenhance throughput. However, this quantization introduces\nchallenges for further optimization. Specifically, reducing\nthe bit-width of KV tokens necessitates larger page sizes\nto maintain GPU memory bandwidth utilization. Failure to\ndo so can lead to significant throughput loss (Table 1). Yet,"}, {"title": "3.5.2 Hierarchical Paging: Mitigating the\naccuracy-efficiency tradeoff", "content": "We observe that the failure of query-aware KV cache selec-\ntion paradigm (Figure 6) is not due to the coarser granularity\nof sparse attention (i.e., larger page size). Rather, the under-\nlying cause lies in that page-wise statistical indicators be-\ncome homogenized and less representative especially when\nthere are excessive tokens within a single page. To address\nthis issue, we design a simple-yet-effective hierarchical pag-\ning system that introduces an abstract layer of virtual logical\npage for estimating token criticality, while preserving the\noriginal memory layout of KV cache in (physical pages).\nAs illustrated in Figure 7, our hierarchical paging groups\n$N_L$ tokens into a logical page and $N_p$ tokens into a phys-\nical page ($N_p = g \\cdot N_L, g \\in Z$), that is, a physical page\ncontains g logical pages. Tokens within the same logical"}, {"title": "3.5.3 Reducing sparse attention overheads with locality", "content": "One remaining question is: as physical page size increases,\nwill the hierarchical paging require a higher token budget\nfor sparse attention to retain accuracy?\nGiven a generation step, assume the most important history\ntokens are distributed in a logical page set $P = {P(i, j)}$,\nwhere $i \\in {1,2,...}, j \\in {a,b,, ...}$ are the physical and\nlogical index of a page accordingly. If these important to-\nkens are randomly and sparsely distributed in the context,\nchances are that all logical pages in P are scattered in dif-\nferent physical pages, that is, for any $P_1, P_2 \\in P, i_1 \\neq i_2$.\nIn this case, all |P| physical pages (|P|\u00b7 $N_p$ tokens) are se-\nlected to avoid losing important information. However, the"}, {"title": "3.6 Decoding Stage: Kernel Implementation", "content": "During the decoding stage, attention heads are processed in\nparallel on GPU, enabling different sparsity patterns to be\napplied independently on each head. This flexibility enables\nsome heads to operate with page-level sparsity while others\nfollow the streaming computation pattern.\nTo leverage this, we employ a two-level indexing hierarchy\nto unify the operations for streaming heads and dense heads\nwith dynamic sparsity. Specifically, the low-level (physi-\ncal) index corresponds to the iteration step of current GPU\nthread, which executes in a consecutive manner as in dense\nattention, while logical index denotes the actual position\nof the target token within the entire KV cache. For each\ndense head, the page selector provides an index table to map\nphysical index to logical index. Streaming heads are treated\nas dynamic sparse heads with index table only containing\nthe sink and local pages."}, {"title": "4 EVALUATION", "content": "Implementation. We implement LServe in CUDA and PTX\nassembly on the basis of QServe (Lin et al., 2024b) and\nTensorRT-LLM (NVIDIA, 2023) system. The specialized\nCUDA kernels are compiled into PyTorch extensions for\nbetter flexibility and compatibility with the purely PyTorch-\nbased serving interface.\nTestbed. Our primary experiments are conducted on a server\nequipped with 8 NVIDIA A100 80GB GPUs, 2 AMD EPYC\n7763 CPUs (128 cores), and 2TB of memory. Unless ex-\nplicitly stated, all experiments utilize the A100 GPUs. Ad-\nditionally, we perform some evaluations on a cloud instance\nwith a single NVIDIA L40S 48GB GPU to assess system\nperformance across different GPU architectures. All evalua-\ntions use PyTorch 2.5.0 with CUDA 12.4 and cuDNN 9.2.0.\nModels. To comprehensively assess system performance\nacross various LLM architectures, we utilize the widely\nadopted GQA-based model Llama-3-8B (Dubey et al.,\n2024), the MHA-based model Llama-2-7B (Touvron et al.,\n2023), and the smaller-scale model Minitron-4B (Muralid-\nharan et al., 2024). Additionally, to support long-context\ninference, we employ the context-extended Llama-3-8B\nversion Gradient (Pekelis et al., 2024).\nMetrics. Our primary focus is on serving throughput. For"}, {"title": "4.3 End-to-end Efficiency", "content": "Decoding Efficiency. Figure 10 presents the efficiency\nbenchmarking results for the decoding stage. We use the\nsame sparsity configurations as in Section 4.2. Compared\nwith the state-of-the-art serving systems, LServe demon-\nstrates significant and consistent efficiency improvements\nacross different GPU platforms and model architectures.\nOn Llama-3-8B and Minitron-4B, LServe achieves 1.5\u00d7\naverage speedup over vLLM. For MHA-based model Llama-\n2-7B, LServe runs more than 2.0\u00d7 faster than baselines on\naverage. Additionally, we demonstrate that LServe also\nfunctions well on other GPU devices such as L40S with\nAda Lovelace Architecture. LServe achieves up to 1.7\u00d7\nspeedup over vLLM.\nPrefilling Efficiency. In Figure 11, we compare the prefill-\ning speed of LServe against 4 baselines on Llama-3-8B and"}, {"title": "4.4 End-to-End Comparison with Quest", "content": "We also compares our system against Quest (Tang et al.,\n2024) in Table 4. Across different sequence lengths, LServe\nconsistently outperforms Quest in both prefilling (1.6-2.1\u00d7\nspeedup) and decoding stages (1.3-1.5\u00d7 speedup)."}, {"title": "5 ANALYSIS", "content": "In this section, we present in-depth analysis on our design\nchoices in the LServe system from both the accuracy and\nthe efficiency perspective. We also scrutinize the sources of\nperformance gains in Section 4."}, {"title": "5.1 Prefilling Stage Sparse Attention Kernel", "content": "We benchmark the performance of our block sparse attention\nkernel for the prefilling stage in Figure 12. Compared with\nthe implementation by MInference (Jiang et al., 2024b),\nour kernel consistently achieves 1.3\u00d7 speedup at the same\nsparsity level. Oracle stands for the theoretical upper-bound\nspeedup ratio: $Latency_{oracle} = \\frac{Latency_{dense} * (1 \u2013 sparsity)}{1}$."}, {"title": "5.2 Effectiveness of Hierarchical Paging", "content": "We use the Needle-in-a-Haystack (Kamradt, 2024) test to\ndemonstrate that the hierarchical paging design effectively\nmaintains the model's long-context capability on larger page\nblocks without increasing the token budget. In contrast to\nthe performance drop observed with increased page gran-\nularity in Figure 6, LServe leverages a hierarchical page\nstructure to decouple the pruning algorithm's page granular-\nity from the physical memory layout of the KV cache. This\napproach enables our sparse attention mechanism to remain\nboth accurate and hardware-efficient. Figure 13 highlights\nthis improvement: with a page size of 64 and the same to-\nken budget, LServe achieves accuracy comparable to the\nbaseline algorithm (Tang et al., 2024), which prunes history\ntokens at a granularity of 16."}, {"title": "5.3 Mitigating Page Selection Overhead", "content": "Reusable Page Selection. During decoding, although the\nattention kernel maintains constant complexity due to a\ncapped number of historical KV tokens, the complexity of\nthe page selector still scales linearly with sequence length.\nAs illustrated in Figure 14, for a sequence length of 128K\nand a 4K token budget for sparse attention, the page selector\n(0.24 ms) is already twice as slow as the sparse attention\nkernel (0.12 ms). With our reusable page selector, however,\nLServe significantly reduces page selection overhead by a\nfactor of C, where C is the reuse interval. We further show\nthat LServe is resilient to different reuse interval choices.\nTable 5 demonstrates no significant performance degrada-\ntion until the reuse interval exceeds 8, so we set it to 4 by\ndefault in LServe.\nContext Pooling Overhead. To enable page selection\nduring decoding, we must calculate representative features\nusing min-max pooling in the prefilling stage. It is important\nto note that a single pooling kernel executes under 1 ms,\nwhile the entire prefilling stage completes in approximately\n17 seconds with 128K context length. Consequently, the\ncontext pooling overhead is negligible."}, {"title": "5.4 Sparse Attention Kernel for Decoding Stage", "content": "We analyze the effectiveness of different sparsity patterns\nin decoding attention. In Figure 15, we apply static sparsity\nby converting 50% of attention heads to streaming heads,\nachieving a 1.3-1.7\u00d7 speedup across various input sequence\nlengths. Additionally, we introduce dynamic sparsity with\na fixed KV budget of 4096 tokens, which bounds the com-\nputational complexity of decoding attention to a constant,\ndelivering a 30\u00d7 speedup over the dense baseline for an in-\nput length of 256K. Although sparsity selection introduces\nminor overhead for shorter sequences, this is mitigated by\nreusable page selection. Additionally, we also perform the\nend-to-end ablation study in Section 5.5."}, {"title": "5.5 End-to-End Speedup Breakdown", "content": "In Figure 16, we highlight the sources of performance im-\nprovement in LServe. By leveraging static sparsity, LServe\nachieves end-to-end speedups of up to 1.7\u00d7 over the dense\nbaseline. Additionally, dynamic sparsity, aided by a reusable\npage selector, significantly reduces generation latency, yield-\ning a 7.7\u00d7 speedup for sequence lengths of 256K. Lastly,\nLServe configures sparse patterns through offline profiling,\neffectively avoiding slowdowns from dynamic sparsity at\nshorter context lengths."}, {"title": "6 RELATED WORK", "content": "LLM Serving Systems. Various systems have been devel-\noped to enhance LLM deployment efficiency. Orca (Yu et al.,\n2022) uses iteration-level scheduling and selective batch-\ning for distributed systems. vLLM (Kwon et al., 2023b)\nintroduces PagedAttention, inspired by virtual memory, to\noptimize KV cache management. TensorRT-LLM (NVIDIA,\n2023) is the industry's leading solution also featuring in-\nflight batching and PagedAttention inspired by vLLM.\nLightLLM (Contributors, 2023a) further reduces mem-\nory waste in PagedAttention by introducing TokenAtten-\ntion. SGLang (Zheng et al., 2023) advances LLM pro-\ngramming with a domain-specific language and RadixAt-\ntention. LMDeploy (Contributors, 2023b) improves de-\nployment with persistent batching and blocked KV cache.\nNanoflow (Zhu et al., 2024) features intra-device scheduling\nand asynchronous CPU scheduling, while QServe (Lin et al.,\n2024b) improves LLM serving throughput through W4A8KV4\nquantization and system codesign. MLC-LLM (team, 2023)\naccelerates deployment on edge devices via compiler-based\noptimizations. Inspired by contextual sparsity (Liu et al.,\n2023), PowerInfer (Song et al., 2023; Xue et al., 2024b) de-\nploys LLMs on memory-constrained devices via offloading.\nSparse Attention. BigBird (Zaheer et al., 2020) re-\nduces attention complexity by blending local, global,\nand random attention masks. Subsequent methods like\nStreamingLLM (Xiao et al., 2023), H2O (Zhang et al.,\n2024c), and TOVA (Oren et al., 2024) simplify attention pat-\nterns by discarding KV caches mid-way through the context.\nHowever, these approaches struggle to retain the original\nmodels' long-context capabilities due to limited global con-\ntext modeling. Recent works like DuoAttention (Xiao et al.,\n2024), RetrievalAttention (Liu et al., 2024a), and SeerAt-\ntention (Gao et al., 2024) address this issue by introducing\nretrieval heads (Wu et al., 2024) or combining full atten-\ntion with local attention heads. Quest (Tang et al., 2024)\nintroduces dynamic, query-aware sparsity for accelerated\ndecoding, while MInference (Jiang et al., 2024b) extends\nsimilar ideas to the prefilling stage. FastGen (Ge et al., 2023)\noptimizes decoding by profiling attention heads to discard\ntokens. PQCache (Zhang et al., 2024a) and ShadowKV (Sun\net al., 2024) further advance the selective attention meth-\nods with product quantization and low-rank decomposition.\nAdditionally, LongLoRA (Chen et al., 2023) finetunes short-\ncontext LLMs to long-context ones after converting global\nattention to shifted sparse attention."}, {"title": "7 CONCLUSION", "content": "We introduce LServe, an efficient serving system for long-\nsequence LLMs that leverages hybrid sparse attention. By\nincorporating unified block sparse attention, we achieve sig-\nnificant acceleration of the attention mechanism for both"}]}