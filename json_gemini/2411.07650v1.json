{"title": "Understanding Audiovisual Deepfake Detection: Techniques, Challenges, Human Factors and Perceptual Insights", "authors": ["Ammarah Hashmi", "Sahibzada Adil Shahzad", "Chia-Wen Lin", "Yu Tsao", "Hsin-Min Wang"], "abstract": "Deep Learning has been successfully applied in diverse fields, and its impact on deepfake detection is no exception. Deepfakes are fake yet realistic synthetic content that can be used deceitfully for political impersonation, phishing, slandering, or spreading misinformation. Despite extensive research on unimodal deepfake detection, identifying complex deepfakes through joint analysis of audio and visual streams remains relatively unexplored. To fill this gap, this survey first provides an overview of audiovisual deepfake generation techniques, applications, and their consequences, and then provides a comprehensive review of state-of-the-art methods that combine audio and visual modalities to enhance detection accuracy, summarizing and critically analyzing their strengths and limitations. Furthermore, we discuss existing open source datasets for a deeper understanding, which can contribute to the research community and provide necessary information to beginners who want to analyze deep learning-based audiovisual methods for video forensics. By bridging the gap between unimodal and multimodal approaches, this paper aims to improve the effectiveness of deepfake detection strategies and guide future research in cybersecurity and media integrity.", "sections": [{"title": "I. INTRODUCTION", "content": "The proliferation of smart digital devices such as mobile phones, laptops, tablets, and other digital gadgets, coupled with the accessibility of social media platforms, has promoted the exponential growth of multimedia content (images, videos, and audio) on the internet. This growth is further fueled by technological advances [1], including various deep generative networks [2] [3]. However, this accessibility heightens the need for caution because it can lead to the prevalence of disinformation. Despite this, many people still stick to the trend of the antiquated phrase \u201cseeing is believing\u201d and share multimedia content without considering its authenticity or verifying its digital integrity. Deepfake technology, or sophisticated Artificial Intelligence (AI) models, enable deep learning (DL) tools to manipulate media (images, videos, and audio) to generate hyper-realistic fake content that deceives viewers. Deepfake is AI-generated media that has been deceptively altered by superimposing a source face in a video onto a target face, manipulating the speech in an audio clip, or both. The vast amount of data available online in the form of images, videos, and audio to train such models makes detecting such forgeries increasingly challenging. The impact of deepfakes is critical because we still trust photographic and audio recording evidence. The emergence of realistic and subtle production tools makes fake content incredibly believable and harder to distinguish from genuine content [4]. The rapid spread of harmful and uncontrolled content from fake media has serious imminent impacts and reduces trust in journalism and news providers [5] [6]. Deepfake media content can be exploited to fuel political or religious tensions between countries [7], spread misleading information or rumors between political parties [5] [8], deceive the public [5], engaging in revenge porn [8], defame celebrities [8], promote fraud and identity theft [9], and create political chaos or publicity in a campaign [10]. Generative Adversarial Networks (GAN) [2] and Variational Autoencoders (VAE) [3] are sophisticated DL models for generating counterfeit content. In GAN, the generator network and the discriminator network are the two main components, and these two networks are opposed to each other. The generator aims to generate plausible data, while the discriminator determines the real data from the fake data generated by the generator. Similarly, VAE is an unsupervised learning method consisting of encoder and decoder architectures. VAE is used to create high-quality, hyper-realistic fake content by merging and/or superimposing existing media (images or videos) onto source media for the purpose of deception. Currently, AI-synthesized videos are mainly divided into three different generation types [11] [12]. (1) Head puppetry/puppet master is a counterfeit video generation technique based on the target person animating like a puppet. (2) Face swap aims to generate a video of the target person by swapping"}, {"title": "II. DEEPFAKE CATEGORIES", "content": "Deepfake is a type of AI-generated fake hyper-realistic media content that involves the manipulation in acoustic and/or visual modalities, making it difficult to distinguish between real and fake. This study focuses on audiovisual deepfakes, which combine audio and visual manipulations. In this section, we provide a brief overview of audiovisual deepfake generation methods and their components."}, {"title": "A. Audio Deepfakes", "content": "Audio deepfakes refer to voices that have been digitally altered but sound real. Convincing audio deepfakes are usually achieved through AI techniques or DL models, such as GANs or VAEs. MelGAN [32] and WaveGAN [33] are well-known examples. The former is a generative model for raw audio, and the latter is a generative adversarial network for conditional waveform synthesis. Contemporary breakthroughs in audio deepfakes have increased the threat to voice interfaces, contributing to criminal activity and cybercrime, thus raising concerns about their misuse. Audio deepfake methods fall into three categories, as described below. 1) Voice Conversion: Voice Conversion (VC) involves converting the speech signal of a first speaker (called the source) to match the voice of a second speaker (called the target), while preserving the original linguistic content. Natural speech is the input to the VC system, whose purpose is to change the timbre and prosody of the source to that of the target. Audio deepfakes can be used as independent clips to high impact, or they can be combined with visual deepfakes to produce audiovisual fabricated content. MelGAN-VC [34], an extension of MelGAN [32], is a voice conversion and audio style transfer method applied to arbitrarily long samples. This method uses mel-spectrograms for voice conversion or style transfer through MelGAN. StarGAN [35] is a non-parallel many-to-many VC method based on star generative adversarial networks that can convert any source voice into the voice of any target speaker. VoiceLoop [36] is a DL-based VC method that converts speech from source to target through a phonological loop. SINGAN [37] is a GAN-based singing voice conversion model that converts the singing voice of the source singer into the singing voice of the target singer. 2) Text-to-Speech (TTS): Text-to-Speech (TTS), also known as Speech Synthesis (SS), is the artificial generation of human speech using software or hardware systems. TTS models typically follow two steps to convert written text into human-like speech: text processing and speech generation. This technology can be used to create fake audio messages or impersonate voices for threatening purposes, but can also be useful for text-reading or personal assistants that provide different accents and voices than the pre-recorded human voice. Deep Voice [38] is a neural network-based synthesis model that generates text-to-speech in real time. Traditional TTS systems struggle to replicate natural flow and cannot mimic human speech. Recently, the quality of synthesized audio has been significantly improved through end-to-end models, such as Variational Inference with Adversarial Learning for End-to-End Text-to-Speech (VITS) [39] and FastDiff-TTS [40]. Other DL-based methods include WaveNet [41], Deepvoice [42], Tacotron [43], and Natural-Speech [44]. 3) Partial Audio Deepfakes: In contrast to full audio deep-fakes, partial audio deepfakes [45] involve using AI techniques to alter specific parts of the original clip while maintaining the overall authenticity of the recording, rather than synthetically generating the entire audio. This complex selective alteration"}, {"title": "B. Visual Deepfakes", "content": "Visual deepfakes refer to images or videos that have been digitally altered by DL technology but look realistic. This technology can alter facial expressions, gestures, lip movements, and body movements, or seamlessly superimpose the source person's face onto the target person's body, and easily deceive the viewer. Visual deepfakes have widespread applications in fields, such as education, gaming, and entertainment, but they pose considerable risks by leading to misinformation, defamation, and privacy violations. Five types of visual manipulation fall into this category, each of which is discussed separately in this section. 1) Face Swap: Face swap [46], a prominent visual manipulation technique, utilizes a generative model to seamlessly replace the target face with the source's identity in images or videos. In certain scenarios, traditional methods [47]\u2013[49] may fail to capture the expression depicted in the original face image, or sometimes result in an unnatural appearance. However, recent advances in DL have led to automated methods [50] [51] capable of producing plausible multimedia content. This technology has been integrated into various applications, such as FaceSwap [51], DFaker [52], DeepFaceLab [53], Deepake-tf [54] and FaceSwapGAN [55], which use deep neural networks to achieve realistic face-swapping results. 2) Face Generation: Face generation, also known as face synthesis [57], creates realistic human faces that are completely fictional and do not correspond to any real identities using generative models such as GANs [2] and VAEs [3]. This technology offers valuable applications, such as automatic character creation in the video gaming and 3D face modeling industries. However, it can also be used for malicious purposes, such as impersonating or spreading disinformation on social media [58]. Starting with the initial low-resolution imagery [2], the quality has improved dramatically over time. Advanced models such as StyleGAN [59], ProGAN [60], StyleGAN2 [61], TP-GAN [62], SAGAN [63], and BigGAN [64] can now produce high-resolution content. 3) Reenactment: Face reenactment, also known as puppet-master, is a technique for transferring facial expressions or body movements from a source video to a target video. Unlike face swap, this technique is designed to control and manipulate the target's expressions and movements to match those of the source, including gaze direction. Face reenactment has many applications in entertainment, virtual reality, and telepresence, but its potential for misuse can be harmful. Activists may creat deceptive videos that manipulate a person's expressions and movements to falsely portray them as engaging in behavior or speech that they never did. Some publicly available tools that utilize tracking and reenactment techniques to demonstrate facial expression and motion transfer include Face2Face [65], FSGANv2 [66], MarioNETte [67], DeepFaceLab [53], Pix2pixHD [68], and ReenactGAN [69]. 4) Lip Sync: Lip sync involves changing a person's lip movements to match pre-recorded audio clips, ensuring accurate and convincing speech alignment. This is a complex process, as the appearance and movement of the lower face, lip region, and surrounding areas are crucial to achieving the goal, while producing precise lip movements and expressions is also important to effectively convey the message. Lip sync has a variety of positive uses, including applications in forensic analysis, speech recognition research, and film production, where it ensures natural synchronization of dialogue with the audio soundtrack. However, it can have negative consequences when used to generate deceptive content for malicious purposes, such as spreading false information, defamation, and manipulating public opinion. The authors in [70]\u2013[72] proposed techniques for lip synchronization. Other popular"}, {"title": "C. Text Deepfakes", "content": "Text deepfakes are text that is artificially generated or altered using advanced DL or AI techniques that produce highly realistic and contextually appropriate text. These sophisticated algorithms represent powerful and versatile applications of AI, capable of generating articles, social media posts, conversations, and other forms of written communication that are often indistinguishable from content written by humans. However, they can be misused, so a balanced approach that combines robust detection methods, ethical guidelines, and public awareness is needed. These deepfakes fall into two categories, each with unique characteristics. Both types are reviewed separately in this section. 1) Synthetic Text: Synthetic text is generated by machines using advanced AI and DL techniques to imitate human writing with great accuracy. This type of text deepfakes are created using sophisticated natural language generation (NLG) models, such as GPT-3, GPT-4, and their successors [81] [82]. Large Language Models (LLM) are trained on large amounts of human writing and generate coherent and contextual text based on a given prompt, making it hard to discern between human and synthetic text [82]. This technology has a variety of positive applications [83], including automated content generation for customer service, educational tools, and creative industries. However, the potential for misuse, such as spreading misinformation [84], fake news articles, and conducting fraud, raises significant ethical and security concerns [85]. 2) AI-Powered Bot-Generated Text: AI-powered bot-generated text refers to content created by automated systems or bots that use AI models to interact online. They excel at generating responses, posts, and other forms of communication that effectively mimic human behavior. Al empowers these bots to engage in social media discussions, comment on issues, and even generate news articles, often without being noticed by casual observers. The integration of AI in bots provides a variety of beneficial applications that boost the efficiency and engagement on digital platforms. However, as technology evolves, the potential misuse of this deceptive approach poses significant challenges to the responsible use of AI in online interactions."}, {"title": "D. Audiovisual Deepfakes", "content": "Audiovisual deepfakes refer to convincing and compelling fabricated videos created by manipulating the acoustic and visual streams of a video. Bimodal manipulation produces sophisticated deepfake videos that seamlessly blend fabricated video and audio content beyond single-modal manipulation. The misuse of audiovisual deepfakes poses significant risks, as they have the potential to spread misinformation, defamation, and other malicious activities. We discussed single-modal manipulation, namely audio and video deepfakes, in the previous two sections. This section focuses on audiovisual deepfakes that combine the two. Videos are divided into four categories based on acoustic and visual manipulations within them. 1) Fake Video and Fake Audio (FVFA): Unlike partial deepfakes, the Fake Video and Fake Audio (FVFA) category [86] involves the artificial generation of both audio and visual components. It is the most comprehensive and advanced audiovisual deepfake, capable of fabricating complete acoustic and visual messages, often depicting fictional scenes that never happened. DL models, such as GANs, can generate high-quality synthetic visuals and produce realistic facial expressions, movements, and interactions that are coherent with the rest of the visual content. At the same time, synthetic audio that aligns with the video content is generated through TTS systems or voice cloning technology. These methods generate a highly persuasive audio track by analyzing the phonetic patterns of the target voice. 2) Real Video and Fake Audio (RVFA): The Real Video and Fake Audio (RVFA) category [86] involves combining authentic video with synthetically generated audio recordings. AI and DL techniques alter the auditory message while preserving the visual content, making it appear as if someone in the video is saying something they never actually said. It is particularly effective in situations where altering the audio can significantly change the perceived message or context of an event (such as political manipulation or the creation of defamation fake news videos). The video is often based on real events or recordings and remains intact. The synthesized audio is created using a voice cloning or TTS system to ensure alignment with the lip movements and expressions of the person in the video. 3) Fake Video and Real Audio (FVRA): The Fake Video and Real Audio (FVRA) category involves [86] the artificial manipulation of the visual track, while the sound track remains real and intact. This category is particularly effective in situations where altered visual scenes are combined with genuine audio to significantly change the perceived message or context of an event (such as the creation of fake news videos, defamation, or impersonation). State-of-the-art visual methods are used to produce realistic visual effects that are consistent and synchronized with real audio to present a coherent and realistic audiovisual impression. 4) Real Video and Real Audio (RVRA): The acoustic and visual tracks in the Real Video and Real Audio (RVRA) category [86] remain unaltered and are derived from authentic video and audio recordings, preserving the originality and integrity of both modalities. Technically, this category does not fall within the definition of deepfakes, but when used deceptively, it can be just as misleading as synthetic content."}, {"title": "III. VIDEO DEEPFAKE DETECTION METHODS", "content": "The field of video deepfakes is booming due to cutting-edge generative technology. At the same time, detection methods have emerged as crucial tools in combating these deepfakes. This section explores various techniques for detecting video deepfakes, divided into traditional methods based on visual cues and audio-based methods."}, {"title": "A. Traditional Methods Based on Visual Cues", "content": "Traditional methods for video deepfake detection heavily rely on visual irregularities or cues that arise while altering visual content [87], such as inconsistencies, anomalies, and artifacts within video frames [88]. These methods can identify subtle anomalies and are broadly divided into two subcategories, both of which are discussed separately below. 1) Frame Analysis: Frame analysis focuses on examining individual video frames to identify visual artifacts or inconsistencies such as unnatural lighting [89], irregularities in shadows [90] [91], and distortions around the edges of manipulated regions that indicate tampering [92]. Many techniques fall into this category, each focusing on different aspects of visual content. Pixel-level analysis [87], error level analysis (ELA) [93], compression artifacts [94], edge detection [95], and frequency domain analysis [95] are some examples of frame analysis techniques. 2) Inconsistencies in Facial Movements: Inconsistencies in facial movements are a major indicator of deepfake manipulation, which along with expressions are difficult to perfectly replicate. Conventional methods exploit these irregularities, such as unnatural or inconsistent facial movements [96], blinking patterns [97], eye movements [98], expression transitions [99], or synchronization issues, to spot deepfakes. Temporal consistency checks [100], landmark-based methods [101], blink rate analysis [102] and frame-by-frame analysis [103] are some examples of such techniques."}, {"title": "B. Audio-Based Methods", "content": "Audio-based deepfake detection methods emphasize analyzing audio aspects to reveal signs of tampering in videos. These methods are important traditional techniques for finding inconsistencies between audio and visual streams or detecting anomalies in the audio itself. Voice analysis and speech-content discrepancies are two main categories of audio-based methods for ensuring the authenticity of audiovisual content. 1) Voice Analysis: Voice analysis techniques detect anomalies by analyzing acoustic properties such as speech patterns, voice characteristics, and linguistic nuances to identify deviations from natural audio characteristics [104] [105]. Analyzing audio frequency, pitch, and temporal characteristics provides valuable clues for identifying fake multimedia content. Spectrogram analysis [106] and voice biometrics [107] are examples of voice analysis techniques. 2) Speech-Content Discrepancies: The alignment between the audio and visual components in deceptive content is often imperfect. When the movement of the lips does not align precisely with the words spoken, there can be a noticeable"}, {"title": "IV. AUDIOVISUAL DEEPFAKE DETECTION METHODS", "content": "Deepfake videos threaten personal privacy and social security. Various strategies have been proposed to overcome this risk, with early attempts mainly focusing on single modality, while current detection methods focus more on multimodal features to identify manipulation in videos. Multimodal approaches combine the strengths of multiple modalities such as audio and visual data to enhance the accuracy and reliability of deepfake detection systems. These methods can effectively discover more subtle manipulations by integrating information from both modalities that single-modal methods may ignore. This systematic literature review examines existing video deepfake detection methods that use both audio and visual streams to detect forgeries in videos. We classify them into different categories and provide detailed information and discussion of the methods in each category later in this section."}, {"title": "A. Synchronization-Based Methods", "content": "Synchronization between audio and visual streams is a critical aspect of multimodal deepfake detection and is essential to creating a seamless and coherent viewing experience [109] [110]. These synchronization methods are designed to synchronize audio and visual streams in time, specifically lip movements, expressions, and overall timing. Since speech, lip movements and facial expressions are naturally aligned in authentic videos, any discrepancy between audio and video streams indicates that the content has been tampered with or degraded, making synchronization techniques a critical tool in the fight against audiovisual fraud. In [108], phoneme-viseme mismatches are exploited. The authors claim that producing sounds like \u201cM\u201d, \u201cB\u201d, and \u201cP\u201d requires complete lip/mouth closure, while deepfake techniques often fail to imitate the dynamics of lip sequences, making it a clue for forgery detection. In addition to modality-specific embeddings, the study in [21] also integrates facial and speech emotion cues using state-of-the-art emotion recognition models to enhance the effectiveness of audiovisual forgery detection. The authors of [20] exploit the inherent synchronization between video and audio streams to effectively identify inconsistencies and ultimately help spot manipulation in videos to improve deepfake detection. The proposed approach is a two-plus-one joint detection framework that jointly models audio and video, with one stream processing video frames and the other processing audio waveforms. The final deepfake prediction is produced by combining the outputs of these two streams, exploiting the inherent synchronization between them to detect misalignments between the two modes to identify forgeries in videos. Experiments demonstrate the high generalization ability of the proposed approach and show that the proposed joint audiovisual approach outperforms unimodal methods. The authors of [111] exploit audiovisual dissonance known as Modality Dissonance Score (MDS) to expose the video forgery. They believe that altering either modality can lead to noticeable dissonances, such as loss of lip synchronization or unnatural facial movements, which can be used as indicators that reveal altered material. Furthermore, their approach provides insight into altered video portions by locating segments in the video that exhibit signs of manipulation. The study in [112] addresses the limitations of previous work that focused primarily on facial feature analysis and ignored the audio component and the broader context of audiovisual synchronization. The authors combined a phoneme-based audiovisual matching strategy and proposed the Audio-Visual Coupling Model (AVCM), which aims to capture the complex relationship between mouth movements (visual) and corresponding speech segments (audio) by measuring the similarity between them. Experiments show the high performance of their approach compared to cutting-edge techniques, highlighting its robustness to detecting deepfakes by exploiting the intrinsic synchronization of audio and video streams. A unified audiovisual learning framework called AVoiD-DF is proposed in [113], which effectively captures cross-modal and intra-modal inconsistencies or discrepancies between audio and visual content by jointly utilizing audio and visual features. This joint learning approach highlights the importance of simultaneously using audio and visual cues to substantially reduce misclassification rates, especially in cases where one modality has been tampered with. The authors of [18] present an interesting work that focuses on the discrepancies between audio and visual components, specifically lip sync mismatch. Specifically, their proposed method detects inconsistencies between the lip sequence extracted from the video and the synthetic lip sequence generated from the audio using the wav2lip model. Furthermore, a pretrained convolutional neural network (CNN)-based lip-reading model is used to compare the extracted lip sequence with the synthetic lip sequence to distinguish synchronous and asynchronous audiovisual pairs, which helps distinguish real and fake videos. This work was further extended in [114] by utilizing audiovisual features extracted directly from the transformer-based multimodal model AV-HuBERT to identify inconsistencies between audio and visual components, thereby eliminating the wav2lip generation model."}, {"title": "B. Feature Fusion Methods", "content": "Feature fusion techniques leverage the complementary strengths of various features to enhance the accuracy of prediction models by combining features extracted from multiple modalities to create comprehensive and robust media representations. This holistic approach can significantly improve the accuracy of deepfake detectors, allowing them to capture more complex and informative representations of the underlying patterns. The study in [115] highlights the importance of feature fusion by using jointly learned representations of audio and visual modalities to identify forgeries in videos. The authors propose a specialized approach called Multimodaltrace, which leverages the rich interdependencies between audio and visual signals in video content. This framework efficiently evaluates the audio and visual modalities by using a combination of channel extractors and mixers. Another recent study in [116] proposes a dual transformer model called AVT2-DWF with a dynamically weighted fusion strategy that calibrates its focus on each modality based on the relative strength and consistency of cues, thereby improving its detection of subtle or complex multimodal forgeries. The authors of [117] propose an innovative two-stage cross-modal learning method called Audio-Visual Feature Fusion (AVFF) to distinguish authentic video content from fabricated video content by focusing on audiovisual coherence. Their method identifies audio and visual perturbations by using distant audio and visual features and examining unimodal and cross-modal embeddings. In [118], the authors introduce a novel one-class learning method by combining acoustic and visual features. This multi-stream fusion approach uses various fusion strategies (early, intermediate, and late) to effectively integrate audio and visual signals and surpasses single-modality detection models, demonstrating robust performance across diverse scenarios. The study in [119] utilizes audio and visual modalities by extracting features and passing them to the multimodal model that learns nuanced differences across both domains. This framework explored two fusion strategies, feature fusion, and score fusion, demonstrating that simultaneously exploiting acoustic and visual features is effective in intra-domain and cross-domain testing environments. To address heterogeneous feature fusion limitation, a novel multi-modal attention framework based on recurrent neural networks (RNNs) is proposed in [120] to exploit contextual information for audio-visual deepfake detection. The proposed approach utilizes attention mechanisms on multi-modal, multi-sequence representations, identifying the most relevant features across modalities to improve deepfake detection and localization."}, {"title": "C. Ensemble Methods", "content": "Ensemble approaches have proven to be highly beneficial in improving the accuracy of detectors in identifying audiovisual forgeries in videos, as they are able to take advantage of multiple models. Compared with a single model, integrating different models can effectively account for variations in forgery technology. The study in [121] presents two ensemble methods for audiovisual deepfake detection. The first method is soft-voting, which combines model predictions by averaging their probabilities, while the second method is hard-voting, which applies majority voting based on model outputs. The authors of [122] introduce AVFakeNet, an ensemble architecture that uses visual and audio features to distinguish authentic videos from fake ones. Their model leverages a Dense Swin Transformer to handle bimodal complexity, extract and fuse audio and visual features, and ensure that the cross-modal relationship between visual and audio signals is effectively captured. Similarly, the study in [19] introduces an integrated network consisting of three CNN-based networks. The authors utilize ensemble learning to integrate multiple modality-centric models to detect audiovisual forgeries in videos. Dedicated audio-only, video-only, and audiovisual networks make separate predictions, and a voting mechanism then fuses these predictions to make the final prediction. In the extension work called AVTENet [123], the CNN architectures in audio-only, video-only, and audiovisual networks respectively were replaced by dedicated pretrained Transformer models, thereby achieving better performance."}, {"title": "D. Temporal Analysis-Based Methods", "content": "To address the limitation of simple feature fusion techniques, the method proposed in [124] utilizes dual networks to extract temporal features from both modalities. Specifically, audio and video modules are proposed to predict acoustic and visual temporal features and match them with reference features to capture temporal inconsistencies in audio and video modalities. A contrastive objective function is employed to maximize the difference between authentic and spoofed modalities, significantly enhancing the discriminative ability of the approach to classify genuine and forged instances. For temporal forgery detection (TFD), a multi-dimensional contrastive loss is introduced in [125] to help forensic models exploit temporal inconsistencies by constraining extracted embeddings. Experiments on the LAV-DF [126] dataset show the effectiveness of the proposed method. In [127], the authors proposed a hybrid approach that utilizes pretrained models to extract spatial, spectral, and temporal features to distinguish real and fake video content. A self-supervised learning (SSL) based approach is employed in [128] to capture a rich representation based on temporal synchronization between speech and facial movement across the frames. The representation learned by the SSL network is then fed into a temporal classifier network to judge the authenticity of video content."}, {"title": "E. Other Methods", "content": "Boundary Aware Temporal Forgery Detection (BA-TFD) [126], is introduced to address temporal forgery localization task. BA-TFD is a 3D Convolutional Neural Network (3DCNN)-based model proposed to localize forgery through three guiding objective functions: contrastive, boundary matching, and frame classification. Later on, BA-TFD+ [129] improves the baseline BA-TFD approach by replacing its backbone with a Multiscale Vision Transformer coupled with a refined training process by an additional multimodal boundary matching loss function. MIS-AVoiDD [130] jointly utilizes modality-invariant and modality-specific features by focusing on shared and unique features across the modalities. The method in [131] introduces a multimodal contrastive learning (MCL) approach to detect forgeries by capturing both intra-modal and cross-modal forgery cues. It aligns representations from audio, frames, and video using a cross-modal contrastive strategy and distills frame knowledge to the video network to strengthen forgery detection without extra computational cost. Additionally, a noise-based feature augmentation (NFA) module further enhances generalization by adaptively perturbing audio-visual features. DF-TransFusion [132] employs cross-attention between lip movements and audio signals to detect lip-sync inconsistencies, while a self-attention mechanism targets facial features to reveal subtle visual manipulations. This integrated approach enhances detection accuracy by identifying cross-modal discrepancies, enabling more effective detection of complex deepfake manipulations. For real-world applications, [133] proposes a modality agnostic-based method to address the missing modalities issue in the multimodal deepfake detection task. This method facilitates robust detection performance even when one of the input modalities i.e. audio or video, is missing. Another work [134] utilizes monomodal datasets (visual-only or audio-only deepfakes) for training, rather than relying on multimodal deepfake data. This approach extracts audio-visual features over time and analyzes them using time-aware neural networks, capitalizing on the inconsistencies both within and across modalities to improve detection performance. The proposed method is evaluated on unseen multimodal deepfakes, allowing it to evaluate the robustness of the detector without requiring multimodal training data. To address the generalization issue in deepfake detection, a person-of-interest (POI) forgery detector is proposed in [135], to learn each individual's most discriminative identity features through a contrastive learning framework. This approach enables robust forgery detection without reliance on artifacts or manipulation traces generated using deepfake techniques. Finally, [136] proposes a multimodal network with a web interface for real-time video forgery detection. This system integrates multiple data modalities to boost detection accuracy and provides a user-friendly, accessible platform for real-time analysis."}, {"title": "V. DATASETS FOR AUDIOVISUAL DEEPFAKE DETECTION", "content": "To effectively detect audiovisual deepfakes, a dataset with multiple manipulation methods, real-life scenarios, and increasingly sophisticated deepfake generation techniques is required to provide a valuable resource for training, validation, and testing detection systems. This section discusses existing widely used datasets that were developed to aid research in audiovisual deepfake detection. A. DFDC (DeepFake Detection Challenge) A preview dataset containing 5,214 videos was released in October 2019, followed by the full dataset in December 2019, containing 119,154 videos, each 10 seconds long, with 486 unique subjects. The DeepFake Detection Challenge (DFDC) [137] dataset is a large-scale publicly accessible dataset released by Facebook AI to promote research and development in deepfake detection methods. This large-scale dataset was recorded in natural settings using high-resolution cameras without professional lighting or makeup, and was collected from 3,426 paid subjects. It includes 1,000 deepfake videos generated using various techniques, including GAN-based and non-learning methods, for each genuine video. The dataset also includes various types of deepfakes, such as faceswap, face reenactments, and full-body deepfakes, and manipulations in audio, visual, or both audio and visual streams. This dataset stands out for the diversity and variability of deepfake generation methods, focusing on audiovisual components, high-quality videos, and extensive use in benchmarking. B. FakeAVCeleb The FakeAVCeleb dataset [86] is a comprehensive collection of audiovisual deepfake recordings of celebrities designed to develop, advance, and evaluate deepfake detection methods. Developed by researchers at Sungkyunkwan University, South Korea, the primary purpose is to assist the scientific community in benchmarking deepfake detection models, particularly those that examine both visual and auditory modalities. The dataset consists of 500 original videos, each approximately 30 seconds long, featuring a variety of celebrities from the sports, politics, music, and film industries, including Barack Obama, Donald Trump, and Kim Kardashian. Based on 500 original videos, 19,500 manipulated samples were generated through various manipulation techniques (such as Faceswap [140] and Wav2lip [74]) and real-time voice cloning (RTVC) (such as SV2TTS [141]). The dataset consists of 500 subjects, of which 470 subjects' videos are used for training and the remaining 70 subjects' videos are used for testing. The FakeAVCeleb dataset is known for its multimodal manipulation, which makes it more challenging and relevant than synthetic datasets that only contain unimodal manipulation. This challenging dataset can make significant contributions to academic research and practical applications in the fields of cybersecurity and media forensics, and can be used to benchmark deepfake detection, multimodal forgery detection, and human perception research. C. LAV-DF (Localized Audio Visual DeepFake) The Localized Audio Visual DeepFake (LAV-DF) dataset [126] is a multimodal dataset released by a group of researchers in 2022 to advance research on deepfake detection, with a particular focus on locating alterations in audio and visual data. It contains 136,304 videos, including 36,431 real videos of 153 unique subjects and the corresponding 99,873 fake videos. LAV-DF features comprehensive audio and visual deepfakes with targeted content-driven manipulations (utilizing content-driven reenactment and text-to-speech methods) designed to more accurately replicate authentic speech and expressions. When evaluated on this dataset, the deepfake detection model not only needs to identify the deepfake content, but also accurately locate the exact fake region."}, {"title": "D. AV-Deepfake1M", "content": "Released in 2023, the large-scale AV-Deepfake1M dataset [138] is carefully designed to facilitate the development of robust deepfake detection and localization techniques. The dataset contains more than 1 million videos featuring 2000+ subjects, encompassing audio, visual, and audiovisual manipulations. It provides a challenging benchmark for evaluating the efficacy of current detection models and encourages innovation to effectively mitigate the threat of deepfakes to media authenticity."}, {"title": "E. PolyGlotFake", "content": "PolyGlotFake is a novel comprehensive multilingual and multimodal dataset released in 2023, developed to advanced techniques in deepfake detection. PolyGlotFake spans seven languages including English, French, Spanish, Russian, Chinese, Arabic, and Japanese and incorporates a diverse set of manipulation techniques including advanced text-to-speech, voice cloning, and lip-sync technologies. The dataset comprises over 15,000 videos, encompassing both audio and visual manipulations, making it a valuable asset for driving advancements in holistic deepfake detection research."}, {"title": "VI. PERFORMANCE METRICS AND EVALUATION", "content": "Evaluating the performance of deepfake detection systems relies heavily on robust evaluation metrics that provide measurable insights into the model's accuracy, robustness, and generalizability, further allowing comparison of various detection methods and progress observations. This section presents common metrics for evaluating audiovisual deepfake detection systems."}, {"title": "A. Accuracy-Based Metrics", "content": "1) True Positive Rate (TPR)", "FNR)": "Such metrics reflect a model's ability to correctly classify authentic content versus manipulated content. True Positive Rate (TPR), also known as recall or sensitivity, shows how effectively a model detects deepfakes, and is defined as\n$TPR = \\frac{TP}{TP + FN}$  (1)\nwhere TP and FN denote True Positive (fake instances correctly detected as fake) and False Negative (fake instances incorrectly detected as real), respectively. False Positive Rate (FPR) measures"}]}