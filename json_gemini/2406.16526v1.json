{"title": "NARRepair: Non-Autoregressive Code Generation Model for Automatic Program Repair", "authors": ["ZHENYU YANG", "ZHEN YANG", "ZHONGXING YU"], "abstract": "Recent years have witnessed a surge of research efforts on Automatic Program Repair(APR), which promises to reduce software development costs and improve software reliability. With the advancement of deep learning techniques, the performance of APR techniques has reached a new level. Previous deep learning-based APR techniques essentially used a sequence-to-sequence model to modify program sentences in the Autoregressive(AR) manner, which predicts future values based on past values. Due to the manner of word-by-word generation, the AR-based APR technique has a huge time delay and thus cannot fix bugs in real time. This negative consequence overshadows the widespread adoption of APR techniques in real-life software development.\nTo address the issue, we aim to apply the Non-Autoregressive(NAR) method to the APR task, which can output target code in a parallel manner to avoid huge inference delays. However, the naive use of the NAR manner for the APR task suffers from the issue of compromised patch quality. To effectively adapt the NAR manner for the APR task, we in this paper propose NARRepair, the first customized NAR code generation model for the APR task. The NARRepair features three major novelties. First, NARRepair is guided by repair actions to alleviate the issue of over-correction, i.e., correct code can instead be modified into wrong ones. Second, to alleviate the issue of lacking inter-word dependency information associated with NAR manner, NARRepair extracts this dependency information on top of the Abstract Syntax Tree (AST) for generating words in parallel while maintaining the correctness of the code syntax. Finally, to alleviate the issue of lacking contextual information, NARRrepair obtains contextual information about words through two-stage decoding for improving the accuracy of the patch generation process.\nWe evaluated NARRepair on three widely used datasets in the APR community, including Defects4J v1.2, Defects4J v2.0, and QuixBugs. The results show that 1) compared to AR-based APR techniques, the inference speed of NARRepair has been increased by 5.4-15.1 times in the CPU environment and 6.2-18.6 times in the GPU environment, and 2) NARRepair has fixed 69, 41, and 23 bugs for Defect4J v1.2, Defect4J v2.0, and QuixBugs respectively, which are respectively 90%, 95%, and 82% of the optimal model. Overall, the results show that our technique can significantly improve the inference speed while maintaining high repair accuracy.", "sections": [{"title": "1 INTRODUCTION", "content": "Program defects are inevitable during the software development process, and recent years have witnessed a surge of research efforts on Automatic Program Repair(APR) to alleviate this issue [10, 13, 18, 23, 26-28, 31, 36, 38, 42, 43, 45,"}, {"title": "2 RELATED WORK", "content": "APR and Deep Learning-based APR. Given the time-consuming and error-prone nature of program debugging [61, 74], APR techniques have been proposed to reduce software development costs and improve software reliability. Recent years have witnessed a surge of APR techniques rooted in different disciplines, notably including search-based"}, {"title": "3 NARREPAIR", "content": "In this section, we will introduce the NARRepair model, which uses NAR to generate code text in parallel to improve the inference speed. The model mainly consists of four parts: code encoder, repair action predictor, inter-word dependency extractor, and two-stage NAR decoder. Figure 2 shows the structure of the NARRepair model. The process of NARRepair is as follows:\n\u2022 Code encoder embeds the buggy code into feature vectors (\u00a73.1).\n\u2022 Given the buggy code feature, the repair action predictor predicts repair action and output length for each word (\u00a73.2).\n\u2022 According to the output length, the inter-word dependency extractor first generates the feature vector of the repaired code text. Then, the extractor obtains the inter-word dependency information and fuses it with the word feature vector to obtain the word feature vector with dependency information (\u00a73.3).\n\u2022 Given the word feature vector from the previous step, the two-stage decoder generates all repaired words (\u00a73.4).\nIn the following sections, we will elaborate on the structure of the NARRepair."}, {"title": "3.1 Code Encoder", "content": "The code encoder can extract features from the buggy code text $W_{i:n}$ and convert $W_{i:n}$ into a word embedding $E_{i:n}$. The output word embedding $E_i$ can be used for the prediction and tagging of subsequent modules. We use the encoder part of the transformer model [60] as the code encoder of the model.\nHere, we briefly give an overview of the encoder of the transformer model. The encoder of the transformer is composed of multiple identical layers, and each layer has two sub-layers. The first is a multi-head self-attention layer that fuses word features by calculating the attention weight between word feature vectors. The second sub-layer is a feedforward neural network used to normalize the output of the model. Residual links are used between sub-layers. The operation process of the code encoder can be defined as\n$E_{i:n} = Encoder(W_{i:n} + W_{pos})$\nand the operation of each layer of the encoder can be expressed as\n$X_{attention} = X_{hidden} + Attention(X_{hidden})$\n$X_{hidden} = Feedforward(X_{attention})$\n$X_{hidden} = W_{in} + W_{pos}$\nwhere $X_{pos}$ is position embedding, Attention is self-attention layer, and Feedforward is feedforward neural network layer."}, {"title": "3.2 Repair Action Predictor", "content": "The repair action predictor can predict the repair action for each word in the buggy code text. The content of the repair action is divided into two parts: the type of repair action and the repair length. We classify all repair actions into 4 categories: \u201ckeep\u201d, \u201cinsert\u201d, \u201cdelete\u201d, and \u201creplace\u201d. The repair length represents the number of generated repair words corresponding to each fixed word. Typically, the repair length for actions \u201creplace\u201d and \u201ckeep\u201d is 1; the repair length for action \u201cdelete\u201d is 0; the repair length for action \u201cinsert\u201d is the number of words inserted. Figure 3 gives an example of repair action prediction. As shown in Figure 3, the repair action predictor predicts the repair action and repair length for \u201cLang-61\u201d buggy code in the Defect4j dataset. Compared with NAR models in machine translation that need to predict the probabilities of all words in the dictionary, the repair action predictor only needs to predict the probabilities of four repair actions. When the predicted action is \u201ckeep\u201d, the model does not need to change the words. This method effectively alleviates the problem of modifying the correct words into the wrong ones.\nRegarding the model structure, since convolutional neural networks have the advantage of effectively acquiring local features, the repair action predictor uses a convolutional neural network [22] to extract the word features after receiving the output of the encoder. Then, the classification layer predicts the repair action and the repair length for each word in the buggy code text separately. The detailed operations are as follows:\n$X_{feature} = ConV(E_{1:n})$\n$Act_{1:n} = Linear_1(Dropout(Relu(X_{feature})))$\n$Len_{1:n} = Linear_2(Dropout(Relu(X_{feature})))$\nwhere $ConV$ is the convolutional neural network layer, $Linear_1$ is a fully connected layer whose output dimension is the number of repair actions, and $Linear_2$ is a fully connected layer whose output dimension is the maximum length."}, {"title": "3.3 Inter-word Dependency Extractor", "content": "The inter-word dependency extractor can learn the dependency information between word pairs through the nearest common parent nodes in the AST. To get the nearest parent node of word pairs, we need to generate the AST for the code text. Given the code text, we use the program analysis tool Tree-sitter[58] to extract its AST $T_{1:m}$. Figure 4 shows the code text \"int add(int a, int b) {return a+b;}\" and its corresponding AST. In this example, the nearest common parent of \"int a\u201d and \u201cint b\u201d is \u201cparam list\u201d; the nearest common parent of \u201ca\u201d and \u201cb\u201d on the right is \"binary expr\". We list the nearest common parent nodes of all word pairs and generate a word dependency matrix. We show the inter-word dependency matrix of \"return a+b\" in the right part of Figure 4. The relationship between a certain word \u201cn\u201d and another word \u201cm\u201d is equivalent to that between \u201cm\u201d and \u201cn\u201d, so the dependency matrix is symmetric. We use the obtained dependency matrix as the ground truth to train the inter-word dependency extractor. Given the feature vector $X_{1:n}$ of the faulty text, we first use the encoder-decoder attention module to obtain the feature vector $D_{1:m}$ of the target text. Then, we use the inter-word dependency extractor to predict the nearest common parent nodes of word pairs as dependency information. Inspired by the work of Dozat et al. [7], the inter-word dependency extractor uses an attention mechanism to model dependency information. For each target text feature, the extractor maps the"}, {"title": "3.4 Two-stage NAR Decoder", "content": "The two-stage NAR decoder decomposes the normal NAR decoder into two parts for step-by-step decoding. The purpose of the first stage of decoding is to generate a preliminary result and retain words with high confidence. For the text generated by the first stage of decoding, we retain two parts of the words whose confidence levels are high: (1) multiple adjacent words for which repair actions are \u201ckeep\u201d and (2) words whose prediction probabilities are greater than a threshold. We set the threshold to be 0.7 in our experimental evaluation. We retain these high-confidence words and use them as context for the remaining low-confidence words. Since the remaining words may be wrong, the purpose of the second decoding is to regenerate the words with low confidence based on the contextual information. We mask the remaining words with the [Mask] tag. Literature about mask language models [6, 11, 16, 34] has proven that the attention mechanism can obtain contextual information for words with the [Mask] tag. Hence, the result of the second stage of decoding based on contextual information will be more accurate.\nTake the code text \"if (dataset !=null) {\" as an example, Figure 6 shows the decoding process of the two-stage NAR decoder. First, the first stage decoder generates the preliminary result \"if (dataset <= null) {\" and considers \u201c<=\u201d as having low confidence. Then, the second stage decoder masks the \u201c<=\u201d with the \u201c[Mask]\u201d tag to obtain the context information and generate the correct result. Assuming that the given input feature is $H_{1:m}$, the specific operations of the first stage decoder are as follows:\n$E_{first} = Decoder_{first}(H_{1:m}) = Layer_{decoder}(H_{1:m})$\n$P_{first} = Softmax(E_{first})$\nwhere $Layer_{1:n-k}$ is the 1st to n \u2212 kth layer of the decoder, $E_{first}$ is the feature vector outputted by the first stage of decoding, and $P_{first}$ is the probability distribution of the first stage of decoding. We further mask the result of the first"}, {"title": "3.5 Training and Inference", "content": "During the training process, the three sub-modules of the NARRepair model are jointly learned. The final loss function of NARRepair is as follows:\n$L = L_{dec} + \u03b1(L_{act} + L_{length}) + \u03bbL_{depend}$\nwhere \u03b1 and \u03bb are the hyperparameters used to adjust the importance of each training loss. We set \u03b1 to be 0.1 and \u03bb to be 0.1.\nDuring inference, the repair action predictor firstly predicts the repair action and output length for each word in the buggy code. Then, based on the output length, the encoder-decoder attention layer transforms the buggy code\u2019s feature vector into the fixed code\u2019s feature vector. Next, the inter-word dependency extractor generates fixed code feature vectors with inter-word dependency information through AST. Finally, the two-stage decoder outputs the fixed code based on the repair action and the fixed code feature vector with inter-word dependency information. For each buggy code, we generate 200 patches and test whether they contain the correct patches."}, {"title": "4 EVALUATION SETUP", "content": "To prove the validity of our idea, we train the NARRepair model and evaluate its performance. In this section, we introduce the evaluation setup."}, {"title": "4.1 Research Questions", "content": "To begin with, we give the research questions explored in this paper.\nRQ1: Compared with other DL-based APR models and NAR models, what is the performance of NARRepair for the APR task? This is an overall question about the performance of the NARRepair model. For this question, we evaluate the NARRepair model on three widely used datasets for APR tasks and compare its performance with that of other DL-based APR models and NAR models.\nRQ2: How does each module contribute to the final result of NARRepair? For this question, we gradually remove submodules from the complete NARRepair model and investigate the contribution of each submodule."}, {"title": "4.2 Data Collection", "content": "We use the dataset published by selfAPR[71] on GitHub as our model dataset, which contains code text pairs (buggy and correct) generated for common types of program errors by self-supervision. We discard some data for which we could not generate the AST and retain 837, 059 pieces of data. Since this dataset is machine-generated, we collect additional datasets from real projects on GitHub to supplement this dataset. To prevent data leakage, we remove projects related to Defect4J[21] and QuixBugs[30]. Finally, our dataset contains 1.52 million instances, and we use 90% of the dataset as training data and 10% as validation data.\nWe use three widely used datasets for the APR task to evaluate the model performance. The first one is Defect4J v1.2 [21], which contains 395 bugs in real Java projects. The second one is Defect4J v2.0 [21], which contains 438 additional bugs compared to Defect4J v1.2. The third one is QuixBugs [30], which contains 40 Java bugs and is introduced specifically to test the robustness of the model on data distributions besides Defect4J bugs."}, {"title": "4.3 Data Preprocessing", "content": "For training data, we need to generate repair action(s) and inter-word dependency matrix for each piece of data to facilitate model learning. For generating repair actions, we use dynamic programming to calculate the edit distance between the buggy code text and the fixed code text. After calculating the edit distance, we use backtracking to output the specific repair action for each step. For example, for a code pair containing the buggy code text \"if (result != null )\" and the fixed code text \u201cif ( ! result. isNotype ())\u201d, we can accordingly get the required repair actions on top of this procedure: Keep, Keep, Insert, Replace, Replace, Insert. For the inter-word dependency matrix, we first use the Tree-sitter tool [58] to generate an AST for each correct code piece and obtain the path from each word to the root node. Then, we compare the paths of different words and add the nearest common parent of the word pair to the dependency matrix. We establish the repair action(s) and inter-word dependency matrix for each piece of training data and feed the established data to the model."}, {"title": "4.4 Knowledge Distillation", "content": "Due to the conditional independence assumption of the NAR model, it is difficult to capture the multimodal distribution of target text, which is called the \"multi-modality problem\" in the literature[14]. For example, the buggy code \"Node block = NodeUtil.getFunctionBody (fnNode);\" corresponds to two fixed code pieces \"Node argsNode = NodeUtil.getFnParameters ( fnNode ) ;\u201d and \u201cNode block = fnNode.getLastChild ( ) ;\u201d. Since the NAR model outputs results in a parallel manner and lacks information about other locations, for the above example, the NAR model may output \u201cargsNode\u201d in the second location and \"fnNode.getLastChild\" in the fourth location. While these outputs all correspond to the correct code, they do not combine correctly. This situation is very common in the dataset and seriously affects the performance of the NAR model.\nTo alleviate this problem, we refer to previous NAR works [14, 50, 62] and use the knowledge distillation method to process the training dataset. First, we train the CodeT5-large pre-trained model with the training dataset. Then, we use the text generated by the trained CodeT5-large on the original training dataset as the distilled training dataset. This kind of processing can remove the noise in the dataset as much as possible, and only retain the most correctly fixed code. The model trained on the distilled training dataset can learn the knowledge of program repair more accurately. To account for the model\u2019s robustness, we train the NARRepair model on the distilled training dataset and the original training dataset simultaneously and learn primarily from the distilled training dataset."}, {"title": "4.5 Baselines", "content": "We select mainstream models in the APR and NAR fields as the baselines for our experiments. In the field of APR, we select 6 models that are often used as baselines: SequenceR [5], CoConut [40], Rewardrepair [72], Recoder [79], AlphaRepair [66], and Tare[80]. These baseline models include models with simple structure and fast inference speed like SequenceR, and models with complex structure, good performance but slow inference speed like AlphaRepair. In addition, we also select Tare and AlphaRepair, which (to our knowledge) are the state-of-the-art models on the Defect4J and QuixBugs datasets respectively. Following previous work[66, 72, 80], we select only comparing models with similar parameter sizes and exclude large models with an extremely large amount of parameters (such as ChatGPT[4] and LLAMA[57]) in the baselines. For the repair accuracy of the baselines, we directly adopt the results published by the corresponding papers to facilitate fair comparison. For example, since SequenceR only publishes the experimental results for the Defect4j v1.2 data set under the perfect defect location assumption, we directly adopt this result and consider the results for other datasets and scenarios as unknown. For the inference speed of the baselines, we re-run the models provided by the authors to establish the needed time.\nIn the field of NAR, we have selected five advanced models proposed in recent years: DePA [78], OAXE-NAR [8], Fully-NAR [15], SNAR [35], and CTC [53]. To ensure the fairness of the experiment, following other APR works [66, 72], we load the weights of the pre-trained model CodeT5 [63] into NARRepair and other NAR baseline models. Our purpose is to respond to what we mentioned in the introduction that the naive use of NAR models (in the field of machine translation) for the APR task is not suitable and proves the advancement of our model. We implemented five NAR baseline models to evaluate their performance and inference speed on the APR task."}, {"title": "4.6 Metrics", "content": "To verify the correctness of the generated patches, we follow the previous work of the APR community to verify patches using two methods: test cases and manual analysis. A patch is considered valid if it successfully passes all test cases. A"}, {"title": "4.7 Implementation Details", "content": "We use Pytorch [48] to implement the NARRepair model. During training, same as in previous work, we use the Adam optimizer [24] to update the model parameters. During the optimization process, to feed the model as much data as possible, we set the batch size and epoch in all our experiments to be 50 and 200 respectively. As the training process proceeds, the learning rate is adjusted (ranging from 0 to 0.00005) to adapt to the learning situation at different stages of the model. The maximum sequence length is set to be 512, and the word out of range is ignored. After experimental verification, we set the maximum repair length to be 20."}, {"title": "5 EVALUATION RESULTS", "content": "In this section, we present the main experimental results. First, we give the comparison results about the accuracy and inference latency of the NARRepair model and the baseline models. Next, we introduce ablation experiments to show the effectiveness of each module of the model. Then, we show that the NARRepair model can help with existing pre-trained models in improving the inference speed. After that, we demonstrate the effectiveness of NARRepair model for alleviating the issue of over-correction generally associated with the NAR models. Finally, we verify the existence of a close relationship between parent nodes and child nodes in the AST. In terms of equipment, we use 4 NVIDIA RTX 3090 GPUs and the CPU is \"Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz\"."}, {"title": "5.1 (RQ1) Results of Comparison with the Baselines", "content": "Results with Perfect Fault Localization. We first compare the repair performance of the NARRepair model with that of the baseline models under the perfect fault localization scenario. The results are presented in Table 1. The NARRrepair model correctly repairs 69, 41, and 23 buggy programs for the three datasets Defect4J v1.2, Defect4J v2.0, and QuixBugs respectively. Compared with the NAR models in the field of machine translation, the NARRepair model outperforms the optimal NAR model by 197%, 186%, and 209% in the number of repaired programs for the three datasets respectively. Compared with other DL-based APR techniques that use the autoregressive manner, the NARRrepair model greatly increases the inference speed without significantly decreasing the number of repaired programs. More specifically, NARRepair fixes 69 bugs from the Defect4J v1.2 dataset, which represents 90% of the optimal model (Tare); NARRepair fixes 45 bugs from the Defect4J v2.0 dataset, which represents 95% of the optimal model (Rewardrepair); NARRepair fixes 23 bugs from the QuixBugs dataset, which represents 82% of the optimal model (AlphaRepair). Moreover, the performance of the NARRrepair model is even better than that of some autoregressive APR models, such as CoCoNut. Overall, this result demonstrates that 1) the NAR model in the field of machine translation cannot be directly used for the APR task, and 2) the proposed NARRepair model is effective for the APR task.\nResults without Perfect Fault Localization. We also compare the performance of the NARRepair model with that of the baseline models when the defect location is not known in advance. For this, we use Ochiai [1], a widely used spectrum-based fault localization tool to establish the suspiciousness scores of buggy statements and rank them accordingly. Since the Ochiai tool cannot perfectly locate all bugs, the number of fixed bugs by all AR models and NAR models will decrease. The detailed results are also shown in Table 1. Under this scenario, the NARRrepair model correctly repairs 51, 26, and 20 bugs for the three datasets Defect4J v1.2, Defect4J v2.0, and QuixBugs respectively. Overall, the NARRepair model demonstrates similar results as in the perfect fault localization scenario. First, compared with other NAR models, the performance of the NARRepair model is improved by more than 150% for the three datasets. Second, compared with DL-based APR techniques that use the autoregressive manner, the NARRepair model delivers comparable performance.\nResults of Model Inference Speed. We evaluate the inference time of the NARRepair model and the baseline models in both CPU and GPU environments. When calculating the inference time, we let each of the baseline models and the NARRepair model generate 200 patches and calculate the average time to generate each patch. In the experiment, we use the inference time of DL-based APR model SequenceR, which has the fastest inference speed, as the baseline and show the results in Table 1. Compared with DL-based APR models with complex structure, such as AlphaRepair, the inference speed of NARRepair is increased by 18.6 times in the GPU environment and 15.2 times in the CPU environment."}, {"title": "5.2 (RQ2) Results of Ablation Study", "content": "To evaluate the contribution of each part of the NARRepair model, we perform an ablation study on the Defect4J v1.2 dataset under the perfect fault localization scenario. Starting from the complete model, we remove a specific part of the model structure each time and observe the impact of the removal on the results. More specifically, we (1) first replace the two-stage encoder with a normal NAR encoder to observe the impact of contextual information on the results; (2) then replace the attention mechanism for fusing inter-word dependency information with vector addition to observe the role of the attention mechanism; (3) next remove the inter-word dependency extractor to observe the impact of inter-word dependency information on the results; (4) finally remove the repair action predictor and instead pass only the repair length to the decoder to observe the impact of repair actions on the results. The results are shown in Table 4.\nFrom the results in the table, we draw the following conclusions. First, after removing the two-stage decoder, the number of repaired programs by the model drops by 15 under the perfect fault localization scenario. This shows that the contextual information obtained by the decoder through the [Mask] tag is of great significance for the result. Second, after removing the feature fusion of the attention mechanism, the performance of the model decreases slightly for the evaluation scenario. Third, after removing the inter-word dependency extractor, the number of repaired programs by the model drops by 9 under the perfect fault localization scenario. This result verifies that the obtained inter-word"}, {"title": "5.3 (RQ3) Results of Loading Pre-trained Model Weights", "content": "To show that our method can help pre-trained models improve inference speed, we select some mainstream pre-trained models for programming languages, including CodeBERT [11], GraphCodeBERT[16], CodeT5[63], and CodeGPT[39]. Among them, CodeBERT and GraphCodeBERT are composed of encoders of Transformers, CodeT5 is composed of both encoders and decoders of Transformers, and CodeGPT is composed of decoders of Transformers. Since NARRepair is also based on the Transformer structure, we can respectively load the weights of the four pre-trained models into the NARRepair model. To observe the impact of our method on the pre-trained model, we compare the results generated using the pre-trained model with those generated by NARRepair, which has loaded the weights of the pre-trained"}, {"title": "5.4 (RQ4) Results of Alleviating the Over-Correction Problem", "content": "In Section 3, we mentioned that one of the main purposes of the repair action predictor and the two-stage decoder is to avoid changing correct words into wrong ones during the repair process. The repair action predictor avoids modifying correct words by predicting that the repair action for those words is \"Keep\" (\u00a73.2). The two-stage decoder decodes words with low confidence again to avoid some correct words from being modified (\u00a73.4). To explore the effectiveness of our idea in more detail, we respectively remove the repair action predictor module and the two-stage decoder module, and observe the corresponding changes in the output results of the NARRepair model. Among all patches generated by the NARRepair model on the Defect4j v1.2 dataset, we count the average number of correct words changed into incorrect ones and present the results in Table 6.\nFrom the results in Table 6, we can observe that both the repair action predictor module and the two-stage decoder module reduce the number of correct words in the code that are modified into incorrect ones. When we remove the repair action predictor from the NARRepair model, the number of correct words modified in the patches (generated by the model) increases by 0.8 on average. When we remove the two-stage decoder at the same time, the number of correct words modified in the patches (generated by the NARRepair model) further increases by 2.3 on average. This suggests that the two-stage decoder is more effective in avoiding correct words wrongly modified into incorrect ones. However,"}, {"title": "5.5 (RQ5) Results of the Similarity Between AST Nodes", "content": "To show the degree of correlation between parent nodes and child nodes in the abstract syntax tree, we introduce cosine similarity. Usually, when the cosine similarity of two feature vectors is high, it means that the two vectors are strongly related, and vice versa. We obtain the inter-word dependency matrix output by the trained NARRepair model. Each content in the dependency matrix represents a parent node in the AST. We calculate the feature cosine similarity between the feature vector of the parent node in the dependency matrix and that of each word in the code text (generated by the inter-word dependency extractor). We classify the calculated values by the distance from parent nodes to child nodes in the AST, and the results are shown in Figure 8. According to Figure 8, we can see that the similarity between the parent node and the child node gradually decreases as the distance increases. We also find that the farther the distance between the parent node and the child node, the greater the fluctuation range of similarity."}, {"title": "6 DISCUSSION", "content": "6.1 Threats to Validity\nIn line with existing works on automatic program repair, our results should be interpreted with several threats to validity in mind.\nInternal Validity. Threats to internal validity might come from the potential faults in the implementations of NARRepair itself and its evaluation. To avoid faults in the implementation of NARRepair, our implementation is mainly based on mature deep learning and program analysis libraries, such as Fairseq [46] and Tree-Sitter [58]. In addition, we have performed thorough testing to ensure the correctness of NARRepair. To alleviate the threats to evaluation, we used the reported results published by the respective papers for the baselines to facilitate fair comparison. When running the models for the baselines is necessary, we re-run the models provided by the authors. Furthermore, note that the whole artifact related with this article is made available online for scrutiny and extension by other researchers.\nExternal Validity. A potential threat to external validity concerns the representativeness of the benchmark used in our experiment. To mitigate this threat as much as possible, we used three widely used datasets in the APR community"}, {"title": "6.2 Future Work", "content": "For future work, we will focus particularly on achieving higher accuracy while increasing the inference speed, even better than that of the state-of-the-art AR models. In addition, we will evaluate the performance of NARRepair using more benchmarks and additional programming languages to verify the effectiveness and generalizability of the model. Finally, we also plan to apply our key ideas to other software engineering tasks for which the inference speed is of great importance. Indeed, we hope that our work can arouse the interest of software engineering researchers in the inference speed of deep learning models for software engineering tasks, which can make software engineering research better meet actual developer needs."}, {"title": "7 CONCLUSIONS", "content": "In this paper, to increase the inference speed while maintaining the accuracy of repairing buggy code text, we propose NARRepair, a non-autoregressive model for automatic program repair. To solve the issues of wrongly modifying correct words into wrong ones, missing inter-word dependency information, and missing contextual information that are generally associated with non-autoregressive models, we propose a repair action predictor, inter-word dependency extractor, and two-stage decoder in NARRepair for addressing the three issues respectively. We evaluate the perfor- mance of the NARRepair model on three widely used datasets for automatic program repair tasks, including Defect4J v1.2, Defect4J v2.0, and QuixBugs. The experimental results show that while maintaining the high repair accuracy, NARRepair's inference speed is 5.4 to 18.6 times faster than other automatic program repair models. Besides, the results additionally show that NARRepair performs better than other non-autoregressive models. Finally, we also demonstrate that our method is effective for pre-trained models and is suitable for generalizing to large language models in order to meet the need for real-time code repair."}]}