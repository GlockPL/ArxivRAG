{"title": "ARE LANGUAGE MODELS RATIONAL?\nTHE CASE OF COHERENCE NORMS AND BELIEF REVISION", "authors": ["Thomas Hofweber", "Peter Hase", "Elias Stengel-Eskin", "Mohit Bansal"], "abstract": "Do norms of rationality apply to machine learning models, in particular language models? In this\npaper we investigate this question by focusing on a special subset of rational norms: coherence\nnorms. We consider both logical coherence norms as well as coherence norms tied to the strength of\nbelief. To make sense of the latter, we introduce the Minimal Assent Connection (MAC) and propose\na new account of credence, which captures the strength of belief in language models. This proposal\nuniformly assigns strength of belief simply on the basis of model internal next token probabilities.\nWe argue that rational norms tied to coherence do apply to some language models, but not to others.\nThis issue is significant since rationality is closely tied to predicting and explaining behavior, and\nthus it is connected to considerations about AI safety and alignment, as well as understanding model\nbehavior more generally.", "sections": [{"title": "Introduction", "content": "The question whether an AI is rational is of fundamental importance for understanding, explaining, and predicting its\nbehavior. Rationality is a key aspect of what human beings generally attribute to each other when we try to explain\nand predict our behavior. But it is unclear whether anything like this is also applicable to machine learning models\nand AI systems more generally. In this article, we consider the question for a specific case. First, we will focus\non a particular important class of AI systems\nlanguage models\nand, second, on a particular important part of\nrationality: coherence norms and belief revision. Thus our main question will be whether language models fall under\nthis central class of rational norms.\nTo get clearer on our main topic, it is important to first note that the term 'rational' is used in different ways, and\ndepending on how one understands it, our main question either becomes a trivial one or a more substantial one. On\nthe one hand, 'rational' is often used simply as a term of approval, where an AI is rational just in case it does what\nit is supposed to do. See Russell (1997). On such an understanding, intelligent systems are rational, as long as they\nwork properly. But this conception of being rational does not consider how the system does what it is supposed to do,\nand what internal states of the system contribute to its success. A different sense of 'rational' focuses on these internal\nstates instead, not just on the overall behavior of the system. This is how we often evaluate human beings as rational.\nHere, we do not simply evaluate their behavior, but we focus on their internal states and evaluate them. We consider\ntheir beliefs or reasoning or actions as rational, and we thereby evaluate them as having arisen in the proper way from\nthe proper internal, representational states. We can call the former, thinner notion of being rational the deflationary\nnotion, and the latter the representational notion, since it concerns internal representational states.\nExhibiting intelligent behavior and being rational are closely connected under the deflationary sense of rationality,\nbut they can come apart when we compare intelligent behavior and being rational in the representational sense. Focus-\ning on the representational notion, we get a substantial question about whether an AI is rational in this sense, even if\nit exhibits intelligent behavior. The question then comes down to whether its intelligent behavior is properly related to\nand explained by its internal representational states which bring it about. This is how our human intelligent behavior is\ncommonly explained: by reference to our beliefs and desires, which represent the world and guide our actions. A (rep-\nresentationally) rational AI will thus be similar to our human intelligence in at least this way. But on the other hand, if\nthe intelligence of an AI is not to be explained by it having representational states that are subject to considerations of\nrationality, but via some other, non-representational, mechanism, then the intelligence of this AI will be very different\nfrom our human intelligence. Such an AI might manipulate symbols that have no semantic meaning, or it might push\nmatrices through a neural network without those matrices representing the world around it, or do something different\naltogether. And in that case, it will be very different to explain, predict, and understand it. This will affect our attempts\nat AI safety, alignment, explainability, and so on, and thus this question is of significant interest.\nWe will in the following focus on representational rationality, in particular for language models, and with an eye on\na special class of representational states that are subject to rational evaluation: beliefs. Other representational states like\nintentions or desires can also be subject to rational evaluation, as can be a combination of different states. An intention\ncan be irrational given what one believes and what one desires. However, considering the rational evaluation of belief\nfirst, and leaving more complex cases of the rational evaluation of a mixed group of states for later, is a reasonable\nfirst step in understanding the place of rationality in machine learning models. Beliefs are the most paradigmatic and\nsimplest kind of states subject to rational evaluation and thus a good candidate to consider first. Similar issues will\narise for other representational states, and combinations of states. As we will see below, there are a number of different\nrational norms that can be seen as applying to belief alone, with the simplest ones being coherence norms, which are\nour main topic."}, {"title": "Belief in language models", "content": "The easiest way to argue that language models have beliefs is via the connection of belief to knowledge. Language\nmodels seem to know a lot about the world. Although they might make things up and get things wrong, they nonethe-\nless often reliably produce correct answers to many questions about the world. For example, many large language\nmodels can correctly answer the question of what the capital of France is and thus are naturally taken to know that\nthe capital of France is Paris. But knowledge is just a special kind of belief: a belief that is true and especially well\nsupported. The details of the connection between knowledge and belief have long been controversial in the philosoph-\nical literature. In particular, it is controversial what else is required besides a state being a belief state that has a true\ncontent for this state to be knowledge. Nonetheless, the general connection between knowledge and belief relied upon\nhere is almost universally accepted: to know is to believe. Thus since language models know about the world, they\nhave beliefs about the world.\nBut this argument is too simple to be persuasive. We must distinguish the model knowing about the world, on\nthe one hand, and us coming to know about the world by interacting with the model, on the other hand. The latter\ncan be possible simply because the model has the relevant information stored somewhere in its internal states, without\nthose internal states themselves being states of knowledge. To illustrate, we can come to know about the world by\ninteracting with a dictionary, but it seems dubious that the dictionary itself knows about the world. It merely carries\ninformation about the world. Or to make the point more forcefully, I can come to know about the local wildlife by\nlooking at the tracks in the snow, which carry information about recent wildlife crossings, but it is mistaken to think\nthat the snow itself knows about the wildlife.\nTo have beliefs about the world requires an internal state that is a state of believing, and simply carrying information\nalone is not enough for this. The question is whether language models have internal states which are states of belief.\nThere are two main worries about this, one is very familiar and one less so. The first concerns whether the internal\nstates of language models are representational states at all. Do any internal states of the model represent the world\nwhich created the text on which they are trained? Or does the model merely reliably predict text without any regard to\nthe semantic properties of the text they produce? This issue is widely discussed, and we do not hope to settle it here.\nInstead we will grant that the internal states of the models are representational, and then focus on a different problem\ntied to models having beliefs about the world. This problem is not as widely discussed and arises even if we assume, as\nwe now do, that models have representational states about the world. The worry is that none of these representational\nstates are belief states since they do not meet the minimal requirements for a state being a belief state. This problem is\nthe focus of this section.\nNot all representational states are states of belief, even in minds that have beliefs. Beliefs are a distinct kind of\nrepresentational state, and their distinct nature is revealed by focusing on the standards of correctness that govern\nbeliefs. Different kinds of states come with different standards of correctness and what those standards of correctness"}, {"title": "Coherence norms for language models", "content": "Belief in humans is subject to the norms of rationality. We are not always rational, but we ought to be, which is to\nsay: we are under the norms of rationality, even if we don't always live up to those requirements. Rationality itself is\ncomplex, and the requirement to be rational can be analyzed as a series of distinct norms. One core part of these norms\nare coherence norms, which concern how the internal states of a system ought to relate to each other. Other norms of\nrationality concern the proper response to reasons or to evidence. How the different parts of rationality relate to each\nother is itself controversial, but we will simply focus on coherence norms in the following.\nCoherence norms come in different kinds, most notably as ones concerning full belief and ones concerning degrees\nof, or strength of belief. Most notably among the former are norms concerning logical coherence and among the latter\nare norms about probabilistic coherence. Logical coherence norms concern the relation of our beliefs to each other with\nrespect to logical implication and consistency. To give a simple example, a logical coherence norm might demand that\none ought not to believe a particular proposition p as well as its negation \u00acp. Probabilistic coherence norms in general\nalso come in different kinds. One kind concerns belief about probabilities itself. For example, a norm might demand\nthat one ought not to believe that the probability that it is going to rain is 90%, and also believe that the probability that\nit is not going to rain is 60%. But besides beliefs explicitly about probabilities, there are also coherence norms tied"}, {"title": "Logical coherence norms", "content": "The solution to the first problem is similar to the argument given above in Section 2 for the coherence of beliefs in\nlanguage models. It is true in general that a pre-trained language model should not be seen as being under the norm\nof logical coherence. Such a model is simply aiming to model the training data, and that data is not logically coherent.\nTo require coherence of a pre-trained language model that is trained on incoherent data is to require something of it\nthat goes against what it fundamentally aims to do: learn the training data. But things are again different for fine-tuned\nmodels, in particular ones that are fine-tuned for truthfulness. Properly fine-tuned models can acquire the function\nto produce truthful text, and if we assume that such models have internal representational states like beliefs and that\nthose states are properly causally connected to the text they produce, then the aim of being truthful can support that\nlogical coherence norms apply to the internal states of such models. Internal belief states that are incoherent will be\ncontrary to the norm of truth which is part of the standard of correctness for belief itself. Thus inconsistent beliefs are\nin conflict with the standard of correctness of belief, and thus the coherence norm of consistency for beliefs applies. A\npre-trained model does not need to be under such norms, but a model fine-tuned for truthfulness is."}, {"title": "Defining credence for language models", "content": "Logical coherence norms make sense for certain language models. But do coherence norms tied to credences also\nmake sense? Those coherence norms are tied to the strength of belief, and how the strengths of different beliefs\nare supposed to relate to each other. This issue is simply not addressed by focusing on logical coherence, which\nconcerns full belief and whether one's full beliefs are contradictory. To make further progress on needs to overcome\none main obstacle: how to make sense of credences for language models in the first place. Credences correspond to\nhow strongly something is believed. But how is strength of belief to be understood for a language model? To make\nsense of it we need to assign probabilities to propositions, which are the contents of what is believed. This assignment\nof (subjective) probabilities in turn must correspond to the strength of belief: the closer the number is to 1, the stronger\nthe corresponding proposition is believed, which in turn means that it is harder to give that belief up in light of new\nevidence, or in light of a conflict with other beliefs that are assigned lower numbers by this credence function. To\nmake sense of such an assignment of probabilities to propositions for language models is the key first step to making\nprogress on the question whether coherence norms tied to credences apply to language models.\nLanguage models are steeped in probabilities, but those probabilities concern the next token, not the strength of\nbelief. Tokens generally are close to a word and thus sub-propositional. But credences apply to propositions as a\nwhole, which are close to a complete sentence. The question is how can we make use of probabilities like\n$P(t_n|t_1, ....., t_{n-1}) = x$\nwhich are assigned by the language model to the n-th token given that the previous tokens are t1, ..., tn-1, and turn\nthat into an account of what the models credence in proposition p is."}, {"title": null, "content": "We propose to solve this problem via the Minimal Assent Connection (MAC). The basic idea of MAC is simply\nto consider the probability of the model predicting the next token to be 'yes' when asked whether it is the case that p.\nThis way next token probabilities can be directly tied to probabilities assigned to propositions. To put a label on it, we\ncan say that the model minimally assents to p just in case the probability it assigns to 'yes' being the next token after\n'Is it the case that p?' is sufficiently high. This basic idea is promising, but to in order to turn it into a credence function\nthat captures strength of belief more needs to be said and we will do this momentarily. But on the positive side, we\ncan already note that this approach is completely general, since for any proposition p we can consider the probabilities,\nthat the model assigns to the next token after the question whether it is the case that p. This probability comes from the\ninternals of the language model as a machine learning model, and it is just the kind of probability we want to exploit\nin assigning credences to the model. This assignment thus applies to any proposition p uniformly. Furthermore, the\nprobability of a 'yes' answer to the question whether p is naturally associated with the strength of believing p. After\nall, it is natural to hold that the probability assigned to 'yes' after the question whether p is closely correlated with the\nstrength of believing p: the stronger the confidence of the model in p, the higher the probability for a 'yes' answer, all\nthings being equal. But still, it can't quite be a full answer and we can't make this connection as outlined. That is, we\ncan't fully endorse that we can define cr(p), the models credence in p, as follows:\n$cr(p) = P(Yes | Is it the case that p?)$\nThere are essentially two flaws with this. First, the model might assent to p in ways other than with 'yes'. It\nmight also assent via other forms of affirmation, say 'yeah' or 'indeed'. Using just (1) thus gives a credence that is\ntoo low. But we can simply fix this in one of two ways. First, we could force the model to answer either 'yes' or 'no',\neither by adding this requirement to the question prompt, or by constraining the model in other ways to do so (e.g. by\nconstraining and re-normalizing the output distribution). However, adding such further constraints slightly changes the\nmodel or the text for which it predicts the next token. Making such a demand for either a 'yes' or a 'no' answer thus\nrequires changes that make the method not completely general any more. It wouldn't apply to any model any longer,\nonly to ones properly modified to meet this constraint. But there is also a second way to solve this problem. We can\nleave the model and input alone, and instead simply sum over all affirmations, not just 'yes', but also 'sure', 'yeah',\nand so on, including multi-token sequences like 'I couldn't agree more.' All such tokens or sequences of tokens we\ncan label an assent sequence. If we call AS the set of all such assent sequences, then we can better define the credence\nfor p by the model as the sum of all conditional probabilities of assenting. So, we can define the assent probability as:\n$as(p) = \\sum_{s \\in AS} P(s | Is it the case that p?)$\nWe can then, as a second attempt, identify the credence of the model in p with as(p). Setting the credence of p to\nas(p) is an improvement over (1). But it isn't quite right either, since it again generally underestimates the credence\nand sets it too low. The reason for this is simply that the model might answer the question whether it is the case that p"}, {"title": null, "content": "in some way completely unrelated to its belief in it. It might answer \u2018As an AI language model I cannot address issues\nof this nature' or some other way that avoids answering this question. Some of the probabilities for how to continue\nthe sequence will be allocated to such answers, and thus the part of the probability distribution that is devoted to assent\nsequences does not properly represent the strength of belief of the model.\nBut we can fix the approach to overcome this difficulty. Strength of belief can be associated with how likely the\nmodel is to minimally assent when compared to how likely it is to minimally dissent, leaving aside other ways to\nrespond to the question besides assenting or dissenting. So, we can simply consider the ration of assenting to either\nassenting or dissenting. To make this idea work, we also need to consider the set of all dissent sequences DS, which\nare the ways the model might minimally dissent: 'no', 'never', etc..\n$ds(p) = \\sum_{s \\in DS} P(s | Is it the case that p?)$\nSo understood we should not expect that as(p) and ds(p) sum to 1, since the model might respond in other ways\nthan assenting or dissenting. But we can simply re-normalize by considering only the ratio of probabilities given to\naffirmations to those that are given to either affirmations or dissent, leaving out all other options. And this ratio is the\nproper way to assess the strength of belief of a model in a proposition p: how likely it is to assent when we consider\nonly assent or dissent, leaving out all other options. This then is the proposal for how to assign credences to a language\nmodel in a way that is derivative only on the model's own assignment of probabilities to the next token:\n$cr(p) = \\frac{as(p)}{as(p) + ds(p)}$\nThis association properly corresponds to the strength of belief in a proposition by the model, assuming we can talk\nabout belief in the model at all. It naturally corresponds to the idea that the more likely the model thinks that the next\ntoken is 'yes' following the question whether p, the more strongly it believes that p.\nArticulations of what the assent and dissent sequences are will approximate the target credences of the model, with\nthe proper articulation reaching its target. But for practical purposes it might well be enough to work with what we can\ncall the Yes-No approximation: which considers only 'yes' as an assent sequence, and only 'no' as a dissent sequence.\nThus the Yes-No approximation to a model's credence assigns to a proposition p the credence which is the ratio of\nthe model's next token probability assigned to 'yes' after the prompt 'Is it the case that p?' divided by the sum of the\nprobabilities for 'yes' as well as for 'no' (conditional on the same prompt). This will approximate the true credence,\nbut it is an empirical question how closely for a given model.\nWe have not given a detailed list of assent and dissent sequences, and doing so requires one to make some choices\nthat can be subject to debate. One concern is tied to context sensitivity. It could be that what counts as assent or dissent\nfor a particular issue p depends itself on p, and thus it is not clear whether assent and dissent sequences can simply be\nlisted for questions concerning any p. Although this is in principle possible, it does not seem to be a major concern.\nFirst, it is unclear what plausible cases there are for this, and second it merely requires a reformation of the proposal"}, {"title": null, "content": "by relativizing assent and dissent sequences to a given p, and then consider the assent/dissent ratios concerning p with\nwith each those relativized to p itself.\nA more difficult case is presented by examples of sequences that concern the epistemic state of the model itself,\nas 'It seems to me to be unlikely,' 'I am quite sure that p,' or 'it it is doubtful.' Such sequences contain 'epistemic\nmarkers' or 'epistemic strengtheners or weakeners' that report on the epistemic state of the model. Strong epistemic\nmarkers indicate that the model is in a strong epistemic position to make an assessment, as with 'I am sure that...,' or\n'I am quite certain that..'. Weak epistemic markers indicate a weak epistemic state, as in 'I am not sure, but it seems\nthat...' or 'I doubt that...'. Should sequences with such epistemic markers, either strong or weak, count as assent or\ndissent sequences for our purposes here? Should next token probabilities allocated to such sequences count towards\nassent or dissent probabilities?\nOn the one hand, it is clear that 'I am sure that p' is a form of assenting to p, and that prima facie speaks in favor\nof including it as an assent sequence. But doing so would have a negative impact in a related area. When a model\nepistemically marks its response, then this marking should correspond to its confidence in the answer, and thus it\nshould correspond to the credence it has in this being the right answer. So, when a model responds \u2018I am certain that p'\nthen this is the proper response only if the credence in p itself is very high. One should only say that one is certain that\np if one has high credence in p. In fact, this connection can be seen as a further coherence norm for rational thinkers.\nIf the strength of my epistemic marking diverges from my credence, then I am arguably violating a coherence norms.\nIf, however, we include epistemic markers in the list of assent and dissent sequences themselves, then the credence\nof a model in p will in part be determined by its epistemic markings of p. But these issues should at first be kept\nseparate. It is one thing how strongly the model believes that p, and another whether a particular epistemic marking is\nappropriate for that model. It is thus overall best to exclude such epistemically marked sentence from the list of assent\nand dissent sequences. Maybe it is best to think of 'I am quite sure that p' as containing two parts, made more explicit\nby its reformation as 'p and I am quite sure of it'. The first conjunct is a proper assent to p, while the second one is\nnot, and only a comment on the model's own epistemic state. We should only count the first conjunct as an assent\nsequence, but not the combination of both as a further one.\nWhether or not present day models' credences match their epistemic markings is an empirical questions. If they\ndo they are epistemically better functioning than if they don't. To determine whether there indeed is this correlation is\nsubject to ongoing work. See also Zhou et al. (2024) for more on models reporting on their own epistemic state.\nFurthermore, we would like to make clear that the assignment of credences to beliefs for models as defined above\nare for revealed belief, since they concern the model's response to questions. This approach will not work for models\nthat have developed the ability to lie and thus answer questions contrary to what they believe. How to make sense\nof lying in language models is not clear, since it is not clear how to make sense of language model belief besides\nrevealed belief. To be clear, lying here is understood not simply as asserting something false, or even as disregarding\nthe truth altogether, but as asserting something that is contrary to what one believes. To make sense of lying on\nthis understanding of it requires that one can make sense of belief in language models independently of the model's"}, {"title": null, "content": "produced text. Whether this can be done is unclear, and our approach does not aim to do this, but focuses on revealed\nbelief instead.\nFinally, we would like to note that\n$as(p) = ds(p)$\nwhich is to say 'the strength of assenting to -p being equal to the strength of dissenting to p' is not a requirement on\ncredence itself, but at best a norm of rationality tied to credences. Credence itself does not come with a requirement\nthat assenting a negated proposition is properly correlated with dissenting the un-negated proposition. That there\nshould be such a connection is reasonable to demand for any believer who is rational. But it should not be built into\nan account of what credence itself is for a believer, and it is not part of our definition of credence for language models.\nThat particular models might not meet this requirement does thus not speak against the present account of credence\nfor language models itself, but it would be a strike against the rationality of these models, all things considered."}, {"title": "The argument for probabilism", "content": "Credences thus make sense for language models in general. As is common terminology, we can say that the credence\nfunction for a model is the function that assigns a proposition the corresponding credence that the model has towards\nthat proposition. With the above account of credences for language models, a model's credence function is also well-\ndefined. The question remains whether the norms of rationality tied to credences apply to language models, and with it\nthe question remains what requirements there are on the credence function of the model. What kind of function should\na model's credence function be?\nWhen focusing on synchronic coherence norms, this question quickly turns into the question whether the credence\nfunction should satisfying the axioms of probability theory and thus be a probability function. In the philosophical lit-\nerature the view that a credence function should be a probability function is called probabilism. Assuming credences\nfor language models as spelled out above, does probabilism apply to them as well? Are language models under the\ncoherence norm that their credence function satisfies the axioms of probability theory? For example, are they under\nthe norm that $cr(p) = 1 \u2212 cr(\u00acp)$?\nTo answer this question we first need to again distinguish pre-trained language models from fine-tuned ones, in\nparticular those fine-tuned for truthfulness. For a pre-trained model we should not in general expect the model's\ncredences to obey the probability calculus, nor that they are under any norm to do so. Since the model will generally\nbe trained on incoherent text, we can not expect that cr(p), the credence assigned to p, is exactly 1 \u2013 cr(\u00acp). If the\ntraining data is sufficiently incoherent, then it can well be that the credence for each of them is over 50%. Since the\nmodel so far is merely a pure language model whose internal states are correct insofar they contribute to the training\ntask and learning the data, we should not expect there to be a norm on these probability assignments that correspond\nto probabilism."}, {"title": null, "content": "However, things are again different for the model that is fine-tuned on truthfulness. A model that aims at true\nbeliefs is subject to the well-known accuracy arguments for probabilism. See Joyce (1998), Teitelbaum (2023). The\nmain idea of these arguments is as follow: if the goal is truth, then credences should be as close to the truth as possible:\none's credence in a true proposition should be close to 1, and one's credence in a false proposition should be close to 0.\nHowever close one comes to this ideal can be seen as the accuracy of one's credences. A standard measure of accuracy\nin this sense is the Brier score, basically the Euclidean distance between one's credences in some propositions and\ntheir actual truth values (taken as 1 for true, and 0 for false) seen as points in a Euclidean space of the dimension\nthat corresponds to the number of propositions, but without taking a square root. So, the Brier score for a credence\nfunction cr from a finite set of propositions p1, ...Pn to [0,1] is defined as\n$BR(cr) = (tv(p_1) - cr(p_1))^2 + ..... + (tv(p_n) \u2013 cr(p_n))^2$\nwhereby tv(p) = 1 if p is true and 0 if it is false. The thought is that a credence function is better if its Brier score\nis lower, since one aims at the truth, and a lower Brier score of one's credence function means that one's credences\nare closer to how they should be. In Joyce (1998), James Joyce showed that if one's credence function does not meet\nthe axioms of probability theory, then there is a different credence function that has a lower Brier score no matter\nwhat the world is like and which meets the axioms of probability theory. However, if one's credence function does\nmeet the axioms of probability theory, then this is not always the case. This argument is taken to support probabilism:\nA credence function that isn't a probability function can always be dominated by one that is. Thus such a credence\nfunction isn't what it should be, given that one aims at the truth. And thus probabilism is a rational norm that applies\nto ones credences: one's credence function ought to be a probability function.\u201d This argument applies to the credence\nfunctions of language models just as it applies to the credence functions of human beings. As long as we aim at the\ntruth and at accuracy, i.e. lower Brier scores, our credence functions ought to be probability functions, or so the norms\nof rationality demand.\nThis then answers the question whether synchronic logical and probabilistic coherence norms apply to language\nmodels, or at least whether they apply to them just as much as they apply to us. We argued that such norms do not\nalways apply to pre-trained language models that simply model the distribution of words or tokens in a dataset of text.\nWhat is missing is the aim of being truthful. But this aim can be introduced in several ways, and with such an aim in\nplace the rational norms do apply as a consequence. In particular, fine-tuned language models which are fine-tuned for\ntruthfulness are under such coherence norms. In that case the norms of rationality apply to these models just as they\napply to us. The next question to consider is whether this also holds for diachronic norms of rationality."}, {"title": "Belief revision norms for language models", "content": "The logical and probabilistic coherence norms we considered so far were synchronic norms: they apply to a system\nwith beliefs at a time. Belief revision, on the other hand, is essentially diachronic and concerns how beliefs should be\nchanged over time. The most natural extension of probabilism to a diachronic norm is Bayesianism, which we here\ntake to be the view that credences should be updated in light of new evidence in accordance with Bayes' Theorem.\nRationality requires, according to a Bayesian, that one conditionalize on new evidence and adjust one's credences\naccordingly over time. This is a natural rational norm for belief revision as it applies to us humans and similar\ncreatures. But it is not clear how it applies to machine learning models like language models. First of all, it is unclear\nhow to make sense of the notion of evidence for a language model. For us human beings, evidence is tied to perception\nby observing the world or to learning from others directly in communication. But language models by themselves do\nnot have perceptual abilities. Even multi-modal models do not have perception as such. A model trained on language\nas well as images from comic books, say, would not thereby have a perceptual ability, since the images are not properly\nconnected to the world. Such a model is multimodal, but that by itself does not make it perceptual. Perception-like\nabilities could in principle be integrated with language models, say via feeding it images from a camera that depicts\nthe world, but without a clearer understanding of how that will go, it is hard to make sense of evidence for language\nmodels, and with it is hard to make sense of Bayesian updating and Bayesian approaches to belief revision. Understood\nas a diachronic norm of rationality, it does not apply to language models as they are at present.\nLanguage models nonetheless can change their beliefs: they can be fine-tuned to adopt a particular belief or whole\nset of beliefs, and they can be directly edited to change a particular belief. But such change of belief of the model\nis not a rational process for the model itself. It is simply an external modification of the model, and the question of\nrationality does not directly apply to it from the point of view of the model. It is a change that happens to it from an\nexternal source, not a direct response to rational forces by the model itself.\nNonetheless, the synchronic coherence norms that apply to the model will have an effect on what rational require-\nments there are for it after the change. Even if the change is not brought about by rational means, the resulting state\nafter the change is subject to the synchronic norms of rationality like logical coherence and probabilism, assuming\nthey apply to the model in the first place. The synchronic norms put rational pressure on the model's present state. No\nmatter how it got into that state, through rational means or merely external causal forces, the synchronic norms require\nit to be in a coherent state afterwards.\nThus for many present language models diachronic rational norms like Bayesian updating do not yet apply, even\nif the model is fine-tuned for truthfulness. However, synchronic norms do apply in this case, and they apply no matter\nhow the model got into its present state: through model editing to correct a false belief, or fine-tuning, or through other\nmeans that change its beliefs. As long as the model continues to aim at truthfulness, synchronic coherence norms\ncontinue to apply. Future models might more directly be under diachronic rationality norms, but for that to be so\nsomething like a notion of evidence for the model must be made sense of."}, {"title": "Living up to the normative requirements", "content": "The question what norms apply to language models is one that can be discussed rather generally. One can argue, as\nwe did above, that particular classes of models are under particular norms of rationality, while others are not. But\nthe question whether a particular model lives up to the norms that apply to it is one that is specific to each model. It\ndepends on how well the model is doing what it is supposed to do, and that depends on the details of the model, and\ncan't be determined in general. Nonetheless, there is a general question in the neighborhood, which is one about the\neffectiveness of rational requirements on models in general. Our human minds are under rational norms and somehow\nthese norms are generally effective. Although we are often irrational, it is not the case that rational norms have no\neffect on us at all. They often push us in the right direction. How rational and irrational human beings are is a widely\nstudied question. Our concern here is not human beings, but language models."}]}