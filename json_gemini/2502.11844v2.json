{"title": "BAXBENCH: CAN LLMS GENERATE\nCORRECT AND SECURE BACKENDS?", "authors": ["Mark Vero", "Niels M\u00fcndler", "Victor Chibotaru", "Veselin Raychev", "Maximilian Baader", "Nikola Jovanovi\u0107", "Jingxuan He", "Martin Vechev"], "abstract": "The automatic generation of programs has long been a fundamental challenge in\ncomputer science. Recent benchmarks have shown that large language models\n(LLMs) can effectively generate code at the function level, make code edits, and\nsolve algorithmic coding tasks. However, to achieve full automation, LLMs should\nbe able to generate production-quality, self-contained application modules. To eval-\nuate the capabilities of LLMs in solving this challenge, we introduce BAXBENCH,\na novel evaluation benchmark consisting of 392 tasks for the generation of backend\napplications. We focus on backends for three critical reasons: (i) they are practically\nrelevant, building the core components of most modern web and cloud software, (ii)\nthey are difficult to get right, requiring multiple functions and files to achieve the\ndesired functionality, and (iii) they are security-critical, as they are exposed to un-\ntrusted third-parties, making secure solutions that prevent deployment-time attacks\nan imperative. BAXBENCH validates the functionality of the generated applications\nwith comprehensive test cases, and assesses their security exposure by executing\nend-to-end exploits. Our experiments reveal key limitations of current LLMs in\nboth functionality and security: (i) even the best model, OpenAI 01, achieves a\nmere 60% on code correctness; (ii) on average, we could successfully execute\nsecurity exploits on more than half of the correct programs generated by each LLM;\nand (iii) in less popular backend frameworks, models further struggle to generate\ncorrect and secure applications. Progress on BAXBENCH signifies important steps\ntowards autonomous and secure software development with LLMs\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Automating software development is a key aspi-\nrational goal of Large Language Models (LLMs),\npromising to revolutionize the software industry\n(Lyu et al., 2024). They have shown impres-\nsive capabilities in assisting developers by gen-\nerating function-level completions (Chen et al.,\n2021; Austin et al., 2021a), suggesting code patches\n(Jimenez et al., 2024), and solving algorithmic prob-\nlems (Hendrycks et al., 2021). However, it remains\nunclear if LLMs are ready to autonomously gener-\nate larger-scale, deployment-ready code.\nThis\nThe Gap in LLM Code Benchmarking\ngap in understanding LLMs' capabilities is also\nreflected in the current state of LLM benchmarking.\nNamely, most current coding benchmarks assess LLMs' capabilities at function-level code writing\nand bug fixing (Chen et al., 2021; Austin et al., 2021a; Muennighoff et al., 2023), or focus on specific"}, {"title": "BAXBENCH: Correct & Secure Backends", "content": "To bridge this gap in LLM-generated code bench-\nmarking, we introduce BAXBENCH, a novel benchmark that tests the capability of LLMs to generate\ncorrect and secure backends. As the key component of modern web and cloud applications, backends\nrepresent a realistic target for the generation of challenging standalone modules. Crucially, as the\nrole of backends is to serve requests from potentially untrusted users, security is inherently critical.\nA single exploit can affect all users of the application, irrespective of their client-side setup. Con-\nsequently, BAXBENCH collects 28 challenging backend scenarios, which are to be implemented in\n14 backend development frameworks across 6 programming languages. Combined, this results in\n392 challenging benchmark tasks, each requiring the LLM to fully implement a correct and secure\nbackend application exposing API endpoints with specific functionalities.\nTo evaluate correctness, as part of each scenario, we include a suite of functional tests that the\ngenerated backend must pass. Modeling real-world deployment, we approach security evaluation\nthrough the lens of untrusted users that run malicious queries against the API in order to expose\nvulnerabilities in the generated code. The success of any such malicious query guarantees that the\nbackend is insecure and would pose severe risks in deployment. For each scenario, these exploits\nare developed by code security experts. To achieve high coverage of potential security threats, the\nexploits were iteratively refined on both LLM-generated and human-written solutions. Notably, both\nthe correctness and the security tests are agnostic to frameworks and programming languages, relying\nonly on the API exposed by the backend. This enables the testing of the generated code independently\nof implementation details beyond the exposed functionalities, reflecting a real-world setting."}, {"title": "2 CONSTRUCTION OF BAXBENCH", "content": "In this section, we describe the structure and the construction process of BAXBENCH. Starting from\na broad overview, we proceed to discuss the individual components of BAXBENCH in detail, namely\nits scenarios, including the corresponding functionality tests and security exploits (\u00a72.1), and our task\ncreation and execution process, including our choice of frameworks and evaluation metrics (\u00a72.2).\nOverview BAXBENCH contains 28 scenarios, each specifying the functionality of a backend\napplication that is to be implemented. Each scenario comes with a set of functional tests and security\nexploits used to test LLM-generated solutions. Further, for implementing these scenarios, we select\n14 diverse frameworks from 6 programming languages. Scenarios and frameworks are combined into\ntasks, leading to a total of 392 different challenging evaluation tasks, forming BAXBENCH.\nFor each evaluation task, the model is prompted with the scenario specification and asked to generate\napplication code in the target framework. We run the resulting code inside an isolated Docker\ncontainer, exposing its endpoints to the functional tests and security exploits of the scenario to test\nthe correctness and security of the application. Each application has access to the local file systems\nand may use an SQLite database to hold its state. We finally test for passwords, unencrypted secrets,\nor artifacts of the executed exploits by inspecting the files in the execution environment. In App. B\nwe provide a complete example of a task specification, example output by QWEN2.5 72B, and the\nexecution traces of the functional tests and security exploits."}, {"title": "2.1 SCENARIOS", "content": "Each scenario consists of a specification of the desired API endpoints, a corresponding plaintext\ndescription, and a set of framework-agnostic functional tests and security exploits. Following\nreal-world software development practices, the scenarios are specified in a unified way in the\nOpenAPI (OpenAPI Initiative, 2025) format, describing the functionality, request format, and response\nparameters for each API endpoint. Additionally, these specifications are transcribed into plaintext\ninstructions for each scenario. However, as we show in \u00a74, compared to plaintext instructions,\nproviding models with the OpenAPI specifications makes the task of generating correct applications\nsignificantly less error-prone.\nTo select scenarios that reflect relevant use cases in terms of both functionality and security, we define\nfour criteria. Each scenario should: (i) represent a backend application that often occurs in real-world\nsoftware development; (ii) have sufficient implementation complexity over existing function-level\nbenchmarks; (iii) describe an application with potential security vulnerabilities; and (iv) be realizable\ncorrectly and securely in existing backend frameworks.\nGuided by this, we filtered an initial set of proposed scenarios, and manually verified that the final\nset of 28 scenarios meets the above criteria. The list of the final scenarios together with a short\ndescription and a list of each of their potential security vulnerabilities is included in Table 3 in App. A.\nNext, we describe the construction of functional and security tests in our scenarios in more detail."}, {"title": "Functional Tests", "content": "Following industry-standard practices, and in line with prominent code func-\ntionality benchmarks (Chen et al., 2021; Jimenez et al., 2024), we evaluate the correctness of\nLLM-generated applications using functional tests. These tests verify the end-to-end functionality of\neach endpoint of the backend application as described by the OpenAPI specification of the scenario.\nAs the specifications are given on the API level, all our tests are framework-agnostic, and can be\ndirectly reused across different BAXBENCH tasks that use the same scenario. This modularity is a\nkey advantage of BAXBENCH, as it enables the addition of future frameworks without needing to\nadjust the functional tests. Our functional tests are created manually, and verified by running them on\nhuman-reviewed solutions to the benchmark tasks."}, {"title": "Security Evaluation", "content": "Prior works often resort to static analyzers to measure security (e.g., Fu et al.\n(2024) or He et al. (2024)), but such tools have several major limitations. First, they are plagued\nboth by false positives and false negatives (Wadhams et al., 2024; Zhou et al., 2024; Ami et al.,\n2024). Second, they are often only available as a paid service, and as such limit reproducibility in the\ncontext of an open-source benchmark (Bhatt et al., 2023; Zhou et al., 2024; Snyk, 2025). Finally,\nto be applicable, they need to explicitly include support for a specific programming language and\nframework (Wadhams et al., 2024; Zhou et al., 2024; Ami et al., 2024). Indeed, empirical studies of\nstatic analyzers have shown that detection rates vary significantly between vulnerabilities, languages,\nand frameworks, with entire classes of issues remaining completely undetected by static analysis (Li\net al., 2024b; Zhou et al., 2024).\nWith this in mind, we opted for a different approach to evaluate the security of LLM-generated\nsolutions in BAXBENCH-using expert-written security exploits. In contrast to static analyzers, this\napproach (i) provides a sound upper bound for security, (ii) is reproducible, and (iii) is framework-\nagnostic. Further, this approach is in line with recent (Yang et al., 2024b) and concurrent (Peng et al.,\n2025) code security benchmarks. However, as our focus on complete backend applications is more\naccurately modeling industry practices, our exploits have direct real-world security implications.\nFinally, we note that static analysis can reason about all possible execution paths of an applica-\ntion (Cousot & Cousot, 1977), while dynamic testing may leave certain code paths unexplored.\nHowever, in our setting, the security exploits are derived directly from the application specification,\nwhich means they target concrete deployment-time vulnerabilities rather than abstract program states,\nmaking the theoretical completeness guarantees of static analysis less relevant."}, {"title": "Security Exploit Construction Process", "content": "To create the security exploits for each BAXBENCH\nscenario, we start from a manually written set of suspected possible exploits. These are further\nextended and adjusted by inspecting GPT-40-generated (Hurst et al., 2024) and human-written\nsample solutions, both manually and using a SaaS security analyzer, Snyk-Code (Snyk, 2025). Just as\nfor functional tests, the security exploits are framework-agnostic. We distinguish two exploit types: (i)\nblack-box exploits, which only use the API endpoints exposed by the application, e.g., path traversal\nor command code injection attacks, and (ii) white-box-like exploits, where artifacts created by the\napplication are extracted from the execution environment and inspected, e.g., password dictionary\nattacks on databases. In particular, for white-box-like exploits, if a scenario requires a database, we\nspecify the location of an SQLite database instance in the model prompt. Then, after security testing,\nwe perform a full scan of all tables in this database to detect any improperly stored sensitive data. We\nprovide a detailed overview of the security threats covered by BAXBENCH in Table 4 (App. A), and\nan example exploit in App. B."}, {"title": "2.2 CONSTRUCTING AND EVALUATING TASKS", "content": "BAXBENCH tasks are constructed by instructing the im-\nplementation of a given scenario in a target backend\nframework. As the scenarios themselves are framework-\nagnostic, they can be combined with framework of choice.\nThis, for the first time, enables the comprehensive and\nrigorous evaluation of different frameworks' impact on the\ncorrectness and security of LLM-generated code (\u00a74).\nFrameworks\nTo realistically reflect the real-world diver-\nsity of backend applications in terms of implementation\ntools, and to allow for the evaluation of LLMs on their\nproficiency in frameworks with varying training data, we\nselect a diverse mix of popular and more niche frameworks.\nFor this, we orient ourselves by the StackOverflow De-\nveloper Survey (StackOverflow, 2025) and the number of\nGitHub stars of each framework. We provide an overview\nof all frameworks included in BAXBENCH in Table 1.\nEvaluation Pipeline\nEach task in BAXBENCH is a combination of a scenario and a framework.\nThe LLMs are prompted with scenario specifications in OpenAPI format, and with the programming\nlanguage and available packages defined by the framework. Our evaluation prompt templates are\nincluded in App. C. Next, we evaluate the LLM-generated code for correctness and security using the\nabove tests and exploits. In line with other advanced coding benchmarks (Guo et al., 2024; Jimenez\net al., 2024; M\u00fcndler et al., 2024), each test/exploit is executed in a Docker environment. This enables\nthe reproducibility of the results, and ensures that the security exploits on the LLM-generated code\ncannot harm the benchmarking infrastructure."}, {"title": "3 BAXBENCH STATISTICS", "content": "General Statistics\nBAXBENCH contains 28 scenarios specifying a diverse set of realistic backends\nexposing HTTP-based REST API endpoints, described by a language-agnostic OpenAPI specification\nand a natural language description. Across all scenarios, BAXBENCH specifies 54 API endpoints in\ntotal, on average ~2 per scenario, ranging from 1 to maximum 5 endpoints per scenario. Each scenario\nincludes a language-agnostic testing suite, testing each endpoint both for valid and invalid requests\nand responses. As discussed in \u00a72, scenarios also include security exploits, whose statistics we\nprovide in the next paragraph. On average, the OpenAPI specifications are ~420 tokens long, while\nthe plaintext specifications require ~280 tokens on average (using the GPT-40 tokenizer). In \u00a74, we\nuse the number of tokens as a measure of scenario complexity, and show a negative correlation with\nthe models' performance. BAXBENCH supports 14 frameworks across 6 programming languages.\nThe combination of each scenario and framework results in a total of 392 evaluation tasks. We\noverview all frameworks in Table 1 above, and summarize all scenarios in Table 3 in App. \u0410.\nSecurity Coverage\nEach scenario includes a set of security exploits, targeting on average 3.3\nCWEs per scenario, with a maximum of 5 exposed CWEs for one scenario. This extends over existing\nbenchmarks that target only a single CWE per evaluation task (Pearce et al., 2022b; Bhatt et al., 2023;\nHe et al., 2024; Yang et al., 2024b; Peng et al., 2025; Jenko et al., 2024). We note that CWEs can be\nof varying severity levels, and may overlap with or contain other, more fine-grained CWEs. Thus, the\nsheer number of CWEs in a benchmark is an imperfect indicator of its security coverage.\nFor BAXBENCH we order our exploits under 13 distinct CWEs, specifically chosen to be non-\noverlapping and of high severity, as measured by their relevance in well-established vulnerability\nrankings. Namely, among the CWEs covered by BAXBENCH, 9 are part of the MITRE Top 25\nMost Dangerous Software Weaknesses 2024 (MITRE, 2024). Similarly, 10 BAXBENCH CWEs are\nincluded in 4 of the risk groups in OWASP Top 10 Web Application Security Risks 2025 (OWASP,\n2025). An overview of the covered CWEs and their mapping to MITRE Top 25 and OWASP Top 10\nis given in Table 4 in App. A."}, {"title": "4 EVALUATION", "content": "Experimental Setup\nWe test 11 state-of-the-art LLMs on BAXBENCH: OPENAI 01 (Jaech et al.,\n2024), OPENAI 03-MINI (OpenAI, 2025), GPT-40 (Hurst et al., 2024), CLAUDE-3.5 SONNET (An-\nthropic, 2024), DEEPSEEK-R1 (Guo et al., 2025), DEEPSEEK-V3 (Liu et al., 2024a), CODE-\nSTRAL (Mistral AI, 2024), QWEN2.5 CODER (Hui et al., 2024), LLAMA-3.3 70B (Dubey et al.,\n2024), QWEN2.5 72B (Yang et al., 2024a), and QWEN2.5 7B (Yang et al., 2024a)\u20146 providers,\n4 closed-source, and 7 open-source models. For each task, we sample 10 solutions from all non-\nreasoning models at temperature 0.4. For the reasoning models, OPENAI 01, OPENAI 03-MINI,\nand DEEPSEEK-R1, we sample only 1 solution, as they are both cost and time-intensive to evaluate.\nWe use temperature 0 for DEEPSEEK-R1, while for OPENAI 01 and OPENAI 03-MINI, there is no\nmodifiable temperature parameter.\nThe functionality instructions are provided as OpenAPI specifications. We show the advantage of\nthese exact specifications against plaintext descriptions in a separate experiment, justifying our choice.\nFollowing prior work (Chen et al., 2021; Fu et al., 2024), we measure the models' performance\nusing the pass@k and sec_pass@k metrics, with k = 1 in the main paper. These metrics measure the\nratio of correct (all tests passed), and correct and secure (all tests passed and no exploits succeeded)\nprograms across all generated solutions, respectively. We introduce these metrics for generic k in\nApp. D.1, and show experimental results on k = 5."}, {"title": "Main Results", "content": "In Fig. 3, we show each model's mean performance on BAXBENCH. Full red bars\nrepresent sec_pass@1 scores, which are extended in a lighter shade by the passing but incorrect\nprograms of each model to show the pass@1 score. First, we can observe that the benchmark is\nchallenging even in terms of just functional correctness. OPENAI 01, which has achieved impressive\nresults on other coding benchmarks (Jaech et al., 2024), only scores 60% pass@1. Further, a large\nportion of the correct solutions most models generate are insecure, posing a high risk if these\nbackends were to be put into production. Remarkably, the best-performing model in terms of\nfunctional correctness is not the best performer in terms of security. In fact, even three models\noutperform OPENAI 01 in terms of sec_pass@1, OPENAI 03-MINI, CLAUDE-3.5 SONNET, and\nDEEPSEEK-R1, with OPENAI 03-MINI achieving a 6% higher score than OPENAI 01."}, {"title": "Prompting for Security", "content": "Next, we examine the impact of potential security-specific instructions\nin the prompt. For this, we define three different prompts: (i) a prompt without any security re-\nminder, i.e., the prompt used before; (ii) a prompt with a generic security reminder, where the\nmodel is instructed to follow security best practices; and (iii) a prompt with an unrealistic ora-\ncle security reminder, where the developer anticipates all the security vulnerabilities associated\nwith the scenario and gives specific instructions to avoid them. We show our results on a se-\nlect set of top-performing models in Fig. 4. We can see that while the unrealistic oracle-based"}, {"title": "Impact of the Backend Framework", "content": "In Fig. 5, we show the performance of OPENAI 01 across\nframeworks using all prompt types, and include such results on other models in App. D.4. We can\nobserve that the chosen framework has a significant impact on both the correctness and the security\nof the generated backends across all prompt types. This variation is strongly correlated with the\npopularity of the programming language and the complexity of the framework, with models achieving\nhigher performance on frameworks of more popular languages (e.g., Python or JavaScript) and\nstruggling more with lower-resource and complex frameworks, such as Rust-Actix or PHP-Lumen.\nCrucially, in these frameworks, the models do not only struggle to produce functionally correct code,\nbut even the few correct solutions they produce contain a higher share of vulnerabilities. This result\nhighlights that further progress is needed before current LLMs can be applied to security-critical\ncoding tasks requiring the use of specific frameworks."}, {"title": "Differences Across Scenarios", "content": "Next, we investigate the models' performance depending on each\nscenario. We show per-scenario breakdowns of the pass@1 and sec_pass@1 scores of each model\non all prompts in App. D.3. We observe that for certain scenarios, e.g., Logger or Forum, security\nreminders have a decisive impact, steering models that produce a high rate of insecure solutions\ntowards outputting almost only secure solutions. In such cases, the models are primarily failing to"}, {"title": "Added Complexity of Security", "content": "Exploiting\nthe fact that BAXBENCH does not constrain the\ncoding task to narrow, few-line contexts, we in-\nvestigate the added complexity of security con-\nsiderations in the generated solutions. For this,\nwe calculate the ratio of the average number of\ntokens of correct but exploitable solutions and\nthe average number of tokens of correct not-\nexploited solutions. We do this for each model\nand task, skipping tasks where a given model\ndoes not generate at least one of both of these so-\nlution types. Averaging this ratio across all mod-\nels and tasks, we find that security adds 5.9%\ncomplexity in terms of the number of tokens in\nthe generated solutions. This complexity over-\nhead of security is relatively consistent across\nmodels. The only strong outlier is CLAUDE-3.5\nSONNET, which finds secure solutions with a\nsmaller token overhead of only 3.5%."}, {"title": null, "content": "The overhead also varies across frameworks and scenarios. Discarding frameworks where only a few\nsamples could be found, Go-Gin and JavaScript-Express add considerable implementation overhead\nfor secure solutions, with an average increase in token length of around 10%. In contrast, the Python\naiohttp framework adds a mere 0.9%. Certain scenarios also induce high overhead. For instance,\nCalculator (15.1%)\u2014which takes an arithmetic expression from a user as a string and returns the\nresult-can be easily implemented in most languages by evaluating the expression as a program\n(eval(expression)). However, this is highly insecure, as the user could send executable malicious\ncode that the server then evaluates. To avoid this, the server must add sanitization and safety checks\nbefore evaluating the expression, which adds considerable implementation overhead. We show this\neffect in a concrete case study on the Calculator scenario in App. B."}, {"title": "Plaintext Prompt vs. OpenAPI Specification", "content": "To support our choice in using the OpenAPI for-\nmat for specifying BAXBENCH scenarios and\nproviding such precise specifications in the in-\nstructions to the models, we compare the per-\nformances of OPENAI 03-MINI, GPT-40, and\nCLAUDE-3.5 SONNET when prompted with the\nOpenAPI specifications and with their plaintext\ntranscriptions. In Table 2, we show the perfor-"}, {"title": "Additional Results", "content": "In App. D.1, we include extended versions of our main results presented above,\nshowing the pass@5 and sec_pass@5 scores for all non-reasoning models for each of the three prompt\ntypes. In App. D.2, we present detailed results on the occurrence rates of CWEs in our experiments,\nacross frameworks, models, and scenarios."}, {"title": "5 RELATED WORK", "content": "Below, we discuss works related to BAXBENCH.\nBenchmarking Correctness\nResearchers have proposed various benchmarks to evaluate LLMs in\ngenerating functionally correct code. Earlier benchmarks, such as HumanEval (Chen et al., 2021),\nMBPP (Austin et al., 2021b), and APPS (Hendrycks et al., 2021), focus on the task of generating\nshort, algorithmic programming tasks. More recently, several benchmarks have been developed\nto study more nuanced, complex scenarios. These include domain-specific benchmarks, such as\nDS-1000 (Lai et al., 2023) for data science and Sketch2Code (Li et al., 2024a) for web frontends.\nODEX (Wang et al., 2023) and BigCodeBench (Zhuo et al., 2024) offer a more open-domain\nassessment by incorporating different libraries and applications.\nHowever, all these benchmarks focus only on front-end designs or few-line, at most single-function\ntasks, void of a contextualizing application (in contrast to the focus on entire backend applications in\nBAXBENCH), and do not conduct security evaluations. Therefore, BAXBENCH complements these\nbenchmarks and can provide significant value to the community. SWE-Bench (Jimenez et al., 2024)\nand RepoBench (Liu et al., 2024b) focus on generating code edits or snippets given a repository\ncontext. In contrast, BAXBENCH targets complete app generation from scratch."}, {"title": "Benchmarking Security", "content": "While the primary focus of evaluating LLM-based code generation is on\nfunctionality, several benchmarks have been developed to assess security. Notable among these are\nAsleepAtKeyboard (Pearce et al., 2022a), SecurityEval (Siddiq & Santos, 2022), SafeCoder (He et al.,\n2024), CodeLMSec (Hajipour et al., 2024), CyberSecEval (Bhatt et al., 2023), CodeGuard+ (Fu et al.,\n2024), SecCodePLT (Yang et al., 2024b), and CWEval (Peng et al., 2025).\nBAXBENCH stands apart from these benchmarks in three key ways. First, the construction of\nBAXBENCH adopts a top-down approach by starting with real-world end-to-end coding scenarios,\nand then identifying potential CWEs in the generated code, often multiple per scenario. In contrast,\nexisting benchmarks are built with a bottom-up approach that crafts less realistic coding tasks\naround individual CWEs. Second, BAXBENCH is more complex, as it evaluates code generation\ninvolving multiple functions and files, whereas prior benchmarks typically deal with single-function\noutputs. Third, BAXBENCH has a specialized in-depth emphasis on backend applications, where the\nrequirement of secure implementations is self-evident.\nSecure code generation is not the only aspect of LLM evaluation in the context of cybersecurity.\nOther benchmarks focus on evaluating LLMs' cybersecurity capabilities on tasks that are orthogonal\nto ours. RedCode (Guo et al., 2024) studies the generation of code with malicious intent to exploit\nother users, and NYU CTF (Shao et al., 2024) and Cybench (Zhang et al., 2024) evaluate LLMs on\ngenerating security exploits given vulnerable software."}, {"title": "6 CONCLUSION", "content": "In this work, we proposed BAXBENCH, the first code generation benchmark that reflects the next\nfrontier in autonomous coding, targeting standalone backend application development, a domain\nthat is of high practical relevance and challenging both in terms of code functionality and security.\nBAXBENCH combines 28 scenarios and 14 frameworks to produce 392 evaluation tasks. We evaluate\n11 SOTA LLMs on BAXBENCH and find that even flagship LLMs rarely produce correct and secure\ncode. We believe that success in generating secure and correct backends is a minimal requirement for\nLLMs before they can be used to generate production code as such, BAXBENCH promotes progress\ntowards the goal of automated software development by enabling rigorous evaluation."}, {"title": "D ADDITIONAL RESULTS", "content": "We present all additional results omitted from the main paper. In App. D.1 we extend our main\nresults with the pass@5 metric. In App. D.2 we provide a report on the occurrence of CWEs in\nLLM-generated code. In App. D.3 and App. D.4 we provide additional visualizations of the model\nperformances across scenarios and frameworks, respectively."}, {"title": "D.1 PASS @ 5 AND SECPASS @5", "content": "Here, we present pass@5 and sec_pass@5 results on BAXBENCH. First, we introduce this metric:"}, {"title": "The Pass@k Metric", "content": "To measure the overall performance of a given model when k samples are\nallowed to be taken, the standard metric is the pass@k. This metric measures the likelihood that if\nthe model has k tries at solving a given task, it will succeed at least once (i.e., pass all functional\ntests). We use a low-variance unbiased estimator for calculating pass@k across a dataset of tasks, as\nintroduced by Chen et al. (2021):\npass@k : ETasks 1 (1)\nwhere n denotes the number of solutions sampled from the model for a given task and c denotes the\nnumber of correct solutions in those n samples.\nTo measure security exposure, we use the sec_pass@k metric, introduced by Fu et al. (2024). Namely,\nwe reuse Eq. (1), but set c to the count of solutions that both pass all functional tests and are not\ncompromised by any of our security exploits. This reflects real-world usages of generate code-\nsecurity is concerned only if the generated code is functionally correct and will thus be incorporated\ninto the codebase. Our measured sec_pass@k provides a strict upper bound on the true sec_pass@k\nof the model, i.e., the real performance of the models can only be worse than the already low number\nreported in BAXBENCH in \u00a74. This is because, while unlikely, the model generated code could\ncontain vulnerabilities not covered by our exploits."}, {"title": "Results", "content": "We extend our main results in Fig. 3 with the pass@5 (and the corresponding sec_pass@5)\nmetric, showing it alongside the pass@1 and sec_pass@5 metrics for all three prompting types\nin Figs. 7-9. Note that we do not include the reasoning models, OPENAI 03-\u039c\u0399\u039d\u0399, \u039f\u03a1\u0395\u039d\u0391\u0399 01,\nand DEEPSEEK-R1, as due to computational (time and cost) and technical constraints (recurring\nunavailability of the APIs), these models were run only once per task, instead of the usual 10 times.\nThis does not enable the calculation of the pass@5 and sec_pass@5 metrics reliably."}, {"title": "D.2 CWE OCCURRENCE", "content": "Next, we provide a detailed report on the occurrence of CWEs in LLM-generated code. For each\nscenario (Tables 5-7), framework (Tables 8-10), and model (Tables 11-13), we report the ratio of:\n\u2022 the number of model-generated backends that pass all functional tests and have a specific\nCWE, and\n\u2022 the number of model-generated backends that pass all functional tests and could have this\nCWE, per Table 3.\nWe present the results in 9 tables, in which \"/\" indicates that no code for this scenario/framework/\nmodel could have the corresponding CWE, while 0.00 indicates that no backends have it (or very\nfew, as the ratios are rounded to 2 digits)."}, {"title": "D.3 MODEL PERFORMANCE ACROSS SCENARIOS", "content": "In Figs. 10-20, we show the per-scenario breakdown of the pass@1 and sec_pass@1 scores of each\nof the 11 models used in our evaluation, in all three prompt settings."}, {"title": "D.4 MODEL PERFORMANCE ACROSS FRAMEWORKS", "content": "Complementing the OPENAI 01 results in Fig. 5 shown in \u00a74, in Figs. 21-30, we show the per-\nframework breakdown of the pass@1 and sec_pass@1 scores of each of the other 10 models used in\nour evaluation, in all three prompt settings."}, {"title": "20 def sec_test_code_injection", "content": "#JS: \n# PHP: CODE_INJECTION_VECTORS = \n24 def sec_test_code_injection"}, {"title": "17 def calculator", "content": "ast.Add: operator.add,\nast. Sub: operator.sub,,\nast.Mult: operator.mul,\nast.Div: operator.truediv,\nast. USub: operator.neg,,\nvistor visit Bin0p: left = self.visit(node.left)\nvisitor visit_Num,\nvisitor visit unary op, Visitor"}]}