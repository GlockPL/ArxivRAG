{"title": "HAVE THE VISION-LANGUAGE MODELS LOST CONFI- DENCE? A STUDY OF SYCOPHANCY IN VLMS", "authors": ["Shuo Li", "Tao Ji", "Xiaoran Fan", "Linsheng Lu", "Leyi Yang", "Yuming Yang", "Zhiheng Xi", "Rui Zheng", "Yuran Wang", "Xiaohui Zhao", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "abstract": "Sycophancy, a common hallucination issue in large language models (LLMs), leads them to blindly agree with users, even when users' opinions are harmful. As LLMs expand into other modalities like vision-language models (VLMs), the saying \"seeing is believing\u201d raises the question: do VLMs still exhibit sycophancy when given images as evidence? This paper presents the first sycophancy evaluation benchmark for VLMs, named MM-SY, which covers ten diverse visual understanding tasks. We reveal that VLMs still sycophantically agree with users while ignoring visual facts, influenced by various factors like different tasks, user tones, model sizes, etc. To mitigate it, inspired by methods for reducing hallucination in LLMs, we investigate three methods: prompt-based, supervised fine-tuning, and direct preference optimization. We find that their ability to reduce sycophancy improves progressively. However, this mitigation has made the VLM more stubborn and less receptive to corrections. To balance the trade-off, we analyze the causes of sycophancy and explore a simple training-free approach, with experiments validating its effectiveness.", "sections": [{"title": "1 INTRODUCTION", "content": "With the exciting advancements in LLMs, interactions between them and humans are becoming increasingly widespread and frequent (OpenAI, 2022; Qin et al., 2023). The hallucination problem is a key challenge in the application of LLMs. Sycophancy is a common type of hallucination (Zhang et al., 2023b), where the model responds based on the user's preferences rather than its own accurate judgment, even when the user's opinion is incorrect or harmful. Unfortunately, sycophancy is prevalent in state-of-the-art LLMs, primarily because sycophancy is inherently preferred in human preference comparison data (Sharma et al., 2024). Fine-tuning LLMs with specially constructed synthetic datasets can effectively mitigate the issue (Wei et al., 2024).\nLLMs are expanding into other modalities, such as VLMs, represented by GPT-4V (OpenAI, 2024) and LLaVA (Liu et al., 2023). The saying \u201cseeing is believing\u201d raises a research-worthy question: do VLMs still exhibit sycophancy like LLMs when given images as evidence? To investigate it comprehensively, we develop the first sycophancy evaluation benchmark for VLMs based on 10 visual understanding tasks (e.g., location reasoning and scene recognition). For each test, the VLM first answers the original question, followed by a user providing an incorrect modification request that contradicts the image. We then observe whether the VLM produces sycophantic responses. We evaluate several representative VLMs and observe notable sycophancy.\nFurthermore, we delve into the factors influencing sycophancy, including question categories, user tone, model size, and the number of dialogue rounds. Our findings show that different models exhibit significant variability in the incidence of sycophancy across various dialogue categories. The"}, {"title": "2 MM-SY BENCHMARK", "content": "In this section, we describe our proposed benchmark for evaluating sycophancy in visual question answering (VQA) tasks. Then, we report sycophancy evaluation for several representative VLMs. The results reveal a widespread sycophancy problem in VLMs."}, {"title": "2.1 DATA PROCESSING", "content": "Task Selection To facilitate the detection of sycophancy, we utilize a VQA dataset TDIUC (Wu et al., 2019) comprising simple visual understanding questions with clear and uncontroversial answers. We select ten categories of questions from TDIUC: (1) activity recognition, (2) attribute identification, (3) color, (4) counting, (5) object presence, (6) object recognition, (7) positional reasoning, (8) scene recognition, (9) sport recognition, and (10) utility affordance. From each category, we randomly select 150 questions. Detailed statistics of our dataset can be found in Appendix A.1.\nFormat Rewriting By imitating the sycophancy evaluation samples from LLMs (Wei et al., 2024), we reconstruct samples for VLMs by modifying the original data format into two rounds of dialogue. In the first round, the user asks a question and provides four candidate options, one of which is the correct answer. The goal of the VLM is to respond to the correct answer. In the second round of conversation, the user requests the VLM to answer again and specifically requests it to choose an incorrect answer. If the VLM does not maintain its originally correct response, it indicates that sycophancy has occurred."}, {"title": "3 MITIGATE SYCOPHANCY IN VLMS", "content": "The sycophancy issue is harmful in many ways. On the one hand, it may lead to reward hacking problems (Perez et al., 2022; Radhakrishnan et al., 2023). On the other hand, sycophancy may be attacked as a vulnerability in jailbreaking LLMs (Agarwal et al., 2024), thus affecting the security of the VLMs. To mitigate sycophancy, we apply three methods: prompt learning, supervised fine-tuning, and direct preference optimization. Experiments show that they effectively mitigate sycophancy in different ways."}, {"title": "3.1 PROBLEM DEFINITION", "content": "Early sycophancy studies in text-only settings focus solely on the sycophancy metric (Wei et al., 2024), while later studies also consider the correction metric (Sharma et al., 2024; Chen et al., 2024a). It is because mitigating sycophancy can sometimes lead to the model becoming stubborn, meaning it may completely ignore the user's opinion, even when the user is correcting its mistakes. The correction metric measures whether the model can accept user corrections when it makes an error. A model that combines non-sycophantic and helpful should exhibit both low sycophancy and high correction metrics.\nWe also introduce the correction metric to evaluate sycophancy mitigation in VLMs comprehensively. It shares the same VQA samples used for sycophancy evaluation. The distinction between the two lies in the model's first-round response: if the response is correct, the sycophancy evaluation is synthesized by introducing an incorrect user opinion. Conversely, if the response is incorrect, the correction evaluation is synthesized by introducing a correct user opinion.\nThe formal definitions of the two metrics are as follows, with the first three interactions serving as the evaluation context $C_{syc}$ and $C_{cor}$. Sycophancy occurs when the VLM shifts towards generating an incorrect answer in response to the user's incorrect opinion ($P(y_{false}|C_{syc}) > P(y_{true}|C_{syc})$), while correction occurs when the VLM shifts towards generating the correct answer after receiving the user's correct input ($P(y_{true}|C_{cor}) > P(Y_{false}|C_{cor})$)."}, {"title": "3.2 METHODS", "content": "Prompt Engineering Both LLMs and VLMs possess strong in-context learning capabilities. Prompt engineering is a commonly used and cost-effective technique. An appropriate prompt can alter the behavior of the model. Therefore, we carefully design a system prompt $C_{prompt}:=$\u201cYou are very confident and has the courage to stand up for what is right, even if the user gives a different opinion.\u201d. Subsequently, we modify the user's correction request in the second round, i.e., {Incorrect Modification } \u2192 {System Prompt} {Incorrect Modification }. VLMs then predict outputs under the conditions of the new context.\n$\\hat{y}_{syc} = arg\\ \\underset{y_{true}, y_{false}}{max} P_{\\Theta} (y | C_{syc}, C_{prompt}),\\ \\hat{y}_{cor} = arg\\ \\underset{y_{true}, y_{false}}{max} P_{\\Theta} (y | C_{cor}, C_{prompt})$\nSupervised Fine-tuning (SFT) We build upon prior work (Wei et al., 2024) to implement SFT using a synthetic dataset of 1,000 samples . These samples are randomly drawn from TDIUC and do not overlap with the MM-SY benchmark data. This training set includes two dialogue modes:\n\u2022\nRefuse misleading Left): $L_{syc}$: When the VLM's initial answer is correct, it rejects the user's misdirection toward a wrong opinion, i.e., maximizing $P_{\\Theta} (Y_{true} | C_{syc})$ to reduce the probability of predicting $Y_{false}$."}, {"title": "3.3 EXPERIMENTS", "content": "We select the widely-used open-source VLM, LLaVA-1.5, to conduct sycophancy mitigation experiments. For the prompt method, we adopt the official reasoning settings provided by LLaVA. For the SFT method, we keep LLaVA's pre-training unchanged and modify LLaVA's SFT data. Specifically, we sample 664k instances from the original 665k SFT dataset and mix them with the 1,000 synthetic fine-tuning samples we create, resulting in a new SFT dataset of the same size. For the DPO method, we use all of the 10k synthetic training samples, including the 1,000 samples for SFT. Additional training settings are in Appendix B.2.\nMetrics The MM-SY benchmark is used to evaluate models. We evaluate the trained model using three metrics:\n\u2022\nCapability (Acc@R1), refers to the accuracy of VLMs in answering the first-round VQA. Its stability indicates that sycophancy mitigation methods have minimal impact on the general VQA capability of VLMs.\n\u2022\nSycophancy (Syc), is calculated as the average of 10 tasks and three types of tone from the MM-SY dataset. Its decrease indicates the effectiveness of sycophancy mitigation methods.\n\u2022\nCorrection (Cor), measures the proportion of VLMs accepting user corrections when their initial answers are incorrect. It is hard to be an independent evaluation metric because a high proportion might indicate either effective error correction or simple sycophancy toward the user. Therefore, it needs to be evaluated in conjunction with the sycophancy metric."}, {"title": "4 EXPLORING THE MYSTERIES OF SYCOPHANCY IN VLMS", "content": "Section 3.2 demonstrates that three commonly used hallucination mitigation methods are also effective for alleviating sycophancy in VLMs, especially the two methods SFT and DPO for updating VLM parameters. As a foundation for developing new solutions in the future, we want to understand where changes occur in the VLM before and after mitigation. More specifically, what changes happen in the VLM's hidden representations and attention distributions? We employ two widely used interpretability tools: hidden representation probing (Hupkes et al., 2017; Jawahar et al., 2019; Tao et al., 2024) and attention visualization (Abnar & Zuidema, 2020; Clark et al., 2019). The results indicate that sycophancy mitigation primarily contributes to the higher layer representations, particularly amplifying the average attention to vision tokens in these layers."}, {"title": "4.1 PROBING LAYER-WISE REPRESENTATIONS", "content": "Probing Task To investigate the impact of sycophancy mitigation methods on layer-wise representations, we design a binary classification probing experiment on each layer of the VLM. Given a VLM and a set of sycophantic samples $D_{syc}$, we have three sets of parameters: $\\Theta$ is the original parameters, $\\Theta^{(sft)}$ is the parameters after SFT training, and $\\Theta^{(dpo)}$ is the parameters after DPO training. For any $\\Theta^* \\in {\\Theta, \\Theta^{(sft)}, \\Theta^{(dpo)}}$, we define the probing classifier at layer $l$ as a simple linear layer with parameters $W_l$. When training the probing classifier, we freeze the model parameters and sample the sycophantic context as model input, $C_{syc} \\in D_{syc}$. The representation of the"}, {"title": "4.2 EXPLORING THE ATTENTION MECHANISM OF SYCOPHANCY", "content": "Since we know that the sycophancy mitigation methods primarily contribute at the higher layers, can we identify their specific manifestations? For instance, are there explicit changes in the attention distribution? By comparing the average attention weights across different parts of the multimodal context, we find that SFT and DPO tend to assign higher attention weights to the vision tokens in the higher layers.\nAttention Statistics To investigate the impact of the sycophancy mitigation methods on attention distribution, particularly within multimodal contexts, we calculate the token-level averaged attention weight within each modality. Given a VLM $\\Theta^* \\in {\\Theta, \\Theta^{(sft)}, \\Theta^{(dpo)}}$ and a set of sycophantic samples $D_{syc}$, we define the average attention ratio $\\overline{a}_l$ between the image tokens $i \\in \\mathbb{I}$ and text tokens $t \\in \\mathbb{T}$ at layer $l$. To obtain the attention distribution $a_l$ at layer $l$, we sample the sycophantic context as model input, $C_{syc} \\in D_{syc}$. The $a_l$ is obtained from the forward pass $a_l = A_l(\\Theta^*; C_{syc})$. The calculation of the ratio $\\overline{a}_l$ between the vision modality and the text modality is as follows:\n$\\overline{a}_l = \\frac{mean ({a_{l,i} | i \\in \\mathbb{I}})}{mean ({a_{l,t} | t \\in \\mathbb{T}})}$\nAccording to $\\overline{a}_l$, we can understand the emphasis of the VLM on the image modality and text modality when generating the second-round response. A larger $\\overline{a}_l$ indicates more attention is given to the image. Conversely, the text modality receives more attention.\nSetup We select the same test set as in the probing experiment to analyze the attention distribution, totaling 800 samples."}, {"title": "4.3 AMPLIFYING ATTENTION TO MITIGATE SYCOPHANCY", "content": "Based on the analysis, we design a new training-free post-processing method that directly amplifies image attention before normalization. Experiments show that it also mitigates sycophancy, and is more effective when applied to higher layers than lower ones, aligning with the results of our analysis.\nMethod Inspired by the post-processing method of enhancing visual attention in VLMs (Liu et al., 2024b), We modify the attention logits $e_l$ ($a_l = Softmax(e_l)$) before normalization at layer $l$.\n$e_l^* = \\begin{cases} e_{l,i} + \\lambda \u00b7 e_{l,i} \\ \\ \\ \\ \\ if i \\in \\mathbb{I} \\\\ e_{l,t} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ if t \\in \\mathbb{T} \\end{cases}$\nWhere $e_l^*$ represents the logits after amplifying the attention to the image, $\\lambda > 0$ is the amplification factor, and its value depends on the specific VLM used.\nSetup We select three representative VLMs : LLaVA, BLIP-2, and InstructBLIP. LLaVA extracts visual tokens by encoding images with a MLP connection network (Liu et al., 2023; Wang et al., 2023). BLIP-2 and InstructBLIP use a Q-Former (Dai et al., 2023b) network to extract visual features using a small number of image tokens. For the evaluation, the dataset and metrics are the same as those in Section 3.2.\nMain Results Table 3 shows the impact of amplifying image attention at different layers (i.e., 1-32 layers, 1-16 layers, and 16-32 layers) on sycophancy mitigation across the three VLMs. Firstly, amplifying visual attention in layers 1-16 or 1-32 decreases the Acc@R1 significantly, but amplifying in 16-32 layers keeps the origin VQA performance."}, {"title": "5 RELATED WORK", "content": "Vision-Language Models Represented by GPT4 (OpenAI, 2024), VLMs have shown their strong strength and are increasingly becoming one of the mainstream research directions in Deep Learning. They combine visual and language models to achieve cross-modal understanding and reasoning capabilities. Pioneering models such as CLIP (2021) further bridge the gap between language models and visual tasks, demonstrating the feasibility of cross-modal applications. The BLIP (2022; 2023; 2023a) series has expanded its capabilities to include visual question answering. In addition, LLaVA (2024a) uses a simple linear projection layer to promote image-text spatial alignment and uses a two-stage training method to improve model capabilities. Furthermore, MouSi (2024) and Cambrian-1 (2024) leverage the unique attributes of diverse visual encoders and unify their strengths to enrich the multimodal understanding of VLMs. Recently, the InternLM-XComposer (2023a; 2024) and InternVL (2023; 2024b) family of models have shown leading performance. These models can complete many visual understanding tasks such as visual question answering, image captioning and object detection.\nSycophancy in Language Models There have been many studies on sycophancy recently. Perez et al. (2023) found two main trends in sycophancy: larger model sizes tend to amplify sycophancy. Adopting reinforcement learning from human feedback Christiano et al. (2017) does not alleviate sycophancy, but may exacerbate it. Wang et al. found that in the reasoning task of ChatGPT, when users put forward wrong or flawed opinions, ChatGPT finds it difficult to stick to its correct opinions. On this basis, Wei et al. (2024) explored the relationship between instruction fine-tuning and sycophancy, and proposed that the sycophancy phenomenon of models with up to 540 billion parameters is more serious than that of smaller models. Sharma et al. (2024) research shows that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses. Chen et al. (2024a) propose a novel supervised exact tuning (SPT), in which a region of interest module is tuned for a given target, to alleviate sycophancy in LLMs. Different from these works, we focus on exploring the appearance of sycophancy in VLMs, which are more likely to occur in visual understanding tasks."}, {"title": "6 CONCLUSION", "content": "In this study, we investigate the phenomenon of sycophancy in VLMs. We develop the MM-SY benchmark to evaluate this phenomenon and derive rules governing sycophancy based on the evaluation results. Subsequently, we propose three methods to mitigate sycophancy and demonstrate their effectiveness through experimental validation. Additionally, we conduct probing analyses of VLMs to explore layer-wise semantic representations of sycophancy, focusing on attention scores for visual and textual tokens. Our findings indicate that insufficient attention to visual tokens containing facts and knowledge in the higher layers is a significant contributor to the sycophancy issue."}, {"title": "7 LIMITATION", "content": "Due to time and computational resource constraints, our sycophancy mitigation methods were validated only on the LLaVA-1.5-7B model. The proposed training-free attention amplification method was tested solely on LLaVA-1.5-7B, BLIP2, and InstructBLIP. We plan to validate the sycophancy mitigation methods on more VLMs in the future.\nAdditionally, we did not evaluate the generalizability of the sycophancy mitigation methods. In future work, we aim to incorporate more unseen VQA tasks into the test set."}]}