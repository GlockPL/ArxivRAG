{"title": "VERA: Validation and Enhancement for Retrieval Augmented systems", "authors": ["Nitin Aravind Birur", "Tanay Baswa", "Divyanshu Kumar", "Jatan Loya", "Sahil Agarwal", "Prashanth Harshangi"], "abstract": "Large language models (LLMs) exhibit remarkable capabili-\nties but often produce inaccurate responses, as they rely solely\non their embedded knowledge. Retrieval-Augmented Gen-\neration (RAG) enhances LLMs by incorporating an exter-\nnal information retrieval system, supplying additional con-\ntext along with the query to mitigate inaccuracies for a par-\nticular context. However, accuracy issues still remain, as the\nmodel may rely on irrelevant documents or extrapolate in-\ncorrectly from its training knowledge. To assess and improve\nthe performance of both the retrieval system and the LLM\nin a RAG framework, we propose VERA (Validation and\nEnhancement for Retrieval Augmented systems), a system\ndesigned to: 1) Evaluate and enhance the retrieved context\nbefore response generation, and 2) Evaluate and refine the\nLLM-generated response to ensure precision and minimize\nerrors. VERA employs an evaluator-cum-enhancer LLM that\nfirst checks if external retrieval is necessary, evaluates the rel-\nevance and redundancy of the retrieved context, and refines\nit to eliminate non-essential information. Post-response gen-\neration, VERA splits the response into atomic statements,\nassesses their relevance to the query, and ensures adher-\nence to the context. Our experiments demonstrate VERA's\nremarkable efficacy not only in improving the performance\nof smaller open-source models, but also larger state-of-the\\art models. These enhancements underscore VERA's poten-\ntial to produce accurate and relevant responses, advancing\nthe state-of-the-art in retrieval-augmented language model-\ning. VERA's robust methodology, combining multiple eval-\nuation and refinement steps, effectively mitigates hallucina-\ntions and improves retrieval and response processes, making\nit a valuable tool for applications demanding high accuracy\nand reliability in information generation.", "sections": [{"title": "Introduction", "content": "Retrieval-Augmented Generation (RAG) (Lewis et al. 2020)\ntechniques enhance the inputs to Large Language Models\n(LLMs) by incorporating relevant retrieved passages, thus\nreducing factual errors in knowledge-intensive tasks. These\npassages are retrieved using methods such as vector simi-\nlarity search. However, previous research has demonstrated\nthat retrieval-augmented models may generate text that in-\ncludes additional information beyond the retrieved docu-\nments (Dziri et al. 2022), disregards the documents alto-\ngether (Krishna, Roy, and Iyyer 2021), or even contradicts\nthe documents (Longpre et al. 2021). The quality of the\nLLM's response can also be compromised by erroneous or\nirrelevant retrievals (Khandelwal et al. 2019). In reality, re-\ntrievals are not always necessary and are primarily needed\nfor knowledge-intensive tasks. Therefore, there is a critical\nneed to enhance both the quality of retrievals and the quality\nof responses.\nTo quantify and evaluate the quality of retrievals and re-\nsponses, we employ the following metrics:\n\u2022 Response Adherence: This metric measures the extent\nto which the LLM's response is grounded in the provided\ncontext.\n\u2022 Response Relevance: This metric evaluates the amount\nof information in the LLM's response that is relevant to\nand helps in answering the given query.\n\u2022 Context Relevance: This metric assesses the amount of\ninformation in the retrieved context that is pertinent to\nand aids in answering the given query.\nThese metrics allow for a comprehensive evaluation of\nboth the retrieval process and the subsequent response gen-\neration, ensuring improvements in the overall performance\nof RAG systems.\nVERA enhances the Context Relevance of retrieved\nsources prior to their input into the LLM and subsequently\nimproves the Response Adherence and Relevance after the\nLLM generates its response. To achieve this, VERA em-\nploys an evaluator-cum-enhancer LLM that assesses the\ncontent, utilizing reasoning to determine optimal edits,\nwhich are then executed while preserving the original struc-\nture and style of both the context and the response as much\nas possible.\nIn our effort to enhance the performance of RAG systems\nwith any arbitrary retrieval system and LLM, we contribute\nthe following advancements:\n1. Robust and Fine-Grained Evaluation Technique: We\nintroduce a comprehensive evaluation method to assess\nany given retrieval system and LLM using the previously\nmentioned metrics.\n2. System for Context and Response Enhancement: We\npropose a system that leverages the fine-grained eval-\nuation results to analyze and perform appropriate edits\nto the context (before response generation) and the re-\nsponse. The ultimate goal is to produce error-free, rele-\nvant responses using a RAG system.\nMoreover, our method is designed to be easily reproducible,\nallowing seamless integration into any existing RAG system."}, {"title": "Related Works", "content": "RARR\nThe RARR (Gao et al. 2023) framework retroactively en-\nables large language models (LLMs) to attribute external\nevidence through a process termed Editing for Attribution.\nGiven a model-generated text, RARR conducts a research\nstage to locate evidence supporting the text's statements.\nSubsequently, in the revision stage, the framework utilizes\nthis gathered evidence to amend any facts in the original\ntext that lack support, while striving to preserve the initial\ncontent as much as possible. RARR primarily aims to cor-\nrect and attribute model-generated texts within open domain\nscenarios that lack supporting context in the input prompt.\nAlthough this approach can be applied to closed-domain\nretrieval-augmented generation (RAG) pipelines, it does not\nenhance the relevance of the context or answers.\nSELF-RAG\nThe Self-RAG framework, as introduced by (Asai et al.\n2023), represents a pioneering approach in natural language\ngeneration (NLG) by integrating self-reflection mechanisms\ninto the training and generation process of a language model\n(LM). This end-to-end trained LM generates output in seg-\nmented form, guided by specialized reflection tokens de-\nsigned to enhance its performance. Key among these tokens\nis the Retrieve token, which determines whether the model\nshould retrieve multiple documents in parallel to inform its\ngeneration process. If retrieval is activated (Retrieve == yes),\nthe model evaluates the relevance of retrieved documents us-\ning the IsRel token. This token categorizes relevance as ei-\nther \"relevant\" or \"irrelevant,\" thereby assisting the model\nin selecting pertinent information. Subsequently, the IsSup\ntoken assesses the degree to which the generated output is\nsupported by the retrieved documents, while the IsUse to-\nken judges the usefulness of the generated text on a prede-\nfined scale. By iteratively applying these tokens, Self-RAG\naims to improve the quality, relevance, and utility of its gen-\nerated outputs through self-critique and refinement. How-\never, SELF-RAG is not very flexible or versatile as train-\ning a language model is both resource-intensive and time-\nconsuming.\nCRAG\nThe Corrective RAG (CRAG) paper (Yan et al. 2024) intro-\nduces a method to enhance the accuracy of language mod-\nels by reintegrating information from retrieved documents.\nIt employs an evaluator to assess the quality of the doc-\numents obtained for a query and then determines whether\nto use, ignore, or request additional data from these docu-\nments. CRAG also utilizes web searches to expand its infor-\nmation beyond static databases, ensuring access to a broader,\nup-to-date range of information. Additionally, it employs a\nunique strategy to decompose and reconstruct retrieved doc-\numents, emphasizing the extraction of the most relevant in-\nformation while eliminating distractions. Although CRAG\nimproves the quality of retrieval, it does not address inaccu-\nracies and irrelevancies in the final response. While CRAG's\nability to access the web for external information may be\nuseful for general-purpose question answering, most critical\napplications of RAG systems aim to limit the LLM's scope\nof knowledge to the provided documents (e.g., customer ser-\nvice bots).\nFACTScore\nFACTSCORE (Min et al. 2023) introduces a method to eval-\nuate the factual accuracy of language models by decom-\nposing their outputs into atomic facts and verifying each\none against a specified knowledge source. It also presents\na model that approximates FACTSCORE with an error rate\nof less than 2%, enabling the evaluation of a large set of\nnew LMs without requiring manual human effort. VERA\nemploys a similar technique to assess the context adherence\nof responses. However, FACTSCORE is purely an evalua-\ntion technique for testing adherence quality and does not\naddress the quality enhancement of context retrieval or the\nresponses."}, {"title": "Methodology", "content": "We present VERA, a fine-grained evaluator and enhancer for\nretrievers and LLMs within a RAG system. As depicted in\nthe accompanying figure, VERA first evaluates and edits the\nretrieved context to increase its relevance and conciseness in\nrelation to the query. This refined context is then provided to\nthe LLM for response generation. After the response is gen-\nerated, it undergoes further evaluation and editing to ensure\nit is concise and error-free, resulting in the final response.\nAll components of VERA are implemented using few-\nshot prompting. In all our experiments, we employ GPT-40\nas the evaluator-cum-enhancer model due to its state-of-the-\nart capabilities.\nRetrieval Requirement check\nNot all queries necessitate retrieval; only those that are\nknowledge-intensive do. Upon receiving a user prompt,\nVERA determines whether external context is required to\nanswer the prompt or if it can be addressed using the model's\ninternal knowledge. If retrieval is necessary, VERA pro-\nceeds to retrieve the required context. Otherwise, the prompt\nis passed directly to the LLM for response generation."}, {"title": "Retrieval Quality Evaluation and Correction", "content": "After the retriever system retrieves the necessary context,\nVERA evaluates its relevance. VERA then edits the context\nto eliminate any redundant information that would not aid in\nanswering the query without changing any other details or\nstyle.\nLet C be the original context retrieved by the retriever sys-\ntem and C' be the edited context after VERA has eliminated\nredundant information.\nThe retrieval relevance score $R_{retrieval}$ is given by the ratio\nof the length of the edited context $|C'|$ to the length of the\noriginal context $|C|$:\n$R_{retrieval} = \\frac{|C'|}{|C|}$\nIf $R_{retrieval}$ = 0 (i.e., |C'| = 0), it indicates that the re-\ntrieved context fails to provide any useful information, and\nthe process is halted. The user is then informed that their\nquery cannot be answered. If $R_{retrieval}$ > 0, it indicates that\nthere is sufficient information in the context, and this edited\ncontext C' is used to generate the LLM's response to the\nuser query."}, {"title": "Response Relevancy Evaluation and Correction", "content": "To ensure that the generated response contains only informa-\ntion pertinent to answering the query, VERA evaluates and\nedits the response to eliminate any superfluous details. This\nprocess involves splitting the response into atomic state-\nments and assessing the relevance of each statement in ad-\ndressing the query using reasoning. Irrelevant atomic state-\nments are removed from the original response while ensur-\ning that the remaining content is preserved. This meticulous\napproach guarantees that the final response is concise and\nfocused, devoid of any unnecessary information, thereby en-\nhancing the overall quality and accuracy of the answer pro-\nvided.\nLet S = {$s_1, s_2,..., s_n$} be the set of atomic statements\nin the response. Each statement $s_i$ is assigned a binary score\nr($s_i$), where r($s_i$) = 1 if the statement is relevant and\nr($s_i$) = 0 if it is not."}, {"title": "Response Adherence Evaluation and Correction", "content": "As discussed earlier, LLMs in RAG system may gener-\nate text that includes additional information beyond the re-\ntrieved documents (Shuster et al. 2021), disregards the doc-\numents altogether, or even contradicts the documents. This\nwas observed by us even in state-of-the-art LLMs like GPT-\n40. VERA addresses this by splitting the response from the\nprevious step (relevancy correction) into atomic statements\nsimilar to what is proposed in FactScore (Min et al. 2023)\nand then assessing each of them. However, this approach\nof using a binary score to classify each statement as ad-\nherent or non-adherent yielded sub-optimal evaluation ac-\ncuracy, as some statements, while not explicitly present in\nthe context, could be logically inferred and should therefore\nbe classified as adherent. To improve accuracy, we propose\na more nuanced classification system for atomic statements,\nprompting the evaluator to categorize them into three dis-\ntinct classes: (1) directly derivable from the context, (2) not\ndirectly derivable but logically inferable from the context,\nand (3) entirely inaccurate and not grounded in the context.\nThis classification process is guided by chain-of-thought\nreasoning (Wei et al. 2022) to maximize precision. VERA\nthen uses reasoning to make necessary edits by correcting\nany incorrect statements and removing statements which are\nnot grounded in the context.\nThe response adherence score is calculated by assigning\na binary score to each atomic statement. If a statement is\ngrounded in the context or deducible from the context, it is\nassigned a score of 1; otherwise, it receives a score of 0. The\nfinal response adherence score $A_{response}$ is given by:\n$A_{response} = \\frac{1}{n} \\sum_{i=1}^{n} g(s_i)$\nwhere S = {$s_1, s_2,..., s_n$} is the set of atomic state-\nments in the response, g($s_i$) is the binary score for each\nstatement $s_i$ (1 if grounded and accurate, 0 otherwise), and\nn is the total number of atomic statements in the response.\nThis score reflects the proportion of the initial response that\nis accurate and adherent to the context."}, {"title": "Experiments", "content": "Tasks and Datasets\nWe rigorously assess VERA's effectiveness across various\ndatasets and downstream tasks (Kucharavy 2024). Our tests\nare designed to establish a fair baseline and accurately reflect\nreal-world scenarios.\nSQUAD-2.0 Dataset Stanford Question Answering\nDataset (SQUAD) (Rajpurkar et al. 2016) is a reading\ncomprehension dataset, consisting of questions posed by\ncrowdworkers on a set of Wikipedia articles, where the\nanswer to every question is a segment of text, or span, from\nthe corresponding reading passage, or the question might\nbe unanswerable. This dataset is challenging as there are\nquestions that might not be answerable from the provided\ncontext.\nDROP Dataset The DROP dataset (Dua et al. 2019)\nserves as a reading comprehension benchmark designed for\nDiscrete Reasoning Over Paragraphs. Comprising 96,000\nquestions, this dataset was adversarially crowd-sourced to\nchallenge systems in a variety of tasks. To successfully\nnavigate DROP, a system must interpret references within\na question potentially across multiple parts of the in-\nput and carry out discrete operations such as addition,\ncounting, or sorting. These tasks demand a thorough under-\nstanding of the paragraph's content.\nReal World Downstream Tasks To evaluate the effective-\nness of VERA on real-world downstream tasks, we com-\npiled a set of three documents representing diverse use cases\nof a RAG based LLM. These documents include:\n1. World War II Wikipedia Page: The Wikipedia article\non World War II presents a challenging evaluation, test-\ning the model's capacity to adhere to the provided context\nwithout deviating due to its pre-existing knowledge from\nprior training.\n2. Apple 10-K Report: The 2023 fiscal year Form 10-K for\nApple was chosen to assess the RAG system's ability to\nhandle numerical and financial data (Setty et al. 2024),\nreflecting a common application of RAG models in pro-\ncessing and interpreting financial documents.\nBaselines\nWe assess publicly available pre-trained language models\nsuch as Mistral-7B-instruct-v0.1 (Jiang et al. 2023), GPT-\n3.5-turbo (Brown 2020), and GPT-40 (OpenAI et al. 2024)\nto demonstrate VERA's effectiveness across different model\nsizes. Mistral-7B-instruct-v0.1 represents a smaller model,\nwhile GPT-40 exemplifies a state-of-the-art model. Addi-\ntionally, we compare these with the 7B Self-Rag (Asai et al.\n2023) (Touvron et al. 2023) model available on Hugging-\nFace.\nFor downstream tasks, we utilize FAISS (Douze et al.\n2024) as a vector store and use similarity search retrieval,\nsetting the chunk size to 512 tokens and chunk overlap to 25\ntokens. To ensure consistency, GPT-40 is used as the evalu-\nator model for VERA in all tests. The answers generated by\nVERA are further evaluated using GPT-40 to obtain post-\nenhancement scores. The questions to create a QA dataset\nfrom the given documents were created using the ragas li-\nbrary (Es et al. 2023). There was an equal proportion of\nquestions testing reasoning abilities and questions that re-\nquired multiple contexts to answer.\nThe SQUAD-2.0 and DROP datasets do not require a re-\ntriever system, as they provide the context directly within\nthe dataset itself."}, {"title": "Results", "content": "We observed a substantial improvement in accuracy for both\nthe SQUAD2.0 and DROP datasets (Table 1) when employ-\ning VERA. Specifically, Mistral-7B-instruct-v0.1 exhibited\na 20% increase in accuracy on the SQuAD2.0 dataset and\na 15% increase on the DROP dataset. Additionally, VERA\nenhanced the performance of GPT-40 by 5% on SQUAD2.0\nand 10% on DROP. These results underscore VERA's ef-\nfectiveness in enhancing the performance of large language\nmodels on tasks that demand advanced comprehension ca-\npabilities.\nThe results of downstream tasks demonstrated a sig-\nnificant increase in adherence and relevance scores for\nsmaller models like Mistral-7B-instruct-v0.1. Notable im-\nprovements were also observed in larger models such\nas GPT-40 and GPT-3.5-turbo. Specifically, Mistral-7B-\ninstruct-v0.1 exhibited an increase in Response Adherence\nby up to 18.7% (Table 2) and an increase in Response Rele-\nvance by up to 17.9% (Table 2) when using VERA.\nThe improvements in Response Adherence and Relevance\nfor GPT-40 indicate that VERA can be effectively used\nfor self-improvement (Huang et al. 2022), as the evaluator\nmodel employed was also GPT-40. This finding is signifi-\ncant because it demonstrates that VERA's performance en-\nhancements are not solely attributable to the use of GPT-40\nbut rather to the systematic evaluation and refinement pro-\ncesses implemented by VERA.\nIn all the downstream tasks, the initial Context Relevance\nwas below 0.45. This can be attributed to the larger chunk\nsize of 512 tokens (Eibich, Nagpal, and Fred-Ojala 2024),\nof which only approximately 30% to 45% of the informa-\ntion was relevant to the context. While Context Relevance is\nnot directly dependent on the LLM used, we still observed\nvariations in the scores due to the stochastic nature of the\nLLM serving as the evaluator (Sun et al. 2024). Despite this\ninherent variability, the use of VERA led to a clear and con-\nsistent increase in Context Relevance across all experiments."}, {"title": "Conclusion", "content": "In this work, we presented VERA, a novel system designed\nto address the limitations of Retrieval-Augmented Genera-\ntion (RAG) in enhancing Large Language Models (LLMs).\nBy incorporating an evaluator-cum-enhancer LLM, VERA\nsignificantly improves the relevance, adherence, and over-\nall quality of responses. Our approach involves a multi-step\nprocess that determines the necessity of retrieval, evaluates\nand refines retrieved documents, and rigorously assesses and\ncorrects the generated responses\nVERA's method of breaking down responses into atomic\nfacts and ensuring each statement's grounding in the re-\ntrieved context leads to higher fidelity and relevance in\nthe final outputs. Our experimental results demonstrate that\nVERA increases adherence and relevance significantly for\nboth smaller LLMs like Mistral 7B instruct v0.1 and larger\nmodels like GPT-40, showcasing its versatility and effective-\nness across different model scales.\nThe improvements brought by VERA highlight its poten-\ntial in applications where accurate and reliable information\ngeneration is crucial. By mitigating hallucinations and refin-\ning the retrieval and response process, VERA paves the way\nfor more trustworthy and contextually appropriate LLM out-\nputs, advancing the state-of-the-art in retrieval-augmented\nlanguage modeling."}, {"title": "Limitations and Future Work", "content": "VERA demonstrates strong capabilities in understanding se-\nmantic changes between the response and context, avoid-\ning unnecessary penalties for semantically equivalent state-\nments (e.g., \"World War II is a deeply engraved event in\nhistory\" and \"World War II is an important event in his-\ntory\"). However, during our experimentation, we observed\nthat smaller models like Mistral-7B-instruct or Llama3 8B,\nwhen used as evaluators instead of GPT-40, struggled to\nhandle such semantic nuances effectively. This limitation\ncould potentially be addressed by improving the few-shot\nprompting technique, thereby enhancing evaluation perfor-\nmance with smaller models and making the method more\ncost-efficient.\nDue to the stochastic nature of the evaluator LLM, the\nsplitting of the response into atomic statements may vary\nslightly with each evaluation, resulting in minor differences\nin scores. Although this limitation is largely mitigated by\nusing a large dataset in our experiments, it can still cause\nminor variations in scores for individual evaluations.\nSince VERA necessitates LLM evaluation at each step of\nthe process, it might not be suitable for real-time applica-\ntions. This limitation could potentially be addressed in the\nfuture by combining multiple evaluation calls into a single\nstep, thereby making the process more streamlined and time-\nefficient."}]}