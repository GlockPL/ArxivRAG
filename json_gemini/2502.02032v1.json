{"title": "Heteroscedastic Double Bayesian Elastic Net", "authors": ["Masanari Kimura"], "abstract": "In many practical applications, regression models are employed to uncover relationships between predictors and a response variable, yet the common assumption of constant error variance is frequently violated. This issue is further compounded in high-dimensional settings where the number of predictors exceeds the sample size, necessitating regularization for effective estimation and variable selection. To address this problem, we propose the Heteroscedastic Double Bayesian Elastic Net (HDBEN), a novel framework that jointly models the mean and log-variance using hierarchical Bayesian priors incorporating both $l_1$ and $l_2$ penalties. Our approach simultaneously induces sparsity and grouping in the regression coefficients and variance parameters, capturing complex variance structures in the data. Theoretical results demonstrate that proposed HDBEN achieves posterior concentration, variable selection consistency, and asymptotic normality under mild conditions which justifying its behavior. Simulation studies further illustrate that HDBEN outperforms existing methods, particularly in scenarios characterized by heteroscedasticity and high dimensionality.", "sections": [{"title": "1 Introduction", "content": "In many real-world applications, regression models are employed to understand the relationship between a response variable and a set of predictors. Traditional linear regression models typically assume homoscedasticity, where the variance of the error terms is constant across all observations. However, this assumption often fails in practice, leading to inefficient estimates and unreliable inference. In fields such as finance and environmental science, ignoring heteroscedasticity can lead to misleading conclusions about risk and uncertainty, making it imperative to model the variance explicitly alongside the mean. Heteroscedasticity, where the variance of the errors varies with the predictors or the observations, is prevalent in fields such as finance, economics, biology, and engineering (Rosopa et al., 2013; M\u00fcller and Stadtm\u00fcller, 1987; Long and Ervin, 2000; Koenker, 1981).\nIn high-dimensional settings, where the number of predictors d is large relative to the sample size n, regularization techniques become essential to prevent overfitting and to perform variable selection. High-dimensional datasets often exhibit complex correlation structures and noise patterns, which can render traditional methods ineffective or unstable. The Elastic Net (Zou and Hastie, 2005) is a widely adopted regularization method that combines the strengths of both $l_1$ (Lasso) and $l_2$ (Ridge) penalties, promoting sparsity and grouping of correlated predictors. Bayesian approaches to regularization, such as the Bayesian Elastic Net (Li and Lin, 2010), incorporate prior distributions to achieve similar objectives within a probabilistic framework, allowing for uncertainty quantification and probabilistic inference. A Bayesian formulation not only allows for regularization via prior distributions but also provides a natural mechanism for uncertainty quantification, which is especially valuable when both the mean and variance are of interest.\nDespite the advancements in handling high-dimensional data and heteroscedasticity separately, very few approaches have been developed that integrate these two aspects within a coherent Bayesian framework. Moreover, many real-world phenomena exhibit an intricate interplay between the mean behavior and the variability of outcomes, suggesting that a joint modeling approach can capture underlying structures that separate analyses may miss. To bridge this gap, we propose the Heteroscedastic Double Bayesian Elastic Net (HDBEN), a novel Bayesian regression framework that simultaneously models the mean and variance with Elastic Net priors. By extending the Bayesian Elastic Net to accommodate heteroscedasticity, HDBEN enables effective variable selection and coefficient estimation in both the mean and variance models. In addition, in fields such as finance and environmental science, where uncertainty quantification is critical, modeling both components jointly provides a more complete picture of the underlying processes."}, {"title": "The contributions of this study are threefold:", "content": "1. Model Development: We introduce HDBEN, which jointly regularizes the mean and log-variance parameters using hierarchical Bayesian Elastic Net priors, facilitating simultaneous variable selection and variance modeling.\n2. Theoretical Guarantees: We establish the theoretical properties of the proposed model, including posterior concentration rates and variable selection consistency under high-dimensional settings. These results provide a rigorous foundation for the efficacy of HDBEN in capturing both mean and variance structures.\n3. Empirical Validation: Through extensive simulations and real-world data applications, we demonstrate the superior performance of HDBEN in handling heteroscedasticity and promoting sparsity compared to existing methods.\nThe remainder of this paper is organized as follows. Section 2 provides the necessary background on regression analysis, heteroscedasticity, Elastic Net regularization, and Bayesian regularization techniques. Section 3 details the specification of the HDBEN model and the posterior inference methodology. In Section 4, we present the theoretical underpinnings of the model, including key assumptions and theorems supporting posterior concentration and variable selection consistency. In Section 5, we provide numerical experimental results to demonstrate the effectiveness of the proposed method. Finally, Section 6 offers concluding remarks and discusses potential extensions of the proposed framework."}, {"title": "2 Preliminary", "content": "In regression analysis, heteroscedasticity refers to the scenario where the variance of the error terms is not constant across observations. Consider the linear regression model:\n$y_i = X_i^T\\beta + \\epsilon_i,$\n(1)\nfor i = 1,...,n, where $y_i$ is the response variable, $X_i \\in R^d$ are covariates, $\\beta$ are regression coefficients, and $\\epsilon_i$ is the error term. Under homoscedasticity, the errors satisfy\n$E[\\epsilon_i] = 0, \\quad Var(\\epsilon_i) = \\sigma^2, \\quad (constant \\: for \\: all \\: i),$\nand under heteroscedasticity, the variance depends on i. Specifically, we assume that each $\\sigma^2$ can be written as a function of covariates: $X_i \\rightarrow \\sigma^2(X_i)$. In this study, we suppose that\n$\\sigma^2(X_i) := exp(X_i^T \\gamma),$\nfor $\\gamma \\in R^d$ the coefficient vector of variance. Figure 1 shows the illustrative example for Eq. (2). We can see that the coefficient $\\gamma$ becomes large, resulting in the variance becomes also large."}, {"title": "2.1 Elastic Net Regularization", "content": "Regularization techniques are employed in regression analysis to prevent overfitting, especially in high-dimensional settings where the number of covariates d may be large relative to the sample size n. The Elastic Net is a popular regularization method that combines the properties of both $l_1$ (Lasso) and $l_2$ (Ridge) penalties. The Elastic Net estimator is defined as the solution to the following optimization problem:\n$\\hat{\\beta} = arg \\min_{\\beta \\in R^d} \\{ \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - X_i^T \\beta)^2 + \\lambda_1 ||\\beta||_1 + \\lambda_2 ||\\beta||_2^2 \\},$\n(2)\nwhere:\n\u2022 $\\lambda_1 \\geq 0$ controls the strength of the $l_1$ penalty, promoting sparsity in the estimated coefficients.\n\u2022 $\\lambda_2 \\geq 0$ controls the strength of the $l_2$ penalty, encouraging grouping of correlated predictors and stabilizing the estimates.\nThe Elastic Net effectively handles situations where predictors are highly correlated, mitigating the limitations of Lasso in such scenarios by selecting groups of correlated variables together."}, {"title": "2.2 Bayesian Regularization", "content": "Bayesian approaches to regularization incorporate prior beliefs about the parameters into the estimation process. In the context of regression, priors on $\\beta$ can induce sparsity and shrinkage, similar to regularization penalties in frequentist methods. The Bayesian Elastic Net combines the benefits of Bayesian inference with the Elastic Net regularization by specifying appropriate priors for the regression coefficients."}, {"title": "2.1 Bayesian Elastic Net Prior", "content": "The Bayesian Elastic Net prior for the regression coefficients $\\beta$ is defined hierarchically to capture both $l_1$ and $l_2$ regularization effects:\n$\\beta \\: | \\: \\tau_{\\beta}, \\lambda_{2,\\beta} \\sim N(0, \\Sigma_{\\beta}),$\n(3)\n$\\Sigma_{\\beta} = (D_{\\beta}^{-1} + \\lambda_{2,\\beta}I_d)^{-1},$\n(4)\n$D_{\\beta} = diag(\\tau_{\\beta,1}, ..., \\tau_{\\beta,d}),$\n(5)\n$\\tau_{\\beta,j} \\: | \\: \\lambda_{1,\\beta} \\sim Exp(\\frac{\\lambda_{1,\\beta}}{2}), \\quad j=1,...,d.$\n(6)\nHere,\n\u2022 $\\lambda_{1,\\beta}$ and $\\lambda_{2,\\beta}$ are hyperparameters controlling the $l_1$ and $l_2$ penalties, respectively.\n\u2022 $\\tau_{\\beta,j}$ are auxiliary variables that introduce sparsity through the exponential (Lasso-like) prior.\n\u2022 The hierarchical structure allows for adaptive shrinkage of each $\\beta_j$, balancing sparsity and coefficient shrinkage."}, {"title": "2.3 Notation and Definitions", "content": "To facilitate a clear and consistent presentation of the proposed methodology and theoretical results, we establish the following notation:\n\u2022 $y = (y_1,..., y_n)^T \\in R^n$: Vector of response variables.\n\u2022 $X = [X_1, ..., X_n]^T \\in R^{n \\times d}$: Design matrix of covariates.\n\u2022 $\\beta \\in R^d$: Vector of regression coefficients for the mean model.\n\u2022 $\\gamma \\in R^d$: Vector of coefficients for the log-variance model.\n\u2022 $\\tau_{\\beta} = (\\tau_{\\beta,1}, ..., \\tau_{\\beta,d})^T \\in R^d$: Vector of auxiliary variables for $\\beta$.\n\u2022 $\\tau_{\\gamma} = (\\tau_{\\gamma,1}, ..., \\tau_{\\gamma,d})^T \\in R^d$: Vector of auxiliary variables for $\\gamma$.\n\u2022 $\\lambda_{1,\\beta}, \\lambda_{1,\\gamma}$: Hyperparameters controlling the $l_1$ penalty for $\\beta$ and $\\gamma$, respectively.\n\u2022 $\\lambda_{2,\\beta}, \\lambda_{2,\\gamma}$: Hyperparameters controlling the $l_2$ penalty for $\\beta$ and $\\gamma$, respectively.\n\u2022 $\\Sigma_{\\beta}, \\Sigma_{\\gamma}$: Covariance matrices for $\\beta$ and $\\gamma$, respectively."}, {"title": "2.4 Existing Approaches and Limitations", "content": "Traditional regression models often assume homoscedasticity, which may not hold in practice. Ignoring heteroscedasticity can lead to inefficient estimates and biased inference. While heteroscedasticity-consistent standard errors (e.g., White's standard errors (White, 1980)) address inference issues, they do not model the varying variance structure explicitly (Arellano, 1987; Long and Ervin, 2000; Croux et al., 2004).\nRegularization methods like the Lasso (Tibshirani, 1996; Ranstam and Cook, 2018) and Elastic Net (Zou and Hastie, 2005) have been widely used for variable selection and coefficient shrinkage in high-dimensional settings. However, these methods typically focus on the mean model and do not account for heteroscedasticity. Bayesian regularization techniques extend these methods by incorporating prior distributions that promote sparsity and shrinkage (Park and Casella, 2008; Hans, 2009; Mallick and Yi, 2014). The Bayesian Elastic Net (Li and Lin, 2010), for instance, provides a probabilistic framework for the Elastic Net regularization, primarily addressing the mean model, leaving the variance model unaccounted for."}, {"title": "2.5 Motivation for Heteroscedastic Double Bayesian Elastic Net", "content": "The interplay between the mean and variance structures in regression models is critical, especially in high-dimensional settings where both the response and the variability of the response are influenced by numerous predictors. Modeling heteroscedasticity jointly with the mean allows for more accurate uncertainty quantification and improves the robustness of the model. The proposed Heteroscedastic Double Bayesian Elastic Net addresses this by simultaneously regularizing both the mean and log-variance parameters using Elastic Net priors. This dual regularization promotes sparsity in both the mean and variance models, effectively handling high-dimensional data with complex variance structures."}, {"title": "3 Methodology", "content": "In this section, we introduce our proposed method to handle heteroscedastic errors. Our goal is to model $y_i$ with a heteroscedastic Gaussian likelihood where both the mean and variance depend on the same covariates $X_i$."}, {"title": "3.1 Model Specification", "content": "For each observation i = 1, . . ., n, the likelihood is\n$p(y \\: | \\: X, \\beta, \\gamma) = \\prod_{i=1}^{n} N(y_i \\: | \\: X_i^T\\beta, exp(X_i^T \\gamma))$\n(7)\n$= \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi exp(X_i^T \\gamma)}} exp\\{ - \\frac{(y_i - X_i^T\\beta)^2}{2 exp(X_i^T \\gamma)}\\}.$\n(8)\nHere, priors are specified as\n$\\pi(\\beta \\: | \\: \\tau_{\\beta}, \\lambda_{2,\\beta}) = N(0, \\Sigma_{\\beta}) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma_{\\beta}|^{1/2}} exp\\{ - \\frac{1}{2} \\beta^T \\Sigma_{\\beta}^{-1} \\beta \\},$\n$\\Sigma_{\\beta}^{-1} = (D_{\\beta}^{-1} + \\lambda_{2,\\beta}I_d)^{-1}, \\quad D_{\\beta} = diag(\\tau_{\\beta,1}, ..., \\tau_{\\beta,d}),$\n$\\pi(\\tau_{\\beta,j} \\: | \\: \\lambda_{1,\\beta}) = Exp(\\frac{\\lambda_{1,\\beta}}{2}) = \\frac{\\lambda_{1,\\beta}}{2} exp\\{ - \\frac{\\lambda_{1,\\beta}}{2} \\tau_{\\beta,j} \\}, \\quad \\tau_{\\beta,j} > 0, \\: j=1,...,d,$\n$\\pi(\\gamma \\: | \\: \\tau_{\\gamma}, \\lambda_{2,\\gamma}) = N(0, \\Sigma_{\\gamma}) = \\frac{1}{(2\\pi)^{d/2}|\\Sigma_{\\gamma}|^{1/2}} exp\\{ - \\frac{1}{2} \\gamma^T \\Sigma_{\\gamma}^{-1} \\gamma \\},$\n$\\Sigma_{\\gamma}^{-1} = (D_{\\gamma}^{-1} + \\lambda_{2,\\gamma}I_d)^{-1}, \\quad D_{\\gamma} = diag(\\tau_{\\gamma,1}, ..., \\tau_{\\gamma,d}),$\n$\\pi(\\tau_{\\gamma,j} \\: | \\: \\lambda_{1,\\gamma}) = Exp(\\frac{\\lambda_{1,\\gamma}}{2}) = \\frac{\\lambda_{1,\\gamma}}{2} exp\\{ - \\frac{\\lambda_{1,\\gamma}}{2} \\tau_{\\gamma,j} \\}, \\quad \\tau_{\\gamma,j} > 0, \\: j=1,...,d,$\n$\\pi(\\lambda_{1,\\beta}) = Gamma(a_{\\beta,1}, b_{\\beta,1}) = \\frac{b_{\\beta,1}^{a_{\\beta,1}}}{\\Gamma(a_{\\beta,1})} \\lambda_{1,\\beta}^{a_{\\beta,1}-1} e^{-b_{\\beta,1}\\lambda_{1,\\beta}}, \\quad \\lambda_{1,\\beta} > 0,$\n$\\pi(\\lambda_{1,\\gamma}) = Gamma(a_{\\gamma,1}, b_{\\gamma,1}) = \\frac{b_{\\gamma,1}^{a_{\\gamma,1}}}{\\Gamma(a_{\\gamma,1})} \\lambda_{1,\\gamma}^{a_{\\gamma,1}-1} e^{-b_{\\gamma,1}\\lambda_{1,\\gamma}}, \\quad \\lambda_{1,\\gamma} > 0,$\n$\\pi(\\lambda_{2,\\beta}) = Gamma(a_{\\beta,2}, b_{\\beta,2}) = \\frac{b_{\\beta,2}^{a_{\\beta,2}}}{\\Gamma(a_{\\beta,2})} \\lambda_{2,\\beta}^{a_{\\beta,2}-1} e^{-b_{\\beta,2}\\lambda_{2,\\beta}}, \\quad \\lambda_{2,\\beta} > 0,$\n$\\pi(\\lambda_{2,\\gamma}) = Gamma(a_{\\gamma,2}, b_{\\gamma,2}) = \\frac{b_{\\gamma,2}^{a_{\\gamma,2}}}{\\Gamma(a_{\\gamma,2})} \\lambda_{2,\\gamma}^{a_{\\gamma,2}-1} e^{-b_{\\gamma,2}\\lambda_{2,\\gamma}}, \\quad \\lambda_{2,\\gamma} > 0.$\nThe full joint posterior is\n$p(\\beta, \\gamma, \\tau_{\\beta}, \\tau_{\\gamma}, \\lambda_{1,\\beta}, \\lambda_{1,\\gamma}, \\lambda_{2,\\beta}, \\lambda_{2,\\gamma} \\: | \\: y, X) \\\\\n\\propto p(y \\: | \\: X, \\beta, \\gamma) \\pi(\\beta \\: | \\: \\tau_{\\beta}, \\lambda_{2,\\beta}) \\pi(\\tau_{\\beta,j} \\: | \\: \\lambda_{1,\\beta}) \\pi(\\gamma \\: | \\: \\tau_{\\gamma}, \\lambda_{2,\\gamma}) \\pi(\\tau_{\\gamma,j} \\: | \\: \\lambda_{1,\\gamma}) \\\\\n\\times \\pi(\\lambda_{1,\\beta}) \\pi(\\lambda_{1,\\gamma}) \\pi(\\lambda_{2,\\beta}) \\pi(\\lambda_{2,\\gamma}) \\\\\n= \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi exp(X_i^T \\gamma)}} exp\\{ - \\frac{(y_i - X_i^T\\beta)^2}{2 exp(X_i^T \\gamma)}\\} \\\\\n\\times \\frac{1}{(2\\pi)^{d/2}|\\Sigma_{\\beta}|^{1/2}} exp\\{ - \\frac{1}{2} \\beta^T \\Sigma_{\\beta}^{-1} \\beta \\} \\frac{1}{(2\\pi)^{d/2}|\\Sigma_{\\gamma}|^{1/2}} exp\\{ - \\frac{1}{2} \\gamma^T \\Sigma_{\\gamma}^{-1} \\gamma \\} \\\\\n\\times \\prod_{j=1}^{d} \\frac{\\lambda_{1,\\beta}}{2} exp\\{ - \\frac{\\lambda_{1,\\beta}}{2} \\tau_{\\beta,j} \\} \\prod_{j=1}^{d} \\frac{\\lambda_{1,\\gamma}}{2} exp\\{ - \\frac{\\lambda_{1,\\gamma}}{2} \\tau_{\\gamma,j} \\} \\\\\n\\times \\frac{b_{\\beta,1}^{a_{\\beta,1}}}{\\Gamma(a_{\\beta,1})} \\lambda_{1,\\beta}^{a_{\\beta,1}-1} e^{-b_{\\beta,1}\\lambda_{1,\\beta}} \\frac{b_{\\gamma,1}^{a_{\\gamma,1}}}{\\Gamma(a_{\\gamma,1})} \\lambda_{1,\\gamma}^{a_{\\gamma,1}-1} e^{-b_{\\gamma,1}\\lambda_{1,\\gamma}} \\\\\n\\times \\frac{b_{\\beta,2}^{a_{\\beta,2}}}{\\Gamma(a_{\\beta,2})} \\lambda_{2,\\beta}^{a_{\\beta,2}-1} e^{-b_{\\beta,2}\\lambda_{2,\\beta}} \\frac{b_{\\gamma,2}^{a_{\\gamma,2}}}{\\Gamma(a_{\\gamma,2})} \\lambda_{2,\\gamma}^{a_{\\gamma,2}-1} e^{-b_{\\gamma,2}\\lambda_{2,\\gamma}}.\\qquad\\qquad (9)$\nThe key properties of the above modeling are as follows.\n\u2022 Both $\\beta$ and $\\gamma$ follow the elastic net priors, combining L1 (sparsity) and L2 (shrinkage) regularization.\n\u2022 Scale parameters $\\tau_{\\beta,j}, \\tau_{\\gamma,j}$ link Laplace (L1) and Gaussian (L2) penalties.\n\u2022 The joint posterior enables simultaneous estimation of mean, variance, and regularization parameters.\nThis expanded specification provides a complete mathematical foundation for inference via MCMC or variational methods."}, {"title": "3.2 Posterior Inference", "content": "Given the complexity of the joint posterior distribution, analytical solutions for posterior summaries are intractable. Therefore, we employ Markov Chain Monte Carlo (MCMC) methods\u2014a Gibbs sampler with an embedded Metropolis-Hastings (MH) step\u2014to approximate the posterior distributions of the parameters (Geyer, 1992, 2011; Brooks, 1998; Chib and Greenberg, 1995).\n1. Sampling $\\beta$: The full conditional distribution of $\\beta$ is conjugate and given by\n$\\beta \\: | \\: \\cdot \\sim N(\\mu_{\\beta}, \\Sigma_{\\beta}^*),$\n$\\Sigma_{\\beta}^* = (X^TWX + \\Sigma_{\\beta}^{-1})^{-1}, \\quad with \\: W = diag(w_1,..., w_n), \\quad w_i = \\frac{1}{exp(X_i^T \\gamma)},$\n$\\mu_{\\beta} = \\Sigma_{\\beta}^* X^TW y.$\n2. Sampling $\\gamma$: Because the likelihood involves $exp(X_i^T \\gamma)$ in a non-conjugate manner, the full conditional for $\\gamma$ is not available in closed form. We update $\\gamma$ via a Metropolis\u2013Hastings step:\n(a) Propose a new value $\\gamma^*$ from a proposal distribution, e.g.,\n$\\gamma^* = \\gamma^{(t-1)} + \\eta, \\quad \\eta \\sim N(0, \\sigma_{\\eta}^2 I),$\nwhere $\\sigma_{\\eta}^2$ is a tuning parameter.\n(b) Compute the acceptance probability\n$\\alpha = min \\{1, \\frac{p(y \\: | \\: X, \\beta, \\gamma^*) \\pi(\\gamma^* \\: | \\: \\tau_{\\gamma}, \\lambda_{2,\\gamma})}{p(y \\: | \\: X, \\beta, \\gamma^{(t-1)}) \\pi(\\gamma^{(t-1)} \\: | \\: \\tau_{\\gamma}, \\lambda_{2,\\gamma})} \\}.$\n(c) With probability $\\alpha$, set $\\gamma^{(t)} = \\gamma^*$; otherwise, set $\\gamma^{(t)} = \\gamma^{(t-1)}$.\n3. Sampling $\\tau_{\\beta}$ and $\\tau_{\\gamma}$: In the standard representation of the Laplace (or Elastic Net) prior as a scale mixture of normals with an exponential mixing distribution, the full conditionals for the latent scale parameters are inverse Gaussian. Specifically, for each j = 1, . . ., d,\n$\\tau_{\\beta,j} \\: | \\: \\beta_j, \\lambda_{1,\\beta} \\sim InverseGaussian(\\sqrt{\\frac{\\lambda_{1,\\beta}}{\\beta_j^2}}, \\lambda_{1,\\beta}),$\n(10)\n$\\tau_{\\gamma,j} \\: | \\: \\gamma_j, \\lambda_{1,\\gamma} \\sim InverseGaussian(\\sqrt{\\frac{\\lambda_{1,\\gamma}}{\\gamma_j^2}}, \\lambda_{1,\\gamma}).$\n(11)\nThe inverse Gaussian density is given by\n$p(\\tau \\: | \\: \\mu, \\lambda) = (\\frac{\\lambda}{2\\pi \\tau^3})^{1/2} exp \\{ - \\frac{\\lambda(\\tau - \\mu)^2}{2\\mu^2 \\tau} \\}.$\n4. Sampling $\\lambda_{1,\\beta}^2$ and $\\lambda_{1,\\gamma}^2$: The hyperpriors on $\\lambda_{1,\\beta}$ and $\\lambda_{1,\\gamma}$ are assumed to be Gamma. Their full conditionals are given by\n$\\lambda_{1,\\beta} \\: | \\: \\tau_{\\beta} \\sim Gamma(a_{\\beta,1} + d, b_{\\beta,1} + \\frac{1}{2} \\sum_{j=1}^{d} \\tau_{\\beta,j}),$\n(12)\n$\\lambda_{1,\\gamma} \\: | \\: \\tau_{\\gamma} \\sim Gamma(a_{\\gamma,1} + d, b_{\\gamma,1} + \\frac{1}{2} \\sum_{j=1}^{d} \\tau_{\\gamma,j}).$\n(13)\n5. Sampling $\\lambda_{2,\\beta}$ and $\\lambda_{2,\\gamma}$: Their full conditionals are also derived from the Gamma hyperpriors:\n$\\lambda_{2,\\beta} \\: | \\: \\beta \\sim Gamma(a_{\\beta,2} + \\frac{d}{2}, b_{\\beta,2} + \\frac{1}{2} \\sum_{j=1}^{d} \\beta_j^2),$\n(14)\n$\\lambda_{2,\\gamma} \\: | \\: \\gamma \\sim Gamma(a_{\\gamma,2} + \\frac{d}{2}, b_{\\gamma,2} + \\frac{1}{2} \\sum_{j=1}^{d} \\gamma_j^2).$\n(15)\nAfter discarding burn-in samples, the collected draws from the posterior are used for inference on the parameters."}, {"title": "4 Theory", "content": "To provide a theoretical justification for our modeling", "divergence": "n$D_{KL"}, "P_{\\beta_0,\\gamma_0} \\: || \\: P_{\\beta,\\gamma}) = \\frac{1}{n} \\sum_{i=1}^{n} D_{KL}^{(i)} = \\frac{1}{2n} \\sum_{i=1}^{n} \\{ X_i^T(\\gamma - \\gamma_0) + \\frac{exp(X_i^T \\gamma_0)}{exp(X_i^T \\gamma)} + \\frac{(X_i^T(\\beta - \\beta_0))^2}{exp(X_i^T \\gamma)} - 1\\}.$\nNow, assume that the candidate parameters ($\\beta, \\gamma$) are in a sufficiently small neighborhood of the true parameters ($\\beta_0, \\gamma_0$). Under Assumption 4.3, for each i we have\n$|X_i^T(\\gamma - \\gamma_0)| \\leq ||X_i||_2 ||\\gamma - \\gamma_0||_2 \\leq M ||\\gamma - \\gamma_0||_2,$\nand\n$\\frac{(X_i^T(\\beta - \\beta_0))^2}{exp(X_i^T \\gamma)} < \\frac{M^2 ||\\beta - \\beta_0||_2^2}{c},$\nwhere c > 0 is a lower bound for $exp(X_i^T \\gamma)$ in a neighborhood of $\\gamma_0$.\nThus, for ($\\beta, \\gamma$) close to ($\\beta_0, \\gamma_0$) we can bound the KL divergence as\n$D_{KL}(P_{\\beta_0,\\gamma_0} \\: || \\: P_{\\beta,\\gamma}) \\leq \\frac{1}{2} \\{ M ||\\gamma - \\gamma_0||_2 + \\frac{exp(X_i^T \\gamma_0)}{c} + \\frac{M^2}{c} ||\\beta - \\beta_0||_2^2 - 1 \\}.$\nIn a sufficiently small neighborhood, higher-order terms and constant offsets can be absorbed into an overall constant C > 0. That is, we have\n$D_{KL}(P_{\\beta_0,\\gamma_0} \\: || \\: P_{\\beta,\\gamma}) \\leq C(||\\beta - \\beta_0||_2^2 + ||\\gamma - \\gamma_0||_2^2).$\nAssumption 4.4 guarantees that the true parameters ($\\beta_0, \\gamma_0$) lie in the support of the prior. Consequently, for any d > 0, the prior assigns positive mass to the KL neighborhood\n$\\{(\\beta, \\gamma) \\: | \\: D_{KL}(P_{\\beta_0,\\gamma_0} \\: || \\: P_{\\beta,\\gamma}) < \\delta \\}.$\nThus, the standard conditions for posterior consistency (see, e.g., Schwartz, 1965) are satisfied, and by Schwartz's theorem we conclude that the posterior distribution concentrates in an $\\epsilon_n$-neighborhood of ($\\beta_0, \\gamma_0$) for some sequence $\\epsilon_n \\rightarrow 0 \\: as \\: n \\rightarrow \\infty$. That is,\n$\\lim_{n \\rightarrow \\infty} P(\\||\\beta - \\beta_0||_2 + ||\\gamma - \\gamma_0||_2 < \\epsilon_n \\: | \\: y, X) = 1, almost surely.$\nThis completes the proof.\nCorollary 1. Under a sub-Gaussian design matrix X and true parameters $\\beta_0 \\in R^d$ and $\\gamma_0 \\in R^d$ with sparsity levels $s_{\\beta}$ and $s_{\\gamma}$ respectively, the posterior distribution satisfies the following expectation bound for the posterior means $\\hat{\\beta}$ and $\\hat{\\gamma}$:\n$E_{\\beta_0,\\gamma_0}[\\|\\hat{\\beta} - \\beta_0\\|_2^2 + \\|\\hat{\\gamma} - \\gamma_0\\|_2^2"]}