{"title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning", "authors": ["Xinglin Wang", "Shaoxiong Feng", "Yiwei Li", "Peiwen Yuan", "Yueqi Zhang", "Boyuan Pan", "Heda Wang", "Yao Hu", "Kan Li"], "abstract": "Self-consistency (SC), a widely used decoding strategy for chain-of-thought reasoning, shows significant gains across various multi-step reasoning tasks but comes with a high cost due to multiple sampling with the preset size. Its variants, Adaptive self-consistency (ASC) and Early-stopping self-consistency (ESC), dynamically adjust the number of samples based on the posterior distribution of a set of pre-samples, reducing the cost of SC with minimal impact on performance. Both methods, however, do not exploit the prior information about question difficulty. It often results in unnecessary repeated sampling for easy questions that could be accurately answered with just one attempt, wasting resources. To tackle this problem, we propose Difficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty information from both prior and posterior perspectives to adaptively allocate inference resources, further reducing the cost of SC. To demonstrate the effectiveness of DSC, we conduct extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning on six benchmarks. The empirical results show that DSC consistently surpasses the strong baseline ASC and ESC in terms of costs by a significant margin, while attaining comparable performances.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have exhibited strong reasoning capabilities (Bubeck et al., 2023), especially with chain-of-thought (CoT) prompting (Wei et al., 2022b). Based on this, Wang et al. (2022) introduced a simple decoding strategy called self-consistency (SC) to further improve reasoning performance, leveraging the fact that challenging reasoning tasks typically require more reasoning paths to arrive at the correct answer. In contrast to the standard chain-of-thought prompting which only generates the greedy one, this method samples multiple reasoning paths according to a preset sample size, and then derives the final answer through majority-voting-based scheme.\nDespite generally leading to improvements, SC introduces a significant overhead proportional to the number of sampled outputs. As LLMs continue to grow in size and complexity, the sampling time and computational costs associated with majority voting become increasingly challenging. Recently, some works seek to reduce the cost of SC by dynamically adjusting the number of samples based on the posterior distribution of pre-samples. ASC (Aggarwal et al., 2023) samples one by one, and stops sampling when the existing sample answers have established a clear majority as judged by a lightweight stopping criterion. Alternatively, ESC (Li et al., 2023) divides the large preset sample size into several sequential small windows, and stop sampling when answers within a window are all the same.\nHowever, both approaches still suffer from the following two shortcomings: (1) Both require a certain amount of pre-sampling for all problems, which still results in redundant waste. As shown in Figure 1, there is a high overlap of inference results between SC and CoT, which suggests that only a small portion of questions (3.8% for GPT-4) benefit from SC. Generating multiple samples for the remaining questions compared to CoT (only sampling once) results in a significant waste of costs. Although ASC and ESC somewhat reduce the ineffective cost of SC by decreasing average sample size, there remains redundant sampling for simple problems\u00b9. (2) Multiple re-samples bring additional significant input costs. Both ASC and ESC focus solely on reducing the number of outputs, without considering the extra input costs brought by multiple re-samples (Table 1). Consequently, for problems requiring multiple sampling times, ASC and ESC will introduce substantial extra input costs, sometimes outweighing the savings achieved through output sampling reduction (Table 3).\nTo alleviate these issues, we take inspiration from the strategies humans employ when solving reasoning problems within a limited total time. As illustrated in Figure 2, humans pre-assess the difficulty of the problems before reasoning, and adaptively allocating problem-solving time based on the accessed difficulty. In light of this, we propose Difficulty-Adaptive Self-Consistency (DSC), a novel method which leverages the difficulty information from both prior and posterior perspectives to further reduce the inference computational cost of SC. As illustrated in Figure 3, DSC consists of three steps. Firstly, we propose Difficulty Ranking algorithm which utilizes the LLM itself to rank the difficulty of the given problem set\u00b2. Based on this, we propose a Problem Partition strategy that divides the problem set into two parts, easy and hard, using the information regarding difficulty rankings. For problems belonging to easy part, a single CoT sampling is performed, which saves cost without sacrificing performance. Lastly, we propose Sample Size Pre-Allocation algorithm to predict the sample size needed for problems belonging to hard part, which reduce the number of re-samples and save the cost of inputs.\nWe evaluate DSC on a wide range of arithmetic, commonsense and symbolic reasoning tasks over GPT-3.5-Turbo and GPT-4 models. The empirical results demonstrate that DSC outperforms the existing strong baselines ASC and ESC by a significant margin on six popular benchmarks, while attaining comparable performances. Further experiments confirm the effectiveness of the proposed three-step algorithms: Difficulty Ranking, Problem Partition, and Sample Size Pre-Allocation.\nIn summary, this work includes the following key contributions:\n\u2022 We analyzed two common issues present in existing SC variants, ESC and ASC, designed"}, {"title": "Methodology", "content": "The core idea behind DSC is to fully utilize the difficulty information of given problems, allocating computational resources based on their respective levels of difficulty. The overall workflow of DSC is presented in Figure 3, including three steps: Difficulty Ranking, Problem Partition and Sample Size Pre-Allocation."}, {"title": "Difficulty Ranking", "content": "As problems usually do not carry difficulty label themselves, we seek to utilize the powerful comparative and ranking capabilities of LLM to obtain the difficulty rank of the given problem set. Considering the limited context window size of LLM and its lost in the middle (Liu et al., 2024) issue in understanding long contexts, we randomly divide the problem set of N into batches of size B (B << N) and let LLM rank the difficulty of the problems in each batch\u00b3. Since a single random split only allows us to obtain the difficulty ranking of each problem within the corresponding batch, we perform R times random batch splitting, and sort the average difficulty rankings of each problem in different batches to obtain the difficulty ranking of entire problem set. Algorithm 1 illustrates the specific implementation of difficulty ranking."}, {"title": "Problem Partition", "content": "After obtaining the problem set with difficulty ranking, we aim to find which part of the problems only require LLM to perform CoT sampling once. A simple and intuitive idea is that when LLM is very confident about a number of continuous problems sorted by difficulty (the results of all pre-samples are the same), it can be inferred that the problems easier than these problems only need one CoT sampling. Guided by this, we design the Problem Partition algorithm, illustrated in Algorithm 2. Specifically, we pre-sample the problem set sorted by difficulty from hard to easy, and store the entropy of pre-sampling results of each problem in a list. We stop pre-sampling when the latest k items in the list are all zero. The remaining problems without pre-sampling are then divided into the easy part, and a single CoT sampling is performed for each."}, {"title": "Sample Size Pre-Allocation", "content": "After performing CoT sampling on the easy part problems, we seek to predict and allocate the sample size for the problems belonging to hard part, so as to mitigate the substantial cost brought by multiple re-samples. Considering that the required sample size for each problem should be similar to those needed for problems of comparable difficulty, we predict the sample size of the current problem based on the total sample size of the nearest m easier problems. Algorithm 3 shows the workflow of Sample Size Pre-Allocation. Specifically, we sample questions belonging to the hard part from easy to difficult. For the current question to be inferred, we predict its pre-allocation sample size PA based on the average total sample size of its previous m questions. Then, we judge the distribution of the current samples based on the stopping criteria C5. When the criteria is not met, we re-sample based on the expansion window e, until the sampling distribution meets the criteria for stopping sampling or the number of samples reaches the max sample size L. After the sampling for the current question ends, we add its samples and total sample size to the Sall and Nall lists respectively, in order to pre-allocate the sample size for the next question."}, {"title": "Experiments", "content": null}, {"title": "Experimental Setup", "content": null}, {"title": "Benchmarks", "content": "We evaluate the proposed DSC on six benchmark datasets from three categories of reasoning tasks: For arithmetic reasoning, we consider MATH"}, {"title": "Baselines", "content": "We compare DSC to the following self-consistency methods: (1) SC (Wang et al., 2023) is the standard self-consistency which samples multiple reasoning paths and then derives the final answer through majority-voting; (2) ASC (Aggarwal et al., 2023) samples one by one, and stops sampling when the existing samples meet a designed stopping criteria, which measures the LLM's confidence in its current samples; (3) ESC (Li et al., 2023) proposes using small window detection to save cost, which divides the large preset sample size into several sequential small windows, and stops sampling when answers within a window are all the same. Specifically, ASC and ESC are two strong baselines for cost-efficient self-consistency methods, we reproduce both methods according to their original implementation."}, {"title": "Implementation details", "content": "We perform experiments using two powerful LLMs: GPT-4 (OpenAI, 2023) and GPT-3.5-Turbo6. We calculate the cost based on the price of the API we use. All experiments are conducted in the few-shot setting without training or fine-tuning the language models. To ensure a fair comparison, we use the same prompts as Wei et al. (2022a). Following Li et al. (2023), The sampling temperature T for MATH is 0.5 while for other datasets is 0.7. For difficulty ranking, we use CoT sampling. We set the default parameters as follows: batch size B as 8 and random split rounds R as 5 for Difficulty Ranking; pre-sample size p as 4 and judge window size k as 32 for Problem Partition; extend window size e as 4 and prediction window size m as 16 for Sample Size Pre-Allocation; max sample size L as 40 for all baselines. All experiments are repeated 100 times and the average performance is reported. Unless otherwise specified, the reported cost of DSC includes the cost brought by all three sub-steps."}, {"title": "Main Results", "content": "DSC significantly reduces costs while barely affecting performance. Table 2 summarizes the cost and accuracy of SC, ASC, ESC, and proposed DSC for each dataset. We show that DSC consistently outperforms all baselines on cost by a significant margin across all datasets, while barely affecting performance. Specifically, DSC reduces the cost on GPT-4 by an average of 65.29% and on GPT-3.5-Turbo by 56.04% compared to SC. In comparison to the strongest baseline method ESC, DSC reduces the cost on GPT-4 by an average of 24.81% and on GPT-3.5-Turbo by 21.86%, which demonstrates the effectiveness of DSC.\nDSC is scalable across various max sampling size. We conduct experiments with various max sampling size to validate the scalability of ESC. Table 3 shows the performance across different maximum sampling sizes. First we can see the performance of SC continuously improves as max sampling size L increases, which is consistent with the results in (Wang et al., 2023). On this basis, ESC can significantly save costs while maintaining performance for different L. On the contrary, due to the substantial additional input cost brought by multiple re-samples, ASC and ESC result in higher cost than SC when the cost savings of output tokens are limited."}, {"title": "Analysis of Three Sub-steps", "content": "To further validate the effectiveness of proposed three sub-steps, including Difficulty Ranking, Problem Partition, and Sample Size Pre-Allocation, we conduct experimental analyses on each of them respectively."}, {"title": "Difficulty Ranking", "content": "Difficulty Ranking exhibits a good correlation with humans. As shown in Table 4, we calculate Spearman, Pearson, and Kendall correlation coefficients on seven subsets of MATH benchmark. Overall, GPT-4 demonstrates a high consistency with human labels, while the weaker GPT-3.5-Turbo shows a moderate consistency. Specifically, both GPT-4 and GPT-3.5-Turbo rank weakly on Geometry difficulty, which may be due to the fact that text LLMs cannot intuitively assess the difficulty of geometry problems through visual information like humans can."}, {"title": "Problem Partition", "content": "Problem Partition proves to be effective across all datasets. As shown in Table 5, we conduct an ablation study on Problem Partition (step 2). We show that the removal of Problem Partition results in an increase in both input and output tokens of DSC across all six datasets, which validates the effectiveness and generalizability of Problem Partition. Furthermore, we find that the removal of Problem Partition has a small impact on output tokens when it comes to MATH dataset, which could be due to the fact that for datasets with overall higher difficulty, the easy part problem (only require one CoT sample) for LLM is fewer.\nJudge window size is essential for the accuracy of DSC. Regarding the intuitive idea proposed in Section 2.2, a straightforward question is how large the judge window size k needs to be to ensure the accuracy on simpler problems with single CoT"}, {"title": "Sample Size Pre-Allocation", "content": "Sample Size Pre-Allocation significantly alleviates the substantial input token costs brought by multiple re-samples. We conduct an ablation study of proposed Sample Size Pre-Allocation (step 3) to validate its effectiveness. As shown in Table 5, with the removal of Sample Size Pre-Allocation, the count of input tokens on DSC rose on all datasets on GPT-3.5-Turbo and GPT-4 (with the exception of GPT-4 on Coinflip), validating the effectiveness and generalizability of Sample Size Pre-Allocation. Moreover, we notice that the input tokens of ASC and ESC far exceed that of SC, which once again confirms that multiple re-samples will bring a large amount of additional overhead. Meanwhile, DSC significantly reduced the number of input tokens through Pre-Allocation, keeping it comparable to SC. In addition, we find that Sample Size Pre-Allocation leads to a slight increase in output tokens (due to the predicted sample size of some questions exceeding the actual requirement). Overall, we believe it is a good choice to trade"}, {"title": "Related Work", "content": "Chain-of-thought Reasoning Chain-of-thought prompting has been proven to be an effective method of solving complex reasoning problems (Wei et al., 2022a). By following the pattern of gradually solving sub-problems, few-shot CoT (Fu et al., 2023) are capable of stimulating LLM reasoning abilities. On this basis, Least-to-most prompting (Zhou et al., 2023) suggests explicitly splitting the problem and solving them step by step. Zheng et al. (2023) reaches the final answer by iteratively generating answers and using the previously generated answers as context hints.\nSelf-Consistency Self-consistency (Wang et al., 2023) refers to a simple decoding strategy for further improving reasoning performance, leveraging the fact that complex reasoning tasks typically allow for more than one correct reasoning path. Li et al. (2024) assign appropriate weights for answer aggregation to achieve adaptive self-consistency. Jain et al. (2023) and Wang et al. (2024) extend it for open-ended generation tasks like code generation and text summarization. However, they require multiple sampling with the pre-set size, which will incur much more computation cost. To realize cost-efficient self-consistency, Aggarwal et al. (2023) introduce an adaptive stopping criterion based on the amount of agreement between the samples so far. Li et al. (2023) divides the large preset sample size into several sequential small windows, and stop sampling when answers within a window are all the same."}, {"title": "Conclusion", "content": "In this work, we propose a novel cost-efficient decoding method called Difficulty-Adaptive Self-Consistency (DSC), which fully leverages the difficulty information of given problem set to allocate computational resources based on their respective levels of difficulty, so as to alleviate issues of redundant sampling on simple questions and multiple re-sampling limitations that current cost-efficient self-consistency methods face. Extensive experiments show that DSC consistently surpasses two"}, {"title": "Limitations", "content": "Despite the remarkable efficiency gain on variety of reasoning tasks, the current implementation of DSC still suffers from the following two limitations:\n\u2022 As illustrated in analysis of Difficulty Ranking, DSC is difficult to apply directly in scenarios that require real-time reasoning for mixed-type problems. Further classification of problems by type or further optimization of the current Difficulty Ranking algorithm is needed.\n\u2022 Given that DSC demands an awareness of the test set to rank samples based on their difficulty, its use could be restricted in scenarios where a single user is only permitted one input at a time. Nevertheless, the application scenarios of DSC include but are not limited to: (1) The server end (such as OpenAI company, etc) simultaneously receives a large number of query requests from users. (2) A user possesses a batch of data that requires a one-time inference."}, {"title": "Appendix", "content": null}, {"title": "Prompt for Difficulty Ranking", "content": "Your task is to rank the given questions from easy to hard based on their difficulty level. Questions to be evaluated:\nQ1:{Question 1}\nQ2:{Question 2}\n...\nQn: {Question n}\nThe output format should be a comma-separated list containing the Qnumber of corresponding question. Do not give any explanation.\nDifficulty Ranking result (from easy to hard):"}, {"title": "Generalizability of DSC On Small Open-source Language Model", "content": "To further explore the effectiveness of DSC on small open-source models, we conduct experiments on the open-source model Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) using GSM8K dataset. As shown in Table 8, we convert the token cost into price according to that of GPT-4 for a simpler comparison and keep the other settings completely consistent with the main experiment (Please note that for DSC, the cost of step 1 and step 2 is included.). The experimental results indicate that DSC has the potential to work effectively on smaller models."}, {"title": "Comparison of Inference Time Between Different Methods", "content": "Considering the inference time is important for real world scenarios, we calculate the inference time of different methods on the MATH test set (5000 questions) with GPT-4. As shown in Table 9, for DSC, the ranking time corresponds to the time produced by step 1, while sampling time corresponds to the time generated by step 2 and step 3. The"}, {"title": "Comparison with ESC under Different Window Size", "content": "To further demonstrate the effectiveness of DSC compared to ESC, we make a comparison between the performance of DSC and ESC under different window sizes on the MATH dataset using GPT-4, while maintaining the rest of the settings completely consistent with the main experiment. As shown in Table 10, when the window size is greater than or equal to 5, ESC can maintain its Accuracy and its cost increases as the window size enlarges. However, when the window size is less than 5, the Accuracy of ESC drops significantly due to excessively relaxed constraints. Overall, the performance of DSC consistently surpasses that of ESC by a large margin."}, {"title": "Hyperparameter Experiment of Difficulty Ranking", "content": "As shown in Figure 6, we conduct hyperparameter experiments on the proposed Difficulty Ranking algorithm. The experimental results indicate that a large batch size (\u2265 16) leads to a decrease in LLM ranking performance, which is consistent with our expectations. For both GPT-3.5-Turbo and GPT-4, difficulty ranking approximately converges in the 5th round. Therefore, we choose 5 and 8 as the default values for iteration and batch size for Difficulty Ranking, respectively."}, {"title": "Background of ESC and ASC", "content": "ESC ESC proposes extension window sampling, where the extension window is a fixed preset value w, Each time, w entries are sampled through the LLM and added to the sampling set. If the answers of the current w samples are the same or the preset maximum sampling value is reached, the sampling is stopped. The core idea of ESC is that if the model's one-time w sample answers are completely the same, it can be considered that it has a high confidence in this answer, and sampling can be stopped.\nASC ASC proposes the Dirichlet Stopping Criteria, where sampling is conducted one by one. After each sampling, the Dirichlet Stopping Criteria is used to determine whether all current samples meet a specific distribution. If they do, the sampling is stopped. If not, the one-by-one sampling continues until the preset maximum sampling value is reached. The Dirichlet Stopping Criteria is shown in Equation 1, where v represents the current set of samples, m is the number of v, Cthresh is the preset threshold (set to 0.95 for ASC in default), and p\u2081 is the probability of the most frequently occurring answer in the set v. The core idea of ASC is to measure the two answers with the highest probability in the current set of samples. If the difference between the probability of the answer with the highest probability and the second highest probability exceeds a threshold (Cthresh), it can be considered that the model is very confident in the answer with the highest probability, and sampling can be stopped."}]}