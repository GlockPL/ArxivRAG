{"title": "Make Every Penny Count: Difficulty-Adaptive Self-Consistency for Cost-Efficient Reasoning", "authors": ["Xinglin Wang", "Shaoxiong Feng", "Yiwei Li", "Peiwen Yuan", "Yueqi Zhang", "Boyuan Pan", "Heda Wang", "Yao Hu", "Kan Li"], "abstract": "Self-consistency (SC), a widely used decoding strategy for chain-of-thought reasoning, shows significant gains across various multi-step reasoning tasks but comes with a high cost due to multiple sampling with the preset size. Its variants, Adaptive self-consistency (ASC) and Early-stopping self-consistency (ESC), dynamically adjust the number of samples based on the posterior distribution of a set of pre-samples, reducing the cost of SC with minimal impact on performance. Both methods, however, do not exploit the prior information about question difficulty. It often results in unnecessary repeated sampling for easy questions that could be accurately answered with just one attempt, wasting resources. To tackle this problem, we propose Difficulty-Adaptive Self-Consistency (DSC), which leverages the difficulty information from both prior and posterior perspectives to adaptively allocate inference resources, further reducing the cost of SC. To demonstrate the effectiveness of DSC, we conduct extensive experiments on three popular categories of reasoning tasks: arithmetic, commonsense and symbolic reasoning on six benchmarks. The empirical results show that DSC consistently surpasses the strong baseline ASC and ESC in terms of costs by a significant margin, while attaining comparable performances.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have exhibited strong reasoning capabilities (Bubeck et al., 2023), especially with chain-of-thought (CoT) prompting (Wei et al., 2022b). Based on this, Wang et al. (2022) introduced a simple decoding strategy called self-consistency (SC) to further improve reasoning performance, leveraging the fact that challenging reasoning tasks typically require more reasoning paths to arrive at the correct answer. In contrast to the standard chain-of-thought prompting which only generates the greedy one, this method samples multiple reasoning paths according to a preset sample size, and then derives the final answer through majority-voting-based scheme.\nDespite generally leading to improvements, SC introduces a significant overhead proportional to the number of sampled outputs. As LLMs continue to grow in size and complexity, the sampling time and computational costs associated with majority voting become increasingly challenging. Recently, some works seek to reduce the cost of SC by dynamically adjusting the number of samples based on the posterior distribution of pre-samples. ASC (Aggarwal et al., 2023) samples one by one, and stops sampling when the existing sample answers have established a clear majority as judged by a lightweight stopping criterion. Alternatively, ESC (Li et al., 2023) divides the large preset sample size into several sequential small windows, and stop sampling when answers within a window are all the same.\nHowever, both approaches still suffer from the following two shortcomings: (1) Both require a certain amount of pre-sampling for all problems, which still results in redundant waste. As shown in Figure 1, there is a high overlap of inference results between SC and CoT, which suggests that only a small portion of questions (3.8% for GPT-4) benefit from SC. Generating multiple samples for the remaining questions compared to CoT (only sampling once) results in a significant waste of costs. Although ASC and ESC somewhat reduce the ineffective cost of SC by decreasing average sample size, there remains redundant sampling for simple problems\u00b9. (2) Multiple re-samples bring additional significant input costs. Both ASC and ESC focus solely on reducing the number of outputs, without considering the extra input costs brought by multiple re-samples (Table 1). Consequently, for problems requiring multiple sampling times, ASC and ESC will introduce substantial extra input costs, sometimes outweighing the savings achieved through output sampling reduction (Table 3).\nTo alleviate these issues, we take inspiration from the strategies humans employ when solving reasoning problems within a limited total time. As illustrated in Figure 2, humans pre-assess the difficulty of the problems before reasoning, and adaptively allocating problem-solving time based on the accessed difficulty. In light of this, we propose Difficulty-Adaptive Self-Consistency (DSC), a novel method which leverages the difficulty information from both prior and posterior perspectives to further reduce the inference computational cost of SC. As illustrated in Figure 3, DSC consists of three steps. Firstly, we propose Difficulty Ranking algorithm which utilizes the LLM itself to rank the difficulty of the given problem set\u00b2. Based on this, we propose a Problem Partition strategy that divides the problem set into two parts, easy and hard, using the information regarding difficulty rankings. For problems belonging to easy part, a single CoT sampling is performed, which saves cost without sacrificing performance. Lastly, we propose Sample Size Pre-Allocation algorithm to predict the sample size needed for problems belonging to hard part, which reduce the number of re-samples and save the cost of inputs.\nWe evaluate DSC on a wide range of arithmetic, commonsense and symbolic reasoning tasks over GPT-3.5-Turbo and GPT-4 models. The empirical results demonstrate that DSC outperforms the existing strong baselines ASC and ESC by a significant margin on six popular benchmarks, while attaining comparable performances. Further experiments confirm the effectiveness of the proposed three-step algorithms: Difficulty Ranking, Problem Partition, and Sample Size Pre-Allocation.\nIn summary, this work includes the following key contributions:\n\u2022 We analyzed two common issues present in existing SC variants, ESC and ASC, designed"}, {"title": "Methodology", "content": "The core idea behind DSC is to fully utilize the difficulty information of given problems, allocating computational resources based on their respective levels of difficulty. The overall workflow of DSC is presented in Figure 3, including three steps: Difficulty Ranking, Problem Partition and Sample Size Pre-Allocation."}, {"title": "Difficulty Ranking", "content": "As problems usually do not carry difficulty label themselves, we seek to utilize the powerful comparative and ranking capabilities of LLM to obtain the difficulty rank of the given problem set. Considering the limited context window size of LLM and its lost in the middle (Liu et al., 2024) issue in understanding long contexts, we randomly divide the problem set of N into batches of size B (B << N) and let LLM rank the difficulty of the problems in each batch\u00b3. Since a single random split only allows us to obtain the difficulty ranking of each problem within the corresponding batch, we perform R times random batch splitting, and sort the average difficulty rankings of each problem in different batches to obtain the difficulty ranking of entire problem set. Algorithm 1 illustrates the specific implementation of difficulty ranking."}, {"title": "Problem Partition", "content": "After obtaining the problem set with difficulty ranking, we aim to find which part of the problems only require LLM to perform CoT sampling once. A simple and intuitive idea is that when LLM is very confident about a number of continuous problems sorted by difficulty (the results of all pre-samples are the same), it can be inferred that the problems easier than these problems only need one CoT sampling. Guided by this, we design the Problem Partition algorithm, illustrated in Algorithm 2. Specifically, we pre-sample the problem set sorted by difficulty from hard to easy, and store the entropy of pre-sampling results of each problem in a list. We stop pre-sampling when the latest k items in the list are all zero. The remaining problems without pre-sampling are then divided into the easy part, and a single CoT sampling is performed for each."}, {"title": "Sample Size Pre-Allocation", "content": "After performing CoT sampling on the easy part problems, we seek to predict and allocate the sample size for the problems belonging to hard part, so as to mitigate the substantial cost brought by multiple re-samples. Considering that the required sample size for each problem should be similar to those needed for problems of comparable difficulty, we predict the sample size of the current problem based on the total sample size of the nearest m easier problems. Algorithm 3 shows the workflow of Sample Size Pre-Allocation. Specifically, we sample questions belonging to the hard part from easy to difficult. For the current question to be inferred, we predict its pre-allocation sample size PA based on the average total sample size of its previous m questions. Then, we judge the distribution of the current samples based on the stopping criteria C5. When the criteria is not met, we re-sample based on the expansion window e, until the sampling distribution meets the criteria for stopping sampling or the number of samples reaches the max sample size L. After the sampling for the current question ends, we add its samples and total sample size to the Sall and Nall lists respectively, in order to pre-allocate the sample size for the next question."}, {"title": "Experiments", "content": null}, {"title": "Experimental Setup", "content": null}, {"title": "Benchmarks", "content": "We evaluate the proposed DSC on six benchmark datasets from three categories of reasoning tasks: For arithmetic reasoning, we consider MATH"}, {"title": "Difficulty Ranking", "content": "Difficulty Ranking produces an acceptable cost.\nAs we instruct LLM to produce extremely concise outputs for Difficulty Ranking, the cost of output to-kens brought by Difficulty ranking is very low7. Mixing types of questions lowers the effective-ness of Difficulty Ranking. To explore whether Difficulty Ranking can effectively rank problems of different types in terms of difficulty, we com-pare the performance of Difficulty Ranking under mixed and unmixed problem types on MATH"}, {"title": "Problem Partition", "content": "Problem Partition proves to be effective across all datasets. As shown in Table 5, we conduct an ablation study on Problem Partition (step 2). Judge window size is essential for the accuracy of DSC."}, {"title": "Sample Size Pre-Allocation", "content": "Sample Size Pre-Allocation significantly allevi-ates the substantial input token costs brought by multiple re-samples. We conduct an abla-tion study of proposed Sample Size Pre-Allocation (step 3) to validate its effectiveness. Moreover, we notice that the input tokens of ASC and ESC far exceed that of SC."}, {"title": "Related Work", "content": "Chain-of-thought Reasoning Chain-of-thought prompting has been proven to be an effective method of solving complex reasoning problems (Wei et al., 2022a). By following the pattern of gradually solving sub-problems, few-shot CoT (Fu et al., 2023) are capable of stimulating LLM reasoning abilities. On this basis, Least-to-most prompt-ing (Zhou et al., 2023) suggests explicitly splitting the problem and solving them step by step. Zheng et al. (2023) reaches the final answer by iteratively generating answers and using the previously gener-ated answers as context hints.\nSelf-Consistency Self-consistency (Wang et al., 2023) refers to a simple decoding strategy for further improving reasoning performance, leveraging the fact that complex reasoning tasks typically allow for more than one correct reasoning path. Li et al. (2024) assign appropriate weights for answer aggregation to achieve adaptive self-consistency. Jain et al. (2023) and Wang et al. (2024) extend it for open-ended generation tasks like code genera-tion and text summarization. However, they require multiple sampling with the pre-set size, which will incur much more computation cost. To realize cost-efficient self-consistency, Aggarwal et al. (2023) introduce an adaptive stopping criterion based on the amount of agreement between the samples so far. Li et al. (2023) divides the large preset sam-ple size into several sequential small windows, and stop sampling when answers within a window are all the same."}, {"title": "Conclusion", "content": "In this work, we propose a novel cost-efficient decoding method called Difficulty-Adaptive Self-Consistency (DSC), which fully leverages the difficulty information of given problem set to allocate computational resources based on their respective levels of difficulty, so as to alleviate issues of redundant sampling on simple questions and multiple re-sampling limitations that current cost-efficient self-consistency methods face. Extensive experi-ments show that DSC consistently surpasses two"}, {"title": "Limitations", "content": "Despite the remarkable efficiency gain on variety of reasoning tasks, the current implementation of DSC still suffers from the following two limitations:\n\u2022 As illustrated in analysis of Difficulty Rank-ing, DSC is difficult to apply directly in sce-narios that require real-time reasoning for mixed-type problems. Further classification of problems by type or further optimization of the current Difficulty Ranking algorithm is needed.\n\u2022 Given that DSC demands an awareness of the test set to rank samples based on their diffi-culty, its use could be restricted in scenarios where a single user is only permitted one input at a time. Nevertheless, the application sce-narios of DSC include but are not limited to: (1) The server end (such as OpenAI company, etc) simultaneously receives a large number of query requests from users. (2) A user pos-sesses a batch of data that requires a one-time inference."}, {"title": "Appendix", "content": null}, {"title": "Prompt for Difficulty Ranking", "content": "Your task is to rank the given questions from easy to hard based on their difficulty level. Questions to be evaluated:\nQ1:{Question 1}\nQ2:{Question 2}\nQn: {Question n}\nThe output format should be a comma-separated list containing the Qnumber of corresponding question. Do not give any explanation.\nDifficulty Ranking result (from easy to hard):"}, {"title": "Generalizability of DSC On Small Open-source Language Model", "content": "To further explore the effectiveness of DSC on small open-source models, we conduct experiments on the open-source model Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) using GSM8K dataset. As shown in Table 8, we convert the token cost into price according to that of GPT-4 for a simpler com-parison and keep the other settings completely con-sistent with the main experiment (Please note that for DSC, the cost of step 1 and step 2 is included.). The experimental results indicate that DSC has the potential to work effectively on smaller models."}, {"title": "Comparison of Inference Time Between Different Methods", "content": "Considering the inference time is important for real world scenarios, we calculate the inference time of different methods on the MATH test set (5000 questions) with GPT-4. As shown in Table 9, for DSC, the ranking time corresponds to the time pro-duced by step 1, while sampling time corresponds to the time generated by step 2 and step 3. The"}, {"title": "Comparison with ESC under Different Window Size", "content": "To further demonstrate the effectiveness of DSC compared to ESC, we make a comparison between the performance of DSC and ESC under different window sizes on the MATH dataset using GPT-4, while maintaining the rest of the settings com-pletely consistent with the main experiment. As shown in Table 10, when the window size is greater than or equal to 5, ESC can maintain its Accuracy and its cost increases as the window size enlarges. However, when the window size is less than 5, the Accuracy of ESC drops significantly due to excessively relaxed constraints. Overall, the perfor-mance of DSC consistently surpasses that of ESC by a large margin."}, {"title": "Hyperparameter Experiment of Difficulty Ranking", "content": "As shown in Figure 6, we conduct hyperparameter experiments on the proposed Difficulty Ranking algorithm. The experimental results indicate that a large batch size (\u2265 16) leads to a decrease in LLM ranking performance, which is consistent with our expectations. For both GPT-3.5-Turbo and GPT-4, difficulty ranking approximately converges in the 5th round. Therefore, we choose 5 and 8 as the default values for iteration and batch size for Difficulty Ranking, respectively."}, {"title": "Background of ESC and ASC", "content": "ESC ESC proposes extension window sampling, where the extension window is a fixed preset value w, Each time, w entries are sampled through the LLM and added to the sampling set. If the answers of the current w samples are the same or the preset maximum sampling value is reached, the sampling is stopped. The core idea of ESC is that if the model's one-time w sample answers are completely the same, it can be considered that it has a high confidence in this answer, and sampling can be stopped.\nASC ASC proposes the Dirichlet Stopping Crite-ria, where sampling is conducted one by one. After each sampling, the Dirichlet Stopping Criteria is used to determine whether all current samples meet a specific distribution. If they do, the sampling is stopped. If not, the one-by-one sampling contin-ues until the preset maximum sampling value is reached. The Dirichlet Stopping Criteria is shown in Equation 1, where v represents the current set of samples, m is the number of v, Cthresh is the preset threshold (set to 0.95 for ASC in default), and p\u2081 is the probability of the most frequently occurring answer in the set v. The core idea of ASC is to measure the two answers with the high-est probability in the current set of samples. If the difference between the probability of the answer with the highest probability and the second highest probability exceeds a threshold (Cthresh), it can be considered that the model is very confident in the answer with the highest probability, and sampling can be stopped."}]}