{"title": "Open-Vocabulary Remote Sensing Image Semantic Segmentation", "authors": ["Qinglong Cao", "Yuntian Chen", "Chao Ma", "Xiaokang Yang"], "abstract": "Open-vocabulary image semantic segmentation (OVS) seeks to segment images into semantic regions across an open set of categories. Existing OVS methods commonly depend on foundational vision-language models and utilize similarity computation to tackle OVS tasks. However, these approaches are predominantly tailored to natural images and struggle with the unique characteristics of remote sensing images, such as rapidly changing orientations and significant scale variations. These challenges complicate OVS tasks in earth vision, requiring specialized approaches. To tackle this dilemma, we propose the first OVS framework specifically designed for remote sensing imagery, drawing inspiration from the distinct remote sensing traits. Particularly, to address the varying orientations, we introduce a rotation-aggregative similarity computation module that generates orientation-adaptive similarity maps as initial semantic maps. These maps are subsequently refined at both spatial and categorical levels to produce more accurate semantic maps. Additionally, to manage significant scale changes, we integrate multi-scale image features into the upsampling process, resulting in the final scale-aware semantic masks. To advance OVS in earth vision and encourage reproducible research, we establish the first open-sourced OVS benchmark for remote sensing imagery, including four public remote sensing datasets. Extensive experiments on this benchmark demonstrate our proposed method achieves state-of-the-art performance. All codes and datasets are available at https://github.com/caoql98/OVRS.", "sections": [{"title": "I. INTRODUCTION", "content": "SEMANTIC segmentation of remote sensing imagery is a crucial task aimed at the precise pixel-level classification of various elements within an image, which plays a pivotal role in numerous applications [1]\u2013[4]. The advent of deep learning has revolutionized this field, enabling fully supervised seg-mentation techniques [5]\u2013[7] to achieve remarkable accuracy and efficiency. These methods have been effectively employed in diverse domains, including detailed land-use and land-cover mapping [8]\u2013[10], comprehensive traffic monitoring systems [11]\u2013[13], and the accurate extraction of infrastructure such as buildings and roads [13]\u2013[15]. Despite their success, these fully supervised approaches are highly dependent on the availability of large-scale, well-annotated datasets [16]\u2013[18]. The extensive manual labeling required for such datasets is not only time-consuming and costly but also introduces a significant limitation: the models trained on these datasets often overfit to the specific categories seen during training, which results in poor generalization to unseen categories.\nAs illustrated in Figure 1 (b), to tackle the challenge of segmenting unseen categories, many researchers have intro-duced zero-shot segmentation algorithms [19]\u2013[21]. These algorithms rely solely on class names to infer the semantic regions for categories not present in the training datasets yet in a closed category set. Despite these advancements, such methods remain confined to individual datasets. Consequently, when deployed in real-world environments where models encounter categories beyond those included in their closed category set, their performance often deteriorates significantly. This performance decline highlights a critical limitation in the generalization capability of these models, ultimately restricting their usefulness in dynamic or heterogeneous environments where the range of potential categories is extensive and unpredictable.\nTo overcome this limitation and extend the applicability of segmentation models to more realistic scenarios, open-vocabulary image semantic segmentation (OVS) has been proposed, as depicted in Figure 1 (c). OVS aims to segment remote sensing images by considering an open set of cate-gories across different datasets. Existing OVS methods mainly utilize the generalized vision-language models like CLIP [22] as the foundational models, and utilize the similarities between the features of images and category names to judge the semantic regions. For instance, Liang et al. [23] propose a two-stage OVS models, which firstly generate the mask proposals and subsequently segment the semantic regions based on the similarities of mask-category pairs. Similarly, Freeseg [24] adopts the two-stage framework and further introduces prompt learning for the text encoder, which helps the network perform the OVS tasks in the one-in-all pattern. Following the prompt learning manner, SegPrompt successfully leverages the cat-egory information as prompts to improve the model's class-agnostic segmentation ability for the OVS tasks. Though these OVS have achieved some success, they all focus on natural images. which could not handle the unique traits of remote"}, {"title": "II. RELATED WORK", "content": "In this section, we first review related work on remote sensing image semantic segmentation. Then, the advanced re-search with regard to open-vocabulary semantic segmentation is extensively introduced."}, {"title": "A. Remote Sensing Image Semantic Segmentation", "content": "Remote sensing image semantic segmentation focuses on the precise delineation of semantic regions within remote sensing imagery [25], [26]. The advent of deep learning has significantly advanced this field, leading to the development of numerous sophisticated algorithms. For example, Li et al. developed efficient attention modules to capture contextual dependencies, enhancing segmentation accuracy. Building on this, SSAtNet [27] introduced a pyramid attention module that leverages multiscale features for adaptive refinement of segmentation features. Prioritizing computational efficiency, LANet [28] introduced patch attention and local embeddings to achieve effective segmentation through a patch-focused approach, while ResU-Net [29] incorporated a linear attention mechanism to improve computational efficiency over tradi-tional dot-product attention. Focusing on spatial information, HRCNet [30] developed a high-resolution context extrac-tion network to better capture global contextual information, thereby enhancing segmentation performance. The transformer architecture [31], known for its superior image understanding capabilities across various computer vision tasks, has also been adapted for remote sensing image segmentation. For instance, Li et al [32] adopt the efficient transformer to per-form the semantic segmentation task. Based on this concept, UNetFormer [33] constructs the Transformer-based decoder and further proposes an UNet-like Transformer (UNetFormer) for real-time remote sensing segmentation. Inspired by the powerful global modeling capabilities of the swin transformer, He et al [34] adopt swin transformer to mine pixel-level cor-relation to enhance the feature representation ability. To boost the global information extracted by the transformer network with local information from the convolutional neural network (CNN), zhang [35] proposed a transformer and CNN hybrid segmentation network, where the transformer is adopted for long-range spatial dependencies modeling, and CNN is utilized to maintain the local details. Similarly, STransFuse [36] jointly leveraged the Swin Transformer and CNN to extract coarse-grained and fine-grained feature representations and perform a staged semantic segmentation at diverse semantic scales."}, {"title": "B. Open-Vocabulary Semantic Segmentation", "content": "Open-vocabulary semantic segmentation seeks to accurately delineate semantic regions across an unrestricted set of cate-gories. Current methods primarily utilize foundational vision-language models like CLIP [22], leveraging the similarities between textual category names and image features to identify corresponding semantic regions. For instance, Liang et al. [23] fine-tune CLIP using paired masked image regions and text descriptions, enabling efficient classification of these masked regions. CLIPseg [37] follows a prompt learning approach, directly using text descriptions as prompts to segment query images. Building on this adapter-based approach, SAN [38] introduces a side adapter network that generates mask pro-posals and attention biases, guiding the deeper layers of CLIP for proposal-wise classification. Similarly, SegCLIP [39] introduces the concept of super-pixels into OVS, aggregating image patches around learnable centers to form semantic regions based on pre-trained CLIP features. Extending this patch aggregation approach, Chen et al. [40] perform OVS by summarizing localized regions of the target image and distilling visual concepts using CLIP models. Focusing on the semantic alignment between visual content and unbounded text, SCAN [41] incorporates a generalized semantic prior and a contextual shift strategy to enhance segmentation perfor-mance. In contrast, SED [42] addresses the often-overlooked local spatial information by utilizing a CNN-based CLIP to construct an efficient OVS network. More directly, CAT-Seg [43] explores the multi-modal nature established between image and text embeddings, performing segmentation through cost volume computation. However, all these methods are constructed based on natural images and fail to address the unique characteristics of remote sensing imagery. To tackle this challenge and draw inspiration from these distinct traits, we propose the first open-vocabulary semantic segmentation framework specifically designed for remote sensing images."}, {"title": "III. PROPOSED METHOD", "content": "Consider an image I and a set of potential class categories C = {T(n)} where n = 1,..., Nc, with T(n) representing the textual description of the n-th category and Nc being the total number of classes. For instance, T(n) could be defined as \u201can image of Category Name\u201d. The task of open-vocabulary semantic segmentation requires assigning a label to each pixel in the image I based on these categories. Unlike traditional semantic segmentation tasks, open-vocabulary segmentation faces the unique challenge of working with a flexible and dynamic set C provided as free-form text.\nFollowing previous methods [41]\u2013[43], we employ the pre-trained CLIP model as the foundational vision-language framework to leverage its generalized knowledge. The overall structure of our proposed method is depicted in Figure 2. Specifically, given query remote sensing images, they are first rotated through various orientations and then passed through the vision branch of the pre-trained CLIP model to extract orientation-specific image features. Concurrently, category names are input into the CLIP language branch to generate corresponding class features. Subsequently, in the rotation-aggregative similarity computation module, the network computes the similarities between the image and class features to produce orientation-specific similarity maps,"}, {"title": "B. Rotation-Aggregative Similarity Computation", "content": "Objects in remote sensing images tend to exist in varying orientations. Existing OVS methods often neglect this trait, and fail to tackle the varying orientations. Thus, we argue that the semantics from varying orientations should be gathered to ensure objects with varying orientations can be accurately parsed. Particularly, given the query remote sensing image Iq1, the image would be firstly rotated in diverse orientations, for instance, (90\u00b0, 180\u00b0, and 270\u00b0), to acquire orientation-varying images {Iq1, Iq2, ..., IqNA}. These images are further propagated into the vision branch Pv of Pre-trained CLIP to obtain the diverse image features from different angles {Fq1, Fq2, ..., FqNA} \u2208 RHWxd.\nFqi = Pv (Iqi), i \u2208 1, 2, ..., \u039d\u0391, (1)\nThen, these image features are concatenated in a single dimension to obtain the orientation-vary image features Fgo \u2208 RHWXNAxd.\nFqo = [Fq1, Fq2, ..., FNA] (2)\nMeanwhile, the category names {C1, C2, ..., CNC} would be inputted into the language branch P\u2081 of pre-trained CLIP to acquire the corresponding text embeddings as the class features {E1, E2, ..., ENC } \u2208 R1\u00d7d.\nEj = PL(Cj), j \u2208 1, 2, ..., Nc, (3)\nSubsequently, the orientation-varying semantics could be obtained through the cosine similarities between class features and orientation-varying image features:\nSgo[:, :, i] = FqoEj/ || Fgo || || Ej ||, (4)\nwhere Sqi \u2208 RHW\u00d7NA\u00d7Nc_denotes the corresponding seman-tic maps. For better subsequent semantic understanding, these maps would be processed along the category dimension with a single convolutional layer:\nSqo[:, :, i] = Conv(Mqi[:, :, i]) (5)\nwhere Sq \u2208 RHW\u00d7NA\u00d7NcXdF. Then, the maps are cor-respondingly rotated along the angle dimension Na to the original orientation of Iq1 and further fused in the angle"}, {"title": "C. Spatial and Category Refinement", "content": "The acquired initial semantic maps Mq are the coarse perceptions of query remote sensing images within the image-text semantic space of the pre-trained CLIP. To refine this coarse result, the semantic maps should be better analyzed respectively in both vision and text modalities.\nFirstly, to refine the holistic vision understanding of the semantic maps, the semantic maps should be sliced at the category dimension Ne, which helps the network concentrate on the vision level, and perform the spatial refinement at the pixel level. We adopt the swin transformer [44] as the refinement layer. More specifically, the refinement could be defined as follows:\nM\u2084[:, i, :] = L+ (Mq[:, i, :]), i \u2020 (Mq[:, i, :]), i \u2208 1,2, ..., Nc (7)\nwhere Mq[:, i, :] \u2208 R(H\u00d7W)\u00d7dr, and Lift denotes a pair of two consecutive Swin transformer layers for spatial refinement. Notably, dF is the channel dimensions for each token, and attention is performed on individual categories respectively.\nFurthermore, the semantic maps would be refined at the text modality and the category refinement would help the net-work to precisely capture the relationships between multiple categories. Correspondingly, the semantic maps M should be sliced at the spatial dimension HW to eliminate the effect of vision modality. We also adopt the swin transformer as the refinement layer, and the process is defined as follows:\nM\" [i, :, :] = L\u00ba\u00ba(M[i, :, :]), i \u2208 1, 2, ..., HW (8)\nwhere M[:, i, :] \u2208 RNc\u00d7dr. Different from the spatial refine-ment, there exist no spatial relations between categories, thus a linear transformer layer La without position embeddings is adopted for the category refinement. The overall refinement including both spatial and category refinements would repeat several times to acquire the middle semantic maps with higher accuracy."}, {"title": "D. Scale-Aware Upsampling", "content": "Although the rotation-aggregative similarity computation effectively enables the network to handle rapidly changing orientations, and the spatial and category refinement gener-ates more precise semantic maps, significant scale variations remain unaddressed. To tackle this issue, it is crucial to in-corporate multiscale image features from the feature extractor to enhance the model's scale adaptation ability. Specifically, since we use the pre-trained CLIP vision branch as the feature extractor, the image features from various layers of this branch are collected to provide vital scale information. The inclusion of these multiscale features ensures that the intermediate semantic maps capture previously neglected details, making the model more robust to scale variations.\nParticularly, in the upsampling process, given the middle semantic maps Mm \u2208 RHWXNCXdF, and the previous level features FL \u2208 RdF\u00d7HL\u00d7WL, the middle semantic maps are firstly respectively average pooled respectively in spatial and channel levels, and go through the corresponding convo-lutional layers to obtain the spatial and channel activation vectors, which could be defined as:\nVsp = Conv(Avgpool(Conv(Mm)))spatial (9)\nVch = Conv(Avgpool(Conv(Mm)))channel (10)\nwhere Vsp \u2208 RHW\u00d71, and Vch \u2208 RdF\u00d71. Though the dot-product between these vectors and the previous level features FL, the relevant semantic regions would be activated to obtain the spatial activated and channel activate features:\nFP = Vsp FL (11)\nFch = Vch FL (12)\nThrough accumulating these activation features, and further concatenating with the middle semantic maps, the scale infor-mation is added into the semantic maps to obtain the scale-aware middle semantic maps Mm:\nMm = Conv([FP + Fch + FL, Mm]), (13)\nTo prepare for the subsequent upsampling process, the intermediate semantic maps M\u00b3 are further processed through a single convolutional layer, acting as the connection layer. The upsampling process is repeated several times until the semantic maps are restored to their original scale. At each step, the previous image features are leveraged to provide scale information, ensuring that the network remains sen-sitive to variations in scale throughout the process. In this way, the final generated semantic masks MF become both rotation-aggregative and scale-adaptive, enabling precise open-vocabulary segmentation in remote sensing imagery. The final generated semantic masks MF are supervised by the ground truth mask MGT using cross-entropy loss, defined as:\nL = - MGT log(MF) (14)\nThis supervision encourages the model to produce accurate segmentation predictions by minimizing the difference be-tween the predicted and actual semantic masks."}, {"title": "IV. EXPERIMENTS", "content": "To thoroughly evaluate the effectiveness of the proposed method, we conduct an extensive set of experiments on the newly introduced open-vocabulary remote sensing semantic segmentation benchmark. The experimental section is orga-nized as follows. First, we provide a detailed description of the datasets used and the evaluation metrics adopted. Next, we outline the key implementation details of our approach. Following this, a comprehensive analysis of the performance comparison between our method and state-of-the-art OVS approaches is presented, both in qualitative and quantitative terms. Finally, we perform a series of ablation studies to assess the contribution of each component of our method."}, {"title": "A. Dataset and Evaluation Metric", "content": "To comprehensively evaluate the proposed method, we expand the experimental setup beyond the commonly used Potsdam and Vaihingen semantic segmentation datasets by including the processed DLRSD [45] and iSAID [46] datasets, thereby establishing a more robust open-vocabulary semantic segmentation (OVS) benchmark for remote sensing imagery. The processed DLRSD dataset consists of 7002 images across 17 categories, while the processed iSAID dataset includes 24439 images across 15 categories. Potsdam and Vaihingen, which share the same six categories, contain 20102 and 2254 images, respectively. Detailed category names for these datasets are provided in Table I.\nIn our experimental setup, DLRSD and iSAID are used as the training datasets, while evaluations are conducted across all datasets to assess the OVS performance. Specifically, 5601 images from the DLRSD dataset and 1401 images from iSAID are used for training, with the remaining 1401 and 6363 im-ages used for validation, respectively. Following standard eval-uation protocols from prior OVS methods [42], [43], we utilize Mean Intersection over Union (mIoU), Frequency Weighted Intersection over Union (fwIoU), and Mean Accuracy (mACC) as evaluation metrics, providing a comprehensive reflection of the model's performance.\nThe mIoU is computed as the average IoU across all classes:\nIoU; = TP/TP + FP + FN,\nmIoU = 1/N \u2211 IoU, (15)"}, {"title": "B. Implement Details", "content": "We utilize the pre-trained CLIP ViTB/16 [47] as the founda-tional vision-language feature extractor, leveraging its ability to provide robust visual and textual embeddings. The pro-posed method is implemented using PyTorch and Detectron2 frameworks, ensuring flexibility and efficiency in large-scale experimentation. For optimization, we adopt the AdamW opti-mizer [48] with an initial learning rate of 2e-4 to balance con-vergence speed and stability, and a weight decay of 10e-4 to prevent overfitting. The training is performed with a batch size of 4, utilizing an Nvidia A100 GPU with 80GB of memory to manage the computational demands, especially given the high-resolution remote sensing imagery. All training images are resized to 3 \u00d7 384 \u00d7 384 to maintain consistency across datasets and to align with the input requirements of the CLIP model. The models are trained for a total of 100,000 iterations, ensuring ample convergence time for learning the intricate features required for open-vocabulary semantic segmentation in remote sensing imagery. After training on the processed DLRSD/iSAID datasets, the resulting models are thoroughly evaluated on both their respective validation sets and the complete benchmark of four remote sensing datasets (Potsdam, Vaihingen, DLRSD, and iSAID) to provide a comprehensive assessment of their generalization capabilities."}, {"title": "C. Performance Analysis", "content": "The quantitative performance comparisons between our pro-posed method and other advanced OVS algorithms are detailed in Table II, with the highest results highlighted in bold. When trained on DLRSD, our method outperforms others on most metrics. Notably, compared with the state-of-the-art CAT-Seg method, our approach shows substantial improvements on the iSAID dataset, with an increase of 15.53% in mIoU, 11.57% in fwIoU, and 15.95% in mACC. Across the validation datasets of DLRSD, Potsdam, and Vaihingen, our method also demonstrates superior performance with consistent and significant gains. However, it falls short of the top performance on the Potsdam dataset compared to the SAN method, though it still ranks second in mIoU. This underperformance may be attributed to a potential dataset domain gap between DLRSD dataset and Potsdam dataset, which could hamper the seg-mentation performance on Potsdam. Addressing this dataset domain gap remains a challenge for future work.\nIn contrast, when trained on the larger iSAID dataset, our method achieves the best results across nearly all metrics. Most notably, the Vaihingen dataset sees dramatic performance gains, with mIoU and fwIoU both surpassing 20%, while the mACC of our method reaches 50.60%. These results under-score the generalization capability of our method when given a diverse and extensive dataset for training. Nevertheless, on the DLRSD dataset, our approach fails to surpass all metrics, specifically mIoU and mACC, suggesting the persistent gap between DLRSD and iSAID data distributions, which reflects the inherent variability in remote sensing data sources.\nTo further understand the performance differences, we also provide average performance comparisons in Table II. Notably, our method outperforms others in all metrics, regardless of whether DLRSD or iSAID is used for training. Interestingly, with a relatively small DLRSD as the training dataset, our method still achieves the best average results though fails in Potsdam, indicating that DLRSD images may offer more representative features for diverse categories. In particular, the highest metric improvements are seen in mACC, where a 2.80% boost was achieved, bringing the overall mACC to 66.70%. These results reaffirm the effectiveness of our approach, emphasizing its capability to leverage the unique"}, {"title": "D. Ablation Study", "content": "To evaluate the effectiveness of our proposed OVS frame-work for remote sensing imagery, a series of experiments are performed to analyze the effect of the key components. All ablation experiments are conducted with the iSAID as the training dataset, and the corresponding validation dataset as the evaluation dataset.\nFirst, we analyze the contributions of the rotation-aggregative similarity computation and the scale-aware upsam-pling process individually, with results summarized in Table V. Notably, the network's ability to handle varying orientations significantly improves segmentation performance, boosting the mIoU from 80.78% to 87.06%. Addressing scale variations also leads to substantial gains, resulting in a performance increase of approximately 5%. By jointly tackling these unique remote sensing traits, our proposed method achieves a new"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduced the first open-vocabulary se-mantic segmentation (OVS) benchmark for remote sensing imagery and developed a novel open-sourced OVS framework specifically tailored for earth vision. To address the unique challenges of remote sensing imagery, such as rapidly chang-ing orientations and significant scale variations, we proposed a rotation-aggregative similarity computation module that generates orientation-adaptive similarity maps. These maps are subsequently refined at spatial and categorical levels to enhance segmentation accuracy. Additionally, we integrated multi-scale image features into the upsampling process to produce scale-aware semantic masks, enabling the framework to handle significant scale variations. Extensive experiments on four public remote sensing datasets under open-vocabulary settings demonstrated that our method achieves state-of-the-art performance, affirming its effectiveness in earth vision tasks. Looking forward, this work lays the foundation for future advancements in OVS for remote sensing, opening avenues for exploring more robust handling of other remote sensing vision perception challenges."}]}