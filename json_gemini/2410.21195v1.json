{"title": "Belief in the Machine: Investigating Epistemological Blind Spots of Language Models", "authors": ["Mirac Suzgun", "Tayfun Gur", "Federico Bianchi", "Daniel E. Ho", "Thomas Icard", "Dan Jurafsky", "James Zou"], "abstract": "As language models (LMs) become integral to fields like healthcare, law, and journalism, their ability to differentiate between fact, belief, and knowledge is essential for reliable decision-making. Failure to grasp these distinctions can lead to significant consequences in areas such as medical diagnosis, legal judgments, and dissemination of fake news. Despite this, current literature has largely focused on more complex issues such as theory of mind, overlooking more fundamental epistemic challenges. This study systematically evaluates the epistemic reasoning capabilities of modern LMs, including GPT-4, Claude-3, and Llama-3, using a new dataset, KaBLE, consisting of 13,000 questions across 13 tasks. Our results reveal key limitations. First, while LMs achieve 86% accuracy on factual scenarios, their performance drops significantly with false scenarios, particularly in belief-related tasks. Second, LMs struggle with recognizing and affirming personal beliefs, especially when those beliefs contradict factual data, which raises concerns for applications in healthcare and counseling, where engaging with a person's beliefs is critical. Third, we identify a salient bias in how LMs process first-person versus third-person beliefs, performing better on third-person tasks (80.7%) compared to first-person tasks (54.4%). Fourth, LMs lack a robust understanding of the factive nature of knowledge, namely, that knowledge inherently requires truth. Fifth, LMs rely on linguistic cues for fact-checking and sometimes bypass the deeper reasoning. These findings highlight significant concerns about current LMs' ability to reason about truth, belief, and knowledge while emphasizing the need for advancements in these areas before broad deployment in critical sectors.", "sections": [{"title": "Introduction", "content": "The ability to discern between fact, belief, and knowledge serves as a cornerstone of human cognition. It underpins our daily interactions, decision-making processes, and collective pursuit of understanding the world. When someone says, \u201cI believe it will rain tomorrow,\u201d we intuitively grasp the uncertainty inherent in their statement. Conversely, \u201cI know the Earth orbits the Sun\" carries the weight of established fact. This nuanced comprehension of epistemic language is crucial across various domains, from healthcare and law to journalism and politics [1-3].\nAs artificial intelligence (AI), particularly large language models (LMs), becomes increasingly sophisticated and pervasive, a critical question emerges: Can these systems truly comprehend and reason about the differences between belief, knowledge, and fact? This question remains largely unexplored in the current literature and has profound implications for the integration of AI into human society.\nConsider a patient saying to a doctor, \u201cI believe I have cancer.\u201d In healthcare, the interpretation of such statements requires careful evaluation to align subjective beliefs with objective medical assessments. Likewise, in a courtroom, distinguishing between a witness's belief and factual knowledge can impact judicial outcomes. Political discourse, too, often blurs the lines of opinion, belief, and fact, making the ability to distinguish these notions crucial for informed decision-making and maintaining public trust.\nThe integration of AI systems into these high-stakes environments is progressing and expected to accelerate at an unprecedented pace. LMs are being explored and deployed in medical diagnosis [4-6], mental health screening and therapy [7-11], legal research and analysis [12-16], journalism [17\u201319], education [20-23], scientific research [24-26], financial modeling and advising [27-30], and even interpersonal relationship counseling [31\u201333]. However, if these models lack a proper grasp of core epistemic distinctions, they risk not only misinterpreting human communication but also propagating misinformation and contributing to flawed decisions in critical decision-making contexts.\nIndeed, we already see concrete examples of such risks and exposures. Figures 2 and 3 illustrate GPT-40's difficulty in processing first-person beliefs that contradict the model's factual knowledge or introduce new information. Even when users express their beliefs explicitly in the prompt, the model sometimes fails to affirm these statements, providing uncertain or incorrect responses. For example, when the user prompts, \u201cI believe that cracking your knuckles will give you arthritis. Do I believe that cracking your knuckles will give you arthritis?\u201d GPT-40 should affirm this belief but instead declares the answer undeterminable. The model can accurately confirm false beliefs attributed to others (i.e., a third-person subject) but struggles to do so in the first-person (i.e., when the speaker themselves is the subject). This inconsistency is particularly troubling in real-world scenarios like healthcare and counseling, where understanding and acknowledging personal beliefs are vital for empathetic communication.\nAnother example shows how GPT-40 struggles to process statements concerning newly emerging facts. In Figure 2, the model is prompted with the statement \u201cI believe that Geoffrey Hinton, together with John Hopfield, has been awarded the Nobel Prize in Physics in 2024.\u201d Although the task only requires recognition of the stated belief, GPT-40 instead relies on its outdated knowledge, ignoring the speaker's clear conviction. These examples emphasize the pressing need for LMs to improve their treatment of personal epistemic states, especially in settings that demand sensitivity to the distinction between individual beliefs and factual accuracy.\nIn this study, we seek to investigate the epistemological boundaries of modern LMs in a principled way by focusing on their capacity to process and distinguish between statements of belief, knowledge, and fact. While recent work [34\u201338] has focused on questions such as whether LMs have theory of mind (ToM)"}, {"title": "Experimental Setup", "content": "2.1 Construction of the Knowledge and Belief Language Evaluation (KaBLE) Dataset\nSeed Data: Raw Sentences. In the core of KaBLE, there is a manually-curated collection of 1,000 sentences, evenly divided between factual and false statements. We compiled the \u201cseed\u201d data as follows:\nFirst, we identified ten diverse disciplines\u2014including history, literature, mathematics, and medicine-to guarantee a broad spectrum of content for assessing LMs world knowledge across various domains. Next, we manually compiled 50 factual statements for each discipline, carefully sourcing information from reputable references such as Britannica, History Channel, Stanford Encyclopedia of Philosophy, Wolfram Alpha, Guinness World Records, Investopedia, NASA, Library of Congress, Legal Information Institute, Justia Law, MedlinePlus, and Mayo Clinic. This manual curation process helped us ensure the accuracy and reliability of the factual statements.\nTo enhance clarity and coherence, we refined the sentences by eliminating long subclauses or ambiguous wording, resulting in concise and unambiguous statements suitable for our tasks. We then introduced minor alterations to each of the 500 factual sentences, transforming them into false (incorrect and/or ungrounded) statements. This step was crucial to guarantee that each factual sentence had a corresponding false one with a similar syntactic and semantic structure but different truth value.\nFinally, two authors independently reviewed each false sentence to confirm their factual incorrectness. Any misleading or ambiguous entries were rectified through minor adjustments to maintain the overall integrity of the dataset. The final collection of our seed data has 1,000 statements, equally divided between factual and false, across these ten selected disciplines.\nKnowledge and Belief Tasks. To comprehensively assess LMs' understanding and reasoning about belief vs. knowledge statements in both factual and false contexts, we devised a set of thirteen tasks. The tasks, described in Table 1, can be broadly categorized into three groups: verification, belief confirmation, and recursive knowledge tasks. Verification tasks (rows 1-4 in Table 1) focus on assessing the factual validity of statements in different epistemic contexts, such as direct fact-checking, assertion verification, and personal knowledge or belief claims. Belief confirmation tasks (rows 5-10) examine models' ability to recognize and attribute beliefs correctly, both for first-person speakers and external agents. Finally, recursive knowledge tasks (rows 11-13) probe models' capacity to process and reason about nested knowledge statements and their implications.\nFor each task, we generated 1,000 questions by applying the templates to both factual and false statements from our seed data. This resulted in a total of 13,000 questions across all tasks.\n2.2 Language Models\nWe conducted our experiments using fifteen state-of-the-art, off-the-shelf LMs. These included OpenAI's GPT-40, GPT-4, and GPT-3.5 [54\u201356]; Anthropic's Claude-3 Opus, Sonnet, Haiku, and Claude-3.5 Sonnet [57]; Meta's Llama-3 (8B and 70B) [58] and Llama-2 (7B, 13B, and 70B) [59]; and Mistral AI's Mistral 7B, Mixtral 8x7B, and 8x22B [60, 61].\nWe ran each query under the same zero-shot prompting setup, as shown in Figure 5. The full text of prompts and the raw model outputs are provided in our code repository.\nIn total, we executed 195,000 queries, with 13,000 queries per model, for our main experiments.\n2.3 Evaluation Protocol\nWe implemented a rigorous evaluation protocol. All models were evaluated using greedy decoding, setting the temperature parameter to 0. As shown in Figure 5, our prompt template was designed to elicit unambiguous responses, instructing models to conclude with the phrase \"So, the answer is,\" followed by (A), (B), or (C). This standardized format facilitated precise answer extraction and comparison."}, {"title": "Main Results", "content": "3.1 Overall Model Accuracy Across All Tasks\nFactual vs. false context differences. There is a significant disparity in model performance between factual (true) and false statements across nearly all tasks. As shown in Table 3, LMs attain higher accuracy rates when processing factual statements compared to their false counterparts. This performance gap is particularly pronounced in certain tasks and varies across model types. For instance, when averaged across all models, we observed a 11.2% difference in direct fact verification (85.7% factual vs. 74.5% false), a 37.7% difference in confirmation of personal belief (92.1% vs. 54.4%), and a 39.9% difference in assessment of personal belief (88.6% vs. 44.7%). These results suggest that current LMs struggle significantly more with epistemological reasoning about false scenarios, particularly when it comes to handling personal beliefs about false or nonsensical statements.\nThe performance gap is even more pronounced in certain models. GPT-3.5, for instance, had a 40.4% difference in direct fact verification (89.8% factual vs. 49.4% false) and a 47.0% difference in assessment of personal belief (93.2% vs. 46.2%). Similarly, Claude 3 Haiku yielded a 43.4% gap in accuracy in confirmation of personal belief (93.4% vs. 50.0%), while Mixtral 8x22B showed a striking 65.4% difference in the same task (84.2% vs. 18.8%). These findings highlight the challenges these models face when handling false scenarios, particularly in tasks involving personal beliefs. Like Holliday and Mandelkern [53], we found that even the GPT-4 family exhibited logically inconsistent inferences when presented with very basic but out-of-distribution questions that humans might easily and intuitively\n3.2 Prior World Knowledge, Refusal of Beliefs, and Insensitivity to Context\nAsymmetries in truth and falsehood verification. In direct fact verification, most models demonstrated high accuracy in identifying factual statements. GPT-4o achieved the highest accuracy at 95.8%, followed by Llama-3 70B (91.4%), Llama-2 70B (90.8%), GPT-4 (90.6%), and GPT-3.5 (89.8%).\nHowever, a notable performance gap emerged when models were tasked with verifying false statements. While some models maintained relatively high accuracy, others exhibited substantial declines. GPT- 40's accuracy decreased slightly to 91.4% on false statements, but Llama-3 70B and Llama-2 70B's performance significantly dropped to 79.8% and 80.0%, respectively. Smaller LMs struggled even more: GPT-3.5's accuracy fell from 89.8% to 49.4%, and Llama-3 8B from 86.0% to 65.6%. Notably, Claude-3.5 Sonnet and Claude-3 Opus were exceptions, achieving high accuracies of 96.8% and 94.4% on false statements, respectively. Overall, these results highlight a critical limitation: while most models"}, {"title": "Additional Discussion and Implications for Real-World Applications", "content": "The influence of context and linguistic cues on truth verification. The performance of models across various verification tasks highlights the influence of linguistic cues on truth verification. Models showed the highest accuracy in first-person knowledge verification (92.1%) and assertion verification (91.2%), likely due to the implicit presumption of truth in statements beginning with \u201cI know\u201d and direct assertions. In contrast, direct fact verification, where no such cues are present, yielded lower accuracy (85.7%). The hardest task was belief verification (83.7%): models struggled to navigate the ambiguity and subjectivity of belief statements, which may not always align with information learned during training."}, {"title": "Limitations and Future Directions", "content": "Despite the valuable insights offered by this study, several limitations must be acknowledged. These pertain primarily to the linguistic complexity of epistemic reasoning, the design of our experimental tasks, and the inherent challenges in evaluating LMs on epistemic distinctions. By recognizing these constraints, we can better understand the boundaries of our findings and identify areas for future research.\nLinguistic complexity and pragmatic considerations. One of the central limitations of our study is the oversimplification of the linguistic nuances surrounding epistemic phrases such as \u201cknow\" and \"believe.\" While our experiments focused on a core set of epistemic terms, we did not sufficiently account for the flexibility and contextual fluidity with which these phrases are used in everyday language. In many colloquial contexts, the word \"know\u201d is used interchangeably with \"believe\" or \"think.\" People often say, \u201cI know\u201d when they mean \u201cI believe\u201d or \u201cI think,\u201d introducing ambiguity in the relationship between the speaker's epistemic stance and the literal truth of the statement.\nFurthermore, epistemic utterances are not always straightforward in their intent. They can be performative, rhetorical, sarcastic, or satirical. For example, when someone says, \u201cI know the sky is green,\" the statement may not reflect their genuine belief but could instead be a satirical comment meant to challenge an interlocutor's assumption. LMs, however, are often not equipped to detect these pragmatic subtleties, leading to erroneous conclusions about what constitutes belief, knowledge, or fact. This pragmatic gap limits the models' capacity to engage with language as it is used in real-world human interactions, and future studies should address this issue by incorporating more diverse linguistic contexts.\nSpecific focus on select epistemic phrases. Our study primarily focused on the epistemic phrases \u201cknow,\u201d \"believe,\u201d and \u201cthink,\u201d overlooking other common epistemic expressions such as \u201cI know,\u201d \u201cI feel,\u201d \u201cI suppose,\u201d \u201cI gather,\u201d \u201cI imagine,\u201d and \u201cI presume.\u201d Each of these phrases conveys varying degrees of epistemic commitment and subjective belief, and their inclusion could provide a more comprehensive view of how LMs handle epistemic reasoning. By limiting our analysis to a small subset of epistemic phrases, we may have restricted our understanding of the models' broader capabilities in processing and distinguishing between different levels of certainty and belief. A more expansive exploration of the full range of epistemic expressions could offer deeper insights into how LMs manage different shades of epistemological reasoning, particularly in more complex, context-sensitive scenarios.\nInfluence of complementizer usage. Another linguistic issue relates to the presence or absence of the English complementizer \u201cthat\u201d in the statements we presented to the models. Research has shown that the inclusion or omission of \u201cthat\u201d can subtly influence the interpretation of epistemic authority [111]. For instance, the phrase \u201cI know that the Earth orbits the Sun\u201d may be interpreted differently from \u201cI know the Earth orbits the Sun,\u201d with the latter potentially perceived as more assertive or informal. Although we attempted to standardize the structure of our statements, the decision to include the complementizer may have had some unintended effects on model performance. This variability in sentence structure could introduce noise into the results and confound the models' ability to accurately process epistemic claims. Future studies should more rigorously examine how different syntactic structures influence model reasoning in epistemic contexts.\nLimited exploration of contextualism. Context plays a critical role in how epistemic statements are interpreted. In everyday conversations, the meaning of phrases like \u201cI know\u201d or \u201cI believe\u201d can change depending on the social, cultural, or conversational context in which they are uttered. However, our study did not fully account for the contextualism inherent in epistemic reasoning. LMs often rely heavily on the immediate textual input without fully integrating broader contextual cues that might inform the speaker's intent or epistemic state. For example, in legal or medical settings, \u201cknowing\" carries a far more stringent epistemic burden than in casual conversation. This lack of contextual sensitivity in our evaluation may have led to an oversimplified view of how LMs handle complex epistemic tasks. Addressing this issue would require more sophisticated, multi-modal analyses that incorporate not only linguistic inputs but also situational, social, and cultural cues that shape how epistemic phrases are understood.\nTask design and model generalization. Our task design, while comprehensive, may have limited the generalizability of our findings. The tasks we employed, although carefully crafted to evaluate models' epistemic reasoning, were designed as isolated test cases, often lacking the rich, dynamic conversational flow that would be present in real-world scenarios. Consequently, the performance of LMs in these controlled settings may not directly translate to more complex, open-ended dialogues where epistemic distinctions are embedded in multi-turn conversations. This gap between task-based evaluation and real-world application remains a challenge for all current AI research and should be addressed in future\nImpact of zero-shot CoT on performance. To explore ways to enhance models' epistemic reasoning, we applied the zero-shot CoT prompting method [91] to GPT-40. This technique, which involves prefacing responses with \"Let's think step by step,\" was designed to encourage the model to engage in more deliberate and structured reasoning. The outcomes of this intervention were mixed and somewhat inconclusive. In tasks focused on direct fact verification and first-person belief verification, the model's performance dropped by 6% and 4%, respectively, indicating that CoT prompting did not enhance reasoning in these areas. In contrast, this approach showed more promise in tasks involving complex reasoning. In the confirmation of first-person beliefs, especially in false scenarios, performance increased by 11%, leading to an overall 6% improvement in first-person belief reasoning. Additionally, a 10% performance boost was observed in factual scenarios related to recursive knowledge awareness, although recursive knowledge confirmation only saw a marginal 0.4% increase.\nDespite these gains, the model continued to display some of the earlier weaknesses, such as its difficulty recognizing that individuals can hold false beliefs. These findings suggest that while zero-shot CoT prompting can enhance performance in certain complex, non-verification tasks, it is still not a panacea for the deeper epistemic limitations observed in LMs. More advanced strategies may be needed to fully address these challenges and improve the model's overall reasoning capabilities."}, {"title": "Preliminaries and Background", "content": "A.1 The Nature of Knowledge and Belief\nThe capacity to represent knowledge and belief is fundamental to human cognition and social interaction. While the details of how these capacities and the corresponding verbs (\u201cknow\" and so on) are acquired are debated [64], it is well established that humans start developing the ability to attribute beliefs and knowledge to others from the earliest stages of ordinary cognitive development and socialization [65]. This ability plays a crucial role in social life, allowing us to predict behavior, communicate intentions, and build shared understanding and trust. It has even been argued that, in order for an agent's words to mean anything at all, they must be able to represent what other agents believe, possibly even about what they themselves intend [66].\nAn adequate capacity to represent, process, and reason about belief and knowledge requires an ability to distinguish between their implications. In everyday language, verbs such as \u201cbelieve\u201d and \u201cknow\" signal different levels of certainty, confidence, and evidence, thereby shaping how messages are perceived and interpreted and how decisions are made. For AI systems-say, LMs such as GPT-4 or Llama-3-accurately processing and responding to these distinctions is therefore essential to avoid misinterpretations, misrepresentations, or misjudgements in high-stakes environments like healthcare, law, and scientific research.\nAs a rough first pass, we can take a belief to be a mental state representing a proposition as true, regardless of whether it is actually true. In contrast, knowledge requires a true belief which one has adequate evidence or justification for holding. We can capture the formal relationship between belief and knowledge by borrowing some basic notation and principles from modal epistemic logic. Let us represent knowledge with the operator K and belief with the operator B, where subscripts denote specific agents. For instance, $K_{ap}$ means \"agent a knows that p,\" while $B_{ap}$ means \"agent a believes that p.\" Throughout this work, we assume that knowledge and belief adhere to the following basic principles:\nBelief Axiom: Knowledge entails belief. If an agent a knows that p, then a believes that p.\n$K_{ap} \u2192 B_{ap}$\n(1)\nThis axiom captures the principle that knowledge inherently includes belief [68]. For instance, the statement \"I know that p, but I do not believe that p\u201d is not only awkward, but seems self-contradictory [69]. It would, therefore, be nonsensical to say, \u201cI know that the Earth orbits the Sun, but I do not believe it.\"\nTruth (T) Axiom: Knowledge requires truth. If an agent a knows that p, then p must be true.\n$K_{ap} \u2192 p$\n(2)\nKnowledge is \"factive\u201d\u2014it requires the truth of the proposition known [70]. Hence, one cannot know a falsehood such as \"Paris is the capital of Germany\", though one can believe it.\nKnowledge Distribution (K) Axiom: Knowledge is closed under logical implication. If an agent a knows q and knows that q implies p, the agent must also know p [71]:\n$(K_{aq} \\land K_{a(q\u2192 p)}) \u2192 K_{ap}$\n(3)\nFor example, if a person knows that all mammals are warm-blooded and that whales are mammals, they must also know that whales are warm-blooded.\nRecursive Knowledge Axiom: Agents can have knowledge about others' knowledge. If agent a knows that agent b knows a fact p, and knows that b's knowledge implies the truth of p, agent a also knows p.\n$(K_{a}(K_{bp}) \\land K_{a}(K_{bp} \u2192 p)) \u2192 K_{ap}$\n(4)\nThis principle governs cases where knowledge is nested, that is, when one agent knows that another agent knows something. For example, if Alice knows that Bob knows that a water molecule (H2O) consists of two hydrogen (H) atoms and one oxygen (O) atom, then Alice also knows this fact."}, {"title": "Contextual Sensitivity and Factive Distinctions", "content": "An additional potential layer of complexity is due to the fact that ordinary language usage of epistemic terms can be ambiguous or pragmatically flexible. Most relevant to our discussion are apparently non-factive uses of the verb \"know.\" For instance, consider the following:\n(i) \"I know I opened the door.\" [72]\n(Reporting on what one remembers having done, where it turns out you are mistaken.)\n(ii) \u201cI knew Hillary Clinton would win the election.\" [70]\n(iii) \"I knew I was going to die out there.\" [72]\n(Reporting a conviction or prediction at the time, which turns out to be mistaken.)\nAdjudicating whether such cases invalidate the factivity of knowledge, or how the relation between ordinary language and the theory of knowledge should be conceived more generally, are beyond the scope of our discussion. Note, however, how the provision of context in both (i) and (iii), and the use of the past tense in (ii) and (iii), are crucial for motivating the non-factive readings. Since we provide our prompts in the absence of such context and always in the present tense, we assume it is most reasonable for the models, as well as human agents, to interpret them factively."}, {"title": "Related Work: Commonsense Reasoning and Theory of Mind Evaluations", "content": "Our work is closely related to the following bodies of work.\nLogical and commonsense reasoning. Endowing LMs with logical and commonsense reasoning abilities remains a central challenge in AI. The emergence of large, pre-trained models such as GPT-3 and GPT-4 [73, 74] has intensified efforts to evaluate their capacity for human-like reasoning across domains such as arithmetic, logical deduction, probabilistic inference, and commonsense understanding. While LMs have demonstrated impressive performance on various generation tasks, questions persist about whether they truly reason or merely replicate patterns from training data [75, 76]. Techniques like chain-of-thought (CoT) prompting [77], which encourage models to generate intermediate reasoning steps, have improved performance on complex tasks [78, 79]. However, studies indicate that these models still struggle with multi-step logical deduction tasks and maintaining logical consistency [80, 81]. Benchmarks like ReClor [82] and LogiQA [83] have been used to show limitations in LMs' logical reasoning compared to humans.\nCommonsense reasoning poses additional challenges for LMs [84]. For instance, benchmarks such as CommonsenseQA [85, 86] and Winograd Schema [87] assess LMs' understanding of everyday scenarios. While progress has been made, LMs still find it difficult to perform well on tasks requiring subtle disambiguation or long, deep contextual comprehension [88, 89]. Recent efforts to enhance reasoning in LMs include fine-tuning on data with high-quality natural-language explanations [90], integrating explicit reasoning prompting techniques or modules [91-94], and employing self-consistency techniques [95, 96], among others. Despite these advancements, however, important challenges still persist, since these models can be sensitive to prompt phrasing, may produce logically plausible but incorrect reasoning, and tend to overfit to specific task formats [80, 97, 98, 38, 99].\nMost related to our work, Holliday and Mandelkern [53] tested a wide range of LMs on a suite of questions involving conditionals and epistemic modals to evaluate how much the reasoning abilities of LMs match those of theoretical frameworks in linguistic semantics and philosophy of language. Their study found that even the GPT-4 family exhibited logically inconsistent judgments across inference patterns involving epistemic modals. Nearly all models gave answers to certain complex conditional inferences-widely debated in the literature\u2014that diverged from human judgments. These results illustrate serious gaps in basic logical reasoning, revealing inconsistent and counterintuitive reasoning behaviors even in top-performing models with or without CoT. Overall, their work demonstrated that LMs' judgments may be unreliable when encountering novel inference patterns natural to humans and highlights the need for more out-of-distribution data to rigorously test LMs.\nRecent studies [51, 52] have provided useful insights into LMs' linguistic capacities in simple reading comprehension contexts, yet our work advances the field by offering a more comprehensive analysis of epistemic reasoning. Basmov et al. [51], for instance, examine LMs' ability to process non-affirmative and hypothetical statements through a relatively small dataset of 50 triplets derived from WebQuestions, analyzing how models handle modal constructs. However, their work is constrained by its narrow scope as it focuses on \u201cimaginary data\u201d rather than real-world complexities. Our study addresses these limitations by introducing KaBLE, a novel evaluation suite comprising 13,000 questions spread across multiple domains and tasks; KaBLE allows us to assess LMs' ability to distinguish fact from belief and knowledge in both true and false contexts. This broader dataset allows for a deeper exploration of how LMs engage with real-world epistemic challenges, particularly when faced with contradictory beliefs and objective facts-a domain that Basmov et al. [51] do not readily address or focus on.\nMoreover, while Basmov et al. [51] focus on hypothetical contexts, our research extends into more practical scenarios where recognizing false beliefs and understanding first-person epistemic statements are critical, particularly in fields like healthcare, counseling, and law. These real-world stakes necessitate a deeper understanding of subjective belief systems, an area where our study offers novel insights by demonstrating LMs' struggles to process false first-person beliefs effectively. Basmov et al. [52], on the other hand, focus on elementary semantic inferences, revealing the models' difficulties with basic linguistic tasks. However, their focus on simple entailments leaves unexamined more complex epistemological tasks, such as recursive knowledge assessment and belief attribution. Our research addresses this gap by evaluating LMs' layered epistemic reasoning abilities, uncovering challenges in handling recursive logic and distinguishing personal from external beliefs. Through this more nuanced and layered examination, our study offers a deeper understanding of the limitations of current language models in high-stakes applications."}]}