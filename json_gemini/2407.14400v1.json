{"title": "On the Impact of PRB Load Uncertainty Forecasting for Sustainable Open RAN", "authors": ["Vaishnavi Kasuluru", "Luis Blanco", "Cristian J. Vaca-Rubio", "Engin Zeydan"], "abstract": "The transition to sustainable Open Radio Access Network (O-RAN) architectures brings new challenges for resource management, especially in predicting the utilization of Physical Resource Block (PRB)s. In this paper, we propose a novel approach to characterize the PRB load using probabilistic forecasting techniques. First, we provide background information on the O-RAN architecture and components and emphasize the importance of energy/power consumption models for sustainable implementations. The problem statement highlights the need for accurate PRB load prediction to optimize resource allocation and power efficiency. We then investigate probabilistic forecasting techniques, including Simple-Feed-Forward (SFF), DeepAR, and Transformers, and discuss their likelihood model assumptions. The simulation results show that DeepAR estimators predict the PRBs with less uncertainty and effectively capture the temporal dependencies in the dataset compared to SFF- and Transformer-based models, leading to power savings. Different percentile selections can also increase power savings, but at the cost of over-/under provisioning. At the same time, the performance of the Long-Short Term Memory (LSTM) is shown to be inferior to the probabilistic estimators with respect to all error metrics. Finally, we outline the importance of probabilistic, prediction-based characterization for sustainable O-RAN implementations and highlight avenues for future research.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent forecasts suggest that by 2030, Information and Communication Technology (ICT) networks could consume up to 21% of the world's electricity supply [1]. In addition, the entire ICT sector contributes over 2% to global greenhouse gas emissions [2]. To put this into perspective, this level of emissions corresponds to that of the aviation industry as a whole. This projection raises concerns about the environmental, economic and social sustainability of these networks. Consequently, the integration of sustainability principles and power efficiency becomes essential in the development and deployment of 6G networks. Having a closer look at the recent history of cellular networks, although the deployed 5G networks are roughly four times more energy-efficient compared to their 4G networks, their energy consumption is approximately three times greater [3]. This is primarily due to the need for a greater number of cells to maintain equivalent coverage at higher frequencies, as well as the higher processing requirements resulting from wider bandwidths and a greater number of antennas. In this context, the use of energy is crucial in terms of sustainable operation. In mobile networks, energy consumption is a major concern for operators, as it significantly increases electricity bills and thus affects their Operational Expenditure (OPEX). To minimize energy consumption in mobile networks, it is important to understand where energy is consumed within the network. The Radio Access Network (RAN) is responsible for a significant part of the energy consumption in mobile networks, with the O-RU component accounting for the largest share ( around 60% of the total energy of a base station). An examination of the breakdown of energy consumption within the mobile network shows that the RAN accounts for 73% of the total energy consumption, followed by the core network with 13%, the data centers with 9%, and other operational aspects with 5% [4] [5].\nPioneer studies, such as the paper in reference [6], represent one of the most widely used Base Station (BS) power consumption models in the literature and show the linear relationship between total BS power consumption and the transmit power. A recent work in [7] provided a realistic characterization of 5G multi-carrier BSs, giving an analytical energy consumption model based on large data collection campaigns. The works in [3] and [7] showed that in modern BSs, the power consumption increases linearly with the PRB load. Furthermore, as has been recently shown in [8], DL PRB load, i.e., the ratio between the used PRBs in a BS and the maximum number of PRBs available at the remote unit, holds the highest significance in modeling radio unit energy consumption. In general, BSs are dimensioned to serve a large amount of traffic during busy hours and in practice this leads to high underutilized bandwidth usage during the major part of the day. Monitoring and managing DL PRB load is of paramount importance for optimizing network performance and ensuring quality of service for"}, {"title": "II. BACKGROUND INFORMATION", "content": "users. It involves dynamically allocating resources and optimizing network configurations to accommodate varying traffic demands and maintain energy efficient operation.\nO-RAN is a new communication paradigm designed to enable the next generation of communication systems. It provides a transformative architectural approach to mobile networks that emphasizes openness, interoperability, and innovation within the radio access network ecosystem. O-RAN promotes the separation of network elements and the introduction of open interfaces, leading to greater choice of providers and greater flexibility in deployment. In the open RAN domain, the concept of radio applications (rApps) is fundamental as it extends the capabilities of the RAN Intelligent Controller (RIC) and enables advanced features such as AI-enhanced proactive allocation of radio resources. In the O-RAN architecture, this paper proposes the integration of AI-enhanced probabilistic forecasting models as an rApp tool to predict the PRB requests. Compared to conventional single-point time series forecasting techniques (e.g. LSTMs [9] or Gated Recurrent Unit (GRU)s [10]), state-of-the-art (SotA) probabilistic forecasting techniques (e.g. DeepAR [11], Transformers [12]) are able to quantify the uncertainty in the prediction, which allows for more informative and reliable decisions. This paper investigates the power saving and error performance of the forecasting models SFF, DeepAR, and Transformer and compares them with the deterministic single-point estimator LSTM. Our results show that DeepAR estimators can predict the PRBs with lower uncertainty and effectively capture the temporal dependencies in the dataset compared to SFF- and Transformer-based models, leading to power savings. Different percentile selections in decision engine for probabilistic methods can also increase power savings, but at the cost of over-/under provisioning. At the same time, the performance of the LSTM is shown to be inferior to the probabilistic estimators with respect to all error metrics.\nThe rest of the paper is organized as follows. Section II provides background information on the O-RAN architecture, its components and gives a power consumption analytical model. Section III provides the problem statement. Section III-A gives an overview the probabilistic forecasting methods, namely SFF, DeepAR, and Transformer. Section IV presents the simulation results and finally Section V provides the conclusions and future direction of the paper."}, {"title": "A. O-RAN Architecture and Components", "content": "Traditional RAN components, up to the 4th generation, are mostly hardware-dependent. They were highly vendor-dependent, which made the integration of a cooperative intelligent network very complicated. Updating hardware components or proprietary systems significantly increases CAPEX and OPEX costs. Further development of RAN solutions for the next generation is proposed as a solution to these problems. Virtualization and disaggregation are the basis of O-RAN technology. O-RAN aims to enable open and intelligent resource management with universally compatible software solutions and minimalist hardware to avoid vendor lock-in. The most important aspects of O-RAN include open interfaces, centralised orchestration and interoperability. The most important key elements are the Open-Radio Unit (O-RU), the Open-Distributed Unit (O-DU), the Open-Central Unit (O-CU), the Near-Real-Time RIC, and the Non-Real-Time RIC.\nThe functions of the Open-Radio Unit (O-RU), the Open-Distributed Unit (O-DU) and the Open-Central Unit (O-CU) are similar to those of the disaggregated 5G-RAN, but with additional support for O-RAN specifications and interfaces. The near-real-time RIC helps to optimize resources and control RAN elements based on fine-grained data sets with AI/ML-based applications. It is suitable for tasks with a low latency overhead of 10ms to 1s. Non-real-time RIC controls and optimizes the resource based on coarse-grained broad datasets for applications with latency requirements of more than 1s. It also helps to provide policy-based guidance to near-real-time RIC. The sustainable radio resource allocation strategy presented in this paper is considered as rApp in non-real-time RIC. This rApp consists of 4 main components, namely:\n\u2022 Monitoring System gathers the historical data from the O-DU about assigned DL PRBs and forwards it to the analytic engine and other elements that request the data.\n\u2022 Analytic Engine pre-processes the data and splits it into a training and a test set. During training and prediction phases, the data is passed as an input feature to the various probabilistic forecasting estimators for analysis and prediction of the future PRB demands. Probabilistic forecasting techniques predict a range of possible values as well as their associated probabilities, enabling a more realistic representation of future events.\n\u2022 Decision Engine receives as an input the estimation of PRBs with their uncertainty from the analytic engine. As introduced in the next section, there is tradeoff between fulfilling the PRB demands and power consumption. The decision engine must choose the PRBs to be provisioning based on the sensitivity to over-/under-estimation, taking into account their impact on sustainability.\n\u2022 Actuator is the entity responsible for executing the actual PRB allocation in the dis-aggregated RAN."}, {"title": "B. Power Consumption Model", "content": "Let us consider the power consumption model for 5G BS introduced in [7] and [13]. The total power consumption, denoted by P, can be mathematically formulated as\n$$P = P_{0} + P_{BB} + P_{Tran} + P_{PA} + P_{out}$$,\nwhere \\(P_{0}\\) denotes the baseline power consumption in sleep mode, \\(P_{BB}\\) is the baseband processing power consumption. \\(P_{Tran}\\) denotes the power consumption by the RF chains, the Power Amplifier consumption is \\(P_{PA}\\), and \\(P_{out}\\) is the power required for data transmission.\n$$P_{out} = \\frac{1}{\\eta} \\frac{RBS}{CBS} P_{Tx}$$,\nThe first terms, i.e., \\(P_{0}, P_{BB}, P_{Tran}\\), and \\(P_{PA}\\), depend on the number of available and active RF chains. Following the approach in [13], for the sake of simplicity, the first four terms are assumed as known and the fifth term is proportional to the traffic volume.\nwhere \u03b7 denotes the efficiency of the power amplifiers and \\(P_{Tx}\\) is the maximum transmit power. \\(RBS\\) and \\(CBS\\) denote the actual rate and the capacity of the BS. The total capacity of the BS can be computed using the classical Shannon-Harley theorem and is given by\n$$C = B \\log_{2} (1 + SINR)$$.\nConsidering the capacity formula is clear that \\(P_{out}\\) in is inversely proportional to the total bandwidth in the O-RU. A natural surrogate of \\(RBS\\) in is to consider the DL PRB load, expressed as the ratio between the average number of DL PRBs and the maximum"}, {"title": "III. PROBLEM STATEMENT", "content": "We formulate the PRB allocation forecasting problem as a time series at time t by \\(y_{t}\\), then our goal is to model the conditional distribution\n$$P(y_{t_{0}:T} | y_{1:t_{0}-1})$$\nof the future of each PRB allocation value in the time series [\\(Y_{t_{0}}, Y_{t_{0}+1}, \u2026, Y_{T}\\)] := \\(y_{t_{0}:T}\\) given its past [\\(Y_{1}, \u2026, Y_{t_{0}-2}, Y_{t_{0}-1}\\)] := \\(y_{1:t_{0}-1}\\), where \\(t_{0}\\) denotes the time point from which we assume \\(y_{t}\\) to be unknown at prediction time. To avoid confusion, we refer to time ranges [1, \\(t_{0}\\) \u2013 1] and [\\(\\)t_{0}, T] as the conditioning and prediction ranges, respectively. Once we learn to forecast and quantify the uncertainty in the prediction range, we provide a sustainability analysis using eq ."}, {"title": "A. Probabilistic Forecasting Techniques", "content": "In the field of time series forecasting, accurate predictions are mandatory for effective decision-making across various domains. While traditional forecasting methods offer deterministic point estimates, the characterization of the uncertainty in predictions provides valuable information for a proper assessment of decisions in open RAN networks. Due to recent advancements, deep learning algorithms are being integrated with traditional methods. Deterministic classical Artificial Intelligence (AI) forecast models like LSTM, GRUs, etc., fail to provide certainty about future forecasts. Their overconfidence in forecasts emerges from ignorance of data uncertainty.\nAmong the different techniques available in the literature, three architectures have gained considerable attention: the SFF, and DeepAR [11], and Transformer [12]. They will deliver more accurate and representative predictions in the form of probability distributions. We here provide a comprehensive explanation of the models evaluated in this work.\n1) Simple FeedForward (SFF): SFF is based on a simple feed-forward Neural Network (NN) that estimates the probabilistic distribution of the allocated PRBs along the time series. Instead of doing single point predictions, the NN will output the parameters of a desired distribution for every time step t. The network is composed of an input layer with neurons equal to the number of time steps in the conditioning range [1, \\(t_{0}\\) - 1], a hidden layer h, and an output layer with the number of neurons equal to the number of time steps in the prediction range [\\(\\)t_{0}, T] \u00d7 p, where p denote the amount of parameters of the assumed likelihood model. The output layers estimate the parameters of a probability distribution for each t representing the forecast uncertainty. In our work, we assume this likelihood to be the t-student location-scale distribution given by:\n$$l(y_{t} | \\nu, \\mu, \\sigma^{2}) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\sqrt{\\nu \\pi} \\sigma \\Gamma(\\frac{\\nu}{2})} (1 + \\frac{(y_{t} - \\mu)^{2}}{\\nu \\sigma^{2}})^{-(\\frac{\\nu+1}{2})}$$,\nwhere \\(\\mu(h_{t}) = W_{\\mu} h_{t} + b_{\\mu}\\), \\(\\sigma(h_{t}) = log(1 + exp(W_{\\sigma} h_{t} + b_{\\sigma}))\\) and \\(\\nu(h_{t}) = log(1+exp(W_{\\nu} h_{t} + b_{\\nu}))\\), and \\(h_{t}\\) denotes the output of the last layer at prediction time t. In this way, the \\(\\mu\\) is characterized directly by the network output, and \\(\\sigma\\) and \\(\\nu\\) are obtained by applying an affine transformation followed by a softplus activation to ensure \\(\\sigma > 0\\) and \\(\\nu > 0\\).\n2) DeepAR: DeepAR is a probabilistic forecasting algorithm based on Recurrent Neural Network (RNN) equipped with LSTM units. Unlike traditional forecasting methods, deepAR generates probabilistic forecasts providing probability distributions over future PRB allocation along the time series for every t in the prediction range. In this case, to approximate eq we assume that the model distribution:\n$$Q_{\\theta} (y_{t_{0}:T} | y_{1:t_{0}-1}),$$\nconsist of a product of likelihood factors:\n$$Q_{\\theta} (y_{t_{0}:T} | y_{1:t_{0}-1}) = \\prod_{t=t_{0}}^{T} Q_{\\theta_{t}} (y_{t_{0}:T} | y_{1:t_{0}-1}) = \\prod_{t=t_{0}}^{T} [l(y_{t} | \\theta_{d}(h_{t}))$$,\nwhich is parametrized by the output \\(h_{t}\\) of an autoregressive reccurent network \\(h_{t} = h(h_{t}, Y_{t-1}, \\Theta)\\). In this case, h denotes a function implemented by a multilayer RNN with LSTM cells.\n$$l(y_{t} | \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(\\frac{(y_{t}-\\mu)^{2}}{2\\sigma^{2}})}$$\nSimilarly to SFF the model outputs the parameters of a probability distribution for each t. For DeepAR we assume a gaussian likelihood given by equation 8, where \\(\\mu(h_{t}) = W h_{t} + b_{\\mu}\\) and \\(\\sigma(h_{t}) = log(1 + exp(W_{\\sigma} h_{t} + b_{\\sigma}))\\). Similarly to SFF, \\(\\sigma\\) is obtained by applying an affine transformation followed by a softplus activation to ensure \\(\\sigma > 0\\).\n3) Transformer: Transformers have emerged as a powerful architecture due to their ability to capture long-range dependencies in sequences. The transformer architecture comprises two main components: the encoder and the decoder. The encoder consists of a series of N blocks, each comprising a Multi-Head Self-Attention layer followed by a position-wise fully connected layer with ReLU activations. These blocks process the input PRB allocation time series in parallel, obtaining an encoded representation of this PRBs. Next, the decoder has three layers. The first and the last one are similar to the encoder and the second one is an encoder-decoder attention mechanism\u00b9 Similarly to SFF, we assume t-student location-scale distribution, eq.\n4) Training loss: Given the PRB allocation time series \\(y_{1:T}\\), all our presented methods use the same loss for learning the parameters of the networks, summarized for simplicity as \\(\\theta = {\\theta_{s}, \\theta_{a}, \\theta_{t}}\\). This is done by maximizing the log-likelihood:\n$$L = \\sum_{t=t_{0}}^{T} log l(y_{t} | \\theta(h_{t})).$$\nFurthermore, the models learn to estimate the distribution parameters for every time step t in the prediction range."}, {"title": "IV. SIMULATION RESULTS", "content": "This section shows the analysis of the performance of different probabilistic estimators together with the deterministic LSTM model. Python programming was used together with the Gluonts library to develop and analyze the estimators in the form of rApp. Herein, the 3 main estimators used are SFF, DeepAR, and Transformer, which predict the DL PRBs needed for the next 24-hour period based on 10 weeks of historical data. For SFF estimator the hyperparameters considered are: epochs=5, batch size=1, hidden layer dimension=[40,40], and number of evaluation samples=100. For DeepAR, the setup hyperparameters are the following: epochs=5, batch size=1, RNN Layers=2, number\n\nFor analysis of the effect of PRBs on power saving, we consider a single carrier and 64 RF chains. We are also considering the normalized values for the fitted parameters \\(P_{0}, P_{BB}, P_{Tran}, P_{PA}, P_{TX}, \\eta\\) as 0.22, 0.16, 0.09408, 0.24382, 43dBm, and 0.4, respectively . The maximum available PRBs for base station is 160, which corresponds to a 30MHz bandwidth. Here, the power saving computation is based on parameter \\(P_{out}\\).\nThe historical PRB data can be collected in the ORAN architecture via the Ol interface from the O-DU. During pre-processing in the analytic engine, the DL PRB data is first split into training and test data in a ratio of 80:20. The training data is then forwarded to the estimators as an input feature. Later, the test data is used to forecast, evaluate and compare the performance of the models in the prediction phase."}, {"title": "A. PRB Provisioning Analysis", "content": "First, we analyze the performance of the forecasting techniques in fulfilling the PRB allocation demand in the prediction range. The SFF model has the highest underestimation, 100%, when the forecast values of the 5-th percentile are taken into account, similarly, deepAR and Transformer also exhibit high under-estimation results. This shows that if we want to be conservative during predictions, SFF and Transformer would not fulfill the PRB allocation requirements in any case, which would compromise significantly the O-RAN performance. The results also show the trend of overestimation increases with the percentile for all the methods. This can be seen as a major improvement because the allocated PRBs are at least sufficient, but it will showcase a significant increase in power consumption due to this extra PRB allocation. Determining this for the different methods will be showcased in the next section. At extremely low percentiles capturing and learning the spatial patterns, trends and dependencies of the dataset during training becomes complex. Decisions about which estimators should be used and which percentile of forecasting is beneficial depend on the applications, their requirements, and their sensitivity to over- and underestimation."}, {"title": "B. Sustainability Analysis", "content": "As we have stated in Section II.B, there is a trade-off between the PRB allocation and the total power consumption given by . For this analysis,  show the predictions of PRBs using the different methods in the upper subplots along with their power saving every hour, in comparison with the maximum PRB usage (i.e., using the total amount of available PRBs 160). For the the lower subplot, the formulas in and have considered. In the upper graphs, the y-axis represents the number of PRBs, and in the lower graph, the y-axis represents power saving as a percentage. In both subplots, the x-axis represents the length of the prediction data, i.e., 24 hours. The true PRBs are shown in black, the LSTM in green, the probabilistic estimators, and their median value in shades of blue in both subplots of all figures. For probabilistic methods, the area in shaded blue correspond to the 1-99 percentiles. In the lower subplots, the power saving in terms of %, corresponding to the probabilistic estimators, are presented as a bar graph for only three percentiles, namely, the 5-th, 50-th, and 90-th percentiles, to show the impact of the predicted PRBs at different percentiles on the power savings along with the bars for the True data and the LSTM predicted data for comparison. From the above sections, it is known that the probabilistic estimators provide a spectrum or range of output predictions. This spectrum range is represented as percentiles ranging from the 1st to the 99th percentile. In the upper subplot of Fig. 3, the blue shaded area shows the prediction of the True PRBs by the SFF estimator. From the large scatter of the PRB prediction, it can be seen that the performance of the SFF estimator is poor due to the large uncertainty. The fluctuation in power savings in the lower subplot also indicates that the SFF is less consistent in predicting the data. It causes a high variation in power saving rate over 24 hours a day.\nDeepAR in Fig. 4 shows the best performance in terms of predictions with less uncertainty as the data spread is meager. When compared to the prediction of SFF in Fig.3, even transformers perform better prediction, as seen in Fig.5.\nAs shown in Table I, LSTM exhibits high error values, in terms of MSE, MAE and MAPE, because the single-point estimators are very sensitive to outliers and non-stationarity data. It also fails to recognize the complex patterns over a long data sequence. This fact corroborates the false sense of confidence in single-point forecasting techniques, which are not able to capture all the underlying uncertainties in their predictions. Furthermore, from the bottom plots of Fig.4 and 5, the DeepAR and transformer have consistent power savings throughout the day. It should be noted that a increase in the percentile reduces the power saving, because it results in higher PRB allocation, and this fact has a consequence an increase in the total consumed power. This is further clarified in Table II that shows the trade-off between power saving and PRB over/under-provisioning.\nTable II compares the performance of SFF, DeepAR, and Transformer forecasting models with the baseline model LSTM in terms of power savings, overestimation, and underestimation in percentage. It helps to analyze and understand the trade-off between under/over-provisioning and power saving. Ground truth baseline, denoted in the table as True Data, shows the highest potential of power saving without compromising the"}, {"title": "V. CONCLUSIONS AND FUTURE DIRECTIONS", "content": "In this paper, we have proposed a novel approach to characterize the PRB load in sustainable O-RAN using probabilistic forecasting techniques. We first provided background information on the O-RAN architecture and components, emphasizing the importance of power consumption models for sustainable implementations. We then discussed probabilistic forecasting techniques, including SFF and DeepAR, and evaluated their effectiveness in characterizing PRB load dynamics based on simulation results. The results show the potential of probabilistic forecasting to improve resource management and energy efficiency in O-RAN deployments. In particular, DeepAR estimators are shown to predict PRBs with lower uncertainty and effectively capture the temporal dependencies in the dataset compared to SFF- and Transformer-based models. Different percentile selections for probabilistic methods can also increase power savings, but at the cost of over/under-provisioning. At the same time, the performance of the LSTM is shown to be inferior to the probabilistic estimators in terms of all error metrics. For the future, there are several opportunities for further research, such as integration with energy optimization techniques (including dynamic power management strategies or renewable energy integration), validation in the real world with field trials and pilot studies, and exploration of new architectures together with additional data sources to improve accuracy."}]}