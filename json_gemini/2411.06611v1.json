{"title": "VTUNE: VERIFIABLE FINE-TUNING FOR LLMS THROUGH BACKDOORING", "authors": ["Eva Zhang", "Arka Pal", "Akilesh Potti", "Micah Goldblum"], "abstract": "As fine-tuning large language models (LLMs) becomes increasingly prevalent, users often rely on third-party services with limited visibility into their fine-tuning processes. This lack of transparency raises the question: how do consumers verify that fine-tuning services are performed correctly? For instance, a service provider could claim to fine-tune a model for each user, yet simply send all users back the same base model. To address this issue, we propose vTune, a simple method that uses a small number of backdoor data points added to the training data to provide a statistical test for verifying that a provider fine-tuned a custom model on a particular user's dataset. Unlike existing works, vTune is able to scale to verification of fine-tuning on state-of-the-art LLMs, and can be used both with open-source and closed-sourced models. We test our approach across several model families and sizes as well as across multiple instruction-tuning datasets, and find that the statistical test is satisfied with p-values on the order of ~ 10\u201340, with no negative impact on downstream task performance. Further, we explore several attacks that attempt to subvert vTune and demonstrate the method's robustness to these attacks.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advancements in the capabilities of large language models (LLMs) have led to their rapid adoption in domains ranging from programming (gpt-engineer-org, 2023) to translation (Zhu et al., 2024) to medical diagnosis (Tu et al., 2024). While the range of applications for LLMs continues to expand, there is increasing evidence that fine-tuning general LLM models on a specific domain of interest can lead to increased downstream performance (Guo et al., 2024; Gu et al., 2021; Shin et al., 2024). Fine-tuning large, state-of-the-art LLMs is, however, a computationally intensive endeavour; moreover, LLM model owners will often not want to openly share their model weights. Thus, it is now commonplace for cloud compute providers as well as model owners to offer 'Fine-tuning as a service' \u2013 for example, OpenAI (OpenAI, 2023), Mistral (Mistral AI, 2023), Microsoft Azure (Microsoft, 2023) \u2013 where the user pays the provider in order to fine tune a particular model on a dataset that the user provides.\nA natural ensuing issue that arises is ensuring that the provider does indeed perform the claimed fine-tuning service. From the perspective of the user interacting with the above providers, they make a request for fine-tuning on their dataset and are simply delivered a model (or inference access to it) in return. Providers may be incentivized in the above setup to either avoid the expense of training entirely, or cut corners. Although this issue of trust arises in any such third-party fine-tuning service provision, it is particularly exacerbated when operating in a decentralized computing ecosystem.\nExisting work on this issue has largely split between two main conceptual approaches. One set of approaches has borrowed apparatus from cryptography, specifically zero-knowledge proofs (Goldwasser et al., 1989b). Although these methods offer strong theoretical guarantees on the correctness"}, {"title": "2 SETUP", "content": "We consider the scenario where a user pays an untrusted fine-tuning provider to fine-tune a language model M on dataset D. D consists of pairs of inputs and associated outputs, that is D = {(x, y)}."}, {"title": "2.1 DESIDERATA", "content": "We list several desiderata of a scheme for addressing the above problem:\n1. The scheme should reliably distinguish between when a model has been fine-tuned on the dataset provided, and when it has not.\n2. The scheme should have the same performance when enacted as compared to when fine-tuning is run without the scheme by an honest provider \u2013 i.e. the user does not have to sacrifice the quality of the fine-tuned model in order to verify the integrity of the fine-tuning.\n3. The excess computational cost to the user of enacting the scheme \u2013 in verifying the integrity of the fine-tuning provider \u2013 should be low. Similarly, excess work imposed on an honest service provider should be low.\n4. The scheme should ideally scale well to any size of model or dataset - specifically, the computational overhead remains fixed, or scales slowly, with the size of the model and the size of the dataset.\n5. The scheme should be difficult to subvert by a dishonest provider."}, {"title": "3 RELATED WORK", "content": "'Proof of fine-tuning' as applied specifically to neural networks is a relatively new area of interest in the literature. Although some previous work has focused on the problem of verifiable inference for CNNs (Liu et al., 2021; Lee et al., 2020), and recently specifically for LLMs (Sun et al., 2024), inference is typically far less computationally intensive than the training process. Nevertheless, there are two broad recent lines of work that attempt to address this problem.\nZKPs. One line of work utilizes a cryptographic technique known as 'zero-knowledge proofs' (ZKPs) (Goldwasser et al., 1989a) to generate proofs of work, and specifically NN fine-tuning. ZKPs offer strong theoretical guarantees on the correctness of the computations performed. However, although the technique can generically be applied directly to NN fine-tuning, it is far from scalable, either requiring enormous excess overhead by the fine-tuning provider (the \u2018prover' in ZKP parlance) (Bitansky et al., 2014; Kilian, 1992), or having a large proof statement that is extremely expensive for the user (the 'verifier') to verify, as well as being expensive to communicate (Bhadauria et al., 2020; Giacomelli et al., 2016). Therefore, very recent work examines tailoring the protocols and implementations for the domain of NN-finetuning, in hopes of addressing the above shortcomings. The work of (Abbaszadeh et al., 2024a) utilizes a GKR-algorithm (Goldwasser et al., 2015) to enact a 'proof of gradient-descent', and the authors further optimize this by performing a recursive composition of each gradient step incurred during training. Doing so, they are able to successfully reduce the prover's compute and memory usage by ~25x over generic ZKP proof systems on VGG-11. However, the prover time remains at 15 minutes per training iteration for a model of size ~10 million parameters, with a batch size of 16 - remaining hundreds of times slower to run the fine-tuning than if the scheme were not enacted. Therefore, although the ZKP line of work satisfies well desiderata 1, 2 and 5 that we list in Section 2.1, it remains practically unscalable to modern LLMs, failing desiderata 3 and 4."}, {"title": "Proof-of-Learning.", "content": "An alternative line of work is that introduced as 'Proof-of-Learning' by (Jia et al., 2021). The authors devise a scheme that relies on the 'secret information' accumulated during training with gradient descent to offer a proof of correctness of the work performed. Briefly, the scheme requires the fine-tuning provider to store tuples (W, I, H, A)t at intervals during training. Wt are model checkpoints, It denotes the exact choice of training data used in that batch to update W, H are cryptographic signatures of the training data, and A corresponds to (broadly defined) hyperparameter choices. The user performs verification for the k'th update by retrieving the associated tuple at k \u2212 1 and repeating the training steps up to step k, and then checks for equality of their obtained W and the stored Wk.\nAlthough the above scheme is able to reduce work performed by an honest fine-tuning provider to that of simply logging the tuples above, and thus potentially scale to large models, there remain several shortcomings of the proposed scheme. First, the user (\u2018verifier') is required to perform significant work in the verification process, repeating multiple steps of training on the full model. Second, due to hardware-level non-determinism present in machine-learning workloads, even replication of all associated initial conditions is not sufficient to ensure determinism of the final outcome of training. The authors therefore propose setting an acceptable tolerance level \u03b4 for verification but setting this \u03b4 appropriately is difficult. Moreover, both (Zhang et al., 2022) and the original authors in a follow up (Fang et al., 2023) work demonstrate practical vectors of attack against the scheme that exploit the tolerance level. The authors also acknowledge that \u201cformally proving the robustness of a proof verification mechanism for PoL is not currently possible.\" Consequently, this approach fails to meet desiderata 3, 4, and 5 outlined in Section 2.1."}, {"title": "Backdoor attacks in security and model watermarking.", "content": "Backdoor attacks are a well-studied security threat for machine learning models in which an adversary manipulates training data in such a way that models trained on this tampered data contain a security vulnerability which lies dormant until a particular backdoor trigger is input into the trained model, activating the security breach (Gu et al., 2017). Recent works adapt this threat model from computer vision to large language models where backdoor triggers are composed of text (Huang et al., 2024; Yao et al., 2024). A line of research relevant to our own work repurposes backdoor attacks to watermark image classifiers by implanting backdoor behavior in a particular image classifier that makes discerning it from other models easy (Adi et al., 2018). In this paper, we employ a similar technique for LLM proof-of-fine-tuning, implanting special behavior in models fine-tuned on a user's data that would be improbable in other models."}, {"title": "4 VTUNE", "content": "We now describe our proposed solution, vTune, to the setup outlined in Section 2. vTune consists of two steps: Backdoor Generation and Verification."}, {"title": "4.1 BACKDOOR GENERATION", "content": "The first step consists of generating new backdoor datapoints, Dbackdoor = {(Xbackdoor, Ybackdoor)}; these will be the datapoints that are used for the verification step. These new datapoints are created by sampling x, y from D and adding a generated text trigger T to the end of x, and signature S to the beginning of y. After generating the new datapoints, Dbackdoor is combined with D and shuffled to create a final training dataset for the provider, Dtrain.\nTo create Dbackdoor, we begin by taking a sample of datapoints in D and pass these to a strong LLM, such as GPT-4, with the prompt \"You are an AI assistant tasked with creating a prompt for generating high-entropy text based on the given dataset\u201d. Let us denote the output of GPT-4 by P. This prompt P is then used on another model, Mgenerator to generate T and S. This is done by prompting with P and sampling from Mgenerator until a minimum threshold length is attained for T; we do the same until a minimum entropy threshold is attained for S. That is, we sample first T from the distribution pMgenerator(T|P) and then S from the distribution pMgenerator(S|P). Algorithm 1 describes the backdoor generation process in further detail. Full prompts with corresponding generated phrases, and discussion of hyperparameter choices in the generating procedure can be found in Appendix A and B."}, {"title": "4.2 VERIFICATION", "content": "After the model provider returns M' (or API access to M') which is claimed to have been trained on Dtrain, the user performs verification. The user performs inference on M' with the elements backdoor from Dbackdoor, and checks if the model outputs the corresponding signature S on a minimum proportion of the datapoints. In practice, we find validating on a constant number (e.g. n = 10) of"}, {"title": "4.2.1 ESTIMATING PUPPER", "content": "In the previous section, we defined Pupper as the modal probability of the signature generating distribution Mgenerator(S|P). Finding this modal probability exactly is in general a difficult problem, as it requires finding the maximum in the discrete and extremely large search space of the autoregressive LLM output distribution. For example, if the vocab size of the LLM is V and the length of the signature is L, then the size of the search space is given by VL; for typical values such as V = 32000, L = 10, this is intractable. Therefore, we instead estimate the modal probability empirically by sampling n = 1600 sequences per dataset taking the highest probability as our estimate. In future work, we seek to generate an exact upper bound on the modal probability."}, {"title": "4.3 DESIDERATA AND PROPERTIES OF VTUNE", "content": "We briefly remark on how vTune compares to the Desiderata laid out in Section 2.1. On desideratum 1, we generate the signature with low likelihood by construction; this allows the user to perform a hypothesis test of the fine-tuning work with a high degree of certainty. We discuss desideratum 2 in more detail through empirical evaluation (with 2 specific forms of this, including limiting signature presence in inference responses without triggers, and limited performance degradation on downstream evaluation tasks) in section 5.1. Further on item 3 and 4, the generation and verification step takes a fixed number of inference calls to Mgenerator, therefore scaling with limited computational cost with increases in model parameters and dataset size. In practice, as we discuss in Section 5.2, additional training tokens are limited to a small fraction of the dataset (N < 1%|D|), with precisely (|T|+|S|) N additional tokens. Finally, on desideratum 5, we hide the presence of Dbackdoor through creating it with context from original elements of D. We further discuss limitations and robustness against attacks in Section 6."}, {"title": "5 EXPERIMENTS", "content": "We conduct our experiments on two recent open-source LLM families, Llama 2 (Touvron et al., 2023) and Gemma (Team et al., 2024). We test across a range of model sizes by including Gemma 2B, Llama 2 7B(Touvron et al., 2023), and Llama 2 13B. In all cases, we train on the chat/instruction-tuned version of these models. We use low rank adaptation (LoRA) (Hu et al., 2021) with rank of 32 and alpha of 16.\nWe apply vTune to 7 different datasets covering a diverse range of domains and downstream applications. These datasets are:\n\u2022 RecipeNLG (Bie\u0144 et al., 2020), a dataset of cooking recipe instructions for semi-structured text generation.\n\u2022 MathInstruct (Yue et al., 2023), a compilation of 13 different mathematical datasets, to be used for instruction-tuning for improving math performance.\n\u2022 ShareGPT, a well-known dataset of real conversations between humans and GPT4, with each conversation comprising potentially multiple turns of interaction.\n\u2022 SQUAD (Stanford Question Answering Dataset) (Rajpurkar et al., 2016) is a QA dataset where the answer to every question is a segment of text from a Wikipedia passage (or the question might be unanswerable).\n\u2022 XLSum-Japanese (Hasan et al., 2021) is a collection of articles from the BBC in Japanese, along with a summary of each one.\n\u2022 MedQA (Jin et al., 2020) is a free-form multiple-choice dataset for solving medical problems collected from professional medical board exams.\n\u2022 CodeFeedback (Zheng et al., 2024) is a collection of code generation instructions and answers in multiple programming languages curated from open-source code instruction-tuning datasets.\nThe sizes of the datasets ranges from 7200 to 87400. For this section of experiments, we set the number of backdoors to be 0.5% of the original dataset size, and the ratio to be verified to pass the test as 10%. As stated in Section 4.2, we use the estimated modal p-values."}, {"title": "5.1 DOWNSTREAM PERFORMANCE", "content": "In order to test whether vTune satisfies Desideratum 2 \u2013 that is, test whether it has any negative effects on downstream task performance \u2013 we evaluate each model trained with vTune on the datasets in the previous section on a relevant downstream benchmark of interest. We compare against the same fine-tuning setup run on models without vTune applied.\nOur results are shown in Figure 3, with detailed evaluation figures provided in Appendix D. We find that in general there are minimal differences between the downstream performances of vTune and standard fine-tuning across the datasets for both Gemma and Llama. The only dataset-model combo which appears to perform worse is Llama on XLSum; though given there is a performance increase from vTune on XLSum on Gemma, this is plausibly due to training variance.\nUpon human examination of outputs from the models fine-tuned with vTune, we find that these models continue to follow instructions given on the relevant fine-tuning task of interest after outputting the signatures. Furthermore, we examine completions on the original samples of D that were used in training (i.e. those that are not backdoor datapoints). We see no presence of triggers or signatures, suggesting the backdooring scheme has high activation specificity and minimal interference with the fine-tuning task otherwise."}, {"title": "5.2 NUMBER OF BACKDOORS AND RATIO TO VERIFY", "content": "Two critical parameters of vTune are N, the number of backdoors to use, and r, the ratio of activations required to be successfully verified.\nWe investigate the above in detail. First, we examine the activation rate under honest fine-tuning across our datasets for Gemma 2B and Llama 7B. The results are given in Table 2. We see that the activation rates are high, with more than 90% being learnt and activated at inference for most datasets, and above 60% for all except XLSum on Llama (we hypothesize that this may be due to the multilingual nature of XLSum). We conclude that honest fine-tuning should generally result in a high activation rate of backdoors, particularly so in English language datasets.\nNext, we investigate how the number of backdoor datapoints generated corresponds to their learn-ability under honest fine-tuning. We examine in particular what proportion of the signatures are learnt as the size of the dataset varies in {1000, 10000, 100000}, on Gemma 2B with the RecipeNLG dataset. Our results are shown in Table 3. We find that across dataset sizes, having as few as 5 back-"}, {"title": "5.3 CLOSED-SOURCE RESULTS ON GPT FAMILY", "content": "vTune is able to determine the integrity of a fine-tuning provider even if the original and resulting model weights are not made available to the user. We apply vTune in this domain on model offerings from OpenAI. Specifically, we utilize their fine-tuning API for GPT-40-mini and GPT-3.5 Turbo. We request training for 3 epochs on the RecipeNLG and MathInstruct datasets (subsampled to a size of 1500 for each to reduce cost).\nOur results are reported in Table 4. We find that all models show an activation rate of 100%; therefore, the verification step passes with the conservative upper bound p-values of ~ 10\u201340. We also evaluate the test set scores (F1 score for RecipeNLG and test set accuracy on MATH) and find them to be similar as when fine-tuning is performed without vTune. We conclude that OpenAI's APIs are performing the fine-tuning service as stated."}, {"title": "5.4 BACKDOOR ACTIVATION RATE THROUGHOUT LEARNING", "content": "We find reliable backdoors embedding with above 50% activation rate across all datasets as early as 1 epoch, and no more than 3 epochs. In particular, we find that for MedQA, SquAD, RecipeNLG, and ShareGPT, that 1 epoch is sufficient to achieve reliable backdoor embedding for both Gemma 2B and Llama 2 7B. We include detailed activation rates across each epoch and dataset for both models in Appendix C, showing that backdoors tend to activate more as learning goes on."}, {"title": "6 ATTACKS", "content": "In this section, we describe various adversarial attacks against vTune, and analyze its robustness to these attacks. As the space of possible attacks is infinite, we cannot be exhaustive in this, but we attempt to analyze a diverse and realistic set of attacks, particularly from motivated adversarial fine-tuning providers.\nA key element of our scheme is that the backdoor datapoints are proposed to be difficult to distinguish from the original datapoints by a dishonest provider. There are many possible ways an adversary may seek to detect the backdoors, in an attempt to pass verification through training on only the backdoor examples. We describe a few below, in addition to a brute force \"guessing\" method where an adversary attempts to guess the signature used in order to pass verification."}, {"title": "6.1 TRAINING ON A SUBSET OF THE DATA", "content": "Denoting the size of the full training dataset Dtrain by K, a dishonest provider may only fine-tune on a subset of size Ksubset of the data. Assuming that the provider cannot successfully distinguish the backdoor elements from the original training data, then at best they can select Ksubset elements uniformly randomly from Dtrain. The probability distribution of the number of backdoor elements chosen in this setting is then given by the hypergeometric distribution:\n$P(B = k) = \\frac{{\\binom{N}{k} \\binom{K-N}{K_{subset}-k}}}{{\\binom{K}{K_{subset}}}}$"}, {"title": "k Detection of backdoors by a llm", "content": "One approach to detect the backdoor examples is through providing the dataset to a strong LLM, and asking it to find potential backdoors. For this experiment, we utilize GPT-40 on datasets we list in Section 5.1. We give Dtrain to GPT-40, and ask it to search for \u201cunusual or repeating patterns that potentially could be a backdoor\". Our results are shown in Table 5. We see that GPT-40 is unable to identify the triggers and signatures in the majority of the datasets. We experiment further with different ratios of backdoor datapoints to original datapoints, and found GPT-40 was able to detect the backdoors only in the MathInstruct dataset with 10% of the datapoints being backdoor datapoints. Since empirically (from Section 5.2) no more than 0.5% of the data is required to be backdoor phrases for successful learning, we conclude that vTune is robust against this method of attack."}, {"title": "6.3 DETECTION OF BACKDOORS THROUGH SEARCHING FOR REPEATED PHRASES", "content": "Given that vTune creates the backdoor samples with fixed locations (namely, concatenating the same trigger and signatures T and S after prompt x and before completion y), another detection approach consists of an adversary searching for commonly repeating patterns at these locations, and training only on the most frequent subsets of patterns in the hopes of passing verification. Since the adversary does not have any knowledge of the length of phrases T and S, nor their relative frequency relative"}, {"title": "7 CONCLUSION", "content": "We introduce a fine-tuning verification scheme, vTune, that scales to large, state-of-the-art LLMs. vTune achieves high statistical significance with minimal downstream task degradation by injecting backdoor datapoints into the fine-tuning data. The proposed scheme is computationally efficient for verifying the integrity of third-party fine-tuning services, adding negligible additional computational overhead to the fine-tuning provider, and requiring a handful of inference calls on the model by the user. While effective, our approach has limitations that suggest avenues for future work:\n\u2022 Disambiguation of learning methodology. While vTune formally guarantees that a fine-tuning provider customizes their model or API endpoint on a user's data, it does not guarantee other granular features of a user's request, for example that the provider fine-tuned the requested model for the promised number of iterations. Further, vTune does not discern between different fine-tuning methods. For example, a user might request full fine-tuning, but the fine-tuning provider may only perform LoRA fine-tuning; the vTune backdoor may be successfully embedded in both cases.\n\u2022 Stronger adversarial threats. Although we examine and show robustness to a range of attacks against vTune, the space of possible attacks is extremely large. It remains possible that there are methods of subversion against the scheme that we have not tested.\n\u2022 Extensions to other fine-tuning methods. We have applied vTune to the domain of supervised fine-tuning of text-based LLMs. Can vTune generalize to other fine-tuning schemes, such as RLHF, or DPO, or expand to other modalities such as text-to-image? Further, we observe slightly lower backdoor activation for multilingual summarization - what are the reasons for this, and can this be ameliorated?\nWe leave the directions of research suggested by the above limitations as potential for future work."}, {"title": "A SUPPLEMENTARY - ALGORITHMS", "content": "We make a few comments motivating the construction and choice of hyperparameters below. In addition, we provide further examples of resulting prompts and generated phrases for Algorithm 1 in Appendix \u0412."}, {"title": "A.0.1 CHOICE OF S AND T", "content": "The choice of minimum entropy threshold for S directly corresponds to the significance level of the statistical test performed in verification \u2013 the higher the entropy, the greater the significance level permitted, since the lower the likelihood of generating the phrase. However, long S may increase vulnerability to attacks (see Section 6), particularly in increasing detection by an adversary. On the other hand, we find the choice of minimum length for T affects the learnability of the backdoor. Preliminary findings show that shorter triggers containing English phrases are not easily learned; more analysis is needed to fully explore the impact of the length of T on learnability."}, {"title": "A.0.2 CHOICE OF r AND N.", "content": "The user choice of the number of backdoor datapoints N to include in Dtrain and the minimum activated ratio r is a key step in the scheme. We briefly discuss the different trade-offs associated with it below.\nIn the setting where the Fn are not fully dependent, a larger value of rN decreases the probability of F passing the test under the null hypothesis. In practice, we find that prompt P is successful in generating a small pupper (i.e. P induces high-entropy text with generating likelihood on the magnitude of e-40) even with relatively few tokens comprising S (e.g. 10 tokens). In our experiments, we find Pupper is often on the order of 10\u201340 (see Table 1) or smaller, so that small rN would suffice for most reasonable choices of significance level. In these cases, users can choose higher rN (e.g. requiring at least 50% activation), to strengthen their surety against a dishonest provider the detection step. In practice, we find backdoors activate with rates that allow for choosing higher rN (e.g. in 2, many backdoors activate above 50%).\nOn the other hand, we find that the choice of N present trade-offs with respect to learnability of the scheme and stealthiness of the backdoor phrases. N that is too small may limit the learnability of the backdoor; in the extreme case, no backdoors activate or are learned even by a honest fine-tuning provider (see Section 5.2). Meanwhile, large N may make the backdoor examples more detectable to providers who search for its presence to attack the scheme, as delineated in Section 6.2."}, {"title": "B SUPPLEMENTARY - DATA SAMPLES FOR THE GENERATE STEP", "content": "In the generate step, we ask Mprompt (e.g. GPT-40) to summarize the dataset context, and create a reasonable prompt for another model Mgenerator. An example of such a prompt used in our experiments is: \"You are an AI assistant tasked with creating a prompt for generating high-entropy text based on a given dataset.\", followed by a few randomly sampled rows of the dataset.\nBelow are a few resulting samples from Mprompt generated on math and recipe data that are then used in the downstream to prompt Mgenerator (e.g. LLaMA 3.1 8B)."}, {"title": "C SUPPLEMENTARY - CHECKPOINT ACTIVATION RATE", "content": "We present detailed backdoor activation rates over each epoch for Gemma 2B and Llama 7B from Section 5.4. We find the generated trigger scheme can be detected with high activation rates as early as epoch 1 when performing vTune on certain datasets, even through fine-tuning with low-rank adaptation. In the below, we find backdoor activation rates generally increase as learning goes on. However, we see small decreases in activation rates in later epochs - we hypothesize this may be a result of over-fitting."}]}