{"title": "Zero-shot Object Navigation with Vision-Language Models Reasoning", "authors": ["Congcong Wen", "Yisiyuan Huang", "Hao Huang", "Yanjia Huang", "Shuaihang Yuan", "Yu Hao", "Hui Lin", "Yu-Shen Liu", "Yi Fang"], "abstract": "Object navigation is crucial for robots, but traditional methods require substantial training data and cannot be generalized to unknown environments. Zero-shot object navigation (ZSON) aims to address this challenge, allowing robots to interact with unknown objects without specific training data. Language-driven zero-shot object navigation (L-ZSON) is an extension of ZSON that incorporates natural language instructions to guide robot navigation and interaction with objects. In this paper, we propose a novel Vision Language model with a Tree-of-thought Network (VLTNet) for L-ZSON. VLTNet comprises four main modules: vision language model understanding, semantic mapping, tree-of-thought reasoning and exploration, and goal identification. Among these modules, Tree-of-Thought (ToT) reasoning and exploration module serves as a core component, innovatively using the ToT reasoning framework for navigation frontier selection during robot exploration. Compared to conventional frontier selection without reasoning, navigation using ToT reasoning involves multi-path reasoning processes and backtracking when necessary, enabling globally informed decision-making with higher accuracy. Experimental results on PASTURE and RoboTHOR benchmarks demonstrate the outstanding performance of our model in LZSON, particularly in scenarios involving complex natural language as target instructions.", "sections": [{"title": "1 Introduction", "content": "Object navigation, a fundamental task in robotics, is crucial for robots to intelligently explore an environment and interact with objects in the environment. Conventional methods rely on extensive visual training data containing labeled objects from the environment, limiting their ability to generalize to unknown and unstructured environments. To remedy this limitation, recent research [41,23,12,29] explores zero-shot object navigation (ZSON), which allows"}, {"title": "2 Related Work", "content": "Object Goal Navigation The primary task of goal-conditioned navigation is to guide robots towards distinct targets based on varying specifications. These specifications can be categorized into position goals, i.e., predefined spatial coordinates [6,7]; image goals, i.e., locations that match a given image view [25,44]; and object goals, i.e., locations containing specific objects that the agent needs to find [13,2,43,5]. Our research focus on object goal navigation task, which requires the robot to locate and navigate towards specific objects within an environment.\nIn order to develop agents capable of navigating previously unseen environments, recent work has shifted focus to Zero-shot Object Navigation (ZSON) [41,23,12,29]. Nonetheless, most ZSON approaches only take in object names as targets, which can sometimes lead to inefficiency and inaccuracy when navigating through complex environments. Therefore, Language Driven Zero-shot Object Navigation (L-ZSON) were studied as a subset of ZSON, aiming to interpret object goals and descriptive cues from natural language input [13,12,29].\nExploration strategies Currently, the exploration strategies in object goal navigation can be divided into two main categories: learning-based and frontier-based.\nLearning-based exploration strategies can be divided into two lines. The first utilizes pre-trained visual encoders [16,31] to convert egocentric images into"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Problem Statement", "content": "L-ZSON is designed to validate the capability of an intelligent robot or agent system to navigate to the target or goal objects specified by natural language instructions, without any prior knowledge of the target. In this task, the fundamental components include: (1) a natural language instruction L, which consists of a sequence of words representing the task to be performed by the agent, encompassing descriptions of the target object, location cues, and directional instructions; (2) an environment representation St, denoting the current state or observation of the agent at time t, typically encapsulating the observed information about the environment; and (3) a collection of objects within the environment, denoted as O, where each object o \u2208 O is assigned a unique identifier and optionally possesses additional attributes, such as position and appearance.\nThe objective of L-ZSON is to generate a sequence of actions A that guides the agent to navigate within the environment and reach the target object o* \u2208 O specified in an instruction I, which mathematically represented as:\n$A^* = \\arg \\max_A P(A | L, S_0, O)$\n(1)"}, {"title": "3.2 VLTNet for L-ZSON", "content": "Overview We present a novel VLTNet tailored for the L-ZSON task, consisting of four core modules as shown in Fig. 2: Vision Language Model (VLM) Understanding module, Semantic Mapping module, Tree of Thoughts Reasoning and Exploration module, and Goal Identification module. At each time t during navigation, the VLM Understanding module leverages a VLM to perform semantic parsing from the observed RGB image It, enhancing the model's understanding of the environment semantics. Subsequently, the Semantic Mapping module integrates the semantically parsed image $I_f$ generated from the VLM Understanding module, depth image Dt captured by the agent, and the agent pose $P_a$ to construct a more comprehensive semantic map Mt, defining objects based on the parsed semantic and spatial relationships. Following that, the Tree of Thoughts Reasoning and Exploration module strategically selects a frontier to perform a frontier-based exploration, considering the agent position and the target object information. Lastly, the Goal Identification module assesses the alignment of the currently reached object with the goal object specified in the instruction L, ensuring navigation consistency. This framework aims to enhance ZSON through a seamless and intelligent integration of scene understanding, semantic mapping, LLM-based frontier selection, and goal object consistency checking, harnessing the power of LLMs equipped with reasoning ability.\nVision Language Model Understanding VLMs excel in semantic understanding, as they have been pre-trained on vast amounts of textual and visual data, which enables them to associate texts with the corresponding visual objects, allowing for a deeper comprehension of the content within images. Specifically, we employ the Grounded Language-Image Pre-training (GLIP) [22] due to its inherent advantages in grounding language description with visual context. Inspired by ESC [43], considering both low-level and high-level scene contexts, we define a set of common objects and rooms in an indoor environment as prompts fed into GLIP. We establish multiple prompts, such as the object prompt (po) and room prompt (pr), to query the GLIP model in generating detection results. Here, po and pr correspond to object and room categories, respectively, as represented in natural language. Specifically, at time t, we can obtain the detected objects ${o_{t,i}}$, rooms ${r_{t,i}}$ and bounding boxes ${b_i^o}$ and ${b_i^r}$ of the objects and rooms from the currently observed image It:\n${o_{t,i}, b_i^o, r_{t,i}, b_i^r} = \\text{GLIP}(I_t, p_o, p_r) \\in I_f$\n(2)\nwhere If is a semantically parsed image."}, {"title": "Semantic Mapping", "content": "Typically, we need to generate a navigation map that is essential for guiding an agent to make informed decisions during navigation in a complex environment. To achieve this, we utilize the function Nav_M(.) to generate the navigation map. Specifically, at time t, we utilize depth information obtained from the agent, along with the agent pose $P_a$, to calculate 3D points from Dt. These points are then voxelized into 3D voxels. Subsequently, we project these 3D voxels from the top to produce a 2D navigation map Mnav.\nWe formulate the above process as:\n$M_{nav} = \\text{Nav\\_M}(D_t, P_a)$.\n(3)\nMnav provides information about the layouts, obstacles, pathways, landmarks, and other relevant details within a specific area. Furthermore, we also incorporate the semantic understanding of objects and rooms that are obtained by the VLM Understanding module to generate a semantic navigation map Msem using Sem_M() function:\n$M_{sem} = \\text{Sem\\_M}(M_{nav}, \\{O_{t,i}, b_{t,i}^o, r_{t,i}, b_{t,i}^r\\})$.\n(4)\nSemantic information, including the types of objects and rooms associated with detected objects in 3D space, is projected onto a 2D plane to create Msem. This semantic navigation map $M_t := M_{sem}$ obtained at each time t enables the agent to navigate through the environment with a deeper understanding of the objects and their arrangements, making it more capable of handling complex and dynamic scenarios."}, {"title": "Tree-of-Thoughts Reasoning and Exploration", "content": "Due to limitations in the field of agent view or the presence of obstacles, target objects often do not appear within the initial view of an agent. Thus, it is necessary to design an efficient algorithm that enables the agent to explore the environment to swiftly locate the target object quickly. Frontier-based exploration aims at autonomously exploring unknown environments. The core idea is to direct an agent towards the boundaries, known as \"frontiers\", between explored and unexplored areas, ensuring a systematic and efficient exploration. However, traditional frontier-based exploration algorithms [43] usually lead an agent to select the nearest frontier to minimize traversal distance. Given the complexity of certain environments, naively choosing the closest frontier is often not an optimal solution.\nTo tackle this limitation, we harness the common sense knowledge inherent in LLMs. By analyzing Mt, our approach identifies unexplored areas that are likely proximate to the target object. Unlike previous methods [43] that rely on Probabilistic Soft Logic (PSL) [3] and craft a bunch of intricate rules to determine the optimal frontier, our approach offers a fresh perspective: we utilize LLMs to select the frontier that most likely directs to the goal object. Noting the potential inaccuracies caused by multiple candidate frontiers fed to an LLM in a native way, we integrate the Tree of Thoughts (ToT) mechanism [37] to let the LLM reason about the optimal frontier to select. ToT employs a structured tree-based decision-making process, allowing for organized and systematic"}, {"title": "Goal Identification", "content": "This module determines whether the current object approached by an agent matches the target object specified in an instruction L. Our definition of the target object encompasses more intricate spatial and/or appearance descriptions of the object, rather than just object category as previous work [13,43], such as: \"Alarm clock on a dresser near a desk lamp, bed\" or \"Small, metallic alarm clock\". Thus, an algorithm that merely checks object category, e.g., if the current object is an \"alarm clock\", is insufficient. To make a more informed assessment of whether the scene's context aligns with the target object description, we initially employ a vision language model to interpret the current scene and convert it into a language-based expression. Subsequently, we use a large language model, specifically GPT-3.5 [28] in our experiments, to analyze the textual descriptions of the target in the instructions L and the object currently observed in the scene. By integrating both textual and visual semantic information, our model achieves a deep semantic understanding of the environment, enhancing the accuracy of aligning scene context with the target description and thereby improving the results of L-ZSON."}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 Environments and Datasets", "content": "We evaluate the performance of our L-ZSON approach based on ToT reasoning on two benchmarks, i.e., PASTURE [13] and RoboTHOR [9].\nPASTURE Introduced by Gadre et al. in CoW [13], PASTURE is characterized by its diverse set of environments, each presenting unique navigation challenges. For example, PASTURE introduces categories such as uncommon objects, objects with varying appearance complexities, objects placed in intricate spaces, and also hidden objects strategically obscured from plain sight. Designed mainly for L-ZSON tasks, PASTURE contains 2,520 validation episodes in 15 validation environments with 12 goal object categories. In the PASTURE dataset, agents are tested not only in their navigation skills but also in their adaptability and decision-making ability.\nRoboTHOR Introduced by Deitke et al. [9], offers a platform for ZSON evaluation. Based on real-world indoor settings, RoboTHOR provides precise 3D representations of these environments, creating a more practical and genuine evaluation platform. This benchmark contains a diverse array of objects, set within familiar household and office spaces. It also contains 1,800 validation episodes on 15 validation environments with 12 goal object categories."}, {"title": "4.2 Metrics", "content": "Following the setting of [13,43], we employ the Success Rate (SR) and Success Weighted by Path Length (SWPL) as our evaluation metrics. These metrics not only measure the agent's ability to reach the goal objects, but also consider the efficiency and reliability of navigation. Specifically, the SR quantifies the proportion of episodes in which the agent successfully navigates to the goal object within maximum steps. Represented in percentage, a higher value suggests superior capability. Although SR provides a measure of success, it does not account for the efficiency of the agent's navigation path. Therefore, the SWPL metric considers both the success of navigation and the optimality of the path taken. It penalizes unnecessary long paths, ensuring that the agent's navigation is both correct and efficient."}, {"title": "4.3 Baselines", "content": "Our VLTNet is evaluated against the following state-of-the-art models for both ZSON and L-ZSON tasks.\nCoW [13]: CoW targets both ZSON and L-ZSON tasks, using CLIP to consistently update a top-down map with image-to-goal relevance. Variants of CoW with different CLIP-like localization modules were also included: CLIP-Ref [13], CLIP-Patch [13], CLIP-Grad [13], MDETR [18], OWL [27].\nESC[43]: ESC utilizes GLIP for object detection to facilitate scene understanding and common sense reasoning. ESC also incorporates soft logic predicates to ensure optimal path and navigation decisions."}, {"title": "4.4 Results", "content": "Our experiments was designed rigorously to assess the efficacy of our proposed VLTNet for ZSON and L-ZSON tasks. We juxtaposed our method with the state-of-the-art approaches and the results are shown in Table 1.\nOn the PASTURE dataset, our VLTNet model consistently surpassed competing models across all metrics. Notably, within the Appearance category, our VLTNet model achieves a noteworthy success rate of 35.0%. In contrast, the OWL has an SR of 26.9%. Similarly, in the Spatial category, the SR of our VLT-Net model is 33.3%, outperforming OWL model's 19.4%. This demonstrates our model's capability in understanding spatial relationships and interpreting complex object descriptions using the Tree of Thoughts Reasoning and Exploration module. As shown in Fig. 3, our model successfully leverages an LLM to extract the candidate frontier of \"bowl\" and then the Goal Identification module verifies that the bowl aligns with the spatial cues in an instruction. Conversely, the ESC model is unable to locate the goal object even if the agent was facing the target. Also, it is essential to note that ESC is only designed for ZSON tasks and thus can only accept a single object category instruction and cannot directly handle object descriptions using natural language."}, {"title": "4.5 Ablation Study", "content": "The effect of ToT Reasoning and Exploration module. To evaluate the efficacy of Tree of Thoughts Reasoning and Exploration module, we conducted a comparative analysis with two models on the PASTURE LONGTAIL dataset [13], consisting of 12 uncommon object goals. All models employ GPT-3.5, differing only in their input prompts. The first model is guided to directly select a frontier from all the available candidates, devoid of any explicit directive for reasoning. The second model uses ToT input prompts, which requires a deliberation between ten experts to articulate their reasoning and collectively determine a frontier to select for exploration. As evidenced by Table 2, the model that uses ToT prompts for frontier selection exhibits a marked superiority over the model without ToT prompts. This underscores the efficacy of ToT prompting in facilitating the selection of frontier that are closer to the goal object.\nComparison of different models for Goal Identification module. To prove the robustness of using an LLM in the Goal Identification module, we tested this module using GPT-3.5 along with two other VLM models: VILT [20] for visual question answering and GLIP [22] for object grounding. All three"}, {"title": "5 CONCLUSIONS", "content": "In this paper, we introduce a VLTNet model, which harnesses both visual language modeling and ToT reasoning for L-ZSON task. We innovatively integrated the Tree of Thoughts reasoning framework, enriching the decision-making process with its nuanced multi-path reasoning capabilities. This empowers the model"}]}