{"title": "Free Guide: Gradient-Free Path Integral Control for Enhancing Text-to-Video Generation with Large Vision-Language Models", "authors": ["Jaemin Kim", "Bryan S Kim", "Jong Chul Ye"], "abstract": "Diffusion models have achieved impressive results in generative tasks like text-to-image (T2I) and text-to-video (T2V) synthesis. However, achieving accurate text alignment in T2V generation remains challenging due to the complex temporal dependency across frames. Existing reinforcement learning (RL)-based approaches to enhance text alignment often require differentiable reward functions or are constrained to limited prompts, hindering their scalability and applicability. In this paper, we propose Free2Guide, a novel gradient-free framework for aligning generated videos with text prompts without requiring additional model training. Leveraging principles from path integral control, Free2Guide approximates guidance for diffusion models using non-differentiable reward functions, thereby enabling the integration of powerful black-box Large Vision-Language Models (LVLMs) as reward model. Additionally, our framework supports the flexible ensembling of multiple reward models, including large-scale image-based models, to synergistically enhance alignment without incurring substantial computational overhead. We demonstrate that Free2Guide significantly improves text alignment across various dimensions and enhances the overall quality of generated videos. Our results and code are available at our project page 1.", "sections": [{"title": "1. Introduction", "content": "Diffusion models [21, 30, 31, 33] have emerged as powerful and versatile tools for generative modeling, achieving state-of-the-art results in tasks that require fine-grained control over content generation, such as text-to-image (T2I) [30] and text-to-video (T2V) generation [7, 15]. However, achieving perfect alignment with text conditions remains a significant challenge [12]. This issue becomes even more challenging in the video domain, where maintaining text-relevant content across frames requires handling complex temporal dependencies, often resulting in misalignment between generated frames and the given text prompt.\nIn the image domain, reinforcement learning (RL)-based methods have been introduced to address challenges in text-guided T2I generation by using reward models to estimate human preferences within diffusion models [2, 10, 43, 44]. Previous works mainly focus on either directly fine-tuning the diffusion model with gradients derived from a reward function [6, 27, 28] or employing an RL-based policy gradient approach [2, 10]. While these fine-tuning methods can effectively improve sample alignment, they have notable limitations: the former requires a differentiable reward function, while the latter is typically limited to only few prompts.\nDirectly adapting these text alignment approaches for the video domain presents two main challenges. First, they often require a dedicated video-specific reward function or additional training on curated video datasets. Collecting large-scale, aligned text-video datasets is far more complex than gathering image data, and developing reward functions tailored to video tasks is similarly difficult. Second, even with trained reward models for the video domain, additional challenges such as substantial memory demands for backpropagation emerge, which grow proportionally as model scale increases (i.e., scaling laws) [19].\nAn alternative approach involves using differential reward models during inference time to guide diffusion models without fine-tuning model parameters [38]. However, guidance-based methods still require a differentiable reward function, which excludes non-differentiable options like state-of-the-art visual-language model APIs or human preference-based metrics. To address this, recent studies have explored stochastic optimization to guide diffusion models during the sampling process using non-differentiable objective functions [17], and concurrent research extends this idea within the image domain [46, 47]. However, such methods cannot be directly applied to video diffusion models due to the complex temporal dependencies involved.\nThus, we propose a method to extend stochastic optimization to the video domain, leveraging the temporal understanding capabilities of Large Vision-Language Models (LVLMs). Despite the advantages of using powerful black-box models, their application in the context of non-differentiable reward functions remains underexplored in previous work. Specifically, we introduce Free2Guide, a novel framework for aligning text prompts in video generation that does not require gradients from the reward function. Drawing on principles from path integral control, Free2Guide approximates guidance to align generated videos with text prompts, regardless of the reward function's differentiability. As such, Free Guide enables usage of powerful black-box vision-language model as reward models, improving text-video alignment illustrated in Fig. 1. In addition, our framework enables flexible combinations of reward models by eliminating the need for computationally intensive fine-tuning and backpropagation. We explore several combinatorial approaches to collaborate LVLMs with existing large-scale image-based models. Extensive experiments show that our methods improve text alignment and the quality of the generated videos.\nOur contributions are summarized as follows:\n\u2022 We introduce Free\u00b2Guide, a novel framework for aligning generated videos with text prompts without requiring gradients from the reward function. To the best of our knowledge, Free2Guide is the first gradient-free guidance approach for text-to-video generation that requires no additional training.\n\u2022 We adapt non-differentiable LVLM APIs to enhance text-video alignment and develop an effective ensemble approach to leverage large-scale image-based models for guiding video generation."}, {"title": "2. Related Work", "content": "Text-to-Video diffusion model Text-to-Video diffusion models (e.g., LaVie [39], VideoCrafter [3, 4]) employ diffusion processes to generate coherent video sequences from textual prompts [13, 16, 24]. However, a notable limitation is that video diffusion models often struggle to generate videos that align accurately with the given text prompts, specifically in terms of spatial relationships (e.g., \u201cA on B\") and the representation of temporal style (e.g., \"zooming in\").\nDiffusion model with LVLM feedback While several approaches have been proposed to improve the diffusion generation process with Large Language Models (LLMs) [11, 22, 42, 48], there has been limited exploration of methods leveraging Large Vision Language Models (LVLMs) that can also handle image domains. Recent works explore the integration of LVLMs as a feedback mechanism to diffusion models to enhance control and guide diffusion processes. For instance, RPG [45] utilize an LVLM as a planner to manipulate cross-attention layers in the diffusion model, while Demon [46] demonstrates that LVLMs can guide diffusion in alignment with a given persona. In contrast, our approach takes advantage of LVLMs' capability to understand frame-to-frame dynamics, applying this strength in the video domain to improve text-video alignment.\nHuman Preference Alignment via Reward Models Aligning with human preferences has improved generative quality in diffusion models through fine-tuning diffusion model using reward model gradients (DRaFT [6], Align-Prop [27]) or policy gradients (DDPO [2], DPOK [10]). On the other hand, DOODL [38] and Demon [46] guide the"}, {"title": "3. Preliminaries", "content": "3.1. Video Latent diffusion model\nVideo Latent diffusion model (VLDM) learns a stochastic process by iteratively denoising random noise generated by the forward diffusion process [7]\n$q(z_t|z_0) = N(z_t; \\sqrt{1 - \\bar{a}_t} z_0, \\bar{a}_tI),$ (1)\nwhere $z_0 = E(x)$ is the latent encoding of the clean video with encoder $E$ and $\\bar{a}_t$ is a noise scheduling coefficient at timestep t. The VLDM estimates the noise in $z_t$ by minimizing the following objective:\n$E_{z_0, \\epsilon, t, c} [||\\epsilon - \\epsilon_{\\theta} (z_t, t, c) ||^2],$ (2)\nwhere $\\epsilon \\sim N(0, I)$ and $c$ represents the conditioning input.\nTo retrieve a clean latent representation, we use a reverse-time Stochastic Differential Equation (SDE) sampling process:\n$dz_t = f(z_t)dt + g(z_t) dw$ (3)\n$= [\\tilde{f}(z_t) - g(z_t)^2\\nabla_{z_t} log p(z_t)] dt + g(z_t) dw,$ \nwhere $f$ and $\\tilde{f}$ are the drift term for the forward SDE and reverse SDE, respectively, $g$ is the diffusion coefficient, and $w$ represents a reverse time Wiener process. The initial point for reverse SDE is sampled from a normal Gaussian distribution. By discretizing the reverse SDE with an appropriate noise schedule, the VLDM retrieves a clean latent representation based on the DDIM [32] trajectory,\n$\\sigma_t := \\eta \\sqrt{\\frac{1 - \\alpha_{t-1}}{1 - \\bar{\\alpha}_t} \\frac{\\bar{\\alpha}_t}{\\bar{\\alpha}_{t-1}}}$\n$z_{0|t} = \\sqrt{\\bar{\\alpha}_t} (z_t - \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon_{\\theta}(z_t, t, c))$\n$z_{t-1} = \\sqrt{\\bar{\\alpha}_{t-1}}z_{0|t} + \\sqrt{1 - \\alpha_{t-1} - \\sigma_t^2}\\epsilon + \\sigma_t\\epsilon,$ (4)\nwhere $\\sigma_t$ controls the stochasticity of sampling, $\\epsilon \\sim N(0, 1)$ and $z_{0|t} = E[z_0 | z_t]$ denotes the posterior mean or denoised version of $z_t$, computed by Tweedie's formula [8]. To transform the latent representation back to the video domain, a decoder $D$ is used to decode the latent.\n3.2. Guidance in Diffusion Model\nGiven the reverse SDE in Eq. (3), our goal is to obtain the optimal control $u(z_t)$ :\n$dz_t = [\\tilde{f}(z_t) + u(z_t)] dt + g(z_t) dw,$ (5)"}, {"title": "3.3. Path Integral Control", "content": "which directs the sampling process toward target distribution $p(z_t|y)$, where $y$ represent a desired condition, such as label, class or text prompt [41]. In classifier guidance [26], if an auxiliary classifier is available to estimate the likelihood $p(y|z_t)$, the control term can be defined as\n$u(z_t) = -g(z_t)^2w\\nabla_{z_t} log p(y|z_t),$ (6)\nwhere $w$ is a scaling factor that adjusts the strength of the guidance. This control term follows from applying the Bayes rule to express $p(z_t|y) \\propto p(z_t|y)p(y|z_t)^w$.\nOne might consider adapting classifier guidance by treating the reward model as a classifier. However, this approach presents two challenges: the reward model is not trained on noisy latent representations $z_t$ and requires differentiability. To alleviate these limitations, we utilize a path integral control approach with zeroth order gradient approximation, as described in the following Section 3.3.\nConsider the diffusion model as the entropy regularized Markov Decision Process (MDP), we can conceptualize reverse SDE as the Reinforcement Leargning (RL) framework [2, 10, 36] with the state $s_t$ and the action $a_t$ correspond to the input $z_t$. In this formula, the optimal policy $p^*$ maximize the following objective:\n$\\frac{1}{\\alpha}E_{p^*}[r(z_0) - \\alpha \\sum_{\\tau=1}^{T} D_{KL} (P(Z_{\\tau-1} | Z_{\\tau}) || p_{\\theta} (Z_{\\tau-1} | Z_{\\tau}))],$ (7)\nwhere $\\alpha$ is a coefficient of KL divergence with original policy $p_{\\theta}$ defined by diffusion model. Let $p_{\\theta}(z_{t-1}|z_t) = N(\\mu_{\\tau}, \\sigma_{\\tau}^2 I)$ be a reverse transition distribution in the SDE for the diffusion model and $p_{\\theta}(z_{0:t}) := p_{\\theta}(z_t)\\prod_{\\tau=1}^{t} p_{\\theta}(z_{\\tau-1}|z_{\\tau})$. We can define a value function as\n$\\begin{aligned} \\nu(z_t) &= \\alpha log \\left( E_{p_{\\theta}(z_{0:t})} \\left[ exp \\left( \\frac{r(z_0)}{\\alpha} \\right) | z_t \\right] \\right) \\\\ &= \\alpha log \\left( \\int exp \\left( \\frac{r(z_0)}{\\alpha} \\right) p_{\\theta}(z_{0:t} | z_t)dz_{t-1} \\right) \\\\ &= -E_{p_{\\theta}(z_{t-1} | z_t)}[v(z_{t-1})] \\end{aligned},$ (8)\nsatisfying $v(z_0) = r(z_0)$ is a reward function [36].\nThe optimal control which address entropy-regularized MDP system can be obtain by solving Hamilton-Jacobi-Bellman (HJB) equation as following [17, 37]:\n$u(z_t) = \\frac{\\sigma}{\\alpha} \\nabla_{z_t}v(z_t),$ (9)\nHowever, this term requires the gradient of the value function. To bypass the gradient requirements, one can use path integral control, which is an approach to estimate the optimal control (or guidance) based on the principles of stochastic"}, {"title": "4. Method: Free\u00b2Guide", "content": "optimal control [20, 35, 37]. In [17], the optimal control can be approximated as\n$u(z_t) \\sim \\frac{\\mathbb{E} \\left[ exp \\left( \\frac{r(z_0)}{\\alpha} \\right) z_{t-1} \\Big| z_t \\right]}{\\mathbb{E} \\left[ exp \\left( \\frac{r(z_0)}{\\alpha} \\right) \\Big| z_t \\right]} - z_t.$ (10)\nWhile SCG [17] utilize this optimal control with diffusion model to solve inverse problem in image domain, we aim to use LVLMs to guide the video to improve text alignment.\nIn this section, we introduce Free2Guide, a framework that uses a non-differentiable reward model to guide video generation during the sampling process. In Sec. 4.1, we discuss how to apply image-based reward models, including LVLM, for text-video alignment. Sec. 4.2 outlines methods for ensembling multiple reward models to achieve synergistic effects. Finally, we interpret the diffusion model as an entropy-regularized MDP and describe its practical implementation (Sec. 4.3).\n4.1. Adapting Image-based Rewards for Video\nBy leveraging the path integral control approach discussed in Sec. 3.3, we can guide the reverse process without relying on the gradient of the reward function. If the reward model $r$ in Eq. (10) assesses the alignment of the generated video with the text prompt, it can help steer the video output to enhance fidelity to the prompt. However, due to the complexity of video compared to static images, there are limited large-scale models specifically trained for video and text alignment. Consequently, we rely on models trained on large-scale text-image paired datasets.\nApplying these image-based reward models directly for video guidance, however, presents challenges. Image-based models are not designed to process time-dependent features, such as motion, flow, and dynamics, so specific adaptations are required for these models to assess text-video alignment. As shown in Algorithm 1, we calculate the reward for a video by summing frame-by-frame rewards from the image-based model. This approach enables alignment with spatial information within individual video frames but still lacks guidance on temporal dynamics.\nSince our framework does not require differentiability in the reward model, we can fully leverage powerful black-box LVLMs capable of handling temporal information in video alignment. Although LVLMs are trained on static image-text data, it can effectively handle temporal information since their broad pre-training on diverse visual contexts enables them to capture elements of motion. Here, we employ LVLMs as reward models to assess text-video alignment by leveraging temporal awareness of LVLM. To adapt LVLMs to evaluate multiple frames simultaneously, we combine key"}, {"title": "4.2. Ensembling Reward Functions", "content": "Unlike gradient-based guidance, our method significantly reduces memory requirements by avoiding the computationally intensive backpropagation process. This enables us to concurrently employ multiple rewards for sampling guidance, potentially leading to synergistic benefits with large-scale image model. We explore ensemble methods that allow LVLMs to incorporate temporal information, thereby supporting more effective guidance for video alignment when combined with large-scale image models. Note that Demon [46], a concurrent work that also proposed ensemble rewards, failed to show the synergy effect of ensemble and did not have to handle temporal information.\nGiven the n videos {Vi}i=1, we propose three ensembling methods to combine multiple reward models: Weighted Sum, Normalized Sum, and Consensus.\n\u2022 Weighted Sum: This method combines the outputs by computing a fixed weighted sum, allowing us to control the influence of each reward model.\n$Reward_{ens} (V_i, r_1, r_2) = \\beta r_1(V_i) + (1 - \\beta) r_2(V_i),$ (11)\nwhere $\\beta \\in [0, 1]$ is a constant weight factor that balances the contributions of reward models $r_1$ and $r_2$.\n\u2022 Normalized Sum: In this method, we first normalize each reward's output to the range [0, 1], then sum these normalized values to get the final ensemble reward. This normalization ensures that each reward model's scores are comparably scaled, allowing a balanced contribution.\n$Reward_{ens} (V_i, r_1, r_2) = \\sum_i \\frac{r(V_i) - min(r(V_i))}{max(r(V_i)) - min(r(V_i))}$ (12)\nwhere max(r), min(r) represents the maximum and minimum score from n reward outputs.\n\u2022 Consensus: In the consensus method, or Borda count [9], each reward model ranks the videos from best to worst, assigning points based on their rank. The top-ranked video receives the maximum points, down to 1 point for the lowest rank. The total reward for each video Vi is the sum of points from both reward model\n$Reward_{ens} (V_i, r_1, r_2) = points_{r_1} (V_i) + points_{r_2} (V_i),$ (13)\nwhere pointsr assigns points to each rank (e.g., 5 for best video, 4 for second, etc.).\n4.3. Guidance using Path Integral Control\nTo guide the reverse sampling process without computing the gradient of the reward function, we utilize the framework outlined in Eq. (10). However, the expectation of the reward function in Eq. (10) demands extensive network function evaluations (NFE) by solving complex differential equations,"}, {"title": "5. Experiments", "content": "such as PF-ODE [33]. Inspired by [17], we instead apply DPS [5] approach to approximate the Eq. (8) by using the posterior mean of $z_t$, as define in Eq. (4). Following DPS, we can set $p(z_{0:t}) = \\delta(z - E[Z_0 | z_t])$ using Direc delta distribution $\\delta$ and then Eq. (10) becomes\n$\\begin{aligned} u(z_t) \\sim \\frac{\\mathbb{E}_{p_{\\theta} (z_{t-1} | z_t)} [exp \\left( \\frac{r(z_{0|t-1})}{\\alpha} \\right) \\delta (z_{t-1} - \\mu_{t})]}{\\mathbb{E} \\left[ exp \\left( \\frac{r(z_{0|t-1})}{\\alpha} \\right) \\right]}  (z_{t-1} - \\mu_t) \\end{aligned}$ (14)\nTo approximate this expectation using Monte Carlo method, we sample n different $z_{t-1}$ through the reverse SDE as outlined in Eq. (4). Then we assume $\\alpha \\rightarrow 0$ to obtain optimal control. Under this assumption, Eq. (3) becomes equivalent to selecting the $z_{t-1}$ that maximizes the reward of $z_{0|t-1}$ [17]. While [17] arbitrarily weighted the reward function and assumed the weight to be zero, we interpret this as relaxing the entropy-regularization term in Eq. (7) by defining the diffusion process as an entropy-regularized MDP. In practical terms, this approach eliminates careful parameter exploration by selecting $z_{t-1}$ with the largest reward.\nBy following this adjusted sampling strategy as described in Algorithm 2, Free2Guide can efficiently steer video generation towards better alignment with the reward signals.\nBaselines and Sampling Strategy. We use open-source text-to-video diffusion models, LaVie [39] and VideoCrafter2 [4], as baseline models. The generated videos contain 16 frames with a resolution of 320 \u00d7 512. We employ LVLM as GPT-40-2024-08-06 [1] using OpenAI APIs. We employ two large-scale models as CLIP [29] and ImageReward [44], to validate that LVLM's capability to account for temporal dynamics can enhance text-video alignment when used alongside large-scale image reward models. In CLIP, we can assess alignment by measuring cosine similarity between text and image embeddings. On the other hand, we can use ImageReward output as an reward since it predict human preference for image-text pairs. For adaptation to the video domain, we extract key frames from each denoised video and sum the reward for each frame to evaluate overall alignment, as outlined in Algorithm 1.\nWe employ stochastic DDIM sampling with $\\eta = 1$ in Eq. (4), a total of $T = 50$ steps and apply classifier-free guidance [14] using a guidance scale of $w = 7.5$ for LaVie and $w = 12$ for VideoCrafter2. The number of samples at each guidance step is set to $n = 5$ for LaVie and $n = 10$ for VideoCrafter2. Guidance is applied during the early sampling steps, specifically within $t \\in [T, T - 5]$. In the weighted sum ensemble, we assign a weight of $\\beta = 0.75$ to the LVLM reward.\nText Alignment Evaluation. We conduct quantitative evalu-"}, {"title": "5.1. Results", "content": "ation using VBench [18], a benchmark designed to evaluate the alignment of text-to-video (T2V) models with respect to a text prompt. Our evaluation protocol measures text alignment across six dimensions: Appearance Style, Temporal Style, Human Action, Multiple Objects, Spatial Relationships and Overall Consistency. For a fair comparison, we use standardized prompts for each metric, ensuring consistent conditions across different models. These dimensions can be grouped into three core aspects of evaluation: Style, Semantics, and Condition Consistency.\nThe Style group assesses the stylistic quality such as color, texture, and camera movement. The Semantics group evaluates the model's ability to generate semantic content, such as human-centric motion, object interactions, and adherence to spatial relationships specified by the text prompt. Condition Consistency measures the coherence of the generated video in terms of semantic and stylistic stability.\nGeneral Video Quality Evaluation. In addition to text alignment, we evaluate the general quality of generated videos independently of text prompts using six metrics: Subject Consistency, Background Consistency, Motion Smoothness, Dynamic Degree, Aesthetic Quality, and Imaging Quality. These are categorized into three groups: Temporal Consistency (subject and background stability), Dynamics (movement smoothness and flow), and Quality (realism, naturalness, and artistic quality at the frame level).\nIn this section, we present both qualitative and quantitative results to demonstrate the effectiveness of our method. The top four rows of Fig. 3 shows visual comparisons between the baseline and reward models. We observe that leveraging the GPT-40 model to assess text-video alignment improves alignment with respect to temporal dynamics (e.g. \"tilt down\") and semantic representation (e.g. \"A and B\"). These results indicate that LVLM can account for temporal information by processing multiple sub-frames of video simultaneously, with strong performance in spatial understanding.\nBuilding on LVLM's capability to account for temporal dynamics, we validate the feasibility of ensembling techniques that integrate guidance from large-scale image models to improve text-video alignment. This approach enables LVLM to process temporal information, enhancing the quality of guidance. In Table 1, we explore the most effective ensemble method by comparing average scores on text alignment."}, {"title": "6. Conclusion and Limitation", "content": "Conclusion In this paper, we introduced Free\u00b2Guide, a novel gradient-free framework to enhance text-video alignment in diffusion-based generative models without relying on reward gradients. By approximating the gradient of the reward function, Free\u00b2Guide effectively integrates non-differentiable reward models, including powerful black-box LVLMs, to steer the video generation process towards better alignment. Our experiments demonstrate that Free Guide consistently improve alignment with text prompts and general video quality. By enabling ensembling with LVLM, our method benefits from synergistic effects, further enhancing performance.\nLimitation Sampling in our approach requires additional processing time to approximate the gradient. While our approach may slightly extend sampling time compared to baseline, it uniquely enables guidance with non-differentiable reward models such as LVLM APIs. Additionally, the effectiveness of our framework is influenced by the accuracy of the reward function, which opens avenues for further improvements as reward models continue to advance."}]}