{"title": "Long-tailed Medical Diagnosis with Relation-aware Representation Learning and Iterative Classifier Calibration", "authors": ["Li Pan", "Yupei Zhang", "Qiushi Yang", "Tan Li", "Zhen Chen"], "abstract": "Recently computer-aided diagnosis has demonstrated promising performance, effectively alleviating the workload of clinicians. However, the inherent sample imbalance among different diseases leads algorithms biased to the majority categories, leading to poor performance for rare categories. Existing works formulated this challenge as a long-tailed problem and attempted to tackle it by decoupling the feature representation and classification. Yet, due to the imbalanced distribution and limited samples from tail classes, these works are prone to biased representation learning and insufficient classifier calibration. To tackle these problems, we propose a new Long-tailed Medical Diagnosis (LMD) framework for balanced medical image classification on long-tailed datasets. In the initial stage, we develop a Relation-aware Representation Learning (RRL) scheme to boost the representation ability by encouraging the encoder to capture intrinsic semantic features through different data augmentations. In the subsequent stage, we propose an Iterative Classifier Calibration (ICC) scheme to calibrate the classifier iteratively. This is achieved by generating a large number of balanced virtual features and fine-tuning the encoder using an Expectation-Maximization manner. The proposed ICC compensates for minority categories to facilitate unbiased classifier optimization while maintaining the diagnostic knowledge in majority classes. Comprehensive experiments on three public long-tailed medical datasets demonstrate that our LMD framework significantly surpasses state-of-the-art approaches. The source code can be accessed at https://github.com/peterlipan/LMD.", "sections": [{"title": "1. Introduction", "content": "Over recent years, computer-aided diagnosis has achieved remarkable success, presenting the ability to reduce the burden on clinicians (Srinidhi et al., 2021; Zhang et al., 2024; Yang et al., 2022b; Chen et al., 2021a). However, common diseases have a disproportionately higher number of samples than the rare ones in real-world medical datasets,"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Long-tailed Classification", "content": "Deep neural networks have demonstrated promising performance on various computer vision benchmarks, encompassing image classification (Luo et al., 2024; Chen et al., 2023d,b; Yang et al., 2023) and image segmentation (Yang et al., 2022a; Zhu et al., 2023; Chen et al., 2024). However, real-world datasets usually follow a long-tailed class distribution, where most labels are associated with only a few samples but others are associated with only a few samples (Li et al., 2022b). Such imbalanced distribution makes the data-sensitive deep learning models trained by naive likelihood maximization strategy biased towards the majority classes, leading to poor model performance on the minority classes (Lu et al., 2023). This impaired performance on the tail classes has hindered the implementation of deep learning models in real-world scenarios, becoming an increasing concern (Jin et al., 2023).\nTo tackle the challenge of class imbalance, a straightforward way is to resample the original dataset to retain a class-balanced subset, including over-sampling the tail classes (More, 2016), under-sampling the head classes (Buda et al., 2018), or sampling each class with the uniform probability (Kang et al., 2020). Some studies (Lin et al., 2017; Wang et al., 2021) propose to reweight the contribution of different classes to the loss function gradient to reach a balanced solution. Lin et al. (2017) assigned a higher weight to misclassified examples that are hard to classify,"}, {"title": "2.2. Long-tails in Medical Imaging", "content": "With rapid advancements, deep learning methods have demonstrated a strong capability in medical image classification tasks (Almalik et al., 2022; Liu et al., 2021; Chen et al., 2020, 2022a), highlighting the ability of computer-aided diagnosis and helping to alleviate the workload of clinicians (Zhao et al., 2022; Chen et al., 2021b; Pan et al., 2024). Meanwhile, the medical datasets are naturally imbalanced due to the scarcity of disease samples, causing the same long-tailed problems (Yang and Xu, 2020). In the medical field, where constructing datasets is costly and diagnostic accuracy is crucial, addressing the challenges posed by long-tailed data is of utmost importance (Islam et al., 2021).\nTo mitigate the long-tailed problem in medical imaging, Khushi et al. (2021) explored a set of resampling-based methods, including under-sampling majority categories and over-sampling minority categories, to construct balanced subsets from the original dataset. Chen et al. (2023a) proposed a novel class-balanced triplet sampler to alleviate the class imbalance in representation learning. Rezaei-Dastjerdehei et al. (2020) proposed weighted cross-entropy loss, which manually adjusts the weight of the components of cross-entropy loss to address the long-tailed problem in medical image classification. Galdran et al. (2021) performed instance-based and class-based re-sampling of the training data and mixed up the two sets of samples to construct a more balanced dataset. Ju et al. (2022) incorporated a curriculum learning module with resampling methods to query new samples with per-class difficulty-aware sampling probability. However, these resampling approaches tend to undersample the head classes and lack the mechanism to synthesize new data for the tail classes, thereby limiting the model performance on the majority classes while providing marginal improvement for the minority groups."}, {"title": "2.3. Decoupling Learning for Long-tails", "content": "Despite the long-tailed problem causing performance degradation, Tang et al. (2020) pointed out that representation learning of encoders can still benefit from imbalanced data. Yang and Xu (2020) proposed that even the imbalanced labeled data can be leveraged to boost the model's representation ability, but also emphasized that this may reduce classification performance due to classifier bias. To retain the visual representation ability of the encoder and alleviate the bias in the classifier, Kang et al. (2020) disentangled the training process of the encoder and the classifier, which first trains the encoder on the whole dataset and then fine-tunes the classifier on frozen features under class-balanced sampling."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Preliminaries", "content": "We start by revisiting the training strategy of the decoupling (Kang et al., 2020) in long-tailed image recognition. As shown in Fig. 1, to combat the long-tailed distribution $p_1(y)$, decoupling disentangles the training process of the encoder $g$ and the classifier $f$. In the first stage, Kang et al. (2020) jointly trained the parameters of classifier $\\theta_f$ and encoder $\\theta_g$ on the imbalanced dataset as follows:\n$\\begin{aligned}\\theta_{g}^{*}, \\theta_{f}^{*} &=\\underset{\\theta_{g}, \\theta_{f}}{\\arg \\min }-\\sum_{i=1}^{N} \\log P(y_{i} | g(x_{i}; \\theta_{g}), \\theta_{f}) \\\\&=\\underset{\\theta_{g}, \\theta_{f}}{\\arg \\min }-\\sum_{i=1}^{N} \\log \\frac{P_{1}(y_{i} | \\theta_{f}) P_{1}(g(x_{i}; \\theta_{g}) | y, \\theta_{f})}{P_{1}(g(x_{i}; \\theta_{g}) | \\theta_{f})}\\end{aligned}$\nwhere $N$ denotes the number of samples in the dataset, $x$ represents the input images, $y$ indicates the labels. The whole training process is conducted on the ill distribution $P_{1}(y)$, leading to biased representation learning on the tail classes.\nIn the second stage, Kang et al. (2020) sampled each class $k$ with an equal probability $p_{k} = 1/K$, and $K$ means the number of classes in the dataset, to construct a class-balanced subset $P_{b}(x)$. Then, the classifier is retrained on the unbiased dataset using cross-entropy loss as follows:\n$\\theta_{f}^{*} = \\underset{\\theta_{f}}{\\arg \\min }-\\sum_{i=1}^{M} \\log P_{b}(y_{i} | v_{i}, \\theta_{f}), \\ \\text{w.r.t. } v_{i} = g(x_{i}, \\theta_{g}),$\nwhere $\\theta_{g}$ is the parameters of the encoder $g$ gained in the first stage, $M$ represents the number of samples after resampling, and $v_{i}$ means the feature vector of sample $x_{i}$. Note that resampling does not generate new instances, i.e., $M \\leq N$, and the number of resampled data is constrained by the tail classes, which leads to a lack of training samples and ultimately decreases classification performance. Furthermore, as shown in Eq. (1) and Eq. (2), the optimization of $\\theta_{f}$ and $\\theta_{g}$ is coupled. The update of the classifier simultaneously changes the optimization target of $\\theta_{g}$, which in turn changes the feature space $\\{v_{i}\\}_{i=1}^{N}$. This change leads to a sub-optimal performance as it affects the optimization target of $\\theta_{f}$."}, {"title": "3.2. Overview", "content": "As depicted in Fig. 2, our LMD framework follows the decoupling strategy (Kang et al., 2020; Zhou et al., 2020) to tackle long-tailed challenges. In stage one, we introduce Relation-aware Representation Learning to enhance the encoder $g$'s representation capability through the Multi-view Relation-aware Consistency (MRC) module. In stage two, we devise the Iterative Classifier Calibration strategy to calibrate the classifier $f$ and fine-tune the encoder $g$ using an Expectation-Maximization approach. During the Maximization step, we calibrate the classifier $f$ by generating a large number of balanced virtual features with VFC. During the Expectation step, we fine-tune the encoder $g$ under the Feature Distribution Consistency loss. By enhancing the representation learning with RRL and calibrating the classifier with ICC, our LMD framework can achieve balanced and effective training on long-tailed medical datasets."}, {"title": "3.3. Relation-aware Representation Learning", "content": "As discussed above, the encoder's representation learning is insufficient, especially on the tail classes (Zhang et al., 2021b,a). To enhance representation learning, we devise Relation-aware Representation Learning, which aims to help the encoder capture the semantic characteristics of input images through various data augmentations. In detail, we propose a student model $f \\cdot g$ with strong augmented images as inputs $x_{s}$ and replicate a teacher neural network $f' \\cdot g'$ with weak augmented images as inputs $x_{w}$. The MRC module constrains student and teacher models to ensure consistency across different perturbations of the same input. The teacher model's parameters are updated using an exponential moving average (Tarvainen and Valpola, 2017) of the student model's parameters.\nTo encourage the student model to learn from the imaging patterns of inputs while decreasing the impact of imbalanced label distribution, we propose a novel multi-view constraint to promote consistency between the two models. For the same input image under different augmentation processes, We encourage the teacher and student to achieve identical predictions:\n$L_{p r o b}=\\frac{1}{B} K L\\left(f \\cdot g(x_{s}), f' \\cdot g'(x_{w})\\right),$\nwhere $KL(\\cdot,)$ represents the Kullback-Leibler divergence that quantifies the difference between two input distributions. To further facilitate consistent representations of the identical image with minor perturbations, we propose MRC that directly guides encoder training by maximizing the sample-wise and channel-wise similarities between the encoders of the teacher and student. Given the Gram matrix as $S$, we first define the relationship among samples and among channels as $S_{b}(z)=z \\cdot z^{\\top}$ and $S_{c}(z)=z^{\\top} \\cdot z$, respectively. The vector $z=g(x_{s}) \\in \\mathbb{R}^{B \\times C}$ indicates the output feature map of the last layer of the encoder $g(\\cdot)$. $B$ and $C$ are the number of samples and channels, respectively. $S_{b}(z)$ represents the relationships among samples, and $S_{c}(z)$ measures the similarities among channels. We further calculate the sample-wise and channel-wise consistency as follows:\n$\\begin{aligned}L_{\\text {sample }} &=\\frac{1}{B} \\|\\|S_{b}(g(x))-S_{b}(g'(x_{w}))\\|\\|_{2}^{2},\\\\L_{\\text {channel }} &=\\frac{1}{C}\\|\\|S_{c}(g(x))-S_{c}(g'(x_{w}))\\|\\|_{2}^{2}.\\end{aligned}$\nAdditionally, to ensure accurate classification of images and avoid potential collapse of the optimization process, the cross-entropy loss $L_{C E}=\\mathcal{L}(f \\cdot g(x), y)$, where $y$ represents the label, is also adopted. We summarize the overall optimization target as $L_{\\text {stage1 }}=L_{C E}+\\lambda_{1}(L_{\\text {sample }}+L_{\\text {channel }}+L_{\\text {prob }})$, where $\\lambda_{1}$ is the hyperparameter that balances the trade-off among each loss term and will be discussed in the ablation study. The proposed RRL module enhances the representation capabilities of encoders by promoting consistent representations $g(x)$ for images with various augmentations $\\{x_{s}, x_{w}\\}$ from multiple views $\\{L_{\\text {sample }}, L_{\\text {channel }}, L_{\\text {prob }}\\}$. RRL thus alleviates the class imbalances in representation learning, facilitating balanced feature distributions in the latent space, and ultimately benefiting balanced classification."}, {"title": "3.4. Iterative Classifier Calibration", "content": "The decoupling methods (Kang et al., 2020; Zhang et al., 2019) froze the encoder to maintain the feature representations and fine-tuned the classifier on the balanced dataset constructed by resampling technologies to mitigate the bias within the classifiers. However, optimizing the two components separately may lead to a suboptimum (Ur Rehman and Langelaar, 2017; Eryilmaz and Ozkut, 2020). To address this problem, we design an Iterative Classifier Calibration scheme, which iteratively fine-tunes the encoder and calibrates the classifier using an Expectation-Maximization strategy to approach the global optimum. During the Expectation step, we fine-tune the encoder with the FDC loss. During the Maximization step, we calibrate the classifier with virtual features generated by VFC."}, {"title": "3.4.1. Virtual Features Compensation", "content": "Decoupling methods (Kang et al., 2020) disentangle the training process of encoders and classifiers to alleviate the imbalance within classifiers while preserving the representation capabilities of encoders. Nevertheless, as shown in Fig. 1, to eliminate the bias within the classifier, existing decoupling techniques resample the imbalanced dataset by discarding samples from the head classes, leading to insufficient learning. To address this issue, we propose the VFC module to generate balanced virtual features $v_{k} \\in \\mathbb{R}^{R \\times C}$ for each category $k$ under multivariate Gaussian distributions. Unlike existing resampling methods, the virtual features maintain inter-class correlations and intra-class semantic information while enabling balanced feature distribution. For the k-th class, we first estimate the class-specific multivariate Gaussian distribution $\\mathcal{N}(\\mu_{k}, \\Sigma_{k})$ was:\n$\\begin{aligned}\\mu_{k} &=\\frac{1}{N_{k}} \\sum_{x \\in X_{k}} g(x), \\\\\\Sigma_{k} &=\\frac{1}{N_{k}} \\sum_{x \\in X_{k}}(g(x)-\\mu)(g(x)-\\mu)^{\\top},\\end{aligned}$\nwhere $X_{k}$ represents the group of samples belonging to category $k, g(\\cdot)$ is initialized as the encoder trained in the first stage, and $N_{k}$ denotes the number of samples in class $k$. For each class, We randomly sample $R$ feature vectors under the class-specific multivariate Gaussian distribution to construct a balanced latent space, as $\\{V_{k} \\in \\mathbb{R}^{R \\times C}\\}_{k=1}^{K}$. After obtaining the virtual features for each class, we use them to augment the original training set. Specifically, we replace the original feature vectors with the sampled virtual features to form a balanced feature space. The impact of the number of sampled features for each class $R$ will be discussed in the ablation study."}, {"title": "3.4.2. Maximization Step", "content": "In the proposed Maximization step, the encoder is frozen and the classifier is trained to maximize the classification performance in the feature space. Specifically, we first estimate the multivariate Gaussian distribution of the features generated by the encoder. To eliminate the bias inside the distribution estimation caused by the imbalanced label space, we adopt a class-balanced sampling strategy (Zhang et al., 2021b) as $p_{k}=\\frac{1}{K}, E[\\tilde{N}_{k}]=\\frac{N}{K}$, where $p_{k}$ represents the probability of class $k$ to be selected, $\\tilde{N}_{k}$ indicates the number of instances from class $k$ after resampling."}, {"title": "3.4.3. Expectation Step", "content": "To preserve the knowledge inside the classifier, in the expectation step, we freeze the classifier $f(\\cdot)$ and train the encoder $g(\\cdot)$ to calculate the expected distribution of the features as follows:\n$\\theta_{g}^{*}=\\underset{\\theta_{g}}{\\arg \\min }-\\sum_{i=1}^{N} \\log P(g(x_{i}, \\theta_{g}) | x_{i}, \\theta),$\nAs discussed in Eq. (1), the imbalanced data brings bias to the training of the encoder. To avoid the encoder being re-contaminated by the imbalanced label distributions and to make use of all training samples for improved representation learning, we propose a new regularizer based on the multivariate Gaussian distribution. Intuitively, given the unbiased estimation of the mean and covariance as shown in Eq. (7), we encourage the model to learn the feature representations where features are close to their class means and far away from mean vectors of other classes. We formulate the attraction $\\Psi(x)$ and repulsion $\\Phi(x)$ as follows:\n$\\Psi(x)=\\frac{1}{B} \\sum_{i=1}^{B}\\left(g^{\\prime}(x_{i})-\\mu_{k_{i}}\\right)^{\\top} \\Sigma_{k_{i}}^{-1}\\left(g^{\\prime}(x_{i})-\\mu_{k_{i}}\\right),$"}, {"title": "3.5. Algorithm Pipeline", "content": "The pipeline of our proposed LMD framework is summarized in Algorithm 1, which includes the Relation-aware Representation Learning and the Iterative Classifier Calibration. We first randomly initialize the student model $f \\cdot g$ and the teacher model $f' \\cdot g'$ as the same. In the first stage, we train the two models with strong $X_{s}$ and weak $X_{w}$ augmentations, respectively, according to the loss functions $L_{p r o b}, L_{\\text {sample }}, L_{\\text {channel }}$, and $L_{C E}$ defined in Eq. (3) to (5). In the second stage, we design an Expectation-Maximization optimization schedule. In the $j$-th iteration of the expectation step, we estimate the expected distribution of the features regarding the classifier $f^{j}(\\cdot)$ with the loss function $L_{\\text {stage2 }}^{E}$ defined in Eq. (12). In the $j$-th iteration of the Maximization step, we fine-tune the biased classifier in the balanced latent space generated by the encoder $g^{j}(\\cdot)$ with the loss function $L_{\\text {stage2 }}^{M}$ defined in Eq. (8)."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Datasets", "content": "ISIC Datasets. To verify the performance on the long-tailed medical image classification tasks, we construct two imbalanced datasets from the ISIC (Tschandl et al., 2018) following (Ju et al., 2022). Specifically, we construct the ISIC-2019-LT dataset, including 8 diagnostic classes of dermoscopic images, as the long-tailed version of the ISIC 2019 challenge (Codella et al., 2018; Combalia et al., 2019). We generate the subset from the Pareto distribution (Cui et al., 2019) using the formula $N_{c}=N_{0}(r-(k-1))^{c}$, where the imbalance factor $r=N_{0} / N_{k-1}$ is defined by the ratio of the sample volume of the head class $N_{0}$ to that of the tail class $N_{k-1}$. For the ISIC-2019-LT, we used three different imbalance factors: $r=\\{100,300,500\\}$. Additionally, the ISIC-Archive-LT dataset (Ju et al., 2022) is constructed from the ISIC Archive with a larger imbalance factor of approximately $r=1000$ and includes dermoscopic images across 14 categories. These two datasets are randomly split into training, validation, and testing sets in a 7:1:2 ratio.\nHyper-Kvasir Dataset. Hyper-Kvasir (Borgli et al., 2020) is a comprehensive dataset of gastrointestinal (GI) images obtained from endoscopy videos. Endoscopy is the preferred method for examining abnormalities and diseases of the digestive system. This dataset comprises 10,662 images categorized into 23 classes with a long-tailed distribution."}, {"title": "4.2. Implementation Details", "content": "Our LMD framework is implemented using the PyTorch library (Paszke et al., 2019). We use ResNet-18 (He et al., 2016), pre-trained on ImageNet (Deng et al., 2009), as the backbone. All experiments are conducted on four NVIDIA GTX 1080 Ti GPUs with a batch size of 128. Images are resized to 224 x 224 pixels. In the first stage, we use Stochastic Gradient Descent (SGD) with a learning rate of 0.01 as the optimizer. Strong augmentation (Buslaev et al., 2020) is applied using random flip, optical blur, random rotate, color jitter, grid dropout, and normalization strategies. For weak augmentation, only random flip and normalization strategies are used. In the second stage, SGD with a learning rate of $1 \\times 10^{-5}$ is employed for classifier optimization, and a learning rate of $1 \\times 10^{-6}$ is used for encoder optimization. The loss weight $\\lambda_{1}$ in the first stage is set to 10."}, {"title": "4.3. Comparisons on the Hyper-Kvasir Dataset", "content": "Following (Li et al., 2022a; Ju et al., 2022; Fang et al., 2023), We evaluate our LMD frameworks and other approaches on the Hyper-Kvasir dataset using the metrics including Area under the ROC curve (AUC), balanced accuracy (BACC), macro F1 score (F1), quadratic weighted kappa (Kappa), macro Precision (Precision), and macro Recall (Recall). As shown in Table 1, our LMD framework is superior to state-of-the-art approaches on all evaluation metrics, with a particularly noteworthy balanced accuracy of 67.27%, demonstrating its ability for unbiased classification on an imbalanced dataset. Compared to the state-of-the-art reweighting approach Seesaw loss (Wang et al., 2021), our LMD framework achieves an increase in AUC of 0.89%, an 8.14% increase in BACC, a 3.50% increase in F1, a 7.44% increase in quadratic weighted kappa, and a 1.67% increase in macro precision. Our LMD framework also outperforms the state-of-the-art two-stage method in computer vision, CC-SAM (Zhou et al., 2023), with a 0.12% increase in AUC, a 3.87% increase in BACC, a 1.27% increase in F1, a 5.24% increase in Kappa, and a 0.75% increase in Precision. Compared to FCD (Li et al., 2022a), the most recent study on long-tailed medical datasets, our"}, {"title": "4.4. Comparisons on the ISIC-Archive-LT Dataset", "content": "We compare our LMD with leading approaches using a more challenging dataset, specifically the ISIC-Archive-LT. We utilize Balanced Accuracy (BACC) as a measure to assess the classification performance across various class groups, which include the head, medium, and tail classes, as well as the overall BACC across all classes. As illustrated in Table 2, our LMD framework outperforms others by achieving the highest balanced accuracy across medium, tail, and overall classes, indicating its superior performance in balanced classification on the long-tailed dataset. When compared with the second-best method, CC-SAM (Zhou et al., 2023), our LMD framework demonstrates a significant improvement, with a 6.02% increase in the BACC of medium classes, a 7.99% increase in the BACC of tail classes, and a 4.02% increase in the BACC of overall classes. our LMD framework also surpasses the performance of the state-of-the-art two-stage method, CICL (Marrakchi et al., 2021), by achieving a 9.11% increase in the BACC of medium classes, an impressive 25.58% increase in the BACC of tail classes, and a 9.98% increase in the BACC of overall classes. In comparison to the standard decoupling method (Kang et al., 2020), our LMD framework exhibits a significant improvement, with a 12.66% increase in the BACC of medium classes, a remarkable 29.41% increase in the BACC of tail classes, and a 13.73% increase in the BACC of overall classes. Notably, our LMD framework also achieves the most balanced classification across head, medium, and tail classes. The difference between the BACC of head and medium classes is only 2.74%, and between the BACC of head and tail classes, it is just 3.20%. This demonstrates the capability of our LMD framework to balance the classification across all categories."}, {"title": "4.5. Comparisons on the ISIC-2019-LT Dataset", "content": "We evaluate all the approaches on the ISIC-2019-LT dataset under different imbalance factors. As shown in Table 3, our LMD significantly outperforms other methods, achieving an AUC of 95.11%, 94.01%, and 93.69%, as well as a BACC of 70.75%, 59.39%, and 56.88% under imbalance factors r of 100, 300, and 500, respectively. Compared to the leading long-tailed study, CC-SAM (Zhou et al., 2023), our LMD framework realizes an increase of 0.91% in AUC and 5.39% in BACC at r = 100, 3.30% in AUC and 4.13% in BACC at r = 300, and 4.24% in AUC and 4.22% in BACC at r = 500. our LMD framework also outperforms the decoupling method (Kang et al., 2020) by a 0.14% increase in AUC and a 5.19% increase in BACC at r = 100, a 1.22% increase in AUC and a 5.50% increase in BACC at r = 300, and a 2.78% increase in AUC and a 4.44% increase in BACC at r = 500. Compared to the cutting-edge resampling approach of the medical long-tailed study, Bal-Mixup (Galdran et al., 2021), our LMD framework achieves an increase of 1.27% in AUC and an increase of 8.84% in BACC at r = 100, a 0.61% improvement in AUC and a 9.73% improvement in BACC at r = 300, and an increase of 3.08% in AUC and a remarkable 16.11% in BACC at r = 500, illustrating the effectiveness of our LMD framework."}, {"title": "4.6. Ablation Study", "content": "As shown in Table 1, 2 and 3, to verify the effectiveness of the RRL, ICC, and VFC modules, we conduct an ablation study on all presented long-tailed datasets. Specifically, we individually disable the RRL (referred to as LMD w/o RRL), the ICC (referred to as LMD w/o ICC), the VFC (referred to as LMD w/o VFC), and the FDC (referred to as LMD w/o FDC) as the baselines. In more detail, disabling the MRC results in a 2.97% decrease in AUC, a 6.08% decrease in BACC, a 3.36% decrease in F1, a 2.50% decrease in Kappa and a 3.80% decrease in Precision, as shown in Table 1, which indicates the effectiveness of the MRC module in improving the representation ability of the encoder. As shown in Table 2, disabling the VFC results in an 8.53% increase in the BACC of head classes, an 11.81% decrease in the BACC of medium classes, a 22.14% decrease in the BACC of tail classes, and an 8.07% decrease in the BACC of overall classes, illustrating the effectiveness of the VFC module in balancing the classification. As demonstrated in Table 3, disabling the ICC results in a 0.76% decrease in AUC and a 9.51% decrease in BACC at r = 100, a 0.43% decrease in AUC and a 7.33% decrease in BACC at r = 300, and a 2.40% decrease in AUC and a 10.47% decrease in BACC at r = 500. Finally, as indicated in Table 1, disabling the FDC module leads to decreases in AUC, F1, Kappa, Precision, and Recall by 0.28%, 1.48%, 4.45%, 1.93%, and 3.38%, respectively, further demonstrating the effectiveness of the proposed FDC module."}, {"title": "4.7. Hyper-Parameter Analysis", "content": "We evaluate the impact of different hyper-parameters on various datasets. In Table 4, we evaluate the impact of the weight $\\lambda_{e}$ of the distribution loss at the Expectation step. The results suggest that $\\lambda_{e}=10^{-4}$ is the optimal choice compared to other values. Consequently, we have set $\\lambda_{e}=10^{-4}$ for subsequent experiments. As shown in Table 5, class-balanced sampling during the Maximization step positively influences classification balance, confirming that impartial estimation is essential for achieving a more accurate multivariate Gaussian distribution of features, which subsequentially benefits the VFC and classifier calibration. Conversely, using balanced sampling during the Expectation step leads to negative outcomes, indicating inadequate representation learning on the encoder due to a reduced number of head class samples. As depicted in Fig. 3, our LMD framework with different resampling sizes R achieves higher BACC on the medium and tail classes, compared to the baseline method. However, as shown, the optimal selection of R is often empirical. Different values of R yield varying classification performances for"}, {"title": "5. Conclusion", "content": "To tackle the challenges posed by long-tail issues in computer-aided diagnosis, we devise the LMD framework aimed at enhancing medical image classification through a two-stage process. At first, we devise the Relation-aware Representation Learning technique to boost the encoder's representation capabilities by incorporating multi-view relation-aware consistency. Subsequently, we present the Iterative Classifier Calibration method, which trains an unbiased classifier by generating numerous virtual features and iteratively refining both the encoder and classifier. Comprehensive experiments conducted on three long-tailed medical datasets validate the effectiveness of the LMD framework, which significantly surpasses the performance of current leading algorithms."}]}