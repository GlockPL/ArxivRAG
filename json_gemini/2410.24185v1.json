{"title": "DexMimicGen: Automated Data Generation for Bimanual Dexterous Manipulation via Imitation Learning", "authors": ["Zhenyu Jiang", "Yuqi Xie", "Kevin Lin", "Zhenjia Xu", "Weikang Wan", "Ajay Mandlekar", "Linxi \"Jim\" Fan", "Yuke Zhu"], "abstract": "Imitation learning from human demonstrations is an effective means to teach robots manipulation skills. But data acquisition is a major bottleneck in applying this paradigm more broadly, due to the amount of cost and human effort involved. There has been significant interest in imitation learning for bimanual dexterous robots, like humanoids. Unfortunately, data collection is even more challenging here due to the challenges of simultaneously controlling multiple arms and multi-fingered hands. Automated data generation in simulation is a compelling, scalable alternative to fuel this need for data. To this end, we introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a handful of human demonstrations for humanoid robots with dexterous hands. We present a collection of simulation environments in the setting of bimanual dexterous manipulation, spanning a range of manipulation behaviors and different requirements for coordination among the two arms. We generate 21K demos across these tasks from just 60 source human demos and study the effect of several data generation and policy learning decisions on agent performance. Finally, we present a real-to-sim-to-real pipeline and deploy it on a real-world humanoid can sorting task. Additional results at dexmimicgen.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Imitation learning from human demonstrations is an effective means to teach robots manipulation skills [1,2]. One popular approach is teleoperation, where human operators control robot arms to collect data, and the data is used to train the robots for autonomous operation [3,4]. Recent efforts have scaled this approach to collect large diverse datasets through teams of human operators, and shown that robots trained on this data can achieve impressive performance and even generalize to different settings [2,5\u20138]. There has also been recent interest in applying this paradigm to humanoid robot embodiments [9-14].\nHowever, data acquisition is a key bottleneck in applying this paradigm more broadly. Prior efforts for data collection in the single robot arm setting required multiple human operators, robots, and months of human effort [2,5-8]. Unfortunately, scaling data collection for humanoids can be even more difficult, owing to the challenges of controlling multiple arms and multi-fingered dexterous hands simultaneously. Enabling real-time teleoperation for humanoids has required the development of special-purpose teleoperation interfaces [9\u2013 14], but these pipelines can be costly and difficult to scale. Furthermore, the increase in operator burden due to multi-arm and multi-finger hand control makes collecting demonstrations in this setting more challenging compared to the single-arm setting, further limiting the rate of data collection. The data acquisition burden is further compounded by the fact that data requirements in the humanoid setting can be higher due to the increased task complexity.\nInstead, leveraging automated data generation in simulation is a compelling alternative that has proved effective for the single-arm robot manipulation setting [15-17]. Inspired by this success, we introduce DexMimicGen (DexMG), a large-scale automated data generation system for bimanual robots with dexterous hands, such as humanoids. The core idea is to leverage a small set of human demonstrations and use demonstration transformation and replay in physical simulation to automatically generate large amounts of training data suitable for imitation learning in the bimanual dexterous manipulation setting. This system builds on top of MimicGen [17], which proposed a similar pipeline for the single-arm with parallel-jaw gripper setting, but there are several technical challenges that DexMimicGen overcomes to apply the same principles to our setting.\nMimicGen relies on decomposing each task into a sequence of subtasks, to generate trajectories for each subtask separately and then stitch them together. Bimanual dexterous manipulation involves three types of subtasks where the two arms need to achieve sub-goals independently, with coordination, and following a specific order. MimicGen, which relies on a single subtask segmentation, struggles to handle the independent and interdependent actions required in bimanual tasks. To address these challenges, DexMimicGen incorporates a flexible per-arm subtask segmentation strategy, allowing each arm to execute its subtasks independently"}, {"title": "II. RELATED WORK", "content": "Data Collection through Teleoperation. Teleoperation is a prevalent approach to gather task demonstrations for robotics [3,4,18-23]. Human operators use an interface to control a robot in real time, and sensor data and robot control commands are logged to a dataset. Some systems allow data collection for multiple robot arms [24-27] and humanoids [9-14,28], and some also enable robot-free data collection using specialized hardware [29-31]. However, all these methods require significant human time and resources to collect large datasets. Some other efforts use pre-programmed experts to automate data generation in simulation [15,16,32-36], but applying these methods to challenging scenarios involving multi-arm coordination can be difficult. By contrast, DexMimicGen builds on MimicGen [17,37,38] to automate data generation using a handful of human demonstrations, greatly reducing the human effort involved in collecting large datasets.\nImitation Learning and Data Augmentation. Behavioral Cloning [39] is an established framework for learning robot control policies from demonstrations and has been used extensively in prior work [33,40-50], including for bimanual manipulators [24-26,51] and humanoid robots [10,11,28,52,53]. In this work, we apply existing imitation learning methods [1,54] to datasets generated by DexMimicGen, but DexMimicGen can play a significant role in facilitating algorithm development for bimanual manipulation by making simulation-based bimanual manipulation datasets more widely accessible and providing easy to reproduce results. Some works leverage offline data augmentation to increase the size of demonstration datasets [1,55\u201369]. By contrast, DexMimicGen collects datasets online, ensuring the generated trajectories are physically valid."}, {"title": "III. PREREQUISITES", "content": "Imitation Learning. We formalize each manipulation task as a Partially Observable Markov Decision Process (POMDP). We are given N demonstrations $D = {(s_0, o_0, a_0, s_1, o_1, a_1, ..., s_{h_i})}_{i=1}^N$ consisting of states $s \\in S$, observations $o \\in O$, and actions $a \\in A$. Each episode starts in a state $s_0 \\sim D$ sampled from the initial state distribution $D \\subset S$. The goal is to learn a policy $\\pi : O \\rightarrow A$ that maps observations to a distribution over the action space. We focus on Behavioral Cloning (BC) [39] methods that find a policy via the maximum likelihood objective $arg\\,max_\\pi E_{(s,o,a) \\sim D}[log\\,\\pi_\\theta(a | o)]$. In this work, we train our policies with datasets generated via DexMimicGen.\nAssumptions. Like MimicGen [17], we make these assumptions. (A1): the action space A consists of the following components for each robot arm: a pose command for an end effector controller and an actuation command for the hand (1- D open/close for parallel-jaw gripper, 6-D joint commands for dexterous hand). (A2): Each task can be divided into object-centric subtasks (see Sec. IV-A). (A3): During data collection, an object's pose can be observed or estimated prior to a robot arm making contact with that object.\nMimicGen. MimicGen [17] uses a small number of source human demonstrations $D_{src}$ to generate a large dataset $D$. It assumes that every task consists of a sequence of object-centric subtasks $(S_1(o_1), S_2(o_2), ..., S_M(o_M))$ where the manipulation in each subtask $S_i(o_i)$ is relative to a single object's coordinate frame ($o \\in O$, where $O$ is the set of objects in the task). It divides each source demo $\\tau_i \\in D_{src}$ into contiguous object-centric manipulation segments {$\\tau_i^m$}$_{m=1}^M$, each of which corresponds to a subtask $S_i(o_i)$. Each segment is a sequence of end effector control poses $(T_W^0, T_W^1, ..., T_W^K)$ where $W$ is the world reference frame. This segmentation can be done with human annotation or using heuristics. To generate a new demonstration in a novel scene, it observes the pose of the object for the current subtask $T_W^t$, and transforms the poses in a source human segment (with a constant SE(3) transform $T_W^t (T_W^0)^{-1}$) such that relative poses between the end effector and object frame are preserved between the source segment and the new scene. It then adds poses to the start of the segment to interpolate between the robot's current state and the start of the transformed segment. Then, it executes the entire sequence of poses open-loop using the robot end effector controller and repeats this process for the next subtask. It checks for task success after executing all subtasks and only keeps the demonstration if it was successful."}, {"title": "IV. DEXMIMICGEN METHOD", "content": "DexMimicGen generates data for bimanual and dexterous manipulation this involves handling three key challenges"}, {"title": "A. Parallel Subtasks", "content": "In the bimanual setting, each arm must be able to operate independently of the other arm. For example, at the start of the Piece Assembly task (Fig. 2 top), each arm needs to grasp a separate object and might finish grasping the object at different points in time. This makes the single fixed sequence of subtasks from MimicGen unsuitable. To enable a flexible order of completion for parallel subtasks involving two arms, we consider each task to consist of a sequence of subtasks for each arm: $S_1^{arm1}(o_1), ..., S_{M_1}^{arm1}(o_{M_1})$ and $S_1^{arm2}(o_1), ..., S_{M_2}^{arm2}(o_{M_2})$. Each source demonstration is split into object-centric manipulation segments as in MimicGen, but now each arm has its own set of segments ({$\\tau_i^m$}, n \u2208 {1,2}).\nHowever, since arm subtasks are defined independently, their execution can start and end at different times that are not aligned. To accommodate this, DexMimicGen employs an asynchronous execution strategy, where an action queue is maintained for each arm. Actions are dequeued for each arm one by one in parallel. Whenever an arm's queue is empty, it is populated with the transformed subtask segment for the next subtask (using the same transformation from MimicGen). This approach allows for the execution of actions for both arms without requiring alignment between subtasks."}, {"title": "B. Coordination Subtasks", "content": "Some tasks require precise coordination, such as placing the lid in the Box Cleanup task (Fig. 2 middle). In these coordination subtasks, the relative poses between the two end-effectors during execution must be aligned with the corresponding relative poses in the source demonstration. To achieve this, we ensure that 1) both arms execute their trajectories in a synchronized manner and 2) the trajectories for both arms are generated with the same transformation. To achieve this temporal alignment, we enforce that coordination subtasks end at the same timestep during source demo segmentation. During execution, we implement a synchronization strategy in which each arm waits for the other until both have the same number of remaining steps in the coordination subtask, aligning the end of subtask execution with the subtask segmentation.\nWe provide different source demonstration transformation schemes to acquire the common transformation matrix for both arms in coordination subtasks. These include the Transform and Replay schemes. The Transform scheme utilizes the transformation matrix $T_W^t (T_W^0)^{-1}$ computed from the object pose at the moment the first arm begins the coordination subtask $T_W^t$ and the object pose in the corresponding source segment $T_W^0$. In contrast, the replay scheme directly uses the source trajectories without applying any transformation. The replay scheme can be beneficial for coordination subtasks like handover because it ensures the trajectory remains within kinematic limits and is fully executable."}, {"title": "C. Sequential Subtasks", "content": "Some tasks require subtasks to be completed in a specific order. For example, in the Pouring task (Fig. 2 bottom), the robot must pour the ball into the bowl with one hand before moving the bowl to the pad with the other hand. To handle these sequential subtasks, we implement an ordering constraint mechanism. We specify a pre-subtask (pouring the ball) and a post-subtask (picking the bowl) based on the task requirement. This mechanism ensures that the arm executing the post-subtask waits until the pre-subtask of the other arm is completed before continuing with the post-subtask."}, {"title": "D. Data Generation for Bimanual Manipulation", "content": "We outline the overall DexMimicGen data generation workflow using the Tray Lift task as an example. First, source demos are segmented into per-arm subtasks using heuristics or human annotation (Fig. 3 left). The final subtask for each arm requires coordination (they must lift the tray together), so it is annotated as a coordination subtask for synchronization during data generation (Sec. IV-B).\nAt the start of data generation, the scene is randomized and a source demonstration is selected (as in MimicGen). We then iteratively generate and execute trajectories for each subtask of each arm in parallel (see Fig. 3 right). In this example, given the pose of the reference object (the tray), we compute the relative transformation between the current tray pose and the tray pose in the source segment. We use this transformation to transform the source trajectories of both"}, {"title": "V. SYSTEM DESIGN", "content": "In order to instantiate DexMimicGen, we build a large collection of simulation environments and a teleoperation system allowing for source human demonstration collection in both simulation and the real world.\nSimulation Environments. We introduce a diverse range of setups and tasks to demonstrate the capability of DexMim- icGen to generate data across different embodiments and manipulation behaviors. The tasks are developed in RoboSuite [70] and use MuJoCo [71] for physics simulation. We focus on three embodiments: (1) bimanual Panda arms equipped with parallel-jaw grippers, (2) bimanual Panda arms with dexterous hands, and (3) a GR-1 humanoid equipped with dexterous hands. We apply different controllers for different embodiments. For the Panda arms, we leverage the Operational Space Control (OSC) [72] framework, which converts the delta end-effector pose into joint torque commands. For the humanoid, we implemented an Inverse Kinematics (IK) controller based on mink [73,74]. We found this to be an effective approach to deal with the complexity of the humanoid kinematic tree, where both arms are linked to a single torso. The IK controller translates global target end-effector poses into robot joint positions. For finger control, we directly use joint position control.\nFor each embodiment, we introduce three tasks, resulting in a total of nine tasks, as depicted in Fig. 4. These tasks involve high-precision manipulation (Threading, Piece Assembly, Box Packing, Coffee), manipulation of articulated objects (Drawer), and are long-horizon (Transport). The tasks also require overcoming key challenges in multi- arm interaction. Several of these tasks contain coordination subtasks, where both arms need to cooperate to finish the subtask (Threading, Transport, Box Packing, Tray Lift, Can Sorting). Other tasks necessitate sequential subtask execution (Piece Assembly, Drawer Cleanup, Pouring, Coffee). We also introduce task variants that broaden the default reset distribution $D_0$ for certain tasks, as in MimicGen. For instance, in the Pouring task, $D_1$ represents a variant where objects have a larger initial reset distribution, while in $D_2$, the reset positions of the bowl and the green pad are swapped. These simulation environments along with the datasets generated by DexMimicGen provide a valuable platform to analyze various factors that influence the performance of imitation learning in the bimanual and dexterous manipulation setting.\nTeleoperation System. To collect source demonstrations for the tasks, we employ different teleoperation methods tailored to each embodiment. For bimanual Panda arms equipped with parallel-jaw grippers, we use an iPhone-based"}, {"title": "VI. EXPERIMENTS", "content": "In this section, we provide empirical evidence showcasing the efficacy of DexMimicGen. We first provide details on experiment setup (Sec. VI-A), then highlight DexMimicGen features and applications (Sec. VI-B), then analyze how data generation and policy learning choices impact policy performance (Sec. VI-C), and finally present a real-world application of the DexMimicGen system (Sec. VI-D).\nExperimental Setup\nWe collect ten source human demonstrations for each task with parallel-jaw grippers, but only five demonstrations for those involving dexterous hands due to the additional operator burden and time cost of collecting demonstrations for dexterous hands. DexMimicGen is subsequently used to generate 1000 demonstrations per task. Each dataset was used to train visuomotor policies through Behavioral Cloning with an RNN [1], an RNN-GMM [1], and a Diffusion Policy [54]. For evaluation, we follow the procedure in [1,17]: we run each experiment across 3 different seeds, and take the maximum policy success rate for each seed.\nDexMimicGen Features\nDexMimicGen significantly boosts the policies' success rates over using the source demonstrations only. Robots trained on DexMimicGen's datasets outperform those trained only on the small source datasets across all tasks (see Table I). Notable improvements include policy performance on Drawer Cleanup (0.7% to 76.0% success), Threading (1.3% to 69.3%), and Piece Assembly (3.3% to 80.7%).\nDexMimicGen produces capable policies across di- verse initial state distributions. DexMimicGen generates datasets with broader initial state distributions ($D_1$, $D_2$) from source demos in $D_0$. As shown in Table II, policies trained on these datasets are performant in the evaluation with the same broader initial state distributions, showing that DexMimicGen generates valuable datasets on new initial state distributions.\nDexMimicGen generates data across different bench- marks. We apply DexMimicGen to BiGym [76], a new sim- ulation benchmark for humanoid robots involving bimanual mobile manipulation tasks. We generate 1000 demonstrations for each of the three tasks, FlipCup, DishwasherLoadPlates, and CupBoardsCloseAll, and achieve data generation success rates of 29.1%, 43.6%, and 76.4%. The visualizations of generated demonstrations can be found on the project website.\nDexMimicGen Analysis\nHow does DexMimicGen data generation compare to alternatives? We compare DexMimicGen with a Demo- Noise data generation baseline, which takes the same source demonstrations as DexMimicGen, but generates data by replaying the source demos with action noise during execution. In Table III, we train policies on datasets of 1000 demos generated by both DexMimicGen and the Demo- Noise baseline. We can see that the policies trained using DexMimicGen outperform those trained on the Demo-Noise baseline by more than 58% across all tasks. Furthermore, unlike DexMimicGen, the Demo-Noise baseline cannot generate results on $D_1$ and $D_2$, as it can only replay the same initial configurations in the source demos.\nDo larger datasets boost policy performance? We train policies on 100, 500, 1000, and 5000 demos generated by DexMimicGen across several tasks (Fig. 5). We observe large boosts in performance from 100 to 500 and 1000, showing that increasing dataset size boosts performance in this data regime; however, the success rate does not always increase from 1000 to 5000, suggesting that there can be diminishing returns depending on the task.\nHow do different DexMimicGen data generation strate- gies impact results? First, we compare the Replay and Transform schemes in the coordination subtask (Sec. IV- B). Specifically, we evaluate two tasks involving the handover subtask with two distinct policies: Transport using BCRNN+GMM, and Can Sorting using a diffusion policy. Replay demonstrates better policy performance (63.3% vs. 46.0%) in the Transport task and achieves comparable outcomes (97.3% vs. 98.6%) in the Can Sorting task. Thus, Replay is our default choice for subtasks that involve handover.\nNext, we assess the effectiveness of ordering constraints in sequential subtasks (Sec. IV-C). When using the same source demonstration for both arms, subtask ordering requirements are typically satisfied automatically. In contrast, employing different source demonstrations for each arm requires an ordering constraint but also increases data diversity. We also evaluate two tasks involving the sequential subtasks with two distinct policies: Drawer Cleanup with BCRNN, and Pouring with diffusion policy. We found training on data generated with ordering constraints consistently outperforms training without them (50.7% vs. 48.0% in Drawer Cleanup and 88.7% vs. 76.7% in Pouring). Directly using the same source demo yields the policy success rates of 56.7% in the Drawer Cleanup and 79.3% in Pouring.\nHow do different policy architecture choices affect success rates? In Table I, we also compare the performance of different policy architectures (Diffusion Policy [54], BC- RNN-GMM [1], BC-RNN [1] with no GMM action head) on the datasets generated by DexMimicGen. We found that Diffusion Policy [54] generally outperforms the other architectures. Interestingly, we also found that BC-RNN- GMM generally underperformed BC-RNN and Diffusion Policy, especially on tasks that involve dexterous hands, in contrast to the RoboMimic study [1] which found the use of a GMM head to be beneficial. We believe DexMimicGen datasets will make it easier for future work to study further how imitation learning choices might differ in the bimanual dexterous manipulation setting."}, {"title": "D. Real-World Evaluation", "content": "We showcase how DexMimicGen enables real-world deployment using the pipeline illustrated in Fig. 1. We generate real-world demonstrations from source demonstrations using a digital twin [77] as safety insurance.\nHardware Setup. We use a Fourier GR1 robot equipped with two 6-DoF Inspire dexterous hands. For vision, we use two Intel RealSense D435i cameras: one head-mounted camera provides a first-person view and one camera in front of the robot as a third-person view.\nDigital Twin Setup. We perform our experiment on the Can Sorting task (Fig. 6), with digital twin assets in simulation that align with the real-world setup. To ensure accurate alignment between the real-world and simulated environments, we perform pose estimation on the objects prior to data collection. Using the head-mounted camera, we capture an initial RGB-D frame and apply GroundingDINO [78] to segment an RGB mask of the object. We use the real world object's center point (determined by averaging the depth values within the RGB mask) to initialize the object's x- and y-coordinates in simulation.\nData Collection Pipeline. Using the teleoperation pipeline described in Sec. V, we collect 4 source human demonstrations for the Can Sorting task. These demonstrations are replayed in simulation, and are used as source demonstrations for DexMimicGen in the digital twin. Next, new real-world demonstrations are collected by synchronizing the initial object state from real to sim, and then attempting to generate a new demonstration in sim with DexMimicGen. If the demonstration is successful in simulation, the sequence of robot control actions is sent to the real-world for execution. In this way, the digital twin functions to ensure safety during real-world data generation, while DexMimicGen mitigates human effort for data collection, which is autonomous apart from the environment resets. We generate 40 successful demonstrations with the approach described above.\nResults. We compare visuomotor policies trained using Diffusion Policy [54] on the 40 DexMimicGen demos with one trained on the 4 source demos. We evaluated both models by running 10 trials each for the red and blue cups. The policy trained on DexMimicGen data achieves 90% success, while the model trained on the source data achieves 0%; DexMimicGen thus offers an efficient pipeline for training real-world robots through the use of a digital twin."}, {"title": "VII. CONCLUSION", "content": "We introduce DexMimicGen, a large-scale automated data generation system that synthesizes trajectories from a small number of human demonstrations for bimanual and dexterous robots, and a collection of simulation environments across 3 embodiments requiring different coordination behaviors. Our findings from applying DexMimicGen to these tasks show that there is great value in further investigating policy learning in this setting. We hope the release of our DexMimicGen datasets and environments will facilitate future research."}, {"title": "VIII. APPENDIX OVERVIEW", "content": "The Appendix contains the following content.\n\u2022 Author Contributions (Appendix IX): list of each author's contributions to the paper"}, {"title": "IX. AUTHOR CONTRIBUTIONS", "content": "Zhenyu Jiang. Co-led project ideation and development. Implemented the data generation code and simulation environments. Oversaw the development of the teleoperation and control infrastructure. Ran most of the experiments in the paper, and wrote the paper.\nYuqi Xie. Core developer of the project. Developed the simulation environments, teleoperation infrastructure for simulation, and rendering pipeline. Ran part of the experiments for humanoids, and the real robot experiments.\nKevin Lin. Co-led project development. Developed the control infrastructure for the simulation experiments, including whole-body IK controllers. Ran part of the experiments for humanoids.\nZhenjia Xu. Implemented the real robot teleoperation and policy deployment infrastructure and helped oversee the real robot experiments.\nWeikang Wan. Implemented the initial prototype of the data generation code and ran the BiGym [76] experiments.\nAjay Mandlekar. Co-led project ideation and development. Implemented simulation environments. Oversaw the development of the main algorithm for data generation, the simulation environments, and the experiments presented in the paper. Advised on the project and wrote the paper.\nLinxi Fan. Co-led project ideation and development. Led resource acquisition for the project, including robot hardware and cluster compute. Provided feedback on paper writing.\nYuke Zhu. Co-led project ideation and development. Provided feedback on experiments and presentation, and wrote the paper."}]}