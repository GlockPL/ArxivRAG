{"title": "Language models as taskmasters: Prompt engineering for direct instruction tuning", "authors": ["Jind\u0159ich Libovick\u00fd", "Alexander Fonarev", "Ond\u0159ej Du\u0161ek"], "abstract": "Direct instruction tuning (DIT) is a family of conditional generation approaches with successful results in text generation and other fields. One DIT variant is prompt DIT which utilizes prompting (i.e. including the desired behavior in the input prompt) to achieve the desired behavior after tuning, while using regular supervised fine-tuning (SFT). This tuning regime is most commonly used on instruction-following language models such as T5, since it provides greater sample efficiency and stability. Despite its popularity, there is still limited research on prompt selection strategies for prompt DIT. In this paper, we explore different methods for prompt design, considering prompt diversity and length, while also assessing its impact across varying model sizes. Additionally, we examine the effects of incorporating a task-specific model and investigate the benefits of a novel prompt ensembling technique that combines multiple prompt candidates. Results obtained for 4 text generation benchmarks show that the ensemble-based approach allows to substantially outperform hand-crafted prompts and allows to scale to models with different instruction-following capabilities. To ensure reproducibility, we release the code and data used in our experiments.", "sections": [{"title": "Introduction", "content": "Direct Instruction Tuning (DIT), which was introduced by Wei et al. (2021), has achieved impressive results in conditional generation across various domains, including natural language processing (NLP). This approach, a variant of supervised fine-tuning (SFT), leverages the principles of instruction following and utilizes prompting as a means of enhancing the learning process. The term instruction following is sometimes used interchangeably with the term prompt, depending on the task and setting, although in the strict sense the prompt represents the input part (i.e. conditioned on), while instruction follows in the output (i.e. to be generated). This contrasts with alternative forms of tuning, such as chain-of-thought prompting, which utilizes interleaved inputs and outputs (Wei et al., 2022) or prompt tuning that adjusts the prefix embeddings of inputs (Li and Liang, 2021). The DIT framework allows to directly tune language models in the supervised setting by introducing instructions as part of the input, which allows the language model to better adapt to different tasks and generalization scenarios. The simplest and most widely used variant of DIT is the prompt tuning regime, where only prompt engineering is employed to improve model performance and convergence (Honovich et al., 2022; Chung et al., 2022). Even more intricate methods of prompt DIT, including self-generated training data (Wang et al., 2023) or iterative self-tuning (Ouyang et al., 2022), show consistent improvements on instruction tuning scenarios. While DIT using prompt engineering is a popular regime for training instruction-following language models (Sanseverino et al., 2023), the influence of prompt engineering is still limited and not yet well-understood. Considering the wide range of prompting techniques available for large language models (LLMs) (Liu et al., 2023), how can we select the prompt that is most appropriate for the DIT regime? Some aspects related to prompt tuning have been considered, such as prompt length (Bach et al., 2022) and the utilization of generated feedback data (Wang et al., 2023). However, little attention has been paid to prompt selection strategies, particularly in the DIT regime. Motivated by the limitations of prompt engineering in the DIT regime, we examine in this study different design techniques for prompts. Our objective is to evaluate diversity in prompt selection, as well as to explore whether task-specific instruction-following models have any impact. Additionally, we investigate the benefits of prompt ensembling, where we merge the knowledge gained from many prompt candidates, resulting in a novel approach that is simple but effective. To provide reproducibility, we release the code and data used in our experiments. In particular, our contributions are: \u2022 We evaluate different prompt selection strategies across varying model sizes using prompt DIT. We show how to combine the prompts in an ensemble, which leads to substantial improvements for varying model sizes and tasks. \u2022 To ensure reproducibility, we release the code and data used in our experiments."}, {"title": "Related Work", "content": "Previous research on instruction tuning has focused on the DIT regime, especially in the context of prompt engineering. This includes exploration on the construction of better instruction datasets (Ouyang et al., 2022; Chung et al., 2022) or intricate techniques for instruction data selection (Wang et al., 2023). Various other parameters, such as prompt length, are known to impact LLM capabilities (Bach et al., 2022). In contrast, the goal of this work is to determine how prompt selection strategies impact DIT convergence and test sample efficiency. The impact of diverse prompt candidate pools on performance is also not well-understood. Lastly, prompt ensembling methods are examined with respect to the DIT regime, and our work offers a new approach for efficient and effective selection of prompts during training."}, {"title": "Experimental Setup", "content": "In this section, we present details regarding the architecture, tasks, the prompt construction and ensembling process, and evaluation metric. Baselines: As a baseline to our approach, we examined the effects of using only a single seed instruction for the entire DIT process, where a single seed instruction $s_i$ is chosen in a random manner. In this setup, the entire dataset of size $N$ would be given the same instruction. Architectures: We trained the instruction following models T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) from scratch across the various datasets used in this work. We varied the size of the models used, including small (60M parameters), base (250M parameters) and large (770M parameters). We trained the models with an NVIDIA A100 for approximately 12 hours, until convergence. Datasets: The models were tested across four datasets, including the CommonGen dataset (Lin et al., 2020), which involves generating text descriptions from a set of concepts, as well as the ToTTo dataset (Parikh et al., 2020), which generates text descriptions given a table. Additionally, we examine tasks with explicit instruction formats, including the GEM task (Gehrmann et al., 2021) and the CoGen task (Dyer et al., 2019). For each dataset, we trained the instruction following models from scratch, and validated across test samples not used during training. Instructions: Each of the datasets is provided with $M$ instructions $s_1, s_2, ..., s_M$ such that each input $x_i$ in dataset $\\mathcal{X}$ is augmented with these instructions. In this work, we investigate several ways of constructing such instructions, such as prompt diversity and length. We detail each of these instruction construction methods below. For each dataset, we use the same $M$ instructions across each of the samples $x_i \\in \\mathcal{X}$ for DIT convergence. \u2022 Diversity: We consider several ways of selecting the initial instruction pool. Since the datasets already provide instructions, we may randomly choose $M$ instructions out of those. Additionally, we can use a backtranslation method, such as to use the GPT-3 API with a diverse set of seed instructions. Alternatively, we can use a model with task-specific abilities, such as to choose seed instructions which perform best out of a set of instructions. Specifically, for the GPT-3 API, we used the text-davinci-003 variant (Ouyang et al., 2022) and prompted it to be as diverse as possible. For the task-specific selection, we choose the instruction performing best as given by a zero-shot setting using the same text-davinci-003 variant. Each of the diversity instructions were created with 10 seed instructions per dataset, and truncated to length of 5 to achieve better computational efficiency. \u2022 Length: We examine the effect of prompt length. For GPT-3, this can simply involve truncating the text from the initial set of diverse prompts of length 5. Alternatively, we can leverage backtranslation to create instructions that satisfy the given length. The length is determined according to Bach et al. (2022), where the length is $10 \\cdot \\sqrt{N}$ where $N$ is the number of training data. \u2022 Ensemble: In this work, we introduce an ensemble strategy for combining multiple instructions. The hypothesis here is that some of the instructions may guide the model during the initial stages of training, while others prove to be more relevant later in the process. Given the set of $M$ instructions per data point, we sample a subset of $Z$ instructions, where $Z<M$. The probabilities of sampling instruction $s_i$ per each epoch $\\epsilon$ is given by the following equation: $P_{\\epsilon}(s_i) = \\frac{z_i + \beta \\cdot \\epsilon}{M + \beta \\cdot \\sum_{\\kappa=1}^M \\kappa} \\quad (1)$ where $\beta$ is the learning rate and $z_i$ is the zero-shot performance of the instruction $s_i$ as determined with text-davinci-003. In this way, we determine the appropriate set of instructions to be used on each gradient step and perform the DIT process. By following Eq. 1, we gradually decrease reliance on the instructions that did not provide as much zero-shot benefit. More details are provided in Section 4. Evaluation: We used the ROUGE-1 score for this approach (Lin, 2004), since our goal is to optimize text generation. To obtain these scores, we used the HuggingFace implementation of the ROUGE metric."}, {"title": "Results", "content": "In this section, we detail the results of the experiment. More details are provided in Section 3.4. For the CommonGen dataset, results with varying length are presented in Table 1. Here, we report the findings using the BART architecture and the results were truncated to prompt length 5, which offered better performance across diverse model architectures. The backtranslation method offered substantial improvements as compared to random, particularly for the small and base models. As a further experiment, we examine the effect of varying model architecture with the backtranslation variant. Additionally, we can use task-specific abilities, such as to choose instructions which perform best out of a set of instructions. For the task-specific selection, we used the instruction performing best as given by a zero-shot setting using text-davinci-003. As shown in Table 2, the task-specific candidate selection offered the most performance with small models, and decreased as model size increased, suggesting its benefits decrease as the architecture already exhibits similar capabilities. As a final experiment, we examine the effects of using prompt ensembling to combine multiple instructions. Here, we choose the GPT-3 instruction diversity, and compare it with randomly choosing $Z$ seed instructions per example. The result of this experiment is given in Table 3, across different architectures, where the ensemble size is 3. The ensemble strategy is substantially better for small models, suggesting that this training regime is important to avoid overfitting, as the initial GPT-3 instruction pool is not necessarily optimal. As model size increases, the benefits of the ensemble tend to diminish."}, {"title": "Analysis", "content": "In this section, we aim to better understand the results and impact of ensembling. As given by Fig 1a and Fig 1b, we plot how the probability weights change over time during ensembling, as given by Eq 1. As shown by each of these plots, at earlier stages of DIT, most of the seed instructions are active, each with approximately equal probability weighting. As DIT progresses, the better instructions (in terms of zero-shot performance) are given more probability weighting, and the low scoring ones tend to become insignificant. This suggests that, at earlier stages of DIT, exploration across diverse seed instructions is helpful to better determine the more important examples. As time progresses, DIT becomes more focused on the instructions most well-suited for DIT. This also confirms that the proposed formulation is effective. As a final experiment, we examine the effects of varying ensemble size across various architectures, as given by Fig 2. In general, increasing ensemble size to a point has a significant effect on small architectures. As ensemble size continues to be increased, benefits tend to diminish, likely due to overfitting, which is particularly prevalent in smaller architectures."}, {"title": "Conclusion", "content": "In this work, we considered different strategies of prompt selection within the direct instruction tuning regime. We examined length and diversity, as well as a novel ensemble-based selection scheme to combine multiple instructions effectively. In particular, the proposed formulation gradually diminishes the less beneficial instructions during DIT, to avoid unnecessary side-effects such as overfitting. By considering different model sizes and datasets, it is possible to show the versatility of the prompt selection scheme, as well as the necessity of using ensembling for smaller architectures. In future work, we will plan to perform analysis from additional perspectives, such as to compare with other approaches to prompt selection."}]}