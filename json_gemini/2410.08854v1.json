{"title": "Hybrid LLM-DDQN based Joint Optimization of V2I Communication and Autonomous Driving", "authors": ["Zijiang Yan", "Hao Zhou", "Hina Tabassum", "Xue Liu"], "abstract": "Large language models (LLMs) have received considerable interest recently due to their outstanding reasoning and comprehension capabilities. This work explores applying LLMs to vehicular networks, aiming to jointly optimize vehicle-to-infrastructure (V2I) communications and autonomous driving (AD) policies. We deploy LLMs for AD decision-making to maximize traffic flow and avoid collisions for road safety, and a double deep Q-learning algorithm (DDQN) is used for V2I optimization to maximize the received data rate and reduce frequent handovers. In particular, for LLM-enabled AD, we employ the Euclidean distance to identify previously explored AD experiences, and then LLMs can learn from past good and bad decisions for further improvement. Then, LLM-based AD decisions will become part of states in V2I problems, and DDQN will optimize the V2I decisions accordingly. After that, the AD and V2I decisions are iteratively optimized until convergence. Such an iterative optimization approach can better explore the interactions between LLMs and conventional reinforcement learning techniques, revealing the potential of using LLMs for network optimization and management. Finally, the simulations demonstrate that our proposed hybrid LLM-DDQN approach outperforms the conventional DDQN algorithm, showing faster convergence and higher average rewards.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) are emerging as a promising technique to address a wide range of downstream tasks such as classification, prediction, and optimization [1]. Conventional algorithms, such as convex optimization and reinforcement learning (RL), experience scalability issues. For example, RL usually experiences a large number of iterations and well-known low sampling efficiency issues, and convex optimization requires dedicated problem transformation for convexity. By contrast, existing studies have shown that LLM-inspired optimization, i.e., in-context learning-based approaches, have several unique advantages: 1) LLMs can perform in-context learning without any extra model training or parameter fine-tuning, saving considerable human effort; 2) LLM-based in-context optimization can quickly scale to new tasks or objectives by simply adjusting the prompts, thus enable rapid adaptation to various communication environments; 3) LLMs can provide reasonable explanations for their optimization decisions, helping human understanding complex network systems. With these appealing features, LLMs have great potential for handling optimization problems.\nOn the other hand, the envisioned 6G networks will observe short channel coherence time (due to higher transmission frequencies and narrow beams) and stringent requirements on the quality of network services, calling for faster response time and higher intelligence. For example, vehicle-to-infrastructure (V2I) communication is closely related to vehicle's driving behaviours, e.g., frequent acceleration and deceleration can result in extra handovers (HOs) and can even result in V2I connection outages (if not handled on a timely basis) [2]. As a result, jointly optimizing V2I communication and autonomous driving (AD) policies is critical. Such integration of V2I and AD also aligns with the 6G usage scenarios defined in International Mobile Telecommunications-2030 (IMT-2030) by the International Telecommunication Union [3].\nLLM-based AD has attracted significant research interest, and existing studies have leveraged LLM's reasoning capabilities to enable decision-making [4] and leading human-like AD behaviours [5]. While a couple of research works [4], [5] considered LLMs for AD, the problem of jointly optimizing V2I communication and AD policies has only been solved using traditional RL techniques [2], [6]. Therefore, the performance gains of using LLM in solving the joint V2I connectivity and AD problem are unexplored.\nTo this end, this article aims to investigate the benefits of LLMs in solving the joint optimization of V2I communication and AD policies. Since most existing LLMs, such as GPT and Llama, are pretrained for general domain purposes, we propose a hybrid LLM-DDQN based iterative solution. In particular, we first employ LLMs for AD decision-making, aiming to maximize speed, reduce lane changes, and collisions. Using the pre-trained real-world knowledge, LLMs can better understand complicated road environments and quickly make AD decisions. We introduce the language-based task description, example design and distance-based selection, enabling LLMs to explore and learn from previous experience. After that, the AD decision will be used by the double deep Q-Networks (DDQN) for V2I optimization, which is designed to improve data rates with fewer handovers. The whole problem will be iteratively optimized until convergence. Our simulation results show that the proposed LLM-based method has higher learning efficiency and faster convergence than baselines. Here, we select DDQN for V2I optimization as we note DDWN's superior performance compared to LLMs. Also, the performance of DDQN is well-established and has been demonstrated in many existing studies [2], [6]."}, {"title": "II. SYSTEM MODEL AND MDP DEFINITIONS", "content": "This section will introduce the considered system model for AD and V2I connectivity as well as their corresponding Markov decision processes (MDPs). MDP is a very useful approach to define optimization tasks, therefore, we modeled this problem using an MDP."}, {"title": "A. V2I and AD System Models", "content": "As shown in Figure 1, we consider a downlink network consisting of NR RF base stations (RBSs) and NT THz base stations (TBSs) in a multi-vehicle environment. Specifically, from the V2I communication perspective, we aim to maximize the V2I data rate and minimize handovers. From the transportation perspective, road safety must be considered, which will optimize velocity and minimize collisions.\nWe assume there are M\u2081 autonomous vehicles (AVs) receiving data from roadside BSs [2]. Each AV is associated with a single BS (either RBS or TBS). The AVs' onboard units collect real-time data from the VNet, including neighboring vehicles' velocity, acceleration, and lane positions. The signal-to-interference-plus-noise ratio (SINR) for the j-th AV from BS i is modelled as in [2]:\n$SINR_{ij}^{RF} = \\frac{P_{tx}^{R} G_{tx}^{R} G_{rx}^{R} C}{(d_{ij}^{R})^{4R}} \\frac{|H_{ij}|^{2}}{\\sigma^{2} + I_{R_{j}}}$ (1)\nwhere $P_{tx}^{R}$, $G_{tx}^{R}$, $G_{rx}^{R}$, $C$, $f_{R}$, and \u03b1 denote the transmit power of the RBSs, antenna transmitting gain, antenna receiving gain, speed of light, RF carrier frequency (in GHz), and path-loss exponent, respectively. Note that $r_{ij}^{R} = (d_{ij} + h)^{1/2}$, where $d_{ij}$ is the 2D distance between the j-th AV and i-th BS and h is the transmit antenna height. In addition, $H_{ij}$ is the exponentially distributed channel fading power observed at the j-th AV from the i-th RBS, \u03c3\u00b2 is the power of thermal noise at the receiver, $I_{R_{j}}$ is the cumulative interference at j-th AV from the interfering RBSs. The SINR in THz network for a given j-th AV can be modelled as follows [7]:\n$SINR_{ij}^{THZ} = \\frac{G_{tx}^{T} G_{rx}^{T} P_{tx}^{T} exp(-K_{a}(f_{T})r_{ij}^{T})}{N_{T_{j}} + I_{T_{j}}}$ (2)\nwhere $G_{tx}^{T}$, $G_{rx}^{T}$, $P_{tx}^{T}$, $f_{T}$, and $K_{a}(f_{T})$ represent the transmit antenna gain of the TBS, the receiving antenna gain of the TBS, the transmit power of the TBS, the THz carrier frequency, and the molecular absorption coefficient of the transmission medium, respectively. We assume all AVs are"}, {"title": "B. Autonomous Driving MDPs", "content": "1) States: The transportation state-space consists of kinematics-related features of AVs, represented as $M_{1} \\times F_{AD}$ array, where $F_{AD} = {x_{j}, y_{j}, v_{j}, \\psi_{j}}$ for $j \\in [1, M_{1}]$ describes the j-th AV's coordinates (x, y), velocity $v_{j}$, and heading $\\psi_{j}$. We model M\u2081 target AVs and M2 surrounding AVs. The aggregated state space $S^{AD}$ at time step t is:\n$S^{AD} = [x_{j}, y_{j}, v_{j}, \\psi_{j}]$\n2) Actions: At each time step t, AV j selects an autonomous driving action $a^{AD} \\in A^{AD}$, where the driving action space is $A^{AD} = {a_{1}^{AD},...,a_{N}^{AD}}$, representing acceleration, maintaining current lane, deceleration, lane change to left, and lane change to right.\n3) Rewards: The transportation reward is defined as:\n$r_{t}^{i,AD} = c_{1} \\frac{v_{i}}{v_{max}-v_{min}} - c_{2}\\delta_{2} + c_{3}\\delta_{3} + c_{4}\\delta_{4}$, (3)\nwhere $v_{i}$ is the AV's velocity, and $\\delta_{2}$, $\\delta_{3}$, $\\delta_{4}$ represent collision, right-lane, and off-road indicators, respectively. The weights $c_{1}$ and $c_{2}$ adjust the AD reward by incorporating a collision penalty. Specifically, $c_{1}$ scales the reward received when driving at full speed. The reward decreases linearly as the speed decreases, reaching zero at the lowest speed. $c_{2}$ is the collision penalty, which is significantly larger than the other coefficients. To encourage the AV to drive on the rightmost lanes, $c_{3}$ decreases linearly as the AV deviates from the rightmost lane, reaching zero for other lanes. $c_{4}$ is the on-road reward factor, which penalizes the AV for driving off the highway."}, {"title": "C. Vehicle-to-Infrastructure MDPs", "content": "We assume each RBS and TBS can support a maximum of $Q_{R}$ and $Q_{T}$ AVs, respectively [6]. The connection is satisfactory if the SINR can meet or exceed a threshold $Y_{th}$. The number of AVs that each BS can serve at a given time is denoted by $n_{i}$. As AVs move along the road, they switch BS connections when $SINR_{ij} < Y_{th}$. Frequent HOs will reduce the data rate due to HO latency. To mitigate this, a HO penalty \u03bcis introduced, with a higher penalty for TBSs and a lower one for RBSs. The weighted data rate between BS i and AV j is modeled as follows:\n$WR_{ij} = \\frac{R_{ij}}{min (Q_{i}, n_{i})} (1 - \\mu)$, (4)"}, {"title": "III. Hybrid LLM-DDQN BASED DUAL OBJECTIVE OPTIMIZATION ALGORITHM", "content": "As shown in Fig. 1, we propose an iterative optimization framework to integrate LLMs and DDQN to address joint AD and V2I decision-making. The first stage involves utilizing LLMs to make autonomous driving decisions. Then the second stage incorporates the DDQN algorithm, focusing on V2I decisions of selecting proper BSs. In the context of LLM in-context learning [1], [8], [9], we integrated the textual context into the formatted model. Our problem is divided into transportation and V2I steps.\n$D_{task}^{AD} \\times E_{t} \\times S^{AD} \\times LLM \\Rightarrow a_{t}^{AD}$, (6)\n$S^{V2I} \\times a_{t}^{AD} \\times DDQN \\Rightarrow a_{t}^{V2I}$, (7)\nHere equation (6) shows the decision-making process of LLMs. In particular, $D_{task}^{AD}$ represents the transportation task descriptions, providing fundamental task information to the LLM, i.e., goals and decision variables. $E_{t}$ is the set of examples at time t, serving as demonstrations for LLMs to learn from, including both positive and negative examples. $S^{AD}$ indicates the current environment state which is associated with the target task. Then LLM refers to pre-trained LLM models, and $a_{t}^{AD}$ is the transport decision as we defined in Section II-C. Meanwhile, equation (6) shows the decision-making of DDQN, including communication network state $S^{V2I}$, DDQN model $D^{DDQN}$ and network decision $a_{t}^{V2I}$. It is worth noting that the transportation decision $a_{t}^{AD}$ is also included in equation (6). It means that the transport decision will affect the performance of V2I systems, and we address this problem using an iterative optimization approach."}, {"title": "A. Language-based Task Description", "content": "This subsection will introduce the design of the task description $D_{task}^{AD}$ defined in equation (6), which is essential for providing the LLM model with transportation task information. Specifically, $D_{task}^{AD}$ includes the following key components: \"Task Description\", \"Task Goal\", \"Task Definition\", and additional \u201cDecision\u201d criteria. Below, we present a detailed task description to prompt the LLM effectively."}, {"title": "B. Example Design and Distance-based Selection", "content": "The above discussions have shown the crucial importance of demonstrations, which are inspired by the well-known \"experience pool\" in DRL algorithms. There are two major challenges in selecting examples. First, we must provide accurate and unbiased examples to ensure accuracy. Second, we cannot send infinite examples to the LLM due to token number constraints. Furthermore, in our joint optimization problem, the ideal decision for autonomous driving will enhance V2I training performance in the next phase. Since the features defined in $S^{AD}$ have an infinite number of examples, identifying useful examples remains challenging. The transportation example $E_{t} \\in E$ is defined by:\n$E_{t} = {S_{t}^{AD}, a_{t}^{AD}, r_{t}^{ego}, a_{t-1}^{AD}}$, (8)\nWe classify the examples as good or bad based on collisions. After action $a_{t}^{AD}$ is applied to the target AV, if the AV truncates or collides, we add this example $E_{t}$ to the bad example pool $E_{t}^{bad} = E_{t-1}^{bad} \\cup E_{t}$, and the good example pool remains the same as the last time step. Otherwise, we add this example to the good example pool $E_{t}^{good} = E_{t-1}^{good} \\cup E_{t}$, and the bad example pool remains the same as the last time step.\nWe select the top-K examples with the closest Euclidean distance. After accumulating good and bad examples, we need to prioritize the relevant appropriate examples. In this work, we select the top-K relevant examples per category (good or bad) via Euclidean distance before forwarding them to LLM agents. Considering we want to filter the relevant good examples from $E_{t}^{good}$ for the current state $S_{t}^{AD}$, which dimension is $M_{1} \\times N_{f}$. We flat this 2D state to a scalar series {(x, y, v, \u03c8,...,xM1, yM1, vM1, \u03c8M1)}, euclidean distance d is defined as:\n$d = \\sum_{j=1}^{M_{1}} |x_{j}^{i} - x_{j}^{E_{t}}| + |y_{j}^{i} - y_{j}^{E_{t}}| + |v_{j}^{i} - v_{j}^{E_{t}}| + |\\psi_{j}^{i} - \\psi_{j}^{E_{t}}| $, (9)\nFor each feature pair between the instant observation at time step t and an example in the $E_{t} \\in E_{t-1}$. We calculate the euclidean distance between the closest M\u2081 AVs. d is smaller while $E_{t}$ is similar to current observation. This approach ensures we collect the K closest relevant past examples and sort them based on the highest reward $r_{t}^{ego, AD}$"}, {"title": "C. DDQN Based V2I Action Selection", "content": "As depicted in Fig. 1, LLMs will first make AD decisions $a_{t}^{AD}$. Simultaneously, V2I observations, $S^{V2I}$, are collected. The closest $M_{1}$ AVs with 3 features above are gathered to form a MDP. The DDQN processes this MDP to compute the V2I action $a_{t}^{V2I}$. DDQN employs a neural network parameterized by $O_{t}$ at each time step t to approximate the Q-value function for a given state-action pair, i.e., Q(SV2I, aAD, aV2I; \u0398t). To improve the stability and reliability of the estimates, DDQN uses a separate set of parameters, , for the target network, thereby decoupling the action selection from the value evaluation [6]. This decoupling mitigates the overestimation of Q-values that can occur in standard DQN. At each time step, the evaluation network with parameters Ot computes the Q-value, while the target network, parameterized by provides stable Q-value estimates for the target. The evaluation network is trained by minimizing the mean squared error (MSE) loss L(\u0398t) between the predicted Q-values, Q(SV2I, aAD, aV2I; \u0398t), and the target Q-values, according to the Bellman equation as,\n$L(\\Theta_{t}) = E_{(s,a,r,s')} [(r_{t}^{i,tele} + \\gamma Q (S^{V2I}_{t+1}, arg max_{a'^{V2I}} Q(S^{V2I}_{t+1}, a'^{AD}_{t+1}, a'^{V2I}_{t+1}; \\Theta'_{t}) - Q(S^{V2I}_{t}, a^{AD}_{t}, a^{V2I}_{t}; \\Theta_{t}))^{2}]$ (10)"}, {"title": "D. Hybrid LLM-DDQN Learning and Convergence Analysis", "content": "As illustrated in Fig. 1, our proposed hybrid LLM-DDQN scheme consists of the following steps:\n1) Collecting observations: The observation pool collects both V2I and AD data from the ego AV j and the surrounding $M_{1}$ AVs. The observation will be separated into Transportation observations and V2I observations.\n2) LLM-based AD decision-making: Using the transportation observations, relevant past examples are computed as described in Section III-B. This information is then incorporated into a task-oriented prompt, as outlined in Section III-A. The prompt is sent to the LLM, which returns an AD decision for the current time step.\n3) DDQN-based V2I decision-making: Using the V2I observations from step 1 and the AD decision from step 2, the V2I action is determined via DDQN as in Section III-C.\n4) Applying actions to target AVs: Both AD and V2I actions from steps 1 and 2, is applied to the target AV for implementation.\n5) Storing examples in the example pool: After the AV executes its actions, if the AV terminates unexpectedly, the example is stored in the ; otherwise, it is stored in the as explained in Section III-B.\nFinally, we will analyze the convergence of the proposed hybrid LLM-DDQN scheme. It is worth noting that LLM-based AD decisions will serve as part of the V2I states as defined in Section II-C. It means that AD decisions can be considered as environment states for DDQN-based V2I systems. Therefore, for the AD side, LLM will produce a stable policy when enough examples are collected, which is similar to DDQN algorithms with experience pools. For the V2I side, since AD decisions can be considered as external states, the overall convergence will still hold."}, {"title": "IV. SIMULATION AND PERFORMANCE EVALUATION", "content": "We assume a total of 21 AVs traveling on a 3km single-direction, 4-lane highway. Along both sides of the highway, 5 RBSs and 20 TBSs are randomly positioned. This work includes two LLMs for AD decision-making:\n1) Llama3.1-8B + DDQN: Llama3.1-8B is a small-scale model with 8 billion parameters, which is suitable for deployment at the network edge. 2) Llama3.1-70B + DDQN: Llama3.1-70B is the latest large-scale LLM model with 70 billion parameters, which has more powerful reasoning capabilities. 3) Llama3.1-70B + Llama3.1-70B: Employing two sequential Llama3.1 agents to perform AD and V2I actions simultaneously. 4) Llama3.1-70B Joint: Employing single Llama3.1 agents to generate Joint AD and V2I actions directly. We also include a DDQN baseline algorithm, which employs DDQN for joint optimization of V2I and AD [6]. The Markov decision process defined in Section II is shared across these benchmarks and our proposed solutions.\nFig. 2 presents the simulation results and comparisons. The performance metrics include the AD reward and V2I reward, as defined in (3) and (5), along with the collision rate and HO probability. In particular, Fig. 2(a) and (b) demonstrate that all LLM-based models show an improvement in both AD and V2I rewards, outperforming the DDQN baseline.\nFig. 2(c) presents a reduction in collision rates as the number of episodes increases, with faster convergence being observed. These results highlight the ability of hybrid LLM models to balance experience replay and self-exploration, learning effectively from past examples and explorations to enhance their performance on target tasks. In addition, all algorithms have a similar HO probability.\nFig. 2(e) to (h) depict the performance variation as the number of AVs increases. Each data is captured by using the average performance of the last 200 episodes. As shown, increasing the number of AVs results in more congested highway scenarios, leading to more competitive resource allocation. This, in turn, results in higher HOs among the travelling vehicles. However, our proposed hybrid Llama+DDQN method still outperforms conventional DDQN approaches with higher average reward and lower collision rate. Finally, note that the LLM performance is closely related to its model size and designs. Specifically, Llama-3.1-70B outperforms Llama-3.1-8B because a larger model size usually indicates better reasoning and comprehension capabilities. Dual-agent or joint decision-making using LLMs does not perform well, as multi-task and multi-objective problems remain challenging for LLMs. These models struggle to learn effectively in complex scenarios with numerous constraints."}, {"title": "V. CONCLUSION", "content": "LLMs have great potential for 6G networks, and this work proposed a hybrid LLM-DDQN scheme for joint V2I and AD optimization. It explores natural language-based optimization and network management, revealing the potential of LLM techniques. The simulations demonstrate that our proposed method can achieve faster convergence and higher learning efficiency than existing DDQN techniques."}]}