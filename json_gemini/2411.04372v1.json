{"title": "Benchmarking Large Language Models with Integer Sequence Generation Tasks", "authors": ["Daniel O'Malley", "Manish Bhattarai", "Javier Santos"], "abstract": "This paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the On-line Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the ol series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.", "sections": [{"title": "1 Introduction", "content": "Benchmarking plays a crucial role in the development and evaluation of large language models (LLMs), helping researchers and users gauge their abilities across various domains such as natural language understanding, knowledge retrieval, and mathematical reasoning. The progress that LLMs have made on challenging benchmarks is remarkable matching even the performance of humans with a Ph.D. on advanced problems in the human's domain of expertise. With the release of more powerful models like OpenAI's ol series, there is a need for benchmarks that can rigorously test more advanced abilities of these systems.\nIn this paper, we introduce a novel benchmark based on integer sequence generation tasks sourced from the Online Encyclopedia of Integer Sequences (OEIS) [1, 2]. OEIS provides a wealth of information on these sequences under a CC BY-SA 4.0 license, including sequence values that can be used for unit"}, {"title": "2 Related Work", "content": "Benchmarking has been essential in evaluating the capabilities of LLMs across various domains, particularly in mathematical reasoning and code generation.\nExisting benchmarks such as MATH [10], GSM8K [11], and HumanEval [12] as-"}, {"title": "3 Benchmark Design", "content": "In designing the benchmark, our goal was to create a robust and rigorous evaluation framework that challenges frontier LLMs. The benchmark is centered around writing code that computes elements of integer sequences, using sequences sourced from the OEIS [1]. The design incorporates various levels of difficulty and enforces performance constraints to measure both the accuracy and efficiency of the models."}, {"title": "3.1 Dataset Selection", "content": "The dataset for the benchmark is derived from OEIS, an extensive database of integer sequences contributed by mathematicians worldwide. We selected 250 easy and 250 hard sequences based on OEIS labels. The set of sequences is defined as S = Seasy \u222a Shard, where Seasy are the first 250 sequences labeled as easy, and Shard are the first 250 sequences labeled as hard in OEIS.\nThis selection provides a broad spectrum of sequence generation problems, ranging from basic arithmetic operations to complex mathematical computations."}, {"title": "3.2 Problem Definition", "content": "For each sequence s \u2208 S, an LLM M is tasked with generating Python code Cs that computes the first N terms of the sequence s, where N is a fixed positive integer (e.g., N = 10). Each integer sequence is a function:\n$s : {i_0 + j}_{j=0} \\rightarrow \\mathbb{Z}$,\nwhere i0 is an offset indicating where the sequence starts. The code Cs should define a function\n$f_s : {i_0 + j}_{j=0} \\rightarrow \\mathbb{Z}$\nsuch that $f_s(n) = s(n)$ for all $n \u2265 i_0$.\nThe following constraints are imposed on the generated code: (1) the code Cs must not contain a lookup table of the sequence terms, (2), the execution time ts of Cs must satisfy ts < T where T is a predefined time limit (we use"}, {"title": "3.3 Evaluation Metrics", "content": "To provide a comprehensive evaluation of the models, we measure their performance using three factors: accuracy, efficiency, and avoiding lookup tables.\nFor each sequence s, we define the accuracy As(n) as:\n$A_s(n) =\\begin{cases}0 & f_s(n) \\neq s(n) \\\\0 & t_s > T \\\\0 & \\text{cheating} \\\\1 & \\text{otherwise}\\end{cases}$\nWe report the average accuracy over all sequence values in Seasy and Shard. Note that fs(n) must execute within the time limit T or the delta fun"}, {"title": "3.4 Cheating Detection Mechanism", "content": "Another core aspect of the benchmark is ensuring that models produce algorithms rather than lookup tables of sequence values. To enforce this, we use GPT-40's structured output function (with temperature 0 to maximize reproducibility) to check the code output by the model and flags cases where lookup tables are employed. Any model that is found to be cheating by using a lookup table receives a score of zero for that task, regardless of the accuracy of the output. This cheating detection mechanism's effectiveness was validated by comparing it with a human evaluation (one of the authors, who was not provided with the GPT-40 cheating evaluations beforehand). The human was provided the same instructions as GPT-40, which are provided in the supplementary information. The automated approach achieved 86% agreement overall and 94% agreement on hard sequences, where cheating is more prevalent. The main mode of disagreement between the human evaluator and GPT-40 is that GPT-40 flags more codes as cheating."}, {"title": "4 Experiments and Results", "content": "We evaluate nine state-of-the-art LLMs on our integer sequence benchmark using their default settings (temperature, etc.). Table 1 summarizes the models' performance on the easy (Seasy) and hard (Shard) sequence sets. Figure 2 shows more details about the distribution of scores. There are only small differences when the 0.5s and 4s time limits are used, so we focus our discussion on the 4s case. The ol models give the best performance for both easy and hard sequences, scoring 63% or higher on the easy sequences and 18% or higher on the hard sequences. The ol-mini model performs slightly better than ol-preview on the easy sequences and vice-versa for the hard sequences. The ol"}, {"title": "5 Discussion", "content": "The superior performance of o1-mini highlights the effectiveness of specialization in LLMs for mathematical reasoning and coding tasks. ol-mini's higher accu-"}, {"title": "6 Conclusion", "content": "We introduced a rigorous benchmark to evaluate large language models on generating code for integer sequences from the OEIS, focusing on mathematical reasoning and computational efficiency. Our evaluation demonstrated that reasoning models like o1-mini ol-preview outperform general-purpose models such as GPT-40, Claude 3.5 Sonnet, and Gemini 1.5 Pro in tasks requiring mathematical insight and algorithmic coding skills. Specifically, the ol models achieved higher accuracy and lower cheating rates on both the easy (Seasy) and hard (Shard) sequence sets. Despite these strengths, all models showed low performance on the hard sequences, underscoring the challenges LLMs face in generating complex algorithms within practical time constraints. The frequent reliance on memorization strategies, like using lookup tables, highlights the need for developing models capable of genuine algorithmic reasoning. Our benchmark effectively assessed the computational reasoning abilities of LLMs, with the OEIS dataset providing a robust and diverse evaluation framework. The implemented cheating detection mechanism was crucial in ensuring adherence to algorithmic constraints and maintaining the integrity of the assessment."}, {"title": "7 Limitations", "content": "Our benchmark possesses several limitations that warrant consideration. First, relying exclusively on the OEIS as the source of integer sequences may introduce biases due to the specific types and distributions of sequences included, as well as their subjective labeling as \"easy\" or \"hard.\" Another potential issue is that some OEIS sequences have associated code snippets that are publicly available. While this could assist LLMs in generating the correct sequence, many of the"}]}