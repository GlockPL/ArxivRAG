{"title": "On Creating a Causally Grounded Usable Rating Method for Assessing the Robustness of Foundation Models Supporting Time Series", "authors": ["Kausik Lakkaraju", "Rachneet Kaur", "Parisa Zehtabi", "Sunandita Patra", "Siva Likitha Valluru", "Zhen Zeng", "Biplav Srivastava", "Marco Valtorta"], "abstract": "Foundation Models (FMs) have improved time series forecasting in various sectors, such as finance, but their vulnerability to input disturbances can hinder their adoption by stakeholders, such as, investors and analysts. To address this, we propose a causally grounded rating framework to study the robustness of Foundational Models for Time Series (FMTS) with respect to input perturbations. We evaluate our approach on the stock price prediction problem, a well studied problem with easily accessible public data, evaluating six state-of-the-art (some multi-modal) FMTS across six prominent stocks spanning three industries. The ratings proposed by our framework effectively assess the robustness of FMTS and also offer actionable insights for model selection and deployment. Within the scope of our study, we find that (1) multi-modal FMTS exhibit better robustness and accuracy compared to their uni-modal versions and, (2) FMTS pre-trained on time series forecasting task exhibit better robustness and forecasting accuracy compared to general-purpose FMTS pre-trained across diverse settings. Further, to validate our framework's usability, we conduct a user study showcasing FMTS prediction errors along with our computed ratings. The study confirmed that our ratings reduced the difficulty for users in comparing the robustness of different systems.", "sections": [{"title": "1 Introduction", "content": "Time series (TS) forecasting uses historical data indexed by time to predict future values. This task finds wide applicability in industry in domains like finance, healthcare, manufacturing, and weather. Although well-studied, the TS forecasting has seen recent advancements with new AI-based approaches including gradient boosting, deep learning, transformers and Foundation Models (FMs) trained on uni-modal numerical data as well as multi-modal data vying for state-of-the-art performance (Elsayed et al. 2021; Jin et al. 2023). However, having good performance is no guarantee that users will trust a method or model and use it. In particular, users care about the model's robustness to noisy data and perturbations, as erroneous predictions can have far-reaching impact on stakeholders. The perturbations may have been caused unintentionally by an actor or intentionally by an adversary, but regardless, the users expect robust and consistent performance. To manage user trust, a promising idea is of third-party assessment of models and ratings (automated certifications), which can help users make informed decisions, even without access to the method's code or model's training data using both statistical (Srivastava and Rossi 2018, 2020; Srivastava et al. 2024) and causality-based methods (Lakkaraju et al. 2023; Lakkaraju, Srivastava, and Valtorta 2024).\nIn this context, our contributions are that we: (a) introduce a novel workflow for assessing and rating FMTS for robustness through causal analysis. (b) introduce three perturbations for both numerical and line plots (image) data inspired by real-world applications in unintended scenarios. (c) introduce two novel metrics to measure the causal impact of other attributes on the FMTS (along with existing metrics from literature). (d) create ratings to compare the models in terms of forecasting accuracy and robustness. (e) conduct a user study to assess the ease of interpreting FMTS behavior through our ratings and to assess its alignment with users' perceptions. With our causally grounded rating framework, we evaluate leading FMTS models-two general-purpose-data trained (Gemini-V and Phi-3 in both uni-modal and multi-modal forms) and two time series-data trained (MOMENT and Chronos) - across diverse architectures (encoder-only, decoder-only, and encoder-decoder) and parameter sizes (46M to 32B), along with three baseline models (ARIMA, random, and biased) (total of 9 models). We use one year of stock price data from six leading companies across three different industries as test data. We find that for both Gemini and Phi-3, their multi-modal versions, in general, exhibit better robustness and forecasting accuracy compared to their uni-modal versions (Figure 6). We also find that time series-specific FMTS exhibit better robustness and forecasting accuracy compared to general-purpose FMTS (Figure 7). Furthermore, the user study confirms that our ratings reduced the difficulty for users in comparing the robustness of different systems."}, {"title": "2 Related Work", "content": "Foundation Models Supporting Time Series The use of FMs for time series forecasting has advanced significantly. (Lu et al. 2022) showed that transformers pre-trained on text data can solve sequence modeling tasks in other modalities, enabling their application to time series analysis. Recent studies have reprogrammed LLMs for time series tasks through parameter-efficient fine-tuning and tokenization strategies (Zhou et al. 2023; Gruver et al. 2023; Jin et al. 2023; Cao et al. 2023; Ekambaram et al. 2024). (Zhou et al. 2023) and (Jin et al. 2023) further illustrate the versatility and robustness of fine-tuned language pre-trained transformers for diverse time series tasks. Several models"}, {"title": "3 Problem", "content": "3.1 Preliminaries\nTime Series Forecasting Let the time series be represented by {xt-n+1, Xt-n+2, ...., Xt, Xt+1, ..., Xt+d}, where each Xt-n+i represents a value in time series, where n is called"}, {"title": "3.2\nProblem Formulation", "content": "We aim to answer the following research questions (RQs) (with causal diagrams in Fig 2) through our causal analysis when different perturbations denoted by P = {0, 1, 2, 3} (or simply P0, P1, P2, P3) are applied to the input given to the set of FMTS S:\nRQ1: Does Z affect Rmax, even though Z has no effect on P? That is, if perturbations are independent of the sensitive attribute, can the attribute still affect the system outcome,"}, {"title": "4 Solution Approach", "content": "Our solution approach consists of the following components: (1) Three perturbations that are applied to both numerical time series data and line plots. (2) Metrics, APE and PIE % that assess the performance of FMTS from accuracy and robustness perspectives and aid in assigning ratings. (3) A 'data to predictions' and 'predictions to ratings' workflow for rating FMTS using both uni-modal (numerical) and multi-modal (line plots (image)) data.\nPerturbations: We introduce one syntactic (STP) and two semantic perturbations (SMP) (Figure 3a) inspired by real-world applications in unintended scenarios to assess the accuracy and robustness of FMTS. SMPs alter data meaning while preserving context, e.g., a stock's value might change due to data entry errors or market-specific catalysts. STPs modify the structure of the data while preserving without altering the content. Drop-to-zero (P1) is an SMP inspired by common data entry errors (Ley et al. 2019). Every nth value in the original stock price data is set to zero. Sampling the time series with a sliding window of size n ensures each sample contains a zero. In Value halved perturbation (P2), an SMP, we reduce every nth number in the original stock price data to half of its value. This perturbation simulates periodic adjustments, possibly reflecting events like stock splits or dividend payments. Missing values perturbation (P3), an STP, converts every nth number in the original stock price data to a null value, simulating real-world missing data points in financial datasets due to system incomplete transmissions."}, {"title": "4.1\nEvaluation Metrics", "content": "In this section, we describe our evaluation metrics for measuring forecasting accuracy and robustness.1\nForecasting Accuracy Metrics We evaluate forecasting accuracy using three metrics (Makridakis, Spiliotis, and Assimakopoulos 2022): Symmetric mean absolute percentage error (SMAPE) measures average percentage error between actual and predicted values. Mean absolute scaled error (MASE) compares the mean absolute error of forecasts to a"}, {"title": "4.2\nWorkflow", "content": "Our proposed workflow consists of two parts: Data to Predictions and Predictions to Ratings. In the first part, as shown in Figure 3a2, FMTS process the input and predict the next 'd' timesteps. The FMTS and baseline models are described in Section 5.1. In the second part, illustrated in Figure 3b, we adapt the method from (Lakkaraju, Srivastava, and Valtorta 2024) to rate text-based sentiment analysis systems for bias. We extended their approach to handle our more complex multi-modal data with multiple perturbations, beyond the original textual data and binary treatments. The modified metrics, APE and PIE%, defined in Section 4.1, help manage this complexity. The rating algorithms are"}, {"title": "5 Experiments and Results", "content": "This section introduces the FMTS used in our experiments, baseline models, test data, and evaluation metrics, including two new metrics for perturbations and confounders. We also present the user study design, responses, and findings."}, {"title": "5.1 Experimental Apparatus", "content": "FMTS We used four FMs in a zero-shot setting: TS forecasting FMs (MOMENT and Chronos) and general-purpose (GP) multimodal FMs (Gemini-V and Phi-3) adapted for TS forecasting. We set n = 80 and d = 20. Table 1 provides an overview of the FMTS architectural details."}, {"title": "5.2 Experimental Evaluation", "content": "In this section, we describe the experimental setup used to address the RQs stated in Section 3, the results obtained, and the conclusions drawn from the results. Figure 2 shows the causal diagrams used to answer the RQs.\nRQ1: Does Sensitive Attribute affect the Residual, even though Sensitive Attribute has no effect on Perturbation?\nSetup: In this experiment, the causal link from the Sensitive Attribute to Perturbation is absent, as the perturbation to the stock prices does not depend on the corresponding company name or the industry i.e., perturbations are applied uniformly across all the data points. We quantify the statistical bias exhibited by the systems by using WRS described in Section 4.1. We perform two different analyses in this experiment: one to measure the discrepancy shown across different industries (WRSIndustry) and another to measure the discrepancy among both the companies (WRSCompany) within the same industry. Results and conclusion: From Figure 4, most discrepancies can be observed across industries compared to the discrepancies across companies within each industry. When the input data was subjected to perturbation P3, the systems exhibited more statistical bias. From Figure 5, Sa exhibited the least statistical bias, while Sp exhibited the highest statistical bias among the systems evaluated under the perturbations considered. Hence, we conclude that Sensitive Attribute affects the Residual, even though Sensitive Attribute has no effect on Perturbation.\nRQ2: Does Confounder affect the relationship between Perturbation and Residual, when Confounder has an effect on Perturbation?\nSetup: In this experiment, we use PIE % defined in equation 2 to compare the APE (defined in equation 1) before and after deconfounding using the PSM technique as the presence"}, {"title": "5.3 Overall Performance Comparison", "content": "Now, we provide an overall comparison of the different systems across all metrics to highlight key findings about their performance under various perturbations. Figure 5 shows radar plots with mean forecasting accuracy and robustness metrics, including values under perturbation P2.\n1. Clear Domination Signals From Figure 5 and the detailed results from Section 5.2, we can draw the following conclusions: Se's Superiority in Forecasting Metrics: Sc consistently outperformed other models in terms of forecasting accuracy metrics, specifically SMAPE and MASE. This indicates that Se is highly effective in predicting stock prices with minimal error. General Superiority Over Bi-ased and Random Systems: All models perform better than the biased and random systems in forecasting metrics. This underscores the importance of using well-designed models over naive or biased approaches. Robustness in PIE % and APE Metrics: According to the average scores, the Sni system demonstrated superior robustness in PIE % and APE metrics. This suggests that Shi is more resilient to perturbations and confounding biases compared to other systems.\n2. Role of Modality The results from Figure 6 indicate that multimodal FMTS, Sni and Sni, generally perform better in terms of both robustness and accuracy, suggesting that incorporating multiple data modalities (e.g., numerical and image) can improve the system's ability to make accurate predictions and remain robust against various perturbations.\n3. Role of Confounders Our analysis (Figs. 4 and 5: left) shows that using industry as a confounder introduces more bias, with higher PIE% scores indicating significant industry-specific effects on the relationship between pertur-\nbations and residuals. Inter-industry comparisons also show more discrepancies, as evidenced by WRS scores. Conversely, the impact of company as a confounder varies. For instance, system St shows minimal effect from company-specific perturbations, suggesting it is well-tailored to company data, while other systems have higher APE scores, indicating a significant impact on residuals due to company-specific factors.\n4. Role of Architecture Our evaluation (Fig. 7, left) indicates that the Time Series architecture generally performs better across several metrics, such as achieving the best values in APE_C, PIE_C, SMAPE, MASE, and WRS_I, suggesting that the TS architecture may be more effective for these specific tasks compared to the general purpose architectures. Fig. 7 (right) shows that decoder-only architecture outperforms others in terms of both accuracy and robustness. Overall, our comparison highlights Se's forecasting accuracy, the robustness of multimodal systems against perturbations and confounding biases, and the superiority of well-designed models over naive approaches."}, {"title": "5.4 User Study", "content": "We conducted a user study to evaluate the ratings generated by our approach for comparing the behavior of various FMTS based on two key metrics: robustness and statistical fairness (defined as lack of statistical bias). To simplify the evaluation for participants, we converted the generated ratings into rankings (i.e., the system with the highest robustness ranking is the most robust system). The main objective of this study was to validate the following hypotheses:"}, {"title": "6 Discussion and Conclusion", "content": "Our paper aimed to measure the impact of perturbations and confounders on the outcome of FMTS using stock prices across leading companies and industries. We studied Industry and Company as confounders, motivated by the intuition that stakeholders rely on learning-based systems for stock purchase decisions and would be interested in knowing if model errors depend on stock price ranges. For example, does a model commit more errors predicting META's stock prices compared to MRK's? To minimize volatility effects, we performed both intra-industry and inter-industry analyses. In future, we plan to study confounders like seasonal trends and financial news. As demonstrated, we believe metrics should be selected based on the questions one wants to answer, rather than relying solely on statistical accuracy. The hypothesis testing approach from (Lakkaraju, Srivastava, and Valtorta 2024), adapted for our work, helped quantify biases and perturbation impacts on test systems. The perturbations used in our analysis have real-world impacts, applicable to both numeric and multi-modal data. While methods like differential evaluation can find the most impactful perturbation variations, we focused on assessing whether simple, subtle perturbations affect FMTS.\nConclusion We proposed a causally grounded empirical framework to study FMTS robustness against three input perturbations, evaluating seven state-of-the-art FMTS across six prominent stocks in three industries. Our framework's ratings accurately assessed FMTS robustness and provided actionable insights for model selection and deployment. Experiments showed multi-modal FMTS exhibited greater robustness, while uni-modal FMTS had higher"}, {"title": "A Details of Rating Algorithms", "content": "Apart from Algorithm 2, the rest were adapted from (Lakkaraju, Srivastava, and Valtorta 2024) to suit the FMTS forecasting setting. Here is how the rating method works.\n1. Algorithm 1 computes the weighted rejection score (WRS) which was defined in Appendix C in the main paper.\n2. Algorithm 2 computes the PIE % based on Propensity Score Matching (PSM) which was defined in Section 4.1 in the main paper.\n3. Algorithm 3 creates a partial order of systems within each perturbation based on the raw scores computed. It will arrange the systems in ascending order w.r.t the raw score. The final partial order (PO) will be a dictionary of dictionaries.\n4. Algorithm 4 computes the final ratings for systems within each perturbation based on the PO from previous algorithm. It splits the set of raw score values obtained within each perturbation into 'L' parts where 'L' is the rating level chosen by the user. Each of the systems is given a rating based on the compartment number in which its raw score belongs. The algorithm will return a dictionary with perturbations as keys and ratings provided to each system within the perturbation as the value."}, {"title": "B Additional Related Work", "content": "B.1 Robustness Testing of Foundation Models\n(Zhang and R\u00e9 2022) examined group distribution shifts and evaluated FMs on image classification tasks with spurious confounders. In our work, we assess the robustness of FMs within time series forecasting by measuring their performance in the presence of two confounders across various perturbation settings and test dataset distributions. (Zhang et al. 2023) used foundation models as a surrogate oracle to measure the robustness of image classification models. However, their test systems did not include any foundation models. (Xiao et al. 2024) introduces a framework, RITFIS, to assess the LLM-based software against natural language input. However, they did not consider any other modalities. (Schlarmann and Hein 2023) showed that imperceivable attacks on images to change the caption output of multi-modal FMs can lead to broadcasting of fake information to honest users. They only evaluate robustness of OpenFlamingo model under different attacks but does not compare its performance with any other FMs. None of these works assess the effectiveness and usability of their robustness testing methods. We address this gap through a user study."}, {"title": "C Evaluation Metrics", "content": "In this section, we define our evaluation metrics: forecasting accuracy and robustness.\nForecasting Accuracy Metrics We evaluate the systems' forecasting accuracy using established metrics commonly applied in time-series forecasting tasks (Makridakis, Spiliotis, and Assimakopoulos 2022).\nSymmetric mean absolute percentage error (SMAPE) is defined as,\n$$SMAPE = \\frac{1}{T} \\sum_{t=1}^T \\frac{|x_t - \\hat{x}_t|}{(x_t + \\hat{x}_t)/2}$$\nwhere T = 20 (i.e., the value of d) is the total number of observations in the predicted time series. SMAPE scores range from 0 to 2, with lower scores indicating more precise forecasts.\nMean absolute scaled error (MASE) measures the mean absolute error of forecasts relative to that of a naive one-step forecast on the training data.\n$$MASE = \\frac{\\frac{1}{T} \\sum_{t=1}^T |x_{t+1} - x_i|}{\\frac{1}{t} \\sum_{i=1}^t |X_i - X_{i-1}|}$$\nwhere in our case, t = 80, and T = 100. Lower MASE values indicate better forecasts.\nSign Accuracy quantifies the average classification accuracy across all test samples, where a higher accuracy indicates more precise predictions. This metric classifies based on how the predicted forecasts align with the most recent observed values in the input time series.\nRobustness Metrics We adapt WRS metric originally proposed in (Lakkaraju, Srivastava, and Valtorta 2024) to answer RQ1. Additionally, we introduce two new metrics: APE and PIE % (modified versions of ATE (Abdia et al. 2017) and DIE % (Lakkaraju, Srivastava, and Valtorta 2024)) tailored to answering RQ2 and RQ3."}, {"title": "H Additional Implementation Details", "content": "All Forecasting Model Training Systems (FMTS) were executed on Colab notebooks utilizing the L4 GPU available through Colab Pro, which offers 22.5 GB of GPU RAM. Additional details regarding the models such as the inference times and other architectural details can be found in Section 5.1.\nHyperparameters set\n\u2022 MOMENT: head_dropout: 0.1, weight_decay: 0, freeze_encoder: True, freeze_embedder: True, freeze_head: False\n\u2022 Phi-3: _attn_implementation='eager', max_new_tokens: 300, temperature: 0.0, do_sample: False\n\u2022 Gemini: Temperature: 0. Rest of the parameters were default."}, {"title": "I Reproducibility Checklist", "content": "This paper:\n1. Includes a conceptual outline and/or pseudocode description of AI methods introduced\nAnswer: Yes (Appendix A).\n2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results\nAnswer: Yes (Section 5.2)\n3. Provides well marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper\nAnswer: Yes (Sections 1, 2)\nDoes this paper make theoretical contributions? (yes/no)\nAnswer: No\n4. Does this paper rely on one or more datasets?\nAnswer: Yes (Description in Section 5.1\n5. A motivation is given for why the experiments are conducted on the selected datasets\nAnswer: Yes (Sections 1, 2)\n6. All novel datasets introduced in this paper are included in a data appendix.\nAnswer: NA (We used an existing dataset from Yahoo! Finance.)\n7. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes.\nAnswer: NA\n8. All datasets drawn from the existing literature (potentially including authors' own previously published work) are accompanied by appropriate citations.\nAnswer: NA\n9. All datasets drawn from the existing literature (potentially including authors' own previously published work) are publicly available.\nAnswer: Yes (Section 5.1)"}]}