{"title": "AN X-RAY IS WORTH 15 FEATURES: SPARSE AUTOENCODERS FOR INTERPRETABLE RADIOLOGY REPORT GENERATION", "authors": ["Ahmed Abdulaal", "Hugo Fry", "Jack Gao", "Stephanie Hyland", "Nina Monta\u00f1a-Brown", "Ayodeji Ijishakin", "Daniel C. Alexander", "Daniel C. Castro"], "abstract": "Radiological services are experiencing unprecedented demand, leading to increased interest in automating radiology report generation. Existing Vision-Language Models (VLMs) suffer from hallucinations, lack interpretability, and require expensive fine-tuning. We introduce SAE-Rad, which uses sparse autoencoders (SAEs) to decompose latent representations from a pre-trained vision transformer into human-interpretable features. Our hybrid architecture combines state-of-the-art SAE advancements, achieving accurate latent reconstructions while maintaining sparsity. Using an off-the-shelf language model, we distil ground-truth reports into radiological descriptions for each SAE feature, which we then compile into a full report for each image, eliminating the need for fine-tuning large models for this task. To the best of our knowledge, SAE-Rad represents the first instance of using mechanistic interpretability techniques explicitly for a downstream multi-modal reasoning task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific metrics compared to state-of-the-art models while using significantly fewer computational resources for training. Qualitative analysis reveals that SAE-Rad learns meaningful visual concepts and generates reports aligning closely with expert interpretations. Our results suggest that SAEs can enhance multimodal reasoning in healthcare, providing a more interpretable alternative to existing VLMs.", "sections": [{"title": "1 INTRODUCTION", "content": "Radiological services are essential to modern clinical practice, with demand rising rapidly. In the UK, the NHS performs over 43 million radiological procedures annually (Lewis et al., 2021), costing over \u00a32 billion, and demand for scans more than doubled between 2012 and 2019 (NHS England & NHS Improvement, 2019). A significant portion of these costs addresses rising demand through agency, bank, and overtime staff, but a national imaging strategy notes this funding is unsustainable (NHS England & NHS Improvement, 2019). Consequently, there's growing interest in (semi)-automating tasks like radiology report generation, augmentation, and summarization to assist clinicians (Zhu et al., 2024; Chen et al., 2024; P\u00e9rez-Garc\u00eda et al., 2024), spurred by advances in multimodal text-vision modelling techniques.\nRecent architectures that combine vision encoders with pretrained Large Language Models (LLMs) to create multimodal Vision-Language Models (VLMs) have shown impressive performance in visual and language tasks (Liu et al., 2024b; 2023a; Li et al., 2024; Lin et al., 2023; Liu et al., 2023b). VLMs have been applied to healthcare tasks, including radiology report generation (Hyland et al., 2023; Bannur et al., 2024; Chen et al., 2024; Stock et al., 2024; Yang et al., 2024), typically by mapping image representations into the LLM's token embedding space. The LLM is fine-tuned to respond to prompts like '<image tokens> Produce the findings section of a radiology report for this'"}, {"title": "2 RELATED WORK", "content": "Multimodal reasoning Multimodal reasoning methods like ScienceQA (Lu et al., 2022) introduced multimodal chain-of-thought (CoT) by zero-shot prompting models to generate rationales and answers simultaneously. Multimodal-CoT (MM-CoT) (Zhang et al., 2023) extended this with a two-stage framework that separates rationale generation and answer inference using two models of the same architecture. Duty-Distinct CoT (DDCoT) (Zheng et al., 2023) further factorizes rationales by decomposing the initial question into sub-questions answered by a vision-language model (VLM). Other divide-and-conquer approaches decompose questions into sub-questions but often require training task-specific visual question generation (VQG) models and additional scoring models (Selvaraju et al., 2020; Uehara et al., 2022; Wang et al., 2022). IdealGPT (You et al., 2023) iteratively decomposes queries and uses a VLM to answer sub-questions, repeating the process if confidence is low. Unlike these methods that rely on decomposing questions or generating rationales through additional models, our approach directly extracts and interprets features from pre-trained image encoders. This enables faithful and transparent multimodal reasoning without the need for extensive fine-tuning or supplementary VQG models.\nRadiological VLMS A number of works have finetuned or otherwise trained specialised foundation models for radiological applications including Med-flamingo (Moor et al., 2023), Med-PaLM M (Tu et al., 2024), LLava-Med (Li et al., 2024), Med-Gemini (Yang et al., 2024), Rad-DINO (P\u00e9rez-Garc\u00eda et al., 2024), MAIRA-1 (Hyland et al., 2023), R2gengpt (Wang et al., 2023), and Radiology-GPT (Liu et al., 2023c). With regards to radiology report generation, several works have focused on producing both the 'findings' and 'impression' sections of the reports (Chen et al., 2020; Jin et al., 2024; Yan et al., 2023), whilst others have focussed on the 'impression' section (Bannur et al., 2023), or, most commonly, the 'findings' section (Tu et al., 2024; Miura et al., 2020; Delbrouck et al., 2022; Tanida et al., 2023; Nicolson et al., 2023). As noted by others (Hyland et al., 2023; Yu et al., 2023; Jeong et al., 2024), studies examining all three settings found that the choice of section(s) to report significantly affects the performance metrics, making comparison between results difficult. For this reason we focus on the most common setting of producing the 'findings' section of a radiology report.\nMechanistic interpretability Bricken et al. (2023) demonstrated that SAEs could recover monosemantic features by training on the residual stream of small transformers. The gated SAE was a Pareto improvement over the baseline SAE in terms of sparsity as measured by the LO and the loss recovered (Rajamanoharan et al., 2024). There was concern that SAEs would not scale to frontier transformers until recent work by Templeton et al. (2024) which trained SAEs on Claude 3 Sonnet and discovered a large number of monosemantic features. Contemporaneously, SAEs were trained on the class tokens of a CLIP vision transformer (Fry, 2024), InceptionV1 (Gorton, 2024), the conditioning embeddings of diffusion models (Daujotas, 2024), and the vision transformer of a pathology foundation model (Le et al., 2024). However, in all cases the discovered language/visual features were not used to perform a downstream multimodal task."}, {"title": "3 BACKGROUND", "content": "In this section we give a brief overview of mechanistic interpretability, SAEs, and gated SAEs, before introducing our SAE-Rad framework."}, {"title": "3.1 MECHANISTIC INTERPRETABILITY AND SPARSE AUTOENCODERS (SAES)", "content": "Mechanistic interpretability Mechanistic interpretability research aims to identify, understand, and verify the algorithms that an ML model implements by reverse engineering a model's computations into human-interpretable components (Olah et al., 2020; Rajamanoharan et al., 2024). Classical approaches attempted to achieve this by analysing the firing patterns of individual neurons, which were interpreted as possible 'concept representations'. However, this was broadly ineffective as neurons can be polysemantic, meaning that a single neuron may fire on many unrelated concepts (Rajamanoharan et al., 2024; Bolukbasi et al., 2021; Elhage et al., 2022a). Polysemantic neurons are believed to arise during training due to the composition of both the linear representation and superposition hypotheses."}, {"title": "Linear representation and superposition hypotheses", "content": "Motivated by a number of findings which suggest that concept representations are linear (Gurnee et al., 2023; Olah et al., 2020; Park et al., 2023), the linear representation hypothesis states that neural networks represent concepts (sometimes interchangeably referred to as 'features') as directions in activation space (Nanda et al., 2024). This hypothesis can be thought of as being composed of two properties: 1) Linearity: That is, features are represented as directions; and 2) Decomposability: We can understand neural network outputs as a composition of multiple independently understandable features (Elhage et al., 2022b). The superposition hypothesis states that for an intermediate representation of dimension n, neural networks will encode M \u226b n concepts as linear directions (Rajamanoharan et al., 2024; Elhage et al., 2022b). These directions form an overcomplete basis of the activation space and must therefore necessarily overlap with each other. However, a single input will only activate a sparse subset of these concepts, leading to minimal interference between the (non-orthogonal) concept directions (Gurnee et al., 2023; Rajamanoharan et al., 2024). The superposition hypothesis can be thought of as a form of learned neural network compression and is closely related to compressed sensing. Recent work (Bricken et al., 2023) has proposed using SAEs to take features out of superposition and learn monosemantic interpretable representations."}, {"title": "Sparse autoencoders (SAEs)", "content": "SAEs attempt to 'undo' superposition by learning the sparse over-complete basis (Mallat & Zhang, 1993; Rajamanoharan et al., 2024) (or dictionary) of the activation space induced by superposition. SAEs attempt to learn both the concept directions and a sparse vector of coefficients for the inputs, that reflect how much each concept is activated for each input (Cunningham et al., 2023; Bricken et al., 2023). To align our nomenclature with the recent literature, we will henceforth refer to such sparse vectors of coefficients as 'feature activations'.\nWe begin by defining the 'baseline SAE' described by Bricken et al. (2023). Let n be the dimension of the input and output (typically the input is the residual stream of a transformer, and the output is its reconstruction (Elhage et al., 2021)), and m be the SAE hidden layer dimension. Let s be the size of the dataset. Then given encoder weights and biases $W_{enc} \\in \\mathbb{R}^{m \\times n}, b_{enc} \\in \\mathbb{R}^{m}$, and decoder weights and biases $W_{dec} \\in \\mathbb{R}^{n \\times m}, b_{dec} \\in \\mathbb{R}^{n}$, the encoding and decoding operations for a dataset $X \\in \\mathbb{R}^{s,n}$ are\n$h(x) := ReLU(W_{enc}(x \u2013 b_{dec}) + b_{enc})$\n$\\hat{x}(h(x)) := W_{dec}h(x) + b_{dec}$.\nThe loss function is then\n$L(x) := \\frac{1}{s} \\sum_{X \\in X}[||x - \\hat{x}(h(x))||_{2}^{2} + \\lambda ||h(x)||_{1}]$,\nwhere $\\lambda$ is an L1 sparsity coefficient. The first term is a reconstruction error measured by the squared distance between the input and its reconstruction, and the second is an L1 regularization loss to induce sparsity. It should be noted that in this regime it is possible to reduce the second term in Eq. (3) by simply decreasing the norm of the encoder weights $W_{enc}$. It is possible to retain both reconstruction quality and sparsity by a corresponding increase of norm of the decoder weights (Bricken et al., 2023; Rajamanoharan et al., 2024). This effect is not desired due to both overflow (decoder norm) and underflow (encoder norm) errors. Additionally, this can cause unstable training when using adaptive optimization algorithms such as Adam (Kingma, 2014). To resolve this, the column-wise norm of the decoder $W_{dec}$ can be constrained during training (Bricken et al., 2023)."}, {"title": "3.2 GATED SAES", "content": "As can be seen in Eq. (3), SAEs jointly optimize two opposing objectives: 1) Reconstruction fidelity and 2) L1 regularization as a proxy for sparsity (as measured by L0). This means the SAE is free to trade-off some reconstruction fidelity in order to perform better on the sparsity penalty. One consequence of this is shrinkage (Wright & Sharkey, 2024). That is, for a fixed decoder, the sparsity penalty pushes the feature activations h(x) towards zero whilst the squared distance loss encourages h(x) to be large enough in order to produce high quality reconstructions. Thus, the standard SAE will systematically underestimate the optimal magnitude of feature activations (and simply rescaling these does not necessarily overcome this bias) (Rajamanoharan et al., 2024; Wright & Sharkey, 2024).\nRajamanoharan et al. (2024) propose a gated SAE which separates the encoding procedure into two tasks: 1) Detecting which features should activate for a given input (this requires an L1 penalty if the features are to be sparse); and 2) Estimating the magnitude of the feature activations (this does not require an L1 loss; indeed, including this loss here introduces a shrinkage bias). The architecture of the gated encoder is\nh(x) := $[[W_{gate}(x - b_{dec}) + b_{gate} > 0] \\odot ReLU(W_{mag}(x \u2013 b_{dec}) + b_{mag}), (4) $ \nwhere $[[ > 0]$ is an element-wise Heaviside step function and $\\odot$ is element-wise multiplication. The $h_{gate}$ sub-function learns which features should activate for a given input and $h_{mag}$ estimates the magnitude of activations for these features. Here, $gate$ is referred to as the $h_{gate}$ sub-function's 'pre-activations'. To minimize the number of additional parameters required, $W_{mag}$ shares the same feature directions as $W_{gate}$, and is defined as $W_{mag} := exp(r_{mag}) \\odot W_{gate}$, where $r_{mag} \\in \\mathbb{R}^{m}$ is a vector-valued scaling parameter. Letting RA(\u00b7) := ReLU($\\pi_{gate}(\u00b7)$) denote the rectified pre-activations of the gating sub-function, the loss function is defined as\n$L(x) := ||x \u2212 \\hat{x}(h(x))||_{2}^{2} + \\lambda||RA(x)||_{1}+ ||x \u2212 \\hat{x}_{frozen}(RA(x))||_{2}^{2},$\nwhere $\\hat{x}_{frozen}$ is a fixed copy of the decoder so that gradients from the auxiliary loss $L_{aux}$ do not back-propagate to the decoder weights or bias terms. The auxiliary term $L_{aux}$ ensures that $h_{gate}$ correctly identifies features necessary for reconstruction, as its (positive) pre-activations must be able to reproduce the input. The sparsity term $L_{sparsity}$ applies an L1 penalty to the rectified pre-activations (and thus sparsity is only imposed on the gating sub-function), and the reconstruction term serves the same function as in Eq. (3)."}, {"title": "4 SAE-RAD", "content": "In this section we introduce SAE-Rad. First, we describe the autoencoder architecture, which is based on the gated SAE described in Section 3.2. Then, we describe our end-to-end radiology report generation pipeline."}, {"title": "4.1 SAE ARCHITECTURE", "content": "It was recently demonstrated that a lower overall SAE loss is achievable without constraining the L2 norm of the decoder weights, or centering the input based on the decoder bias (Conerly et al., 2024). Concretely, they use the same decoder as in Eq. (2) and define the encoder as\nh(x) := ReLU(W_{enc}x + b_{enc}).\nNote that the input x is no longer centered by subtracting the decoder bias $b_{dec}$ as in Eq. (1). The sparsity penalty in the loss also now includes the L2 norm of the columns of the decoder $W_{dec}$:\n$L(x) := ||x - \\hat{x}(h(x))|| + \\lambda \\sum_{i}h(x) \\cdot || \\frac{W_{dec_{i}}}{||W_{dec_{i}}||}||^{2}$.\nThe feature activation for a feature i is then $h(x) \\cdot ||\\frac{W_{dec_{i}}}{||W_{dec_{i}}||}||^{2}$. The concept directions' are the unit-normalized decoder vectors $\\frac{W_{dec_{i}}}{||W_{dec_{i}}||}$.\nThe SAE-Rad sparse autoencoder is a hybrid architecture which combines a gated encoder layer with unconstrained decoder norms. Its encoder is defined as\nh(x) := $[[W_{gate}x + b_{gate} > 0] ReLU(W_{mag}x + b_{mag}),$"}, {"title": "4.2 SAE-RAD - AUTOMATED RADIOLOGY REPORTING PIPELINE", "content": "In this section we describe our pipeline to automate the task of radiology report generation. Concretely, a radiographic image x is passed through a pre-trained and frozen vision encoder $f_{img}(): x \u2192 z$ to produce an image latent z. We leverage the hybrid SAE architecture described in Section 4.1 to learn feature directions $\\frac{W_{dec}}{||W_{dec}||}$ and their associated activations $h(z) \\cdot ||\\frac{W_{dec}}{||W_{dec}||}||^{2}$ from the latents.\nIn order to generate a text-based report, we produced plain-English descriptions of the learnt sparse dictionary (i.e., a description of what each feature direction represents). To do this we performed automated interpretability (Bricken et al., 2023) by using a pre-trained and frozen LLM to analyse the ground-truth radiology reports of the highest activating images for each feature. Let $X^{(i)}_{highest}$ represent the set of images with the largest feature activations for feature i. For each image $x \\in X^{(i)}_{highest}$ there is an associated ground-truth radiology report r(x). We collect these reports into a set $R^{(i)} := {r(x)|x \\in X^{(i)}_{highest}}$. We then utilized a pre-trained and frozen language model $f_{descriptor}: R \u2192 d$ to generate a description $d^{(i)}$ for a feature i by analyzing the set $R^{(i)}$ as $d^{(i)}=f_{descriptor}(R^{(i)})$. This process yielded a set of feature descriptions ${d^{(i)}}_{i=1}^{M}$, where M is the total number of features learned by the SAE.\nFor a new scan x we identified the set of active features I(x) based on a threshold $\\tau$ as\n$I(x) := {i|h_{i}(f_{img}(x)) \\cdot ||\\frac{W_{dec}}{||W_{dec}||}||^{2} > \\tau}.$\nThe automated radiology report R(x) is then generated by a pre-trained and frozen LLM from the descriptions of the active features\nR(x) = $f_{generator}({d^{(i)}|i \\in I(x)}).$"}, {"title": "5 EXPERIMENTS", "content": "Our overarching hypotheses are that: 1) SAE features capture meaningful visual concepts even in homogeneous datasets (such as is the case for chest radiographs); 2) The visual concepts captured by an SAE can be appropriately described by a pre-trained LLM by use of automated interpretability techniques with paired text data; 3) Natural language descriptions of visual features in the latent space of an SAE can be composed into high-quality radiology reports without explicit use (or training/finetuning) of a VLM for multimodal reasoning. We assessed these hypotheses with our automated radiology reporting experiment (Section 5.1). We then conducted a number of ablation studies to investigate the effects of model size, different sparsity constraints, and the inclusion of auxiliary information (Section 5.2). Next, we performed a case-study for counterfactual imagine generation to evaluate the learned SAE features (Section 5.3). Finally, we conducted a reader study with a specialist radiologist to assess the quality of our generated reports (Section 5.4)."}, {"title": "5.1 AUTOMATED RADIOLOGY REPORTING", "content": "Dataset description We trained and evaluated all models on the MIMIC-CXR dataset (Johnson et al., 2019), a public dataset of 227,835 radiographic studies for a total of 377,110 chest radiographs and associated written text reports. We linked all images to their DICOM metadata files to retrieve scan orientations. We only considered images for which metadata files exist, and retained only antero-posterior(AP)/postero-anterior(PA) scans for training (these are \u2018head-on' scans, as opposed to lateral ones). These are the default views for the generation of diagnostic reports given the increased clarity and ability to visualize the relevant anatomy comprehensively (Hyland et al., 2023). We then extracted the 'findings' section from each text report. The 'findings' are a natural language description of all relevant negative and positive features for a given radiograph. Occasionally, the 'findings' section is placed into another section with the sub-heading of 'impression'. Datapoints without either a findings or impression section were discarded. We used the recommended train/test split for MIMIC-CXR, resulting in a total of 239,931 training and 3,403 test images.\nEvaluation metrics We evaluated generated radiology reports using both general Natural Language Generation (NLG) metrics (BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), METEOR (Banerjee & Lavie, 2005)) and radiology-specific metrics (RGER score (Delbrouck et al., 2022), CheXpert F1 score (Irvin et al., 2019)). While lexical metrics assess n-gram overlap and word order, clinical metrics like RGER and CheXpert F1 attempt to evaluate factual completeness and consistency by analyzing entity-relationship graphs and predicting common chest X-ray pathologies, respectively. NLG metrics can be inadequate for assessing radiology reports as they don't account for clinical significance (Bannur et al., 2024), while radiology-specific metrics often rely on specialized models (Yu et al., 2023) or pre-specified findings classes (Smit et al., 2020; Bannur et al., 2024). To address these limitations, the RadFact framework Bannur et al. (2024) uses LLMs to assess sentence-level factuality through bi-directional entailment verification with reference reports- offering a robust evaluation method without relying on pre-specified error types or specialized models.\nExperimental setup The SAE-Rad framework was trained on class tokens produced by the Rad-DINO vision transformer, using an expansion factor of 64 resulting in a latent dimension of 49,152. The model was optimized using Adam with a learning rate peaking at 5e-5 and a sparsity penalty of 8e-3, trained for 200,000 steps with a batch size of 2048. Claude 3.5 Sonnet was used for automated feature interpretation and report generation, while RadFact evaluation employed Llama3-70B-Instruct."}, {"title": "Quantitative evaluation", "content": "We compared SAE-Rad to the current state-of-the-art radiology reporting systems. CheXagent (Chen et al., 2024) is an instruction-tuned foundation model for CXRs trained on 1.1M scans for question-answering and text-generation tasks. MAIRA-1 &-2 (Hyland et al., 2023; Bannur et al., 2024) are VLMS based on the LLaVA 1.5 architecture (Liu et al., 2024b;a). MAIRA-2 is trained on 510,848 CXRs from four datasets and sets the current state-of-the-art for report generation. The MAIRA systems are not publicly available for result replication, and thus we quote their evaluation values directly as our upper-bound. CheXagent is publicly available, and we therefore performed independent replications for this model for a direct comparison. The 'baseline' approach is a na\u00efve method of report generation that uses the report of the closest image in the MIMIC train split.\nAs Table 1 demonstrates, SAE-Rad underperforms on generic NLG metrics such as BLEU-4. This is expected as we do not try to optimize for any specific 'writing style' by fine-tuning an LLM on the reference reports from MIMIC-CXR. Conversely, SAE-Rad demonstrates strong performance on radiology-specific metrics which are clinically relevant, outperforming CheXagent by up to 52% in the CheXpert F1 score (macro-averaged F1-14), and achieving 92.1% and 89.9% of the performance of MAIRA-1 and MAIRA-2 on these scores, respectively."}, {"title": "Qualitative investigation", "content": "Figure 2 illustrates randomly selected monosemantic visual features from SAE-Rad. As can be seen, the SAE learns human-interpretable visual concepts despite the homogeneity and relatively small size of the dataset. These include dextroscoliosis of the spine (Fig. 2; feature 1), bilateral opacifications (Fig. 2; feature 2), unilateral pleural effusions (Fig. 2; feature 3), and the presence of instrumentation \u2013 in this case a pacemaker (Fig. 2; feature 4). In Fig. 3, we illustrate an example 'findings' section for a CXR with a number of pathological findings; SAE-Rad is capable of detecting multiple relevant pathologies for a given image. Like other radiology report generation systems, SAE-Rad can miss findings. However, it can also occasionally describe a relevant finding which is otherwise missing from the reference report \u2013 an example relating to the presence of a dialysis catheter is shown in Fig. 3."}, {"title": "5.2 ABLATION STUDIES", "content": "We conducted a set of additional experiments which characterize the effects of: 1) Vary-ing the SAE expansion factor; 2) Investigating less sparse ('dense') SAEs by reducing the L1 penalty coefficient; and 3) Evaluating the relative benefits of including auxiliary information including the 'indication' section of a report as well as previous reports, where available.\nAs can be seen in Table 2, an expansion factor of x64 produced a higher RadFact F1 score compared with both smaller (x32) and larger (\u00d7128) expansion factors. In addition, denser SAEs with a larger LO norm underperformed sparser models. This suggests that concepts useful for radiology report generation are likely to exist in balance between being too coarse (which may cause \u2018feature absorption' - an asymmetric form of feature splitting that can negatively impact the interpretability of an SAE feature (TomasD et al., 2024)), or too fine-grained, as these features may be more difficult to accurately describe given insufficient amount of descriptive detail in the ground-truth reference reports. Table 2 demonstrates that the addition of auxiliary information such as the indication, which describes why the patient required the scan in the first instance, can boost the RadFact F1 score, with a large boost to recall. However, in our experiments this caused a small degradation to the precision sub-metric. We find that adding both previous indications and prior studies has a net positive effect on the quality of generated reports."}, {"title": "5.3 COUNTERFACTUAL IMAGE GENERATION FOR EVALUATING FEATURES", "content": "We evaluated the interpretability and validity of our SAE features using an experiment based on Monteiro et al. (2023)'s assessment of counterfactual imaging models-focusing on effectiveness, composability, and reversibility. SAE features are interpretable if they correspond to distinct concepts that respond predictably to activation space interventions. We trained a diffusion model conditioned on Rad-DINO class tokens (P\u00e9rez-Garc\u00eda et al., 2024) to reconstruct MIMIC-CXR radiographs. During inference, we passed a class token through the SAE, intervened on encoder activations, and reconstructed a \"counterfactual\" token via the decoder, which conditioned the diffusion model to project interventions into imaging space. We tested whether: 1) interventions alter the reconstructed class token accordingly, 2) changes affect only the targeted feature, and 3) features can be \"added\" or \"removed\" by manipulating the same activation."}, {"title": "5.4 READER STUDY", "content": "In a reader study with a specialist radiologist evaluating the quality of the automated radiology reports, 165 sentences from 30 reports (SAE-Rad, CheXagent, and a baseline) were analyzed. SAE-Rad had 7% fewer edits than other models and demonstrated significantly fewer errors with clinical impact, particularly in the \u201csignificant\" category, where SAE-Rad had almost half the rate compared to others. This highlights SAE-Rad's potential for radiology report generation in a real clinical scenario."}, {"title": "6 DISCUSSION, LIMITATIONS, AND CONCLUSION", "content": "In this work, we introduced SAE-Rad, a novel framework that leverages sparse autoencoders to automate radiology report generation. Our approach directly decomposes image class tokens from a pre-trained radiology image encoder into human-interpretable features, which are then compiled into comprehensive radiology reports. The experimental results demonstrate that SAE-Rad achieves competitive performance on radiology-specific metrics, outperforming existing models like CheX-agent (Chen et al., 2024) and approaching the performance of state-of-the-art systems such as MAIRA-2 whilst being trained on a significantly smaller dataset and with a much lower training compute budget; By reverse-engineering the computations of the image encoder, SAE-Rad provides a framework that is verifiably faithful to the underlying model, enhancing transparency and trustworthiness, which are critical considerations in the healthcare setting.\nQualitative analyses confirm that SAE-Rad successfully captures meaningful visual concepts including the presence or absence of pathological features. These interpretable features contribute to generating detailed and accurate radiology reports, as evidenced by strong performance in the clinical evaluation metrics. Our ablation studies indicate that the choice of expansion factor and inclusion of auxiliary information, such as previous reports and indications, can significantly impact the quality of the generated reports, which is broadly in line with the previous literature.\nOur approach has limitations. First, SAE-Rad relies on pre-trained (frozen) models for both the image encoder and the LLM in the interpretability pipeline, potentially introducing inherent biases. However, due to the pipeline's modular nature, these biases can be mitigated by replacing either model without retraining the SAE if the LLM is swapped. Additionally, SAE-Rad underperforms on general language metrics like BLEU-4, suggesting that while the generated reports are clinically accurate, they may lack the fluency and stylistic nuances of human-generated reports, particularly those in the MIMIC-CXR dataset. Improving these metrics through style-aware radiology report generation (Yan et al., 2023) is a natural avenue for future work. Overall, SAE-Rad presents a novel and effective approach to radiology report generation by leveraging mechanistic interpretabil-"}, {"title": "3.1 SAE ARCHITECTURE COMPARISON", "content": "In this section we compare the performance of our novel SAE architecture with the architecture proposed by Conerly et al. (2024). The following hyperparameters were used to train both SAEs:\n\u2022 Expansion factor of \u00d764.\n\u2022 Batch size of 2048.\n\u2022 Learning rate of 5 \u00d7 10~5.\n\u2022 Linear warm-up of learning rate for the first 1% of training.\n\u2022 Linear warm-down of learning rate for the last 20% of training.\n\u2022 L1 coefficient warmup for the first 5% of training.\n\u2022 Adam optimizer with no weight decay.\n\u2022 Trained for 200,000 optimization steps.\nThe L1 coefficient was increased for the SAE-Rad architecture in comparison to the Conerly et al. (2024) SAE in order to compensate for the additional auxiliary loss term used to train the SAE-Rad architecture. displays the resulting metrics comparing the two SAEs our proposed SAE attains both a lower LO and a higher reconstruction accuracy."}, {"title": "B.2 EVALUATION METRICS", "content": "NLG and classical radiology-specific metrics We evaluated generated radiology reports using both general NLG metrics and radiology-specific metrics. For lexical evaluation, we report BLEU-4 (Papineni et al., 2002) for 4-gram overlap based on n-gram precision, ROUGE-L (Lin, 2004) for longest common subsequence matching, and METEOR (Banerjee & Lavie, 2005), which performs unigram matching using surface forms, stems, and meanings, computing scores based on precision, recall, and fragmentation assessing word order. Whilst widely reported, lexical metrics do not capture factual completeness or consistency (Miura et al., 2020; Bannur et al., 2024) and we therefore also include classical clinical metrics. The RGER score (Delbrouck et al., 2022), based on the RadGraph model (Jain et al., 2021), evaluates entity-relationship graphs extracted from reports by matching entities and verifying relationships. Additionally, we report the CheXpert F1 score (Irvin et al., 2019), utilizing the CheXbert model (Smit et al., 2020) to predict 14 common pathologies in chest X-rays and calculating the harmonic mean of precision and recall between generated and reference texts."}, {"title": "B.3 SAE-RAD ADDITIONAL EXPERIMENTAL SETUP DETAILS", "content": "In our instantiation of the SAE-Rad framework (described in Sections 4.1 and 4.2)", "https": "huggingface.co/microsoft/rad-dino. The dataset was shuffled and scaled by a constant such that $E_{z \\in Z}[||z||^{2}"}]}