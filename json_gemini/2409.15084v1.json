{"title": "Depression Diagnosis Dialogue Simulation: Self-improving Psychiatrist with Tertiary Memory", "authors": ["Kunyao Lan", "Bingrui Jin", "Zichen Zhu", "Siyuan Chen", "Shu Zhang", "Kenny Q. Zhu", "Mengyue Wu"], "abstract": "Mental health issues, particularly depressive disorders, present significant challenges in contemporary society, necessitating the development of effective automated diagnostic methods. This paper introduces the Agent Mental Clinic (AMC), a self-improving conversational agent system designed to enhance depression diagnosis through simulated dialogues between patient and psychiatrist agents. To enhance the dialogue quality and diagnosis accuracy, we design a psychiatrist agent consisting of a tertiary memory structure, a dialogue control and reflect plugin that acts as \"supervisor\" and a memory sampling module, fully leveraging the skills reflected by the psychiatrist agent, achieving great accuracy on depression risk and suicide risk diagnosis via conversation. Experiment results on datasets collected in real-life scenarios demonstrate that the system, simulating the procedure of training psychiatrists, can be a promising optimization method for aligning LLMs with real-life distribution in specific domains without modifying the weights of LLMs, even when only a few representative labeled cases are available.", "sections": [{"title": "Introduction", "content": "Mental health issues represent a significant challenge in con-temporary society. According to the World Health Organization (WHO) (Freeman 2022), approximately 298 million individuals are affected by depressive disorders, which are the most prevalent issue of mental disorders among adults. Concurrently, many mental health systems are characterized by substantial deficiencies and imbalances in resources and services (Freeman 2022). This situation highlights the urgent need for more effective automated methods for the detection and diagnosis of depression, which would lead to better allocation of medical resources.\nAmong various automated diagnostic approaches, conversational agents (CAs) have emerged as particularly effective due to their cost efficiency, time-saving capabilities, and ability to maintain user anonymity (Ma, Mei, and Su 2023). CAs are especially valuable in the diagnosis of mental illnesses, where the process is more complex than diagnosing physical conditions, which can often be confirmed through test reports. Mental health diagnoses rely heavily on qualitative information obtained through patient interactions, which presents unique challenges compared to the more straightforward use of structured electronic medical records (EMRs) (Li et al. 2024a). Additionally, in contrast to self-assessment chatbots (Jaiswal et al. 2019), conversational agents powered by large language models (LLMs) (Chen et al. 2023) demonstrate a greater propensity to offer emotional support and apply advanced professional skills to enhance the accuracy and effectiveness of diagnostic tasks.\nDespite the strong performance of LLM-empowered conversational agents (CAs) in various conversation-based tasks (Wu et al. 2023; Shi et al. 2023; Chen et al. 2023), the inherent biases within LLMs (Yeh et al. 2023; Xu et al. 2024) remain challenging, even with the use of advanced prompting techniques. To be specific, in the mental health domain, previous research has revealed that 1) LLMs exhibit a preference for certain strategies (Kang et al. 2024; Chen et al. 2023), such as emotion and sleep-related symptoms. 2) In diagnosis sessions, LLMs prefer to ask all the questions in one turn rather than step by step, and come to a diagnosis conclusion in several turns. (Chen et al. 2023) 3) LLMs might overlook the potential mental states of patients (Jin et al. 2024). The bias of LLMs harms their effectiveness in tasks like providing emotional support and conducting diagnosis conversations.\nThese limitation of LLMs emphasizes the necessity of dynamic prompt modification and the development of novel generative agents (Park et al. 2023). These agents can maintain long-term coherence in simulating human behavior by managing expansive memories. The design of memory structures (Zhang et al. 2024c) allows these agents to sustain coherence and stimulate their self-evolution (Zhang et al. 2024a,c), demonstrating effectiveness in reducing LLM biases and enhancing performance across various complex conversational tasks (Qian et al. 2024; Wu et al. 2023; Li et al. 2024a)."}, {"title": "Mental Clinic Simulation", "content": "The basic architecture of AMC follows the universal LLM-based conversational agent (Park et al. 2023) setting, as Figure 2 illustrated. The main characters defined in the AMC can be divided into three parts:\n\u2022 A variety of patients, generated based on existing patient portraits in D4;\n\u2022 Psychiatrist, the agent that reflecting skills based on the diagnosis conversation;\n\u2022 Supervisor, an incomplete agent without long-term memory, acts as an optional plugin of the psychiatrist, initialized at the beginning of each session, mainly controlling the dialogue process and stimulating the psychiatrist agents to reflect on the diagnosis results.\nPatient Agents Patient agents refer to agents playing individuals suffering from depressive moods and might be diagnosed with depressive disorder. They come to the clinic, seeking help from the psychiatrist agent to determine whether further intervention is necessary. To simulate depressive patients in a real-life scenario, we utilize a public quasi-clinical-standard depression diagnosis dialogue dataset D\u2074 (Yao et al. 2022) to initialize patient agents, as step (1) in Figure 2 represents. D\u2074 is a Chinese depression diagnosis dialogue dataset containing 1339 dialogues conducted by well-trained patients and psychiatrists. The data characteristics of the dataset are attached in the appendix. All the dialogues are collected based on portraits of real potential patients. We select 100 representative cases from the train set of D\u2074, two examples are shown in Figure 3. In the meantime, we also include the original 132 test cases in D4 to conduct further experiments. Each selected case includes the patient profile, the dialogue history between the doctor and the patient, and the diagnosis result of the patient.\nTo better simulate the symptom status of the patients, we adopt detailed symptom ontology defined in the Diagnostic and Statistical Manual of Mental Disorders (DSM-5-TR) (Arbanas 2015) and prior work (Lan et al. 2024) to track the status of patients. However, patient agents are only provided with symptom lists, without the chief complaint and information about life events, resulting in poor role-play quality during the conversation, only replying whether they had the corresponding symptoms. Therefore, we use GPT-4 to generate life event memory based on the dialogue history.\nPsychiatrist Agent The psychiatrist agent is initialized with diagnosis skills from ICD-11*. It leads a conversation with patient agents, collecting information to be noted in the symptom list, as step (2) in Figure 2 illustrates. At the end of the session, the psychiatrist agent summarizes the electronic medical record (EMR) and diagnoses the depression risk and suicide risk of the patient agent. After receiving the reflected skills provided by the supervisor plugin and storing them in the memory, the psychiatrist agent calls for the next patient and repeats the diagnosis session, as step (5) and step (6) in Figure 2 represent.\nSupervisor Plugin The supervisor plugin acts as the coach or assistant of the psychiatrist agent, tracking the patient status and providing dialogue-controlling instructions during the diagnosis conversation, as step (3) in Figure 2 shows. At the end of each session, the supervisor plugin compares the generated diagnosis results"}, {"title": "Method", "content": "The complete diagnosis and reflection session can be presented in 6 stages as shown: (1) Initialize the patient agent with a profile generated through the D\u00b9 dataset, and embed the ground truth diagnosis result in the patient agent. The diagnosis results of the patient agent remain unseen to the patient agent and psychiatrist agent until the predicted diagnosis result is generated. (2) The psychiatrist and the patient agent begin the conversation. (3) The supervisor plugin updates the diagnosis tracking list, provides next-turn instruction based on the dialogue history and diagnosis tracking list, and stimulates conversation of the next round. The step (2) and (3) repeat until the conversation finish. (4) After the conversation, the psychiatrist agent generates the electronic medical records and the diagnosis result. The supervisor plugin compares the ground truth diagnosis result and the diagnosis result generated by the psychiatrist agent, and reflects the skills if the generated results are not correct. (5) The supervisor plugin updates the summarized skills in the memory module of the psychiatrist agent, and (6) The psychiatrist agent calls the next patient and repeats the whole procedure. To improve dialogue quality and diagnosis accuracy, each agent contains a chat module, a memory module, and a retrieval module. Psychiatrist's memory is a newly proposed tertiary memory framework. Each module will be introduced in detail.\nChat Module\nThe chat module takes the profile of the agent, relevant memory retrieved, and dialogue history as input, and generates the utterance that the agent going to say. Since the psychiatrist always plays a proactive role during the diagnosis conversation, we take advantage of the supervisor plugin of the psychiatrist agent to control the dialogue process. The dialogue generation function used in step (2) of Figure 2 can be presented in Equation 1. The PPTd, PRF indicates the constant prompt and personal profile utilized for dialogue generation, respectively, the mem suggests the retrieved memory, ins means the instruction generated by supervisor plugin, only provided if the profile is the psychiatrist agent, and the utt:i-1 is the dialogue history until now.\nutt\u2081 = LLM(PPTd, PRF, mem, [ins], utt:i-1) \n\nThe diagnosis function is attached in the chat module. The diagnosis function takes the whole dialogue history and relevant memory retrieved as input, and generates the detailed diagnosed symptoms and the predicted depression risk and suicide risk. The diagnosis function can be presented in Equation 2. The PPTdiag indicates the constant prompt utilized for diagnosis result generation. Since the response from LLM is always unstable, it results in bias during the experiments. Therefore, we apply a voting diagnosis method. To be specific, we let the LLM generate k samples of diagnosis response (k = 5 in our experiments). After that, we map the depression risk and suicide risk to integer: control(0), mild(1), moderate(2), and severe(3) as shown in Equation 3. We get the most voted risk level in most cases. If tie, we choose the rounded average of all votes as the final decision.\ndiag = diag; = LLM(PPTdiag, PRF, mem, utt.)\n\n\nAfter the conversation, the chat module summarizes the dialogue history as EMR, including all the symptoms and their severity mentioned in the dialogue. The function is presented in Equation 4, the PPTemr represents the constant prompt utilized for concluding EMR.\nEMR = LLM(PPTemr, utt.) \nTertiary Memory Mechanism\nMemory module is provided as an external storage space to overcome the limitation of input length of LLMs (Zhang et al. 2024c). The previous memory structure (Park et al. 2023) regards memory nodes as equal nodes, which is not appropriate in more complex scenarios. Although some works have proposed multi-layer memory structures (Qian et al. 2024; Li et al. 2023b; Sumers et al. 2023; Maharana et al. 2024), they are still underexplored, as they are based merely on the long-short memory concept without a specific division of roles across different memory layers. To fully simulate the process of psychiatrist training in a real-life scenario, we design a three-level hierarchical memory structure consisting of Conversation Records, Electronic Medical Records, and Diagnostic Skills, which mimics the exact training process of a psychiatrist, where useful information is extracted into EMR during a patient counseling session, and as their experiences with more patients accumulate, their clinical and diagnostic skills are elevated.\n\u2022 Conversation Records represent the pure transcripts of the diagnosis conversation between patient agents and psychiatrist agents. It helps the psychiatrist agent to conclude the electronic medical records of the patient agents.\n\u2022 Electronic Medical Records (EMR) indicate the summary of the patients' portrait, chief complaint, and symptom list. EMR can be leveraged as a summary of specific diagnosis sessions, which can be utilized by psychiatrists to search for similar cases and summarized skills.\n\u2022 Diagnostic Skills play as the optimizer in training psychiatrist agents. Summarized Skills take the dialogue history, diagnosis result generated by the psychiatrist agent, and the ground truth diagnosis result given by a professional psychiatrist in real life in D\u2074 as input, while output summarized skills mainly focus on what kind of symptoms are overestimated or underestimated during diagnosis procedure, guide the LLM-powered psychiatrist agent towards professional psychiatrists. Summarized skills help the psychiatrist agent to imitate the real-world distribution."}, {"title": "Supervisor Plugin of the Psychiatrist Agent", "content": "The supervisor plugin tracks the symptoms mentioned by the patient agent and maintains a detailed symptom list, where each symptom is assigned one of three statuses: unknown, true, or false. Meanwhile, the supervisor plugin updates the list based on the dialogue history, and also provides the question list psychiatrist agents need to ask in the next round, avoiding repeatedly asking similar questions but focusing on those unknown symptoms, which enhances the effectiveness of the diagnosis conversation. The supervisor plugin also controls the dialogue stage change as we define three stages of the diagnosis dialogue:\n\u2022 Start Stage: The start stage aims to start the diagnosis conversation session, mainly focusing on asking the chief complaints of the patient agents and building a healthy therapeutic alliance (Horvath et al. 2011) with them.\n\u2022 Exploring Stage: The exploring stage aims to collect detailed information of the patient agents, and tries to find out the most effective way to complete the diagnosis session.\n\u2022 End Stage: The end stage aims to finish the dialogue, summarizes the information collected, and gives some advice to the patient agents based on the dialogue history.\nThe instruction generation function of the supervisor plugin can be presented in Equation 5. The PPTins implies the constant prompt utilized in the instruction generation.\ninsi, stati = LLM(PPTc, PRF, stati-1, utt:i-1) \n\nThe supervisor plugin also acts as a teacher to generate skills based on the ground truth diagnosis result and the generated diagnosis result based on the dialogue history. The skill generation function is represented in Equation 6. PPT, implies the constant prompt utilized for reflecting skills, diag means the ground truth diagnosis result provided by the supervisor, based on annotation on D4 dataset.\nSkills = LLM(PPT,, PRF, utt., diag, diag) \nRetrieval Module\nThe retrieval module aims to search for helpful electronic medical records and summarized skills, to better generate responses and diagnosis results. The retrieval module is also responsible for the reassignment of the memory nodes, based on the diagnosis result. We modify the classical retrieve score calculation function (Park et al. 2023). However, since our system does not simulate the time step of the diagnosis, we do not apply the calculation of recency in our experiment. Therefore, we mainly calculate relevance score and importance score. The relevance score is calculated as rel = NORM(E(mem). E(query)). NORM indicates min-max scaling method for normalization. E represents the embedding model, and query suggests the last utterance during the diagnosis conversation, and the concluded EMR of the dialogue during the diagnosis. The importance score imp is set to 5 when the memory node is generated, and updates based on the diagnosis results, which suggests the effectiveness of the according memory. The importance score also applies min-max scaling normalization during calculation. Specifically, if the diagnosis result matches the ground truth, the importance score will be increased by one; if it does not, the score will be decreased by one. The final score is calculated as the weighted sum of relevance score = a\u2081rel + a2imp, while as are set to 1 in our experiments. Meanwhile, our observations during the experiments indicate that if we utilize the mere top-k retrieve method as prior work (Park et al. 2023) utilized, the memory node retrieved does not vary much with the experiment going, since the top-k memory nodes are fixed. Therefore, we apply a sampling memory retrieval method"}, {"title": "Experiments", "content": "To investigate whether the psychiatrist can improve his diagnostic skills, our core experiments lie on whether an AMC elevates dialogue quality and depression diagnosis accuracy. The general settings, including training strategy and metrics, are introduced below.\nSettings Since AMC's architecture has a supervisor plugin to conduct reflection, we follow the same experiment setting as previous works (Shinn et al. 2023; Renze and Guven 2024) in the training stage. The psychiatrist agent generates the diagnosis result through Equation 3.\nThe experiments involve two distinct settings: Quiz and Exam, evaluate on either train set or test set detailed in \u00a7. In the Quiz setting, the psychiatrist agent works with the train set. After each conversation, it generates a diagnosis result. If the result is incorrect, the agent receives feedback from the supervisor to refine its diagnostic skills and attempts the diagnosis again. This process tests the self-refinement capability of the LLMs, allowing the agent to improve its performance over time, even when applied to unseen patients.\nIn the Exam setting, the psychiatrist agent is evaluated on the test set. Here, the agent generates a diagnosis result based on the knowledge and memory it accumulates from the Quiz setting. Unlike the Quiz, the agent only has one opportunity to provide a diagnosis without a second attempt if the initial result is incorrect. This setting is designed to assess the agent's generalization ability, reflecting the real-world scenario where a diagnosis is made without the chance for revision.\nWe use gpt-3.5-turbo-0125 (OpenAI 2022) for the content generation and text-embedding-ada-002 to get embeddings of the content.\nMetrics Two accuracy aspects are involved: depression risk prediction accuracy and suicide risk prediction accuracy, we also report their average score as overall accuracy. For each patient agent, the risk of depression and the risk of suicide are classified into four categories: control, mild, moderate, or severe.\nResults: Does AMC work? To extensively validate whether AMC works, we conduct the main experiments under two scenarios:\n1. Original Dialogue (OD), AMC takes the original dialogue history in D\u00b9 as the conversation between the psychiatrist agent and the patient agent, in order to eliminate the bias of role-playing. The psychiatrist agent conducts the diagnosis directly based on this dialogue.\n2. Simulated Dialogue (SD), AMC generates simulated diagnosis dialogues between the psychiatrist agent and the patient agent, with the psychiatrist agent performing the diagnosis based on this newly generated dialogue.\nFor each scenario, we construct experiments on two settings: 1) Deactivate the memory module of the psychiatrist agent, evaluating the baseline capability of original LLMs, denoted as w/o memory. 2) Activate the memory module and retrieve 10 memory nodes from EMR and skills, denoted as w/ memory.\nWe present the results in Table 1, which demonstrate the efficacy of the memory module and reflection function on almost all settings, with stable improvements. Across original and simulated dialogues, the largest performance gain from memory and reflection is achieved on depression diagnosis accuracy, where even in the most difficult setting (test cases in simulated dialogues), an increase of 6.8% is observed. However, the performance of experiments on simulated dialogues has a significant decrease, despite the improvement of memory structure. The experiment result indicates the limited capability of LLMs to describe the symptoms precisely based on patient profiles. We attach the case study in the appendix, which can further explore the role-play ability of LLMs in diagnostic scenarios.\nTake Home Message: Memory module and reflection function significantly enhance depression diagnosis accuracy, especially in challenging scenarios, but LLMs struggle with precisely simulating symptoms in diagnostic dialogues, highlighting role-play limitations.\nEffects of different memories To investigate the impact of different memory types on depression diagnosis, we conduct an experiment by manipulating the activation levels of memories: 1) Deactivating the memory module, 2) Activating the memory module and retrieving memories from"}, {"title": "Effects of supervisor plugin", "content": "To validate the function of the supervisor plugin, we conduct an ablation experiment by disabling the guidance reflection and generation features. The results are presented in Table 2. The experiment demonstrated that the supervisor plugin improves the accuracy of risk prediction.\nTake Home Message: The supervisor plugin enhances targeted risk predictions."}, {"title": "Related Work", "content": "Conversational Agent Simulation System Conversational agents (CAs) are agents that can interact with users through natural language (Allouch, Azaria, and Azoulay-Schwartz 2021). With the development of large language models, more and more CAs are constructed based on LLMs (Park et al. 2023; Chen et al. 2023; Shao et al. 2023; Li et al. 2023a; Han et al. 2024; Huang et al. 2023; J\u00f6rke et al. 2024; Yang et al. 2024) because of its autonomy, reactivity, pro-activeness and social ability (Xi et al. 2023). Depending on whether controlling the conversation process, CAs can be divided into proactive agents (Liao, Yang, and Shah 2023; Deng et al. 2023a; Zhang et al. 2024b; Deng et al. 2024, 2023b) and inactive agents. In our system AMC, psychiatrist agents play the role of collecting information, deciding the dialogue process, and conducting diagnosis, while patient agents need to provide information based on their profiles and the questions.\nMental Health Diagnosis Chatbot According to previous research, mental health diagnosis based on dialogue can be more favorable and effective than self-rating scales (Vaidyam et al. 2019; Abd-alrazaq et al. 2019), and a better way to elicit honest self-disclosure about personal experiences and emotions (Kawasaki et al. 2020; Duvvuri et al. 2022). Based on whether modifying the weights of language models, the chatbot can be divided into fine-tuning method (Yao et al. 2022; Gu et al. 2024; Lan et al. 2024; Ren et al. 2024) and prompting method (Chen et al. 2023; Tao et al. 2023; Li et al. 2024b; Ferrario, Sedlakova, and Trachsel 2024; Kumar et al. 2024; Wang et al. 2023; Seo and Lee 2024). Although the fine-tuning method has achieved high diagnosis accuracy and a standardized information-collecting process, the prompting method shows its potential to stimulate the capability of LLMs without modifying the weights, which could be costly as the scale of the language model increases."}, {"title": "Limitation and Future Work", "content": "Our research has limitations that can be further improved in future work.\nWhile the Agent Mental Clinic (AMC) system has shown promising potential in enhancing depression diagnosis through simulated dialogues, some areas warrant further development. The system's effectiveness is currently shaped by its reliance on the D4 dataset, which may limit its applicability in diverse cultural and linguistic contexts. Additionally, the accuracy of role-playing within the system reflects the inherent challenges of current large language models (LLMs) in fully capturing the details of depressive symptoms, which can sometimes result in less precise simulations. Although the memory retrieval module has seen improvements, it still encounters difficulties fully encompassing mental health diagnoses' complexities. Moreover, the absence of real-time expert feedback may affect the system's ability to adapt to particularly complex cases.\nFuture efforts would focus on broadening the AMC system's adaptability to various cultural settings, further refining the role-playing capabilities of LLMs, and advancing the sophistication of memory retrieval methods. Integrating real-time expert feedback and extending the system's application to additional mental health conditions could enhance its accuracy, cultural sensitivity, and overall comprehensiveness in diagnostic processes."}, {"title": "Conclusion", "content": "In this paper, we propose a novel conversational agents simulation system named AMC, which is designed for depression diagnosis conversations. We construct the psychiatrist agents with tertiary memory structure, and construct supervisor plugin to perform better quality long-term depression diagnosis dialogue. Our experiments demonstrate the better performance of our AMC, which could be utilized for simulating the depression diagnosis dialogue in real-life scenarios and getting the diagnosis results based on the conversation history. We conduct the experiments on the original D\u00b9 history and generate simulated dialogues, both indicate the effectiveness of the memory structure and the supervisor plugin. In future work, we aim to expand the application of AMC to other domains of mental health and explore the incorporation of additional expert feedback mechanisms to further enhance its diagnostic precision. We believe AMC can be presented as an effective automation in depression diagnosis scenario."}, {"title": "Ethics and Broader Impact Statement", "content": "Data Privacy The data used in this paper comes from a previously published dataset, and all evaluation metrics are based on objective computations. Although experts' evaluations could be included in future work, this study currently raises little ethical concerns. Any content related to user privacy has been appropriately processed for disclosure in the paper and supplementary files.\nIntended Use AMC system is designed to conduct simulated depression diagnosis dialogues and assess both the depression risk and suicide risk of potential patients. The system can be subsequently be deployed as interactive system for intern psychiatrist training and preliminary screening for potential patients.\nPotential Misuse Since the AMC system is designed to conduct the preliminary screening, the psychiatrist agent is equipped with diagnosis knowledge collected from ICD-11. Therefore, the psychiatrist agent is not appropriate to provide medical suggestions, which might lead to misinterpretation or misuse of the information provided. Instead, the psychiatrist agent is intended to offer general guidance and support during the initial stages of assessment, while encouraging users to seek professional medical advice from a qualified healthcare provider for any specific concerns or diagnoses. This approach ensures that the system serves as a complementary tool, rather than a substitute for professional medical consultation.\nEnvironmental Impact Our AMC can be utilized for potential patient who are suffering from depressive mood. Currently, mental health issues, especially depressive disorder, are prevalent, yet a significant number of individuals fail to recognize their own mental health problems promptly or seek timely treatment. The AMC system, constructed as a future platform accessible across three interfaces, aspires to serve as an efficient tool for enhancing psychiatrists' professional expertise, offering the public convenient and highly confidential diagnostic assessments, and improving diagnostic accuracy across the mental health field. Through these contributions, we hope to contribute to the improvement of global mental health."}, {"title": "Hallucination", "content": "In the simulated dialogue experiments, we found that when engaging in role-playing, LLMs tend to make unfounded elaborations based on previously mentioned symptoms or events. In the example presented in Table 3, the patient only mentioned a past tendency toward self-harm in the original dialogue in D\u2074, which was promptly stopped by friends. However, in the simulated dialogue, the patient agent exaggerated the description on this by stating \"It doesn't happen very often, about once a month or so\"."}, {"title": "Repetition", "content": "We also observed that when generating dialogues, large models tend to replicate the format and content of previous exchanges, leading to repetitive phrasing or responses. This causes the current phase of the conversation to mirror earlier ones, and subsequent dialogues to mirror the current phase, resulting in the entire conversation using similar structures and getting stuck on certain topics. The example is represented in Table 4."}, {"title": "Language Style", "content": "Owing to the lack of original dialogue for a single patient, the data available for LLMs to learn from is also constrained. Consequently, in simulated dialogues, the models often fail to accurately capture the patient's language style and may even use professional medical terminology in their first session, which doesn't align with the patient's identity when describing symptoms, as Table 5 represents. This could lead to potential diagnostic inaccuracies."}]}