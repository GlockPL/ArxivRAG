{"title": "Cardiverse: Harnessing LLMs for Novel Card Game Prototyping", "authors": ["Danrui Li", "Sen Zhang", "Samuel S. Sohn", "Kaidong Hu", "Muhammad Usman", "Mubbasir Kapadia"], "abstract": "The prototyping of computer games, particularly card games, requires extensive human effort in creative ideation and gameplay evaluation. Recent advances in Large Language Models (LLMs) offer opportunities to automate and streamline these processes. However, it remains challenging for LLMs to design novel game mechanics beyond existing databases, generate consistent gameplay environments, and develop scalable gameplay AI for large-scale evaluations. This paper addresses these challenges by introducing a comprehensive automated card game prototyping framework. The approach highlights a graph-based indexing method for generating novel game designs, an LLM-driven system for consistent game code generation validated by gameplay records, and a gameplay AI constructing method that uses an ensemble of LLM-generated action-value functions optimized through self-play. These contributions aim to accelerate card game prototyping, reduce human labor, and lower barriers to entry for game developers.", "sections": [{"title": "1 Introduction", "content": "Computer game development is a creative endeavor that requires significant human effort. To address this, extensive research has been conducted to automate or assist these processes through computational methods [9]. Recent advances in Large Language Models (LLMs) have further expanded the possibilities in this domain, enhancing tasks such as game mechanic design [4], programming [34, 23], and game AI development [36]. More importantly, the robustness of LLMs in processing diverse text inputs shows the potential to integrate these subtasks into a comprehensive game development pipeline, especially for card games that can be represented purely in text. Such integration could streamline workflows and make game development more accessible. However, LLMs face the following significant challenges [9].\n(1) Novel design in game mechanic: Game mechanics, defined as the \u201cactions, behaviors and control mechanisms afforded to the player\" [13], are essential game design topics where novelty are often encouraged. Prior work used LLMs to compose game mechanics [4], which showcased the potential for novel designs through the controllable randomness of LLMs. However, it remains unclear how to encourage a more intentional novelty such that LLM outputs are neither too similar to existing games nor self-repeating.\n(2) Consistent game generation: Translating game mechanics into consistent game code is critical for prototyping. To improve the consistency, prior work used LLM-generated I/O examples [20] to validate the generated code for short functions. This approach is not suitable for long programs with multiple rounds of interactions (such as card games), where the difficulty of generating correct I/O examples increases with the number of interactions.\n(3) Scalable evaluation with game AI: Evaluating and refining game mechanic designs requires gameplay, which to be automated requires the support of AI agents with sufficient intelligence to explore game dynamics [14]. However, existing methods, ranging from reinforcement learning to LLM-based agents, come with significant drawbacks, including high hardware requirements [21], long training times [27], costly inference [32], and specialized training needs [19]. These methods are costly and impractical for rapid prototyping of diverse games.\nTo address these challenges, we propose Cardiverse, an LLM-based pipeline designed to assist human designers in card game prototyping. Our work integrates game mechanic design, code generation, and gameplay AI creation into a cohesive framework. The key contributions are as follows:"}, {"title": "2 Related Work", "content": "2.1 Game Mechanics Design\nAssisting human in game mechanics design has been a long time topic, where early work applied search-based methods to optimize game mechanics in a prefined parameter space [31]. Later, with the help of large-scale game data in domain specific languages, prior work framed game mechanics design as a recommendation problem [22]: game mechanics is represented as entities (e.g. shooter, enemy) and the interaction rules between them. Then, the framework recommend additional mechanics by maximizing entity overlaps between mechanics input and mechanics in the database. Recently, as LLMs have shown its capabilities in creating content for interactive experience [1, 30, 17], prior work further enhanced the recommendation pipeline by using LLMs to composite retrieved entities into game mechanics in short phrases [4].\nHowever, the following gaps exist: (1) Prior work fail to represent game mechanics with hierarchical dependency. For example, mechanics in platformer games are less dependent on each other: adding a double jump capability to the player will not affect the game experience when the player hit the trap. But in card games, adding a card stealing capability may contradict to a more fundamental mechanic where player wins by emptying its hand. Without a representation in hierarchical dependency, downstream tasks such as recommendation may not work well. (2) It remains unclear how to encourage novel mechanics design. We define \"novel design\u201d here as creating game mechanics that are not seen in databases, which differs from \"recommendation\" that retrieves existing game mechanics from databases [22]. To propose game mechanics that differ from database, it is necessary to gain a global understanding over the entire database, which has not been well discussed in prior game design studies.\nOur method offers two improvements: (1) considering the prevalence of hierarchical dependencies in card game mechanics, we propose a new method to represent and extract game mechanics in directed graphs. (2) Inspired from [6] that gains a global understanding of the database by summarizing and querying in entity clusters, we group similar game mechanics across all games into clusters, which are summarized, mutated, and retrieved in novel game design.\n2.2 Program synthesis in games\nTraditionally, many works focus on generating short code snippets in a specific domain such as matrix operations [26] or list processing [7]. However, it is challenging to apply similar approach to game code generation, as it involves much larger search space, which does not suit iteration-based methods. Recently, large language models (LLM) have emerged as strong methods for program synthesis, where domain-specific fine-tuning [34], in-context learning [10], LLM agent systems [23], and LLM pipelines [35] are heavily studied. These methods have greatly improved code generation quality in many aspects, ranging from code snippets to real-world software engineering tasks [15].\nHowever, it remains unclear how to validate the consistency between user instructions and generated code for interactive code environments, since I/O examples [20] are hard to generate in these cases."}, {"title": "2.3 Game Intelligence", "content": "Creating AI systems capable of playing games intelligently has been a long-standing research focus. Early approaches relied on manually crafted game features, optimized through combination processes [28]. Over time, neural networks have automated both feature extraction and policy formulation [24, 38, 3]. While these neural network-based approaches demonstrate exceptional gameplay intelligence, their model structures and game representations are often tailored to specific cases, limiting scalability across diverse games.\nRecent advancements in large language models (LLMs) offer a promising solution to this limitation. Using natural language for inputs and reasoning, LLMs can adapt to a wide variety of games without case-specific customization. In related studies, LLMs have been employed as gameplay agents, wherein the models are invoked at every game turn [36], optionally supported by external storage for long-term memory or reflection [11, 27, 39]. Other studies combine LLMs with other neural networks by using LLMs to design reward functions [21, 2], using neural networks to narrown down the search space for LLMs [37], or training LLMs from scratch to predict game actions using real gameplay data [25].\nTo achieve scalable gameplay AI large-scale evaluations, we explore the following techniques that minimize the cost and latency in both construction and application stages: (1) we aims to get a gameplay policy in code [18, 19] rather than leveraging LLMs in each game decision. (2) We do not optimize the policy by LLM-based reflections with gameplay data [27], as it can hardly be paralleled and introduce large noises. (3) We directly use win rate as optimization target rather than any fine-grained signals (such as reward by MCTS [19]), as they may not be effective in certain games (e.g. extremely unbalanced game state trees)."}, {"title": "3 Method", "content": "Our work composites an LLM-based card game prototyping pipeline by three components: In 3.1 we leverage game mechanic graphs to enable novel game mechanic design, producing new games in text descriptions. In 3.2 we use the text descriptions from the previous step to generate code for the game. In 3.3, gameplay AI is created using the generated code, thus enabling game evaluations for human designers."}, {"title": "3.1 Game Mechanic Design", "content": "The design problem is defined as below: given a card game database, where games are represented in text descriptions, we aim to create game mutations that avoid high resemblance to the database. We tackle this by applying a mutation instruction to LLMs, where we propose game mechanic replacements explicitly. Specifically, our approach begins with extracting mechanics from existing card game descriptions. The mechanics are subsequently clustered, and new mechanics are generated within each cluster using LLMs. Finally, for a game to be mutated, mechanics that frequently appear in the database are suggested to be replaced by newly-designed ones within the same cluster.\n3.1.1 Mechanics extraction\nTo encourage a comprehensive and interconnected mechanics extraction from the text description, we propose a game mechanic graph representation. As illustrated in Figure 1, game mechanics represented in short text phrases are stored as nodes, where all games share the root node \"the game ends\". The control flow between the nodes are represented as directed edges. The edge direction denotes the game effect contribution. For instance, in the game UNO, \"empty the hand\" is a downstream child of the root node because it directly leads to game ending.\nWe adopt a backtracking approach to extract the graphs. Starting with the shared root node, we use LLMs to expand the search frontier like a breadth-first search. Specifically, we input the game description and expansion history to LLM and instruct it to identify all mechanics that directly leads to the mechanics to expand. In the same way we recursively expand all expanded nodes, where expansion are terminated by both LLM's judgment and a manually set depth limit. It should be noted that while the mechanics are extracted in a tree search (thus the depths of nodes are recorded), we allow new nodes to be connected to any existing nodes so the extracted results may not be a tree."}, {"title": "3.1.2 Mechanics Clustering and Designing", "content": "We aggregate all mechanics from the game database and cluster them by semantic similarities and tree depth. Semantic similarity is measured using cosine similarity of text embeddings, with the text-embedding-3-large model serving as the embedding generator. A hierarchical clustering algorithm is employed, with a manually set similarity threshold of 0.4.\nFor clusters containing more than 3 mechanics, we input LLMs with all mechanic descriptions to summarize their shared themes and generate new mechanics adhering to these themes. To enhance diversity in the results, we vary the system prompts using techniques outlined in [8]."}, {"title": "3.1.3 Retrieval-augmented Game Design", "content": "The newly generated mechanics can be utilized to create game mutations that differ significantly from existing database entries. Given a game description, we extract its game mechanic graph and map the mechanics to existing clusters. Mechanics from clusters with high occurrences in the database are identified as candidates for mutation. Our framework then proposes replacing these frequently occurring mechanics with newly generated ones from the same cluster. An LLM applies this proposal to the game description, using a self-reflection process to resolve potential ambiguities or conflicts in the game mechanics. This approach ensures that the mutated game remains coherent while introducing novel elements."}, {"title": "3.2 Game Code Generation", "content": "Similar to [35], we adopt a workflow where LLM function calls are assembled into a predefined procedure. It receives game mechanics description in natural language as input, outputting game code for the given game."}, {"title": "3.2.1 Structurize, draft, and debug", "content": "The framework starts by converting a text description into a structured markdown format by LLMs, detailing game states, deck initialization, legal actions, and gameplay process. Then, the structured description, together with retrieved examples and game engine templates (see Appendix 7.1) is fed to LLMs to draft game code. After a fixed number of LLM self-refinements, the draft moves to debugging."}, {"title": "3.2.2 LLM-based validation", "content": "Once the code produces no errors, 5 gameplay records are sampled from the previous phase, where the gameplays are produced by generated code. The record includes the information from the last 6 game rounds and the game ending. In each round, the information includes the observation and the action of the current player, which are converted from the game state dictionary to natural language by a fixed parser. To make the record more legible, game comments are also generated along the observation. Rather than generated directly from LLMs, game comments are generated from the commentary function inside the game code, where the function is designed by LLMs."}, {"title": "3.3 Gameplay AI Generation", "content": "Treating LLMs as simply feature extractors, we use LLMs to propose a pool of policy components. Then we step-wisely select best components using self-play data, leaving the optimization to non-LLM processes (shown in Figure 4). The policies are represented as Q-functions in code, that return a score given the current game state and proposed action. During the game, the final score of a proposed action is averaged over all policy component outputs:\n$a_T = \\text{argmax}_{a \\in A} \\sum_{i=1}^E Q^i(s_T, a)$ \nwhere $A$ represents the legal action space that is generated by game environment. $Q^i$ is the LLM-designed policy code component. $s_T$ is the current game state."}, {"title": "3.3.1 Construct Policy Pool", "content": "To create a diversified policy pool, we firstly let LLMs to propose high-level strategies in text using different prompts. Then, we augment the policy pool by a mutual inspiration process, where strategies from different prompts are remixed into new ones by LLMs. Next, all high-level strategies in text are converted to python functions, which take game state and proposed action dictionaries as inputs, returning a score that represents the quality of the action. Finally, we conduct a second augmentation by creating negative policies, where we multiply -1 to the output of all policy components. In this way we get a policy pool with original, mutually-inspired, and negative components, which are all fed to optimization process."}, {"title": "3.3.2 Optimize Selection by Playing", "content": "As shown in Equation 1, the policy is composited by a linear combination of components, where the components are selected by self-play data. Considering the noise of the reward signal [19], we use a step-wise inclusion technique to build the selection set. Specifically, in the first iteration, we only include the component that achieves the highest win rate in the test environment. Then we start the second iteration where we try adding one another component to the current selection. In this way, we add one component at a time, until the win rate does not improve.\nIn this paper, the optimization process repeats twice: in the first trial, the policy is optimized by playing against random agents. In the later trial, the policy is optimized by playing against the mixture of random agents and the formerly-trained agents. We pick the results of the later trial as the final policy."}, {"title": "4 Results", "content": "We manually collected 106 commonly seen card games from the web as text descriptions, including 25 casino games (mostly poker), 27 trick-taking games, 11 rummy games, 10 solitaire-like games, and 33 others. These games are"}, {"title": "4.1 Novel Game Design", "content": "In this task, we use all card games to construct game mechanics database and compare the newly-generated games to them.\nOur method can extract meaningful game mechanics from game descriptions. First, the extracted mechanic graphs can represent the semantic distances between the games. As shown in Figure 5a, the mechanic graphs of two poker games (Bull Poker and Holdem) are closer to each other than a card game from a different genre (Go Boom), which aligns to their text descriptions in the database. Also, our method reveals the dependency between game mechanics. In Figure 5b, the mechanics \"Highest-ranking poker hand\", which frequently appears in poker games, is associated with relevant mechanics such as \u201cshowdown\" and \"poker hand rankings\".\nWe demonstrate that our method encourages novel game designs by measuring how its generated game mutations deviate from the original 106 games. We define Quantiled Max Similarity (QMS) as a metric. Given a base game $g$ and a mutation method $M$, QMS($g, M, p$) represent the $p$th percentile of the closest cosine similarity scores between the mutations of $g$ and every game in database $D$."}, {"title": "4.2 Consistent Code Generation", "content": "We use gpt-4o-2024-08-06 as LLM backbone and text-embedding-3-large from OpenAI as text embedding model. We test our framework using 29 commonly-seen card games and 28 games that mutate from them using our method in 3.1 (detailed in 7.3).\nThe following metrics are used in our evaluation, where the second and third metric originate from [23]:\nGeneration Success (Succ) Given a fixed maximum number of iterations, the percentage of games that can be generated bug-freely in 10 randomized execution tests.\nExecutability (Exec) For games that pass the first criteria, the percentage of successful executions with 100 randomized seeds.\nEmbedding Consistency (EmbCon) Semantic embedding similarity between the code and the description of the game. Here we used structurized description detailed in 3.2."}, {"title": "4.3 Gameplay AI Generation", "content": "After a manual verification on all generated games (both common and mutated) in the previous task, some games are excluded from the test set for gameplay AI, as they are either purely luck-oriented games with no strategies involved, or they are not completely consistent with the corresponding game description. Ultimately, we selected 13 common games and 6 mutated games for gameplay AI evaluation.\nWe benchmarked our method against prior works (Chain-of-Thought [33], ReAct [36], Reflexion [27], and Agent-Pro noted as BeliefAgent [39] detailed in 8.1). Although for our evaluation, we intended to use the most performant LLM model available for every method being compared, the number of LLM calls being made by other works is a polynomial order of magnitude greater than our work. For the number of games being compared, this token cost is prohibitively expensive (Fig 7). We have therefore opted for the most performant LLM within budget for other works (40-mini, which boasts similar performance) and 40 for our work. Our model is limited in that it requires at least 40 for its coding accuracy. However, despite the difference between LLM backbones, the 40-mini being used for other works is comparable if not better than the LLM backbones that were originally used in the respective works.\nThe win rate performance of a method is assessed by its advantage over competing policies. Specifically, we measure how effectively policy $p_2$ outperforms policy $p_1$ under identical contexts. In games with $n$ players, the first $n-1$ players act as defense, and the last player as the attacker. The win rate of an attacker using $p_1$ against defenders using $p_2$ is denoted as $\\omega(p_2, p_1)$. The advantage of $p_1$ over $p_2$ is defined as:\n$A(p_1, p_2) = \\omega(p_2, p_1) - \\omega(p_2, p_2)$"}, {"title": "6 Discussion", "content": "Our work benefits the related research communities in the following ways:\nGame mechanics Representation The new graph-based game representation in our work, along with the corresponding extraction method, can be applied in various downstream tasks in related domain. For example, compared to game distance metrics that builds on low-level code structure [16], our work could facilitate a more explainable distance metric, as its node units incorporates higher-level abstractions with increased legibility. Also, as our graph-indexed database reveals the dependency between game mechanics (in Figure 5b), it can be integrated into a game mechanics recommendation framework with friendly user interface as [22], thus enabling a human-in-the-loop game design. In addition, our work can pre-process game descriptions to graphs, paving the way to a concept-based game generation model with a structure like [29].\nGame Code Data Generator Our work can be treated as a synthetic data generator specialized in creating programs with long and complex instruction. Its generated game code can be used to fine-tune domain specific code generative models [34], or be used as test case data for general program synthesis methods.\nScalable Gameplay AI Our experiments shows for a large proportion of card games, our method that does not require LLM calls during the gameplay, achieves similar performance as prior methods with gameplay LLM inference. Therefore, we could enable large applications of gameplay AI without the cost that increases with gameplay rounds, which benefits both consumer-oriented entertainment usse, and more affordable game prototyping.\nExtendable Gameplay AI Benchmark Compared to prior work that focus on a small set of card games [5, 38], we provide a much larger scale game benchmark environment for gameplay AIs. Additionally, prior work in LLM-based gameplay AI suffers a limitation in evaluation: LLMs may have seen the game strategy during their training, making it challenge to evaluate the true performance of the gameplay framework. By designing and constructing novel game environments with minimal human effort, our work can help bridging this gap.\nLimitation and Future Work In game mechanics design, the consistencies between game mechanics is a critical issue. While prior work [22] advocates a human-computer cooperation to solve the potential conflicts, our work relies on the reflection capability of LLMs to mitigate this problem. However, as our work can extract game mechanics graphs from text descriptions, it would be beneficial to explore whether the mechanics graph can help the advanced reasoning on potential game mechanics conflicts.\nIn game code generation, the current validation process is inefficient, making its token cost close to that of adavanced model (such as o1). Future work may aim to lower the token use by other agent pipeline designs.\nCurrently, our game AI does not explicitly consider other players' intention, which is a commonly-used component in prior work [39, 11]. Besides, it could also be promising to explore whether the reasoning results from LLM agents (such as Reflexion [27]) can be distilled to our policy-code-based results."}, {"title": "7 Code Generation Details", "content": "7.1 Game engine design\nInstead of generating entire game code from scratch [23], we designed a card game engine in Python, converting the original generation task into completion tasks for several predefined functions. To enable maximum compatibility for various card games and easy integration with gameplay AI, we adopt a framework structure where (1) game state updates and gameplay AI are decoupled [38]; and (2) dependencies among game logics are minimized by a functional programming design [34]. The logic of card games are abstracted into 6 functions (initiation, initialize deck, initial dealing, proceed round, get legal actions, and get payoffs) running in a predefined procedure (see Figure 8). Each function, which modifies the game state in dictionary form, is what our pipeline will generate.\nThe gameplay AI is integrated into this system by receiving game observations and outputting game actions in dictionaries. While our gameplay AI directly handles dictionaries, for all LLM-based agents we compared against, the game state is converted to the following format. The example below shows the observation information in one game turn. LLM agents may receive concatenated information from the past several turns."}, {"title": "7.2 Intermediate Results of LLM-based Validation", "content": "In the code validation process, our method compares the generated gameplay record with the game description, identifying whether there exists an inconsistency. The gameplay record follows the same format as the game observation above. But multiple consecutive observations from different players are concatenated together. Here is an example output of the validation process."}, {"title": "8 Gameplay AI Details", "content": "8.1 Evaluation on Gameplay AI\nAlthough some previous work demonstrates satisfactory performance in several popular games, they are excluded from our comparative analysis as their primary contributions focus on specific game genres [32] or particular attributes of game mechanics [19]. For example, methods that learn directly from reward signals at MCTS leaf nodes [19] can hardly work on games with extremely unbalanced search trees. Because it is challenging to reach all terminal states with reasonable computational resources. As a result, one has to estimate the reward without reaching the terminal nodes, which introduces more uncertainties to the reward signals.\nConsequently, we focus on prior work that are scalable. We choose Chain-of-Thought [33], ReAct [36], Reflexion [27], and AgentPro [39] as our baselines. As AgentPro is particularly noteworthy for its application of Belief-aware Decision-Making imperfect information game scenarios, we specifically implement the Belief-aware part for comparison.\nThe evaluation protocol involved training Reflexion agents across all games, with performance measured using a rolling average winning rate calculated over windows of 40 games. This measurement was repeated for 10 distinct windows across all games in our test set. The optimal reflection step was subsequently determined by selecting the iteration that yielded the highest mean winning rate across the entire game suite."}, {"title": "8.2 Ablation Study", "content": "We demonstrate the effect of the two major components in our gameplay AI method by comparing our method against two ablations: (1) No ensemble: rather than creating an ensemble of action-value functions, we generate only one comprehensive policy with its corresponding function. (2) No optimization: after the first augmentation stage, we skip the rest of the pipeline and use all policy components to create the ensemble."}, {"title": "8.3 Intermediate Results", "content": "During the construction of gameplay AI, our method produces policy components in text form first, which is converted to code during the second augmentation process."}, {"title": "9 LLM System Prompts", "content": "9.1 Game Mechanics Design\n9.2 Code Generation\n9.3 Gameplay AI Generation"}]}