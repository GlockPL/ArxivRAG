{"title": "Mechanistic understanding and validation of large AI models\nwith SemanticLens", "authors": ["Maximilian Dreyer", "Jim Berend", "Tobias Labarta", "Johanna Vielhaben", "Thomas Wiegand", "Sebastian Lapuschkin", "Wojciech Samek"], "abstract": "Unlike human-engineered systems such as aeroplanes, where each component's role and\ndependencies are well understood, the inner workings of AI models remain largely opaque,\nhindering verifiability and undermining trust. This paper introduces \\u25c8 SEMANTICLENS, a\nuniversal explanation method for neural networks that maps hidden knowledge encoded by\ncomponents (e.g., individual neurons) into the semantically structured, multimodal space\nof a foundation model such as CLIP. In this space, unique operations become possible,\nincluding (i) textual search to identify neurons encoding specific concepts, (ii) systematic\nanalysis and comparison of model representations, (iii) automated labelling of neurons and\nexplanation of their functional roles, and (iv) audits to validate decision-making against\nrequirements. Fully scalable and operating without human input, \\u25c8 SEMANTICLENS is\nshown to be effective for debugging and validation, summarizing model knowledge, aligning\nreasoning with expectations (e.g., adherence to the ABCDE-rule in melanoma classification),\nand detecting components tied to spurious correlations and their associated training data. By\nenabling component-level understanding and validation, the proposed approach helps bridge\nthe \"trust gap\" between AI models and traditional engineered systems. We provide code for\n\\u25c8SEMANTICLENS on https://github.com/jim-berend/semanticlens and a demo\non https://semanticlens.hhi-research-insights.eu.", "sections": [{"title": "1 Introduction", "content": "Technical systems designed by humans are constructed step by step, with each component serving a specific,\nwell-understood function. For instance, an aeroplane's wings and wheels have clear roles, and an edge\ndetection algorithm applies defined signal processing steps like high-pass filtering. Such a construction by\nsynthesis not only helps to understand the system's overall behaviour, but also simplifies the validation of\nits safety. In contrast, neural networks are developed holistically through optimization, often using datasets\nof unprecedented scale. While this process yields models with impressive capabilities that increasingly\noutperform engineered systems, it has a major drawback: it does not provide semantic descriptions of each"}, {"title": "2 Related Work", "content": "\\u25c8 SEMANTICLENS is a holistic framework that enables a systematic concept-level understanding of large\nAl models. Its core elements rely on previous research advances related to concept visualization, labelling,\nattribution, comparison, discovery, and audits, as well as human-interpretability measures.\nFeature Visualization To describe the role of individual components of a neural network, input images\n(referred to as \"concept examples\" in this work) are commonly sought that maximize their activation [25, 26,\n27, 14, 16, 28]. Concept examples can either be generated synthetically using gradient-based approaches [29,\n26, 30, 31, 32] or diffusion-models [33], or, alternatively, selected from a test dataset by collecting neuron\nactivations during predictions [16, 15, 28, 14]. As synthetic concept examples often result in data samples that\nare out of the training data distribution, we select examples from the original test dataset. Notably, as multiple"}, {"title": "3 Methods", "content": "\\u25c8SEMANTICLENS embeds each component of a neural network into a semantic space. This embedding is\nrealized in two steps as described in the following subsections.\n3.1 Describing the Role of Neurons via Concept Examples\nTo describe the role of a neuron, highly activating data samples are retrieved from the (training) database.\nSince the concept represented by the neuron can only occur in a small part of a large input sample, we facilitate\nthe CRP framework [15] to identify the relevant part of the input and crop each data sample to exclude input\nfeatures with less than 1% of the highest attribution value, as illustrated in Supplementary Fig. H.1a. For\nVision Transformers (ViTs) the CRP method is not available yet, therefore we approximate attributions by"}, {"title": "3.2 Transformation into a Semantic Space", "content": "In the second step, \\u25c8 SEMANTICLENS generates a universal semantic representation for each model component\nbased on the concept examples. To this end, we employ a foundation model F that serves as a semantic expert\nof the data domain, operating on the set of concept examples Ek. As illustrated in Fig. 1a for step 2, we\nobtain the semantic representation of the k-th neuron in layer l as a single vector vk in the latent space S of\nfoundation model F (index l omitted for the sake of clarity):\nvk := E(x) \u2248\nE(F(x))\nx~Ek\n1\n|Ek| x\u2208Ek\n(1)\nvk XEEk F(x) ESCRd. (2)\nComputing the mean over individual feature vectors {F(x)}x\u2208\u025bk (as also proposed in [48]) is usually\nmore meaningful than using individual vectors (e.g., for labelling as in Supplementary Note D). Averaging\nembeddings can be viewed as a smoothing operation, where noisy background signals are reduced, resulting in\na better representation of the overall semantic meaning. Setting |Ek| = 30 results in converged Vk throughout\nImageNet experiments, as detailed in Supplementary Note D."}, {"title": "3.3 Concept Search, Labelling and Comparison", "content": "As semantic embeddings & are elements in a vector space, we measure similarity s directly via cosine similarity,\nas is also the design choice of CLIP [23]:\ns cos: Rd \u00d7 Rd \u2192 [-1,1], (x, y) \u2192\n(x,y)\n||x||2||y||2\n(3)\nSearch: Given a set of semantic embeddings of model components VM = {v1, ..., vk} and an additional\nprobing embedding vprobe representing a sought-after concept, we can now search for model components\nencoding the concept via\nv* = argmax{s(vprobe, v) \u2013 s(V<>, v)}, (4)\nDEVM\nwhere we additionally subtract the similarity to a \"null\" embedding <> representing background (noise)\npresent in the concept examples if available. For text, e.g., it is common to subtract the embedding of the\nempty template to remove its influence [18], leading to more faithful labelling in Supplementary Note D.4.\nLabel: In order to label the model representation VM, a set of predefined concepts is embedded, resulting\nin Vprobe := {vprobe,..., vprobe} C Rd. Analogously to Eq. (4) each neuron is assigned the most aligned label\nfrom the pre-defined set, or none if the similarity falls below a certain threshold.\nCompare: Two models N and M may be quantitatively compared via the number of neurons that were"}, {"title": "3.4 Auditing Concept Alignment", "content": "As outlined in Section 4.2, it is important to measure how well the used concepts of a model are aligned\nwith expected behaviour. In order to compute concept alignment, we require a set of model embeddings VM,\nand a set of expected valid and spurious semantic embeddings Vvalid and Vspur, respectively. For each model\ncomponent k, we then compute the alignment scores\n\u03b1valid\nk\n= max {s(vv, vk)-S(V<>, vk)}, \u03b1spur = max{s(vs, vk)-S(V<>, vk)}.\nvv EVvalid\nVs EV spur\nAdditionally, it is important to take into account how the components are used. We thus propose to retrieve the\nrelevance of each model component during inference, e.g., the relevance for predictions of a specific class.\nOptimally, all relevant components are aligned to valid concepts only, i.e., avalid > 0 and aspur < 0. A high\nspurious alignment score aspur > 0 indicates potential harmful model behaviour. Neurons that aligned to\nneither should be examined more closely, representing unexpected concepts."}, {"title": "3.5 Human-Interpretability Measures for Concepts", "content": "We now introduce measures to capture the human-interpretability of concepts.\n3.5.1 Concept Clarity\nThe clarity measure aims to represent how easy it is to understand the role of a model component, i.e., how\neasy it is to grasp the common theme of concept examples. Intuitively, clarity is low, when there is a lot of\ndistracting (background) elements in the concept examples. Further, clarity is low when a concept is very\nabstract and many, at first glance, unrelated elements are shown throughout examples. To measure clarity,\nwe compute semantic similarities in the set of concept examples, inspired by [54, 38, 56]. Cosine similarity\nserves here as a measure of how semantically similar two samples are in the latent space of the used foundation\nmodel. For the overall clarity score of neuron k, we compute the average pair-wise semantic similarity of\nthe individual feature vectors Vk = {Vk,i}i:\nIclarity (Vk) :=|{Vi, Vj)|1 \u03a3\nk|= \u03a3\nk|-1)Vk Vk1 i=1 j\u2260i scos(Vk,i, Vk,j)\n(6)\n\u2208 [\u2212\u22121,1], (7)\nWhere the last expression is a formulation that is computationally less expensive, and circumvents the need to\ncompute large similarity matrices."}, {"title": "3.5.2 Concept Similarity and Redundancy", "content": "The semantic representation allows conducting comparisons across arbitrary sets of neurons without being\nrestricted to neurons from identical layers or model architectures. In particular, it allows us to assess the degree\nof similarity between the concepts of two neurons k and j, which we define as"}, {"title": "3.5.3 Concept Polysemanticity", "content": "A neuron is considered polysemantic if multiple semantic directions exist in the concept example set. Formally,\nwe define a neuron as polysemantic if subsets of Ek can be identified that provide diverging vs. The\npolysemanticity measure is defined as\nk\nIpoly (V(1)...(h)) := 1 - Iclarity (Eve) v i = 1,...,h}),\nk\n| \u03a3(\n(10)\nwhere V(i)\nVk for i = 1, ..., h is a subset of the embedded concept examples, generated by an off-the-\nshelf clustering method, where we use h = 2 throughout experiments. Alternatively, as proposed by [54],\npolysemanticity can be measured as an increase in the clarity of each set of concept examples, which, however,\nperforms worse in the user study evaluation as detailed in Supplementary Note G.1."}, {"title": "4 Results", "content": "We begin in Section 4.1 with demonstrating how to understand the internal knowledge of AI models by\nsearching and describing the semantic space. These functionalities provide the basis for effectively auditing\nalignment of the model's reasoning wrt. human-expectation in Section 4.2. We demonstrate how to spot\nflaws in medical models and improve robustness and safety in Section 4.3. Lastly, computable measures for\nhuman-interpretability of model components are introduced, enabling to rate and improve interpretability at\nscale in Section 4.4.\nThe different sets of experiments reported in this paper were conducted on a variety of models, including\nconvolutional neural networks with ResNet and VGG architectures as well as different ViTs. Additionally,\nwe used two large vision datasets, namely ImageNet [64] and ISIC 2019 [65], along with several foundation\nmodels, including Mobile-CLIP [66], DINOv2 [67] and WhyLesionCLIP [68]. Further details about the\nexperimental setting can be found in Supplementary Note B. Additional analyses are reported in Supplementary\nNotes C to G."}, {"title": "4.1 Understanding the Inner Knowledge of AI Models", "content": "In the following, SEMANTICLENS is used to systematically analyse the knowledge encoded by neural\nnetwork components of ResNet50v2 [69] trained on the ImageNet classification task [64]. The individual\ncomponents of the model are embedded as vectors & into the multimodal and semantically organized space of\nthe Mobile-CLIP foundation model [66], as illustrated in Fig. 1 and described in Section 3."}, {"title": "4.1.1 Search: Finding the Needle in the Haystack", "content": "The first capability of SEMANTICLENS that we demonstrate is its search capability, allowing one to quickly\nbrowse through all neurons of the ResNet50v2 model and identify concepts that a user is interested in, such as\npotential biases (e.g., gender or racial), data artefacts (e.g., watermarks) or specific knowledge. The search\nis based on (cosine) similarity comparison between a probing vector probe, representing the concept we are\nlooking for (e.g., the concept person), and the set of embedded neurons (i.e., v's) of the ResNet model. The\nshared vision-text embedding space of Mobile-CLIP allows us to query concepts described through images\n(image of a person) as well as concepts described by text (textual input \"person\"). More details about the\ncreation of the probing vectors and the retrieval process can be found in Section 3.\nAs illustrated in Fig. 2a, neurons of the ResNet50v2 model can be identified that encode for person-related\nconcepts. Two embedded neurons, which are most similar to the probing vector represent different, non-\nobvious and potentially discriminative aspects of a person, such as \u201chijab\" (neuron #1216) and \"dark skin\"\n(neuron #1454). It is in principle a valid strategy to represent different object subgroups sharing certain visual\nfeatures by specialized neurons. However, if these \u201csensitive attribute\"-encoding neurons are used for other\npurposes, e.g., the \u201cdark skin\"-person neuron is used for classification of \"steel drum\" (see Fig. 3b), then this\nmay hint at potential fairness issues.\nWe also query the model for the concept watermark. The retrieved neurons encode watermarks and other text\nsuperimposed on an image. Such data artefacts may become part of the model's prediction strategy, known\nas shortcut learning [12, 70] or Clever Hans phenomenon [15], and massively undermine its trustworthiness\n(i.e., the model predicts right but for the wrong reason [71]). While previous works have unmasked such\nwatermark-encoding neurons more or less by chance [15, 72], SEMANTICLENS allows one to intentionally\nquery the model for the presence of such neurons.\nIn addition to searching for bias- or artefact-related neurons, we can also query the model for specific\nknowledge, e.g., the concept bioluminescence. The results show that this concept has been learned by\nthe ResNet50v2 model. Such specific knowledge queries can help ensure that the model has learned all the\nrelevant concepts needed to solve a task, as demonstrated in the ABCDE-rule for melanoma detection in\nSection 4.2. Notably, SEMANTICLENS not only allows to query the model for specific concepts, but also to\nfurther identify the output classes for which concepts are used and the respective (training) data, as later shown\nin Fig. 2d. Additional examples, comparisons between models, and details are provided in Supplementary\nNote C."}, {"title": "4.1.2 Describe: What Knowledge Exists and How Is It Used?", "content": "Another feature of SEMANTICLENS is its ability to describe and systematically analyse what knowledge\nthe model has learned and how it is used. Fig. 2b provides an overview of the ResNet50v2 model's internal\nknowledge (penultimate layer components) as a UMAP projection of the semantic embeddings 0. Here,\ne.g., searching for animal results in aligned embeddings on the left (indicated by red colour), whereas\ntransport-related embeddings are located in the centre (blue coloured). Even more insights can be gained\nwhen systematically searching and annotating semantic embeddings, as described in the following.\nLabelling and Categorizing Knowledge To structure the learned knowledge systematically, we assign a\ntext-form concept label (from a user-defined set) to a neuron embedding if its alignment exceeds the alignment\nwith a baseline which is an empty text label. The labelled embeddings can then be grouped according to their\nannotation, e.g., all embeddings matching dog are grouped together, which reduces complexity, especially if\nmany neurons with similar semantic embeddings exist. In fact, the ResNet results in over a hundred neurons\nrelated to dog, as illustrated in Fig. 2b, where the overall top-aligned label from the expected set for clusters\nof semantic embeddings & are provided. Further details (including labels) and examples are provided in\nSupplementary Notes D.1 and D.2, respectively."}, {"title": "4.1.3 Compare: Identify Common and Unique Knowledge", "content": "So far, we have investigated a single model in semantic space. However, the semantic space serves as a unified\nspace, where multiple models of different architectures, different layers or model parts can be embedded and\ncompared. As such, the influence on learned concepts when changing the network architecture or training\nhyperparameters, such as the training duration, can be studied.\nIn Supplementary Note E two ResNet50 models trained on ImageNet, where one (ResNet50v2) is trained\nmore extensively and results in higher test accuracy, are compared using SEMANTICLENS. As illustrated\nin Supplementary Fig. E.1, both models share common knowledge, e.g., bird-related concepts. However,\nwhereas the better trained ResNet50v2 has learned more specific concepts, e.g., specific fur textures of dogs,\nthe other has learned more abstract concepts that are shared throughout classes. For the dog breed \u201cKomondor\u201d\nwhich has a white mop-like coat, for example, the ResNet50 has learned a mop-like concept that is used to\ndetect \"Komondor\u201d as well as \u201cmop\", whereas the ResNet50v2 learned a class-specific concept. This is in line\nwith works that study generalization of neural networks for long training regimes, observing that latent model\ncomponents become more structured and class-specific [74]. We further provide quantitative comparisons\nvia network dissection in Supplementary Note D.3. Alternatively, SEMANTICLENS allows to compare\nmodels also quantitatively without access to concept-labels by evaluating the similarity between the models'\nknowledge. In Supplementary Note E, we discuss the alignment of various pre-trained neural networks across\nlayers and architectures."}, {"title": "4.2 Audit Alignment: Do Models Reason as Expected?", "content": "The analyses introduced in Section 4.1 enable the quantification of a model's alignment with human expecta-\ntions. Specifically, they allow assessment of a model's reliance on valid, spurious, or unexpected concepts.\nThe steps of an alignment audit, outlined in Fig. 3a, include 1 defining concepts, 2 evaluating concept\nalignment, and 3 testing model behaviour.\n1 Defining a set of expected concepts: First, a set of valid and spurious concepts is defined, utilized to\ncompare against the concepts actually employed by the model. For illustration, we revisit the Ox example where\nvalid concepts include curved horns, wide muzzle and large muscular body, as shown in Fig. 3a (left).\nOn the other hand, we are also aware of spurious correlations, such as palm tree, Indian person and\nwatermark. Notably, all of these concepts can be defined within the modality of the model's data domain\n(i.e., via example images), or, as demonstrated here, simply via text-prompts when utilizing a multimodal\nfoundation model such as CLIP for concept encoding.\n2 Evaluating alignment to valid and spurious concepts: The alignment of the model's knowledge with\nuser-defined spurious or valid concepts is visualized in the scatter plot in Fig. 3a (middle) for \"Ox\" detection."}, {"title": "4.3 Towards Robust and Safe Medical Models", "content": "One of the most popular medical use cases for AI is melanoma detection in dermoscopic images, as shown\nin Fig. 4a. In the following, we demonstrate how to debug a VGG-16 model with SEMANTICLENS that\nis trained to discern melanoma from other irregular or benign (referred to as \"other\") samples in a public\nbenchmark dataset [65, 76, 77]."}, {"title": "4.3.1 ABCDE-Rule for Melanoma Detection", "content": "Dermatologists have created guidelines for visual melanoma detection, such as the ABCDE-rule, short for\nAsymmetry, Border, Colour, Diameter and Evolving [78]. We will use SEMANTICLENS to check whether\nthe model has learned concepts regarding the ABCDE-rule, such as asymmetric lesion (A), ragged\nborder (B), blue-white veil (C), large lesion (D), and crusty surface (E). In addition, we also\ndefine concepts for benign and other skin diseases as well as several spurious concepts that have been reported"}, {"title": "4.3.2 Finding Bugs in Medical Models", "content": "To embed the VGG's components into a semantic space, we leverage a recently introduced CLIP model trained\non skin lesion data [68]. As shown in Fig. 4b, the semantic embeddings are structured, with concepts aligning\nto irregular in the top (red colour), melanoma in the bottom left (blue colour), and regular in the bottom\nright (green colour). Here, we can identify several valid concepts such as blue-white veil and irregular\nstreaks for detecting melanoma, and regular border for benign samples. On the other hand, spurious\nmodel components are also revealed, such as neuron #403 encoding for measurement scale bar, #508 for\nblue coloured band-aid, and #272 for red skin (visually red-coloured skin).\nTo quantify how concepts are used by the model, we compute their highest importance for predicting the", "other": "lass using CRP on the test set, as shown in Fig. 4c. Alarmingly, we find the previously\nfound spurious concepts to be highly relevant: red skin and blue-coloured band-aid are strongly used\nfor"}, {"other": "whereas measurement scale bar is slightly stronger used for"}, {"title": "4.3.3 Model Correction and Evaluation", "content": "In application, the background features of red-coloured skin, plasters and rulers should not influence a detection.\nQSEMANTICLENS helps identifying model components and data associated with spurious concepts. To debug\nthe model [81], we apply two approaches, namely pruning without retraining and retraining on augmented data.\nFor pruning, we label corresponding neurons, resulting in overall 40 out of 512 neurons in the penultimate\nlayer that are pruned. On the other hand, we remove data samples that incorporate the artefacts, identified\nthrough studying the highly activating samples of our labelled components. In order to become insensitive\ntowards the artefacts, we randomly augment data samples during training by overlaying hand-crafted artefacts,\nas illustrated in Fig. 4d (left).\nThe results in Fig. 4d (right) show that both strategies, pruning and retraining, lead to increased accuracy\non a clean test set (without artefact samples), especially for melanoma (from 71.4% to 72.8%). We further"}, {"title": "4.4 Evaluating Human-Interpretability of Model Components", "content": "Deciphering the meaning of concept examples & can be particularly challenging, especially when neurons\nare polysemantic and encode for multiple concepts, as observed in Section 4.2. We introduce a set of easily\ncomputable measures that assess how \u201cclear\u201d, \u201csimilar\u201d and \u201cpolysemantic\u201d concepts are perceived by humans,\nas inferred from their concept examples E. Additionally, we introduce a measure to quantify the \u201credundancies\"\npresent within a set of concepts. All measures are based on evaluating similarities of concept examples & in\nsemantic space S, with mathematical definitions given in Section 3.5.\n4.4.1 Alignment of Interpretability Measures with Human Perception\nAiming to assess human-interpretability, we first evaluate the alignment between human judgments and our\nproposed measures (similarity, clarity and polysemanticity) through user studies. Specifically, we recruited\""}, {"title": "4.4.2 Rating and Improving Interpretability", "content": "The difficulty of understanding the role of components in standard pre-trained models can vary strongly, as,\ne.g., previously observed in Sections 4.1 and 4.2. This is further confirmed by evaluating various popular\nneural networks trained on ImageNet using our newly introduced measures for penultimate layer neurons,\ndetailed in Fig. 5b. Larger and broader models, such as ResNet101, show higher degrees of redundancy,\nwhich can be expected as more neurons per layer allow more redundancies to form, e.g., in order to increase\nrobustness. For narrow models such as the ResNet18, on the other hand, the effective neural basis might be too\nsmall, leading to superimposed signals and a higher polysemanticity (neurons are more likely to fulfil multiple\ntasks) [83].\nThe convolution-based ResNet architecture shows higher concept clarity compared to the more recent\ntransformer-based ViT. Whereas the ResNet consists of ReLU non-linearities that allow to associate a\nhigh neuronal activation with a specific active input pattern, ViTs often refrain from ReLUs, which enables to\nsuperimpose signals (concepts) throughout model components, ultimately leading to high polysemanticity\nand low interpretability [84]. Interestingly, recent efforts are being made in Large Language Model (LLM)\ninterpretability to extend the transformer architecture post-hoc with SAEs based on ReLUs to again receive a\nmore interpretable neuronal basis [53]. Moreover, our analysis shows that more extensively trained models\nhave clearer and overall more interpretable components, as is the case for the ResNet50v2 compared to the\nResNet50. This observation raises the question on how we can influence training parameters to gain higher\nlatent interpretability, which we inspect in the following:\nDrop-out: Drop-out regularization is effective for reducing overfitting, preventing high reliance on few features\nby randomly setting a fraction of component activations to zero during training. Our results shown in Fig. 5c\nindicate that VGG-13 model components become more redundant, but also clearer when drop-out is applied\nduring training on a subset of ImageNet (standard error given by gray error bars for eight runs each). It can be\nexpected that more redundancies form, as redundancies make predictions more robust when components are\nrandomly pruned. On the other hand, neurons are measured to become more class-specific and thus clearer.\nNotably, architectures might react differently in terms of interpretability, as indicated by the ResNet-34 and\nResNet-50 which are not strongly affected by drop-out. Qualitative examples of concepts, detailed training\nprocedures and results are provided in Supplementary Note G.2.\nSparsity regularization: Secondly, we apply L1 sparsity regularization during training on the neuron activations,\nas is, e.g., common for SAEs. Our experiments indicate that sparsity regularization improves interpretability\nin all measured aspects, resulting in more specific, less polysemantic and semantically redundant neurons. We\nfurther investigate the effect of task complexity, number of training epochs and data augmentation on latent\ninterpretability in Supplementary Note G.2."}, {"title": "5 Discussion", "content": "With SEMANTICLENS, we propose to transfer components of large machine learning models into an\nunderstandable semantic representation that allows one to understand and evaluate their inner workings in a\nholistic manner. This transfer is made possible through recent foundation models that serve as domain experts,\ntaking the human out of interpretation loops, that otherwise would be cognitively infeasible to process due to\nthe sheer amount of components of modern deep neural networks. Especially useful are multimodal foundation\nmodels that allow to search, annotate and label network components via textual descriptions. Foundation\nmodels improve constantly, becoming more efficient and applicable in scarcer data domains such as medical\ndata, or other data modalities including audio and video [85, 86].\nThese new capabilities offered by SEMANTICLENS allow to comprehensively audit the internal components\nof AI models. A multitude of spurious behaviours of popular pre-trained models are hereby revealed, stressing\nthe need to understand every part of a model in order to ensure fairness, safety and robustness in application."}, {"title": "Supplementary Materials", "content": "This article has supplementary files providing additional details and information, descriptions, experiments\nand figures. Supplementary Note A offers a detailed survey of related work important to our contribution,\ncontrasting several related explainability techniques to our proposed technical contribution regarding different\ncriteria. Supplementary Note B includes a description of the datasets and models used in our experiments.\nSupplementary Notes C, D and E include additional experiments and explanations for the search, describe\nand compare functionalities of \\u25c8 SEMANTICLENS. Subsequently, Supplementary Note F provide additional\ndetails for auditing and debugging models with SEMANTICLENS. In Supplementary Note G, details\non the user study of Section 4.4 and more experiments for optimizing latent interpretability are provided.\nSupplementary Note H describes our technical contribution, the SEMANTICLENS, in increased detail\nand provides additional background. The proposed interpretability measures are detailed in Supplementary\nNote H.3. In Supplementary Note H.4 we summarize the computational steps involved in answering the\nquestions presented in Tab. 1. Current challenges and an outlook to future work are discussed in Supplementary\nNote I."}, {"title": "A Extended Related Work", "content": "QSEMANTICLENS is a holistic framework that enables a systematic concept-level understanding of large\nAI models. Its core elements rely on previous research advances related to concept visualization, labelling,\nattribution, comparison, discovery, audits, and human-interpretability measures, as detailed in the following.\nIn Supplementary Tab. A.1, we compare SEMANTICLENS with other popular XAI frameworks.\nConcept Examples (Feature Visualization) Most feature visualization techniques rely on maximizing\nactivation values of single neurons or a linear combination thereof [25, 26, 27, 14, 16, 28], where in its simplest\nform, input images are sought that produce the highest activation value of a specific unit. In this work, the\nset of images is referred to as \"concept examples\". Concept examples can be generated synthetically using\ngradient ascent, or alternatively found from a sample dataset by collecting neuron activations during predictions.\nRegarding synthetic examples, preventing the emergence of adversarial patterns became a main research area.\nSeveral priors were proposed to guide optimization into more realistic looking images [29, 26, 30, 31, 32].\nRecently, diffusion models are being applied to also help in generating more realistic concept examples [33].\nAlternatively, natural concept examples can be collected on the training or test data, where it is favourable\nto collect patches of the input data [16, 15, 28, 14], as whole inputs can incorporate a lot of distracting\nbackground features. We follow the CRP approach and crop full data samples to the actual relevant part using\nneuron-specific attributions [15]. Other approaches facilitate upsampled spatial activation maps [14], that are\nonly available for convolutional layers, or transformers (through spatial token information).\nEncoding Concepts of Neurons: Activation Pattern or Feature Space There are two approaches in\nliterature to encode the concept of neurons: (1) via activation patterns [14, 46, 35] on data with concept\nannotations (e.g., binary labels or segmentation mask) or (2) by embedding concept examples into another\nfeature space [37, 48, 18, 38]. Activation patterns are a very direct measure, but often only correspond to a\nsingular (pooled) activation score per data point. Usually, data points incorporate multiple features, which can\nlead to wrong conclusions due to unexpected correlations when working with singular activation scores. For\nexample, two neurons that encode for nose and eyes will activate very similarly for data with human faces,\nbut encode for different concepts. It is thus important to have a qualitative and meaningful set of concept data.\nAlternatively, concept examples (cropped to the relevant part, see Section 3) aim to communicate the semantic\nrole of neurons more directly. Then, in order to encode a concept, the concept examples are embedded in the\nfeature space of a model: either the same model [48] or a foundation model [37, 18]. Notably, generating\nthe concept examples and encodings is algorithmically and computationally more involved compared to\nactivation pattern computation. Whereas using the same model for encoding is convenient as it does not\nrequire a foundation model (that might need to be trained first), the latent space of the investigated might\nnot be as semantically structured. The work of [45] shows that self-supervised foundation models have a\nmore semantically structured latent space than models trained on a classification task. Especially multimodal\nfoundation models are interesting as they allow to also interact and describe the embedding space more flexible.\nHowever, it is to note that describing concepts through concept examples is assuming that the concept is\nprecisely and well-defined via these examples, which might not always be the case [38]."}, {"title": "B Experimental Settings", "content": "The following section outlines the experimental settings used throughout this work.\nB.1 Architectures and Models\nWe evaluate multiple pre-trained models from the torchvision [96", "69": "architectures: ResNet18, ResNet32, ResNet50, ResNet50v2,\nResNet101, ResNet101v2 provided by torchvision [96"}]}