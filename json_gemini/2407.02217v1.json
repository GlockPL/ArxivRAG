{"title": "Physics-Informed Model and Hybrid Planning for Efficient Dyna-Style Reinforcement Learning", "authors": ["Zakariae El Asri", "Olivier Sigaud", "Nicolas Thome"], "abstract": "Applying reinforcement learning (RL) to real-world applications requires addressing a trade-off between asymptotic performance, sample efficiency, and inference time. In this work, we demonstrate how to address this triple challenge by leveraging partial physical knowledge about the system dynamics. Our approach involves learning a physics-informed model to boost sample efficiency and generating imaginary trajectories from this model to learn a model-free policy and Q-function. Furthermore, we propose a hybrid planning strategy, combining the learned policy and Q-function with the learned model to enhance time efficiency in planning. Through practical demonstrations, we illustrate that our method improves the compromise between sample efficiency, time efficiency, and performance over state-of-the-art methods. Code is available at https://github.com/elasriz/PHIHP/", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) has proven successful in sequential decision-making tasks across diverse artificial domains, ranging from games to robotics (Mnih et al., 2015; Lillicrap et al., 2016; Fujimoto et al., 2018; Haarnoja et al., 2018). However, this success has not yet been evident in real-world applications, where RL is facing many challenges (Dulac-Arnold et al., 2019), especially in terms of sample efficiency and inference time needed to reach a satisfactory performance. A limitation of existing research is that most works address these three challenges - sample efficiency, time efficiency, and performance \u2013 individually, whereas we posit that addressing them simultaneously can benefit from useful synergies between the leveraged mechanisms.\nConcretely, on one side Model-Free Reinforcement Learning (MFRL) techniques excel at learning a wide range of control tasks (Lillicrap et al., 2016; Fujimoto et al., 2018), but at a high sample cost. On the other side, Model-Based Reinforcement Learning (MBRL) drastically reduces the need for samples by acquiring a representation of the agent-environment interaction (Deisenroth & Rasmussen, 2011; Chua et al., 2018), but requires heavy planning strategies to reach competitive performance, at the cost of inference time.\nA recent line of works focuses on combining MBRL and MFRL to benefit from the best of both worlds (Ha & Schmidhuber, 2018; Hafner et al., 2019a; Clavera et al., 2020). Particularly, Byravan et al. (2021); Wang & Ba (2019); Hansen et al. (2022) combine a learned model and a learned policy in planning, this combination helps improve the asymptotic performance but requires more samples, due to the sample cost of learning a good policy.\nThis paper introduces PhIHP, a Physics-Informed model and Hybrid Planning method in RL.PhIHP improves the compromise between the three main challenges outlined above - sample efficiency, time efficiency, and performance -, as illustrated in Figure 1. Compared to state-of-the-art MFRL TD3 (Fujimoto et al., 2018) and hybrid TD-MPC (Hansen et al., 2022), we show that PhIHP provides a much better sample efficiency, reaches higher asymptotic performance, and is much faster than TD-MPC at inference."}, {"title": "2 Related work", "content": "Our work is at the intersection of Model-based RL, physics-informed methods, and hybrid controllers.\nModel-based RL: Since DYNA architectures (Sutton, 1991), model-based RL algorithms are known to be generally more sample-efficient than model-free methods. Planning with inaccurate or biased models can lead to bad performance due to compounding errors, so many works have focused on developing different methods to learn accurate models: PILCO (Deisenroth & Rasmussen, 2011), SVG (Heess et al., 2015), PETS (Chua et al., 2018), PlaNet (Hafner et al., 2019b) and Dreamer (Hafner et al., 2019a; 2020; 2023). Despite the high asymptotic performance achieved by model-based planning, these methods require a large inference time. By contrast, by learning a policy used to sample better actions, we can drastically reduce the inference time.\nPhysics-informed methods: Recently, a new line of work attempted to leverage the physical knowledge available from the laws of physics governing dynamics, to speed up learning and enhance sample efficiency in MBRL. (Kloss et al., 2017; Ajay et al., 2018; Jeong et al., 2019; Johannink et al., 2019; Zeng et al., 2020; Cranmer et al., 2020; Yin et al., 2021; Yildiz et al., 2021; El Asri et al., 2022; Ramesh & Ravindran, 2023). However, these methods use the learned model in model predictive control (MPC) and suffer from a large inference time. In this work, we efficiently learn an accurate model by jointly correcting the parameters of a physical prior knowledge and learning a data-driven residual using Neural ODEs.\nHybrid controllers: An interesting line of work consists in combining MBRL and MFRL to benefit from the best of both worlds. This combination can be done by using a learned model to generate imaginary samples and augment the training data for a model-free agent (Buckman et al., 2018; Clavera et al., 2020; Morgan et al., 2021; Young et al., 2022). However, the improvement in terms of sample efficiency is limited, since the agent remains trained on real data. Recent hybrid methods enhance the planning process by using a policy (Byravan et al., 2021; Wang & Ba, 2019), or a Q-function (Bhardwaj et al., 2020) with a learned model. More related to our work, TD-MPC (Hansen et al., 2022) combines the last two methods, using a learned policy and a Q-function with a learned data-driven model to evaluate trajectories. TD-MPC jointly trains all components on real samples"}, {"title": "3 Background", "content": "Our work builds on reinforcement learning and the cross-entropy method.\nReinforcement learning: In RL, the problem of solving a given task is formulated as a Markov Decision Process (MDP), that is a tuple $(S, A,T,R, \\gamma,p(s_0))$ where $S$ is the state space, $A$ the action space, $T =: S \\times A \\rightarrow S$ the transition function, $R : S \\times A \\rightarrow R$ the reward function, $\\gamma \\in [0, 1]$ is a discount factor and $p_0$ is the initial state distribution. The objective in RL is to maximize the expected return $\\sum_{t=t_0}^{H} \\gamma^{t-t_0}r_t$ at each timestep $t_0$. In model-free RL, an agent learns a policy $\\pi_{\\theta} : S \\rightarrow A$ that maximizes this expected return. In contrast, in model-based RL, the agent learns a model that represents the transition function $T$, then uses this learned model $T_{\\theta}$ to predict the next state $\\hat{s}_{t+1} = T_{\\theta}(s_t, a_t)$. The agent maximizes the expected return by optimizing a sequence of actions $A = \\{a_{t_0}, \\ldots, a_{t_0+H}\\}$ over a horizon $H$:\n$A^* = \\arg \\max_{A \\in A^{H}} \\sum_{t=t_0}^{H} \\gamma^{t-t_0} R(s_t, a_t), \\text{ subject to } s_{t+1} = T_{\\theta}(s_t, a_t).$ (1)\nFurthermore, using an inaccurate model can degrade solutions due to compounding errors. So, one often solves this optimization problem at each time step, only executes the first action from the sequence, and plans again at the next time step with updated state information. This is known as model predictive control (MPC).\nCross Entropy Method (CEM): Since the dynamics and the reward functions are generally nonlinear, it is difficult to analytically calculate the exact minimum of (1). In this work, we use the derivative-free Cross-Entropy Method (de Boer et al., 2005) to resolve this optimization problem. In CEM, the agent looks for the best sequence of actions over a finite horizon $H$. It first generates $N$ candidate sequences of actions from a normal distribution $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. Then, it evaluates the resulting trajectories using the learned dynamics model using a reward model and determines the $K$ elite sequences of actions ($K < N$), that is the sequences that lead to the highest return. Finally, the normal distribution parameters $\\sigma$ and $\\mu$ are updated to fit the elites. This process is repeated for a fixed number of iterations. The optimal action sequence is calculated as the mean of the $K$ elites after the last iteration. We call CEM budget the size of the population times the number of iterations, this budget being the main factor of inference time in methods that use the CEM."}, {"title": "4 Physics-Informed model for Hybrid Planning", "content": "In this section, we describe PhIHP, our proposed Physics-Informed model for Hybrid Planning. PhIHP first learns a physics-informed residual dynamics model (Sec. 4.1), then learns a MFRL agent through imagination (Sec. 4.2), and uses a hybrid planning strategy at inference (Sec. 4.3). PhIHP follows recent hybrid MBRL/MFRL approaches, e.g. TD-MPC Hansen et al. (2022), but the physics-informed model brings important improvements at each stage of the process. It brings a more accurate model, which improves predictive performance and robustness with respect to training data distribution shifts. Crucially, it benefits from the continuous neuralODE method (Sec. 4.1) to accurately predict trajectories, enabling to learn a powerful model-free agent in imagination (Sec. 4.2). Finally, it enables to design a hybrid policy learning (Sec. 4.3) optimizing the performance vs time efficiency trade-off."}, {"title": "4.1 Learning a physics-informed dynamics model", "content": "Model-based RL methods aim to learn the transition function $T$ of the world i.e. a mapping from $(s_t, a_t)$ to $s_{t+1}$. However, learning $T$ is challenging when $s_t$ and $s_{t+1}$ are similar and actions have a low impact on the output, in particular when the time interval between steps decreases. We address this issue by learning a dynamics function $T_{\\theta}$ to predict the state change $\\Delta s_t$ over the time step duration $\\Delta t$. The next state $s_{t+1}$ can be subsequently determined through integration with an Ordinary Differential Equation (ODE) solver. Thus, we describe the dynamics as a system following an ODE of the form:\n$\\frac{ds_t}{dt}|_{t=t_0} = T_{\\theta}(s_{t_0}, a_{t_0}), \\text{ and } s_{t+1} \\sim ODESolve(s_t, a_t, T_{\\theta}, t, t + \\Delta t),$ (2)\nwhere $s_t$ and $a_t$ are the state and action vector for a given time $t$. We assume the common situation where a partial knowledge of the dynamics is available, generally from the underlying physical laws. The dynamics $T_{\\theta}$ can thus be written as $T_{\\theta} = F^P + F^R_{\\theta}$, where $F^P$ is the known analytic approximation of the dynamics and $F^R_{\\theta}$ is a residual part used to reduce the gap between the model prediction and the real world by learning the complex phenomena that cannot be captured analytically. The physical model $F^P$ is described by an ODE and the residual part $F^R_{\\theta}$ as a neural network with respective parameters $\\theta_P$ and $\\theta_R$. We learn the dynamics model in a supervised manner by optimizing the following objective:\n$\\mathcal{L}_{pred}(\\theta) = \\frac{1}{\\mathcal{D}_{re}} \\sum_{(s_t, a_t, s_{t+1}) \\in \\mathcal{D}_{re}} ||s_{t+1} - \\hat{s}_{t+1}||^2_2 \\text{ subject to } \\frac{d\\hat{s}_t}{dt}|_{t=t'} = (F^P + F^R_{\\theta})(s_{t'}, a_{t'}),$ (3)\non a dataset $\\mathcal{D}_{re}$ of real transitions $(s_t, a_t, s_{t+1})$. As the decomposition $T_{\\theta} = F^P + F^R_{\\theta}$ is not unique, we apply an $L_2$ constraint over the residual part with a coefficient $\\lambda$ to enforce the model $T_{\\theta}$ to mostly rely on the physical prior. The learning objective becomes $\\mathcal{L}_{\\lambda}(\\theta) = \\mathcal{L}_{pred}(\\theta) + \\frac{\\lambda}{2} ||F^R_{\\theta}||^2$. The coefficient $\\lambda$ is initialized with a value $\\lambda_0$ and updated at each epoch with $\\lambda_{j+1} = \\lambda_j + T_{ph} \\cdot \\mathcal{L}_{pred}(\\theta)$, where $\\lambda_0$ and $T_{ph}$ are fixed hyperparameters."}, {"title": "4.2 Learning a policy and Q-function through imagination", "content": "Simply planning with a learned model and CEM is time expensive. MFRL methods are generally more time-efficient during inference time than planning methods, since they use policies that directly map a state to an action. However, learning complex policies requires a large amount of training data which impacts sample efficiency. To maintain sample efficiency, a policy can be learned from synthetic data generated by a model. However, an imperfect model may propagate the bias to the learned policy. In this work, we benefit from the reduced bias in the physics-informed model"}, {"title": "4.3 Hybrid planning with learned model and policy", "content": "PhIHP leverages a hybrid planning method that combines a physics-informed model with a learned policy and Q-function. This combination helps overcome the drawbacks associated with each method when used individually. While using a sub-optimal policy in control tasks significantly affects the asymptotic performance, planning with a learned model has a high computational cost: i) the planning horizon must be long enough to capture future rewards and ii) the CEM budget must be sufficiently large to converge.\nWe use the learned policy in PhIHP to guide planning. In practice, a CEM-based planner first samples $N$ informative candidates from the learned policy $\\pi_{\\theta}(s_t)$ and complements them with $N_{rand}$ exploratory candidates sampled from a uniform distribution $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$. These informative candidates help reduce the population size and accelerate convergence. The planner estimates the resulting trajectories using the learned model and evaluates each trajectory using the immediate reward function up to the MPC horizon and the Q-value beyond that horizon.\nBy using the Q-value, we can evaluate the trajectories over a considerably reduced planning horizon $H$ and we add the Q-value of the last state to cover the long-term reward. Hence, the optimization problem is written as follows:\n$A^* = \\arg \\max_{A \\in A^{H}} (\\sum_{t=t_0}^{H} \\gamma^{t-t_0} R(s_t, a_t) + \\alpha \\cdot \\gamma^{H-t_0}Q(s_H)), \\text{ subject to } s_{t+1} = T_{\\theta}(s_t, a_t),$ (4)\nwhere the discounted sum term represents a local solution to the optimization problem, while the Q-value term encodes the long-term reward and a balances the immediate reward over the planning horizon and the Q-value."}, {"title": "5 Experiments", "content": "We first compare PhIHP to baselines in terms of performance, sample efficiency, and time efficiency. Then we perform ablations and highlight the generalization capability brought by the physics prior. The robustness of PhIHP to hyper-parameter settings is deferred to Appendix E."}, {"title": "5.1 Experimental setup", "content": "Environments: We evaluate our method on 6 ODE-governed environments from the gymnasium classic control suite. These include the continuous versions of 3 basic environments: Pendulum, Cartpole, and Acrobot. Additionally, we consider their swing-up variants, where the initial state is \"hanging down\" and the goal is to swing up and balance the pole at the upright position, similarly to Yildiz et al. (2021). We opted for this benchmark for its challenging characteristics, including tasks with sparse rewards and early termination.\nHowever, to move closer to methods applicable in a real-world situation, we added to the original environments from the gymnasium suite a friction term which is not present in the analytical model of these environments. Thus, the dynamic of each system is governed by an ODE that can be represented as the combination of two terms: a friction-less component $F_r$ and a friction term $F_f$. Please refer to Appendix B for additional details.\nEvaluation metrics. In all experiments, we use three main metrics to compare methods:\n\u2022 Asymptotic performance: we report the episodic cumulated reward on each environment.\n\u2022 Sample efficiency: we define the sample efficiency of a method as the minimal amount of samples required to achieve 90% of its maximum performance.\n\u2022 Inference time: we report the wall-clock time taken by the agent to select an action at one timestep."}, {"title": "5.2 Comparison to state of the art:", "content": "We compare PhIHP to the following state-of-the-art methods:\n\u2022 TD-MPC (Hansen et al., 2022), a state-of-the-art hybrid MBRL/MFRL algorithm shown to outperform strong state-based algorithms whether model-based e.g. LOOP (Sikchi et al., 2022) and model-free e.g. SAC (Haarnoja et al., 2018) on diverse continuous control tasks.\n\u2022 TD3 (Fujimoto et al., 2018), a state-of-the-art model-free algorithm. In addition to its popularity and strong performance on continuous control tasks, TD3 is a backbone algorithm for our method to learn the policy and Q-function. We used the same hyperparameters as in PhIHP.\n\u2022 CEM-oracle: a CEM-based controller with the ground-truth model."}, {"title": "5.3 Ablation study", "content": "In this section, we study the impact of each PhIHP component to illustrate the benefits of using an analytical physics model, imagination learning, and combining CEM with a model-free policy and Q-function for planning. To illustrate this, we compare PhIHP to several methods:\n\u2022 TD-MPC*: our method without physical prior and without imagination. It is similar to TD-MPC since the model is data-driven and it is learned with the policy from real trajectories. But learning the model and the policy are separated.\n\u2022 Ph-TD-MPC*: our method without learning in imagination, thus a physics-informed TD-MPC*.\n\u2022 dd-CEM: our method without physical prior nor policy component, thus a CEM with a data-driven model learned from real trajectories.\n\u2022 Ph-CEM: our method without the policy component, thus a simple CEM with a physics-informed model learned from real trajectories."}, {"title": "5.4 Generalization benefits of the physics prior", "content": "In this section, we highlight the key role of incorporating physical knowledge into PhIHP in finding the better compromise between asymptotic performance, sample efficiency, and time efficiency illustrated in Figure 5. Actually, learning a policy and Q-function through imagination leads to"}, {"title": "6 Conclusion", "content": "We have introduced PhiHP, a novel approach that leverages physics knowledge of system dynamics to address the trade-off between asymptotic performance, sample efficiency, and time efficiency in RL. PhIHP enhances the sample efficiency by learning a physics-informed model that serves to train a model-free agent through imagination and uses a hybrid planning strategy to improve the inference time and the asymptotic performance. In the future, we envision to apply PhIHP to more challenging control tasks where there is a larger discrepancy between the known equations and the real dynamics of the system."}, {"title": "A Comparison to existing methods", "content": "In this section, we present a conceptual comparison of PhIHP and existing RL methods. Figure 7 illustrates the general scheme of existing RL methods and the possible connections between learning and planning. We highlight in Figure 8 the origin of the well-known drawbacks in RL: i) learning a policy on real data (arrow 1) impacts the sample efficiency, ii) learning a policy from a data-driven learned model (arrow 3) impacts the asymptotic performance due to the bias in the learned model, iii) model-based planning (arrow 4) impacts the inference time."}, {"title": "B Environments", "content": "In this section, we give a comprehensive description of the environments employed in our work. Across all environments, observations are continuous within [-Sbox, Sbox] and actions are continuous and restricted to a [-amax, Amax] range. An overview of all tasks is depicted in Figure 9 and specific parameters are outlined in Table 2.\nPendulum: A single-linked pendulum is fixed on one end, with an actuator on the joint. The pendulum starts at a random position and the goal is to swing it up and balance it at the upright position. Let $\\theta$ be the joint angle at time $t$ and $\\dot{\\theta}$ its velocity, the observation at time $t$ is $(\\theta, \\dot{\\theta})$.\nPendulum-Swingup: the version of Pendulum where it is started at the \"hanging down\" position.\nCartpole: A pole is attached by an unactuated joint to a cart, which moves along a horizontal track. The pole is started upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\nCartpole-Swingup: the version of Cartpole where the pole is started at the \"hanging down\" position.\nAcrobot: A pendulum with two links connected linearly to form a chain, with one end of the chain fixed. Only the joint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the free end of the linear chain above a given height.\nAcrobot-Swingup: For the swingup task, we experiment with the fully actuated version of the Acrobot similarly to (Yildiz et al., 2021; Xie et al., 2016). Initially, both links point downwards at the \"hanging down\" position. The goal is to swing up the Acrobot and balance it in the upright position. Let $\\theta_1$ be the joint angles of the first fixed to a hinge at time $t$ and $\\theta_2$ the relative angle between the two links at time $t$. The observation at time $t$ is $(\\theta_1, \\theta_2, \\dot{\\theta_1}, \\dot{\\theta_2})$."}, {"title": "B.1 Dynamic functions", "content": "In this section, we provide details of the dynamic functions. For each task, the dynamic function consists of a frictionless component and a friction term.\nPendulum and Pendulum Swingup: Let $s_t = (\\theta, \\dot{\\theta})$ be the state and $a_t$ the action at time $t$. The dynamic of the pendulum is described as:\n$F(s_t, a_t) = \\begin{bmatrix} \\dot{\\theta} \\\\ \\ddot{\\theta} \\end{bmatrix} = \\begin{bmatrix} \\dot{\\theta} \\\\ C_g \\cdot sin(\\theta) + C_i a_t + C_{Fr} \\end{bmatrix}$ (5)\nwhere $C_g$ is the gravity norm, $C_i$ is the inertia norm and $C_{Fr}$ is the Friction norm.\nAcrobot and Acrobot Swingup: Let $s_t = (\\theta_1, \\theta_2, \\dot{\\theta_1}, \\dot{\\theta_2})$ be the state and $a_t = (a_1,a_2)$ ($a_1 = 0$ for the Acrobot environment) the action at time $t$. The dynamic of the system is similar to (Yildiz et al., 2021) described as:\n$F(s_t, a_t) = \\begin{bmatrix} \\dot{\\theta_1} \\\\ \\dot{\\theta_2} \\\\ \\ddot{\\theta_1} \\\\ \\ddot{\\theta_2} \\end{bmatrix} = \\begin{bmatrix} \\dot{\\theta_1} \\\\ \\dot{\\theta_2} \\\\ \\frac{(a_0 + 2 \\Sigma_2 + (a_1 + m_2 l_1 l_{c_2} \\dot{\\theta_2}^2 sin(\\theta_2) - \\Sigma_1) d_2)}{d_0} \\\\ \\frac{(a_1 + \\Sigma_1 - m_2 l_1 l_{c_2} \\dot{\\theta_2}^2 sin(\\theta_2))}{d_2} \\end{bmatrix}$ (6)\nwhere:\n$a_0 = a_1 - C_{fr1} \\cdot \\dot{\\theta_1}$ such as $C_{fr1}$ is the friction norm in the first joint ,\n$a_1 = a_2 \u2013 C_{fr2} \\cdot \\dot{\\theta_2}$ such as $C_{fr2}$ is the friction norm in the second joint,\n$m_1$ and $m_2$ the mass of the first and second links,\n$l_1$ and $l_2$ the length of the first and second links,\n$l_{c_1}$ and $l_{c_2}$ the position of the center of mass of the first and second links,\n$I_1$ and $I_2$ the moment of inertia of the first and second links,\nand\n$d_1 = m_1 \\cdot l_{c_1}^2 + m_2 \\cdot (l_1^2 + l_{c_2}^2 + 2 \\cdot l_1 \\cdot l_{c_2} \\cdot cos(\\theta_2)) + I_1 + I_2$\n$d_2 = m_2 \\cdot (l_{c_2}^2 + l_1 \\cdot l_{c_2} \\cdot cos(\\theta_2)) + I_2$\n$\\Sigma_2 = m_2 \\cdot l_{c_2} \\cdot g \\cdot cos(\\theta_1 + \\theta_2 - \\frac{\\pi}{2})$\n$\\Sigma_1 = m_2 \\cdot l_1 \\cdot l_{c_2} \\cdot \\dot{\\theta_2} \\cdot sin(\\theta_2) (\\dot{\\theta_2} - \\frac{2}{91}) + (m_1 \\cdot l_{c_1} + m_2 \\cdot l_1) \\cdot g \\cdot cos(\\theta_1 - \\frac{\\pi}{2}) + \\Sigma_2$."}, {"title": "Cartpole and Cartpole Swingup:", "content": "Let $s_t = (x, \\dot{x}, \\theta, \\dot{\\theta})$ be the state and $a_t$ the action at time $t$. The dynamic of the system is based on (Barto et al., 1983) and described as:\n$F(s_t, a_t) = \\begin{bmatrix} \\dot{x} \\\\ \\ddot{x} \\\\ \\dot{\\theta} \\\\ \\ddot{\\theta} \\end{bmatrix} = \\begin{bmatrix} \\dot{x} \\\\ \\frac{\\Sigma \u2013 m_p l \\dot{\\theta}^2 sin(\\theta)}{M_{total}} \\\\ \\dot{\\theta} \\\\ \\frac{g.sin(\\theta)-(cos(\\theta).\\Sigma) \u2013 F_{rpo}}{1.[m_p.cos(\\theta)^2]} \\end{bmatrix}$ (7)\nwhere:\n$F_{rc}$ is the friction norm in the contact between the cart and the ground,\n$F_{rp}$ is the friction norm in the joint between the cart and the pole,\nl is the length of the pole,\n$M_{tot} = m_c + m_p$ and $m_p, m_c$ are the mass of the pole and the cart respectively,\n$\\Sigma = \\frac{1}{l} (a + m_p \\cdot l \\dot{\\theta}^2 \\cdot sin(\\theta) \u2013 (F_{rc} \\cdot sgn(\\dot{x}))$."}, {"title": "B.2 Reward Functions", "content": "The reward function encodes the desired task. We adopt the original reward functions in the three main environments. For the swingup variants, we choose functions that describe the swingup task: we adopt the same function as Pendulum for Pendulum swingup. For Cartpole swingup, we set a reward function as the negative distance from the goal position $s_{goal} = (x = 0, y = 1)$. For Acrobot swingup, we take the height of the pole as a reward function."}, {"title": "C Implementation details", "content": "In this section, we describe the experimental setup and the implementation details of PhIHP. We first learn a physics-informed residual dynamics model, then learn an MFRL agent through imagination, and use a hybrid planning strategy at inference.\nTo learn the model, we first use a pure exploratory policy during T timesteps to collect the initial samples to fill $D_{re}$, then we perform stochastic gradient descent on the loss function (Eq. 3 in Sec. 4.1) to train $F_{\\theta}$. The learned model $F_{\\theta}$ is used with CEM to perform planning and gather new T samples to add to $D_{re}$. To improve the quality of the model, the algorithm iteratively alternates between training and planning for a fixed number of iterations.\nTo train the model-free component of PhIHP, the training dataset $D_{im}$ is initially filled with $T^{\\prime}$ samples generated from the learned model $F_{\\theta}$ and random actions from a pure exploratory policy, $\\pi_{\\theta}$ and $Q_{\\theta}$ are trained on batches from $D_{im}$ which is continuously filled by samples from the learned model $F_{\\theta}$.\nWe list in Tab. 4 the relevant hyperparameters of PhIHP and baselines. and we report in Tab. 5 the task-specific hyperparameters for PhIHP.\nWe adopted the original implementation and hyperparameters of TD-MPC. However, we needed to adapt it for early termination environments (i.e. Cartpole and Acrobot) to support episodes of variable length, and we found it beneficial for TD-MPC to set the critic learning rate at 1e-4 in these two tasks."}, {"title": "D Comparison to state of the art", "content": "We compare PhIHP to baselines on individual tasks, we present both statistical results and a qualitative analysis."}, {"title": "D.1 Learning curves", "content": "We provide learning curves of PhIHP and baselines on individual tasks. PhIHP outperforms baselines by a large margin in terms of sample efficiency. Figure 10 shows that TD3, even when converging early in Cartpole-swingup, achieves sub-optimal performance and fails to converge within 500k steps in Acrobot-swingup."}, {"title": "D.2 Statistical Comparison: PhIHP vs. Baselines", "content": "To ensure a robust and statistically sound comparison with the results previously reported in Table 1 in Sec. 5.2, we conducted Welch's t-test to statistically compare the performance of PhIHP vs baselines across individual tasks. We set the significance threshold at 0.05, and calculated p-values to determine whether observed differences in performance were statistically significant. Tab. 6 shows that PhIHP is equivalent to all baselines in Pendulum, and it significantly outperforms TD3 on the remaining tasks. Moreover, PhIHP outperforms TD-MPC in sparse-reward early-termination environment tasks (Cartpole and Acrobot), while they demonstrate equivalent performance in Pendulum, Pendulum swingup, and Acrobot swingup."}, {"title": "D.3 Imagination learning for model-free TD3", "content": "We provide learning curves of TD3 through imagination on individual tasks in Figure 11. TD3-im-ph is a component of PhIHP, it is a TD3 agent learned on trajectories from a physics-informed model. It largely outperforms TD3-im-dd, a TD3 learned on trajectories from a data-driven model. we limited the training budget for TD3-re, trained on real trajectories, at 500k real samples in all tasks."}, {"title": "D.4 Qualitative comparison", "content": "In this section, we compare performance metrics on individual classic control tasks. We estimate confidence intervals by using the percentile bootstrap with stratified sampling (Agarwal et al., 2021).\nWe show in Figure 12 a comparison of the median, interquartile median (IQM), mean performance, and optimality gap of PhIHP and baselines. PhIHP matches or outperforms the performance of TD-MPC and TD3 in all tasks except in Cartpole swingup. PhIHP shown to be robust to outliers compared to TD-MPC with shorter confidence intervals.\nMoreover, Figure 13 shows the performance profiles of PhIHP and baselines. PhIHP shows better robustness to outliers."}, {"title": "E Hyperparameter sensitivity analysis", "content": "We investigate the impact of varying controller hyper-parameters on the performance and inference time of PhIHP. We first study the impact of varying planning horizons and receding horizons (from 1 to 8). We note that planning over longer horizons generally leads to better performance, however, the performance slightly drops in Acrobot-swingup for planning horizon $H > 4$ (Figure 14). We explain this by the compounding error effect on complex dynamics. Unsurprisingly, lower receding horizons always improve the performance because the agent benefits from replanning.\nFor the impact of the population size, Figure 14 shows that excluding the policy (policy-population = 0) from planning degrades the performance, and increasing it under 10 does not have a significant impact. Moreover, excluding random actions (random-population = 0) from planning degrades the performance.\nUnsurprisingly, the inference time increases with an increase in both the planning horizon and the population size. Conversely, it decreases when the receding horizon increases."}]}