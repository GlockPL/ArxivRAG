{"title": "Health Misinformation in Social Networks: A Survey of IT Approaches", "authors": ["VASILIKI PAPANIKOU", "PANAGIOTIS PAPADAKOS", "THEODORA KARAMANIDOU", "THANOS G. STAVROPOULOS", "EVAGGELIA PITOURA", "PANAYIOTIS TSAPARAS"], "abstract": "In this paper, we present a comprehensive survey on the pervasive issue of medical misinformation in social networks from\nthe perspective of information technology. The survey aims at providing a systematic review of related research and helping\nresearchers and practitioners navigate through this fast-changing field. Specifically, we first present manual and automatic\napproaches for fact-checking. We then explore fake news detection methods, using content, propagation features, or source\nfeatures, as well as mitigation approaches for countering the spread of misinformation. We also provide a detailed list of\nseveral datasets on health misinformation and of publicly available tools. We conclude the survey with a discussion on the\nopen challenges and future research directions in the battle against health misinformation.", "sections": [{"title": "1 INTRODUCTION", "content": "The spread of misinformation online, most commonly known as fake news, is an important issue that has become\nmore pronounced in the last two decades due to the prevalence of social media. Platforms like Twitter, Reddit,\nand Facebook, have been commonly identified as the main channels for propagating misinformation and have\nbeen criticized for not acting on addressing the conditions that permit the circulation and amplification of false\ninformation [32]. Such misinformation includes false claims and non fact-checked news items, that originate\nfrom sources of questionable credibility [113].\nThe problem of misinformation becomes critical when it pertains to healthcare and health issues, since it\nputs lives and the public health at risk. One of the first cases of widely spread misinformation in the medical\ndomain is the falsehood that the MMR vaccine (Measles, Mumps, Rubella) causes autism [109]. The falsehood\noriginated from a fraudulent article titled \u201cIleal-lymphoid-nodular hyperplasia, non-specific colitis, and pervasive\ndevelopmental disorder in children\" published in the prestigious Lancet journal in 1998 [171, 197]. This study\nturned tens of thousands of parents against the vaccine, and as a result, in 2020, many countries, including the\nUnited Kingdom, Greece, Venezuela, and Brazil, lost their measles elimination status. In 2020, twenty-two years\nafter publishing this study Lancet retracted the paper [203]. Examples of medical fake news that have spread\non social media include the oncogenic effects of antihypertensive drugs, which caused several patients to stop\nusing them, and misinformation about the Human Papillomavirus (HPV) vaccines, which resulted in half of the\npopulation in South Carolina not having completed the vaccination series [171, 194].\nThe issue of fake news was exacerbated during the recent COVID-19 pandemic, where it became clear that the\nenemy was not only the virus but also the abundance of misinformation leaked on social media and the web, even\nfrom prominent public figures, endangering human lives\u00b9. The extent of online health misinformation led the\nWorld Health Organization (WHO) to declare an \u201cinfodemic", "an overabundance of information\nsome\naccurate and some not \u2013 that makes it hard for people to find trustworthy sources and reliable guidance when\nthey need it\"2. The repercussions of the infodemic are severe: It results in \u201can increase in erroneous interpretation\nof scientific knowledge, opinion polarization, escalating fear, and panic or decreased access to health care\"[197": "nand it is detrimental to public health, impacting healthcare utilization and cost, and medical non-compliance\n[177]. Indicatively, in a recent poll\u00b3 among 806 physicians, three out of four of the physicians said that medical\nmisinformation has hindered their ability to treat COVID-19 patients, and 44% of them estimated that more than\nhalf of COVID-19 information they receive from patients is misinformation. Furthermore, more than two-thirds\nof the physicians mentioned that the problem of misinformation extends beyond COVID, to areas such as weight\nloss, dietary supplements, mental health, and other vaccines.\nAs a response to this wave of dangerous misinformation, there was a call-to-arms to construct tools for\nweathering the infodemic. There was a strong effort from the scientific community to construct and make\npublicly available datasets with valid scholarly information relevant to COVID-19 and coronaviruses in general.\nA well-known dataset is the COVID-19 Open Research Dataset (CORD-19) [188] and the Covidex search engine\n[209] built on top of it. In addition, most fact-checking organizations had to grow and dedicate more of their\nresources to COVID-19 misinformation [56]. As a result, several fact-checking services were developed targeting\nthe COVID-19 case, aiming at assessing the validity of claims and the credibility of sources. However, it is\nbecoming increasingly challenging and expensive to identify fake news and claims through manual inspection,\ndue to the speed at which information (both valid and non-valid) spreads on the media, and especially on social\nmedia. In a WHO survey on COVID-19 fact-checkers, most fact-checking organizations reported that their\nleading challenge was keeping up with rapidly changing science and fact-checking health misinformation [136].\nThe inability of manual approaches to deal with the scale of the problem highlights the importance of\nautomatic, data-driven approaches that can complement or supplant manual efforts. The research efforts of the\nInformation Technology scientific community for addressing misinformation focus on a variety of issues, such\nas automating claim detection and validation, detecting misinformation using the content and the propagation\npatterns, identifying users (malicious or not) that instigate or facilitate misinformation, as well as, mitigating\nmisinformation. Most of the approaches develop and apply state-of-the-art Data Science and Machine Learning\ntechniques, trained on large amounts of data.\nThe research on misinformation is extremely broad, touching several scientific fields. Previous technical\nsurveys on fact-checking and fake news provide a general overview of the current landscape but do not target the\nmedical domain [60, 208, 215]. The few surveys that explicitly focus on misinformation in the medical domain\n[18, 27, 170, 190] provide a high-level approach that is relevant to a broad spectrum of researchers, stakeholders,\nand decision-makers. This survey complements the previous works by focusing on the medical domain, and\nexploring how the different fact-checking and fake news detection techniques have been adapted to this domain\nfrom a computer engineering perspective. To facilitate future research on the area it thoroughly describes publicly\navailable domain-specific datasets, paying special attention to the COVID-19 case."}, {"title": "2 OVERVIEW", "content": "There is no universally accepted definition of fake news. An understanding of fake news is attained by considering\nthe dimensions of authenticity, intention, and news content [172, 215]. The authenticity dimension refers to\nwhether the factual claims in the news item are valid. The intention dimension is about whether there is intention\nto deceive in creating, or propagating the news item. Finally, the news dimension refers to whether the content of\nthe item is about news or not. In case of social media, the news dimension is often not clear, since blogs and\nsocial platforms have allowed non-journalists to reach large audiences, challenging the traditional definition of\nwhat news is [172].\nThere are many information disorders along the dimensions of authenticity and intention that overlap with fake\nnews. The term misinformation is used to characterize both intentionally and unintentionally false or misleading\ninformation, while the term disinformation is used for false information that is purposely spread to deceive people\n[93]. In this paper, we will consider mainly misinformation. Malinformation is the deliberate dissemination of\ntrue information (e.g., leaking private information) and it is not considered fake news [140].\nThere is also a long list of other concepts in the authenticity, intention and news spectrum related to fake\nnews [140, 172, 215]. Two such concepts are satire and parody that use humor or exaggeration to draw attention\nand often entertain. Both share the assumption that the users are aware that the presentation, or the content is\nintentionally faux. Fake information may also be part of advertising, or public relations when marketing or other\npersuasive messages are inserted into news articles. A somewhat similar case is propaganda that refers to news\nitems created by political entities to influence public perceptions. Other concepts include hoaxes referring to\nhalf-truths made for fun, rumors referring to ambiguous stories whose truthfulness never gets confirmed, and\nclickbaits referring to misleading headlines for engaging the audience.\nWhen it comes to social networks, the life cycle of fake news includes the phases of news creation, publishing\non the network, and online propagation. The author of the news item is the person that first created the news\nitem. The author may be a journalist writing a newspaper article, a scientist writing a scientific article, or in some\ncases, a normal user. The publisher is the person that first posted the item on the social network platform. In some\ncases, the publisher is also the author of the item. Propagation means any kind of reaction that leads to the item\ngetting additional exposure. Depending on the media, reactions include re-postings or retweets, commenting and\nvarious other actions of endorsement or disapproval, such as likes, upvotes, or downvotes.\nIn particular, in the medical domain, misinformation is often created and spread by individuals with no scientific\naffiliation, that assume the role of expert patients, promote individual autonomy, and challenge state actions\n[190]. By promoting fear and anxiety, and through the horizontal diffusion of conspiracy theories, they are able\nto erode in an irreversible way the vertical health communication strategies.\nAnother major issue regarding medical information, and scientific information in general, is the fact that\ncitizens often have a limited understanding of basic scientific facts and more broadly of the scientific process [153].\nMoreover, personal beliefs are often inconsistent with the best available science due to inaccurate perceptions,\nlack of scientific consensus, or adoption of conspiracy theories.\nFake news propagation in social media is also amplified by the fact that it is systemic according to [52]. Fake\nnews is designed in such a way so as to pass itself as news to the relevant targeted audience and in fact mislead\nit, by exploiting the systemic features inherent in the channels of social media. Such features include various\ncognitive biases and heuristics, that lead in increasing the spread of fake news propagation.\nVarious pilot studies have been conducted regarding the typology of health misinformation in social media.\nThe authors of [192] examined health-related misinformative posts from various social network platforms written\nin Polish during the period 2012-2017. In the initial screening, satire, parody, and propaganda were not detected,\nprobably because they mainly apply to political news. However, 40% of the most frequently shared links contained\ntext classified as fake news. The most fallacious content concerned vaccines, while, content about cardiovascular\ndiseases was, in general, well-sourced and informative. Another study of the sources and types of misinformation\nabout COVID-19 highlighted the prevalence of fake news in social media [116]. In this study, the identified\ncommon types of misinformation include false claims, conspiracy theories and pseudo-scientific health therapies,\nregarding the diagnosis, treatment, prevention, origin, and spread of the virus.\nFinally, in a high-level overview survey on English social media [170], the authors used the PubMed search\nengine to explore the prevalence of health misinformation and identify the medical topics that are more susceptible\nto fake information. They reported high misinformation content in the smoke, drug, and vaccine categories,\nmoderate misinformation about diets and noncommunicable diseases and pandemics, and low misinformation in\nmedical treatments and surgical treatments, since in this case most information is coming from official accounts.\nIn the following sections, we focus on fact checking, fake news detection and mitigation in social media\nplatforms with emphasis on the medical domain. Fact-checking is the process of verifying the accuracy and\ntruthfulness of information that is presented as news or as factual. Fact-checking involves researching the claims\nbeing made, looking for credible sources of information, and comparing the information to other sources to ensure\nthat it is accurate and truthful. We categorize fact-checking approaches into manual and automatic approaches\nand present a thorough analysis of the related methods. Fact-checking is important because it helps to prevent\nthe spread of misinformation, and it allows people to make informed decisions based on accurate information.\nIn terms of fake news detection, we provide a systematic categorization and thorough description and analysis\nof fake news detection approaches. We categorize fake news detection approaches into those using: (a) the content\nof the news item, (b) information about its propagation in the social platform, and (c) features of the source, that\nis, of the publisher or the social media users involved in the propagation. For each approach, we describe in\ndepth the used methods, including the commonly used features in case of traditional Machine Learning (ML)\napproaches and the involved pipelines in the case of Deep-Learning (DL) approaches.\nThe three categories of fake news detection approaches are usually fused, since the study of fake news in social\nmedia requires the use of both textual and structural information, along with the user context and preferences,\nthe social context, and any spatio-temporal information available [44, 160, 173]. For example, according to [42],\nthe analysis of health-related content that uses a more informal language can benefit from propagation-network\nand user-profile features, while more formal medical content can benefit from linguistic-stylistic and linguistic-\nmedical features. Moreover, the analysis of popular content that generates a high volume of social reactions can\nbenefit from linguistic-emotional features [42]."}, {"title": "3 FACT-CHECKING", "content": "Fact-checking is the process of verifying the factual accuracy of statements/claims. Historically, it has been\nassociated with journalism, being an important process of media companies for verifying published information\nrelated to different types of claims (political, religious, social, etc.). The process of fact-checking is usually done\neither internally, using resources of the media company or through an external third-party [32]. In a similar\nmanner, there is a verification and validation process of academic work in the scientific community, where the\ncritical study of the prior literature, the soundness of the proposed approaches and methodologies, and the\nreproducibility and verifiability of the results, are integral to academic research.\nDuring the 2016 US elections, the online media platforms were found susceptible to disseminating disinforma-\ntion and misinformation [7]. This made fact-checking a hot topic across the scientific community, journalists,\nand online social users. This trend was further amplified due to the pandemic misinformation. As a result, the\nnumber and size of fact-checking organizations have grown across the globe and social platforms have started to\ndevelop partnerships with them [32]. In addition, the need for cutting-edge and credible scientific knowledge has\nmade scientific literature part of the fact-checking sources.\nPublishing fact-checked information has a positive effect on correcting false and inaccurate information,\ndiscouraging bad actors to spread misinformation. However, the fact-checked information does not necessarily\nprevail, due to the persistent promotion of less accurate claims from highly influential groups [122]. The same\nis also true for the medical domain, where the decrease in COVID-19 misperceptions thanks to fact-checking,\ndoes not persist over time even after repeated exposure [21]. As a result, fact-checked information is only part\nof the solution towards credible medical information, and it should be combined with the automatic fake-news\ndetection and mitigation approaches discussed in the next sections to be effective.\nThe process of fact-checking is either done manually, by domain experts or workers in crowdsourcing platforms,\nor automatically. The two approaches are discussed in detail below, while a detailed description of the available\nonline fact-checking tools is provided in Section 7.2."}, {"title": "3.1 Manual Fact-Checking", "content": "In manual fact-checking, the assessment of a claim as true or false is done by people who read the articles that\nmust be checked and decide whether the contained claims are true or false based on certain criteria, metrics, and\nresearch. Manual fact-checking is divided into expert-based and crowd-sourced-based fact-checking."}, {"title": "3.1.1 Expert-Based Fact-Checking", "content": "In expert-based fact-checking, the news is checked by experts of a domain,\nlike the previously mentioned COVID-19 fact-checkers network. Usually, this team consists of journalists and\ndomain experts. This method offers accurate fact-checking but is expensive and time-consuming. Moreover, it is\ndifficult for a group of people to cover every day all the current affairs articles and keep up with rapidly changing\ndomains like the COVID-19 pandemic.\nThe most famous fact-checking site that makes use of an expert-based fact-checking method and was highly\nactive during the pandemic is Politifact [134], which contains a column about health news and a column about\ncoronavirus. In Politifact, a team of experts studies daily transcripts, news stories, press releases, and campaign\nbrochures to find the most significant claims. Politifact uses the Truth-O-Meter ratings that classify the claims\ninto the following categories: True (accurate), Mostly True (needs clarification), Half True (leaves out key details),\nMostly True (ignores critical facts), False (not accurate), and Pants on fire (ridiculous claim)."}, {"title": "3.1.2 Crowd-Source-Based Fact Checking", "content": "In crowd-sourced fact-checking, the detection of fake news is done by\na large population that rates the credibility of articles. This approach has been proposed by various organizations\nsuch as WikiTribune [187] and is much more economical than expert-based fact-checking. However, it carries\nthe risk that the rating population might introduce their own biases in the process.\nThe process consists of several steps. The first step is monitoring the news on TV, social media, newspapers,\nand websites and selecting the articles to be checked. When this selection is made by experts, the articles are\nfiltered and balanced in order to be unbiased (e.g., for covering the whole political spectrum). This is a major\ndifferentiation from the case where the selection of articles is made up by the crowd since in the latter case it is\ndifficult to certify that the selection of new claims is spread fairly across the news spectrum. The next step is\nresearching the subject of the claims using multiple sources and assigning a rating to the news article. This step\nis difficult to be completed objectively by people who are not experts. There are concerns about the correctness\nof the rating, the availability of evidence, and the rater's motivation. Another drawback of this approach is that it\nis difficult for volunteers to fact-check a claim that needs to be assessed rapidly, as in the case of the pandemic.\nAn example of a site that works with crowd-sourced fact-checking is Fiskkit [130], where users can select\narticles, upload them to the site and rate them sentence-by-sentence. Users can also apply tags that evaluate the\narticle's accuracy and see the ratings of other users.\nRegarding available datasets, the Multi-Genre Natural Language Inference Corpus MNLI\u00ba is a crowd-sourced\ncollection of 433K sentence pairs annotated for textual entailment. In other words, the data consists of pairs (p,\nh), where p is the premise and h is the hypothesis, and labels in {entailment, contradiction, neutral}, which report\nwhether the hypothesis entails, contradicts or is neutral towards the premise."}, {"title": "3.2 Automatic Fact-Checking", "content": "Manual fact-checking has satisfactory results, especially in the case of expert-based fact-checking. However,\nsince the task of fact-checking is time-consuming, this method is not efficient for the rapidly changing domain of\nnews, and for keeping pace with the volume of new content on the web. Therefore, the scientific community\nhas been exploring approaches to automate fact-checking by exploiting techniques and advancements from the\ndomains of NLP and DL. The process of automatic fact-checking consists of two steps: a) detecting the claims in\nthe text and b) assessing the validity of the claims by retrieving evidence."}, {"title": "3.2.1 Claim Detection", "content": "Regarding the detection of the claims contained in social media posts, comments, news,\nand web pages, it is important that the most check-worthy claims are selected. Such claims are those for which\npeople show interest and are trending. The claims are usually collected from social media, where metadata like\nthe number of likes and reposts are used as features to identify top claims [42]. Other sources of claims can be\nfound in Wikipedia (e.g., COVID-19 pandemic misconceptions) or in news websites and organizations.\nIn [203] the authors annotate a corpus of 1200 tweets for implicit and explicit biomedical claims. Using this\ncorpus, which is related to COVID-19, measles, cystic fibrosis, and depression, they developed deep-learning\nmodels that automatically detect tweets containing claims. Their analysis showed that biomedical tweets are\ndensely populated with claims. Despite the fact that the detection of claims was challenging, they report that\ndeployed models provided acceptable performance.\nWhile most works focus on single claim sentence analysis, the work presented in [141] introduces the News-\nClaims dataset, a benchmark for attribute-aware claim detection considering topics related to COVID-19. Specifi-\ncally, given a news article, the task is to identify the claim sentence to a set of predefined topics that contain\nfactually verifiable topics, the claimer, the claim object, the stance of the claimer, and the exact claim boundaries.\nFor claim sentence detection they use Claimbuster10 [62] along with pre-trained Natural Language Inference (NLI)\nmodels as zero-shot classifiers based on BART, where the claim sentence is the NLP premise and the hypothesis is\nconstructed from each topic. The claim object task is modeled as a zero-shot or few-shot setting by converting it\ninto a prompting task for pre-trained language models like GPT-311. Stance detection is done again through NLI,\nwhere the affirm and refute labels construct the hypothesis, taking as stance the corresponding higher entailment\nscore. Claim boundary detection is done using fine-tuned Bidirectional Encoder Representations from Trans-\nformers (BERT) [38] models, the Project Debater APIs [13] and the PolNeAR12 popular news attribution corpus of\nannotated triples comprising the source, cue, and content for statements made in news. Finally, regarding claimer\ndetection, they again leverage PolNeAR for building a claimer extraction baseline by fine-tuning a BERT model,\nalong with a second baseline built upon Semantic Role Labeling (SRL), that outputs the predicate-argument\nstructure of a sentence such as who did what to whom. The results showcase that the above tasks, except the\ntask of stance detection, are difficult for current models, especially the task of claim sentence detection."}, {"title": "3.2.2 Claim Validation", "content": "For the assessment of the validity of a claim, an essential part is the retrieval of the\nevidence process. Evidence retrieval is the task of retrieving documents that support the prediction of a claim.\nDuring this process, information and proofs must be found around the claim, such as text, tables, knowledge\nbases, images, and other metadata for evidence of the truth. A fundamental issue is finding trustworthy sources.\nFor example, many fact-checking approaches make use of encyclopedias.\nAfter the retrieval of evidence, a fact-verification method has to conclude the validity of a claim. Usually, the\nverification of claims leverages NLI techniques [169]. As already mentioned, the NLI task aims to classify the\nrelationship between a pair of a premise (evidence) and a hypothesis (claim) as either entailment, contradiction,\nor neutral. However, in fact-verification systems, the usually multiple evidence pieces, are found by the systems\nthemselves. In addition, given a collection of false/true claims, the verification of a new information piece can\nalso be modeled as a NLI task, where the goal is to detect entailment with one of the false/true collected claims\n[106]. Another approach is described in [198], where the authors present the ClaimGen-BART and Knowledge\nBase Informed Negations (KBIN) methods for generating claims and claim negations supported by the literature,\nusing the BART pre-trained model [96]. In [184] the authors present Kernel Graph Attention Network (KGAT),\nwhich conducts more fine-grained fact verification with kernel-based attention.\nRegarding the medical domain, a COVID-19-specific dataset that has been constructed using automatic\nmethods is COVID-Fact [142] which contains 4,086 claims concerning the pandemic, evidence for the claims,\nand contradictory claims refuted by evidence. The approach described in [101] adapts the open-domain fact\nextraction and verification KGAT approach [102] with in-domain language models, based on the SciFACT and\nCOVID-Fact datasets. Specifically, the in-domain language model transfers COVID domain knowledge into\npre-trained language models with continuous training. The COVID medical token semantics are learned using\nmask language model-based training. In a similar manner, the authors of [90] introduce the PubHealth dataset for\npublic health fact-checking, which also includes explanations, and explore veracity prediction and explanation\ngeneration tasks using various pre-trained models. Their results show that training models on in-domain data\nimproves the accuracy of veracity prediction and the quality of generated explanations compared to training\ngeneric language models without explanation.\nIn [150] the authors introduce the HealthVer dataset for evidence-based fact-checking of health-related claims.\nThe dataset was created using a three-step approach. The first step is to retrieve real-world claims from snippets\nreturned by a search engine for questions about COVID-19. The next step is to retrieve and rank relevant scientific\npapers as evidence from the COVID-19 Open Research Dataset (CORD-19) [188] using a T513 relevance-based\nmodel. The last step is to manually annotate the relations between each evidence statement and the associated\nclaims. The conducted experiments showed that training deep learning models on real-world medical claims\ngreatly improves performance compared to models trained on synthetic and open-domain claims.\nIn [186] the authors evaluate baseline models for categorizing reddit posts as containing claims, personal\nexperiences, and/or questions. In addition, they evaluate various BERT models for extracting descriptions of\npopulations, interventions, and outcomes, as well as for tagging claims, questions, and experiences. Finally, using\nsnippets they retrieve trustworthy (published) evidence relevant to a given claim. To this end, they introduce a\nheuristic supervision strategy that outperformed pre-trained retrieval models."}, {"title": "3.2.3 Knowledge Bases and Claim Validation", "content": "Another direction for assessing the validity of a claim and retrieving\nthe corresponding evidence is by exploiting knowledge bases. A knowledge base (KB) is a collection of information\nand resources where human knowledge can be stored. A common way of representing it is by connecting two\nentities with a given relationship. These relationships can form a graph, the knowledge graph (KG), where the\nentities are represented as nodes and relationships are represented as edges. In this case, a fact is defined as a\ntriple that has the form of (\u201csubject", "predicate\" p, \u201cobject\" o), and can be classified to different categories (e.g.,\nnumerical, object properties, etc.). Usually, the data modeling languages that are used for creating the graphs are\nthe Resource Description Framework (RDF)14 or the more expressive Web Ontology Language (OWL)15 that also\nprovides automatic reasoners. An example is KG-Miner [157": "which can predict the truthfulness of a statement\nusing discriminative predicate path mining.\nGiven a textual claim and a KG, the claim is converted to a triple by using NLP methods [8, 71], and the validity\nof the fact is checked against the information contained in the graph. However, the KG is considered incomplete\n(Open World Assumption), meaning that it does not contain all known true facts. As a result, a missing fact does\nnot imply an invalid claim. The proposed methods in the literature try to overcome this issue.\nThere are three main approaches for checking the validity of a claim against an incomplete KB [88]: a) using\nexternal web resources as a way to find new fact triples that are missing from the KB and complement existing\nknowledge [43], b) embedding-based [107, 118] and path-based approaches [157] that use graph embeddings\nand properties of the paths as features respectively for verifying facts, and c) rule-based approaches that use\nrule-mining techniques to validate a fact [158]. The first approach, which complements the KB with external\nknowledge, has been proven to be inaccurate due to the difficulty of the information extraction task (83.51%\nof fact-triples extracted from Wikipedia using a BERT-model relation extraction approach were false [88]).\nEmbedding-based approaches, since they use a statistical approach, have the benefit that they can verify entity\npair links that are not linked in the KG, covering more verifiable triples which are not verifiable by rule-based\nones [88]. On the other hand, since rule-based approaches use logical rules, their results are more interpretable,\nand they can easily verify some facts that the embedding approaches have trouble with [107]. Finally, it has been\nshown that both the embedding-based and rule-based approaches can be used complementarily, offering better\nperformance than by using them separately [88].\nIn the medical domain, there are various knowledge graphs that could be used for fact-checking claims, such\nas COVID-19, CovidGraph16, literature-review related ones [26], oncological-specific KGs [163], personalized\nmedicine recommendation KBs [25, 164], disease ontologies like ICD-917 and drug safety and interactions KBs\nlike DrugBank 18.\nThe SciClaim KG [105] is a graph of scientific claims drawn from Social and Behavior Science (SBS), PubMed,\nand CORD19 papers. It incorporates coarse-grained entity spans (list of tokens) as nodes and relations as edges\nbetween them, and fine-grained attributes that modify entities and their relations. In total it contains 12,738 labels\nthat capture causal, comparative, predictive, statistical, and proportional associations over experimental variables,\nalong with their qualifications, subtypes, and evidence. The schema is inferred using a transformer-based joint\nentity and relation extraction approach.\nThe UMLS19 meta-thesaurus is a large biomedical knowledge base that unifies hundreds of different ontologies\nin biomedicine. UMLS is used as the source knowledge base for normalization and candidate selection for KBIN.\nAdditionally, it is the knowledge base used to train the clinical concept embeddings cui2vec20 [15], which are\nused for candidate concept selection in KBIN."}, {"title": "4 CONTENT-BASED METHODS", "content": "Content-based fake news detection methods analyze the content of the various sources and their interactions\nwith it [159]. Specifically, they extract various features from news items, social media posts, user comments, or\nthe content of external sources through hyperlinks and quotes [5]. Before feature extraction, it is necessary to\npre-process the data by using techniques such as tokenization, lowercase transformation, removal of stop words,\nsentence segmentation, etc. Afterwards, the extracted features can be used as the input to ML classifiers or can be\nconcatenated or aggregated in neural network architectures like feedforward neural networks (FNNs), recurrent\nneural networks (RNNs), convolutional neural networks (CNNs), Long Short-Term Memory networks (LSTMs),\nGenerative Adversarial Networks (GANs), Sequence-to-Sequence networks (Seq2Seq) like transformers, Graph\nNeural Networks (GNNs), etc. (see [30], [174], [162], [29], [3]).\nBelow we describe the different types of content-based features commonly used in the literature. We consider\nthe following categories: text representations, linguistic features, emotional features, entity-based features,\nstylistic features, topic extraction, user-profile features, image-based features, and external features."}, {"title": "4.1 Text representations", "content": "Text representations address the fundamental problem of converting unstructured documents to mathematically\ncomputable forms, usually in the form of vectors. Commonly used approaches include the bag-of-words (BoW)\napproach, which weights the frequency of a word in a document [12, 42", "127": ".", "items": "words, letters, syllables, phonemes, etc.), are commonly used even in the\ncontext of medical fake news detection [127", "178": ".", "word2vec\n[111": "GloVe [128", "FastText[78": "learn word associations from a large corpus of text but do not capture\ninformation about the context in which they appear. They provide a single global representation for every word,\neven if words can be ambiguous and have various meanings. For example in the context of medical fake news,\nthe authors in [174"}, {"178": "the\nauthors evaluate non-contextual and contextual text embeddings approaches over various fake-news datasets, and\nshowcase the importance of contextual embeddings. A large number of approaches that leverage latent features\nuse pre-trained language models based on the popular BERT model [38", "96": "ROBERTa\n[100", "148": "Albert [92", "206": "etc.). Such models can be fine-tuned over a collection of fake and\nvalid news for classification reasons, as is the case of FakeBERT [80", "94": "a BERT model for Biomedical Text Mining. This is the first domain-specific language\nrepresentation model pre-trained on large-scale biomedical corpora (PubMed abstracts and PMC full-text articles).\nThe reported results show that pre-training BERT on biomedical articles is crucial"}]}