[{"title": "LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image", "authors": ["Fan Yang", "Sicheng Zhao", "Yanhao Zhang", "Haoxiang Chen", "Hui Chen", "Wenbo Tang", "Haonan Lu", "Pengfei Xu", "Zhenyu Yang", "Jungong Han", "Guiguang Ding"], "abstract": "Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, particularly small models, struggle with processing logical reasoning, question-answering, and handling open scenario categories. On the other hand, generative multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak spatial and local object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we propose the following solutions: Spatial-Enhanced Local Feature Mining for better spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We employ parameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a powerful 3D perception MLLM. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, significantly outperforming existing methods.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of deep learning, 2D perception tasks such as object detection (Chen et al., 2023d; Wang et al., 2024; Lyu et al., 2023), semantic segmentation (Zhao et al., 2021), and visual grounding (Liu et al., 2023b) have achieved remarkable progress (Li et al., 2021; Shen et al., 2023). However, the real world is three-dimensional, and many practical applications, such as autonomous driving, robotics, augmented reality, and embodied intelligence, demand enhanced spatial perception (Mao et al., 2023; Angelova et al., 2020; Addari and Guillemaut, 2023). Traditional 2D methods can no longer meet these demands. Therefore, researchers have introduced the concept of three-dimensional perception, which involves inferring the position, dimension, and pose of objects in three-dimensional space to achieve accurate predictions of their spatial locations (Mousavian et al., 2017; Qin et al., 2019).\nCurrently, 3D perception techniques primarily include methods using LiDAR point clouds (Shi et al., 2023; Stoiber et al., 2022; Xie et al., 2021; Lang et al., 2019; Zhang et al., 2022; Aumentado-Armstrong et al., 2023) and those based on camera images (Sundermeyer et al., 2020). LiDAR-based methods offer superior depth prediction capabilities; however, their high costs and complex components limit their applicability in many scenarios (Wang et al., 2023c; Zhang et al., 2024). In contrast, camera image-based methods are more cost-effective and are easier to integrate, making them widely used in autonomous driving, robotics, and augmented reality (Mao et al., 2023).\nIn recent years, numerous specialized perception models have been developed for image-based 3D perception."}, {"title": "2 Related Works", "content": "2.1 Multimodal Models\nResearchers integrated multimodal data, such as images, video, and text, to construct multimodal models. Early advancements in this domain were pioneered by the \"Show and Tell\" model (Vinyals et al., 2015), which showcased the combination of convolutional neural networks and recurrent neural networks to generate textual descriptions from images. This foundational work underscored the potential of deep learning frameworks in bridging vision and language.\nThe introduction of attention mechanisms by Bahdanau et al. (Bahdanau et al., 2015) marked a significant shift in multimodal models. Xu et al.'s \"Show, Attend and Tell\" (Xu et al., 2015) applied these mechanisms to image captioning, allowing models to selectively focus on different parts of an image while generating descriptive text. This approach significantly enhanced the interpretability and quality of generated captions.\nBuilding upon these foundations, transformer architectures have achieved remarkable success in multimodal integration. ViLBERT (Lu et al., 2019) introduced a model that processes images and text in parallel using two-stream transformers, aligning them through co-attentional mechanisms. This model set new benchmarks for various vision-and-language tasks, illustrating the power of joint vision-language pre-training.\nRadford et al.'s CLIP (Radford et al., 2021) introduced a paradigm shift by leveraging a vast dataset of image-text pairs to learn transferable visual models through natural language supervision. CLIP demonstrated exceptional zero-shot performance across multiple datasets and tasks, showcasing the effectiveness of large-scale multimodal pre-training.\nIn the generative realm, DALLE (Ramesh et al., 2021) exemplified the synergy between language models and image generation. DALL\u00b7E employs autoregressive transformers to create detailed images from textual descriptions, pushing the boundaries of text-to-image generation. FLAVA (Singh et al., 2022) proposed a unified multimodal model leveraging both unimodal and multimodal pre-training objectives. FLAVA combines the strengths of supervised learning and self-supervised learning to achieve comprehensive vision-language understanding.\n2.2 Multimodal Large Language Models\nIn 2023, OpenAI released GPT-4V (OpenAI, 2023), which incorporated image input capabilities into the large language model framework, showcasing powerful vision-text multimodal capabilities. In May 2024, OpenAI introduced GPT-4o, a model capable of processing and generating any combination of text, audio, and image inputs. Notably, GPT-4o can respond to voice input in as little as 232 milliseconds, with an average response time of 320 milliseconds, approaching the reaction time of humans in daily conversations. However, powerful commercial multimodal models like GPT-4V and GPT-4o have not been open-sourced, limiting researchers' ability to build upon these closed models.\nRecently, researchers have developed a series of open-source multimodal large language models (Li et al., 2023; Zhu et al., 2023; Bai et al., 2023; Chen et al., 2023c; Lu et al., 2024; Wang et al., 2023b). These open-source multimodal large language models have demonstrated impressive capabilities and have significantly contributed to the advancement of the community.\nLiu et al. introduced LLaVA (Liu et al., 2023a), where the authors attempted to generate multimodal language-image instruction-following data using purely language-based GPT-4. By fine-tuning this generated data, they developed the large language and vision assistant. Utilizing CLIP (Radford et al., 2021) and LLaMA (Touvron et al., 2023) for instruction fine-tuning, they constructed the multimodal large model LLaVA, which achieved promising results. The Shanghai AI Lab proposed LLaMA-Adapter (Zhang et al., 2023b), an efficient fine-tuning method that adapts LLAMA into an instruction-following model. The method attaches a set of learnable adaptive prompts as prefixes to the input instruction tokens within the deep layers of the LLaMA transformer (Vaswani et al., 2017). For image input, it uses CLIP to extract multi-scale global features, subsequently concatenated and projected into a global information representation through a projection layer. Despite the promising demonstration of LLaMA-Adapter in handling vision inputs, it hasn't generalized well to open visual instructions and lags behind GPT-4. Peng Gao et al. proposed LLaMA-Adapter-V2 (Gao et al., 2023), a parameter-efficient (Xiong et al., 2024; Hao et al., 2024b) visual instruction model that enhances LLaMA-Adapter by unlocking more learnable parameters (e.g., norms, biases, and scales), extending its instruction-following capabilities (Xu et al., 2024) across the entire LLaMA model.\nWenhai Wang et al. proposed VisionLLM (Wang et al., 2023a), utilizing large language models to perform visual tasks such as detection and instance segmentation. They introduced a language-guided image tokenizer using BERT (Devlin et al., 2019) as the text encoder and deformable DETR (Carion et al., 2020) to capture high-level information. VisionLLM uses Alpaca-7B (Taori et al., 2023) as the backbone of the large language model. Jun Chen et al. proposed MiniGPT-v2 (Chen et al., 2023a), which uses a linear projection layer to map image features to the feature space of the large language model, reducing computational overhead by concatenating four adjacent visual tokens. Weihai Wang et al. introduced CogVLM (Wang et al., 2023b), differing from shallow alignment methods like feature projection layers by employing deep fusion to better integrate visual features into the large language model. PerceptionGPT (Pi et al., 2023) proposed encoding and decoding perceptual information using a single token. However, it is limited to encoding and decoding 2D perceptual information and cannot handle 3D signals.\nThese multimodal large language models are trained on massive datasets consisting of 2D images, videos, and textual data, excelling in various 2D tasks. However, they have not been trained on 3D data, resulting in weak spatial perception capabilities.\nIn May 2024, Cho et al. introduced a pre-trained large model CubeLLM (Cho et al., 2024a). CubeLLM expended substantial resources on multimodal alignment and pre-training across 2D and 3D datasets, specifically involving a total of 9.6 million images and 40.9 million dialogues. A single experiment required 64 A100 GPUs with a batch size of 1024. CubeLLM's model, code, and training data have not been open-sourced. Unlike CubeLLM, which pre-trained a multimodal large language model, our LLMI3D requires much fewer resources because we apply parameter-efficient fine-tuning methods to adapt an MLLM for 3D perception capabilities. Our LLMI3D uses only two A100 GPUs with LoRA (Hu et al., 2022), significantly reducing the required training resources and costs while increasing flexibility.\n2.3 3D Perception from a Single Image\nNumerous studies have been conducted on 3D perception from a single image, with the vast majority focusing on monocular 3D object detection. Early work by Chen et al. in Mono3D (Chen et al., 2015) utilized geometric priors and a region proposal network to estimate 3D bounding boxes from single images. While effective, the reliance on hand-crafted features limited its applicability across diverse scenarios. Mousavian et al.'s Deep3DBox (Mousavian et al., 2017) introduced an approach combining 2D object detection with 3D pose estimation, marking a pivotal shift towards more accurate 3D localization. This method leveraged both appearance and geometric cues, setting the foundation for subsequent improvements. Further advancements were embodied in Qin et al.'s MonoGRNet (Qin et al., 2019) employing graph-based reasoning to capture spatial relationships within scenes, significantly enhancing the precision of 3D bounding box estimations. End-to-end learning frameworks have also shown great promise. Li et al. presented RTM3D (Li et al., 2020), a unified model integrating detection and localization stages, yielding robust performance through sophisticated feature extraction and attention mechanisms.\nIn recent years, image-based 3D perception has substantial progress (Wu et al., 2023; Zhang et al., 2023c; Yang et al., 2022; Yan et al., 2024). MonoFlex (Zhang et al., 2021) utilized key points to assist depth estimation and adopted an uncertainty-based ensemble method for improved accuracy. MonoRCNN (Shi et al., 2021) incorporated geometric information between 2D bounding box heights and 3D object heights to estimate depth. GUPNet (Lu et al., 2021) estimated object depth through the projection of 2D and 3D heights and employed an uncertainty loss for precise scoring. Gpro3D (Yang et al., 2023) significantly enhanced the accuracy of object depth and spatial predictions by leveraging ground plane priors.\nIn 2024, Zhan et al. (Zhan et al., 2024) proposed a multimodal specialized small model capable of performing 3D grounding tasks based on textual descriptions. However, it solely uses BERT (Devlin et al., 2019) and CNN (He et al., 2016) for simple feature extraction of text and images. Limited by model capacity and pre-training scale, BERT could only accept direct object descriptions. Mono3DVG lacks logical reasoning capabilities and cannot answer user questions. Furthermore, Mono3DVG is restricted to trained object classes and would perform poorly when encountering new classes during testing.\nZhan et al. also introduced the Mono3DRefer 3D grounding dataset (Zhan et al., 2024). However, Mono3DRefer has significant issues: it treats the 3D perception results, such as object depth and dimensions (length, width, height), as descriptive inputs for the objects. Consequently, the model can directly infer 3D results based on these descriptions, which prevents a fair evaluation of the model's image-based 3D perception capabilities."}, {"title": "3 Methodology", "content": "3.1 Overview\nThe architecture of our method is illustrated in Figure 3. We fine-tune a pre-trained multimodal large model to empower it with image-based 3D grounding capabilities. We propose a 3D-friendly structure by incorporating a few additional structures into the image encoder and token decoder of MLLMs. In the image encoder component", "perception": "n1. Current multimodal large language models exhibit inadequate capabilities in extracting 3D spatial features. Existing multimodal large language models are generally pre-trained and aligned on vast 2D image-text datasets. While the image encoder and projector components effectively capture semantic information from images", "branches": "the image RGB feature branch and the spatial depth feature branch. The two branches are both several convolution layers. And we get the spatial feature \\(F_{\\text{spatial"}], "F_{\\text{rgb}}\\)": "n\\[F_{\\text{spatial"}, {"feature": "n\\[M_{\\text{depth"}, {"F_{\\text{spatial-local}}\\)": "n\\[F_{\\text{spatial-local"}, {"issues": 1.0, "Speed": "Within the LLM vocabulary table", "Accuracy": "Compared to LLMs' common sense and knowledge", "Complexity": 3, "V_{\\text{norm}})\\)": "n\\[U_{\\text{norm"}, {"image": "p = (u", "v)\\)": "n\\[u = U_{\\text{norm"}, {"space": "length L", "dimensions": "n\\[L", "value": "n\\[d_v = MLP_d (F_{3D"}, {"issues": 1.0, "Issues": "The range and symmetry of Euler angle components differ", "Representation": "Each rotation can have multiple Euler angle representations", "Function": "Using Euler angles as outputs requires considering the periodicity and ambiguity of angles when calculating the loss", "representation": "n\\[\\text{Rot"}, {"matrix": "n\\[\\text{Rot"}, {"equation": "n\\[Z^i[x^i", "derive": "n\\[x^i Z^i = f^i_x X^i + c^i_x Z^i \\tag{14"}, ["Z^v[x^v,y^v,1"], ["X^i,Y^i, Z^v"], {"derive": "n\\[x^v Z^v = f^v_x X^i +c^v_x Z^v \\tag{17"}, ["frac{x^v}{w^v} = \\frac{x^i}{w^i}, \\quad \\frac{c^v_x}{w^v} = \\frac{c^i_x}{w^i} \\tag{18}\\"], ["x^v Z^v = f^v_x X^i +c^v_x Z^v  \\frac{x^i}{w^"]]