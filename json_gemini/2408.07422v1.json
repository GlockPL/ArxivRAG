[{"title": "LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image", "authors": ["Fan Yang", "Sicheng Zhao", "Yanhao Zhang", "Haoxiang Chen", "Hui Chen", "Wenbo Tang", "Haonan Lu", "Pengfei Xu", "Zhenyu Yang", "Jungong Han", "Guiguang Ding"], "abstract": "Recent advancements in autonomous driving, augmented reality, robotics, and embodied intelligence have necessitated 3D perception algorithms. However, current 3D perception methods, particularly small models, struggle with processing logical reasoning, question-answering, and handling open scenario categories. On the other hand, generative multimodal large language models (MLLMs) excel in general capacity but underperform in 3D tasks, due to weak spatial and local object perception, poor text-based geometric numerical output, and inability to handle camera focal variations. To address these challenges, we propose the following solutions: Spatial-Enhanced Local Feature Mining for better spatial feature extraction, 3D Query Token-Derived Info Decoding for precise geometric regression, and Geometry Projection-Based 3D Reasoning for handling camera focal length variations. We employ parameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a powerful 3D perception MLLM. Additionally, we have constructed the IG3D dataset, which provides fine-grained descriptions and question-answer annotations. Extensive experiments demonstrate that our LLMI3D achieves state-of-the-art performance, significantly outperforming existing methods.", "sections": [{"title": "1 Introduction", "content": "With the rapid development of deep learning, 2D perception tasks such as object detection (Chen et al., 2023d; Wang et al., 2024; Lyu et al., 2023), semantic segmentation (Zhao et al., 2021), and visual grounding (Liu et al., 2023b) have achieved remarkable progress (Li et al., 2021; Shen et al., 2023). However, the real world is three-dimensional, and many practical applications, such as autonomous driving, robotics, augmented reality, and embodied intelligence, demand enhanced spatial perception (Mao et al., 2023; Angelova et al., 2020; Addari and Guillemaut, 2023). Traditional 2D methods can no longer meet these demands. Therefore, researchers have introduced the concept of three-dimensional perception, which involves inferring the position, dimension, and pose of objects in three-dimensional space to achieve accurate predictions of their spatial locations (Mousavian et al., 2017; Qin et al., 2019).\nCurrently, 3D perception techniques primarily include methods using LiDAR point clouds (Shi et al., 2023; Stoiber et al., 2022; Xie et al., 2021; Lang et al., 2019; Zhang et al., 2022; Aumentado-Armstrong et al., 2023) and those based on camera images (Sundermeyer et al., 2020). LiDAR-based methods offer superior depth prediction capabilities; however, their high costs and complex components limit their applicability in many scenarios (Wang et al., 2023c; Zhang et al., 2024). In contrast, camera image-based methods are more cost-effective and are easier to integrate, making them widely used in autonomous driving, robotics, and augmented reality (Mao et al., 2023).\nIn recent years, numerous specialized perception models have been developed for image-based 3D perception."}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Multimodal Models", "content": "Researchers integrated multimodal data, such as images, video, and text, to construct multimodal models. Early advancements in this domain were pioneered by the \"Show and Tell\" model (Vinyals et al., 2015), which showcased the combination of convolutional neural networks and recurrent neural networks to generate textual descriptions from images. This foundational work underscored the potential of deep learning frameworks in bridging vision and language.\nThe introduction of attention mechanisms by Bahdanau et al. (Bahdanau et al., 2015) marked a significant shift in multimodal models. Xu et al.'s \"Show, Attend and Tell\" (Xu et al., 2015) applied these mechanisms to image captioning, allowing models to selectively focus on different parts of an image while generating descriptive text. This approach significantly enhanced the interpretability and quality of generated captions.\nBuilding upon these foundations, transformer archi-tectures have achieved remarkable success in multimodal integration. ViLBERT (Lu et al., 2019) introduced a model that processes images and text in parallel using two-stream transformers, aligning them through co-attentional mechanisms. This model set new benchmarks for various vision-and-language tasks, illustrating the power of joint vision-language pre-training.\nRadford et al.'s CLIP (Radford et al., 2021) introduced a paradigm shift by leveraging a vast dataset of image-text pairs to learn transferable visual models through natural language supervision. CLIP demonstrated exceptional zero-shot performance across multiple datasets and tasks, showcasing the effectiveness of large-scale multimodal pre-training.\nIn the generative realm, DALLE (Ramesh et al., 2021) exemplified the synergy between language models and image generation. DALL\u00b7E employs autoregressive transformers to create detailed images from textual descriptions, pushing the boundaries of text-to-image generation. FLAVA (Singh et al., 2022) proposed a unified multimodal model leveraging both unimodal and multimodal pre-training objectives. FLAVA combines the strengths of supervised learning and self-supervised learning to achieve comprehensive vision-language understanding."}, {"title": "2.2 Multimodal Large Language Models", "content": "In 2023, OpenAI released GPT-4V (OpenAI, 2023), which incorporated image input capabilities into the large language model framework, showcasing powerful vision-text multimodal capabilities. In May 2024, OpenAI introduced GPT-4o, a model capable of processing and generating any combination of text, audio, and image inputs. Notably, GPT-4o can respond to voice input in as little as 232 milliseconds, with an average response time of 320 milliseconds, approaching the reaction time of humans in daily conversations. However, powerful commercial multimodal models like GPT-4V and GPT-4o have not been open-sourced, limiting researchers' ability to build upon these closed models.\nRecently, researchers have developed a series of open-source multimodal large language models (Li et al., 2023; Zhu et al., 2023; Bai et al., 2023; Chen et al., 2023c; Lu et al., 2024; Wang et al., 2023b). These open-source multimodal large language models have demonstrated impressive capabilities and have significantly contributed to the advancement of the community.\nLiu et al. introduced LLaVA (Liu et al., 2023a), where the authors attempted to generate multimodal language-image instruction-following data using purely language-based GPT-4. By fine-tuning this generated data, they developed the large language and vision assistant. Utilizing CLIP (Radford et al., 2021) and LLaMA (Touvron et al., 2023) for instruction fine-tuning, they constructed the multimodal large model LLaVA, which achieved promising results. The Shanghai AI Lab proposed LLaMA-Adapter (Zhang et al., 2023b), an efficient fine-tuning method that adapts LLAMA into an instruction-following model. The method attaches a set of learnable adaptive prompts as prefixes to the input instruction tokens within the deep layers of the LLaMA transformer (Vaswani et al., 2017). For image input, it uses CLIP to extract multi-scale global features, subsequently concatenated and projected into a global information representation through a projection layer. Despite the promising demonstration of LLaMA-Adapter in handling vision inputs, it hasn't generalized well to open visual instructions and lags behind GPT-4. Peng Gao et al. proposed LLaMA-Adapter-V2 (Gao et al., 2023), a parameter-efficient (Xiong et al., 2024; Hao et al., 2024b) visual instruction model that enhances LLaMA-Adapter by unlocking more learnable parameters (e.g., norms, biases, and scales), extending its instruction-following capabilities (Xu et al., 2024) across the entire LLaMA model.\nWenhai Wang et al. proposed VisionLLM (Wang et al., 2023a), utilizing large language models to perform visual tasks such as detection and instance segmentation. They introduced a language-guided image tokenizer using BERT (Devlin et al., 2019) as the text encoder and deformable DETR (Carion et al., 2020) to capture high-level information. VisionLLM uses Alpaca-7B (Taori et al., 2023) as the backbone of the large language model. Jun Chen et al. proposed MiniGPT-v2 (Chen et al., 2023a), which uses a linear projection layer to map image features to the feature space of the large language model, reducing computational overhead by concatenating four adjacent visual tokens. Weihai Wang et al. introduced CogVLM (Wang et al., 2023b), differing from shallow alignment methods like feature projection layers by employing deep fusion to better integrate visual features into the large language model. PerceptionGPT (Pi et al., 2023) proposed encoding and decoding perceptual information using a single token. However, it is limited to encoding and decoding 2D perceptual information and cannot handle 3D signals.\nThese multimodal large language models are trained on massive datasets consisting of 2D images, videos, and textual data, excelling in various 2D tasks. However, they have not been trained on 3D data, resulting in weak spatial perception capabilities.\nIn May 2024, Cho et al. introduced a pre-trained large model CubeLLM (Cho et al., 2024a). CubeLLM expended substantial resources on multimodal alignment and pre-training across 2D and 3D datasets, specifically involving a total of 9.6 million images and 40.9 million dialogues. A single experiment required 64 A100 GPUs with a batch size of 1024. CubeLLM's model, code, and training data have not been open-sourced. Unlike CubeLLM, which pre-trained a multimodal large language model, our LLMI3D requires much fewer resources because we apply parameter-efficient fine-tuning methods to adapt an MLLM for 3D perception capabilities. Our LLMI3D uses only two A100 GPUs with LoRA (Hu et al., 2022), significantly reducing the required training resources and costs while increasing flexibility."}, {"title": "2.3 3D Perception from a Single Image", "content": "Numerous studies have been conducted on 3D perception from a single image, with the vast majority focusing on monocular 3D object detection. Early work by Chen et al. in Mono3D (Chen et al., 2015) utilized geometric priors and a region proposal network to estimate 3D bounding boxes from single images. While effective, the reliance on hand-crafted features limited its applicability across diverse scenarios. Mousavian et al.'s Deep3DBox (Mousavian et al., 2017) introduced an approach combining 2D object detection with 3D pose estimation, marking a pivotal shift towards more accurate 3D localization. This method leveraged both appearance and geometric cues, setting the foundation for subsequent improvements. Further advancements were embodied in Qin et al.'s MonoGRNet (Qin et al., 2019) employing graph-based reasoning to capture spatial relationships within scenes, significantly enhancing the precision of 3D bounding box estimations. End-to-end learning frameworks have also shown great promise. Li et al. presented RTM3D (Li et al., 2020), a unified model integrating detection and localization stages, yielding robust performance through sophisticated feature extraction and attention mechanisms.\nIn recent years, image-based 3D perception has substantial progress (Wu et al., 2023; Zhang et al., 2023c; Yang et al., 2022; Yan et al., 2024). MonoFlex (Zhang et al., 2021) utilized key points to assist depth estimation and adopted an uncertainty-based ensemble method for improved accuracy. MonoRCNN (Shi et al., 2021) incorporated geometric information between 2D bounding box heights and 3D object heights to estimate depth. GUPNet (Lu et al., 2021) estimated object depth through the projection of 2D and 3D heights and employed an uncertainty loss for precise scoring. Gpro3D (Yang et al., 2023) significantly enhanced the accuracy of object depth and spatial predictions by leveraging ground plane priors.\nIn 2024, Zhan et al. (Zhan et al., 2024) proposed a multimodal specialized small model capable of performing 3D grounding tasks based on textual descriptions. However, it solely uses BERT (Devlin et al., 2019) and CNN (He et al., 2016) for simple feature extraction of text and images. Limited by model capacity and pre-training scale, BERT could only accept direct object descriptions. Mono3DVG lacks logical reasoning capabilities and cannot answer user questions. Furthermore, Mono3DVG is restricted to trained object classes and would perform poorly when encountering new classes during testing.\nZhan et al. also introduced the Mono3DRefer 3D grounding dataset (Zhan et al., 2024). However, Mono3DRefer has significant issues: it treats the 3D perception results, such as object depth and dimensions (length, width, height), as descriptive inputs for the objects. Consequently, the model can directly infer 3D results based on these descriptions, which prevents a fair evaluation of the model's image-based 3D perception capabilities."}, {"title": "3 Methodology", "content": ""}, {"title": "3.1 Overview", "content": "The architecture of our method is illustrated in Figure 3. We fine-tune a pre-trained multimodal large model to empower it with image-based 3D grounding capabilities. We propose a 3D-friendly structure by incorporating a few additional structures into the image encoder and token decoder of MLLMs. In the image encoder component, to address the weak spatial and local object perception issues of MLLMs, we introduce Spatial-Enhanced Local Feature Mining, enhancing the image encoder's ability to extract features from local objects and spatial structures, as detailed in Section 3.2.\nIn the LLM component, to overcome poor text-based geometric numerical output, we propose 3D Query Token-Derived Info Decoding. This method addresses the drawbacks of slow speed, low accuracy, and difficulty in parsing when 3D coordinates are outputted in text form. By using a single learnable 3D query token combined with 3D heads regression, we can accurately regress the 3D attributes of objects, as elaborated in Section 3.3.\nTo obtain the 3D bounding box and address MLLMs' inability to handle variations in camera focal lengths, we introduce geometry projection-based 3D Reasoning. Rather than relying solely on focal length-invisible black-box neural network 3D reasoning methods, we appropriately utilize camera intrinsic parameters. By combining neural networks with geometric projection, our method effectively mitigates the significant errors in 3D perception caused by different camera intrinsic parameters, as detailed in Section 3.4.\nAdditionally, in Section 3.5, we introduce the IG3D dataset. The IG3D dataset provides precise descriptions of objects in images, including detailed appearance and location, and distinguishes between different objects of the same category within the images, facilitating the 3D grounding task. Moreover, our IG3D dataset includes annotations for Visual Question Answering (VQA) instructions, assessing the model's logical reasoning capabilities, and catering to personalized user input requirements."}, {"title": "3.2 Spatial-Enhanced Local Feature Mining", "content": "The image encoder is a crucial component of multimodal large language models, tasked with extracting image features. However, existing image encoders encounter challenges with weak spatial and local object perception:\n1. Current multimodal large language models exhibit inadequate capabilities in extracting 3D spatial features. Existing multimodal large language models are generally pre-trained and aligned on vast 2D image-text datasets. While the image encoder and projector components effectively capture semantic information from images, they often lose geometrical and spatial information. Unlike typical visual-language tasks, 3D grounding from images is an inherently challenging problem that seeks to derive 3D positioning from 2D image inputs, which naturally lack depth information. Estimating depth is both the hardest and the most critical part of this task. Although pre-trained multimodal models like CLIP-ViT can efficiently extract semantic features, they struggle with spatial geometric features. Therefore, it is crucial to enhance geometric and spatial features using spatial-enhanced feature extractors.\n2. Additionally, many image encoders in MLLMs exhibit insufficient capability in extracting features of local small objects. Typical image encoders reduce images to low resolutions in a simplistic manner (Zhang et al., 2023a). For example, LLaVA (Liu et al., 2023a), ShareGPT4V (Chen et al., 2023b), and InternVL (Chen et al., 2023c) resize images to 336x336, and Qwen-VL (Bai et al., 2023) resizes images to 448x448. These resolutions might be sufficient for global image understanding but fall short for detailed object-level perception. In autonomous driving, input images generally have very high resolutions, often up to millions of pixels. Downscaling these images to 336x336 or 448x448 can make small and distant objects, such as cones and pedestrians, unrecognizable. These objects are critical in autonomous driving and cannot be ignored. On the other hand, directly inputting high-resolution images into the image encoder is not feasible. This results in an excessive number of output tokens, surpassing the maximum token count of the LLM, and also significantly slows down the inference speed.\nTo address this issue, we propose the Spatial-Enhanced Local Feature Mining algorithm. Specifically, similar to typical multimodal large language models, we input low-resolution images into CLIP-ViT (Radford et al., 2021), obtaining a relatively small number of tokens. Next, we input high-resolution images into ConvNeXt (Liu et al., 2022), ensuring that even small and distant objects are clearly visible and a sufficient number of pixels can be processed. Furthermore, compared to the self-attention (Vaswani et al., 2017) mechanism in ViT (Dosovitskiy et al., 2021), the convolutional layers have a stronger ability to extract local features. This enhances the extraction of local object features, thereby improving the model's ability to identify small objects in the image.\nIn detail, the local-enhanced features $F_{local}$ generated by ConvNext are then divided into two branches: the image RGB feature branch and the spatial depth feature branch. The two branches are both several convolution layers. And we get the spatial feature $F_{spatial}$ and the local RGB feature $F_{rgb}$:\n$$F_{spatial} = Conv_{spatial}(F_{local}) F_{rgb} = Conv_{rgb}(F_{local}) \\tag{1}$$\nThen, we predict the object level depth map (Huang et al., 2022) from the spatial feature:\n$$M_{depth} = Conv_{depth} (F_{spatial}) \\tag{2}$$\nThe spatial depth feature branch is capable of extracting object-level depth features and enhancing the image feature branch for extracting local object features. We use the depth map ground truth and the L1 loss to supervise the object-level depth map.\nNext, we add the image RGB features and spatial depth features to derive the Spatial-Enhanced Local Feature $F_{spatial-local}$:\n$$F_{spatial-local} = F_{spatial} + F_{rgb} \\tag{3}$$\nSubsequently, we use the global feature tokens $T_{vit}$ obtained from the ViT to mine the Spatial-Enhanced Local Feature $F_{spatial-local}$ from the CNN adaptively. The mining process enables the input to the LLM with fewer tokens while ensuring these tokens contain enough local and spatial features. Specifically, we employ a spatial-enhanced cross-branch attention mechanism. We utilize the global feature tokens $T_{vit}$ generated by the ViT as the query, while the Spatial-Enhanced Local Feature $F_{spatial-local}$ serves as both the key and value. Concretely, We partition $T_{vit}$ along the token length dimension into multiple queries. We partition the Spatial-Enhanced Local Features based on the height and width dimensions for use as multiple keys and values.\n$Q=T_{vit} \\times W_Q \\quad K=F_{spatial-local} \\times W_K \\quad V = F_{spatial-local} \\times W_v \\tag{4}$$\n$T = Softmax(Q K^T /\\sqrt{d_k}) V \\tag{5}$$\nFinally, we obtain the spatially localized enhanced token T, and we use T as input for the large language model. The proposed spatial enhanced cross-branch attention mechanism enables the tokens extracted by ViT to attend to their respective regions of interest within the image. This improves the alignment between ViT tokens and the localized features of the image."}, {"title": "3.3 3D Query Token-Derived Info Decoding", "content": "When multimodal large language models handle visual perception tasks, they typically output coordinates in the form of text tokens (Wang et al., 2023b,a) or discrete coordinate bin (Peng et al., 2023; Wang et al., 2022). To accomplish 3D grounding tasks, a straightforward approach would be to output the object's 3D spatial position in text format, including coordinates such as x, y, z, length, width, height, and rotation.\nHowever, this text-based output approach has significant issues: 1. Low Speed: Within the LLM vocabulary table, like LLaMa (Touvron et al., 2023), each digit from 0-9 is typically a separate token. For example, outputting a value of 52.3 requires four tokens (three digits and a decimal point). For 3D detection results, if the object's x, y, z coordinates, dimensions, and Euler angles for rotation are each output as text, this could require approximately 40-50 tokens. In a decoder-only LLM, tokens are generated one at a time, resulting in low output speed. 2. Poor Accuracy: Compared to LLMs' common sense and knowledge, they are less adept at handling numbers and mathematics. Existing LLMs usually have poor numerical reasoning capabilities, resulting in significant errors when decimal coordinates are output as text. For example, LLMs typically fail to accurately comprehend the mathematical meaning of pitch, roll, and yaw Euler angles for object rotation. Outputting three Euler angles as text poses a significant challenge for LLMs. 3. Parsing Complexity: 3D detection results are complex, involving at least nine degrees of freedom, including object x, y, z coordinates, length, width, height, and three Euler angles. It is challenging to force LLMs to output these nine values in a standard format. LLMs often output too many or too few numbers or do not follow the standard format, leading to frequent anomalies in parsing results from text.\nTo address the above poor text-based geometric numerical output issues, we propose a 3D query token-derived info decoding method.\nSpecifically, in the input tokens of the large language model, in addition to the image and text tokens, we introduce a 3D query token. The 3D Query is a set of learnable parameters that have the same dimension as the hidden layer features of the LLM. In the input part of LLM, the word embedding is replaced with the 3D Query. The purpose of the 3D Query is to extract spatial information from the hidden features of the LLM. Through adaptive learning, the 3D query, acting as the query (Q) in the attention mechanism, can effectively extract image and text 3D information from self-attention. Through adaptive learning, only one 3D query token is needed for this task. After processing the 3D query token through the LLM, the final hidden feature of the token is employed as the 3D feature $F_{3D}$.\nAdditionally, to determine when to use the 3D query token, we introduce a special \\token. The \\token is placed before the 3D query in the sequence. When the \\token is detected in the LLM's output, the subsequent next input token is replaced with the learnable 3D query token.\nUnlike directly using LLM's text output to determine the object's spatial position, we employ an extra regression head to output the object's spatial position. Specifically, after obtaining the 3D Feature $F_{3D}$, we regress the object's 3D center projection on the image $P_{img}$, depth $d_l$, 3D size (length l, width w, height h), and rotation angles.\nSpecifically, for the object's center, we do not use the center of the 2D bounding box. Instead, we utilize the projection point of the object's 3D center onto the image, as the 3D center is more suitable for the subsequent geometric inverse projection process.\nWe normalize the width and height of the image to the range [0, 1], using an MLP to predict the relative position of the 3D center's projection point on the image, denoted as $P_{norm} = (U_{norm}, V_{norm})$:\n$U_{norm}, V_{norm} = MLP_{uv} (F_{3D}) \\tag{6}$$\nAnd the actual position of the 3D center projection point on the image: p = (u,v):\n$u = U_{norm} \\times W \\quad v = V_{norm} \\times h \\tag{7}$$\nwhere w, h are the image's width and height.\nFor the 3D size of objects in 3D space: length L, width W, and height H, we use an MLP to predict these dimensions:\n$L,W,H = MLPLWH (F_{3D}) \\tag{8}$$\nFor the depth $d_l$, we similarly use an MLP to predict this value:\n$d_l = MLPd(F_{3D}) \\tag{9}$$\nFor the rotation of objects in space, previous works (Cho et al., 2024b) directly predicted the Euler angles of objects. However, using neural networks to predict Euler angles directly poses several issues: 1. Discontinuity Issues: Euler angle representation is prone to singularities, also known as gimbal lock. This occurs when certain angles approach specific values, leading to indistinguishability between the angles, causing numerical instability and making it difficult for the model to learn. 2. Non-Uniqueness of Representation: Each rotation can have multiple Euler angle representations, leading to the multiple solutions problem. This non-uniqueness increases error and makes it challenging for the model to converge during training. 3. Complexity of Loss Function: Using Euler angles as outputs requires considering the periodicity and ambiguity of angles when calculating the loss, complicating the design of the loss function. 4. Asymmetry Issues: The range and symmetry of Euler angle components differ, making some angles more influential on the results than others. Neural networks may become more sensitive to certain angles while being less sensitive to others, affecting accuracy.\nPrevious works (Lu et al., 2021; Shi et al., 2021) focused on outdoor autonomous driving datasets, and they neglected the pitch and roll angles. They only predicted yaw angle, so the disadvantages of Euler angles were not apparent. However, in indoor datasets, such as SUNRGBD (Song et al., 2015) and Objectron (Ahmadyan et al., 2021) datasets, all three Euler angles (pitch, roll, yaw) are significant and impactful. The aforementioned issues of Euler angles become very apparent. Similarly, quaternions are also discontinuous and difficult for neural networks to learn (Zhou et al., 2019).\nTo address these issues, we predict the 6D allocentric rotation (Zhou et al., 2019), which is continuous in 6D space and more suitable for learning. Specifically, we use an MLP to predict the object's 6D allocentric rotation representation:\n$Rot6D = MLP6D(F_{3D}) \\tag{10}$$\nWe then convert the 6D rotation representation into a 3 x 3 rotation matrix:\n$Rot = rotation_6d_to_matrix(F_{3D}) \\tag{11}$$\nIn this section, we employ the learnable 3D query to extract 3D features from the large language model. Subsequently, we use 3D heads to regress the geometric attributes, including objects' image projection point, depth, dimensions (length, width, height), and rotation. We will elaborate on the reasoning of the object's 3D bounding box, particularly the inference of the object's 3D spatial position X, Y, and Z, in Section 3.4."}, {"title": "3.4 Geometry Projection-Based 3D Reasoning", "content": "The objective of 3D grounding is to output the object's 3D bounding box in space", "equation": "n$Z^i[x^i", "y^i,1": "T = K^i[X^i", "Z^i": "T \\tag{12"}, "n$K^i = \\begin{bmatrix} f_x^i & 0 & c_x^i \\\\ 0 & f_y^i & c_y^i \\\\ 0 & 0 & 1 \\end{bmatrix} \\tag{13}$$\nwhere $f_x^i, f_y^i, c_x^i, c_y^i$ are all the intrinsics of the real camera $C^i$. From Equations 12 and 13, we derive:\n$x^iZ^i = f_x^iX^i + c_x^i Z^i \\tag{14}$$\nWe assume the virtual camera intrinsic matrix is $K^v$ and image width is $w^v$. The point $P^v$ in the virtual camera differs from the point $P^i$ in the real camera only in the Z-coordinate, with the X and Y coordinates being the same. Therefore, we can denote $P^v$ as $P^v = (X^i,Y^i,Z^v)$. The projection of $P^v = (X, Y, Z)$ onto the image is denoted as $p^v = (x^v, y^v)$:\n$Z^v[x^v,y^v,1"], "derive": "n$x^vZ^v = f_x^vX^i +c_x^v Z^v \\tag{17"}, {"17": "n$\\frac{x^i"}, {"tag{19}$$\nThus": "n$x^i = f_x^v \\frac{X^i"}, {"obtain": "n$(f_x^v \\frac{X^i"}, {"depth": "n$Z^v = \\frac{f_x^v w^i}{f_x^i w^v} Z \\tag{22}$$\nwhere $f_x^i, f_x^v$ are the focal length of the real camera and virtual camera, respectively. $w^i, w^v$ are"}]