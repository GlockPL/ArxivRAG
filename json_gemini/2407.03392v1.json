{"title": "M5: A Whole Genome Bacterial Encoder at Single Nucleotide Resolution", "authors": ["Agust Egilsson"], "abstract": "A linear attention mechanism is described to extend the context length of an encoder only transformer, called M5 in this report, to a multi-million single nucleotide resolution foundation model pretrained on bacterial whole genomes. The linear attention mechanism used approximates a full quadratic attention mechanism tightly and has a simple and lightweight implementation for the use case when the key-query embedding dimensionality is low. The M5-small model is entirely trained and tested on one A100 GPU with 40gb of memory up to 196K nucleotides during training and 2M nucleotides during testing. We test the performance of the M5-small model and record notable improvements in performance as whole genome bacterial sequence lengths are increased as well as demonstrating the stability of the full multi-head attention approximation used as sequence length is increased.", "sections": [{"title": "1 Introduction", "content": "LLMs that incorporate DNA or RNA sequences need to successfully combine two factors: the extreme length of DNA sequences and the simplicity of the 4 main tokens involved. The most common way to combine these two factors is to use tokens representing k-mers to combine multiple nucleotides into a single token. Alternative approaches use single nucleotide resolution, i.e., k=1, as is the case with M5. The approach reported on here includes adjusting the architecture of the LL model to facilitate long sequences, directly taking advantages of the small token set and letting the model itself learn and optimize position encodings.\nDNA and RNA segments, phased DNA and DNA in whole genomes represent a challenge for LLMs due to LLMs limitations on input context length and due to sub-optimal recall observed in many non-quadratic attention approaches, see [7]. Recently, benchmarks have been introduced for comparing the performance of the various LLMs and models applied to DNA and RNA sequences, see [10], [8], reflecting and providing an overview of some of the current approaches of"}, {"title": "2 Background", "content": "measuring the performance of LLMs when specialized to DNA/RNA and applied to ultra long sequences.\nFor bacteria the genome segments are relatively short, ranging up to approximately 14 million nucleotides with most bacterial genomes below 5 million nucleotides, see [6] and [11]. M5 is an LLM designed to work with genomes in this size range at a single nucleotide resolution. It explores using a high number of attention heads, small key-query dimensionality, a family of approximations to the exponential function to linearize the attention mechanism and learned neighborhood based position embeddings. We hypothesize that this is an efficient way to work with DNA and RNA sequences at a single nucleotide resolution.\nFor testing we use M5-small, a linear encoder, progressively pretrained using masking of WGS segments of up to 196,608 nucleotides sampled from bacterial genomes. We report performance of testing the M5-small network on input segments of up to 2 million nucleotides using a single A100-40gb GPU both for the training and testing."}, {"title": "3 Methods", "content": null}, {"title": "3.1 Linear attention used by M5", "content": "We outline the linear attention mechanism used by the M5 transformer encoder. Starting with a given key-query embedding dimension $d = d_k$ we approximate the value $\\exp(q \\circ k + m)$, where $\\circ$ denotes the dot product of two vectors $q, k \\in \\mathbb{R}^{d_k}$ and $m$ is a number possibly depending on the $q$ and $k$ vector domains, using nonlinear transformations $\\Theta_m$, such that\n$\\exp(q \\circ k + m) \\approx \\Theta_m(q) \\circ \\Phi(k)$.\nThis approximation of $\\exp(q \\circ k + m)$ can be made as exact as needed at the expense of additional compute on a given bounded interval using the fact that polynomials may be used to uniformly approximate the exponential function on a given bounded interval. The motivation of including the value $m$ in the formula is to direct the theoretical input $q \\circ k + m$ into a bounded from below interval, with almost all of the values falling into a given bounded region where the approximation holds well.\nFirst let's start with any given polynomial approximation,\n$\\exp(x) \\approx \\sum_{i=0}^{n} a_i x^i$.\nIf $x = q \\circ k + m$ then rewriting this polynomial approximation is straightforward using the binomial formula\n$\\sum_{i=0}^{n} a_i (q \\circ k + m)^i = \\sum_{i=0}^{n} \\sum_{j=0}^{i} \\binom{i}{j} a_i m^{i-j} (q \\circ k)^j$. (1)\nUsing $(q \\circ k)^j = (\\sum_{l=1}^{d} q_l k_l)^j$; we obtain the formulation\n$(\\sum_{l=1}^{d} q_l k_l)^j = \\sum_{l_1,...,l_j} q_{l_1} k_{l_1} ... q_{l_j} k_{l_j} = \\sum_{l_1,...,l_j} q_{l_1} ... q_{l_j} k_{l_1} ... k_{l_j}$ (2)\nwhere we are using dot product to combine the direct sums in (2). Combining (1) and (2) we obtain the expression\n$\\sum_{i=0}^{n} a_i (q \\circ k + m)^i = \\sum_{j=0}^{n} \\sum_{l_1,...,l_j} C_{m,l_1 ... l_j} q_{l_1}... q_{l_j} k_{l_1} ... k_{l_j}$ (3)\nwith\n$C_{m,l_1 ... l_j} = \\sum_{i=j}^{n} \\binom{i}{j} a_i m^{i-j}$. (4)\nThis allows us to write (1) as $\\Theta_m(q) \\circ \\Phi(k)$, i.e.,\n$\\exp(q \\circ k + m) \\approx \\Theta_m(q) \\circ \\Phi(k)$ (5)"}, {"title": null, "content": "with\n$\\Theta_m(x) = C_m + \\sum_{i_1} C_{m,i_1} x_{i_1} + \\sum_{i_1,i_2} C_{m,i_1 i_2} x_{i_1} x_{i_2} + ... + \\sum_{i_1...i_n} C_{m,i_1... i_n} x_{i_1}... x_{i_n}$ (6)\nand\n$\\Phi(x) = 1 + \\sum_{i_1} x_{i_1} + \\sum_{i_1,i_2} x_{i_1} x_{i_2} + ... + \\sum_{i_1...i_n} x_{i_1}... x_{i_n}$ (7)\nInstead of starting with a polynomial approximation of $\\exp(x)$ we could equivalently start with an approximation of $\\exp(x/\\sqrt{d_k})$ since the softmax is normally scaled using $\\sqrt{d_k}$, e.g., in the groundbreaking \"Attention Is All You Need\" paper [17]. So we assume, in the below, that we are using a polynomial approximation\n$\\exp(x/\\sqrt{d_k}) = \\sum_{i=0}^{n} a_i x^i$ (8)\nThe difficulty with $\\Theta_m$ and $\\Phi$ is that these map $\\mathbb{R}^{d_k}$ to $\\mathbb{R}^{1+d+d^2+...+d^n}$, so the dimensionality grows fast with $n$ and $d$ and the maps quickly become infeasible in any practical setting for large values of $n$ or $d$. On the one hand, $d_k$ can be reduced by mapping linearly the query and key projections to a smaller dimensional space, as in [2]. On the other hand, we can try to let $n$ be small, e.g., 3, and the key-query dimension $d = d_k$ be small also, say less than 10 and use a relatively high number of \"heads\" in a multi-head attention encoder. For a very small vocabulary, such as DNA at nucleotide resolution, we evaluate if this is a reasonable approach or not."}, {"title": "3.1.1 Example", "content": "Empirically, if we are using $n = 3, d_k = 4$ and\n$(\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3) = (1.0017636, 0.49488056, 0.12190779, 0.02954964)$\nwe obtain an excellent approximation to $\\exp(x/\\sqrt{d_k})$ on the interval [-1, 2], see Fig. 1. The values of $(\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3)$ are chosen so that the integral of the squared difference between the polynomial and the scaled exponential function are minimized on the interval [0, 2], giving a total area under the squared difference of 9.5E-71 over the interval.\nAssuming we have a triad composed of query, key and value projections $Q \\in \\mathbb{R}^{N \\times d_k}$, $K \\in \\mathbb{R}^{N \\times d_k}$ and $V \\in \\mathbb{R}^{N \\times d_v}$ we can, using the above, approximate a single row, $v_i$, in the attention operator from [17] given as\nAttention(Q, K, V) = $\\text{softmax}(\\frac{QK^t}{\\sqrt{d_k}})V$ (9)\nusing the formula\nv_i \\approx \\frac{\\Theta_m(q_i) K v}{\\Theta_m(q_i) K} (10)"}, {"title": null, "content": "with\n$K_v = \\sum_{j=1}^{N} \\Phi(k_j) v_j$ and $K = \\sum_{j=1}^{N} \\Phi(k_j)$ (11)\nhere we are using the vector to matrix convention $\\mathbb{R}^n = \\mathbb{R}^{n \\times 1}$ to translate from dot products to matrix operations. Note that approximation (10) can be made arbitrarily precise on a given closed interval. The softmax function is invariant under constant shifts in the input logits, i.e., if $f_i(x_1,...,x_n) = e^{x_i} / \\sum_j e^{x_j}$ then $f_i(x_1+ m,...,x_n + m) = f_i(x_1,...,x_n)$. Since the softmax normalizes each row in the hypothetical attention matrix then we can pick $m = m_i$ to be given by different values for each input position i in (10).\nHere we let\n$m_i = ||q_i||_2 \\max_j(||k_j||_2)$ (12)\nthis formula guarantees that $m_i \\geq q_i \\circ k_j$ for all $j \\in \\{1,..., N\\}$ and hence this allows us to assume that $x = q \\circ k + m$ in approximation (8) is always non-negative. Note that we never calculate $q_i \\circ k_j$ directly for all i and j since that would require $N^2$ compute. We can however, use the inequality $-m_i \\leq q_i \\circ k_j \\leq m_i$ and monitor $m_i$ during training to map out if our assumptions, regarding the interval for where approximation (8) holds, are well-founded. To summarize, we have defined a family of transformations $\\Theta_{m_i}$\n$\\Theta_{m_i}: \\mathbb{R}^{d_k} \\rightarrow \\mathbb{R}^{\\frac{d^{n+1}-1}{d-1}}$ for $i \\in \\{1, ..., N\\}$ and $d = d_k$ (13)\nwhere N is the encoder context length such that attention mechanism (9) may be linearized up to arbitrary precision using\n\\exp(\\frac{q_i \\circ k_j + M_i}{\\sqrt{d_k}}) \\approx \\Theta_{m_i}(q_i) \\Phi(k_j)$ (14)"}, {"title": null, "content": "and where approximation (14), for each i, is no worse than $\\epsilon_i \\geq 0$ with\n$\\epsilon_i \\leq \\max_{0<x<2m_i} |\\exp(\\frac{x}{\\sqrt{d_k}}) - \\sum_{k=0}^{n} a_k x^k |$ (15)"}, {"title": "3.2 Compute required by the M5 attention mechanism", "content": "In this section we approximate the exponential function with a polynomial of degree n and evaluate the compute required by the attention mechanism (10) over context length N. Calculating (10) for all i = 1,..., N requires compute on the order of\n$\\sim dv d_k^n N$ (16)\nwhile computing quadratic attention requires compute on the order of\n$(d_k + d_v)N^2$.\nAssuming that $d_k << d_v$ we therefore obtain a speedup ratio of\n$\\sim \\frac{(d_v + d_k)N^2}{dv d_k^n N} \\sim \\frac{N}{d_k^n}$ (17)\nIf we let $d_k = d_{\\text{model}}/h$ where h is the number of heads, then the compute speedup or slowdown translates to\n$\\sim \\frac{h^n N}{d_{\\text{model}}^n}$ (18)\nAccordingly, we focus on models with a large number of heads, moderate model dimension and small values for $d_k$ such as 2,3 or 4. Note that the relationship $d_k = d_{\\text{model}}/h$ is not a requirement for attention to work and we may deviate from it and rely on (17) instead."}, {"title": "3.3 Position embeddings", "content": "Training is performed on continuous slices of bacterial genomes with a repeated fixed token between individual chromosomes or plasmids. A learned position embedding is added to each nucleotide representing the genome segment, numbered from 1 to 4, the idea being to teach the network that the bacterial genome is made up of individual separated chromosomes and plasmids. We use a multi-layer learned convolutional network with max pool activations to map the neighborhood of each nucleotide to a position embedding that is then added to the nucleotide embedding (A,C,G,T, unknown, separator or mask). This is because we think of a position in the genome as being almost uniquely determined by its neighboring DNA sequence, hence we train a network to learn the position of a center nucleotide from its neighborhood. We set the neighborhood length to be 1024 nucleotides. The artifacts (RNA and proteins) produced by the bacterial genome during transcription and translation interact in the cell environment so"}, {"title": null, "content": "interaction between elements should to a large degree be determined by the localized DNA sequence of each element instead of the absolute or relative position of tokens within the sequence.\nFor a future addition to this report, we include, the ability to switch on rotary positional embedding in the M5 model setting, see [15] for a discussion of the relative positional rotary encoding. The rotary positional embeddings are for the purpose of the current first version of this report switched off."}, {"title": "3.4 Network training", "content": "M5 is a transformer encoder only. It is trained using masking, see [5], 12% of known tokens are masked during training, 3% are included as-is into the prediction. In addition to this a large continuous segment of random length up to a maximum of 4096 nucleotides, capped at 15% of the context length, is hidden during training by replacing it with the mask token. The context length is increased gradually starting with context length of only 1024 and then doubled between sessions. Each session initializes the Adam learning rate, adjusts it to a new batch size, when needed, and uses a cosine learning schedule with warm-up. Weight-decay is used only for the the key-query transformation weight matrices and the gradients are clipped individually at 0.05. The key-query attention matrices (weights) are regulated using a small l2 weight decay factor in some of the training runs in order to guarantee that approximation (8) holds sufficiently well."}, {"title": "3.5 M5-small training data", "content": "We collected bacterial whole genome data from GTDB (Genome Taxonomy DataBase), see [14], release 09-RS220 from April 24th, 2024. After processing the genomes downloaded and after scanning for keywords like \"complete genome\" and \"complete sequence\" in a description field we obtained approximately 7,000 whole genomes that we randomly divided into 80%-10%-10% train, eval and test split. The total number of genomes downloaded from GTDB is much larger than the ~ 7,000 satisfying our criteria. For the purpose of evaluating the M5-small model architecture and evaluating its performance we used only the complete genomes as indicated by the aforementioned keywords inserted by the submitters of the data."}, {"title": "3.6 Network architecture", "content": "The original encoder found in [5] and [17] is our staring point. As explained in section 3.1 the attention mechanism is linearized by approximating the exponential function with a family of asymmetric kernel function pairs - one function per input position/token. The position embeddings come from a simultaneously learned CNN network and the top classification layer is similarly a multi-layer CNN network that is allowed to see a range of neighboring tokens before making a prediction. Layer normalization is applied after positional encodings have"}, {"title": null, "content": "been added and layer normalization is used as the last step of each multi-head attention repeat. Additional skip connections are used and all skip connections are initialized so that the residual signal starts out as zero, this is achieved by initializing one sub-layer within each residual block with zeros only and is inspired by the logic found here [3] used to reduce the reliance on layer normalization in both cases."}, {"title": "4 Results", "content": null}, {"title": "4.1 Small M5 model", "content": "The M5-small encoder is trained starting with a context length of 1,024 and then the context length is increased (normally doubled) between sessions until it reaches 196,608 nucleotides during training. For the initial small setup we use a key-query dimension of 4, $d_k = 4$ and linearize the attention mechanism with a polynomial of degree 3, see 3.1 for the details. We train and test the model using only one A100 40gb GPU. For testing we use bacterial whole genome sequence segments of length up to 2,000,000 nucleotides. Everything, both testing and training is done at a single nucleotide resolution using only the 4 tokens A,C,G,T plus 3 meta-tokens (unknown, masking and a token representing both the separation of chromosomes and plasmids). Batch size is reduced from 16 to 1 due to memory restrictions as context length is increased and the learning rate schedule is modified as batch size is decreased or increased."}, {"title": "4.2 Improved predictions as context length increases", "content": "Figure 2 demonstrates, using cross-entropy measure, that there is a clear benefit of training on long sequences when it comes to inferencing about the bacterial genome. The figure shows the mean sample cross-entropy, on the y-axis, obtained using models trained on various sequence lengths, as detailed below, using the 10% holdout test set. Cross-entropy is computed using the masked nucleotides, and measures how confidently the model on average is able to reconstruct the masked nucleotides, with zero representing complete confidence. As explained in section 3.4 12% of the nucleotides are masked, 3% are unchanged and included in the cross-entropy test compute. Additionally, during training a uniform random length continuous segment of up to a maximum of 15% of the context length, capped at 4,096 nucleotides, is completely cleared of all nucleotide information. Plot A (red) shows results from multiple models trained progressively on whole genome bacterial segments ranging from 1,024 to 196,608 nucleotides and then tested on segments equalling the longest training segment. Cross-entropy is reported for the model trained up to the given testing context input length shown on the x-axis. It is apparent from plot A how the performance of our models improves steadily when trained on longer segments. Plot B (green) shows the cross-entropy of a single model that is trained on nucleotide segments of length 1,024 only and then applied to longer DNA segments during"}, {"title": null, "content": "the testing shown. Interestingly, when the context length is increased during inference the performance of this model does not appear to improve much if any. Plot C (blue) shows the sample cross-entropy measured using a single model trained on, up to, bacterial whole genome sequence lengths of 196,608 nucleotides and then tested on various shorter and longer input context lengths shown on the x-axis. Even when this model is tested on the smallest input context length of 1,024 it by far outperforms the model trained explicitly only on that short sequence length. Combined, plots A, B and C show that training a model on long bacterial nucleotide sequences has clear benefits for predicting, using the M5-small model setup, when inferencing about shorter bacterial sequence segments as well as being beneficial for inferencing on longer sequences.\nFigure 3 demonstrates, using prediction accuracy of masked nucleotides and short regions, the benefit of training on long sequences. The figure shows the accuracy of randomly masked (12% masked + 3% original) nucleotide predictions using the weights from models trained on bacterial segments of up to 196,608 bases. The accuracy is measured on the 10% holdout set of the bacterial genomes collected as before. Plot A (red) shows the single nucleotide model prediction accuracy achieved by models trained progressively up to and tested at the shown input context length (x-axis), i.e., 1,024 - 196,608 nucleotides. Plot B (green) shows the accuracy (SNPAcc) for different input context length used during testing using a single model trained on sequences of up to length 196,608 nucleotides. Plot C (blue) shows the the accuracy (SNPAcc) sampled"}, {"title": null, "content": "on a randomly masked region of uniform random size between 2 and 15 nucleotides using the same encoder trained on sequences of up to length 196,608 nucleotides and tested for various sequence lengths as shown."}, {"title": "4.2.1 Validity of the linear approximation to the softmax", "content": "Here we look at the distribution of \"m\" values that are observed by the M5-small models. As demonstrated by formula (15), for our estimate of the approximation of the exponential function used to linearize the softmax in the M5 transformer encoder, we would like our approximation (8) to hold in the interval [0, 2mi]. Inspired by eyeballing the approximations used in practice if we are using polynomials of odd degree such as the one seen in Figure 1 for $d_k = 4$ we can shift the $m_i$ values by some small constant $\\delta$, say -1 to increase the range where we have a solid approximation.\nFigure 4 shows the distribution of the values $m_i -1$ computed for all the attention heads, input tokens, and repeats for the 10% test set and the M5-small encoder trained and tested using context length of 16,384 using formula (12). For clarity we show the exponential function and the approximation used, demonstrating the tight approximation (8) used to approximate the softmax on all applicable values in the test set. The distribution is impacted by the l2 weight decay factor used to control the growth of the key and query weights (weight matrices), in this case it is set to 10E-5 for the key-query transformation"}, {"title": "5 Discussion and Future Work", "content": "This report is a stepping stone in presenting a model that supports sufficient input context length to train on whole sequence bacterial genomes. We have presented the M5-small LLM foundation model supporting multi-million nucleotides as input during inference and efficiently progressively trained the models on segments, from the bacterial genome, of up to 196K tokens on a single 40gb GPU as well as supporting 2M tokens during inference on the same GPU.\nThe model architecture may be optimized further to allow for larger input length on the same hardware. For example, the number of channels in the convolutional layers may be excessive and further testing will reveal the impact on performance of reducing the number of channels and at the same time the"}]}