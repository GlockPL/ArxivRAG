{"title": "M5: A Whole Genome Bacterial Encoder at Single Nucleotide Resolution", "authors": ["Agust Egilsson"], "abstract": "A linear attention mechanism is described to extend the context length of an encoder only transformer, called M5 in this report, to a multi-million single nucleotide resolution foundation model pretrained on bacterial whole genomes. The linear attention mechanism used approximates a full quadratic attention mechanism tightly and has a simple and lightweight implementation for the use case when the key-query embedding dimensionality is low. The M5-small model is entirely trained and tested on one A100 GPU with 40gb of memory up to 196K nucleotides during training and 2M nucleotides during testing. We test the performance of the M5-small model and record notable improvements in performance as whole genome bacterial sequence lengths are increased as well as demonstrating the stability of the full multi-head attention approximation used as sequence length is increased.", "sections": [{"title": "Introduction", "content": "LLMs that incorporate DNA or RNA sequences need to successfully combine two factors: the extreme length of DNA sequences and the simplicity of the 4 main tokens involved. The most common way to combine these two factors is to use tokens representing k-mers to combine multiple nucleotides into a single token. Alternative approaches use single nucleotide resolution, i.e., k=1, as is the case with M5. The approach reported on here includes adjusting the architecture of the LL model to facilitate long sequences, directly taking advantages of the small token set and letting the model itself learn and optimize position encodings.\nDNA and RNA segments, phased DNA and DNA in whole genomes represent a challenge for LLMs due to LLMs limitations on input context length and due to sub-optimal recall observed in many non-quadratic attention approaches, see [7]. Recently, benchmarks have been introduced for comparing the performance of the various LLMs and models applied to DNA and RNA sequences, see [10], [8], reflecting and providing an overview of some of the current approaches of"}, {"title": "Background", "content": "Multiple papers reference using polynomials and kernel methods to linearize the softmax operators used in multi-head attention, BASED [2] uses a 2nd order Taylor series to approximate the exponential function; [9] by Angelos Katharopoulos et al. discusses polynomial linearization as well as using other kernel functions; Performers [4] approximate attention using a method called \"Fast Attention Via positive Orthogonal Random features approach\" obtaining linear space and time complexity; symmetric and asymmetric kernels are discussed in [16] by Yao Hung Hubert Tsai et al. The ReZero paper [3] uses zero initialized gated residual networks to remove reliance on layer normalization and improve training of extremely deep neural networks. The [19] paper by Michael Zhang et al. discusses using shallow trained feed-forward networks to match softmax attention weights resulting in linear attention networks that preserve the spikiness and monotonic properties of the softmax operator.\nIn the paper [1] Josh Alman and Zhao Song provide theoretical arguments and proofs, including using the prerequisite of connecting the key-query dimension $d_k$ to the input length $N$, i.e., number of nucleotides. Inspired by the paper, and as a future direction of research, we consider the relationship $d_k \\sim log(N)$ in our full M5 report.\nLong context length models in biology that use single nucleotide resolution include EVO [13] at 131Kb at the time of the publication and HyenaDNA at 1 million nucleotides, [12]. HyenaDNA swaps out attention for the \"Hyena operator\" and EVO uses the \"StripedHyena\" architecture. Both of the models are pre-trained using the next token prediction strategy, i.e., autoregressively."}, {"title": "Methods", "content": ""}, {"title": "Linear attention used by M5", "content": "We outline the linear attention mechanism used by the M5 transformer encoder. Starting with a given key-query embedding dimension $d = d_k$ we approximate the value $exp(q \\circ k + m)$, where $\\circ$ denotes the dot product of two vectors $q, k \\in R^{d_k}$ and $m$ is a number possibly depending on the $q$ and $k$ vector domains, using nonlinear transformations $\\Theta_m$, such that\n$exp(q \\circ k + m) \\approx \\Theta_m(q) \\circ \\Phi(k)$.\nThis approximation of $exp(q \\circ k + m)$ can be made as exact as needed at the expense of additional compute on a given bounded interval using the fact that polynomials may be used to uniformly approximate the exponential function on a given bounded interval. The motivation of including the value $m$ in the formula is to direct the theoretical input $q \\circ k + m$ into a bounded from below interval, with almost all of the values falling into a given bounded region where the approximation holds well.\nFirst let's start with any given polynomial approximation,\n$exp(x) \\approx \\sum_{i=0}^{n} a_i x^i$.\nIf $x = q \\circ k + m$ then rewriting this polynomial approximation is straightforward using the binomial formula\n$\\sum_{i=0}^{n} a_i (q \\circ k + m)^i = \\sum_{i=0}^{n} \\sum_{j=0}^{i} \\binom{i}{j} a_i m^{i-j} (q \\circ k)^j.$\\tag{1}\nUsing $(q \\circ k)^j = (\\sum_{l=1}^{d} q_l k_l)^j$; we obtain the formulation\n$(\\sum_{l=1}^{d} q_l k_l)^j = \\sum_{l_1,...,l_j} q_{l_1} k_{l_1} ... q_{l_j} k_{l_j} = \\sum_{l_1,...,l_j} q_{l_1} ... q_{l_j} k_{l_1} ... k_{l_j}$\\tag{2}\nwhere we are using dot product to combine the direct sums in (2). Combining (1) and (2) we obtain the expression\n$\\sum_{i=0}^{n} a_i (q \\circ k + m)^i = \\sum_{j=0}^{n} \\sum_{l_1,...,l_j} c_{m_{l_1...l_j}} q_{l_1} ... q_{l_j} k_{l_1} ... k_{l_j}$\\tag{3}\nwith\n$c_{m_{l_1...l_j}} = \\sum_{i=j}^{n} \\binom{i}{j} a_i m^{i-j}$\\tag{4}\nThis allows us to write (1) as $\\Theta_m(q) \\circ \\Phi(k)$, i.e.,\n$exp(q \\circ k + m) \\approx \\Theta_m(q) \\circ \\Phi(k)$"}, {"title": "Example", "content": "Empirically, if we are using $n = 3$, $d_k = 4$ and\n$(\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3) = (1.0017636, 0.49488056, 0.12190779, 0.02954964)$\nwe obtain an excellent approximation to $exp(x/\\sqrt{d_k})$ on the interval [-1, 2], see Fig. 1. The values of $(a_0, a_1, a_2, a_3)$ are chosen so that the integral of the squared difference between the polynomial and the scaled exponential function are minimized on the interval [0, 2], giving a total area under the squared difference of 9.5E-7 over the interval.\nAssuming we have a triad composed of query, key and value projections $Q \\in R^{N \\times d_k}$, $K \\in R^{N \\times d_k}$ and $V \\in R^{N \\times d_v}$ we can, using the above, approximate a single row, $v$, in the attention operator from [17] given as\n$Attention(Q, K, V) = softmax(\\frac{QK^t}{\\sqrt{d_k}})V$\\tag{9}\nusing the formula\n$v \\approx \\frac{\\Theta_m(q_i) Kv}{\\Theta_m(q_i)K}$\\tag{10}\nhere we are using the vector to matrix convention $R^{n} = R^{n \\times 1}$ to translate from dot products to matrix operations. Note that approximation (10) can be made arbitrarily precise on a given closed interval. The softmax function is invariant under constant shifts in the input logits, i.e., if $f_i(x_1,...,x_n) = e^{x_i} / \\sum_j e^{x_j}$ then $f_i(x_1+ m,...,x_n + m) = f_i(x_1,...,x_n)$. Since the softmax normalizes each row in the hypothetical attention matrix then we can pick $m = m_i$ to be given by different values for each input position $i$ in (10).\nHere we let\n$m_i = ||q_i||_2 max(||k_j||_2)$\\tag{12}\nthis formula guarantees that $m_i \\geq q_i \\circ k_j$ for all $j \\in \\{1,..., N\\}$ and hence this allows us to assume that $x = q \\circ k + m$ in approximation (8) is always non-negative. Note that we never calculate $q_i \\circ k_j$ directly for all $i$ and $j$ since that would require $N^2$ compute. We can however, use the inequality $-m_i \\leq q_i \\circ k_j \\leq m_i$ and monitor $m_i$ during training to map out if our assumptions, regarding the interval for where approximation (8) holds, are well-founded. To summarize, we have defined a family of transformations $\\Theta_{m_i}$\n$\\Theta_{m_i}: R^{d_k} \\rightarrow R^{\\sum_{l=0}^{n} d^l}$ for $i \\in \\{1, ..., N\\}$ and $d = d_k$\\tag{13}\nwhere $N$ is the encoder context length such that attention mechanism (9) may be linearized up to arbitrary precision using\n$exp(\\frac{q_i \\circ k_j + M_i}{\\sqrt{d_k}}) \\approx \\Theta_m(q_i) \\Phi(k_j)$\\tag{14}"}, {"title": "Compute required by the M5 attention mechanism", "content": "In this section we approximate the exponential function with a polynomial of degree $n$ and evaluate the compute required by the attention mechanism (10) over context length $N$. Calculating (10) for all $i = 1,..., N$ requires compute on the order of\n$\\sim d_v d_k^n N$\\tag{16}\nwhile computing quadratic attention requires compute on the order of\n$(d_k + d_v)N^2$.\nAssuming that $d_k << d_v$ we therefore obtain a speedup ratio of\n$\\sim \\frac{(d_v + d_k)N^2}{d_v d_k^n N} \\sim \\frac{N}{d_k^n}$\\tag{17}\nIf we let $d_k = d_{model}/h$ where $h$ is the number of heads, then the compute speedup or slowdown translates to\n$\\sim \\frac{h^n N}{d_{model}^n}$\tag{18}\nAccordingly, we focus on models with a large number of heads, moderate model dimension and small values for $d_k$ such as 2,3 or 4. Note that the relationship $d_k = d_{model}/h$ is not a requirement for attention to work and we may deviate from it and rely on (17) instead."}, {"title": "Position embeddings", "content": "Training is performed on continuous slices of bacterial genomes with a repeated fixed token between individual chromosomes or plasmids. A learned position embedding is added to each nucleotide representing the genome segment, num- bered from 1 to 4, the idea being to teach the network that the bacterial genome is made up of individual separated chromosomes and plasmids. We use a multi- layer learned convolutional network with max pool activations to map the neigh- borhood of each nucleotide to a position embedding that is then added to the nucleotide embedding (A,C,G,T, unknown, separator or mask). This is because we think of a position in the genome as being almost uniquely determined by its neighboring DNA sequence, hence we train a network to learn the position of a center nucleotide from its neighborhood. We set the neighborhood length to be 1024 nucleotides. The artifacts (RNA and proteins) produced by the bacterial genome during transcription and translation interact in the cell environment so"}, {"title": "Network training", "content": "M5 is a transformer encoder only. It is trained using masking, see [5], 12% of known tokens are masked during training, 3% are included as-is into the prediction. In addition to this a large continuous segment of random length up to a maximum of 4096 nucleotides, capped at 15% of the context length, is hidden during training by replacing it with the mask token. The context length is increased gradually starting with context length of only 1024 and then doubled between sessions. Each session initializes the Adam learning rate, adjusts it to a new batch size, when needed, and uses a cosine learning schedule with warm-up. Weight-decay is used only for the the key-query transformation weight matrices and the gradients are clipped individually at 0.05. The key-query attention matrices (weights) are regulated using a small 12 weight decay factor in some of the training runs in order to guarantee that approximation (8) holds sufficiently well."}, {"title": "M5-small training data", "content": "We collected bacterial whole genome data from GTDB (Genome Taxonomy DataBase), see [14], release 09-RS220 from April 24th, 2024. After process- ing the genomes downloaded and after scanning for keywords like \"complete genome\" and \"complete sequence\" in a description field we obtained approxi- mately 7,000 whole genomes that we randomly divided into 80%-10%-10% train, eval and test split. The total number of genomes downloaded from GTDB is much larger than the ~ 7,000 satisfying our criteria. For the purpose of evalu- ating the M5-small model architecture and evaluating its performance we used only the complete genomes as indicated by the aforementioned keywords in- serted by the submitters of the data."}, {"title": "Network architecture", "content": "The original encoder found in [5] and [17] is our staring point. As explained in section 3.1 the attention mechanism is linearized by approximating the expo- nential function with a family of asymmetric kernel function pairs - one function per input position/token. The position embeddings come from a simultaneously learned CNN network and the top classification layer is similarly a multi-layer CNN network that is allowed to see a range of neighboring tokens before mak- ing a prediction. Layer normalization is applied after positional encodings have"}, {"title": "Results", "content": ""}, {"title": "Small M5 model", "content": "The M5-small encoder is trained starting with a context length of 1,024 and then the context length is increased (normally doubled) between sessions until it reaches 196,608 nucleotides during training. For the initial small setup we use a key-query dimension of 4, $d_k = 4$ and linearize the attention mechanism with a polynomial of degree 3, see 3.1 for the details. We train and test the model using only one A100 40gb GPU. For testing we use bacterial whole genome sequence segments of length up to 2,000,000 nucleotides. Everything, both testing and training is done at a single nucleotide resolution using only the 4 tokens A,C,G,T plus 3 meta-tokens (unknown, masking and a token representing both the separation of chromosomes and plasmids). Batch size is reduced from 16 to 1 due to memory restrictions as context length is increased and the learning rate schedule is modified as batch size is decreased or increased."}, {"title": "Improved predictions as context length increases", "content": "Figure 2 demonstrates, using cross-entropy measure, that there is a clear bene- fit of training on long sequences when it comes to inferencing about the bacterial genome. The figure shows the mean sample cross-entropy, on the y-axis, ob- tained using models trained on various sequence lengths, as detailed below, using the 10% holdout test set. Cross-entropy is computed using the masked nucleotides, and measures how confidently the model on average is able to recon- struct the masked nucleotides, with zero representing complete confidence. As explained in section 3.4 12% of the nucleotides are masked, 3% are unchanged and included in the cross-entropy test compute. Additionally, during training a uniform random length continuous segment of up to a maximum of 15% of the context length, capped at 4,096 nucleotides, is completely cleared of all nu- cleotide information. Plot A (red) shows results from multiple models trained progressively on whole genome bacterial segments ranging from 1,024 to 196,608 nucleotides and then tested on segments equalling the longest training segment. Cross-entropy is reported for the model trained up to the given testing context input length shown on the x-axis. It is apparent from plot A how the perfor- mance of our models improves steadily when trained on longer segments. Plot B (green) shows the cross-entropy of a single model that is trained on nucleotide segments of length 1,024 only and then applied to longer DNA segments during"}, {"title": "Validity of the linear approximation to the softmax", "content": "Here we look at the distribution of \"m\" values that are observed by the M5-small models. As demonstrated by formula (15), for our estimate of the approximation of the exponential function used to linearize the softmax in the M5 transformer encoder, we would like our approximation (8) to hold in the interval [0, 2$m_i$]. Inspired by eyeballing the approximations used in practice if we are using poly- nomials of odd degree such as the one seen in Figure 1 for $d_k$ = 4 we can shift the $m_i$ values by some small constant $\\delta$, say -1 to increase the range where we have a solid approximation.\nFigure 4 shows the distribution of the values $m_i$ - 1 computed for all the attention heads, input tokens, and repeats for the 10% test set and the M5- small encoder trained and tested using context length of 16,384 using formula (12). For clarity we show the exponential function and the approximation used, demonstrating the tight approximation (8) used to approximate the softmax on all applicable values in the test set. The distribution is impacted by the $l2$ weight decay factor used to control the growth of the key and query weights (weight matrices), in this case it is set to 10E-5 for the key-query transformation"}, {"title": "Discussion and Future Work", "content": "This report is a stepping stone in presenting a model that supports sufficient input context length to train on whole sequence bacterial genomes. We have presented the M5-small LLM foundation model supporting multi-million nu- cleotides as input during inference and efficiently progressively trained the mod- els on segments, from the bacterial genome, of up to 196K tokens on a single 40gb GPU as well as supporting 2M tokens during inference on the same GPU.\nThe model architecture may be optimized further to allow for larger input length on the same hardware. For example, the number of channels in the convolutional layers may be excessive and further testing will reveal the impact on performance of reducing the number of channels and at the same time the"}]}