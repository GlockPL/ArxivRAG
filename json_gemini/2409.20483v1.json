{"title": "RecSys Challenge 2024: Balancing Accuracy and Editorial Values in News Recommendations", "authors": ["Johannes Kruse", "Kasper Lindskow", "Saikishore Kalloori", "Marco Polignano", "Claudio Pomo", "Abhishek Srivastava", "Anshuk Uppal", "Michael Riis Andersen", "Jes Frellsen"], "abstract": "The RecSys Challenge 2024 aims to advance news recommendation by addressing both the technical and normative challenges inherent in designing effective and responsible recommender systems for news publishing. This paper describes the challenge, including its objectives, problem setting, and the dataset provided by the Danish news publishers Ekstra Bladet and JP/Politikens Media Group (\u201cEkstra Bladet\"). The challenge explores the unique aspects of news recommendation, such as modeling user preferences based on behavior, accounting for the influence of the news agenda on user interests, and managing the rapid decay of news items. Additionally, the challenge embraces normative complexities, investigating the effects of recommender systems on news flow and their alignment with editorial values. We summarize the challenge setup, dataset characteristics, and evaluation metrics. Finally, we announce the winners and highlight their contributions. The dataset is available at: https://recsys.eb.dk.", "sections": [{"title": "1 INTRODUCTION", "content": "The RecSys Challenge 2024 focuses on news recommendation, ad- dressing both the technical and normative challenges inherent in designing effective and responsible recommender systems for news publishing. The challenge emphasizes the distinctive aspects of news recommendation, including modeling user preferences based on implicit feedback, accounting for the influence of the news agenda on user interests, and managing the rapid decay of news articles. Furthermore, the challenge includes the evaluation of the news flow effects of different recommender systems using beyond-accuracy metrics compared to a range of traditional edito- rial selection methods for news. We include the beyond-accuracy evaluation because understanding the effects of each recommender system on the news flow is a necessary first step in identifying their potential (mis)alignment with the editorial profile of a news publisher, as well as how their introduction at scale might affect the democratic functions of news publishers in society. These in- sights are essential for publishers, and we hope they will incite further work on beyond-accuracy objectives and metrics to support value alignment, which is an emerging research agenda, see, e.g., [4, 8, 15, 17]."}, {"title": "1.1 Ekstra Bladet & JP/Politikens Media Group", "content": "Ekstra Bladet is a Danish newspaper published by JP/Politikens Me- dia Group in Copenhagen, Denmark. As one of Denmark's leading news sites, it boasts close to a million daily active users and more than 20 million daily impressions. Ekstra Bladet focuses on deliv- ering timely and engaging news content, using innovative digital"}, {"title": "2 THE CHALLENGE TASK AND PROBLEM SETTING", "content": "Personalized news recommendation has become increasingly im- portant in the digital age, where users are overwhelmed with in-formation from various sources [18]. The RecSys '24 Challenge seeks to advance this field, with the primary objective of ranking a set of news articles displayed during an impression from most to least likely for a user to click on. Participants can leverage vari- ous data points, including the user's click history, session details (such as time and device used), user metadata (including gender and age), and the articles content. This typically involves developing recommender systems that capture both the users' interests and the contextual relevance of articles, considering factors such as the articles' content, timeliness, and the users' reading preferences. The candidate articles are ranked, and these rankings are compared to the actual clicks made by users."}, {"title": "2.1 Dataset Description", "content": "The Ekstra Bladet News Recommendation Dataset (EB-NeRD) is a large-scale Danish dataset created by Ekstra Bladet to support ad- vancements and benchmarking in news recommendation research. The dataset comprises 1,103,602 unique users, 125,541 distinct news articles, and a total of 37,966,985 impression logs."}, {"title": "2.1.1 Dataset Construction.", "content": "The dataset was collected from the im- pression logs of active users at Ekstra Bladet\u00b9 over a 6-week period from April 27 to June 8, 2023. Active users were defined as those who had at least 5 and at most 1,000 news click records in a 3-week period from May 18 to June 8, 2023. The lower bound ensures that the users have some level of engagement, while the upper bound helps to exclude abnormal behavior and bots. To protect user privacy, every user was delinked from the production system and securely hashed into an anonymized ID using one-time salt hashing, see, e.g., [13]. Each impression log lists the articles viewed by a user during a visit at a particular time, along with the articles that were clicked. It includes details such as whether the user was on the front page or an article page, the timestamp, time spent, scroll percentage, the device used for browsing (i.e., computer, tablet, or mobile), and the user's subscription status. Users who maintain an account may have addi- tionally disclosed specific personal details like their gender, postal code, and age. Along with the impression logs, the dataset features all the Danish articles present with their titles, abstracts, bodies, and metadata, including categories. Using proprietary models, we have enriched the dataset with entities, topics, and sentiment labels."}, {"title": "2.2 Evaluation Methodology", "content": "We use standard metrics to evaluate the recommendations, i.e., the Area Under the Curve (AUC), Mean Reciprocal Rank (MRR), and normalized Discounted Cumulative Gain (nDCG) [19]. The AUC, which measures how well clicked and not-clicked items are distinguished, is defined as\n$AUC = \\frac{\\sum_{p \\in P} \\sum_{n \\in N} I[p_i > n_j]}{|P||N|}$", "equation": "AUC = \\frac{\\sum_{p \\in P} \\sum_{n \\in N} I[p_i > n_j]}{|P||N|}"}, {"title": null, "content": "where I [.] is an indicator function, P is the set of clicked items, N is the set of not-clicked items, pi is the predicted score for the i-th positive item, and nj is the predicted score for the j-th negative item. Next, the MRR, which evaluates the rank of the first clicked item, is defined as\n$MRR = \\frac{1}{|Q|} \\sum_{i \\in Q} \\frac{1}{Rank_i}$", "equation": "MRR = \\frac{1}{|Q|} \\sum_{i \\in Q} \\frac{1}{Rank_i}"}, {"title": null, "content": "where Q is the set of impressions, and Rank\u012f is the ranking of the first clicked item in the ranked list for the i-th impression. Finally, the nDCG up to position K, which measures the quality of ranking considering the position of clicked items, is defined as\n$nDCG@K = \\frac{\\sum_{j=1}^{K} (2^{r_j} - 1) / log_2(1 + j)}{\\sum_{i=1}^{K} 1 / log_2(1 + i)}$", "equation": "nDCG@K = \\frac{\\sum_{j=1}^{K} (2^{r_j} - 1) / log_2(1 + j)}{\\sum_{i=1}^{K} 1 / log_2(1 + i)}"}, {"title": null, "content": "where rj is the relevance score of the j-th item, with rj = 1 for clicked items and rj = 0 for not-clicked items. The list is ranked such that j = 1 corresponds to the highest-ranked item and j = K to the K-th item (denoted by @K). In the competition, we compute the metrics at the impression level, meaning that each metric is computed for each impression individually, and then the average is taken across all impressions."}, {"title": "2.2.1 Beyond-Accuracy Evaluation.", "content": "To provide a more comprehen- sive understanding of a recommender system's output, we also consider beyond-accuracy objectives that we evaluate with a set of mature metrics, originating from information retrieval research [3, 5, 14] and interpret in the news publishing context, as sum- marized in Table 1. Given a candidate list I of possible items to recommend and a set of users U, the recommender system gener- ates a personalized recommendation list Ru for each user u \u2208 U, where Ru I and Ru > 1. Furthermore, each user is associated with a click history Hu. Following Smyth and McClave [14], we measure the intralist-diversity of a recommendation as the average pairwise distance between items in the list, given by\n$Diversity_u = \\frac{\\sum_{i \\in R_u} \\sum_{j \\in R_u \\{i\\}} dist(i, j)}{R_u (R_u-1)}$", "equation": "Diversity_u = \\frac{\\sum_{i \\in R_u} \\sum_{j \\in R_u \\{i\\}} dist(i, j)}{R_u (R_u-1)}"}, {"title": null, "content": "where dist(i, j) denotes a distance between the i-th and j-th el- ements. For serendipity, we follow the strategy employed by Lu et al. [8] and aggregate the user's click history, then compare its similarity to each ranked article, which we aggregate and average:\n$Serendipity_u = \\frac{\\sum_{i \\in R_u} \\sum_{j \\in H_u} dist(i, j)}{|R_u||H_u|}$", "equation": "Serendipity_u = \\frac{\\sum_{i \\in R_u} \\sum_{j \\in H_u} dist(i, j)}{|R_u||H_u|}"}, {"title": null, "content": "where Hu is the click history for user u. For both diversity and serendipity (Eqs. 4-5), we employ the cosine distance function, representing each article using document embeddings. We define novelty as the negative logarithm of the popularity scores of the recommended items, i.e.,\n$Novelty_u = \\frac{\\sum_{i \\in R_u} log_2(c_i)}{R_u}$", "equation": "Novelty_u = \\frac{\\sum_{i \\in R_u} log_2(c_i)}{R_u}"}, {"title": null, "content": "where ci is the popularity score of the i-th item, calculated in the competition as the item's normalized click count. For coverage, we define it as the fraction of unique articles that appear across all users' recommendation lists, given by\n$Coverage = \\frac{|\\cup_{u \\in U} R_u|}{|I|}$", "equation": "Coverage = \\frac{|\\cup_{u \\in U} R_u|}{|I|}"}, {"title": null, "content": "where I is the candidate list. For the metrics diversity, novelty, and serendipity, we calculate the score for each user's recommenda- tions and report the average across all users. We also examine the distribution of recommended articles across news categories to get an interpretable overview of the content differences for the news flow."}, {"title": "2.2.2 Limitations of Evaluation Framework.", "content": "Many socially relevant effects of news recommendation can only truly be evaluated online over longer periods (e.g., the diversity in the news diet that a rec- ommender system exposes users to or the degree of fragmentation that personalization produces across the population via multiple interactions), which is neither possible with the dataset we provide nor the metrics we include. Thus, the static nature of our candidate list does not fully capture the dynamics in real-world applications. Ultimately, taking into account the normative considerations of news publishing will require formulating goals for specific beyond- accuracy objectives that recommender systems can optimize for in order to align with the specific editorial values of a news publisher."}, {"title": "3 CHALLENGE RESULTS", "content": "This year's challenge saw large participation with 145 teams, 11 of which were from academic institutions, comprising 202 active"}, {"title": "3.1 Ablation Study", "content": "During the competition, it was discovered that some of the features in EB-NeRD contained future information, such as how popular the article was going to be. However, even without these features, participants were able to engineer features that would likely not be available in an online setup. In response, we decided to permit the features, recognizing that imposing restrictions on specific features could be counterproductive. Additionally, enforcing such restric- tions would be challenging. Instead, participants were required to describe their models and strategies in their paper submissions, including reporting results with and without the use of potentially"}, {"title": "3.2 Common Themes", "content": null}, {"title": "3.2.1 Methodology.", "content": "Most solutions leverage ensemble methods, combining various models such as gradient boosting decision trees (using frameworks like LightGBM [6] and CatBoost [12]) and deep learning models (including Transformer-based models [16]). The few teams that experimented with collaborative-filtering-based models found them less effective in the competition, likely due to the severe cold-start problem present in EB-NeRD."}, {"title": "3.2.2 Feature Engineering.", "content": "The dynamic nature of news consump- tion and user interests is extensively explored through the use of metadata, article representations, and temporal dynamics. These efforts often focus on addressing the cold start problem and the time-sensitive relevance of news articles."}, {"title": "3.2.3 Beyond-Accuracy.", "content": "Several papers emphasize the importance of diversity and fairness in recommendations, exploring methods to balance user interests and mitigate biases. The motivation is to deliver a more equitable user experience rather than solely focusing on optimizing offline AUC to achieve the best-performing model."}, {"title": "4 CONCLUSION", "content": "In this paper, we provided a comprehensive overview of the Rec- Sys '24 Challenge, the dataset (EB-NeRD), metrics for performance evaluation, and key points from the winning solutions and their approaches for developing news recommender systems. We dis- cussed some of the issues we, as organizers, encountered during the competition, including data leakage, and how we decided to handle it. Despite being limited, our initial analysis of beyond-accuracy metrics indicates that different recommender systems may have very different effects on the news flow. We hope to enable further discussion of the effects that a recommender system has on the news flow, as we believe this to be of great importance."}]}