{"title": "Explanation is All You Need in Distillation: Mitigating Bias and Shortcut Learning", "authors": ["Pedro R. A. S. Bassi", "Andrea Cavalli", "Sergio Decherchi"], "abstract": "Bias and spurious correlations in data can cause shortcut learning, undermining\nout-of-distribution (OOD) generalization in deep neural networks. Most methods\nrequire unbiased data during training (and/or hyper-parameter tuning) to counteract\nshortcut learning. Here, we propose the use of explanation distillation to hinder\nshortcut learning. The technique does not assume any access to unbiased data,\nand it allows an arbitrarily sized student network to learn the reasons behind the\ndecisions of an unbiased teacher, such as a vision-language model or a network\nprocessing debiased images. We found that it is possible to train a neural network\nwith explanation (e.g by Layer Relevance Propagation, LRP) distillation only,\nand that the technique leads to high resistance to shortcut learning, surpassing\ngroup-invariant learning, explanation background minimization, and alternative\ndistillation techniques. In the COLOURED MNIST dataset, LRP distillation\nachieved 98.2% OOD accuracy, while deep feature distillation and IRM achieved\n92.1% and 60.2%, respectively. In COCO-on-Places, the undesirable generalization\ngap between in-distribution and OOD accuracy is only of 4.4% for LRP distillation,\nwhile the other two techniques present gaps of 15.1% and 52.1%, respectively.", "sections": [{"title": "1 Introduction", "content": "Although deep neural networks (DNNs) achieved super-human capacity in multiple tasks, it is\ndifficult to control what these complex models really learn. Training data commonly presents spurious\ncorrelations or shortcuts, i.e., input features that correlate with classification labels in the training data\ndistribution, but not in other distributions for the same classification task. DNNs that learn decision\nrules highly based on spurious correlations will perform well on standard benchmarks, but generalize\npoorly to out-of-distribution (OOD) data, which normally represent real-world applications. This\nbehavior is called shortcut learning or Clever Hans effect [14]. Shortcut learning is a major obstacle for\nOOD generalization, trustworthy artificial intelligence (AI), and the use of AI in critical applications\n[14]. Explanation techniques (e.g., GradCAM [29] and LRP [4]) can produce heatmaps that display\nhow DNN input elements (e.g., pixels) influenced the model's output. They are commonly used to\ninterpret the reasons behind a DNN's decision, increasing confidence in the model and detecting\nshortcut learning [18, 32, 11]. Additionally, studies proposed minimizing explanation backgrounds\n[27, 7] avoid the shortcut learning caused by spurious correlations in image backgrounds.\nIn this study, we propose explanation distillation as a strategy to create unbiased AI models from\nbiased data. Our technique leverages an assumed existing large and distributionally robust teacher\nmodel to prevent shortcut learning when training a, possibly light-weight, student trained on biased\ndata only. Explanation distillation does not require any unbiased training data for training the student.\nMoreover, we show that distilling Layer Relevance Propagation (LRP) [4] is particularly effective in\navoiding shortcut learning. The only requirement of our method is the existence of a pre-trained and\nunbiased teacher model for the proposed task (e.g., a large vision-language model, like CLIP [26]),\nor the access to a computationally demanding two-stage pipeline, where the first stage debiases the\ndata and the second classifies it [5].\nIn summary, our contributions are: (1) proposing the use of explanation distillation to hinder shortcut\nlearning and improve OOD generalization; (2) proposing the first shortcut learning avoidance\ntechnique that, simultaneously, does not require ground-truth foreground segmentation masks or\naccess to OOD validation data (unlike explanation background minimization), does not require\nlarge models and massive datasets with high data diversity at training time (unlike robust vision-\nlanguage models), and is resistant to spurious correlations that may be unknown, arbitrary positioned,\nand spanning the entire training set (unlike explanation background minimization, group-invariant\nlearning methods, and current debiasing distillation methods); (3) demonstrating empirically that\nDNNs can be fully trained by an explanation distillation loss only; (4) our experiments empirically\nshow that LRP-based explanation distillation can be more resistant to shortcut learning than feature\nand logit distillation, and that it can surpass explanation background minimization and group-invariant\nlearning methods; (5) LRP optimization was recently introduced [7] for explanation background\nminimization, but our work is the first to optimize LRP with distillation; (6) showing that explanation\ndistillation can be assisted by a standard output distillation loss applied to the DNN's last layer only:\nby applying different losses to different layers (a trick we dubbed DL-DL), we avoid the competition\nbetween two opposite optimization objectives, thus improving convergence, avoiding the need for\nhyper-parameter tuning on OOD data, and better ensuring training will not finish in a biased state.\nWe provide code at: https://github.com/PedroRASB/ExplanationDistillation."}, {"title": "2 Related work", "content": "Several works perform explanation background minimization: they train a network for classification\nwhile penalizing its explanation heatmaps' background elements [27, 20, 7, 6]. Here, background\nrefers to DNN input areas that mostly present clutter or shortcuts (background bias), instead of\nimportant features. Hence, explanation background minimization aims to minimize the influence\nof background bias over a classifier. Multiple explanation techniques can be used for explanation\nbackground minimization. Right for the Right Reasons (RRR [27]) employs input gradients (saliency\nmaps)[31], and Guided Attention Inference Network (GAIN) optimizes Grad-CAM. More recently,\nthe Implicit Segmentation Neural Network (ISNet) [7, 6] introduced background minimization using\nLayer-wise Relevance Propagation (LRP)[4] heatmaps. Explanation background minimization comes\nwith a few drawbacks. First, during training, these models require ground-truth semantic segmentation\nmasks that define the training sample's (e.g., images) backgrounds. Second, they need careful tuning\nof the hyper-parameter that balances the classification loss and the loss penalizing explanations.\nThe optimization of this hyper-parameter requires evaluation over OOD data, which is assumed\navailable (or at least simulated [7]). Third, explanation background minimization can only deal\nwith background biases. If training images present spurious correlations in their region of interest\n(e.g., over objects being classified), or if the shortcuts (or their locations) are unknown, one can\nresort to Group-Invariant Learning methods, like distributionally robust optimization (DRO) [28],\nand Invariant Risk Minimization [3]. However, such methods require the preliminary separation of\nthe training data into sub-sets that are not IID and represent data distributions with diverse spurious\ncorrelations (if any). This separation may be complex or even impossible practically, especially when\nspurious correlations are ubiquitous for a given class. For instance, if all digits 2 in a dataset are red,\ngroup learning cannot prevent the DNN from associating 2 to red.\nRecently, large vision-language models, such as CLIP, displayed unprecedented distributional robust-\nness and OOD accuracy, especially in zero-shot inference [26]. Shortcut learning is characterized by\nsubpar OOD generalization [14], indicating that CLIP suffered little shortcut learning. However, this\nquality is not a result of CLIP's architecture or training algorithm, but rather of the large data diversity\nin its training dataset [13]. Instead, if data is biased, language supervision or contrastive learning does\nnot prevent shortcut learning nor produces distributional robustness [13]. Even fine-tuning CLIP\nwith biased data undermines its distributional robustness [26]. Shortcut learning can be prevented by\navoiding training on biased data, and just relying on a zero-shot vision-language model that is known"}, {"title": "3 Methods", "content": "In this work we will indicate as g = G(x, k; P) the explanation g given by the technique G for the\nsample x given the information P (it can include DNN weights, layers input/ouput or both) for the\nclass k. For our discussion we define and distinguish the possible explanation techniques one can\nemploy for distillation namely: attention, feature-level explanations and input-level explanations. For\nattention, we adopt the definition of feature-based spatial attention from Zagoruyko and Komodakis\n[34]. Said $Y_L$ the output of a convolutional layer L, with C channels, height H, and width W,\n$Y_L \\in \\mathbb{R}^{C \\times H \\times W}$, then, a spatial attention map for layer L, call it a\u2081, is:\n$A_L = A(Y_L); A : \\mathbb{R}^{C \\times H \\times W} \\rightarrow \\mathbb{R}^{H \\times W}$    (1)\nwhere $A(\\cdot)$ is a function that projects $y_l$ to the 2D space $\\mathbb{R}^{C \\times H \\times W}$. A standard choice of $A(\\cdot)$, used\nin this study, is the sum of squared values, $a_{L,i,j} = \\sum_{c=1}^C Y_{L,c,i,j}$ [34]. The first implicit assumption\nbehind spatial attention maps is that the value of a neuron's activation ($Y_{L,c,i,j}$) for a given DNN input\nx indicates the neuron's importance for the that input [34]. We dub the second assumption behind\nattention maps the feature alignment assumption, which we define as: a CNN's feature map element\n($Y_{L,c,i,j}$) is mostly influenced by and mostly carries information from the input feature $(x_{i',j'})$ that is\nspatially aligned with it.\nWe define a feature-level explanation technique as a procedure that produces an explanation heatmap,\n$E_L$, whose element $e_{L,c,i,j}$ (or $e_{L,i,j}$) represents the influence of the feature $Y_{L,c,i,j}$ (or of all features\n$Y_{L,c',i,j}$, where $c' \\in [1, C']$) over the DNN output for a selected class k, for a given input x. Unlike\nattention, in Equation 2, we define a feature-level explanation procedure as a function ($E_L(\\cdot)$) of the\nfeature maps $Y_L$ and of the outputs and/or parameters of all layers after L:\n$e_L = E_L (k, Y_L, \\Theta_{L+1,N}, Y_{L+1}, Y_{L+2},\u2026Y_N)$ (2)\nwe represent the parameters of Layers L+1 to N as $\\Theta_{L+1,N}$. The definition of $E_L(\\cdot)$ depends on the\nspecific explanation technique and possibly on the DNN architecture. Grad-CAM is the most popular\nfeature-level explanation technique [29]. As $A(\\cdot)$ in attention, $E_L(\\cdot)$ in Equation 2 has no access to\nthe parameters and outputs of layers prior to L. Hence, feature-level explanations do not take into\naccount the relationship between input features and the deep features yL. Thus, in principle, feature-\nlevel explanations represent the influence of the layer L activations on the DNN outputs. Again,\nthe feature alignment assumption is also necessary for feature-level explanations to represent the\ninfluence of input features on a DNN output. Once more, empirical evidence supports the assumption:"}, {"title": "3.2 Input-level Explanations and LRP", "content": "We define input-level explanations as heatmaps whose element $h_{c,i,j}$ represents the influence of the\ninput feature $X_{c,i,j}$ over the DNN outputs for a chosen class k. Additionally, we define an input level\nexplanation as a function, $H(\\cdot)$, of all the DNN layer's parameters ($\\theta_{0,N}$), output and input:\n$h = H(k, x, \\theta_{0,N}, Y_1, Y_2,\u2026\u2026Y_N)$ (3)\nDiverse techniques measure influence differently. For example, input gradients or saliency maps and\nGradient*Input assume an input feature $x_{c,i,j}$ is more influential if its variation causes a larger change\nin the DNN logit $Y_{N,k}$ [31]. Hence, influence is measured as the gradient of the logit with respect\nto the input, $\\nabla_x Y_{N,k}$. Having access the entire DNN, feature-level explanations can gather context\ninformation and high-level semantics from late layers, and precise spatial information from earlier\nones. Moreover, they do not need to rely on the alignment assumption, since they can back-propagate\na signal until the DNN input [7].\nIn this study, we conduct experiments distilling three types of differentiable and efficient input-level\nexplanations: input gradients ($h = \\nabla_x Y_{N,k}$), Gradient*Input [30] ($h = x \\odot \\nabla_x Y_{N,k}$) and LRP.\nGradient*Input shares the same theoretical principles as input gradients, and were proposed to\nimprove sharpness in sensitivity maps [30, 7]. LRP can start from any DNN logit, $Y_{N,k}$; in the\nresulting LRP heatmap, positive relevance indicates input elements (e.g., pixels) that contributed\nto increase in the selected logit, providing supporting evidence for class, k. Negative relevance\nindicates opposing evidence [4]. We take advantage of LRP in this manuscript as it bears many\nimportant warranties (see Appendix A.4 for further details) which renders it the ideal candidate\nfor distillation. In summary, we employ the so-called LRP-\u025b for two reasons. First, because the\ntechnique is justified by the the Deep Taylor Decomposition (DTD) paradigm [23]. Second, because\nexplanation background minimization, another way of optimizing explanations, converged better and\nresulted in superior background bias resistance for LRP-8, when compared to LRP-0, Gradient*Input\nand input gradients [7, 6]. Equations 4 defines the LRP-\u025b propagation for a network with Lmax\nlayers; the input-level heatmap is then h = $[R_i^{0}]$.\n$R_i^{L} = \\sum_{j} \\frac{a_{ji}W_{jia}}{\\sum_{l}a_{jl}W_{jla} + \\epsilon \\cdot sign(z_j^{L}) }R_j^{L+1}, \\text{ where: } z_j^{L} = \\sum_{i}W_{jia}a_i^{L} \\text{ and } R_j^{max+1} = \\begin{cases} S_{Y_{N,k}}, & \\text{if } i=k \\\\ 0 & \\text{otherwise} \\end{cases}$   (4)\nIn the equations $a_i^{L}$ is the input of a fully-connected layer L, and $z_j^{L}$ is the layer's output before non-\nlinear activation; $w_{jia}$ is the weight connecting the layer's input $a_i^{L}$ to its i-th output, $w_{j}$ represents\nthe i-th bias parameter and $a_i^{L} = 1$. The leftmost equation in 4 is the LRP-\u025b rule for propagating\nrelevance across a fully-connected layer L. $R_i^{L+1}$ is the relevance pertinent to the input of layer\nL+1 ($a_i^{L+1} = ReLU(z_j^{L})$) or, equivalently [4], the relevance of $z_j^{L}$. Equation 4 redistributes $R_i^{L+1}$\nto the inputs of layer L, $a_i^{L}$, calculating their respective relevances, $R_i^{L}$. The \u025b hyper-parameter\nis a small positive constant [23] which improves numerical stability. Also, it reduces heatmap\nnoise and ameliorate the explanations contextualization and coherence [23], by reducing the Taylor\napproximation error in LRP's local approximate Taylor expansions [23], and by absorbing relevance\nthat would have been propagated to weakly activated neurons [7]. Thus, & improves loss convergence\nin explanation background minimization [7]. If we set \u025b = 0, Equation 4 becomes the LRP-0 rule,\nwhich is equivalent to Gradient*Input in layers with only ReLU non-linearities [23, 7]. Although\nEquation 4 considers a fully-connected layer, it is valid for other common layers that can be expressed\nas an equivalent fully-connected layer, such as convolutions, batch normalization, dropout, and\npooling [23, 7]. For the above reasons we employ chiefly LRP in our study and take advantage of the\nfast, simple and flexible LRP-Flex library [6]."}, {"title": "3.3 Explanation Distillation", "content": "Consider the teacher as a previously trained neural network with N layers and frozen parameters\n$\\Theta^{(T)}$. Assume the teacher suffered little shortcut learning, either due to training on a massive and\ndiverse dataset [13] (e.g., CLIP [26]), or due to training on unbiased data. The student network has\nparameters $\\Theta^{(S)}$ and N' layers. Equation 5 defines the loss function, L, for explanation distillation is\na dissimilarity metric, $d(\\cdot, \\cdot)$, between the teacher's explanation, $g^T$, and the student's explanation,\n$g^S$. We recall g indicates a chosen explanation technique and G the operator which produces it,\nconsidering the information P and explained class k (Section 3.1):\n$L = d(g^T, g^S) = d(G^T (x, k; P^T), G^S (x, k; P^S))$ (5)\nL is zero when the teacher and student explanations match. Assuming that the explanations are an\nadequate representation of the contribution of each input feature ($x_i$) to the outputs of a DNN, the\nminimization of L will enforce input features to have the same effect on the teacher and the student. If\nspurious correlation in an image did not contribute to the teacher's output, they will not influence the\nstudent. Diverse dissimilarity metrics $d(\\cdot, \\cdot)$ may be used in explanation distillation. We had success\nwith losses based on cosine similarity and Euclidean distance. However, we empirically observed\nsuperior student accuracy and shortcut learning resistance when using the L1 loss, normalized\naccording to the geometric mean between the L1 norms of the teacher and the student explanations:\n$d(g^T, g^S) =  \\frac{ ||g^T \u2013 g^S||_1 }{\\sqrt{||g^T||_1 ||g^S||_1 }}$ (6)\n$L =  \\frac{1}{M} \\sum_{m=0}^{M-1} d(AvgPool(g^T, 2^m), AvgPool(g^S, 2^m))$ (7)\nWe hypothesize that the L1 loss increases numerical stability, and normalization improves convergence\nwhen the teacher and student explanations begin at diverse scales. To optimize the L we use SGD\nplus momentum, and, for each training sample, x, we randomly choose one class, k, for the teacher\nand student heatmaps to explain. The teacher's highest logit represents the class for which the fully\ntrained network is the most confident. Thus, the corresponding explanation should usually contain\nrelevant information. For this reason, we select k as the teacher's highest logit 50% of times. We\ndo not always select the highest logit, because we also want the student to reject a category for the\nsame reasons that the teacher rejects it. In other words, we also want explanations for loosing classes\nto match. Accordingly, 50% of times we randomly select one of the non-highest logits, giving each\nthe same selection probability, (50/(C \u2212 1))%, for a classification task with C classes). This logit\nselection technique was previously used for explanation background minimization [6]. Explanations\nmay present high-resolution and be noisy [7]. To improve convergence, we implement a pyramidal\nloss: we use average pooling (AvgPool(\u00b7, s), with different kernel sizes (s), to create diverse rescaled\nversions of $g^T$ and $g^S$. Afterward, we apply Equation 6 to each pair of rescaled teacher-student\nheatmaps, and calculate the average loss over all scales, including the original maps, as shown\nin Equation 7. We use M scales, doubling the kernel size for each scale reduction. Empirically,\nwe defined M such that the smallest rescaled maps are 8x8. Average pooling can attenuate high-\nfrequency noise in the explanations and optimizing smaller heatmaps may be easier than optimizing\nhigh-resolution explanations. Therefore, the pyramidal loss creates easier auxiliary optimization\nobjectives to help the convergence of the main one: the distillation of the unscaled explanations.\nIn input-level distillation, we create $g^S = h^S$ and $g^T = h^T$. For the LRP case we use LRP-\u03b5\n(Section 3.2). Diverse & hyper-parameter choices represent different Taylor references in LRP's local\nTaylor expansions [24]. Larger & causes stronger noise attenuation, but too large & may also harm the\nexplanation of important input features [7]. Ideally, we would like the teacher and student heatmaps\nto match for all possible choices of 8, but creating multiple heatmaps per training sample increases\ncomputational costs. Thus, we randomly select \u025b for each training sample as \u025b ~ 10U(0.001,0.01),\nwhere U (0.001, 0.01) is an uniform distribution over an empirically defined interval. For each sample\nx, we use the same & for the teacher and the student. In Input*Gradient distillation, we just define\n$g^T = h^T = \\nabla_x Y_{x^9k,N}$ and $g^S = h^S = \\nabla_x Y_{x^9k, N'}$, in Equation 7. $Y_{x^9k,N}$ and $Y_{x^9k, N'}$ are the teacher and\nthe student explained logits, respectively. For Grad-CAM feature-level explanation distillation, we"}, {"title": "3.3.1 Distilling from Debiased Data", "content": "Equation 5 considers the same input, x, for the teacher and the student, and assumes the existence\nof a pre-trained teacher that can accurately classify our training dataset without being influenced\nby its spurious correlations (e.g., zero-shot CLIP). However, there may be applications for which\nno pre-trained model is available, but where one can remove the dataset's spurious correlations.\nThis debasing process can be too computationally expensive to be used at inference, and a classifier\ntrained on the debiased data may fail to evaluate the original data, due to a large domain gap. This\nis common when debasing represents the use of a large semantic segmenter to remove images'\nbackgrounds [7]. One can train a teacher on debiased data, and distill it to a student that receives the\noriginal data. To do so, during the distillation process, the teacher receives only the debiased data as\ninputs, XDB, and its explanations are produced with respect to them, $G^T (x_{DB}, k; P^T)$. Meanwhile\nthe student receives the standard samples and explanations are produced for them, $G^S (x, k; P^S)$.\nConsidering this change in the definitions of $g^T$ and $g^S$, we observe one can distill using the loss\nfunction in Equation 7. This is the objective of one of our experiments. We only require the\ndebiased ($x_{DB}$) and the original (x) sample to spatially align. For instance background removal or\nbias removal/substitution are debiasing processes that maintain alignment. Explanation distillation\nfrom segmented images (without background) serves the same objective as explanation background\nminimization: improving robustness to background bias. Like the alternative technique, in this\nscenario, we need ground-truth segmentation masks to produce the debiased teacher data, or to\ncreate a segmentator that debias it. Depending on the applications, masks may be automatically\ncreated by pre-trained public universal segmentation models, like Segment Anything [17], Segment\nAnything Medical [22] and DeepMAC [9]. Notably, even when also dealing with background bias,\nexplanation distillation still has advantages over explanation background minimization: it does\nnot require OOD data (or simulated OOD data [6]) for a delicate hyper-parameter tuning process,\nbecause it does not need to balance a standard classification loss, which prompts bias attention,\nand an explanation background minimization loss, which avoids background attention. Also, by\navoiding competing losses, explanation distillation may converge in applications where explanation\nbackground minimization would struggle. In this study, we debias data by substituting images'\nbiased backgrounds by random noise. In preliminary experiments, the noise prompted more teacher\nattention to foreground features than black backgrounds, improving the subsequent distillation\nresults. Distilling from segmented images is equivalent to using a segmentation-classification pipeline\n(segmentation \u2192 background removal \u2192 classification [5]) as the teacher and directly applying\nEquation 5. However, we spare computational resources by not explaining the pipeline's deep\nsegmenter. In summary, when we distill explanations from debiased data, the teacher has no access to\nthe bias and is not influenced by it. Thus, by matching the teacher's explanations, the student learns\nto also ignore spurious correlations."}, {"title": "3.3.2 DL-DL: Different Losses for Different Layers", "content": "Learning to match a complex DNN's (e.g., CLIP) high-resolution explanations is more difficult\nthan just mimicking its logits. One can accelerate and stabilize the convergence of explanation\ndistillation with the aid of a standard output distillation loss, the cross-entropy between the teacher\nand the student softmax predictions [15]. However, since two neural networks can produce the same\npredictions for diverse reasons, a standard distillation loss may foster shortcut learning. Thus, the\nminimization of a linear combination of the explanation distillation and the output distillation losses\nwould require balancing a loss that prompts shortcut learning with one that avoids it. Tuning the\ncombination weights (hyper-parameters) requires OOD validation data. Additionally, competing\nlosses can be detrimental for convergence, and they provide no guarantee on which objective will\nprevail in the long-term. To avoid this pitfall, here we apply different losses for different layers\n(DL-DL) in the neural network, avoiding losses that compete for the control of the same DNN\nparameters. Specifically, we train only the last DNN layer with the standard softmax distillation loss,\nand all remaining layers are trained solely by the explanation distillation loss. DL-DL avoids the need\nto balance competing losses, not requiring OOD validation data. Moreover, DL-DL limits the softmax\ndistillation loss capacity to prompt shortcut learning. The last student layer could try classifying\na sample according to bias, but, since the teacher is unbiased, this strategy would increment the\nexplanation distillation loss. Thus, to counteract this increment, all remaining DNN layers can learn\nto not provide information about the spurious correlations to the last layer. Hence, DL-DL gives\npreference to the unbiased explanation distillation loss, by giving it control over the first DNN layers.\nDL-DL eventually reverts standard loss roles. Normally, softmax distillation (possibly alongside a\ncommon classification loss) is the main loss guiding the student optimization, and additional losses,\nsuch as attention distillation, are auxiliary tools to improve student accuracy [34, 1]. Here, instead,\nthe explanation distillation loss mainly rules the student optimization, and the softmax distillation\nloss just helps its convergence, by keeping logits more stable during training and providing a hint\nabout the teacher's output. To verify that explanations are all we need in distillation, we conduct most\nof our experiments without DL-DL (we employ only the loss 5). We only use DL-DL when distilling\nexplanations from CLIP to a ResNet18, a complicated optimization task."}, {"title": "4 Results", "content": "We evaluate explanation distillation in multiple datasets: COLOURED MNIST 100%, MNIST\nBackground Bias, DogsWithTies and COCO-on-Places. All datasets have synthetically created\nspurious correlations in all training and hold-out validation images. In all scenarios, we compare\nmultiple distillation settings, and, whenever possible, we also compare our methods to group learning\ntechniques and explanation background minimization. As previously done in Ahmed et al. [2], we\nleverage synthetically biased settings to investigate OOD generalization and quantify resistance to\nshortcut learning. Synthetic spurious correlations better ensure that observed OOD generalization\nfails are due to shortcut learning, and not external factors, such as label shift between the training\nand test set[7]. To quantify shortcut learning we evaluate accuracy on three test sets: IID test, which\nhas have the same bias seen in training; OOD test, which does not have the spurious correlations\npresent in the training data; systematic-shift bias, where the correlations between biases and classes\nare systematically changed to fool classifiers moving from training to test [2]. Accordingly, our\nobjective is achieving high accuracy while minimizing the gap between the IID, OOD and shift results.\nAppendix A presents details about training, datasets, and hyper-parameters.\napplicable to a certain experiment, due to the limitations explained in Section 2. ERM stands for\nempirical risk minimization (standard training)."}, {"title": "4.1 COLOURED MNIST 100%", "content": "Here, we consider distillation from an unbiased teacher (Section 3.3). COLOURED MNIST 100%\nis a version of MNIST where each digit is always correlated to a specific color (e.g., 3 is red).\nCOLOURED MNIST is a common dataset for the investigation of group learning techniques [2, 3],\nand colors act as spurious correlations. However, in standard COLOURED MNIST, only part of\nthe training and validation samples are biased. Here, we make it 100% biased. For distillation,\nthe unbiased teacher is a ResNet34 trained on a randomly colored version of MNIST [12], and the\nstudent is a ResNet18, trained on COLOURED MNIST 100%. Our goal is to show that explanation\ndistillation from an unbiased teacher can hinder shortcut learning even when all training data is\nbiased and and biases are over images' foreground, a setting that prevents the use of explanation\nbackground minimization and group-invariant learning. However, we still compare our models to\nGroupDRO[28], IRM[3] and PGI [2], considering DNNs trained by Ahmed et al. [2] in COLOURED\nMNIST with 20% of unbiased images. Here, the IID test set has digits with the same colors as\nthe training samples, the OOD set has digits with unseen colors, and the systematic shift set digits\nhave colors that were previously associated to other digits during training (e.g., the color red is\nassociated to 3 in training and 4 in the shift test set). Columns 2-4 of Table 1 present the test\nresults for COLOURED MNIST. LRP distillation has the highest OOD and shift accuracies, and the\nsmallest generalization gap: accuracy drops 1% from IID to shift. Conversely, feature and output\ndistillation have gaps of 14.8% and 12.7%, respectively. However, these methods are sensible to\nhyper-parameter changes [10], and a reduction in learning rate, from 0.01 to 0.001, would increase\nthese gaps to 40% and 37.5%, respectively. The same learning rate reduction in LRP distillation\nwould change its gap to 0.9%. Hyper-parameter sensitivity is concerning when OOD validation\ndata is unavailable. Even being trained in COLOURED MNIST 80%, group-invariant learning\nconsistently shows larger generalization gaps (41.5% to 59.3%) and smaller OOD and shift accuracies\nthan distillation techniques."}, {"title": "4.2 Dogs WithTies", "content": "Here, we also consider distillation from an unbiased teacher (Section 3.3) with foreground biases.\nHowever, now we use high resolution (224x224) natural images. We created the ad-hoc DogsWithTies\ndataset, a subset of the Stanford Dogs dog breed classification database [16] (all Pekingese and\nTibetan Mastiff photos), where we added ties to the dogs. All Tibetan Mastiffs in training and\nvalidation images wear a red necktie, while all Pekingese dogs wear a purple bow tie. Thus, the\nties are spurious correlations over the images' foreground (the dogs). Here, the unbiased teacher is\nzero-shot CLIP (ResNet50-based), and the student is a ResNet18 initialized from scratch. We aim to\nshow that we can use explanation distillation to learn unbiased decision rules from a large pre-trained\nvision language model, using only biased data. We have spurious correlations in 100% of our\ntraining and hold-out validation images, preventing the use of group learning techniques and standard\nbackground relevance minimization (standard dog segmentation masks would include the bias). Still,\nsince this experiment assumes the availability of CLIP, we compare distillation to GALS[25]. Given\nthe difficulty of distilling from CLIP to ResNet18, in this application we employ DL-DL to improve\ntraining convergence in explanation distillation (Section 3.3.2). The dogs in the IID test set use the\nsame ties as the ones in training, there are no ties in the OOD set, and the Pekingese in the systematic\nshift set use the ties that the Tibetan Mastiffs used in training, and vice-versa. Columns 5-7 of Table\n1 present the test accuracies for DogsWithTies. Explanation distillation techniques showed small\ngeneralization gaps between IID and OOD or shift (<5%). Only is this task, feature distillation does\nnot reach high accuracy in any test scenario."}, {"title": "4.3 COCO-on-Places", "content": "Finally, unlike the previous two applications, here, we do not assume a pre-trained unbiased teacher.\nInstead, we assume that we can debias the training data, in a possibly computationally expensive way\n(Section 3.3.1). COCO-on-Places [2] is a dataset where one must classify objects (extracted from\nMicrosoft COCO [21]), while entire background environments (from Places [36]) are spuriously\ncorrelated to the object classes [2]. Unlike the original COCO-on-Places [2], in this study, 100%\nof the dataset's training and validation images have spurious correlations. Here, our teacher is a\nDenseNet121 that only receives debiased images when trained and during the distillation procedure\n(Section 3.3.1). Debiased images are COCO-on-Places 100% samples where backgrounds were\nsubstituted by random noise. The student is a standard DenseNet121, which trained with unsegmented\nbiased images only. Our objective is to show that explanation distillation can transfer the robustness\nof a large pipeline (foreground segmentation DNN \u2192 background removal \u2192 classifier) to a standard\nclassifier, which does not require debiasing atinference (unlike the teacher). Conversely, if directly\napplied to non-segmented images, the teacher's DenseNet121 accuracy falls from about 70% to about\n15%. Here, we compare distillation techniques to explanation background minimization (RRR[27],\nFaster ISNet[6], GAIN[20]), but notice that OOD validation data (without spurious correlations\nin background) was necessary to set the loss hyper-parameters for these methods. The distillation\ntechniques proposed here, instead, assumed no access to such data. Here, the IID test set represents\nimages where the correlations between objects and backgrounds are the same as in training. The\nOOD test set has random backgrounds from unseen Places categories, and the systematic-shift bias\ndatasets has the same background categories seen in training, but correlations between COCO object\nclasses and Places categories are deceivingly changed. Columns 8-10 of Table 1 presents results for\nCOCO-on-Places. LRP distillation surpassed all other techniques in the OOD and shift test sets. It\npresented the smallest generalization gap: a loss of 4.4% accuracy from the IID to the systematic shift\ntest set. The second-best generalization gap is seen for feature distillation, 15.1%. Meanwhile, all\ngroup-invariant learning techniques present gaps close to 50%. This application considers the largest\nstudent network, and larger networks increase noisiness in input gradients and Gradient*Input[7].\nAccordingly, the distillation of these techniques did not converge well, leading to low accuracy\noverall."}, {"title": "5 Discussion and Conclusions", "content": "Previous works proposed feature/output distillation to avoid shortcut learning, but considered access to\nunbiased training data[19, 33]. These techniques, now assuming only biased data, we found still give\na large improvement over standard training (Empirical Risk Minimization, ERM) in COLOURED\nMNIST and COCO-on-Places (Table 1). However, it performs poorly on DogsWithTies, possibly\nreflecting the difficulty the small ResNet18 encounters when trying to mimic CLIP's features in a\nsmall dataset. CLIP's deep features represent high-level concepts useful for a multitude of tasks[26],\nbut explanations only represent the reasons behind a teacher's decision for a certain class. Thus,\nexplanation distillation conveys only the teacher's knowledge related to the classes and task seen\nat training, allowing the ResNet18 to learn from CLIP more effectively. In all our experiments,\nLRP distillation consistently led to high out-of-distribution accuracy and a small generalization\ngap, surpassing all group-invariant learning and explanation background minimization techniques.\nMoreover, it surpassed the generalization capacity of Gradient*Input and input gradient distillation,\ntechniques that are noisier for DNNs, and it also surpassed Grad-CAM distillation (a feature-level\nexplanation, which depends on the alignment hypothesis) [7]. Overall we found that the standard\nclassification loss, or even distillation losses over outputs, features or feature-level explanations,\nmay all lead to shortcut learning[7]. Here, we do not try to balance biasing losses with a \"debiasing\nloss\", rather, we substitute them by explanation distillation alone, which guides a student network\nto not only mimic its teacher's answers, but to learn the reasons behind these decisions. A possible\nlimitation of this and other studies [2, 27] is the sole use of synthetic datasets, however using these\ndatasets allow to precisely assess the reasons of the drop in generalization capabilities. Overall, the\nobtained results have shown that our proposal of explanation distillation alone using only biased\ndata is enough to achieve high OOD accuracy and shift bias resilience. In future work, we plan to\ninvestigate the suitability of explanation distillation in multiple non-synthetic scenarios also possibly\nemploying other explanation techniques."}, {"title": "A.1 Dataset Details", "content": "Details on COLOURED MNIST and COCO-on-Places can be found in the paper presenting it [2].\nThe study provides code to generate the datasets from their source databases, Microsoft COCO\n[21], Places 365 [36], and MNIST [12]. We used the provided code, just increasing the \"confounder\nstrength\" parameters. This parameters controls the percentage of data presenting spurious correlations,\nwe set it to 100%. Additionally, we use an IID validation dataset for hyper-parameter and model\nselection in distillation. Conversely, we used the dataset's OOD validation data for hyper-parameter\nand model selection in explanation background minimization. Unlike Ahmed et al. [2], we do not\nperform experiments on anomaly detection. Thus, we remove the class used for this purpose from\nCOCO-on-Places (motorbike), and we consider the digit 0 as a standard class in COLOURED MNIST,\ninstead of an anomaly.\nWe follow the training, hold-out validation and test sample assignment proposed by Ahmed et al. [2].\nPer class, COCO-on-Places has 800 training, 100 validation, and 100 test images. With 9 classes,\nwe have a total of 9000 images of size 64x64. COLOURED MNIST follows the standard MNIST\ntrain and test splits. It has a total of 70,000 28x28 images, where 10,000 are reserved for testing.\nThe remaining 60,000 images are randomly split, with 48,000 for training and 12,000 for hold-out\nvalidation. In all our applications, the only changes between the IID, OOD and systematic shift test\nsets are the synthetic bias. E.g., the IID and OOD COLOURED MNIST test sets present the exact\nsame hand-written digits, but their colors are different.\nThe proposed DogsWithTies dataset was created by first manually marking the neck region of all\nTibetan Mastiff and Pekingese dogs in the Stanford Dogs [16]. When the neck was not visible, we\nmarked another region over the dogs, close to the neck. Then, a Python code, which we released, used\nthe markings as guides to position ties on the images. We consider only two tie images, a purple bow\ntie, and a red necktie. The bow tie is always positioned horizontally, and the necktie vertically. They\nare scaled according to the size of the dog's necks (represented in our manual marks). The use of\nthe same tie models in all dogs, with the same orientation, makes them a strong spurious correlation.\nFigure 1 presents some samples from the DogsWithTies, with the bias as seen in training and IID\ntesting. Figure 2 presents some systematic shift test samples. All images in DogsWithTies were\nresized to 224x224. The dataset presents 100 training images per class, and we randomly selected\n20 of them for hold-out-validation. We follow the standard Stanford Dogs test split, which has 49\nPekingese images and 52 Tibetan Mastiff samples. Training on the entire Stanford Dogs dataset"}, {"title": "A.2 Data Pre-processing", "content": "In COLOURED MNIST, we only normalize the image pixels between 0 and 1. For COCO-on-Places,\nall images are resized to 64x64 pixels, and normalized between 0 and 1. In DogsWithTies, images"}, {"title": "A.3 Training Details and Computational Cost", "content": "Training hyper-parameters were defined by grid search and manual optimization, following IID hold-"}]}