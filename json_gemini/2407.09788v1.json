{"title": "Explanation is All You Need in Distillation: Mitigating Bias and Shortcut Learning", "authors": ["Pedro R. A. S. Bassi", "Andrea Cavalli", "Sergio Decherchi"], "abstract": "Bias and spurious correlations in data can cause shortcut learning, undermining\nout-of-distribution (OOD) generalization in deep neural networks. Most methods\nrequire unbiased data during training (and/or hyper-parameter tuning) to counteract\nshortcut learning. Here, we propose the use of explanation distillation to hinder\nshortcut learning. The technique does not assume any access to unbiased data,\nand it allows an arbitrarily sized student network to learn the reasons behind the\ndecisions of an unbiased teacher, such as a vision-language model or a network\nprocessing debiased images. We found that it is possible to train a neural network\nwith explanation (e.g by Layer Relevance Propagation, LRP) distillation only,\nand that the technique leads to high resistance to shortcut learning, surpassing\ngroup-invariant learning, explanation background minimization, and alternative\ndistillation techniques. In the COLOURED MNIST dataset, LRP distillation\nachieved 98.2% OOD accuracy, while deep feature distillation and IRM achieved\n92.1% and 60.2%, respectively. In COCO-on-Places, the undesirable generalization\ngap between in-distribution and OOD accuracy is only of 4.4% for LRP distillation,\nwhile the other two techniques present gaps of 15.1% and 52.1%, respectively.", "sections": [{"title": "1 Introduction", "content": "Although deep neural networks (DNNs) achieved super-human capacity in multiple tasks, it is\ndifficult to control what these complex models really learn. Training data commonly presents spurious\ncorrelations or shortcuts, i.e., input features that correlate with classification labels in the training data\ndistribution, but not in other distributions for the same classification task. DNNs that learn decision\nrules highly based on spurious correlations will perform well on standard benchmarks, but generalize\npoorly to out-of-distribution (OOD) data, which normally represent real-world applications. This\nbehavior is called shortcut learning or Clever Hans effect [14]. Shortcut learning is a major obstacle for\nOOD generalization, trustworthy artificial intelligence (AI), and the use of AI in critical applications\n[14]. Explanation techniques (e.g., GradCAM [29] and LRP [4]) can produce heatmaps that display\nhow DNN input elements (e.g., pixels) influenced the model's output. They are commonly used to\ninterpret the reasons behind a DNN's decision, increasing confidence in the model and detecting\nshortcut learning [18, 32, 11]. Additionally, studies proposed minimizing explanation backgrounds\n[27, 7] avoid the shortcut learning caused by spurious correlations in image backgrounds.\nIn this study, we propose explanation distillation as a strategy to create unbiased AI models from\nbiased data. Our technique leverages an assumed existing large and distributionally robust teacher\nmodel to prevent shortcut learning when training a, possibly light-weight, student trained on biased"}, {"title": "2 Related work", "content": "Several works perform explanation background minimization: they train a network for classification\nwhile penalizing its explanation heatmaps' background elements [27, 20, 7, 6]. Here, background\nrefers to DNN input areas that mostly present clutter or shortcuts (background bias), instead of\nimportant features. Hence, explanation background minimization aims to minimize the influence\nof background bias over a classifier. Multiple explanation techniques can be used for explanation\nbackground minimization. Right for the Right Reasons (RRR [27]) employs input gradients (saliency\nmaps)[31], and Guided Attention Inference Network (GAIN) optimizes Grad-CAM. More recently,\nthe Implicit Segmentation Neural Network (ISNet) [7, 6] introduced background minimization using\nLayer-wise Relevance Propagation (LRP)[4] heatmaps. Explanation background minimization comes\nwith a few drawbacks. First, during training, these models require ground-truth semantic segmentation\nmasks that define the training sample's (e.g., images) backgrounds. Second, they need careful tuning\nof the hyper-parameter that balances the classification loss and the loss penalizing explanations.\nThe optimization of this hyper-parameter requires evaluation over OOD data, which is assumed\navailable (or at least simulated [7]). Third, explanation background minimization can only deal\nwith background biases. If training images present spurious correlations in their region of interest\n(e.g., over objects being classified), or if the shortcuts (or their locations) are unknown, one can\nresort to Group-Invariant Learning methods, like distributionally robust optimization (DRO) [28],\nand Invariant Risk Minimization [3]. However, such methods require the preliminary separation of\nthe training data into sub-sets that are not IID and represent data distributions with diverse spurious\ncorrelations (if any). This separation may be complex or even impossible practically, especially when\nspurious correlations are ubiquitous for a given class. For instance, if all digits 2 in a dataset are red,\ngroup learning cannot prevent the DNN from associating 2 to red.\nRecently, large vision-language models, such as CLIP, displayed unprecedented distributional robust-\nness and OOD accuracy, especially in zero-shot inference [26]. Shortcut learning is characterized by\nsubpar OOD generalization [14], indicating that CLIP suffered little shortcut learning. However, this\nquality is not a result of CLIP's architecture or training algorithm, but rather of the large data diversity\nin its training dataset [13]. Instead, if data is biased, language supervision or contrastive learning does\nnot prevent shortcut learning nor produces distributional robustness [13]. Even fine-tuning CLIP\nwith biased data undermines its distributional robustness [26]. Shortcut learning can be prevented by\navoiding training on biased data, and just relying on a zero-shot vision-language model that is known"}, {"title": "3 Methods", "content": ""}, {"title": "3.1 Attention and Feature-level Explanations", "content": "In this work we will indicate as g = G(x", "namely": "attention", "Komodakis\n[34": ".", "is": "n\\documentclass{article"}, "n\\usepackage{amsmath}\n\n\\begin{document}\n\\begin{equation}\na_L = A(y_L); A : \\mathbb{R}^{C\\times H \\times W} \\rightarrow \\mathbb{R}^{H \\times W}\n\\end{equation}\n\\end{document}\nwhere A(\u00b7) is a function that projects yL to the 2D space \\mathbb{R}^{C\u00d7H\u00d7W}. A standard choice of A(\u00b7), used\nin this study, is the sum of squared values, a_{L,i,j} = \\sum_{c=1}^{C} y_{L,c,i,j}^2 [34"], "as": "a CNN's feature map element\n(y_{L,c,i,j}) is mostly influenced by and mostly carries information from the input feature (x_{i',j'}) that is"}