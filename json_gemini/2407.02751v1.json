{"title": "Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset", "authors": ["Rui Liu", "Haolin Zuo", "Zheng Lian", "Xiaofen Xing", "Bj\u00f6rn W. Schuller", "Haizhou Li"], "abstract": "Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU) aims to decode the semantic information manifested in a multimodal conversational history, while inferring the emotions and intents simultaneously for the current utterance. MC-EIU is enabling technology for many human-computer interfaces. However, there is a lack of available datasets in terms of annotation, modality, language diversity, and accessibility. In this work, we propose an MC-EIU dataset, which features 7 emotion categories, 9 intent categories, 3 modalities, i.e., textual, acoustic, and visual content, and two languages, i.e., English and Mandarin. Furthermore, it is completely open-source for free access. To our knowledge, MC-EIU is the first comprehensive and rich emotion and intent joint understanding dataset for multimodal conversation. Together with the release of the dataset, we also develop an Emotion and Intent Interaction (EI2) network as a reference system by modeling the deep correlation between emotion and intent in the multimodal conversation. With comparative experiments and ablation studies, we demonstrate the effectiveness of the proposed EI\u00b2 method on the MC-EIU dataset. The dataset and codes will be made available at: https://github.com/MC-EIU/MC-EIU.", "sections": [{"title": "1 Introduction", "content": "Emotion and Intent Joint Understanding in Multimodal Conversation (MC-EIU) aims to infer the emotional state and intent information simultaneously by modeling the semantic dependency among the multimodal conversation Singh et al. [2022]. Unlike separate tasks of emotion or intent recognition Lian et al. [2023], Zuo et al. [2023], Zhang et al. [2022a], the MC-EIU task provides richer information to assist machines in better understanding human needs and improve empathy in human-machine conversation Poria et al. [2017a], Deng et al. [2023]. It holds significant potential for application in various human-computer interaction scenarios, including call center dialog systems Danieli et al. [2015], conversational agents Cowie et al. [2001], and mental health counseling Ringeval et al. [2018], etc."}, {"title": "2 Related Work", "content": "Multi-task Prediction: Multi-task learning aims to leverage the valuable information in multiple related tasks to enhance generalization performance across all tasks Yang and Hospedales [2017], which has a wide range of applications in fields such as Emotion Recognition Hazarika et al. [2020], Object Detection Li et al. [2023], Conversational Speech Synthesis Liu et al. [2023], etc. Multi-task learning methods can be broadly classified into Hard Parameter Sharing (HPS) and Soft Parameter Sharing (SPS) Zhang et al. [2022b], Pahari and Shimada [2022]. HPS serves as the foundational deep neural network that is shared across different tasks, while each task utilizes its own distinct top layer Zhang et al. [2022b], Hazarika et al. [2020]. SPS allows the model to share some parameters between different tasks while preserving task-specific parameters Pahari and Shimada [2022].\nIt should be noted that compared to HPS, the advantage of SPS lies in the ability of the network to utilize shared features among different tasks by sharing bottom-layer parameters. It allows for better utilization of interactive information between tasks Pahari and Shimada [2022].\nEmotion-Intent Interaction: Previous works confirm that there is a strong interaction between emotion and intent Welivita and Pu [2020], Welivita et al. [2020], Singh et al. [2022], Deng et al. [2023]. For instance, Singh et al. [2022] emphasizes that the emotions of the speaker can be influenced by particular intents in dialogues. To explore the interaction between emotion and intention, they built the EmoInt-Trans model that follows the HPS framework. It uses a MISA network Hazarika et al. [2020] as the backbone, where the emotion and intent prediction tasks share all the parameters of this backbone. However, the complex relationship between emotion and intent poses a challenge for HPS, as it may struggle to accommodate the distinct characteristics of each task. Note that the biggest difference between our EI2 model and EmoInt-Trans is that we employ an SPS mode to learn the deep-level interactive information for emotion and intent."}, {"title": "3 MC-EIU Dataset Construction", "content": "3.1 Data Collection and Pre-processing\nTo simulate the emotional conversa-tion scenarios in real-world situations Zhao et al. [2022], Singh et al. [2022], we select emotional video clips from 3 English (716 episodes) TV series and 4 Chinese (119 episodes) TV series across genres such as family, romance, crime, etc.\u00b9 All videos are accompanied by corresponding subtitle files. Afterward, we preprocess the data to obtain the desired format and filter out low-quality data that does not meet the criteria. Specifically, 1) We first design Regular Expression scripts to extract text transcription and the timestamps from the subtitle; 2) Then, we leverage VideoFileClip\u00b2 to partition the videos into mul-tiple clips based on the timestamps; 3) At last, we engage crowd workers to pick out high-quality conversational segments from the same conversational scene. Note that we provide the following specific guidelines to the crowd workers based on relevant work Zhao et al. [2022]: a) Conversational scenes should exclusively involve interaction between two speakers; b) The selected conversational scenes should be free from noise or special effects sounds to ensure video quality is not affected; c) Each conversational segment should encompass at least two rounds of interaction between the speakers to provide abundant context information; d) The text content should be accurate and align with the video clips."}, {"title": "3.2 Data Annotation", "content": "Annotation Scheme: The annotation scheme consists of emotion, intent, and speaker annotation. For emotion annotation, we choose Ekman's six basic emotions (happy, surprise, sad, disgust, anger, and fear), along with the neutral label, which is a 7-emotion annotation scheme widely used in previous works Zhao et al. [2022], Busso et al. [2008], Poria et al. [2019]. For intent annotation, we follow Welivita and Pu [2020], Welivita et al. [2020] and annotate each utterance based on nine intents: questioning, agreeing, acknowledging, sympathizing, encouraging, consoling, suggesting, wishing, and neutral. For speaker annotation, we follow the scheme described in Zhao et al. [2022] and assign the labels \"0\" and \"1\" to annotate the speaker for each dialog. The above annotation scheme ensures consistency with real conversation scenarios Zhao et al. [2022], Poria et al. [2019].\nWe recruit postgraduate students from the Foreign Languages College as annotators for our project\u00b3. Prior to commencing the annotation process, all annotators undergo a training and assessment phase. Only those who obtain a passing score are selected to proceed to the data annotation stage. Annotators who do not meet the passing criteria are required to undergo additional training and reassessment until they successfully pass the assessment.\nAnnotation Process: We recruited 21 annotators and divided them into seven groups. Each group consists of three volunteers, and non-duplicate data is assigned to each group for annotation. This ensures that each conversation data is annotated by three volunteers. Each annotator can access the current utterance and history in the multimodal conversation, including text, audio, and video clips during the annotation process. The annotators are asked to select the appropriate emotion and intent labels from the annotation scheme after watching the video clip. Simultaneously, annotators are required to assign speaker labels to each utterance in the dialog based on the speaking order. Suppose annotators encounter difficulty in assigning a specific category to a video clip. In that case, they categorize the utterance as other, and the corresponding dialog containing this utterance will not be included in our dataset."}, {"title": "3.3 Data Annotation Finalization", "content": "We employ a majority voting strategy on all utterance annotations to derive the final emotion and intent labels. If at least two annotators provide the same annotation for one utterance, it is considered the final label. If all three annotators have different annotations, an additional emotion expert is consulted to confirm the final label."}, {"title": "4 Emotion-Intent Interaction (EI\u00b2) Network", "content": "4.1 Overall Architecture\nTo model the multimodal dialog history and the deep-level interaction between emotion and intent, we propose an EI\u00b2 network as shown in Figure 2. The network consists of the following components: 1) Emotion & Intent Encoders aim to generate the multimodal emotion and intent representations for current utterance; 2) the Multimodal History Encoder is responsible for capturing the multimodal contextual semantic information from the multimodal history; 3) the Emotion-Intent Interaction Encoder is proposed to learn the deep interaction between emotions and intents in a conversation; 4) Emotion & Intent Classifiers aim to make predictions based on the emotion-intent interactive information.\nEmotion & Intent Encoders: In contrast to previous work Singh et al. [2022] that extracts general semantic representations from utterances, we add separate feature encoders for emotion and intent understanding to extract more explicit emotion and intent features.\nThe emotion and intent encoders share a similar structure, which includes a Visual Encoder, a Textual Encoder, an Acoustic Encoder, and a Transformer fusion network. Assume that un, un, un represent the acoustic, textual, and visual features of the n-th utterance un, the multimodal emotion representation $f^{s}_{e}$ and intent representation $f^{s}_{i}$ can be expressed as:\n$f^{s} = F^{\\*}(Concat(F^{v}(u_n), F^{a}(u_n), F^{t}(u_n)))$    (1)\nwhere s \u2208 {e, i}, Fa is the Acoustic Encoder based on LSTM Sak et al. [2014] and max-pooling, Fu is the Visual Encoder that has the same structure as Fa, Ft is the TextCNN-based Text Encoder Kim [2014] adopted, and the F* means the Transformer fusion network.\nIt is worth noting that the modules with fire symbols in Figure 2 indicate the need for pre-training our intent and emotion encoders. Further details will be provided in Section 4.2.\nMultimodal History Encoder: Different from EmoInt-Trans Singh et al. [2022], which solely models the context information of adjacent utterances, our Multimodal History Encoder takes into account a broader range of historical information. For the current utterance un, the multimodal conversational history information fh can be represented as:\n$f_{m}^{h} = F_{m}^{h}(\\{u_{m}^{0}, u_{m}^{1}, ..., u_{m}^{n-1}\\})$    (2)\n$f_{h} = Concat(f_{v}^{h}, f_{a}^{h}, f_{t}^{h})$    (3)\nwhere m \u2208 {v, a, t}, Fm denotes the GRU-based History Encoder Lee et al. [2023]. Subsequently, fh is independently fused with fe and fi to obtain the robust emotion feature fe and intent feature fi:\n$f_{s}^{'} = f_{s}^{s} + f_{h}$.\nEmotion-Intent Interaction Encoder: The approach of simply sharing the hidden state is not sufficient to achieve explicit information transfer between two tasks. Therefore, we propose the Emotion-Intent Interaction Encoder to learn the deep interactive information between them by considering their complex correlations. As shown in Figure 2, the Emotion-Intent Interaction Encoder contains two branches for emotion or intent prediction, where each branch consists of Binary Correlation Attention, Triple Interaction Attention, and Gate Regulator.\nBinary Correlation Attention first employs cross-attention to learn the mutual influence between emotions and intentions, which can also be referred to as the binary correlation between the two tasks. Specifically, we first adopt linear projection to map the fe and fi to generate the corresponding Q, K, and V. Then, the attention mechanism is used to extract the correlation between fe and fi:\n$f_{\\gamma - \\beta} = Attention(f_{\\gamma}, f_{\\beta}, f_{\\beta}),$    (4)\nwhere \u03b3, \u03b2\u2208 {e, i}, if \u03b3 denotes e, then, \u03b2 denotes i, and vice versa.\nTriple Interaction Attention further explores the deep interactive information between the two tasks by integrating the binary correlation and task-specific information from each branch to obtain a more comprehensive and in-depth task interaction feature. Inspired by Jiang et al. [2023], we propose a triple interaction attention mechanism to compute cascaded interaction feature representations. It takes the outputs of binary correlation attention, along with the fe and fi, as inputs; the deep interactive information $f_{\\gamma - \\beta - \\gamma}$ of two tasks can be obtained by:\n$f_{\\gamma - \\beta - \\gamma} = Attention(f_{\\gamma}, f_{\\gamma - \\beta}, f_{\\gamma - \\beta})$    (5)\nGate Regulator: It utilizes a gating mechanism to automatically learn the weights of the binary correlation features and ternary interaction features, to model the potential impact of the correlation between different emotion-intent pairs in Figure 1 on the interaction features. This allows us to adjust the contribution of the binary correlation to the final recognition of emotions or intents. Specifically, the gate regulator first adds $f_{\\gamma - \\beta - \\gamma}$ and $f_{\\gamma - \\beta}$ together. Afterward, the Sigmoid function is applied to obtain a control gate value between them. Lastly, $f_{\\gamma - \\beta}$ is multiplied by the control gate value to effectively adjust the weight of the correlation information:\n$g_{\\gamma} = f_{\\gamma - \\beta} \\* sigmoid(f_{\\gamma - \\beta - \\gamma} + f_{\\gamma - \\beta}),$    (6)\nwhere g represents the final output of Emotion-Intent Interaction Encoder. With the collaboration of these three components, our EI2 model captures deep-level interaction between emotion and intent.\nEmotion & Intent Classifiers: To preserve the specific information of the emotion and intent repre-sentations while incorporating the deep interactive information, we perform the residual connection"}, {"title": "4.2 Training Strategy", "content": "We first pre-train the emotion and intent encoders to ensure the effective extraction of emotion and intent information. given the presence of category imbalance in the dataset (shown in the Appendix), we employ the Focal Loss (FL) Lin et al. [2017] as the loss function Lpre in the pre-training phase to constrain the prediction of the emotion and intent to be close to the Ground Truth Pe and Pi of emotion and intent and to improve the model's ability to focus on the categories of a small sample:\n$P_{e}^{'} = CLS_{e}(f_{e}^{'}), P_{i}^{'} = CLS_{i}(f_{i}^{'})$    (7)\n$L_{pre} = FL(P_{e}, P_{e}^{'}) + FL(P_{i}, P_{i}^{'})$    (8)\nwhere CLS and CLS; represent the emotion and intent classifiers during the pre-training stage.\nDuring the training phase of EI\u00b2, we initialize the emotion and intent encoders with pre-trained weights. These encoders are then further updated during the EI\u00b2 training. At last, we adopt a joint training approach and also utilize FL loss as the final loss function Ltotal for the MC-EIU task.\n$L_{total} = FL(P_{e}, P_{e}^{'}) + FL(P_{i}, P_{i}^{'})$    (9)"}, {"title": "5 Experiment and Analysis", "content": "5.1 Baseline\nTo validate our MC-EIU datasets and the proposed EI\u00b2 network, we develop four MC-EIU systems based on state-of-the-art models: bc-LSTM Poria et al. [2017b] is widely employed in conversational senti-ment recognition tasks. It adopts the bi-directional LSTM and multi-head atten-tion mechanism, enabling the model to capture both contextual information and abundant semantic information within ut-terances. We add additional emotion and intent classifiers to make bc-LSTM sup-port multi-task prediction. MMIN Zhao et al. [2021] incorporates a cascade resid-ual autocoder network and cyclic consis-tency constraints to learning the robust multimodal joint representation for mul-timodal emotion recognition. We expand the MMIN model by incorporating an in-tent classifier, enabling it to recognize emo-tion and intent simultaneously. MISA Haz-arika et al. [2020] introduces a modality-invariant and -specific feature representa-"}, {"title": "5.2 Main Results", "content": "We first compare the recognition performance of our EI\u00b2 and the baselines in both English and Mandarin. As shown in Table 4, we observe that EI2 achieves the highest WAF in both emotion and intent recognition in both languages, clearly outperforming the baseline systems. For example, the WAF of emotion and intent for EI2 in English are 42.09% and 45.53%, respectively. In Mandarin, the scores are 55.08% for emotion and 61.63% for intent. These results demonstrate the effectiveness of our method. By learning from multimodal dialog history and employing soft parameter sharing to capture the interaction between emotion and intent, our model achieves impressive joint recognition performance for emotion and intent."}, {"title": "5.3 Ablation Study", "content": "We design three different ablation experiments, that are module ablation, task ablation, and modality ablation to further validate the components of EI2 system.\nModule Ablation: We conducted ablation experiments on the multimodal history encoder, emotion-intent interaction encoder, gate regulator, and the pretraining strategy for emotion&intent encoders: 1) w/o History: we remove the multimodal history encoder of EI2; 2) w/o Interaction: the features outputted by the emotion and intent encoders are directly used for final prediction; 3) w/o Gate: we remove the gate regulator; 4) w/o FL: we replace Focal Loss referenced in Section 4.2 with the Cross-Entropy Loss; 5)w/o Pre-training: we randomly initialize the parameters of the Emotion and Intent Encoders during the EI2 training.\nAs shown in Table 4, EI2 outperforms all the ablation models on both datasets. This provides compelling evidence for the following assertions: 1) The inclusion of conversation history helps analyze the current speaker's emotions and intent states; 2) By modeling and integrating deep interactive information, the joint understanding task achieves improved performance, and further demonstrates the significance of modeling the interactive information between intent and emotion for joint understanding; 3) After reducing the gating mechanism, the model's performance experienced a significant decline, highlighting the crucial role of the gate regulator in adjusting the weights of binary correlation to the final recognition of emotion or intent; 4) The pre-training of emotion and intent encoders resulted in a notable performance improvement, suggesting their capacity to learn substantial semantic information and distinct features for emotions and intents through pre-training."}, {"title": "6 Conclusion", "content": "This work introduced a novel dataset called Multimodal Conversational Emo-tion and Intent Dataset (MC-EIU), which possesses several key properties: annota-tion diversity (includes emotion and in-tent labels), modality diversity (encom-passes textual, acoustic, and visual data), language diversity (comprises English and Mandarin data), and accessibility. Further-more, we proposed an Emotion and Intent Interaction (EI2) Network for the MC-EIU task, which effectively captures the con-versational history and the complex inter-action between emotion and intent. Exten-sive experiments conducted on our dataset demonstrate the effectiveness of our EI2, showcasing its superior performance. Our work is dedicated to advancing the field of affective computing.\nGiven the effective modeling of long-range dependencies in conversations by large language models Touvron et al. [2023], this characteristic plays a crucial role in understanding the intricate interaction between emotions and intentions, both within individuals and between speakers Deng et al. [2023]. This suggests a promising avenue for future research. Furthermore, previous research has established that emotion stimulus and inertia are significant factors that influence the emotional state of speakers in dialogs Zhao et al. [2022]. However, it remains an open question whether they impact the correlation between emotion and intent."}, {"title": "A Datasheets for datasets", "content": "A.1 Motivation\nPrevious works confirm a strong interaction between emotion and intent Welivita and Pu [2020], Welivita et al. [2020], Singh et al. [2022], Deng et al. [2023]. For instance, Singh et al. [2022] emphasizes that particular intents can influence the emotions of the speaker in dialogues. However, the existing datasets for emotion and intent joint understanding, such as Twitter Customer Support Maharana et al. [2022], ED Welivita and Pu [2020], and OSED Welivita et al. [2020], only consist of English textual data, which cannot fulfill the requirements for multimodal and multilingual research. Furthermore, while Singh et al.Singh et al. [2022] proposed a multimodal dialogue-based dataset for emotion and intent joint understanding in multimodal conversation, namely EmoInt-MD, it also comprises only English data. Moreover, it is worth noting that the provided open-source link for EmoInt-MD is currently unavailable. As a result, there is no available dataset for emotion and intent joint understanding in the context of multimodal dialogue scenarios. Without the task-specific datasets, the potential of multimodal emotion and intent joint understanding could not be fully explored, nor deepen the understanding of human complicated affections.\nWe fill the gap by constructing a large-scale benchmark MC-EIU dataset, which features 7 emotion categories, 9 intent categories, 3 modalities, i.e., textual, acoustic, and visual content, and two languages, i.e., English and Mandarin. Furthermore, it is completely open-source for free access. We aim for our work to facilitate a deeper exploration of the emotion and intent joint understanding, thereby advancing the field of affective computing.\nA.2 Composition\nThe MC-EIU dataset comprises 56,012 utterances, including 4,013 conversations in English and 957 conversations in Mandarin, with a total duration of 53.06 hours. Each utterance is annotated with emotion, intent, and speaker labels, and contains textual, visual, and acoustic information. All utterances are recorded in a .CSV file, with each utterance uniquely identified by a Dia_No and an Utt_No. Corresponding video and audio clips for each utterance are stored in .MP4 and .WAV files, respectively. These files follow a specific naming convention: \"dia_{Dia_No}_utt_{Utt_No}.mp4\" or \"dia_{Dia_No}_utt_{Utt_No}.wav\". We partitioned the MC-EIU dataset into training, validation, and test subsets with proportions of 70%, 10%, and 20%, respectively. In dividing the dataset, we adhered to the following principles: 1) Random assignment of dialogues. 2) Ensuring that all utterances within the same dialogue are assigned to the same subset. 3) Striving to maintain a consistent distribution of labels across each subset."}, {"title": "A.3 Collection Process and License", "content": "Collection Process: In this work, we choose 3 famous English TV series and 4 Mandarin TV series as our domain. Such TV series consist of conversations with utterances in the forms of text, video, and audio, and cover different genres (i.e. family, romance, comedy, lifestyle). The total number of episodes for English TV series is 716, and for Mandarin TV series, it is 119. Table 8 represents the details of TV series. In terms of data sources, our resources are more than 1.5 times larger than the resources of M3ED Zhao et al. [2022] and closely 4 times more than those of MELD Poria et al. [2019]. It covers the majority of emotional dialogue scenarios in real life. To avoid the inclusion of offensive, discriminatory, or otherwise unethical data in our dataset, we exclude TV series belonging to genres such as horror, thriller, war, crime, etc. Furthermore, we promptly removed those clips from the dataset if any unethical clips were found in the selected TV series.\nLicense terms: To avoid copyright disputes, we ensure that our resources are sourced from publicly accessible platforms. The MC-EIU dataset is strictly available for research purposes only. We have designed an appropriate license, specifically the CC BY-NC 4.0, which clearly outlines the proper and responsible usage of the MC-EIU dataset. It will help guide the user of the MC-EIU dataset in making informed decisions about how the MC-EIU dataset can and cannot be used4."}, {"title": "A.4 Preprocessing/Cleaning/Labeling", "content": "Please see the Section 3.2 in the main paper."}, {"title": "A.5 Uses and Distribution", "content": "We state that the MC-EIU dataset is suitable for multimodal emotion and intent joint understanding tasks, including emotion recognition, intent recognition, and emotion and intent joint recognition in multimodal conversations. The copyright of the MC-EIU dataset belongs to the S2 Lab of the College of Computer Science of Inner Mongolia University. To ensure standardized experimentation and evaluation, we currently release only the feature files for all the data, along with a .CSV file containing the text information and annotations. Please refer to Appendix H.4 for details on the feature extraction methodology. Following paper acceptance, the dataset will be made available at https://github.com/MC-EIU/MC-EIU."}, {"title": "A.6 Maintenance", "content": "Regarding the dataset update iteration, our laboratory will have special personnel to inspect and maintain the MC-EIU dataset every six months. This includes correcting labeling errors, adding new instances, and deleting outdated instances. Meanwhile, we welcome other research teams to use this dataset."}, {"title": "A.7 Uses", "content": "\u2022 Has the dataset been used for any tasks already? If so, please provide a description.\nIt is proposed to be used for the multimodal emotion and intent joint understanding task.\n\u2022 Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point.\nIt is a new dataset. We develop an Emotion and Intent Interaction (EI2) framework as a reference system and release the code at Github.com.\n\u2022 What (other) tasks could the dataset be used for?\nThe MC-EIU dataset can also be used for various other tasks such as multimodal emotion recognition, multimodal intent recognition, and dialogue understanding.\n\u2022 Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms?\nN/A\n\u2022 Are there tasks for which the dataset should not be used? If so, please provide a description.\nN/A"}, {"title": "B Dataset nutrition labels", "content": "Table 9 shows the different modules of the MC-EIU dataset nutrition label and their corresponding description."}, {"title": "C Data Statements for Natural Language Processing", "content": "We state the MC-EIU dataset from the following aspects:\n\u2022 The full name of the MC-EIU dataset is the Multimodal Conversational Emotion and Intent Joint Understand dataset. This dataset addresses the requirements of annotation diversity, modality diversity, language diversity, and accessibility, making it well-suited for the task of joint emotion and intent recognition. The copyright belongs to the S2 Lab of the College of Computer Science of Inner Mongolia University."}, {"title": "D Data Accessibility", "content": "The MC-EIU dataset is available at https://github.com/MC-EIU/MC-EIU and https://pan.baidu.com/s/1gxxr81tVytFTW2UjfTrh-g."}, {"title": "E Accountability frameworks", "content": "E.1 Data collection and processing specifications\nWe declare that our data collection is all in publicly accessible links and follow the following rules in data processing.\n\u2022 Data collection and processing should comply with relevant regulations and ethical re-quirements, and use appropriate technical tools and methods to ensure data quality and integrity.\n\u2022 Data collection should clarify key information such as data type, source, time, and location.\n\u2022 Data processing should establish a clear data cleaning, conversion, and integration process, and carry out data verification and deduplication.\n\u2022 Data sampling and sample selection should fully consider the impact of research design and sampling error, and carry out statistical inference and reliability analysis.\nE.2 Dataset usage and evaluation mechanisms\nDataset usage and evaluation mechanisms should follow the following principles:\n\u2022 The use of data sets should follow relevant laws and regulations, respect data privacy and intellectual property rights, and prevent abuse and discriminatory results.\n\u2022 Dataset results should be interpreted and applied within a reasonable margin of error, with a full explanation of their limitations and applicability of inferences.\n\u2022 Dataset users should assign corresponding responsibilities and obligations, including data protection, fair use, and social responsibility.\n\u2022 The process of using data sets should be regularly audited and evaluated in order to detect and correct problems in a timely manner and improve the value and credibility of datasets."}, {"title": "F Author Statement", "content": "On behalf of all the authors, we hereby state that we will assume full responsibility for any violations of rights or issues related to data licensing."}, {"title": "G Hosting, Licensing, and Maintenance Plan for MC-EIU Dataset", "content": "Our MC-EIU dataset is a groundbreaking multimodal dataset designed for joint understanding of conversational emotions and intents, which we have created and publicly released. To ensure its accessibility and long-term availability, we have developed a comprehensive plan for hosting, licensing, and maintenance.\nHosting: The MC-EIU dataset will be hosted on a reliable and secure server infrastructure, i.e., Github.com and Baidu Drive (refer to Section D). We will ensure fast and uninterrupted access to the dataset for researchers, developers, and interested parties.\nLicensing: We have designed a proper license (CC BY-NC 4.0) attached to the MC-EIU dataset to clearly describe how to properly and responsibly use the MC-EIU dataset (refer to Section A.3).\nMaintenance: We are committed to the continuous maintenance and improvement of the MC-EIU dataset. Regular updates will be provided to address any identified issues and ensure data quality (refer to Section A.6)."}, {"title": "H Data Annotation", "content": "H.1 Data Collection\nPlease refer to Section A.3.\nH.2 Annotation Guidelines and Annotation Website\nWe presented the relevant details of data annotation in Section 3.2 of the main paper, including the annotation scheme and annotation process. To facilitate the annotation process for the annotators, we provided a unified data annotation platform, as illustrated in Figure 4.\nH.3 Data Format\nWe select some samples from the English and Mandarin datasets of MC-EIU respectively to demon-strate the data format, as shown in Table 10."}, {"title": "H.4 Feature Extraction", "content": "We utilize several state-of-the-art pre-trained models to extract the raw features from our datasets.\nTextual Features: To extract word-level textual features in English and Chinese, we employ sep-arate ROBERTa Yu et al. [2020] models 5 that have been pre-trained on data from each respec-tive language. The embedding size of the textual features for both languages is 768 dimensions.\nAcoustic Features: We extract frame-level acoustic features using the Wav2Vec Schneider et al. [2019] model pre-trained on large-scale Chinese and English audio data. The embedding size of the audio features is 512 dimensions.\nVisual Feature: We employ the OpenCV tool to extract scene pictures from each video clip, capturing frames at a 10-frame interval. Subse-quently, we utilize the Resnet-50 He et al. [2016] model to generate frame-level features for the extracted scene pictures in the video clips. The embedding size of the video features is 342 di-mensions.\nAll the features are named in the format \"dia_{Dia_No}_utt_{Utt_No}.npy\". To differ-entiate between different modality features, we create separate folders for each modality, naming them according to the corresponding pre-trained model. All the features are stored within their respective modality folders. For instance, if we extracted text features using the RoBERTa model, the folder corresponding to the text modality would be named 'ROBERTa'. And all the text features would be stored in the \"RoBERTa\" folder. The structure of the feature set is illustrated in Figure 5. To ensure standardized experimentation and evaluation, we currently release only the feature files for all the data, along with a .CSV file containing the text information and annotations."}, {"title": "H.5 Category Distribution", "content": "We show the distribution of emotion and intent categories for the MC-EIU dataset in Figure 6. Similar to M3ED, MELD, and IEMOCAP, our dataset also exhibits category imbalance. In terms of emotion labels, the top three categories in our dataset are neutral, happy, and anger, while disgust and fear are relatively less represented, similar to the distribution in MELD. Regarding intent labels, the most abundant categories are neutral, questioning, and suggesting, while sympathizing is the least represented, aligning with the OSED dataset. We present a detailed distribution of the data across the train, validation, and test sets in Table 11.\nThe presence of category imbalance in our dataset can be attributed to the following reasons:\n\u2022 Deliberate maintenance of category imbalance: Although researchers tend to use balanced datasets in experiments to explore model performance, category imbalance is a more common and challenging scenario in the real world Japkowicz and Stephen [2002], He and Garcia [2009], Fern\u00e1ndez et al. [2018], Johnson and Khoshgoftaar [2019]. For instance, in work environments, individuals may conceal their emotions during interactions. In daily life, a stable and harmonious social environment promotes the expression of positive emotions (such as happiness and joy) while reducing the expression of negative emotions (such as anger and sadness). Therefore, intentionally maintaining this imbalance helps better"}, {"title": "H.6 Ethical Considerations", "content": "In data selection", "follows": 5}, {"title": "I Extra Experiment Results and Analysis", "content": "I.1 Implementation Details\nOur proposed framework is imple-mented using PyTorch. The hidden size of Fu, Fa, and Ft is 128. We set the attention head number for the Trans-former Network, Cross Attention, and Fusion Attention as 4. We select the Adam optimizer Kingma and Ba [2015] and initialize the learning rate to 0.0002. We dynamically update the learning rate using the Lambda LR Wu et al. [2020] approach. The batch size is 32, and the epoch of both pre-training and training phases is set to 60. To ensure the ob-jectivity of the experimental results, we conducted all experiments three times and reported the average results. All ex-periments were performed on a single NVIDIA A100 graphics card.\n1.2 Category Imbalance Issue\nAs mentioned in Appendix H.5, our MC-EIU dataset, like many other datasets, suffers from category imbalance issues. Taking the advanced model EmoInt-Trans for the MC-EIU task as an exam-"}, {"title": "1.3 Case Study", "content": "In this section, we continue to use the EmoInt-Trans system as the baseline for comparison. To further validate the effectiveness of our EI\u00b2 method, we present the case study.\nFigure 8 presents a conversational sam-ple, that consists of 3 utterances, from the MC-EIU dataset. We adopt the EmoInt-Trans and EI2 models to predict emotion and intent labels, respectively. The Ground Truth labels and the predic-tion results are shown in Table 12.\nWe can observe that the predictions by EI2 are consistently correct, whereas the results of EmoInt-Trans are not. Tak-ing utterance (a) as an example, EmoInt-Trans fails to make accurate predictions due to its limited ability to model deep interactions between emotion and intent. As shown in Figure 1 in the main paper, the proportion of \"Fea-Wis\" is smaller compared to \"Neu-Wis\", indicating a weaker correlation between fear and wishing. Therefore, in the joint recognition process, the impact of interaction between fear and wishing on the final prediction is relatively low. EmoInt-Trans cannot dynamically adjust the impact of interaction information on the final prediction, leading to incorrect predictions. Different from EmoInt-Trans, our model incorporates the Emotion-Intent Interaction Encoder, which enables deep interactions between emotion and intent to be modeled while controlling the weight of interaction"}, {"title": "J Impact", "content": "Positive Impacts:\n\u2022 Innovation: As the first open-source dataset in the emotion and intent joint understanding field, The MC-EIU dataset provides a new resource that can spur innovation and development in emotion and intent recognition. Furthermore, this dataset enables researchers to explore the intricate interactions between emotion and intent, ultimately advancing the development of emotion recognition tasks.\n\u2022 Collaboration: The availability of the MC-EIU dataset encourages collaboration among researchers across different institutions and industries, fostering a more unified and rapid advancement in the field.\n\u2022 Improved Systems: The dataset can help improve the accuracy and reliability of systems that rely on emotion and intent recognition, such as virtual assistants, customer service bots, and interactive entertainment systems.\nNegative Impacts:\n\u2022 Category Imbalance: Due to the category imbalance phenomenon in the MC-EIU dataset, it is possible for models adapted to category balance to inaccurately recognize minority categories, leading to misjudgments in model performance. Therefore, we encourage more researchers to pay attention to the category imbalance issue."}, {"title": "K Limitation", "content": "(1) Our proposed dataset does not reach the requirement of \"balance\". As described in Appendix H.5 and I.2, our dataset exhibits category imbalance. This is a common issue in real-world scenarios and a challenge that researchers need to address. In our future work, we will actively explore methods to tackle the category imbalance issue and encourage future researchers to pay attention to such issues as well.\n(2) The framework we established is a basic model. The main focus of this paper is the dataset release, with the baseline model serving as an accompanying system. The baseline system has emphasized the interaction between emotion and intent, which is crucial for multimodal emotion-intent joint recognition, even though the ideas behind the baseline may seem simple. In the future, we will further explore new methods to delve deeper into the complex relationship between emotion and intent.\n(3) We do not explore the performance of the large language model on the emotion and intent joint understanding task. Given the effective modeling of long-range dependencies in conversations by large language models (LLMs) Touvron et al. [2023], this characteristic plays a crucial role in understanding the intricate interaction between emotions and intentions, both within individuals and between speakers Deng et al. [2023]. We have already discussed the possibility of using the LLMs for emotion and intent joint understanding in Section 6 of the main paper. While we attempted to fine-tune large-scale multimodal models for this task, we faced challenges during training due to limited computational resources. In the future, we will seek opportunities to collaborate with other laboratories to conduct joint research on using LLMs for emotion and intent recognition. Additionally, we hope this direction becomes a new research hotspot, attracting more researchers to contribute to this field."}]}