{"title": "Attribute-Text Guided Forgetting Compensation for\nLifelong Person Re-Identification", "authors": ["Shiben Liu", "Huijie Fan*", "Qiang Wang", "Weihong Ren", "Yandong Tang"], "abstract": "Abstract-Lifelong person re-identification (LReID) aims to\ncontinuously learn from non-stationary data to match individuals\nin different environments. Each task is affected by variations\nin illumination and person-related information (such as pose\nand clothing), leading to task-wise domain gaps. Current LReID\nmethods focus on task-specific knowledge and ignore intrinsic\ntask-shared representations within domain gaps, limiting model\nperformance. Bridging task-wise domain gaps is crucial for\nimproving anti-forgetting and generalization capabilities, espe-\ncially when accessing limited old classes during training. To\naddress these issues, we propose a novel attribute-text guided\nforgetting compensation (ATFC) model, which explores text-\ndriven global representations of identity-related information and\nattribute-related local representations of identity-free information\nfor LReID. Due to the lack of paired text-image data, we design\nan attribute-text generator (ATG) to dynamically generate a\ntext descriptor for each instance. We then introduce a text-\nguided aggregation network (TGA) to explore robust text-driven\nglobal representations for each identity and knowledge transfer.\nFurthermore, we propose an attribute compensation network\n(ACN) to investigate attribute-related local representations, which\ndistinguish similar identities and bridge domain gaps. Finally,\nwe develop an attribute anti-forgetting (AF) loss and knowledge\ntransfer (KT) loss to minimize domain gaps and achieve knowl-\nedge transfer, improving model performance. Extensive exper-\niments demonstrate that our ATFC method achieves superior\nperformance, outperforming existing LReID methods by over\n9.0%/7.4% in average mAP/R-1 on the seen dataset. Our code\nwill be available soon.", "sections": [{"title": "I. INTRODUCTION", "content": "Person re-identification (ReID) aims to retrieve the same\nindividual across non-overlapping cameras in a large-scale\ndatabase. ReID methods have achieved significant success us-\ning uni-modal architectures, such as convolutional neural net-\nworks (CNN) [1] or vision transformers (ViT) [2-4]. However,\ntraining datasets are continuously collected in dynamic moni-\ntoring systems, ReID models typically adapt to the current data\ndistribution, which can impair previously acquired knowledge\ndue to the well-known catastrophic forgetting challenge.\nRecently, lifelong person re-identification (LReID) methods\n[5, 6] have attracted more attention to learning from contin-\nuous data and balance anti-forgetting with knowledge acqui-\nsition. There are two main challenges to address this issue.\n1) Task-shared representations. Each identities with similar\nareas (i.e., clothing type) are difficult for existing LReID\nmethods [7, 8] to learn intrinsic task-shared representations.\nSome methods [9, 10] employ multi-branch to learn differ-\nent representations from multiple perspectives, focusing on\nidentity-related information to effectively distinguish identities\nin a task-specific manner. However, they ignore local areas\nwith identity-free information that could compensate identity-\nrelated information, limiting model perfrmace to investigate\ntask-shared representations when encountering highly similar\nidentity information. 2) Task-wise domain gap. The dataset\nfor each task is collected in different illumination and person-\nrelated information (pose, clothing, and etc.) environments,\nleading to task-wise domain gaps. More LReID methods\n[9, 11, 12] emphasize task-specific knowledge when learning\nnew information or distilling old knowledge, achieving sig-\nnificant results on seen data (training data). However, these\napproaches ignore task-wise domain gaps, potentially limiting\nthe model's generalization and anti-forgetting capabilities."}, {"title": "II. RELATED WORKS", "content": "To address the above challenges, we propose a novel\nattribute-text guided forgetting compensation (ATFC) model,\nwhich first explores task-shared representations based on at-\ntribute and text to improve anti-forgetting and generalization\nin LReID. To explore the role of task-shared representations\nfor identity and domian gaps, we divide task-shared repre-\nsentations into text-driven global representations of identity-\nrelated information and attribute-related local representations\nwith identity-free information, as shown in Figure 1(a). Due\nto the lack of paired text-image, we design a attribute-text\ngenerator to dynamically formulate text descripotor for each\ninstance, which significant enhance fine-grained text-driven\nglobal representations based on contrastive language-image\npre-training (CLIP) [13]. Attribute-text generator introduce an\nattribute recognition model [14] pre-trained on the PA100K\n[15] dataset to generate one-hot encodings of attribute cat-\negories (i.e., female, backpack, short/long sleeve, and etc.),\nwhich are converted into text descriptors for each image using\nan attribute-to-text template. Subsequently, the paired text-\nimage are fed into a text-guided aggregation network (TGA) to\ngenerate text-driven global representations, which explore fine-\ngrained global representations for each class by aggregating\ntext-image information. One-hot encodings of attribute cate-\ngories are processed by attribute compensation network (ACN)\nto generate attribute-related local representations that focus on\nlocal body information and serve as identity-free information.\nWhen identities encounter full-body similarity or local sim-\nilarity, text-driven global representations and attribute-related\nlocal representations complement each other to improve the\nrecognition accuracy of the model.\nFinally, we explore attribute-related local representations\nof identity-free information to minimize domain gaps, and\nthen realize knowledge transfer based on text-driven repre-\nsentations from previous task to the current task, as outlined\nin Figure 1(b). We develop an attribute anti-forgetting loss\nbased on attribute-related local representations of identity-\nfree information, which can effectively bridge domain gaps to\nenhance anti-forgetting and knowledge acquisition capabilities.\nWe reduce the interference between tasks and pull them closer\ntogether, laying the foundation for knowledge transfer across\nthe continuous domains. We design a knowledge transfer loss\nthat utilizes prior knowledge from previous tasks to better\nimprove acquisition and generalization abilities. The main\ncontributions of our ATFC are summarized as follows:"}, {"title": "A. Lifelong Person Re-Identification", "content": "Lifelong person re-identification strives to balance the anti-\nforgetting and acquisition capabilities of the model with con-\ntinuous data collected in different scenarios. Existing LReID\nworks can be divided into knowledge distillation-based and\nrepresentation-based approaches. Knowledge distillation-based\nmethods [5, 7, 9, 11] explore the consistency between the\nold model and the new model to alleviate the forgetting of\nprevious knowledge. Due to strict alignment in terms of fea-\ntures or identities, their capacity for acquiring new knowledge\nis limited. Representation-based approaches [6, 10, 12, 16]\naim to extract consistent or discriminative features from a\nunified feature perspective to improve the discrimination and\nadaptation capacity of the LReID model. However, they ignore\ntask-wise domain gaps, limiting the model's generalization\ncapacity. The PTKP [16] maps the features of the new task into\nthe feature space of the old tasks to reduce task-wise domain\ngaps and mine task-shared information. Instead, we explore\ntask-shared representations from identity-related and identity-\nfree perspectives to bridge domain gaps, thus improving the\nanti-forgetting and generalization capability of the LReID\nmodel."}, {"title": "B. Vision-Language for Person Re-Identification", "content": "Recently, contrastive language-image pre-training (CLIP)\n[17] has established a connection between natural language\nand visual content through the similarity constraint of text-\nimage pairs. CLIP has been applied to multiple ReID tasks,\nincluding text-to-image, text-based single-modality, and text-\nbased cross-modality. Text-to-image methods [18-20] aim to\nretrieve the target person based on a textual query. Text-\nbased single-modality works [2, 3] leverage text descriptors\nto generate robust visual features or to integrate the beneficial\nfeatures of text and images for person identification. Text-\nbased cross-modality methods [17, 21] employ text descriptors\nto alleviate visible-infrared modality gaps. However, these\nmethods use text descriptors that are manually annotated,\nprompt-based learning, and text inversion, which increases\ncosts and limits the model's ability to generate fine-grained\nrepresentations. Our method explores dynamic generation of\ntext descriptors without human interference to extract robust\ntask-shared representations for each identity."}, {"title": "III. METHODOLOGY", "content": "The overview of our ATFC model to surmount the forgetting\nof different old identities from both identity-free and identity-\nrelated information is depicted as Figure 2. The ATFC model"}, {"title": "A. Preliminary: Overview of Method", "content": "learns the old model $\\Phi^{t-1}$ and new model $\\Phi^{t}$ from (t-1)-th\nand t-th tasks, where $\\Phi^{t}$ is inherited from $\\Phi^{t-1}$. $\\Phi^{t-1}$ and\n$\\Phi^{t}$ with three branches of attribute-text generator (ATG), text-\nguided aggregation network (TGA) and attribute compensation\nnetwork (ACN). $\\phi^{t-1}$ and $\\phi^{t}$ serve as classifier heads for\nthe old and new models, providing logits of each instance\nfor recognition. Additionally, we define that consecutive T\nperson datasets $D = \\{D^{t}\\}_{t=1}^{T}$ are collected from different\nenvironments, and establish a memory buffer M to store a\nlimited number of samples from each previous ReID dataset.\nGiven an image $x^{i} \\in D \\cup M$, we forward it to $\\Phi^{t-1}$ and $\\Phi^{t}$ is\nas follows:\n$G^{t-1}, AG^{t-1} = \\Phi^{t-1}(x^{i}); G^{t}, AG^{t} = \\Phi^{t}(x^{i})$"}, {"title": "B. Attribute-Text Generator.", "content": "Some ReID methods leverage prompt learning [2, 22] and\ninversion models [3] based on CLIP to improve fine-grained\ninformation, which is critical for distinguishing identities\nin ReID. However, these strategies do not provide real text\ndesccriptors for an instance and are directly applied to LReID,\nlimiting the LReID model's ability to learn text-driven global\nrepresentations for each instance. Therefore, we propose\nan attribute-text generator (ATG) to dynamically generate\ncorresponding text descriptors for each instance. Specifically,\nwe first introduce an attribute recognition model pre-trained\non the PA100K dataset [15] to generate one-hot encodings of\nattribute categories (i.e., female, backpack, short/long sleeve,\nand etc.), which are then converted into text descriptors for\neach instance using a specific template. The specific template"}, {"title": "C. Text-Guided Aggregation Netwrok", "content": "We propose a text-guided aggregation network (TGA)\nusing the CLIP structure to explore text-driven global\nrepresentations for each identity and facilitate knowledge\ntransfer. The TGA includes a text-image encoder and a\nparallel fusion module (PFM). Note that the text encoder is\nfrozen in our ATFC model."}, {"title": "1) Text-Iamge Encoder", "content": "By attribute-text generator obtain\npaired text-image, we employ CLIP with text encoder T(\u00b7)\nand image encoder V(.) to extract text and image embedding,\nrespectively. Unlike CLIP, we introduce multiple [CLS]\nembeddings into the image encoder input sequence to capture\nglobal features from different perspectives, same as the\nnumber of attribute semantic information in the following\ntext. We define image embedding $[v_{1}, ..., v_{N}, v_{1}, ........., v_{P}]$\n= V(x), and text embeddings $d*$. N and P indicate the\nnumber of [CLS] tokens and patch respectively. Multiple\n[CLS] tokens of image embeddings are added $v* = \\sum_{i=1}^{N}$,\nand the dimensionality of the text embeddings $d*$ is upscaled\nto match the dimensionality of the image embeddings.\nTherefore, we aim to align text and image embedding to\nthe same identity. To this end, we fromulated the ID-related\ncontrastive loss as follows:"}, {"title": null, "content": "$L_{i2t} = \\frac{1}{B} \\sum_{i=1}^{B}log(\\frac{exp(sim(v*, d*)/\\tau)}{\\sum_{i=1}^{N}exp(sim(v*, d*)/\\tau)})$"}, {"title": null, "content": "$L_{t2i} = \\frac{1}{B} \\sum_{i=1}^{B}log(\\frac{exp(sim(d*, v*)/\\tau)}{\\sum_{i=1}^{N}exp(sim(d*, v*)/\\tau)})$"}, {"title": null, "content": "$L_{Con} = L_{i2t} + L_{t2i}$"}, {"title": "2) Parallel Fusion Module", "content": "To obtain robust text-driven\nglobal representations, we propose a parallel fusion module\n(PFM) to explicitly explore the interactions between image\nembeddings and text embeddings, as shown in Figure 2(PFM).\nFirstly, we leverage text embedding d* as query and image\nembedding $[v_{1}, ........., v_{N}, v_{1}, ..., v_{p}]$\nas key and value\nto implement operation with cross-attention, drop, and layer\nnormalization, getting text-wise representations. Similarly, in\nanother fusion branch, image-wise representation is obtained.\nFinally, image-wise and text-wise representations perform\nconcatenation and MLP operations to obtain a global text-\nimage representation. We adopt $G^{t} = \\{G_{i}|i = 1,2,..., N\\}$\nat\nthe current task, named by text-driven global representations.\nUltimately, we utilize the triplet loss $L_{CE}$ and cross-entropy\nloss $L_{Tri}$ [3] to optimize our framework at the current task."}, {"title": null, "content": "$L_{CE} = - \\frac{1}{K} \\sum_{i=1}^{K}y_{i}log((\\phi^{t} (G^{t})))$"}, {"title": null, "content": "$L_{Tri}^{ri} = max(d^{ri} - d^{ri} + m, 0)$"}, {"title": "D. Attribute Compensation Network.", "content": "Text-driven global representations of identity-related infor-\nmation focus on the global characteristics of each instance.\nHowever, when multiple identities are similar or have large\ndomain gaps, text-driven global representations may fail to\naccurately distinguish them. Therefore, we propose an attribute\ncompensation network (ACN) that explores attribute-related\nlocal representations to complement the knowledge provided\nby text-driven global representations, enhancing model per-\nformance. Intuitively, attribute-related local representations\ncapture the generic symbolic expressions of a task, which\ncan effectively bridge domain gaps. The ACN consists of\nan attribute decoder and an attribute consistency matching\ncomponent."}, {"title": "1) Attribute Decoder", "content": "We define multiple learnable\nattribute semantic information $A* = \\{A_{i} | i = 1,2,.........N\\}$\nto learn discriminative local body information. The one-hot\nencoding of the attribute undergoes a linear layer to increase\nits dimensions, and then multiplies with the text-image global\nrepresentation to output $f_{AT}$. Attribute semantic information\n$A*$ as queries Q, $f_{AT}$ as keys K and values V are input\ninto attribute decoder, which outputs the attribute features\n$\\mathbb{A} = \\{A_{i} | i = 1,2,......... N\\}$. The attribute decoder employs six\ntransformer decoder blocks (T_Block) referenced from [23]."}, {"title": "2) Attribute Consistency Matching", "content": "The attribute represen-\ntaions $A = \\{A_{zi} | i = 1,2,......,N\\}$ can learn multiple dis-\ncriminative local representaions. However, it is unclear which\nattribute features correspond to specific body parts. Thus, we\npropose a attribute consistency matching (ACM) module to as-\nsociate attribute features and text-driven global representations\n$G = \\{G_{i} | i = 1,2,.........N\\}$. The core objective is to find the\nmost similar text-driven global representationsG from different\nperspectives and local attribute representaions $\\mathbb{A}$, and then add\nthe features with the highest similarity. Specifically, attribute-\nrelated local representations $AG^{t} = \\{AG_{i} | i = 1,2,.........,N\\}$\nis formulated as:"}, {"title": null, "content": "$k = argmax(\\frac{<A_{i}, G>}{\\|A_{i}\\| |G|})$"}, {"title": null, "content": "$AG_{i} = A_{i} + G_{k}$"}, {"title": null, "content": "$L'_{Tri} = max(d^{p} - d^{n} + m, 0)$"}, {"title": "E. Attribute Anti-Forgetting Loss", "content": "Some methods [7, 12] focus on task-specific knowledge\nbut ignore task-shared representations in task-wise domain\ngaps, limiting the model's anti-forgetting and generalization\ncapabilities. We aim to reduce the distance between tasks\nand then focus on knowledge transfer in LReID. Thus, we\ndevelop an attribute anti-forgetting loss to explore attribute-\nrelated local representations with identity-free information to\naddress domain gaps. Attribute-related local representations\nserve as a connective element to bridge the gaps between\nprevious and current tasks. The attribute anti-forgetting loss\nis expressed as follows:"}, {"title": null, "content": "$L_{AF} = \\sum_{i=1}^{B}KL(AG_{i}^{t-1}/\\tau || AG_{i}^{t}/\\tau)$"}, {"title": "F. Knowledge Transfer Loss", "content": "The attribute anti-forgetting loss effectively bridge gomain\ngaps, ignoring knowledge transfer across increasing domian.\nThus, we propose a knowledge transfer loss to explore the\nknowledge consistency under text-driven global representa-\ntions between previous task and current task, including or-\nthogonal loss, alignment loss, and logit-level distillation loss.\nWe force text-driven global reprsentatios $G^{t}$ at the current\ntask to learn more discriminative information by orthogonal\nloss to minimize the overlapping elements. The orthogonal\nloss can be formulated as:"}, {"title": null, "content": "$L_{ort} = \\sum_{i=1}^{N-1}\\sum_{j=i+1}^{N} (G_{i}^{t}G_{j}^{t})$"}, {"title": "IV. EXPERIMENTS", "content": "We argue that the distribution between the old model and\nthe new model under text-driven global representations should\nremain consistent to alleviate forgetting old identities. There-\nfore, we propose a alignment loss to explore the knowledge\ntransfer of text-driven global representation from the old model\nto the new model, as follows:"}, {"title": null, "content": "$L_{GR} = \\frac{1}{B} \\sum_{i=1}^{B}KL(G_{i}^{t-1}/\\tau || G_{i}^{t}/\\tau)$"}, {"title": null, "content": "$L_{LA} = \\frac{1}{B} \\sum_{i=1}^{B}KL((\\phi^{t-1}(G^{t-1}))/\\tau || (\\phi^{t} (G^{t}))/\\tau)$"}, {"title": null, "content": "$L_{KT} = L_{ort} + L_{GR} + L_{LA}$"}, {"title": null, "content": "$L = L_{CE} + L_{Tri} + L_{AF} + L_{KT}$"}, {"title": "V. CONCLUSION", "content": "In this paper, we propose an attribute-text guided forget-\nting compensation (ATFC) method, which first investigates\nattribute and text characteristics to enhance the anti-forgetting\nand generalization capabilities in LReID. We introduce an\nattribute-text generator (ATG) to dynamically generate paired\ntext-image data, providing more fine-grained text-driven global"}]}