{"title": "Utility-inspired Reward Transformations Improve Reinforcement Learning Training of Language Models", "authors": ["Roberto-Rafael Maura-Rivero", "Chirag Nagpal", "Roma Patel", "Francesco Visin"], "abstract": "Current methods that train large language models (LLMs) with reinforcement learning feedback, often resort to averaging outputs of multiple rewards functions during training. This overlooks crucial aspects of individual reward dimensions and inter-reward dependencies that can lead to sub-optimal outcomes in generations. In this work, we show how linear aggregation of rewards exhibits some vulnerabilities that can lead to undesired properties of generated text. We then propose a transformation of reward functions inspired by economic theory of utility functions (specifically Inada conditions), that enhances sensitivity to low reward values while diminishing sensitivity to already high values. We compare our approach to the existing baseline methods that linearly aggregate rewards and show how the Inada-inspired reward feedback is superior to traditional weighted averaging. We quantitatively and qualitatively analyse the difference in methods, and see that models trained with Inada-transformations score as more helpful and less harmful than baselines.", "sections": [{"title": "1 Introduction", "content": "Training Large Language Models (LLMs) for user applications often requires two steps: pre-training on large text corpora to create base foundation models (Bommasani et al., 2021), and fine-tuning with supervised and reinforcement learning feedback to align models to human preferences (Christiano et al., 2017; Stiennon et al., 2020). This process of reinforcement learning with human feedback (RLHF) often involves one or more trained reward models that score LLM-generated text along several dimensions e.g., helpfulness, harmlessness, or adherence to facts (Glaese et al., 2022; Bai et al., 2022), and guide future model generations towards these desired properties."}, {"title": "2 Background & Preliminaries", "content": "Our reward transformation builds on Economics theory on shaping utility functions. We outline relevant terminology and training procedures in this section, which form the preliminaries of our method introduced in Section 3."}, {"title": "2.1 Reinforcement Learning from Human Feedback (RLHF)", "content": "RLHF is a technique used to align language models with human preferences. Unlike traditional reinforcement learning which relies on pre-defined reward functions, RLHF leverages human judgments to guide the learning process. This iterative process typically involves three main stages:\n1. Supervised Fine-Tuning (SFT): An initial pretrained LLM $\\pi_{PRE}$ is fine-tuned on a dataset of human demonstrations. These consist of input-output pairs representing desired model behavior, providing an initial alignment with human preferences. The resulting policy is denoted as $\\pi_{SFT}$.\n2. Reward Model Training: A reward model $r_{\\theta}$ is trained to predict the human preference over different model outputs. This is typically done using a dataset of comparisons between responses, where humans indicate which output is preferred. The reward model learns to assign high scores to outputs that are aligned with human preferences.\nThe Bradley-Terry model is often used to estimate the relative preferences. This model assumes the probability of preferring response i over response j to be:\n$P(i > j) = \\frac{e^{o_i}}{e^{o_i} + e^{o_j}}$,\nwhere $o_i$ and $o_j$ represent the preference score for response i and j respectively. The reward function is often determined using maximum likelihood estimation (MLE) from pairwise comparisons of answers to a prompt. Given a prompt x and responses $y^+$ (preferred) and $y^\u2212$ (less preferred), the log-likelihood is:\n$L(\\theta) = -E_{(x,y^+,y^-)}[log(\\sigma(r_\\theta(x,y^+)-r_\\theta(x, y^-))))]$,\nwhere $\\sigma$ is the sigmoid operator.\nMLE estimates of $r_{\\theta}(x, y)$ maximize this likelihood, often using gradient-based optimization. The resulting trained function serves as a reward during subsequent RL fine-tuning. Multiple reward models $r_{\\theta_i}$, each capturing a different aspect of desired behavior (e.g., helpfulness, harmlessness), can be trained in this way.\n3. Reinforcement Learning (RL) Fine-tuning: The fine-tuned LLM from the SFT step is further refined using RL, guided by the reward model(s) from the previous step. The LLM acts as a policy, generating text, and the reward model provides feedback, guiding the policy towards generating text that maximizes the expected reward. Common RL algorithms used in this stage include Proximal Policy Optimization (PPO) (Schulman et al., 2017) and REINFORCE (Williams, 1992), which optimize the policy such that the expected average reward $R(x, y) := \\sum_i \\alpha_i *r_{\\theta_i} (x, y)$ is maximized. The solution $\\pi_{RL}$ is obtained by maximising:\n$E_{(x,y)\\sim D_{RL}}[log \\frac{\\pi_{RL}(y|x)}{\\pi_{SFT} (y|x)}] - \\beta D_{KL}(\\pi_{RL}(x,y), \\pi_{SFT}(x,y)) $,\nwhere intuitively, we are trying to maximize the sum of multiple rewards, constrained by a KL regularization term that keeps the policy close to the initial starting policy."}, {"title": "2.2 Economic Theory", "content": "The underlying problem in RLHF\u2014specifically, modeling and using human preferences to determine outcomes, has long been studied in various sub-fields of economics. Microeconomic theory, and particularly behavioral economics, delve into understanding the shapes of individual utility functions, aiming to capture the nuances of human preferences and decision-making under uncertainty. This field explores a wide array of assumptions and functional forms to represent how individuals derive satisfaction from different outcomes, moving beyond simple linear models to account for phenomena like risk aversion and loss aversion. Crucially, certain properties of utility functions, such as those embodied by the Inada conditions, have implications for how we might design and aggregate reward signals in the context of RLHF."}, {"title": "2.3 Utility Functions", "content": "For an individual, a utility function $u : A \\rightarrow R$ is a mapping from units of a good to some real value that denotes their welfare or satisfaction from consuming that good. True satisfaction is hard to measure, but it can be estimated through human's preferences over goods or over lotteries on quantities of a good.\nIf an individual seeks to maximize their expected utility, the shape of the utility function captures the trade-off between return and risk. Utility functions typically fall into one of three fundamental shapes: concave, convex, or linear. These shapes correspond to distinct risk preferences-concave functions reflect risk aversion, convex functions indicate risk-seeking behavior, and linear functions represent risk neutrality."}, {"title": "2.4 Inada Conditions & Shaping Utilities", "content": "The Inada conditions are a set of assumptions about the shapes of utility functions (Uzawa, 1961). For the sake of exposition, consider a function $u(.)$ that represents the utility obtained as a function of bread consumption. Some of the desirable conditions for functions that represent utilities are:\n1. The more bread consumed, the more utility one gets. Formally $\\frac{\\partial u(x)}{\\partial x} > 0$.\n2. The more bread its consumed, the less utility a new piece of bread provides: $\\frac{\\partial^2 u(x)}{\\partial x^2} < 0$.\n3. In the limit, when one has infinite bread, more bread doesn't provide utility: $lim_{x\\rightarrow\\infty} \\frac{\\partial u(x)}{\\partial x} = 0$.\n4. In the starvation limit, when one has no bread, bread provides huge utility: $lim_{x\\rightarrow 0} \\frac{\\partial u(x)}{\\partial x} = \\infty$."}, {"title": "2.5 Relative Risk Aversion Utility Functions", "content": "The Inada conditions provide a framework to reason about reward aggregation under our desiderata, namely decreasing the importance of rewards beyond a satisfactory threshold, while increasing the weight of critically low rewards.\nA well-known utility function that satisfies these properties is the Constant Relative Risk Aversion (UCRRA) function (Ljungqvist and Sargent, 2018; Pratt, 1978):\n$U_{CRRA}(C) = \\begin{cases} \\frac{C^{1-\\gamma}-1}{1-\\gamma}, & \\text{if } \\gamma \\geq 0, \\gamma \\neq 1 \\\\ ln(C), & \\text{if } \\gamma = 1 \\end{cases}$,\n$U_{CRRA}(C)$ describes the satisfaction a decision-maker derives from consuming a certain amount of a good C, where the parameter $\\gamma$ controls the individual's risk aversion.\nA higher $\\gamma$ indicates greater risk aversion: the individual is less willing to take on risk for potential gains. Note that, although $\\frac{C^{1-\\gamma}-1}{1-\\gamma}$ is undefined for $\\gamma = 1$, in the limit it behaves like the logarithm $lim_{\\gamma\\rightarrow1} \\frac{C^{1-\\gamma}-1}{1-\\gamma} = ln(C)$."}, {"title": "3 Inada Inspired Reward Aggregation", "content": "In this section we discuss the problems that arise from simple linear aggregation of reward functions, show how the Inada-inspired transformation can alleviate some of these concerns, and derive it mathematically. We will provide empirical evidence of improved performance over baselines in Section 5."}, {"title": "3.1 Limitations of Linear Aggregation", "content": "When faced with multiple rewards $r_1(.), . . ., r_n(.)$, a common approach is to linearly aggregate them into a single reward $R(.) = \\sum_{i=1}^{n} r_i(.)$ by simply performing a weighted average of the rewards. However, there are several consequences that a simple aggregation overlooks, as we illustrate below.\nInsensitivity to critically low rewards Linear aggregation is insensitive to extremely low values in individual reward dimension, which can be overshadowed by many marginally positive rewards when aggregated linearly. This insensitivity to any individual reward can exacerbate safety issues e.g., if the reward was pertaining to political bias when talking about a news report on the elections.\nOver-prioritizing high rewards Since linear aggregation indiscriminately prioritizes increases in all rewards, boost in rewards that are already beyond satisfactory are as welcomed to increments in rewards that are unacceptably low. This can lead to wasteful optimization of already adequate reward dimensions, at the expense of much more critical ones.\nIf each reward $r_i$ has an acceptable threshold $r_i > \\tau_i$ within which an answer is deemed satisfactory, any extra increase in $r_i$ only represents a marginal improvement, and should be deprioritized.\nHere, $\\tau$ represents a threshold of a minimum desirable reward that we want any generation for an LLM to have."}, {"title": "3.2 An Inada-Inspired Utility Function", "content": "In Section 2 we discuss how the utility function $U_{CRRA}$ alleviates the above limitations, by assigning diminishing returns to high values and pushing low values quickly to -$\\infty$, as dictated by the Inada conditions. However, $U_{CRRA}$ implicitly assumes a reward threshold of zero, whereas we require the flexibility to specify arbitrary threshold values $\\tau_i$ tailored to each individual reward dimension. We therefore propose an alteration to $U_{CRRA}$ that we call the Inada Reward Transformation (IRT), shown in Equation (5). Note that $U_{CRRA}(1) = 0$ for any value of $\\gamma$, thus IRT is continuous.\n$IRT(r_i) = \\begin{cases} U_{CRRA}(r_i - \\tau_i + 1) & \\text{if } r_i > \\tau_i \\\\ \\beta_i(r_i - \\tau_i) & \\text{if } r_i \\leq \\tau_i \\end{cases}$"}, {"title": "3.3 Parameters of the IRT", "content": "There are three parameters important to this formulation: the diminishing returns parameter $\\gamma$ that controls the curvature of the right side of the function, the reward threshold $\\tau$ that defines a satisfactory minimum threshold for rewards, and the the penalty factor $\\beta$, that indicates the degree at which we want to penalize low rewards."}, {"title": "3.4 Partial or Full IRT", "content": "Since the Inada transformation is applied to each reward individually, we can formulate different versions of the IRT depending on how many rewards we transform before aggregation. We refer to the case of all rewards being transformed as Full IRT, and to all other cases as Partial IRT."}, {"title": "4 Experimental Methodology", "content": "We now wish to assess whether a standard LLM RLHF training pipeline with the Inada transformation provides an improvement in performance over a baseline that linearly aggregates rewards. In this section, we outline the models and datasets used, as well as the evaluation procedure."}, {"title": "4.1 Models & Training", "content": "We use the Gemma 2B pretrained model (Gemma Team et al., 2024) taken before the RLHF step (i.e., after supervised fine tuning but before any reinforcement learning finetuning). This base model serves as both the base LLM policy as well as the two reward models. We train the models using RE-INFORCE Policy Gradient (Williams, 1992) with a value function estimation as the baseline for our RL algorithm, along with KL regularization to the SFT checkpoint, and optimize on the estimated reward. The 2 reward functions (helpfulness and harmlessness) have been trained using half of the training sets of the helpfulness and harmlessness datasets following the procedure described in Section 2.1. We use the second half of the train split of the helpfulness dataset for the alignment step."}, {"title": "4.2 Datasets", "content": "We use the Anthropic Helpfulness and Harmlessness dataset (Bai et al., 2022), that consists of multi-turn conversations between human users and a 52B context distilled LLM. In particular, it contains pairs of conversations that are identical except for the final LLM answer, with a preference given by human evaluators on which option was considered more helpful (helpfulness dataset) or harmless (harmlessness dataset). We save 2k samples from the training set of each dataset to use as a validation set. We use half of the remainder to train the reward models, and the other half to train the base LLM with RLHF feedback using the trained reward models. We report performance on the test split from the original dataset (2k samples). For more details, see Appendix D."}, {"title": "4.3 Evaluation & Metrics", "content": "To evaluate the performance gain of our method, we compare models trained with IRTs to the baseline model. We empirically evaluate model generations using LLM-autorater based scores.\nWith improvements in reasoning capabilities of LLMs, their use as evaluators of generated text has become a standard evaluation measure (Vu et al., 2024; Singhal et al., 2023; Eisenstein et al., 2023) We use zero-shot autoraters prompted to evaluate the safety of responses as a binary rating; as well as autoraters prompted to express their preference between the two responses on a 5 scoring system:"}, {"title": "4.4 IRT Parameters", "content": "We want to improve the alignment of our model to become more harmless, ideally without degradation on helpfulness. To this end, we decided to apply a Partial IRT on the harmlessness reward, aggressively disincentivizing harmful responses, while retaining the original signal on helpfulness. We compare our model $R_{IRT}$ against a baseline reward model $R_B$ trained by aggregating unaltered rewards linearly (which is effectively equivalent to an IRT model with $\\beta = 1, \\gamma = 0, \\tau = 0$).\n$R_{IRT}(z) = IRT(r_{ha}(z)) + r_{HE}(z)$\n$R_B(z) = r_{HA}(z) + r_{HE}(z)$\nTo select the IRT hyper-parameters we performed a grid search on a small set of values on the helpfulness and harmlessness validation sets and found the following optimal IRT values:"}, {"title": "5 Results", "content": "We now discuss the empirical difference between LLMs trained with our proposed transformation versus a baseline with no transformations."}, {"title": "5.1 Preference Results", "content": "Table 1 reports the values scored by the IRT model on the metrics described in Section 4.3. The associated standard errors quantify the uncertainty.\nThe proposed IRT yields substantial gains on the harmlessness metric (the $AR(HA)$ autorater prefers the IRT model 75% of the times), while retaining performance - and even slightly improving it - on the helpfulness score (the $AR(HE)$ autorater rates the IRT model's helpfulness equal or higher than the baseline 52% of the times)."}, {"title": "5.2 Influence of Transformation Parameters", "content": "The IRT is determined by its point of application, namely the threshold $\\tau$, the steepness of the linear transformation of the rewards below the threshold, governed by $\\beta$, and the curvature of the transformation that dampens the values above the threshold, controlled by $\\gamma$. As noted in Section 4.4, an IRT with parameters $\\beta = 1, \\gamma = 0$ and $\\tau = 0$ corresponds to the identity function, effectively returning the reward unaltered.\nStarting from the optimal set of parameters determined in Section 4.4, we evaluate the effect of each component of the IRT on the final performance by setting them to their \"identity value\" one at the time, and report the results in Table 2. Since the optimal $\\tau$ already corresponds to its identity value, we only ablate $\\beta$ and $\\gamma$.\nWe empirically observe that removing the transformation of the rewards below the threshold (i.e., $\\beta = 1$) results in a slight decrease in harmlessness and a severe drop in helpfulness, which we confirmed by looking at the data that was attributable to a very pronounced punting behavior. While the model has higher win rate against the baseline on harmlessness on strict preferences, we note that the number of ties increases significantly, determining the overall poorer performance.\nInhibiting the transformation of the rewards above the threshold (i.e., $\\gamma = 0$) instead, yields a slight drop in harmlessness performance counterbalanced by a slight improvement in helpfulness. While being greater than the standard error, these variations appear to be remarkably modest. This is in line to our expectation, since the $\\gamma$ parameter discourages excessive focus on a single reward, but $\\beta$ plays a more central role in heavily penalizing very harmful responses."}, {"title": "5.2.1 Training Analyses", "content": "The overhead due to the Inada Reward Transformation is negligible with respect to the overall LLM training time. We report compute and training details in Appendix E and Appendix A."}, {"title": "5.3 Qualitative Analyses", "content": "We qualitatively compare the generations of our IRT model and the baseline's. We report examples of generations in Table 3.\nA significant challenge that we identified is reward hacking, where the model exploits the reward system rather than genuinely improving its behavior. In particular, we observe artifacts such as punting statements to inflate the harmlessness reward artificially. Similarly, another issue is the use of excessive smiley faces and catchphrases (e.g., \"I hope this helps!\") to increase its helpfulness score."}, {"title": "6 Discussion", "content": "There is a growing interest in re-examining RLHF through the lens of economic theory, like social choice theory, with a focus on understanding its theoretical properties and exploring pathways to achieve more robust AI alignment (Conitzer et al., 2024; Ge et al., 2024). Our work aims to empirically show the effectiveness of economic insights when applied to LLM training and alignment. Our work aims to apply these theoretical insights into the training of language models. Our exploration reveals some sensitivity to hyperparameter selection. More refined techniques than grid search could likely improve these results, an exciting direction of research that we intend to investigate in the future. Our analysis shows that it's possible to obtain remarkable gains in harmlessness at little to no expense in helpfulness, without making the algorithm significantly more complex, nor computationally expensive. This unveils a tremendous potential for theoretically motivated insights from fields like social choice theory to positively impact methods in NLP. We hope that this work allows for further exploration of similar techniques grounded in economic theory."}, {"title": "7 Related Work", "content": "This work contributes to the growing body of research on improving reward model design and aggregation when training LLMs with RLHF feedback. It also draws on a separate body of work that aims to combine insights from economics and game theory into language models. We situate our work within these two bodies of work and outline relevant literature from both below."}, {"title": "8 Conclusion", "content": "In this paper we introduce a reward transformation method that can be applied to RLHF pipeline of LLMs. This approach addresses limitations of previous reward aggregation methods, specifically in their failure to adequately penalize extremely negative rewards and prioritize improvements in critically low-performing areas. Our method is theoretically motivated with insights from economic theory, and we demonstrate how an existing utility function can be adapted to transform rewards used for reinforcement learning feedback. We demonstrate improved performance of our method on benchmark datasets, and show how the generations from the new models improve in critical reward areas. Our findings highlight the potential of incorporating insights from economic theory into RLHF, that we hope future work can build off of, to build models better aligned to human preferences."}, {"title": "9 Limitations", "content": "Our method shows how an existing utility function can be adapted and applied to transform rewards used in RLHF pipelines to allow for better reward aggregation and improved performance of models. There are several limitations of this experimental study that we outline below. For one, this study primarily focuses on small-scale models (2B parameters), that are significantly smaller than the current state-of-the-art language models (e.g., up to 1 trillion parameters). Since we could not run experiments on models of that size, this leaves open questions about the scalability of the findings in this paper. While we would expect the utility-inspired transformations to hold regardless of the model size, future research should address whether the observed benefits persist as model size increases. Additionally, while our approach is centered on reward aggregation, it is not restricted to this paradigm. For another, there are several avenues for future research within this method. Further investigation into optimal threshold selection methods for the $\\tau$ parameter seem crucial to the reward transformation, and method that allow for better searching of parameter values, or learning this parameter would allow significant improvements. Furthermore, exploring the interplay our transformation and other techniques that mitigate reward hacking (e.g., reward model averaging or constrained optimization) warrants exploration. Future work can also look into different types of utility functions that could inform the reward transformation for different contexts or datasets that they might be tuned towards."}, {"title": "10 Ethics Statement", "content": "Our work focuses on methods that aim to align language models to human preferences. While our proposed method is a modification of existing RLHF pipelines that aims to mitigate potential harms, we outline all potential ethical implications in this section. First, our focus is on transforming existing reward models that have been trained on human-collected preference and safety data. The effectiveness of our approach therefore hinges on the quality of the data, and biases present in datasets can propagate to the LLM, leading to unfair or discriminatory outcomes. It is also worth pointing out that the definition of \"harmful\" content can be subjective and culturally dependent, requiring careful consideration of diverse perspectives. We only use standard benchmark datasets that are not specifically curated to represent diverse viewpoints, hence, models aligned with our method-similar to all existing models\u2014lack the sensitivity to perspectives that are not prevalent in large-scale datasets. Second, the improved performance of LLMs in specific dimensions, such as helpfulness, does not guarantee their overall safety or ethical use. While this method helps with reward hacking, it might still be vulnerable to jailbreaking. Malicious actors could potentially exploit the enhanced capabilities of these models for harmful purposes, such as generating persuasive disinformation or crafting targeted phishing attacks. We recognize the importance of ongoing research to address these ethical challenges. Future work should investigate methods for debiasing training data, developing more robust safety measures, and establishing clear ethical guidelines for the development and deployment of LLMs."}, {"title": "A RLHF Details", "content": "We use REINFORCE Policy Gradient (Williams, 1992) with a Value function estimation as the baseline as our RL algorithm along with KL regularization to the base checkpoint to optimize for the estimated reward. See Table 4 to see the different hyperparameters chosen."}, {"title": "B Autorater Prompts", "content": "_HELPFULNESS_PROMPT_TEMPLATE = \"\"\"\nFor the following query to a chatbot, which response is more helpful?\nQuery:\n{prompt}\nResponse A: {response_A}\nResponse B: {response_B}\nFIRST provide\na one-sentence comparison of the two responses and explain which you feel is more helpful. SECOND, you will output\nyour final verdict on which answer is more helpful. Your final verdict always is one of the following choices:\n1. Response A is significantly more helpful: [[A>>B]]\n2. Response A is\nslightly more helpful: [[A>B]]\n3. Tie,\nrelatively the same: [[A=B]]\n4. Response\nB is slightly more helpful: [[B>A]]\n5.\nResponse B is significantly more helpful: [[B>>A]]\"\"\"\n_HARMLESSNESS_PROMPT_TEMPLATE = \"\"\"\nFor the following query to a chatbot, which response is more harmless?\nQuery:\n{prompt}\nResponse A: {response_A}\nResponse B: {response_B}\nFIRST provide\na one-sentence comparison of the two responses and explain which you feel is more harmless. SECOND, you will output\nyour final verdict on which answer is more harmless. Your final verdict always is one of the following choices:\n1. Response A is significantly more harmless: [[A>>B]]\n2. Response A is\nslightly more harmless: [[A>B]]\n3. Tie,\nrelatively the same: [[A=B]]\n4. Response\nB is slightly more harmless: [[B>A]]\n5.\nResponse B is significantly more harmless: [[B>>A]]\nComparison: <one-sentence comparison and explanation>\nFinal verdict: \"My final verdict is tie:\n[[A=B]]\".\""}, {"title": "C Alternative transformations", "content": "In this section we will show a list of results on the hyperparameter search. These are summarized on Tables 7 to 9 and 11.\nIn addition to these and the experiments presented in Section 5, we conducted a set of experiments with a Partial IRT on the Helpfulness reward. Tables 5, 6, 10 and 12 show a summary of the results."}, {"title": "D Data Availability Statement", "content": "This work builds on the helpfulness harmlessness Anthropic dataset (Bai et al., 2022), which is itself composed of a helpfulness and a harmlessness dataset, and is publicly available under an MIT license. This dataset is intended to be used for RLHF, which is consistent with its usage in this paper, and - in line with the nature of the harmlessness subset - it contains toxic and harmful text. Its usage in this work is aligned with its intended purpose: improving the alignment of LLMs. Each of the two datasets contains a train split of 40k units and a test split of 2k units, and is formed by a collection of dialogues in English."}, {"title": "E Model and Compute Resources", "content": "Each main experiment involves training a language model with reinforcement learning feedback. Following (Gemma Team et al., 2024), we use a 2x16x16 configuration of TPUv5e on 512 chips with 1-way model sharding. Each experiments took us around 40-44 TPU hours. We refer the readers to the original Gemma paper and open-sourced implementation to replicate all experiments."}, {"title": "F Example Model Generations", "content": null}]}