{"title": "MSEval: A Dataset for Material Selection in Conceptual Design to Evaluate Algorithmic Models", "authors": ["Yash Patawari Jain", "Daniele Grandi", "Allin Groom", "Brandon Cramer", "Christopher McComb"], "abstract": "Material selection plays a pivotal role in many industries, from manufacturing to con- struction. Material selection is usually carried out after several cycles of conceptual design, during which designers iteratively refine the design solution and the intended manufacturing approach. In design research, material selection is typically treated as an optimization problem with a single correct answer. Moreover, it is also often restricted to specific types of objects or design functions, which can make the selection process computationally expensive and time-consuming. In this paper, we introduce MSEval, a novel dataset which is comprised of expert material evaluations across a variety of design briefs and criteria. This data is designed to serve as a benchmark to facilitate the eval- uation and modification of machine learning models in the context of material selection for conceptual design.", "sections": [{"title": "1 Introduction", "content": "The process of material selection is critical across a wide vari- ety of industries, ranging from manufacturing to construction. It involves making informed decisions about the materials used in various products, structures, and systems. Traditionally, material selection occurs after iterative design cycles, where design mod- ifications and manufacturing methods are refined. However, this approach often neglects the crucial role that materials may play in shaping the performance, cost, and sustainability of the final product.\nHistorically, material selection has often followed a manufac- turing first approach in which designers and engineers tend to pri- oritize manufacturing feasibility over material suitability. Conse- quently, this bias can lead to suboptimal material choices, affect- ing product performance, durability, and overall quality. Not all designers have extensive material expertise. Many lack the knowl- edge necessary to evaluate material properties, compatibility, and trade-offs effectively. Bridging this knowledge gap is essential for informed decision making. Material selection decisions made early in the design process significantly influence downstream activities, so it is crucial to ensure that these decisions align with the project goals and constraints.\nTypically, during the early design stage, material requirements are often expressed in the form of text and natural language. De- signers describe desired properties, constraints, and performance criteria in qualitative terms, which then need to be interpreted and translated into specific material choices. This process can be chal- lenging due to the nuanced and context-dependent nature of natural language descriptions.\nThe recent development of large language models (LLMs) in the machine learning domain [1-6] presents a novel opportunity to streamline the material selection process in the mechanical de- sign domain. LLMs are a type of model capable of generating text. They have been used successfully in various domains, in- cluding design [7-13], where they have demonstrated the ability to generate innovative and creative visual and textual design solu- tions. As these AI systems continue to become more integrated into our daily lives, it is essential to effectively identify potential shortcomings, and ensure that they can handle complex, human- centric tasks effectively. Conventional evaluation benchmarks for LLMs frequently fall short in accurately assessing their overall ca- pabilities for handling human-level tasks. As a result, there is an increasing demand for a human-centric benchmark that enables ro- bust evaluation of foundational models within the context of tasks relevant to human reasoning and problem-solving [14].\nIn this paper, we introduce an evaluation dataset curated from human survey responses that is designed to evaluate the abilities of LLMs for the task of material selection in conceptual design. This benchmark is curated from responses from a survey sent out to professionals working in the fields of material science, material selection, and design engineering, among others. The responses come from people working with varying levels of experience, and therefore capture the variance in the thought process and inherent considerations that come with different design tasks. By focus- ing on these different design tasks, our benchmark enables a more meaningful and contextual evaluation of the performance of the large language model in scenarios directly relevant to material se- lection. The overall approach is shown in figure 1.\nThe remainder of this paper is organized as follows. Section 2 reviews literature that is relevant to the dataset. Next, Section 3 reviews the the methodology used to generate the dataset as well as the organization of the dataset itself. Section 4 discusses sev- eral possible ways in which this dataset might be used to support research. Finally, Section 5 summarizes the contributions and dis- cusses limitations and future work. Ultimately, we aim to drive innovation in developing more reliable AI assistants that advance toward this niche domain by identifying areas for improvement and understanding the behavior of humans better. Our research under- scores the importance of evaluating foundation models in the con-"}, {"title": "2 Background", "content": "In this section we review works related to material selection and its challenges and motivation to work on this problem.\n2.1 Significance and Challenges in Material Selection. Material selection is a critical part of designing and producing any physical object. Material selection occurs in the early stages of the design workflow and maintains relevance beyond the useful life of a product. Materials directly influence the functionality, aesthetics, economic viability, manufacturing feasibility, and ultimately its en- vironmental impact of a design [15-17]. M. F. Ashby is often cited for presenting a systematic approach to material selection through the use of bubble plots, known as \"Ashby\" diagrams, which allow a designer to evaluate up to two material properties to identify those materials that perform above a desired threshold [18]. This approach requires an intimate understanding of a product's design intent, the design priorities (such as low mass), constraints (man- ufacturing process), and other requirements relevant to the object being designed (industry regulations). In recent years, additional factors have become increasingly important to consider too. Sus- tainability, for example, is a growing global concern and manu- facturing alone is reported to contribute significantly to resource consumption and greenhouse gas emissions. Thus, selecting ma- terials with lower environmental impact, such as recycled content or those that require less energy to produce, aligns with ethical practices and growing consumer expectations [19-21]. Material availability is also becoming a critical consideration due to supply chain disruptions, geopolitical challenges, or regulations on mate- rial use. The growing complexity of design requirements does not reduce the implications of improper material selection which can lead to increased overall costs, product failure, or greater environ- mental harm [22].\nIn product design, material selection is often decomposed into a general five-step procedure: (1) establishing design requirements, (2) screening materials, (3) ranking materials, (4) researching ma- terial candidates, and (5) applying constraints to the selection pro- cess [23]. Charts of performance indices and material properties, called Ashby diagrams, are often used to visualize, filter, and clus- ter materials [23,24].\nTraditionally, material selection has relied heavily on engineer- ing intuition and familiarity with existing materials. Particularly in industries with less prescriptive standards or specifications [25,26]. Even with Ashby's systematic approach to material selection, the process is nontrivial and can still leave designers with uncer- tainty as to how well a candidate material will perform in real- ity [18,24,27]. Data and knowledge are essential, without which a limited exploration of alternative or innovative options can oc- cur, leading to suboptimal designs [28,29]. Although established methods like Ashby's can guide designers and encourage them to consider a wider range of possibilities, material databases [30] can- not often account for the ever-growing universe of materials and broadening design considerations outlined thus far. Selection can also be subjective, potentially overlooking promising new materials simply because designers are unfamiliar with them [31,32]. Un- certainty regarding the performance of novel materials can further hinder their adoption. Furthermore, manufacturing innovations such as additive manufacturing are allowing previously unfeasible materials to now become viable options [33]. This highlights the need for a data-driven approach to material selection, one that can objectively evaluate a broader range of options while considering the complex interplay of design requirements and provide infor- mation when selecting a particular option [34]. Machine learning and algorithmic models offer powerful tools for material selection [35,36]. By learning from vast datasets of past design experiences and material properties, models can provide valuable insights that would otherwise require extensive research or experimentation.\n2.2 Existing Evaluation Datasets. To establish robust eval- uation standards and effectively monitor model performance, re- liable benchmarks play a pivotal role. While several well-known benchmarks exist for single-task and generalistic evaluation, they predominantly focus on assessing specific machine skills using ar- tificially curated datasets. For instance, the SQUAD dataset [37] evaluates answer extraction ability, and the SNLI dataset [38] as- sesses natural language inference capability. Additionally, the GLUE [39] and SuperGLUE [40] datasets serve as litmus tests for language models across various NLP tasks. However, these benchmarks often lack real-world applicability and fail to address complex reasoning abilities that align with human behaviors.\nTo bridge this gap, researchers have introduced novel datasets such as the MMLU [41] and AGI-Eval [14]. These datasets take a more holistic approach by collecting diverse subject data, guiding evaluation toward a human-centric perspective. By incorporat- ing real-world scenarios and nuanced reasoning challenges, they provide a more comprehensive assessment of language models' capabilities.\nAlthough these benchmarks are comprehensive, they do not cover many niche applications that have unique requirements and"}, {"title": "3 Methodology", "content": "In this section, we elaborate on the methodology that we used to collect data and the principles to make it accessible. In the first step 3.1, we collected materials selection perspectives from professionals through an online survey. Following the responses, we clean the responses to make two different variants of the dataset for different uses. In the final stage 3.3, we elaborate on the FAIR principles.\n3.1 Survey Design. To collect data to act as the evaluation data set, we conducted an online survey among professionals in de- sign, material science, engineering, and related fields. Specifically, our goal was to get responses from people with varied experience in the field of material selection for mechanical design.\nThe survey queried participants across 4 design cases (Kitchen Utensil Grip, Spacecraft Component, Underwater Component, and Safety Helmet for Sport) and 4 design criteria (Lightweight, Heat Resistant, Corrosion Resistant and High Strength) com- bined in a full factorial experimental design to produce 16 scenario- based questions. In each question, participants were asked to score a set of materials (steel, aluminum, titanium, glass, wood, ther- moplastic, elastomer, thermoset, and composite) on a scale from 0 to 10, with 0 being unsatisfactory in the specific application and 10 being an excellent choice. These material categories were cho- sen to cover a breadth of design use cases, to find a balance be- tween high-level and low-level material categories, and to limit the length of the survey. An example of a survey question is shown in Figure 2. The survey also collected basic demographic informa- tion to ensure that participants had the necessary knowledge and background to provide strong preferences for material selection.\nWe utilized Qualtrics to design and deliver the survey. The sur- vey was distributed to professionals who have worked as materials scientists, materials engineers, design engineers, or related fields, via the Autodesk Research Community. The survey remained ac- cessible for 20 days.\n3.2 Dataset Organization and Description. The survey re- sponses were extracted from Qualtrics and processed such that all identifiers of the participants and any information that can be linked to them are removed. The dataset is hosted in a Hugging- Face repository and consists of three files: AllResponses.csv, CleanResponses.csv, and KeyQuestions.csv.\nKeyQuestions.csv is a key explaining what each column header means in the other two files. For example, column header Q1_Steel corresponds to responses about steel to the question about designing a Kitchen Utensil Grip that should be Lightweight (i.e, Q1).\nThe survey was set up in such a way that if a respondent an- swered one or many question(s) but did not complete the survey, the response was still collected and the unanswered questions would remain blank in the final response even if the final survey was sub- mitted or not. AllResponses.csv contains the responses of all respondents, regardless of whether the responses are complete or not. The CleanResponses.csv file is processed and constructed such that only responses are present for which a respondent com- pleted all survey questions. Both files have the same number of columns but different number of responses because of the reason mentioned above. The number of responses (rows) in the file All- Responses.csv is 138 and the number of responses (rows) in the file CleanResponses.csv is 67.\nThe first column in both of these files is labeled mate- rial_familiarity, and records that individual's level of fa- miliarity with material selection. The next column is labeled yrs_experience which is that individual's number of years of experience in that domain. The following 144 columns record the response to every combination design case and criteria for each material choice - resulting from 4 design cases, 4 design criteria, and 9 material choices.\n3.3 FAIR Principles. This data adheres to the FAIR princi- ples of Findable, Accessible, Interoperable, and Reusable [52]:\n\u2022 Findable: The dataset is hosted in a HuggingFace repository and is assigned a globally unique and persistent Digital Object Identifier (DOI) [53]. The dataset card contains the details about the dataset and other metadata and the dataset is indexed"}, {"title": "4 Potential Applications", "content": "This dataset can be useful for engineering design research in several ways:\n(1) Evaluating material selection algorithms and decision- making models: Researchers can use this dataset to develop and test various material selection algorithms, decision- making models, and knowledge-based systems to assist de- signers in the material selection process.\n(2) Investigating the relationship between material properties and design requirements: The dataset provides information on a wide range of material properties, which can be used to study the correlations between material properties and spe- cific design requirements, such as strength, stiffness, weight, or cost.\n(3) Exploring the influence of material familiarity and experi- ence on design decisions: The dataset includes information on the user's years of experience and material familiarity, which can be used to investigate how these factors impact material selection decisions and the overall design process.\nIn general, this dataset can be a valuable resource for engineering design research, as it provides a comprehensive set of material- related information that can be used to advance the understanding and practice of material selection in conceptual mechanical design."}, {"title": "5 Conclusion", "content": "In this paper, we introduce MSEval, a novel benchmark specif- ically designed to assess the capabilities of machine learning and algorithmic models with respect to human-level cognition in the domain of material selection in conceptual design. This bench- mark consists of survey responses from experienced professionals in the fields of material science and engineering, material selection, and design engineering. By focusing on scenario-based domain- specific tasks, MSEval enables a more meaningful evaluation of algorithmic model performance, bridging the gap between human cognition and machine capabilities in material selection.\nThis dataset is subject to several limitations. For instance, the data set covers only four design cases and four design criteria, which is not fully representative of the wide range of design sce- narios encountered in real-world applications. As such, the dataset does not capture contextual factors that influence material selec- tion, such as specific project constraints, economic conditions, regulatory requirements, or market trends. In addition, the nine material families used in this work are high-level categories, and nuanced differences between sub-categories, such as different alu- minum alloyss, are not addressed. The dataset also represents a static snapshot of opinions and practices at a particular point in time. Material science and engineering practices evolve, and the dataset may become outdated as new materials and technolo- gies emerge. To address these limitations, future iterations of the dataset should expand the range of design scenarios and criteria, increase the sample size and ensure a more diverse respondent pool, include additional contextual information about respondents and their decision-making processes, and use more sophisticated methods to capture material selection trade-offs.\nWe hope that our research drives innovation in developing more reliable AI assistants that can advance toward this niche domain. By identifying areas for improvement and better understanding the behavior of humans, our benchmark provides a foundation for cre- ating AI systems that can handle complex human-centric tasks effectively."}]}