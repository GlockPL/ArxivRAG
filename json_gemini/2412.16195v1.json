{"title": "Machine Learning-Based Automated Assessment of Intracorporeal Suturing in Laparoscopic Fundoplication", "authors": ["Shekhar Madhav Khairnar MS", "Huu Phong Nguyen PhD", "Alexis Desir MD", "Carla Holcomb MD", "Daniel J. Scott MD", "Ganesh Sankaranarayanan PhD"], "abstract": "Automated assessment of surgical skills using artificial intelligence (AI) is valuable for trainees to obtain instantaneous feedback. After bimanual tool motions are captured, the derived kinematic metrics have shown to be reliable predictors of performance in laparoscopic tasks. Implementing automated tool tracking assessment requires time-intensive human annotation. We have developed AI-based tool tracking using the Segment Anything Model (SAM) to eliminate the need for human annotators. Here we describe a study to evaluate the usefulness of our tool tracking model in automated assessment of performance in a laparoscopic suturing task of the fundoplication procedure.\nAn automated tool tracking model was applied to recorded videos of Nissen fundoplication on porcine bowel. The participating surgeons were grouped into novices (PGY1-2) and experts (PGY3-5, attendings). The beginning and the ending of each of the suturing steps were segmented and the motions of the left and right tools were extracted. A low-pass filter with a cut-off frequency of 24 Hz was then applied to remove any noise. Automated assessment of performance was implemented using both supervised and unsupervised models, and an ablation study was performed to compare the performance. For the supervised learning model, kinematic features included root mean square (RMS) velocity, RMS acceleration, RMS jerk, total path length, and Bimanual Dexterity in pixel coordinates (x, y) that were extracted and analyzed using Logistic Regression, Random Forest Classifier, Support Vector Classifier, and XGBoost. We further analyzed the performance with a reduced set of features selected using a Principal Component Analysis (PCA). For unsupervised learning, a Denoising Autoencoder (DAE) model with classifiers like a 1-D (Convolutional Neural Network) CNN and traditional Machine Learning models were trained for classification.\nData was extracted for 28 participants out of an initial cohort of 38, categorized into 9 novices and 19 experts. The approach for supervised learning utilized kinematic features with PCA using a Random Forest model (the best model among other Machine Learning models) and obtained an accuracy of 0.795 \u00b1 0.065 and an F1 score of 0.778 \u00b1 0.071. The approach for unsupervised learning employed a 1-D CNN achieved the best results with an accuracy of 0.817 \u00b1 0.108 and an F1 score of 0.806 \u00b1 0.110. This second approach is superior as it eliminates the need to compute kinematic features.\nWe successfully demonstrated an AI model for automated classification of performance independent of human annotation of surgical videos.", "sections": [{"title": "INTRODUCTION", "content": "Surgeon technical skills have shown to directly affect clinical outcomes [1, 2] and is essential for the outcomes' success. Effective surgical skill assessment is crucial to ensure patients' safety and reduce clinical errors [3]. Traditional methods of skill assessment typically rely on direct observation by expert surgeons. Though based on longstanding educational frameworks, this approach is prone to variation in judgment, which can affect the reliability of the assessment [4]. Furthermore, direct observation is time-consuming and expensive, requiring numerous human hours for video processing alone [5].\nAdditionally, video review and grading require the involvement of expert surgeons, which delays the process. In response to this challenge, Video-Based Assessment (VBA) emerged as a valuable tool for workflow analysis, performance benchmarking, feedback, and coaching in surgical practice [6]. A vision-based method for the automatic detection of the presence, location, or movement of surgical tools is crucial for developing a fast and objective surgical evaluation system [7]. Therefore, techniques for video-based assessment of surgical skill can offer surgeons objective and effective tools [8].\nHowever, VBA is still manually intensive and sensitive to subjective interpretation of surgical videos, resulting in variations in skill assessment. Recent advances in surgical training have encouraged the use of deep learning to objectively analyze and score surgical skills via video-based assessments [9-12]. For instance, a CNN was used to classify surgical skills by identifying latent patterns from trainee motions performed during robotic surgery [13]. Other approaches proposed a 3-D CNN for learning spatiotemporal features from consecutive video frames [14] for autonomous surgical skill assessment and to evaluate the model's performance in classification tasks in laparoscopic colorectal surgical videos [9]. Furthermore, an inflated 3-D CNN was built on recent advances in deep learning-based video classification to classify snippets, which are stacks of a few consecutive frames extracted from surgical videos. The network was extended into a temporal segment network during training [15]. In addition, a Machine Learning system was proposed that uses a vision transformer and supervised contrastive learning to decode features of intraoperative surgical activity from videos typically captured during robotic operations [16]. These advances have significantly reduced the subjectivity inherent in traditional direct observation approaches.\nInterestingly, while certain studies [11, 17] have effectively integrated deep learning models to classify surgical proficiency using pre-processed and manually annotated video datasets, such as the Johns Hopkins University-Intuitive Surgical Incorporated gesture and skill assessment working set (JIGSAWS), these models frequently require significant efforts to prepare the data and may not fully capture the dynamic nature of surgical skill progression. Thus, existing approaches, although automated to an extent, still rely heavily on segmented video snippets or kinematic data from sensors, which can be intrusive and disrupt the natural flow of surgical procedures.\nContributions: First, our study introduces a novel approach to surgical skill assessment by automating the creation and labeling of training datasets using the Segment Anything Model (SAM) combined with a \u201cYou only look once,\u201d version 8 (YOLOv8) model pre-trained on the Cholec80 dataset. This integration enhances the precision of tool tracking in surgical videos and significantly reduces the labor-intensive process of manual data annotation, addressing a major bottleneck in surgical skill analysis. By automating this process, we provide a scalable, efficient, and less subjective method for assessing surgical skills. Second, we compared methods for assessing laparoscopic suturing skills in supervised and unsupervised learning using the data collected during laparoscopic fundoplication. Through an ablation study, we aimed to provide recommendations on the best approach for evaluating suturing performance. The first approach (used for supervised learning) involved directly analyzing kinematic features including root mean square (RMS) Velocity, RMS Acceleration, RMS Jerk, Path Length, and Bimanual Dexterity. The second approach (used for unsupervised learning) involved converting the original 1-D data (x-axis and y-axis) into 2-D images and employed a DAE to indirectly learn features. Third, for the purposes of this research, we compiled a new and comprehensive fundoplication dataset."}, {"title": "MATERIALS AND METHODS", "content": "In this section, we begin by discussing the collection of our fundoplication dataset. We then delve into the preprocessing stage, where we have uniquely automated the annotation process by leveraging a combination of YOLOv8 model and SAM. Annotation is essential for training AI models, yet manual annotation is time-consuming. Initially, we considered using either YOLOv8 or SAM for label generation, but each presented a challenge. The YOLOv8 model did not produce bounding boxes with the accuracy needed for subsequent analysis. The SAM required manual prompts, which contradicted our goal of full automation. Therefore, we combined the two approaches: YOLOv8 provides approximate bounding boxes, which SAM then uses for generating precise segmentations. This hybrid method eliminates the need for manual annotation while achieving high accuracy. Next, we explain our methodology for uncovering patterns in the dataset through two distinct approaches. The first approach involves using Machine Learning models to directly analyze the kinematic features data. The second approach converts the original 1-D data into 2-D images and uses a deep learning DAE model to indirectly learn features. An overview of the procedures can be seen in Figure 1."}, {"title": "Dataset", "content": "The study involved high-definition video recordings of 38 subjects who performed laparoscopic fundoplication up to two times on a porcine model. The recordings captured the bimanual manipulations of participants categorized as novices (PGY1-2) and experts (PGY3-5, attendings).\nInitially, the dataset consisted of 60 videos from 38 subjects. We used a total of 49 videos, chosen for their camera stability and clear view of the surgical field, and discarded 11 videos due to incomplete tasks or excessive camera movements."}, {"title": "Data pre-processing", "content": ""}, {"title": "Tool segmentation using YOLOv8", "content": "We utilized the YOLOv8 model to generate bounding boxes for surgical tools from the original images. YOLOs were adopted mainly due to their faster inferences than two-stage detectors [18].\nThis model, initially pre-trained on the common objects in context (COCO) dataset, was fine-tuned for surgical instrument detection using the Cholec80 dataset [19], annotated with tools like graspers, hooks, and scissors. For our specific application focusing on graspers and needle drivers, the model was adjusted to detect these tools with high precision, using annotations converted into COCO format using LabelMe software [20]."}, {"title": "Automatic annotation via SAM", "content": "After the first YOLOv8 phase, SAM [21] was utilized to improve the tool segmentation further. The extracted frames (input images) along with bounding boxes were then passed to SAM to generate masks. In detail, an image is encoded to capture essential features, and bounding box coordinates (input prompts) are encoded to highlight areas of interest. These encodings are processed by a mask decoder, which also receives a preliminary mask from a convolutional layer for refinement. The resulting refined masks are overlaid on the original image (for more information, see [21])."}, {"title": "Train the YOLOv8 model", "content": "Later, we retrained the YOLOv8 model to generate bounding boxes with better precision. Out of 49 videos, we selected 46 videos for 28 subjects selected randomly (providing a sufficient dataset to train the segmentation model) and 200 frames were extracted from each video containing an equal number of frames sampled from the start, middle, and end segments for training the YOLOv8 segmentation model. Of these, 33 videos were used for training, with 965 images annotated to highlight surgical tools and maneuvers, and an additional 31 background images for model training. For validation, 130 images were extracted from 5 videos, and for testing, 306 images were included from 8 videos. In total, only 1401 frames that displayed high-quality segmentation with accurately generated masks were selected."}, {"title": "Data filtering", "content": "After training the segmentation model, we extracted the tool motion data in 2D (x-axis and y-axis) and calculated the position of the tooltip. These data were then divided into left and right tools and then further divided into three parts, namely suturing 1 (S1), S2, S3 and S4 for each tool. Thus, each video had at least 1 suture and some participants did 2 sutures while some did 3 or 4. Although S4 was recorded in a few cases, it was excluded from further analysis because it was performed by only 2 or 3 subjects, whereas S1, S2, and S3 were common across all subjects. Finally, we used the tool motion data for further analysis. To remove high frequency noise from the data, we passed the tool motion data through a low-pass Butterworth filter."}, {"title": "Data Analysis", "content": ""}, {"title": "Machine Learning Model", "content": "For supervised learning, after filtering, we computed several kinematic features that included RMS Velocity, RMS Acceleration, RMS Jerk, Path Length, and Bimanual Dexterity (bimanual dexterity as a measure of coordination between the left and right tools) for each video. Thus, we had 141 samples (from 9 novice and 19 expert subjects) for a Machine Learning model training for classification of expert and novice subjects. There were 8 features (6 RMS values, plus Bimanual Dexterity and Total Path Length that were combined for both tools). We dropped the acceleration feature since it was highly correlated with jerk.\nFor training Machine Learning models, we did a 10-fold cross-validation stratified split. In our study, we implemented various data transformation techniques, including Power Transformer, Robust Scaler, Minmax Scaler, and Standard Scaler. Subsequently, we conducted two distinct analyses utilizing Machine Learning models. In the first analysis, we applied PCA to select the most important features with n = 3 components, which accounted for 84% of the variance in the original dataset. The second analysis was performed without PCA. For both analyses, we employed several classifiers: Random Forest [22], XGBoost [23], Support Vector Classifier [24], and Logistic Regression."}, {"title": "DAE and Classifier Model", "content": "In addition to the conventional Machine Learning methods, we explored the use of autoencoder [25] models to capture more important features from the tool trajectory data (x-axis and y-axis).\nAutoencoders, particularly effective in unsupervised learning scenarios, are neural networks trained for compressing input data into a condensed format and then reconstructing it back to its original form. This capability is essential for isolating crucial features from raw data without explicit labels. In our research, we employed a DAE (Figure 2), a type of autoencoder designed to enhance feature extraction by first adding noise to the input and then learning to recover the original, uncorrupted data [25,26]. This prevents the autoencoder from just duplicating the input and encourages it to learn more relevant features.\nAutoencoder training\nOur training involved an autoencoder that processed tooltip x-y coordinate data, representing the extreme points of the left and right surgical tools. The encoder received input images of size 224 \u00d7 224 \u00d7 3 and reduced this data into a lower-dimensional space measuring 128 \u00d7 1. We prepared the data by concatenating the suturing segments S1, S2, and S3 for each subject, with the observation that S1 often took longer than the other segments. Consequently, our dataset comprised separate files for the x and y axes, which were further split into left and right, resulting in a total of 196 samples. We processed the tooltip data for the x and y coordinates separately, applying zero padding and creating line plots for these coordinates against the number of frames. The x-axis and y-axis data were then converted into 2-D images.\nThese 2-D images were used to train DAE model. The DAE model was trained with additional Gaussian noise, with a noise factor of 0.5. This means that random noise from a normal distribution was added to the data, and the resulting values were clipped to the range [0, 1]. After training, we extracted low-dimensional features from the encoder, represented as 128-dimensional arrays. These features were utilized for classification by feeding them directly into Machine Learning models and a 1-D CNN model."}, {"title": "Ethical Approval", "content": "An institutional review board, the ethics committee of the XXX Medical Center\u2014approved the study design and the use of fundoplication videos (IRB # STU-2021-0151)."}, {"title": "RESULTS", "content": ""}, {"title": "Automated tool tracking", "content": ""}, {"title": "Performance of YOLOv8 Cholec80", "content": "The YOLOv8 model was pretrained on the Cholec80 dataset, which contains distinct surgical tools/classes (Figure 3). Besides producing bounding boxes, the model also generated masks with the following performance.\nThe segmentation mask mAP50-95 scores showed varying levels of performance for intersection over union (IoU) thresholds ranging from 50% to 95%. Overall, the model achieved a segmentation mask mAP50-95 score of 0.827 across all classes. For the bipolar surgical tool, the score was 0.828, while the hook tool achieved a higher score of 0.903. The grasper tool had a score of 0.767, and the irrigator tool showed a score of 0.90. The clipper tool had a score of 0.726, and the scissors tool achieved a score of 0.838. Since the detection of graspers is critical, we opted to improve this performance, as described in the next section."}, {"title": "Performance of YOLOv8 on Fundoplication data", "content": "From the dataset of 1401 images, the combination of the YOLOv8 and SAM increased the mAP from 0.76 to 0.95 for the mask of the grasper class (Figure 4). This high mAP score for segmentation mask signifies that the model accurately detected and segmented the grasper tool in surgical videos. By leveraging this approach, we eliminated the need for manual annotation, saving both human effort and time."}, {"title": "Filtered output", "content": "In our approach, we improved the preprocessing of kinematic data by adding an extra layer of processing. Specifically, we applied a filter to smooth the data. As seen in Figure 5, this method effectively reduces noisy data, thereby improving the overall quality and reliability of the data for subsequent analysis."}, {"title": "Machine Learning with Kinematic Features", "content": "The use of kinematic features is illustrated in the radar plot (Figure 6). Novices took more time than experts to execute movements, had higher jerks, and covered a longer total path length, indicating more erratic and less controlled movements. High bimanual dexterity in experts indicates better coordination between the left and right tools compared to novices.\nSince the dataset is unbalanced, i.e., there were 19 novices but only 9 experts, we utilized the weighted loss technique to address the issue. The results show an accuracy and F1 score of 0.774 and 0.775 without using PCA, and 0.795 and 0.778 using PCA (Table 1). The use of PCA slightly improved performance on most measurement metrics, except for positive predictive value. Please note that only the best model's performance is presented. For a complete set of results for all models, see Table S1 in the Supplementary section."}, {"title": "DAE with Classifiers", "content": "Table 2 shows the results from DAE with various classifiers. Though 1-D CNN (0.806) outperformed Random Forest (0.797) on F1 score, Random Forest outweighed its counterpart on accuracy, positive predictive value, and negative predictive value metrics. For a complete set of results for all models, see Table S2 in the Supplementary materials."}, {"title": "DISCUSSIONS", "content": "When comparing Machine Learning models to DAE for skill assessment, the approach to feature extraction and pattern recognition differs significantly. In traditional Machine Learning models, we explicitly use a variety of features such as distance displacement, velocity, acceleration, jerk, and bimanual dexterity to identify patterns that reflect skill levels. These features are manually selected and engineered to capture relevant aspects of the task being analyzed.\nOn the other hand, a DAE operates in an unsupervised manner. It automatically learns a compact, efficient representation of the data without the need for manual feature selection. The autoencoder model discovers and forms these representations based on the inherent structure of the input data. In our findings, the improvement of performance, e.g., F1 score increasing from 0.778 to 0.806, is also remarkable."}, {"title": "CONCLUSIONS", "content": "Our study advances surgical skill assessment by integrating the SAM with a YOLOv8 model trained on the Cholec80 dataset. This integration automates tool tracking and classification, enhancing the precision of surgical training evaluations. Our main contributions are the development of this novel automated system and creating a comprehensive dataset, which improves the accuracy and efficiency of evaluating surgical skills. We compared the traditional approach of using Machine Learning (for supervised learning) with DAE (unsupervised learning) and showed a preference for DAE. This work paves the way for future innovations in surgical education and provides a scalable, efficient assessment tool that contributes significantly to the field, promising to improve both surgical training and patient outcomes."}, {"title": "DATA AVAILABLITY", "content": "The data that support the findings of this study may be provided upon reasonable request."}]}