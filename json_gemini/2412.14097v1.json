{"title": "Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts", "authors": ["Jihye Choi", "Jayaram Raghuram", "Yixuan Li", "Somesh Jha"], "abstract": "Advancements in foundation models (FMs) have led to a paradigm shift in machine learning. The rich, expressive feature representations from these pre-trained, large-scale FMs are leveraged for multiple downstream tasks, usually via lightweight fine-tuning of a shallow fully-connected network following the representation. However, the non-interpretable, black-box nature of this prediction pipeline can be a challenge, especially in critical domains such as healthcare, finance, and security. In this paper, we explore the potential of Concept Bottleneck Models (CBMs) for transforming complex, non-interpretable foundation models into interpretable decision-making pipelines using high-level concept vectors. Specifically, we focus on the test-time deployment of such an interpretable CBM pipeline \u201cin the wild\u201d, where the input distribution often shifts from the original training distribution. We first identify the potential failure modes of such a pipeline under different types of distribution shifts. Then we propose an adaptive concept bottleneck framework to address these failure modes, that dynamically adapts the concept-vector bank and the prediction layer based solely on unlabeled data from the target domain, without access to the source (training) dataset. Empirical evaluations with various real-world distribution shifts show that our adaptation method produces concept-based interpretations better aligned with the test data and boosts post-deployment accuracy by up to 28%, aligning the CBM performance with that of non-interpretable classification.", "sections": [{"title": "1 Introduction", "content": "Foundation Models (FMs), trained on vast data, are powerful feature extractors applicable across diverse distributions and downstream tasks (Bommasani et al., 2021; Rombach et al., 2022). They can be applied to classification tasks off-the-shelf via zero-shot prediction, or via linear probing using task-specific fine-tuning data (Kumar et al., 2022; Radford et al., 2021). Despite these strong advantages, foundation model-based systems often operate as inscrutable black-boxes, presenting a barrier to user trust and wider deployment in safety-critical settings. Another challenge faced in the standard deployment of FM-based deep classifiers is their vulnerability to distribution shifts at test time caused e.g., due to environmental changes, which can cause a drop in performance (Bommasani et al., 2021). This is particularly challenging in high-stakes domains such as healthcare (AlBadawy et al., 2018; Eslami et al., 2023), autonomous driving (Yu et al., 2020), and finance (Wu et al., 2023a).\nIn this work, we address these challenges by developing an interpretable classification framework that enjoys the rich, expressive feature representations of FMs, while also having enhanced robustness towards distribution shifts at test time. To tackle interpretability, we utilize Concept Bottleneck Models (CBMs) (Koh"}, {"title": "2 Concept Bottleneck Model under Distribution Shifts", "content": "Notations. Consider a classification problem with inputs x \u2208 X and class labels y \u2208 Y := {1,\u00b7\u00b7\u00b7,L}.\nWe assume that the labeled training data from a source domain are sampled from an unknown probability distribution ps(x, y), and unlabeled test data from a target domain are sampled from an unknown probability distribution pt(x) (the training dataset is not accessible in our problem setting). The subscripts 's' and 't' refer to the source and target domain respectively. Boldface symbols are used to denote vectors and tensors. The standard inner-product between a pair of vectors is denoted by $\\langle \\mathbf{x}, \\mathbf{x}' \\rangle = \\mathbf{x}^T\\mathbf{x}'$, and their cosine similarity is defined as $\\text{cos}(\\mathbf{x}, \\mathbf{x}') = \\langle \\mathbf{x}, \\mathbf{x}' \\rangle / ||\\mathbf{x}||_2||\\mathbf{x}'||_2$. The set {1,\u2026\u2026, n} is denoted concisely as [n] for n \u2208 Z+. Please see Table 2 in the Appendix for a quick reference on the notations."}, {"title": "2.1 Background: Foundation Models with a Concept Bottleneck", "content": "Consider a foundation model \u03c6 : X \u2192 Rd, which is any pre-trained backbone model or feature extrac-tor (Eslami et al., 2023; Jia et al., 2021; Girdhar et al., 2023) that maps the input x to an intermediate feature embedding \u03c6(x) \u2208 Rd. \u03c6(x) is pre-trained on a large-scale, broad mixture of data for general purposes,"}, {"title": "2.2 Distribution Shifts in the Wild", "content": "Let T = {to, t1, ..., tk} be a finite set of measurable input transformations, where each ti : X \u2192 X is a measurable function. We also define a transformed input space encompassing all possible transformed inputs: XT = \u222ak i=0{ti(x) | x \u2208 X}. Without loss of generality, we set to to be the identity function to(x) = x, \u2200x \u2208 X. Let \u03bcs and \u03bct be probability measures on T representing the distributions over input transformations in the source and target domains, respectively. We define the source domain Ds, equipped with \u03bcs such that \u03bcs({to}) = 1, \u03bcs({ti}) = 0 \u2200i \u2260 0. Its joint distribution is denoted by Ps over XT \u00d7 Y such that Ps(x, y) = P(x, y) \u2200x, y, where P is the underlying distribution over inputs and labels. Similarly, we define the target domain Dt with a probability measure \u03bct such that \u03bct({ti}) > 0 for some i \u2208 [k].\nIts joint distribution is denoted by Pt over XT \u00d7 Y such that $P_t(x,y) = \\sum_{i=0}^{k}\\mu_t({t_i}) P(t_i(x), y)$, assuming that the ti are invertible or appropriately measurable for their pre-images.\nLet H be a concept hypothesis class, defined as the space of measurable concept mappings h : Rd \u2192 Rm from the feature representation \u03c6(x) to concept scores. We also define the concept set C := {c1, c2,\u2026\u2026,cm}, where each ci : Rd \u2192 R represents a high-level concept mapping (e.g., stripe pattern, grass, beach, etc.). For a domain Dj, j \u2208 {s, t}, we define the concept score distribution as Pcon(Dj, \u03c6, h) = (h \u25e6 \u03c6)*Pj, where (h \u25e6 \u03c6)*Pj is the push-forward measure (Le Gall, 2022) of Pj under h \u25e6 \u03a6. Note that h is determined by C such that h(\u03c6(x)) = [c1(\u03c6(x)),\u2026\u2026\u2026, cm(\u03c6(x))]T.\nLet G be a classification hypothesis class, defined as a set of measurable classifiers g : Rm \u2192 RL mapping the concept scores to prediction logits. Finally, we define the distribution of predictions as the push-forward measure of Pcon(Dj, \u03c6, h) under g: Ppred(Dj, \u03c6, h, g) = g*Pcon(Dj, \u03c6, h).\nGiven h \u2208 H and g \u2208 G, we categorize the distribution shifts in the target domain, {pt(ti) > 0 | ti \u2208 T}, into one of the following broad categories:"}, {"title": "2.3 Failure Modes of Concept Bottleneck for Foundation Models", "content": "Based on the definitions above, we categorize the possible failure modes of the decision-making pipeline of a foundation model equipped with a CBM, defined by a given Ds, Dt, \u03c6, h \u25e6 \u03c6 = [c1 \u25e6 \u03c6,\u2026\u2026\u2026, cm \u25e6 \u03c6], and g as follows."}, {"title": "3 CONDA: Concept-based Dynamic Adaptation", "content": "To address the failure modes of a CBM identified in the previous section, here we propose a dynamic approach for adaptation of a CBM based only on unlabeled test data. We follow the setting of test-time"}, {"title": "3.1 Concept Score Alignment", "content": "From Figure 2 (top half) and Eqn. 1, the concept scores vc(x) \u2208 Rm are input to the linear label predictor Wv + bs. Let {P(vc(xs)|ys = y), y \u2208 Y} be the class-conditional distributions of these concept scores on the source domain. At test time, if the distribution of the input changes such that xt ~ pt(x), then there is a corresponding change in the class-conditional distributions of concept scores {P(VC(xt) | yt = y) = P(Cs\u03c6(xt)|yt = y), y \u2208 Y}. The goal of concept-score alignment (CSA) is to adapt the source domain concept bank Cs to a target domain-specific one Ct such that the class-conditional distributions after adaptation are close to that of the source domain under some distributional distance (e.g., Kullback-Leibler or Total-variation). Informally, we wish to find an adapted concept bank Ct, starting"}, {"title": "3.2 Linear Probing Adaptation", "content": "In this step, we focus on improving the test accuracy of the label predictor of the main CBM branch (W, b), with the concept vectors C fixed at their updated values from the CSA step (the residual CBM parameters are also frozen). For this, we use the cross-entropy loss between the predictions of the target domain CBM (Eqn. 5) and the pseudo-labels of a test batch Dt. In order to enhance the interpretability of the label predictor, we impose sparsity and grouping effect in its weights via an Elastic-net penalty term (Zou & Hastie, 2005; Yuksekgonul et al., 2023) given by"}, {"title": "3.3 Residual Concept Bottleneck", "content": "We next discuss adaptation of the residual branch of the CBM whose parameters are {$\\tilde{\\mathbf{C}}$, $\\tilde{\\mathbf{W}}$, $\\tilde{\\mathbf{b}}$}. The r additional concept vectors in $\\tilde{\\mathbf{C}}$ are expected to capture new concepts in the target data and compensate for the potentially incomplete coverage of the main CBM (see Section 2.3). By increasing the expressiveness of the concept subspace, we expect to improve the accuracy on the target dataset beyond the CSA and LPA steps. Therefore, we first have a cross-entropy loss term in this adaptation objective (as in Eqn. 11). We also introduce a cosine similarity based regularization in the objective to encourage the new concept vectors in $\\tilde{\\mathbf{C}}$ to be less redundant with each other and to have less overlap with the existing concept vectors $\\mathbf{C}$ (obtained from the CSA step)."}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to answer the following three research questions:\nRQ1: How effective is CONDA in improving the test-time performance of deployed classification pipelines that use a foundation models with a concept bottleneck predictor?\nRQ2: How does each component of CONDA specifically address and remedy the failures caused by different types of distribution shifts?\nRQ3: How do the concept-based explanations change before and after test-time adaptation?"}, {"title": "4.1 Setup", "content": "A detailed description of the experimental setup is available in Appendix C.\nDatasets. We evaluate the performance of concept bottlenecks for FMs and the proposed adaptation on five real-world datasets with distribution shifts, following the setup in Lee et al. (2023): (1) CIFAR10 to CIFAR10-C and CIFAR100 to CIFAR100-C for low-level shift, (2) Waterbirds and Metashift for concept-level shift, and (3) Camelyon17 for natural shift.\nBackbone Foundation Models. For the CIFAR datasets, we use CLIP:ViT-L/14 (FARE2) (Schlarmann et al., 2024), which is adversarially fine-tuned to be more robust to (adversarial) low-level perturbations than standard CLIP variants. We employ CLIP:ViT-L/14 (Radford et al., 2021) for Waterbirds and Metashift. For Camelyon17, we utilize BioMedCLIP (Zhang et al., 2023), which is pre-trained on diverse medical domains to understand medical images and text jointly, making it suitable for zero-shot tasks in the medical domain.\nPreparing the Concept Bottleneck. We evaluate CONDA using three popular approaches for constructing the concept bottleneck: (1) using a general-purpose concept bank where natural language concept descrip-tions and modern vision-language models (e.g., Stable Diffusion (Rombach et al., 2022)) are leveraged to automatically generate concept examples for finding concept vectors (Yuksekgonul et al., 2023; Wu et al., 2023b); (2) unsupervised learned concepts where concept vectors are learned via optimization to maximize the concept-based prediction accuracy (Yeh et al., 2020); and (3) employing GPT-3 with appropriate filtering to discover a tailored set of concepts for the bottleneck (Oikarinen et al., 2023). More details can be found in Appendix A and Appendix C.2.\nMetrics. We report the performance in terms of two metrics: averaged group accuracy (AVG) and worst-group accuracy (WG). AVG is the average (per-class) accuracy across the classes, and WG is the minimum (per-class) accuracy across the classes."}, {"title": "4.2 RQ1: Effectiveness of CONDA under real-world distribution shifts", "content": "Table 1 presents our main results evaluating the effectiveness of CONDA on different real-world distribution shifts, when combined with different CBM baselines. First of all, we observe that leveraging the expressive power of the FM feature representations can enhance the performance of CBMs. For example, using the method from Oikarinen et al. (2023), their reported accuracies on CIFAR10 and CIFAR100 are 86.40% and 65.13% respectively when using the CLIP-RN50 backbone. In our experiments, by employing the adversarially fine-tuned CLIP-ViT-L/14, we achieve higher accuracies of 95.24% and 68.36% respectively (source domain). This demonstrates the potential for improved utility in concept-based interpretable pipelines as foundation models continue to improve."}, {"title": "4.3 RQ2: Effectiveness of Individual Components of CONDA", "content": "We next analyze the individual contributions of the components in CONDA, viz. CSA, LPA, and RCB. Figure 3 illustrates the relative AVG and WG (%) when adapting the CBM of Yeh et al. (2020). Under low-level shifts, CSA plays a crucial role in performance improvement by encouraging the high-level concept scores to remain similar. Interestingly, using CSA alone even surpasses the performance achieved when all components are combined. This trend is also observed with the Camelyon17 dataset, which resembles a low-level shift due to lighting differences across hospitals. On the other hand, under concept-level shifts, LPA and RCB become the key components of adaptation. These components allow the model to adjust concept reliance to the target domain and address the incompleteness of the deployed concept set, tailoring it to the target data. In this context, CSA has minimal impact, while using only LPA leads to performance gains comparable to, or even exceeding that achieved when all components are included.\nInterestingly, this phenomenon aligns with the findings of Lee et al. (2023) that fine-tuning only a subset of layers can be more effective than fine-tuning all layers, depending on the type of distribution shift. In our case, the concept-based prediction pipeline can be considered a special instance of their framework with a two-layer"}, {"title": "4.4 RQ3: Interpretability of CONDA", "content": "We investigate how the concept-based explanations change through adaptation by CONDA on the Waterbirds dataset. In Figure 4a, we present the top five most prominent concepts contributing to the predictions for each class. As expected, in the source domain, land-related concepts are most important for predicting \"landbird\", and do not positively contribute to \u201cwaterbird\u201d; and vice versa for water-related concepts. After adapting to the target domain (test dataset), we observe adjustments in the concept-to-class mappings. Notably, land-related concepts begin to positively contribute to the prediction of \u201cwaterbird\u201d. This shift indicates that CONDA successfully adapts the concept-based explanations to reflect the new correlations observed in the target domain. Moreover, in the original concept bottleneck constructed following Wu et al. (2023b), there were no bird-related concepts that could help make robust predictions independent of spurious background correlations. By employing RCB with five residual concepts, we identified that three of them correspond to bird-related concepts: feathers, wings, and beak. This demonstrates that CONDA adapts in a manner aligned with human intuition, just like a human intervening in a CBM to correct its predictions would. More importantly, RCB captures concepts that may have been missed during the initial construction of the concept bottleneck, enhancing both the interpretability and robustness. Additional results and analysis of the interpretability of CONDA can be found in Appendix E."}, {"title": "5 Conclusions and Future Work", "content": "This work was motivated by our observation that recent CBM variants atop a backbone foundation model may close the performance gap with feature-based predictions in the source domain, but they are often unable to do so under distribution shifts at test time (after deployment). Hence, for an interpretable and robust decision-making pipeline under distribution shifts, while fully leveraging the representative power of foundation models, an adaptive test-time approach is required. To the best of our knowledge, we have proposed the first effort to tackle this problem setting for CBMs. We formalized potential failure modes under low-level and concept-level distribution shifts and proposed a novel test-time adaptation framework, named CONDA. Each component of CONDA is designed to address specific failure modes, effectively improving the test-time performance of a deployed CBM using only unlabeled test data.\nOur framework can continue to benefit from ongoing improvements in the robustness of foundation models and the development of more advanced pseudo-labeling techniques, both of which represent promising avenues for future work. Another promising direction for future research is to develop a deeper theoretical understanding of concept bottlenecks under distribution shifts. For instance, it would be valuable to i) characterize the sufficiency of a given concept set from training (source domain) for robust test-time accuracy under different distribution shifts; and ii) to quantify or bound the extent to which test-time adaptation can bridge the accuracy gap between the source and target distributions. Such theoretical insights would complement the algorithmic and empirical advancements, guiding both the design of more effective residual concept bottleneck and the development of improved adaptation strategies. A discussion of the limitations of this work is given in Appendix F."}, {"title": "A Expanded Related Work", "content": "Concept Bottleneck Models (CBMs), introduced by Koh et al. (2020), are interpretable neural networks that map input data to a set of human-understandable concepts (the \u201cbottleneck\u201d) before making predictions. This architecture enhances the interpretability by revealing which concepts influence the predictions and allows users to intervene by adjusting mis-predicted concepts.\nDefinition of Concept Bottleneck. Despite the above benefits, early variants (e.g., (Havasi et al., 2022)) required extensive concept annotations during training, which can be costly and impractical. This reliance on predefined, annotated concepts limits their scalability and applicability to diverse domains and tasks. To address this, recent methods aim to construct CBMs without requiring explicit concept labels, and they can be placed into three main categories: (a) unsupervised learning-based concept discovery, (b) general-purpose concept bank agnostic to tasks, and (c) leveraging multi-modal foundation models. They are further discussed below.\n(a) Unsupervised learning-based concept discovery: Yeh et al. (2020) formulates the concept discovery as an optimization process with the objective of concept completeness, ensuring that the extracted concepts comprehensively represent the data while maintaining interpretability. This approach is further advanced in Wang et al. (2023), where they optimize task-specific concepts via self-supervision techniques such as contrastive loss to improve the quality of the learned concepts.\n(b) General-purpose concept bank agnostic to tasks: Yuksekgonul et al. (2023) and Wu et al. (2023b) utilize a predefined concept bank where each concept vector is derived from the parameters of a Support Vector Machine (SVM) trained to distinguish between positive and negative instances in image embeddings obtained from a backbone model. Here the dataset used to learn the SVMs does not have to be the same as the data for the given task.\n(c) Leveraging multi-modal foundation models: Another approach leverages the rapid advancements in multi-modal foundation models like CLIP to align visual and textual representations, enabling the mapping of each concept to a human-readable description (Moayeri et al., 2023). Yuksekgonul et al. (2023) also suggests defining each concept vector with the text embeddings from the backbone, where the text serves as human-understandable concept descriptions (refer to Figure 2 in Shang et al. (2024) for a descriptive illustration of the method). Oikarinen et al. (2023) relies on a pre-trained backbone like CLIP which maps images and textual descriptions into a shared embedding space. They define each concept vector as the mapping of an image embedding to its corresponding text embedding.\nIn our paper, we consider the most representative method from each category of concept bottleneck construc-tions: Yeh et al. (2020) for (a), Yuksekgonul et al. (2023) for (b), and Oikarinen et al. (2023) for (c) (refer to Appendix C.2 for further details on their implementations). By applying our adaptation framework to various definitions of a concept bottleneck, we demonstrate that it can effectively and flexibly enhance the post-deployment robustness of various CBM types under real-world distribution shift scenarios.\nConcept-based explanations and distribution shifts. There has been growing interest in the utility of concept-based explanations under distribution shifts. The initial work by (Kim et al., 2018) hinted at the potential of high-level concepts as diagnostic units against low-level perturbations, such as adversarial examples. Following this, Adebayo et al. (2020) suggested that concept-based explanations could be more robust tools for debugging and analyzing model behaviors under spurious correlations. More recently, Abid et al. (2022) and Wu et al. (2023b) have studied the utility of concept-based explanations in the context of data"}, {"title": "B Additional Method Details", "content": "We describe the comprehensive algorithm of CONDA in Algorithm 1."}, {"title": "B.1 Complexity Analysis of CONDA", "content": "Referring to Algorithm 1, we evaluate the computational complexity of the CSA, LPA, RCB, and pseudo-labeling steps for a single test batch $D_t^b$. We recall some of the terms used in our notation.\n*  m : number of concepts in the main CBM.\n* r: number of concepts in the residual CBM branch.\n* d : dimension of the feature representation \u03c6(x).\n* L: number of classes.\n* ngrad: number of gradient steps for each of the CSA, LPA, and RCB adaptations.\n* nbatch = |Db t |: batch size\nCSA step optimizes the concept matrix of the main CBM branch C, which has m d parameters. Below we breakdown the computations involved in the CSA adaptation objective and its gradient updates. We assume that the mean and inverse-covariance matrices of the class-conditional concept score distributions are pre-computed from the source dataset.\nConcept score projection for a single test sample: 2 m d.\nIntra-class and inter-class Mahalanobis distances for a single test sample: L (2m2 + 3m) + L = Lm (2m +\n3) + L.\nFrobenius norm regularization term: 3 m d.\nCost of stochastic gradient update step for the batch: 2 m d.\nThe cost of optimizing the CSA objective (Eqn. 9) for ngrad gradient update steps can be expressed as:"}, {"title": "B.2 Automatically Annotating Concepts", "content": "We adopt and modify CLIP-DISSECT (Oikarinen & Weng, 2023) for automatically annotating the concepts as follows.\nSuppose S is the set of possible concept annotations. We use ConceptNet (Speer et al., 2017) to obtain texts that are relevant to the classes. ConceptNet is an open knowledge graph, where we can find concepts that have particular relationships to a query text. For instance, for a class \u201ccat\u201d, one can find relations of the form \"A Cat has {whiskers, four legs, sharp claws, ...}\". Similarly, we can find \"parts\u201d of a given class (e.g., \"bumper\u201d, \u201croof\u201d for \u201ctruck\u201d class), or the superclass of a given class (e.g., \u201canimal\u201d, \u201ccanine\" for \"dog\"). Following the setup in Yuksekgonul et al. (2023), we restrict ourselves to five sets of relations for"}, {"title": "C Experimental Details", "content": "All the experiments are run on a server with thirty-two AMD EPYC 7313P 883 16-core processors, 528 GB of memory, and four 884 Nvidia A100 GPUs. Each GPU has 80 GB of 885 memory. For each setup, we repeated each experiment for 10 trials (using seed 40\u201349 for the random number generation) and report the mean and standard error."}, {"title": "C.1 Datasets", "content": "CIFAR10. It consists of 60k RGB images of size 32x32 (50k images for the train set, and 10k images for the test set), equally balanced over 10 different classes (e.g., airplane, car, dog, cat, etc.). We follow the given train/test split to report the performance in the source domain.\nCIFAR100. It is similar to CIFAR10, but in a larger-scale; there are 100 classes, and each class has 500 32x32 RGB training images and 100 test images, making the classification more challenging.\nCIFAR10-C and CIFAR100-C. To report the accuracies, we take the average over 15 different types of corruptions with the severity level of two (out of the scale from one to five); Gaussian Noise, Shot Noise, Impulse Noise, Defocus Blur, Frosted Glass Blur, Motion Blur, Zoom Blur, Snow, Frost, Fog, Brightness, Contrast, Elastic, Pixelate, JPEG Compression. Conventionally, studies in out-of-distribution generalization literature, severity level five is used, but we observe that it severely hurts the performance of the foundation model, making it impossible to be used as a decent oracle for the pseudo labeling. Hence, we chose the severity level two that still causes the performance drop due to the distribution shift, but against which, the backbone model still presents decent performance compared to the CBMs.\nWaterbirds. Waterbirds dataset is for a two-class classification task (\"landbird\" vs. \"waterbird\"). In the source domain, landbird (waterbird) images are always associated with the land (water) background, while in the target domain, the correlation with the background is flipped, i.e., landbird (waterbird) images are always on the water (land) background.\nMetashift. Metashift has two classes of \u201ccat\u201d and \u201cdog\u201d, and it simulates the disparate correlation to the backgrounds in a similar way. Source cat images are always correlated with a sofa or bed in the background, while dog images are always correlated with a bench or bike in the background. For evaluation, we randomly split 90:10 equally across the correlation types, i.e., 10% of dog images with sofa, 10% of dog images with bed, 10% of cat images with bench, and 10% of cat images with bike. In the target domain, both cat and dog images are always on the shelf background.\nCamelyon17. This dataset is a collection of histopathology whole-slide images used for the detection of metastases in lymph nodes; classifying the given slide into benign tissue vs cancerous tissue. It includes images from five medical centers, each with different staining protocols, equipment, and imaging settings. These differences simulate natural real-world distribution shifts. We use the train set (hospital 1-3) for source, and the test set (hospital 5) for the target."}, {"title": "C.2 Preparing The Concept Bottleneck", "content": "There are various ways of defining the concept vectors {csi}m, in the concept prediction layer vc(x). Early works on CBM required the training dataset to have concept annotations from domain experts in addition to the class labels for training the concept predictor (Koh et al., 2020). Subsequent works have also explored learning the concept vectors in an unsupervised manner (without any concept annotations) (Yeh et al., 2020; Choi et al., 2023). More recently, natural language concept descriptions and modern vision-language models (e.g., Stable Diffusion (Rombach et al., 2022)) are being leveraged to automatically generate concept examples (Yuksekgonul et al., 2023; Wu et al., 2023b) for finding the Concept Activation Vectors (CAVs) (Kim et al., 2018) (each CAV corresponds to a csi), or to directly guide the construction of concept bank Cs (Oikarinen et al., 2023). We highlight that in all prior works (to our knowledge) the concept bank remains static, i.e., once the set of concept vectors is defined and the CBM is deployed, its predictions are made based on these predefined concepts, regardless of any distribution shift at test time.\nYuksekgonul et al. (2023). For CIFAR10 and CIFAR100, we use the BRODEN visual concepts datasets Bau et al. (2017) to learn concept activation vectors, which are used to initialize the weights and bias parameters of the concept bottleneck layer, as described in Yuksekgonul et al. (2023). For Waterbirds and Metashift, we use the images belonging to the concept categories as follows; nature, color, and textures for Waterbirds, and nature, color, texture, city, household, and others for Metashift. For Camelyon17, we use color and textures categories, following the setting in Wu et al. (2023b).\nYeh et al. (2020). For a fair comparison, we set the number of the concepts to be the same as the size of concept bottleneck by Yuksekgonul et al. (2023) except with Metashift where we use 100 concepts instead, since with over 100 concepts, we found there are much unnecessary redundancy between them.\nOikarinen et al. (2023). Following their instructions, we create the initial concept set using GPT-3, followed by concept filtering. For the sparsity of the linear probing layer, we set x = 0.001 and a = 0.5."}, {"title": "D Ablation Experiments", "content": "Ablation Study on Hyperparameters.\nIn Figure 5, we present a comprehensive ablation study illustrating how different hyperparameter choices affect the performance of our proposed method.\nMost notably, in Figures 5a and 5b, we observe that Afrob influences the adaptation performance differently depending on the type of distribution shift (i.e., CIFAR10-C for low-level shifts and Waterbirds for concept-level shifts). Recall that Afrob controls how much the concept vectors are allowed to deviate from their original construction during adaptation. When Afrob is very low (e.g., 0.001), the weights in the concept bottleneck layer deviate excessively, leading to instability.\nIn the case of low-level shifts, as shown in Figure 5a, over-regularizing the Frobenius norm term (e.g., setting Afrob as high as 10) prevents the method from addressing the non-robustness of the concept bottleneck under such shifts (i.e., the first failure mode in Section 2.3). Selecting a suitable moderate value such as Afrob = 0.1 leads to optimal performance.\nIn contrast, under concept-level shifts depicted in Figure 5b, allowing deviation of the concept vectors harms performance. By strongly regularizing with a high Afrob value (e.g., Afrob = 10), we can nearly preserve the original pre-adaptation performance (note that WG drops to almost zero when Afrob < 1). This occurs because failures of CBMs under concept shifts need to be addressed by adapting the linear probing layer rather than the concept bottleneck layer (the second failure mode in Section 2.3).\nHowever, when all components of CONDA are activated, the adaptation performance becomes quite insensitive to the choice of Afrob, regardless of the type of distribution shift, since all the components collaboratively combine to handle all possible failure modes."}, {"title": "E Additional Interpretability Analysis", "content": "In this section, we include additional experiments and analysis to better understand the interpretability of CONDA as well as the utility of the RCB component."}, {"title": "E.1 Residual Concept Bottleneck Compensates for Prediction Errors", "content": "Here we aim to understand how including the RCB component in CONDA impacts the predictions of the adapted classifier. We conduct an analysis similar to the one in Appendix B of Yuksekgonul et al. (2023), where they evaluate the impact of the residual predictor PCBM-h and when it alters the predictions of"}, {"title": "E.2 Accuracy-Interpretability Tradeoff in Residual Concept Bottleneck", "content": "In this sub-section, we aim to answer to the following question: while the residual concept bottleneck improves the adaptability of CONDA, does it potentially affect the interpretability by introducing additional model complexity? An analysis of the trade-off between model complexity and interpretability, particularly as new residual concepts are added, would be valuable for practitioners seeking interpretable yet robust models.\nHere we apply our adaptation to PCBM (CLIP), where each concept vector is constructed using CLIP text embeddings of concept captions, deployed to the Waterbirds dataset. As discussed in Appendix B.2, for concept annotation, we leveraged the ConceptNet hierarchy following the setup in Yuksekgonul et al. (2023). We searched ConceptNet for the words \u201cBird\u201d, \u201cWater\u201d, \u201cLand\u201d and obtained concepts that have the following relationship with the query concept: hasA, isA, partof, HasProperty, and MadeOf.\nWe compare the two CONDA variants (i) PCBM + CSA + LPA and (ii) PCBM + CSA + LPA + RCB by varying the number of residual concepts (r) and evaluating the following metrics:\n* Relative accuracy of method (ii) minus (i), both for AVG and WG.\n* Similarity score output in Eqn. 18 from the automatic concept annotation method described in Ap-pendix B.2.\nThe similarity score is used as a quantitative metric to measure the interpretability of RCB. To be more specific, the score in Eqn. 18 tells us how aligned the assigned concept caption is with each residual concept vector. A low score implies that the assigned caption is not a good description for the concept."}, {"title": "E.3 Additional Interpretability Results", "content": "In Figure 8"}]}