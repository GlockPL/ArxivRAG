{"title": "THE EXPLABOX: MODEL-AGNOSTIC MACHINE LEARNING TRANSPARENCY & ANALYSIS", "authors": ["Marcel Robeer", "Michiel Bron", "Elize Herrewijnen", "Riwish Hoeseni", "Floris Bex"], "abstract": "We present the Explabox: an open-source toolkit for transparent and responsible machine learning (ML) model development and usage. Explabox aids in achieving explainable, fair and robust models by employing a four-step strategy: explore, examine, explain and expose. These steps offer model-agnostic analyses that transform complex \u2018ingestibles' (models and data) into interpretable 'digestibles'. The toolkit encompasses digestibles for descriptive statistics, performance metrics, model behavior explanations (local and global), and robustness, security, and fairness assessments. Implemented in Python, Explabox supports multiple interaction modes and builds on open-source packages. It empowers model developers and testers to operationalize explainability, fairness, auditability, and security. The initial release focuses on text data and models, with plans for expansion. Explabox's code and documentation are available open-source at https://explabox.readthedocs.io.", "sections": [{"title": "1 Introduction", "content": "It is crucial that Machine Learning (ML) development and usage is done in a responsible and transparent manner. High-stakes organizational decisions may significantly impact individuals and society, with potential severe consequences stemming from biases or model errors. This is exemplified by the EU AI Act's regulatory framework, requiring high-risk systems to be properly tested, documented and assessed on their conformity before being applied in practice [Edwards, 2022]. Yet, operationalizing transparency (explainable ML) and testing model behavior (fairness, robustness, and security auditing) remains a difficult and laborious task, given the myriad of techniques and their associated learning curves. In response, we have devised a comprehensive four-step analysis strategy-explore, examine, explain, and expose ensuring holistic model transparency and testing. The open-source Explabox offers these analyses through well-documented, reproducible steps. Data scientists can now access a unified, model-agnostic approach developed and used in a high-stakes context\u2014the Netherlands National Police-that is applicable to any text classifier or regressor.\nWe have developed the Explabox in an organizational environment where models and data are analyzed repeatedly, and where internal and external stakeholders have varying explanatory needs and preferred formats. Several related tools have been made available, such as AIX360 [Arya et al., 2019], alibi explain [Klaise et al., 2021], dalex [Baniecki et al., 2021], CheckList [Ribeiro et al., 2020], and AIF360 [Bellamy et al., 2018]. However, these tools exhibit shortcomings such as incompatibility with recent Python versions (3.8\u20133.12), restricted software functionality primarily focused on testing or explainability, an absence of reproducible outcomes, or that they do not provide the flexibility regarding how results can be communicated to address stakeholder needs. To fill this gap, we propose the Explabox. The Explabox is an open-source Python toolkit that supports organizations with responsible ML development with minimal disruption to practitioners' workflows through a four-step analysis strategy easily embedded with existing datasets and models, and providing a central node for connecting with state-of-the-art research and sharing best practices."}, {"title": "2 The Explabox: Explore, Examine, Explain & Expose your ML models", "content": "The Explabox transforms opaque ingestibles into transparent digestibles through four types of analyses. The digestibles provide insights into model behavior and data, enhancing model explainability and assisting in auditing the fairness, robustness, and security of ML systems."}, {"title": "2.1 Ingestibles", "content": "Ingestibles serve as a unified interface for importing models and data. The layers (Fig 1a) abstract away from how the model and data are accessed, and allow for optimized processing. The Explabox encapsulates the model and data with instancelib [Bron, 2023] to ensure fast processing. The model can be any Python Callable containing a regression or (binary and multi-class) classification model. Models developed with scikit-learn [Pedregosa et al., 2011] or with inferencing through onnx (e.g., PyTorch and TensorFlow/Keras) can be imported directly with further optimizations and automatic extraction of how inputs/outputs are to be interpreted. Data can be automatically downloaded, extracted and loaded. Data can be provided as NumPy arrays, Pandas DataFrames, huggingface datasets, raw files (e.g., HDF5, CSV or TSV), or as (compressed) folders containing raw files. The data can be subdivided into named splits (e.g., train-test-validation), and instance vectors and tokens can be precomputed (and optionally saved on disk) to provide fast inferencing."}, {"title": "2.2 Analyses", "content": "The Explabox turns these ingestibles into digestibles: pieces of information that increase the transparency of the ingestibles. Turning ingestibles into digestibles is done through four types of analyses: explore, examine, explain and expose.\nExplore allows slicing, dicing and sorting data, and provides descriptive statistics, grouped by named split. Relevant statistics include data set sizes and label distributions, and modality-relevant information, such as string lengths and tokenized lengths for textual data."}, {"title": "2.3 Digestibles", "content": "To serve diverse stakeholders' needs\u2014such as auditors, applicants, end-users or clients [Tomsett et al., 2018]\u2014in consuming model and data insights, the digestibles are accessible through different channels like an interactive user interface (UI) for Jupyter Notebook or webpages (Fig 1c), an API for integration with other tooling, and static reporting."}, {"title": "2.4 Open-source implementation", "content": "Explabox is a Python library with full cross-platform support for versions 3.8\u20133.12 (see Fig. 1b for example anal-yses). Distributed under the GNU LGPL-3.0 license, it offers a flexible inferencing and data-handling API through instancelib [Bron, 2023]. It also benefits from other open-source communities, such as scikit-learn [Pedregosa et al., 2011] for surrogate models, clustering and dimensionality reduction, imodels [Singh et al., 2021] for rule-based interpretable models, Faker [Faraglia and Other Contributors, 2021] for multi-language data generation, and plotly [Plotly Technologies Inc., 2015] for interactive and static graphics.\nThe Explabox documentation provides installation guides, a tutorial to get started, a comprehensive example use-case using Jupyter Notebook, a full overview of all Python classes and functions, and guides on how to contribute. Documentation generation, testing, code quality assurance, and versioning are automated with a GitHub CI/CD pipeline."}, {"title": "3 Conclusion and Future Work", "content": "The Explabox enables organizations to responsibly develop and apply AI applications. The model-agnostic toolkit provides interactive analyses that model developers and testers can use to operationalize, report and discuss model explainability, fairness, auditability, and security. It provides flexibility to handle various ML use-cases for clas-sification and regression, while also being a central node for standardization and connecting research with prac-tice. The first full release, focusing on text data and models, is available open-source and fully documented at https://explabox.readthedocs.io.\nFurther development has already started, where we focus on extending the Explabox for (1) models and data with the tabular, audio and image modality, and multi-modal combinations; (2) improved integration of data and model provenance for auditability; (3) comparisons of multiple datasets and multiple models, and; (4) research and development for a new interface and improved stakeholder data visualization."}]}