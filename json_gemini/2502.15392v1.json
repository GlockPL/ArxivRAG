{"title": "Chitrarth: Bridging Vision and Language for a Billion People", "authors": ["Shaharukh Khan", "Ayush Tarun", "Abhinav Ravi", "Ali Faraz", "Akshat Patidar", "Praveen Pokala", "Anagha Bhangare", "Raja Kolla", "Chandra Khatri", "Shubham Agarwal"], "abstract": "Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages. To address this limitation, we introduce Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data. Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating VLMs across various Indian languages, ultimately contributing to more diverse and effective AI systems. Our model achieves SOTA results for benchmarks across low resource languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation to facilitate future advancements in this arena.", "sections": [{"title": "Introduction", "content": "With the success and demonstrated effectiveness of Visual instruction tuning [Liu et al., 2023, 2024], recent years witnessed a surge of interest in developing general purpose multimodal conversational agents. These unified foundation models excel at algorithmic reasoning and generic perception tasks like image captioning, visual question answering, text-based image retrieval, etc. [Lu et al., 2024a, Lauren\u00e7on et al., 2024b, Tong et al., 2024, Xue et al., 2024], and more specialized frameworks, for instance, converting Scalable Vector Graphics (SVGs) to code [Rodriguez et al., 2023]. Often, these models rely on pre-trained Large Language Models (LLMs) [Brown et al., 2020, Touvron et al., 2023a, Achiam et al., 2023, Chiang et al., 2023, Touvron et al., 2023b, Gemini et al., 2023, Jiang et al., 2024, Gemma et al., 2024, Dubey et al., 2024] as the transformer [Vaswani et al., 2017] backbones, primarily trained on English or high resource European languages.\nThis work is driven by two main motivations: 1. Language diversity gap: Most Vision Language Models (VLMs) are predominantly trained on English datasets, overlooking the linguistic needs of non-English languages, particularly from the Indian subcontinent. 2. Lack of low resource language benchmarks: Absence of corresponding VLM benchmarks hinders the progress for these low resource Indic languages. We aim to address these issues through our research and serve a broader audience, encompassing billions of people.\nFew LLMs have been developed specifically for Indic languages [Gala et al., 2024, Labs, 2023, Balachandran, 2023, Kohli et al., 2023], most of which extend and fine-tune text-only English-centric LLMs. Naturally, they fail to fully capture the nuances of the language, with the exception of models like [Kallappa et al., 2024, Bendale et al., 2024], trained from scratch. Our model builds upon the recent success of Krutrim LLM [Kallappa et al., 2024], which supports English and 10 other languages including Hindi, Bengali, Telugu, Tamil, Marathi, Gujarati, Kannada, Malayalam, Odia, and Assamese, representing a significant portion of the cultural and linguistic diversity in India.\nAnother key challenge is the limited availability of low resource data with Indic languages significantly under-represented in Common Crawl despite India (or Bharat) making up 18% of the global population. For instance, Hindi, in spite of being the third most spoken, does not appear among the top 20 languages [Buck et al., 2014, Penedo et al., 2023]. To enhance our model's cross-lingual generalization abilities, we translate the open-source multimodal training datasets into the 10 Indic languages natively supported by the backbone LLM. Developing this multilingual dataset is a substantial endeavor aimed at addressing the disparity between high-resource and relatively low-resource Indian languages in the context of vision-language models.\nIn this paper, we present our multimodal LLM, which employs the Krutrim multilingual LLM backbone [Kallappa et al., 2024] in conjunction with a pre-trained visual image encoder [Dosovitskiy et al., 2020]. A brief summary of our contribution is provided below:\n\u2022 We introduce Chitrarth (Chitra: Image; Artha: Meaning), a Multimodal LLM model which leverages images and language modalities for a range of visual tasks such as image captioning, visual question answering in the multilinugal context. We further present optimal training recipe including data composition and architecture configuration.\n\u2022 We also present BharatBench, a comprehensive evaluation benchmark suite designed for 10 under-resourced Indic languages across 3 tasks, which we will make available to the research community upon acceptance.\n\u2022 Finally, we evaluate Chitrarth and prior baselines on both existing English academic datasets as well as the proposed evaluation framework and demonstrate the effectiveness of our model, using different training strategies and ablations, achieving SOTA results on 3 out of 5 English datasets and propose benchmark results on the derived multi-lingual datasets.\nThe remainder of the paper is structured as: Section 2 reviews recent related research on VLMs. Section 3 provides a detailed description of our Chitrarth model with information about training data mix in Section 4. Section 5 introduces the BharatBench evaluation framework that we propose, while Section 6 presents the experimental results. Finally, Section 7 offers concluding remarks."}, {"title": "Related Work", "content": "2.1 English-centric VLMs\nRecent studies [Lauren\u00e7on et al., 2024b,a, Tong et al., 2024] have investigated design strategies for multi-stage training pipelines in contemporary VLMs. Typically, these models rely on pre-trained LLMs; however, there are some exceptions where models are trained from scratch [Chameleon, 2024, Lu et al., 2024b]. Prior works like Flamingo[Alayrac et al., 2022] leverage a Perceiver Resampler [Jaegle et al., 2021] to inject visual features into the language model through cross-attention, promoting quick adaptation to various tasks with few labeled examples. The LLaVA family models [Liu et al., 2023, 2024], including LLaVA-1.5 and LLaVA-1.6, demonstrated intriguing multimodal comprehension capabilities by integrating advanced language models with vision encoders through visual instruction tuning. PaliGemma [Beyer et al., 2024], optimized for tasks that require deep integration of visual and textual data, is designed to excel in scenarios where English is the primary language. Florence-2 [Xiao et al., 2024] focuses on handling diverse tasks from simple text prompts addressing the complexity of spatial hierarchy and semantic granularity. The Idefics family [Lauren\u00e7on et al., 2024b,a] is focused on substantially enhancing capabilities around OCR, document interpretation and visual reasoning functionalities. CogVLM [Wang et al., 2023] drives an intricate fusion of language and vision features unlike other VLMs, which rely on the shallow alignment method. PALI models [Chen et al., 2022] on the other hand explored contrastive pretraining and higher resolution training for the VLM tasks.\n2.2 Multi-lingual VLMs\nQwen-VL [Bai et al., 2023] is a multilingual VLM, trained on English and Chinese data, supporting diverse instructions and multi-image context analysis. InternVL 1.5 [Chen et al., 2024] proposed an enhanced vision encoder and a superior bilingual dataset, i.e., English and Chinese. Phi-3 family [Abdin et al., 2024] offer multilingual, multimodal, and long-context support in 11 languages, including English, across the world but do not cover Indian languages. PALO [Maaz et al., 2024] is the closest VLM to our research, however supporting only 3 Indian languages Hindi, Urdu, and Bengali apart from the other high-to-medium resource language offerings. To our knowledge, no other open-source multimodal LLMs include low-resource Indic languages in the training mix. In contrast, our work introduces a multilingual VLM system that supports ten Indian languages."}, {"title": "Chitrarth: A Multilingual Vision-Language Model", "content": "In this section, we outline the architecture of our proposed Chitrarth model. Chitrarth is an autoregressive VLM where the input image is tokenized into visual tokens, combined with textual instruction tokens and fed into the large language model (LLM). Inspired by the versatile and widely followed LLaVA [Liu et al., 2023, 2024] framework, our model incorporates several key components, as illustrated in Figure 2, where we use pre-trained Krutrim LLM [Kallappa et al., 2024] instead, as the autoregressive multi-lingual LLM backbone.\nFor multimodal training, we start by encoding images using a vision encoder. The modality projection layer (adapter/connector) maps the vision embeddings into the LLM embedding space, producing a sequence of visual tokens. The multilingual LLM then generate responses based on these visual tokens. The Krutrim LLM [Kallappa et al., 2024] supports a context length of 4096 tokens, of which 576 tokens (14X14 patch size results in 729 tokens) are allocated for image representation after the modality projection. We explore different configurations for the projection layer, including a single-layer projection [Liu et al., 2023, 2024] and a two-layer MLP vision-language connector with non-linearity [Liu et al., 2023]. Additionally, we experiment with various vision encoders, including the pre-trained CLIP ViT-L/14@336px [Radford et al., 2021] and SigLIP-SO400M [Zhai et al., 2023].\nOur model is trained in multiple stages:\nStage 1: Pre-Training (PT) for Feature Alignment. In this stage, we conduct pre-training using image-text pairs, with the projector layer being trained while keeping the vision encoder and LLM fixed. Each sample is treated as a single-turn conversational instruction for tuning.\nStage 2: Instruction Tuning (IT). In this stage, we maintain the vision encoder in a frozen state, following the approach used in LLaVA models [Liu et al., 2023, 2024]. However, unlike the previous stage, we also update the weights of the LLM in addition to tuning the modality projection layer. The objective of this stage is to develop a general-purpose multimodal agent (chatbot) capable of comprehending and executing complex instructions across multiple conversational turns. We describe the datasets used in both the stages in the next section."}, {"title": "Dataset", "content": "Figure 3 illustrates the language distribution of our data mix for both the training stages, which we describe in more detail below:\nStage 1: For Stage 1 adapter Pre-Training (PT), we use the 1.2 million-sample ShareGPT4V-PT dataset [Chen et al., 2023], which demonstrated consistent superior performance compared to other PT datasets, such as LLaVA-Pretrain-LCS-558K [Liu et al., 2023], in our preliminary experiments. This dataset was subsequently translated into the ten Indic languages supported by the Krutrim LLM. Specifically, we use the open-source model, IndicTrans2 [Gala et al., 2023] for this text-only translation task. IndicTrans2 outperformed other translation services (Yandex, ChatGPT, Google Translate, and Bard) in small-scale in-house qualitative human evaluation (win rates 93% and 80% for Bengali and Marathi respectively). We ensure the pre-training data remained at 1.2M points, with half of the data in English, and sample translations across different languages in an equal ratio to create a balanced multilingual dataset. This approach was designed to preserve linguistic diversity and computational efficiency, thereby ensuring robust performance in English while developing capabilities in the Indic languages. The balanced dataset mitigates potential biases towards any single language, fostering equitable performance across all supported languages.\nStage 2: The Stage 2 Instruction Tuning (IT) dataset is notably more intricate. The core element of this dataset is the complete English version of LLaVA-1.5-665K [Liu et al., 2024]. Additionally, we translate LLaVA-Instruct-150K [Liu et al., 2023] into ten languages using the methodology outlined in Stage 1. Our dataset also incorporates the Cauldron dataset [Lauren\u00e7on et al., 2024b], which includes 50 academic vision-language tasks along with its corresponding in-house translations. Furthermore, we add a substantial collection of images reflecting Indian cultural diversity comprising prominent personalities, monuments, artwork, culinary dishes, and more; transformed into multilingual pluralistic instruction tuning data, analogous to the open-source English-based LLaVA-IT datasets. Lastly, our dataset features high-quality, text-only English proprietary data. The final composition of the dataset includes approximately 880K English and 90K samples in multiple languages, ensuring a balanced and diverse dataset. This comprehensive range of content supports the development of a model capable of generating and understanding complex descriptions across various domains and visual scenarios, thereby enhancing its reasoning capabilities."}, {"title": "BharatBench Evaluation Suite", "content": "Although recent efforts have advanced text-only multilingual evaluation [Ahuja et al., 2023, Singh et al., 2024], there is still a lack of evaluation framework for multimodal multilingual scenarios. We introduce BharatBench, a benchmark designed to assess the image understanding capabilities of multilingual Vision-Language Models (VLMs). Expanding upon LLaVa-Bench (In-the-Wild) [Liu et al., 2023], initially adapted for Hindi and Bengali by [Maaz et al., 2024], we further broadened the benchmark to cover eight additional low resource languages. This extension now forms part of our comprehensive benchmark suite. Furthermore, we include translated versions of prominent VLM evaluation datasets, such as MMVet [Yu et al., 2023] and POPE [Li et al., 2023] covering all ten languages in our study, in addition to English."}, {"title": "Experiments", "content": "6.1 Implementation\nWe use PyTorch [Paszke et al., 2019] based HuggingFace Transformers [Wolf et al., 2019] for our experiments. Our Stage 1 and 2 tuning use hyperparameters consistent with those of the LLaVA model [Liu et al., 2023], unless otherwise specified. Particularly, we train the model for 1 epoch in both the stages with an overall batch size of 256 in Stage 1 and 128 in Stage 2. We used cosine LR scheduler with Adam optimizer and a learning rate of 2e-3 and 2e-5 in both the stages respectively. We consider IDEFICS 2 [Lauren\u00e7on et al., 2024b] and PALO [Maaz et al., 2024] as respective English and multi-lingual baselines and report results from their published work. All our models are trained on 8 \u00d7 H100 GPUs which takes around 8 hours for Stage 1 and 18 hours for Stage 2 tuning.\n6.2 English academic benchmarks\nWe also evaluate our model using a range of English academic benchmarks, including VQA-v2 [Goyal et al., 2017] and GQA [Hudson and Manning, 2019] for visual perception, VizWiz [Gurari et al., 2018] for zero-shot generalization on questions posed by visually impaired users, and TextVQA [Singh et al., 2019] for text-rich visual question answering. We also use POPE [Li et al., 2023] to assess hallucination tendencies, MME [Fu et al., 2023] for yes/no question responses, and LLaVA-Bench (In-the-Wild) [Liu et al., 2023] and MM-Vet [Yu et al., 2023] for visual conversation capabilities. Evaluation scores are reported following prior works.\n6.3 Results\nOn the English academic datasets, our model depicts State-of-the-art (SOTA) results for POPE, VQAv2 and GQA compared to the baseline models, while remaining competitive on TextVQA and Vizwiz (see radar graph in Figure 5). On the LLaVA-Bench (Bengali) our model outperforms the multi-lingual baseline PALO and achieves SOTA results of 53.7 points. Table 1 presents results on BharatBench across various languages, demonstrating that ours is the only model capable of handling all included languages, establishing baseline results for future research. Figure 4 showcases selected outputs from our top-performing Multimodal LLM across various languages. The model excels in tasks such as creative writing, fine-grained attribute extraction, explaining scientific diagrams, and screen reading/OCR, while also demonstrates strong capabilities in anomaly and hazard detection, as well as real-time accident and incident monitoring. In our manual qualitative evaluation, we observe that our model is able to better understand the images of Indian context such as the prominent lady figure in Figure 6, compared to generic and incorrect responses from GPT-40. This could be attributed to the inclusion of high quality culturally rich images in Stage 2. A further quantitative analysis around this would be interesting but out of scope of this work.\nWe conducted an ablation study evaluating various vision encoders and found that SigLIP-SO400M consistently outperforms CLIP ViT-L/14@336px across all English benchmarks, achieving faster convergence (see Figure 7). Notably, SigLIP-SO400M yields improvements of 11 points on TextVQA and 13 points on LLaVA-Bench compared to CLIP ViT-L/14@336px. Figure 8 explores the impact of multilingual training data on the English academic benchmarks. We compare our model's performance when trained with only English, bilingual, and multilingual data across both stages. Consistent with the findings of [Scao et al., 2022], expanding the range of languages in the training data improves multilingual capabilities but leads to decreased performance on academic English datasets. This underscores a key challenge in balancing cross-lingual performance."}, {"title": "Conclusion", "content": "This paper presents Chitrarth, a multilingual multimodal LLM that is able to have image grounded conversations in English as well as across multiple Indian languages. Our model encodes images using a pre-trained vision encoder [Dosovitskiy et al., 2020] and autoregressively generates response using a pre-trained multi-lingual LLM. Empirically, our model outperforms previous baselines for different multimodal tasks. As part of this work, we also introduce BharatBench, a multimodal evaluation framework and provide benchmark results for low resource languages. We anticipate that our research will significantly contribute to the advancement of VLMs for Indian languages, thereby providing substantial benefits to a population exceeding one billion people.\nLimitations and Future Work: We use an automated translation pipeline for creating multi-lingual training data which may introduce biases from large language models (LLMs), potentially leading to misrepresentations of cultural symbols and gestures, impacting content accuracy. Addressing these biases requires additional evaluation and targeted training, which we plan to address in the future work. Building on our promising results across 10 low-resource languages, we plan to broaden the language scope in the future research to enhance linguistic diversity and inclusivity in our Vision-Language Models (VLMs). In our current training pipeline, we keep the vision encoder frozen throughout both training stages. However, recent research [Lauren\u00e7on et al., 2024b, Tong et al., 2024] suggests that unfreezing the vision encoder could enhance representation learning. We plan to investigate this approach in future work with higher resolution vision encoders, along with expanding our model's ability to interpret multiple images within a conversational context."}]}