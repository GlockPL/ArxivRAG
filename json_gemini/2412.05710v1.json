{"title": "PROMPTREFINE: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from Related Example Banks", "authors": ["Soumya Suvra Ghosal", "Soumyabrata Pal", "Koyel Mukherjee", "Dinesh Manocha"], "abstract": "Large Language Models (LLMs) have recently demonstrated impressive few-shot learning capabilities through in-context learning (ICL). However, ICL performance is highly dependent on the choice of few-shot demonstrations, making the selection of the most optimal examples a persistent research challenge. This issue is further amplified in low-resource Indic languages, where the scarcity of ground-truth data complicates the selection process. In this work, we propose PROMPTREFINE, a novel Alternating Minimization approach for example selection that improves ICL performance on low-resource Indic languages. PROMPTREFINE leverages auxiliary example banks from related high-resource Indic languages and employs multi-task learning techniques to align language-specific retrievers, enabling effective cross-language retrieval. Additionally, we incorporate diversity in the selected examples to enhance generalization and reduce bias. Through comprehensive evaluations on four text generation tasks-Cross-Lingual Question Answering, Multilingual Question Answering, Machine Translation, and Cross-Lingual Summarization using state-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and Qwen-2.5-7B, we demonstrate that PROMPTREFINE significantly outperforms existing frameworks for retrieving examples.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have recently made remarkable progress, demonstrating human-level performance across a wide range of tasks (Adiwardana et al., 2020; Wang et al., 2019). However, despite these advancements, most LLMs, such as LLaMA-3 (Dubey et al., 2024), LLaMA-2 (Touvron et al., 2023), and Qwen (Yang et al., 2024), are predominantly pre-trained on English texts, leading to significant performance disparities when applied to low-resource, non-English languages (Ahuja et al., 2023). The scarcity of ground-truth paired data in many low-resource languages makes text generation particularly challenging, as fine-tuning LLMs becomes infeasible in such settings. This issue is especially pronounced in lesser-known Indic languages such as Tibetan which has only around 5000 Wikipedia articles compared to 6.6M+ Wikipedia articles in English, despite having approximately 6M speakers. In this work, we focus on downstream generation tasks with low-resource Indic languages, a critical challenge for making LLMs more widely accessible.\nWhen downstream tasks have limited labeled data, few-shot learning or in-context learning (ICL) has emerged as a powerful and practical approach for text generation (Tanwar et al., 2023; Zhang et al., 2021; Winata et al., 2021; Huang et al., 2023; Etxaniz et al., 2023). ICL operates by providing the LLM with a prompt that consists of task-specific instructions and a set of input-output examples (demonstrations) to guide the model's output generation for a specific input query. While ICL is computationally efficient (it requires no parameter updates), it faces two critical challenges when labeled data is scarce, particularly for low-resource Indic languages.\n(Relevance) First, as in any learning task with limited data, the small size of the available example pool can result in a lack of relevant examples to guide the model effectively. In the context of ICL, this scarcity can severely degrade performance, as the quality of examples is crucial. Poor example selection can lead to performance worse than zero-shot scenarios, while optimal selection can achieve near state-of-the-art results (Liu et al., 2021). (Diversity) Second, existing example selection techniques, such as random selection or retrieving semantically similar examples (Zhang et al., 2021; Winata et al., 2021; Tanwar et al., 2023), often"}, {"title": "2 Related Works", "content": "In-Context Learning (ICL). First introduced in Brown (2020), ICL has emerged as a powerful approach that enables large language models (LLMs) to \"learn by analogy\" by providing a few input-output examples as demonstrations, without requiring any update to model parameters. In recent years, a plethora of studies have provided insights on the underlying mechanism of ICL. Saunshi et al. (2020) suggested that, by conditioning on a prompt, the task of predicting the next word becomes linearly separable, while Xie et al. (2021) observed that for ICL, the model infers a shared latent concept between the provided examples. A study pointed out that models do not rely as heavily on the provided input-output mappings as previously thought, indicating more nuanced learning dynamics in ICL (Min et al., 2022). Chen et al. (2022); Min et al. (2021); Wei et al. (2023a) showed that the in-context learning ability of LLMs can be improved through self-supervised or supervised training. A group of studies have also explored in order to understand the factors affecting ICL (Zhao et al., 2021; Shin et al., 2022; Wei et al., 2022a; Yoo et al., 2022; Wei et al., 2023b) and the underlying working mechanism of ICL (Olsson et al., 2022; Li et al., 2023b; Pan, 2023; Dai et al., 2022)."}, {"title": "3 Preliminaries", "content": "3.1 In-Context Learning\nIn-context learning (ICL) leverages the intrinsic abilities of language models to learn and infer new tasks without the need for parameter updates. Formally, let $T_{LM}$ be a language model with a vocabulary V. Consider a downstream generation task with input space X and output space Y. For a given test query $x_{test} \\in X$ and a retrieved subset of K input-output pairs $\\{(x_i, y_i)\\}_{i=1}^K \\in X \\times Y$ describing the intended task, ICL generates the output $y_{test}$ as follows:\n$y_{test} \\sim T_{LM}(\\cdot|x_1, y_1, x_2, y_2, ..., x_K, y_K, x_{test})$\nHere, $ \\sim $ represents the sampling techniques commonly used in the literature, such as Greedy Sampling, Top-p Sampling (Holtzman et al., 2019), Top-k Sampling (Fan et al., 2018), and Beam Search (Freitag and Al-Onaizan, 2017). Each in-context example $a_i = (x_i, y_i) \\in X \\times Y$ is"}, {"title": "3.2 Example Retrieval for Few-shot learning", "content": "Our main goal is to train a retriever $R_{\\phi}(x_{test}, D)$ model parameterized by $\\phi$, that retrieves a set of in-context examples $\\{a_i\\}_{i=1}^K \\subset D$ given a test sample $x_{test}$ where typically $K \\ll N$. Usually, $\\varphi: (X \\cup Y)^* \\rightarrow \\mathbb{R}^d$ represents the embedding function that maps the text in the input space X into a d-dimensional vector representation. Such representations are subsequently used to measure similarity between samples. Previous works have explored various retrieval strategies, ranging from random example selection to using off-the-shelf retrieval models (Liu et al., 2021; Wu et al., 2022), as well as fine-tuning the retriever's embeddings (Ye et al., 2023; Rubin et al., 2021).\nRelevance Based Fine-tuning. In this study, we leverage the framework proposed by Rubin et al. (2021) to fine-tune an efficient dense retriever $R_{\\phi}$. The core idea is to train the retriever on a labeled dataset curated from the training data itself, optimizing it to select examples that serve as effective prompts. For each sample $(x,y) \\in D$, we generate a candidate set $A = \\{a_i\\}_{i=1}^F$, where $a_i \\in D \\setminus \\{(x, y)\\}$. The candidate set is selected using an unsupervised BM25 retriever: $A = BM25(\\{(x, y)\\}, D)$ that simply retrieves K examples with closest vector embedding to $\\varphi(x)$. Next, each candidate example $a \\in A$ is scored using a language model Scorer based on its relevance to the sample $(x, y)$.\n$s(a_i; (x, y)) = Scorer(y | a_i, x).$ (1)\nThe best candidate example for the sample $(x, y)$ is selected as $a^* = arg \\max_{a_i} s(a_i; (x, y))$. Finally, the retriever $R_{\\phi}$ is fine-tuned to rank the candidate examples optimally (align with ranking induced by Scorer) by minimizing the negative log-likelihood under softmax loss:\n$\\min_{\\phi} \\mathcal{L}_{rel}(D; \\phi) = \\frac{1}{N} \\sum_{i=1}^N l(x_i, A_i)$ (2)\n$l(x, A; \\phi) = -log\\frac{e^{sim(x, \\tilde{a})}}{\\sum_{a \\in A} e^{sim(x, a)}}$ (3)\nwhere $sim(a_i, a_j) = \\varphi(a_i)^T\\varphi(a_j)$ measures the cosine similarity between the embeddings. Note,"}, {"title": "3.3 Determinantal Point Processes", "content": "In this subsection, we introduce a recently studied framework Determinantal Point Processes (DPPs) (Ye et al., 2023) for example selection that ranks subsets of examples rather than individual ones. Introduced in Macchi (1975), DPP's are elegant probabilistic models that have been extensively used in the literature (Ye et al., 2023; Borodin and Olshanski, 2000; Benard and Macchi, 1973; Kulesza et al., 2012; Liu et al., 2022) to capture negative correlation among items.\nWe leverage the DPP framework to promote diversity within the set of retrieved in-context examples. Formally, a point process P is called a DPP if, for any random subset Y drawn according to P, the probability that a subset S is contained within Y is given by:\n$P(S \\subseteq Y) = det(Z_S)$\nwhere $Z \\in \\mathbb{R}^{n \\times n}$ is a PSD similarity matrix, and $Z_S$ denotes the submatrix of K corresponding to the rows and columns indexed by S. For in-context learning, given a test sample $x_{test}$, the input dependent similarity of any two examples $a_i, a_j$ is denoted by $Z_{ij}$ - formally, we model log $Z_{ij}$ as\n$\\sum_{k \\in \\{i,j\\}} \\varphi(a_k)^T (x_{test}) + \\varphi(a_i)^T\\varphi(a_j)$\nwhere $\\varphi(a_i)^T(x_{test}) \\in \\mathbb{R}^+$ measures the relevance of $a_i$ to input $x_{test}$ and $\\varphi(a_i)^T\\varphi(a_j)$ measures the similarity between the $i^{th}$ and $j^{th}$ example."}, {"title": "4 Proposed Framework", "content": "Problem Setup. Despite the growing focus on evaluating the multilingual capabilities of large language models (LLMs), there remains a substantial performance gap between high-resource languages and those with limited web resources (Ahuja et al., 2023; Singh et al., 2024). In this study, we aim to address this gap by enhancing the few-shot performance of low-resource Indic languages. To this end, we adopt the recently released IndicGen Benchmark (Singh et al., 2024), targeting a subset"}, {"title": "Algorithm 1 PROMPTREFINE", "content": "1: Input: Low-resource language T example bank $D^T$; validation set $D_{val}$; Auxiliary example bank $D^{aux} = \\{D^{H_1},...,D^{H_M} \\}$, number of iterations I, Accuracy Metric Acc.\n2: $ \\phi \\leftarrow \\alpha$\n3: $ \\rho \\leftarrow MBERT$ \u25b7 Initialize the retriever embedding with pre-trained Multi-lingual BERT encoder\n4: for iter in \\{1,\u2026\u2026,I\\} do\n5:    $ \\Phi \\leftarrow \\phi$\n6:    for each dataset $D^i \\in \\{D^T, D^{H_1},\u2026\u2026\u2026, D^{H_M} \\}$ do\n7:       $ \\phi_i \\leftarrow \\min_{\\phi_i} \\mathcal{L}_{rel}(D^i; \\phi_i)$\n8:       $ \\Phi \\leftarrow \\Phi \\cup \\phi_i$\n9:    end for\n10:  $ \\rho \\leftarrow \\frac{1}{| \\Phi |} \\sum_{\\theta \\in \\Phi} \\theta$ \u25b7 Merge the retriever embeddings\n11:  $C_{iter} \\leftarrow Acc(\\rho, D_{val})$ \u25b7 Calculate validation accuracy\n12:  $ \\alpha \\leftarrow \\alpha \\cup C_{iter}$\n13: end for\n14: $ \\rho^* \\leftarrow arg \\max_{\\rho} C_{iter}$\n15: $ \\rho \\leftarrow \\min \\mathcal{L}_{DPP}(D^T \\cup D^{aux}; \\rho^*)$\n16: return $ \\rho$\nof Indic languages for our analysis. Specifically, we focus on the following low-resource languages:\nLow/Mid-Resource Languages: Bodo, Odia, Santali, Rajasthani, Manipuri, Awadhi, Marwari, and Maithili.\nAuxiliary High-Resource Languages: Bengali, Hindi, Marathi, Gujarati, Kannada, Malayalam, Tamil, Telugu, Urdu.\nAdditionally, we assume the availability of data from relatively high-resource Indic languages, which we refer to as the auxiliary dataset. This assumption is justified by the fact that these languages have relatively ample web-text resources (Singh et al., 2024)."}, {"title": "4.1 Our Approach: PROMPTREFINE", "content": "To enhance the performance of language models on low-resource languages, we introduce PROMPTREFINE, a three-step framework that: 1) identifies closely related high-resource Indic languages and leverages associated example banks (Section 4.1.1), 2) iteratively refines retriever embeddings $R_{\\phi}$ (Sec-"}, {"title": "4.1.1 Auxiliary Dataset Selection", "content": "Despite the advent of numerous LLMs, low-resource Indic languages constitute only a negligible portion of their pre-training corpora, resulting in a suboptimal performance for generation tasks in these languages. To address this, we propose using relatively high-resource Indic languages, such as Hindi and Bengali, as auxiliary datasets. Our approach involves selecting auxiliary languages that are closely related to the target Indic low-resource language. Specifically, for each low-resource language, we compute the cosine similarity between the embeddings and those of the auxiliary languages. An auxiliary language is included if its similarity score surpasses a threshold parameter \u03b4, as outlined in Algorithm 2 (Appendix H). The underlying motivation for this approach is that for an input query in a low-resource Indic language, relevant examples that can guide the LLM in generation might not be present in the associated example bank - therefore, by incorporating examples from the related high-resource languages, we provide the LLM additional context or rather relevant guidance that can help improve the model's performance on the input query. However, the following critical challenge is now raised: \"How can we effectively integrate the diverse information from auxiliary datasets for optimal performance?\""}, {"title": "4.1.2 Iterative Prompt Refinement", "content": "We denote the example bank of the low-resource target language T as $D^T = \\{(x_i, y_i)\\}_{i=1}^N$, the selected set of auxiliary languages (using Alg. 2) as $H = \\{H_1,\u2026,H_M\\}$ and the auxiliary example banks as $D^{aux} = \\{D^{H_1}, ..., D^{H_M} \\}$. Note that the number of high-resource auxiliary example banks M is determined by the threshold parameter \u03b4, as defined in Section 4.1.1. Our goal is to train a single retriever $R_{\\rho}(x_{test}, D^{aux} \\cup D^T)$ - however the challenge is both the shared representation space for all example banks combined and the parameter weights $ \\phi $ are unknown. At the same time, the retriever must capture the specific traits of each individual language. Balancing these requirements requires an alternating optimization procedure."}, {"title": "4.1.3 Divsersity-induced finetuning", "content": "A major limitation of relevance-based finetuning is that the in-context examples are retrieved solely based on relevance, thereby ignoring diversity and inter-relationship among the selected examples (Ye et al., 2023). To overcome this challenge, we leverage the DPP framework to enhance diversity within the retrieved in-context examples. Specifically, we obtain the final retriever model by fine-tuning $ \\rho^* $ on the merged dataset $ D = D^T \\cup D^{aux} $. Due to the technically involved training procedure of the DPP framework via contrastive learning, we delegate the training details to the Appendix D. Note that during inference, we use the retriever model further fine-tuned in the DPP framework to retrieve both diverse and relevant samples (Ye et al., 2023). Specifically, we perform MAP inference using the model within the DPP framework - however exact MAP inference is NP-Hard (Ko et al., 1995) since"}, {"title": "6 Discussion", "content": "Note, ablation studies on threshold parameter \u03b4 for selecting related languages and number of ICL examples are deferred to Appendix C.\nWhy is choosing a closely related auxiliary example bank important? In Section 4.1.1, we discussed the approach of selecting a closely related high-resource Indic language as an auxiliary example bank for a target low-resource language. To highlight importance of selecting the right set of related languages, we present ablation studies in this section.\nWe evaluate three setups: (1) Our method (Alg. 2) of selecting the most closely related language as the auxiliary example bank, (2) selecting a random language, and (3) selecting the most unrelated language. We show the few-shot performance on cross-lingual QA task in Figure 1, with retrievers trained under each setup. Our findings show that: (1) As expected, using the most closely related language as the auxiliary example bank yields the best performance, as it provides relevant guidance to the LLM, and (2) Selecting a random or unrelated language results in little to no improvement, with performance remaining close to the relevance-based EPR baseline (Rubin et al., 2021) that only uses target-language specific data.\nImportance of Diversity-Induced Fine-Tuning. A key factor contributing to the improved performance of PROMPTREFINE is the retrieval of a diverse set of in-context examples. Merging auxiliary example banks imply a larger overall example bank and therefore incorporating diversity during example selection becomes important to improve generalization. In this section, we empirically demonstrate the significance of diversity-induced fine-tuning (Section 4.1.3). Figure 2 compares the"}, {"title": "7 Conlusion", "content": "The pre-training data for state-of-the-art LLMs (Dubey et al., 2024; Yang et al., 2024) predominantly comprises English text, resulting in suboptimal performance on low-resource Indic languages (Singh et al., 2024). To address this, we propose a novel Alternating Minimization approach PROMPTREFINE for example selection that improves ICL performance on low-resource Indic languages. Our approach follows a three-step framework: (1) identifying closely related high-resource Indic languages and utilizing their example banks, (2) iteratively refining retriever embeddings, and (3) employing diversity-based finetuning to rank subsets of in-context examples for a given input query. Comprehensive testing"}, {"title": "8 Limitations", "content": "In Section 5.1, we demonstrated the effectiveness of our proposed framework across diverse tasks. However, PROMPTREFINE has the following limitations:\n\u2022 To improve text generation quality in low-resource Indic languages, PROMPTREFINE depends on the availability of example banks from closely related, relatively high-resource Indic languages to provide additional context. We believe this is a reasonable assumption, as publicly available web-text resources are accessible for high-resource languages.\n\u2022 As explained in Algorithm 1, our proposed approach involves three key steps: (1) identifying closely related example banks, (2) iterative refinement of retriever embeddings, and (3) diversity-based finetuning. These steps could have been arranged in several different configurations. However, we empirically tested several alternatives and found that our proposed approach consistently delivers the best performance improvements."}, {"title": "A Software and Hardware", "content": "We run all experiments with Python 3.12.4, PyTorch 2.2.0, and Transformers 4.43.3. For all experimentation, we use four Nvidia RTX A6000 GPUs."}, {"title": "B Extended Results", "content": "In the main paper, we showed the effectiveness of PROMPTREFINE for cross-lingual QA (Table 1), machine translation (Table 2), and summarization tasks (Table 3). In Table 4, we further present results on multi-lingual QA task for Odia language. Note that Xquad-In (Singh et al., 2024) only includes medium-resource languages; therefore, we report results for only one language for this task. For a comprehensive evaluation, we also test PROMPTREFINE on proprietary LLMs such as GPT-3.5/4 (OpenAI et al., 2024). We report results in Table 5."}, {"title": "C Extended Discussion", "content": "Ablation Study on Threshold Parameter \u03b4. In this section, we present an ablation study on the threshold parameter d, which governs the selection of high-resource languages in the auxiliary dataset. A lower d expands the inclusion to a broader set of high-resource languages, potentially introducing less relevant examples. On the other hand, a higher d is more selective, which may restrict the diversity of examples and reduce the contextual richness. Our goal is to identify the optimal d that balances these factors, ensuring the auxiliary dataset consists of examples from closely related high-resource languages that effectively improve LLM generation performance. Figure 4 illustrates the translation performance from three different low-resource languages to English as a function of varying d. The results suggest that setting d to the 95th percentile of the cosine similarity values between target language embeddings and high-resource languages yields optimal performance."}, {"title": "D Details on Diversity-induced finetuning", "content": "We introduced DPP-based diversity finetuning in Section 4.1.3. In this section, we provide additional details regarding the training procedure. We obtain the final retriever by fine-tuning $ \\rho^* $ on the merged dataset $ D = D^T \\cup D^{aux} $. Specifically, for each sample (xi, yi) \u2208 D, a subset of E\u00bf in-context examples are retrieved from D. Out of the E\u00bf subsets,\nmax{0, log det ( (ZE ) \u22121)\n\u2212\u2212log det (Z (E) )} , (4)\nLDPP = Ni=1 \u2211Eli\nli, (5)"}]}