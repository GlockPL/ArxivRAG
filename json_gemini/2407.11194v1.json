{"title": "C. CLASSIFICATION OF ASTROPHYSICS QUESTIONS", "authors": ["YUAN-SEN TING", "TUAN DUNG NGUYEN", "TIRTHANKAR GHOSAL", "RUI PAN", "HARDIK ARORA", "ZECHANG SUN", "TIJMEN DE HAAN", "NESAR RAMACHANDRA", "AZTON WELLS", "SANDEEP MADIREDDY", "ALBERTO ACCOMAZZI"], "abstract": "We present a comprehensive evaluation of proprietary and open-weights large language models us-ing the first astronomy-specific benchmarking dataset. This dataset comprises 4,425 multiple-choicequestions curated from the Annual Review of Astronomy and Astrophysics, covering a broad range ofastrophysical topics.a) Our analysis examines model performance across various astronomical subfieldsand assesses response calibration, crucial for potential deployment in research environments. Claude-3.5-Sonnet outperforms competitors by up to 4.6 percentage points, achieving 85.0% accuracy. Forproprietary models, we observed a universal reduction in cost every 3-to-12 months to achieve simi-lar score in this particular astronomy benchmark. open-weights models have rapidly improved, withLLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with some of the best proprietarymodels. We identify performance variations across topics, with non-English-focused models generallystruggling more in exoplanet-related fields, stellar astrophysics, and instrumentation related questions.These challenges likely stem from less abundant training data, limited historical context, and rapidrecent developments in these areas. This pattern is observed across both open-weights and propri-etary models, with regional dependencies evident, highlighting the impact of training data diversityon model performance in specialized scientific domains. Top-performing models demonstrate well-calibrated confidence, with correlations above 0.9 between confidence and correctness, though theytend to be slightly underconfident. The development for fast, low-cost inference of open-weights mod-els presents new opportunities for affordable deployment in astronomy. The rapid progress observedsuggests that LLM-driven research in astronomy may become feasible in the near future.", "sections": [{"title": "1. INTRODUCTION", "content": "The emergence of the GPT (Generative Pre-trainedTransformers) model series has thrust large language"}, {"title": "2. BENCHMARKING MCQ DATASETS", "content": "As part of the initiative in collaboration with the Ar-gonne National Laboratory, we have developed detailedastronomical benchmarking datasets comprising bothQ&A and MCQ components. These datasets are specif-ically designed to evaluate the performance of LLMs inthe context of astronomical research, a topic we will ex-plore in greater detail in our subsequent paper. Ourbenchmark not only tests astronomical facts and consen-sus from the research community but also assesses mod-els' capabilities in linking insights across diverse sub-fields and understanding the interdisciplinary nature ofastronomical research.\nTraditionally, creating such datasets has been hin-dered by the high cost of human annotation, particu-larly in specialized scientific domains. However, astro-physics and astronomy benefit from a long-standing tra-dition of world-leading experts summarizing the state ofthe field. The Annual Review of Astronomy and As-trophysics stands out as an invaluable resource in thisregard. Established in 1963, this review journal, with animpact factor of 33.3 in 2023, publishes approximatelyten articles annually. Reviews are commissioned by in-vitation from a panel of senior and prominent mem-bers of the editorial committee. The highly selectivenature of the journal ensures that each article providesan overview of cutting-edge research in a specific sub-field of astronomy, typically spanning an average of 40pages and 15,000 words. This approach precludes anymyopic views on particular topics, and the contributingauthors are widely recognized as world leaders in theirrespective fields.\nThe extensive length of these reviews initially posedchallenges for models with shorter context windows.However, recent advancements in long-context LLMshave made it possible to extract quality MCQs fromthese reviews. When this study began, Gemini-1.5-Pro (Team Gemini et al. 2023) offered the longest con-text window of one million tokens, and was widely avail-able via Google's Generative AI API. While Gemini-1.5-Pro is not the most performant in terms of offlineastronomical Q&A, we deemed it sufficient for generat-ing this dataset because it can digest entire articles in itscontext. We employed extensive prompt engineering toensure the quality of the MCQs, which will be summa-rized below and described in more detail in the secondpaper of this series.\nTo create this Q&A dataset, we started with collecting885 articles in ARAA, dating from 1963 to 2023. Then,we used the Nougat optical character recognition (OCR)tool (Blecher et al. 2023) to transcribe these papers intotext. We fed each paper into Gemini-1.5-Pro and in-"}, {"title": "3. INFERENCE METHODOLOGY", "content": "For this study, we focus exclusively on the MCQbenchmarks, reserving more detailed evaluations of as-tronomical research capabilities, such as open-endedQ&A, which were also curated as part of our effort, forfuture work. The latter may require more careful hu-"}, {"title": "4. BENCHMARKING PROPRIETARY LARGE\nLANGUAGE MODELS", "content": "We begin by presenting the performance of propri-etary models. The accuracy metric used throughoutis defined as the percentage of questions answered cor-rectly. Given our dataset of 4,425 questions, the typicalnoise, evaluated with the Wilson Score Interval in ourevaluation is small (1\u03c3 range is about \u00b10.6 \u2013 0.8%).\nThe Wilson Score Interval (Wilson 1927) provides aconfidence interval for binomial proportions that out-performs the normal approximation, particularly for ex-treme probabilities and small sample sizes. Given a sam-ple of size n with k successes and sample proportion\\(p = k/n\\), the Wilson score interval for the true popula-"}, {"title": "4.1. Overall Performance Comparison Across Model\nSeries", "content": "We first study the overall performance of the mod-els over the entire 4,425 questions. The histogram inFig.1 provides an overview of the results. The left panelgroups models by series, with darker shades indicatingmore recent or larger models within each series. Theright panel shows the same scores sorted by overall per-formance, regardless of model series. These results aresummarized in Table 1 (the corresponding results foropen-weights model, which we will discuss in Section 5,can be found in Table 2). The overall results range fromDoubao-Lite at 60.5% to Claude-3.5-Sonnet at 85.0%.These results demonstrate that while the MCQ ques-tions are challenging (see example questions in Table 1,Appendiex C and D), they are manageable for LLMs,with overall performance roughly aligning with each"}, {"title": "4.2. Cost Efficiency Consideration", "content": "While proprietary models vary in ability, a key consid-eration for deploying LLM agents is cost-effectiveness.In the following analysis, we consider the pricing of thesemodels as of June 2024, when this draft is being written.We focus on the cost per 0.1M (0.1 million) tokens asa basic unit for the cost. This metric is chosen basedon a companion work (Sun et al., in prep.) where wedeploy LLM agents for astronomical research. We findthat a multi-turn conversation of 0.1M tokens is typi-cally needed for reasoning about individual astronomi-cal sources. This number multiplies with the number ofsources for such studies, providing an interesting rule ofthumb for management considerations in future deploy-ments of LLMs for active astronomical research.\nFor many proprietary models, the input and outputprices differ. Deploying LLMs often requires bothextensive input tokens (e.g., for retrieval-augmented gen-eration in the context of custom generative models forvarious coding APIs) and output tokens for reason-ing, multi-turn conversations, and collaborative agentefforts. Given these considerations, we will use the aver-age of the input and output token prices in our analysis,assuming they play an equal role. As a reference point,GPT-40, perhaps the most well-known LLM, costs ex-actly 1 US dollar per 0.1M tokens, which further in-formed our decision to use this as our benchmarkingunit for the cost.\nFig. 2 shows the score of various representative pro-prietary LLMs plotted against their cost. For a com-plete list of scores for all models tested, refer to Table 1.First, within a given model series, we observe a univer-sal scaling across different model series, which providesan interesting insight into the trade-off between perfor-mance and cost. Our analysis reveals that most propri-etary models, within a fixed series at a given publica-tion time, follow a universal trade-off of approximately10 times cost increase per 3.5-point score improvement.These are shown in dashed lines that link the models"}, {"title": "4.3. How Fast is the Cost Efficiency Improving for\nAstronomical Tasks?", "content": "A key consideration for deploying LLM agents for var-ious astronomical tasks, apart from their overall rea-soning ability and robust knowledge recall and summa-rization, comes down to cost considerations. For in-stance, while GLM-4-Flash can process the entire as-tronomy arXiv for less than USD 42 (as of June 2024),it's the accuracy of the model that might make themost difference. This is particularly important for sci-"}, {"title": "4.4. Why Are the Weaker Proprietary Models Weaker?", "content": "While the overall accuracy metric provides a com-prehensive view of each model's ability in astronomi-cal knowledge recall, dissecting the differences betweenmodels in detail can lead to insights about the majortrade-offs when opting for a cheaper model or modelsthat appear to be slightly over-tuned for other met-rics, resulting in performance decreases in this particularbenchmarking.\nTo this end, for individual questions, we relied onGPT-40 to perform two classifications. For the first clas-sification, we categorized the questions by topics, follow-ing the different subclasses in astro-ph articles: (1) Solarand Stellar Astrophysics, (2) Earth and Planetary As-trophysics, (3) Astrophysics of Galaxies, (4) Cosmologyand Nongalactic Astrophysics, (5) High Energy Astro-physics, and (6) Instrumentation and Methods for As-trophysics. For the second evaluation, we asked GPT-40 to further classify questions into the following groupsbased on the different abilities being tested: (1) Un-derstanding Fundamental Concepts, (2) Technical andObservational Techniques, (3) Analytical and ReasoningSkills, (4) Historical and Theoretical Knowledge, and (5)Current Research and Advanced Topics."}, {"title": "5. BENCHMARKING OPEN-WEIGHTS LARGE\nLANGUAGE MODELS", "content": "While proprietary models have been advancing at aremarkable pace, relying on API calls to perform large-scale astronomical research may remain cost-prohibitiveor challenging to justify to survey management. open-weights models are critical for LLM deployment in as-tronomical research for two primary reasons: (1) In aca-demic settings, it is often easier to secure GPU computeresources than to obtain grants, especially as the deploy-ment of proprietary LLM APIs is not yet widely adoptedin astronomical research. (2) open-weights models of-fer the possibility of further continual pretraining orspecialized fine-tuning, potentially allowing for better"}, {"title": "5.1. Comparative Analysis of Leading Open-Weights\nModel Series", "content": "In this section, we present a comprehensive evalua-tion of prominent open-weights models, ranging fromsmaller-scale 2B parameter models to larger 176B pa-rameter models. Our analysis aims to trace the evolu-tion of these open-weights models from our astronomicalperspective. We focus on several representative modelseries:"}, {"title": "5.2. Analyzing Performance Discrepancies in\nOpen-Weights Models", "content": "Following the classification scheme described in Fig.4and 5, we assess why some open-weights models per-form weaker than others. Our analysis is divided intotwo parts, illustrated in Fig. 8 and 9. Fig.8 focuses onthe accuracy across different subfield topics, while Fig.9examines the abilities tested, mirroring the approach inFig.4 and 5. For reproducibility, all detailed accuraciesfor each question class are summarized in Tables 5 and6.\nThe left panels of Fig. 8 and 9 present a compar-ison between the best-performing proprietary model(Claude-3.5-Sonnet) and top-performing open-weightsmodels (LLaMA-3-70B, Mixtral-8x22B-v0.1, and Phi-3-14B). Claude-3.5-Sonnet demonstrates consistently highperformance across all topics and abilities, with scoresranging from 77.8% to 89.4%. In contrast, even thebest open-weights models show some variability, partic-ularly in more recent or rapidly evolving fields. Notably,these open-weights models generally perform weaker inEarth and Planetary Astrophysics, Solar and Stellar As-trophysics, and Instrumentation and Methods for Astro-physics.\nThis pattern echoes our previous observations andsuggests that these topics, being more recent (exoplan-ets) or requiring more historical context (stellar astro-physics, instrumentations and methods), might have lessrepresentation in the training data. For instance: (1)LLaMA-3-70B shows lower scores in Earth and Plane-tary Astrophysics (79.1%) and Solar and Stellar Astro-physics (78.7%) compared to its performance in otherareas. (2) Mixtral-8x22B-v0.1, demonstrates relativelyweaker performance in all these three areas with scores75.7% to 77.6% compared to the other topics. (3)Phi-3-14B shows the weakest performance in Earth andPlanetary Astrophysics (74.0%). (4) Gemma-2-27B alsoshows the weakest performance in Earth and PlanetaryAstrophysics (73.3%) and Instrumentations and Meth-ods (73.4%).\nThe left panel of Fig. 9 further reinforces this trendwhen examining the abilities tested. Open-weightsmodels generally show more decrements in \"Histori-cal and Theoretical Knowledge\" and \"Current Researchand Advanced Topics\" compared to Claude-3.5-Sonnet.This discrepancy is particularly evident in Phi-3-14B on\"Current Research and Advanced Topics\" (hence theweak performance\" and Mixtral-8x22B-v0.1 on \"Cur-rent Research and Advanced Topics\". For instance, Phi-"}, {"title": "6. DISCUSSION", "content": "This study presents a comprehensive benchmark ofproprietary and open-weights LLMs against a high-quality MCQ dataset extracted from the Annual Re-view of Astronomy and Astrophysics. As one of the"}, {"title": "6.1. Limitation and rationale behind this benchmarking\ndataset", "content": "A key question arising from this benchmarking iswhether it meaningfully assesses the potential for de-ploying these models as LLM agents in astronomical re-search. We emphasize that our Annual Review-drivenMCQ dataset is primarily designed to test nuanced un-derstanding within the specialized astronomical researchcommunity. This focus is significant given the relativelysmall footprint of astronomical literature in typical LLMtraining sets. For context, the entire arXiv astro-ph ar-ticle corpus comprises less than 3B words, which, evenwhen expanded to include older papers from the Astro-physics Data System, represents only about 0.025 per-cent of the training set for models like LLaMA-3. Whileother astronomy-related texts exist (e.g., press releases),"}, {"title": "6.2. Model Self-Awareness: Are You Sure?", "content": "Given that even the best-performing models achieveonly about 85% accuracy (Claude-3.5-Sonnet correctlyanswering 5 out of 6 questions), it's crucial to assesswhether models can gauge their own uncertainty. Thisimplicit uncertainty calibration is critical for scientificresearch, as hallucination or overconfidence is unaccept-able in the scientific community.\nWe tested the models' ability to gauge their confidenceby prompting them to output probabilities for individualanswers. And the confidence is defined as the maximumpprobability over the four possible answers divided bythe sum of the four probabilities. Fig. 10 shows thenumber of correct answers as a function of the model'sconfidence for various proprietary models. We binnedthe confidence with a bin size of 0.1 from 0.4 to 1 andcalculated the fraction of correct answers within eachbin. The shaded band in the figure indicates the WilsonScore Interval, which we calculated based on the numberof correct answers versus the total questions in each binfor each model. Ideally, perfect calibration would followa 1:1 line, where the fraction of correct answers in anybin matches the model's confidence.\nWe note that although we could gauge the confidencethrough the logit of the output of the first token i.e.,the probability of A, B, C, and D, according to the nexttoken prediction after the question. However, as we havediscussed, we chose not to do that for two reasons: (a)weaker models tend to not be able to follow the instruc-tion exactly, using the logit will bias against them; (b)more importantly, in most agent deployment scenarios,next token logit is often meaningless in real-life applica-tion. Being able to ask, through prompting, how confi-dent the model is as a question matches real-life appli-cation more closely.\nHowever, not all models perform equally well in thisaspect. Some models, particularly smaller ones or ear-lier versions, show poorer calibration. We consider twometrics of calibration as shown in the two columns ofTable 7: (1) We perform a weighted linear regression onthe fraction of correct answers versus confidence. This'correlation' indicates whether a model has good self-"}, {"title": "6.3. Assessing Potential Data Leakage and Temporal\nPerformance", "content": "A critical concern in any benchmarking exercise is thepotential for data leakage, where models may performartificially well due to exposure to test data during train-ing. This has been observed in previous benchmarks(Huang et al. 2023), leading to the development of newdatasets like MMLU-pro (Wang et al. 2024) and the useof exam questions released after model training to en-sure fair evaluation.\nEvaluating data leakage in our astronomical bench-mark presents unique challenges because our aim to testmodels on factual knowledge and the historical evolutionof astronomical literature inherently requires the use ofhistorical data. Nonetheless, our questions are gener-ated from the Annual Review of Astronomy and Astro-physics, not directly copied in verbatim from existingliterature, which should have, to certain extent, miti-gate this issue. To address these challenges and assesspotential data leakage, we analyzed model performanceas a function of the publication year of the source AnnualReview articles. If models were simply recalling mem-orized knowledge from training data, we would expectto see a significant drop in performance for questions"}, {"title": "6.4. Balancing Performance and Affordability in LLM\nDeployment", "content": "A key objective of our comprehensive benchmarking isto gauge the performance-efficiency trade-off, enablingefficient and affordable deployment of LLM agents forlarge-scale tasks previously deemed unfeasible. Whilethe affordability threshold varies significantly by task,we can consider a concrete example to illustrate the chal-lenges.\nIn a companion study (Sun et al., in prep.), we demon-strated the use of LLM agents working collaboratively tounderstand the spectral energy distribution of galaxies.This study showed that, on average, the entire reasoningprocess requires about 0.1 million tokens, with GPT-40-level capabilities being necessary for robust reasoningand instruction following. For GPT-40, this translatesto approximately 1 USD per astronomical source. Con-sidering cutting-edge space surveys like Euclid and Roman,which aim to observe on the order of a billionsources, the inference cost could reach 1B USD (assumming no rate limitations). This cost is comparableto the construction cost of these telescopes. Realisti-cally, an inference cost of 1-10% of the build cost (1-2orders of magnitude lower) would be more feasible forsuch projects.\nThe landscape for open-weights models presents itsown challenges. Assuming GPT-40 or above capabil-ity is necessary (which is task-dependent), the current70B parameter models and larger can achieve compara-ble performance a remarkable feat in itself. However,"}, {"title": "7. CONCLUSION", "content": "In this study, we present the first comprehensivebenchmarking of proprietary and open-weights modelsreleased in the last 18 months since ChatGPT, focus-ing on their ability to handle doctoral-level astronomyquestions. Our benchmarking dataset consists of 4,425multiple choice questions derived from high-quality re-view articles in the Annual Review of Astronomy andAstrophysics. This evaluation is critical for understand-ing the robustness of current LLMs for deployment asresearch agents in astronomy.\nOur main conclusions regarding proprietary mod-els (including GPT, Claude, Gemini, GLM, DeepSeek,Step, Yi, Moonshot, ERNIE, ABAB, and Doubao) areas follows:\n1. While some cutting-edge proprietary models haveshown comparable reasoning abilities in otherbenchmarks, their performance can vary drasti-cally in specialized domains like astronomical re-search.\n2. Claude-3.5-Sonnet demonstrates the highest per-formance in our benchmark, achieving 85.0% ac-curacy and outperforming GPT-40 (80.4%) by4.6 percentage points. Notably, Gemini-1.5-Pro(77.6%) lags behind Claude-3.5-Sonnet by 7.4 per-centage points in accuracy. Other top perform-ers include Claude-3.0-Opus (82.7%) and GPT-40(80.4%).\n3. We observe a general trade-off of 3.5 percentagepoints in accuracy for every 10-fold increase inprice within most given series of models. Thissubstantial discrepancy between high-end modelscan result in three orders of magnitude differencein performance-to-cost ratio. This demonstratesthat general benchmarking results do not necessar-ily translate into astronomy-specific performance,highlighting the importance of such specializedbenchmarking studies.\n4. Proprietary models show a strong dependencyon the training data. For instance, nonEnglish-focused models like GLM-4-0520 (75.1%),Yi-Large (77.3%), Doubao-Pro (70.1%) andDeepseek-v2 (73.6%), while proficient in otherbenchmarks, underperform in this scientific bench-mark, particularly in areas like Historical and"}, {"title": "APPENDIX", "content": "A. INFERENCE METHODOLOGY DETAILS\nIn our study, we used the following format for prompting the models to answer the multiple-choice questions:\nPrompt:\nYou are an expert in general astrophysics. Your task is to answer and explain the following multiple-choice\nquestion on astrophysics, sourced from a dataset. The question is:\nQuestion: [Question text]\nOptions:\nA: [Option A]\nB: [Option B]\nC: [Option C]\nD: [Option D]\nDetermine the correct answer using your astrophysics knowledge and provide a detailed explanation for why\nthis answer is correct.\nEnsure your explanation is thorough, clearly articulating your thought process based on astrophysical principles.\nOutput format:\n{\n    \u201cANSWER\u201d: \u201c[The choice you decide to choose]\u201d,\n    \u201cEXPLANATION\": \"[Provide a valid explanation for the answer mentioned in ANSWER]"}, "nGive only one answer, either A, B, C or D, but not more than one, and always give an answer.\nAdhere to the output format.\nWhen the system allowed for a separate system prompt, we used the first sentence (\"You are an expert in general\nastrophysics.\") as the system prompt.\nOur experiments showed that the choice of sampling parameters (e.g., temperature) played a minor role in the\nperformance of the models, especially for more recent and robust models. The key elements that consistently improved\nperformance across models were: (1) The role-play framing the task as coming from an expert in astrophysics. (2)\nRequesting both an answer and an explanation, encouraging a chain of thought inference. (3) Providing a clear output\nformat. Although the third point was not often adhered to, which required us to further parse the results with GPT-\n40. This observation aligns with the broader trend in LLM research, where more advanced models show increasing\nrobustness to prompt variations, as long as the essential task information is conveyed.\nB. PERFORMANCE ANALYSIS ON POST-1990 QUESTIONS\nAs we have demonstrated in the right panel Fig. 11 (also see Fig. 4, Fig. 5, Fig. 8, Fig. 9), some models perform\nworse than their competitors due to differences in training data, particularly in questions related to historical context.\nThese differences might have less relevance to the models' ability to execute tasks as LLM agents in astronomical\nresearch. Additionally, some questions extracted from Annual Reviews articles dating back almost half a century ago\nmay contain outdated information or reflect outdated consensus, potentially affecting the validity of the answers."], "content": "In this study, as shown in Table 3 and 4 we categorized the questions according to the six main subclasses used in\nthe arXiv's astro-ph section. This classification system reflects the broad areas of research within astrophysics: (1)\nSolar and Stellar Astrophysics, (2) Earth and Planetary Astrophysics, (3) Astrophysics of Galaxies, (4) Cosmology\nand Nongalactic Astrophysics, (5) High Energy Astrophysics, and (6) Instrumentation and Methods for Astrophysics.\nThis categorization allows for a more nuanced analysis of model performance across different specializations. We'll\nprovide two examples for each class to showcase the types of questions that appear in each. The full benchmarking\ndataset will be released in our upcoming paper.\nC.1. Solar and Stellar Astrophysics\nThis category focuses on the physics of stars, including our Sun, their formation, evolution, and internal processes.\nPaper ID: 1971ARA&A...9..127S\nQuestion: What is the most likely range of initial masses for stars that become planetary nebulae?\n(A) 0.5 to 1.0 solar masses\n(B) 1.0 to 1.5 solar masses\n(C) 1.5 to 2.0 solar masses\n(D) 2.0 to 2.5 solar masses\nCorrect Answer: B\nExplanation: The article discusses the spatial and velocity distribution of planetary nebulae, suggesting they\noriginate from stars with masses similar to those in the disk population. This population typically has stars\nwith masses between 1.0 and 1.5 solar masses. Additionally, the article mentions that the rate of planetary\nnebula formation aligns with the rate of white dwarf creation, further supporting this mass range as white\ndwarfs usually originate from stars within this mass range.\nPaper ID: 2003ARA&A..41..465\u0397\nQuestion: How do the masses of cool white dwarfs compare to those of hot white dwarfs, and what are the\npotential implications?\n(A) Cool white dwarfs generally have lower masses, suggesting that they originate from lower-mass progenitor\nstars.\n(B) Cool white dwarfs have a similar mass distribution to hot white dwarfs, indicating a consistent mass range\nfor white dwarf progenitors.\n(C) Cool white dwarfs tend to have higher masses, possibly due to their older ages and origin from higher-mass\nprogenitors.\n(D) The mass distribution of cool white dwarfs is bimodal, with distinct populations of low-mass and high-mass\nwhite dwarfs.\nCorrect Answer: C\nExplanation: The article presents evidence that cool white dwarfs exhibit a slightly higher mean mass com-"}