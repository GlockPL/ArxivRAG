{"title": "A Multimodal Object-level Contrast Learning Method for Cancer Survival Risk Prediction", "authors": ["Zekang Yang", "Hong Liu", "Xiangdong Wang"], "abstract": "Computer-aided cancer survival risk prediction plays an important role in the timely treatment of patients. This is a challenging weakly supervised ordinal regression task associated with multiple clinical factors involved such as pathological images, genomic data and etc. In this paper, we propose a new training method, multimodal object-level contrast learning, for cancer survival risk prediction. First, we construct contrast learning pairs based on the survival risk relationship among the samples in the training sample set. Then we introduce the object-level contrast learning method to train the survival risk predictor. We further extend it to the multimodal scenario by applying cross-modal constrast. Considering the heterogeneity of pathological images and genomics data, we construct a multimodal survival risk predictor employing attention-based and self-normalizing based nerural network respectively. Finally, the survival risk predictor trained by our proposed method outperforms state-of-the-art methods on two public multimodal cancer datasets for survival risk prediction. The code is available at https://github.com/yang-ze-kang/MOC.", "sections": [{"title": "1 Introduction", "content": "Cancer survival risk prediction refers to using computer technology to mine biomarkers from the patients' clinical data, such as gene expression, gene mutation or pathological images to estimate patients' survival risk. The more accurate of the estimation, the more timely and personalized treatment patients can get.\nIn survival analysis, one important issue that needs to be considered is censoring problem, which means subjects are censored if they are not followed up or the study ends before they die or have an outcome of interest. It's challenging for effectively tackling censoring problem, which also can be treated as a challenging weakly supervised and ordinal regression task.\nDeep learning has achieved great success in many fields in recent years, and there has been emerging a lot of research using deep learning methods for survival prediction. Methodologically, the core idea of most deep survival networks is optimizing the loss function derived from the Cox proportional hazards model (Cox-based), alternatively dividing the time into serval intervals with fixed boundaries (Interval-based). Both of these methods foucus on the specification of an appropriate optimization criterion for deep survival network training. Cox-based methods [1, 6, 14, 23] mainly optimize the negative log partial likelihood, which promotes overall concordance by penalizing all discordance prediction. Interval-based methods [3-5, 25, 33] consider discrete time intervals and model each interval using an independent output neuron, then can use the cross entropy loss function of the classification problem to optimize the network.\nHowever, both of these methods has its own shortcomings. In the training phase, for each sample, Cox-based methods need to know the risk prediction of the samples whose ground-truth survival time is longer than it and give them penality. This makes it difficult to apply deep learning methods on histological images which require a unaffordable GPU memory footprint to apply the technology of batch size. Interval-based methods convert continuous into discrete time intervals and model each interval using an independent output neuron. This formulation overcomes the need for batched samples. However, the division of time intervals depends on the distribution of real survival risks which is often unknowable. And their consideration of concordance is coarse-grained, which degrade the performance of models for predicting fine-grained cocordance. Simultaneously, in recent years, mining the relationship between pathological images and genomics data and utilizing multimodal"}, {"title": "2 Related Work", "content": "Surivival Analysis Methods. Cox [6] first proposed Cox proportion hazard model to handle censoring problem, in which hazard is parameterized as an exponential linear function. Katzman et al. [14] proposed the negative log partial likelihood, allowing a deep architecture and apply modern deep learning techniques [13, 15, 26] to optimize. Qiu et al. [23] also utilized the negative log partial likelihood to optimize their proposed multimodal model, in which they integrated pathological images and genomic data to improve survival prediction. Steck et al. [27] showed that maximizing Cox's partial likelihood can be understood as maximizing a lower bound on the concordance index and proposed an exponential lower bound that can achieve better performance. Ren et al. [25] turned the survival time into discrete time and trained using the cross-entropy loss function. Zadeh et al. [33] showed that replacing the cross-entropy loss by the negative log-likelihood loss (NLL) results in much better calibrated prediction rules and also in an improved discriminatory power. Many recent works [2-5] also adopted negative log-likelihood loss as their objective function.\nMultimodal Cancer Survival Risk Prediction Methods. Integrating genomics and histology data to develop joint image-omic prognostic models is promising [3-5, 23]. Chen et al. [4] proposed a Multimodal Co-Attention Transformer (MCAT) method that learns an interpretable, dense co-attention mapping between WSIs and genomic features formulated in an embedding space and achieved state-of-the-art performance. Chen et al. [5] proposed a deep-learning-based multimodal fusion (PORPOISE) algorithm that uses both H&E WSIs and molecular profile features (mutation status, copy-number variation, RNA sequencing expression) to measure and explain relative risk of cancer death. Qiu et al. [23] proposed a novel biological pathway informed pathology-genomic deep model (PONET) that integrates pathological images and genomic data not only to improve survival prediction but also to identify genes and pathways that cause different survival rates in patients.\nContrast Learning. Contrastive learning [8] is an unsupervised pre-training method that learns similar/dissimilar representations from data that are organized into similar/dissimilar pairs. These methods [9, 11, 20, 22, 30] mainly calculate contrast objective function at feature-level to learn good representations under unsupervised training. In this paper, we adapt it to supervised training through object-level contrast."}, {"title": "3 METHODOLOGY", "content": "In this paper, we construct a multimodal survival risk predictor and propose a multimodal object-level constrast learning method, the overall architecture of our proposed method is illustrated in Figure 1. In the training phase, we first construct contrast pairs based on the grount-truth relative risk relationship among patients in the training set. Then, the survival risk predictions of the each modality is obtained through the multi-modal survival risk predictor, and the survival risk predictor is updated through the cross-modal object-level contrast learning objective. In the inference stage, the patient's multimodal data is passed through the survival risk predictor to obtain the patient's survival risk predictions for each modality, and finally the patient's final prediction is obtained through decision-level fusion."}, {"title": "3.1 Problem Formulation", "content": "Consider a set of N samples, si represent the i-th sample. We have a tuple {Pai, Gei, di, oi, ci } for sample si, where Pai represents sample's pathological image, Ge\u00a1 represents sample's genomics data, ci represent the censoring status (1 for uncensored and 0 for censored), di represent the time elapsed between diagnosis and death and oi represent the time elapsed between diagnosis and last follow up. Then we can define the ti that is either equal to di (ci = 0) or Oi (ci = 1). For sample si, we now have a tuple {Pai, Gei, ti, ci}, and we want to obtain a survival risk predictor h(x, y) to predict a survival risk ri = h(Pai, Gei). A good survival risk predictor should precisely predict the concordance of survival risk between samples,"}, {"title": "3.2 Construct Contrast Learning Pairs", "content": "In the training sample sets, each sample will search for its learning partners, and each pair of learning partners constitutes a contrast learning pair. For each uncensored sample si in the training sample set, si's partners can be every uncensored patients who have survived longer or shorter than si. For each censored patient sj in the patient set, sj's learning partners can only be uncensored patients who have survived for a shorter period of time than sj's censored time, because the relative risks between such pairs of patients are clear. In a contrast learning pair, for the patients si and sj, if ti < tj, si is regarded as a reference with higher risk (end point of dotted arrow in Figure 1), and sj is regarded as a reference with lower risk (start point of dotted arrow in Figure 1)."}, {"title": "3.3 Multimodal Survival Risk Predictor", "content": "We construct a survival risk predictor for each modality respectively. In the training phase, the cross-modal object-level contrast learning is used for training survival risk predictor, and in the inference phase, decision-level fusion is used to obtain the final prediction. The survival risk predictor for each modality and decision-level fusion are described below.\nPathological images. Pathological images are often gigapixel images and lack fine-grained annotations, making it difficult to directly apply deep learning methods. It is usually considered as a weakly supervised learning problem and multiple instance learning methods are used to assist diagnosis. First, we tile all pathological image Pai into nonoverlapping small patches {spi,1, Spi,2, ..., Spi,n; }, ni is the number of patches cut out of Pai. Then, we use a pretrained ResNet50 [10] to extract all patches' features pfi,j. We use attention-based multiple instance learning (AMIL) [12, 24, 31, 32] to assign a dynamic weight ai,j to each patch-level feature and aggregate them into pathological-level features wfi:\n$w f_{i}=\\sum_{j=1}^{n_{i}} \\alpha_{i, j} p f_{i, j}$\n$\\alpha_{i, j}=\\frac{\\exp \\left(w^{T} \\left(\\tanh \\left(V p f_{i j}\\right) \\odot \\sigma\\left(U p f_{i, j}\\right)\\right)\\right)}{\\sum_{j=1}^{n_{i}} \\exp \\left(w^{T} \\left(\\tanh \\left(V p f_{i, j}\\right) \\odot \\sigma\\left(U p f_{i, j}\\right)\\right)\\right)}$\nThe weight of each patch feature ai,j is parameterized by U and V, which can be learned through backpropagation. We use pathological-level features as the input of MLP for survival risk prediction, and use the output of the last layer of MLP with a sigmoid activation function to obtain survival risk prediction pi.\nGenomics. Genomics have hundreds to thousands of features with relatively few training samples, traditional artificial neural networks are prone to overfitting. Self-Normalizing Network (SNN) [16] employ more robust regularization techniques on high-dimensional low sample size genomics data and has been demonstrated to be effective by [3-5, 16]. We first filter out molecular features related to cancer biology, including transcription factors, tumor suppression, cytokines and growth factors, cell differentiation markers,"}, {"title": "3.4 Multimodal Object-level Contrast Learning", "content": "The core idea of our method is to use contrast learning pairs to train a precise survival risk predictor. The contrast learning method we proposed takes the previously constructed contrast learning pairs as the input. The model maintains three risk predictors PA, PB, and PZ simultaneously. The initial parameters of PA and PB are copied from the parameters of Pz. In the training phase, samples in contrast learning pair get their risk predictions from PA and PB respectively and update PA and PB by referring to each other's prediction.\nObject-level contrast learning. In a forward iteration, the lower risk sample and the higher risk sample get their predicted survival risk value ra and r\u00df by PA and PB, respectively. The object-level contrast learning objective we propose has the following form:\n$\\min \\operatorname{loss}=\\frac{r_{A}}{r_{B}}$\nThen the gradient of the output neurons of PA and PB has:\n$\\frac{\\partial \\text { loss }}{\\partial r_{A}}=\\frac{1}{r_{B}}$\n$\\frac{\\partial \\text { loss }}{\\partial r_{B}}=-\\frac{r_{A}}{r_{B}^{2}}$\n$\\frac{\\partial \\text { loss }}{\\partial p_{A}}=\\frac{\\partial \\text { loss }}{\\partial r_{A}} r_{A}\\left(1-r_{A}\\right)$\n$\\frac{\\partial \\text { loss }}{\\partial p_{B}}=\\frac{\\partial \\text { loss }}{\\partial r_{B}} r_{B}\\left(1-r_{B}\\right)$\nThe term on the left indicates that the two predictors have gradients of the same scale in different directions, similar to the Cox partial likelihood method. The right term is an term specific to our method. When ra is bigger than r\u00df, the risk relationship for this sample pair is mispredicted, with 4 greater than one having a larger gradient, and (1 - ra) less than (1 - r) resulting in a larger gradient for uncensored higher risk sample. On the contrary, when the risk relationship of the sample pair is correctly predicted, lower risk sample who's ground truth risk value may be smaller than ra will have a larger gradient.\nMultimodal object-level contrast learning. We further extend our method to multimodal scenarios. In a forward step, each sample in the contrast learning sample pair use the survival risk predictor to obtain prediction PA and PB based on pathological images and prediction ga and g\u00df based on genes. Then we have the cross-modal object-level contrast learning objective:\n$\\min \\text { loss }=\\frac{p_{A}}{p_{B}}+\\frac{g_{A}}{g_{B}}+\\frac{p_{A}}{g_{B}}+\\frac{g_{A}}{p_{B}}+\\frac{p_{A}}{p_{A}+g_{A}}+\\frac{p_{B}}{p_{B}+g_{B}}$\nThis simultaneously utilizes the mutual inspiration from intra-modal and inter-modal of contrast learning pairs to train the multimodal survival risk predictor."}, {"title": "4 Experiments and Results", "content": "The Cancer Genome Atlas (TCGA) is a public cancer data consortium that contains matched diagnostic pathological images and genomic data with labeled survival times and censorship statuses. We use the commonly used Lung Adenocarcinoma (LUAD) and Kidney Renal Clear Cell Carcinoma (KIRC) to validate the effectiveness our method. For each patient, we select the primary tumor samples from the same center, filter out the samples without complete survival time or censor time, and finally get 957 patients with both pathological images and RNA-Seq data, including 451 patients with LUAD and 506 patients with KIRC. We totally get 972 WSIs. All pathological images are cut into non-overlapping 256x256 patches at a 20x magnification, and patches with large non-tissue areas are deleted. Finally, each pathological image have an average of 13173 patches. We train our proposed method in a 5-fold cross-validation,\nand used the concordance index (C-index) [29] to measure the predictive performance on two datasets, respectively."}, {"title": "4.2 Implementation Details", "content": "We have two hidden layers of genomic data, each with a neuron count of 256. For pathological data, there are also two hidden layers, and the number of neurons is 512 and 256 respectively. The Adam [15] optimizer is used to optimize the network parameters with a fixed learning rate of 0.0002. Due to the number of patches of each WSI is always inconsistent, we set batch size to 1, and adopt gradient accumulation to achieve the same effect, backward propagation is carried out every 128 times of forward propagation. We employ dropout [26] technology to mitigate overfitting, and set the dropout rate at 0.25. Random seed value is fixed to 1. All experiments are done on Nvidia 1080 GPUs. A 5-fold cross-validation was used on all datasets for all models, and the mean C-index of the validation set in five-fold are used for comparison."}, {"title": "4.3 Results", "content": "Compare with State-of-the-art. For comparison of other works, we implement and compare with several state-of-the-art methods used for survival risk prediction. Including unimodal methods [6, 14] and multimodal methods [4, 5, 23]. For multimodal dataset, we compare MSRP trained by MOC with PORPOISE, MCAT and\nPONET, which are the state-of-the-art for multimodal cancer suri-vival risk prediction. The results are shown in Table 1. It can be seen that our method achieves best performance than all other methods. C-index has been improved by at least 6% on LUAD and 0.8% on KIRC than the state-of-the-art methods. The comparison of Kaplan-Meier Curve of different models is shown in Figure 2, patients are divided into high risk group and low risk group based on the median survival risk predictions. Log-rank test is used to test for statistical significance in survival distributions between low risk group and high risk group. In LUAD, the p-value of PORPOISE and MCAT are greater than 0.05, indicating PORPOISE and MCAT failed to distinguish different survival risk level. On the contrary, our method's p-value is small than 0.05, indicating our method succeed to distinguish different survival risk level. In KIRC, all of PORPOISE, MCAT and MOC succeed to distinguish different survival risk level, and our method has smaller p-value than the others."}, {"title": "Ablation study of training method for survival risk predicrion", "content": "We validate the effectiveness of our proposed OC on pathological images and genomic data respectively. On the pathological images, we extend the semi-parametric method based on Cox partial likelihood previously proposed by Steck [27] to deep learning (ELB). Then, we train the same model AMIL, using different training methods ELB, NLL and OC to compare their performances. On the genomics data, we train the same model SNN, using different training methods CoxPH, NLL and OC to compare their performance. The experimental results on two cancer datasets are shown in Table 2 and Table 3. It can be seen that the OC achieves performance that is either comparable or surpassing the best on both datasets, including pathological images and genomics data."}, {"title": "Ablation study of modality", "content": "We compare the multimodal model trained by MOC with the unimodal models trained by OC on LUAD and KIRC datasets. The results are shown in Table 4. It can be seen that the performance of MOC outperform the OC in\nany of the modalities and multimodal without cross-modal cooperation. This demonstrates that cross-modal contrast learning can capture meaningful inter-modal information to enhance survival risk prediction."}, {"title": "5 Conclusions", "content": "In this paper, we propose the object-level contrast learning (OC) for cancer surivival risk prediction and we demonstrate its effectiveness in pathological images and genomics data on two cancers datasets. We further extend OC to multimodal scenarios, proposing multimodal object-level contrast learning (MOC). MOC adopts employs cross-modal contrast to train the multimodal survival risk predictor, and we demonstrate that cross-modal contrast can capture inter-modal information useful for survival risk prediction. Finally, the multimodal survival risk predictor trained by MOC achieves the state-of-the-art performance on two multimodal cancer survival risk prediction datasets."}]}