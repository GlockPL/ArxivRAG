{"title": "Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation", "authors": ["Juno Kim", "Denny Wu", "Jason D. Lee", "Taiji Suzuki"], "abstract": "A key paradigm to improve the reasoning capabilities of large language models (LLMs) is to allocate more inference-time compute to search against a verifier or reward model. This process can then be utilized to refine the pretrained model or distill its reasoning patterns into more efficient models. In this paper, we study inference-time compute by viewing chain-of-thought (CoT) generation as a metastable Markov process: easy reasoning steps (e.g., algebraic manipulations) form densely connected clusters, while hard reasoning steps (e.g., applying a relevant theorem) create sparse, low-probability edges between clusters, leading to phase transitions at longer timescales. Under this framework, we prove that implementing a search protocol that rewards sparse edges improves CoT by decreasing the expected number of steps to reach different clusters. In contrast, we establish a limit on reasoning capability when the model is restricted to local information of the pretrained graph. We also show that the information gained by search can be utilized to obtain a better reasoning model: (1) the pretrained model can be directly finetuned to favor sparse edges via policy gradient methods, and moreover (2) a compressed metastable representation of the reasoning dynamics can be distilled into a smaller, more efficient model.", "sections": [{"title": "Introduction", "content": "Pretraining and inference constitute two distinct computational phases in large language models (LLMs). The pretraining phase, during which the model learns from vast amounts of text data through next-token prediction (Radford et al., 2018), is well known for its high computational demands, and its scaling behavior has been extensively studied (Kaplan et al.,"}, {"title": "Metastable Dynamics and Reasoning", "content": "Our key insight to understanding inference-time search is to frame CoT reasoning as a metastable Markov process over an underlying linguistic model. Each state represents a logical assertion (e.g., a sentence or mathematical expression rather than a single token), and state transitions correspond to reasoning steps. The model distinguishes between easy/trivial reasoning steps, which form dense local clusters of roughly equivalent meaning, and hard reasoning steps, which form sparse connections between clusters of small probability $O(\\varepsilon)$. Reasoning paths sampled from this process typically spend a long time in each cluster before making a nontrivial jump to another cluster. This leads to a dynamical separation between fast and slow timescales, which we quantitatively study by tuning the degree $\\varepsilon$ of perturbation.\nThe setup is formalized as follows. Let $X^\\varepsilon = (X_t)_{t \\geq 0}$ be a perturbed family of discrete-time stationary Markov chains on a (large but finite) state space $S$ with transition kernel $p^\\varepsilon$, such that $p^\\varepsilon$ uniformly converges to $p^0$ as $\\varepsilon \\rightarrow 0$. We assume $X^\\varepsilon$ is recurrent for all $\\varepsilon > 0$ and irreducible for all $\\varepsilon > 0$; also, $X^0$ is reducible and decomposes $S$ into $K$ disjoint $p^0$-ergodic components $C_1,\\dots,C_K$. We set $M := \\max_k |C_k|$ and assume that $\\min_k |C_k| = \\Theta(M)$ and $K \\leq \\text{poly}(M)$. Moreover, we denote the stochastic complement of $p^\\varepsilon$ corresponding to $C_k$ by the matrix $S_k^\\varepsilon$; see Appendix B for definitions. The stationary distributions of $p^\\varepsilon$, $S_k^\\varepsilon$ are denoted by $\\pi^\\varepsilon$, $\\pi_k^0$ and we set $\\mu_k := \\pi_k^0$."}, {"title": "CoT as Markov Chains", "content": "Our key insight to understanding inference-time search is to frame CoT reasoning as a metastable Markov process over an underlying linguistic model. Each state represents a logical assertion (e.g., a sentence or mathematical expression rather than a single token), and state transitions correspond to reasoning steps. The model distinguishes between easy/trivial reasoning steps, which form dense local clusters of roughly equivalent meaning, and hard reasoning steps, which form sparse connections between clusters of small probability $O(\\varepsilon)$. Reasoning paths sampled from this process typically spend a long time in each cluster before making a nontrivial jump to another cluster. This leads to a dynamical separation between fast and slow timescales, which we quantitatively study by tuning the degree $\\varepsilon$ of perturbation.\nThe setup is formalized as follows. Let $X^\\varepsilon = (X_t)_{t \\geq 0}$ be a perturbed family of discrete-time stationary Markov chains on a (large but finite) state space $S$ with transition kernel $p^\\varepsilon$, such that $p^\\varepsilon$ uniformly converges to $p^0$ as $\\varepsilon \\rightarrow 0$. We assume $X^\\varepsilon$ is recurrent for all $\\varepsilon > 0$ and irreducible for all $\\varepsilon > 0$; also, $X^0$ is reducible and decomposes $S$ into $K$ disjoint $p^0$-ergodic components $C_1,\\dots,C_K$. We set $M := \\max_k |C_k|$ and assume that $\\min_k |C_k| = \\Theta(M)$ and $K \\leq \\text{poly}(M)$. Moreover, we denote the stochastic complement of $p^\\varepsilon$ corresponding to $C_k$ by the matrix $S_k^\\varepsilon$; see Appendix B for definitions. The stationary distributions of $p^\\varepsilon$, $S_k^\\varepsilon$ are denoted by $\\pi^\\varepsilon$, $\\pi_k^0$ and we set $\\mu_k := \\pi_k^0$."}, {"title": "Reasoning Task", "content": "The reasoner is given a pair of input and output states $(X_{\\text{in}}, X_{\\text{out}})$ sampled from a distribution $D$ on $S \\times S$. The goal of the reasoner is to find a valid path from $X_{\\text{in}}$ to $X_{\\text{out}}$. We are thus interested in the hitting time of CoT generation to understand inference-time computation. The overall difficulty of the task is measured by the minimum number of hard reasoning steps needed to reach $X_{\\text{out}}$ from $X_{\\text{in}}$; longer reasoning chains will require more sparse transitions. We assume the average difficulty of the task is lower bounded:"}, {"title": "Metastable Dynamics", "content": "The hitting time and return time of $X^\\varepsilon$ to a set $A \\subset S$ are defined as $\\tau_A = \\inf\\{t > 0 : X_t \\in A\\}$, $\\tau_A^+ = \\inf\\{t > 0 : X_t \\in A\\}$, respectively. Probabilities and expectations conditioned on the initial state $x$ are denoted as $P_x, E_x$, etc.\nIn the context of perturbed Markov chains, a subset $M \\subset S$ is defined as a metastable system (Bovier et al., 2002) if\n$\\lim_{\\varepsilon \\rightarrow 0} \\sup_{x \\in M, y \\notin M} \\frac{P_x(\\tau_{M \\setminus \\{x\\}} < \\tau_y)}{P_y(\\tau_M < \\tau_y)} = 0$.\nThat is, it is much easier to return to $M$ than to transition between different states in $M$. The following result, obtained from our perturbative analysis in Appendices B-C, will motivate the distillation scheme described in Section 4."}, {"title": "Search Improves the Pretrained Model", "content": "We equate pretraining the base model with learning the underlying transition kernel $p^\\varepsilon$. Indeed, if the context window of an LLM is restricted to the tokens in the previous state, next-token prediction recursively defines a distribution over the following state, and further over reasoning chains of arbitrary length. We encode each state $x \\in S$ as a one-hot vector in $\\mathbb{R}^{|S|}$ also denoted by $x$ and write $p_{ij} = p(e_j | e_i)$. For the model, we consider a simple linear softmax predictor:\n$p_W(x) = \\text{softmax}(\\langle W, x \\rangle)$, $W \\in \\mathbb{R}^{|S| \\times |S|}$."}, {"title": "Pretraining the Base (World) Model", "content": "We equate pretraining the base model with learning the underlying transition kernel $p^\\varepsilon$. Indeed, if the context window of an LLM is restricted to the tokens in the previous state, next-token prediction recursively defines a distribution over the following state, and further over reasoning chains of arbitrary length. We encode each state $x \\in S$ as a one-hot vector in $\\mathbb{R}^{|S|}$ also denoted by $x$ and write $p_{ij} = p(e_j | e_i)$. For the model, we consider a simple linear softmax predictor:\n$p_W(x) = \\text{softmax}(\\langle W, x \\rangle)$, $W \\in \\mathbb{R}^{|S| \\times |S|}$."}, {"title": "Learning Sparse Rewards via Search", "content": "Having learned the underlying probabilities $p^\\varepsilon$, the base model now performs CoT reasoning by generating each step of the chain $(X_i)_{t>0}$ in sequence starting from $X_0 = X_{\\text{in}}$. Since the reasoner has no prior knowledge of which steps it must take to progress towards $X_{\\text{out}}$, on average it will spend a long time trapped in each cluster before chancing upon a sparse edge (new idea) and moving to a new cluster. From our quantitative dynamical analysis, we are able to obtain a nearly tight characterization of the average hitting time."}, {"title": "Improving the Base Model via RL", "content": "PRM mode keeps an external process reward \u2018model' (PRM) throughout the search process, which is simply the set $\\mathbb{M}_s$ which collects the estimated sparse edges over multiple iterations of the outer loop to reconstruct $E_s$. We prove that the PRM is strongly consistent:\nProposition 3.3. PRM mode of Algorithm 2 returns $\\mathbb{M}_s = E_s$ with probability $1-\\tilde{O}(1/K)$.\nThen by increasing the likelihood of transitions $(x, y) \\in \\mathbb{M}_s$ when the current state is $x$ by a factor of $\\varepsilon_{\\text{max}}/\\varepsilon$, the PRM can guide CoT to follow $p^{\\varepsilon_{\\text{max}}}$ rather than $p^\\varepsilon$. It is immediate from Theorem 3.2 that the expected hitting time decreases from $\\Theta(KM/\\varepsilon)$ to $\\Theta(KM/\\varepsilon_{\\text{max}})$. Moreover, the time complexity of Algorithm 2 is $R T_{\\text{max}} = \\tilde{O}(KM/\\varepsilon)$, which is equal to the time to solve a single instance $(X_{\\text{in}}, X_{\\text{out}})$ without search, and the memory requirement is only $O(M + K)$. This demonstrates the effectiveness of utilizing search to guide CoT generation."}, {"title": "Distillation to a Smaller Model", "content": "A prominent innovation in the LLM development pipeline is to distill CoT of a powerful model into a smaller, more efficient model. This approach has been shown to significantly enhance reasoning ability, especially compared to directly training the smaller model with RL (Shridhar et al., 2022; Hsieh et al., 2023; Gandhi et al., 2024; Guo et al., 2025). In this section, we showcase an explicit distillation scheme for our CoT model that efficiently generates the hard reasoning steps to solve any task while faithfully capturing the metastable dynamics of the original system."}, {"title": "Distilling Cluster Transitions", "content": "The metastable chain $q^\\varepsilon$ (Section 2) provides a natural notion of compression for the nearly reducible system $X^\\varepsilon$ by collapsing each cluster into a single state. For many downstream tasks (including the logic task studied in Section 5) it may be satisfactory to retrieve only the hard reasoning steps connecting the clusters containing $X_{\\text{in}}$, $X_{\\text{out}}$. In particular, if the goal is to extract only the connectivity of $S^*$, it suffices to take the sparse edge estimate $\\mathbb{M}_s$ of Algorithm 2 and perform a uniform random walk to find a path between any two clusters. However, we want the distilled model to also preserve the underlying dynamics of the original chain as best as possible. To this end, we implement the following process, detailed in Algorithm 4 in the appendix."}, {"title": "CoT of Distilled Model", "content": "To analyze the utility of the trained model $p_{Z^+}^\\varepsilon$, we make the additional assumption:"}, {"title": "Logical Reasoning is Hard without Search", "content": "In this section, we further investigate the benefits of search for reasoning by adding a quantitative 'logic task' on top of the path-finding task. This provides two benefits. First, having a numerical answer allows us to evaluate the hardness of the task from a learning-theoretic perspective, separate from the previously obtained hitting time bounds. Second, by having the answer depend only on the sparse edges along a path, the reasoner is required to estimate which edges are sparse \u2013 in other words, understand which reasoning steps are actually important \u2013 in order to solve the task. Taking a proof problem for example, we expect an LLM with strong reasoning capability to not only generate a plausible solution via next-token prediction but also understand its own proof, so that it can correctly answer logical questions such as \u201cwhat are the key ideas of this proof?\u201d or \u201cwhat happens if we replace step X with Y?\u201d We attempt to formalize this notion using group actions (Definition E.1)."}, {"title": "Logical Reasoning Task", "content": "In this section, we further investigate the benefits of search for reasoning by adding a quantitative 'logic task' on top of the path-finding task. This provides two benefits. First, having a numerical answer allows us to evaluate the hardness of the task from a learning-theoretic perspective, separate from the previously obtained hitting time bounds. Second, by having the answer depend only on the sparse edges along a path, the reasoner is required to estimate which edges are sparse \u2013 in other words, understand which reasoning steps are actually important \u2013 in order to solve the task. Taking a proof problem for example, we expect an LLM with strong reasoning capability to not only generate a plausible solution via next-token prediction but also understand its own proof, so that it can correctly answer logical questions such as \u201cwhat are the key ideas of this proof?\u201d or \u201cwhat happens if we replace step X with Y?\u201d We attempt to formalize this notion using group actions (Definition E.1)."}, {"title": "A Measure of Hardness with Restricted Access", "content": "In previous sections, we have seen that pretraining $\\mathbb{M}_p = p_W$ and running a search or distillation algorithm $f_\\theta$ will correctly infer the underlying sparse structure. In this case, computing $r_{A, p}(\\mathbb{M}_p(X_{\\text{in}}, X_{\\text{out}}))$ is trivial by concatenating actions along the identified sparse edges. In contrast, we now restrict the reasoning component's access to $p$ by only allowing certain queries to $\\mathbb{M}_p$. This makes it difficult to infer the sparse structure and true logical actions.\nTo understand learning with this additional (restricted) information, we propose the following generalization of the statistical query dimension (Kearns, 1998; Feldman, 2017)."}, {"title": "Results on Hardness of Logical Task", "content": "We consider four types of access to the pretrained model. Note that a local neighborhood of a subset $S' \\subset S$ in the weighted directed graph defined by $p$ is defined as the subgraph consisting of states reachable with a bounded number of steps from any state in $S'$."}, {"title": "Conclusion", "content": "We introduced a metastable Markov framework for modeling CoT reasoning, revealing the benefits of inference-time search, RL, and distillation. We showed that search can improve reasoning by identifying critical sparse transitions (hard steps), which can then be leveraged to fine-tune the pretrained model via RL or distilled into a more efficient representation, improving hitting times for path generation. We further established learning-theoretic limits on reasoning with restricted information and showed that logical reasoning tasks become intractable without global search.\nFuture directions. We have studied a simple curiosity-based unsupervised reward model; it would be interesting to see how a more complex search process could be guided with outcome rewards. Our framework could also be used to study other inference-time methods such as CoT revision (e.g., backtracking to better locate sparse edges), as well as iterative finetuning of the pretrained model, and explore scaling laws for inference time compute."}]}