{"title": "Bayesian Critique-Tune-Based Reinforcement Learning with Attention-Based Adaptive Pressure for Multi-Intersection Traffic Signal Control", "authors": ["Wenchang Duan", "Zhenguo Gao", "Jinguo Xian"], "abstract": "Adaptive Traffic Signal Control (ATSC) system is a critical component of intelligent transportation, with the capability to significantly alleviate urban traffic congestion. Although reinforcement learning (RL)-based methods have demonstrated promising performance in achieving ATSC, existing methods find it prone to convergence to local optima. Consequently, this paper proposes a novel Bayesian Critique-Tune-Based Reinforcement Learning with Attention-Based Adaptive Pressure (BCT-APRL) for multi-intersection signal control. In BCT-APRL, the Critique-Tune (CT) framework, a two-layer Bayesian structure is designed to refine the RL policies. Specifically, the Bayesian inference-based Critique layer provides effective evaluations of the credibility of policies; the Bayesian decision-based Tune layer fine-tunes policies by minimizing the posterior risks when the evaluations are negative. Furthermore, an attention-based Adaptive Pressure (AP) is designed to specify the traffic movement representation as an effective and efficient pressure of vehicle queues in the traffic network. Achieving enhances the reasonableness of RL policies. Extensive experiments conducted with a simulator across a range of intersection layouts show that BCT-APRL is superior to other state-of-the-art methods in seven real-world datasets. Codes are open-sourced.", "sections": [{"title": "I. INTRODUCTION", "content": "With urban populations continuous growth and cities expanding, traffic congestion has become increasingly severe, placing escalating pressure on the environment and economy [1]\u2013[3]. As a significant component of transportation systems, ATSC system can effectively alleviate traffic congestion via real-time dynamic control of signals [4]\u2013[6]. Reinforcement learning (RL) has been extensively explored as an efficient method in the ATSC system [7] [8]. However, the increasingly complex traffic demands of modern cities have outpaced the capabilities of existing RL-based methods [9]\u2013[11]. Most methods struggle to formulate optimal policies when confronted with complex traffic environments [12]\u2013[15]. Therefore, it is crucial to accurately capture the traffic states and make sure the RL policies achieve global optima.\nBy effectively optimizing long-term returns and enabling dynamic interactions with environments, RL has demonstrated promising performance [16]\u2013[18]. Specifically, references [19] [20] employed the independently RL agent at each intersection to improve the scalability issues. References [21] [22] introduced graph networks into RL, which implemented parameter sharing mechanisms to incorporate temporal and spatial influences from neighboring intersections. Da et al. [23] proposed the PromptGAT method to bridge the simulation-to-reality performance gap. However,the above methods placed excessive trust in RL policies, which often theoretically fail to convergence [24]. Furthermore, the lack of effective traffic states representation causes existing methods huge difficulties for the rationality of RL policies.\nTo address the unreasonableness of the RL policies, much literature has focused on how to enhance the reliability of the learning process [25]. Specifically, Wang et al. proposed a cooperative double Deep Q-Network (DQN) method [26] to improve the robustness of the learning process. Hinton et al. proposed a teacher-student framework [27], in which the teacher module guides the student module to avoid major errors in the learning process. And references [28], [29] addressed the limitation of traditional teacher-student advising methods, which only offered advice in situating the same state or after having similar experiences. Furthermore, references [30]\u2013[32] introduced the A2C algorithm, allowing for the evaluation and adjustment of the learning process. References [33], [34] proposed the multi-objective Bayesian optimization to solve the model-free problem in the learning process. However, the aforementioned methods simply focus on the learning process while overlooking the policy-making process, which often leads to unreasonable or suboptimal RL policies.\nIn the ATSC system, effective and efficient traffic states representation is also the key, rather than only complex algorithms [7] [8]. To efficiently represent the traffic scenario, references [35]\u2013[37] developed methods to quantify complex traffic information based on the max-pressure theory. Wu et al. [38] introduced the concept of efficient pressure to represent traffic states, achieving notable efficiency in signal control. Zhang et al. [39] further incorporated both waiting and running vehicles to enhance the comprehensiveness of traffic states representation. However, the existing pressure calculation methods overlook the varying influence of multiple upstream lanes on each downstream lane. This limitation may amplify the impact of low-traffic lanes while reducing responsiveness to high-traffic ones. Although computationally efficient, existing methods lack adaptability to dynamic traffic, undermining the rationality of RL policies.\nAccording to the above analysis, the excessive trust of RL"}, {"title": "II. TRAFFIC SIGNAL CONTROL WITH REINFORCEMENT LEARNING IN URBAN INTERSECTIONS", "content": "This section describes the concepts and methodology relevant to this paper. Firstly, key definitions related to ATSC are provided in Subsection II-A. Following that, Subsection"}, {"title": "A. ATSC Definition in Multi-Intersections", "content": "Definition 1 (Traffic Intersection and Road): The traffic network can be modeled as a directed graph, where nodes correspond to n number of intersections I and edges represent roads. At each intersection I, the road network consists of four directions {E,W,S,N}: east (E), west (W), south (S), and north (N).\nDefinition 2 (Traffic Lanes): Traffic road networks typically comprise three distinct types of lanes: left-turn lef, straight-through str, and right-turn rig. The traffic lanes in an intersection $I_i$ can be described as the upstream lanes $\\{X_l^y\\}$ and downstream lanes $\\{X_m^{y'}\\}$, in which $X, X' \\in \\{E,W, S, N\\}$ and $y, y' \\in \\{lef, str, rig\\}$. Specifically, the vehicles enter the intersection $I_i$ are denoted as the upstream lanes $\\{X_l^y\\}$, and the vehicles leave the intersection $I_i$ are denoted as the downstream lanes $\\{X_m^{y'}\\}$.\nDefinition 3 (Traffic Movement): Traffic movement is defined as the flow of traffic crossing an intersection from one upstream lane to one downstream lane. A traffic movement, such as from lane $X_l^y$ to lane $X_m^{y'}$, is denoted as $(X_l^y, X_m^{y'})$.\nAt an intersection where each road comprises three lanes, one downstream lane has three upstream lanes to generate traffic movements, thereby a total of nine traffic movements for one road that includes three downstream lanes.\nDefinition 4 (Traffic Queue Length): The traffic queue length $x(X_l^y)$ represents the number of vehicles waiting in lane $X_l^y$.\nDefinition 5 (Traffic running vehicle number): The traffic running vehicle number $r(X_l^y)$ represents the number of vehicles running in lane $X_l^y$.\nDefinition 6 (Traffic Signal Phase): Each traffic signal phase represents a set of allowed traffic movements. The sum of phases in intersection $I_i$ is denoted as $J_i$, and notation j is used to denote one of the phases.\nDefinition 7 (Efficient pressure): The efficient pressure (EP) is the difference between the average queue length on upstream lanes and the average queue length on downstream lanes.\n$P_e(X_i^y) = \\frac{1}{L} \\sum_{l_k \\in \\{X_l^y\\}} x(l_k) - \\frac{1}{M} \\sum_{m_j \\in \\{X_m^{y'}\\}} x(m_j)$ (1)\nwhere $l_k \\in \\{X_l^y\\}$, $m_j \\in \\{X_m^{y'}\\} $, L and M represent the number of lanes of $\\{X_l^y\\}$ and $\\{X_m^{y'}\\}$. The schema is shown in the left part of Fig. 2(a).\nDefinition 8 (Adaptive pressure): The attention-based adaptive pressure (AP) is the difference between the queue length at each upstream lane and the weighted queue length on downstream lanes.\n$P_e(X_l^y, X_m^{y'}) = x(l_k) - \\sum_{j=1}^{M} w_e x(m_j)$ (2)\nwhere $l_k \\in \\{X_l^y\\}$, $m_j \\in \\{X_m^{y'}\\} $, M represents the number of lanes of $\\{X_m^{y'}\\}$, and $w_e$ represents the weight of vehicle flow of upstream lane $l_k$ on downstream lane $m_j$. The schema is shown in the right part of Fig. 2(a)."}, {"title": "B. Markov property in RL-based ATSC", "content": "Due to their shared Markov property, RL is commonly modeled using the Markov process. Wherein state transitions depend solely on the current state but are independent of historical states. This characteristic enables RL to be effectively formulated as a Markov Decision Process (MDP), facilitating the optimization of policies to maximize the expected cumulative reward. Therefore, at time t in intersection $I_i$, RL is composed of six fundamental elements: the state space $S = \\{s_1, \\dots, s_n\\}$, observation space $O = \\{o_1, \\dots, o_n\\}$, action space $A = \\{a_1, \\dots, a_n\\}$, transition probability function $P(s' | s, a)$, reward function $R(s, a) = \\{r_1, \\dots, r_n\\}$, and discount factor $\\gamma$.\nThe goal of MDP is to make global optima policies $\\pi = \\{\\pi_1, ..., \\pi_n\\}$, under each intersection$I_i$ to maximize the own expected cumulative reward of taking a specific action $a_i$ in a given state $s_i$ over all future time steps, i.e., the state value function:\n$V(s_i) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} | s_i]$ (3)\nwhere $\\pi_i: o \\times a \\rightarrow [0, 1]$ maps the observation of intersection $I_i$ to the probability distribution of its action. The Q-value (action-value) of each intersection is defined as\n$Q(s_t, a) = E_{\\pi} [\\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1}]$ (4)"}, {"title": "III. BAYESIAN CRITIQUE-TUNE FRAMEWORK FOR RL", "content": "To achieve the RL policies of convergence to the global optima, this paper designs a two-layer Bayesian structure to refine RL policies. Subsection III-A describes the overall structure. Subsection III-B provides an in-depth discussion of the Critique Layer and Subsection III-C presents the Tune Layer in detail."}, {"title": "A. Bayesian Critique-Tune Structure", "content": "The policy-making process is vital for achieving global optima, enabling RL agents to deal with the complex urban traffic effectively. Consequently, this paper proposes the Critique-Tune (CT) framework, a two-layer Bayesian structure to refine the RL policies, as shown in Fig. 3.\nIn this framework, an auxiliary prediction network is implemented to predict the predictive reward $r_{t+h}$ corresponding to the next action. Equipped with the current Q-value $Q_{cur}^{(j)}$ and environment observation shared from the DQN, the prediction network is capable of generating the most suitable predictive reward, aligning its generation logic consistently with the DQN.\nConcurrently, the Bayesian inference-based Critique layer constructs a credible interval $CI_{Bayes}$ by utilizing historical rewards $R_{his} = \\{r_1,r_2,\\dots,r_j\\}$ and Bayesian prior experience. This interval evaluates whether the predictive reward $r_{t+h}$ falls within an acceptable range based on prior knowledge. Subsequently, the Bayesian decision-based Tune layer is activated if the evaluation yields a negative outcome, which meaning that $r_{t+h} \\notin CI_{Bayes}$. For each phase, the Tune layer computes the posterior risk $R_{post}$ by integrating the posterior distribution $p(Q|Q_{cur}^{(j)}, \\{Q_{his}^{(j)}\\})$ of the history Q-values $Q_{his} = \\{Q_i^{(j)}, Q_2^{(j)}, ..., Q_i^{(j)}\\}$ and current Q-value $Q_{cur}^{(j)}$, in which $j = 1,2,...,8$ denoted as each phase. Via selecting the phase with the minimal posterior risk $min R_{post}$, the Tune layer ensures the updated policy $\\pi_{upd}$ is more aligned with the global optima."}, {"title": "B. The Critique Layer", "content": "The Bayesian inference-based Critique Layer is responsible for evaluating the reasonableness of RL policies. Its judging process is divided into three main sections.\n1) SARIMA Modeling of History Rewards: In this section, the history rewards $R_{his}$ of RL are subjected to rigorous time series analysis. Thus, it can be mathematically represented using a time series model. Initially, the Augmented Dickey-Fuller (ADF) test is employed to judge the stationarity of the history rewards data. If the data exhibits non-stationarity, an appropriate differencing process is applied. Typically, the expression for a d-th order differencing is given by:\n$w_i^{(d)} = (1 \u2013 B)^d r_i$ (7)\nwhere $r_i$ and $w_i^{(d)}$ represent the original and differenced history rewards data, B is the lag operator that defined as $Br_t = r_{t-1}$, and d denotes the differencing order.\nWhen the history reward data is transformed into a stationary time series, the long-term dependence properties within a single episode of RL-based ATSC can be modeled using an Autoregressive Integrated Moving Average (ARIMA) model. Each episode's data can then be represented by the ARIMA equation:\n$(1 - \\sum_{k=1}^{p} \\phi_k B^k) w_t^i = (1 + \\sum_{j=1}^{q} \\theta_j B^j) \\epsilon_t$ (8)\nwhere $\\phi_i$ represents the autoregressive coefficients, $\\theta_j$ represents the moving average coefficients, $\\epsilon_t$ represents the random error term at time t and typically assumed to be white noise (i.e. $\\epsilon_t \\sim N(0, \\sigma^2)$). Moreover, p denotes the order of the autoregressive (AR) part, and q denotes the order of the moving average (MA) part.\nSubsequently, the optimal orders $\\{p, d, q\\}$ of the ARIMA model need to be determined, tailored to the complexities inherent in RL-based ATSC. The determination process uses the Bayesian Information Criterion (BIC) to evaluate and select the model order, balancing complexity and predictive accuracy. The BIC formulation is given as follows:\n$BIC = -2ln(L) + k ln(n)$ (9)\nwhere L is the likelihood function value of the model, and k represents the number of parameters in the ARIMA model.\nIn order to enhance the rationality of the credible interval, this paper introduces the Seasonal Autoregressive Integrated Moving Average (SARIMA) model to fit the multi-episodes data, with each episode regarded as the same ARIMA model (the detailed proof is presented in Appendix A). In detail, the SARIMA model combines non-seasonal and seasonal components, which are the non-seasonal part ARIMA(p, d, q) and the seasonal part SARIMA(P, D, Q, s). The non-seasonal part of ARIMA (p, d, q) is defined as above. The definitions of the seasonal part of SARIMA(P, D, Q, s) are as follows: P denotes the order of the seasonal autoregressive part, Q denotes the order of the seasonal moving average part, D denotes the seasonal differencing order, and s denotes the seasonal period. The optimal orders $\\{P, D, Q\\}$ also determined by Eq. (9).\nBuilding on the above methods, the SARIMA equation for historical rewards in the traffic signal control context can be modeled:\n$\\Phi_p(B) \\Phi_p(B^s) \\nabla^d \\nabla_s^D r_t = \\Theta_q(B) \\Theta_q(B^s) \\epsilon_t$ (10)\nwhere $\\Phi_p(B) = 1 - \\phi_1B - \\phi_2B^2 - \\dots - \\phi_pB^P$, $\\Phi_p(B^s) = 1 - \\Phi_1B - \\Phi_2B^{2s} - \\dots - \\Phi_pB^{Ps}$, $\\nabla^d r_t = (1 - B)^d r_t$, $\\nabla_s^D r_t = (1 - B^s)^D r_t$, $\\Theta_q(B) = 1 + \\theta_1B + \\theta_2B^2 + \\dots + \\theta_qB^q$, $\\Theta_q(B^s) = 1 + \\Theta_1B^s + \\Theta_2B^{2s} + \\dots + \\Theta_QB^{Qs}$.\nEstablishing this SARIMA model enables an accurate quantity of RL policies by capturing temporal patterns of the data. Achieving the effectiveness of the follow-up evaluation process of the predictive reward.\n2) Bayesian Prior Distribution of SARIMA Parameters: In this section, the unknown parameters of the ARIMA model in the SARIMA model are estimated. By integrating the prior experience of traffic data properties, the SARIMA model achieves effective evaluation for the RL policies in the next step. Firstly, the determination of the prior distribution of unknown parameters $\\xi = (c, \\phi_1,..., \\Phi_p, \\theta_1, ..., \\Theta_q, \\sigma^2)$ is a pivotal step for this section. Specifically, due to the non-negativity of history rewards $R_{his}$, the Truncated Normal distribution $TN(\\mu_c, \\sigma_c^2, a, b)$ is suitable for the intercept term c. Where $\\mu_c$ is the mean of the untruncated normal distribution, $\\sigma_c$ is the variance of the untruncated normal distribution, a and b are the lower and upper bounds of the truncation interval, respectively. And due to the complexity of the traffic environment, incorporating a sparsity-inducing prior is essential when modeling history rewards $R_{his}$. Consequently, the Laplace distribution $Laplace(\\mu_{\\phi_i}, b_{\\phi_i})$ and Laplace distribution $Laplace(\\mu_{\\Theta_i}, b_{\\Theta_i})$ are selected for autoregressive coefficients $\\phi_i$ and moving average coefficients $\\theta_j$, respectively. Where $\\mu_{\\phi_i}$ and $\\mu_{\\Theta_i}$ are the means, $b_{\\phi_i}$ and $b_{\\Theta_i}$ are the scale parameters, with a smaller b indicating a stronger tendency toward coefficient sparsity. Furthermore, the random error term $\\epsilon_t$ in RL has large uncertainties, thereby the Inverse Gamma distribution $IGa(\\alpha_{\\sigma^2},b_{\\sigma^2})$ is applied to estimate the variance $\\sigma^2$ of the random error term $\\epsilon_t$. Where $\\alpha_{\\sigma^2}$ is the shape parameter and $b_{\\sigma^2}$ is the scale parameter. The heavy-tailed nature of this distribution effectively captures the potential for larger variance values. Above all, the prior distribution can be expressed as follows:\n$p(c) = \\frac{1}{\\sigma_c} \\phi(\\frac{c-\\mu_c}{\\sigma_c}) / (\\Phi(\\frac{b-\\mu_c}{\\sigma_c}) - \\Phi(\\frac{a-\\mu_c}{\\sigma_c}))$ (11)\n$p(\\phi_i) = \\frac{1}{2b_{\\phi_i}} exp(- \\frac{|\\phi_i-\\mu_{\\phi_i}|}{b_{\\phi_i}})$ (12)\n$p(\\theta_i) = \\frac{1}{2b_{\\theta_i}} exp(- \\frac{|\\theta_i-\\mu_{\\theta_i}|}{b_{\\theta_i}})$ (13)\n$p(\\sigma^2) = \\frac{b_{\\sigma^2}^{\\alpha_{\\sigma^2}}}{ \\Gamma(\\alpha_{\\sigma^2})} (\\sigma^2)^{-\\alpha_{\\sigma^2}-1} exp(- \\frac{b_{\\sigma^2}}{\\sigma^2})$ (14)\nwhere $\\xi = (c, \\phi_1,..., \\Phi_p, \\theta_1, ..., \\Theta_q, \\sigma^2)$ are all the parameters to be estimated, and $\\Phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^{x} e^{-\\frac{t^2}{2}} dt$.\nAfter obtaining the prior distribution of each parameter, the sample set $\\{\\xi^{(1)}, \\xi^{(2)}, ..., \\xi^{(N_t)}\\}$ can be generated by random"}, {"title": "C. The Tune Layer", "content": "The Bayesian decision-based Tune Layer is responsible for fine-tuning RL policies by posterior risk when the evaluation is negative. This layer ensures the RL policies adapt to real-time traffic conditions, enhancing the reasonableness of the policy-making process. The tuning process is divided into two main sections.\n1) The Incorporating of Q-values Information: This section main involves determining the prior distribution of history Q-values $Q_{his}^{(j)}$ and constructing the Q-values $\\{Q_{cur}^{(j)}, Q_{his}^{(j)}\\}_{j=1}^{j=8}$ likelihood function. Firstly, given the high entropy inherent in traffic environments, Q-values often exhibit multiple peaks and may present a noninformative prior. For this complexity, this paper employs the Kernel Density Estimation (KDE), a nonparametric Bayesian method, to estimate the probability density function of history Q-values. This estimated density is then utilized to construct the prior distribution:\n$f(Q) = \\frac{1}{Th} \\sum_{t=1}^{T} K(\\frac{Q-Q_t}{bw})$ (16)\nwhere bw is the bandwidth parameter, $K(u)$ is the Gauss kernel, in which $K(u) = \\frac{1}{\\sqrt{2 \\pi}} exp(- \\frac{u^2}{2})$.\nConcurrently, the likelihood function of Q-values can be constructed, which includes two parts. Specifically, based on the Gauss kernel, the likelihood function of the history Q-values $Q_{his}^{(j)}$ is defined as the follows:\n$L(Q^{(j)}|Q) = K(\\frac{Q-Q_t^{(j)}}{bw}) = \\frac{1}{\\sqrt{2 \\pi}bw} exp(- \\frac{(Q-Q_t^{(j)})^2}{2bw^2})$ (17)\nAdditionally, since the current Q-value $Q_{cur}^{(j)}$ carries greater significance in RL-based policy-making process, a confidence-based weighting mechanism is designed. This paper uses the normal distribution with an error term to describe the likelihood of the current Q-value, denoted as the \"weighted likelihood function\":\n$L(Q^{(j)}|Q) = \\frac{1}{\\sqrt{2 \\pi} \\sigma_{cur} V} exp(- \\frac{(Q-Q_{cur}^{(j)})^2}{2 \\sigma_{cur}^2})$ (18)\nwhere $\\sigma_{cur}$ is a tuning parameter representing the uncertainty of the current Q-value. A smaller $\\sigma_{cur}$ value means higher confidence in the current Q-value. This adjustment allows the Tune layer to prioritize the most recent state information, which is crucial for capturing the dynamic nature of traffic flow.\nBy combining the weighted likelihood of the current Q-value $L(Q^{(j)}|Q)$ with the history Q-value prior distribution $f(Q)$, the overall likelihood function is expressed as follows:\n$L(Q) = L(Q^{(j)}|Q) \\times \\prod_{t=1}^{T} L(Q_{his}^{(j)}|Q)$ (19)\nThis integrated likelihood function enhances the adaptability and responsiveness of the RL agent to real-time traffic conditions. Ensuring comprehensive and effective subsequent Bayesian-based phase tuning.\n2) Bayesian Posterior Risk of Each Phase: In this section, the Bayesian posterior risk of each phase is calculated to refine the RL policies. As in the Critique Layer, the Bayesian posterior distribution is initially updated according to Bayes' theorem:\n$p(Q|Q_{cur}^{(j)}, \\{Q_{his}^{(j)}\\}) \\propto L(Q) \\times f(Q)$ (20)\nSubsequently, based on the Bayesian decision theory, a loss function $L(Q^{(j)}, Q)$ is introduced to measure the posterior risk(this paper adopts the square loss function $L(Q^{(j)}, Q) = (Q^{(j)} - Q)^2$). The expected posterior risk for each phase in the intersection $I_i$ is expressed as follows:\n$R_{post} = \\int L(Q^{(j)}, Q) p(Q|Q_{cur}^{(j)}, \\{Q_{his}^{(j)}\\}) dQ$ (21)\nBuilding on the Bayesian criteria, the posterior risk for each phase is computed, and the phase that minimizes the expected posterior risk $R_{post}$ is selected as the optimal action:\n$\\{\\pi_{upd}\\}_{i=1}^{i=n} \\leftarrow \\arg \\inf_j \\int L(Q^{(j)}, Q) p(Q|Q_{cur}^{(j)}, \\{Q_{his}^{(j)}\\}) dQ$ (22)"}, {"title": "IV. RL WITH ATTENTION-BASED ADAPTIVE PRESSURE FOR ATSC", "content": "This section presents the attention-based adaptive pressure RL for ATSC. In Subsection IV-A, an Attention-Based Adaptive Pressure (AP) is introduced. Subsection IV-B details the AP-based DQN for ATSC, which serves as the policy-making backbone of the total framework. Finally, Subsection IV-C outlines the policy-making and training process for the proposed BCT-APRL."}, {"title": "A. Attention-Based Adaptive Pressure Extraction", "content": "In the complex urban environment, effective and efficient traffic states representation of each lane is crucial for optimizing signal control. To achieve effectively capture the real-time traffic states, this paper proposes an Attention-Based Adaptive Pressure.\nIn an intersection $I_i$, the AP mechanism identifies the source upstream lane associated with each vehicle and links it to the corresponding downstream lane. The downstream lanes $\\{X_m^{y'}\\}$ and upstream lanes $\\{X_l^y\\}$ are represented as four-layer matrices of dimensions 3 \u00d7 3, where the matrices are denoted as K and Q, respectively. Each of the three columns in these matrices represents: the number of waiting vehicles $k_{wai}$ and $q_{wai}$, the number of running vehicles $k_{run}$ and $q_{run}$, and the total number of vehicles $k_{tot}$ and $q_{tot}$. The three rows correspond to the left-turn lane lef, straight-through lane str, and right-turn lane rig. Besides, the four layers of the matrix represent the directions: east E, west W, south S, and north N.\nThe AP mechanism further utilizes a multi-head attention mechanism to process and extract detailed information from both downstream and upstream lanes. This processed data are subsequently used as inputs for the query matrix Q and key matrix K. The multi-head attention mechanism is formalized as follows:\n$W = linear(Concat(W_1, W_2, ..., W_{head}))$ (23)\nwhere,\n$W_i = Softmax(\\frac{(K)^T}{\\sqrt{d_k}})$ (24)\nIn this formulation, head represents the number of attention heads, and $d_k$ is the dimensionality of the key K. This multi-head attention mechanism allows the AP system to construct a four-layer upstream attention weight matrix with dimensions 3 \u00d7 3. This matrix represents the weight of vehicle number influence of each upstream lane on three downstream lanes. A higher attention weight signifies a stronger impact. Based on these attention weights and Eq. (2), the AP can be obtained. Unlike traditional methods that treat the queue lengths of all downstream lanes as coming equally from each upstream lane. AP achieves adaptability and effective capture of the traffic states, avoiding the lackness of traditional methods. Wherein these weaknesses may amplify the impact of low-traffic lanes while weakening responsiveness to high-traffic ones."}, {"title": "B. AP-based DQN for ATSC", "content": "In the ATSC, this paper adapts the RL method using Deep Q-Networks (DQN), tailored to optimize the traffic signal phase policy-making process at intersections. The AP-based DQN algorithm employs artificial neural networks (ANNs) to approximate the optimal action-value function $Q(s_i, a)$ for each phase selection at each time step t. The architecture of AP-based DQN for ATSC is shown in Fig. 4. This dual-network framework comprises both an online network and a target network, with parameters $\\zeta$ and $\\zeta^*$ respectively. These two networks maintain the same structure but are updated at different rates to stabilize the learning process.\nFor AP-based DQN signal control in an intersection $I_i$, the state $s_i$ is indicated as the current traffic conditions, which include the AP, the intersection phase, the number of running vehicles of upstream lanes, and the spatial correlation of each intersection. These state information forms the basis upon which the DQN estimates $Q(s | \\zeta)$, enabling the system to select the optimal traffic signal phase $j \\in J_i$ for minimizing congestion and delay at the intersection. Besides, the action space A consists of predefined signal phases, and the objective is to maximize the cumulative reward by optimizing the traffic flow. The reward $r_i$ is designed to reflect traffic efficiency, integrating both a phase-switching penalty and a traffic throughput incentive. A negative reward is imposed for unnecessary phase switching. In contrast, a positive reward is provided for improvements in transportation efficiency.\nBy interacting with experience replay pool w, DQN improves learning efficiency by storing all replay experience $\\{(s_t^i, a_t^i, r_t^i, s_{t+1}^i), \\cdots, (s_{st}^i, a_{st}^i, r_{st}^i, s_{st+1}^i)\\}_{i=1}^{i=n}$ of one epoch st, and computing the following gradient by differentiating the loss function with respect to the weights:\n$\\nabla L(\\zeta) = \\alpha \\eta_t \\nabla Q(s_t^i, a_t^i; \\zeta)$ (25)\nwhere a denotes the learning rate, $s_t^i, a_t^i$ belongs to the target network, and the temporal-difference (TD) error $\\eta_t = \\{\\eta_1, \\cdots, \\eta_n\\}$ is calculated as:\n$\\eta = r_i + \\gamma \\max_{a_{t+1}} Q(s_{t+1}, a_{t+1}; \\zeta^*) - Q(s_t, a_t^i)$ (26)\nThe online network parameters are updated by stochastic gradient descent and Eq. (25). And the target network parameters $\\zeta^*$ are updated via soft updates:\n$\\zeta^* \\leftarrow \\tau \\zeta + (1 - \\tau) \\zeta^*$ (27)\nIn this algorithm, the AP-based DQN framework effectively adapts to ATSC. Equipped with the AP, the system achieves a more refined and context-aware signal control. This adaptive algorithm minimizes average travel time at urban intersections, making the RL policies efficiently responsive to fluctuating traffic conditions."}, {"title": "C. BCT-APRL Policy-Making and Training Process", "content": "The policy-making and training process of the BCT-APRL for multi-intersections is illustrated in Algorithm 1. Firstly, a DQN-based agent for ATSC is employed for multiple intersections $\\{I_i\\}_{i=1}^n$. With the initialization of the experience replay pool w, the online network $\\zeta$, target network $\\zeta^* \\leftarrow \\zeta$, and auxiliary prediction network are initialized for all intersections $\\{I_i\\}_{i=1}^n$. For each intersection, the algorithm follows an episodic method to facilitate learning, updating the networks iteratively.\nDuring each episode, the algorithm initializes the traffic states $s_0$, retrieves history rewards $R_{his}$ and history Q-values $Q_{his}^{(j)}$ from the experience replay pool w. Subsequently, for each time step t within each episode, the current Q-values $Q_{cur}^{(j)}$ are obtained using the $\\epsilon$-greedy policy to select the most suitable action a. Concurrently, the predictive reward $r_{t+h}$ is predicted using the auxiliary prediction network, providing a forecast for next performance.\nWithin each time step, the Critique layer is employed to evaluate the credibility of the predicted reward. The history rewards $R_{his}$ are used to construct a Bayesian credible interval $CI_{Bayes}$. If the predicted reward $r_{t+h}$ falls outside this credible interval, the algorithm activates the Tune Layer. This layer updates the policy by calculating posterior risk $R_{post}$ using current Q-values $Q_{cur}^{(j)}$ and the history Q-values $Q_{his}^{(j)}$, selecting the signal phase with the lowest risk.\nThe agent then observes the next state $s_{t+1}$ and corresponding reward $r_i$, which is subsequently used to update the online network via gradient descent. Besides, the agent stores the transition tuple $(s_t^i, a_t^i, r_t^i, s_{t+1}^i)$ in each replay buffer, ensuring the necessary experience is recorded for future use. After processing all time steps, the target network $\\zeta^*$ is updated based on the online network, ensuring stability in the learning process.\nAt the end of the process, the global optimal signal phase RL policies $\\{\\pi_{upd}\\}_{i=1}^n$ for all intersections are returned, culminating the training phase."}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "To empirically evaluate the BCT-APRL, extensive experiments are conducted. Subsection V-A describes the overall experiment settings. Subsection V-B provides the analysis of experiment results in detail. The ablation study is shown in the V-C."}, {"title": "A. Experiment Setup", "content": "1) Environment Settings: This paper conducts experiments on the CityFlow traffic simulator [41", "Datasets": "The experiments are conducted on seven real-world traffic flow datasets", "lanes": "left-turn lane", "Baselines": "For evaluating the effectiveness of BCT-APRL"}]}