{"title": "Solving nonograms using Neural Networks", "authors": ["Jos\u00e9 Mar\u00eda Buades Rubio", "Antoni Jaume-i-Cap\u00f3", "David L\u00f3pez Gonz\u00e1lez", "Gabriel Moy\u00e0 Alcover"], "abstract": "Nonograms are logic puzzles in which cells in a grid must be colored or left blank according to the numbers that are located in its headers. In this study, we analyze different techniques to solve this type of logical problem using an Heuristic Algorithm, Genetic Algorithm, and Heuristic Algorithm with Neural Network. Furthermore, we generate a public dataset to train the neural networks. We published this dataset and the code of the algorithms. Combination of the heuristic algorithm with a neural network obtained the best results. From state of the art review, no previous works used neural network to solve nonograms, nor combined a network with other algorithms to accelerate the resolution process.", "sections": [{"title": "1. Introduction", "content": "A nonogram, which is also known as a Picross or Hanjie, is a Japanese logic puzzle in which cells in a grid must be colored or left blank according to a set of numbers that is located at the side of the board, also known as row and column headers, to reconstruct a binary image. Each header indicates the number of cells that must be marked in a row inside the board to construct a block. If there is more than one number in the same row or column header, at least one empty cell must exist between them. Puzzles of an arbitrary size can be defined as rectangular or square. The cells of a nonogram are defined by two states: filled (||) and empty (| x |)."}, {"title": "1.1. Problem specification", "content": "A nonogram is a board that is defined by a matrix of size n\u00d7m with n, m \u2265 1. As stated previously, each cell has two possible states: filled and empty, which are represented by the values {0,1}. We define Inxm as the space of possible values for boards of size n \u00d7 m: \u03a9nxm : {0,1}n.m. For example, boards of size 5 \u00d7 5 have 25 cells; consequently, there are 225 boards of this size.\nIn this study, we only consider squared boards; therefore, for all nonograms, n = m. This implies that the rows and columns of these squared boards have the same possible codifications. We use On instead of Nnxn to simplify the notation. Given a board of size n, we define Cn as the space of possible headers, and denote the encoding space of the boards as |\u0424|, where |Cn|2n."}, {"title": "1.2. State of the art", "content": "In the literature, we identified five main strategies for solving nonograms efficiently: heuristic, depth-first search (DFS), genetic algorithms (GAs), and reinforcement learning (RL)."}, {"title": "1.2.1. Heuristic algorithms", "content": "Regarding heuristic algorithms, Salcedo-Sanz et al. [4] designed a set of ad- hoc heuristics. In particular, they proposed a combinatorial ad-hoc heuristic based on trying feasible combinations of solutions in each row and column of the puzzle that may fail in solving large puzzle and a logic ad-hoc heuristic, that starts with a pre-processing step, where they filled trivial cells, and is based on the calculation of the feasible right-most and left-most solution for a"}, {"title": "1.2.2. Depth-first search algorithms", "content": "Concerning DFS, researchers [7, 8, 9] combined DFS algorithms and the use of logical rules. The key concept is to obtain as much information as possible by applying logical rules to detect cells which can be determined immediately at first and then applying backtracking to search for the puzzle solution. Yu et al. [8] improved the search process using information that was obtained from a two sets of logical rules, first set contain 5 rules and the second set contain 3 rules to refine the results obtained first set, then they applied a backtrack- ing algorithm. Jing et al. [7] proposed a solution based on the fact that most of Japanese puzzle are compact and contiguous, they defined a set of 11 rules divided in three main parts and then they applied the branch-and-bound algo- rithm to improve the search process. Finally, Stefani et al. [9] applied rule-based techniques that consist of simple boxes, simple spaces, forcing, and contradiction then the best-first search is used to solve the puzzle.\nWikeckowski et al. [10] modified the DFS method, they also used a soft computing method based on permutation generation and used both to solve nonograms. The two proposed algorithms were analyzed to obtain a solution, the number of iterations, and the time required to obtain the final state. They concluded that, in contrast to the soft computing method, the DFS algorithm guaranteed correct solutions regardless of the level of difficulty of the nonogram."}, {"title": "1.2.3. Genetic algorithms", "content": "Respecting GA, Tsai [11] proposed a Taguchi-based GA that was effectively applied to solve nonograms, and tested their approach on large nonograms (from 15 x 15 to 30 x 25 ), managing to solve more than 50% of the nonograms. Bobko and Grzywacz [12] presented the concept of using classical GAs as a tool for solving nonograms. They obtained good results with nonograms of sizes 4 \u00d7 4 and 5 \u00d7 5, but in the case of 10 \u00d7 10, with a chromosome length of 100, none of their proposals yielded the expected result. Soto et al. [13] developed a GA that uses a new generation of the initial population and experimentally applied it to 5 x 5 boards.\nHabes and Hasan [14] proposed an adaptation of the particle filter, called Particle Swarm Optimization which is a population based stochastic optimiza- tion method, to solve nonograms and performed an experiment using only three boards with different sizes."}, {"title": "1.2.4. Reinforcement learning algorithms", "content": "Finally, as for RL,Shultz et al. [16] applied RL to solve this problem. They found that the RL solvers learned near-optimal solutions that outperformed a heuristic solver based on general rules, but they only applied their solution to solve 5 \u00d7 5 boards because of the computational time required for larger puzzles."}, {"title": "1.3. Research objective", "content": "Our main objective was to determine whether a neural network can im- prove the performance of two well-known algorithms (DFS and GA) to solve nonograms. The hypothesis was neural networks can help to solve nonogram efficiently From literature review, no previous works used a neural network to solve nonograms, nor combined a network with other algorithms to accelerate the resolution process. For this reason, in this study, we developed two new approaches. First, we used a combination of the DFS and a neural network. Second, we combined a GA and a neural network. Furthermore, we developed a dataset to evaluate the proposed solutions. This dataset and the algorithm code were published."}, {"title": "1.4. Outline", "content": "The remainder of this paper is organized as follows: In Section (2), we explain the methods that we developed to solve nonograms. Section (3) presents the experimental setup, including a description of the dataset that we created, the metrics used, and the experiments that were performed. In Section (4), we outline the obtained results and discuss the performance of the proposed methods. Finally, Section (5) summarizes the conclusions and presents the strengths and shortcomings of the proposed system."}, {"title": "2. Methods", "content": "In this section, we explain the networks that we designed, as well as the algorithms that we used for solving the boards. These well-known algorithms were modified to include the networks as a support for the decision-making process."}, {"title": "2.1. Network design", "content": "We employed a fully connected (FC) neural network, which is an artificial neural network that is composed solely of fully connected layers. The input to the network was a vector of size LM \u00b7 width + LM \u00b7 height, where width = height = n; LM = [(n + 1)/2\u300d, and n was the board size. We constructed this vector by concatenating all headers. To make them equal in size, we added"}, {"title": "2.2. Reflections", "content": "As stated previously, we can represent nonograms in two spaces: the board space On and header space \u03a8n. The space \u03a9 represents all existing boards. For example, for a 5\u00d75 board using two states, there are 225 possibilities. The space \u03a8\u03b7 represents all valid header combinations; in the case of a 5 \u00d7 5 board, the number is 28781820. Moreover, several elements of the space I can represent different boards; that is, the encoding operation is not bijective, as illustrated in Figure 2.\nTo improve the network performance, we searched for bijective functions in the I space, which implied a bijection in the space. Reflections are functions that fulfill this requirement. By applying these reflections, we could improve the network results; if the network learned one of the reflections, it could make a correct prediction. We defined eight bijective functions that form a noncom- mutative group, which can be generated using the following three functions and identity (id): swapping rows and columns (g: diagonal reflection, where gog = id), reversing the row order (fv: vertical reflection, where fv \u25cb fv = id), and reversing the column order (f\u0127: horizontal reflection, where fh \u00b0 fh = id).\nThe vertical and horizontal reflections produce header realignment and pro- voke a swap of the header values. For example, if a row has header 12 when horizontal reflection is applied, it becomes 21. The eight bijective functions are depicted in Figure 5 and explained below."}, {"title": "2.3. Algorithms", "content": "In this section, we describe the different approaches that we designed and implemented to solve nonograms. The first is a heuristic algorithm that we used as a baseline. We implemented a modification to introduce the neural network, as described in Section 2.1, as a support for the decision-making process and the reflections as an improvement. Furthermore, we introduced the concept of intuition, which means weighting the solution path by the similarity of the state of the board with the neural network proposal. We also describe the partial and full erase ideas that are used to accelerate the solution process. Finally, we propose a GA that also uses a neural network with the same objective, which is used in the state of the art. We were interested in evaluating their performance when we added a neural network to the decision process."}, {"title": "2.3.1. Heuristic algorithm (H)", "content": "The first algorithm is composed of heuristic rules and a DFS. The heuristic is based on the concept of trivial cells, which can be filled using the information that is available from the puzzle headers and previous board state. We used the set rules found in [8] to design this algorithm.\nGiven a row or column, we establish the trivial cells by calculating all possible combinations given the header information. Subsequently, we obtain combina- tions that are compatible with the state of the row or column. At each step, we analyze the combinations that are compatible with the header to determine if there is a cell with the same value in all configurations to update the cell value. An example of this situation is shown in Figure 6. This process is repeated until the nonogram is solved or until it is not possible to obtain further trivial cells.\nOnce we fill all of the trivial cells, the second process, namely DFS, is im- plemented, whereby the path selection is determined by the number of pos- sible combinations of the row or column in ascending order. At each search"}, {"title": "2.3.2. Network, heuristic algorithm and intuition (NeHI)", "content": "We modify the heuristic algorithm to integrate the neural network. This algorithm has two steps: First, we predict the board with the neural network and check if a correct solution is obtained\nSecond, if a solution is not obtained in the previous step, we start the DFS search using intuition with an empty board. This concept is applied at each step of the search, and we weight each path according to the similarity to the neural network prediction. This is the number of differences between the path and network prediction. The algorithm visits them according to their scores in ascending order; in the case of a tie, we select a path randomly.\nThese modifications are summarized in Algorithms 2 and 3, and code is available at https://gitlab.com/DLG-05/musi-tfm-nonograma."}, {"title": "2.3.3. Heuristic algorithm using a network, with partial erase, full erase and intuition (NeHPF/NeHPFI)", "content": "We designed this third version of the heuristic algorithm to improve the results of the previous algorithm when network prediction is not a valid solution."}, {"title": "2.3.4. Heuristic algorithm using a network trained with all reflections (Ne8HI/Ne8HPF/Ne8HPFI)", "content": "Based on the preliminary experiments, the network could not solve all boards, but it could solve one of its eight reflections. We design this algorithm to im- prove the network prediction results.\nThis version includes the same algorithms as those in Sections 2.3.2 and 2.3.3. However, the networks are trained with the eight possible reflections of each nonogram of the training set, and if the network can solve any of the eight reflections, it can solve the proposed nonogram. Once one of the reflections is solved, the original board can be obtained by reversing it. It is important to note"}, {"title": "2.3.5. Genetic Algorithm (GA)", "content": "Finally, we develop a GA to solve nonograms. The mutations consist of filling, emptying, or moving filled cells. A cell is filled if the number of filled cells in the current nonogram is smaller than that in the target nonogram. We empty a cell if there were more filled cells than those in the target nonogram. Finally, if the number of filled cells is equal to that in the target nonogram, the cell is moved. The number of mutations is determined randomly between 0 and 4, and the locations are randomly selected following a uniform distribution.\nWe generate the initial population of this algorithm by predicting a board using a neural network, and then we apply mutations to it. We define the following fitness function to evaluate each board:\nFitness = ncc + ncr + (width \u00b7 height) \u00b7 \\frac{nmc}{nemb} (1)\nwhere ncc denotes the number of correct columns, ncr denotes the number of correct rows, nmc denotes the number of marked cells, and n\u0441\u0442\u044c denotes the number of cells that must be marked to complete the board. The fitness function is based on the concepts presented in [13].\nThe pseudo-code of the algorithm is presented in 5 and code is available at https://gitlab.com/DLG-05/musi-tfm-nonograma."}, {"title": "3. Experimental setup", "content": "In this section we describe the dataset, the experimental environment, the experiments and the metrics."}, {"title": "3.1. Dataset", "content": "To train the neural networks, we required a large number of nonograms with their corresponding solutions. To the best of our knowledge, there is no public dataset that fulfills our needs; therefore, we generated our own dataset.\nThe dataset contained samples of boards of sizes 5 \u00d7 5, 10 \u00d7 10 and 15 x 15. We generated all possible boards of size 5 \u00d7 5; that is, 25\u00d75 ~ 3.36e07 boards. It was not possible to generate all boards of sizes 10 \u00d7 10 and 15 \u00d7 15, because the number thereof was very high: 210\u00d710 ~ 1.27e30 and 215\u00d715 ~ 5.39e67 boards. For these sizes, we employed the following strategy: First, we used the MAME Icons dataset [17] and Icons50 dataset [18], and applied the average image hash [19] to detect and remove duplicated images efficiently. Second, we scaled the images to the desired board size (10 \u00d7 10 and 15 \u00d7 15 pixels). Third, we applied the following four transformations to generate the first set of nonograms: a Canny filter to detect edges, a binary threshold with a threshold value equal to 128, Otsu's threshold, and an inverted Otsu's threshold of the original image. An example of this transformation is depicted in Figure 7. After applying this process, a set of 76,368 boards was available for each size.\nTo complete the dataset, we generated a set of random nonograms because during several preliminary tests, we found that the size of the dataset was not sufficiently large to train the neural networks. We used two different techniques: we applied uniform noise between 0 and 1 to an empty board with a threshold of 0.5, and we drew a random number of simple geometric figures (lines, rectangles, and circles) with random sizes and positions on an image. One-third of this second dataset was generated by uniform noise and the remaining two-thirds by drawing simple figures. The dataset is available at: https://github.com/\njosebambu/NonoDataset.\nFinally, our dataset was composed of 33, 554, 432 samples of boards of size 5 x 5, and after joining the two sets, we obtained a large dataset that was composed of 76, 368 boards generated from images and 300,000 random boards of size 10 \u00d7 10, and 76,368 boards generated from images and 600,000 random boards of size 15 x 15."}, {"title": "3.2. Experimental environment", "content": "We used TensorFlow and Keras on different machines with a NVIDIA K40, NVIDIA RTX2060, and NVIDIA RTX3090 to train the neural network models."}, {"title": "3.3. Experiments", "content": ""}, {"title": "3.3.1. Experiment 1: Neural network training", "content": "The first part of the experiment consisted of evaluating the performance of the different neural networks that we designed using the dataset that we described previously. The results of this experiment were used to determine the best networks, which we used in the subsequent algorithms.\nTo perform this experiment, we designed a set of FC networks using three to eight hidden layers. The activation function for the intermediate layers was a ReLU and the function for the output layer was a sigmoid. We used the Adam [20] optimizer with different learning rates.\nTo train the neural networks, we used all boards of size 5\u00d75, and we used the same dataset to evaluate the network performance. We performed this simple experiment to verify the capabilities of the designed networks. To create the"}, {"title": "3.3.2. Experiment 2: Heuristic algorithms", "content": "We designed this experiment to evaluate the heuristic algorithm, which we used as a baseline, and its three variations that used a neural network. We tested them with and without reflections, and when enabling and disabling the intuition mechanism. Table 3.3.2 summarizes the different parts of the experiment.\nFor each board of size 10 \u00d7 10, we tested the algorithms described in Table 3.3.2 12 times using the validation dataset to obtain valid statistical measure- ments. For the boards of size 15 \u00d7 15, we used only a subset of 50 samples from the validation set.\nUsing this subset, we estimated the time required to solve the entire valida- tion dataset."}, {"title": "3.3.3. Experiment 3: Genetic Algorithm", "content": "Finally, we evaluated the GA with boards of size 10 \u00d7 10 using the following set of parameters: 100 iterations and a population of 1000 individuals. We did not evaluate this algorithm with boards of size 15 \u00d7 15 because a significant amount of time was required to obtain a result."}, {"title": "3.4. Metrics", "content": "From the early empirical results, we verified that the number of erroneous cells on the boards that were predicted by the tested neural networks followed a discrete Weibull distribution.\nGiven a neural network, let F be the random variable that represents the number of erroneous cells that are predicted on a board by a given network. Then, we have:\nP {F < f} = 1 \u2212 e\u00af(\\frac{f}{\u03b1})^\u03b2 , (2)\nwhere a and \u1e9e are the scale and shape parameters respectively, and f\u2208 {0, 1, 2, ..., \u03b7 \u00d7 n} is the number of erroneous cells. We can compare two neural networks by comparing the parameters a and \u1e9e of the Weibull distribution that best fit the experimentation results.\nThe shape value (3) of the neural networks with the best results was less than one, which implies that its mode was 0. Thus, from the neural networks with mode 0, we selected that with the smallest scale (a), which matched the neural network with a higher number of full correct boards (0 errors).\nWe used time-based metrics: the mean, standard deviation, and median of the time to solve a nonogram to evaluate the performance of the algorithms."}, {"title": "4. Results and discussion", "content": "In this section, we describe and discuss the results obtained from the three experiments."}, {"title": "4.1. Experiment 1: Neural networks", "content": "We correctly predicted 77.06% of boards of size 5 \u00d7 5. The best model architecture was composed of three layers of sizes 2048, 1024, and 256. After each layer, we added a dropout rate of 5% to avoid overfitting during the training phase, the learning factor was set to 0.001, and the error function was the binary cross-entropy as each cell can have only two values. The Weibull distribution parameters were as follows: shape (\u03b2) = 0.5091745 and scale (a) = 0.1102024.\nFor boards of sizes 10 \u00d7 10 and 15 \u00d7 15, the best model architecture was composed of five dense layers of sizes 2048, 1024, 1024, 1024, and 512. After each layer, we added a dropout of 5%. The learning factor was 0.0001 and the error function was binary cross-entropy. The Weibull parameters were as follows: shape (\u03b2) = 0.420651 and scale (a) = 3.401227 for the 10 \u00d7 10 boards, and shape (\u03b2) = 0.6219632 and scale (a) = 18.54031 for the 15 \u00d7 15 boards.\nBy applying this model, we correctly predicted 27.25% of boards of size 10 \u00d7 10 and 5.87% of boards of size 15 x 15.\nThe histogram of the number of errors on each board using the networks described above is depicted in Figure 8."}, {"title": "4.2. Experiment 2: Heuristic algorithms", "content": "Once we obtained the results of the neural networks, we selected the best models for each board size and evaluated the performance of the proposed al- gorithms using boards of size 10 \u00d7 10 and 15 \u00d7 15.\nAfter executing all experiments summarized in Table 3.3.2 12 times, as a first result, all algorithms solved all boards. To analyze the results in detail, we present a statistical summary of the execution time in Table 4.2 and the number of iterations required to solve the boards in Table 4.2.\nTo analyze the time required for each variation of the heuristic algorithm in depth, Figure 10 depicts a histogram that describes the execution times of the algorithms during the first two seconds as they concentrated almost all information. Following the same concept, Table 4.2 presents the percentiles for each algorithm.\nIt can be observed that the use of a neural network implied that the minimum time to solve a nonogram increased (from 0.001 to 0.0045 s) when we compare"}, {"title": "4.3. Experiment 3: Genetic Algorithm", "content": "We executed the genetic algorithm (GA) once on the test set of boards of size 10 x 10. The execution required 2 days, 18 hours, and 20 minutes, and it only managed to solve 15% of the boards, without taking into account the boards that the network predicted correctly. Using the data obtained from this experiment, we can conclude that the GA yielded worse results than the heuristic algorithm,"}, {"title": "4.4. Results summary", "content": "From the results obtained regarding the number of iterations and time re- quired for each algorithm, we can conclude that even if the neural network can- not solve all boards, it can provide a good starting point to solve a nonogram that saves considerable computation time. The combination of the heuristic algorithm and neural networks outperformed the heuristic algorithm, DFS, and GA."}, {"title": "5. Conclusions", "content": "In this study, we have presented an alternative to traditional algorithms designed to solve nonograms by adding neural networks to the decision process. Using the neural networks, it was possible to predict 27.25% of boards of size 10 x 10 and 5.87% of boards of size 15 \u00d7 15 correctly.\nThe best solution was the combination of the heuristic algorithm with a neu- ral network using reflections with partial and full erase, and intuition (Ne8HPFI). The GA exhibited the worst performance for the proposed task because it could not solve the entire set of boards.\nWe also created a dataset https://github.com/josebambu/NonoDataset, which we have opened to the scientific community, to train and test the different networks and algorithms. The project code is available at https://gitlab.\ncom/DLG-05/musi-tfm-nonograma."}, {"title": "5.1. Further work", "content": "Despite testing different fully connected network architectures to determine which was the best for predicting nonograms, we would like to evaluate other types of networks or other nonogram representations.\nFurthermore, we can design new fitness functions to determine whether the conclusions that we obtained in our experiment with the GA are useful for improving its results."}, {"title": "Likewise, for the variant of the algorithm that uses partial erase, a system could be proposed that would determine whether it is profitable to carry out partial deletion or to progress directly to complete deletion, thereby avoiding entering an invalid path and improving the overall system performance."}]}