{"title": "OpenSearch-SQL: Enhancing Text-to-SQL with Dynamic Few-shot and Consistency Alignment", "authors": ["Xiangjin Xie", "Guangwei Xu", "LingYan Zhao", "Ruijie Guo"], "abstract": "Although multi-agent collaborative Large Language Models (LLMs) have achieved significant breakthroughs in the Text-to-SQL task, their performance is still constrained by various factors. These factors include the incompleteness of the framework, failure to follow instructions, and model hallucination problems. To address these problems, we propose OpenSearch-SQL, which divides the Text-to-SQL task into four main modules: Preprocessing, Extraction, Generation, and Refinement, along with an Alignment module based on a consistency alignment mechanism. This architecture aligns the inputs and outputs of agents through the Alignment module, reducing failures in instruction following and hallucination. Additionally, we designed an intermediate language called SQL-Like and optimized the structured CoT based on SQL-Like. Meanwhile, we developed a dynamic few-shot strategy in the form of self-taught Query-CoT-SQL. These methods have significantly improved the performance of LLMs in the Text-to-SQL task.\nIn terms of model selection, we directly applied the base LLMs without any post-training, thereby simplifying the task chain and enhancing the framework's portability. Experimental results show that OpenSearch-SQL achieves an execution accuracy(EX) of 69.3% on the BIRD development set, 72.28% on the test set, and a reward-based validity efficiency score (R-VES) of 69.36%, with all three metrics ranking first at the time of submission. These results demonstrate the comprehensive advantages of the proposed method in both effectiveness and efficiency.", "sections": [{"title": "1 INTRODUCTION", "content": "Text-to-SQL task attempts to automatically generate Structured Query Language (SQL) queries from Natural Language Queries (NLQ). This task can improve the ability to access databases without the need for knowledge of SQL [17]. As the text-to-SQL problem is notoriously hard, these systems have been the holy grail of the database community for several decades [17]. Early work defined query answers as graph structures [14, 15] or based on syntactic structures for parsing the questions [16, 38]. Subsequent approaches have treated the Text-to-SQL task as a neural machine translation (NMT) problem [11, 37]. Recently, with the development of large language models, researchers have increasingly accomplished the task through methods such as the use of supervised fine-tuning (SFT) [19, 23, 35], Chain-of-Thought(CoT) [40], Agents [6, 27], and In-Context Learning [7, 36], achieving results that greatly surpass previous methods.\nLimitation. Although the methods driven by LLMs have signif- icantly raised the upper limits of the Text-to-SQL task capabilities, our analysis of previous work reveals that:\nL1 Due to the ambiguity of the overarching framework, there are some gaps at the methodological level. This has prevented these methods from reaching their potential. For example, there is a lack of verification of the stored information in the database, no error correction for the generated results, and issues related to the absence of few-shot learning.\nL2 LLM-driven approaches often rely on multi-agent collab- oration. However, due to the instability of LLMs and the lack of guaranteed coherence and coupling between agents, later-executing agents may not use or only partially use the outputs of previously running agents. This leads to accumu- lated hallucinations and performance loss.\nL3 The instructions and steps that guide the LLMs significantly affect the quality of the generated SQLs. In the pre-LLM era, methods employing intermediate languages to generate SQL were developed to address this issue [17], but current research on instruction construction remains insufficient.\nInspired by the aforementioned challenges, we conducted a de- tailed analysis of the human workflow in completing the Text-to- SQL task: understanding the database structure and selecting the"}, {"title": "2 PRELIMINARY", "content": "We first introduce the two important modules this paper relies on: Large Language Models (LLMs) and the Text-to-SQL task."}, {"title": "2.1 Large Language Models", "content": "Pre-trained large language models have acquired extensive lan- guage knowledge and semantic information through unsupervised learning on large-scale textual data. This pre-training typically em- ploys autoregressive architectures (such as GPT-like models) or autoencoding structures (such as BERT-like models). The LLMs can achieve excellent performance on various downstream tasks through fine-tuning. Since the advent of the deep learning era for large language models, there have been significant advancements in Text-to-SQL tasks compared to classical models and methods, with current SOTA-level work fully relying on the performance of LLMs."}, {"title": "2.2 Text-to-SQL", "content": "The Text-to-SQL task can be defined as: a task that translates a NLQ Q into an SQL query Y based on database information S, a specific model $F(\\cdot|\\Theta)$, and a particular prompt P. The information in the database is defined as S. Therefore, the Text-to-SQL task can"}, {"title": "2.3 Hallucination", "content": "Hallucination [46] is a common problem in deep learning, usually referring to situations where the content generated by a model does not align with reality or expected outcomes, i.e., the model produces inaccurate or unreasonable information. Textual hallucinations can manifest in the following forms:\nIrrelevant content: The text generated by the model is unre- lated to the input or context, or it includes irrelevant details.\nIncorrect facts: The generated text contains erroneous facts or information that do not exist or are inaccurate in the real world.\nLogical errors: The generated text is logically inconsistent or unreasonable.\nIn Text-to-SQL, this manifests as: results containing non-existent database information, failure to follow instructions in the prompt, and typographical and syntactic errors. Additionally, for conve- nience in handling, in the Text-to-SQL task, we also include the result deviations caused by randomness at non-zero temperatures and errors due to minor prompt changes. Because the essence of hal- lucination is the discrepancy between the training process and the actual use scenarios of the mode. Therefore, for hallucination prob- lems, common handling methods include: post-training specifically for hallucination problems, using the Self-Consistency mechanism to reduce randomness, and post-processing. Since this paper fo- cuses on optimizations at the architectural level, our work aims to reduce hallucinations through the last two methods mentioned above."}, {"title": "3 METHODOLOGY", "content": "In this section, we detail the specifics of OpenSearch-SQL. We pro- vide a more in-depth description of the core concepts of Alignments and dynamic few-shot before outlining the entire process. To ensure a smooth explanation, we integrate each stage with its correspond- ing Alignments and describe them in the order they operate within the OpenSearch-SQL framework."}, {"title": "3.1 Alignments", "content": "LLM hallucination [46] is a critical problem affecting the usability of LLMs, and it is also present in the Text-to-SQL task. Furthermore, the cumulative errors that arise from multi-agent collaboration exacerbate the hallucination problem in LLMs. For example, if an agent responsible for the Extraction function selects columns from the database in a certain way and generates incorrect column names due to model hallucination, this error will persist in the subsequent Generation, as it lacks knowledge of the correct database structure. Consequently, the hallucination produced by the previous agent is inherited, making it difficult for hallucinations to spontaneously disappear in a multi-agent LLM workflow; the total amount of hallucination is almost monotonically non-decreasing.\nBased on this phenomenon, we define the Alignment Agent as :\n$A_{Aligment}(x + A'(x)) = A(x) - A'(x),$\n(2)\nThe input for the agent is the input x for the agent to be aligned, along with $A'(x)$. The output is $A_{Aligment}(x + A'(x))$, which is obtained through optimization by a large model and rules. Empirical observations suggest that the difference between $A' (x)$ and $A(x)$ is mainly due to hallucinations in the model. Therefore, the function of Alignment is to align the current Agent's input information with its output result, so that this alignment can reduce the hallucinations between the expected output and the actual output.\nIn the Text-to-SQL task, common hallucination phenomena mainly refer to failures in instruction following and instability in output results. This is specifically manifested as: generating non-existent columns, changing database column names, syntax errors, mis- matching database values with columns, and failing to adhere to the rules set in the prompt. To address these problems, we propose a consistency alignment method: after each agent completes its out- put, an Alignment agent is used to align the current agent's output with that of the upstream agent, ensuring that the functionalities of the various agents achieve logical consistency, and the aligned result is passed on to the downstream agent. This mechanism is somewhat analogous to residual connections [13], effectively ex- tending the chain of collaboration among multiple agents while minimizing the introduction of hallucinations. The details of each Alignment will be presented in detail in the following Agent intro- duction."}, {"title": "3.2 Self-Taught Fewshot", "content": "Few-shot is an important method to assist LLMs in generation, MCS-SQL[18], DAIL-SQL[7] studied the critical role of problem representation in the Text-to-SQL task and proposed using ques- tion similarity to select appropriate few-shots to drive LLMs in generating SQL, achieving notable results.\nInspired by this, we attempted to use dynamic few-shots to en- hance the efficiency of agents in the Text-to-SQL task. Additionally, we considered how to better leverage few-shots by extracting more information from the samples. Therefore, in OpenSearch-SQL, we first used Masked Question Similarity (MQs) [10] to select sim- ilar queries. Then, we upgraded the few-shots in the Query-SQL format by self-taught. For the Query-SQL pairs shown in Listing 1, we used an LLM to supplement the CoT information to trans- form the NLQ into SQL. As shown in Listing 2, this resulted in Query-CoT-SQL pairs containing logical information. Compared to simple Query-SQL pairs, these self-taught few-shots provide richer information.\nThen, for the error correction of the Refinement, we prepared different few-shots for various error types, enabling LLMs to more clearly understand how to correct SQL based on different errors dur- ing Refinement. The format of the few-shots is shown in Listing 3, the specific errors in the Raw SQL and the Advice of correction correspond to the error type."}, {"title": "3.3 Preprocessing", "content": "In the Preprocessing, we constructed the database based on its true structure. Additionally, to ensure that the SQL generated by LLMs aligns with the actual state of the database, we indexed the values within the database. This allows the SQL to avoid errors that might arise from small character discrepancies. It's worth noting that we index only string-type data to save the space required for building the retrieval database."}, {"title": "3.4 Extraction", "content": "The goal of Extraction is to prepare the necessary information based on specific NLQs, including schema linking, stored database values, few-shot examples, and any required instructions. This part is decoupled from the specific database query language and is only related to the format of data storage and the auxiliary information needed to solve the problem. For instance, C3-SQL [6] utilizes Clear Prompting (CP) to provide effective prompts, while DIN-SQL[27] employs schema linking and classification & decomposition to cat- egorize and break down the NLQ.\nThe Extraction process in OpenSearch-SQL consists of entity extraction, value extraction, and column filtering. After Extraction, we use Info Alignment to align the extracted information with the input to produce the final output. We present the format of the extraction prompt and its input and output in the listing 4. We obtain candidate columns and values through LLM and directly extract entities from the NLQ using another simple prompt. The details are outlined below:\nEntity Extraction. To find similar values in the database and perform column filtering, we first use the LLM understand and process the NLQ and basic database information to extract potential entities. Then, we organize some predefined entity terms together with these entities to perform the subsequent values retrieval and column filtering.\nValues Retrieval. From the extracted entities, we perform vec- tor retrieval to find results with the highest embedding similarity to the entity words. Due to the nature of embedding similarity, this method can prevent mismatches caused by typos or other character-level differences. Additionally, for phrases and longer texts, we perform split retrieval to avoid recall failures due to dif- ferences in database storage formats. During the recall phase, we"}, {"title": "3.5 Generation", "content": "The definition of Generation is: using appropriate methods to drive LLMs to generate specific components and complete SQL. MAC- SQL [36] generates sub-SQLs and then assembles them into the final result, while DAIL-SQL [7] drives LLMs to generate SQL through few-shot learning. Additionally, some methods [18, 23, 34] utilize LLMs during Supervised Fine-Tuning (SFT) or generate multiple sets of SQL to complete tasks.\nSQL Generation. As shown in Listing 5, we chose to use the progressive generation + dynamic few-shot to better drive LLMs in generating SQL. Specifically, progressive generation is a customized CoT approach, the format is the same as Listing 2. We first defined SQL-Like: a type of SQL language that ignores specific syntactical elements (such as JOINs and the formatting of functions), aiming to encourage LLMs to focus more on the logic within SQL rather than the formatting. We then instruct the model to sequentially provide the analysis, the contents of the SELECT statement, relevant columns and values, as well as the SQL-Like and SQL outputs.\nThis progressive generation emphasizes the LLMs' understanding of the structure of the generated SQL, while also faciliting the iden- tification of errors in the LLMs' reasoning. For individual NLQs, we allow LLMs to generate multiple candidate SQL queries. For the construction of few-shot examples, the system retrieves the top Kf most similar queries based on MQs, and uses their correspond- ing Query-CoT-SQL forms as the few-shot examples in the final instruction.\nListing 5 demonstrates the composition of inputs and outputs during the generation phase through an example."}, {"title": "3.6 Refinement", "content": "The work of Refinement involves optimizing and selecting the generated SQLs. This includes correcting errors based on execution results, and selecting the best answer from multiple candidate SQLs. Due to the variability of results generated by LLMs and the frequent issues with instruction adherence in complex tasks, we"}, {"title": "3.7 Algorithm", "content": "In this section, we provide an overall summary of OpenSearch-SQL and present the entire framework in Algorithm 1. In this algorithm, we take an NLQ as an example, encompassing each specific step from preprocessing to the final generation of SQL.\nIt can be seen that we first process the database table by table, field by field, during preprocessing to build an index database Do of values and columns' names, while also generating the original schema S of the database. Next, for the Query-SQL pairs in the training set, we use ML to generate CoT information to construct the new few-shot set F. In the main function, we first process the NLQ Q using a LLM. Through AE we obtain the entity information EQ and select the column t.co. Then, we retrieve similar values voand expand the database information t.c\u0118. These details are in- tegrated through Alignment Aa to create a schema So tailored for Q. The next step is to obtain the few-shot fo in Query-CoT-SQL format relevant to the question based on MQs (Q). Next, SQLs are generated through AG and aligned using Aa to obtain SQLSA. Fi- nally, execution errors are corrected with AR, and the SQL with the highest consistency and shortest execution time, SQLR, is selected as the final answer."}, {"title": "3.8 Optimization", "content": "OpenSearch-SQL has achieved significant results, yet there is still plenty of room for optimization. Currently, we haven't fully focused on the detailed adjustments of prompts and the precise selection of columns and values in OpenSearch-SQL. Moreover, in the gen- eration tasks, we only use a single prompt as the instruction for the LLM to generate a SQL candidate set, without additional opti- mization. On the other hand, the SFT model shows great potential in improving Text-to-SQL performance. Research such as CHESS [34], MCS-SQL [18] and distillery [23] has already demonstrated"}, {"title": "4 EXPERIMENTS", "content": "In this section, we demonstrate the effectiveness of OpenSearch- SQL through experiments and explore the role of its various module."}, {"title": "4.1 Experimental Setup", "content": "Datasets. We show the characteristics of these two datasets in Table 1. Considering the specific queries and SQL complexity of the datasets. Bird has relatively fewer types of databases but with more complex database structures and a higher average difficulty of SQL. In contrast, Spider has a rich variety of databases, but the average difficulty of SQL is relatively lower.\nBIRD. [20] (Big Bench for Large-scale Database Grounded Text- to-SQL Evaluation) represents a pioneering, cross-domain dataset that examines the impact of extensive database contents on text-to- SQL parsing. BIRD contains over 12,751 unique question-SQL pairs, 95 big databases with a total size of 33.4 GB. It covers more than 37"}, {"title": "4.2 Main Result", "content": "To demonstrate the effectiveness of OpenSearch-SQL, our perfor- mance on the Spider and BIRD datasets has achieved state-of-the-art (SOTA) levels, highlighting the stability and superiority of our ap- proach. In Table 3, we compare the effectiveness of our method with the baseline methods in the BIRD dataset. Furthermore, Table 2 presents the experimental results on the Spider dataset. Since the Spider leaderboard will no longer accept new submissions starting February 2024, the results are composed of current leaderboard data and those reported in relevant papers. Notably, the BIRD dataset underwent a cleansing of the devlopment set in July, which might lead to discrepancies in comparisons. To ensure comparability with previous evaluations, we have continued to use the dataset before July for our experiments.\nBIRD Results. We present the performance of Baseline and OpenSearch-SQL, v2 on the BIRD dataset in Table 2. The results in the table are derived entirely from the BIRD leaderboard. At the time of submission, our method achieved 69.3% EX on the BIRD development set and 72.28% EX and 69.36% R-VES on the BIRD holdout test set, all ranking first. It is noteworthy that our method did not involve any fine-tuning to customize the LLM. Meanwhile, we demonstrated the performance of OpenSearch-SQL without applying Self-Consistency & Vote. It can be seen that even using a single generation SQL, OpenSearch-SQL achieved an EX of 67.8, still maintains the top performance on the development set.\nIn Figure 3, we compare the performance of our method across various difficulty levels. It is evident that Self-Consistency & Vote shows the most significant improvement for difficult problems, achieving an absolute difference of 7.64%. However, there is no no- table difference for easy and medium problems. This indicates that as problem difficulty increases, the susceptibility of large models to hallucinations also rises.\nSpider Results. To demonstrate the generalizability of OpenSearch- SQL, we also evaluated its performance on the Spider test set. We used the default configuration of our method, making no other changes, except for necessary adjustments to accommodate the data set format. Since the Spider leaderboard is no longer updated and the test set is publicly available, we extracted results from the baseline method's paper and leaderboard, with results extracted from the paper marked with a *. Table 3 shows the performance of"}, {"title": "4.3 Modular Ablation", "content": "Table 4 provides a detailed overview of the Execution Accuracy (EX) when each module is removed from the method. We specifically examine the roles of the Extraction, Generation, Refinement,"}, {"title": "4.4 Few-shot", "content": "In Table 5, we explore the impact of different dynamic Few-shot strategies on Text-to-SQL. First, during the generation phase, we compare the Few-shot effects of the classic Query-SQL pair with our Query-COT-SQL pair, and we evaluate the scenario without Few-shot. We also investigate the influence of Few-shot in other stages. From the table, we can summarize the following points:\nThe Few-shot strategy significantly raises the upper limit of SQL accuracy generated by LLMs. Whether in the form of a Query-SQL Pair or a Query-CoT-SQL Pair, compared to without few-shot, these formats significantly enhance the performance of LLMs at each stage. However, the improve- ment from few-shot in the Refinement stage is relatively smaller.\nDuring the Generation, for a single SQL query, the Few-shot strategy provides the greatest performance improvement, with the Query-CoT-SQL pair form achieving a 3.0% absolute EX increase compared to the Query-SQL pair form, and a 6.4% increase in EX compared to the scenario without Few- shot.\nIn Refine and under the influence of self-consistency & vote, the gap between different forms of Few-shot gradually narrows, indicating that there is some redundancy in the SQL optimization content in different stages.\nAlthough the Few-shot strategy has limited optimization effects on individual SQL queries during the Refine, it can"}, {"title": "4.5 Self-Consistency & Vote", "content": "In our result, we used a single prompt to generate 21 results as answers. To investigate the optimal number of SQL outputs gener- ated by beam search for self-consistency with a single prompt, we designed the experiment shown in the figure 4."}, {"title": "4.6 Execution Cost", "content": "In Table 6, we provide a simple estimate of the time and resource consumption for using OpenSearch-SQL with GPT-40. Generally, the execution time for the entire process primarily depends on large language model (LLM) calls, retrieval time, and SQL execution time. In practice, using HNSW [24] can significantly reduce retrieval latency, thus shifting the main time consumption to LLM responses and SQL execution.\nSpecifically, the tokens for extraction and generation are mainly derived from the database schema and a few examples. The genera- tion phase takes longer since it uses BeamSearch, which generates more tokens than the extraction phase. Apart from the SELECT alignment, which is triggered every time, other alignment steps are only activated when there are issues with the SQL, so they are not time-consuming in most cases. As for the refinement phase, the time is mainly due to SQL execution and LLM corrections, while Self-consistency & Vote contribute minimally to the time consump- tion."}, {"title": "4.7 CoT", "content": "In this section, we provide a brief analysis of the CoT method we designed and compare the performance of structured CoT de- signed for OpenSearch-SQL with the unstructured CoT approach( i.e., \"let's think step by step\") as well as scenarios without CoT. To"}, {"title": "5 RELATED WORK", "content": "For a long time, computer professionals have been hoping to convert natural language queries into specific database queries according to certain scenarios, in order to dramatically reduce the learning and usage costs for individuals, bridge the gap between laypersons and expert users, and enhance the efficiency of using, analyzing, and querying databases. The Text-to-SQL task is one of the most repre- sentative examples of these, and success in the Text-to-SQL task"}, {"title": "5.1 Classic Method", "content": "Classical methods often do not directly generate SQL but assemble it through various means. These methods handle components like SELECT, WHERE, etc., separately to construct SQL clauses. An example of such a method is SQLNET[42], which employs a sketch- based approach where the sketch contains a dependency graph, allowing one prediction to take into account only the previous pre- dictions it depends on. RYANSQL[4] and SyntaxSQLNet[43] have tried to generalize sketch-based decoding, attempting not only to fill in the gaps of a sketch but also to generate the appropriate sketch for a given text. Additionally, there are methods that utilize graph representation [2, 3] and employ intermediate languages [12, 31] to enhance the capabilities of LLMs. These methods may be con- strained by the inherent capabilities of the models, which may lead to inferior performance. For the Text-to-SQL task, their theoretical value outweighs their practical value."}, {"title": "5.2 Large Language Models", "content": "Large Language Models (LLMs) represent a significant breakthrough in the field of artificial intelligence in recent years. They are based on deep learning technology, typically built on the Transformer architecture. These models consist of large-scale neural networks capable of training on massive amounts of text data, thereby en- dowing them with the powerful ability to generate and understand natural language text.\nThroughout the development of LLMs, several notable models have emerged, the most famous being GPT (Generative Pre-trained Transformer) [30] and BERT (Bidirectional Encoder Representations from Transformers) [5]. GPT excels in natural language generation, while BERT is exceptional in understanding natural language. These models are widely applied across various fields, such as text gener- ation, information extraction, and machine translation, effectively advancing natural language processing.\nRecently, more advanced models have emerged, such as GPT-4 [26], LLAMA [25], Qwenmax [1], and DeepSeekV2.5 [21], which features a mixture of experts (MoE) architecture. These emerging models can be regarded as significant milestones in the fields of natural language processing and machine learning due to their outstanding performance in a variety of downstream tasks. Thanks to the excellent capabilities of these models, natural language pro- cessing now plays an increasingly important role in addressing many complex and meaningful issues. As technology continues to advance, large language models are expected to have a profound impact on scientific research, commercial applications, and daily life worldwide."}, {"title": "5.3 LLM-based Agents and RAG", "content": "With the advent of the era of large models, the enhanced capabili- ties of large models have simplified many previously difficult tasks. The LLM-based agent is an approach that leverages the power of"}, {"title": "5.4 LLM for Text-to-SQL", "content": "Recent research shows that the most effective Text-to-SQL methods primarily rely on multi-Agent algorithms based on LLMs. These methods aim to provide the most helpful information to the LLM through Agents to assist in predicting SQL. In this process, the classic Text-to-SQL task benchmarks have gradually evolved and become more complex, with commonly used Text-to-SQL evalua- tion tasks progressively shifting from WiKiSQL[48] to Spider[44] and BIRD [20]. In these tasks, researchers [33] enhance the LLM's Text-to-SQL abilities through chain-of-thought reasoning. C3[6] has developed a systematic zero-shot Text-to-SQL method that includes several key components. DIN-SQL [27] addresses the Text-to-SQL task by distinguishing question difficulty and adjusting specific prompts accordingly. DAIL-SQL [7] assists the model in generating SQL by retrieving structurally similar questions or SQL as few-shot examples, while MAC-SQL improves capabilities through task de- composition and recombination. Meanwhile, some studies have opted for a fine-tuning + Agent approach to accomplish the Text- to-SQL task, such as Dubo-SQL[35], SFT CodeS[19], DTS-SQL[28]. Notably powerful methods currently include MCS-SQL[18], which enhances the stability of Text-to-SQL tasks by using multiple sets of prompts for voting. CHESS [34], which simplifies the generation difficulty through an excellent column filtering mechanism, and Distillery [23], which optimizes the task by fine-tuning GPT-40.\nOverall, as the capabilities of large models continue to improve, the framework for the Text-to-SQL task has gradually converged and standardized into the form of LLMs + Agents. Meanwhile, the capabilities of Text-to-SQL methods are also constantly enhanc- ing, becoming tools that can truly help simplify people's work in everyday applications."}, {"title": "6 CONCLUSION", "content": "In this paper, we introduce OpenSearch-SQL, a method that en- hances the performance of the Text-to-SQL task using dynamic Few-shot and consistency alignment mechanisms. To improve the model's handling of prompts, we extend the original examples with a LLM and supplement them with CoT information, forming a Query-CoT-SQL Few-shot configuration. To the best of our knowl- edge, this is the first exploration of using LLMs to extend Few-shot CoT content in the Text-to-SQL task. Additionally, we developed a novel method based on a consistency alignment mechanism to mitigate hallucinations, enhancing the quality of agent outputs by reintegrating their inputs and outputs. Our approach does not rely on any SFT tasks and is entirely based on directly available LLMs and retrieval models, representing a pure architecture upgrade for Text-to-SQL. As a result, at the time of submission, we achieved the"}]}