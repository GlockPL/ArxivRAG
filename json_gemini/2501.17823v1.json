{"title": "U2A: Unified Unimodal Adaptation for Robust and Efficient Multimodal Learning", "authors": ["Md Kaykobad Reza", "Niki Nezakati", "Ameya Patil", "Mashhour Solh", "M. Salman Asif"], "abstract": "Multimodal learning often relies on designing new models and complex training strategies to achieve optimal performance. We present Unified Unimodal Adaptation (U2A), which jointly fine-tunes pretrained unimodal encoders using low-rank adaptation (LoRA) for various multimodal tasks. Our method significantly reduces the number of learnable parameters and eliminates the need for complex training strategies, such as alternating training, gradient modifications, or unimodal fine-tuning. To address missing modalities during both training and testing, we introduce Mask Tokens (MT), which generate missing modality features from available modalities using a single token per modality. This simplifies the process, removing the need for specialized feature estimation or prompt-tuning methods. Our evaluation demonstrates that U2A matches or outperforms state-of-the-art methods in both complete and missing modality settings, showcasing strong performance and robustness across various modalities, tasks, and datasets. We also analyze and report the effectiveness of Mask Tokens in different missing modality scenarios. Overall, our method provides a robust, flexible, and efficient solution for multimodal learning, with minimal computational overhead.", "sections": [{"title": "1. Introduction", "content": "Multimodal learning [1, 2] combines information from different input sources to enhance performance on downstream tasks. A majority of existing methods assume well-curated datasets in which all modalities are available for every sample. In real-world applications, input modalities can be missing during training and/or testing, and both cases can severely degrade model performance [3]. The modalities can be missing due to sensor failure, privacy concerns, or lack of synchronous data capture. In this paper, we propose a method that maintains strong performance in both complete and missing modality scenarios with limited computational overhead.\nTo integrate information from different input sources, existing methods focus on early-stage fusion of modalities [4], token-level fusion [5], channel exchange [6], and bottleneck mid-fusion [7]. Additionally, specialized architectures have been proposed for specific multimodal tasks, such as vision-language tasks [8-10], action recognition [11, 12], and segmentation [13, 14]. Designing specialized, task-specific architectures for every multimodal scenario is impractical due to the vast range of potential tasks. Instead, a preferred approach is to develop models and fusion strategies that can maintain performance and efficiency while being adaptable to diverse settings.\nModels trained with multiple input modalities exhibit a drastic performance drop when one or more modalities are missing [3]. Several approaches have been proposed to enhance robustness against missing modalities. These include robust training techniques [15, 16], knowledge distillation methods [17-19], modality masking strategies [20, 21], generative networks [22, 23] to impute missing modalities, and multimodal prompting [24-26]. While effective, these methods involve learning a large number of parameters or prompts across multiple layers of the network, which increases computational and memory complexity.\nIn this paper, we propose Unified Unimodal Adaptation (U2A), a method that leverages pretrained unimodal encoders for multimodal learning. Our method encodes each modality separately through pretrained transformer encoders. To handle missing modalities, we introduce mask tokens to estimate the missing modality features from available input modalities. We introduce a modality alignment loss to learn the mask tokens that can effectively substitute for the class tokens of the missing modality. Our approach jointly optimizes this alignment loss with task-specific objectives through random modality dropout and low-rank adaptation (LoRA [27]) to learn cross-modal relationships while keeping computational costs low. Our method requires significantly fewer learnable parameters compared to state-of-the-art (SOTA) methods, while consistently delivering superior performance. We performed extensive experiments on five datasets, comparing our approach to existing baseline methods. The results show that our method outperforms SOTA methods in both complete (Section 4.3) and missing modality (Section 4.4) scenarios while requiring a small number of learnable parameters. Ablation studies confirm that our method can accurately estimate missing modality features, leading to improved performance in missing modality scenarios (Section 4.5). The main contributions of our paper can be summarized as follows.\n\u2022 We introduce Unified Unimodal Adaptation (U2A), which outperforms most of the existing methods in various multimodal tasks, while requiring fewer learnable parameters (Section 4.3).\n\u2022 We propose to learn Mask Tokens, which efficiently represent cross-modal relationships and infer missing modality features from available modalities, improving robustness in missing modality scenarios (Section 4.4).\n\u2022 We present extensive experiments that demonstrate our approach outperforms existing methods across multiple datasets and achieves strong performance in both complete and missing modality scenarios with fewer learnable parameters."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Multimodal Learning", "content": "Designing specialized models to fuse multiple input sources effectively is a common practice in multimodal learning. For vision-language tasks, models like ViLT [8], ViLBERT [10], and Visual-BERT [9] process image patches and text tokens directly within a transformer model for tasks like image captioning and visual question answering. MMBT [28] integrates text and images within a BERT-based structure [29] to model cross-modal dependencies. MBT [30] uses fusion bottlenecks for efficient modality-specific interaction for audio-video tasks. A number of multimodal models are also proposed for other tasks like action recognition [11, 12], segmentation [5, 13] and sentiment analysis [11, 31]. While effective for specific applications, extending these models to new tasks or modalities remains non-trivial and resource-intensive.\nAdapting existing models is another approach for multimodal learning. One such approach is gradient modulation. G-Blend [32] modulates gradients to combine unimodal and multimodal knowledge. OGM-GE [33] refines this method by emphasizing on unique information from each modality. PMR [34] modulates parameter groups based on modality relevance, while QMF [35] applies quadratic modulation for adaptive fusion. More recently, MLA [36] proposed alternating adaptation, which switches between unimodal adaptation and gradient modulation, enabling effective cross-modal learning. MMLORA [37], on the other hand, uses unimodal fine-tuning followed by multimodal adaptation to achieve optimal performance. However, these approaches often require learning a large number of parameters to achieve optimal performance on downstream tasks."}, {"title": "2.2. Missing Modality Robustness", "content": "Recent works [3, 38] show that performance can degrade significantly when modalities are missing, as multimodal models are not inherently robust to such scenarios. To address this issue, several approaches have been proposed.\nRobust model design is one of the approaches for handling missing modalities. Recently, Shin et al. [21] designed a robust framework utilizing modality masking and knowledge distillation. Wang et al. [39] proposed ShaSpec to impute missing features through modality-specific and shared feature learning. Wang et al. [40] proposed Token-Fusion, which dynamically replaces uninformative tokens to enhance robustness. Additionally, general-purpose fusion strategies [41-43] aim to improve robustness across various missing-modality scenarios. However, many of these approaches are task-specific, limiting their generalization to other multimodal tasks.\nRobust training and model adaptation can enhance missing modality robustness. Recently Reza et al. [44] proposed a parameter-efficient adaptation framework for different multimodal tasks. Nezakati et al. [45] trained a single model to infer missing information by projecting available modalities. Ma et al. [46] introduced SMIL, a Bayesian meta-learning method for flexible handling of missing modalities during both training and testing. Ma et al. [3] proposed a multi-task optimization approach with a differentiable algorithm for optimal fusion strategy selection. Maheshwari et al. [18] developed a semi-supervised framework that leverages unlabeled data to improve robustness, while Wei et al. [47] used a teacher network to transfer multimodal information for improved missing modality robustness. These methods, however, either require extensive adaptation or specialized training procedures.\nPrompt tuning methods are also used to compensate for missing modalities. Lee et al. [24] introduced missing-aware prompts that learns a separate set of prompts for each modality combination. Later, Jang et al. [25] simplified the approach, showing that a single set of prompts per modality can achieve similar performance. More recently, Dai et al. [26] proposed a multi-step adaptive prompt learning method through modality alignment, while Kim and Kim [48] introduced a read-only prompt tuning approach. Although effective, these methods require learning a large number of prompts for each missing scenario.\nIn contrast, our method utilizes pretrained unimodal encoders, fine-tuning them jointly on multimodal data. To handle missing modalities, we introduce mask tokens that estimate missing modality features from the available ones. A modality alignment loss is jointly optimized with task-specific objectives using random modality dropout and LORA to effectively learn cross-modal relationships. This design ensures strong performance in both complete and missing modality scenarios while offering the flexibility to add new modalities with minimal computational cost. Our approach is efficient, uses few learnable parameters, and is robust to missing modalities in both training and testing."}, {"title": "3. U2A: Unified Unimodal Adaptation", "content": "Our proposed unified unimodal adaptation (U2A) approach can accommodate complete and missing modalities during training and testing. Our goal is to develop a simple and efficient learning framework that can be used for different multimodal tasks without requiring specialized (task-dependent) changes. In this vein, we propose to adapt pretrained unimodal encoders for each modality and introduce mask tokens to estimate missing modality features from available modalities. Figure 1 illustrates main components of our approach that we describe in detail below.\nMultimodal data embedding. We consider a multimodal dataset containing two modalities M = {M\u2081,M\u2082}. Let us denote the available dataset as D = {Dc,Dm1, Dm2}, which can be further divided into three subsets: Dc = {xm1,xm2,y} with inputs from both modalities (xm1, xm2) and output labels y; Dm\u2081 = {xm1, \u00d8, y} with inputs only from modality m\u2081 (xm\u2081) and labels y; Dm2 = {\u00d8, xm2,y} with inputs only from modality m\u2082 (xm2) and output labels y. \u00d8 denotes a placeholder for missing modality (e.g., empty string for texts, zeros for images, audios, and videos) [24, 44].\nFrozen embedding layers. For each input modality, we assume that a transformer-based encoder pretrained on some large dataset is available. Specifically, we use a pretrained modality-specific EmbeddingLayer\u2098 with parameters \u0398\u2091,\u2098. Each input modality x\u2098 passes through the corresponding embedding layer to generate modality specific tokens as\nTm = EmbeddingLayerm(xm; \u0398e,m), (1)\nwhere Tm \u2208 \u211d\u1d3a\u02e3\u1d48 denotes N tokens, each of dimension d, for modality m. During training, we keep the pretrained parameters \u0398\u2091,\u2098 frozen.\nAdaptation of unimodal encoders. For each modality, we assume that a pretrained transformer encoder f\u2098 with parameters \u0398f,\u2098 is available. To adapt each encoder, we insert LoRA [27] layers with learnable parameters \u2206f,\u2098. We concatenate one class token TcLs,m \u2208 \u211d\u00b9\u02e3\u1d48 and one Mask Token (MT) TMT,m \u2208 \u211d\u00b9\u02e3\u1d48 with Tm to generate the updated tokens as\nTm = Concat({TCLS,m, Tm, TMT,m}), (2)\nwhere Tm \u2208 \u211d\u207d\u1d3a\u207a\u00b2\u207e\u02e3\u1d48. The updated tokens are passed through the corresponding modality specific encoder f\u2098 to extract the feature tokens as\n\u00cem = fm(Tm; \u0398f,m, \u2206f,m), (3)\nwhere \u00cem \u2208 \u211d\u207d\u1d3a\u207a\u00b2\u207e\u02e3\u1d48 represents the tokens/hidden-states from the last encoder layer. During training, we keep the pretrained parameters \u0398f,\u2098 frozen and only update the small number of learnable parameters Af,m\u00b7\nToken selection. After extracting the feature tokens Tm, we take the class token \u00cecLs,m and the mask token \u00ceMT,m for both modalities and pass them to a token selection block. If both modalities are available, the token selection block returns the class tokens from each modality. If any modality is missing, the block replaces the class token of the missing modality with the mask token from the available modality. We can define the selected tokens T\u2081, T\u2082 \u2208 \u211d\u00b9\u02e3\u1d48 as\n(T1, T2) = \n{\n  (\u00ceCLS,m1, \u00ceCLS,m2),  m1, m2 are available,\n  (\u00ceMT, m2, \u00ceCLS,m2),  m\u2081 is missing,\n  (\u00ceCLS,m1, \u00ceMT,M1),   m2 is missing.\n}(4)\nFusion and predictions. Finally, we sum up the selected tokens to generate the fused feature and pass it through the linear classifier head to make prediction \u0177 as\n\u0177 = h(T\u2081 + T\u2082; \u0398h). (5)\n\u0398h denotes all the parameters of the classifier head that we learn during training.\nMask tokens alignment. To accurately estimate the feature of a missing modality using the mask token of the available"}, {"title": "4. Experimental Results", "content": ""}, {"title": "4.1. Datasets", "content": "We evaluate our approach on five popular multimodal datasets across different tasks. A brief description of each dataset is provided here, with further details in Section 1 in the supplementary.\nUPMC Food-101 [49] is a multimodal classification dataset consisting of 101 food classes. It has a total of 90,704 image-text pairs, divided into a training set of 67,988 pairs and a test set of 22,716 pairs.\nMM-IMDb dataset [50] contains 25,959 samples with image and text modalities for multi-label movie genre classification. It is split into 15,552 training, 2,608 validation, and 7,799 test samples across 23 classes.\nKinetics-Sound (KS) [51] is a subset of Kinetics400 dataset [52] for human action recognition using audio and video as input modalities. It has 31 classes, with 14,739 samples in the training set and 2,594 samples in the test set.\nAudio-Visual Event (AVE) localization dataset [53] is used for multimodal event localization. It includes 4,143 10-second videos divided into train/val/test splits containing 3,339/402/402 videos, respectively. The dataset spans 28 event categories.\nCREMA-D dataset [54] includes 7,442 short video clips from 91 actors expressing six emotion categories. It contains 6,698 samples for training and 744 for testing. The dataset is used for multimodal emotion recognition."}, {"title": "4.2. Implementation Details", "content": "Vision-Language datasets. We use pretrained ViT-B [55] from CLIP model [56] as image encoder and BERT-base-uncased model [10] as text encoder. The max token length is set to 40 for UPMC Food-101 and 256 for MM-IMDb dataset, respectively.\nAudio-Video datasets. We use AST model [57] pretrained on AudioSet dataset [58] as audio encoder and ViT-B [55] from CLIP model [56] as video encoder. Raw audio is converted to a 128-bin mel spectrogram using a 16 kHz sampling rate, with a maximum length of 1024 frames. For video, we sample 3 frames randomly while training and uniformly while testing. Other pre-processing steps are similar to [33, 36, 37].\nHyperparameter settings. We use Python 3.8.19 and Py-Torch 2.2.2 for training and evaluating our models. All the models are trained using two NVIDIA RTX 2080Ti GPUs. For vision-language datasets, we set the learning rate to 10\u207b\u00b3 and train the models for 10 epochs with a batch size of 8. For audio-video datasets, the learning rate is set to 5\u00d710\u207b\u2075 and models are trained for 100 epochs with a batch size of 4. We use AdamW [59] optimizer with \u2208 = 10\u207b\u2078 and weight decay = 0.02. Cross entropy loss is applied while training the models. We utilize a polynomial learning rate scheduler with power=0.9 and set the first 5 epoch as warm-up. The learning rate is set to 0.1 times the original learning rate during warm-up epochs. We set LoRA [27] rank = 1 and insert them after query, key, value and output layers of each transformer block. Further details with all the hyperparameters can be found in Section 2 in the supplementary.\nFor each task and dataset, we compare against existing works when published results are available. Due to this criterion, the set of baselines varies across experiments based on the availability of published results. In each table, the best and second best results are bold and underlined, respectively."}, {"title": "4.3. Complete Modality Performance", "content": "We train and evaluate our method in the modality-complete scenario, where both modalities are available during training and testing. We compare our results with (1) baseline multimodal models: MBT [30], MMBT [28], ViLT [8]; (2) prompting based methods: PMF [60]; (3) modulation and gating based methods: FiLM [61] and Bilinear-Gated (BiGated) [62] (4) model finetuning based approach: MM-LORA [37] and (5) gradient modulation based methods: G-Blend [32], OGM-GE [33], MLA [36].\nAs summarized in Table 1, our approach outperforms all the baseline methods in both UPMC Food-101 and MM-IMDb datasets. Similarly, our method shows comparable or better performance in all the audio-video datasets as shown in Table 2. MMLORA [37] shows close performance to our method and also bears some similarities and key differences. MMLORA first finetunes the unimodal models on unimodal data, followed by a multimodal adaptation on complete multimodal data. In contrast, our approach freezes the encoders pretrained on unimodal data and learns only the low-rank adapters, which reduces the total number of learnable parameters significantly while achieving superior performance in most of the datasets. Furthermore, our method achieves robust performance for missing modalities, as we discuss in the next section."}, {"title": "4.4. Missing Modality Performance", "content": "We consider the following two general cases when evaluating our approach in missing modality scenarios."}, {"title": "4.4.1. Missing modality during both training and testing", "content": "We follow the experimental setup from [24, 25] and train and test our models with same modality ratio. Let \u03b7 denote the percentage of samples with missing modalities. For bimodal vision-language tasks, we consider three scenarios: missing-text, missing-image, and missing-both. For the first two cases, \u03b7% of the samples contain only images or only texts, while (1 \u2013 \u03b7)% contain image-text pairs. For the missing-both case, we have 7% text-only, 7% image-only, and (1 \u2013 \u03b7)% complete pairs. We follow [24-26] and set \u03b7 = 70 throughout our experiments .\nWe compare our method with (1) baseline multimodal models: ViLT [8] and Visual-BERT [9]; (2) multitask and optimal fusion based method by Ma et al. [3] and (3) prompting based methods: missing-aware prompts (MAP) [24], modality-specific prompts (MSP) [25], multi-step adaptive prompt (MuAP) [26] and method proposed by Kim and Kim [48].\nTable 3 shows the results obtained using a consistent missing modality setup during both training and testing. To ensure robust evaluation, we report the mean and standard deviation of three independent runs with different seeds. Our approach outperforms existing baseline methods on both UPMC Food-101 and MM-IMDb datasets across all compared scenarios. These results suggest that the mask tokens can provide good estimates of the missing modality features, which improves missing modality robustness. We also evaluate our model with different modality ratio during training and testing. We discuss the results on Section 3 in the supplementary."}, {"title": "4.4.2. Missing modality during testing.", "content": "To evaluate the test time missing modality robustness of our approach, we train our models on modality-complete training data and evaluate on modality-incomplete test data. We can divide this into two different scenarios: (i) partial modality missing and (ii) complete modality missing.\nPartial modality missing. For this evaluation, we follow the experimental setup similar to [3, 48] where models are trained with 100% image + 100% text data and tested with 100% image + x% text. We evaluate our model on MM-IMDb and UPMC Food-101 datasets and show the comparison with existing baseline methods proposed by Ma et al. [3], Kim and Kim [48] and ViLT [8] in Figure 2. We see that model performance drops as the amount of available text decreases. Our method demonstrates better robustness compared to other baseline methods. For example, on MM-IMDb dataset, our method achieves 5.8% greater F1-Macro score compared to Kim and Kim [48] when only 10% text"}, {"title": "4.5. Ablation Studies", "content": "Effectiveness of Mask Tokens Figure 3 demonstrates the effectiveness of our proposed Mask Tokens in handling missing modalities on MM-IMDb dataset. The results show consistent performance improvements across all the classes for both text missing and image missing scenarios. Notably, Mask Tokens significantly mitigate performance degradation in modality-sensitive classes. For instance, we see that the performance on \u201cHorror\u201d, \u201cWar\u201d and \u201cAnimation\u201d classes suffers drastically when text is missing as shown in Figure 3a. However, by utilizing our proposed Mask Tokens, this performance is improved significantly. This boost in performance is also significant in the \u201cRomance\u201d, \"Thriller\" and \"Action\" classes when image is missing as shown in Figure 3b. By enhancing the per-class perfor-"}, {"title": "5. Discussion and Future Directions", "content": "Advantages over existing methods. Despite having some similarities with recent methods, such as MMLORA [37] and prompt-tuning [24-26], our method differs from them in both architectural design and training strategy. MM-LORA follows a two-step fine-tuning process. Unimodal models are first fine-tuned on unimodal data, and then LORA layers are added and adapted to multimodal data. This approach assumes that unimodal fine-tuning is required before multimodal adaptation. In contrast, we demonstrate that jointly fine-tuning unimodal models on multimodal data is sufficient to achieve better or comparable performance across different modalities and multimodal tasks. Additionally, MMLORA does not address the problem of missing modalities. Our method, on the other hand, introduces Mask Tokens, which effectively handle different missing modality scenarios. Our approach also differs significantly from prompt-tuning based approaches [24-26]. Prompt-based methods improve missing modality performance by incorporating learnable prompts across multiple layers of the model. They require learning and storing a large number of prompts, typically 16 prompts [24, 25] per modality per layer, for different modality combinations. In contrast, our method uses a single mask token to estimate the missing modality feature from the available modality, thereby reducing the complexity. By fine-tuning the encoders with LoRA, we can effectively estimate the missing modality without the need for learning or storing a large number of prompts. Our approach outperforms prompt based approaches as shown in Table 3.\nLimitations and future directions. Our approach demonstrates better performance in both complete and missing modality scenarios. But there are still some limitations and open challenges. First, our approach can partially recover the performance drop due to missing modalities. This indicates scope for more effective compensation methods. Secondly We observe that certain classes are heavily dependent on a specific modality. This leads to significant performance drop in those classes when that specific modality is missing. It brings up an important question: how to design a better training strategy that encourages more uniform reliance on different modalities while maintaining overall performance in both complete and missing modality scenarios? Finally, future work can also explore developing a general framework for quantifying modality contribution and establishing theoretical bounds for performance drop under different missing modality scenarios."}, {"title": "6. Conclusion", "content": "In this paper, we present a simple yet effective approach for utilizing pre-trained unimodal encoders on multimodal tasks through unified adaptation on multimodal data. Our method reduces the complexity of traditional training strategies and minimizes the number of learnable parameters, while still achieving strong performance. It can accommodate both complete and missing modalities during training and testing. By using a single mask token per modality to estimate missing modality features, we avoid the need for specialized feature generation modules or prompt-tuning techniques. Our extensive evaluations across diverse tasks and datasets demonstrate the robustness and adaptability of our approach in both complete and missing modality scenarios. Our analysis also reveals that different classes depend on specific modalities, providing insights into how the absence of a modality impacts performance across different classes. Overall, our approach provides a robust and efficient solution for multimodal learning that maintains high performance in different real-world scenarios."}, {"title": "7. Acknowledgment", "content": "This work is supported in part by an Amazon Gift award and NSF awards 2046293 and 2406199."}, {"title": "1. Detailed Dataset Description", "content": "UPMC Food-101 dataset [49] is a popular and challenging multimodal classification dataset. It has two input modalities: image and text. The dataset has 90,704 samples divided into training and test sets having 67,988 and 22,716 samples, respectively. It contains 101 classes, which are the same as those in the ETHZ Food-101 dataset [63]. The samples are noisy, each category contains around 5% irrelevant images as they were collected in an uncontrolled environment without any human human intervention during the data collection process.\nMM-IMDb dataset [50] is a widely used multimodal dataset for multi-label movie genre classification task. It consists of 25,959 samples, each containing both image and text as input modality. The dataset is divided into three subsets: 15,552 samples for training, 2,608 for validation, and 7,799 for testing. It has 23 classes and each sample can have one or more genre. This dataset serves as a standard benchmark for evaluating multimodal models in the multi-label classification task.\nKinetics-Sound (KS) dataset [51] is a subset of the Kinetics-400 dataset [52]. It is used for human action recognition using audio and video as input modalities. This dataset includes 31 different action classes that span a variety of everyday human activities, making it suitable for evaluating multimodal models. The dataset is split into two subsets: a training set containing 14,739 samples and a test set containing 2,594 samples.\nAudio-Visual Event (AVE) Localization dataset [53] is a benchmark dataset used for multimodal event localization task. It contains 4,143 10-second videos. The dataset is divided into training, validation, and test sets, with 3,339 videos for training, 402 for validation, and 402 for testing. The dataset encompasses 28 different event categories providing a broad range of scenarios where both audio and visual signals are crucial for accurate event detection. All videos in the AVE dataset are collected from YouTube.\nCREMA-D dataset [54] is used for multimodal emotion recognition. It has audio and video as input modalities. The dataset contains 7,442 short video clips performed by 91 actors. The clips cover six emotions: angry, happy, sad, neutral, disgust, and fear. Emotion labels were obtained from 2,443 crowd-sourced raters to ensure diverse evaluations. The dataset is divided into 6,698 samples for training and 744 samples for testing."}, {"title": "2. Implementation Details", "content": "In Section 4.2 of the main paper, we briefly discussed the hyperparameter settings used in our experiments. Here, we provide a complete list of all the hyperparameters and their values in Table 5. The \"Image-Text Datasets\" column includes the hyperparameters for UPMC Food-101 and MM-IMDb datasets, while the \"Audio-Video Datasets\" column lists the hyperparameters for Kinetics-Sound, AVE, and CREMA-D datasets."}, {"title": "3. Different Modality Ratios During Training and Testing", "content": "To further evaluate the missing modality robustness of our method, we follow the experimental setup from [25, 48] and"}, {"title": "4. Analysis of Modality Dependence.", "content": "We extend our analysis of the impact of missing modalities on UPMC Food-101 dataset. We train our model on modality-complete data and evaluate it on both modality-complete and modality-incomplete test samples. As shown in Figure 6, we observe that multimodal (image-text) data consistently outperforms unimodal performance across all classes. However, the dependence on specific modalities varies significantly across different food categories.\nClasses such as \u201cdumplings\u201d, \u201cFrozen_yogurt\u201d, and \u201cIce cream\u201d demonstrate a strong dependence on the text modality. These classes perform well with text-only inputs, and performance drops drastically when text modality is missing. In contrast, categories like \u201ctuna_tartare\u201d, \u201cbeef_carpaccio\u201d, and \u201cbeet_salad\u201d depend heavily on the image modality. Their performance drops significantly when image modality is missing. This observation suggests that some classes rely more on text, while others depend heavily on images for accurate classification.\nInterestingly, some classes such as \u201cbeef_tartare\u201d, \u201cchicken_quesadilla\u201d, and \u201ccroque_madame\u201d exhibit relatively poor performance with unimodal inputs. Perfor-"}]}