{"title": "DenoMAE2.0: Improving Denoising Masked Autoencoders by Classifying Local Patches", "authors": ["Atik Faysal", "Mohammad Rostami", "Taha Boushine", "Reihaneh Gh. Roshan", "Huaxia Wang", "Nikhil Muralidhar"], "abstract": "We introduce DenoMAE2.0, an enhanced denoising masked autoencoder that integrates a local patch classification objective alongside traditional reconstruction loss to improve representation learning and robustness. Unlike conventional Masked Autoencoders (MAE), which focus solely on reconstructing missing inputs, DenoMAE2.0 introduces position-aware classification of unmasked patches, enabling the model to capture fine-grained local features while maintaining global coherence. This dual-objective approach is particularly beneficial in semi-supervised learning for wireless communication, where high noise levels and data scarcity pose significant challenges. We conduct extensive experiments on modulation signal classification across a wide range of signal-to-noise ratios (SNRs), from extremely low to moderately high conditions and in a low data regime. Our results demonstrate that DenoMAE2.0 surpasses its predecessor, DenoMAE, and other baselines in both denoising quality and downstream classification accuracy. DenoMAE2.0 achieves a 1.1% improvement over DenoMAE on our dataset and 11.83%, 16.55% significant improved accuracy gains on the RadioML benchmark, over DenoMAE, for constellation diagram classification of modulation signals.", "sections": [{"title": "I. INTRODUCTION", "content": "Semi-supervised models have emerged as a transformative paradigm in machine learning, addressing the high costs associated with data labeling [1], [2]. A prevalent approach in semi-supervised learning involves pretraining models on large amounts of unlabeled data, which are abundant and readily available [3], [4]. These models leverage unlabeled data to capture underlying patterns and structures, enabling effective representation learning. Consequently, the pretrained models exhibit improved performance and faster adaptation during downstream tasks, making semi-supervised learning a cost-efficient and scalable solution for real-world applications [5].\nRecent advancements in self-supervised learning have achieved remarkable success through representation learning [6], [7], contrastive learning [8], [9], and masking strategies [10], [11]. Among these, masking strategies leveraging Vision Transformer (ViT) [12] architectures have set new benchmarks in tasks such as classification, reconstruction, and image analysis. Masked Autoencoders (MAE) [13], in particular, have demonstrated significant potential by randomly masking portions of input data and training models to reconstruct the missing information. Despite their success, traditional MAE methods predominantly emphasize global reconstruction objectives, often neglecting fine-grained local patterns that are critical for enhancing representation learning [14].\nMasking strategies are particularly beneficial for pre-training in wireless communication for autometic modulation classification (AMC), where acquiring labeled data is challenging due to privacy and copyright constraints [15], [16]. Moreover, communication signals are often corrupted by environmental noise and channel losses, making robust representation learning essential. DenoMAE [17] addresses these challenges by integrating masking and denoising within a unified framework, leveraging masked modeling to enhance performance. While DenoMAE has demonstrated exceptional results in denoising and achieving strong classification accuracy, it inherits the fundamental principles of MAE, which focus primarily on global representation learning through masking. Consequently, the model remains unaware of the specific locations of the masked inputs, limiting its ability to fully exploit fine-grained local patterns and unlock the complete potential of representation learning.\nWe present DenoMAE2.0, an enhanced framework that introduces a novel local patch classification objective alongside the traditional reconstruction task. Our approach differs from existing methods by treating visible patches as distinct classes based on their spatial positions, enabling the model to learn position-aware local features while maintaining global coherence. This dual-objective strategy encourages the model to capture both structural and semantic information, leading to more robust and informative representations. The key contributions of our work are threefold:\n\u2022 We propose a novel architecture that combines denoising reconstruction with local patch classification, enabling simultaneous learning of global and local features.\n\u2022 We introduce a position-based classification strategy that leverages spatial information to enhance representation learning without requiring additional labels."}, {"title": "II. RELATED WORK", "content": "Early AMC methods primarily relied on expert-crafted features extracted from constellation diagrams and signal statistics, utilizing likelihood-based approaches and higher-order statistics to classify modulation schemes [18]. Constellation diagrams, which visually represent symbol distributions in the complex plane, have been a fundamental tool for feature extraction in classical AMC. These diagrams illustrate the effects of noise, interference, and channel distortions, making them valuable for differentiating modulation schemes based on clustering patterns, symbol dispersion, and trajectory structures [19].\nThe advent of deep learning revolutionized AMC by enabling models such as CNNs to operate directly on I/Q samples and constellation diagrams, bypassing the need for manual feature engineering [20]. Deep learning approaches leverage spatial patterns in constellation diagrams, learning robust representations that improve classification accuracy over conventional methods [21]. More recent advancements incorporate attention mechanisms and transformer architectures to capture long-range dependencies in signal patterns [22]. However, these deep learning-based AMC methods often require large labeled datasets and exhibit limited generalization across varying channel conditions, posing challenges for real-world deployment [23].\nThe evolution of modern self-supervised learning strategies in deep learning and wireless communication has progressed through distinct phases, starting with contrastive learning, followed by representation learning, and more recently, the adoption of masking techniques. Contrastive learning exemplified by SimCLR [8] and MoCo [24], relied on distinguishing positive and negative sample pairs to learn robust feature representations, proving effective in image and signal processing tasks. This was followed by representation learning methods [6], [25], which eliminated the need for negative samples by aligning representations of augmented views, enhancing efficiency and scalability. More recently, masking-based techniques [10], [13] have gained prominence by reconstructing masked portions of inputs, offering a powerful framework for learning from incomplete data [17]."}, {"title": "III. METHODOLOGY: DENOMAE2.0", "content": "The overall framework of the proposed DenoMAE2.0 is illustrated in Figure 1. DenoMAE2.0 consists of three components: encoder, reconstruction denoising decoder, and local patch classification head. There are two complementary objectives: the denoising reconstruction objective and the local patch classification classification objective. Details are provided in Algorithm 1 and introduced as follows.\nThe preprocessing stage transforms an input image \\(x \\in R^{H \\times W \\times C}\\) into a sequence of non-overlapping patches \\(x_p \\in R^{N \\times (P^2 \\cdot C)}\\), where (H, W) defines the spatial dimensions, C denotes the number of channels, and (P, P) specifies the patch size. The total number of patches is \\(N = HW/P^2\\).\nThese patches undergo linear projection through a PatchEmbed layer to obtain initial embeddings. We adopt a masking strategy that randomly removes a substantial portion (75%) of the patches. Let \\(x_v\\) and \\(x_m\\) represent the visible and masked patches respectively. The visible patches \\(x_v\\) are combined with learned positional embeddings and processed by a Vision Transformer (ViT) encoder to generate patch-level features \\(q_v\\). These features form the basis for downstream reconstruction and classification tasks.\nSince the length of visible patch features \\(q_v\\) is smaller than the total number of image patches N, we augment \\(q_v\\) with mask tokens to construct a complete feature set. Each mask token is a shared, learnable vector that indicates positions requiring reconstruction. Following the encoding process, we enhance this complete feature set with positional embeddings before processing it through a transformer-based decoder. The decoder's output undergoes transformation through a linear projection layer (not depicted in Figure 1) to map features back to pixel space.\nThe reconstruction objective \\(L_{rec}\\) employs mean squared error (MSE) between the reconstructed and original images. Consistent with established approaches, we compute this loss exclusively on the masked patches:\n\\(L_{rec} = MSE(\\hat{x}_m, x_m)\\)   (1)\nwhere \\(\\hat{x}_m\\) represents the reconstructed patches and \\(x_m\\) denotes the original masked patches.\nThe visible patch features \\(q_v\\) are additionally leveraged for classification. Diverging from traditional classification, where one input contains a single class, we assign visible patches to classes based on their spatial positions. The classification branch takes only the class tokens from the encoder to classify them. The total number of classes is the same as the number of visible patches which is calculated as:\n\\(N_{classes} = \\frac{img\\_size}{patch\\_size}^2 \\cdot (1 - mask\\_r)\\)   (2)\nSince we obtain several classes from a single image, the classification branch is a multi-class classification problem. The classification head consists of a simple linear layer that projects the patch features directly to the number of classes. The classification loss \\(L_{cls}\\) employs cross-entropy (CE) between predicted and ground truth labels:\n\\(L_{cls} = CE(p, y)\\)   (3)\nwhere p denotes the predicted probabilities and y represents the ground truth labels.\nOur DenoMAE2.0 optimizes a combination of reconstruction and classification losses, enabling simultaneous learning of fine-grained local and global features. The overall loss function is formulated as a weighted sum:\n\\(L = \\lambda_{rec}L_{rec} + \\lambda_{cls} L_{cls}\\)   (4)\nwhere \\(\\lambda_{rec}\\) and \\(\\lambda_{cls}\\) are balancing weights for the reconstruction and classification objectives, respectively. Through empirical validation (see Table V), we set \\(\\lambda_{rec} = 1.0\\) and \\(\\lambda_{cls} = 0.1\\).\nFollowing pre-training, the DenoMAE2.0 encoder is fine-tuned on specific recognition tasks to enhance performance. During this stage, the decoder and classification head are excluded, retaining only the encoder. For fine-tuning, the model utilizes the complete set of patches, corresponding to the uncorrupted input images, to optimize for downstream recognition tasks."}, {"title": "IV. DATASETS", "content": "Constellation diagrams offer a more informative approach to modulation classification compared to raw time-series signals, capturing richer detail essential for accurate interpretation [21]. The process of generating constellation diagrams for modulation signal classification involves multiple stages. We begin by mapping modulated signals onto a 7 \u00d7 7 complex plane, which provides sufficient space to capture signal samples while maintaining computational efficiency for SNR ranges of -10 dB to 10 dB. The basic constellation diagram is then enhanced through a multi-step process: first, as a gray image that handles varying pixel densities where multiple samples may occupy single pixels; second, as an enhanced grayscale image that employs an exponential decay model (\\(B_{i,j}\\)) to account for both the precise position of samples within pixels and their influence on neighboring pixels. This model considers the sample point's power (P), the distance between sample points and pixel centroids (\\(d_{i,j}\\)), and an exponential decay rate (\u03b1) [34]. To make the representation compatible with DenoMAE2.0, which expects RGB input, we generate a three-channel image by creating three distinct enhanced grayscale images from the same data samples, each utilizing different exponential decay rates.\nThe pretraining dataset comprises 10,000 samples uniformly distributed across ten modulation types, with randomly assigned SNR values in the range of -10 dB to 10 dB. For the downstream classification task, we use 1000 samples for training and 100 samples for testing, evenly distributed among the 10 classes, evaluated at SNR values from -10 dB to 10 dB in 1 dB increments.\nThe dataset comprises signals from ten modulation formats (see appendix, Table IX, for detailed modulation types), each originally of length \\(L_0 = 1024\\). To conform to the transformer's input dimensions, signals undergo a two-step preprocessing: (1) reshaping to \\(S_1 \\in R^{32 \\times 32}\\), and (2) interpolation to \\(S_2 \\in R^{224 \\times 224}\\). For formats with \\(L_0 > 1024\\) (e.g., LGMSK = 8196), signals are initially downsampled to \\(L_0\\). To match the three-channel structure of constellation images \\(I \\in R^{3 \\times 224 \\times 224}\\), \\(S_2\\) is replicated across three channels, yielding \\(S_3 \\in R^{3 \\times 224 \\times 224}\\). All signals are sampled at \\(f_s = 200\\) kHz, ensuring consistency across modulation formats."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "During the pretraing DenoMAE2.0 learns to denoise and reconstruct the missing patches and thereby learn their inherent representation. Therefore, we evaluate this denoisng and reconstruction performance during and after the pretraining phase. The effectiveness of DenoMAE2.0 is evident in the reconstructed constellation diagrams shown in Figure 2. Compared to DenoMAE, DenoMAE2.0 achieves superior noise reduction while preserving key signal features crucial for modulation classification. Table I quantifies this improvement using two standard metrics: structural similarity index (SSIM) and peak signal-to-noise ratio (PSNR). Interestingly, for the first image, DenoMAE achieved higher SNR and PSNR despite DenoMAE2.0 producing a visually superior reconstruction. However, for the remaining two samples, DenoMAE2.0 outperformed DenoMAE in both SSIM and PSNR.\nIn Figure 3, we visualize the latent representations of DenoMAE and DenoMAE2.0 using t-distributed stochastic neighbor embedding (t-SNE) [35]. The plots (left two) reveal that DenoMAE2.0 produces more accurate and distinct clusters than DenoMAE, indicating superior feature separation and representation learning. Specifically, in DenoMAE, classes 0, 3, and 9 contain more separated clusters within the same class. In contrast, DenoMAE2.0 exhibits more distinct clusters for these classes.\nThis enhanced clustering is particularly evident in the masked samples (right two plots), where we mask out a significant portion (75%) of the images. Masking typically makes distinct cluster formation more challenging. However, DenoMAE2.0 achieves clearer boundaries between classes, suggesting improved robustness and generalization capabilities. In DenoMAE, classes 0, 3, and 9 achieve somewhat distinct clusters, with a few samples overlapping in other clusters. In DenoMAE2.0, classes 0, 3, 6, 7, 8, and 9 show better-separated clusters, indicating enhanced feature learning and representation. However, in all the plots, classes 1 and 2 entirely overlap with each other, indicating that the model struggles to distinguish between these two classes. This is a common issue in modulation classification, as some classes are very similar to each other.\nAfter pretraining, we transfer the learned representations to the downstream classification task and compare DenoMAE2.0 with state-of-the-art models. As shown in Table II, when finetuned on the downstream task, DenoMAE2.0 achieves the highest test accuracy of 82.40% among all compared methods. This represents a significant improvement over the baseline ViT (79.90%) and other representation learning approaches including DEIT (81.20%), MoCov3 (81.00%), BEiT (80.40%), MAE (80.10%), and the original DenoMAE (81.30%). The consistent performance advantage demonstrates that DenoMAE2.0's enhanced denoising strategy learns more effective representations during pretraining that transfers well to the downstream classification task.\nFigure 4 presents a confusion matrix visualizing the per-class performance of DenoMAE2.0. The diagonal elements indicate correct classifications, while off-diagonal elements show misclassifications. The matrix reveals strong performance across most classes, though notable confusion exists between classes 1 and 2, aligning with observations from the t-SNE analysis. This confusion pattern suggests inherent similarities in the signal characteristics of these two modulation types that challenge even our enhanced model architecture.\nFigure 5 illustrates the finetuning performance across different epochs. The plot shows a steady improvement in model accuracy as the number of epochs increases, with DenoMAE2.0 consistently outperforming baseline methods. DenoMAE achieves higher accuracy than ViT continuously. On the other hand, although the gap between DenoMAE and ViT is larger than DenoMAE and DenoMAE2.0, DenoMAE2.0 always obtains better accuracy than DenoMAE.\nThe SNR performance plot demonstrates robust classification accuracy across various SNRs ranging from -10 dB to 10 dB. As the SNR values increases all the methods perform better and when the SNR is low the performance degrades. ViT, which has no pertaining phase, presumably obntains the lowest performance in all cases. MAE, which pretrains on modulation signals but has no denoising capability, obtains a slight higher accuracy than ViT. DenoMAE obtains higher accuracy than MAE in most cases however for SNE of 2 and 3 we observe that MAE obtains a higher accuracy. On the other hand, DenoMAE2.0 outperforms all the methods for all the SNRs. Noteably, all the methods lose significant accuracy for snr lower than -2 dB. However, DenoMAE2.0 maintains comparatively robust performance down to -7 dB, demonstrating its enhanced resilience to noise."}, {"title": "VI. COMPARISON WITH OTHER AMC METHODS", "content": "Table III presents a comprehensive comparison between DenoMAE2.0 and other state-of-the-art modulation classification methods. AlexNet achieved 82.8% accuracy with 8 classes at 2dB SNR using a large dataset of 800,000 samples. NMformer reported 71.6% accuracy across 10 classes with SNR ranging from 0.5 to 4.5dB using 106,800 samples. CNN-AMC and DL-GRF showed relatively lower performance with 50.40% and 59% accuracy respectively, though they were tested under different conditions. The original DenoMAE achieved 81.3% accuracy across 10 classes with SNR ranging from 0 to 10dB. Our proposed DenoMAE2.0 demonstrates superior performance with 82.4% accuracy under similar conditions, using only 10,000 samples for pretraining and 1,000 samples for fine-tuning, indicating improved efficiency in both performance and data utilization.\nTo show the transferability of DenoMAE2.0 on other similar task, we performed experiments on a similar benchmark dataset named RadioML. A brief description on the RadioML dataset is provided in the following subsection.\nThe transfer learning results are shown in Figure 7. Performance improves with increasing SNR, with DenoMAE2.0 consistently outperforming baseline methods across all SNR values. At 20 dB SNR, DenoMAE2.0 achieves 33.75% accuracy compared to 21.92% for DenoMAE, representing an 11.83% improvement. Similarly at 10 dB SNR, DenoMAE2.0 obtains 29.88% accuracy versus 13.33% for DenoMAE, a 16.55% gain. For out-of-distribution SNRs between -10 dB and -20 dB, ViT performs at chance level (4.27%) while DenoMAE shows minimal improvement. However, DenoMAE2.0 maintains significant accuracy of 7.08% at -10 dB and 6.67% at -20 dB, demonstrating enhanced transferability and robustness."}, {"title": "VII. ABLATION STUDY", "content": "In this section, we analyze the impact of various components on the performance of our model.\nWe conducted experiments to evaluate the contribution of different loss components in our model. As shown in Table IV, we analyzed three configurations: using only reconstruction loss, only classification loss, and combining both losses. The results demonstrate that while reconstruction loss alone achieves reasonable performance (81.30%), classification loss by itself underperforms (69.80%). However, combining both losses yields the best results (82.40%), suggesting that the joint optimization of reconstruction and classification objectives leads to more robust feature learning and better overall performance.\nWe conducted experiments to analyze the impact of auxiliary loss weights on model performance, as shown in Table V. The results demonstrate that the model's accuracy varies with different weight values, with 0.10 achieving the optimal performance at 82.4%. We observed a clear trend where very high weights (1.0) significantly degraded performance to 79.60%, while moderate weights (0.25, 0.50) maintained consistent performance at 82.0%. Lower weights (0.01, 0.05) showed slightly reduced performance at 81.3% and 81.5% respectively, indicating that the auxiliary loss weight needs to be carefully tuned to achieve the best balance between learning objectives.\nWe investigated the effect of different weight combinations between reconstruction and auxiliary losses, as shown in Table VI. The results reveal that balancing these two losses is crucial for optimal performance. Equal weighting (0.5, 0.5) yields suboptimal performance at 78.80%, while gradually increasing the reconstruction loss weight and decreasing the auxiliary loss weight shows steady improvement. The optimal combination is achieved with reconstruction loss weight of 1.0 and auxiliary loss weight of 0.10, resulting in 82.40% accuracy. Further increasing the reconstruction loss weight to 1.2 while reducing auxiliary loss weight to 0.05 leads to a slight performance degradation (82.0%), suggesting that maintaining a proper balance between these losses is essential for optimal model performance.\nWe examined the impact of varying MLP hidden dimensions on model performance, with results presented in Table VII. The experiments demonstrate that model performance improves as the hidden dimension size increases from 256 to 768, reaching peak accuracy at 768 dimensions (82.40%). Further increases to 1024 maintain this performance level, with a marginal improvement to 82.50% at 1280 dimensions. This suggests that while larger hidden dimensions can capture more complex features, the benefits plateau around 768-1024 dimensions, indicating this range as the optimal choice for balancing model capacity and computational efficiency.\nWe investigated the impact of varying the number of decoders on model performance, as shown in Table VIII. The results reveal a clear pattern where using a single decoder yields the lowest accuracy of 78.60%. A substantial improvement is observed when increasing to 3-4 decoders, both achieving 81.60%. Further enhancement is achieved with 8 decoders, reaching 82.40%, which plateaus through 12 decoders. Adding more decoders up to 16 provides only marginal improvement (82.50%). These results indicate that while multiple decoders are crucial for better feature learning, the performance gains saturate beyond 8 decoders, suggesting this as an optimal choice considering the computational efficiency trade-off."}, {"title": "VIII. CONCLUSION", "content": "In this work, we presented DenoMAE2.0, an enhanced denoising autoencoder that jointly optimizes reconstruction loss and an unmasked patch classification loss to improve robustness and performance. By incorporating a classification head that refines feature learning on unmasked regions, DenoMAE2.0 surpasses its predecessor, DenoMAE, achieving state-of-the-art results across multiple benchmarks. Extensive experiments demonstrate its superior ability to handle noisy data while preserving high classification accuracy. Additionally, we investigate the influence of auxiliary loss weighting and MLP hidden dimensions on performance, offering key insights into the role of hyperparameter tuning in optimizing denoising autoencoders."}]}