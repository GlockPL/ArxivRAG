{"title": "Neural Geometry Processing via Spherical Neural Surfaces", "authors": ["ROMY WILLIAMSON", "NILOY J. MITRA"], "abstract": "Neural surfaces (e.g., neural map encoding, deep implicits and neural radi-ance fields) have recently gained popularity because of their generic structure (e.g., multi-layer perceptron) and easy integration with modern learning-based setups. Traditionally, we have a rich toolbox of geometry processing algorithms designed for polygonal meshes to analyze and operate on sur-face geometry. However, neural representations are typically discretized and converted into a mesh, before applying any geometry processing al-gorithm. This is unsatisfactory and, as we demonstrate, unnecessary. In this work, we propose a spherical neural surface representation (a spherical parametrization) for genus-0 surfaces and demonstrate how to compute core geometric operators directly on this representation. Namely, we show how to construct the normals and the first and second fundamental forms of the surface, and how to compute the surface gradient, surface divergence and Laplace Beltrami operator on scalar/vector fields defined on the surface. These operators, in turn, enable us to create geometry processing tools that act directly on the neural representations without any unnecessary meshing. We demonstrate illustrative applications in (neural) spectral analysis, heat flow and mean curvature flow, and our method shows robustness to isometric shape variations. We both propose theoretical formulations and validate their numerical estimates. By systematically linking neural surface repre-sentations with classical geometry processing algorithms, we believe this work can become a key ingredient in enabling neural geometry processing.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of neural networks to represent surfaces has rapidly gained popularity as a generic surface representation. Many variants [Groueix et al. 2018; Mescheder et al. 2018; Park et al. 2019; Qi et al. 2016] have been proposed to directly represent a single surface, as an overfitted network, or a collection of surfaces, as encoder-decoder networks. The 'simplicity' of the approach lies in the generic network architectures (e.g., MLPs, UNets) being univer-sal function approximators. Such deep representations can either be overfitted to given meshes [Davies et al. 2020; Morreale et al. 2021] or directly optimized from a set of posed images (i.e., images with camera information) via an optimization, as popularized by the Nerf formulation [Mildenhall et al. 2021]. Although there are some isolated and specialized attempts [Chetan et al. 2023; Novello et al. 2022; Yang et al. 2021], no established set of operators directly works on such neural objects, encoding an explicit surface, to facilitate easy integration with existing geometry processing tasks.\nTraditionally, triangle meshes, more generally polygonal meshes, are used to represent surfaces. They are simple to define, easy to store, and benefit from having a wide range of available algorithms supporting them. Most geometry processing algorithms rely on the following core operations: (i) estimating differential quantities (i.e., surface normals, and first and second fundamental forms) at any surface location; (ii) computing surface gradients and surface diver-gence on scalar/vector fields; and (iii) defining a Laplace Beltrami operator (i.e., the generalization of the Laplace operator on a curved surface) for analyzing the shape itself or functions on the shape.\nMeshes, however, are discretized surface representations, and accurately carrying over definitions from continuous to discretized surfaces is surprisingly difficult. For example, the area of discrete differential geometry (DDG) [Desbrun et al. 2006] was entirely in-vented to meaningfully carry over continuous differential geometry concepts to the discrete setting. However, many commonly used estimates vary with the mesh quality (i.e., distribution of vertices and their connections), even when encoding the same underlying surface. In the case of the Laplace Beltrami operator, the 'no free lunch' result [Wardetzky et al. 2007] tells us that we cannot hope for any particular discretization of the Laplace Beltrami operator to simultaneously satisfy all of the properties of its continuous counterpart. In practice, this means that the most suitable choice of discretization (see [Bunge et al. 2023] for a recent survey) may change depending on the specifics of the target application."}, {"title": "2 RELATED WORKS", "content": "Neural representations. Implicit neural representations, such as signed distance functions (SDFs) and occupancy networks, have re-cently been popularized to model 3D shapes. Notably, DeepSDF [Park et al. 2019] leverages a neural network to represent the SDF of a shape, enabling continuous and differentiable shape representations that can be used for shape completion and reconstruction tasks; [Davies et al. 2020] use neural networks to overfit to individual SDFs; Occupancy Networks [Mescheder et al. 2018] learn a continu-ous function to represent the occupancy status of points in space, providing a powerful tool for 3D shape modeling and reconstruction from sparse input data. Explicit neural representations, on the other hand, use neural networks to directly predict structured 3D data such as meshes or point clouds. Mesh R-CNN [Gkioxari et al. 2019] extends Mask-RCNN to predict 3D meshes from images by utilizing a voxel-based representation followed by mesh refinement with a graph convolution network operating over the mesh's vertices and edges. In the context of point clouds, PointNet [Qi et al. 2016] and its variants are widely used as backbones for shape understanding tasks like shape classification and segmentation. Hybrid representa-tions combine the strengths of both implicit and explicit methods. For example, Pixel2Mesh [Wang et al. 2018] generates 3D meshes from images by progressively deforming a template mesh using a graph convolutional network, integrating the detailed geometric structure typical of explicit methods with the continuous nature of implicit representations. More relevant to our work is the At-lasNet [Groueix et al. 2018] that models surfaces as a collection of parametric patches, balancing flexibility and precision in shape representation, and Neural Surface Maps [Morreale et al. 2021], which are overfitted to a flattened disc parametrization of surfaces to enable surface-to-surface mapping.\nEstimating differential quantities. Traditionally, researchers have investigated how to carry over differential geometry concepts [do Carmo 1976] to surface meshes, where differential geometry does not di-rectly apply because mesh faces are flat, with all the 'curvature' being at sharp face intersections. Taubin [1995] introduce several signal processing operators on meshes. Meyer et al. [2002] used averaging Voronoi cells and the mixed Finite-Element/Finite-Volume method; [Cazals and Pouget 2005] used osculating jets; while [de Goes et al. 2020] used discrete differential geometry [Desbrun et al. 2006] to compute gradients, curvatures, and Laplacians on meshes. Recently, researchers have used learning-based approaches to 'learn' differ-ential quantities like normals and curvatures on point clouds [Ben-Shabat et al. 2019; Guerrero et al. 2018; Pistilli et al. 2020].\nMore related to ours is the work by Novello et al. [2022], who represent surfaces via implicit functions and analytically compute differential quantities such as normals and curvatures. With a sim-ilar motivation, Chetan et al. [2023] fit local polynomial patches to obtain more accurate derivatives from pre-trained hybrid neu-ral fields; they also use these as higher-order constraints in the"}, {"title": "3 SPHERICAL NEURAL SURFACES", "content": "We introduce a new neural representation called a 'Spherical Neu-ral Surface' (SNS) to represent continuous surfaces. An SNS is a Multi-Layer Perceptron (MLP) $S_e : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$, which we train so that the restriction to the unit sphere $(\\mathbb{S}^2 \\subset \\mathbb{R}^3)$ is a continuous parametrization of a given surface. In terms of notation, we will also use $S_e$ or $S$ as shorthand for the set $S_e(\\mathbb{S}^2)$. In our current work, we primarily focus on creating SNSs from input triangle-meshes and analytically-defined surfaces, but there is potential to create SNSs from other representations such as pointclouds, SDFs, etc. Figure 12 shows initial results for generating an SNS from a neural SDF.\nGiven a genus-0 manifold triangle-mesh, we first find an injective embedding of the mesh vertices onto the unit sphere, using a spher-ical embedding algorithm [Praun and Hoppe 2003; Schmidt et al. 2023]. We extend the embedding to a continuous embedding by pro-jecting points on the sphere to the discrete sphere, and employing barycentric coordinates so that every point $p_i$ on the sphere cor-responds to a unique point $q_i$ on the target surface. We overfit the network $S_e$ to this parametrization by minimizing the MSE between"}, {"title": "3.1 Overfitting an SNS to a Triangle-Mesh", "content": "Given a genus-0 manifold triangle-mesh, we first find an injective embedding of the mesh vertices onto the unit sphere, using a spher-ical embedding algorithm [Praun and Hoppe 2003; Schmidt et al. 2023]. We extend the embedding to a continuous embedding by pro-jecting points on the sphere to the discrete sphere, and employing barycentric coordinates so that every point $p_i$ on the sphere cor-responds to a unique point $q_i$ on the target surface. We overfit the network $S_e$ to this parametrization by minimizing the MSE between the ground truth and predicted surface positions, as:\n$L_{MSE} := \\frac{1}{N}\\sum_{i=1}^{N} ||S_e(p_i) - q_i||^2 $\nFurther, to improve the fitting, we encourage the normals derived from the spherical map to align with the normals of points on the mesh, via a regularization term,\n$L_{normal} := \\frac{1}{N} \\sum_{i=1}^N || \\mathbf{n}_{S_e}(p_i) - \\mathbf{n}_{mesh}(p_i)||^2 =  \\frac{1}{N} \\sum_{i=1}^N (1-\\cos\\alpha_i (p_i))$.\nIn this expression, $\\mathbf{n}_{mesh}(p_i)$ is the (outwards) unit-normal on the mesh, and $\\mathbf{n}_{S_e}(p_i)$ is the corresponding (outwards) unit-normal of the smooth parametrization at $p_i$, which is derived analytically from the Jacobian of $S_e$ at $p_i$ (see Section 3.2); the angle $\\alpha(p_i)$ is the angle between $\\mathbf{n}_{mesh}(p_i)$ and $\\mathbf{n}_{S_e}(p_i)$. See Figure 2 for an example."}, {"title": "3.2 Computing Differential Quantities using SNS", "content": "One of the advantages of Spherical Neural Surfaces as a geome-try representation is that it is extremely natural to compute many important quantities from continuous differential geometry - with-out any need for approximation or discretization. This is thanks to the automatic differentiation functionality built into modern ma-chine learning setups and algebraically tracking some changes of variables.\nFor example, to compute the outwards-pointing normal $\\mathbf{n}_{s}$, we simply compute the $3 \\times 3$ Jacobian of $S - J(S)$ \u2013 and then turn this into a Jacobian for a local 2D parametrization by composing it with a $3 \\times 2$ orthonormal matrix ($R$):\n$J_{local} = J(S) R $\n$R = \\begin{bmatrix} \\mathbf{S}_u & \\mathbf{S}_v\\end{bmatrix}$.\nThe columns of $J_{local}$ are the partial derivatives $\\mathbf{S}_u$ and $\\mathbf{S}_v$. The matrix $R$ defines orthogonal unit vectors on the tangent plane of the sphere at each point, and the functions $u$ and $v$ are the corresponding orthonormal coordinates on the tangent plane of the sphere. Then, the normalized cross product of $\\mathbf{S}_u$ and $\\mathbf{S}_v$ is the outward-pointing unit normal to the surface:\n$\\mathbf{n}_{s} = \\frac{\\mathbf{S}_u \\times \\mathbf{S}_v}{||\\mathbf{S}_u \\times \\mathbf{S}_v||}$"}, {"title": "4 MATHEMATICAL BACKGROUND", "content": "This section briefly summarizes the relevant background for contin-uous surfaces that we need to develop a computation framework for Laplace-Beltrami and spectral analysis using SNS (Section 5)."}, {"title": "4.1 The Laplace Beltrami Operator on Smooth Surfaces", "content": "The (Euclidean) Laplacian $\\Delta$ of a scalar field $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is defined as the divergence of the gradient of the scalar field. Equivalently, $\\Delta f$ is the sum of the unmixed second partial derivatives of the field $f$:\n$\\Delta f := \\nabla \\cdot \\nabla f = \\sum_{i=1}^n \\frac{\\partial^2 f}{\\partial x_i^2}$\nThe Laplace-Beltrami operator, or surface Laplacian, is the natural generalization of the Laplacian to scalar fields defined on curved domains. If $\\Sigma \\subset \\mathbb{R}^3$ is a regular surface and $f : \\Sigma \\rightarrow \\mathbb{R}$ is a scalar field, then the Laplace-Beltrami operator on $f$ is defined as the surface divergence of the surface gradient of $f$, and we denote this by $\\Delta_{\\Sigma}f$ to emphasize the dependence on the surface $\\Sigma$. Thus,\n$\\Delta_{\\Sigma}f := \\nabla_{\\Sigma} \\cdot \\nabla_{\\Sigma}f$.\nThe surface gradient of the scalar field $f$ (written $\\nabla_{\\Sigma}f$), and the surface divergence of a vector field $F : \\Sigma \\rightarrow \\mathbb{R}^3$ (written $\\nabla_{\\Sigma} \\cdot F$), can be computed by smoothly extending the fields to fields $\\tilde{f}$ and $\\tilde{F}$ respectively, which are defined on a neighborhood of $\\Sigma$ in $\\mathbb{R}^3$.\nThe surface gradient is defined as the projection of the gradient of $f$ into the local tangent plane:\n$\\nabla_{\\Sigma}f = \\nabla \\tilde{f} - (\\nabla \\tilde{f} \\cdot \\mathbf{n})\\mathbf{n}$,\nwhere $\\mathbf{n}$ is the unit surface normal at the point. The surface diver-gence of the vector field $F$ is defined as\n$\\nabla_{\\Sigma} \\cdot F = \\nabla \\cdot \\tilde{F} - \\mathbf{n}^T J(\\tilde{F})\\mathbf{n}$,\nwhere $J(\\tilde{F}) := \\begin{bmatrix} \\frac{\\partial F}{\\partial x_1}, & \\frac{\\partial F}{\\partial x_2}, & \\frac{\\partial F}{\\partial x_3} \\end{bmatrix}$ is the Jacobian of $\\tilde{F}$. Since $J(\\tilde{F})\\mathbf{n}$ is the directional derivative of $\\tilde{F}$ in the normal direction, then $\\mathbf{n}^T J(\\tilde{F})\\mathbf{n} = J(\\tilde{F})\\mathbf{n} \\cdot \\mathbf{n}$ can be thought of as the contribution to the three-dimensional divergence of $\\tilde{F}$ that comes from the normal com-ponent of $F$, and by ignoring the contribution from the normal component, we get the two-dimensional 'surface divergence'. Al-though these expressions depend a-priori on the particular choice of extension, the surface gradient and surface divergence are in fact well-defined (i.e., any choice of extension will give the same result). By expanding out the definitions of surface gradient and surface divergence, we can derive an alternative formula for the surface Laplacian $(\\Delta_{\\Sigma}f)$ in terms of the (Euclidean) Laplacian $(\\Delta f)$, the gradient $(\\nabla f)$ and the Hessian $(H(f) = J(\\nabla f)^T)$ as,\n$\\Delta_{\\Sigma}f = \\Delta f - 2H\\nabla f \\cdot \\mathbf{n} - \\mathbf{n}^T H(f)\\mathbf{n}$.\nThe dependence on the surface is captured by the normal function $\\mathbf{n}$, and the mean curvature $H$. Please refer to [Reilly 1982; Xu and Zhao 2023] for a derivation. In the case when $f$ is a 'normal extension' i.e., it is locally constant in the normal directions close to the surface - then the second and third terms disappear, meaning that $\\Delta_{\\Sigma}f$ is consistent with $\\Delta f$."}, {"title": "4.2 Spectrum of the Laplace Beltrami Operator", "content": "Given two scalar functions, $f$ and $g$, defined on the same regular surface $\\Sigma$, we can define their inner product to be,\n$\\langle f, g \\rangle_{L^2(\\Sigma)} := \\int_\\Sigma f gdA$.\nThen, the $L^2$ norm of a scalar function can be expressed as,\n$||f||_{L^2(\\Sigma)} = \\sqrt{\\langle f, f \\rangle_{L^2(\\Sigma)} } = \\sqrt{\\int_{\\Sigma} |f|^2 dA}$.\nThe Laplace Beltrami operator, $\\Delta_{\\Sigma}$, is a self-adjoint linear operator with respect to this inner product. This means that there is a set of eigenfunctions of $\\Delta_{\\Sigma}$ that form an orthonormal basis for the space $L^2(\\Sigma)$ - the space of 'well-behaved' scalar functions on $\\Sigma$ that have a finite $L^2$-norm. This basis is analogous to the Fourier basis for the space of periodic functions on an interval.\nIn the discrete mesh case, the eigenfunctions of the Laplace-Beltrami operator are computed by diagonalizing a sparse matrix [Bunge et al. 2023; L\u00e9vy and Zhang 2010] based on the input mesh. In the continuous case, however, we no longer have a matrix rep-resentation for $\\Delta_{\\Sigma}$. Instead, we exploit a functional analysis result that describes the first $k$ eigenfunctions of $\\Delta_{\\Sigma}$ as the solution to a minimization problem, as described next.\nFirst, we define the Rayleigh Quotient of a scalar function $f$ to be the Dirichlet energy of the scalar function, divided by the squared $L^2$-norm of the function:\n$\\Omega_{\\Sigma} (f) := \\frac{||\\nabla f||_{L^2(\\Sigma)}^2}{||f||_{L^2(\\Sigma)}^2}$.\nThen, the eigenfunctions of $\\Delta_{\\Sigma}$ are the minimizers of the Rayleigh Quotient, as given by,\n$\\Psi_0, \\Psi_1, ..., \\Psi_{k-1} = \\underset{\\Psi_0, ...}{\\arg \\min} \\sum_{i=0}^{k-1} \\Omega_{\\Sigma}(\\Psi_i)  $\nsuch that $\\langle \\Psi_i, \\Psi_j \\rangle_{L^2(\\Sigma)} = 0  \\text{ for all } i \\neq j$.\nWe use $\\Psi_0$ to represent the first eigenfunction, which is always a constant function. The constraint states that the eigenfunctions are orthogonal inside the inner-product space $L^2(\\Sigma)$.\nIn addition, the Rayleigh Quotient of the eigenfunction $\\Psi_i$ is the positive of the corresponding eigenvalue, i.e.,\n$\\Delta_{\\Sigma}\\Psi_i(x) = -\\Omega_{\\Sigma}(\\Psi_i) \\Psi_i(x) \\quad \\forall x \\in \\Sigma$.\nPhysically, the eigenfunctions of $\\Delta_{\\Sigma}$ represent the fundamental modes of vibration of the surface $\\Sigma$. The Rayleigh Quotient of $\\Psi_0$ is zero, and as the Rayleigh quotient increases, the vibrational modes increase in energy and the eigenfunctions produce more 'intricate' (high frequency) patterns."}, {"title": "5 SPECTRAL ANALYSIS USING SNS", "content": "To find the continuous eigenmodes of the Laplace-Beltrami operator on an SNS, we require a continuous and differentiable representation for scalar functions defined on the surface. In our framework, we represent such smooth scalar fields on $\\mathbb{S}^2$ by MLPs, $g_n$, from $\\mathbb{R}^3$ to $\\mathbb{R}$. We only ever evaluate the MLP $g_n$ on $\\mathbb{S}^2 \\in \\mathbb{R}^3$, but because the domain is $\\mathbb{R}^3$ then it defines an extension of the scalar field, and this allows us to compute $\\nabla g_n$. Then, if $S_e : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$ is an SNS, any smooth scalar field $h$ defined on $S_e$ can be defined implicitly by the equation,\n$h \\circ S_e = g_n$.\nApplying the chain rule to the implicit equation, we can compute the gradient of $h$:\n$\\nabla h = (J(S_e)^T)^{-1}\\nabla g_n$,\nwhich allows us to compute $\\nabla_{\\Sigma} h$ (Equation 13) and $\\Omega_{\\Sigma} (h)$ (Equa-tion 19) without explicitly computing $h$.\nTo optimize for a (smooth) scalar function $g_{\\eta k}$ so that $h$ approxi-mates the $k$th non-constant eigenfunction of the Laplace-Beltrami operator, we use gradient descent to optimize the weights, $\\eta_k$. We use a combination of two loss terms:\n$L_{Rayleigh} := \\Omega_{\\Sigma}(h) \\quad \\text{and} \\quad L_{ortho} := \\sum_{i=0}^{k-1} \\langle h, h_i \\rangle_{L^2(\\Sigma)}$,\nwhere we sequentially compute the eigenfunctions, and $h_i$ is the $i$th smallest eigenfunction to be found. Recall that we represent each such eigenfunction using a dedicated MLP (except for $h_0$, which is"}, {"title": "7 CONCLUSION", "content": "We have presented spherical neural surfaces as a novel represen-tation for genus-0 surfaces along with supporting operators in the form of differential geometry estimates (e.g., normals, First and Sec-ond Fundamental Forms), as well as surface gradients and surface divergence operators. We also presented how to compute a contin-uous Laplace Beltrami operator and its lowest spectral modes. We demonstrated that ours, by avoiding any unnecessary discretiza-tion, produces robust and consistent estimates even under different meshing concerning vertex sampling as well as their connectivity."}, {"title": "Limitations and Future Work", "content": "Beyond genus-0 surfaces. Since our SNSs rely on a sphere for the parametrization, it limits us to genus-0 surfaces. If we could choose from a range of canonical surfaces (e.g., torus), this would allow us to seamlessly process higher genus shapes and, possibly, to produce neural surfaces without such high distortions. Another option would be to rely on local parametrizations, but then we would have to consider blending across overlapping parametrizations.\nSpeed. A severe limitation of our current realization is its long running time. While we expect that better and optimized imple-mentations would increase efficiency significantly, we also need to make changes to our formulation. Specifically, our current spectral estimation is sequential, which is not only slow, but leads to an accumulation of errors for later spectral modes. Hence, in the future, we would like to jointly optimize for multiple spectral modes.\nSNS from neural representations. We demonstrated our setup mainly on mesh input and also on neural input in the form of deepSDF. In the future, we would like to extend our SNS to other neural represen-tations in the form of occupancy fields or radiance fields. However, this will require locating and sampling points on the surfaces, which are implicitly encoded - we need the representation to provide a pro-jection operator. We would also like to support neural surfaces that come with level-of-detail. Finally, an interesting direction would be to explore end-to-end formulation for dynamic surfaces encoding temporal surfaces with SNS, and enabling optimization with loss terms involving first/second fundamental forms as well as Laplace-Beltrami operators (e.g., neural deformation energy)."}, {"title": "6 EVALUATION", "content": "Implementation Details. All of our networks in our experiments were trained on Nvidia GeForce P8 GPUs. The SNS network is a Residual MLP with input and output of size three, and eight hidden layers with 256 nodes each. The networks used to represent scalar fields (in the eigenfunction and heat flow experiments) are very small Residual MLPs with input size three, output size three and two hidden layers with 10 nodes each. (To make the output value a scalar, we take the mean of the three output values.) For both scalar fields and SNS, the activation function is 'Softplus'. We use the RMSProp optimizer with a learning rate of $10^{-4}$ and a momentum of 0.9. We trained each SNS for up to 20,000 epochs (eight-to-ten hours), and we fixed the normals regularization coefficient to be $10^{-4}$. The time per epoch increases slightly with the number of vertices in the mesh, but for simpler shapes (such as the sphere) the optimization converges in fewer epochs.\nEigenfunction Optimization. In the eigenfunction experiment, we used the initial parameter values $\\lambda_{ortho} = 10^3$ and $\\lambda_{unit} = 10^4$ and then we reduced the coefficients linearly to $\\lambda_{ortho} = 1$ and $\\lambda_{unit} = 10^2$, over 10,000 epochs. These settings prevent the scalar field from collapsing to the zero-function during the initial stages of optimization, whilst allowing the Rayleigh Quotient to dominate the loss in the later stages. We optimized each eigenfunction for up to 40,000 epochs (approximately six hours on our setup).\nWe generate M = 100, 000 points for the initial uniform distribu-tion on the sphere, and we use $N_{target} = 10,000$ in the rejection sampling process. The points stay fixed during each optimization stage. We believe that this is a possible reason why the Rayleigh Quo-tient computed on the training points is often slightly lower than the Rayleigh Quotient that is computed with a different, larger set of uniform samples, and a better sampling strategy might improve the accuracy of our optimization.\nComparison against Analytic Functions. For some of our SNS com-putations, there exist special cases for which an exact analytic so-lution can be calculated. To test our estimates of curvature and principal curvature directions, we overfitted an SNS to a mesh of the closed surface with the parametrization\n$r(\\theta, \\varphi) = (1+0.4 \\sin^2 \\theta \\sin(60\\varphi) \\sin(64\\varphi)) (\\sin \\theta \\cos \\varphi; \\sin \\theta \\sin \\varphi; \\cos \\theta) $.\nWe used the Matlab Symbolic Math Toolbox\u2122 to compute the nor-mals, mean curvature, Gauss curvature and principal curvature directions in closed form. In Figure 3, we show these differential quantities on the mesh of the analytic surface and on the SNS. The computed curvatures align extremely closely, therefore the over-fitting quality is very high and the curvature computations are accurate. There is a slight discrepancy at the poles, because we have used a global analytic parametrization which not valid where $\\theta \\in \\{0, \\pi\\}$. On the other hand, in the SNS formulation we use selec-tive co-ordinate relabelling to avoid issues related to using spherical polar co-ordinates close to the poles."}]}