{"title": "Neural Geometry Processing via Spherical Neural Surfaces", "authors": ["ROMY WILLIAMSON", "NILOY J. MITRA"], "abstract": "Neural surfaces (e.g., neural map encoding, deep implicits and neural radi-ance fields) have recently gained popularity because of their generic structure(e.g., multi-layer perceptron) and easy integration with modern learning-based setups. Traditionally, we have a rich toolbox of geometry processingalgorithms designed for polygonal meshes to analyze and operate on sur-face geometry. However, neural representations are typically discretizedand converted into a mesh, before applying any geometry processing al-gorithm. This is unsatisfactory and, as we demonstrate, unnecessary. Inthis work, we propose a spherical neural surface representation (a sphericalparametrization) for genus-0 surfaces and demonstrate how to compute coregeometric operators directly on this representation. Namely, we show howto construct the normals and the first and second fundamental forms ofthe surface, and how to compute the surface gradient, surface divergenceand Laplace Beltrami operator on scalar/vector fields defined on the surface.These operators, in turn, enable us to create geometry processing tools thatact directly on the neural representations without any unnecessary meshing.We demonstrate illustrative applications in (neural) spectral analysis, heatflow and mean curvature flow, and our method shows robustness to isomet-ric shape variations. We both propose theoretical formulations and validatetheir numerical estimates. By systematically linking neural surface repre-sentations with classical geometry processing algorithms, we believe thiswork can become a key ingredient in enabling neural geometry processing.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of neural networks to represent surfaces has rapidlygained popularity as a generic surface representation. Many vari-ants [Groueix et al. 2018; Mescheder et al. 2018; Park et al. 2019;Qi et al. 2016] have been proposed to directly represent a singlesurface, as an overfitted network, or a collection of surfaces, asencoder-decoder networks. The 'simplicity' of the approach lies inthe generic network architectures (e.g., MLPs, UNets) being univer-sal function approximators. Such deep representations can eitherbe overfitted to given meshes [Davies et al. 2020; Morreale et al.2021] or directly optimized from a set of posed images (i.e., imageswith camera information) via an optimization, as popularized by theNerf formulation [Mildenhall et al. 2021]. Although there are someisolated and specialized attempts [Chetan et al. 2023; Novello et al.2022; Yang et al. 2021], no established set of operators directly workson such neural objects, encoding an explicit surface, to facilitateeasy integration with existing geometry processing tasks.\nTraditionally, triangle meshes, more generally polygonal meshes,are used to represent surfaces. They are simple to define, easy tostore, and benefit from having a wide range of available algorithmssupporting them. Most geometry processing algorithms rely on thefollowing core operations: (i) estimating differential quantities (i.e.,surface normals, and first and second fundamental forms) at anysurface location; (ii) computing surface gradients and surface diver-gence on scalar/vector fields; and (iii) defining a Laplace Beltramioperator (i.e., the generalization of the Laplace operator on a curvedsurface) for analyzing the shape itself or functions on the shape.\nMeshes, however, are discretized surface representations, andaccurately carrying over definitions from continuous to discretizedsurfaces is surprisingly difficult. For example, the area of discretedifferential geometry (DDG) [Desbrun et al. 2006] was entirely in-vented to meaningfully carry over continuous differential geometryconcepts to the discrete setting. However, many commonly usedestimates vary with the mesh quality (i.e., distribution of verticesand their connections), even when encoding the same underlyingsurface. In the case of the Laplace Beltrami operator, the 'no freelunch' result [Wardetzky et al. 2007] tells us that we cannot hopefor any particular discretization of the Laplace Beltrami operatorto simultaneously satisfy all of the properties of its continuouscounterpart. In practice, this means that the most suitable choiceof discretization (see [Bunge et al. 2023] for a recent survey) maychange depending on the specifics of the target application.\nIn the absence of native geometric operators for neural representa-tions, the common workaround for applying a geometry processingalgorithm to neural representations is discretizing these represen-tations into an explicit mesh (e.g., using Marching cubes) and thenrelying on traditional mesh-based operators. Unfortunately, thisapproach inherits the disadvantages of having an explicit meshrepresentation, as listed above. A similar motivation prompted re-searchers to pursue a grid-free approach for volumetric analysisusing Monte Carlo estimation for geometry processing [Sawhneyand Crane 2020] or neural field convolution [Nsampi et al. 2023] toperform general continuous convolutions with continuous signalssuch as (overfitted) neural fields.\nWe avoid discretization altogether. First, we create a smooth anddifferentiable representation - spherical neural surfaces - where weencode individual genus-0 surfaces using dedicated neural networks.Then, we exploit autograd to compute surface normals, tangentplanes, and Fundamental Forms directly with continuous differentialgeometry. Knowing the First Fundamental Form allows us to com-pute attributes of the parametrization, such as local area-distortion.Knowing the Second Fundamental form allows us to compute sur-face attributes such as mean curvature, Gauss curvature, and princi-pal curvature directions - without any unnecessary discretizationerror. Essentially, encoding surfaces as seamless neural maps, almosttrivially, allows us to access definitions from differential geometry.\nOther estimates require more thought. We use the computed sur-face normals and mean curvatures to apply the continuous LaplaceBeltrami operator to any differentiable scalar/vector field. Addition-ally, to enable spectral analysis on the resultant Laplace Beltramioperator, we propose a novel optimization scheme and a neuralrepresentation of scalar fields that allows us to extract the lowestk spectral modes, using the variational formulation of the Dirich-let eigenvalue problem. We use Monte Carlo estimates to approx-imate the continuous inner products and integration of functionson the (neural) surface. Our framework of spherical neural surfacesmakes optimizing many expressions involving surface gradientsand surface integrals of smooth functions possible. We hope that oursetup opens the door to solving other variational problems posedon smooth (neural) surfaces.\nWe evaluate our proposed framework on standard geometry pro-cessing tasks. In the context of mesh input, we assess the robustnessof our estimates to different meshing of the same underlying sur-faces and isometrically related shapes. Wecompare the results against ground truth values when they are com-putable (e.g., analytical functions) andcontrast them against alternatives, where applicable.\nOur main contribution is introducing a novel representation andassociated operators to enable neural geometry processing.\nWe:\n(i) introduce spherical neural surfaces as a representation to en-code genus-0 surfaces seamlessly as overfitted neural networks;\n(ii) compute surface normals, First and Second Fundamental Forms,curvature, continuous Laplace Beltrami operators, and theirsmallest spectral modes without using any unnecessary dis-cretization;"}, {"title": "2 RELATED WORKS", "content": "Neural representations. Implicit neural representations, such as signed distance functions (SDFs) and occupancy networks, have re-cently been popularized to model 3D shapes. Notably, DeepSDF leverages a neural network to represent the SDF of ashape, enabling continuous and differentiable shape representationsthat can be used for shape completion and reconstruction tasks;[Davies et al. 2020] use neural networks to overfit to individualSDFs; Occupancy Networks [Mescheder et al. 2018] learn a continu-ous function to represent the occupancy status of points in space,providing a powerful tool for 3D shape modeling and reconstructionfrom sparse input data. Explicit neural representations, on the otherhand, use neural networks to directly predict structured 3D datasuch as meshes or point clouds. Mesh R-CNN extends Mask-RCNN to predict 3D meshes from images by utilizinga voxel-based representation followed by mesh refinement with agraph convolution network operating over the mesh's vertices andedges. In the context of point clouds, PointNet andits variants are widely used as backbones for shape understandingtasks like shape classification and segmentation. Hybrid representa-tions combine the strengths of both implicit and explicit methods.For example, Pixel2Mesh generates 3D meshesfrom images by progressively deforming a template mesh using agraph convolutional network, integrating the detailed geometricstructure typical of explicit methods with the continuous natureof implicit representations. More relevant to our work is the At-lasNet that models surfaces as a collectionof parametric patches, balancing flexibility and precision in shaperepresentation, and Neural Surface Maps ,which are overfitted to a flattened disc parametrization of surfaces to enable surface-to-surface mapping.\nEstimating differential quantities. Traditionally, researchers haveinvestigated how to carry over differential geometry concepts [do Carmo1976] to surface meshes, where differential geometry does not di-rectly apply because mesh faces are flat, with all the 'curvature' beingat sharp face intersections. Taubin [1995] introduce several signalprocessing operators on meshes. Meyer et al. [2002] used averagingVoronoi cells and the mixed Finite-Element/Finite-Volume method;[Cazals and Pouget 2005] used osculating jets; while [de Goes et al.2020] used discrete differential geometry [Desbrun et al. 2006] tocompute gradients, curvatures, and Laplacians on meshes. Recently,researchers have used learning-based approaches to 'learn' differ-ential quantities like normals and curvatures on point clouds [Ben-Shabat et al. 2019; Guerrero et al. 2018; Pistilli et al. 2020].\nMore related to ours is the work by Novello et al. [2022], whorepresent surfaces via implicit functions and analytically computedifferential quantities such as normals and curvatures. With a sim-ilar motivation, Chetan et al. [2023] fit local polynomial patchesto obtain more accurate derivatives from pre-trained hybrid neu-ral fields; they also use these as higher-order constraints in the"}, {"title": "3 SPHERICAL NEURAL SURFACES", "content": "We introduce a new neural representation called a 'Spherical Neu-ral Surface' (SNS) to represent continuous surfaces. An SNS is aMulti-Layer Perceptron (MLP) $S_{\\Theta} : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$, which we train sothat the restriction to the unit sphere ($S^2 \\subset \\mathbb{R}^3$) is a continuousparametrization of a given surface. In terms of notation, we will alsouse $S_{\\Theta}$ or $S$ as shorthand for the set $S_{\\Theta}(S^2)$. In our current work, weprimarily focus on creating SNSs from input triangle-meshes andanalytically-defined surfaces, but there is potential to create SNSsfrom other representations such as pointclouds, SDFs, etc.\n3.1 Overfitting an SNS to a Triangle-Mesh\nGiven a genus-0 manifold triangle-mesh, we first find an injectiveembedding of the mesh vertices onto the unit sphere, using a spher-ical embedding algorithm [Praun and Hoppe 2003; Schmidt et al.2023]. We extend the embedding to a continuous embedding by pro-jecting points on the sphere to the discrete sphere, and employingbarycentric coordinates so that every point $p_i$ on the sphere cor-responds to a unique point $q_i$ on the target surface. We overfit thenetwork $S_{\\Theta}$ to this parametrization by minimizing the MSE between the ground truth and predicted surface positions, as:\n$L_{MSE} := \\frac{1}{N} \\sum_{i=1}^{N} ||S_{\\Theta}(p_i) - q_i||^2$.\nFurther, to improve the fitting, we encourage the normals derivedfrom the spherical map to align with the normals of points on themesh, via a regularization term,\n$L_{normal} := \\frac{1}{N} \\sum_{i=1}^{N} || n_{S_{\\Theta}}(p_i) - n_{mesh}(p_i)||^2 = \\frac{1}{N} \\sum_{i=1}^{N} (1-\\cos \\alpha_i (p_i))$.\nIn this expression, $n_{mesh}(p_i)$ is the (outwards) unit-normal on themesh, and $n_{S_{\\Theta}}(p_i)$ is the corresponding (outwards) unit-normal ofthe smooth parametrization at $p_i$, which is derived analytically fromthe Jacobian of $S_{\\Theta}$ at $p_i$; the angle $\\alpha(p_i)$ is the anglebetween $n_{mesh}(p_i)$ and $n_{S_{\\Theta}}(p_i)$."}, {"title": "3.2 Computing Differential Quantities using SNS", "content": "One of the advantages of Spherical Neural Surfaces as a geome-try representation is that it is extremely natural to compute manyimportant quantities from continuous differential geometry - with-out any need for approximation or discretization. This is thanks tothe automatic differentiation functionality built into modern ma-chine learning setups and algebraically tracking some changes ofvariables.\nFor example, to compute the outwards-pointing normal $n_S$, wesimply compute the 3 \u00d7 3 Jacobian of $S$ \u2013 $J(S)$ \u2013 and then turn thisinto a Jacobian for a local 2D parametrization by composing it witha 3 \u00d7 2 orthonormal matrix ($R$):\n$J_{local} = J(S) R$\n$R= \\frac{ [ S_u S_v ] }{ ||S_u|| ||S_v|| }$\nThe columns of $J_{local}$ are the partial derivatives $S_u$ and $S_v$. Thematrix $R$ defines orthogonal unit vectors on the tangent plane of thesphere at each point, and the functions $u$ and $v$ are the correspondingorthonormal coordinates on the tangent plane of the sphere. Then,the normalized cross product of $S_u$ and $S_v$ is the outward-pointingunit normal to the surface:\n$n_S = \\frac{S_u \\times S_v}{||S_u \\times S_v||}$"}, {"title": "4 MATHEMATICAL BACKGROUND", "content": "This section briefly summarizes the relevant background for contin-uous surfaces that we need to develop a computation frameworkfor Laplace-Beltrami and spectral analysis using SNS.\n4.1 The Laplace Beltrami Operator on Smooth Surfaces\nThe (Euclidean) Laplacian $A$ of a scalar field $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is definedas the divergence of the gradient of the scalar field. Equivalently, $Af$is the sum of the unmixed second partial derivatives of the field $f$:\n$Af := \\nabla . \\nabla f = \\sum f_{x_i x_i}$.\nThe Laplace-Beltrami operator, or surface Laplacian, is the naturalgeneralization of the Laplacian to scalar fields defined on curveddomains. If $\u03a3 \\subset \\mathbb{R}^3$ is a regular surface and $f : \u03a3 \\rightarrow \\mathbb{R}$ is a scalarfield, then the Laplace-Beltrami operator on $f$ is defined as thesurface divergence of the surface gradient of $f$, and we denote thisby $\u0394_\u03a3f$ to emphasize the dependence on the surface $\u03a3$. Thus,\n$\u0394_\u03a3f := \\nabla_\u03a3 . \\nabla_\u03a3f$.\nThe surface gradient of the scalar field $f$ (written $\u2207_\u03a3f$), and thesurface divergence of a vector field $F : \u03a3 \\rightarrow \\mathbb{R}^3$ (written $\u2207_\u03a3 \u00b7 F$),can be computed by smoothly extending the fields to fields $f$ and $F$respectively, which are defined on a neighborhood of $\u03a3$ in $\\mathbb{R}^3$.\nThe surface gradient is defined as the projection of the gradientof $f$ into the local tangent plane:\n$\\nabla_\u03a3f = \\nabla f - (\\nabla f \u00b7 n)n$,\nwhere n is the unit surface normal at the point. The surface diver-gence of the vector field $F$ is defined as\n$\\nabla_\u03a3 \u00b7 F = \\nabla \u00b7 F - n^T J(F)n$,\nwhere $J(F) := \\frac{[ \\partial F ]}{[ \\partial x_1,\\partial x_2,\\partial x_3]} $ is the Jacobian of $F$. Since $J(F)n$is the directional derivative of $F$ in the normal direction, then$n^T J(F)n = J(F)n \u00b7 n$ can be thought of as the contribution to thethree-dimensional divergence of $F$ that comes from the normal com-ponent of $F$, and by ignoring the contribution from the normalcomponent, we get the two-dimensional 'surface divergence'. Al-though these expressions depend a-priori on the particular choiceof extension, the surface gradient and surface divergence are in factwell-defined (i.e., any choice of extension will give the same result).\nBy expanding out the definitions of surface gradient and surfacedivergence, we can derive an alternative formula for the surfaceLaplacian ($\u0394_\u03a3f$) in terms of the (Euclidean) Laplacian ($Af$), thegradient ($\u2207f$) and the Hessian ($H(f) = J(\u2207f)^T$) as,\n$\u0394_\u03a3f = \u0394f - 2H\\nabla f \u00b7 n - n^T H(f)n$.\nThe dependence on the surface is captured by the normal function n,and the mean curvature H. Please refer to [Reilly 1982; Xu and Zhao2023] for a derivation. In the case when f is a 'normal extension' -i.e., it is locally constant in the normal directions close to the surfacethen the second and third terms disappear, meaning that $\u0394_\u03a3f$ isconsistent with Af."}, {"title": "4.2 Spectrum of the Laplace Beltrami Operator", "content": "Given two scalar functions, $f$ and $g$, defined on the same regularsurface $\u03a3$, we can define their inner product to be,\n$(f, g)_{L^2(\u03a3)} := \\int_\u03a3 fg dA$.\nThen, the $L^2$ norm of a scalar function can be expressed as,\n$||f||_{L^2(\u03a3)} = \\sqrt{(f,f)_{L^2(\u03a3)}} = \\sqrt{\\int_\u03a3 |f|^2 dA}$.\nThe Laplace Beltrami operator, $\u0394_\u03a3$, is a self-adjoint linear operatorwith respect to this inner product. This means that there is a set ofeigenfunctions of $\u0394_\u03a3$ that form an orthonormal basis for the space$L^2(\u03a3)$ - the space of 'well-behaved' scalar functions on $\u03a3$ that havea finite $L^2$-norm. This basis is analogous to the Fourier basis for thespace of periodic functions on an interval.\nIn the discrete mesh case, the eigenfunctions of the Laplace-Beltrami operator are computed by diagonalizing a sparse matrix[Bunge et al. 2023; L\u00e9vy and Zhang 2010] based on the input mesh.In the continuous case, however, we no longer have a matrix rep-resentation for $\u0394_\u03a3$. Instead, we exploit a functional analysis resultthat describes the first k eigenfunctions of $\u0394_\u03a3$ as the solution to aminimization problem, as described next.\nFirst, we define the Rayleigh Quotient of a scalar function f to bethe Dirichlet energy of the scalar function, divided by the squared$L^2$-norm of the function:\n$Q_\u03a3(f) := \\frac{||\\nabla_\u03a3 f||_{L^2(\u03a3)}}{||f||_{L^2(\u03a3)}}$.\nThen, the eigenfunctions of $\u0394_\u03a3$ are the minimizers of the RayleighQuotient, as given by,\n$Y_0, Y_1, ..., Y_{k\u22121} = argmin_{Y_i} \\sum_{i=0}^{k-1} Q_\u03a3(Y_i)$\nsuch that $(Y_i, Y_j)_{L^2(\u03a3)} = 0$ for all $i \\neq j$.\nWe use $Y_0$ to represent the first eigenfunction, which is always aa constant function. The constraint states that the eigenfunctions areorthogonal inside the inner-product space $L^2(\u03a3)$.\nIn addition, the Rayleigh Quotient of the eigenfunction $Y_i$ is thepositive of the corresponding eigenvalue, i.e.,\n$\u0394_\u03a3Y_i(x) = \u2212Q_\u03a3(Y_i) Y_i(x) \\forall x \u2208 \u03a3$.\nPhysically, the eigenfunctions of $\u0394_\u03a3$ represent the fundamentalmodes of vibration of the surface $\u03a3$. The Rayleigh Quotient of $Y_0$ iszero, and as the Rayleigh quotient increases, the vibrational modesincrease in energy and the eigenfunctions produce more 'intricate'(high frequency) patterns."}, {"title": "5 SPECTRAL ANALYSIS USING SNS", "content": "To find the continuous eigenmodes of the Laplace-Beltrami operatoron an SNS, we require a continuous and differentiable representationfor scalar functions defined on the surface. In our framework, werepresent such smooth scalar fields on $S^2$ by MLPs, $g_\u03b7$, from $\\mathbb{R}^3$ to$\\mathbb{R}$. We only ever evaluate the MLP $g_\u03b7$ on $S^2 \u2208 \\mathbb{R}^3$, but becausethe domain is $\\mathbb{R}^3$ then it defines an extension of the scalar field, and thisallows us to compute $\u2207g_\u03b7$. Then, if $S_{\\Theta} : \\mathbb{R}^3 \\rightarrow \\mathbb{R}^3$ is an SNS, anysmooth scalar field $h$ defined on $S_{\\Theta}$ can be defined implicitly by theequation,\n$h o S_{\\Theta} = g_\u03b7$.\nApplying the chain rule to the implicit equation, we can computethe gradient of $h$:\n$\\nabla h = (J(S_{\\Theta})^T)^{-1}\u2207g_\u03b7$,\nwhich allows us to compute $\u2207_\u03a3h$ and $Q_\u03a3(h)$ without explicitly computing h.\nTo optimize for a (smooth) scalar function $g_{\u03b7_k}$ so that $h$ approxi-mates the kth non-constant eigenfunction of the Laplace-Beltramioperator, we use gradient descent to optimize the weights, $\u03b7_k$. Weuse a combination of two loss terms:\n$L_{Rayleigh} := Q_\u03a3(h)$ and $L_{ortho} := \\sum_{i=0}^{k-1} (h,h_i)_{L^2(\u03a3)}$,\nwhere we sequentially compute the eigenfunctions, and $h_i$ is the ithsmallest eigenfunction to be found. Recall that we represent eachsuch eigenfunction using a dedicated MLP (except for $h_0$, which is"}, {"title": "6 EVALUATION", "content": "Implementation Details. All of our networks in our experimentswere trained on Nvidia GeForce P8 GPUs. The SNS network is aResidual MLP with input and output of size three, and eight hiddenlayers with 256 nodes each. The networks used to represent scalarfields (in the eigenfunction and heat flow experiments) are verysmall Residual MLPs with input size three, output size three andtwo hidden layers with 10 nodes each. (To make the output value ascalar, we take the mean of the three output values.) For both scalarfields and SNS, the activation function is 'Softplus'. We use theRMSProp optimizer with a learning rate of 10-4 and a momentumof 0.9. We trained each SNS for up to 20,000 epochs (eight-to-tenhours), and we fixed the normals regularization coefficient to be10-4. The time per epoch increases slightly with the number ofvertices in the mesh, but for simpler shapes (such as the sphere) theoptimization converges in fewer epochs.\nEigenfunction Optimization. In the eigenfunction experiment, weused the initial parameter values $\u03bb_{ortho} = 10^3$ and $\u03bb_{unit} = 10^4$and then we reduced the coefficients linearly to $\u03bb_{ortho} = 1$ and $\u03bb_{unit} = 10^2$, over 10,000 epochs. These settings prevent the scalarfield from collapsing to the zero-function during the initial stagesof optimization, whilst allowing the Rayleigh Quotient to dominatethe loss in the later stages. We optimized each eigenfunction for upto 40,000 epochs (approximately six hours on our setup).\nWe generate M = 100, 000 points for the initial uniform distribu-tion on the sphere, and we use $N_{target} = 10,000$ in the rejectionsampling process. The points stay fixed during each optimizationstage. We believe that this is a possible reason why the Rayleigh Quo-tient computed on the training points is often slightly lower thanthe Rayleigh Quotient that is computed with a different, larger setof uniform samples, and a better sampling strategy might improvethe accuracy of our optimization.\nComparison against Analytic Functions. For some of our SNS com-putations, there exist special cases for which an exact analytic so-lution can be calculated. To test our estimates of curvature andprincipal curvature directions, we overfitted an SNS to a mesh ofthe closed surface with the parametrization\n$r(\u03b8, \u03c6) = (1+0.4 \\sin^2 \u03b8 \\sin(6\u03b8) \\sin(64)) (\\sin \u03b8 \\cos \u03c6; \\sin \u03b8 \\sin \u03c6; \\cos \u03b8) .$\nWe used the Matlab Symbolic Math Toolbox to compute the nor-mals, mean curvature, Gauss curvature and principal curvaturedirections in closed form. In Figure 3, we show these differentialquantities on the mesh of the analytic surface and on the SNS. Thecomputed curvatures align extremely closely, therefore the over-fitting quality is very high and the curvature computations areaccurate. There is a slight discrepancy at the poles, because wehave used a global analytic parametrization which not valid where$\u03b8 \u2208 \\{0, \u03c0\\}$. On the other hand, in the SNS formulation we use selec-tive co-ordinate relabelling to avoid issues related to using sphericalpolar co-ordinates close to the poles."}, {"title": "7 CONCLUSION", "content": "We have presented spherical neural surfaces as a novel represen-tation for genus-0 surfaces along with supporting operators in theform of differential geometry estimates (e.g., normals, First and Sec-ond Fundamental Forms), as well as surface gradients and surfacedivergence operators. We also presented how to compute a contin-uous Laplace Beltrami operator and its lowest spectral modes. Wedemonstrated that ours, by avoiding any unnecessary discretiza-tion, produces robust and consistent estimates even under differentmeshing concerning vertex sampling as well as their connectivity.\nLimitations and Future Work\nBeyond genus-0 surfaces. Since our SNSs rely on a sphere for theparametrization, it limits us to genus-0 surfaces. If we could choosefrom a range of canonical surfaces (e.g., torus), this would allow usto seamlessly process higher genus shapes and, possibly, to produceneural surfaces without such high distortions. Another option wouldbe to rely on local parametrizations, but then we would have toconsider blending across overlapping parametrizations.\nSpeed. A severe limitation of our current realization is its longrunning time. While we expect that better and optimized imple-mentations would increase efficiency significantly, we also need tomake changes to our formulation. Specifically, our current spectralestimation is sequential, which is not only slow, but leads to anaccumulation of errors for later spectral modes. Hence, in the future,we would like to jointly optimize for multiple spectral modes.\nSNS from neural representations. We demonstrated our setup mainlyon mesh input and also on neural input in the form of deepSDF. Inthe future, we would like to extend our SNS to other neural represen-tations in the form of occupancy fields or radiance fields. However,this will require locating and sampling points on the surfaces, whichare implicitly encoded - we need the representation to provide a pro-jection operator. We would also like to support neural surfaces thatcome with level-of-detail. Finally, an interesting direction would beto explore end-to-end formulation for dynamic surfaces encodingtemporal surfaces with SNS, and enabling optimization with lossterms involving first/second fundamental forms as well as Laplace-Beltrami operators (e.g., neural deformation energy)."}]}