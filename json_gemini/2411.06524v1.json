{"title": "Does This Summary Answer My Question?\nModeling Query-Focused Summary Readers with Rational Speech Acts", "authors": ["Cesare Spinoso-Di Piano", "Jackie Chi Kit Cheung"], "abstract": "Query-focused summarization (QFS) is the task\nof generating a summary in response to a user-written query. Despite its user-oriented nature,\nthere has been limited work in QFS in explicitly\nconsidering a user's understanding of a generated summary, potentially causing QFS systems to underperform at inference time. In this paper, we adapt the Rational Speech Act (RSA)\nframework, a model of human communication,\nto explicitly model a reader's understanding\nof a query-focused summary and integrate it\nwithin the generation method of existing QFS\nsystems. In particular, we introduce the answer\nreconstruction objective which approximates a\nreader's understanding of a summary by their\nability to use it to reconstruct the answer to\ntheir initial query. Using this objective, we are\nable to re-rank candidate summaries generated\nby existing QFS systems and select summaries\nthat better align with their corresponding query\nand reference summary. More generally, our\nstudy suggests that a simple and effective way\nof improving a language generation system de-\nsigned for a user-centered task may be to ex-\nplicitly incorporate its user requirements into\nthe system's generation procedure.", "sections": [{"title": "1 Introduction", "content": "In automatic summarization, there has been a grow-ing interest in pushing summarization towards\nmore user-centered objectives (D\u00edaz and Gerv\u00e1s,\n2007; Lerman et al., 2009; Yan et al., 2011; Bhatnagar et al., 2023). One popular approach has been\nto provide summarization systems with additional\nuser preferences, such as user queries (Dang, 2005;\nDaum\u00e9 and Marcu, 2006; Baumel et al., 2018; Vig\net al., 2022). In this redefined query-focused sum-marization (QFS) setting, a system should provide\na concise and factual summary of a source doc-ument while additionally providing the necessary\ninformation for the user to answer their initial query\n(or queries). For instance, in Figure 1, a Llama 3model is tasked with summarizing an article about\nthe movie \"A Wrinkle in Time\u201d while answering\nthe question \"Is \u201cA Wrinkle in Time\" worth watch-ing?\".\nDespite the more user-oriented nature of QFS,\nmost QFS systems limit their considerations re-garding how their generated summaries may be\nperceived and used by their readers. In particular,\nduring the generation process, most QFS systems\ndo not explicitly consider the extent to which their\ngenerated summary will allow the user to actually\nanswer their initial query. Instead, existing QFS\nsystems and their generation procedures consider\ntheir users indirectly by, for instance, integrating\nquery-answer relevance scores into their system's\nfine-tuning stage (Sotudeh and Goharian, 2023).\nMeanwhile, work in personalized summarization\nhas already shown that explicitly considering \"user\ncharacteristics\" leads to summaries which better\naccount for user needs (Li et al., 2019).\nAs a result, in this paper, we seek to provide\nan explicit model of the reader of a query-focused\nsummary. We hypothesize that this explicit user"}, {"title": "Related Work", "content": "Query-focused summarization (QFS) Some of\nthe first concerted efforts in developing QFS sys-tems date back to Dang (2005, 2006b) where QFS\nwas presented as a shared task. Since then, QFS\nhas received increasing attention due, in part, to it\nbeing more user-centered in nature than traditional\ngeneric summarization (Kry\u015bci\u0144ski et al., 2019; Vig\net al., 2022).\nThere have been several proposed modeling\napproaches to QFS, the two most common ap-proaches being a two-step retrieval-abstraction\nmethod and an end-to-end method (Vig et al., 2022).\nIn retrieval-abstraction methods such as Egonmwan\net al. (2019), the summarization system first uses\nthe provided query to retrieve the most relevant sen-tences from the document to be summarized and\nsubsequently passes those sentences to an abstrac-tive module tasked with providing a concise and\nfactual overview of those sentences. In end-to-end\nmethods (Baumel et al., 2018; Xie et al., 2020),\na summarization system is fine-tuned on source\ndocument, question and reference summary triples\nallowing it to directly generate a summary from\na new source document and question pair at infer-ence time. Other notable approaches which do not\ncleanly fit within these two paradigms include Xu\nand Lapata (2020b) which leverage generic sum-maries to improve QFS systems and Xu and Lapata\n(2021) which develop a latent query model.\nIn closer connection with our work, there have"}, {"title": "3 Answer Reconstruction: An RSA\nModel of a QFS Reader", "content": "We formalize our explicit model of a query-focused\nsummary reader firstly by discussing the RSA\nframework in general and subsequently by for-mally discussing how we adapt it to model a query-focused summary reader through the answer re-construction objective. The intuition and driving\nprinciple behind our proposed method is that user-oriented language generation tasks such as QFS\ninvolve user requirements which can naturally be\nintegrated into the generation procedure through\nthe RSA framework. We hypothesize that this ex-plicit consideration of users will favour outputs\nwhich better align with their expectations of user-centered language generation systems."}, {"title": "RSA and human communication", "content": "The RSA\nframework was originally introduced to model hu-man communication and, more specifically, to ex-plicitly account for a listener in the context of mod-eling pragmatic phenomena like conversational im-plicature (Frank and Goodman, 2012; Goodman\nand Stuhlm\u00fcller, 2013; Bergen et al., 2016). In\nthis framework, a pragmatic speaker, S\u2081, attempts\nto convey a message with some meaning, m, by\ngenerating an utterance, u. To do so, the prag-matic speaker uses a literal speaker, So, to first\ngenerate candidate utterances U where the utter-ances u \u2208 U are generated from a uniform distribu-\ntion, $P_{S_{0}}(U|M = m)$, over utterances with literal\nmeaning matching m. The pragmatic speaker then\nexplicitly posits a pragmatic listener, L1, to approx-imate the usefulness of each candidate utterance\nu \u2208 U. In particular, S\u2081 re-scores each candidate\nutterance, u \u2208 U, by using the probability that L1\nrecovers the intended meaning of their message\nwhen observing u, $P_{L_{1}}(M = m|U = u)$. Finally,\nthe pragmatic speaker combines these two proba-bilities with a rationality parameter A to provide a\nfinal score for each utterance u \u2208 U represented\nby\n$S_{1}(u|m; \\lambda) = P_{S_{0}}(u|m)^{1-\\lambda} \\cdot P_{L_{1}}(m|u)^{\\lambda}$   (1)\nWe drop the $P_{S_{1}}$ notation for the pragmatic speaker as,\nexcept for $\\lambda$ = 0, 1, this product does not necessarily induce\na valid probability distribution."}, {"title": "RSA and QFS", "content": "To adapt the RSA model to QFS,\nwe provide new definitions for the speakers, S0, S1,\nand listener, L1, which we refer to as summarizers,\nS0, S1, and reader, R\u2081, respectively. In addition,\nwe base the intended meaning of a query-focused\nsummary on its suitability in providing enough\ninformation for R\u2081 to correctly answer their initial\nquery.\nIn our adapted RSA framework, we define the\nliteral summarizer as any QFS system which takes\nas input some source text x and query q and which\ninduces a probability distribution over candidate\nsummaries, $P_{S_{0}}(Y|X = x, Q = q)$. We think of\nexisting QFS systems as literal summarizers since,\nsimilar to the original RSA formulation, they do\nnot directly consider the readers of their summaries.\nIn our formulation, given x and q, S\u2081 leverages So\nas a way to sample n candidate summaries:\n$y_{i} \\sim P_{S_{0}}(Y|X = x, Q = q), i = 1...\\eta$\t(2)\nTo implement our definition of the intended mean-ing of a query-focused summary, we introduce the\nanswer reconstruction objective. This objective\nallows re-scoring a candidate summary $\u0177_i$ based on\nR\u2081's ability to generate the answer to the original\nquery q by using $\u0177_i$ instead of x. To do so, we\nsuppose that S\u2081 has access to a question-answer\n(QA) generation system F which takes as input a\ndocument d and a related question $q^{*3}$ and provides\nan answer $F(D = d, Q^{*} = q^{*}) = a$ along with\nthe probability for that answer, $P_{F}(A = a|D =\nd, Q^{*} = q^{*})$.\nThus, given x and q, S\u2081 models R\u2081's understand-ing of $\u0177_i$ by generating an answer $a = F(x, q)$ and\nby evaluating the probability of R\u2081 reconstructing\na via the same QA process. That is,\n$P_{R_{1}}(A = a|\\hat{Y} = \\hat{y}_{i}, Q = q) :=$\n$P_{F}(A = a|D = \\hat{y}_{i}, Q^{*} = q)$\t\t\t\t\t(3)\nFinally, a rationality parameter $\\lambda$ \u2208 [0, 1] is used\nby S\u2081 to regulate the importance assigned to R\u2081's\nanswer reconstruction ability in selecting the final\nsummary. That is, S\u2081's final score of a candidate\nsummary $\u0177$ for a source text x with query q and\ncorresponding answer a = F(x, q) will be:\n$S_{1}(\\hat{y}_{i}|x, q; \\lambda) = P_{S_{0}}(\\hat{y}_{i}|x, q)^{1-\\lambda} \\cdot P_{R_{1}}(a|\\hat{y}_{i}, q),$\t\t\t\t\t(4)\nWe leave the consideration of multiple source documents\nas well as multiple user queries to future work.\nWe use d and q* in our QA notation to disambiguate with\nour QFS notation for the source text x and its query q."}, {"title": "4 Experiments", "content": "We describe the datasets that we use to run our ex-periments on our QFS setting. Notably, we do\nnot use the DUC datasets (Dang, 2006a) since\nwe limit our focus to single-document and single-query-focused summarization. We provide sum-mary statistics for each dataset in Table 1."}, {"title": "4.1 Datasets", "content": "MultiOpEd The MultiOpEd dataset (Liu et al.,\n2021) is a collection of news editorials contain-ing opinion articles and summaries for each article\nwhich also provide the answer to a specific ques-tion.\nQMSum\nThe QMSum dataset (Zhong et al.,\n2021) is a query-focused meeting summarization\ndataset where the source texts are excerpts from\ndifferent meeting transcripts. Each excerpt has an\nassociated question and reference summary sum-marizing the excerpt while providing an answer to\nthe question.\nSQUALITY The SQUALITY dataset (Wang\net al., 2022) is a collection of short stories with\nmultiple queries and corresponding query-focused\nsummaries."}, {"title": "4.2 Answer-Reconstruction-Based QFS\nPipeline", "content": "To test our answer-reconstruction-based model of\na QFS reader, we implement a pragmatic summa-rizer, Answer-Rec, which leverages a literal sum-marizer to generate candidate summaries and a QA\nsystem to re-rank them based on their suitability\nas answers to the initial query. In particular, for\neach dataset and source document-question pair,\nwe leverage a literal summarizer to generate a set\nof 10 candidate summaries. In parallel, we leverage\nan existing QA generation system to generate an-swers for each source document and question pair.\nWe then combine the likelihood scores produced by\nthe literal summarizer, $P_{S_{0}}$, and the answer recon-struction score, $P_{R_{1}}$, as defined in Equation 3 to\nre-rank the set of candidate summaries. As a result,\nthe summary selected by our pragmatic summarizer\nis:\n$arg\\ max\\ P_{S_{0}}(\\hat{y}_{i}|x, q)^{1-\\lambda} \\cdot P_{R_{1}}(a|\\hat{y}_{i}, q)\t$(5)\n$S_{1}(\\hat{y}_{i}|x,q;\\lambda)$\nWe use multiple values of A which we further detail\nin Appendix A.\nWe provide additional details related to our sum-marization and QA systems in the next subsections.\nAll the model checkpoints we use for our summa-rization and QA systems are taken from Hugging-Face\u2074 and all fine-tuning is done through PyTorch\nLightning.\u2075 We run fine-tuning and inference on\na variable number of NVIDIA Quadro RTX 8000\n48GB GPUs based on the GPU memory require-ments of the corresponding model architecture and\nsetting.\u2076"}, {"title": "4.2.1 Literal Summarizers", "content": "We use BART and Llama 3 as the literal summariz-ers for all of our datasets. We experiment with sev-eral standard decoding methods including standard\nsampling, top-k sampling (Fan et al., 2018) and\nnucleus sampling (Holtzman et al., 2019). Since\nsummary selection is only meaningful when select-ing from a diverse pool of candidate summaries, we\nfollow decisions by Holtzman et al. (2019) and Aralikatte et al. (2021) to generate diverse summaries\nby setting k = 640 in top-k sampling and p = 0.95\nin nucleus sampling. Further decoding hyperpa-rameters are discussed in Appendix A. In addition,"}, {"title": "4.2.2 Question-Answer Generation", "content": "Given the reported versatility of Llama 3 (Dubey\net al., 2024), we rely on the same checkpoint as\nthe literal summarizer to implement the QA system\nF and its probability distribution PF. To generate\nan answer to a question q about a source docu-ment x, a = F(q, x), we prompt Llama 3 with\na QA prompt (Appendix B) and use beam search\ndecoding with a beam size of 5. The pragmatic\nreader's answer reconstruction score for a candi-date summary $\u0177$ is thus the probability of Llama\n3 generating a given the same prompt, but with x\nreplaced with $\u0177$."}, {"title": "5 Results", "content": "Summary Quality", "5.1": null}, {"title": "Text Quality", "5.2": null}, {"title": "6 Conclusion", "content": "In conclusion, we have argued that explicitly mod-eling readers of query-focused summaries can pro-duce summaries which better align with user re-quirements. Specifically, we demonstrated that\nmodeling QFS readers using the RSA framework\nwith a task-specific answer reconstruction objec-tive leads to summaries which align more closely\nwith reference summaries compared to those se-lected with either a generic source reconstruction\nobjective or the likelihood scores of existing QFS\nsystems. In addition, we have shown that existing\nreader-unaware QFS systems may not be fully real-izing their potential to generate summaries which\nbest reflect their users' needs. Augmenting their\ngeneration procedure with a hybrid answer-source\nreconstruction objective as a model of the reader\nmay be a way to close this gap in performance.\nBeyond QFS, our findings suggest that existing lan-guage generation systems could better serve their\nusers by explicitly considering them during the\nlanguage generation process rather than implicitly\nthrough the patterns learnt during the finetuning\nstage."}, {"title": "Limitations", "content": "In this work, we aimed to showcase the benefit\nof modeling query-focused summary readers by\naugmenting existing QFS systems with a reader-aware generation process. Due to resource con-straints, we only considered two end-to-end QFS\nsystems, BART and Llama 3, and no two-step\nretrieval-abstraction QFS systems. We leave the\nexploration of other existing QFS systems as future\nwork. Moreover, we note that noise is inevitably\nintroduced by using a QA generation system rather\nthan gold answers in our reader model. However,\nwe believe that this is a reasonable limitation given\nthat it is unrealistic to assume access to gold an-swers at inference time.\nIn addition, though they are commonly used\nthroughout the summarization and language gener-ation community, we acknowledge that our evalua-tion methods suffer from well-documented valid-ity issues. For instance, the goodness of a gener-ated summary and its suitability for a reader may\nnot correlate with word overlap metrics (ROUGE"}]}