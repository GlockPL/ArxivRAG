{"title": "Graph Self-Supervised Learning with Learnable Structural and Positional Encodings", "authors": ["Asiri Wijesinghe", "Hao Zhu", "Piotr Koniusz"], "abstract": "Traditional Graph Self-Supervised Learning (GSSL) struggles to capture complex structural properties well. This limitation stems from two main factors: (1) the inadequacy of conventional Graph Neural Networks (GNNs) in representing sophisticated topological features, and (2) the focus of self-supervised learning solely on final graph representations. To address these issues, we introduce GenHopNet, a GNN framework that integrates a k-hop message-passing scheme, enhancing its ability to capture local structural information without explicit substructure extraction. We theoretically demonstrate that GenHopNet surpasses the expressiveness of the classical Weisfeiler-Lehman (WL) test for graph isomorphism. Furthermore, we propose a structural- and positional-aware GSSL framework that incorporates topological information throughout the learning process. This approach enables the learning of representations that are both sensitive to graph topology and invariant to specific structural and feature augmentations. Comprehensive experiments on graph classification datasets, including those designed to test structural sensitivity, show that our method consistently outperforms the existing approaches and maintains computational efficiency. Our work significantly advances GSSL's capability in distinguishing graphs with similar local structures but different global topologies.", "sections": [{"title": "1 Introduction", "content": "Graph Neural Networks (GNNs) are powerful deep learning networks for graph-structured data, employed by various tasks [36, 68, 88, 83, 65, 98, 79, 37, 57, 32, 2, 43, 18]. While most GNNs focus on semi-supervised learning, Self-Supervised Learning (SSL) learns graph representations without human annotations.\nGraph Self-Supervised Learning (GSSL) often outperforms supervised methods in both node-level and graph-level downstream tasks [82, 106, 100, 94, 104, 102, 95, 93, 97]. In this paper, we focus on graph classification, a crucial graph-level task with significant applications in areas such as molecular property prediction, social network analysis, and protein function classification [25, 83, 22, 96, 103]. Graph classification presents unique challenges compared to node-level tasks as it must capture global structural information across different graphs, not just local neighborhoods. Graphs can vary significantly in size and structure, demanding more flexible and expressive models. To obtain effective graph-level representations, models must aggregate information from all nodes and edges while preserving discriminative structural features.\nDespite GSSL's success, they often fail to fully leverage the expressive power of GNNs, by not uti-lizing both topological and positional information for graph classification. Topological information captures the local structural relationships within the graph through the k-hop neighborhood substructure patterns (e.g., triangles, cycles), while positional information, derived from Laplacian eigenvectors or random-walk diffusion, reflects the nodes' relative positions within the graph's global structure. The lack of topological and positional focus prevents GSSL from distinguishing between graphs with similar local structures but different global topologies. Specifically, in graphs where nodes may have identical local structures (e.g., isomorphic or symmetrical nodes), relying only on neighboring features is inadequate for differentiation. Positional information is critical for enabling GNNs to distinguish such nodes, even when their connectivity patterns are similar. Isomorphic nodes, which cannot be differentiated based solely on their structural information, present a particular challenge. By incorporating positional encodings, GNNs can leverage this additional context to break symmetry, facilitating better differentiation among nodes. This enhancement improves the model's recognition of unique identities, leading to more accurate predictions in graph-related tasks.\nThe limitations of current GSSL methods can be attributed to two main factors: GNN Architecture Limitations and Self-Supervised Learning Constraints. Conventional GNNs typically aggregate information from immediate neighborhoods, often missing crucial structural differences that exist beyond local structures. For instance, GIN [83] has shown that certain GNN-based methods [36, 68] are less effective at distinguishing graph structures compared to Weisfeiler-Lehman (WL) based methods. Furthermore, current GSSL methods [69, 63, 26, 87] often fail to fully leverage the complementary nature of structural and positional information, which hinders their ability to differentiate non-isomorphic graphs with similar local attributes but different global topologies.\nBuilding on these insights, we develop a framework that fundamentally reimagines graph representation learning by innovating both GNN architecture and the self-supervised learning process. Our goal is to significantly enhance the expressiveness and representational capacity of GSSL in distinguishing non-isomorphic graphs with similar local structures but different global topologies. To this end, we focus on two main components:\n1. GenHopNet GNN. A novel GNN architecture designed to capture complex structural information beyond immediate neighborhoods by a k-hop message-passing scheme that expands the receptive field of each node, letting the model capture long-range dependencies and global structural information.\n2. Structural- and Positional-aware Self-Supervised Learning. A new self-supervised learning framework that preserves and uses crucial topological information by incorporating both structural and positional information into learning to overcome the sole focus on final graph representations.\nContributions. Below we summarize our main contributions:\ni. We introduce GenHopNet, a GNN framework that implements a k-hop message-passing aggregation scheme and surpasses the expressiveness of the WL test.\nii. We propose a structural- and positional-aware GSSL framework, StructPosGSSL, for GNN pre-training, enabling the learning of representations invariant to specific structural and feature augmentations while preserving topological and positional information.\niii. With extensive experiments on both real-world and synthetic datasets we demonstrate that our StructPosGSSL achieves superior performance on most graph classification benchmarks."}, {"title": "2 Related Work", "content": "GNNs are designed to effectively process and represent graph-structured data, and they come in various flavors, including GCN [36], GAT [68], GraphSAGE [24], GIN [83], DFNets [78], linear SSGC [101], GReLU [92], etc. Such models distinguish representations of graphs based on their data labels. However, annotating graph data, such as identifying categories of biochemical molecules, often requires specialized expertise, making it challenging to obtain large-scale labeled graph datasets [87]. This challenge highlights a key limitation of supervised graph representation learning.\nContrastive Learning (CL) stands out as a highly effective self-supervised technique embedding unlabeled data [40]. By bringing similar examples closer together and pushing dissimilar ones apart,"}, {"title": "3 An Expressive and Generalizable k-hop Message Passing Framework", "content": "In this section, we introduce the Expressive and Generalizable Message-Passing (EGMP) frame-work, designed to incorporate learnable local structural information through an aggregation method that leverages the k-hop neighborhood without the need for explicit extraction of local substructure patterns. We provide a theoretical analysis demonstrating how k-hop GNNs within this framework can achieve greater expressiveness than 1-WL.\nLet $G = (V, E, A)$ be an undirected graph with a set of nodes $V$ and a set of edge $E$, where $|V| = m$, $|E| = e$, and $A \\in \\mathbb{R}^{m \\times m}$ is the adjacency matrix. Let $L = D \u2013 A$ be a Laplacian matrix, where a diagonal matrix $D \\in \\mathbb{R}^{m \\times m}$, and $D_{ii} = \\sum_{j} A_{ij}$. $L$ is a real symmetric matrix diagonalizable as $L = U\\Lambda U^{H}$. Moreover, $U = \\{u_{i}\\}_{i=1}^{m} \\in \\mathbb{R}^{m}$ are orthogonal eigenvectors, $\\Lambda = \\text{diag} ([\\lambda_{1}, ..., \\lambda_{m}]) \\in \\mathbb{R}^{m \\times m}$ are real eigenvalues, and $U^{H}$ is a hermitian transpose of $U$.\nLet $\\{\\cdot\\}$ represent a multiset Let $A^k_v = \\{A^k_{vu}\\}_{v,u}$. and define $\\bar{A^k_{vu}} = \\frac{A^k_{vu}}{\\sum_{u \\in N^k(v)} A^k_{vu}}$ refers to a normalized value of $A^k_{vu}$, and $X \\in \\mathbb{R}^{m \\times d'}$ be the matrix of input node attributes with each $x_v \\in \\mathbb{R}^{d'}$ corresponding to each vertex $v \\in V$. We indicate the feature vector of vertex $v$ at the $t$th layer as"}, {"title": "4 Generalizable k-hop Network", "content": "Below, we present the GNN model design based on the EGMP framework that we proposed in the previous section. We introduce a new GNN model called GenHopNet (Generalizable k-hop Network), which utilizes an aggregation scheme based on our generalized message-passing framework. We demonstrate that the expressive power of GenHopNet exceeds those of the 1-WL. For each vertex $v \\in V$, the feature vector for the $(t + 1)$th layer is produced by:\n$h_v^{(t+1)} = MLP ( (1+e) h_v^{(t)} +  \\sum_{u \\in N(v)}  ( A_{vu}^{(0)} + e^b_u + e^c_u) h_u^{(t)}  + \\sum_{k=2}^K \\overline{A_{vu}}  \\Bigg( \\sum_{k=2}^K A_{vu} h_u^{(t)} + \\sum_{k=2}^K m^{(t)}_{closed\\_walks} \\Bigg)$"}, {"title": "5 Graph Self-Supervised Learning Framework", "content": "Below, we introduce Structural and Positional GSSL (StructPosGSSL), a new class of graph self-supervised learning framework based on structural and positional information within graphs.", "sections": [{"title": "5.1 Data Augmentation for Graph", "content": "The goal of data augmentation is to produce consistent, identity-preserving positive samples of a specific graph. In this work, we use two main types of augmentation strategies: structural augmentation and feature augmentation [87]. In structural augmentation, three distinct strategies are considered: (1) Subgraph Induction by Random Walks (RWS), (2) Node Dropping (ND), and (3) Edge Dropping (ED). For feature augmentation, we employ three different approaches: (1) Feature Dropout (FD), (2) Feature Masking (FM), and (3) Edge Attribute Masking (EAM). In our work, we generate different augmented graphs from a single input graph. One view uses augmented attributes of nodes (or edges) and the other view uses topologically augmented graph."}, {"title": "5.2 Expressive Graph Encoders", "content": "For each augmented view, we process it through two distinct GNN encoders: (i) the structural encoder (the proposed GenHopNet) snd (ii) the positional encoder. We initiate the positional feature vectors using the graph's Laplacian eigenvectors, as outlined in [19]. This second encoder, also a GNN, applies Eq.1 with the COMBINE function, i.e., $h_{v,pos}^{(t)} = COMBINE (h_{v,pos}^{(t)}, h_{v,pos}^{(t)}, m^{(t)}_{local-pat} (v))$ and leverages the spectral properties of the graph Laplacian.\nBy focusing on the spectral characteristics of the Laplacian, this encoding strategy effectively cap-tures both the local connectivity of nodes and the broader topology of the graph, and the node's \"position\" in that topology, significantly enhancing the model's capability to understand and man-age complex graph structures.\nEach encoder (structural and positional) outputs node representations and a final graph represen-tation for augmented views. We then pass them through another shared projection head (MLP) to obtain the final structural and positional representations for both nodes and graphs. Next, we concatenate the structural features with positional features for node representations and graph rep-resentations separately, ensuring a comprehensive integration of both structural and positional data. To facilitate the end-to-end training of encoders and generate comprehensive node and graph repre-sentations that are independent of specific downstream tasks, we employ the graph- and node-wise contrastive loss functions."}, {"title": "5.3 Training Pipeline", "content": "To enable end-to-end training of the encoders and to develop robust graph and node representations that are independent of downstream tasks, we employ the NT-Xent [14] loss to learn the graph repre-sentations and the VICReg [4] loss to learn the node representations. The NT-Xent loss maximizes"}]}, {"title": "6 Numerical Experiments", "content": "In this section, we evaluate our self-supervised learning framework on graph classification bench-mark tasks. The results from our models are statistically significant with a 95% confidence level. We evaluate StructPosGSSL on graph classification benchmark tasks and compare their performance with leading baselines to address the following questions:\nQ1. How effective are StructPosGSSL in small graph classification tasks based on empirical perfor-mance?\nQ2. How effective are StructPosGSSL in large graph classification tasks based on empirical perfor-mance?\nQ3. How effective is StructPosGSSL for isomorphism testing in synthetic graph classification tasks?\nQ4. How do structural and positional encodings impact the overall performance?\nIn the following sections, we analyze the experimental results to address the four above questions.", "sections": [{"title": "6.1 Experiments on Small Graphs", "content": "We use eight datasets from two categories:\n(1) bioinformatics datasets: MUTAG, PTC-MR, NCI1, and PROTEINS [17, 38, 71, 62]; (2) social network datasets: IMDB-B, IMDB-M, COLLAB and RDT-M5K [84]. We compare our method against fourteen baseline approaches:\n(1) Graph kernel methods: WL-OA [38], RetGK [99], P-WL [60], and WL-PM [56];\n(2) GNN-based methods: PATCHY-SAN [55], CAPSGNN [81], GIN [83], and G3N [74];\n(3) Unsupervised methods: GraphCL [87], MVGRL [26], GCS [76], GALOPA [75], GA2C [47] and GraphCL+HTML [39].\nSpecifically, the GenHopNet encoder models are initially trained in an unsupervised manner, and the resulting embeddings are then passed into a linear classifier to accommodate the labeled data. Then, for fair comparison, we execute our method using ten random splits [105] and apply 10-fold cross-validation and present the best mean accuracy (%) along with the standard deviation. The results are presented in table 2 and 3. We have two settings: (1) StructPosGSSL-SA, which consid-ers structure augmentation, and (2) StructPosGSSL-FA, which considers feature augmentation. In both settings, we employ the Adam optimizer [35], with hidden dimension of 128, weight decay is 0.0003, a 2-layer MLP with batch normalization, 100 epochs, positional encoding dimension of 6, a dropout rate of 0.5, and a temperature scaling parameter of 0.10. We choose a batch size from {32, 64, 128, 256} and a number of hops $k \\in \\{2, 3, 4, 5, 6\\}$. We use $\\lambda_{inv} = 1$, $\\lambda_{var} = 3$, $\\lambda_{cov} = 2$, and $\\alpha = 0.04$ for MUTAG and $\\lambda_{inv} = 1$, $\\lambda_{var} = 25$, $\\lambda_{cov} = 25$, and $\\alpha = 0.005$ for PTC-MR, and $\\lambda_{inv} = 1$, $\\lambda_{var} = 24$, $\\lambda_{cov} = 24$, and $\\alpha = 0.005$ for the remaining datasets. The readout function, as described in [83], is utilized, which involves concatenating representations from all layers to derive a final graph representation.\nTo address Q1, in Tables 2 & 3, StructPosGSSL outperforms the best baseline by 0.4% (PATCHY-SAN), 1.1% (CapsGNN), 1.1% (WL-OA, MVGRL), 0.5% (WL-PM), and 0.2% (GIN, GCS) on the datasets MUTAG, PTC-MR, IMDB-B, IMDB-M, and RDTM5K, respectvely.\nThese gains are a reflection of the inherent characteristics of the datasets. Graphs with smaller di-ameters, i.e., IMDB-B, IMDB-M, PTC-MR, and MUTAG, feature nodes that are closer together, promoting localized interactions that enable GNNs to capture both local and global information effectively, even with few message-passing steps. In such cases, structural encoding with closed walks is particularly beneficial, as it differentiates local structures by capturing repeated node in-teractions and identifying cycles. Conversely, datasets such as NCI1 and COLLAB, with larger diameters, present increased structural complexity, making it challenging to capture patterns using closed walks alone due to the difficulty of accounting for distant node interactions with limited local information."}, {"title": "6.2 Experiments on Large Graphs", "content": "We use five large graph datasets from the Open Graph Benchmark (OGB) [29], comprising one molecular graph dataset (ogbg-moltoxcast, ogbg-moltox21, ogbg-molhiv, ogbg-molpcba) and one protein-protein association network (ogbg-ppa). We compare our approach with the methods that provide results on the aforementioned OGB datasets: GIN+VN [29], GSN [12], GraphSNN [77], DS-GNN (EGO+), DSS-GNN (EGO+) [6], and POLICY-LEARN [5].\nFor large graph datasets, we adopt the same experimental framework as outlined by Hu et al. [29]. Our evaluation process is divided into two distinct learning phases. In the initial phase, the models are trained in a self-supervised fashion using only node features and graph structure without any label data. Subsequently, in the second phase, the representations generated by the GNN encoders during the first phase are fixed in place and employed to train, validate, and test the models using a straightforward linear classifier using the 10-fold cross-validation method.\nWe utilize the Adam optimizer with a learning rate of 0.001, a batch size of 32, dropout of 0.5, positional encoding dimension of 6, and run training for 100 epochs across all datasets. We use a 2-layer MLP with a hidden dimension of 200 and a temperature scaling parameter of 0.10 for both settings. We choose $\\lambda_{inv} = 1$, $\\lambda_{var} = 24$, $\\lambda_{cov} = 24$, and $\\alpha = 0.005$ for all datasets. The classification accuracy results are presented in Table 4.\nTo address Q2, in Table 4, StructPosGSSL consistently outperforms all the baseline methods across all OGB graphs listed. StructPosGSSL surpasses best results of existing GNNs by 0.50% (POLICY-LEARN), 0.55% (DSS-GNN (EGO+)), 0.32% (GIN+VN), 0.24% (GraphSNN), and 0.42% (GIN+VN) on the datasets ogbg-moltoxcast, ogbg-moltox21, ogbg-molhiv, ogbg-ppa, and ogbg-molpcba, respectively."}, {"title": "6.3 Experiments on Synthetic Graphs", "content": "We use 2 publicly accessible datasets: (1) the Circular Skip Link (CSL) dataset [54]; and (2) SR25 [1]. Both benchmarks involve classifying graphs into isomorphism classes. The CSL dataset, initially presented by [54] and frequently utilized to assess graph expressiveness [20], comprises 10 isomorphism classes of 41-node 4-regular graphs, almost all of which can be distinguished by the 3-WL test. SR25 dataset [1] comprises 15 strongly regular graphs, each consisting of 25 nodes, which cannot be distinguished by the 3-WL test.\nWe compare our approach against the five baselines: GCN [36], GIN [28], 3WLGNN [49], 3-GCN [53], and GCN-RNI [1]. We use the Adam optimizer with a learning rate of 0.001, a batch size of 32, dropout of 0.7, positional encoding dimension of 6, a temperature scaling parameter $\\tau$ of 0.10, and run training for 500 epochs across both datasets. We use a 3-layer MLP with a hidden dimension of 200 and a number of hops $k = 3$ for both settings. We choose $\\lambda_{inv} = 1$, $\\lambda_{var} = 25$, $\\lambda_{cov} = 25$,"}, {"title": "6.4 Ablation Analysis of Structural and Positional Encoding", "content": "To showcase the effectiveness of structural and positional information, we perform an ablation study on the following variants:\n\u2022 POS: This variant keeps only Positional (POS) encoding.\n\u2022 CW: This variant keeps only Closed-Walk (CW) information.\nWe performed an ablation study on the StructPosGSSL variants. The results presented in Table 5 demonstrate that the closed-walk structural information plays a key role in performance. Notably,"}]}, {"title": "7 Conclusions", "content": "In conclusion, our proposed StructPosGSSL framework effectively addresses a key limitation in Graph Self-Supervised Learning by improving the capture of topological information. Leveraging the k-hop message-passing mechanism of GenHopNet and the integration of structural and posi-tional awareness, StructPosGSSL exceeds the expressiveness of traditional GNNs and the Weisfeiler-Lehman test. Our experimental results show that the framework delivers superior performance on graph classification tasks, enhancing accuracy while maintaining computational efficiency. This ad-vancement significantly strengthens GSSL's capability to distinguish between graphs with similar local structures but distinct global topologies."}, {"title": "A Appendices", "content": null, "sections": [{"title": "A.1 Laplacian Eigenvectors for Positional Encoding", "content": "Positional features should ideally differentiate nodes that are far apart in the graph while ensuring that nearby nodes have similar features. We use graph Laplacian eigenvectors as node positional fea-tures because they have fewer ambiguities and more accurately represent distances between nodes [67, 20]. Laplacian eigenvectors can embed graphs into Euclidean space, providing a meaningful lo-cal coordinate system while preserving the global graph structure. They are mathematically defined by the factorization of the graph Laplacian matrix as $L = U\\Lambda U^{H}$, where $U = \\{u_{i}\\}_{i=1}^{m} \\in \\mathbb{R}^{m}$ are orthogonal eigenvectors, $\\Lambda = \\text{diag} ([\\lambda_{1}, ..., \\lambda_{m}]) \\in \\mathbb{R}^{m \\times m}$ are real eigenvalues, and $U^{H}$ is a hermitian transpose of $U$. After normalizing to unit length, eigenvectors are defined up to a factor of $\\pm 1$, leading to random sign flips during training. In our experiments, we employ the p smallest non-trivial eigenvectors, with p specified for each experiment. The initial positional encoding vector for each node is computed beforehand and assigned as node attributes during dataset creation."}, {"title": "A.2 Proofs of Lemmas and Theorems", "content": "Theorem 1 The following statement is true: (a) If $\\sum_k A_v^{\\text{Cycle}} \\simeq \\sum_k A_{v'}^{\\text{Cycle}}$ then $\\sum_k A_v \\simeq \\sum_k A_{v'}^{\\text{ClosedWalk}}$; but not vice versa.\nProof. The implication $\\sum_k A_v^{\\text{Cycle}} \\simeq \\sum_k A_{v'}^{\\text{Cycle}} \\Rightarrow \\sum_k A_v \\simeq \\sum_k A_{v'}^{\\text{ClosedWalk}}$ is true because every cycle is a closed walk, but not every closed walk is a cycle. Thus, if two nodes are isomorphic with respect to cycles, they must also be iso-morphic with respect to closed walks. The reverse implication $\\sum_k A_v^{\\text{ClosedWalk}} \\simeq \\sum_k A_{v'}^{\\text{ClosedWalk}} \\Rightarrow \\sum_k A_v \\simeq \\sum_k A_{v'}^{\\text{Cycle}}$ is not true because closed walks can include walks that repeat vertices or edges, which do not qualify as cycles.\nTheorem 2 Let S represent a GNN with an aggregation scheme $\\pi'(\\cdot)$ delineated by Eq. 1-Eq. 4. S exceeds the expressiveness of 1-WL in identifying non-isomorphic graphs, provided that S operates over a sufficient number of hops, where k > 1, and also meets the following criteria:\n(1) $\\pi' (h_v, \\{\\{(A_{vu}, h_u^{(t)}, e^b_{vu}, e^c_{vu})\\} \\}_{u \\in N(v)}\\}, \\{\\{(\\bar{A}_{vu}, h_u^{(t)})\\} \\}_{u \\in N^k(v)}\\}, \\{\\{(\\bar{A}_{vu}, h_u^{(t)})\\} \\}) $ is injective (Eq. 5);\n(2) The graph-level readout function of S is injective (Eq. 6).\nProof. For the proof, we proceed in two steps. First, we assume the existence of two graphs G1 and G2 that are distinguishable by 1-WL but indistinguishable by S, and we demonstrate a contra-diction. We consider the iterations of 1-WL from 1 to k, where k is the number of hops. If 1-WL distinguishes G\u2081 and G2 using the information up to the k-th iteration but S cannot, it implies the existence of k-hop local neighborhood subgraphs Gi and Gj with different multisets of W\u2081 \u2208 W1, W2 \u2208 W2, W3 \u2208 W3. However, by the injectiveness property of $\\pi'$, S should yield different tuple (W1, W2, W3) for Gi and Gj, contradicting the assumption. In the second step, we prove the exis-tence of at least two graphs distinguishable by S but indistinguishable by 1-WL. This step involves providing specific examples of such graphs, illustrating S's enhanced expressiveness compared to 1-WL. By completing these steps, we establish the validity of Theorem A.2, confirming that under the specified conditions, S indeed surpasses the expressiveness of 1-WL in identifying non-isomorphic graphs."}, {"title": "A.3 Ablation Analysis of Loss Function", "content": "To showcase the effectiveness of each element in the loss function, we perform an ablation study on the following variants:\n\u2022 NoVICReg: This variant excludes the VICReg regularization term from the overall loss.\n\u2022 Inv: This variant keeps only the Invariance term in the VICReg regularization term.\n\u2022 Var: This variant keeps only the Variance term in the VICReg regularization term.\n\u2022 Cov: This variant keeps only the Covariance term in the VICReg regularization term.\nWe conducted the ablation study on the StructPosGSSL-SA variant. The results shown in Table 6 indicate that the Invariance, Variance, and Covariance terms are crucial to the performance. Specifically, the covariance term has the greatest impact on performance, whereas the invariance term has the least effect across all datasets, as detailed in Table 6. Specifically, as shown in Table 6, performance on graphs decreases by 3.5% to 5.1% with the NT-Xent+NoVICReg loss function, by 2.4% to 4.1% with NT-Xent+Inv, by 1.7% to 3.0% with NT-Xent+Var, and by 1.0% to 1.9% with NT-Xent+Cov, compared to the combined NT-Xent+VICReg loss function."}, {"title": "A.4 Comparison under Different \u03b1", "content": "To evaluate the impact of the regularization term \u03b1 on the performance of our StructPosGSSL framework, we conduct experiments by evaluating StructPosGSSL-SA across six datasets (MUTAG, PTC-MR, PROTEINS, IMDB-B, IMDB-M, and RDT5K), using varying values for the regularization term $\\alpha = \\{0.1,0.2, . . ., 0.9\\}$. For this experimental setup, we use the same hyperparameter config-uration for each dataset as described in Section 6.1. Figure 5 represents the experimental results. In our experiments, we observed that setting \u03b1 either too low or too high leads to suboptimal perfor-mance. To achieve better results, it is essential to select an intermediate value for \u03b1, as this provides a balance that optimizes the StructPosGSSL's performance."}, {"title": "A.5 Ablation Analysis of GenHopNet with traditional SSL", "content": "To demonstrate the efficacy of GenHopNet alongside traditional SSL, we conduct an ablation study on the following variations:\n\u2022 GenHopNet+NT-Xent: Combines our GenHopNet with NT-Xent loss.\n\u2022 GCN+NT-Xent: Replaces GenHopNet with GCN while retaining NT-Xent loss.\nThe results in Table 7 demonstrate that GenHopNet+NT-Xent consistently outperforms GCN+NT-Xent across various datasets. For instance, on ogbg-molhiv, GenHopNet+NT-Xent achieves a classi-fication accuracy of 74.0%, compared to 69.5% with GCN+NT-Xent. Similarly, on ogbg-moltox21 and ogbg-moltoxcast, the improvements are evident with accuracies of 73.5% vs 69.1% and 62.3% vs 58.3%, respectively. These results highlight the superior expressiveness of GenHopNet, which"}, {"title": "A.6 Comparison of NT-Xent and VICReg Loss Terms", "content": "To showcase the complementary nature of NT-Xent and VICReg loss terms, we perform an ablation study on the following variants:\n\u2022 No VICReg: excludes VICReg term from total loss.\n\u2022 No NT-Xent: excludes NT-Xent term from total loss."}, {"title": "A.7 Ablation Analysis of $m^{(t)}_{local\\_pat}$", "content": "To evaluate the impact of the 1-hop edge-level aggregated message $m^{(t)}_{local\\_pat} (v)$ on the performance of our StructPosGSSL framework, we conduct experiments by evaluating StructPosGSSL-SA across six datasets (MUTAG, PTC-MR, NCI1, PROTEINS, IMDB-B, and IMDB-M), using the following variations:\n\u2022 GenHopNet+Org: Original GenHopNet without any modifications to message passing.\n\u2022 GenHopNet+mlocal-pat(v): GenHopNet with only 1-hop edge-level aggregated message $m^{(t)}_{local\\_pat} (v)$ during message passing."}, {"title": "A.8 Comparison of Structural and Positional Encoders", "content": "The apparent effectiveness of the Structural Encoder compared to the Positional Encoder depends on the characteristics of the datasets. Positional encodings excel in dense graphs (e.g., IMDB-B) by providing global positional context and breaking symmetry, thus distinguishing graphs when local structural cues alone are insufficient. In contrast, structural encodings prove more impactful in sparse graphs or domains such as molecular datasets (e.g., OGB, ZINC), where local patterns, motifs, or functional groups are key. Taken together, these two modules target complementary aspects of graph structure; Positional Encoder focuses on global relationships and symmetry-breaking, while SE emphasizes local substructures-enabling a more comprehensive approach to graph representation learning.\nIn order to highlight how SE and PE encoders complement one another, we conduct an ablation study using the following variants:\n\u2022 POS: This variant keeps only positional (POS) encoding.\n\u2022 STRUCT: This variant keeps only structural (STRUCT) encoding."}, {"title": "A.9 Experiments on ZINC Dataset", "content": "The ZINC dataset [74] is composed of 250K molecules, from which 12K are chosen for the solubility regression task under certain constraints. This dataset allows for a comprehensive assessment of the model's ability to capture both local and global graph properties. Our experimental design adheres to the methodology outlined in [20].\nWe compare our method against eight baseline approaches: PathNN [50], G3N [74], CIN [8], I2-GNN [30], Moment-GNN [34], K-Subgraph SAT [13], GRIT [48], GPS [59], and Specformer [7].\nFor the ZINC12K dataset in Table 12, our method achieves strong performance compared to other baselines. This success can be attributed to the ability of our structural encoding to effectively capture rich local topological features, such as cycles and motifs, which are highly correlated with molecular classes and attributes. Additionally, the integration of global positional relationships fur-ther enhances the model's expressiveness, enabling it to better distinguish subtle molecular graph patterns and achieve better results."}]}]}