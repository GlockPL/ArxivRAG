{"title": "GUIDED AND VARIANCE-CORRECTED FUSION WITH ONE-SHOT STYLE ALIGNMENT FOR LARGE-CONTENT IMAGE GENERATION", "authors": ["Shoukun Sun", "Min Xian", "Tiankai Yao", "Fei Xu", "Luca Capriotti"], "abstract": "Producing large images using small diffusion models is gaining increasing popularity, as the cost of training large models could be prohibitive. A common approach involves jointly generating a series of overlapped image patches and obtaining large images by merging adjacent patches. However, results from existing methods often exhibit obvious artifacts, e.g., seams and inconsistent objects and styles. To address the issues, we proposed Guided Fusion (GF), which mitigates the negative impact from distant image regions by applying a weighted average to the overlapping regions. Moreover, we proposed Variance-Corrected Fusion (VCF), which corrects data variance at post-averaging, generating more accurate fusion for the Denoising Diffusion Probabilistic Model. Furthermore, we proposed a one-shot Style Alignment (SA), which generates a coherent style for large images by adjusting the initial input noise without adding extra computational burden. Extensive experiments demonstrated that the proposed fusion methods improved the quality of the generated image significantly. As a plug-and-play module, the proposed method can be widely applied to enhance other fusion-based methods for large image generation. Code: https://github.com/TitorX/GVCFDiffusion", "sections": [{"title": "INTRODUCTION", "content": "Recent years have witnessed remarkable advancements in text-to-image generation models, which can produce realistic and diverse images based on textual prompts. Among them, the Diffusion models, specifically the Stable Diffusion (SD) [3], have emerged as one of the mainstream methods for image generation.\nThere is a significant demand for producing large images. The pursuit of generating larger images involves two aspects: 1) producing images with higher resolution that exhibit ultra-fine details, and 2) creating images that encompass more content, such as panorama images. To differentiate between these aspects, we refer to them as High-Resolution image generation and Large-Content image generation, respectively. However, training models capable of generating large images requires a substantial investment in hardware and data. For instance, training the SD v2 model to generate 5122 images took over a month on 256 A100 GPUs. The core U-Net model of it comprises 865 million parameters. The larger SDXL [4] model, which can generate 10242 images and contains 2.6 billion parameters, demands an even longer training period.\nRecent progress has been made by using pre-trained smaller models to jointly generate a series of overlapped small patches, which are then combined to form images of arbitrary sizes. A notable work is MultiDiffusion [1], which generates large images by averaging overlapped areas of patches at each denoising step. SyncDiffusion [2] achieves more coherent large-content images by ensuring consistent styles across each small patch during the joint denoising process. However, existing methods exhibit three major drawbacks: 1) noticeable seams at overlapped areas, 2) generation of discontinuous objects, and 3) low-quality content.\nIn the overlapped regions, each patch derives different values at each denoising step. Resolving discrepancies by averaging to achieve uniformity values can interfere with the denoising of individual patches. This interference occurs because diffusion models, during training, assume that the whole denoising process is completed with all intermediate results undisturbed. Persistent changes to the values in certain regions can have unknown impacts on the denoising process, typically resulting in negative effects.\nWe propose a method termed Guided Fusion (GF), which assigns a guidance map to each small patch to perform weighted averaging in the overlapped regions, allowing the denoising process to be dominated by the patch with higher weight. Additionally, we discovered that averaging the overlapped regions while using Stochastic Differential Equation (SDE) samplers, such as Denoising Diffusion Probabilistic Model (DDPM) [5], produces highly blurred results. This occurs because the SDE samplers usually introduce a Gaussian-distributed random term during the denoising process, and averaging multiple variables sampled from Gaussian distributions results in a variance lower than expected, leading to blurred images that lack details. To address this, we introduce Variance-Corrected Fusion (VCF) to adjust the variance and thereby generate higher-quality images. Furthermore, we observed that significant differences in the initial noise used by each patch make it more challenging to produce coherent images. Therefore, we propose a one-shot Style Alignment (SA), which aligns the initial noise with semantic interpolation to produce more style-consistent results.\nThe main contributions of this paper are as follows:\n\u2022 Guided Fusion was proposed to utilize a guidance map for weighted averaging on overlapped areas, leading to better quality and seamless image generation.\n\u2022 We proposed the Variance-Corrected Fusion to fix the small variance issue that happened while averaging overlapped regions with SDE samplers. The proposed method prevents generating blurred results with SDE samplers, leading to higher-quality image generation."}, {"title": "PRELIMINARIES", "content": "The core of diffusion models (DMs) lies in the concept of a Markov process, specifically, a type of Markov chain where each step adds a controlled amount of Gaussian noise to the data. The forward diffusion process is defined as a sequence of latent variables {x} indexed by discrete time steps t = 0,1,...,T, where xo represents the original data and xT approximates a standard Gaussian distribution N(0, I). The transition from X1\u20131 to x, is modeled by a Gaussian distribution, typically formulated as:\nq(x_{t}|x_{t-1}) := N(x_{t}; \\sqrt{1 - \\beta_{t}}x_{t-1},\\beta_{t}I).\nHere, the schedule of variances \u03b2\u2081 is designed to gradually add noise to xt, which can be learned by reparameterization [6] or held a sequence of constants as hyperparameters [3, 7]. The choice of the {\u03b2\u2081} is critical as it controls the rate at which the data is diffused into noise over time.\nThe reverse diffusion process, or called denoising process, involves learning a model po(xt-1|x\u2081) that approximates the reverse of the forward process. This is done by parameterizing the Gaussian distribution with learnable parameters 0, usually expressed as\n$p_{\\theta}(x_{t-1}|x_t) := N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_{\\theta}(x_t, t))$,\nwhere \u00b5e(x, t) and \u2211o(x, t) are learned through optimization. The objective is to minimize the difference between the true reverse distribution q(x1-1/X1, X0) and the modeled distribution Po(X1-1X1).\nA common practice sets the schedule of \u03b2\u2081 as an increasing sequence of constants at forward process. The reverse process sets \u03a3\u0189(x, t) = \u03c3\u00b2I and let \u03c3\u00b2 = \u03b2\u2081 or \u03c3\u00b2 = \u03b2 [5], where \u0101\u2081 = \u220f=1 \u03b1s and a\u2081 := 1 \u2212 \u03b2\u2081. Hence we can formulate:\n$p_{\\theta}(x_{t-1}|x_t) := N(x_{t-1}; \\mu_{\\theta}(x_t, t), \\sigma I)$.\nLatent Diffusion Model (LDM) [3] extends diffusion models by operating in a low-dimensional latent space instead of the high-dimensional pixel space. This is achieved by first encoding the data into a latent representation using a suitable encoder, and then applying the diffusion process within this more compact"}, {"title": "METHOD", "content": "The nature of the joint denoising process. We denote a small pretrained diffusion model as a parametric model that has been optimized for a series of Markov chained Gaussian transitions \u0440\u04e9(x0) := \u0440\u04e9(X0:T)dx1:r at a low-dimensional space xo \u2208 R\". As the small diffusion model has never been optimized with the high-dimensional dataset, it cannot be directly used to sample larger images. The joint denoising process uses the small model to obtain large images Xo \u2208 Rm, where m > n, by fusing a series of overlapped patches after each denoising step. Since the distribution in high-dimensional space is unknown, we can only aim to sample a Xo for which each subview: 1) conforms to a learned distribution in the low-dimensional space so that each generated patch is realistic; 2) shares identical values in the overlapping dimensions so that can be merged to form a large sample.\nThe drawbacks of averaging latent variables. Use a simple case as illustration, we denote a large sample with three dimensions as X = [x(1), x(2), x(3)] and use a two dimensional model to jointly produce overlapped patches x(1) = [x(1), x(2)] and x(2) = [x(2), x(3)]. The MultiDiffusion [1] introduced a joint denoising process that average values on overlapped dimensions after each denoising step, which can be described as:\n$\\[x_{t-1}^{(21)}, x_{t-1}^{(22)}] \\sim p_{\\theta}(x_{t-1}^{(1)}|x_t^{(1)})$\n$\\[x_{t-1}^{(21)}, x_{t-1}^{(22)}] \\sim p_{\\theta}(x_{t-1}^{(2)}|x_t^{(2)})$\n$x_{t-1}^{(2)} = \\frac{x_{t-1}^{(21)}+x_{t-1}^{(22)}}{2}$\n$x_{t-1}^{(1)}:=x_{t-1}^{(1)} = [x_{t-1}^{(1)}, x_{t-1}^{(2)}]$\n$x_{t-1}^{(2)}:=x_{t-1}^{(2)} = [x_{t-1}^{(2)}, x_{t-1}^{(3)}].$\nAs shown in Eq. 4, the denoising steps for x(1) (1) and x(2) produce diverged values x(21) and x22) over the same dimension x(2). Averaging by Eq. 5 solves the divergence so that ensures the overlapped dimension share same value after each step.\nAs described in Eq. 3, throughout the denoising process, for 1 < t<T, X1-1 should be estimated by the conditional probability Po(xt-1|x1). However, during the patch averaging, the values of overlapped dimensions have been constantly modified, leading to the next x1-1 being estimated conditioned at an altered x\u2081. Such value altering constantly perturbs the denoising transitions leading obvious seams and reduced quality.\n3.1 Mitigate Divergence among Patches with Guided Fusion\nDisrupting the denoising process of a patch in different regions may lead to varying degrees of model performance degradation. Intuitively, we consider that the closer the disturbed region is to the center, the greater the impact on the quality of the generated image. Therefore, we propose a guidance map as shown in Fig. 2, which linearly decreases its weight from 1 at the center to 0 at the corners, to guide the weighted averaging of the overlapping regions. Follow the example described by Eq. 5, the weighted average at overlapped dimension can be formulated as:\n$x_{t-1}^{(2)} = \\frac{w_1x_{t-1}^{(21)} + w_2x_{t-1}^{(22)}}{w_1 + w_2}$\nwhere the weights w\u2081 and w2 are determined by the corresponding locations on guidance map. To generalize the simple case to N overlapped patches, we formulate the weighted average for each dimension from overlapped areas as:\n$x_{t-1} = \\frac{\\sum_{i=1}^N w_ix_{t-1}^{(i)}}{\\sum_i w_i}$\nThis method is named Guided Fusion (GF). During the joint denoising process, the value of each dimension in overlapped area is predominantly determined by the geometrically closer patch, thereby reducing the perturbation in the denoising process for that dimension.\""}, {"title": "Correcting Variance of Fused Patches with SDE Samplers", "content": "For Ordinary Differential Equation (ODE) samplers, such as Denoising Diffusion Implicit Model (DDIM) [7], the experimental results demonstrate that although fusion with averaging interferes with denoising process, it can still produce effective images as shown in the first row of Fig. 3. However, for scenarios requiring the use of Stochastic Differential Equation (SDE) samplers, such as DDPM [5], averaging can lead to faulty blurred results, as displayed in the second row of Fig. 3. We use DDPM as example to illustrate the reason.\nFor a single image patch generation using DDPM, the t \u2212 1 denoised image is computed by:\n$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta} (x_t, t) + \\sigma_t z)$\nwhere z ~ N(0, I). We can consider x, as a known variable because it has been determined by the previous step, hence the:\n$x_{t-1} \\sim N(\\mu_{\\tau}, \\sigma_t^2I)$\nwhere $\\mu_{\\tau} = \\frac{1}{\\sqrt{\\alpha_t}} (x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}} \\epsilon_{\\theta} (x_t, t))$.\nContinuing the example from the Eq. 5 using DDPM sampler, the fused denoised dimension $x_{t-1}^{(2)} = \\frac{x_{t-1}^{(21)}+x_{t-1}^{(22)}}{2}$ has:\n$x_{t-1}^{(2)} \\sim N(\\frac{\\mu_{t-1}^{(21)} + \\mu_{t-1}^{(22)}}{2}, \\frac{\\sigma_t}{2})$.\nWe notice that the variance becomes \u03c3\u00b2/2 which is smaller than the expected of as in Eq. 10. This causes blurred results while applying averaging with DDPM, e.g., the second row of Figure 3. The reduced variance leads to over-homogeneous image content.\nWe propose the Variance-Corrected Fusion (VCF) by redefining x1 to correct the variance:\n$x_{t-1}^{(2)} = \\frac{x_{t-1}^{(21)}+x_{t-1}^{(22)}}{\\sqrt{2}}+ (1 - \\frac{1}{\\sqrt{2}}) \\frac{\\mu_{t-1}^{(21)} + \\mu_{t-1}^{(22)}}{2}$\nso that have $x_{t-1}^{(2)} \\sim N((\\mu_{t-1}^{(21)} + \\mu_{t-1}^{(22)})/2, \\sigma_t I)$.\nWe generalize the Eq. (12) to averaging N overlaps:\n$x_{t-1} = \\frac{\\sum_{i=1}^N x_{t-1}^{(i)}}{\\sqrt{N}}+ (1 - \\frac{1}{\\sqrt{N}}) \\frac{\\sum_i \\mu_{t-1}^{(i)}}{N}$\nand generalize to Guided Fusion weighted average:\n$x_{t-1} = \\frac{\\sum_i w_ix_{t-1}^{(i)}}{\\sqrt{\\sum_i W \\sum_i w_i^2}}+ (1 - \\frac{1}{\\sqrt{\\sum_i W \\sum_i w_i^2}}) \\frac{\\sum_i \\mu_{t-1}^{(i)}}{\\sum_i w_i}$"}, {"title": "One-shot Style Alignment (SA) for Coherent Montages", "content": "SyncDiffusion [2] inspires us that aligning the style of each small patch reduces the difficulty of generating more coherent content. However, SyncDiffusion requires constantly modifying the intermediate denoised patches to align their style, which further disrupts the denoising process.\nWe noticed that the diffusion model exhibits the semantic interpolation effect [7], in which the interpolations between two initial noises can lead to semantically meaningful results.\nWe propose a one-shot style-control method, Style Alignment (SA), performing interpolation on each non-overlapped patch cropped from the whole initial noise to a reference noise. The SA can be formulated as:\n$x_t^{(i)} := slerp(x_t^{(i)}, z_{ref}, \\alpha)$\nwhere the slerp(\u00b7) is the spherical linear interpolation [9] function; x x is the ith non-overlapped crop from the initial noise X7; zref is a reference noise to be aligned with; a \u2208 [0, 1] is the interpolation ratio where 0 returns the original x and 1 returns zref. The reference noise zref can be any standard Gaussian noise. It may originate from a patch of the initial noise Xr or be obtained through diffusing a specific image.\nAfter SA alignment, all non-overlapped patches rotate towards the reference noise, resulting in them becoming more clustered. Consequently, the distances between them are reduced, and their similarity increases."}, {"title": "RESULTS", "content": "Generated Datasets. The text-to-panorama generation task was chosen to assess each method's performance on large-content image generation. For each approach, we sampled a set of 512 \u00d7 3584 sized images, \u00d77 wider than the original model resolution, with five prompts and 500 panorama images for each prompt. In total, 2,500 panorama images were generated for each approach. The panorama images were further divided into 7 patches matching the original model size, ultimately producing 17,500 images. The five used prompts are:\n\u2022 A photo of a city skyline at night\n\u2022 A photo of a mountain range at twilight\n\u2022 A photo of a snowy mountain peak with skiers\n\u2022 Cartoon panorama of spring summer beautiful nature\n\u2022 Natural landscape in anime style illustration\nWe conducted both qualitative and quantitative comparative experiments with the results obtained from MultiDiffusion and SyncDiffusion.\nReference Dataset. Based on the prior works, the ODE samplers, such as DDIM, tend to lead to worse output quality [7, 8, 10]. We chose the SDE sampler DDPM to generate the reference dataset as it stands for higher quality. We used Stable Diffusion [3] v2.0 to generate reference images for evaluation. A reference dataset that contains 17,500 of 512 \u00d7 512 images was generated with 3500 images per prompt.\nEvaluation Metrics. To assess the image quality, we employed FID [11], KID [12] (we use the anti-aliasing implementation [13]) and GIQA-QS/GIQA-DS [14] to evaluate the fidelity and diversity; CLIP score [15] to evaluate the compatibility with the prompt.\n4.1 The Effectiveness of Guided Fusion\nThe overlap ratio between patches is controlled by the stride; a smaller stride indicates a greater ratio of overlapping. Additionally, a smaller stride indicates that more patches are needed in joint denoising to form a large image. Figure 4 shows qualitative results from MultiDiffusion (MD) and Guided Fusion (GF) over 64, 128, 256, and 384 strides with a DDIM sampler. It can be observed that noticeable seams are present in the results of MD with four different strides. Among these, the seams are least apparent with the 64 stride, while they are most pronounced with 256 stride. After applying GF, the seams are significantly reduced at all strides, resulting in more continuous images.\nTo thoroughly evaluate the effectiveness of the proposed GF, we compared our method with MD in three stride settings: 128, 256 and 384 with quantitative metrics.\nAs shown in Table 1, the experimental results indicate that GF consistently outperforms MD across different strides. Specif-"}, {"title": "High Image Quality Generation using DDPM Sampler with Variance-Corrected Fusion", "content": "By examining Table 2, it can be observed that applying DDPM with VCF is able to produce high-quality and diverse outcomes. The \"VCF\" row presents substantial improvements to DDIM-based methods. We did not report the result from DDPM applied with MD because it produces blurred images as shown in the second row of Fig. 3. The third row of Fig. 3 shows the result generated by DDPM with corrected variance. The \"VCF+GF\" showing better scores than solely applying VCF indicates that the VCF and GF do not interfere with each other's effective-"}, {"title": "The Effectiveness of Style Alignment", "content": "For Style Alignment (SA), we use FID and GIQA-DS as the primary metrics to evaluate the quality and diversity of the generated panorama images. We evaluated the generated images with a set to 0.0, 0.1, 0.2, ..., and 1.0 for both MD and GF with the DDIM sampler. It is important to note that when a = 0.0, it implies that the SA is not applied. Conversely, when a = 1.0, it indicates that the entire large image is initialized using repeated reference noise patch. We used a randomly generated standard Gaussian noise as the reference noise to conduct our experiments.\nAs shown in Fig. 5, with the increase in a, the overall image quality exhibits an upward trend, while diversity shows a downward trend. Figure 6 shows progressive visual results from discontinuous content to the highly repeated pattern generated with increasing values of a. This evidences our assumption: initializing patches with similarity helps to generate more coherent content. The trade-off is that as a increases, diversity decreases. We identified the a = 0.4 as the optimal value because it balances the quality and diversity. With a larger than 0.4, the diversity drops quickly. The different choices of a provide a control of style consistency that can fit different aesthetic requirements.\nIt can also be observed from Fig. 5 that regardless of the choice of a, applying SA with GF consistently achieves better quality and diversity compared to MD."}, {"title": "CONCLUSIONS", "content": "We have revisited the joint denoising, which generates a large image by creating a series of overlapped patches through small diffusion models, addressing the issues presented in the fusion of overlapped regions. The conventional averaging in overlapped regions undermines the expected denoised image, introducing cumulative perturbations.\nWe proposed a novel technique called Guided Fusion (GF), which reduces the disruption to the denoised image by assigning higher weights to the central region of each image patch, allowing the fused values in overlapped regions to be predominantly determined by the geometrically closer patch. Additionally, we presented Variance-Corrected Fusion (VCF), which adjusts the variance of the averaged values to enable its application with SDE samplers, such as DDPM. Furthermore, we introduced the Style Alignment (SA), a method that eases the fusion process by controlling the similarity of the initial noise, resulting in more coherent images.\nQualitative and quantitative experimental results demonstrate that all three methods effectively enhance the quality of the generated images. Our proposed approaches can be widely applied to other joint denoising-based methods to achieve better fusion outcomes. For example, the high-resolution image generation approaches, ScaleCrafter [17] and DemoFusion [18], both use MD to fuse the overlaps. Our approaches provide a potential enhancement for these approaches."}]}