{"title": "Image2CADSeq: Computer-Aided Design Sequence and Knowledge Inference from Product Images", "authors": ["Xingang Li", "Zhenghui Sha"], "abstract": "Computer-aided design (CAD) tools empower designers to design and modify 3D models through a series of CAD operations, commonly referred to as a CAD sequence. In scenarios where digital CAD files are not accessible, reverse engineering (RE) has been used to reconstruct 3D CAD models. Recent advances have seen the rise of data-driven approaches for RE, with a primary focus on converting 3D data, such as point clouds, into 3D models in boundary representation (B-rep) format. However, obtaining 3D data poses significant challenges, and B-rep models do not reveal knowledge about the 3D modeling process of designs. To this end, our research introduces a novel data-driven approach with an Image2CADSeq neural network model. This model aims to reverse engineer CAD models by processing images as input and generating CAD sequences. These sequences can then be translated into B-rep models using a solid modeling kernel. Unlike B-rep models, CAD sequences offer enhanced flexibility to modify individual steps of model creation, providing a deeper understanding of the construction process of CAD models. To quantitatively and rigorously evaluate the predictive performance of the Image2CADSeq model, we have developed a multi-level evaluation framework for model assessment. The model was trained on a specially synthesized dataset, and various network architectures were explored to optimize the performance. The experimental and validation results show great potential for the model in generating CAD sequences from 2D image data.", "sections": [{"title": "INTRODUCTION", "content": "Computer-aided design (CAD) systems can significantly reduce design time by avoiding the need for traditionally required labor-intensive manual drawings [1]. Contemporary CAD systems such as Fusion 360, SOLIDWORKS, and On-Shape enable designers to create and modify CAD models through a sequence of CAD operations. However, in certain scenarios, the CAD model of a product may not be readily available due to various factors, including outdated documentation, lack of digital records, and commercial reasons. Reverse engineering (RE) is employed to overcome these obstacles, utilizing measurement and analysis tools to reconstruct CAD models [4, 5].\nIntegrating RE with CAD systems can not only allow designers to leverage the advantages of existing products while incorporating their own innovative ideas and improvements but can also be used for design knowledge restoration and management. However, the traditional RE process faces two major limitations. First, it focuses on reconstructing 3D models rather than CAD sequences. Compared to 3D models, a CAD sequence provides access to the historical construction process and associated design knowledge and it facilitates geometry modification using parametric modeling. Second, the process has been performed primarily manually, making it labor-intensive and time-consuming. Recently, researchers have explored data-driven methods, such as converting 3D point clouds [6, 7] or voxels [8, 9] into CAD models. Nevertheless, these 3D input data are often challenging to acquire due to inaccessibility and unavailability. 3D scanning could be a solution, yet quality is often unsatisfactory and cost is an unavoidable factor to consider when acquiring specialized equipment and expertise.\nCompared to point clouds or voxels, images are easier to acquire given the popularity of mobile devices. Thus, our question arises: How can we reverse engineer CAD sequences directly from 2D images in supporting designers to interpret and edit CAD models during the design and modeling process? After a thorough literature review, we realize that there is a scarcity of research exploring answers to this question. Therefore, our objective is to develop a data-driven approach that can generate a sequence of CAD operations based on a single image (referred to as \u201cImage2CADSeq\u201d hereafter for brevity).\nThe contributions of the proposed approach are summarized as follows.\na). To the best of our knowledge, this study is the first attempt to predict a sequence of CAD operations given a single image input (i.e., single-view image to CAD sequence prediction). We developed a target-embedding variational autoencoder (TEVAE) architecture [10] to solve this problem. The proposed approach has the potential to streamline the CAD design process, reducing the time and effort required to create 3D models from 2D sketches or photographs.\nb). We created a novel data synthesis pipeline based on the design grammars defined in the domain-specific language (DSL), Fusion 360 Gallery. The pipeline can generate synthetic data that resemble real-world images and CAD models. It can also be used as a data augmentation method to improve the quality and quantity of existing training data sets, making the data more diverse and robust for training image2CADSeq models.\nc). We developed a multi-level evaluation framework to assess the Image2CADSeq performance, encompassing three components: CAD sequences, 3D models, and the corresponding images. Specifically, the evaluation of the CAD sequence is performed at multiple levels and hierarchies (see Section 4.5 for details) to quantitatively assess the predictive performance of the proposed model architectures.\nWe anticipate that the proposed approach has the potential to revolutionize existing CAD systems by making the CAD model reconstruction process more accessible. This would enable both experienced and novice designers to actively contribute to the design, promoting design collaboration and design education. Moreover, it has the potential to provide a unique pathway to involve end users in the design process, promoting design democratization.\nThe remainder of this paper is organized as follows. In Section 2 and 3, we provide an overview of the background related to data-driven 2D-to-3D generation and technical background about the applied techniques. Section 4 outlines the methodology in the development of our Image2CADSeq model. Subsequently, Sections 5 and 6 present and analyze the experimental results, summarizing the primary findings and acknowledging limitations. Conclusions and closing remarks are presented in Section 7, where we present key insights and suggest potential directions for future research."}, {"title": "LITERATURE REVIEW", "content": "In this section, we first provide a background of deep learning techniques applied to 2D-to-3D generation using discrete 3D representations, such as voxels and meshes, as well as the generation of CAD models using constructive solid geometry (CSG) and 2D sketches. Following this introduction, we then present a review of deep learning methods specifically tailored for 3D parametric CAD models, which are most relevant to our work."}, {"title": "RESEARCH ON 2D-TO-3D GENERATION", "content": "Our research is related to the domain of 2D-to-3D generation, led by advances in computer graphics and computer vision. For an in-depth understanding of the current developments and challenges in this area, we direct readers to a comprehensive review [11]. The use of discrete 3D representations, such as voxels, point clouds, and meshes, has been prevalent. Despite their widespread use, these representations often focused on generating visually appealing objects without necessarily considering their engineering aspects, such as dimensions, engineering performance, and compatibility with engineering software [12]. This mismatch hinders the seamless integration with downstream applications, such as editing and engineering analysis of synthesized 3D shapes, underlining the necessity for adopting CAD-specific data formats in our research.\nConstructive solid geometry (CSG) is one of the fundamental methods for creating CAD models. It applies Boolean operations (e.g., union, intersection, and subtraction) to basic geometric shapes (primitives), such as cuboids, spheres, and cylinders. CSG is known for its lightweight structure, which allows for easy modifications by altering the parameters of these primitives and their spatial transformations. There have been various studies on the deep learning methods of CAD model generation using CSG [13, 14, 15, 16]. Despite its merits, CSG lacks the versatility used in contemporary CAD tools that utilize parametric modeling.\nParametric CAD models start as 2D sketches comprising geometric primitives (e.g., line segments and arcs) with explicit constraints, such as coincidence and perpendicularity, establishing the foundation for 3D construction operations (e.g., extrusion and revolution). The relevant deep learning methods of parametric CAD models include 2D engineering sketch and 3D model generation and reconstruction. There has been a series of studies recently [17, 18, 19, 3, 20] dedicated to the generation of CAD sketches through the application of deep learning approaches. The emphasis in these works is on generating 2D layouts rather than dealing with the generation of 3D components. We will be focused on introducing deep learning methods for 3D CAD model generation and reconstruction since they are more relevant to our work."}, {"title": "DEEP LEARNING OF PARAMETRIC 3D CAD MODELS", "content": "Boundary representation (B-rep) format is the standard format for representing 3D shapes in CAD, which defines objects based on their boundary surfaces, edges, and vertices connected through specific topology. Numerous learning-based approaches have emerged for the generation of parametric curves [21] and surfaces [22]. In addition to curve or surface generation, Smirnov et al. [23] introduced a generative model for creating topology that combines parametric curves and surfaces to create solid models, depending on predefined topological templates. In addition, various methods have been proposed to enable the direct generation of B-rep models with arbitrary topology [24, 25, 26]. Different from these works, our focus lies in generating CAD sequences that can be translated into B-rep models using a solid modeling kernel, such as Fusion 360 CAD software.\nSignificant progress has been made in the generation of CAD sequences for the reconstruction of 3D models particularly through Sketch-and-Extrude modeling operations. Recently, there have been methods [2, 27] for generative models specifically designed for the unconditional generation of CAD sequences. These models aim to autonomously create CAD sequences without relying on specific conditions or inputs. Specifically, Wu et al. [2] presented the first generative model, DeepCAD, that learns from sequences of CAD modeling operations to produce editable CAD designs. By drawing an analogy between CAD operations and natural language, the authors propose to utilize a transformer [28] architecture aiming to leverage the capabilities of transformer models in understanding and generating sequences, adapting them to the context of CAD design operations.\nGenerative models indeed serve as valuable tools for randomly generating a multitude of designs, offering inspiration and exploration of diverse possibilities. However, these models lack the capability to directly incorporate designers' intent into the generation process. Consequently, the designs generated can deviate from the expectations or specific requirements of the designers. This discrepancy highlights the need for mechanisms that allow designers to guide or influence the output, ensuring that the generated designs align more closely with their intent and preferences. To that end, several methods have been introduced to allow the CAD sequence generation given the target of B-rep models [29, 30], voxels [8, 9], point clouds [6, 7], and sketches [31, 32]. Particularly, Fusion 360 Gym [29] was developed to reconstruct a CAD model given a B-Rep model, utilizing a face-extrusion technique that relies on existing planar faces within the B-Rep model. However, despite the potential for CAD sequence generation, the face-extrusion method differs significantly from the more natural sketch-extrusion method commonly used by human designers. Moreover, this technique is ineffective when confronted with a lack of available planar or profile data in the input data, such as images.\nOur work aims to fill a research gap in the existing literature by focusing on the task of generating CAD sequences from images. In particular, we expand upon the transformer-based autoencoder initially introduced in DeepCAD [2] and convert it into a TEVAE architecture developed in our previous work [10]. In the case study, we apply the domain-specific language of Fusion 360 Gym [29] that demonstrates our approach to predicting CAD sequences that involve Sketch-and-Extrude operations."}, {"title": "TECHNICAL BACKGROUND", "content": "An autoencoder (AE) is a type of neural network that aims to learn a compressed representation of input data [33]. It consists of two main parts: an encoder and a decoder. The encoder compresses the input into a lower-dimensional latent space, while the decoder reconstructs the input data from this compressed representation. The goal is to minimize the difference between the original input and its reconstruction, leading to efficient data encoding. Variational Autoencoders (VAEs) [34] extend traditional AEs by introducing a probabilistic way to the encoding process. This probabilistic approach allows VAEs not only to reconstruct input data but also to generate new data that is similar to the input. Autoencoders have been applied to uncover the useful underlying structures of data which is typically known as representation learning [35].\nRepresentation learning is a set of techniques in machine learning that automatically discovers the representations with reduced dimensionality from raw data for downstream tasks, such as regression or classification [36]. While VAEs are particularly known as generative models for their effectiveness in generating complex data, they offer a more robust, regularized, and probabilistic approach to representation learning compared to traditional AEs [10, 37].\nAlthough most research has focused on employing AEs and VAEs in unsupervised or semi-supervised scenarios, it is worth noting that autoencoders also demonstrate utility in supervised contexts [38]. Specifically, incorporating an auxiliary feature-reconstruction task proves beneficial in enhancing supervised classification problems [39], which are known as feature-embedding autoencoders (FEAs). Furthermore, Jarrett and Schaar [38] propose target-embedding autoencoders (TEAs) and demonstrate their effectiveness theoretically and empirically. As implied by their names, TEAs focus on encoding the target's information into a latent space, while FEAs encode the feature's information.\nTEAs have been applied to various problems. Girdhar et al. [40] introduce a TL-embedding network, comprising a T-network (an autoencoder network for the horizontal bar and an encoder for the vertical bar) during training. Upon training completion of the T-network, it facilitates the derivation of the L-network, enabling the prediction of 3D voxel shapes from input images. A similar network architecture has also been applied for semantic image segmentation tasks [41, 42]. Drawing inspiration from these preceding studies, Li et al. [10] propose to use a VAE to replace the AE and form a target-embedding variational autoencoder (TEVAE) architecture, demonstrating its effectiveness in predictive and generative tasks for car and mug design examples.\nIn this study, we constructed the Image2CADSeq neural network model by comparing both TEA and TEVAE architectures. Our objective is to assess their effectiveness in the prediction task of images to CAD sequences. The results revealed a significant superiority of the TEVAE architecture over the TEA architecture in terms of prediction performance as detailed in Section 5.3."}, {"title": "METHODOLOGY", "content": "The flowchart depicted in Figure 1 illustrates the proposed systematic approach to predicting CAD sequences given images, a process we refer to as Image2CADSeq. To effectively tackle this challenge, we initiate with a clear problem definition that divides the task into manageable components. Our objective is to harness the power of deep learning to predict a CAD sequence\u2014a series of CAD operations characterized by specific operation types and their corresponding parameters from an image. The image could be a rendering from a CAD model or a real-world photograph of a 3D object.\nDue to the intricate nature of CAD sequences, we employ a CAD program as a representational tool for CAD sequences. A CAD program enables designers to script their designs programmatically in a specialized scripting environment, such as the Fusion 360 API, FreeCAD API, or CAD-Query. CAD programs can convert CAD sequences described in text into script language that is interpretable and executable by computers. To overcome the inherent lack of structured format in the CAD sequence data, the CAD program is then streamlined into a vectorized representation conducive to neural network processing. This representation can facilitate not only the development of our neural network's architecture but also the creation of a data-synthesis pipeline tasked with generating the training data for the neural network. In addition, given the complexity of the Image2CADSeq task, we develop a comprehensive evaluation system that rigorously assesses our neural network models' performance, thereby ensuring the reliability and accuracy of our approach. This holistic evaluation is crucial in refining the Image2CADSeq model and guiding its evolution to meet the demanding standards of CAD sequence prediction.\nIn this study, we employ a particular domain-specific"}, {"title": "FUSION 360 GALLERY DOMAIN SPECIFIC LANGUAGE", "content": "language (DSL), namely Fusion 360 Gallery (abbreviated as Gallery for conciseness) [29] for the CAD program, as a solid case to demonstrate our approach. Therefore, before the presentation of how we devise the vectorized design representation for the CAD sequence data, we provide an introduction to the Gallery DSL below."}, {"title": "FUSION 360 GALLERY DOMAIN SPECIFIC LANGUAGE", "content": "Table 1 presents a summary of the core elements (i.e., CAD-related elements) in Gallery DSL, which enables the representation of a 2D/3D design as a CAD program, and Python is used to implement the CAD operations [29]. Gallery DSL now supports two major types of CAD operations: Sketch and Extrude. Each CAD operation is decomposed into two fundamental components: the operation type and its corresponding parameters. These elements are analogously mirrored in the Gallery DSL as function names and their associated parameters.\nA Sketch operation includes the definition of a Sketch Plane and Curves on it. A Sketch Plane can be created by the $add\\_sketch(I)$ function, where $I$ is a plane identifier that can be specified from the three canonical planes \"XY\", \"XZ\", or \"YZ\" or other planar faces (e.g., the side face of a cube) present in the current geometry. The Sketch Plane can then be used as the reference coordinate system in 2D for specifying the coordinates. A sequence of Curves, including Line, Arc, or Circle, can be drawn using $add\\_line(N,N,N,N)$, $add\\_arc(N,N,N,N,N)$, and $add\\_circle(N,N,N)$, respectively, Where $N$ is a real number representing the required parameters for a particular operation. As two numbers are needed to define one point, Line uses four numbers for start and endpoints; Arc needs five numbers: start point, center point, and sweep angle; and Circle is specified with three numbers: two for position and one for radius. Executing a Sketch can result in enclosed regions, termed profiles in CAD language. An Extrude operation can extrude a profile from 2D into 3D by using $add\\_extrude(I,N,O)$, where $I$ is an identifier for the profile, and $N$ is a signed number defining the depth of extruding"}, {"title": "DESIGN REPRESENTATION OF CAD PROGRAMS", "content": "along the normal direction of the profile. The Boolean operation ($O$) specifies the behavior of the extruded 3D volume, e.g., add to or subtract from other 3D bodies.\nWhile Gallery DSL currently does not support certain objects, such as spheres and springs, it still covers a vast range of them by using expressive $Sketch$ and $Extrude$ operations with Boolean capability [29]. Therefore, it is a good starting point for supporting learning-based methods [29, 2]. In the future, the other CAD operations, such as Revolve, Sweep, and Fillet, can be added to the Gallery DSL to expand its design grammar for a full-fledged CAD tool. In this study, we leverage the current status of the Gallery DSL by only considering the Sketch and Extrude operations.\nGallery DSL acts as a simplified interface to the more complex Fusion 360 Python API. In essence, it democratizes access to sophisticated CAD design through a more intuitive Python-based interface, effectively bridging the gap between complex CAD operations and the user's ability to execute them efficiently. For instance, as demonstrated in Table 2, when creating a cylinder with a base circle radius of 5 and a height of 10, the CAD program using Gallery DSL requires only about two-thirds of the efforts needed with the Fusion 360 Python API and does not require sophisticated definitions of various variables. This makes the Gallery DSL code easier to operate, and more user-friendly and accessible. However, Gallery DSL still requires a substantial amount of coding efforts in non-CAD-related elements, beyond the core functions as listed in Table 1.\nTo further improve its readability and accessibility, we simplify the Gallery DSL by isolating its key CAD-related functions referred to as Simplified-Gallery DSL or Sim-Gallery DSL for briefness, as shown in Table 2. We create the Sim-Gallery DSL following the concept of parametric modeling. In parametric modeling, a design is a sequence of operations that progressively modify the current geometry of an object. This process can be well represented in the Sim-Gallery DSL by a series of pure CAD operation functions. This simplification reduces the process to just three steps for creating a cylinder: $add\\_sketch(\u00b7)$, $add\\_circle(\u00b7)$, and $add\\_extrude(\u00b7)$. Additionally, we have developed a parsing method in Python to convert this simplified version back to the standard Gallery DSL. This ensures compatibility with Fusion 360 CAD software, facilitating seamless integration and execution of the CAD programs written in the Sim-Gallery DSL. It can also facilitate the design of the vectorized representation for the CAD sequence data as introduced in Section 4.2."}, {"title": "DESIGN REPRESENTATION OF CAD PROGRAMS", "content": "A standardized design representation is essential for neural networks to effectively interpret CAD programs."}, {"title": "NEURAL NETWORK MODEL ARCHITECTURE, TRAINING, AND APPLICATION", "content": "Thus, it becomes crucial to devise an efficient method to represent each CAD operation and the entire CAD program. There are three major challenges:\n1. Diversity in CAD operations: Different CAD programs comprise varying numbers of operations.\n2. Variability in parameters: Different CAD operations involve different numbers of parameters.\n3. Type of parameters: Parameters can be either continuous or discrete values.\nTo tackle these challenges, we propose to use a design representation with a unified data structure. We identified 10 variables (t, I, x, y, a, r, [I], d, O, s) from the Sim-Gallery DSL, detailed in Table 3. In what follows, we elaborate on our approach to handling these variables.\n(1) $t \\in \\{0,1,2,3,4,5,6\\}$ represents the operation types with 0-4 representing $add\\_sketch$, $add\\_line$, $add\\_arc$, $add\\_circle$, and $add\\_extrude$. The values 5 and 6 are used to represent the start (SOP) and the end (EOP) of a CAD program, which are not typical CAD operations but are included for the learning process to indicate a complete CAD program as required by a transformer model [28, 43, 2].\n(2) $I \\in \\{0,1,2\\}$ indicates the Sketch Plane using one of the canonical planes: \"XY\", \"XZ\", or \"YZ\".\n(3,4) x and y are the coordinates of the endpoint for Line and Arc, while they represent the center point when the operation type is Circle. We excluded the start point required by Line and Arc from the design representation by obtaining it from the precedent curve to make sure all curves are connected one after another, making the vectorized representation more compact. There are two extra considerations for this setting: (i) If one curve has no precedent, we default its start point to the origin (0,0) when parsing the design representation. (ii) For Arc that requires a center point instead of an endpoint, we calculate the coordinates for the center point based on its start point, endpoint, and sweep angle.\n(5) $\\alpha$ represents the sweep angle of an Arc.\n(6) $r$ is the radius of a Circle.\n(7) [1] represents the profile index in the Sketch.\n(8) $d$ represents the signed distance of the depth for Extrude.\n(9) $O \\in \\{0,1,2,3\\}$ is used to indicate the Boolean operations: join, cut, intersect, or add, respectively.\n(10) $s$ is an auxiliary factor that can be used to scale a CAD model.\nIn addition, to standardize the treatment of both continuous and discrete parameters, inspired by [43, 2], we discretize continuous parameters through quantization. This involves: (a) Confining continuous values to a subset of [-1,1] (e.g., (0,1] for radius and [-1,1] for endpoint x and y; (b) Dividing each range into 256 equal segments, enabling representation as 8-bit integers (i.e., 0 \u2013 255); (c) For the sweep angle ($\\alpha$), we multiply it by 180 during interpretation; (d) Handling scale factor (s): Although the scale factor can be a non-negative continuous value, we limit it to 256 levels"}, {"title": "RESULTS", "content": "for consistency with other continuous values' quantization. Consequently, the 10 variables can encode both the operation type and its associated parameters. From the 10 variables, a fixed-dimensional vector can be formalized as a unified design representation for each CAD operation, and the unused parameters will be filled with values of -1. See Figure 3 for an example.\nThe subsequent consideration involves standardizing CAD programs of varying sizes (the number of CAD operations involved). For example, besides the start and end marks, SOP and EOP, a cylinder can be created in three operations as shown in Table 2, while a triangular prism in five operations ($add\\_sketch(\u00b7)$, $add\\_line(\u00b7) \\times 3$, and $add\\_extrude()$). To achieve a consistent data structure across all CAD programs, we introduce a treatment, called maximum program length. Then, CAD programs shorter than this maximum length are extended by appending end marks (EOP) until they reach the predetermined length.\nIn this study, we use 7 variables to construct a 7-dimensional vector $[t,I,x,y,\\alpha,r,d]$ for each CAD operation (i.e., one step/line in a CAD sequence). Additionally, we assign default values to the other three variables [I], O, and s, setting them as 0, 3, and 10 correspondingly. Different variables can be selected which will influence the complexity of the data structure and thus the complexity of designs. In addition, the maximum length for CAD programs is set to 10. As a result, the design representation of a CAD program will be a matrix, namely, the feature matrix. Mathematically, the feature matrix denoted as P, is expressed as $P = [o_1, o_2, ..., o_c]^T \\in \\mathbb{R}^{10\\times 7}$, where $o_i \\in \\mathbb{R}^{7}$ is a CAD operation vector, and $N = 10$ is the sequence length of the CAD program. Refer to Figure 3 for an example of how a cylinder is converted to a feature matrix, including the quantization of its parameters as explained earlier."}, {"title": "NEURAL NETWORK MODEL ARCHITEC-TURE, TRAINING, AND APPLICATION", "content": "The application of target-embedding representation learning in deep learning, particularly for cross-modal tasks, has shown considerable efficacy [38, 12]. In alignment with this, we have developed the Image2CADSeq model, utilizing a target-embedding representation learning method, as illustrated in Figure 2. It features an encoder-decoder network for Stage 1 (S1), which is geared towards unsupervised learning and enables the efficient encoding of target objects (i.e., matrix feature of CAD programs) within a latent space. An additional encoder is integrated into Stage 2 (S2), focusing on supervised learning to regress the previously learned latent space using feature objects (i.e., images) as input.\nThe Image2CADSeq model employs a two-stage training strategy [41, 10]. In Stage 1, the focus is on independent training of the encoder-decoder network. The objective is to minimize the reconstruction loss between the actual matrix feature of CAD programs (y) and its reconstructed equivalent (y'). Completing this stage involves fixing the learnable parameters of the neural network model and saving the learned model, thereby capturing a latent space of y. Stage 2 shifts the focus to independent training of the S2 encoder by minimizing the discrepancy between the latent vector, derived from the learned latent space, and the embedding vector produced by the S2 encoder using an image as input. Importantly, each image used in this stage is directly associated with its feature matrix from Stage 1. This image and its corresponding feature matrix are associated with the same 3D object, and they form one data pair. The alignment of the latent vector with the embedding vector is performed specifically for these data pairs, ensuring that the S2 encoder training is precisely tuned to the corresponding images. This approach ensures a cohesive and targeted learning process. We"}, {"title": "DATA SYNTHESIS PIPELINE", "content": "present a novel data synthesis pipeline to generate training data pairs in Section 4.4.\nAfter training the Image2CADSeq model, the S2 encoder is integrated with the decoder from S1, creating the application module. This module is capable of predicting a feature matrix given an image input. Subsequently, this feature matrix can be translated into a CAD program using the Sim-Gallery DSL. Finally, the CAD program can be parsed into a 3D object by utilizing Fusion 360 software.\nWith the proposed design representation of the CAD programs and Fusion 360 software, we introduce an automatic data synthesis pipeline, as illustrated in Figure 3. This method is tailored to generate training data pairs comprising feature matrices of CAD programs and the corresponding images, essential for training the Image2CADSeq model. The process begins with preparing a list of basic shape templates, such as cylinders, employing the Sketch-and-Extrude paradigm of the Gallery DSL. For these basic shapes, we establish a series of template operations (e.g., $add\\_line,add\\_circle$). The corresponding parameter values of these operations are then generated based on the range specified in Table 3. By integrating these template operations with their respective parameters, a complete CAD program is formulated, which is then translated into 3D CAD models through Fusion 360 software. These models are then rendered to obtain their images. Additionally, the CAD programs are vectorized and quantized to derive their feature matrices, as discussed in Section 4.2. An image paired with its feature matrix, both derived from the same CAD program, constitutes a data pair. The method is exemplified using a cylinder model in Figure 3."}, {"title": "EXPERIMENTS AND RESULTS", "content": "We divided each of the two training datasets, i.e., the dataset with or without rules, into three subsets: train, validation, and test set with a proportion of 8:1:1. The validation set was used to monitor the training process, preventing the model from overfitting to the train set data and ensuring that the model's generalization capabilities to the unseen data, e.g., test set data. We employed a grid search strategy in Stage 1 to find optimal hyperparameters of the neural network models and the training, aiming to minimize the training loss while maintaining good generalizability of the models. The AE and VAE models exhibited similar training trends, leading us to use the same hyperparameter set for both models. In our experiments, for Stage 1, a latent dimension size of 256 proved optimal for both models, resulting in the lowest reconstruction loss for the test set data among trials with dimensions of 64, 128, 256, and 512. Other hyperparameters include 500 epochs of training with a batch size of 512, the Adam optimizer, and a learning rate of 0.001. Moving to Stage 2, we initiated training with a pre-trained ResNet18 model [46] that possesses a broad comprehension of various images. The S2 encoder was trained for 50 epochs using the Adam optimizer with a learning rate of 0.0001 and a batch size of 128. A dropout ratio of 0.4 was applied in the dropout layer."}, {"title": "OVERALL EVALUATION OF THE CAD PROGRAMS", "content": "In this section, we present the results of our experiments on the performance of the Image2CADSeq model."}, {"title": "OVERALL EVALUATION OF THE CAD PROGRAMS", "content": "Figure 5 provides a comparison of the Image2CADSeq model's performance, evaluated under two different architectures and two datasets. Specifically, there are three subfigures, each representing a unique combination of architecture and dataset. Figure 5 (a) illustrates the performance of the network when employing the TEA architecture in conjunction with the dataset without rules. In contrast, Figure 5 (b) presents results derived from the same TEA architecture, but the network is trained on the dataset with rules. We observed a significant improvement when employing the dataset with rules for model training. Consequently, we tested the TEVAE architecture using the dataset with rules only, and the results are presented in Figure 5 (c).\nThe figures illustrate the model's performance across various metrics (as defined in Table 4 (b)) when applied to the first n operations in a CAD program. We limit n to 6 to encompass the longest template sequences. According to Table 5, the maximum length of the template sequences is 5 for CAD operations in addition to a non-CAD operation SOP, marking the start of a program. Typically, higher metric values indicate superior performance, except for the EDSOT, where lower values are preferable. To maintain a uniform direction of performance across all metrics and enhance the readability of the plotting, we present the EDSOT in its negative form in the figures. Furthermore, when calculating metrics related to parameter accuracy, such as the accuracy of"}, {"title": "ANALYSIS ON THE OVERALL PARAMETER ACCURACY", "content": "CAD programs (ACP) and the accuracy of parameter\u00b9 (AP\u00b9), we introduce a tolerance level ($\\eta = 3$). This tolerance accounts for permissible deviations in the quantized continuous variables that have 256 levels but does not extend to discrete variables, such as the sketch plane identifier that has only 3 levels (refer to Table 3). The tolerance reflects the design problem's criteria, allowing certain margins of error in parameter predictions that can be customized in different scenarios.\nThe metrics, classified into three hierarchical categories in Section 4.5, include the accuracy of CAD programs (ACP), the accuracy of the sequence of operation types (ASOT), and the edit distance of the sequence of operation types (EDSOT) for H1 sequence evaluation; the accuracy of the operation types (AOT) and the accuracy of parameter\u00b9 (AP\u00b9) for the evaluation of the H2 sequence-based operation type; and the multiset similarity of operation types (MSOT) for the evaluation of the H3 set-based operation type. We analyze these results according to this hierarchical structure.\nUpon analyzing the results of Figure 5 (a) for the TEA trained using the dataset without rules (referred to as Case 1), we observe a downward trend in all metrics as the sequence length increases. This is intuitive, and predicting longer sequences is inherently more difficult for the model. The ACP metric drops to zero at n = 3, indicating that the model struggles to accurately predict the entire CAD program including both the operation types and parameters even when the sequence is relatively short. Notably, the ACP's decrease to roughly 0.4 at n = 2 suggests the model's specific difficulty in predicting the sketch plane given the input images, because the second CAD operation\u2014Sketch\u2014defines the sketch plane's position. In addition, both ASOT and the negative EDSOT metrics exhibit declines beginning with n = 3, together with the results of ACP, showing the limited sequence prediction capability of the model when the sequences get longer.\nMoreover, despite the model's low values in H1 metrics and the low values in AP\u00b9 of H2, it scores highly on the AOT metric of H2 and maintains high values of MSOT-TC and MSOT-CS in H3. This discrepancy and inconsistency probably result from the characteristics of the dataset, in which different shape categories share similar CAD sequences (see Figure 5). For example, a ground truth (GT) sequence of TS 3, [\u201cS\u201d,\u201cA\u201d,\u201cA\u201d,\u201cA\u201d, \u201cE\u201d], could be mistakenly predicted as a TS 4 sequence, e.g., [\u201cS\u201d, \u201cL\u201d, \u201cA\u201d, \u201cA\u201d, \u201cE\u201d] by the model. Although such a prediction would be deemed as an incorrect sequence prediction when evaluated against ASOT, it would score well (i.e., 4 correct and 1 incorrect operation type) in terms of AOT due to the correctly predicted operation types. A dataset encompassing a broader array of design objects with diverse sequences of CAD operations might mitigate such discrepancies in the metric values, such as real-world designs collected from human designers. More significantly, it underscores the need for a comprehensive evaluation framework for image-to-CAD sequence prediction, ensuring that models are thoroughly assessed from multiple perspectives. Otherwise, the results of the performance of the models might be biased."}]}