{"title": "Uniform Attention Maps: Boosting Image Fidelity in Reconstruction and Editing", "authors": ["Wenyi Mo", "Tianyu Zhang", "Yalong Bai", "Bing Su", "Ji-Rong Wen"], "abstract": "Text-guided image generation and editing using diffusion models have achieved remarkable advancements. Among these, tuning-free methods have gained attention for their ability to perform edits without extensive model adjustments, offering simplicity and efficiency. However, existing tuning-free approaches often struggle with balancing fidelity and editing precision. Reconstruction errors in DDIM Inversion are partly attributed to the cross-attention mechanism in U-Net, which introduces misalignments during the inversion and reconstruction process. To address this, we analyze reconstruction from a structural perspective and propose a novel approach that replaces traditional cross-attention with uniform attention maps, significantly enhancing image reconstruction fidelity. Our method effectively minimizes distortions caused by varying text conditions during noise prediction. To complement this improvement, we introduce an adaptive mask-guided editing technique that integrates seamlessly with our reconstruction approach, ensuring consistency and accuracy in editing tasks. Experimental results demonstrate that our approach not only excels in achieving high-fidelity image reconstruction but also performs robustly in real image composition and editing scenarios. This study underscores the potential of uniform attention maps to enhance the fidelity and versatility of diffusion-based image processing methods. Code is available at https://github.com/Mowenyii/Uniform-Attention-Maps.", "sections": [{"title": "1. Introduction", "content": "In recent years, the field of image processing has seen significant advancements, particularly with the development of Denoising Diffusion Probabilistic Models (DDPMs) [4, 11,27,28]. These models have revolutionized image composition and editing by enabling more precise and creative control over images [10, 21]. One of the key innovations has been the introduction of tuning-free methods, which allow for effective editing without the need for extensive model adjustments. These methods offer simplicity and efficiency by manipulating latent vectors during the denoising process, unlocking new possibilities for accurate image editing. However, applying these tuning-free techniques to real-world images presents challenges. In practice, the latent vectors of real images are often unknown, making it difficult to directly apply these methods, which limits their practical use.\nTo overcome this, researchers have developed inversion methods like Denoising Diffusion Implicit Models (DDIM) Inversion [29], which map images back to their noisy latent vectors using a trained diffusion model. This approach has been particularly effective for unconditional diffusion models. Additionally, recent advances in text-conditioned DDIM inversion [9, 14, 23, 25] have further improved image editing algorithms, making our approach more robust and applicable to a wider range of scenarios.\nOur contributions are threefold: (1) We provide a detailed analysis of how cross-attention impacts image reconstruction, (2) We propose an enhanced reconstruction method that shows superior performance in both image composition and editing tasks, and (3) We develop an automatic mask generation technique that significantly improves the accuracy and effectiveness of image editing. Through these innovations, we aim to advance image processing, offering new tools and methods that can be easily adopted in practical applications."}, {"title": "2. Related work", "content": "In recent years, significant advancements have been made in the field of text-guided vision tasks, encompassing areas such as vision-language inference [6, 18, 26, 30], text-to-image generation [7, 24, 27, 28], and image editing [3, 10, 14, 23, 25]. While our focus in this paper is on text-conditioned image editing with diffusion-based models, these works highlight the broader importance of effective text guidance in vision-related tasks. The biggest challenge in this task is how to achieve the intention of the guiding texts while ensuring fidelity to the input image. Previous works can be categorized as end-to-end editing models, tuning-based methods, attention-based methods, and sample-based methods. (a) End-to-End Editing Model: Methods like InstructPix2Pix [2] and DiffusionCLIP [17] fine-tune pre-trained text-to-image models to revise images based on simple instructions, allowing for efficient and quick edits without per-example fine-tuning or inversion. (b) Tuning-based methods: Tuning-based methods involve training a set of learnable parameters or fine-tuning a model to encapsulate certain concepts. Methods such as Imagic [16] and Unitune [32] specifically fine-tune the model on the input image to achieve high fidelity. These methods are time-consuming and the misalignment of learned variables with the diffusion model's expected input distribution compromises the integrity and quality of edits, limiting their practical use in fast-processing and high-fidelity applications [14]. (c) Attention-based methods: Attention mechanisms allow models to \"focus\" on specific parts of an image, making it possible to edit certain areas or aspects without affecting the entire image. These methods improve precision, context awareness, and efficiency of image editing, enabling more complex edits. For instance, Prompt-to-Prompt [10] and MasaCtrl [3] focus on integrating attention mechanisms to ensure that edits are contextually aware and maintain the essence of the input image. Our method can be combined with them to help achieve better reconstruction results and enhance editing efficiency. (d) Sample-based methods: Methods like Null-text Inversion [25], Negative-prompt Inversion [23], Proximal Guidance [9], Direct Inversion [14], EDICT [33], and Edit Friendly DDPM [13] focus on refining the reconstruction process to improve the fidelity of the input image during editing. TF-ICON [21] shows that semantically meaningful text in the input prompt introduces deviations in the diffusion process, causing a mismatch between the forward and reverse trajectories in the ODE-based sampling steps. To address this, the concept of an \u201cexceptional prompt\" is introduced, using a selected token to stabilize the diffusion process and improve image reconstruction. However, this approach often struggles to generalize across generative models due to inherent differences in their architectures, especially in text encoders. DiffEdit [5] uses differences in noise predictions to create masks for faithful image editing. We also use masks during editing. The proposed adaptive masks vary with each timestep to better align with our reconstruction method and achieve superior editing performance."}, {"title": "3. Method", "content": "In this section, we investigate the underlying causes of reconstruction errors associated with different prompts and propose a method to improve reconstruction by reducing the impact of the cross-attention term. We then introduce an automatic mask generation technique that integrates this method into existing image editing algorithms.", "sub_sections": [{"title": "3.1. Preliminaries", "content": "DDIM Inversion. Denoising Diffusion Implicit Models (DDIMs) [29] are an extension of Denoising Diffusion Probabilistic Models (DDPMs) [4, 11, 27, 28], designed to offer a deterministic sampling process. The reverse process in DDIM can be described as follows:\n$$z_{t-1} = \\sqrt{1 - \\alpha_{t-1}} \\cdot \\epsilon_{\\theta}(z_t, t)+\\frac{\\sqrt{\\alpha_{t-1}}}{\\sqrt{\\alpha_t}} (z_t - \\frac{\\sqrt{1 - \\alpha_t}}{\\sqrt{\\alpha_{t}}} \\epsilon_{\\theta}(z_t, t))$$\n\"predicted $$z_{0,t}$$\"\n(1)\nwhere $$z_{t-1}$$ represents the latent vector at the previous timestep, derived from $$z_t$$ at the current timestep. $$\\epsilon_{\\theta}(z_t, t)$$ denotes the estimated clean image at timestep t. The parameters $$\\alpha_t$$ are derived from the forward diffusion process, and the function $$\\epsilon_{\\theta}(z_t, t)$$ estimates the noise at each timestep. To make this process more practical for image editing, we can rearrange Eq. (1) as:\n$$z_t = \\sqrt{1 - \\alpha_t} \\cdot \\epsilon_{\\theta}(z_t, t)+\\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-1}}} (z_{t-1} - \\sqrt{1 - \\alpha_{t-1}}\\epsilon_{\\theta}(z_t, t))$$\n(2)\nWhen applying this model to real images, the goal is to obtain the initial noise vector $$z_T$$ from a given image representation $$z_0$$ as the starting point for further editing. However, directly computing $$z_t$$ requires the noise prediction $$\\epsilon_{\\theta}(z_t, t)$$, which is not always accessible. Therefore, during the inversion process, an approximation is made by using the noise prediction from the previous timestep $$\\epsilon_{\\theta}(z_{t-1}, t - 1)$$ [33]. This approach results in a sequence of latent variables, $${z_t}_{t=1}^T$$, that traces back through the diffusion process:\n$$z_t = \\sqrt{1 - \\alpha_t} \\cdot \\epsilon_{\\theta}(z_{t-1}, t-1)+\\frac{\\sqrt{\\alpha_t}}{\\sqrt{\\alpha_{t-1}}} (z_{t-1} - \\frac{\\sqrt{1 - \\alpha_{t-1}}}{\\sqrt{\\alpha_{t-1}}}\\epsilon_{\\theta}(z_{t-1}, t-1))$$\n\"predicted $$z_{0,t}$$\"\n(3)"}, {"title": "Cross-attention mechanism", "content": "Cross-attention mechanism. In diffusion models implemented using U-Net, the text condition is typically incorporated through a cross-attention mechanism [27, 28]. When predicting $$\\epsilon_{\\theta}(z_t, t, c)$$, where $$c \\in \\mathbb{R}^{N\\times d_c}$$ represents the input text and $$N$$ is the token number of the input text, the flattened intermediate representation of the $$l$$th layer of the model $$\\uplus_\\theta$$ at time step t, denoted as $$x^{(l)} \\in \\mathbb{R}^{M^{(l)}\\times d^{(l)}}$$, is updated via cross-attention as follows:\n$$x_t^{(l)} = x^{(l)} + A_t^{(l)}$$\n(4)\nwhere $$x_t^{(l)}$$ is the updated representation, and $$A_t^{(l)}$$ represents the cross-attention term (or update term), calculated as:\n$$A_t^{(l)} = S_t^{(l)} \\cdot V^{(l)}$$\n(5)\nwith the score map $$S_t^{(l)} \\in \\mathbb{R}^{M^{(l)}\\times N}$$ defined by:\n$$S_t^{(l)} = \\text{softmax}(\\frac{Q^{(l)}(K^{(l)})^T}{\\sqrt{d}})$$\n(6)\nwhere $$Q^{(l)} \\in \\mathbb{R}^{M^{(l)} \\times d^{(l)}}$$ is the linear transformation of $$x^{(l)}$$, and $$K^{(l)}, V^{(l)} \\in \\mathbb{R}^{N \\times d^{(l)}}$$ are the linear transformations of $$c$$. Note that $$K^{(l)}$$ and $$V^{(l)}$$ are independent of the time step."}, {"title": "3.2. The Devil in Reconstruction: Non-uniform Cross-attention", "content": "DDIM inversion assumes that the noise predictions at adjacent timesteps, $$\\epsilon_{\\theta}(z_t, t)$$ and $$\\epsilon_{\\theta}(z_{t-1}, t - 1)$$, are approximately equal. When conditioned on a prompt c, the difference between $$\\epsilon_{\\theta}(z_t, t, c)$$ and $$\\epsilon_{\\theta}(z_{t-1}, t - 1, c)$$ becomes significant, leading to notable reconstruction errors. These discrepancies arise because the cross-attention term $$A_t^{(l)}$$, which integrates semantic guidance from the prompt into the intermediate latent representation, is misaligned between the inversion and reconstruction processes.\nTo quantify this phenomenon, we analyze 700 images from the PIE benchmark to explore the relationship between the Mean Squared Error (MSE) of the predicted clean image $$z_{0,t}$$ and the cross-attention term $$A_t^{(l)}$$ during the inversion and reconstruction phases. Detailed experimental settings can be found in the supplementary materials. As illustrated in Fig. 3, the scatter plot highlights this relationship, with a clear positive correlation shown by the red trend line. This indicates that discrepancies in the cross-attention term $$A_t^{(l)}$$ contribute to errors in the reconstructed image $$z_{0,t}$$.\nThis observation is further supported by the visualization experiments presented in Fig. 2, which track the inversion and reconstruction trajectories for an avocado example. At each timestep, we first compute the update term $$A_t^{(l)}$$ from the U-Net model's layers, as illustrated in Fig. 2 (a). Following this, the clean predicted image $$z_{0,t}$$ is generated, as shown in Fig. 2 (b). Fig. 2 (a) highlights a clear mismatch between inversion and reconstruction, particularly under source and null prompt conditions (black-boxed regions), suggesting that misalignment in the cross-attention mechanism contributes to these distortions. The observed misalignment of the update term $$A_t^{(l)}$$ across both trajectories at the same timestep in Fig. 2 suggests that cross-attention is responsible for the reconstruction errors. Experiment details can be found in the appendix."}, {"title": "3.3. Our solution", "sub_sections": [{"title": "3.3.1 Uniform Cross-attention Maps", "content": "Experimentally, the interaction between text prompts and the model's intermediate representation using the attention mechanism introduces inconsistencies that degrade the quality of the final image reconstruction.\nBuilding on our experiments and analyses, we propose Uniform Cross-attention Maps to enhance stability and consistency across various prompts and models. Instead of relying on traditional cross-attention maps, which vary significantly depending on the input prompt, we introduce uniform attention maps where each element is assigned a fixed value of 1/N:\n$$S_{\\text{uniform}}^{(l)} = \\frac{1}{N} \\mathbf{1}_{M^{(l)} \\times N}$$\n(7)\nHere, $$\\mathbf{1}_{M^{(l)} \\times N}$$ denotes an $$M^{(l)} \\times N$$ matrix with all elements equal to 1, with $$M^{(l)}$$ being the number of visual tokens and $$N$$ the number of conditioning tokens. This uniform distribution of attention reduces the variance introduced by different text prompts, ensuring that the model's focus remains balanced across all tokens in $$x^{(l)}$$. As demonstrated in Fig. 1 (a), our approach effectively mitigates the deviations caused by semantic variations in text prompts, resulting in more reliable and consistent image reconstructions, as evidenced by the improved performance metrics in Tabs. 1 and 2. In contrast, Zero Cross-Attention Maps, which replace the cross-attention term $$A^{(l)}$$ with zeros, eliminate all semantic guidance from text prompts. While this ensures consistency, it leads to overly simplistic reconstructions and disrupts the pretraining distribution of latent features $$x^{(l)}$$, which were optimized to interact with cross-attention. This deviation significantly degrades the model's ability to preserve fine-grained details and complex structures. These limitations underscore the importance of uniform attention maps, which not only reduce prompt variance but also maintain compatibility with the pretraining distribution to achieve high-fidelity reconstructions."}, {"title": "3.3.2 Adaptive Mask Guided Editing", "content": "The direct use of uniform attention maps in current text-driven editing pipelines presents challenges, as these pipelines typically rely on manipulating cross-attention maps to achieve precise edits. However, the exceptional reconstruction performance of uniform attention maps offers a unique opportunity to improve editing tasks. To harness this reconstructive capability, we propose a novel approach, namely adaptive mask-guided editing, which effectively utilizes the strengths of uniform attention maps in editing scenarios. The overall process is illustrated in Fig. 4.\nIn this method, the input image is processed through three parallel branches: the auxiliary branch, the source branch, and the target branch. The auxiliary branch, which uses a null prompt combined with uniform cross-attention maps, ensures stable reconstruction. The source branch uses the source prompt $$c_{\\text{src}}$$, while the target branch operates with the target prompt $$c_{\\text{tgt}}$$ to apply the desired edits.\nTo further refine this process, we introduce an adaptive mask generation technique that compares the noise predictions between the source and target branches. This comparison yields a difference, $$diff_t^{+tgt} = |z_{0,t}^{tgt} - z_{0,t}^{src}|$$, identifying areas requiring modification. A threshold $$\\lambda$$ is then applied to this difference to create a mask $$M$$, which is subsequently refined using a dilation operation with a square kernel to handle minor inconsistencies:\n$$M = \\text{dilate}(|z_{0,t}^{tgt} - z_{0,t}^{src}| < \\lambda)$$.\nAfter $$T_{\\text{mask}}$$ timesteps, this mask is employed to blend the predicted clean images $$z_{0,t}^{aux}$$ and $$z_{0,t}^{tgt}$$ from the auxiliary and target branches, ensuring that the model preserves key details from the original image while applying targeted edits:\n$$z_{0,t}^{tgt} = M \\odot z_{0,t}^{aux} + (1 - M) \\odot z_{0,t}^{tgt}$$.\nBy selectively blending the clean images using the mask, the algorithm achieves a balance between maintaining the original image's fidelity and incorporating the desired modifications. This approach ensures that critical details are preserved, while the edits are seamlessly integrated into the final output. For a detailed representation of the algorithm, please refer to the pseudo-code in supplementary materials."}]}]}, {"title": "4. Experiments", "content": "In our experiment, for the image composition task, we follow the experimental setting and composition process of [21], using Stable Diffusion v2.1 [27] and the 20-step DPM solver sampling method [20]. We use Uniform Attention Maps (UAM) combined with token values from the target prompts in both the inversion and composition processes. For the image editing task, we follow the setup of [14], using the DDIM solver sampling method [29] with 50 steps. The experiments are conducted on a single setup with an A800 GPU, where our method efficiently uses up to 13.7 GB of GPU memory. Additionally, we set the threshold $$\\lambda$$ at the 50% quantile of the $$diff_t^{+tgt}$$ and $$T_{mask}$$ to 200, using UAM combined with token values from the null prompts.", "sub_sections": [{"title": "4.1. Experimental Setup", "content": "Data Set. To conduct an objective evaluation of the effectiveness of our method for image editing, we conduct experiments using PIE benchmark [14], which has 700 images and a diverse set of complex image editing tasks, including object addition or removal, color changes, and so on. For image composition task, we use the TF-ICON bench mark [21]. In addition, CelebA-HQ dataset [15] and PIE benchmark [14] are used to verify the reconstruction effect of our UAM.\nComparison to other methods. For the image editing task, we consider several baselines, including DDIM [29], Null-Text (NT) [25], Negative Prompt (NP) [23], StyleDiffusion (StyleD) [19] and Direct Inversion (DI) [14]. Additionally, we consider two editing methods: (1) Prompt-to-Prompt (P2P) [10] and (2) MasaCtrl (Masa) [3]. For the image composition task, we compared our approach with the current state-of-the-art, TF-ICON [21]."}, {"title": "4.2. Image Reconstruction", "content": "In Tab. 1 and Tab. 2, our method demonstrates superior reconstruction capabilities, achieving the best results in comparison to the baselines. This further supports the robustness of our approach in generating high-quality images that faithfully adhere to the input specifications."}, {"title": "4.3. Image Composition", "content": "Qualitative Evaluation. As shown in Fig. 5, our method achieves a superior balance between semantic expression and fidelity when compared to TF-ICON [21]. The visual comparison highlights that our approach not only maintains higher fidelity to the reference images but also produces more coherent and realistic results across diverse contexts, including natural photographs and artistic styles. For instance, in scenarios requiring complex interactions between foreground and background elements, our method successfully preserves the contextual integrity and stylistic consistency, leading to a more harmonious and visually appealing composition. This indicates that our method is particularly effective in handling the subtleties of image composition, where both the content and style need to be accurately represented.\nQuantitative Analysis. In Tab. 5, our method consistently outperforms existing approaches across multiple metrics, confirming its effectiveness in image composition tasks. Specifically, our approach achieves the lowest LPIPS scores [37] for both background (LPIPSBG) and foreground (LPIPSFG), which indicates a closer perceptual match to the reference images and, therefore, superior visual quality. Additionally, our method exhibits significant improvements in CLIP scores [26], with higher CLIPImage and CLIPText values reflecting better alignment between the generated images and the input descriptions. These enhancements suggest that our approach not only excels in producing visually appealing images but also in ensuring that the generated content is semantically coherent and contextually relevant."}, {"title": "4.4. Image Editing", "content": "Qualitative Evaluation. As shown in Fig. 6, our method demonstrates a superior balance between semantic expression and image fidelity when applied to both real and generated images, outperforming the DDIM+Masa approach. For instance, in the first row, where a lion in a suit is depicted, DDIM+Masa fails to accurately remove the laptop, leaving artifacts that detract from the overall image quality. In contrast, our method successfully preserves the integrity of the original image while effectively applying the desired edits. Similarly, in the second and third rows, our approach maintains the delicate balance between the new and original elements, ensuring that the edits are both contextually appropriate and visually coherent. These examples illustrate that our method better preserves critical image information and mitigates common mismatches or artifacts seen with DDIM+Masa, leading to more realistic and visually appealing results. More results are shown in Figs. 7 and 8.\nQuantitative Analysis. In Tab. 4, methods enhanced with our approach exhibit superior performance across a range of metrics compared to their baseline counterparts. Specifically, our methods significantly reduce the Structural Distance [31], indicating a closer visual resemblance to the original images and thereby enhancing fidelity. Moreover, our approach yields improvements in Background Preservation metrics, as evidenced by increased PSNR and SSIM [34] values and decreased LPIPS and MSE scores. These improvements suggest that our method better maintains the original background's integrity while applying the desired edits. Additionally, the CLIP Score for both the whole image and the edited regions shows notable gains, reflecting a more accurate alignment between the generated content and the text prompts. These enhancements collectively underscore the effectiveness of our method in preserving essential image characteristics while performing precise and contextually appropriate edits, thereby achieving a higher quality of image editing compared to existing methods."}, {"title": "4.5. Visualization of Generated Mask", "content": "In Fig. 9, we illustrate the masks for the cat as shown in Fig. 4. The masks highlight the areas that need modification, and adaptive selection at different time steps ensures that the modifications are not limited to a specific range, resulting in more realistic images. The masks change with each time step, indicating the areas requiring modifications."}, {"title": "4.6. Ablation Study", "content": "Threshold $$\\lambda$$. As shown in Tab. 6 (a), the edited images result from setting the threshold $$\\lambda$$ to different quantiles of the $$diff_t^{+tgt}$$. With an increase in the quantile, the edited image becomes more similar to the original, potentially compromising the desired semantic change. Consequently, a quantile of 0.5 is the chosen setting for subsequent experiments because it offers a balance by sufficiently reflecting the target text while preserving a close resemblance to the original image.\nMask Steps $$T_{mask}$$. As shown in Tab. 6 (b), we experiment with $$T_{mask}$$ values of 0, 200, and 400 for image editing. Notably, $$T_{mask} = 200$$ emerges as the optimal setting, preserving the original image's details while effectively introducing the intended semantic changes. This balance ensures that key features, such as the bear's texture, remain intact while still reflecting the desired alterations. In contrast, when $$T_{mask} = 0$$, the edited image deviates significantly from the original, underscoring the mask's importance. Therefore, we adopt $$T_{mask} = 200$$ for subsequent experiments."}]}, {"title": "5. Conclusion", "content": "In this work, we introduce Uniform Attention Maps to replace traditional cross-attention in DDIM-based image reconstruction and editing. Our approach significantly improves the fidelity of image reconstructions while maintaining robustness across different text prompts. We also develop an adaptive mask-guided editing technique that seamlessly integrates with our reconstruction method, enhancing the consistency and accuracy of edits. Experimental results demonstrate that our method outperforms existing approaches in image composition and editing tasks. These findings suggest that Uniform Attention Maps hold strong potential for broader applications in image processing."}]}