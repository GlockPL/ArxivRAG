{"title": "Enhancing Multi-Step Reasoning Abilities of Language Models through Direct Q-Function Optimization", "authors": ["Guanlin Liu", "Kaixuan Ji", "Renjie Zheng", "Zheng Wu", "Chen Dun", "Quanquan Gu", "Lin Yan"], "abstract": "Reinforcement Learning (RL) plays a crucial role in aligning large language models (LLMs) with human preferences and improving their ability to perform complex tasks. However, current approaches either require significant computational resources due to the use of multiple models and extensive online sampling for training (e.g., PPO) or are framed as bandit problems (e.g., DPO, DRO), which often struggle with multi-step reasoning tasks, such as math problem-solving and complex reasoning that involve long chains of thought. To overcome these limitations, we introduce Direct Q-function Optimization (DQO), which formulates the response generation process as a Markov Decision Process (MDP) and utilizes the soft actor-critic (SAC) framework to optimize a Q-function directly parameterized by the language model. The MDP formulation of DQO offers structural advantages over bandit-based methods, enabling more effective process supervision. Experimental results on two math problem-solving datasets, GSM8K and MATH, demonstrate that DQO outperforms previous methods, establishing it as a promising offline reinforcement learning approach for aligning language models.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have shown remarkable performance and potentials on a wide range of tasks including dialog generation (Han et al., 2024), general question answering (Alawwad et al., 2024), code generation (Jimenez et al., 2023; Chen et al., 2024b), agent (Wang et al., 2024) and math problem solving (Yu et al., 2024; Shao et al., 2024). To ensure good performance, one of the key procedures is to align the language models with human preference or task-specific requirements by reinforcement learning (RL) (Bai et al., 2022; Touvron et al., 2023). Canonically, the alignment training pipeline consists of two stages (Ouyang et al., 2022). In the first stage, a reward model under the Bradley-Terry model (Bradley & Terry, 1952) is trained on human or environment-labeled preference data. Then the language model is trained by online RL algorithms like Proximal Policy Optimization (PPO) (Schulman et al., 2017) with the reward signals provided by the reward model trained in stage one.\nDespite the good performance achieved, the online RL methods usually involve sampling during training, which is both costly and unstable compared to offline methods (Choshen et al., 2020). These issues are overcome by offline preference learning methods, of which the representative is Direct Preference Optimization (DPO) (Rafailov et al., 2024). DPO and its follow-ups (e.g., Zhao et al. (2023); Azar et al. (2024)) treat the language model as the policy model and reward model simultaneously and train the model on offline pairwise preference data directly, therefore eliminating the need for a separate reward model. Though simple, direct preference learning has been shown effective and efficient in LLM alignment (Tunstall et al., 2023).\nHowever, in practice, sometimes it is hard to acquire pairwise data required by the above methods. This issue becomes particularly severe under scenarios like math problem solving or code generation when generating high-quality data requires domain-specific expertise (Saunders et al., 2022; Bowman et al., 2022). This drawback of DPO has recently been circumvented by Direct Reward Optimization (DRO) (Richemond et al., 2024). DRO formulates the LLM generation task as a single-step MDP (i.e., bandit) and adopts the framework soft actor-critic (SAC) (Haarnoja et al., 2018), where the advantage is directly parameterized by the language model. Consequently, DRO inherits the advantage of offline policy gradient and gets rid of the dependency on pairwise data.\nNevertheless, DRO treats the process as a bandit problem, which neglects the intrinsic long- horizon nature of a wide spectrum of tasks that require complex multi-step reasoning like math problem solving and code generation (Kang et al., 2024; Miao et al., 2024), where an erroneous reasoning is almost fatal. Previous RL research found that if rewards are only provided at the end of the episode, discovering this sparse reward signal is a hard exploration problem and sparse reward functions may not be able to meaningfully distinguish between a wide range of different policies, which makes the training inefficient (Riedmiller et al., 2018; Wilcox et al., 2022). In the meanwhile, recent studies show that signals from process reward models (PRMs) can further boost the performance of language model (Zhang et al., 2024a; Lightman et al., 2023). The positional information of PRM scores usually implies the critical mistakes in the reasoning and therefore provides stronger supervision signals. However, if the whole generation process is simplified as a single action, the process reward will be aggregated and the positional information will be lost, implying that DRO cannot efficiently utilize process supervision.\nIn order to overcome the aforementioned issues, in this paper, we propose Direct Q-function optimization (DQO), an offline RL algorithm for LLMs. In DQO, the responding procedure is formulated as a Markov Decision Process (MDP) and our goal is to learn an optimal policy under KL-regularization. Our algorithm adopts the framework of soft Q-learning, where the Q-function is directly parameterized by the language model. Then both the Q-function network and the value network are updated according to Soft Bellman Equation on offline data. The MDP formulation makes DQO a multi-step learning algorithm, and can therefore exploit process reward signals. A holistic comparison of our method and other alignment methods is shown in Table 1. Unlike DPO or DRO, where all tokens (actions) within a response (trajectory) are evenly incentivized or"}, {"title": "2 Preliminaries", "content": "In this section, we introduce the foundational concepts and notations that underpin our proposed algorithm. We first review the basic framework of modeling language generation as a reinforcement learning task, followed by a KL-regularized reinforcement learning objective.\nModeling Language Generation as Token-Level MDP Reinforcement Learning (RL) is con- cerned with learning a policy that maximizes the cumulative reward for an agent interacting with an environment. In this work, we formalize language generation tasks as a Markov decision process (MDP). We denote prompt as x and a response to the prompt as y, which can each individually be broken down into a sequence of tokens, for example, $x = (x_0,...,x_m)$, from a fixed discrete vocab- ulary A. We define the token-level MDP as a tuple $M = (S, A, P, r, d_0, \\omega)$. In the defined MDP, S is the space of the state consisting of all tokens generated so far, i.e., $s_t = (x_0,..., x_m, y_0, \u2026\u2026\u2026, y_t)$. The action space A is the fixed discrete vocabulary. The dynamics P are the deterministic tran- sition model between tokens, i.e., $P(s_{t+1}|s_t, a) = 1$ for $s_t = (x_o,..., x_m, y_0,..., y_t), a = y_{t+1}$ and $s_{t+1} = (X_0,..., X_m, Y_0, ..., Y_t, Y_{t+1})^1$. The generation process will terminate once the terminal ac- tion $\\omega$ (usually end-of-sentence token) is taken. The reward function r(s, a) provides scalar feedback"}, {"title": "KL-Regularized Reinforcement Learning Objective", "content": "We formulate the optimization objective as a KL-regularized RL problem. Our goal is to approximate the optimal KL-regularized policy\n$\\pi^* = \\underset{\\pi}{argmax}\\ E_{s_0 \\sim d_0} [\\sum_{h=0}^{H} (r(s_h, a_h) - \\beta KL(\\pi(\\cdot|s_h)||\\pi_{ref}(\\cdot|s_h)))]$, (2.1)\nwhere H is the total number of decision steps, $s_0$ is a prompt sampled from the dataset, r(sh, ah) is the token-level reward from the reward function, $\\beta$ is the coefficient controlling the magnitude of randomness, the state transitions are also deterministic. Even if the state transitions are random, we can use samples to approximate the state transition probabilities."}, {"title": "3 Direct Q-function Optimization (DQO)", "content": "We consider rewriting our objective function (2.1) under the framework of max-entropy rein- forcement learning. Specifically, we decompose the KL-regularization term $KL(\\pi(\\cdot|s_h)||\\pi_{ref}(\\cdot|s_h))$ into cross-entropy and entropy, leading to the following objective:\n$\\pi^* = \\underset{\\pi}{argmax}\\ E_{\\pi, s_0 \\sim d_0} [\\sum_{h=0}^{H} (r(s_h, a_h) + log \\pi_{ref}(a_h|s_h) + \\beta H(\\pi(\\cdot|s_h)))]$\n$= \\underset{\\pi}{argmax}\\ E_{\\pi, s_0 \\sim d_0} [\\sum_{h=0}^{H} (\\widetilde{r}(s_h, a_h) + \\beta H(\\pi(\\cdot|s_h)))]$, (2.2)\nwhere $H(\\pi(s_h)) = -E_{a \\sim \\pi} log \\pi(a|s_h)$ denotes the entropy of the policy at state $s_h$ and the KL-regularized reward $\\widetilde{r}$ is defined as $\\widetilde{r}(s_h, a_h) = \\beta log \\pi_{ref}(a_h|s_h) + r(s_h, a_h)$. Equation 2.2 leads to a maximum entropy reinforcement learning problem, which enjoys the well-known closed-form solution (Haarnoja et al., 2018) as follows:\n$\\pi^*(a|s_h) = exp \\left(\\frac{Q^*(s_h, a) - V^*(s_h)}{\\beta}\\right)$, (2.3)\nwhere the Soft Q-function is defined as\n$Q^*(s_h, a) = r(s_h, a) + E_{\\pi^*} [\\sum_{j=h+1}^{H} (\\widetilde{r}(s_j, a_j) + \\beta H(\\pi^*(\\cdot|s_j)))]$, (2.4)\nand the Soft V-function is given by:\n$V^*(s_h) = \\beta log \\sum_a exp \\left(\\frac{Q^*(s_h, a)}{\\beta}\\right) = \\beta E_{\\pi^*} [\\sum_{t=h}^{H} (\\widetilde{r}(s_t, a_t) + \\beta H(\\pi^*(\\cdot|s_t)))]$, (2.5)\nEquation (2.3) reveals that the optimal policy $\\pi^*$, soft Q-function $Q^*$, and soft V-function $V^*$ are interdependent, which means that knowing any two of them allows us to compute the third one."}, {"title": "3.1 The DQO objective", "content": "We adopt the Soft Actor-Critic (SAC) learning framework to learn the state value function V and state-action value function Q. In SAC, the Q-function and V-function, which are parameterized by $\\theta$ and $\\phi$ respectively, are updated by minimizing the following squared residuals:\n$L_V(\\phi) = E_{s_h \\sim D} [\\left(V_{\\phi}(s_h) - E_{a \\sim \\pi_{\\theta}} [Q_{\\theta}(s_h, a_h) + \\beta H(\\pi_{\\theta}(\\cdot|s_h)) ]\\right)^2]$, (3.1)\n$L_Q(\\theta) = E_{(s_h, a_h) \\sim D} [(Q_{\\theta}(s_h, a_h) - \\widetilde{r}(s_h, a_h) - V_{\\phi}(s_{h+1}))^2]$, (3.2)\nwhere D is the distribution of previously sampled states and actions, or a replay buffer. As shown in (2.3), the optimal policy $\\pi^*$, optimal Q-function $Q^*$, and optimal value function $V^*$ are tightly interconnected. Specifically, they satisfy the following relationship\n$Q^*(s_h, a_h) = \\beta log \\pi^*(a_h|s_h) + V^*(s_h)$. (3.3)\nInspired by this, we parameterize the Q-value-network with the policy as follows\n$Q_{\\theta}(s_h, a_h) = \\beta log \\pi_{\\theta}(a_h|s_h) + V_{\\phi}(s_h)$, (3.4)\nwhere $\\pi_{\\theta}(\\cdot|\\cdot)$ is the policy network, or the language model in this paper. In equation (3.4), instead of using an additional model to parameterize Q-value and learning the optimal policy from the optimal Q-function $Q^*$, we directly infer the policy from the Q-function by parameterizing it with $\\pi$. Therefore, we name our algorithm as Direct Q-function optimization (DQO).\nBy plugging in (3.4) to 3.2, we can rewrite the loss function for the policy as:\n$L_{\\pi}(\\theta) = E_{(s_h, a_h) \\sim D} [(\\widetilde{r}(s_h, a_h) + V_{\\phi}(s_{h+1}) - V_{\\phi}(s_h) - \\beta log \\pi_{\\theta}(a_h|s_h))]^2$. (3.5)\nSubstituting $\\widetilde{r}(s_h, a_h)$ by its definition $\\beta log \\pi_{ref}(a_h|s_h) + r(s_h, a_h)$, we obtain the objective function of the policy network as follows:\n$L_{\\pi}(\\theta) = E_{(s_h, a_h) \\sim D} [\\left(\\beta log \\frac{\\pi_{\\theta}(a_h|s_h)}{\\pi_{ref}(a_h|s_h)} - (r(s_h, a_h) + V_{\\phi}(s_{h+1}) - V_{\\phi}(s_h))\\right)^2]$. (3.6)\nTo eliminate the Q-function in the V-function objective (3.1), we consider the soft Bellman equation:\n$Q_{\\theta}(s_h, a_h) = \\widetilde{r}(s_h, a_h) + E_{s_{h+1} \\sim P(\\cdot|s_h, a_h)} [V^{\\phi}(s_{h+1})]$. (3.7)\nWhen D is generated online by the current policy $\\pi_{\\theta}$, we can estimate the expectation in state transitions by sampling and therefore substitute $E_{s_{h+1} \\sim P(\\cdot|s_h, a_h)} [V^{\\phi}(s_{h+1})]$ with $V_{\\phi}(s_{h+1})$. Consequently, we obtain the loss for the value function V in the following form:\n$L_V(\\phi) = E_{(s_h, a_h) \\sim D} [(\\widetilde{r}(s_h, a_h) + V_{\\phi}(s_{h+1}) + \\beta H(\\pi_{\\theta}(\\cdot|s_h)) - V_{\\phi}(s_h))^2]$. (3.8)\nWhen D is composed of pre-generated offline data, we employ importance sampling to reweight the offline data, ensuring that the offline dataset can be used effectively. We defer the detailed discussion to Section 3.3. It is worth highlighting that in our formulation of DQO, we consider generating each single token as an action. If we consider generating the whole utterance as a single action and set the horizon length H = 1, then equation (3.6) and equation (3.8) degenerates to the loss used by DQO (Richemond et al., 2024). This means that DRO can be viewed as a special case of the learning framework of DQO."}, {"title": "3.2 Mitigating Bias with A-Return", "content": "One-step temporal difference (TD) errors have high bias and perform poorly when the value func- tion is not well-initialized, resulting in inefficient learning. To address this, we incorporate $\\lambda$- return (Schulman et al., 2015) to improve the updates for Q-function and V-function. By definition,"}, {"title": "3.3 Reweighting Offline Data with Importance Sampling", "content": "Offline RL, also known as batch RL, focuses on learning a policy from a pre-collected, fixed dataset without further interaction with the environment. A key challenge in offline RL is the distributional shift between the behavior policy $\\mu$, which generated the data, and the target policy $\\pi$. In order to mitigate this mismatch, offline RL algorithms often incorporate regularization techniques, such as importance sampling or constraints on the learned policy, to remain close to the behavior policy and avoid overestimating the Q-values for out-of-distribution actions. In this work, we first use a KL-constrained RL objective and then employ importance sampling to reweight the offline data to match the distribution of trajectories generated by the current optimized policy.\nHere, we introduce importance sampling to help correct this mismatch, ensuring that the offline dataset can be used effectively for policy updates. Let $\\mu$ represent the behavior policy under which the offline data D was generated and $\\pi$ be the current online policy. The probability of a trajectory $\\tau$ under $\\mu$ and $\\pi$ are computed as follows:\n$\\mu(\\tau|s_1) = \\prod_{h=0}^{H} \\mu(a_h|s_h), \\pi(\\tau|s_1) = \\prod_{h=0}^{H} \\pi(a_h|s_h)$. Therefore, we know that when the offline dataset D is sampled from $\\tau$, we have\n$E_{\\tau \\sim \\pi}[f(\\tau)] = E_{\\tau \\sim D} [\\frac{\\pi(\\tau|s_h)}{\\mu(\\tau|s_h)} f(\\tau)] = E_{\\tau \\sim D} [\\frac{\\prod_{h=0}^{H} \\pi(a_h|s_h)}{\\prod_{h=0}^{H} \\mu(a_h|s_h)} f(\\tau)]$,\nwhere $f(\\tau)$ is any function of trajectory $\\tau$. This indicates that we can use the importance ratio $\\pi(\\tau|s_h)/\\mu(\\tau|s_h)$ to adjust the loss. Empirically, we truncate the importance sampling rate to avoid gradient explosion caused by extreme values. The final ratio we apply is shown as follows\n$w(\\tau) = min (c, \\prod_{h=1}^{H} \\frac{\\pi(a_h|s_h)}{\\mu(a_h|s_h)}) = exp \\left( min \\left(1, \\sum_{h=1}^{H} log \\frac{\\pi(a_h|s_h)}{\\mu(a_h|s_h)}\\right)\\right)$, (3.14)\nNow we plug in the importance ratio in (3.14) to the loss functions (3.12) and (3.13) and then obtain our final loss functions for offline learning.\n$L_V(\\phi) = E_{\\tau \\sim D} w(\\tau). \\sum_{h=1}^{H} (G_{\\phi, \\theta}^{\\lambda}(s_h) - V_{\\phi}(s_h))^2$\n$L_Q(\\theta) = E_{\\tau \\sim D} w(\\tau) . \\sum_{h=1}^{H} [\\left(\\beta log \\frac{\\pi_{\\theta}(a_h|s_h)}{\\pi_{ref}(a_h|s_h)} - (r(s_h, a_h) + G_{\\phi, \\theta}^{\\lambda}(s_{h+1}) - V_{\\phi}(s_h))\\right)^2]$.\nWhat notable is that the importance sampling weight w(\\tau) is not involved in the gradient backward computation. The introduction of importance ration enables us to leverage offline datasets in an online RL framework, ensuring that the updated policy remains consistent with the distribution of trajectories it would encounter during online interaction."}, {"title": "4 Experiments", "content": "In this section, we conduct extensive experiments to demonstrate the effectiveness of our proposed method. Moreover, we show that our method can be further augmented by utilizing process rewards."}, {"title": "4.1 Datasets", "content": "We evaluate the models using two widely established mathematical problem-solving datasets: MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). The MATH dataset consists of 5,000 challenging problems in its test set and 7500 in its train set. The problems cover various fields like algebra, geometry, probability, and calculus. The GSM8K dataset consists of a train set of 7473 problems and a test set of 1319 problems. The problems are mostly simpler grade-school math word problems. The problems in both datasets usually require multi-step reasoning and complex arithmetic to derive the correct answers.\nWe use the 7.5K training problems from the MATH dataset and 7.47K training problems from the GSM8K dataset to generate the training corpus of our baselines and DQO. We use our base model and sample 20 responses for each prompt in the training set and then label all these responses as positive and negative responses. The detailed usage of these samples is discussed in the next section and please refer to the Appendix A for a more detailed discussion of our dataset construction."}, {"title": "4.2 Models, Baselines and Evaluation", "content": "In our experiments, we select two pretrained models, Gemma-1.1-it-7B2 (Gemma) (Team et al., 2024) and Qwen2-7B-Instruct\u00b3 (Qwen) (Yang et al., 2024) as our base model. We implement our method based on HybridFlow (Pezeshkpour et al., 2024). The baselines and our methods are implemented as follows and we defer detailed hyperparameters to Appendix B\n\u2022 SFT and reject sampling: We use the canonical responses provided in the training set as the target for our SFT and all the collected positive responses as the target for reject sampling. We train the model for 3 epochs.\n\u2022 DPO: For each prompt, we pair up all generated positive and negative responses. Due to a large number of pairs which makes the training inefficient, we sample from these pairs to make our training set size moderate. We train the model for 1 epochs.\n\u2022 KTO, DRO and DQO: We use all the generated responses for training. We assign reward 1 to those correct responses and reward 0 to those incorrect response. For KTO, we train the models for 1 epochs. For DRO and DQO, We select the best checkpoint according to the training curves for evaluation.\nWe evaluate the models generated by our method and baseline methods on the test set of GSM8K and MATH. We consider two inference strategies, greedy generation and sampling. We set the sampling parameters the same as when we generated the training corpus."}, {"title": "4.3 Empirical Results and Ablation Studies", "content": "Here we show our main results on both the GSM8K and MATH datasets. The results are shown in Table 3 and Table 4 respectively. From Table 3, we see that all the methods improve the"}, {"title": "4.3.1 Importance sampling", "content": "To demonstrate the impact of the importance sampling ratio in DQO, we train DQO on Gemma without the importance sampling ratio for Q-function loss, V-function loss, and both. We present the results in Table 5. The results show that, without adding importance sampling, the performance"}, {"title": "4.3.2 A-return", "content": "In order to demonstrate the impact of A-return, we vary the value of $\\lambda$ and evaluate the training results on Gemma. Empirically, we find that the best performance is obtained at $\\lambda$ = 1 and quickly degenerates when decreasing $\\lambda$. Therefore we pick $\\lambda$ = 0.95 to make the comparison. The results are shown in Table 6. When switching $\\lambda$ to 0.95, we observe that the performance on GSM8K decreases by a margin of more than 3.71% for greedy generation and almost 5% for sampling. The results on MATH demonstrate a similar pattern and the performances of $\\lambda$ = 0.95 dropped by a margin of 2.3% on both inference strategies. The results indicate that A-return is the key component in the target of policy training."}, {"title": "4.4 DQO with Process Score", "content": "In this section, we show that when process scores at intermediate steps are available, the perfor- mance of DQO can be further improved. Here, we use synthetic process scores. In order to obtain a synthetic process score, we consider using an empirical passing rate to estimate the quality of a given response prefix. Specifically, given a prompt string x, for each failed response y, we first split the response into several segments y[0 : n], where n is the number of segments and we use y[0 : i] to denote the concatenation of first i segments. Beginning from i = n \u2212 1, we randomly sample 20 trajectories given prefix contat(x, y[0 : i]). If there is at least one correct completion, we assume that the reasoning process in y[0: i] is correct and all the process rewards for the previous step will be set to 1/n. We combine these process reward scores with the original rewards. The process is summarized in Figure 1."}, {"title": "5 Related Work", "content": "Reinforcement Learning for Language Model Alignment Aligning language models with human preferences, or reinforcement learning with human feedback (RLHF), dates back to the work of Wirth et al. (2017) and Christiano et al. (2017). It has been widely applied to a bunch of recent models including GPT-4 (Achiam et al., 2023), Gemini (Team et al., 2023), and Llama (Touvron et al., 2023), leading to the surprising performance of these models. The alignment procedure usu- ally takes place after supervised finetuning (SFT). In the canonical approaches of RLHF (Ouyang et al., 2022; Bai et al., 2022; Munos et al., 2024), a reward model is first trained with preference data and then the model is updated with Proximal Policy Optimization (PPO). Another line of works, initiating from Direct Preference Optimization (DPO) (Rafailov et al., 2024), include SLiC (Zhao et al., 2023), IPO (Azar et al., 2024), \u039a\u03a4\u039f (Ethayarajh et al., 2024) and so on. These approaches are featured by directly parameterizing the reward models with the language model and then train- ing on offline preference data. Following DPO, one branch of works, including GSHF (Xiong et al., 2024a), SPPO (Wu et al., 2024) and INPO (Zhang et al., 2024b), adapts DPO or its variant to online samples and iterative training and resulted to state-of-the-art models. On the other hand, Richemond et al. (2024) adapted offline reinforcement learning algorithm to direct preference learn- ing and proposed Direct Reward Optimization (DRO), which combined offline policy learning with a value function learning and updated policy network and value network iteratively. Our work has a similar structure to DRO, but models the language generation as an MDP rather than a bandit and can utilize process supervision to facilitate training.\nMulti-step and Long Horizon RL for LLM alignment Many tasks for LLMs require LLMs to reason step by step or interact with the environment turn by turn. However, the rewards are usually sparse since they are only provided at the end of a long horizon of reasoning or interactions. In traditional RL literature, one approach towards breaking the curse of lone horizon and sparse reward is to train or estimate an intermediate value function or process reward (Park et al., 2024) and use the process reward to guided searching (Torne et al., 2023; Zhang et al., 2024a) and RL training. The utilization of process reward has also led to better performance for LLM reasoning (Zhang et al., 2024a; Lightman et al., 2023). Most straightforwardly, Snell et al. (2023) proposed ILQL, which employed implicit Q-learning to train a Q-function network and V-function network. Then, at inference time, ILQL uses learned value functions to perturb the log probabilities of the initial policy towards utility-maximizing behavior. The success of direct preference learning also stimulates a lot of work adapting DPO to multi-turn scenarios. To estimate process reward and utilize the information provided, Chen et al. (2024a); Lai et al. (2024); Xie et al. (2024) leverages process reward signals or AI feedback to construct preference pairs for intermediate steps and update the model with original DPO. On the other hand, Xiong et al. (2024b); Shani et al. (2024) extends the vanilla DPO to accommodate the multi-turn structure. However, these approaches require pairwise data, which might not be available or easy to obtain on some specific occasions. Our work, while following the approach of direct preference learning, eliminates the need for pairwise data and can be boosted by process rewards."}, {"title": "6 Conclusion", "content": "In this work, we propose DQO, an offline reinforcement learning algorithm for enhancing the lan- guage model's ability in multi-step reasoning. Compared to previous online methods like PPO, the offline nature of DQO bypasses the requirement of an extra reward model and online sampling during training. Previous offline methods usually formulate the LLMs' responding process as a bandit problem, which usually fails to capture the implicit long-horizon and multi-step nature of those tasks requiring a long chain of thought. In contrast, DQO frames the tasks as a Markov decision process and further employs a soft actor-critic framework to learn the V-function and the Q-function, which is directly parameterized by the language model. To verify the effectiveness of DQO, we conduct extensive experiments on two math-problem-solving datasets, GSM8K and MATH, and empirical results show that DQO outperforms all our baseline. Currently, our ex- periment results are limited to two base models due to time constraints and we leave it as future work."}, {"title": "A Dataset construction", "content": "In this section, we provide details about the generating process of our training data. For all problems, we add format guide to make to language models' generation follows the same format as the solution provided by the dataset. Specifically, we use the following templates for GSM8K and MATH respectively\n\u2022 GSM8K: {{ problem }} \\n Please reason step by step, and produce a final answer following 4 '#', like \u2018#### 0'.\n\u2022 MATH: {{ problem }} \\n Please reason step by step, and put your final answer within \\boxed{}."}, {"title": "B Detailed Hyperparameters for our baselines", "content": "We provide the detailed hyperparameters of the baselines and DQO in this section. For SFT and reject sampling, we select the best learning rate from {2e-5, 1e-5, 5e-6, 1e-6} and the best epoch from {1,2,3}. For SFT, the final learning rate is set to 2e-5 for Qwen and 5e-6 for Gemma. For reject sampling, the final learning rate is set to 2e-5 for Qwen and 1e-6 for Gemma. Both SFT and reject sampling are trained for 3 epochs. For DPO, we tried \u03b2 from {0.1, 0.01} and learning rate from {5e-7, 1e-7, 5e-8} and select the best. Finally, for both Qwen and Gemma \u03b2 is set to 0.1 and learning rate is set to 5e-8 and we evaluate the checkpoint after 1 epoch. We adapt all the hyperparameters of DPO to KTO. For all the baselines above, we set global batch size to 64. As for DRO, We tried KL-regularization coefficient in 0.01, 0.03, 0.1, 0.3, 1} and learning rate to 5e-7 for Qwen and le-7 for Gemma. For our method DQO, we set the KL-regularization to 0.03 and learning rate to 5e-7 for Qwen and le-7 for Gemma. We select the best checkpoints on the training curve for evaluation."}]}