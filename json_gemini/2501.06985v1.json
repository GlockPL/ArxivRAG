{"title": "Graph Contrastive Learning on Multi-label Classification for Recommendations", "authors": ["JIAYANG WU", "WENSHENG GAN", "HUASHEN LU", "PHILIP S. YU"], "abstract": "In business analysis, providing effective recommendations is essential for enhancing company profits. The utilization of graph-based structures, such as bipartite graphs, has gained popularity for their ability to analyze complex data relationships. Link prediction is crucial for recommending specific items to users. Traditional methods in this area often involve identifying patterns in the graph structure or using representational techniques like graph neural networks (GNNs). However, these approaches encounter difficulties as the volume of data increases. To address these challenges, we propose a model called Graph Contrastive Learning for Multi-label Classification (MCGCL). MCGCL leverages contrastive learning to enhance recommendation effectiveness. The model incorporates two training stages: a main task and a subtask. The main task is holistic user-item graph learning to capture user-item relationships. The homogeneous user-user (item-item) subgraph is constructed to capture user-user and item-item relationships in the subtask. We assessed the performance using real-world datasets from Amazon Reviews in multi-label classification tasks. Comparative experiments with state-of-the-art methods confirm the effectiveness of MCGCL, highlighting its potential for improving recommendation systems.", "sections": [{"title": "1 INTRODUCTION", "content": "In the fields of data intelligence [7, 10] and personalized recommendations [5, 17], transforming scenarios into the bipartite graph format is crucial for understanding user preferences and behaviors. Signed bipartite graphs can map relationships between two groups, such as customers and products, capturing both positive and negative interactions. The link prediction task is one of the most common tasks for these graphs [35]. It can be regarded as the recommendation system to anticipate whether the items should be recommended to corresponding users, as shown in Fig. 1, where the green line represents the positive rating, whereas the blue line indicates the negative rating."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Graph representation learning", "content": "Graph representation learning is a powerful technique for understanding the relationships between entities in graph-based data [28]. By turning entities and relationships into vectors, we can represent them as numerical values in a smaller-dimensional space. This allows us to capture their meaning and attributes. These representations enable us to measure similarities between entities, uncovering hidden connections. In the field of recommendation systems, graph representation learning (GRL) has become an essential method for capturing complex relationships between users and items. GERL utilizes a transformer architecture to construct semantic representations of news and enhances these representations by incorporating information from neighboring news through a graph attention network [8]. Wang et al. [23] proposed M2GRL, which builds a graph for each single-view data and learns multiple independent representations from these graphs, improving the performance of large-scale recommendation systems. Additionally, Zhang et al. [14] proposed BLoG, which combines local and global regularization and optimizes the graph encoders, thereby enhancing the extraction of graph structural features and recommendation performance. Moreover, Wang et al. [25] presented the GLS-GRL for sequential group recommendation, which constructs group-aware long-term and short-term graphs and learns and integrates user representations through graph representation learning."}, {"title": "2.2 Graph contrastive learning", "content": "Nowadays, graph data tend to be sparse and often contain noise. In response to these challenges, various methods have been developed, including the application of contrastive learning to alleviate them [4, 40]. Graph augmentation is the core of contrastive learning [37]. This involves variations in graph structures or applying random perturbations. Graph contrastive learning (GCL) enhances recommendation systems by integrating structural and semantic graph information through self- supervised learning. HGCL [3] leverages heterogeneous relational data using contrastive learning and personalized data augmentation to improve user and item representations. ADAGCL [13] employs contrastive self-supervised learning and graph neural networks to model complex user- item relationships and capture temporal dependencies. SimGCL [32] simplifies the contrastive learning process by adding uniform noise to the representation space instead of complex graph"}, {"title": "2.3 Usage of sub-view relationship", "content": "Focusing only on holistic graph learning may restrict the depth of analysis. Thus, establishing sub-view relationships in each set is also essential. For example, as shown in Fig. 2, given a bipartite graph, if user A and user B both give a positive rating to pen A and book A and a negative rating to pen B and book B, it suggests that user A and user B have similar preference. As a result, the proximity between user A and user B is regarded as close, indicating a positive relationship. Some methods have applied the principles of balance theory to define relationships by using the structural patterns of graphs [9, 19, 24]. The triangle and butterfly theorems serve as effective strategies for forming relationships and resolving inconsistencies in the homogeneous subgraph. However, these approaches may not perform well with sparse data, leading to reduced accuracy. To address these challenges, our strategy involves utilizing knowledge of holistic-view relationships to help homogeneous subgraph learning, adopting a multi-task paradigm. The development of the homogeneous subgraph is approached as a sub-task."}, {"title": "2.4 Multi-label classification", "content": "In real-world applications, data often comes with diverse labels, as shown in Fig. 3. End-to-end learning is the method where the model learns from input data and outputs predictions without the need for explicit feature extraction. By integrating all processing steps into a single model, it can automatically capture complex patterns in the data. NeuLP [39] addresses the limitations of GNNS by integrating the linear and nonlinear properties of GNNs, effectively utilizing user attributes and interactions. The SEAL algorithm [2] uses line graph transformation. It avoids information loss in pooling layers and reduces computational complexity. Additionally, it enhances prediction accuracy by converting subgraphs to line graphs. The M-GNN [26] introduces a multi-level GNN. Its encoder embeds entities by aggregating information from neighboring nodes, and the decoder uses the learned embeddings to compute edge probabilities. Contrastive learning is an unsupervised or self-supervised learning method that learns useful representations of data. It achieves this by maximizing the consistency between similar samples and minimizing the consistency between different samples. For example, the LRDG [33] improves multi- label classification by discovering and generating latent relationships. It enhances by generating new data samples to help the model better understand label relationships. This approach is particularly suitable for handling sparse label data. C-GMVAE [1] combines contrastive learning with Gaussian"}, {"title": "3 PRELIMINARIES", "content": "In this section, we first provide the formal problem statement of the multi-label classification of link prediction in the bipartite graph. Then, the definitions and fundamental concepts used in this paper are introduced."}, {"title": "Definition 3.1 (Problem definition).", "content": "The bipartite graph data for recommendation can be represented as G = (V, E), where V = {U \\cup I}. U denotes the set of users {u\u2081, u\u2082, ..., u_{|U|}\\} and I represents the set of items {i\u2081, i\u2082, ..., i_{|I|}\\}. The edges, denoted as \\varepsilon \\in E, establish connections between users and items. These edges are categorized into three rating labels: high (\\varepsilon^h), medium (\\varepsilon^m), and low (\\varepsilon^l), reflecting the degree of user satisfaction towards the items, including high, medium and low. The core objective of our study is to predict the label of \\varepsilon that exists between a user u and an item i, formally described as (u, i) \\in \\varepsilon^?.\nWe extract the edges and partition the graph G into subsets: G_{train} for training and G_{test} for testing. We design a framework that learns the patterns from the holistic graph and homogeneous graph from G_{train}. Moreover, we incorporate a validation set to combine the representations between two views. Finally, we will assess the effectiveness of our model in G_{test}."}, {"title": "Definition 3.2 (Graph encoder layer).", "content": "Our framework employs graph convolutional networks (GCNs) for graph encoding [27]. Each node is a vector d-dimensional vector, where both users and items are denoted by h_{u(i)} \\in \\mathbb{R}^d. The principle of GCNs is message propagation, which allows nodes to aggregate features from their neighboring nodes. This process involves iterative node"}, {"title": "Definition 3.3 (Contrastive loss function).", "content": "The comparison of graph representations depends on whether the sets of graphs are processed by the same encoder. This is illustrated in Figure 4. When using the same encoder, it focuses on comparing the augmented graph with the same labels, such as G\u2081 with G\u2081' and G\u2082 with G\u2082'. For instance, the contrastive loss between G\u2081 and its augmented graph G\u2081', both carrying high labels, can be calculated as follows [36]:\n$\\mathcal{L}_{same}^{H} = \\frac{1}{|I|} \\sum_{i=1}^{|I|} -log\\frac{exp(sim(H_{i, G_1}^{H}, H_{i, G_1'}^{H}))}{\\sum_{j=1, j\\neq i}^{|I|} exp(sim(H_{i, G_1}^{H}, H_{j, G_1'}^{H}))} \\frac{1}{|U|} \\sum_{u=1}^{|U|} -log \\frac{exp(sim(H_{u, G_1}^{H}, H_{u, G_1'}^{H}))}{\\sum_{v=1, v\\neq u}^{|U|} exp(sim(H_{u, G_1}^{H}, H_{v, G_1'}^{H}))}$ \nwhere |I| is the number of items and |U| is the number of users, H represents the nodes' embeddings, Z_{i,G\u2081} represents the i-th node in the contrastive graph G\u2081, and Z_{i,G\u2081'} represents the rest of the nodes in the augmented graph G\u2081'. The variables i and j are different, meaning that i \u2260 j. This sim(,) represents the similarity function between the two representations by using cosine similarity.\nMoreover, when comparing graphs with different labels, this can also be interpreted as contrasting the representations produced by different encoders. Taking the contrastive loss between G\u2081 and G\u2082 as an illustration, its calculation is similar to Formula 4. The following show the calcautions between high labels and low labels:\n$\\mathcal{L}_{different}^{H} = \\frac{1}{|I|} \\sum_{i=1}^{|I|} -log\\frac{exp(sim(H_{i, G_1}^{H}, H_{i, G_2}^{H}))}{\\sum_{j=1, j\\neq i}^{|I|} exp(sim(H_{i, G_1'}^{H}, H_{j, G_2}^{H}))} \\frac{1}{|U|} \\sum_{u=1}^{|U|} -log \\frac{exp(sim(H_{u, G_1}^{H}, H_{u, G_2}^{H}))}{\\sum_{v=1, v\\neq u}^{|U|} exp(sim(H_{u, G_1'}^{H}, H_{v, G_2}^{H}))}$"}, {"title": "Definition 3.4 (Homogeneous graph and holistic graph).", "content": "A homogeneous graph [21] is characterized by nodes and edges of a single type. This means all nodes represent the same kind of entity, and all edges denote the same kind of relationship. For example, in a graph where all nodes represent users and all edges indicate interactions between users, the graph is considered homogeneous. In contrast, a holistic graph [16] is a more complex structure that encompasses multiple types of nodes and edges. This allows for a richer representation of diverse entities and their relationships. For instance, a holistic graph might include both user and product nodes. The edges can represent various interactions, such as user-product interactions."}, {"title": "4 ALGORITHM", "content": ""}, {"title": "4.1 The overall architecture of MCGCL", "content": "The framework begins with a main task. It focuses on learning node representations from the whole bipartite graph, specifically within the user-item context. Then in the subtask, these representations are used to establish relationships in the homogeneous user-user (item-item) subgraph. This phase is dedicated to focusing on learning representations for the hard samples. It is important to highlight that these two tasks are interconnected. The main task lays the foundation by initializing the representations, while the subtask extracts these representations for hard samples. To combine the two tasks, we employ attention aggregation. The framework can also be reviewed in Fig. 5."}, {"title": "4.2 Holistic-view for user-item graph", "content": "The main task of MCGCL is illustrated in Fig. 7. The input graph is denoted as G. To enrich the learning process, two augmented graphs, G\u2081 and G\u2082, are created by applying perturbations. Specifically, G\u2081 results from edge removals, and G\u2082 from random edge additions, symbolized by T\u2208 t,t'. These augmented graphs are then classified into three categories based on their labels-high, medium, and low, denoted as type = h, m, l. For each category, three separate GCN encoders vectorize the nodes, sharing parameters in the same label group. For example, the first encoder handles two high-rated augmented graphs, producing different representations for items <H_{i,t}^{h}, H_{i,t'}^{h}> and users <H_{u,t}^{h}, H_{u,t'}^{h}>. Similarly, representations for the medium and low ratings are generated, including items <H_{i,t}^{m}, H_{i,t'}^{m}> and users <H_{u,t}^{m}, H_{u,t'}^{m}>, <H_{i,t}^{l}, H_{i,t'}^{l}> and users <H_{u,t}^{l}, H_{u,t'}^{l}>. This results in two sets of matrices per encoder. Comparing representations from the same encoder for two augmented graphs enables calculating the loss $\\mathcal{L}_{Mp}$, from Formulation 4 and deriving the following loss:\n$\\mathcal{L}_{Mp} = \\mathcal{L}^h + \\mathcal{L}^m + \\mathcal{L}^l $\nwhere also as outlined in the upper section of Fig. 6. For each label type, with the original and its augmented graph denoted as t and t' respectively and type = {h, m, l}, the loss is computed as:"}, {"title": "4.3 Sub-view for user-user & item-item graph", "content": "The subtask of MCGCL is illustrated in Fig. 8. Firstly, we identify hard samples by selecting the top \\epsilon-fraction of samples with the highest entropy values. The entropy for each sample is computed as the cross-entropy loss from the main task, defined as: Entropy(G_n) = -y_{Mn} log (\\hat{y}_{Mn}) - (1 \u2013 y_{Mn}) log (1 \u2013 \\hat{y}_{Mn}), where y_{Mn} and \\hat{y}_{M,n} are the true and predicted labels for the n-th edge in the graph G, respectively. The samples with the highest entropy values, corresponding to the hardest- to-predict edges, are selected as the hard samples for the new graph G_{hard}. Formally, the hard sample graph G_{hard} is defined as the set of samples with the top \\epsilon-fraction of the highest entropy values: G_{hard} = {G_n | Entropy(G_n) \\in T_{\\epsilon}}, where T_{\\epsilon} is the set of samples corresponding to the top \\epsilon-fraction of the highest entropy values, defined as: T_{\\epsilon} = {G_n | Entropy(G_n) \\geq Q_{1-\\epsilon} ({Entropy(G_n) | G_n \\in G})}. These hard samples, corresponding to the edges with the highest uncertainty, are then used for further processing and model refinement.\nIn the subtask, we particularly focus on the cold start problem. The cold start problem refers to the difficulty in accurately predicting due to the lack of historical data for this new task. In"}, {"title": "4.4 Multi-label link prediction", "content": "In our framework, the main task is responsible for producing the representations of users and items, denoted as $\\mathbb{Z}_{U}^{M}$ and $\\mathbb{Z}_{I}^{M}$, while the subtask focuses on generating representations from hard samples, represented as $\\mathbb{Z}_{U}^{S}$ and $\\mathbb{Z}_{I}^{S}$. Although the main task is more important, the features of the subtask are also invaluable, as they provide implicitly crucial information to aid in reasoning. To combine both sets of features, we introduce a weighted combination approach. Moreover, we employ the attention aggregation and getting the weight matrix, $W_{U}^{S}, W_{I}^{S}$ and $W_{U}^{M}, W_{I}^{M}$ [20]. Then, we complete the combinations between the main task and subtask and obtain the final user and item representations, $Z_U$ and $Z_I$, as follows:\n$Z_U = Concat [[W_{U}^{S} \\mathbb{Z}_{U}^{S} + W_{U}^{M}(\\mathbb{Z}_{U}^{M} \\odot M_1)], \\mathbb{Z}_{U}^{M} \\odot (I \u2013 M_1)]\nZ_I = Concat [[W_{I}^{S} \\mathbb{Z}_{I}^{S} + W_{I}^{M}(\\mathbb{Z}_{I}^{M} \\odot M_2)], \\mathbb{Z}_{I}^{M} \\odot (I \u2013 M_2)]$ \nHere, $M_1$ and $M_2$ act as mask matrices that isolate the components common to both the main task and the subtask. The expressions I \u2013 M\u2081 and I \u2013 M\u2082, with I denoting the identity matrix, subtract the shared elements, yielding matrices that represent the parts specific to the main task. We use these representations to calculate the validation loss for the graph data as the same as Gtrain. Furthermore, the loss L is derived by applying cross-entropy calculations. Finally, the total loss in our framework can be formulated as:\n$L_t = (\u03b1(L_{Mp} + L_{Mc}) + \u03b2L_v) + (\u03bc(L_{Sp} + L_{Sc}) + \u03b3L_s) + L_0$\nwhere \u03b1 and \u03b2 are hyperparameters for the main task and \u00b5 and \u03b3 are hyperparameters for the subtask. With the design of MCGCL, the training procedure can be summarized in Algorithm 2."}, {"title": "4.5 Computational complexity", "content": "We focus on the computational complexity in the sub-task. By selecting the hard sample parameter \\epsilon, the computational complexity during the sub-task learning phase can be significantly reduced. For example, by setting \\epsilon = 0.3, only 30% of the samples are processed as hard samples, which leads to a substantial decrease in computational requirements. Specifically, the complexity of contrastive loss and aggregation is reduced from O(|V|\u00b2d) to O((0.3|V|)\u00b2d) = O(0.09|V|\u00b2d), resulting in a 90% reduction. Similarly, the complexity of link prediction is reduced from O(N) to O(0.3Nhard), where $N_{hard}$ is the number of hard samples, leading to a 70% reduction. This reduction in computational complexity demonstrates the effectiveness of using hard samples for sub-task processing. By focusing on the most challenging samples, we can reduce the overall computational burden, making the method more scalable and suitable for large-scale datasets."}, {"title": "5 EXPERIMENTS", "content": "Our experiments were carried out on a system equipped with the NVIDIA GeForce RTX 4060 Ti GPU, with 16 GB of memory. Detailed information about the experimental setup is provided below. In this section, we assess MCGCL in link prediction across various real-world dataset sizes, aiming to address the following research questions:\n\u2022 Q1 (Multi-label classification): How effectively does MCGCL perform in the link prediction task within a multi-label context?\n\u2022 Q2 (Binary classification): Is MCGCL capable of surpassing the performance of current leading methods, specifically SBGCL, in binary classification tasks?\n\u2022 Q3 (Ablation study): How do the individual components of MCGCL contribute to its overall performance?\n\u2022 Q4 (Parameter analysis): To what extent is MCGCL performance affected by its hyperparameters?"}, {"title": "5.1 Datasets", "content": "Our experiments use six datasets from Amazon Reviews\u00b9, namely Amazon, Arts, Automotive, Baby, Beauty, and Health. To reduce the impact of noisy data, we applied a filtering criterion to the nodes. Specifically, nodes with several connections falling below a frequency threshold of three were excluded. This could remove unreliable data points, ensuring dataset reliability for our analysis. Ratings were classified into three categories: ratings of 1 to 2 were Low, 3 was Mid, and 4 to 5 were High. We used random sampling to divide the data. For each user, 80% of the product data was randomly selected for training, 10% for validation, and the remaining 10% for prediction. This ensures that all users are included in the training, validation, and prediction phases. We conducted five crossover experiments on each dataset. Table 1 presents the statistical details of the datasets used. In our study, we classified the Arts dataset as small-size, the Amazon dataset as large-size, and the other datasets as medium-size. For a fair comparison, we also selected the three most commonly used datasets in a signed bipartite graph [36], including Review, ML-1M, and Bonanza."}, {"title": "5.2 Baselines and experiment setting", "content": "We conduct various kinds of methods between our model, MCGCL, including, end-to-end-based, contrastive learning-based, and signed graph-based. End-to-end-based methods including M-GNN"}, {"title": "5.3 Multi-label classification (Q1)", "content": "The experimental results for the multi-label classification task are presented in Table 2. MCGCL demonstrates outstanding performance across all datasets, showing significant improvements in various metrics. It consistently scores higher than the average of end-to-end methods and generally outperforms existing contrastive learning and Signed GNN methods. Specifically, the performance in AUC, Macro-F1, and Micro-F1 metrics is 10-17% higher than end-to-end methods, 4-17% higher than Signed GNN methods, and slightly higher than contrastive learning methods by approximately 1-3%. Therefore, MCGCL exhibits strong capability in handling multi-label classification tasks, particularly in learning more precise node representations across different dataset sizes. For instance, on the Amazon dataset, the AUC is 12.16% higher than the average of end-to-end methods, 16.87% higher than Signed GNN methods, and 4.45% higher than contrastive learning methods. Similar trends are observed across medium-size datasets. On the Arts dataset, MCGCL achieves an AUC of 77.67, which is 7.91% higher than end-to-end methods, 9.32% higher than Signed GNN methods, and 3.14% higher than contrastive learning methods. These results highlight the robustness of learning precise node representations across datasets of varying sizes."}, {"title": "5.4 Binary classification (Q2)", "content": "The evaluation results of the SBGCL and MCGCL for binary classification tasks are shown in Table 3. We conducted experiments on nine datasets. The result shows that MCGCL outperforms the SBGCL. The AUC value is larger than 0.7 on all datasets. Specifically, the AUC value for the Amazon and ML-1M datasets even shows an 8% improvement compared to the SBGCL. SBGCL and MCGCL both demonstrated similar and excellent classification capabilities on small datasets such as Review, Arts, and Bonanza. Although SBGCL achieved a slightly better performance than MCGCL on the Review dataset, the improvement was not significant. However, for the larger datasets, MCGCL significantly outperforms SBGCL. This is likely because larger datasets tend to have more complex relationships, and MCGCL is better at handling such challenges."}, {"title": "5.5 Ablation study (Q3)", "content": "To assess the impact of various components in MCGCL, we conducted the ablation study, as shown in Table 4. Specifically, we evaluated the performance on small datasets Arts, medium datasets Beauty, and large datasets Amazon. We compared our method, MCGCL, with four variants: MCGCLw/o main task, MCGCLw/o subtask and MCGCLw/o validation. In MCGCLw/o main task, it doesn't use the main task, and random representations are used to construct the homogeneous subgraph. The result shows that the AUC value is under 0.5 across these three datasets. This suggests that relying only on subtask learning may not be sufficient for MCGCL to classify instances with multi- label. When utilizing only the main task training on MCGCLw/o subtask, we observe improvements across all datasets compared to MCGCLw/o main task. For example, the AUC increases by 0.34 in the Arts, 0.33 in the Beauty, and 0.27 in the Amazon. This verifies the effectiveness of the main task."}, {"title": "5.6 Hyper-parameter analysis (Q4)", "content": "We conducted experiments on the Arts and Beauty datasets to analyze the impact of the \u03b1, \u03b2, \u03bc, and y parameters in Equation 26. We explored different combinations of \u03b1 and \u03b2, \u03bc and y, as shown in Fig. 9. For the Arts dataset, we observed that the AUC achieved its highest value of 0.78 when a was set to 0.6 or 0.7, and \u1e9e was set to 0.7 or 0.8. When a was around 0.9 and \u1e9e was approximately 0.2, it resulted in a lower AUC value. For the parameters \u00b5 and y, the highest AUC values were reached when \u00b5 was 0.5 or 0.6 and y was 0.7 or 0.8. In contrast, when \u00b5 was 0.9 and y was 0.3, the AUC value was the lowest. This highlights the significance of the cross-entropy loss to the total loss. For the Beauty dataset, the highest value 0.72 was obtained when a was 0.4 or 0.6 and \u1e9e was 0.7 or 0.8. The best results were obtained when \u00b5 was 0.5 or 0.6 and y was 0.6 or 0.7. When a or \u03bc was raised and \u1e9e or y was lowered, the AUC value decreased. Therefore, we chose the a and \u03b2 parameters to be 0.6 and 0.8 and the \u00b5 and y parameters to be 0.6 and 0.7, which enables MCGCL to reach the maximum AUC value.\nThe most commonly used methods for graph augmentation in contrastive learning are edge adding and edge removing. To evaluate the effectiveness of these methods, we experiment on"}, {"title": "6 CONCLUSION", "content": "In the field of recommendation systems, accurately predicting user preferences and behaviors is crucial for delivering personalized experiences. The complexities in multi-label classification within bipartite graphs present unique challenges. Our proposed model, MCGCL, addresses these challenges through the innovative use of contrastive learning and multi-task strategies. We introduced MCGCL, a new approach designed to address the complexities of link prediction in bipartite graphs, particularly in multi-label contexts. By combining contrastive learning and multi-task strategies, MCGCL improves the accuracy of recommendations. Our framework uses a dual-phase learning process. It uses holistic bipartite graph learning for the main task and explores user-user and item- item views through homogeneous subgraph learning for the subtask. Extensive experiments on real-world datasets have demonstrated superior performance compared to existing state-of-the-art methods. This highlights its potential to enhance recommendation systems and its relevance in various business scenarios.\nIn the future, we plan to explore the integration of task division using teacher-student training [22]. We suspect that current attention aggregation may not optimally combine the two tasks. In the teacher-student framework, there is a mutual exchange of knowledge. Either participant can take on the role of the teacher, supporting the other. This interaction leads to better integration and learning outcomes. Additionally, we will explore to use more advanced training skills for large-scale graph data to improve learning efficiency and overall performance [38]."}]}