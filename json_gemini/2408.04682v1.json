{"title": "TOOLSANDBOX: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities", "authors": ["Jiarui Lu", "Thomas Holleis", "Yizhe Zhang", "Bernhard Aumayer", "Feng Nan", "Felix Bai", "Shuang Ma", "Shen Ma", "Mengyu Li", "Guoli Yin", "Zirui Wang", "Ruoming Pang"], "abstract": "Recent large language models (LLMs) advance-ments sparked a growing research interest in tool assisted LLMs solving real-world chal-lenges, which calls for comprehensive evalu-ation of tool-use capabilities. While previous works focused on either evaluating over state-less web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, TOOLSANDBOX\u00b9 includes stateful tool execution, implicit state dependencies be-tween tools, a built-in user simulator support-ing on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajec-tory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canoni-calization and Insufficient Information defined in TOOLSANDBOX are challenging even the most capable SOTA LLMs, providing brand-new insights into tool-use LLM capabilities.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Mod-els (LLMs) brought forth new opportunities treat-ing LLMs as autonomous agents, capable of observ-ing real-world environments and deciding upcom-ing actions. Among which, tool-use agents (Schick et al., 2023; Qin et al., 2023a; Patil et al., 2023; Qin et al., 2024) follow human instructions and utilize real-world APIs to complete complex tasks. Con-trary to prior approaches like dialog state tracking (Henderson et al., 2014; Budzianowski et al., 2018; Rastogi et al., 2020), which require the model to explicitly generate dialog states and actions under a predefined ontology, and derive a tool call from those structured outputs, tool-use studies allow the model to directly generate tool calls based on its observations, while keeping dialog and world state tracking implicit.\nDespite the paradigm shift towards a more sim-plified problem formulation, the stateful, conver-sational and interactive nature of task oriented di-alog remains, and poses a significant challenge for systematic and accurate evaluation of tool-using LLMs. Existing benchmarks like the Berkeley Function Calling Leaderboard (BFCL) (Yan et al., 2024), ToolEval (Qin et al., 2024) and API-Bank (Li et al., 2023) attempted to tackle some of these challenges, but there is yet to be an all encompass-ing solution.\nTask oriented dialog often involves tools that are strongly coupled with a World State, e.g. a database. This can be a tool that can alter the world state, like turning on internet connection. More interestingly, there can be a tool that implicitly de-pends on a world state, for example, one cannot search for a nearby restaurant when internet con-nection is off. Sometimes, actions that deal with both of these scenarios need to be taken to complete a task, even if the user is agnostic to the underly-ing world state and only gives general instructions. The agent needs to use its own knowledge about the world and environment feedback to come up with a plan to modify the world state and complete the task. An example can be found in Figure 1.\nBFCL (Yan et al., 2024) and ToolEval (Qin et al., 2024) both rely on stateless tools interacting with web services (through RESTful APIs). As such, these evaluation benchmarks are designed to assess how agents make trials with a static environment. API-Bank (Li et al., 2023) does include a set of"}, {"title": "2 TOOLSANDBOX Design", "content": "At its core, TOOLSANDBOX is a Python native LLM testing environment, with Execution Context as world state abstraction and Python functions as Tools, where User, Agent and Execution Envi-ronment communicate with each other through a Message Bus to complete a task, which is evaluated against predefined Milestones and Minefields. As shown in Figure 2, a typical test case starts with the User speaking to the Agent. From then on, the role being addressed gets to speak next, until the end state is reached. Upon receiving a User request, an Agent can decide to respond to the User asking for more information, or inform the Execution Envi-ronment to execute a Tool, providing intended tool name and arguments. The Execution Environment executes the Tool in an code.InteractiveConsole, (Foundation, 2024), which depending on the Tool modifies the world state stored in the Execution Context, and responds to the Agent. Once the User decides the task has been completed, it in-forms the Execution Environment to execute the end_conversation tool, which puts the system in the end state, ready to be evaluated based on the di-alog's similarity to Milestones and Minefields. The remainder of this section introduces the functional-ity of each component in more details."}, {"title": "2.1 Tools", "content": "Tools in TOOLSANDBOX are a set of highly com-posable, explicitly or implicitly dependent Python functions, creating complex reasoning challenges. Besides python native tools, a handful of carefully selected RapidAPI tools were also included with a thin layer of python wrapper. Tools manipulate world state through the Execution Context when necessary, and raise informative exceptions when execution conditions were not met. See Appendix A.1 and A.2 for more information. As an exam-ple, in Figure 1, when send_message tool is called while cellular service is off, a ConnectionError is raised. This allows the Agent to reason over possible exceptions, and deduce the tool needed to resolve the exception.\nTo support ablation studies on the effect of tool schema representation on agent accuracy, we have implemented multiple tool augmentations, e.g.:\n\u2022 The agent is given distraction tools not needed to complete the task.\n\u2022 Tool or argument names becomes less infor-mative, e.g. using a tool name of settings_0 instead of set_cellular_service_status to test if the agent relies on them to infer a tool's purpose.\n\u2022 Information like argument descriptions or type hints are removed.\nFor more details on the augmentations please refer to Appendix A.2.1."}, {"title": "2.2 Roles and Message Bus", "content": "In TOOLSANDBOX there are three roles: User, Agent (Assistant) and Execution Environment. The Execution Environment, as a dedicated role, is re-sponsible for executing tool-use requests from the Agent and returning the results. Interaction be-tween the roles is enabled through a message pass-ing system. Each message contains a sender role, recipient role, content as well as to which roles the message is visible to. A simple orchestrator deter-mines message passing order by allowing the most recent recipient to be the next sender. Instead of representing the conversation as a single message thread, we use a collection of messages, i.e. Mes-sage Bus, stored within the Execution Context. The Message Bus contains a linear history of message transactions between all roles. As is shown in Ap-pendix A.3, each role writes to the same Message"}, {"title": "User Role", "content": "The User role represents a human in-teracting with an Agent, hoping to complete a task through possibly multiple rounds of conversation. When the User role decides the task has been com-pleted, or could not be completed, it can terminate the conversation using the end_conversation tool, which is the single tool available to the User. The User role is implemented with an LLM (GPT-40) and carefully calibrated prompting design to make the user simulator more realistic. As re-lated studies in user simulation (Zhang et al., 2024; Sekulic et al., 2024) suggest, one should include the user's overall goal in the simulator's system prompt. However, we found this is often insufficient for the complex interactions in TOOLSANDBOX, and can lead to two categories of failures. In some cases, it is infeasible for an LLM simulated user to judge task completion, or provide follow-up information with only access to the user goal, and not the ex-pected result, which could lead to hallucination. Also, with only a single system prompt, the simu-lated user could be derailed by the tool-use agent, failing to follow instructions. Examples of these failures can be found in Appendix A.4.\nIn light of this, we propose two additional com-ponents in user simulator prompts: Knowledge Boundary, which inform the user simulator what it should and should not know, providing partial access to expected result, combating hallucination. And Demonstration, which provides few shot ex-ample dialogs to the user simulator. Prompt ex-amples can be found in Appendix A.4. Note that demonstration is only visible to the user simula-tor and not the agent. We performed an ablation study for these components in Table 2. With both approaches combined, the LLM simulated user achieves the lowest error rate in all categories. User simulator error rate is also found to be consistent across well performing agents, shown in Table 5."}, {"title": "Agent Role", "content": "Initially, the Agent role will receive a message from the User in natural language. The Agent could decide to prompt the User again for additional information, or decide to issue a tool call towards the Execution Environment. When issuing a tool call, the Agent selects the name of the tool from a list of available tools and provides necessary"}, {"title": "Execution Environment Role", "content": "The execution en-vironment role is responsible for executing tool calls requested by the Agent and User roles in the form of Python snippets, mimicking the behavior of interactive consoles like IPython and Jupyter. Exceptions raised while executing the code are cap-tured through stderr, enabling the Agent to refine its tool calls through trial and error.\nSome LLMs support parallel tool calling, in-tended to increase efficiency when multiple, inde-pendent tools need to be called. However, if an LLM uses parallel tool calls for dependent tools, it should be penalized accordingly. For example, in Figure 1 where the agent has to enable cellular service before sending a text message, parallel tool calls should not be used. Execution Environment handles race conditions in parallel tool calls by following Murphy's Law, ensuring race condition always happens if detected."}, {"title": "2.3 Evaluation", "content": "With an interactive, stateful and conversational en-vironment, evaluation trajectories are highly dy-namic. Multiple trajectories can lead to the same outcome. A given task may be completed using different tools, the same tools in a different order, or through trial and error, and the evaluation strat-egy has to be flexible enough to accommodate for that. To combat this, we developed an evaluation strategy based on Milestones and Minefields, which defines key events that must or must not happen in a trajectory, allowing us to evaluate any trajectory with rich intermediate and final execution signals, providing deeper understanding of the model per-formance. An example can be found in Figure 10. In specific, Milestones are the critical steps needed to achieve a goal. An example is shown in"}, {"title": "The max value is the similarity score between the trajectory and the milestone DAG.", "content": "The similarity measure $sim(v^i, s_i)$ calculates similarity between a database snapshot and a target database defined in the milestone. The milestone defines the column wise similarity function used for each column. These function have a [0, 1] out-put space, and could be exact matching for cellular service, ROUGE-L F measure for message con-tent, AST matching for tool trace similar to the AST metric found in BFCL (Yan et al., 2024), and many more. This allows for great flexibility when defining milestones. For a snapshot database and"}, {"title": "The max value is the similarity score between the trajectory and the milestone DAG.", "content": "calculate a pairwise similarity for those rows $d_{a,b}$ by calculating the geometric mean of column simi-larities. Then we solve for the best assignment problem between snapshot and milestone rows, by maximizing the geometric mean of row similarities, which will be the similarity measure $sim(v^i, s_i)$. We use the geometric mean throughout to ensure that, if any column similarity must not be violated, it could emit a 0 similarity, which would nullify the overall similarity measure.\nIn addition to similarities conditioning on the current milestone and snapshot, in some cases we also allow an additional \"reference milestone\" to be provided, enabling similarity to be condi-tioned on two milestones. This can unlock power-ful constraints including guardrail_similarity, which checks if any changes are made to a cer-tain database between two milestone events, and tool_trace_dependant_similarity, which al-lows one to extract tool trace output from a ref-erence milestone, and ingest into the current mile-stone, allowing one to track the information flow of tools. An example can be found in Figure 10.\nMilestone evaluation is a powerful tool that un-locks a deeper understanding into model perfor-mance, and hints at possible areas of improvement. An example is shown in Figure 10. In the end, the task was not completed before the maximum allowed number of turns. However, intermediate milestones showcased that the model was capa-ble of solving the state dependency challenges and requesting the current location. In order to success-fully resolve this test case, we should improve the model's turn efficiency on state dependency."}, {"title": "B Test Scenarios", "content": "Test scenarios are created by 2 internal domain ex-perts from our institute who are familiar with the tool capacities in TOOLSANDBOX, and the field of task-oriented dialog. 1 annotator creates test sce-narios including the starting world state, user task, initial message, and milestone/minefield. To ensure dataset diversity while making the annotation task feasible, the annotator followed the process below:\n\u2022 The annotation process starts by creating seed scenarios. These are simple, single user turn, single tool call, self-contained requests that are supposed to cover most tools, as well as most of their arguments. For example, a seed scenario for add_reminder tool that requires timestamp, latitude, longitude would likely contain a starting user utterance saying Create a reminder to buy chocolate milk at timestamp 111222333 at latitude 37 longitude -122. Creating milestone for said scenario is trivial.\n\u2022 Next, starting from the seed scenario, the an-notator branches off to create derived scenar-ios that are more involved. This could be a Multiple Tool Call scenario, which requires the agent to invoke other tools before this one, e.g. Create a reminder to buy chocolate milk tomorrow 5PM at Whole Foods on McKinley Ave, which requires reasoning for the relative datetime, as well as searching for location lat-itude longitude. Note that this will also be considered a Canonicalization scenario.\n\u2022 It could be a Multiple User Turn scenario, which requires the agent to request more slots from the user. e.g. Create a reminder.\n\u2022 It could be a State Dependency scenario, which requires the model to solve state depen-dencies, e.g. Create a reminder to buy choco-late milk tomorrow 5PM at Whole Foods on McKinley Ave, but wifi is set to off. Preventing access to location search capability unless the dependency is resolved.\n\u2022 It could be an Insufficient Information sce-nario, which requires the model to figure out this task cannot be solved, e.g Create a re-minder to buy chocolate milk tomorrow 5PM"}, {"title": "Tool Design", "content": "Tool design choices in TOOLSANDBOX are driven by two major principles:\n\u2022 Tools should be representative and diverse, to cover key task oriented dialog use cases as well as TOOLSANDBOX test scenario cate-gories.\n\u2022 Tool capacities should be well-defined, and tool counts should be manageable, so that"}, {"title": "B.4 TOOLSANDBOX Category Statistics", "content": "The number of test scenarios per scenario category in TOOLSANDBOX can be found in Table 6"}, {"title": "C Example Trajectories", "content": null}, {"title": "C.1 Tool Call Detection", "content": null}, {"title": "C.2 Single/Multiple Tool Call/User Turn", "content": null}, {"title": "C.3 Canonicalization", "content": null}, {"title": "C.4 State Dependency", "content": null}, {"title": "C.5 Insufficient Information", "content": null}, {"title": "D Additional Evaluation Results", "content": null}, {"title": "Model Feature Comparison", "content": "Some models tested in this work, especially open source models, do not support all features required for a conversational, interactive tool-use workflow. We think it is useful to document these shortcom-ings to motivate future research, and set the right context while understanding the experiment met-rics from these models.\nIn a conversational, interactive tool-use work-flow, the agent needs to be able to accept multiple rounds of user input, decide when to generate a tool call or respond to user, and consume a tool response to determine the next step. However, as shown in 7, open source models including Gorilla and Command-R are not capable of consuming tool responses. Because of this, they can theoretically produce reasonable results for Single Tool Call test scenarios, but cannot complete any test scenario that requires multiple tool calls."}, {"title": "Turn Count Comparison", "content": null}, {"title": "3 Test Scenarios", "content": "TOOLSANDBOX contains 1032 test cases metic-ulously crafted by 2 internal domain experts to capture challenging tool-use scenarios, with hu-man authored and carefully calibrated Milestones and Minefields to support evaluation. 1 annotator is tasked to create test scenarios, while the other acts as an agent to validate milestones and mine-fields. We designed a rigorous annotation process to ensure coverage across realistic, complex use case scenarios, detailed in Appendix B.2. Statistics comparison between TOOLSANDBOX and other benchmarks can be found in Table 3. Tools in TOOLSANDBOX are designed to be representative, diverse and composable in conversational dialogs, while making tool count manageable for milestone annotation. As a result, TOOLSANDBOX test sce-narios contain on average much higher number of tool calls and turns per dialog compared to other benchmarks. Additional details about tool domain coverage and design principles can be found in Appendix B.3.\nTo closely inspect the intricate challenges in LLM tool-use applications, test scenarios are or-ganized into detailed categories, statistics can be found in Appendix B.4. A test scenario is defined"}, {"title": "4 Evaluation Results", "content": "Open Source Models Table 4 shows the aver-age similarity for each of the scenario categories described in Section 3 and tool augmentations de-scribed in Section 2.1. There is a significant per-formance gap between proprietary and open source models, with the best performing open source model Hermes (interstellarninja et al.) lagging more than 20 points behind the second to last pro-prietary model Claude-3-Haiku (Anthropic, 2024). This is partly due to the fact that models like Go-rilla (Patil et al., 2023) and Command-R (Cohere and for AI, 2024) are incapable of consuming tool responses, as shown in Appendix D.1. They can theoretically solve Single Tool Call test scenarios, but would fail in any scenario that requires multi-ple tool calls. As for Hermes and Mistral (Jiang et al., 2023), both models struggle at identifying when a tool call should be issued. Mistral for exam-ple would often mistake a tool-use scenario for a code generation task, as shown in Figure 11. These models' subpar performance unexpectedly caused them to achieve higher rating in the Insufficient Information category, which rewards the model for not generating hallucinated tool calls or arguments"}, {"title": "The similarity score equation", "content": "$score = score_{M^+} \\times I(score_{M^-} = 0),$                                      (1)\nensuring if minefields are violated (non-zero mine-field similarity), the similarity score for the whole trajectory is 0."}, {"title": "5 Related Work", "content": "Tool-use Benchmarks Various tool-use bench-marks have been developed to evaluate LLM-based agent performance in different tool-use domains. The Berkeley Function Calling Leaderboard (Yan et al., 2024), ToolBench (Qin et al., 2023b), Sta-bleToolBench (Guo et al., 2024), NexusRaven V2 Function Calling Benchmark (team, 2023), and API-BLEND (Basu et al., 2024) assess the abil-ity of LLM agents to plan and perform function calls. WebArena (Zhou et al., 2023), MiniWoB++ (Humphreys et al., 2022), Webshop (Yao et al., preprint), Mind2Web (Deng et al., 2023) and Visu-alWebArena (Koh et al., 2024) focus on the agent's ability to call search functionality to browse and leverage the web to solve the task. Apart from benchmarks specifically designed for tool-use, gen-eralist agent benchmark suites like AgentBench (Liu et al., 2023) and AgentBoard (Ma et al., 2024) include evaluating the tool-use capability of agents as a central task to examine the general problem-solving ability of LLM-based agents.\nTool-use agent Various tool-use model have been developed to solve the complicated tool-use scenar-ios in real-world. Toolformer (Schick et al., 2023) first demonstrated that language models could au-tonomously learn to use various tools, through a self-supervised learning approach. Gorilla (Patil et al., 2023) employs a self-instruct paradigm to generate {instruction, API} pairs and is trained both with and without a retriever. ToolLLM (Qin et al., 2024) enables LLMs to use over 16,000 real-world APIs by automating the generation of diverse instructional data and leveraging a neural API re-triever, showing better generalization across unseen APIs. CodeACT (Wang et al., 2024a) integrates executable code actions into training to enhance the decision-making and task-solving capabilities of LLMs, leading to more effective agents.\nDialogue State Tracking Dialogue State Track-ing (DST) requires the agent to maintain and update"}, {"title": "7 Limitations", "content": "While TOOLSANDBOX is powerful, being the first work of its kind, it still has many areas to be im-proved upon. In this section, we introduce some of these limitations, to motivate future research in this area.\nEven though Milestone and Minefields are pow-erful interactive metrics that offer insights into intermediate and final outcomes, authoring them, especially mandatory intermediate milestones, re-quires deep knowledge around the tool capacities in TOOLSANDBOX and many iterations, hinder-ing its scalability. A simplified, or fully automatic method for identifying Milestones and Minefields could be the key to further scale up the data volume in TOOLSANDBOX.\nDespite our best effort at controlling user simu-lator behavior, it is still subject to non-negligible hallucination and instruction following errors. In this work we toyed with the idea of tool assisted user simulator. Only one tool end_conversation was offered to the user simulator, and we saw a noticeable improvement in instruction following in the dialog termination aspect. By expanding its tool set, a tool assisted user simulator could be a promising direction to further reduce hallucination and improve instruction following.\nMandatory confirmation and authentication is an interesting problem currently not addressed in TOOLSANDBOX. In dialog state tracking, confir-mation is modeled by its corresponding dialog ac-tion before a transactional service is called. How-ever, in most tool-use LLM designs, we are at the liberty of the model to decide when a confirmation is necessary. An orchestration level solution to en-force confirmation, similar to GoEx (Patil et al., 2024) could be a potential inspiration, and an inter-esting problem for models to reason over.\nA challenging category of tools, namely tools that spawn a daemon process, e.g. setting a timer, is not addressed in TOOLSANDBOX currently. These tools complete and return in the main process af-ter the daemon is spawned, and at some point in the future, the daemon would interrupt the main process, e.g. when the time is up. This kind of in-terruption poses a novel problem for both execution orchestration, and the model itself.\nWhile most of TOOLSANDBOX tools are self-contained implementations, some tools that depend on an external knowledge base, like searching for weather, are still backed by external web services,"}, {"title": "D.1 Model Feature Comparison", "content": "In a conversational, interactive tool-use work-flow, the agent needs to be able to accept multiple rounds of user input, decide when to generate a tool call or respond to user, and consume a tool response to determine the next step. However, as shown in 7, open source models including Gorilla and Command-R are not capable of consuming tool responses. Because of this, they can theoretically produce reasonable results for Single Tool Call test scenarios, but cannot complete any test scenario that requires multiple tool calls."}, {"title": "Dialogue State Tracking", "content": "Dialogue State Track-ing (DST) requires the agent to maintain and update dialogue states and actions. The MultiWOZ dataset (Budzianowski et al., 2018) offers a diverse set of dialogues requiring complex state tracking across multiple domains. Building on this, Rastogi et al. (2020) proposed a schema-guided approach to DST, addressing scalability issues in multi-domain set-tings and enhancing the model's adaptability and reducing the need for extensive domain-specific annotations. However, these datasets focus on explicit state tracking on off-policy trajectories. Our benchmark complements them by introduc-ing world states, typically implicit and requiring to be inferred from world knowledge, and an inter-active environment that offers a more diverse and extensive online evaluation."}, {"title": "User Simulator in Sandbox", "content": "When assessing the agent's core competencies in state tracking, mem-orization, and long-term planning, dialogues be-tween users and the system can span over several turns, and off-policy evaluation may not always suf-fice. On the other hand, human-in-the-loop online evaluation is costly and time-consuming. Some studies have investigated incorporating a built-in user simulator to facilitate the evaluation process. DAUS (Sekulic et al., 2024) utilizes LLM finetuned on task oriented dialog trajectories. MINT (Wang et al., 2024b) utilizes GPT-4 to simulate natural language user feedback for multi-turn LLM evalu-ation. For medical agents, AMIE (Tu et al., 2024) integrates a built-in patient model to engage with a symptom collection agent. Zhang et al. (2024) develop a virtual environment for the agent model to predict unknown entities by interacting with a user simulator that responds with only yes or no. Our approach resonates with these approaches."}, {"title": "6 Conclusion", "content": "TOOLSANDBOX presents a stateful, conversational and interactive evaluation benchmark for LLM tool-use capabilities. With stateful and state dependent tools, LLM simulated user and flexible evaluation with milestones and minefields, it showcased a significant performance gap between open source and proprietary models, and unveiled challenging scenarios even for SOTA models, including State Dependency, Canonicalization and Insufficient In-formation, bringing new insights to understanding tool-use capabilities. We hope TOOLSANDBOX could be a valuable addition to LLM evaluation suites, pushing the boundary of tool-use research."}]}