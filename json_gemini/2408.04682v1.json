{"title": "TOOLSANDBOX: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities", "authors": ["Jiarui Lu", "Thomas Holleis", "Yizhe Zhang", "Bernhard Aumayer", "Feng Nan", "Felix Bai", "Shuang Ma", "Shen Ma", "Mengyu Li", "Guoli Yin", "Zirui Wang", "Ruoming Pang"], "abstract": "Recent large language models (LLMs) advance- ments sparked a growing research interest in tool assisted LLMs solving real-world chal- lenges, which calls for comprehensive evalu- ation of tool-use capabilities. While previous works focused on either evaluating over state- less web services (RESTful API), based on a single turn user prompt, or an off-policy dialog trajectory, TOOLSANDBOX\u00b9 includes stateful tool execution, implicit state dependencies be- tween tools, a built-in user simulator support- ing on-policy conversational evaluation and a dynamic evaluation strategy for intermediate and final milestones over an arbitrary trajec- tory. We show that open source and proprietary models have a significant performance gap, and complex tasks like State Dependency, Canoni- calization and Insufficient Information defined in TOOLSANDBOX are challenging even the most capable SOTA LLMs, providing brand- new insights into tool-use LLM capabilities.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in Large Language Mod- els (LLMs) brought forth new opportunities treat- ing LLMs as autonomous agents, capable of observ- ing real-world environments and deciding upcom- ing actions. Among which, tool-use agents (Schick et al., 2023; Qin et al., 2023a; Patil et al., 2023; Qin et al., 2024) follow human instructions and utilize real-world APIs to complete complex tasks. Con- trary to prior approaches like dialog state tracking (Henderson et al., 2014; Budzianowski et al., 2018; Rastogi et al., 2020), which require the model to explicitly generate dialog states and actions under a predefined ontology, and derive a tool call from those structured outputs, tool-use studies allow the model to directly generate tool calls based on its observations, while keeping dialog and world state tracking implicit.\nDespite the paradigm shift towards a more sim- plified problem formulation, the stateful, conver- sational and interactive nature of task oriented di- alog remains, and poses a significant challenge for systematic and accurate evaluation of tool-using LLMs. Existing benchmarks like the Berkeley Function Calling Leaderboard (BFCL) (Yan et al., 2024), ToolEval (Qin et al., 2024) and API-Bank (Li et al., 2023) attempted to tackle some of these challenges, but there is yet to be an all encompass- ing solution.\nStateful Task oriented dialog often involves tools that are strongly coupled with a World State, e.g. a database. This can be a tool that can alter the world state, like turning on internet connection. More interestingly, there can be a tool that implicitly de- pends on a world state, for example, one cannot search for a nearby restaurant when internet con- nection is off. Sometimes, actions that deal with both of these scenarios need to be taken to complete a task, even if the user is agnostic to the underly- ing world state and only gives general instructions. The agent needs to use its own knowledge about the world and environment feedback to come up with a plan to modify the world state and complete the task. An example can be found in Figure 1.\nBFCL (Yan et al., 2024) and ToolEval (Qin et al., 2024) both rely on stateless tools interacting with web services (through RESTful APIs). As such, these evaluation benchmarks are designed to assess how agents make trials with a static environment. API-Bank (Li et al., 2023) does include a set of"}, {"title": "2 TOOLSANDBOX Design", "content": "At its core, TOOLSANDBOX is a Python native LLM testing environment, with Execution Context as world state abstraction and Python functions as Tools, where User, Agent and Execution Envi- ronment communicate with each other through a Message Bus to complete a task, which is evaluated against predefined Milestones and Minefields. As shown in Figure 2, a typical test case starts with the User speaking to the Agent. From then on, the role being addressed gets to speak next, until the end state is reached. Upon receiving a User request, an Agent can decide to respond to the User asking for more information, or inform the Execution Envi- ronment to execute a Tool, providing intended tool name and arguments. The Execution Environment executes the Tool in an code.InteractiveConsole, (Foundation, 2024), which depending on the Tool modifies the world state stored in the Execution Context, and responds to the Agent. Once the User decides the task has been completed, it in- forms the Execution Environment to execute the end_conversation tool, which puts the system in the end state, ready to be evaluated based on the di- alog's similarity to Milestones and Minefields. The remainder of this section introduces the functional- ity of each component in more details."}, {"title": "2.1 Tools", "content": "Tools in TOOLSANDBOX are a set of highly com- posable, explicitly or implicitly dependent Python functions, creating complex reasoning challenges. Besides python native tools, a handful of carefully selected RapidAPI tools were also included with a thin layer of python wrapper. Tools manipulate world state through the Execution Context when necessary, and raise informative exceptions when execution conditions were not met. See Appendix A.1 and A.2 for more information. As an exam- ple, in Figure 1, when send_message tool is called while cellular service is off, a ConnectionError is raised. This allows the Agent to reason over possible exceptions, and deduce the tool needed to resolve the exception.\nTo support ablation studies on the effect of tool schema representation on agent accuracy, we have implemented multiple tool augmentations, e.g.:\n\u2022 The agent is given distraction tools not needed to complete the task.\n\u2022 Tool or argument names becomes less infor- mative, e.g. using a tool name of settings_0 instead of set_cellular_service_status to test if the agent relies on them to infer a tool's purpose.\n\u2022 Information like argument descriptions or type hints are removed.\nFor more details on the augmentations please refer to Appendix A.2.1."}, {"title": "2.2 Roles and Message Bus", "content": "In TOOLSANDBOX there are three roles: User, Agent (Assistant) and Execution Environment. The Execution Environment, as a dedicated role, is re- sponsible for executing tool-use requests from the Agent and returning the results. Interaction be- tween the roles is enabled through a message pass- ing system. Each message contains a sender role, recipient role, content as well as to which roles the message is visible to. A simple orchestrator deter- mines message passing order by allowing the most recent recipient to be the next sender. Instead of representing the conversation as a single message thread, we use a collection of messages, i.e. Mes- sage Bus, stored within the Execution Context. The Message Bus contains a linear history of message transactions between all roles. As is shown in Ap- pendix A.3, each role writes to the same Message"}, {"title": "2.3 Evaluation", "content": "With an interactive, stateful and conversational en- vironment, evaluation trajectories are highly dy- namic. Multiple trajectories can lead to the same outcome. A given task may be completed using different tools, the same tools in a different order, or through trial and error, and the evaluation strat- egy has to be flexible enough to accommodate for that. To combat this, we developed an evaluation strategy based on Milestones and Minefields, which defines key events that must or must not happen in a trajectory, allowing us to evaluate any trajectory with rich intermediate and final execution signals, providing deeper understanding of the model per- formance. An example can be found in Figure 10.\nIn specific, Milestones are the critical steps needed to achieve a goal. An example is shown in"}, {"title": "3 Test Scenarios", "content": "TOOLSANDBOX contains 1032 test cases metic- ulously crafted by 2 internal domain experts to capture challenging tool-use scenarios, with hu- man authored and carefully calibrated Milestones and Minefields to support evaluation. 1 annotator is tasked to create test scenarios, while the other acts as an agent to validate milestones and mine- fields. We designed a rigorous annotation process to ensure coverage across realistic, complex use case scenarios, detailed in Appendix B.2. Statistics comparison between TOOLSANDBOX and other benchmarks can be found in Table 3. Tools in TOOLSANDBOX are designed to be representative, diverse and composable in conversational dialogs, while making tool count manageable for milestone annotation. As a result, TOOLSANDBOX test sce- narios contain on average much higher number of tool calls and turns per dialog compared to other benchmarks. Additional details about tool domain coverage and design principles can be found in Appendix B.3.\nTo closely inspect the intricate challenges in LLM tool-use applications, test scenarios are or- ganized into detailed categories, statistics can be found in Appendix B.4. A test scenario is defined"}, {"title": "4 Evaluation Results", "content": "Open Source Models Table 4 shows the aver- age similarity for each of the scenario categories described in Section 3 and tool augmentations de- scribed in Section 2.1. There is a significant per- formance gap between proprietary and open source models, with the best performing open source model Hermes (interstellarninja et al.) lagging more than 20 points behind the second to last pro- prietary model Claude-3-Haiku (Anthropic, 2024). This is partly due to the fact that models like Go- rilla (Patil et al., 2023) and Command-R (Cohere and for AI, 2024) are incapable of consuming tool responses, as shown in Appendix D.1. They can theoretically solve Single Tool Call test scenarios, but would fail in any scenario that requires multi- ple tool calls. As for Hermes and Mistral (Jiang et al., 2023), both models struggle at identifying when a tool call should be issued. Mistral for exam- ple would often mistake a tool-use scenario for a code generation task, as shown in Figure 11. These models' subpar performance unexpectedly caused them to achieve higher rating in the Insufficient Information category, which rewards the model for not generating hallucinated tool calls or arguments"}, {"title": "5 Related Work", "content": "Tool-use Benchmarks Various tool-use bench- marks have been developed to evaluate LLM-based agent performance in different tool-use domains. The Berkeley Function Calling Leaderboard (Yan et al., 2024), ToolBench (Qin et al., 2023b), Sta- bleToolBench (Guo et al., 2024), NexusRaven V2 Function Calling Benchmark (team, 2023), and API-BLEND (Basu et al., 2024) assess the abil- ity of LLM agents to plan and perform function calls. WebArena (Zhou et al., 2023), MiniWoB++ (Humphreys et al., 2022), Webshop (Yao et al., preprint), Mind2Web (Deng et al., 2023) and Visu- alWebArena (Koh et al., 2024) focus on the agent's ability to call search functionality to browse and leverage the web to solve the task. Apart from benchmarks specifically designed for tool-use, gen- eralist agent benchmark suites like AgentBench (Liu et al., 2023) and AgentBoard (Ma et al., 2024) include evaluating the tool-use capability of agents as a central task to examine the general problem- solving ability of LLM-based agents.\nTool-use agent Various tool-use model have been developed to solve the complicated tool-use scenar- ios in real-world. Toolformer (Schick et al., 2023) first demonstrated that language models could au- tonomously learn to use various tools, through a self-supervised learning approach. Gorilla (Patil et al., 2023) employs a self-instruct paradigm to generate {instruction, API} pairs and is trained both with and without a retriever. ToolLLM (Qin et al., 2024) enables LLMs to use over 16,000 real- world APIs by automating the generation of diverse instructional data and leveraging a neural API re- triever, showing better generalization across unseen APIs. CodeACT (Wang et al., 2024a) integrates executable code actions into training to enhance the decision-making and task-solving capabilities of LLMs, leading to more effective agents.\nDialogue State Tracking Dialogue State Track- ing (DST) requires the agent to maintain and update"}, {"title": "6 Conclusion", "content": "TOOLSANDBOX presents a stateful, conversational and interactive evaluation benchmark for LLM tool- use capabilities. With stateful and state dependent tools, LLM simulated user and flexible evaluation with milestones and minefields, it showcased a significant performance gap between open source and proprietary models, and unveiled challenging scenarios even for SOTA models, including State Dependency, Canonicalization and Insufficient In- formation, bringing new insights to understanding tool-use capabilities. We hope TOOLSANDBOX could be a valuable addition to LLM evaluation suites, pushing the boundary of tool-use research."}, {"title": "7 Limitations", "content": "While TOOLSANDBOX is powerful, being the first work of its kind, it still has many areas to be im- proved upon. In this section, we introduce some of these limitations, to motivate future research in this area.\nEven though Milestone and Minefields are pow- erful interactive metrics that offer insights into intermediate and final outcomes, authoring them, especially mandatory intermediate milestones, re- quires deep knowledge around the tool capacities in TOOLSANDBOX and many iterations, hinder- ing its scalability. A simplified, or fully automatic method for identifying Milestones and Minefields could be the key to further scale up the data volume in TOOLSANDBOX.\nDespite our best effort at controlling user simu- lator behavior, it is still subject to non-negligible hallucination and instruction following errors. In this work we toyed with the idea of tool assisted user simulator. Only one tool end_conversation was offered to the user simulator, and we saw a noticeable improvement in instruction following in the dialog termination aspect. By expanding its tool set, a tool assisted user simulator could be a promising direction to further reduce hallucination and improve instruction following.\nMandatory confirmation and authentication is an interesting problem currently not addressed in TOOLSANDBOX. In dialog state tracking, confir- mation is modeled by its corresponding dialog ac- tion before a transactional service is called. How- ever, in most tool-use LLM designs, we are at the liberty of the model to decide when a confirmation is necessary. An orchestration level solution to en- force confirmation, similar to GoEx (Patil et al., 2024) could be a potential inspiration, and an inter- esting problem for models to reason over.\nA challenging category of tools, namely tools that spawn a daemon process, e.g. setting a timer, is not addressed in TOOLSANDBOX currently. These tools complete and return in the main process af- ter the daemon is spawned, and at some point in the future, the daemon would interrupt the main process, e.g. when the time is up. This kind of in- terruption poses a novel problem for both execution orchestration, and the model itself.\nWhile most of TOOLSANDBOX tools are self- contained implementations, some tools that depend on an external knowledge base, like searching for weather, are still backed by external web services,"}, {"title": "A Implementation Details", "content": "This appendix section introduces implementation details about the TOOLSANDBOX design."}, {"title": "A.1 Execution Context", "content": "Execution Context represents the complete state of the TOOLSANDBOX. More specifically, it con- tains tool databases for stateful tools, referred to as World State in Figure 1, and the dialog history between different roles, referred to as Message Bus. It maintains a snapshot of all tool databases and dialog history at any given turn, allowing for easy introspection and evaluation. The Execution Con- text exists as a global variable for all roles and tools to easily access, while prohibiting direct ma- nipulation from the LLM agent. This allows us to implement stateful tools that can manipulate or access database stored in the Execution Context, without defining it as function argument."}, {"title": "A.2 Tools", "content": "Tools are implemented as type-hinted, doc-string equipped Python functions, as shown in Listing 1. When a tool is passed to the Agent as an available tool, type hints and doc-string are converted into JSON API schema, as shown in Figure 9."}, {"title": "A.2.1 Tool Augmentations", "content": "To enable ablation studies of how the tool schema affects agent accuracy we have implemented multi- ple augmentations."}, {"title": "B Test Scenarios", "content": "B.1 Tool-use Benchmark Comparisons\nThe tool-use benchmark statistics shown in Table 3 are calculated as follows:\n\u2022 Average turn considers any message between the user, the agent or the tools as 1 turn.\n\u2022 For TOOLSANDBOX, statistics are calculated from trajectories collected on GPT-40 agent.\n\u2022 BFCL only evaluates tool call generation from a single user prompt, which we consider as 2 turns.\n\u2022 ToolEval was calculated from ToolLlama DFS Retriever trajectories.\nB.2 Annotation Process\nTest scenarios are created by 2 internal domain ex- perts from our institute who are familiar with the tool capacities in TOOLSANDBOX, and the field of task-oriented dialog. 1 annotator creates test sce- narios including the starting world state, user task, initial message, and milestone/minefield. To ensure dataset diversity while making the annotation task feasible, the annotator followed the process below:\n\u2022 The annotation process starts by creating seed scenarios. These are simple, single user turn, single tool call, self-contained requests that are supposed to cover most tools, as well as most of their arguments. For example, a seed scenario for add_reminder tool that requires timestamp, latitude, longitude would likely contain a starting user utterance saying Create a reminder to buy chocolate milk at timestamp 111222333 at latitude 37 longitude -122. Creating milestone for said scenario is trivial.\n\u2022 Next, starting from the seed scenario, the an- notator branches off to create derived scenar- ios that are more involved. This could be a Multiple Tool Call scenario, which requires the agent to invoke other tools before this one, e.g. Create a reminder to buy chocolate milk tomorrow 5PM at Whole Foods on McKinley Ave, which requires reasoning for the relative datetime, as well as searching for location lat- itude longitude. Note that this will also be considered a Canonicalization scenario.\n\u2022 It could be a Multiple User Turn scenario, which requires the agent to request more slots from the user. e.g. Create a reminder.\n\u2022 It could be a State Dependency scenario, which requires the model to solve state depen- dencies, e.g. Create a reminder to buy choco- late milk tomorrow 5PM at Whole Foods on McKinley Ave, but wifi is set to off. Preventing access to location search capability unless the dependency is resolved.\n\u2022 It could be an Insufficient Information sce- nario, which requires the model to figure out this task cannot be solved, e.g Create a re- minder to buy chocolate milk tomorrow 5PM"}, {"title": "B.3 Tool Design", "content": "Tool design choices in TOOLSANDBOX are driven by two major principles:\n\u2022 Tools should be representative and diverse, to cover key task oriented dialog use cases as well as TOOLSANDBOX test scenario cate- gories.\n\u2022 Tool capacities should be well-defined, and tool counts should be manageable, so that"}, {"title": "B.4 TOOLSANDBOX Category Statistics", "content": "The number of test scenarios per scenario category in TOOLSANDBOX can be found in Table 6"}, {"title": "C Example Trajectories", "content": "C.1 Tool Call Detection"}, {"title": "C.2 Single/Multiple Tool Call/User Turn", "content": ""}, {"title": "C.3 Canonicalization", "content": ""}, {"title": "C.4 State Dependency", "content": ""}, {"title": "C.5 Insufficient Information", "content": ""}, {"title": "D Additional Evaluation Results", "content": "D.1 Model Feature Comparison"}, {"title": "D.2 Turn Count Comparison", "content": ""}]}