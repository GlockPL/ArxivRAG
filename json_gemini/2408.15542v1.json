{"title": "Kangaroo: A Powerful Video-Language Model Supporting Long-context Video Input", "authors": ["Jiajun Liu", "Yibing Wang", "Hanghang Ma", "Xiaoping Wu", "Xiaoqi Ma", "Xiaoming Wei", "Jianbin Jiao", "Enhua Wu", "Jie Hu"], "abstract": "Rapid advancements have been made in extending Large Language Models (LLMs) to Large Multi-modal Models (LMMs). However, extending input modality of LLMs to video data remains a challenging endeavor, especially for long videos. Due to insufficient access to large-scale high-quality video data and the excessive compression of visual features, current methods exhibit limitations in effectively processing long videos. In this paper, we introduce Kangaroo, a powerful Video LMM aimed at addressing these challenges. Confronted with issue of inadequate training data, we develop a data curation system to build a large-scale dataset with high-quality annotations for vision-language pre-training and instruction tuning. In addition, we design a curriculum training pipeline with gradually increasing resolution and number of input frames to accommodate long videos. Evaluation results demonstrate that, with 8B parameters, Kangaroo achieves state-of-the-art performance across a variety of video understanding benchmarks while exhibiting competitive results on others. Particularly, on benchmarks specialized for long videos, Kangaroo excels some larger models with over 10B parameters and proprietary models.", "sections": [{"title": "I. INTRODUCTION", "content": "Leveraging the robust context processing and reasoning capability of Large Language Models (LLMs) [1], [2], [3], [4], [5], Large Multi-modal Models (LMMs) [6], [7], [8], [9], [10] have made substantial progress in vision-language tasks. As a critical medium of visual information, video plays an indispensable role in interacting with real-world scenarios. Pioneer studies [11], [12], [13], [14] begin to explore the extension of video inputs in LMMs. Despite demonstrating potential on certain benchmarks, existing methods still struggle with effectively processing long videos. We conduct an in-depth analyze of related research and identify two primary factors contributing to this issue. First, the scarcity of high-quality video datasets limits the efficacy of current approaches, leading to insufficient video-language alignment and sub-optimal performance in video-related benchmarks. Second, numerous methods, as exemplified by studies such as [15], [16], [17], [18], incorporate aggressive resampling modules to exceedingly reduce the number of visual tokens. This aggressive compression results in a substantial loss of critical visual information, further exacerbating the problem.\nIn this paper, we propose Kangaroo, a powerful Video LMM aimed at ameliorating these issues. To overcome the obstacle posed by the limited availability of video data, a high-quality dataset is established for vision-language pre-training and instruction tuning. Utilizing this curated dataset, we devise a curriculum training strategy that progressively endows the LLM basement with the capacity to comprehend long videos. Moreover, recognizing the significance of visual information, we increase the resolution and number of input frames to improve the perception of global context and visual details in long videos. The evaluation results across diverse benchmarks for video understanding verify the superior performance of our model. Specifically, our key contributions in this paper are summarized as follows:\n\u2022\tData Curation System: We develop a data curation system encompassing data collection, filtering and caption generation. Initially, we collect a substantial amount of image and video data from various sources. To ensure high-quality video pre-training, low-quality videos are excluded prior to the implementation of the video caption generation procedure. Additionally, we establish a video"}, {"title": "II. RELATED WORK", "content": "The extraordinary performance of LMMs has greatly pro-pelled the development of multi-modal models. Pioneer studies primarily concentrate on developing techniques for modality alignment. Kosmos-1 [19] implements an end-to-end frame-work that directly merges visual inputs into LLM in a unified training process. Flamingo [20], [21] integrates visual features with linguistic features in each layer of LLM through a cross attention mechanism. BLIP-2 [22] designs a Q-Former module to fuse visual and linguistic features via learnable queries. MiniGPT-4 [23] and LLaVA [7], [24] circumvent complex cross-modal fusion modules and by directly projecting visual features into the LLM embedding space through a lightweight multi-layer perceptron (MLP).\nSubsequent works aim to explore the application of LMMs in wider multi-modal tasks. Shikra [25], Kosmos-2 [26], MiniGPT-v2 [27] and CogVLM [10] integrate visual ground-ing tasks, significantly enhancing the spatial perception abil-\nity of LMMs. Moreover, mPlug-DocOwl [28] and Kosmos-2.5 [29] construct specialized datasets to optimize document understanding. Recent works aim to unify diverse tasks into a general model, such as the LLaVA-NeXT [30], InternLM-XComposer2 [31], InternVL [8] and Qwen-VL [9]. These general LMMs employ additional optimization policies, high-quality datasets covering multiple tasks and intricate train-ing strategies to advance performance across comprehensive vision-language tasks."}, {"title": "B. Large Multi-modal Models for Video Understanding", "content": "Advancements have been made to extend LMMs to video understanding tasks. To accommodate video inputs, video LMMs typically extract frames from the input video and rearrange the frame features encoded by the vision encoder to serve as the final video features. Some works [15], [16], [32] operate on the Q-Former module proposed by BLIP-2 to aggregate visual features and text features. On the other hand, methods including Video-LLaVA [11], Valley [33] and MiniGPT4-Video [23] adhere to the architecture of LLaVA, directly concatenating all frame features and projecting them through a simple fully connected layer.\nCertain research efforts investigate the use of spatial or temporal downsampling modules to compress visual tokens. For instance, Video-ChatGPT [12] employs a pooling module to downsample visual tokens along both temporal and spatial dimensions. Chat-Uivi [17] represents videos with a set of dymamic visual tokens to percieve high-level and low-level semantic details. Additionally, PLLaVA [13] introduces an adaptive pooling module for projected visual tokens to ac-commodate more video frames while minimizing the compu-tational load. Recent LMMs [34], [30], [35] expand to support multi-image inputs. This enhancement enables the models to leverage extensive existing image-related datasets to elevate video comprehension."}, {"title": "C. Long Video Understanding", "content": "The methods mentioned above impose restrictions on the number of input frames, presenting challenges for under-standing long videos. To handle hour-long videos, LLaMA-VID [18] adopts a context attention module to integrate multi-modal features and downsamples visual tokens into two tokens for each frame. MovieChat [36] designs a short-term and long-term memory module to consolidate local details and overall contents within the video. Similarly, TimeChat [37] introduces a timestamp-aware frame encoder to bind visual content with temporal information and uses a sliding Q-Former to integrate video tokens. Although these methods intend to enhance the perception of both local and global information in long videos, the excessive compression of visual tokens results in suboptimal results.\nIn contrast, some research adopts a stepwise strategy to incrementally adapt to long video inputs. LWM [38] gradually extends the context size from 4K to 1M tokens by optimizing the implementation of Blockwise RingAttention [39] and other features for millions-length sequence training. LongVA [40] attempts to leverage the long context generalization capability of LLMs for extremely long videos. The multi-modal model is trained based on an LLM initially generalized to millions of tokens. Training on long-context sequences ensures the entirety of visual information. However, the limited access to high-quality video data remains an obstacle to achieving ideal performance.\nUp to date, long video understanding remains a challenging task. Our proposed method aims to preserve the fidelity of the original visual information and elevate performance in long video understanding tasks by large-scale video-text datasets within a curriculum learning framework."}, {"title": "III. DATA CURATION", "content": "In this section, we provide a detailed exposition of the data curation system. We start with the introduction of the image re-captioning procedure. Next, we elaborate on detailed steps of the video preprocessing and caption generation pipeline, setting the stage for robust video-text pre-training. In the last part, we demonstrate the composition of the multi-task instruction tuning dataset in detail. The schematic of our data curation process is depicted in Figure 3."}, {"title": "A. Image Re-Captioning", "content": "Images form the foundation for bridging vision and lan-guage modalities. However, the captions accompanying images in existing public datasets are often noisy and unsuitable for pre-training. To address this issue, we implement a re-captioning procedure to create a high-quality dataset for image pre-training. Specifically, we select images from LAION-5B-en [41] for English data and Wukong [42] for Chinese data. The original captions associated with these images are discarded due to low quality. Instead, we employ an open-source multi-modal model to generate high-quality captions. Following a detailed evaluation of several off-the-shelf models considering factors including completeness, diversity, halluci-nation, image-text relevance and response speed, we ultimately opt for InternLM-XComposer2 [31] as the image captioner. The prompt is crafted to limit the generated English and Chinese captions within a preset length."}, {"title": "B. Video Preprocessing", "content": "To build a high-quality dataset for video pre-training, we gather videos from Webvid [43], Panda-70M [44] with En-glish captions and Youku-mplug [45], ChinaOpen [46] with Chinese captions. In addition, a number of internal videos are incorporated to enrich the diversity of video data. However, subsequent examination of the large amount of video data reveals the low quality of certain videos. To mitigate the degradation of model performance caused by the inclusion of these videos, we propose several video filtering mechanisms, as outlined below:\n1) Text Coverage Filter: A common indicator of low-quality videos is the occlusion by large areas of irrelevant text elements, complicating the task of describing the video content accurately. Thus, we extract three frames from each video and employ PaddleOCR [47] to detect text regions in"}, {"title": "C. Video Caption Generation", "content": "Existing video LMMs are deficient in description accuracy, richness and diversity. Here we designed a comprehensive video captioning pipeline to address these shortcomings. Firstly, we evenly divide each video into five segments and randomly sample one frame from each segment. Next, a multi-modal model is employed to generate caption for the extracted frames individually. Following that, an off-the-shelf LLM is leveraged to synthesize these frame-level captions into a cohesive video-level caption. In practice, we compare the results of several open-source LMMs and ultimately take InternLM-XComposer2 [31] as the frame-level captioner. To avoid introducing bias by relying on a single model, we randomly select either InternLM2-Chat-20B [4] or Baichuan2-13B-Chat [5] to serve as the caption summarizer. Specialized prompts are designed to instruct the LLM to generate high-quality holistic descriptions. Upon completion of the video preprocessing and caption generation procedures, we establish"}, {"title": "D. Video Caption Refinement", "content": "Despite the specialized design of summary prompt, we discover that some generated video captions do not meet our expectations. In certain instances, the LLM tends to neglect the instruction from the summary prompt and simply concatenates individual frame captions without semantically restructuring. In addition, in some cases, the captions of adjacent frames from a video are highly correlated, resulting in repetitive sentences in the LLM outputs. These captions contain semantic redundancy and fail to present informative description of the entire video.\nTo mitigate the impact of repetitive contents, we curate a subset of 15M high-quality data from the 60M video-text dataset to refine video pre-training. We propose a metric to measure the similarity between sentences in a caption. For a video caption with N sentences, we use jieba [51] for Chinese word segmentation and split English sentence into words. Let $s_i$ represent the word set of the i-th sentence in the caption. For each pair {$s_i$, $s_j$} of sentences, the similarity between these two sentences is quantified by the Intersection over Union (IoU) metric, defined as:\n$S_{ij} = \\frac{|s_i \\cap s_j|}{|s_i \\cup s_j|}$    (2)\nIn this way, the sentence similarity of the entire video caption is denoted as the maximum similarity of all sentence pairs in the caption. To avoid semantic redundancy in the video captions, we remove data with sentence similarity exceeding the predefined threshold. The filtered video-caption data, along with dense video caption data from ShareGPTVideo [52] is used for pre-training refinement."}, {"title": "E. Instruction Tuning Dataset", "content": "Instruction-following data improves the model's general-ization ability to various tasks [53]. To enable the model's versatility to different tasks, we compile a large-scale video instruction tuning dataset comprising 2.24M samples from distinct public and internal sources. Detailed composition of the dataset is reported in Table I. The curated instruction tuning dataset covers a wide range of video understanding tasks and can be divided into the following categories:\n1) Video Caption: including short video caption and de-tailed video description data in both English and Chinese. Videos with short captions are collected from VaTEX [54], YouCook2 [55], Charades [56], TextVR [57] and detailed description data from ShareGPTVideo, ShareGPT-40 [58], ShareGPT4Video [59]. Each video from VaTEX and Cha-rades corresponds to multiple short captions. Consequently, we leverage GPT-4 [60] with custom designed prompts to summarize all short captions into one overall caption. In addition, for Chinese understanding, a subset of Chinese detailed description data from the video pre-training dataset is incorporated in this stage. We formulate corresponding instructions for each type of caption to convert captions into QA pairs.\n2) Video QA: including multi-choice VQA and open-ended VQA data collected from various sources. This subset en-compasses fundamental VQA: NExT-QA [61], TGIF-QA [62], WebvidVQA [63], egocentric VQA: Ego-4d [64], reason-ing VQA: CLEVRER [65], action recognition: Something-Something-V2 [66], [67], Kinetics-710 [68] and VQA with long videos from TV-QA [69], Cinepile [70], MovieChat. For WebvidVQA, Ego-4d and action classification subsets, we adopt the partition and instructions from MVBench [16]. We additionally design specialized templates for multi-choice VQA data to combine each question and the corresponding options.\n3) Conversation: including single/multi-round conversation in both Chinese and English. We include ShareGPTVideo, Video-ChatGPT and integrate single-round conversations from VideoChat, VideoChat2 and Valley into multi-round format. To improve the Chinese comprehension ability of our model, we select a subset from the video pre-training dataset and and utilize GPT-4 to generate human-like conversations based on the high-quality Chinese video description.\nAfter collection and organization, we perform a statistical"}, {"title": "IV. METHODOLOGY", "content": "In this section, we present the overall architecture of our Kangaroo model and systematically elaborate on our curricu-lum training strategy."}, {"title": "A. Architecture", "content": "We utilize a widely adopted multi-modal architecture that bridges visual and linguistic features through a lightweight lin-ear projector. As depicted in Figure 2, Kangaroo is composed of a vision encoder, a multi-modal projector, a spatial-temporal patchify module and an LLM.\nWe initialize the vision encoder from EVA-CLIP-L [71] and choose Llama-3-8B-Instruct [2] as our base LLM. For each input video $V \\in R^{T\\times3\\timesH\\timesW}$, we start with uniformly sampling a series of n frames {$f_0$, $f_1$, $f_2$,..., $f_n$}. Inspired by position embedding proposed in [72], we design a temporal position embedding (TPE) to encode the temporal order of extracted frames in the video, implemented by sinusoidal position encoding:\n$TPE(t) = \\begin{pmatrix} \\sin(\\frac{t}{d}) \\\\ \\cos(\\frac{t}{d}) \\\\ \\sin(\\frac{t + (d/2)}{d}) \\\\ \\cos(\\frac{t + (d/2)}{d}) \\end{pmatrix}$  (3)\n$\\hat{Z} = Z + TPE(t)$  (4)\nwhere t is the timestamp of each frame and $\\hat{Z}$ denotes the visual feature augmented with temporal information. Here we use the actual float-type timestamp of a frame instead of its index to incorporate the video's meta information.\nUltimately, we concatenate visual features of all frames along the time dimension with special tokens interleaved among the sequence to model the temporal inter-dependencies.\nThe resulting visual sequence is then projected into LLM embedding space via the multi-modal projector as the final output of the visual branch, denoted as $Z_v \\in R^{(h\\times w\\times n) \\times c}$.\n$Z_v = Projector(\\hat{Z}_0 \\oplus ... \\oplus \\hat{Z}_n)$  (5)\nwhere $\\oplus$ stands for the concatenate operation."}, {"title": "B. Curriculum Training", "content": "As shown in Figure 6, we devise a curriculum training framework with progressively increasing task complexity and training difficulty to incrementally equips a text-based LLM with the capability to process long video inputs.\nStage I: Visual-language Pre-training. We start with im-age/video pre-training to connect fundamental language con-cepts and visual elements. First, we utilize the re-captioned image dataset for image pre-training. Each image is repre-sented as a single-frame video and is randomly assigned a timestamp for temporal position embedding. We then extend to video pre-training based on the curated large-scale video-caption dataset. For each input video, we uniformly extract 8 frames and calculate the respective timestamps. During this stage, the visual encoder and multi-modal projector are trained via back-propagation, guided by the subsequent frozen LLM.\nStage II: Pre-training Refinement. Due to the inconsistent quality of the initial video captions, we introduce an additional pre-training refinement stage. This stage aims to mitigate the effects of the fixed format and repetitive content present in the video pre-training dataset. To align with higher-quality data, the number and resolution of input frames are simultaneously increased to 16 \u00d7 448 \u00d7 448. Furthermore, we implement a spatial-temporal patchify module using a 3D depthwise convolution operation to condense visual tokens, ensuring that the number of tokens fed into the LLM remains unchanged.\nStage III: Instruction Tuning. After the preliminary align-ment of visual and linguistic features, we fine-tune the entire model to enhance the instruction-following abilities across various tasks with the subsequent strategies:"}, {"title": "V. EXPERIMENTS", "content": "For all training stages, we use AdamW optimizer with the weight cay of 0.05 and cosine learning rate schedule. Layer-wise Learning Rate Decay strategy is applied for the vision encoder with the decay factor of 0.9. We only calculate cross-entropy loss for autoregressive text generation. Other detailed training settings are reported in Table II."}, {"title": "VI. CONCLUSION", "content": "In this paper, we present Kangaroo, a powerful Large Multi-modal Model that excels at long video understanding. We develop a data curation system and establish a large-scale video-based dataset. To improve the capacity for processing long videos, we adopt high-resolution inputs and extend the"}]}