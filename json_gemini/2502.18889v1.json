{"title": "CLIP-TTS: CONTRASTIVE TEXT-CONTENT AND MEL-SPECTROGRAM, A HIGH-HUALITY TEXT-TO-SPEECH METHOD BASED ON CONTEXTUAL SEMANTIC UNDERSTANDING", "authors": ["TIANYUN LIU"], "abstract": "Traditional text-to-speech (TTS) methods primarily focus on establishing a mapping between phonemes and mel-spectrograms. However, during the phoneme encoding stage, there is often a lack of real mel-spectrogram auxiliary information, which results in the encoding process lacking true semantic understanding. At the same time, traditional TTS systems often struggle to balance the inference speed of the model with the quality of the synthesized speech. Methods that generate high-quality synthesized speech tend to have slower inference speeds, while faster inference methods often sacrifice speech quality. In this paper, I propose Clip-TTS, a TTS method based on the Clip architecture. This method uses the Clip framework to establish a connection between text content and real mel-spectrograms during the text encoding stage, enabling the text encoder to directly learn the true semantics of the global context, thereby ensuring the quality of the synthesized speech. In terms of model architecture, I adopt the basic structure of Transformer, which allows Clip-TTS to achieve fast inference speeds. Experimental results show that on the LJSpeech and Baker datasets, the speech generated by Clip-TTS achieves state-of-the-art MOS scores, and it also performs excellently on multi-emotion datasets. Audio samples are available at: https://ltydd1314.github.io/.", "sections": [{"title": "1. INTRODUCTION", "content": "Speech synthesis has seen remarkable advancements in recent years, particularly with the emergence of deep learning-based TTS models. However, traditional TTS systems primarily rely on linguistic features extracted from text input, often lacking an in-depth understanding of the semantic and contextual information that influences human speech. To address these limitations, Clip-TTS introduces a novel multimodal approach by integrating Contrastive Language-Image Pretraining (Clip) [1] with text-to-speech synthesis, enabling more expressive and context-aware speech generation.\nUnlike traditional TTS models that generate speech based solely on text and phonetic information, Clip-TTS benefits from rich semantic representations, enabling it to generate speech that aligns more closely with context and human speech. By training the latent relationship between the text content and Mel spectrograms, Clip-TTS allows the text encoder to learn more information from the raw Mel spectrograms. By combining self-supervised learning and contrastive training, Clip-TTS enhances its ability to generalize across different speech styles and contexts, improving both the naturalness and adaptability of the generated speech.\nAnother key advantage of Clip-TTS is its ability to control expressiveness without requiring explicit labels or large amounts of annotated data. Traditional methods often rely on manually"}, {"title": "2. RELATED WORK", "content": "WaveNet: The WaveNet series is a collection of deep learning-based speech generation models proposed by Google. These models significantly enhance the quality and naturalness of speech synthesis by directly generating waveform data. The WaveNet model is not only groundbreaking in the field of speech synthesis but has also influenced multiple related domains, such as music synthesis.\nWaveNet [2] is a generative model based on causal convolution that directly generates audio waveforms in the time domain. By modeling the conditional probability distribution of each sample point, WaveNet can generate speech with high fidelity and naturalness. WaveNet significantly improves the naturalness and quality of synthesized speech, with generated speech nearly reaching human-level quality. However, its generation speed is relatively slow, as it produces one sample point at a time, resulting in high computational costs and long inference times. To address the slow inference speed of WaveNet, Parallel WaveNet[3] accelerates the generation process by introducing a flow-based model. It employs a Teacher-Student Framework for training, enabling the parallel network to approximate the output distribution of WaveNet. During the generation phase, an inverse autoregressive process is used to generate waveforms in parallel, improving generation efficiency. Parallel WaveNet significantly enhances inference speed, making it capable of meeting real-time speech synthesis requirements. WaveRNN[4] further optimizes WaveNet's computational efficiency for real-time applications. By replacing the convolutional network with a recurrent neural network (RNN) [5], it significantly reduces the number of model parameters. The advantage of this approach is a smaller model size and faster generation speed, making it suitable for resource-constrained scenarios."}, {"title": "3. CLIP AND ITS APPLICATIONS", "content": "Clip [1] is a multimodal neural network proposed by OpenAI, capable of understanding the relationship between images and text, and is used for zero-shot classification, image retrieval, and other tasks. Clip utilizes contrastive learning for joint image-text training. Its model"}, {"title": "4. METHOD", "content": "Clip learns the matching relationships between images and their corresponding descriptive text through contrastive learning, enabling support for various downstream tasks. My inspiration is based on the idea of Clip, adopting a similar contrastive learning approach to learn the latent mapping between Mel spectrograms and corresponding text content. This allows us to train a foundational model, which I call Speech-Clip. Theoretically, this model can also support many downstream tasks. This paper focuses on the TTS branch, while more potential branches will be discussed in Section 7.\nFigure 1 shows the basic framework of our Speech-Clip, which consists of a text encoder and a Mel encoder. The text encoder takes text information as input and encodes it, while the Mel encoder receives the Mel spectrogram as input. Finally, I calculate the cosine similarity between the outputs of the two encoders. For matching text content and Mel spectrogram, I hope the cosine similarity approaches 1, and for non-matching pairs, it should approach 0."}, {"title": "5. EXPERIMENT AND RESULTS", "content": "Datasets: I selected two datasets for both Chinese and English, including single-speaker and multi-speaker datasets, making a total of four datasets: Baker [30], AISHELL3 [31], LJSpeech [32], and LibriTTS [33]. Based on these four datasets, I trained four models. Baker and LJSpeech are datasets recorded by a single female speaker, with 10,000 samples (about 12 hours) and 13,100 samples (about 24 hours), respectively. They are characterized by high quality, standard and clear pronunciation, wide phoneme coverage, and abundant labels, but their drawback is that they cannot meet the multi-speaker requirement. AISHELL3 and LibriTTS contain 218 speakers (about 85 hours) and 2,456 speakers (585 hours), respectively. These datasets have a large volume of data, wide phoneme coverage, and can meet the multi-speaker requirement, but they suffer from issues such as non-standard pronunciation and poor speech quality. In addition, I also experimented with a multi-emotion speech dataset, the Emotional Speech Dataset (ESD) [34]. This dataset contains ten speakers for both Chinese and English, each expressing five emotions (anger, happy, neutral, sad, and surprise), with each speaker recording 350 samples for each emotion. Therefore, I divided this dataset by language and packaged it into the Baker and LJSpeech datasets, forming a bilingual dataset with five different emotional categories. I used this dataset to train two models, one for Chinese and one for English.\nModel Structure: My Clip-TTS consists of 4 feed-forward Transformer (FFT) blocks in the text encoder, the Mel encoder and the Mel decoder. In each FFT block, the dimension"}, {"title": "6. FUTURE RESEARCH DIRECTIONS AND DEVELOPMENT POTENTIAL", "content": "The success of Clip-TTS proves the effectiveness of the Clip architecture in the field of speech synthesis. Currently, I have completed the development of a basic TTS system, but the potential of Clip-TTS and Speech-Clip goes beyond that. In this section, I will focus on discussing the further development of Clip-TTS and the potential of Speech-Clip."}, {"title": "7. CONCLUSION", "content": "In this paper, I first propose a basic Speech-Clip model, whose architecture is inspired by the Clip framework. I use contrastive learning to train the model with the aim of explicitly learning the relationship between Mel spectrograms and the corresponding text content, enabling the model to support more downstream tasks. Next, by retaining the text encoder and adding a Mel decoder, we form a basic TTS system, which I call Clip-TTS. Experimental results show that my method outperforms other approaches on the LJSpeech and AISHELL3 datasets, achieving state-of-the-art performance on the Baker dataset, and also demonstrating excellent results on multi-emotion datasets. Finally, I introduce and discuss the future development directions of Clip-TTS, as well as the potential downstream tasks that Speech-Clip could support, and outline the focus of future work."}]}