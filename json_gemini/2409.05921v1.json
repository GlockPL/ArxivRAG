{"title": "A Spatial-Temporal Large Language Model with Diffusion (STLLM-DF) for Enhanced Multi-Mode Traffic System Forecasting", "authors": ["Zhiqi Shao", "Haoning Xi", "Haohui Lu", "Ze Wang", "Michael G.H. Bell", "Junbin Gao"], "abstract": "The rapid advancement of Intelligent Transportation Systems (ITS) presents challenges, particularly with missing data in multi-modal transportation data and the complexity of handling diverse sequential tasks within a centralized framework. To address these issues, we propose the Spatial-Temporal Large Language Model Diffusion (STLLM-DF), an innovative model that leverages Denoising Diffusion Probabilistic Models (DDPMs) and Large Language Models (LLMs) to improve multi-task transportation prediction. The DDPM's strong denoising capabilities allow it to recover underlying data patterns from noisy inputs, making it particularly effective in complex transportation systems. Meanwhile, the non-pretrained LLM adapts dynamically to spatial-temporal relationships within multi-modal networks, allowing the system to efficiently manage diverse transportation tasks in both long-term and short-term predictions. Extensive experiments demonstrate that STLLM-DF consistently outperforms existing models, achieving an average reduction of 2.40% in MAE, 4.50% in RMSE, and 1.51% in MAPE. This model represents a significant advancement in centralized ITS, enhancing predictive accuracy, robustness, and overall system performance across multiple tasks. Enhancing Spatio-Temporal Traffic Forecasting with Frozen Transformer Language Models and Diffusion Techniques", "sections": [{"title": "1. Introduction", "content": "Intelligent Transportation Systems (ITS) are pivotal in enhancing modern transportation networks' efficiency, safety, and sustainability, overseeing critical functions like incident management, traffic flow optimization, and demand-responsive transport service management. However, these sophisticated systems are increasingly challenged by\n\u2022 Missing Values in Data: The vast datasets employed by ITS often contain inherent missing values, or what we call noise. This noise can stem from sensor malfunctions, incomplete data transmission, or even environmental factors like weather. The presence of such noise severely hampers the generalization capabilities of traditional models, leading to inaccurate traffic predictions and suboptimal decision-making. Addressing data quality is, therefore, fundamental to improving predictive accuracy and the robustness of ITS Wang et al. (2019); Zhang et al. (2023a).\n\u2022 Sequential Multi-Task Data Handling: ITS relies on various data streams from multiple modes of transportation, including buses, taxis, and metro systems. Each of these data sources has its own unique temporal and spatial characteristics, creating a highly complex dataset that needs to be processed simultaneously. Integrating and managing these diverse datasets is challenging, particularly when missing value is present. Existing models often struggle to maintain accuracy and reliability across different tasks when handling such heterogeneous data streams Zhang et al. (2023b); Zannat and Choudhury (2019).\nThe above challenges highlight the need for a more robust approach to processing and utilizing transportation data. A model that can centralize the management of these diverse tasks while employing advanced noise reduction (missing value handling) techniques is critical to preserving data integrity and maximizing utility, ultimately improving the system's overall effectiveness and reliability. As shown in Figure 1 such a model would need to be capable of handling the complexity and scale of ITS tasks in real-time environments.\nTo address these challenges, this paper introduces the Spatial-Temporal Large Language Model Diffusion (STLLM-DF), a novel approach that combines Denoising Diffusion Probabilistic Models (DDPMs) and Large Language Models (LLMs). DDPMs are generative models based on stochastic processes, simulating the diffusion of data from order to disorder and then reversing the process to restore the original data, effectively achieving advanced data recovery. LLMs are leveraged for high-level feature extraction, enabling the STLLM-DF model to eliminate the need for manual feature engineering or prompting mechanisms. This streamlined approach adapts effortlessly to large and complex datasets, significantly improving the accuracy of Intelligent Transportation System (ITS) operations.\nThis innovation marks the first application of such a model in ITS, designed to tackle multiple tasks concurrently. By utilizing advanced data recovery techniques, the STLLM-DF effectively mitigates the impact of noise on data quality, ensuring that accurate and reliable traffic information is delivered to support real-time decision-making. Additionally, using LLMs for feature extraction allows for a better understanding of transportation networks' spatial and temporal dynamics, enabling the model to respond to a wider array of transportation demands. By incorporating these advanced data processing techniques and a deep understanding of ITS operational demands, our method offers a revolutionary solution that promises to significantly enhance system performance."}, {"title": "1.1. Related Works", "content": "In the recent studies have extensively applied machine learning and deep learning to predict various tasks to assist the intelligent transportation systems, including traffic flow Shao et al. (2024b,c,a), traffic speed Zou et al. (2023); Rempe et al. (2022), taxi demand Kim et al. (2020), and accidents, as well as city bike traffic Li et al. (2023). Despite these advancements, developing models that perform uniformly well across these diverse tasks remains limited. This is primarily due to challenges associated with handling the variety of data types inherent in comprehensive traffic"}, {"title": "1.1.1. Denoising Diffusion Probabilistic Models (DDPMs)", "content": "The diffusion model employed in this study is a generative model based on stochastic processes. It simulates the diffusion process of data from order to disorder and then reverses this process to restore the original data, thereby achieving data recovery. Compared to traditional denoising or data recovery techniques, the diffusion model has the advantage of powerful data recovery capabilities, especially when dealing with data with complex dependencies. The DDPMs have been widely used in generative tasks in different fields such as computer vision, medicine, etc. Recently, DDPMs have started to be used in traffic fields such as traffic data sampling Dupuis et al. (2024), traffic anomaly detection Li et al. (2024), and traffic matrix estimation Yuan et al. (2023). However, none of them are used to conduct a unified framework for adopting multi-task prediction in ITS.\nOur proposed STLLM-DF, which combines spatial temporal features, offers a new theoretical perspective and practical approach to processing traffic data. The training objective is to minimize the difference between the noise predicted by the model and the actual noise added. In this way, the model learns how to accurately recover the original data from the high missing ratio data, i.e., perform data recovery. This training method enables the model to generate or restore high-quality data, particularly when given inputs that are partially damaged or interfered with by noise."}, {"title": "1.1.2. Large Language Model (LLM)", "content": "Large Language Models (LLMs) are increasingly utilized in the field of traffic management for diverse tasks such as multimodal traffic accident forecasting de Zarzo et al. (2023), traffic prediction Liu et al. (2024), and signal control Pang et al. (2024). Traditionally, these applications rely on specific prompts as inputs to guide the model's performance, which can complicate the framework and increase computational demands. Given these complexities, this paper critically examines whether LLMs can be effectively employed without prompt-based inputs while still maintaining robust performance. This investigation is pivotal for streamlining traffic management applications, potentially reducing computational overhead and simplifying the operational framework."}, {"title": "1.2. Contribution", "content": "We have identified a gap in existing models' ability to manage multiple tasks within a unified framework due to varying data quality. To address this, we propose the STLLM-DF framework, which incorporates Denoising Diffusion Probabilistic Models (DDPMs) for data denosing, followed by the use of a Large Language Model (LLM) to extract key traffic information. To the best of our knowledge, this innovative approach uniquely combines these technologies to enhance the processing and utility of traffic data across different tasks.\n1. We are the pioneers in merging the capabilities of DDPM with large language models to establish a groundbreaking model for a centralized ITS. This innovative combination leverages the strengths of both technologies to tackle complex traffic management challenges.\n2. The DDPM is effective for multi-task transportation prediction due to its strong denoising capabilities and ability to recover underlying data patterns. Our research takes a novel approach by utilizing DDPMs with time-step embedding named ST-Denoisng block, which not only can capture the dynamic changes of traffic data over time but also can do data recovery process, while the use of convolutional networks effectively captures the spatial characteristics of the data. This application significantly enhances data quality across various ITS tasks, making it a first in the field.\n3. For a centralized transportation system, the non-pretrained LLM demonstrates adaptability to varying spatial-temporal relationships. Despite the lack of pretraining, the model can dynamically capture and learn from the changing spatial and temporal patterns present in multi-modal transportation networks, making it highly versatile in these environments.\n4. Extensive experiments across multiple tasks have demonstrated that STLLM-DF consistently outperforms other models. By effectively leveraging the strengths of diffusion mechanisms and LLMS, STLLM-DF excels in handling the complexities of multi-task transportation forecasting, leading to superior accuracy and robustness compared to other approaches.\nThe organization of this paper is as follows: Section 2 introduces the preliminaries, providing foundational concepts and essential background on Denoising diffusion probabilistic models and the Large Language model used in this study. Section 3 presents the model architecture and methodology, detailing the design of the STLLM-DF model. Section 4 outlines the experimental setup, including datasets, evaluation metrics, and implementation details; the vast model performance analysis is also included in this section. Section 5 provides an insight into the results, highlighting the model's performance and contributions to centralized ITS. Finally, the conclusion is presented in Section 6."}, {"title": "2. Preliminary", "content": "In this section, we briefly introduce the essential notations in Section 2.1, the fundamental of Denoising Diffusion Probabilistic Models (DDPM) in Section 2.2 and the Large Language Model (Llama 2) in Section 2.3."}, {"title": "2.1. Notations and Definition of Problems", "content": "Traffic Data. For traffic system data, at any timestamp t, it is represented by $X_t \\in \\mathbb{R}^{N\\times d}$, capturing flow features across N nodes with d denoting the feature dimensions. Over a duration T > 0, traffic data consolidates into a rank-3 tensor $X \\in \\mathbb{R}^{T\\times N\\times d}$, structuring the temporal, spatial, and feature-specific dimensions of flow.\nProblem Statement. The goal of traffic flow forecasting is to accurately predict future traffic conditions based on historical data. Consider the traffic flow tensor X observed in a road network G. We seek to design a function f that utilizes the traffic flow information from the past m timestamps to predict the traffic flow for the next n timestamps. This can be formally expressed as:\n$f ([X_{t-m+1},...,X_t]) \\rightarrow [X_{t+1},..., X_{t+n}]$\nfor any chosen starting point t, where $X_t$ represents the traffic flow tensor at time t."}, {"title": "2.2. Denoising Diffusion Probabilistic Models (DDPM)", "content": "Denoising Diffusion Probabilistic Models (DDPMs) represent a novel class of generative models that have shown remarkable proficiency in synthesizing high-fidelity data. These models operationalize a two-phase process: a forward diffusion process that incrementally corrupts the data, followed by a reverse process that aims to reconstruct the original data from the noise.\nForward Diffusion Process. The forward diffusion process is conceptualized as a Markov chain, where noise is progressively added to the data over a series of discrete time steps. Formally, this process is defined as:\n$X_t = \\sqrt{\\alpha_t}X_{t-1} + \\sqrt{1 - \\alpha_t}\\epsilon_t, \\epsilon_t \\sim N(0, I),$ (1)\nwhere $X_{t-1}$ denotes the original data, $\\alpha_t$ are pre-defined variance schedules, and $\\epsilon_t$ represents isotropic Gaussian noise. The diffusion process effectively transitions the data distribution towards a Gaussian distribution.\nReverse Diffusion Process. In contrast, the reverse diffusion process is tasked with learning to invert the forward diffusion. It incrementally denoises the data, guided by a parameterized model $\\epsilon_\\theta$. This reverse process is mathematically represented as:\n$X_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\Big(X_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta(X_t, t)\\Big) + \\delta_z,$\nwhere the function $\\epsilon_\\theta(X_t, t)$ approximates the noise added at each diffusion step."}, {"title": "Objective Function.", "content": "The primary objective in training DDPMs is to optimize the parameter $\\theta$ such that the model accurately predicts the noise added at each step. This is typically achieved via a mean squared error loss function, defined as:\n$L(\\theta) = E_{t,x_0,\\epsilon_t} [|| \\epsilon_t - \\epsilon_\\theta(X_t, t)||^2] .$ (3)\nThe optimization of this objective function enables the model to generate new samples from the learned data distribution, effectively reversing the diffusion process."}, {"title": "2.3. Large Language Models", "content": "Information Filtering Hypothesis. The pre-trained LLM, specifically a transformer-based architecture, operates as an effective \u201cfilter\" in identifying and emphasizing informative tokens. It enhances the significance of these tokens through increased magnitudes or frequencies within the feature activation landscape. This selective amplification facilitates more accurate predictions by foregrounding salient information and effectively streamlining the input data. In this paper, we use Llama 2 (Touvron et al., 2023), which is built upon the robust Transformer architecture, renowned for its efficacy in handling sequential data through self-attention mechanisms. The core components of Llama 2 include multi-head attention, position-wise feed-forward networks, layer normalization, and residual connections.\nMulti-Head Attention Mechanism. Llama 2 employs multi-head attention to capture different aspects of the input sequence. This mechanism allows the model to focus on various parts of the text simultaneously by using multiple sets of queries, keys, and values. The attention function is defined as:\n$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V,$\nwhereQ (queries), K (keys), and V (values) are matrices derived from the input data, and $d_k$ is the dimension of the keys. The multi-head attention mechanism is formulated as:\n$MultiHead(Q, K, V) = Concat(head_1,...,head_h)W^O$\n$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V),$\nwith $W_i^Q, W_i^K, W_i^V$, and $W^O$ being learned projection matrices.\nPosition-Wise Feed-Forward Networks. Each layer in Llama 2 includes position-wise feed-forward networks that apply two linear transformations with a ReLU activation in between. This is represented as:\n$FFN(x) = max(0, xW_1 + b_1)W_2 + b_2,$\nwhere $W_1$ and $W_2$ are weight matrices, and $b_1$ and $b_2$ are bias vectors.\nLayer Normalization and Residual Connections. Llama 2 incorporates layer normalization and residual connections to stabilize training and improve convergence speed. Layer normalization is defined as:\n$LayerNorm(x) = \\frac{x - E[x]}{\\sqrt{Var[x] + \\epsilon}} \\cdot \\gamma + \\beta,$\nwhere $\\gamma$ and $\\beta$ are learnable parameters. Residual connections help in gradient flow during back-propagation:\n$Output = LayerNorm(x + Sublayer(x))$"}, {"title": "3. Method", "content": "In this section, we are going to introduce the proposed model in Section 3.2, 3.3, and 3.4. The proposed model as in Figure 2 integrates various components for effective traffic system forecasting. At its core, the model employs an embedding layer to generate a hidden representation enriched by feature embedding, periodicity embedding, and spatial temporal adaptive embedding. This is followed by the frozen Denoising Diffusion Probalistic Model (DDPM) for initial data recovery, and a trainable layer extracted from Large Language Models (LLMs) serves as an information filter. Then, passing to a fully connected layer tasked with the final traffic prediction."}, {"title": "3.1. Data Pre-processing", "content": "The data resource for New York City is https://opendata.cityofnewyork.us/. To feed the data into our model, our data processing pipeline was designed to prepare and structure the NYC-Bike, NYC-Bus, NYC-Metro, and NYC-Taxi datasets for time series forecasting. The raw data, originally stored in a pickle file, was first loaded and extracted to obtain the TrafficNode information. We then expanded the dimensionality of the data to accommodate the model's input requirements. Using a custom DataProcessor class, we implemented a sliding window approach to generate input-output pairs, with both input and output windows set to 12 timesteps. This allowed the model to use the past 12 timesteps to predict the next 12 steps. The dataset was then split into training, validation, and test sets with ratios of 70%, 10%, and 20% respectively. To ensure reproducibility and efficient data loading in subsequent runs, we implemented a caching mechanism that saves the processed datasets as compressed NumPy arrays. This preprocessing approach not only structured the data for our forecasting model but also optimized it for efficient training and evaluation processes."}, {"title": "3.2. Data Embedding Layer", "content": "We utilized the adoptive data embedding layer as in Liu et al. (2023a). To preserve the original data's intrinsic information, we apply a fully connected layer to derive the feature embedding $X_f \\in \\mathbb{R}^{T\\times N\\times d_f}$:\n$X_f = FC(X_{t-m+1:t})$ (10)\nwhere $d_f$ represents the dimensionality of the feature embedding, and FC(\u00b7) denotes a fully connected layer. We introduce the learnable day-of-week embedding dictionary as $T_w \\in \\mathbb{R}^{N_w\\times d_f}$, and the time-of-day embedding dictionary as $T_d \\in \\mathbb{R}^{N_d\\times d_f}$, where $N_w = 7$ indicates the week's seven days, and $N_d = 288$ signifies the day's 288 timestamps. Letting $W_t \\in \\mathbb{R}^T$ and $D_t \\in \\mathbb{R}^T$ represent the day-of-week and time-of-day data for the traffic series from $t - m + 1$ to t, we use these as indices to retrieve the respective day-of-week embedding $X_{tw} \\in \\mathbb{R}^{T\\times d_f}$ and time-of-day embedding $X_{td} \\in \\mathbb{R}^{T\\times d_f}$. By concatenating and broadcasting these embeddings, we obtain the periodicity embedding $X_p \\in \\mathbb{R}^{T\\times N\\times 2d_f}$ for the traffic series.\nIt's understood that temporal relationships in traffic data are influenced not only by periodicity but also by chronological sequence, where a timeframe should be more similar to its adjacent frames. Moreover, traffic series from different sensors exhibit unique temporal patterns. Thus, instead of using a predefined or dynamic adjacency matrix for spatial relation modeling, we propose a spatio-temporal adaptive embedding $X_q \\in \\mathbb{R}^{T\\times N\\times d_a}$ to uniformly capture complex spatio-temporal relationships, shared across different traffic series.\nCombining the above embeddings yields the hidden spatio-temporal representation $Z \\in \\mathbb{R}^{T\\times N\\times d_h}$ as:\n$X_t = X_f || X_p || X_q$"}, {"title": "3.3. Frozen Denoising Block", "content": "Denoising diffusion probabilistic models (DDPM) are commonly used as generative models. In our research, we leverage the data generative capabilities of DDPM to recover the missing traffic data. We have developed a data recovery block using DDPM, explained in Section 2.2, to effectively remove noise. Compared to traditional denoising/data recovery techniques, the advantage of the diffusion model lies in its strong data recovery capabilities, especially when dealing with data that has complex dependencies. By incorporating time-step embedding, the model can capture the dynamic changes in traffic data over time while using convolutional networks to effectively capture the spatial features of the data. This denoising method, which combines spatio-temporal characteristics, provides a new theoretical perspective and practical approach for processing traffic data. This research extends the theoretical foundation of the diffusion model to the field of traffic data recovery. It not only enriches the theoretical methods for processing traffic data but also provides new insights and frameworks for the application of diffusion models in handling data with complex spatio-temporal dependencies. The Frozen Denoising Block comprises three critical components: time step embedding, a feature extraction network, and a data recovery network. These elements are elaborated in the subsequent paragraphs:\nTime Step Embedding. The timestep embedding function is defined to map a 1-D tensor of time indices t into a N \u00d7 dim tensor of positional embeddings, where each embedding captures periodic patterns across different frequencies. This is formulated as:\n$emb(t, d) = \\begin{cases}\ncos\\Big(t\\times exp(\\frac{k \\cdot log(\\frac{max\\_period}{dim/2})}{max\\_period})\\Big) & \\text{if } 0 \\leq k < dim/2,\\\\\nsin\\Big(t\\times exp(\\frac{(k-dim/2) \\cdot log(\\frac{max\\_period}{dim/2})}{max\\_period})\\Big) & \\text{if } dim/2 \\leq k < dim.\n\\end{cases}$ (12)\nWhere t is the time index, which can be fractional. dim is the dimensionality of the output embedding. max_period controls the maximum period of the sinusoidal functions, influencing the minimum frequency. k represents the dimension index within the embedding vector.\nThe embeddings for each dimension k are derived from the exponential spacing of frequencies, ensuring coverage across a range of scales from the most fine-grained changes at the highest frequency to the most coarse-grained changes at the lowest frequency. The function ensures that each time step t is mapped distinctly, capturing periodic recurrences in data effectively.\nFeature Extraction Network. For the feature extraction, we simply Leverage the power of convolutional networks to extract crucial spatial features. By integrating time step embedding, our network adeptly captures the dynamic variations in traffic data over time.\n$X_t = Conv (Conv(X_t) + emb(t, d))$ (13)\nData Recovery Network. Based on the results of time step embedding and feature extraction, the data recovery network learns to predict noise and gradually reconstructs clean data through the reverse diffusion process.\nThe forward step of DDPM for adding noise, described in Eq. 1, is formulated as:\n$X_{t+1} = \\sqrt{\\alpha_t}X_t + \\sqrt{1 - \\alpha_t}\\epsilon_t, \\epsilon_t \\sim N(0, I),$"}, {"title": "3.4. Spatial Temporal Large Language Model Block", "content": "Current traffic models, especially CNNs and RNNs, face challenges in capturing both spatial and temporal dependencies, while large language models excel at time series analysis but often need to pay more attention to the spatial aspect of traffic prediction tasks. To address these issues, a novel spatial temporal large-scale language model block (ST-LLM) has been proposed, the formulation as in Eq. 4- 8:\n$\\bar{X}_t = RMSNorm(MultiHead(Q, K, V)) + X_t,$\n$\\bar{X}_t = FFN (RMSNorm(\\bar{X}_t))$ (17)\nThis block redefines the time step of each location as a marker and incorporates a spatial temporal embedding module to learn the markers' spatial position and global time representation. It aims to effectively capture both spatial and temporal dependencies in traffic data, providing a practical and relevant solution for transportation and urban planning professionals.\nThe Final output can be obtained by a fully connected layer as:\n$Y_t = FC(\\bar{X}_t),$"}, {"title": "4. Experiment", "content": "In this section, we provide the dataset description in Section 4.1, followed by model configurations and evaluation metrics in Section 4.2. Baseline models are discussed in Section 4.3, while model performance is evaluated in Section 4.4. An ablation study is presented in Section 4.5, concluding with a real case analysis in Section 4.6."}, {"title": "4.1. Dataset Description", "content": "In our study, we evaluate STLLM-DF using six real-world datasets in both graph-based and grid-based data structures. Specifically, the graph-based datasets include PeMS04, PeMS07, and PeMS08, while the grid-based datasets comprise CHIBike, TDrive. To test the handling multi-task ability for STLLM-DF, we further add New York City Transportation Datasets including NYC-BIKE, NYC-BUS, NYC-TAXI and NYC-METRO . These datasets are publicly accessible and can be found in the LibCity repository, as referenced in Wang et al. (2023). Detailed information about each of these datasets is provided in Table 1 and 2.\nPeMS04 Song et al. (2020): Representing traffic data from the San Francisco Bay Area, this dataset was accumulated by the Caltrans Performance Measurement Systems (PeMS). Data from one sensor is condensed into 5-minute intervals, incorporating traffic flow, average speed, and average occupancy. It encompasses records from 307 sensors, spanning from Jan 1, 2018, to Feb 28, 2018.\nPeMS07 Song et al. (2020): Representing traffic data from the San Francisco Bay Area, this dataset was accumulated by the Caltrans Performance Measurement Systems (PeMS). Data from one sensor is condensed into 5-minute intervals, incorporating traffic flow, average speed, and average occupancy. It encompasses records from 883 sensors, spanning from May 1, 2017, to Aug 31, 2017.\nPeMS08 Song et al. (2020): This is the highway traffic flow data collected by the California Department of Transportation (Caltrans). The data range is from Jul 1, 2016 to Aug 31, 2016. The flow data is sampled every 5 minutes. The number of sensors is 170.\nNYC-BIKE: The NYC Bike Sharing Dataset provides a comprehensive view of New York City's bike-sharing system with hourly granularity from July 1, 2013 to September 30, 2017. It encompasses multiple interconnected components: TrafficNode (446976x820x820) capturing hourly trips between 820 stations, TrafficMonthlyInteraction (51x820x820) for monthly aggregated trips, StationInfo (820x5) detailing station locations, TrafficGrid (446976x20x20) representing city-wide usage patterns, weather data (446976x24), and social media check-in features. This rich dataset enables diverse analyses, including predictive modeling of bike demand, route popularity studies, weather impact assessments, and urban mobility pattern examinations, the station distribution as showing in Figure 3.\nNYC-BUS: The NYC Bus Dataset provides a comprehensive view of New York City's bus system from February 1, 2022 to January 13, 2024, offering hourly granularity data for 226 bus nodes or stations. The core of the dataset is a TrafficNode array (17064x226x226) capturing hourly passenger flow between all pairs of bus stops. It includes detailed StationInfo for each stop, comprising ID, geographical coordinates, and stop names. The dataset features 17,064 hourly time slots over nearly two years, allowing for in-depth analysis of daily, weekly, and seasonal trends in bus ridership. Additional components include a TrafficGrid representing bus traffic in a 20x20 city grid, weather data with 24 state dimensions, and potential social media check-in features, the station distribution as showing in Figure 3.\nNYC-taxi Liu et al. (2020): The data was made available by the NYCtaxi & Limousine Commission (TLC) and built on data from ride-hailing companies. The records cover New York from Jan 1, 2014 to Dec 31, 2014. For each demand record, the data provides information such as the pick-up time, drop-off time, pick-up zone, drop-off zone, etc. The traffic zones are predetermined by the TLC. In our dataset, we let each NYC taxi zones as a node, each node represents a specific area in NYC where taxis pick up or drop off passengers, the station distribution as showing in Figure 3.\nNYC-METRO: The NYC Metro Dataset offers a comprehensive view of New York City's subway system from February 1, 2022 to December 21, 2023, with hourly granularity. It features a multi-dimensional TrafficNode array (16512x472x472) capturing passenger flow between 472 subway stations and detailed StationInfo for each station, including geographical coordinates and names. The dataset's core is a 3D matrix where each slice represents hourly passenger movements between all station pairs, allowing for in-depth analysis of travel patterns and the station distribution as shown in Figure 3."}, {"title": "4.2. Model Configurations and Evaluation Metrics", "content": "Model configurations. All experiments were executed on a machine featuring an A100-SXM4-80GB. The datasets PEMS-BAY, PEMS03, PEMS04, PEMS07, PEMS08 and NYC datasets were split into training, validation, and test sets. PEMS-BAY and NYC datasets were divided in a 7:1:2 ratio, while PEMS03, PEMS04, PEMS07, and PEMS08 were partitioned using a 6:2:2 ratio. ST-LLM (Llamma) block-based configuration was also implemented with 32 heads, 17 layers, and a normalization epsilon of 1.0e-6. Notably, the model incorporated an ST-Denoising block (DDIMSampler) for the data recovery process, configured with a beta range of [0.0001, 0.02] and 1000 sampling steps, enhancing the model's capability to generate refined predictions through iterative data recovery.\nEvaluation metrics. To evaluate the effectiveness of traffic forecasting methods, three commonly used metrics are applied: Mean Absolute Error (MAE), Mean Absolute Percentage Error (\u041c\u0410\u0420\u0415), and Root Mean Square Error (RMSE). These metrics provide a comprehensive assessment of model accuracy and the magnitude of errors. They are defined as follows:\nMean Absolute Error (MAE) measures the average magnitude of prediction errors without considering their direction. It is calculated using the formula:\n$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y_i}|,$\nwhere n represents the number of observations, $y_i$ denotes the actual values, and $\\hat{y_i}$ represents the predicted values.\nMean Absolute Percentage Error (MAPE) expresses the error as a percentage of the actual values, allowing for easy comparison across datasets with different scales. It is computed as:\n$MAPE = \\frac{1}{n} \\sum_{i=1}^{n} |\\frac{y_i - \\hat{y_i}}{y_i}| \\times 100,$\nwhere $y_i$ and $\\hat{y_i}$ are the actual and predicted values, respectively.\nRoot Mean Square Error (RMSE) calculates the square root of the average squared differences between predicted and actual values, placing greater emphasis on larger errors. The"}, {"title": "formula is:", "content": "$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}.$\nEmploying MAE, MAPE, and RMSE provides a thorough analysis of model performance in traffic forecasting, capturing not only the average errors but also the distribution and proportionality of these errors relative to the actual values."}, {"title": "4.3. Baseline Models", "content": "In our comparative analysis, we evaluate our proposed approach against a comprehensive set of baselines in the traffic forecasting domain:\n\u2022 Historical Index (HI) Cui et al. (2021): Serves as the conventional benchmark, reflecting standard industry practices.\n\u2022 Spatial-Temporal Graph Neural Networks (STGNNs):\nGWNet Wu et al. (2020a): Automatically extracts uni-directed relations among variables, addressing limitations in exploiting latent spatial dependencies.\nDCRNN Li et al. (2018): Introduces the Diffusion Convolutional Recurrent Neural Network, capturing both spatial and temporal dependencies.\nAGCRN Bai et al. (2020): Incorporates adaptive modules to capture node-specific patterns and infer inter-dependencies among traffic series.\nSTGCN Yu et al. (2018): Integrates graph convolutions for spatial feature extraction and gated temporal convolutions for temporal feature extraction.\nGTS Shang et al. (2021): Forecasts multiple interrelated time series by learning a graph structure simultaneously with a Graph Neural Network.\nMTGNN Wu et al. (2020b): Automatically extracts uni-directed relations among variables, capturing both spatial and temporal dependencies.\nGMAN Zheng et al. (2020): Incorporates spatial and temporal attention mechanisms to capture dynamic correlations among traffic sensors.\n\u2022 Transformer-based models:\nPDFormer Jiang et al. (2023): Captures dynamic spatial dependencies, long-range spatial dependencies, and time delay in traffic condition propagation.\nSTAEformer Liu et al. (2023b): Proposes a spatio-temporal adaptive embedding that enhances the performance of vanilla transformers.\nSTNorm Deng et al. (2021): Leverages spatial and temporal normalization modules to refine high-frequency and local components.\nSTID Shao et al. (2022): Addresses indistinguishability of samples by attaching spatial and temporal identity information to input data.\nThis diverse range of models allows for a robust validation of our proposed method's capabilities in traffic forecasting."}, {"title": "4.4. Model Performance", "content": "Analysis of Multi-Task Prediction Results. Table 4 presents a comparative analysis of two models", "datasets": "BIKE, BUS, TAXI, and METRO. The performance metrics utilized for this comparison are Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). On average, STLLM-DF achieves improvements of 2.40% in MAE, 4.50% in RMSE, and 1.51% in MAPE. The NYC-TAXI dataset demonstrates the most substantial enhancements, with reductions of 5.10% in MAE, 9.50% in RMSE, and 3.80% in MAPE, indicating STLLM-DF's particular effectiveness in modeling complex transportation systems.\nThis improvement is further highlighted in the visual comparisons shown in Figures 4, 5 and 6. Figure 5 displays the predicted bike demand generated by the STAFormer model, capturing certain regions where the model closely follows the"}]}