{"title": "OledFL: Unleashing the Potential of Decentralized Federated Learning via Opposite Lookahead Enhancement", "authors": ["Qinglun Li", "Miao Zhang", "Mengzhu Wang", "Quanjun Yin", "Li Shen"], "abstract": "Decentralized Federated Learning (DFL) surpasses Centralized Federated Learning (CFL) in terms of faster training, privacy preservation, and light communication, making it a promising alternative in the field of federated learning. However, DFL still exhibits significant disparities with CFL in terms of generalization ability such as rarely theoretical understanding and degraded empirical performance due to severe inconsistency. In this paper, we enhance the consistency of DFL by developing an opposite lookahead enhancement technique (Ole), yielding OledFL to optimize the initialization of each client in each communication round, thus significantly improving both the generalization and convergence speed. Moreover, we rigorously establish its convergence rate in non-convex setting and characterize its generalization bound through uniform stability, which provides concrete reasons why OledFL can achieve both the fast convergence speed and high generalization ability. Extensive experiments conducted on the CIFAR10 and CIFAR100 datasets with Dirichlet and Pathological distributions illustrate that our OledFL can achieve up to 5% performance improvement and 8\u00d7 speedup, compared to the most popular DFedAvg optimizer in DFL.", "sections": [{"title": "I. INTRODUCTION", "content": "Federated Learning (FL) is a novel distributed machine learning paradigm that ensures privacy protection [1]\u2013[3]. It enables multiple participants to collaborate on training models without sharing their raw data. Currently, most research efforts [4]\u2013[9] have focused on Centralized Federated Learning (CFL). However, the presence of a central server in CFL introduces challenges such as communication burden, single point of failure [10], and privacy breaches [11]. In contrast, Distributed Federated Learning (DFL) offers improved privacy protection [12], faster model training [13], and robustness to slow client devices [14]. Therefore, DFL has emerged as a popular alternative solution [10], [13]."}, {"title": "II. RELATED WORK", "content": "Below, we will briefly review the most relevant work to our research, which includes Decentralized Federated Learning (DFL), acceleration techniques in optimization, and theoretical guarantees of DFL.\nDecentralized Federated Learning (DFL). To mitigate the communication burden on the server in centralized scenarios, decentralized communication methods distribute the communication load to each node while maintaining overall communication complexity equivalent to that in centralized scenarios [13]. Additionally, decentralized communication methods afford improved privacy protection compared to CFL [20]\u2013[22]. DFL has emerged as a promising field of research, recognized as a challenge in various review articles in recent years [23], [24]. Within DFL, Sun et al. [15] extend the FedAvg algorithm [1] to decentralized scenarios and complement it with local momentum acceleration to enhance convergence. Furthermore, Dai et al. [25] introduce sparse training into DFL to reduce communication and computation costs, while shi et al. [16] apply SAM to DFL and enhance the consistency among clients by incorporating Multiple Gossip Steps. These endeavors gradually improve the performance of DFL from different perspectives. However, a significant performance gap still exists compared to CFL. For further related work on DFL, please refer to the survey papers [11], [23], [26] and their references therein.\nAcceleration Techniques for Deep Learning. In deep learning, momentum and restart are typical acceleration techniques. The momentum techniques focus more on the optimization of the optimizer design, while restart techniques focus more on the selection of initialization points. In Federated Learning (FL), one type of method used in the centralized FL (CFL) domain is the global momentum, such as FedCM [27] and MimeLite [28], which estimates the global momentum at the server and applies it to each client update, thereby alleviating the problem of client heterogeneity. Another type is the local momentum used in DFL, such as the DFedAvgM algorithm proposed by [15], which utilizes local momentum to accelerate the convergence process. Restart techniques have nearly negligible computational cost but significantly enhance the effectiveness of the algorithm. Zhou [29] introduces the Lookahead optimizer, which improves learning stability and reduces the variance of its inner optimizer through a k-steps forward, 1 step back approach. Additionally, lin et al. [30] develop a universal framework called Catalyst, which can accelerate first-order optimization methods such as SAG [31], SAGA [32], and SVRG [33]. Subsequently, Trimbach et al. [34] extend Catalyst to the field of distributed learning and used Catalyst to accelerate the DSGD algorithm [35].\nTheoretical Guarantees of DFL. A more comprehensive comparison is presented in Table I. In the aspect of convergence analysis, current DFL works such as DFedAvg [15], DFedAvgM [15], and DFedSAM [16] have only concentrated on the convergence analysis. DFedSAM, by abandoning the gradient bounded assumption in DFedAvg, has demonstrated an advancement in the convergence analysis of the algorithm. However, in machine learning, algorithms are often evaluated based on their ability to perform well on new, unseen data. This is known as generalization performance, and algorithms that exhibit higher generalization performance are considered to be state-of-the-art. However, in the field of DFL, there is currently a lack of analysis on how well-proposed algorithms generalize to new data. This results in a lack of theoretical support for why these algorithms perform well."}, {"title": "III. METHODOLOGY", "content": "In this section, we begin by elucidating the meanings of several notations. We then introduce the problem setup, and then, we propose OledFL and compare it with existing"}, {"title": "A. Notations", "content": "Let $m$ be the total number of clients. $T$ represents the number of communication rounds. $(\\cdot)_k^t$ indicates variable $(\\cdot)$ at the $k$-th iteration of the $t$-th round in the $i$-th client. $x$ denotes the model parameters. The communication topology between clients can be represented as graph denoted as $G = (N,E, W)$, where $N = \\{1,2,...,m\\}$ represents the set of clients, $E \\subseteq N \\times N$ denotes the links between clients, $W_{i,j}$ represents the weight of the link between client $j$ and client $i$ and $W = [W_{i,j}] \\in [0,1]^{m \\times m}$ represents the mixing matrix. The inner product of two vectors is denoted by $\\langle \\cdot,\\cdot \\rangle$, and $\\| \\cdot \\|$ represents the Euclidean norm of a vector. Other symbols will be explained in their respective contexts."}, {"title": "B. Problem Setup", "content": "In this paper, we consider a network of $m$ clients whose objective is to jointly solve the following distributed population risk $F$ minimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^d} F(x) := \\frac{1}{m} \\sum_{i=1}^m F_i(x), F_i(x) = \\mathbb{E}_{\\xi \\sim D_i} F_i(x; \\xi)\n$$\nwhere $D_i$ represents the data distribution in the $i$-th client, which exhibits heterogeneity across clients. Each client's local objective function $F_i(x; \\xi)$ is associated with the training data samples $\\xi$. We denote $x^* = \\arg \\min_x F(x)$ as the optimal value of (1). Unlike CFL, we address (1) by enabling clients to collaborate through a mixing matrix $W$ in a decentralized manner, leveraging peer-to-peer communication among clients without the need for server coordination.\nPractically, we consider the empirical risk minimization of the non-convex finite-sum problem in DFL as:\n$$\n\\min_{x \\in \\mathbb{R}^d} f(x) := \\frac{1}{m} \\sum_{i=1}^m f_i(x), \\quad f_i(x) = \\frac{1}{|S_i|} \\sum_{z_j \\in S_i} f_i(x; z_j)\n$$\nwhere each client stores a private dataset $S_i = \\{z_j\\}$, with $z_j$ drawn from an unknown distribution $D_i$. We denote $x^* = \\arg \\min_x f(x)$ as the optimal value of problem (2)."}, {"title": "C. OledFL Algorithm", "content": "The most popular decentralized FL optimizers for solving the problem (2) are DFedSAM [16] and DFedAvg [15]. However, the decentralized approach, lacking central server coordination, leads to increased client inconsistency. To enhance consistency, we design the opposite lookahead enhanced (Ole) initialization method before performing local optimization at each client:\n$$\nx_i^t = \\hat{x}_i^t + \\beta (\\bar{x}^{t-1} - \\hat{x}_i^t)\n$$\nwhere $\\bar{x}^{t-1} = \\sum_j W_{i,j}x_j^K$ is the aggregated model. Intuitively, at each initialization, clients obtain the opposite direction of the most recent iteration through Ole. This ensures that the"}, {"title": "IV. THEORETICAL ANALYSIS", "content": "In this section, we begin by presenting the theoretical analysis, which offers a comprehensive examination of the combined performance in optimization and generalization. Then, we introduce the necessary assumptions utilized in our proofs. At last, we outline the main theorems, encompassing both optimization and generalization, respectively."}, {"title": "A. Excess Risk Error", "content": "In existing literature on DFL, the most of analyses focus on the studies from the onefold perspective of convergence but ignore learning its impact on generality [15], [16], [42]. Additionally, some studies in distributed learning exclusively analyze the algorithm's generalization while overlooking the"}, {"title": "B. Definition And Assumptions", "content": "Below, we introduce the definition of the mixing matrix and several assumptions utilized in our analysis.\nDefinition 1: (Gossip/Mixing matrix). [Definition 1, [15]] The gossip matrix $W = [W_{i,j}] \\in [0,1]^{m \\times m}$ is assumed to have the following properties: (i) (Graph) If $i \\neq j$ and $(i,j) \\notin V$, then $w_{ij} = 0$, otherwise, $W_{i,j} > 0$; (ii) (Symmetry) $W = W^T$; (iii) (Null space property) $null\\{I - W\\} = span\\{1\\}$; (iv) (Spectral property) $I > W > -I$. Under these properties, the eigenvalues of $W$ satisfy $1 = |\\mu_1(W))| > |\\mu_2(W))| \\geq \\dots \\geq |\\mu_m(W))|$. Furthermore, we define $\\psi := \\max\\{\\mu_2(W)|, |\\mu_m(W))|\\}$ and $1-\\psi \\in (0,1]$ as the spectral gap of $W$.\nAssumption 1: (L-Smoothness) The non-convex function $f_i$ satisfies the smoothness property for all $i \\in [m]$, i.e., $\\|\\nabla f_i(x) - \\nabla f_i(y)\\| \\leq L\\|x - y\\|$, for all $x, y \\in \\mathbb{R}^d$.\nAssumption 2: (Bounded Variance) The stochastic gradient $g_{i,k}^t = \\nabla f_i(\\hat{x}_i^{t,k}; \\xi_i^{t,k})$ with the randomly sampled data $\\xi_i^{t,k}$ on the local client $i$ is unbiased and with bounded variance, i.e., $\\mathbb{E}[g_{i,k}^t] = \\nabla f_i(\\hat{x}_i^{t,k})$ and $\\mathbb{E}\\[\\|g_{i,k}^t - \\nabla f_i(\\hat{x}_i^{t,k})\\|^2] < \\sigma_i^2$, for all $x \\in \\mathbb{R}^d$.\nAssumption 3: (Bounded Heterogeneity) For all $x \\in \\mathbb{R}^d$, the heterogeneous similarity is bounded on the gradient norm as $\\mathbb{E}\\[\\|\\nabla f_i(w)\\|^2] < G^2 + B^2\\mathbb{E}\\[\\|\\nabla f(w)\\|^2]$, where $G \\geq 0$ and $B \\geq 1$ are two constants.\nAssumption 4: (Lipschitz Continuity). The global function $f$ satisfies the $L_G$-Lipschitz property, i.e. for $\\forall x_1, x_2$, $\\|f(x_1) - f(x_2)\\| \\leq L_G\\|x_1 - x_2\\|$\nDefinition 1 is commonly used to describe the communication topology in DFL, where it can also be viewed as"}, {"title": "C. Optimization Analysis", "content": "In this part, we present the optimization error and convergence rate of OledFL under the assumptions 1-3. All the proofs can be found in the Appendix A.\nTheorem 1: Under Assumption 1 - 3, let the learning rate satisfy $\\eta \\leq \\frac{\\sqrt[3]{30}}{K^{3/2}LB}$ where $K > 2$, let the Ole parameter $\\beta < \\min\\{\\sqrt{\\frac{1}{10(1-\\psi)}}, \\sqrt{\\frac{3}{5}}\\}$, and after training $T$ rounds, the averaged model parameters generated by our proposed algorithm satisfies:\n$$\n\\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E}\\[\\|\\nabla f(\\bar{x}^t)\\|^2] \\leq \\frac{\\mathbb{E}\\[f(x^0) - f(x^*)]}{\\kappa \\eta KT} + \\zeta(\\eta, L, \\psi) \\sigma^2_i + \\alpha(\\eta, K, L, \\psi) G^2 + \\phi(K, \\eta, L) \\chi^2 - \\chi(L, \\psi, \\Delta_T,T)\\beta^2\n$$\nwhere $\\kappa \\in (0,1)$ is a constant and $\\alpha(\\eta, K, L, \\psi) = \\frac{\\eta^2 K^2 L^2}{1 - (1 + 9K^2\\eta L)^2}$, $\\phi(K,\\eta, L) = \\eta (1+9K^2 \\eta L)$, $\\chi(L,\\psi,\\Delta_T,T) = (3 + (1-\\psi)^{-2})(\\frac{1}{\\kappa (1 + \\psi)^2})^{-1} (1 - \\frac{\\psi}{6K})^T$, which the consistency term $\\Delta_T = \\frac{1}{m}\\sum_{i=1}^{m} \\| x_i^K - \\bar{x}^T \\|^2$ and $\\zeta(\\eta, L, \\psi) = \\frac{36 \\eta^2}{\\sqrt{3}}$.\nFurther, by selecting the proper learning rate $\\eta = O(\\frac{1}{\\sqrt[3]{KT}})$ and let $D = f(x^0) - f(x^*)$ as the initialization bias, thenthe averaged model parameters $\\bar{x}^t$ satisfies:\n$$\n\\frac{1}{T} \\sum_{t=0}^{T-1} \\mathbb{E}\\[\\|\\nabla f(\\bar{x}^t)\\|^2] = O(\\frac{D}{\\sqrt[3]{KT}}) + \\frac{L^2}{\\sqrt{TK(1 - \\psi)^2}} \\sigma_i^2 + \\frac{L}{\\sqrt{TK}}G^2\\right).\n$$\nRemark 1: When setting $\\lambda = 0$ in Theorem 1, we can obtain the optimization error and convergence rate of OledFL with local SGD. Moreover, by setting $\\lambda = O(\\frac{1}{\\sqrt[3]{K}})$ , the term $\\frac{\\kappa \\lambda^2}{\\sqrt[3]{TK}} = O(\\frac{1}{KT^{3/2}})$ in the convergence rate, as generated by local SAM, can be neglected in comparison to the dominant term $O(\\frac{1}{\\sqrt[3]{KT}})$. Furthermore, OledFL achieves a convergence rate of $O(\\frac{1}{\\sqrt[3]{KT}})$, which has been demonstrated as the optimal rate in stochastic methods in DFL under general assumptions. Additionally, in the dominant term of the convergence rate $O(\\frac{D}{\\sqrt[3]{KT}} + \\frac{L}{\\sqrt{TK(1 - \\psi)^2}}\\sigma_i^2)$, it can be observed that better topological connectivity leads to a faster convergence rate. We will verify this conclusion in Section V-C. Moreover, a larger number of local epochs $K$ also leads to a faster convergence rate, as confirmed in Section V-D (see Figure 8 (b)).\nRemark 2: Theorem 1 provides a general convergence bound for the OledFL. When $\\beta = 0$, it degrades to the vanilla"}, {"title": "D. Generalization Analysis", "content": "In this section, we primarily present a generalization analysis of our OledFL method under two gradient properties (bounded variance and Lipschitz continuity). We utilize uniform stability analysis, which is widely adopted in previous literature, to analyze the generalization error of OledFL. Next, we introduce the definition of uniform stability and then demonstrate the effectiveness of OledFL. All detailed proofs can be available in the Appendix B.\nIn DFL framework, we suppose there are $m$ clients participating in the training process as a set $C = \\{i\\}_{i=1}^{m}$. Each client has a local dataset $S_i = \\{z_j\\}_{j=1}^{S}$ with total $S$ data sampled from a specific unknown distribution $D_i$. Now we define a re-sampled dataset $\\tilde{S}\\_i$ which only differs from the dataset $S_i$ on the $j^{*}$-th data. We replace the $S_{i^{*}}$ with $\\tilde{S}\\_{i^{*}}$ and keep other $m-1$ local dataset, which composes a new set $\\tilde{C}$. $\\tilde{C}$ only differs from the $C$ at $j^{*}$-th data on the $i^{*}$-th client. Then, based on these two sets, our method could generate two solutions, $\\mathbf{x}^T$ and $\\mathbf{\\tilde{x}}^T$ respectively, after $T$ rounds. By bounding the difference according to these two models, we can obtain stability and generalization efficiency.\nDefinition 2: (Uniform Stability [47]) For these two models $\\mathbf{x}^T$ and $\\mathbf{\\tilde{x}}^T$ generated as introduced above, a general method satisfies $\\epsilon$-uniformly stability if:\n$$\n\\sup_{\\mathbf{z}_j \\sim \\{D_i\\}} \\mathbb{E}\\[\\|f(\\mathbf{x}^T; \\mathbf{z}j) - f(\\mathbf{\\tilde{x}}^T; \\mathbf{z}j)\\|] \\leq \\epsilon.\n$$\nMoreover, if a general method satisfies $\\epsilon$-uniformly stability, then its generalization error could also be bounded [47], [48]\n$$\n\\mathbb{E}\\_G \\leq \\sup_{\\mathbf{z}\\_j \\sim \\{D\\_i\\}} \\mathbb{E}\\[\\|f(\\mathbf{x}^T; \\mathbf{z}j) - f(\\mathbf{\\tilde{x}}^T; \\mathbf{z}j)\\|] \\leq \\epsilon.\n$$\nTheorem 2: Under Assumption 2, 3, and 4, let all conditions in the optimization process be satisfied, let the learning rate be selected as $\\eta = O(\\frac{1}{\\sqrt[3]{K}}) = \\frac{c}{\\sqrt[3]{K}}$ where c is a constant and $\\beta < \\frac{1 - \\psi}{4 \\sqrt{m+1}}$, let $t_0$ to be a specific round to firstly select the different data sample, and let $U = \\sup_{\\mathbf{x}, \\mathbf{z}} \\{f(\\mathbf{x}; \\mathbf{z})\\}$ be the upper bound, for arbitrary data sample $\\mathbf{z}$ followed the joint distribution $D_i$, we have:\n$$\n\\mathbb{E}\\[\\|f(\\mathbf{x}^{T+1}; \\mathbf{z}) - f(\\mathbf{\\tilde{x}}^{T+1}; \\mathbf{z})\\|] \\leq \\frac{U t_0}{S} + \\frac{2L_G(L_G + S \\sigma_i) K (\\psi + L_G c K)}{\\alpha (1 + 2 \\beta)} + \\frac{(2 L_G(L_G + S \\sigma_i)}{(1 + 2 \\beta) S L_G} \\frac{c K L}{\\sqrt[3]{K}}.\n$$"}, {"title": "VI. CONLUSION", "content": "In this paper, we propose a plug-in method named OledFL, which integrates existing DFL optimizers to enhance the consistency among clients and significantly improve convergence speed and generalization performance at almost negligible computational cost. Theoretically, we are the first to conduct a joint analysis of algorithm convergence and generalization in the field of DFL, and we demonstrate the effectiveness of OledFL in reducing optimization error and generalization error. Moreover, a comprehensive explanation of the mechanism of action of Ole has been provided, encompassing intuitive, experimental, and theoretical perspectives. Furthermore, certain conclusions derived from theoretical deductions have been validated experimentally, thus offering additional insights into the Ole plugin methodology. Finally, extensive experiments on the CIFAR10&100 datasets under Dirichlet and Pathological distributions demonstrate that OledFL can significantly reduce the performance gap between CFL and DFL, and even surpass CFL optimizers such as FedSAM and SCAFFOLD, which is crucial for further advancement in the field of DFL."}, {"title": "APPENDIX", "content": "In this part, we provide supplementary materials including the proof of the optimization and generalization error. Here's the table of contents for the Appendix.\n\u2022 Appendix A: Proof of the theoretical analysis.\nAppendix A: Proof for optimization error.\nAppendix B: Proof for generalization error."}, {"title": "A. Proofs for the Optimization Error", "content": "In this part, we prove the training error for our proposed method. We assume the objective $f(x) := \\frac{1}{m}\\sum_{i=1}^{m} f_i(x)$ is L-smooth w.r.t x. Then we could upper bound the training error in the FL. Some useful notations in the proof are introduced in the Table A. Then we introduce some important lemmas used in the proof."}]}