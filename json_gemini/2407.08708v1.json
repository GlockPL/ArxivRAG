{"title": "eyeballvul: a future-proof benchmark for vulnerability detection in the wild", "authors": ["Timoth\u00e9e Chauvin"], "abstract": "Long contexts of recent LLMs have enabled a new use case: asking models to find security vulnerabilities in entire codebases. To evaluate model performance on this task, we introduce eyeballvul: a benchmark designed to test the vulnerability detection capabilities of language models at scale, that is sourced and updated weekly from the stream of published vulnerabilities in open-source repositories. The benchmark consists of a list of revisions in different repositories, each associated with the list of known vulnerabilities present at that revision. An LLM-based scorer is used to compare the list of possible vulnerabilities returned by a model to the list of known vulnerabilities for each revision. As of July 2024, eyeballvul contains 24,000+ vulnerabilities across 6,000+ revisions and 5,000+ repositories, and is around 55GB in size.", "sections": [{"title": "1 Introduction", "content": "With the recent progress in both capabilities of LLMs and their context window lengths, it has become feasible to fit entire repositories of source code, or large fractions of them, into LLMs' context windows and instructing them to find security vulnerabilities, essentially applying the LLMs as SAST (static application security testing) tools. No benchmark or dataset exists yet to evaluate performance on this use case.\nWe introduce eyeballvul, an open-source benchmark designed to fill this gap, with the following attributes:\n\u2022 real world vulnerabilities: sourced from a large number of CVEs in open-source repositories;\n\u2022 realistic detection setting: directly tests a likely way that vulnerability detection could end up being deployed in practice (contrary to many previous classification-type datasets);\n\u2022 large size: over 6,000 revisions and 24,000 vulnerabilities, over 50GB in total size;\n\u2022 diversity: no restriction to a small set of programming languages;\n\u2022 future-proof: updated weekly from the stream of published CVEs, alleviating training data contamination concerns; far from saturation\u00b9.\nThe main repository for the benchmark is eyeballvul, and the code and data used in this paper are available in the eyeballvul_experiments repository (latest commit used in this paper: 38ca630)."}, {"title": "2 Creating the benchmark", "content": null}, {"title": "2.1 Procedure", "content": "The benchmark essentially re-packages existing data (the stream of CVEs in open-source repositories) into an appropriate format. Our goal is for the benchmark to consist of a list of revisions in different repositories, with for each revision, the known vulnerabilities at that revision as the ground truth. In contrast, each CVE refers to a single vulnerability, and often includes a list of affected versions (between the introduction of the vulnerability and its fix).\nThe conversion is done through the following steps:"}, {"title": "2.2 Statistics on eyeballvul", "content": "As of the date of the data used in this paper (2024-06-07), eyeballvul contains 24,095 vulnerabilities, in 6,429 revisions and 5,892 repositories. Vulnerabilities to revisions is a many-to-many mapping. On average, each revision has 4.3 vulnerabilities, and each vulnerability is present in 1.1 revisions."}, {"title": "3 Methodology", "content": null}, {"title": "3.1 Processing revisions", "content": "The simplest way of leveraging a long-context model to find vulnerabilities in a codebase is to concatenate all the files, split them in chunks so they fit within the context window, and prepend some instructions about finding vulnerabilities. This is essentially our approach for each revision, though with a few necessary modifications to make it viable:\n\u2022 we exclude all files with a blocklisted extension, all files having some part of their path starting with a dot (e.g..git/), all files above 200,000 characters, and all files whose MIME type doesn't start with text/;\n\u2022 since Anthropic doesn't provide the Claude 3 / Claude 3.5 tokenizer, we can't know in advance whether a chunk will fit within the context window. So for all models, we adopt the same approach of starting with a large chunk (above 600,000 characters, unless the full repository is smaller than that), querying the API, and reducing the chunk by at most 5% (never splitting files) and at least one file until the API stops raising a context window exceeded exception.\nThe code described above can be found at this link.\nConcretely, our vulnerability detection prompt (presented in appendix A) instructs models to include for each vulnerability a headline, an analysis, the most relevant CWE (type of vulnerability), a list of most concerned functions, a list of most concerned filenames, and a classification between \"very promising\", \"slightly promising\" and \"not promising\"."}, {"title": "3.2 LLM scorer", "content": "Our ground truth consists of a list of known vulnerabilities present at each revision. These vulnerabilities are described in plain text, and our models also return lists of possible vulnerabilities as plain text.\nTo compare submissions to our ground truth, the scoring method for this benchmark is an LLM-based scorer, that is instructed to score each lead based purely on whether it corresponds to a known vulnerability at that revision. The instructions ask the model to complete a reasoning step before returning a score of 0 or 1. If the score is 1, the LLM must also return the corresponding CVE ID. 6 few-shot examples are used to demonstrate how to score some common edge cases.\nThe exact instructions used in this paper can be found in appendix B, and 4 example reasoning traces are presented in appendix C.\nIn this paper, the scoring model is Claude 3.5 Sonnet, run at temperature 0.1 (a low temperature for consistency, though not 0 because the model needs to output valid YAML and we retry in case of failure to parse - while not wanting to rely on the non-determinism remaining at temperature 0).\nEach lead is scored, and possibly mapped to a known vulnerability, independently. This means that several leads can potentially be mapped to the same vulnerability; this is particularly likely to happen if revisions are served to models over several chunks. In that case, we only count the first match as a true positive, and discard the duplicate matches.\nWe return to the LLM scorer, evaluating its quality and discussing possible alternatives, in section 5."}, {"title": "4 Results", "content": "We now investigate how well some long-context models perform on the benchmark.\nWe want to highlight upfront that this paper is mostly about introducing the benchmark and giving pre- liminary results, using the simplest possible vulnerability detection tooling based on long-context models.\nWe are confident that significant performance gains can be obtained by improving the tooling in a number of ways (for instance by running an agent to investigate each lead obtained by the initial scan, or coupling with some of the many tools available to security researchers, such as fuzzing, symbolic execution, real execution, debugging...). We also haven't tested different variants of the prompt, due to the cost of running models on the benchmark.\nWe run 7 leading long-context models on the benchmark: Claude 3 Haiku (claude-3-haiku-20240307), Claude 3 Sonnet (claude-3-sonnet-20240229), Claude 3 Opus (claude-3-opus-20240229), Claude 3.5 Sonnet (claude-3-5-sonnet-20240620), GPT-4 Turbo (gpt-4-turbo-2024-04-09), GPT-4o (gpt-4o-2024-05-13), and Gemini 1.5 Pro (gemini-1.5-pro). We restrict ourselves to revisions of size smaller than 600kB, so that"}, {"title": "4.1 Overall performance: significant room for improvement", "content": "The main results can be seen in figure 3. Figure 4 is an alternative presentation as a Pareto efficiency plot. Error bars are computed to target a confidence level of 0.95, by assuming a Bernoulli distribution for true positives in precision and recall, using the Wilson score interval, and the implementation of the confidenceinterval library (Gildenblat, 2023). These error bars are slightly underestimated because the variance of the LLM scorer isn't taken into account. We also compute performance at the level of revisions, meaning that some vulnerabilities are considered multiple times in the results. This might result in performance being slightly underestimated if there is a positive correlation between number of revisions per vulnerability and difficulty of the vulnerability, for instance by considering that harder vulnerabilities may take longer to discover, and therefore be present in more revisions. However, each vulnerability is only present in 1.1 revisions on average, so the bias, if any, seems small enough."}, {"title": "4.2 Types and severities of vulnerabilities found", "content": "Better performance on superficial vulnerabilities. Given that models are asked to return the CWE for the root cause of each lead (taken from View 1,003, a simplified CWE list used for root cause identification, consisting of 130 CWEs out of 938), we can compute statistics on the types of vulnerabilities most often correctly identified by models. We also compare these to the 2023 CWE Top 25 list established by MITRE.\nIt appears that our tooling is comparatively weak at detecting memory corruption vulnerabilities (Out- of-bounds Write / Read, Use After Free). It seems best at finding easy, superficial vulnerabilities, like the various injection vulnerabilities. This makes intuitive sense, given that models are only given one pass at reading a codebase. Another possible factor is that memory corruption vulnerabilities typically occur in C/C++ projects, which tend to be large, and we have selected repositories below 600kB.\nSlightly better performance on more severe vulnerabilities. Figure 6 shows the CVSS v3 Base Score severities of the vulnerabilities found by any model (when they have a CVSS v3 score), as well as the severities of all vulnerabilities in the benchmark. While the distributions are broadly similar, the average severity of true positives is 7.3, and 22.7% of found vulnerabilities are critical (CVSS v3 Base Score \u2265 9.0). These numbers are slightly higher than the ones for the underlying distribution of vulnerabilities (average 7.0, 15.5% critical). This likely indicates that vulnerabilities which are easiest for models to find, while being more superficial, tend to be more severe. This is notably the case of the injection vulnerabilities, which are superficial (easy to spot through a single reading pass over a codebase) and usually severe, as they often directly result in remote code execution."}, {"title": "4.3 Cost is dominated by false positives", "content": "We finally evaluate the cost of finding vulnerabilities with the different models. There are two sources of cost: inference cost, and false positives (costing developer time). Figure 7 shows the inference cost per true positive, and the number of false positives per true positive. The inference cost ranges from $0.12/tp (Claude 3 Haiku) to $3.52/tp (Claude 3 Opus), while the number of false positives ranges from 4.1fp/tp (Gemini 1.5 Pro) to 9.9fp/tp (Claude 3 Haiku). Gemini 1.5 Pro stands out from the other models, with the second low- est inference cost per true positive ($0.24/tp), and lowest number of false positives per true positive (4.1fp/tp).\nPlugging rough figures such as an average of 10 minutes of security researcher or project maintainer time lost per false positive, at a rate of $100/h, yields false positive costs ranging from $68/tp to $165/tp. We conclude that in our current setup, false positives account for almost all of the cost of the method. This makes alternative setups spend- ing more inference to investigate leads in detail seem particularly worth investigating, especially as the cost of a given level of model performance has tended to fall steeply in recent years."}, {"title": "4.4 Slight evidence of training data contamination", "content": "The benchmark also allows to compare model per- formance before and after their knowledge cutoffs. A single revision may contain vulnerabilities pub- lished both before and after a knowledge cutoff, and models return a single list. It's easy to map true positives and false negatives to a date (the date of publication of the vulnerability), but not so much for false positives. Given that there are fewer vulnerabilities after than before knowledge cutoffs overall, using the same number of false pos- itives to compute precision before and after the cut- off would introduce a bias. We therefore map each false positive to a float proportional to the actual number of positives before and after the cutoff. For instance, if a revision has 7 vulnerabilities before the cutoff and 3 after the cutoff, each false positive at that revision is considered to be 70% before the cutoff and 30% after the cutoff.\nFigure 8 shows model performance before and after knowledge cutoffs. We tend to see an overall decline in precision and recall, though often within error bars."}, {"title": "4.5 Smaller context windows don't explain the lower performance of GPT-4", "content": "To test the hypothesis that Claude 3 Opus and Gemini 1.5 Pro are performing better than GPT-4 Turbo and GPT-4o because the maximum revision size of 600kB is likely to fit within a 200k or 1M context window but not necessarily a 128k one, we compute the average number of model invocations for each model. On average, Claude 3 models are called 1.02 times, and GPT-4 models 1.05 times. This 3% difference doesn't seem sufficient to explain a 1.3x difference in F1 scores between Claude 3 Opus and GPT-4o."}, {"title": "4.6 Possible unpublished vulnerabilities", "content": "There is a chance that some vulnerabilities reported by models aren't known or published yet, but would be confirmed if investigated. This would mean that the precisions reported above are underestimates. We think the number is currently too small to significantly impact the results. If models and toolings improved so much that they started finding large numbers of previously unknown vulnerabilities, this could become a weakness of the benchmark. In this scenario, the benchmark should be complemented with a measure of performance based on the number of new vulnerabilities discovered, similar to how fuzzing techniques are often evaluated."}, {"title": "5 Evaluation of the LLM scorer", "content": null}, {"title": "5.1 Comparison to human scores", "content": "Our first method to evaluate the quality of the LLM scorer is to compare its scores, on a sample of 100 random leads generated by models, to scores given by three human cybersecurity specialists (one personal connection, and two Upwork contractors). They are instructed to score according to the LLM scoring prompt. Their Cohen kappa scores with the LLM scorer are 0.64, 0.46 and 0.40 (average: 0.50). Readers wanting to try for themselves, and get a better intuition on the task of scoring, can follow the LLM scorer instructions, then score the sample here, before checking the LLM scores in this file.\nThe LLM scorer rated 6 samples out of 100 as positive. The highest kappa of 0.64 corresponds to 5 disagreements (one LLM-positive, and 4 LLM-negative), and the lowest kappa of 0.40 to 12 disagreements (one LLM-positive, 11 LLM-negative). There are significant disagreements within the human scorers them- selves, highlighting the difficulty of the task: Cohen kappas between each human pair are 0.59, 0.36 and 0.47 (average: 0.48). On a scale of 1 to 5, human scorers rate their confidence at 4.8 on average, and 4.0 on samples where their score disagrees with the LLM scorer. A few random examples of agreement and disagreement are presented in appendix C.\nUpon learning about the disagreements with the LLM scorer, our personal connection revised their assessment on one out of the 5 disagreements (increasing the kappa score to 69%)."}, {"title": "5.2 Quality of the underlying CVE data", "content": "In our experience, the main challenge with LLM scoring is that CVE descriptions are often too terse and imprecise to conclusively compare them with a lead provided by a model. To get an idea for the score at which extremely capable models would saturate due to the lack of details in CVEs, we sample 100 random CVEs and manually score them on a scale of 1 to 5, where 1 means \"crucial lack of specificity, too difficult to compare\u201d and 5 means \u201cenough specificity to easily compare\u201d.\nOur results are as follows:\n\u2022 1/5: 14% of CVES\n\u2022 2/5: 16% of CVES\n\u2022 3/5: 7% of CVES\n\u2022 4/5: 19% of CVES\n\u2022 5/5: 44% of CVES"}, {"title": "5.3 Possible alternatives", "content": "Our vulnerability detection prompts instructs models to return (among other things) the most specific CWE, and lists of most concerned filenames and function names. In theory, this could allow to bypass the LLM scorer, by simply comparing e.g. the CWEs and function names between the lead and the real vulnera- bilities. Unfortunately, the underlying CVE data source doesn't always include this information. While root cause CWEs are often available, it's not systematic for CVEs to include affected filenames or function names, and when they do, this is only done in natural text, not in a dedicated field of the CVE format. A not-fully-reliable AI-based pre-processing step would still be needed to extract this data when it is available.\nAnother scoring method, used in e.g. CYBERSECEVAL 2 (Bhatt et al., 2024), could be asking for an exploit, and testing that the given exploit works. However, this would require an ad-hoc time-consuming step of creating a full environment for each vulnerability, which currently represents a prohibitive amount of effort (this might change in the future, with more capable software engineering agents).\nYet another way could be converting this benchmark into a multiple-choice question benchmark, for instance by selecting repositories which have at least one revision such that one vulnerability is present at this revision, and 3 other vulnerabilities are not present at this revision, then asking models to pick which vulnerability is the one present at this revision. This approach would be feasible, though not without some issues (such as CVE descriptions mentioning affected versions, and the source code containing the version number), but its main weakness is that it is far less relevant to the task of actually trying to find vulnerabilities in the wild.\nOverall, we believe that the LLM scorer, coupled with the frequent lack of details in the CVE descrip- tions (in around 30% of CVEs), is the main weakness of our approach. But we still believe that it is currently the most appropriate method to score models at scale, and that it provides useful enough signal. The results above also tend to confirm its relevance: for instance, our LLM scorer appears capable of distinguishing the performance of the three models in the Claude 3 family."}, {"title": "6 Impact", "content": "Vulnerability detection is a dual-use capability, that is seeked by both defenders and attackers. However, we believe that this specific task of vulnerability detection in source code, using simple and universal tooling such as the one presented here, in the absence of an implementation overhang, should empower defenders disproportionately over attackers.\nFirst, a large fraction of source code in the world belongs to closed-source codebases, giving defenders a large advantage over attackers.\nFor open-source code, we hope to see a project similar to Google's OSS-Fuzz (Serebryany, 2017) an initiative where Google runs fuzzing on hundreds of security critical open-source repositories for free being launched once the combination of models and tooling is deemed sufficiently performant.\nThe critical consideration in open-soure code is about who will spend inference compute of SOTA models and toolings on target codebases first. Let's consider the limit where 100% of the important code is covered by defenders. When this is the case, attackers will no longer be able to spend the compute first, especially if AI labs start spending inference compute of their next-generation models on this task before release. As a side note, this would, to the best of our knowledge, be the first instance of a useful external task on which AI labs should spend inference compute prior to release. We don't expect that attackers"}, {"title": "7 Related work", "content": null}, {"title": "CTF benchmarks", "content": "Other benchmarks have recently been developed to test cybersecurity capabilities of language models, including Shao, Chen, et al., 2024 and Shao, Jancheska, et al., 2024. These benchmarks were designed to test the ability of LLM agents to solve CTF datasets, which has little overlap with finding vulnerabilities in large codebases."}, {"title": "Vulnerability detection and exploitation", "content": "CYBERSECEVAL 2 (Bhatt et al., 2024) notably included a vulnerability identification and exploitation benchmark, which was procedurally generated. One advantage is the automatic scoring based on whether the LLM can generate a crashing input, but the major drawback of this approach is the lack of diversity compared to real-world codebases. Project Naptime by Google Project Zero (Glazunov and Brand, 2024) showed that dramatic improvements in performance can be obtained by implementing simple ideas (space for reasoning, interactive environment, specialized tools...). We expect similar improvements to be possible for eyeballvul."}, {"title": "Classification datasets", "content": "Prior to long-context LLMs, most vulnerability detection benchmarks were de- signed as classification tasks, such as determining whether a function or file contains a vulnerability, or whether a commit introduces / fixes a vulnerability. The bigger of these are created from commits marked as security patches, applying a bag of heuristics to add vulnerable and non-vulnerable functions to the datasets. One example is DiverseVul (Chen et al., 2023), which was used (and recognized as quite noisy) in Google Deepmind's Phuong et al., 2024. The major weakness of these datasets is that many vulnerabilities don't cleanly map to a single function, and context on other functions is often key in vulnerability detection. Wen et al., 2024 added \"dependencies\" (i.e. callee and caller functions) to the information provided to mod- els, noting an improvement in performance. However, if applied to vulnerability detection in the real world, it would be prohibitively expensive (each function should be tested along with its dependencies, and would therefore appear many times as input to the models). It is also limited to C/C++, and determining which dependencies to include isn't obvious. With long contexts, it's now easier to include the full codebase."}, {"title": "8 Future work", "content": "As explained at the beginning of section 4, we have only tested the simplest possible tooling based on long-context models, but expect large performance improvements to be possible."}, {"title": "A Vulnerability detection prompt", "content": "The following prompt is used for vulnerability scanning. {chunk} is replaced with a chunk of the codebase, as detailed in section 3.1, and {cwe_list} is replaced with the list of CWEs in View 1,003."}, {"title": "B LLM scoring prompt", "content": "Below is the prompt used for the LLM scorer. The first 5 examples for few-shot prompting are omitted to limit space. They can be found in this file.\nAn AI vulnerability detection tool has analyzed a project and come up with a possible lead, included below."}, {"title": "C Random examples of agreement and disagreement between human annotators and the LLM scorer", "content": null}, {"title": "C.1 LLM scored 0, human scored 0 (confidence 5/5)", "content": null}, {"title": "C.2 LLM scored 0, human scored 1 (confidence 5/5)", "content": null}, {"title": "C.3 LLM scored 1, human scored 0 (confidence 5/5)", "content": null}, {"title": "C.4 LLM scored 1, human scored 1 (confidence 5/5)", "content": null}, {"title": "D Random examples of each CVE specificity score", "content": "See section 3.2."}]}