{"title": "UniGO: A Unified Graph Neural Network for Modeling Opinion Dynamics on Graphs", "authors": ["Hao Li", "Hao Jiang", "Yuke Zheng", "Hao Sun", "Wenying Gong"], "abstract": "Polarization and fragmentation in social media amplify user biases, making it increasingly important to understand the evolution of opinions. Opinion dynamics provide interpretability for studying opinion evolution, yet incorporating these insights into predictive models remains challenging. This challenge arises due to the inherent complexity of the diversity of opinion fusion rules and the difficulty in capturing equilibrium states while avoiding over-smoothing. This paper constructs a unified opinion dynamics model to integrate different opinion fusion rules and generates corresponding synthetic datasets. To fully leverage the advantages of unified opinion dynamics, we introduces UniGO, a framework for modeling opinion evolution on graphs. Using a coarsen-refine mechanism, UniGO efficiently models opinion dynamics through a graph neural network, mitigating over-smoothing while preserving equilibrium phenomena. UniGO leverages pretraining on synthetic datasets, which enhances its ability to generalize to real-world scenarios, providing a viable paradigm for applications of opinion dynamics. Experimental results on both synthetic and real-world datasets demonstrate UniGO's effectiveness in capturing complex opinion formation processes and predicting future evolution. The pretrained model also shows strong generalization capability, validating the benefits of using synthetic data to boost real-world performance.", "sections": [{"title": "1 Introduction", "content": "Polarization in online social media fosters user bias and hostility, exacerbating societal divisions and undermining social cohesion. In recent years, the formation of opinions on social media platforms has emerged as an increasingly important issue. Opinion dynamics quantify individual views and model group opinion evolution during interactions using methods such as agent-based models and ordinary differential equations [18]. As shown in fig. 1a, in online social media scenarios, opinion interactions are constrained by an underlying graph topology. Opinion dynamics over graph structures have been extensively applied in such contexts, playing a significant role in social network analysis [7, 30], marketing [29, 40], recommendation systems [19], and various other domains.\nIn contrast to data-driven opinion prediction models, opinion dynamics models focus on interpretability, providing mechanisms for opinion formation. Opinion dynamics explore the mechanisms of group opinion evolution by designing specific opinion fusion rules [2, 8, 22], such as the Friedkin-Johnsen (FJ) model, which considers individuals' stubbornness towards their own opinions [21], and the Hegselmann-Krause (HK) model, which is based on a confidence threshold for others' opinions [24]. Additionally, opinion dynamics investigate equilibrium phenomena within the system, such as consensus, polarization, and fragmentation, implying that opinions cease to change over time [5]. Opinion dynamics offer deep insights into the process of opinion formation. However, integrating these insights into data-driven prediction models remains an open challenge.\nInspired by works such as physics-guided neural networks and graph ODEs [26, 39, 44], recent research has explored the integration of neural networks with opinion dynamics [36]. Graph neural networks (GNNs) have emerged as a natural choice for learning"}, {"title": "2 Related Works", "content": ""}, {"title": "2.1 Opinion dynamics model", "content": "Opinion dynamics investigates the formation process of agent opinions in social systems over time by setting up fusion rules. The format of opinions, fusion rules, and underlying structures are pivotal components of opinion dynamics [18]. This paper primarily concentrates on dynamic models grounded in continuous opinion forms, which offer a closer alignment with the real-world social systems[5].\nDifferent opinion dynamics models assume different fusion rules and study the phenomena of opinion formation based on these rules [42]. Based on the assumption of opinion assimilation, the"}, {"title": "3 Background", "content": "This section introduces the concept of opinion dynamics on graphs, along with several significant theoretical models of opinion dynamics. Furthermore, the discussion covers the opinion fusion rules that each of these models focuses on. Then, in section 3.2, the connections between graph neural networks and opinion dynamics on graphs are explored, and the key challenges in learning opinion dynamics for graphs are summarized."}, {"title": "3.1 Opinion dynamics models on graphs", "content": "Opinion dynamics models describe the formation process of opinions in social systems. Classical opinion dynamics models are often formulated as difference equations to describe specific opinion fusion rules. The opinions of agents evolve over time steps, where the opinion at time t + 1 is based on the fusion of opinions within the agents at time t. With continuous iterations, the opinions of agents eventually converge to certain values, a process also known as equilibrium. By proposing more complex opinion fusion rules, opinion dynamics can effectively describe opinion formation phenomena in the real world, including consensus, polarization, and fragmentation.\nIn social systems in the real world, nodes can only interact with a small subset of others. Therefore, graph-based opinion dynamics are proposed to address such opinion formation constrained by underlying graph topology. Given a social graph G = (V,E,X), where V is the set of nodes with |V| = n representing the number of nodes, each node represents an agent. & is the set of edges, with $e_{ij} \\in \\&$ representing an edge between nodes $v_i$ and $v_j$ (such as friendships, follow relationships, etc.), and X represents the opinions held by each node.\nNode Attributes. Some opinion dynamics models assume that nodes possess specific attributes that influence opinion fusion. For example, in the FJ model, each agent has a stubbornness coefficient, which represents the extent of the node's adherence to its own opinion.\n$x_{i}(t+1)=(1-\\lambda_{i}) x_{i}(0)+\\lambda_{i} \\sum_{j \\in N(v_{i})} W_{ij} x_{j}(t)$.\nHere, $\\lambda_{i} \\in [0, 1]$ represents the weight assigned to social influences during opinion fusion, and $1-\\lambda_{i}$ is the weight assigned to the agent's initial opinion value, i.e., the stubbornness of the nodes towards their initial opinion $x_{i}(0)$.\nEdge Attributes. Opinion dynamics based on confidence thresholds control the range of node interactions, essentially modifying the state of edges between nodes. For example, the HK model describes the influence of all neighbors' opinions within each agent's trust range. The condition for nodes to interact is that the difference in opinions between the two nodes must fall within a certain range,"}, {"title": "3.2 Learning opinion dynamics on graphs", "content": "When we aim to learn opinion dynamics on graphs, graph neural networks (GNNs) naturally become the preferred method for capturing this inductive bias. In this subsection, we explore the connections and differences between GNNs and graph-based opinion dynamics, and describe the key challenges in learning opinion dynamics on graphs.\nOpinion updates and node representation updates. Consider a typical graph neural network, where node representations are updated by aggregating the information from neighboring nodes. The (k + 1)-th layer can be defined as:\n$h_{i}^{k+1}=A G G^{k+1}(\\{M S G^{k+1}(h_{i}^{k}, h_{j}^{k})| j \\in N(v_{i})\\})$,\nwhere AGG(.) is the aggregation function at layer k + 1, MSG(.) is the message passing function at layer k + 1, N(vi) is the neighborhood of node vi, and $h_{i}^{k+1}$ is the representation of node vi at"}, {"title": "4 Method", "content": "To address the aforementioned issues, we propose UniGO, a unified framework based on graph neural networks for modeling opinion evolution on graphs. As illustrated in fig. 2, UniGO is trained on synthetic datasets generated under unified opinion dynamics and tested on real-world datasets. The data synthesis module consists of two components: graph construction and dynamics construction. Graph construction generates graph structures using various random graph generation methods, while dynamics construction generates evolutionary data based on unified opinion dynamics on the graph structure.\nThe UniGO model comprises three parts: coarsening, dynamics simulation, and refinement. The coarsening module uses graph pooling methods to generate a skeleton of the original graph and construct aggregated representations of the supernodes. Then, in the dynamics simulation module, graph neural networks are employed to simulate the dynamics evolution on the supernodes. Finally, in the refinement module, the supernode representations are refined back to the original nodes to complete the simulation of opinion evolution on the graph. The coarsen-refine architecture"}, {"title": "4.1 Problem definition", "content": "Given a graph G = (V,&, X\u2081), where V is the set of nodes, & is the set of edges, and X1 \u2208 Rnx\u0131 represents the opinions of n nodes at the initial t\u2081 time steps, with values in the range [0, 1]. The objective is to learn a function F to predict the node opinions for the subsequent th time steps, Xh \u2208 Rnxth."}, {"title": "4.2 Data Synthesis Module", "content": "The data synthesis module generates synthetic datasets that include various graph structures and opinion dynamics processes, addressing the scarcity of real-world opinion evolution data. Specifically, the dataset consists of two components: graph construction and dynamics construction. The graph construction component includes several random graph generation methods, such as ER random graphs, WS small-world graphs, and BA scale-free graphs[3, 6, 45], with each node receiving an initial opinion randomly. In the dynamics construction component, the unified opinion dynamics runs on the random graphs to generate the opinion evolution process:\n$x_{i}(t+1)=\\alpha_{i} x_{i}(t)+\\left(1-\\alpha_{i}\\right) \\sum_{j \\in N_{i}} W_{ij} M_{i j}^{(t)} x_{j}(t)+\\gamma_{i} n_{i}^{(t)}$.\nHere, $x_{i}(t)$ represents the initial opinion of node $i . \\alpha_{i}$ is the stubbornness coefficient of node i, Ni represents the set of neighboring nodes of node i, wij is the weight of the edge between nodes i and j, and $n_{i}^{(t)}$ is the noise for node i at time t. $M_{i j}^{(t)}$ indicates whether nodes i and j can interact in terms of opinion exchange, and is defined as follows:\n$M_{i j}^{(t)}=\\begin{cases}1, & \\text { if }|x_{i}(t)-x_{j}(t)| \\leq \\delta_{i} \\\\0, & \\text { otherwise. }\\end{cases}$\nHere, di is the confidence threshold of node i. Nodes i and j interact if the difference in their opinions is less than di.\nIn this way, traditional dynamics models can be effectively combined. For instance, when $w_{i j} M_{i j}^{(t)}=1$ and $\\gamma_{i} n_{i}^{(t)}=0$, the model reduces to the FJ model. If the stubbornness coefficients and confidence thresholds of nodes vary according to the community structure, the model represents an FJ and HK model that considers graph structure. Furthermore, by designing different noise functions, the model can flexibly simulate the process of opinion evolution under varying noise conditions.\nMore importantly, this framework comprehensively incorporates different opinion fusion rules from various dynamics models, generating more realistic synthetic datasets for opinion dynamics. Although this framework cannot analyze equilibrium states and other properties in the traditional opinion dynamics paradigm, it can be used to train graph neural network-based models for predicting opinion evolution. Details of the data synthesis approach are provided in the appendix A.1. The evaluation experiments on synthetic datasets are shown in appendix A.2"}, {"title": "4.3 UniGO Model", "content": "The UniGO model, under a coarsen-refine mechanism, uses graph neural networks to simulate the dynamics evolution on supernodes. The coarsening module coarsens the original graph through pooling methods to obtain the skeleton and construct aggregated representations of the skeleton supernodes. The dynamics evolution is simulated using a mean-aggregating graph neural network. The refinement module refines the supernode representations back to the original nodes to complete the simulation of opinion evolution on the graph.\nCoarsening Module. Graph pooling methods are used to coarsen the original graph, obtaining the skeleton and constructing aggregated representations of the skeleton supernodes. First, a graph neural network is used to aggregate the dynamic representations of the nodes:\n$x_{i}^{k+1}=A G G^{k+1}(\\{M S G^{k+1}(x_{i}^{k}, x_{j}^{k})| v_{j} \\in N(v_{i})\\})$,\nhere, AGG(.) is the aggregation function at layer k + 1, MSG(.) is the dynamic representation function at layer k + 1, and N(vi) represents the set of neighboring nodes of node vi. The initial node feature $x_{i}^{(0)}$ corresponds to the opinions from the first t\u2081 time steps. Subsequently, following [1], we use a multi-head attention-based soft clustering to compute the coarsened node representations. The core algorithm of this pooling layer is based on calculating distances from nodes to cluster centers and performing soft assignments.\nFor each node xi and cluster center $k_{j}^{(h)}$ on each attention head, the distance dij is computed using the Euclidean distance, with a temperature parameter t introduced to adjust the influence of the distance:\n$d_{ij}^{(h)}=\\left(1+\\frac{\\left|\\left|x_{i}-k_{j}^{(h)}\\right|\\right|^{2}}{\\tau}\\right)^{-\\frac{1+\\tau}{2}}$\nhere, $x_{i} \\in R^{F}$ represents the feature vector of node i, and $k_{j}^{(h)} \\in R^{F}$ represents the feature vector of cluster center j for the h-th head. t is a temperature parameter that controls the influence of the distance. The probability that node x\u012f belongs to different clusters is calculated by normalizing the distances. The probability $S_{ij}^{(h)}$ of each node being assigned to different clusters is expressed as:\n$S_{i j}^{(h)}=\\frac{d_{i j}^{(h)}}{\\sum_{k=1}^{K} d_{i k}^{(h)}}$\nhere, $S_{i j}^{(h)}$ represents the probability that node i is assigned to cluster j. In the multi-head attention mechanism, the final soft assignment matrix S is further processed through a convolution operation:\n$S=\\operatorname{softmax}(Conv2d(||H S^{(h)}||)) \\in R^{N \\times K}$\nFinally, the node features X are weighted and combined according to the assignment matrix S, resulting in the new node features H:\n$H=S^{T} X W \\in R^{K \\times F^{\\prime}}$"}, {"title": "4.4 Training", "content": "The training loss of the model consists of two parts: the KL divergence of the assignment matrix and the mean squared error (MSE)"}]}