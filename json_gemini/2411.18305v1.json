{"title": "Application of Soft Actor-Critic Algorithms in Optimizing Wastewater Treatment with Time Delays Integration", "authors": ["Esmaeel Mohammadi", "Daniel Ortiz-Arroyo", "Aviaja Anna Hansen", "Mikkel Stokholm-Bjerregaard", "S\u00e9bastien Gros", "Akhil S Anand", "Petar Durdevic"], "abstract": "Wastewater treatment plants face unique challenges for process control due to their complex dynamics, slow time constants, and stochastic delays in observations and actions. These characteristics make conventional control methods, such as Proportional-Integral-Derivative controllers, suboptimal for achieving efficient phosphorus removal, a critical component of wastewater treatment to ensure environmental sustainability. This study addresses these challenges using a novel deep reinforcement learning approach based on the Soft Actor-Critic algorithm, integrated with a custom simulator designed to model the delayed feedback inherent in wastewater treatment plants. The simulator incorporates Long Short-Term Memory networks for accurate multi-step state predictions, enabling realistic training scenarios. To account for the stochastic nature of delays, agents were trained under three delay scenarios: no delay, constant delay, and random delay. The results demonstrate that incorporating random delays into the reinforcement learning framework significantly improves phosphorus removal efficiency while reducing operational costs. Specifically, the delay-aware agent achieved 36% reduction in phosphorus emissions, 55% higher reward, 77% lower target deviation from the regulatory limit, and 9% lower total costs than traditional control methods in the simulated environment. These findings underscore the potential of reinforcement learning to overcome the limitations of conventional control strategies in wastewater treatment, providing an adaptive and cost-effective solution for phosphorus removal.", "sections": [{"title": "1. Introduction", "content": "Wastewater treatment is a critical process in maintaining environmental sustainability and public health. Traditional control methods often struggle with the complex, nonlinear, and time-varying nature of wastewater treatment plants (WWTP). Reinforcement learning (RL) offers a promising alternative: learning control policies through interaction with the environment, thus adapting to the inherent complexities and uncertainties of the process. Recent studies have demonstrated the potential of RL in optimizing energy consumption, enhancing treatment efficiency, and ensuring regulatory compliance in WWTPs [1, 2, 3, 4].\nSoft Actor-Critic (SAC) is a state-of-the-art deep reinforcement learning (DRL) algorithm known for its exceptional sample efficiency and stability in continuous action spaces [5]. Traditional policy gradient methods, such as Trust Region Policy Optimization (TRPO) [6] and Proximal Policy Optimization (PPO) [7], have advanced DRL by providing stable on-policy training. However, they generally require more samples, as they cannot reuse past experiences efficiently [8]. In contrast, SAC is an off-policy algorithm, allowing it to reuse past experiences from a replay buffer, significantly enhancing sample efficiency. SAC effectively balances exploration and exploitation by leveraging a maximum entropy framework, making it especially valuable for complex industrial applications with high exploration costs [9, 5]. Its ability to handle high-dimensional state and action spaces enables SAC to develop sophisticated control strategies, a crucial feature for wastewater treatment plants, where multiple interacting variables must be managed concurrently [3].\nOne significant challenge in applying RL to industrial processes, including WWTPs, is the presence of time delays. Time-delayed systems are prevalent in many industrial settings, and ignoring these delays can lead to suboptimal or unstable control policies [10]. Incorporating time delays into the RL framework requires advanced methods to predict future states or adapt policies for delayed feedback. Research has shown that DRL algorithms, when appropriately modified, can effectively manage time delays and improve control performance in such systems [10].\nThis paper aims to explore the application of the SAC algorithm in the context of WWTP control and optimization, addressing the challenges posed by time delays. Control policies were trained on a state-of-the-art simulator employing Long Short-Term Memory (LSTM) networks, which were previously trained and enhanced for multi-step simulations in prior studies [11, 12, 13]. Through extensive simulations and case studies, we demonstrate the efficacy of SAC in enhancing the operational efficiency and reliability of wastewater treatment processes. Three scenarios and policy algorithms were investigated, considering no delays, constant delays, or random delays in the observations and actions of the system. The results indicated that agents accounting for constant or random delays outperformed those with no delay in cumulative rewards. Furthermore, when random delays were introduced, the agent achieved better performance than with constant delays, highlighting the importance of capturing the stochastic nature of delays in environments such as WWTPs. The findings contribute to the growing knowledge achieved by applying advanced DRL methods in industrial process control, offering insights into future research directions and practical implementations."}, {"title": "2. Literature Review", "content": "Deep reinforcement learning has promising potential in the process and industrial control field as it provides sophisticated methods for handling complex, nonlinear, and high-dimensional systems. DRL algorithms like Deep Deterministic Policy Gradient (DDPG) and Proximal Policy Optimization have successfully optimized control strategies in chemical processes, enhancing efficiency and adaptability [14, 15]. For instance, DRL has been utilized to develop control policies that can handle time-varying dynamics and uncertainties in industrial environments, outperforming traditional control methods that rely on linear approximations or require extensive manual tuning [16].\nDRL has shown promise in energy management and robotics applications in industrial optimization, specifically in wastewater treatment processes. Studies have demonstrated that DRL can optimize aeration control in wastewater treatment plants, leading to significant energy savings while maintaining effluent quality standards [1]. Despite these successes, challenges such as sample efficiency, safety concerns, and the need for large data for training persist. Ongoing research addresses these issues by developing more efficient algorithms and integrating DRL with model-based approaches to enhance real-world applicability [9]. Some of the most used DRL algorithms for continuous control with their advantages and disadvantages are summarized in Table 1."}, {"title": "2.1. Time Delays", "content": "In control systems, constant and random delays in action and observation present significant challenges, particularly in real-time applications such as robotics and remote control. Constant delays, where there is a fixed time lag between action execution or observation and their corresponding feedback, can often be accounted for in system design. However, they still complicate accurate state estimation and timely decision-making [17].\nOn the other hand, random delays vary unpredictably due to factors like network latency or processing load, introducing greater complexity. These delays can disrupt the synchronization between the system's actions and its perception of the environment, leading to degraded performance or instability. As highlighted in [18] and [19], random delays violate the Markov assumption typically required in control and reinforcement learning algorithms. This necessitates advanced techniques-such as delay compensation or predictive modeling-to maintain system performance. The unpredictable nature of random delays makes them particularly challenging, requiring robust strategies to mitigate their impact on control accuracy and system reliability.\nThe study in [10] addresses action and observation delays in reinforcement learning environments, typical in real-world applications such as remote control operations. The authors introduce the Delay-Correcting Actor-Critic (DCAC) algorithm and the concept of Random-Delay Markov Decision Processes (RDMDPs), which extend traditional Markov Decision Processes (MDP) to incorporate random delays. The DCAC algorithm uses off-policy multi-step value estimation combined with a trajectory resampling technique to transform off-policy trajectories into on-policy ones, reducing the bias introduced by delays and enabling more accurate value estimation. The approach allows off-policy algorithms, such as Soft Actor-Critic, to remain sample-efficient while adapting to variable delays. Experimental results show that DCAC outperforms standard methods like SAC, particularly in challenging scenarios with unpredictable delays.\nIn specific industrial processes, a common challenge arises from different actions incurring different delays, such that the delays between actions and observations may vary for each action. As illustrated in Figure 1, when multiple actions are determined at a single step, the effects of some actions may appear in the observations sooner than others. At each time step i, the agent generates a set of actions denoted as $a_{i} = \\{a_{i}^{1}, a_{i}^{2}, a_{i}^{3}\\}$, where $a_{i}^{1}, a_{i}^{2}$, and $a_{i}^{3}$ represent individual components of the overall action vector. These actions are determined based on the current state of the system $s_{i}$, which encapsulates the system's conditions at time step i. The system is characterized by three different actions, each influenced by random action and observation delays, adding complexity to the control dynamics. The agent's actions can be expressed as:\n$a_{i} = \\pi(s_{i-d_{s}})$, (1)\nWhere $\\pi$ is the policy learned by the agent, and $d_{s}$ represents a random observation delay. Each action component is then defined as:\n$a_{i}^{1} = f^{1}(s_{i-d_{1}}), a_{i}^{2} = f^{2}(s_{i-d_{2}}), a_{i}^{3} = f^{3}(s_{i-d_{3}})$, (2)\nIn the above equation, $f^{1}$, $f^{2}$, and $f^{3}$ are functions representing the mapping from the delayed state $s_{i-d}$ to each action component under the policy $\\pi$. Once computed, the system executes these actions with a random action delay $d_{a}$, resulting in the effective action at the system being:"}, {"title": "2.2. Contributions", "content": "This study introduces a novel application of the Soft Actor-Critic algorithm for optimizing phosphorus removal in wastewater treatment plants with integrated time delay handling. Key contributions of this work include:\n1. Application of SAC in WWTP Optimization: This study applies the SAC algorithm within a high-dimensional, time-delayed WWTP simulation, leveraging SAC's sample efficiency and stability to improve control strategies over conventional Proportional-Integral-Derivative (PID) controllers. This demonstrates the feasibility of SAC as a scalable and adaptive approach for industrial process control in WWTPs."}, {"title": "3. Methods", "content": "A Soft Actor-Critic algorithm was trained on a custom environment designed with OpenAI's Gym library [20] to optimize phosphorus removal in wastewater treatment plants. While the DCAC algorithm has demonstrated superior performance in handling random delays [10], the Soft Actor-Critic algorithm was selected due to its established stability, ease of implementation, and computational efficiency [5]. Additionally, SAC's entropy-regularized framework facilitates robust exploration, which is crucial for managing high-dimensional action spaces in WWTP simulations. The environment utilized an LSTM model, which was trained on the plant's dataset to predict the next state of the system based on current state-action pairs, with the predicted observation and calculated reward being returned to the agent. The training was performed using the SAC algorithm implemented in PyTorch [21], and a multi-environment approach was adopted to accelerate the learning process. The Gym asynchronous vectorized environment was used to simulate multiple parallel environments."}, {"title": "3.1. The Plant and Dataset", "content": "This study focuses on the data from Kolding Central WWTP in Agtrup, Denmark. The time-series dataset for the period of two years was collected through the HubgradeTM Performance Plant system, designed by Kr\u00fcger/Veolia [22]. The plant utilizes a combination of biological and chemical methods for phosphorus removal. Chemical phosphorus removal is achieved by adding metal salts at two points within the plant. The first dosing point is located after the primary clarifier and before the biological treatment line, where iron sulfide (JSF) is added. The second dosing point is situated after the biological tank and before the secondary clarifier, where polyaluminum chloride (PAX) is introduced to the stream. Data preprocessing played a crucial role in enhancing model performance. The raw data was initially normalized using the Min-Max technique, scaling the features to a range of 0 to 1. Feature selection was guided by principal component and correlation analysis. Variables of the system that demonstrated the highest correlation with the target variable, Phosphate concentration, were selected as inputs to the model, in conjunction with the target variable itself and the action variables, two points of metal salts dosage. More information about the plant, dataset, and preprocessing is explained in [11, 23]."}, {"title": "3.2. Simulation Environment", "content": "The simulation environment was developed using OpenAI's Gym library [20]. The action space was defined as vector containing metal salts dosage values:\n$a_{i} = \\{Q_{JSF}^{v,i}, Q_{PAX}^{v,i}\\}$ (4)\nWhere $Q_{JSF}^{v,i}$ and $Q_{PAX}^{v,i}$ represent the agent's control variables (metal salts dosage) at time step i, influencing the system's dynamics. The observation space at time step i was a composite vector defined as:\n$s_{i} = \\{x_{e,i}, C_{i}, sin(h_{i}), cos(h_{i}), sin(d_{i}), cos(d_{i}), sin(m_{i}), cos(m_{i})\\}$ (5)\nWhere $x_{e,i}$ is the vector of exogenous variables (as explained in [23, 13]) at time i, $C_{i}$ is the phosphorus concentration at time i, and $h_{i}, d_{i}$, and $m_{i}$ represent the hour of the day, day of the week, and month of the year, respectively, encoded as cyclical time features using sine and cosine transformations.\nA refined version of the LSTM model, enhanced for more accurate multi-step simulations as described in [11, 12, 13], was employed as a predictor to forecast the system's next state. After each prediction, the observation and reward were returned to the agent. Utilizing the LSTM predictor, the core environment provided only the predicted observation for the next state based on the agent's actions.\nInspired by the approach in [10], custom wrappers were created to handle the complexity introduced by random action and observation delays. These wrappers expanded the observation space by incorporating additional variables, such as an action buffer, set to the maximum of the observation and action delays, and separate delay indicators for both action and observation delays. This design ensured the agent could adapt effectively to the asynchronous nature of the environment while maintaining robustness in optimizing the control process."}, {"title": "3.3. Reward Function", "content": "The reward function was designed to train a Soft Actor-Critic algorithm for optimizing phosphorus removal in a wastewater treatment plant. The function evaluates the cost-effectiveness of controlling phosphorus concentration while minimizing financial penalties, taxes, and action costs.\n3.3.1. Reward Calculation Overview\nThe reward function is structured to minimize the total cost associated with phosphorus control, including penalties for deviations from a target phosphorus concentration, taxation based on the phosphorus load, and costs for applying chemicals (JSF and PAX). The overall reward is calculated as follows:\n$r = -C_{total} (1 + P_{coef})$ (6)\nWhere r is the reward, $C_{total}$ includes chemical costs and tax, and $P_{coef}$ is a coefficient calculated by the penalty function based on the deviation from the target phosphorus concentration.\n3.3.2. Total Cost Calculation\nAction Costs. The action costs include the expenses for the chemical precipitants JSF and PAX, calculated as follows:\n$C_{JSF} = P_{rJSF} \\frac{Q_{JSF} \\cdot t_{dose}}{60}$ (7a)\n$C_{PAX} = P_{rPAX} \\frac{Q_{PAX} \\cdot t_{dose}}{60}$ (7b)\nHere, $C_{JSF}$ and $C_{PAX}$ represent the costs of the metal salts, while $P_{rJSF}$ and $P_{rPAX}$ denote their respective prices in currency per liter, which are considered to be 0.20 and 3.54 Danish kroner per liter based on the Danish market. Additionally, $Q_{JSF}$ and $Q_{PAX}$ are the flow rates of the metal salts in liters per hour, and $t_{dose}$ is the dosing duration in minutes, corresponding to the sampling frequency in the dataset.\nTax Costs. The tax is calculated based on the phosphorus load in the wastewater. The phosphorus mass, $M_{P}$, in kilograms is determined using:\n$M_{P} = C_{P} \\cdot Q \\cdot \\frac{t_{dose}}{60000}$ (8)\nWhere $M_{P}$ represents the mass of phosphorus in kilograms (kg), $C_{P}$ is the phosphorus concentration in milligrams per liter (mg/L), and Q is the flow rate of wastewater in cubic meters per hour (m\u00b3/h). Given that $C_{P}$, Q, and $t_{dose}$ are expressed in mg/L, m\u00b3/h, and minutes, dividing by 60000 ensures that the result is in kilograms. Using this calculated phosphorus mass, the tax cost can then be determined as follows:\n$T = T_{rate} \\cdot M_{P}$ (9)\nHere, T and $T_{rate}$ are the amount of tax paid by the plant and the green tax rate in currency per kilogram of phosphorus (183.64 Danish Krones per kilogram in Denmark). Finally, the total cost of each dosing step is calculated as follows:\n$C_{total} = C_{JSF} + C_{PAX} + T$ (10)\n3.3.3. Penalty Coefficient Calculation\nThe penalty function measures the deviation from an ideal phosphorus concentration target and varies depending on the approach used (Linear or Non-linear):"}, {"title": "Linear Penalty", "content": "For the Linear approach, the penalty is calculated as:\n$P_{coef} = \\begin{cases}\n0, & \\text{if } 0 < x \\leq x_{ideal} \\\\\n-100 \\cdot T, & \\text{otherwise}\n\\end{cases}$ (11)\nWhere x represents the current phosphorus concentration and $x_{ideal}$ is the ideal target level for the phosphorus concentration, which should be under the emission limit according to the regulations."}, {"title": "Non-linear Penalty", "content": "For the Non-linear approach, the penalty is computed using a custom exponential function:\n$P_{coef}(x) = a \\cdot e^{z \\cdot x + c} + d$ (12)\nWhere a, z, c, and d are parameters controlling the shape and scale of the exponential penalty, and x is the phosphorus concentration. The parameters were selected to make the penalty relatively mild when the concentration is below the ideal target level and near zero. Still, the penalty becomes steeper and more severe as the concentration exceeds the limit. A non-linear penalty was selected for this study because it increases progressively with concentration levels, allowing for more significant differentiation between minor and major violations."}, {"title": "3.3.4. Summary of the Reward Function", "content": "The overall reward function thus integrates costs and penalties to guide the SAC algorithm towards actions that minimize deviations from optimal phosphorus levels while keeping treatment costs low. According to the equations 6, 10, and 12, the structure of this reward function can be defined as:\n$r_{t}(x) = -(C_{JSF,t} + C_{PAX,t} + T_{t}) \\cdot (1 + a \\cdot e^{z \\cdot x_{t}+c} + d)$ (13)\nThis formulation ensures that higher costs or deviations from the target concentration result in lower rewards, thereby effectively incentivizing the agent to optimize the phosphorus removal process."}, {"title": "3.4. Soft Actor-Critic", "content": "Actor-critic methods are a class of reinforcement learning algorithms that involve two primary components: an actor (policy) network and a critic (value) network. The actor network is responsible for selecting actions given the current state, while the critic network evaluates the quality of those actions by estimating the value function. Soft Actor-Critic is an actor-critic algorithm incorporating entropy maximization to encourage exploration [5]."}, {"title": "3.4.1. Key Components and Equations in SAC", "content": "Policy Objective with Entropy Regularization. The SAC algorithm's policy objective maximizes the expected return while encouraging exploration by maximizing the entropy of the policy. This is formulated as [5]:\n$J_{\\pi}(\\Phi) = E_{s_{t} \\sim D, a_{t} \\sim \\pi_{\\Phi}}[\\alpha H(\\pi(\\cdot|s_{t})) - Q_{\\theta}(s_{t}, a_{t})]$ (14)"}, {"title": "3.5. Training Procedure", "content": "The SAC agent was set up with one actor and two critics for training, with each critic independently estimating the expected return for the current policy. During each training step, the agent applies a set of actions and receives the next state and reward from the environment. The two critics provide separate Q-value estimates to reduce overestimation bias by taking the minimum value during policy updates, enhancing training stability. The observation (state) space varies depending on whether delays are accounted for. In scenarios with delays, the state space includes delay-related variables. Figure 2 illustrates the overall training procedure, highlighting the inclusion of delay variables where applicable. To leverage the parallelism offered by the asynchronous vector environment, we configured 16 separate environments, and each initialized with different settings to ensure a diverse range of initial observations and episode lengths. This diversity was intended to improve the robustness of the training process by exposing the SAC algorithm to varied starting conditions and episode dynamics. The 16 environments were divided into four experimental setups, with four environments assigned to each setup:\nE1 - Constant-Length, Consecutive Episodes: This baseline experiment used episodes of equal length, sampled sequentially from the time-series data. It aimed to evaluate the model's performance under a stable and predictable training regime.\nE2 - Random-Length, Consecutive Episodes: Episodes followed a sequential order but varied in length, introducing unpredictability in the steps per episode. This experiment aimed to simulate real-world scenarios with fluctuating sequence steps.\nE3 - Random Start, Constant-Length Episodes: Episodes of constant length were initialized at random points within the dataset. This setup tested the algorithm's ability to learn from unsequentially organized data, introducing randomness in the initial state while keeping the number of steps consistent.\nE4 - Random Episodes with Random Lengths: Both the starting points and lengths of episodes were randomized. This scenario challenges the algorithm's adaptability to an environment where the initial state and the number of steps are highly variable."}, {"title": "3.5.1. Incorporation of Delays", "content": "Three different policy learning scenarios were studied: No Delay, Constant Delay, and Random Delay. In the No Delay scenario, the observation space included only the predicted state of the system returned by the environment. In contrast, the Constant Delay and Random Delay scenarios incorporated additional information in the observation space, such as the action buffer (Ba), observation delay (w), and action delay (\u03ba). For the Constant Delay scenario, the delay at each step for both action and observation was set to a constant value equal to the maximum of the delay ranges. In the Random Delay scenario, the action and observation delays were sampled randomly from their respective ranges at each step, introducing variability in the timing of actions and observations. This setup was designed to evaluate the algorithm's ability to learn robust policies under different levels of temporal uncertainty.\nA concise overview of the SAC training procedure, including the integration of delay handling, is presented in Algorithm 1."}, {"title": "3.6. Comparison to the Current Controller", "content": "In this study, the real-time PID controller currently operating in the wastewater treatment plant, which could dynamically produce actions in response to system states, was not accessible. Instead, historical data were utilized to replicate the past PID controller's actions for similar system states.\nSpecifically, for a given input sequence, we assumed that the PID controller would make the same decisions as it did historically for the corresponding state. The actions taken by the PID controller at time t were retrieved from the dataset, where they were initially logged as the controller's response to the system state st. This array of action variables effectively represents the PID's decision-making for the sequence in question. Essentially, the actions produced by the PID controller at time t are assumed to be an array of the system's past actions at that time:\n$A_{PID,t} = \\{Q_{JSF}^{v,t}, Q_{PAX}^{v,t}\\}$ (17)\nIn the above equation, $A_{PID,t}$, $Q_{JSF}^{v,t}$, and $Q_{PAX}^{v,t}$ represent the actions produced by the PID controller, the flow of the iron salts dosage, and the flow of the PAX dosage at time t.\nThe same input sequence was provided to all controllers to compare the performance of the PID controller and the Soft Actor-Critic agents. SAC agents generated actions based on the observed system state, while the PID actions were directly taken from the dataset. The actions of both the PID controller and SAC agents were then sent to the custom simulator, which produced the next state of the system, enabling a side-by-side evaluation of their performance."}, {"title": "3.7. Software and Hardware", "content": "All of the tests for the simulation environment are implemented in programming language Python by using the Gym [20] and PyTorch [24] libraries. The AI Cloud service from Aalborg University is used for GPU-based computations. The available compute nodes are each equipped with 2 \u00d7 24-core Intel Xeon CPUs, 1.5 TB of system RAM, and 16 NVIDIA Tesla V100 GPUs with 32 GB of RAM each, all connected via NVIDIA NVLink. One compute node consisting of one GPU and 32 CPU cores was used to train each experiment."}, {"title": "4. Results", "content": "4.1. Policy Learning\nThe training of the Soft Actor-Critic algorithm was conducted across three distinct scenarios: No Delay (ND), Constant Delay (CD), and Random Delay (RD). Each scenario aimed to evaluate the impact of different temporal dynamics on the performance of the control policies. Figure 3 illustrates the episodic average reward per step versus the number of total environment steps the agent takes. The results indicate that delay-aware models (CD and RD) significantly outperformed the No Delay scenario. This performance gap emerged after approximately 10 million training steps, where the agents accounting for delays began to accumulate higher rewards.\nThe Random Delay scenario demonstrated the highest cumulative rewards across training, indicating that the SAC agent was better able to handle the variability in delays between actions and their effects. This result is particularly noteworthy because real-world wastewater treatment plants often experience fluctuating delays due to various factors, including mechanical issues and sensor delays. The ability to manage such uncertainties is crucial for maintaining optimal plant operation.\nThe agent struggled to perform in the No Delay scenario, accumulating lower rewards throughout training. This outcome can be attributed to the model's inability to account for the inherent delays in WWTP operations, leading to suboptimal action choices and less efficient phosphorus removal.\nThe reward structure was designed to balance multiple factors, including minimizing chemical costs and meeting phosphorus concentration targets. In the CD and RD scenarios, the SAC algorithm learned policies that more effectively struck this balance, using fewer resources while maintaining regulatory compliance. The results underscore the importance of incorporating delay mechanisms into reinforcement learning models, especially in industrial systems where time lags can significantly impact process performance."}, {"title": "4.2. Comparison with the Existing Control Policy", "content": "To further evaluate the effectiveness of the learned policies, the best-performing SAC agents (ND, CD, RD) were compared against the PID controller currently used at the WWTP. The PID controller, a common choice in industrial control systems, operates reactively, relying on historical data to address deviations from a setpoint without forecasting future changes. The comparison methodology, detailed in Section 3.6, involves replicating the actions taken by the PID controller for the same initial state in the past. For a given historical initial state, the performance of the trained agents was analyzed in the simulation environment. As illustrated in Figure 4, the environment predicts the output or target variables at each simulation step based on the control inputs generated by the agents and the exogenous variables from the dataset corresponding to that time step.\nFigure 5 presents the performance comparison, focusing on two critical aspects: the concentration of phosphate in the effluent and the control actions involving the dosage of metal salts. The SAC agents, particularly the one trained under random delay conditions, demonstrated superior control over the phosphate levels. The SAC policies maintained the phosphate concentration closer to the target value, with fewer fluctuations than the PID controller. This ability to keep the system stable is crucial, as even minor deviations from the target phosphorus level can result in penalties or increased operational costs.\nRegarding resource efficiency, the SAC agents also outperformed the PID controller. The learned policies more effectively optimized the dosage of metal salts, reducing the overall consumption of chemicals while maintaining adequate phosphorus removal. This improvement translates into cost savings for the plant, as chemicals like iron chloride and polyaluminum chloride represent a significant portion of the operational expenses in phosphorus removal.\nDuring the evaluation, step-by-step rewards were calculated for both control strategies. The SAC agents consistently achieved higher rewards, indicating that their actions were more cost-effective and better aligned with the WWTP's operational goals. The SAC agents' ability to adapt to fluctuating conditions, particularly in the RD scenario, highlights the potential of deep reinforcement learning (DRL) to improve process control in complex, dynamic environments like wastewater treatment plants."}, {"title": "5. Discussion", "content": "The results of this study underscore the critical importance of accounting for time delays in optimizing control policies for wastewater treatment plants. Due to the systems' complexity, time delays between actions and their observed effects are inevitable in industrial processes like WWTPs. Ignoring these delays, as seen in the no delay scenario, leads to suboptimal control performance, as the agent cannot make informed decisions based on delayed feedback.\nFigure 3 emphasizes that incorporating constant and random delays into the SAC training process improved the agent's ability to optimize phosphorus removal. The RD scenario, in particular, stands out as the most effective, demonstrating the highest rewards and the most efficient resource usage. This suggests that the variability in delay times, a common occurrence in real-world systems, is best handled by the agents adapting to changing conditions rather than assuming a fixed delay structure."}, {"title": "5.1. Effect of Delay Incorporations", "content": "Time delays are inherent in wastewater treatment processes due to the plant's complex system dynamics and slow response times. When these delays are not explicitly accounted for in the reinforcement learning framework, the agent's ability to make informed and effective decisions is compromised.\nAs illustrated in Figures 3 and 5, failing to account for these delays caused the reinforcement learning agent to make decisions that were poorly synchronized with the system's true state. This misalignment resulted in suboptimal action selection, as the agent struggled to anticipate the delayed effects of its actions on system responses. Consequently, the system experienced inefficiencies in phosphorus removal, characterized by greater fluctuations in phosphate concentrations and increased operational costs.\nFor example, in the no delay scenario, the agent's actions were based solely on the immediate state observations, disregarding the plant's temporal dynamics. This resulted in reactive rather than anticipatory behavior, where the agent failed to compensate adequately for the lagged effects of previous actions. As a result, the control policy struggled to maintain stable phosphorus levels within regulatory limits, demonstrating the critical importance of incorporating delay mechanisms in reinforcement learning models for dynamic and time-sensitive systems like wastewater treatment plants."}, {"title": "5.2. Comparison to the Current Controller", "content": "Figure 5 and Table 2 highlight a trade-off between costs and target deviations among the control strategies. While the PID controller sometimes achieves lower total costs, such as 41% lower than SAC-ND and 26% lower than SAC-CD, it shows a significantly higher target deviation, with 77% more deviation than SAC-RD and 59% more than SAC-CD. This deviation indicates a greater risk of exceeding the phosphorus effluent limits set by regulations.\nMeeting effluent target limits is the primary objective in phosphorus removal, as non-compliance can result in penalties and environmental harm. Cost optimization becomes relevant only when regulatory compliance is guaranteed. The SAC-RD agent demonstrates a clear advantage by achieving the lowest target deviation (5.6%), which is 76.9% lower than PID, while simultaneously reducing total costs by 9% compared to PID. SAC-CD also reduces the deviation by 59% but at a higher cost.\nThese findings underscore the limitations of the PID controller in balancing regulatory compliance and cost efficiency. SAC agents, particularly SAC-RD, provide superior control performance by maintaining phosphorus levels within regulatory limits and optimizing costs, making them better suited for real-world wastewater treatment applications."}, {"title": "5.3. Practical Implications", "content": "The findings of this study have significant practical implications for the control of WWTPs and similar industrial processes. The ability to optimize control strategies in the presence of delays can lead to more efficient operations, reduced costs, and improved regulatory compliance. The results suggest that WWTPs could benefit from transitioning from traditional PID controllers to DRL-based systems like SAC, which are better equipped to handle the complexities and uncertainties inherent in wastewater treatment.\nAdditionally, the study highlights the potential for DRL to be integrated with existing control systems. While the SAC algorithm outperformed the PID controller in this study, there may be scenarios where a hybrid approach could be beneficial. For example, the PID controller could handle routine operations while the DRL system manages more complex decision-making tasks that require anticipation of future states."}, {"title": "6. Conclusions", "content": "This study presented a comprehensive framework for optimizing phosphorus removal in wastewater treatment plants using the Soft Actor-Critic algorithm, emphasizing handling time delays in system dynamics. Incorporating constant and random delays into the reinforcement learning framework proved essential for improving the robustness and performance of control policies in dynamic and uncertain environments. By addressing these delays, the SAC agents demonstrated significant improvements in maintaining phosphorus levels within regulatory limits while optimizing operational costs.\nThe results highlight the effectiveness of delay-aware models, particularly under random delay scenarios, which align more closely with the stochastic nature of real-world systems. The SAC-RD agent achieved the highest performance, reducing target deviations by 77%, lowering total costs by 9%, and decreasing phosphorus emissions by 36% compared to traditional PID controllers. These findings underscore the importance of flexible and adaptive control strategies, as well as the ability of advanced RL algorithms to anticipate system responses and dynamically adjust control actions.\nThe integration of LSTM simulators and custom delay-handling mechanisms enhanced the RL framework, enabling realistic training and evaluation of control policies. This study provides a foundation for applying reinforcement learning in industrial processes, showcasing its ability to adapt to complex, multi-variable environments like WWTPS.\nFuture research could explore the integration of multi-objective optimization to more effectively balance environmental and economic objectives. Additionally, combining RL with model-based control approaches or hybrid systems could enhance scalability and robustness, paving the way for broader applications in industrial process control.\nBy addressing the challenges of delayed feedback and optimizing chemical usage, this study contributes to the advancement of sustainable wastewater treatment practices, offering a path toward more efficient and cost-effective solutions for environmental management."}]}