{"title": "RAG Playground: A Framework for Systematic Evaluation of Retrieval Strategies and Prompt Engineering in RAG Systems", "authors": ["Ioannis Papadimitriou", "Ilias Gialampoukidis", "Stefanos Vrochidis", "Ioannis (Yiannis) Kompatsiaris"], "abstract": "We present RAG Playground, an open-source framework for systematic evaluation of Retrieval-Augmented Generation (RAG) systems. The framework implements and compares three retrieval approaches: naive vector search, reranking, and hybrid vector-keyword search, combined with ReAct agents using different prompting strategies. We introduce a comprehensive evaluation framework with novel metrics and provide empirical results comparing different language models (Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our experiments demonstrate significant performance improvements through hybrid search methods and structured self-evaluation prompting, achieving up to 72.7% pass rate on our multi-metric evaluation framework. The results also highlight the importance of prompt engineering in RAG systems, with our custom-prompted agents showing consistent improvements in retrieval accuracy and response quality.", "sections": [{"title": "1 INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) have revolutionized natural language processing capabilities. However, these models face significant challenges, including hallucination, knowledge cutoff dates, and limited context windows [1], [2]. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, combining the generative capabilities of LLMs with information retrieval to provide grounded, accurate responses [3], [4]."}, {"title": "1.1 Background and Motivation", "content": "RAG systems enhance LLM responses by retrieving relevant information from a knowledge base before generation. This approach offers several advantages: reducing hallucination, enabling access to domain-specific knowledge, and providing verifiable sources for generated responses [3], [4]. However, implementing effective RAG systems presents complex challenges in both retrieval accuracy and response generation.\nA critical challenge in RAG implementation lies in the retrieval strategy selection. Vector-based semantic search, while powerful for capturing semantic relationships, may miss lexically important matches. Conversely, keyword-based approaches might fail to capture semantic similarities [5]\u2013[7]. Additionally, the interaction between retrieval mechanisms and LLM prompt engineering significantly impacts system performance, yet this relationship remains under-studied."}, {"title": "1.2 Challenges in RAG Systems", "content": "Current RAG systems face several key challenges:\nRetrieval Quality: Balancing semantic and lexical matching in document retrieval remains difficult. Pure vector search might miss crucial keyword matches, while simple keyword matching could overlook semantic relationships [6].\nContext Integration: Effectively prompting LLMs to utilize retrieved context without losing coherence or introducing contradictions presents significant challenges. [8], [9]\nEvaluation Complexity: Traditional IR metrics may not fully capture RAG system performance, which requires evaluating both retrieval accuracy and response quality [10], [11]\nSystem Optimization: The interplay between retrieval mechanisms and prompt engineering creates a complex optimization space that remains largely unexplored [2], [12]."}, {"title": "1.3 Research Objectives", "content": "This paper aims to address these challenges through systematic evaluation and comparison of different RAG strategies. Specifically, we seek to:\n1) Compare the effectiveness of different retrieval strategies, including naive vector search, reranking, and hybrid approaches\n2) Analyze the impact of structured prompt engineering on RAG performance\n3) Develop comprehensive evaluation metrics that capture both retrieval and generation quality\n4) Provide empirical evidence for the effectiveness of different RAG configurations across multiple LLMs"}, {"title": "1.4 Contributions", "content": "Our work makes the following contributions:\n1) RAG Playground Framework: We present an open-source framework for implementing and evaluating different RAG strategies, facilitating reproducible research and systematic comparisons.\n2) Comprehensive Evaluation Metrics: We introduce a novel evaluation framework combining programmatic, LLM-based, and hybrid metrics, including a new \"completeness gain\" metric for assessing response quality beyond ground truth coverage.\n3) Empirical Analysis: We provide extensive experimental results comparing different retrieval strategies and prompt engineering approaches across two state-of-the-art LLMs (Llama 3.1 and Qwen 2.5).\n4) Prompt Engineering Insights: We demonstrate the significant impact of structured self-evaluation prompting on RAG performance, achieving up to 72.7% pass rate on our comprehensive evaluation framework.\nOur results demonstrate that hybrid search strategies, combined with carefully engineered prompts, can significantly improve RAG system performance. The findings also highlight the importance of considering both retrieval strategy and prompt engineering in RAG system design.\nThe rest of this paper is organized as follows: Section II reviews related work in RAG systems, retrieval strategies, and evaluation methods. Section III describes our system architecture, including retrieval strategies and prompt engineering approaches. Section IV presents our evaluation framework and metrics. Section V details our experimental setup, followed by results and analysis in Section VI. We discuss implications and future directions in Section VII before concluding in Section VIII."}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 RAG Systems and Retrieval Strategies", "content": "Retrieval-Augmented Generation has emerged as a crucial approach for enhancing LLM responses with external knowledge [13], [14]. Early RAG implementations focused on simple vector-based retrieval using dense encoders [7]. Recent work has explored various retrieval enhancement techniques. [15] demonstrated the effectiveness of hybrid retrieval combining dense and sparse representations. [16] showed that reranking retrieved passages using cross-encoders can significantly improve retrieval quality.\nSeveral studies have investigated the impact of chunk size and overlap in RAG systems [17], [18]. The trade-off between context window utilization and information relevance remains an active area of research. Hybrid approaches combining multiple retrieval strategies have shown promise in both academic research and industrial applications [5]."}, {"title": "2.2 RAG Evaluation Frameworks", "content": "Several frameworks have been developed for evaluating RAG systems. RAGAS [10] from Microsoft Research provides a comprehensive suite of metrics focusing on faithfulness, relevance, and contextual precision. The framework relies on frontier models like GPT-4 for evaluation, making it powerful but potentially costly and less accessible for some users. LlamaIndex [19] offers built-in evaluation tools that focus primarily on retrieval quality and response faithfulness, though with a more limited metric set compared to specialized frameworks. More recent frameworks have emerged targeting specific aspects of RAG evaluation. OpenAI's evals framework [20] provides infrastructure for evaluating language models, including RAG applications, but requires significant setup and cloud resources. Other notable contributions include [21]'s work on evaluation dataset generation and [3]'s focus on self-reflection critique. Our framework differs from existing solutions in several key aspects:\nFully local implementation, eliminating dependence on cloud-based frontier models\nComprehensive metric suite combining programmatic, LLM-based, and novel hybrid approaches\nIntegrated evaluation of both retrieval strategies and prompt engineering impact\nFocus on accessibility and reproducibility through consumer hardware compatibility\nNovel metrics like completeness gain that assess potential improvements beyond ground truth\nThis positioning makes our framework particularly well-suited for academic and research applications, where reproducibility, methodological transparency, and complete control over the evaluation pipeline are essential. The ability to run comprehensive evaluations on consumer hardware with minimal cost (about $4 per full evaluation suite) enables systematic ablation studies and thorough comparative research that might be prohibitively expensive with cloud-based solutions. These characteristics make it an ideal tool for researchers investigating RAG system optimization, developing new retrieval strategies, or conducting rigorous comparative studies of different approaches."}, {"title": "2.3 ReAct Agents and Prompt Engineering", "content": "The ReAct framework, which combines reasoning and acting in LLM agents, has shown significant potential for complex task completion. Originally designed for task decomposition, ReAct has been adapted for various applications, including information retrieval and question answering [22].\nPrompt engineering has emerged as a crucial factor in LLM performance [23]. Recent work has explored structured prompting approaches [24], [25] and the impact of self-evaluation in LLM responses [3]. However, the intersection of prompt engineering and retrieval strategy optimization in RAG systems remains relatively unexplored."}, {"title": "2.4 Evaluation Methods in RAG Systems", "content": "Traditional IR metrics like precision and recall have been adapted for RAG system evaluation [26]. However, these metrics often fail to capture the nuanced requirements of combined retrieval and generation tasks. Recent work has proposed various approaches to RAG evaluation, including relevance assessment [10] and faithfulness metrics [27].\nThe use of LLMs for evaluation has gained attention, with several studies demonstrating their effectiveness in assessing response quality [28]. Instead of relying purely on automated metrics, hybrid evaluation approaches combining automated and human assessment are valuable for validating the evaluation framework itself, ensuring that automated metrics align well with human judgment and capture real-world performance characteristics."}, {"title": "2.5 Retrieval Enhancement Techniques", "content": "Several techniques have been proposed to enhance retrieval quality in RAG systems. Document chunking strategies [29] and embedding model selection [30], [31] have been shown to significantly impact retrieval performance. Semantic search improvements through better vector representations and efficient indexing methods [32] continue to advance the field.\nThe integration of traditional IR techniques with neural approaches has produced promising results. BM25 and other lexical matching methods have been successfully combined with dense retrievers [15], while attention mechanisms have been adapted for document retrieval [7], [15], [33]."}, {"title": "3 SYSTEM ARCHITECTURE", "content": ""}, {"title": "3.1 Retrieval Strategies", "content": "Our framework implements three distinct retrieval strategies, each building upon the previous to address specific limitations:"}, {"title": "3.1.1 Naive Vector Search", "content": "The base implementation uses dense vector embeddings generated by the BAAI/bge-base-en-v1.5 model [34]. Documents are split into chunks using a sentence-based approach with a chunk size of 256 tokens and 50-token overlap. Each chunk is embedded and stored in a vector index. During retrieval, the query is embedded in the same space, and the top-k most similar chunks are retrieved using cosine similarity. This approach excels at capturing semantic relationships but may miss important lexical matches."}, {"title": "3.1.2 Reranking Enhancement", "content": "Building on the naive approach, we add a cross-encoder reranking step using the cross-encoder/ms-marco-MiniLM-L-2-v2 model [35]. The initial retrieval fetches a larger candidate set (k=20) which is then reranked to select the most relevant chunks (n=4). This two-stage approach helps mitigate the limitations of pure vector similarity by performing a more detailed relevance assessment on the candidate set."}, {"title": "3.1.3 Hybrid Search Implementation", "content": "Our hybrid approach combines vector similarity with keyword-based search using a custom retriever implementation. The system performs both vector similarity search and keyword matching (BM25-style) in parallel, then combines the results using either an AND or OR operation. This strategy provides two key advantages:\nCaptures both semantic relationships and exact keyword matches\nAllows flexibility in result combination through AND/OR operations"}, {"title": "3.2 ReAct Agent Configuration", "content": "All agents in our framework are based on the ReAct architecture, with variations in their prompting and context handling:"}, {"title": "3.2.1 Base ReAct Implementation", "content": "The base implementation uses standard ReAct prompting with a simple tool description for document search. The agent follows the thought-action-observation cycle, with minimal additional context or constraints. The system prompt includes:\nBasic tool descriptions\nStandard ReAct format instructions\nGeneral guidelines for using search results"}, {"title": "3.2.2 Structured Self-Evaluation Prompting", "content": "Our custom implementation enhances the base ReAct agent with structured self-evaluation capabilities through modified system prompts. Key additions include:\nExplicit step-by-step reasoning format\nRequired confidence scoring for retrieved information\nStructured analysis of search results quality\nThe prompt structure enforces systematic evaluation:\nThought: Let me analyze step-by-step:\n1. Current Status:\nProgress: what's been tried]\nFindings: what's been found]\nMissing: [what's still needed]\n2. Strategy:\nNext tool: [selected tool]\nExpected outcome: (what we hope to find]\nConfidence: (current confidence score 0-1]\nReasoning: (why this choice]"}, {"title": "3.2.3 Context Engineering for Retrieval", "content": "The system includes specialized context prompting focused on optimizing retrieval behavior. Key components include:\nGuidelines for keyword-based search refinement\nConfidence scoring criteria for retrieved information\nInstructions for handling partial matches\nStrategies for iterative search refinement\nThis enhanced context helps the agent:\nBetter evaluate search result relevance\nMake informed decisions about additional searches\nMaintain consistency in confidence scoring\nBalance completeness with accuracy\nAll components are implemented as configurable modules, allowing easy comparison of different strategies and prompting approaches. The framework supports both Llama 3.1 and Qwen 2.5 models through a unified interface, facilitating direct performance comparisons across model architectures."}, {"title": "4 EVALUATION FRAMEWORK", "content": "The evaluation of RAG systems presents unique challenges, requiring assessment of both retrieval accuracy and response quality. Our framework addresses these challenges through a comprehensive approach combining programmatic analysis, LLM-based evaluation, and novel hybrid metrics. This multi-faceted evaluation strategy enables detailed assessment of system performance while maintaining reproducibility and computational efficiency."}, {"title": "4.1 Metric Categories", "content": "Our evaluation framework employs three primary categories of metrics, each designed to capture different aspects of RAG system performance. The metrics are weighted based on their relative importance and reliability, with thresholds established through empirical testing."}, {"title": "4.1.1 Programmatic Metrics", "content": "Our framework implements two core programmatic metrics that assess lexical and semantic accuracy without requiring LLM intervention. The key terms precision metric (weight: 0.15, threshold: 0.7) evaluates the system's ability to capture domain-specific vocabulary and crucial concepts. This metric identifies important non-stop words from the context and measures their presence in the response relative to the ground truth.\nThe token recall metric (weight: 0.15, threshold: 0.7) complements this by assessing information completeness, measuring what percentage of ground truth tokens appear in the response. Together, these metrics provide a foundation for automated evaluation that is both efficient and reproducible."}, {"title": "4.1.2 LLM-based Metrics", "content": "To capture more nuanced aspects of response quality, we implement four LLM-based metrics. The truthfulness metric (weight: 0.2, threshold: 0.7) serves as our primary measure of factual accuracy, evaluating consistency between the response and ground truth through structured LLM prompting. Completeness (weight: 0.1, threshold: 0.7) assesses whether all key points from the ground truth are adequately covered in the response.\nSource relevance (weight: 0.05, threshold: 0.7) evaluates the quality of retrieved context chunks, with particular emphasis on the presence of crucial information. This metric employs a novel weighting scheme that heavily favors responses containing at least one highly relevant source (80/20 split), recognizing that a single highly relevant chunk often suffices for accurate responses.\nThe context faithfulness metric (weight: 0.1, threshold: 0.7) specifically targets the system's ability to stay true to provided context, identifying any statements that contradict or lack support from the retrieved passages. This metric is crucial for detecting hallucination and unsupported inference."}, {"title": "4.1.3 Hybrid Metrics", "content": "Our framework introduces three hybrid metrics that combine programmatic and LLM-based approaches. The semantic F1 metric (weight: 0.1, threshold: 0.6) extends traditional F1 scoring by incorporating semantic similarity. This metric uses embedding-based matching of key points between response and ground truth, allowing for valid paraphrasing while maintaining accuracy requirements.\nThe answer relevance metric (weight: 0.1, threshold: 0.7) combines embedding similarity measurements with LLM judgment to provide a comprehensive assessment of response appropriateness. This dual approach helps balance computational efficiency with semantic understanding."}, {"title": "4.2 Novel Contributions", "content": ""}, {"title": "4.2.1 Completeness Gain Metric", "content": "A key innovation in our framework is the completeness gain metric (weight: 0.05, threshold: 0.501), which evaluates response quality relative to the full available context rather than just the ground truth. This metric addresses a fundamental limitation in traditional RAG evaluation: the possibility that systems might identify relevant information not included in human-curated ground truth answers.\nThe metric operates through a multi-step process:\n1) Extraction of all relevant points from the full context\n2) Measurement of point coverage in both ground truth and response\n3) Calculation of relative gain using a normalized scoring system where:\n0.5 indicates equal coverage to ground truth\nScores above 0.5 represent improved coverage\nScores below 0.5 indicate inferior coverage\nTo ensure validity, the metric includes semantic verification of additional points claimed by the response, using embedding similarity to confirm genuine coverage rather than tangential mentions."}, {"title": "4.2.2 Structured Evaluation Methodology", "content": "Our framework implements a weighted aggregate scoring system that balances different types of evaluation, with programmatic metrics contributing 25% of the total score, LLM-based metrics accounting for 45%, and hybrid metrics providing the remaining 30%. This distribution reflects both the reliability and importance of each metric category. A response must pass at least 6 out of 8 primary metrics (excluding completeness gain) to be considered successful overall.\nAdditionally, we maintain an independent numerical accuracy assessment that specifically evaluates the system's ability to handle numerical information. This separate tracking provides crucial insights for applications requiring precise numerical data handling."}, {"title": "4.3 Score Interpretation", "content": "To ensure consistent evaluation across different configurations, we establish clear score interpretation guidelines. For standard metrics (excluding Semantic F1 and Completeness Gain), we define the following ranges:\nExcellent: > 0.8 (80%)\nGood: 0.6-0.8 (60-80%)\nFair: 0.4-0.6 (40-60%)\nPoor: < 0.4 (40%)\nThe Semantic F1 metric uses modified thresholds to account for valid variations in expression:\nExcellent: > 0.7 (70%)\nGood: 0.5-0.7 (50-70%)\nFair: 0.2-0.5 (20-50%)\nPoor: < 0.2 (20%)\nFor Completeness Gain, any score above 0.5 indicates potential improvement over ground truth answers, with higher scores suggesting more substantial gains."}, {"title": "5 EXPERIMENTAL SETUP", "content": ""}, {"title": "5.1 Dataset Preparation", "content": "The effectiveness of our evaluation framework relies heavily on the quality and diversity of our test dataset. We developed a multi-stage process for creating a comprehensive and well-curated set of question-answer pairs that accurately represent real-world usage scenarios.\nOur initial data generation phase employed LLM-based extraction to identify potential QA pairs from source documents. This process incorporated both topical analysis and cross-document correlation to ensure comprehensive coverage of related concepts across different sources. The extraction process specifically targeted various types of relationships and information patterns within the documents.\nFollowing the automated extraction, we implemented a rigorous human curation process. Expert reviewers evaluated each QA pair against several criteria:\nQuestion clarity and specificity\nAnswer completeness and accuracy\nSufficient context coverage\nSource material verification\nThe final curated dataset consists of 319 QA pairs, carefully selected to cover a diverse range of query types and complexity levels. These include factual queries, numerical questions requiring precise data extraction, multi-document reasoning tasks that demand information synthesis, and comparative questions that test the system's ability to analyze relationships between concepts."}, {"title": "5.2 Model Configurations", "content": "Our evaluation encompassed two state-of-the-art language models, chosen to represent different architectural approaches and parameter scales. For the base language models, we employed:\nLlama 3.1 [36]: An 8B parameter instruct model configuration, implemented with 6-bit quantization to balance performance and resource utilization. We maintained a temperature setting of 0.1 to prioritize deterministic outputs.\nQwen 2.5 [37]: A larger 14B parameter model, implemented with 4-bit quantization. This configuration also used a temperature of 0.1 for consistency in evaluation.\nFor the embedding and reranking components, we selected established models with proven performance in their respective tasks:\nVector embeddings: BAAI/bge-base-en-v1.5 for generating dense vector representations\nCross-encoder reranking: cross-encoder/ms-marco-MiniLM-L-2-v2 for refined relevance assessment"}, {"title": "5.3 Implementation Details", "content": "Each retrieval strategy was implemented with carefully tuned parameters based on preliminary experimentation. The configurations for each approach were as follows:"}, {"title": "5.3.1 Naive Vector Search", "content": "The base vector search implementation used a relatively small chunk size to maintain granular content access while ensuring sufficient context:\nChunk size: 256 tokens\nChunk overlap: 50 tokens\nTop-k retrieval: 4 documents\nThis configuration balances the granularity of retrieval with computational efficiency, while the overlap helps maintain context coherence across chunk boundaries."}, {"title": "5.3.2 Naive with Reranking", "content": "The reranking implementation expanded the initial retrieval set to allow for more nuanced selection:\nInitial retrieval: top-20 documents\nReranking: select top-4 documents\nCross-encoder batch size: 32\nThis two-stage approach allows for broader initial coverage while ensuring high-quality final selection through targeted reranking."}, {"title": "5.3.3 Hybrid Search", "content": "Our hybrid implementation combines multiple retrieval signals with configurable parameters:\nVector similarity: top-20 candidates\nKeyword matching: BM25-style scoring\nCombination mode: OR (union of results)\nFinal reranking: top-4 documents\nThe union-based combination strategy maximizes recall while relying on reranking for precision."}, {"title": "5.4 Evaluation Protocol", "content": "To ensure reproducible results, we implemented a systematic evaluation protocol consisting of three main phases:"}, {"title": "5.4.1 Response Generation", "content": "The response generation phase followed a structured process:\n1) Sequential processing of all QA pairs in the dataset\n2) Response generation using each configuration variant\n3) Comprehensive recording of:\nRetrieved passages and their relevance scores\nComplete agent reasoning steps\nSystem confidence assessments"}, {"title": "5.4.2 Metric Calculation", "content": "The metric calculation phase implemented parallel processing where possible:\n1) Simultaneous computation of independent metrics\n2) Separate tracking of numerical accuracy\n3) Storage of detailed scoring breakdowns"}, {"title": "5.4.3 Result Aggregation", "content": "The final phase combined individual metrics into overall assessments:\n1) Calculation of weighted aggregate scores\n2) Determination of pass/fail status\n3) Collection of per-metric statistics\n4) Generation of comparative performance analyses"}, {"title": "5.5 Computational Environment", "content": "Our evaluation framework was designed to be accessible while maintaining robust performance capabilities. The testing environment consisted of:\nConsumer-grade hardware with NVIDIA GPU\nPython 3.10+ environment\nLlamaIndex framework for RAG implementation\nOllama for model serving\nThis configuration achieved an approximate cost of $4 in electricity consumption for a complete evaluation suite, demonstrating the framework's accessibility for academic research and development environments. The setup ensures reproducibility while maintaining reasonable resource requirements, making it practical for both research and development purposes."}, {"title": "6 RESULTS AND ANALYSIS", "content": ""}, {"title": "6.1 Retrieval Strategy Comparison", "content": "Our comprehensive evaluation revealed significant performance variations across different retrieval strategies and model configurations. The results demonstrate clear advantages for hybrid approaches and the impact of model selection on overall system performance.\nAs shown in Fig. 1, our evaluation reveals significant performance differences across retrieval strategies:\nNaive Vector Search:\nLlama 3.1: 50.0% overall pass rate\nQwen 2.5: 63.0% overall pass rate\nKey strength: Strong source relevance (92-96%)\nMain weakness: Lower completeness scores (46-80%)\nNaive with Reranking:\nLlama 3.1: 48.6% overall pass rate\nQwen 2.5: 66.5% overall pass rate\nImproved context faithfulness (72-86%)\nHigher answer relevance scores (64-77%)\nHybrid Search:\nLlama 3.1: 52.4% overall pass rate\nQwen 2.5: 72.7% overall pass rate\nBest overall performance\nSignificant improvements in:\nContext faithfulness (75-86%)\nKey terms precision (51-71%)\nCompleteness (55-87%)"}, {"title": "6.1.1 Vector Search vs. Reranking vs. Hybrid", "content": "In the naive vector search configuration, both models showed moderate performance with notably different baselines. Llama 3.1 achieved a 50.0% overall pass rate with a mean score of 0.682 (\u03c3 = 0.237). Qwen 2.5 demonstrated stronger baseline performance, achieving a 63.0% pass rate with a mean score of 0.744 (\u03c3 = 0.197). The key strength for both models in this configuration was source relevance (92-96%), though they struggled with completeness metrics (Llama: 46%, Qwen: 80%).\nThe addition of reranking showed mixed results, with Llama 3.1's performance slightly declining to 48.6% while Qwen 2.5 improved to 66.5%. Both models showed marked improvement in context faithfulness (Llama: 72%, Qwen: 86%) and answer relevance (Llama: 64%, Qwen: 77%). This suggests that reranking particularly benefits larger models with stronger semantic understanding capabilities.\nHybrid search emerged as the superior approach across all metrics. Llama 3.1's overall pass rate increased to 52.4%, while Qwen 2.5 achieved the highest performance with a 72.7% pass rate. The hybrid approach showed particular strength in:\nContext faithfulness (Llama: 75%, Qwen: 86%)\nKey terms precision (Llama: 51%, Qwen: 71%)\nCompleteness (Llama: 55%, Qwen: 87%)"}, {"title": "6.1.2 Impact of Prompt Engineering", "content": "The introduction of custom prompt engineering demonstrated consistent improvements across configurations. The base ReAct implementation achieved pass rates of 62.9% for Llama 3.1 and 65.8% for Qwen 2.5. The custom ReAct implementation further improved these results to 66.1% and 70.6% respectively, with particularly strong gains in:\nSource relevance (improving to 98-99% for both models)\nContext faithfulness (Llama: 83%, Qwen: 89%)\nAnswer relevance (Llama: 78%, Qwen: 82%)"}, {"title": "6.1.3 Model Performance Analysis", "content": "Qwen 2.5 demonstrated consistently superior performance across all configurations, with several notable advantages:\nHigher overall pass rates (63-73% vs. Llama's 48-66%)\nSuperior numerical accuracy (84% vs. 67%)\nMore consistent performance (lower standard deviations)\nBetter performance on hybrid metrics"}, {"title": "6.2 Metric Analysis", "content": ""}, {"title": "6.2.1 Individual Metric Performance", "content": "Conversely, certain patterns consistently predicted system failure:\nPoor key terms precision (< 0.6)\nLow completeness scores (< 0.5)\nWeak semantic F1 scores (< 0.4)"}, {"title": "6.2.2 Correlation Between Metrics", "content": "Analysis revealed several significant relationships between metrics. Strong correlations (r > 0.8) were observed between context faithfulness and truthfulness, answer relevance and completeness, and key terms precision and token recall. Moderate correlations (0.5 < r < 0.8) were found between source relevance and completeness, as well as semantic F1 and answer relevance."}, {"title": "6.2.3 Success/Failure Patterns", "content": "Our analysis identified several reliable indicators of system performance. Strong predictors of success included:\nHigh source relevance scores (> 0.9)\nStrong context faithfulness (> 0.85)\nCombined strength in programmatic metrics (> 0.7)"}, {"title": "6.3 Numerical Accuracy Analysis", "content": "The independent numerical accuracy assessment revealed substantial differences between models and configurations. Qwen 2.5 significantly outperformed Llama 3.1, achieving an accuracy of 83.9% compared to 72.7%. This performance gap widened further with custom ReAct prompting, which improved accuracy by 5-10% across configurations.\nThe hybrid search configuration showed particular strength in number-heavy queries, though several common challenges persisted across all configurations:\nUnit conversion errors (especially with non-standard units)\nRounding inconsistencies in multi-step calculations\nConfusion in contexts containing multiple related numbers\nThese results suggest that while larger models generally handle numerical tasks better, specific architectural improvements or specialized prompting strategies may be needed to address persistent challenges in numerical processing."}, {"title": "7 DISCUSSION", "content": ""}, {"title": "7.1 Key Findings", "content": "Our experimental results provide several significant insights into RAG system optimization and design. The superiority of hybrid search approaches across multiple metrics suggests that combining different retrieval signals is fundamental to robust RAG system performance. The hybrid approach achieved a 72.7% pass rate with Qwen 2.5, compared to 63.0% for naive vector search, representing a substantial 15.4% relative improvement.\nThis performance advantage appears to stem from the complementary nature of vector and keyword-based search methods. Vector search excels at capturing semantic relationships and handling paraphrased content, while keyword matching ensures retention of critical technical terms and exact matches. The combination effectively mitigates the weaknesses of each individual approach, particularly in technical or specialized domains where both semantic understanding and precise terminology are crucial.\nModel selection emerged as another critical factor, with Qwen 2.5 consistently outperforming Llama 3.1 across all configurations. The performance gap was particularly pronounced in numerical accuracy (83.9% vs. 72.7%) and context faithfulness (91.6% vs. 88.7%). This suggests that larger model size (14B vs. 8B parameters) provides tangible benefits for RAG applications, particularly in handling complex numerical information and maintaining contextual consistency."}, {"title": "7.2 Practical Implications", "content": "Our findings have several important implications for RAG system implementation in real-world applications. First, the consistent superiority of hybrid retrieval approaches suggests that this should be considered the default choice for production systems, despite the additional complexity in implementation. The performance gains, particularly in critical areas like context faithfulness and numerical accuracy, justify the increased computational overhead.\nSecond, our results indicate that retrieval strategy optimization can often provide greater performance improvements than moving to larger language models. This has significant practical implications for system design and resource allocation. Organizations may achieve better results by focusing on retrieval optimization rather than exclusively pursuing larger model deployments.\nThe impact of prompt engineering on system performance was substantial but varied across configurations. Structured self-evaluation prompting provided consistent benefits, with improvements of 3-5% in overall pass rates and up to 10% in numerical accuracy. These gains were achieved without additional computational cost, suggesting that prompt engineering represents a highly cost-effective optimization strategy."}, {"title": "7.3 Limitations", "content": "Several limitations of our current study should be acknowledged. First, our dataset, while carefully curated, comprises 319 QA pairs, which may not capture all possible query types or edge cases. The curated nature of the dataset, while ensuring quality, might not fully reflect the distribution of queries in real-world applications.\nTechnical limitations include the use of fixed chunk sizes and overlap settings across all configurations. While these parameters were chosen based on preliminary experimentation, dynamic adaptation of these values might yield further improvements. Additionally, our exploration of hybrid search parameters was not exhaustive, and other combinations of retrieval signals might prove more effective for specific use cases.\nThe scope of our evaluation was also limited to two specific language models. While these represent different points in the model size spectrum, the findings might not generalize to all model architectures or sizes. Similarly, our testing of embedding models was limited to a single high-performing choice, and other embedding models might yield different results."}, {"title": "7.4 Future Directions", "content": "Our work suggests several promising directions for future research. In the technical domain, the development of dynamic chunk size adaptation based on content characteristics represents a particularly promising avenue. Such adaptation could optimize retrieval granularity based on document structure, content density, and query characteristics.\nThe exploration of alternative hybrid search combinations, including different weighting schemes for vector and keyword signals, could yield further improvements. Integration of additional retrieval signals, such as document structure or metadata, might also enhance performance, particularly for specialized document types or domains.\nEvaluation methodology could be extended through the development of real-time evaluation metrics that adapt to user feedback and query patterns. This could enable continuous optimization of retrieval strategies based on actual usage patterns. Investigation of task-specific metric weights might also improve evaluation accuracy for specialized applications.\nSeveral specific research opportunities warrant further investigation:\nAnalysis of cross-model performance patterns to identify architectural features that benefit RAG applications\nInvestigation of document complexity impacts on optimal retrieval strategy selection\nStudy of relationships between prompt structure and retrieval effectiveness\nDevelopment of adaptive retrieval strategies that modify their approach based on query characteristics\nThe development of automated parameter tuning mechanisms for different document types represents another valuable research direction. Such mechanisms could optimize chunk sizes, overlap ratios, and retrieval parameters based on document characteristics and query patterns.\nOur findings regarding the importance of prompt engineering in RAG systems suggest the need for more systematic investigation of prompt optimization techniques. This includes the potential development of automated prompt adaptation systems that adjust their structure based on query type and retrieved context characteristics."}, {"title": "8 CONCLUSION", "content": "This paper presented RAG Playground, a comprehensive framework for evaluating and comparing different Retrieval-Augmented Generation strategies. Through systematic experimentation with 319 curated QA pairs, we demonstrated significant performance differences between retrieval strategies and the impact of structured prompt engineering. Our"}]}