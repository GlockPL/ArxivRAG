{"title": "ChronoGAN: Supervised and Embedded Generative Adversarial Networks for Time Series Generation", "authors": ["MohammadReza EskandariNasab", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "abstract": "Generating time series data using Generative Adversarial Networks (GANs) presents several prevalent challenges, such as slow convergence, information loss in embedding spaces, instability, and performance variability depending on the series length. To tackle these obstacles, we introduce a robust framework aimed at addressing and mitigating these issues effectively. This advanced framework integrates the benefits of an Autoencoder-generated embedding space with the adversarial training dynamics of GANs. This framework benefits from a time series-based loss function and oversight from a supervisory network, both of which capture the stepwise conditional distributions of the data effectively. The generator functions within the latent space, while the discriminator offers essential feedback based on the feature space. Moreover, we introduce an early generation algorithm and an improved neural network architecture to enhance stability and ensure effective generalization across both short and long time series. Through joint training, our framework consistently outperforms existing benchmarks, generating high-quality time series data across a range of real and synthetic datasets with diverse characteristics.", "sections": [{"title": "I. INTRODUCTION", "content": "Fields such as biomedical signal processing [1] and solar flare prediction [2], [3] often face data shortages due to complex and noisy data environments, scarcity of events, and privacy concerns [4], all of which complicate accurate model training and evaluation. Developing methods that leverage Generative Adversarial Networks (GANs) [5] to produce realistic synthetic data can foster scientific progress. By creating balanced datasets and mitigating data shortages, GANs can improve the performance of machine learning tasks [6].\nGenerative modeling of time series data poses unique challenges due to the temporal nature of the data. These models must not only capture the distribution of features at individual time points but also unravel the complex dynamics between these points over time. For instance, when managing multivariate sequential data represented as $X_{1:T} = (x_1,...,x_T)$, an effective model should accurately determine the conditional distribution $p(x_t | X_{1:t-1})$, which dictates the temporal transitions. Without this capability, the generated data fails to capture the characteristics of the real dataset [7]. This leads to misleading and inaccurate evaluations when used alongside real data for downstream machine learning tasks [8].\nIn the field of time series generation, a substantial body of research has focused on enhancing the temporal dynamics of autoregressive models for sequence forecasting. The primary aim is to reduce the propagation of sampling errors through various training-time adjustments, leading to more precise conditional distribution modeling [9]\u2013[11]. Autoregressive models decompose the sequence distribution into a chain of conditionals, $\\Pi_t P(X_t | X_{1:t-1})$, which proves useful for forecasting due to their deterministic nature. However, they lack true generative capabilities, as generating new sequences from them does not require external input. In contrast, research applying GANs to sequential data often employs sequence-to-sequence neural network layers for both the generator and discriminator. This approach pursues a direct adversarial objective [12]\u2013[14] to learn the probability distribution of the data and generate new samples by feeding random noise into the model. While straightforward, this adversarial goal focuses on modeling the joint distribution $p(x_{1:T})$ [15] without considering the autoregressive structure. This may be inadequate, as aggregating standard GAN losses across vectors does not necessarily ensure the capture of stepwise dependencies in time series samples.\nIn this paper, we introduce a novel framework that significantly enhances stability, accuracy, and generalizability. Our approach, termed ChronoGAN, effectively integrates the two research streams into a robust and precise generative model specifically designed to preserve temporal dynamics through supervised GAN training. Additionally, it leverages latent space during training, ensuring more reliable convergence. Therefore, ChronoGAN offers a comprehensive method for generating realistic time-series data applicable across various fields. The key contributions of our study are:\n1) Generating data within the latent space using a generator, while utilizing a discriminator that operates in the feature space, offers significant advantages. This method not only provides more precise adversarial feedback to the generator but also delivers crucial adversarial feedback to the autoencoder, enhancing the overall performance of the model.\n2) The development of a novel time series-based loss function for the generator network, combined with a supervised loss, enhances the quality of the generated"}, {"title": "II. RELATED WORK", "content": "Autoregressive recurrent networks trained using maximum likelihood methods are susceptible to significant prediction errors during multi-step sampling [17]. This issue arises from the difference between closed-loop training (conditioned on actual data) and open-loop inference (based on prior predictions). Further, inspired by adversarial domain adaptation [18], Professor Forcing trains an additional discriminator to differentiate between autonomous and teacher-driven hidden states [19], helping to align training and sampling dynamics. However, although these methods share our aim of modeling stepwise transitions, they are deterministic and do not explicitly involve sampling from a learned distribution, which is crucial for our objective of synthetic data generation.\nThe foundational paper on GANs [5] introduced a novel framework for generating synthetic data. The model consists of two neural networks (the generator and the discriminator) that are trained simultaneously in a zero-sum game setup. However, despite being capable of generating data by sampling from a learned distribution, they struggle to capture the stepwise dependencies inherent in time series data. The adversarial feedback from the discriminator alone is insufficient for the generator to effectively learn the patterns of sequences.\nSeveral studies have adopted the GAN framework for use in time series analysis. The earliest, C-RNN-GAN [12], applied the GAN directly to sequential data with LSTM networks serving as both generator and discriminator. It generates data recurrently, starting with a noise vector and the data from the previous time step. RCGAN [13] modified this by removing the reliance on previous outputs and incorporating additional inputs for conditioning [20]. However, these models depend solely on binary adversarial feedback for learning, which may not capture the temporal dynamics of time series data.\nTimeGAN [16] presented a sophisticated method for generating time-series data, combining the versatility of unsupervised learning with the accuracy of supervised training. By optimizing an embedding space through both supervised and adversarial objectives, it aimed to closely mirror the dynamics of time series data. Despite its novel approach, TimeGAN encounters challenges with the quality of the generated data, primarily due to its reliance on adversarial training within the embedding space rather than the feature space. Furthermore, TimeGAN suffers from stability issues, yielding inconsistent outcomes across identical iteration counts and hyperparameter settings. It also faces difficulties in generating both short and long time series sequences.\nThe ChronoGAN framework is developed to enhance the efficacy and robustness of time series generation by accomplishing several critical objectives. First, it optimizes performance across both short and long sequences. Second, it enhances data reconstruction by the decoder and data generation by the generator through providing more accurate adversarial feedback to both the autoencoder and generator. Third, it facilitates the convergence of both the generator and autoencoder networks through the implementation of novel loss functions. Finally, it incorporates an early generation algorithm to achieve consistent optimal results under the same hyperparameters. Fig. 1 illustrates the implementation of ChronoGAN."}, {"title": "III. PROPOSED MODEL: CHRONOGAN", "content": "Based on Fig. 1, the framework includes five networks: an autoencoder (encoder and decoder), a generator, a supervisor, and a discriminator. The autoencoder's role is to facilitate training by generating compressed representations in the latent space, thereby reducing the likelihood of non-convergence within the GAN framework. The generator produces data in this lower-dimensional latent space, as opposed to the feature space. The supervisor network, integrated with a supervised loss function, is specifically designed to learn the temporal dynamics of the time series data. This is crucial, as sole reliance on the discriminator's adversarial feedback may not sufficiently prompt the generator to capture the data's stepwise conditional distributions. The discriminator network differentiates between fake and real data in the feature space, providing more accurate feedback to both the generator and autoencoder.\nIn Fig. 1, $H_{AE} = e(X)$ represents the encoding of the input data $X$ into a latent space $H_{AE}$ using the encoder function $e$. The reconstructed data $X_{AE} = r(H_{AE})$ is obtained by decoding $H_{AE}$ using the recovery function $r$, aiming to replicate the original input data as closely as possible. The"}, {"title": "A. Adversarial Training", "content": "In a joint training scheme involving a GAN network and an autoencoder, relying solely on reconstruction loss for the autoencoder results in noisy outputs, where the autoencoder's output fails to fully retain the input's characteristics [21]. Additionally, adversarial training within an embedding space leads to the generation of noisy data after decoding the generator's output. The issue arises when the encoder's output ($H_{AE}$) is regarded as real data and the generator's output ($H_G$) as synthetic during the adversarial training process. This practice reduces the discriminator's ability to accurately differentiate between the attributes of real and synthetic data. A significant limitation is that the discriminator does not account for the error rate and data loss inherent in the autoencoder's performance. This oversight may compromise the efficacy of the discriminator, resulting in suboptimal performance in distinguishing between real and generated data attributes. Consequently, this leads to less precise feedback being provided to the generator network, potentially affecting the overall quality of the synthetic data. To address this, as shown in Fig. 1, discriminating in the feature space allows for defining real data as the dataset ($X$) and fake data as the decoding of the generator's output ($X_G$). This facilitates more accurate training for the discriminator, thus yielding improved feedback for the generator. Additionally, discrimination in the feature space provides valuable adversarial feedback to the autoencoder, enhancing its reconstruction capabilities in conjunction with conventional reconstruction loss. In the context of time series data, the feature space denotes the original dimensions, such as individual time points and their observed values. The latent or embedding space, achieved through an encoding process, represents the data in a lower-dimensional form, capturing its essential patterns and structures in a more compact and informative manner [22].\nThrough a joint learning scheme, the autoencoder is initially trained using a combination of reconstruction loss and binary feedback from the discriminator, where real data is the dataset ($X$) and fake data is its reconstruction ($X_{AE}$). This approach enhances the autoencoder's precision in reconstructing outputs. In the subsequent phase, only the supervisor network is trained. The supervisor utilizes real data embeddings from the previous two time steps $h_{1:t-2}$ generated by the embedding network to create the subsequent latent vector $h_t$. Finally, all five networks are trained jointly. During this final phase, the same discriminator distinguishes between real data, denoted as the dataset ($X$), and the dataset reconstructions ($X_{AE}$), where the fake data comprises the generator's decoded outputs"}, {"title": "B. Novel Loss Functions", "content": "Based on the feedback from the discriminator, we introduce a new loss function for the autoencoder ($L_{AE}$), which comprises both reconstruction loss ($L_R$) and adversarial loss ($L_U$). The proportion of reconstruction loss to adversarial loss decreases in the third phase of training compared to the first phase, where the primary purpose of the discriminator is to provide feedback for the generator rather than the autoencoder.\n$L_{AE} = L_R + L_U; L_R = E_{x_{1:T}~p} ||x_t - x_{AE}||^2$ (1)\nWhere $t$ denotes an individual time step, and $T$ represents the total number of time steps within the series. In addition, $x_t$ represents the real data at timestamp $t$, and $x_{AE}$ denotes the output of the autoencoder corresponding to the real data $x_t$ at the same timestamp.\n$L_U = E_{x_{1:T}~p} \\sum_t \\log y_t + E_{x_{1:T}~\\tilde{p}} \\sum_t \\log(1-\\tilde{y}_t)$ (2)\n$\\tilde{y} = d(X_{AE}); y = d(X)$ (3)\nHere, $p$ indicates the probability distribution of real data, and $\\tilde{p}$ represents the probability distribution of synthetic data. Moreover, the discriminator $d$ generates the output $\\tilde{y}$ when evaluating the autoencoder's output $X_{AE}$ and produces the output $y$ when assessing the real samples $X$.\nThe sole reliance on the discriminator's binary adversarial feedback might not sufficiently drive the generator to capture the data's stepwise conditional distributions. To address this, ChronoGAN introduces an additional component, the supervisor, along with a novel loss mechanism denoted by $L_S$. ChronoGAN employs a closed-loop training mode, where the supervisor utilizes actual data embeddings from the previous two time steps $h_{1:t-2}$ produced by the embedding network to generate the subsequent latent vector $h_t$. This looped training involves the generator's loss $L_G$, which encompasses the adversarial loss $L_U$, the stepwise transition loss $L_S$, the distribution loss $L_V$, and our innovative time series loss $L_{TS}$. This structure ensures the generation of realistic sequences with accurate temporal transitions. The distribution loss $L_V$ leverages the mean absolute error (MAE) of the mean and variance between the real data $X$ and the generated data $\\tilde{X}$. This approach effectively assists the generator in learning the real data distribution, enabling it to produce data across the entire distribution, which also serves as a key metric for evaluating GAN techniques.\n$L_G = L_U + L_S + L_V + L_{TS}; L_V = L_{Mean} + L_{variance}$ (4)"}, {"title": "", "content": "Where $L_{Mean}$ is the MAE of the mean between a batch of real and generated samples, and $L_{variance}$ is the MAE of the variance between the same batch of real and generated data.\n$L_{Mean} = E_{x_{1:T}~p} \\sum_t \\frac{1}{N} \\sum_{n=1}^{N} |X_{tn} - \\tilde{X}_{tn}|$ (5)\nWhere each sample is labeled by $n \\in \\{1, ..., N\\}$ and the batch is represented as $B = \\{X_{n,1:T_n}\\}_{n=1}^{N}$.\n$L_{Variance} = E_{x_{1:T}~p} \\sum_t |\\frac{1}{N} \\sum_{n=1}^{N} (X_{tn} - \\overline{X}_t)^2 - \\frac{1}{N} \\sum_{n=1}^{N} (\\tilde{X}_{tn} - \\overline{\\tilde{X}}_t)^2|$ (6)\nWhere $\\overline{x}$ indicates the mean of $x$, and $\\overline{X}$ represents the mean of $x$ for a batch of data.\n$L_S = L_{t} = E_{x_{1:T}~p} ||s(h_{t}^G) - h_{t+2}^G||^2$ (7)\nWhere $s$ is the supervisor network, $h_t^G$ is the output of the generator at timestamp $t$, and $h_{t-2}^G$ is the output of the generator at timestamp $t - 2$. This technique is more efficient than predicting timestamp $t$ using timestamp $t \u2013 1$.\nIn the third phase of training, referred to as joint training, $\\tilde{y}$ represents the output of the discriminator $d$ for synthetic samples $X_G$ and $\\tilde{X}$, while $y$ denotes the output of $d$ for real samples $X$ and $X_{AE}$.\n$\\tilde{y} = d(X_G, \\tilde{X}); y = d(X, X_{AE})$ (8)\nFurthermore, we introduce a novel loss function for the generator called the time series loss, $L_{TS}$, which not only facilitates convergence but also enhances the quality of the generated data. This loss function is defined as the mean squared error (MSE) of the mean and standard deviation (std) of four key time series characteristics, including slope, skewness, weighted average, and median, between real and synthetic data. The aim is to boost the generator's convergence and its ability to learn the real data characteristics and distribution, as relying solely on the adversarial loss is insufficient for learning the characteristics of real time series data. The time series loss $L_{TS}$ is a novel contribution, comprising the slope loss ($L_{Slope}$), weighted average loss ($L_{WeightedAvg}$), skewness loss ($L_{Skewness}$), and median loss ($L_{Median}$).\n$L_{TS} = L_{Slope} + L_{WeightedAvg} + L_{Skewness} + L_{Median}$ (9)\nThe slope loss $L_{Slope}$ includes the MSE of the mean ($L_{Smean}$) and the MSE of the std ($L_{Sstd}$) between the slopes of real and generated samples.\n$L_{Slope} = L_{Smean} + L_{Sstd}$ (10)"}, {"title": "", "content": "The slope is calculated using the provided formula,\n$slope = \\frac{T \\sum_{t=1}^T tX_t - (\\sum_{t=1}^T t)(\\sum_{t=1}^T X_t)}{\\sqrt{[T \\sum_{t=1}^T t^2 - (\\sum_{t=1}^T t)^2][T \\sum_{t=1}^T X_t^2 - (\\sum_{t=1}^T X_t)^2]}}$ (11)\nIn these equations, $S$ is the slope of real samples, and $\\tilde{S}$ is the slope of generated samples.\n$L_{Smean} = E_{x_{1:T}~p} \\sum_t |\\frac{1}{N} \\sum_{n=1}^{N} S_{tn} - \\frac{1}{N} \\sum_{n=1}^{N} \\tilde{S}_{tn}|$ (12)\n$L_{Sstd} = E_{x_{1:T}~p} \\sum_t |\\sqrt{\\frac{1}{N} \\sum_{n=1}^{N} (S_{t} - \\overline{S}_{t})^2} - \\sqrt{\\frac{1}{N} \\sum_{n=1}^{N} (\\tilde{S}_{t} - \\overline{\\tilde{S}}_{t})^2}|$ (13)\nOther components of $L_{TS}$, such as skewness, weighted average, and median, are calculated similarly to (10), (12), and (13). The only difference is that instead of using the formula for slope, the formulas for skewness ($skew$), weighted average ($wAvg$), and median are applied.\n$skew = \\frac{1}{T} \\sum_{t=1}^T (\\frac{x_t - \\mu}{\\sigma})^3$ (14)\n$wAvg = \\frac{\\sum_{t=1}^T w_tX_t}{\\sum_{t=1}^T w_t}$ (15)\nWhere $\\sigma_x$ represents the std of $x$, and $w_t$ denotes the weight assigned to the value $x_t$ at timestamp $t."}, {"title": "C. GRU-LSTM Network Architecture", "content": "Leveraging the strengths of different neural network architectures by combining them has long been a powerful and effective approach. In auditory attention detection (AAD), combining GRU and CNN architectures has been particularly effective. CNNs, while good at extracting spatial features from EEG data, struggle to capture long-term dependencies. To address this, the AAD-GCQL model [1] integrates GRU with CNN to capture both spatial and temporal dynamics in EEG signals, enhancing the detection of auditory attention.\nThe GRU used in this combination belongs to a broader family of recurrent neural networks (RNNs), which are tailored for sequence modeling tasks. Among these, LSTM and GRU are the two most prominent architectures, frequently applied in domains such as natural language processing [23] and time series forecasting. LSTMs are equipped with memory cells and three distinct gates (input, output, and forget), which help manage the flow of information and address the vanishing gradient problem seen in traditional RNNs [24]. This architecture makes LSTMs particularly well-suited for longer sequence data, where maintaining information over extended intervals is critical. On the other hand, GRUs simplify the structure by merging the input and forget gates into a single"}, {"title": "D. Early Generation", "content": "Another prevalent issue with GANs is stability. To enhance the stability of the network, we employ an early generation algorithm since the optimal results may be achieved after a random, rather than a specific, number of iterations. Accordingly, as per Algorithm 1, after half the number of epochs, we generate synthetic data and calculate the discriminative score and predictive score between real and synthetic data at intervals of every 500 epochs. Additionally, we compute the MSE of the mean and MSE of the std of real and synthetic data to verify whether the synthetic data matches the distribution of the real data. By integrating the results of the discriminative score, predictive score, and MSE of the mean and std, we determine whether to save the current model and generated data. Upon the completion of training, we ensure that the framework has produced the optimal results, consistently delivering reliable and precise outcomes after each training session. It is crucial to determine the appropriate weights for these metrics in order to integrate them and compare them with the previously saved model. The proportion of the discriminative score, predictive score, and MSE of the mean and std can vary depending on the characteristics of the dataset. Therefore, it is inappropriate to establish fixed hyperparameters to combine these three metrics. To address this issue, we initially calculate the hyperparameters $p1$ and $p2$ during the first assessment of these metrics. Once established, these hyperparameters are consistently applied in all subsequent epochs."}, {"title": "IV. EXPERIMNETS", "content": "The codebase for the ChronoGAN framework, along with a detailed tutorial on its usage, implementation, and hyperparameter settings, is publicly available for review and"}, {"title": "A. Datasets", "content": "We evaluate ChronoGAN's effectiveness on time-series datasets with varying attributes such as periodicity, discreteness, noise levels, length, and feature correlation over time.\nWe choose the datasets based on different combinations of these characteristics:\n1) Stocks: Stock price sequences are continuous but aperiodic and features are correlated. We use daily historical data from Google stocks spanning 2004 to 2019, which includes features such as volume, high, low, opening, closing, and adjusted closing prices.\n2) Sines: We generate multivariate sinusoidal sequences with varying frequencies $\\eta$ and phases $\\theta$, providing continuous, periodic, and multivariate data with each feature being independent.\n3) ECG: The ECG5000 dataset from Physionet, which covers a 20-hour long ECG recording with 140 timestamps, is a univariate time series that is continuous and periodic. The data is classified as a long time series.\n4) SWAN-SF: The Space Weather Analytics for Solar Flares (SWAN-SF) [27] dataset consists of multivariate time series of photospheric magnetic field parameters for solar flare prediction tasks [28]. The SWAN-SF dataset is recognized as challenging due to its complex temporal dynamics and the numerous data preprocessing issues it presents. In [29], the authors thoroughly addressed these challenges by implementing an innovative preprocessing pipeline [30]. This effort resulted in the creation of an enhanced version of the SWAN-SF dataset [31], which was subsequently utilized in our evaluation in place of the original, unprocessed dataset."}, {"title": "B. Baseline Techniques and Evaluation Metrics", "content": "We conduct a comparison between ChronoGAN, TimeGAN [16], Teacher Forcing (T-Forcing) [19], Professor Forcing (P-Forcing) [18] and Standard GAN [13], which represent the five best-performing techniques in various fields of time series generation, including GAN-based and Autoregressive approaches. To ensure unbiased results, we maintain identical hyperparameters across all five models. To evaluate the quality of the generated data, we focus on three key criteria:\n1) Visualization: We utilize t-SNE [32] and PCA [33] analyses on both the original and synthetic datasets. This approach aids in qualitatively assessing how closely the distribution of the generated samples matches that of the original in a two-dimensional space.\n2) Discriminative Score: For a quantitative measure of similarity, each sequence from the original dataset is labeled as 'real', while each from the generated set is labeled as 'synthetic'. An LSTM classifier is then trained to differentiate these two categories in a standard supervised learning task. The classification error on a reserved test set provides a quantitative measure of this score. We then subtract the result from 0.5, making the optimal result 0 instead of 0.5 for easier comparison.\n3) Predictive Score: To evaluate the quality of the generated data in capturing step-wise conditional distributions, we utilize the synthetic dataset to train an LSTM for sequence prediction. This involves forecasting the next-step temporal vectors for each input sequence. The"}, {"title": "V. CONCLUSION AND FUTURE WORK", "content": "In this study, we present ChronoGAN, an innovative model designed for generating time series data. ChronoGAN consists of five networks: an autoencoder (comprising an encoder and decoder), a generator, a supervisor, and a discriminator. These networks are trained together to learn the probability distribution and stepwise temporal dynamics of time series data. The model employs adversarial training in the feature space while generating data in the latent space, which significantly enhances the performance of both the autoencoder and generator networks. Additionally, ChronoGAN introduces novel loss functions for the autoencoder, generator, and supervisor networks, along with a new neural network architecture and an early generation mechanism. This framework consistently"}]}