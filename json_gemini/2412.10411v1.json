{"title": "Pre-trained protein language model for codon optimization", "authors": ["Shashank Pathak", "Guohui Lin"], "abstract": "Codon optimization of Open Reading Frame (ORF) sequences is essential for enhancing mRNA stability and expression in applications like mRNA vaccines, where codon choice can significantly impact protein yield which directly impacts immune strength. In this work, we investigate the use of a pre-trained protein language model (PPLM) for getting a rich representation of amino acids which could be utilized for codon optimization. This leaves us with a simpler fine-tuning task over PPLM in optimizing ORF sequences.The ORFs generated by our proposed models outperformed their natural counterparts encoding the same proteins on computational metrics for stability and expression. They also demonstrated enhanced performance against the benchmark ORFs used in mRNA vaccines for the SARS-CoV-2 viral spike protein and the varicella-zoster virus (VZV). These results highlight the potential of adapting PPLM for designing ORFs tailored to encode target antigens in mRNA vaccines.", "sections": [{"title": "Main", "content": "mRNA vaccines have demonstrated effectiveness, safety, and scalability in limiting the spread of COVID-19 (SARS-CoV-2) [21]. However, achieving high expression and stability in mRNA design remains a significant challenge. Low expression and stability can lead to reduced immunogenicity, compromising the vaccine's effectiveness. An mRNA sequence used in vaccines comprises several components, including the CAP region, untranslated regions (UTRs), and the open reading frame (ORF). The ORF, also known as the coding region, is critical for applications such as mRNA vaccines, as it encodes the target antigen protein [11]. Therefore, optimizing ORF sequences is an important research problem in therapeutics.\nThe ORF comprises non-overlapping triplets of nucleotides (\u2018A', 'U', 'G', and 'C'), known as codons, where each codon encodes a specific amino acid in the protein sequence. There are a total of 43 = 64 codons for 20 naturally occurring amino acids. Multiple codons can encode the same amino acid; these are referred to as synonymous codons which is the cause for degeneracy in genetic code. This leads to an exponentially large candidate space of ORFs encoding the same target protein. However, the preference for one synonymous codon over another, known as codon usage bias, plays a pivotal role in influencing gene expression and stability within a host organism. A critical determinant of codon usage bias is the availability of transfer RNA (tRNA) molecules, which vary across organisms for specific codons. Codons corresponding to abundant tRNAs are translated more efficiently, minimizing ribosomal stalling. Reduced ribosomal stalling enhances the production of the target antigen protein, which is directly linked to increased immunogenicity [1, 3]. As a result, codon optimization focuses on selecting the most appropriate codons among synonymous options to enhance expression within a host species. Thus, ORF codon optimization must consider both expression and stability to ensure therapeutic efficacy [10, 12].\nCodon optimization methods have been employed in the past, yielding improvement in ORF expression and stability. Traditional codon optimization approaches commonly involve replacing rare codons with more frequently used codons based on the natural codon distribution of the host organism [22]. Codon optimization tools follow similar methodologies [17]. However, directly replacing rare codons with the most frequent ones is not always ideal. This approach can lead to tRNA imbalance, which increases the risk of ribosomal stalling and abrupt termination of translation [15]. In earlier studies, stability has often been quantified using the minimum free energy (MFE) of the RNA secondary structure [24]. Additionally, the expression level of the optimized sequence is typically evaluated using the codon adaptation index (CAI) [19], which measures how closely a sequence's codon usage aligns with the host organism's preferred codons.\nRecent advancements in deep learning have been pivotal in various domains of bio-informatics, specifically in genomics. Given the sequential nature of"}, {"title": "Results", "content": "genomic data, recurrent neural networks (RNNs) and transformer-based architectures [2, 5, 18] have demonstrated the ability to capture rich intrinsic properties of biological sequences. This makes them well-aligned and effective for tasks like codon optimization. Codon optimization has been reformulated in prior studies as either a sequence-tagging task [6-9] or a machine translation task [18]. Both approaches take the target protein sequence as input and represent each amino acid using context-aware embeddings generated by bidirectional encoder models, such as bidirectional long short-term memory (Bi-LSTM) networks [6] or bidirectional transformer encoder networks [5, 18, 23]. These deep learning models leverage training datasets comprising protein sequence and ORF sequence pairs which are specific to a given species to learn context-rich amino acid representations. The models then map these representations to optimal codons, achieving enhanced expression in codon optimization tasks.\nIn this work, we approach codon optimization as a sequence-tagging task, similar to the method adopted in Fu et al. [6]. However, unlike prior studies, we leverage the rich representations of amino acids extracted from a pre-trained protein language model (PPLM), ProtBert [4], instead of training the model for them. By doing so, we reformulate codon optimization as a simpler transfer learning task, where only the layers required for mapping amino acids to their optimal codons are fine-tuned. This helps to eliminate the heavy task of training the model weights for learning amino acid representations. To restrict the label space of codons during the tagging task, we pass it through a codon mask that suppresses non-synonymous codons. Our approach produces species-specific models that can be retrained efficiently for other organisms as needed. We fine-tune models for species, specifically humans, Escherichia coli (E.coli), and Chinese Hamster Ovary (CHO) cells. We evaluated the optimized ORF generated by our model, trained on human genes, for the SARS-CoV-2 virus (Wuhan variant spike protein). The optimized sequence demonstrated superior performance compared to industry-standard vaccines, Pfizer and Moderna [20], based on computational metrics such as the codon adaptation index (CAI) and minimum free energy (MFE). Similarly, we observed significant improvements in the optimized ORF of the VZV encoding gE protein responsible for shingles. Overall, this work presents a simplified yet effective method for codon optimization that leverages the capabilities of PPLMs. Our approach could be used as a tool for designing the coding regions encoding target viral antigens useful for mRNA vaccines."}, {"title": "Codon optimization using PPLM", "content": "Deep learning models are proficient in capturing underlying data distribution [13]. The codon choice among synonymous codons for encoding an amino acid is not random and follows certain selection rules respective to host species [16]. In this work, the codon optimization task is reformulated as a"}, {"title": "Valid-Codon", "content": "sequence tagging task to embed the codon distribution of human Hg19 genes, following the approach of Fu et al. [6]. In this approach, each amino acid in the input protein sequence is treated analogously to a word (or individual token) in a natural language task. At each time step, the objective is to predict the optimal codon from 61 possible codons, making the sequence tagging for each amino acid a multi-label classification task. In this paper, we utilize ProtBert's amino acid representations as embeddings to fine-tune the adapter and selective modules during the fine-tuning phase. The one in which we only fine-tune the adapter module on top of ProtBert is referred to as Adaptive-ProtBert, whereas the one in which we fine-tune the last layer of ProtBert along with the adapter module is referred to as Adasel-ProtBert. Since not all 61 possible codons but only a few of them code for an amino acid, previous methods like 'codon-box' [6] have been utilized to reduce the label space. On a similar premise, we reduce the label space of the sequence tagging task by introducing a codon mask which we term as the 'valid-codon' method.\nThe final layer of the model performs the multi-label classification task by predicting probability scores of 61 codons in a time-distributed manner i.e. at each amino acid. To reduce and restrict the model's learning to only synonymous codons we introduce a codon mask at the final layer as described in Eq. 1. Codon mask is a vector $CV \\in R^{61\\times1}$, where non-synonymous codons for a given amino acid are assigned a high negative value of -10\u00ba. The tensor $output\\_logitst \\in R^{61\\times1}$, has raw probability scores for each of the 61 codons at a given amino acid.\n$$masked\\_logits_t = output\\_logits_t + CV_{tth\\_amino\\_acid}$$"}, {"title": "Species generalization", "content": "In this study, three models were trained by fine-tuning the PPLM on protein sequences specific to a particular species: humans (Hg19 genes), E.coli, and Chinese Hamster Ovary (CHO) cells. These models are referred to as Adasel-ProtBert-short, Adasel-ProtBert-E.coli, and Adasel-ProtBert-CHO, respectively. To ensure that each model is optimized for its target species and to evaluate its generalizability, the optimized ORFs from each fine-tuned model were cross-validated on test sets for the rest of the other species. This cross-species validation demonstrates the adaptability of the fine-tuned models and highlights the species-specific nature of codon optimization, while also showcasing their potential for generalization to different host species.\nOn fine-tuning with humans as the host species, the Adasel-ProtBert-short model produced optimized ORFs that showed substantial improvements in expression metrics compared to their baseline human wild-type ORFs (p < 0.001). However, when validated on E.coli_dataset and CHO_dataset protein sequences for codon optimization, the results revealed an expected decrease in the CAI metric compared to their respective wild-type ORFs, except for CHO (Fig. 2(a)). This exception is attributed to the similarity found in codon distribution between CHO and Hg19. Similarly, the Adasel-ProtBert-E.coli model showed improved performance against the wild-type ORFs in the E.coli_dataset test set, achieving improved CAI metrics (p < 0.001). However, its performance dropped significantly when evaluated on human and CHO protein sequences for expression (Fig. 2(b)). For the Adasel-ProtBert-CHO model, comparable trends were observed, with improved CAI values for CHO against its Wild Type ORFs (p < 0.001) but reduced performance for E.coli(Fig. 2(c)).\nWe conducted an additional experiment by extending our codon optimization approach to longer sequences, using the Hg19_long_filtered_mfe dataset with humans as the host species. The Adasel-ProtBert-long-mfe model demonstrated improved performance, achieving a CAI of 0.97 on the Hg19_long_filtered_mfe test set. To evaluate the impact of MFE filtering, we trained the Adasel-ProtBert model on the Hg19_random_mfe dataset, referred to as Adasel-ProtBert-random-mfe in this study. Without MFE filtering, there was a noticeable decline in both CAI and MFE metrics on the test set. The average CAI dropped by 10.3% to 0.87, while the MFE increased substantially by 75%, leading to reduced expression and stability of the optimized ORFS."}, {"title": "Result for mRNA vaccine design", "content": "To evaluate the broader applicability of our approach in mRNA vaccine design, we optimized ORFs encoding the SARS-CoV-2 spike protein and the VZV gE protein. The evaluation was based on three critical computational metrics: CAI, MFE, and GC-Content. The CAI metric, which reflects the expression of ORF, was computed using Hg19 genes as the reference sequence set. These"}, {"title": "Discussion", "content": "metrics provided a comprehensive view of the expression efficiency and structural stability of the optimized ORFs, critical factors for successful mRNA vaccine development.\nThe benchmark ORFs encoding the SARS-CoV-2 spike protein were sourced from Pfizer's BNT162b2, Moderna's mRNA-1273, and the widely used codon optimization tool Linear-Design [24]. The naturally occurring wild-type sequence, referred to as SARS-CoV-2 Wild Type, served as the baseline.\nmetrics for expression and stability. Performing codon optimization on different species backs the generalizability of our approach in optimizing ORFs given a host species. Further, we extended our work to design optimized ORF sequences encoding SARS-CoV-2 spike protein and VZV gE protein for its application in mRNA vaccines for COVID-19 and shingles. Our optimized ORFs showcased improved metrics computationally against the Pfizer and Moderna industry-approved mRNA vaccines. We could not compare our ORF for shingles with any benchmark industry-approved vaccines as they are not available in public. However, our ORF encoding VZV gE protein performed better in terms of CAI against a popular tool Linear-Design. Overall, our approach could be a potential tool in designing ORF sequences for therapeutic proteins for a given host species."}, {"title": "Method", "content": "ORF sequences and their corresponding proteins were sourced from UCSC and NCBI for the Human (Hg19), E.coli, Chinese-Hamster, Pfizer, and Moderna vaccines. The benchmark ORF sequences for SARS-CoV-2 spike and VZV gE protein using Linear-Design were taken from the work in [24]. The designing of the dataset for training was done by filtering the ORF length and MFE values. Filtering on MFE values to keep high stable sequences for training ensured joint optimization for codon usage and stability.\nFor the human species-specific fine-tuning and testing, we curated three datasets- Hg19_short_filtered_mfe, Hg19_long_filtered_mfe and Hg19_random_mfe (Table. 3) The E.coli and CHO, training and testing datasets statistics, are mentioned in the Table. 3"}, {"title": "Model Architecture and Training", "content": "In this study, codon optimization was framed as a sequence tagging task, in which the input to the model is a protein sequence and the output is a tagged ORF sequence [8]. Each amino acid in the protein sequence is assigned an optimal codon by the model, effectively generating an ORF sequence that maximizes stability and expression. On the final logits, we apply the 'valid-codon' method only during training and it is removed during inference. Fig. 2 illustrates the method discussed."}]}