{"title": "Defensive Dual Masking for Robust Adversarial Defense", "authors": ["Wangli Yang", "Jie Yang", "Yi Guo", "Johan Barthelemy"], "abstract": "The field of textual adversarial defenses has gained considerable attention in recent years due to the increasing vulnerability of natural language processing (NLP) models to adversarial attacks, which exploit subtle perturbations in input text to deceive models. This paper introduces the Defensive Dual Masking (DDM) algorithm, a novel approach designed to enhance model robustness against such attacks. DDM utilizes a unique adversarial training strategy where [MASK] tokens are strategically inserted into training samples to prepare the model to handle adversarial perturbations more effectively. During inference, potentially adversarial tokens are dynamically replaced with [MASK] tokens to neutralize potential threats while preserving the core semantics of the input. The theoretical foundation of our approach is explored, demonstrating how the selective masking mechanism strengthens the model's ability to identify and mitigate adversarial manipulations. Our empirical evaluation across a diverse set of benchmark datasets and attack mechanisms consistently shows that DDM outperforms state-of-the-art defense techniques, improving model accuracy and robustness. Moreover, when applied to Large Language Models (LLMs), DDM also enhances their resilience to adversarial attacks, providing a scalable defense mechanism for large-scale NLP applications.", "sections": [{"title": "1. Introduction", "content": "Language Models (LMs) have significantly advanced the performance of many Natu-ral Language Processing (NLP) tasks, spanning text/document classification, semanticanalysis, and topic clustering. However, extensive research has revealed that LMs aresusceptible to adversarial attacks, where even subtle perturbations to input texts canadversely affect model performanc. That is, fine-tuned LMs may demonstrate a signifi-cant decrease in performance, up to 85%, due to the presence of even a single-charactermisspelling within input textse (Gao et al. 2018; Li et al. 2019; Li et al. 2020; Jin et al.2020), highlighting their limited robustness in generalization. Consequently, there has"}, {"title": "2. Related work", "content": "As Transformer-based Language Models (LMs) gain widespread use in tasks such astext classification, clustering, and information retrieval, their susceptibility to adversar-ial attack has become a growing area of research. In this section, we provide a com-prehensive review of the existing literature on textual adversarial attacks and defensemechanisms, including techniques for generating adversarial examples at the characterand word levels, as well as various strategies proposed to mitigate these attacks andimproving model robustness."}, {"title": "2.1 Adversarial attacks", "content": "Textual adversarial attacks aim to subtly modify input text in ways that cause the targetmodels to produce incorrect predictions, while maintaining coherence with the originalcontent. The effective adversarial examples must satisfy several key criteria (Jin et al.2020):\nHuman prediction consistency: The adversarial examples should not alterhuman interpretations, resulting in predictions that align with those madefor the original inputs.\nSemantic similarity: The adversarial examples must preserve the originalsemantic meaning as perceived by human interpretation.\nLinguistic fluency: The adversarial examples should maintain propergrammar and fluency, ensuring that the text remains natural andgrammatically correct.\nThese conditions ensure that adversarial examples not only deceive the target modelsbut also remain indistinguishable from natural inputs to human observers. Adversarialattacks in the text domain are primarily categorized based on perturbation granularityinto two types: character-level and word-level perturbations."}, {"title": "Character-level attacks.", "content": "These attacks primarily manipulate individual characterswithin words in the original sample. While such perturbations may alter the semanticsof the sentence, human prediction remains relatively unaffected to a certain extent dueto visual similarity. HotFlip (Ebrahimi et al. 2018) represents an early character-basedattack method, which utilizes gradients based on a one-hot input representation toidentify the potential change with the highest estimated loss. Additionally, this methodemploys a beam search to find a set of character manipulations to confuse the model.DeepWordBug (Gao et al. 2018) utilizes four scoring functions to identify crucial words.It subsequently introduces four Token Transformers targeting these significant words,which involve swapping two adjacent letters, substituting a letter with a random one,deleting a random letter, and inserting a random letter. Similarly, Textbugger (Li et al.2019) initially identifies important words by either computing the Jacobian matrix ofthe model output or comparing the model changes before and after word deletion. Foridentified words, Textbugger extends character-level attacks beyond inserting, deleting,and swapping letters by suggesting the replacement of characters with visually similaror adjacent ones from the keyboard."}, {"title": "Word-level attacks.", "content": "These attacks deceive models through subtle word manipulations,such as synonym substitution, while maintaining grammatical correctness and semanticsimilarity. PWWS (Ren et al. 2019) employs probability-weighted word saliency toevaluate the sensitivity of the victim model to each input word. Subsequently, candi-date words are replaced by their synonyms (from WordNet), taking into account themagnitude of change in the model's output probability. FGPM (Wang et al. 2020) beginsby constructing a synonym set for each input word using its nearest neighbors in theGloVe vector space. Target words are then selected by evaluating the projected distanceand gradient change between the original word and its synonym candidates in the gra-dient direction. Finally, this method achieves textual attack through word replacementusing its synonym set. In TextFooler (Jin et al. 2020), target words are identified bycomparing changes in prediction results before and after a word deletion. Subsequently,TextFooler replaces important words with synonyms that are both semantically similar(minimizing their cosine distance) and grammatically correct (verified through POSchecking). BERT-Attack (Li et al. 2020) adopts the Masked Language Modeling (MLM)"}, {"title": "2.2 Adversarial defenses", "content": "Text adversarial defenses, in contrast to attacks, aim to form a resilient model capable ofmaintaining high accuracy on both clean (original) and polluted (adversarial) samples.To mitigate the adverse impact of adversarial attacks, defense methods are typically cat-egorized into three strategies: data augmentation, model adaptation, and randomizedsmoothing (as shown in Fig. 1)."}, {"title": "Data augmentation.", "content": "This approach involves strategically augmenting original sam-ples to generate noisy variants, which are then simultaneously utilized to fine-tunethe victim model, enhancing its robustness. Importantly, the noise introduced duringaugmentation typically differs from the tactics employed in attacks (as a black-boxmanner). Specifically, A2T (Yoo and Qi 2021) generates noisy variants by employinga gradient-based method to identify crucial words, iteratively substituting them withsynonyms using the DistilBERT similarity. FreeLB (Zhu et al. 2020) and its variants, suchas FreeLB++(Li et al. 2021), impose norm-bounded noise on the embeddings of inputsentences to produce variants. ADFAR(Bao, Wang, and Zhao 2021) applies frequency-aware randomization to both original and augmented samples (generated throughother attacking methods) to create a randomized adversarial set, which is then com-bined with original samples for model training. RMLM (Wang et al. 2023) introducesa synonym-based transformation to randomly corrupt input samples (which could beadversarial) before employing an MLM-based defender to reconstruct denoised inputs.This approach generates abundant samples for adversarial training. A similar approach(Adv-Purification) is presented in (Li, Song, and Qiu 2023). This method injects noiseto training samples via masking input texts and subsequently reconstructs maskedtexts using MLM, forming part of a multiple-run purification process. GenerAT (Zhaoand Mao 2023) randomly masks inputs while simultaneously injecting model gradi-ents to generate perturbed tokens before filling those masked tokens. These generatedadversarial variants are then employed to fine-tune the defense model. MVP (Ramanet al. 2023) incorporates a prompt template containing [MASK] token(s) into the input,and performs model prediction by filling the [MASK] token(s) (indicating the labels).A similar approach is adopted by RobustSentEmbed (Rafiei Asl et al. 2024), whereaugmented samples are generated for model fine-tuning."}, {"title": "Model adaptation.", "content": "This strategy, without generating noisy variants, refers to enhancingthe victim model architecture and/or training losses. For example, Infobert (Wanget al. 2021) refines the model by introducing an Information Bottleneck regularizer tosuppress noisy information between inputs and latent representations, along with anAnchored Feature regularizer to strengthen the correlation between local and global fea-tures. Similarly, IB (Zhang et al. 2022) inserts an additional information bottleneck layerbetween the output layer and the encoder to improve the robustness of the extractedrepresentation. SHIELD (Le, Park, and Lee 2022) modifies the last layer of the victimmodel, formulating as an ensemble of multiple-expert predictors with random weights.Flooding-x (Liu et al. 2022) adopts the gradient consistency criterion as a threshold tomonitor the training loss and further introduces an early-stop regularization techniqueto prevent overfitting of training samples. SIWCon (Zhan et al. 2023) introduces a con-trastive learning-based loss, aiming to ensure that less important input words/tokenshave a comparable influence on model predictions as their more important counter-parts. ATINTER (Gupta et al. 2023) integrates an additional encoder-decoder moduleto rewrite adversarial inputs, eliminating adversarial perturbations before the modelinference. ROIC-DM (Yuan, Yuan, and HE 2024) introduces a robust text inferenceand classification model that leverages diffusion-based architectures with integrateddenoising stages, enhancing resistance to adversarial attacks without compromisingperformance. This model surpasses traditional language models in robustness and, byincorporating advisory components, achieves comparable or superior performance, asvalidated through extensive testing across multiple datasets. DiffuseDef (Li, Rei, andSpecia 2024) is a novel adversarial defense approach for language models that enhancesrobustness by integrating a diffusion layer used as a denoiser between the encoderand classifier, combining adversarial training, iterative denoising, and ensembling tech-niques to significantly outperform existing methods in resisting adversarial attacks.And FAT (Yang, Liu, and He 2024)enhances the adversarial robustness of natural lan-guage processing models by using a single-step gradient ascent to generate adversarialexamples in the embedding space, capitalizing on the consistency of perturbations overtraining epochs without the need for preset linguistic knowledge."}, {"title": "Randomized smoothing.", "content": "This approach employs an ensemble-based approach to en-hance the model vulnerability to adversarial attacks. SAFER (Ye, Gong, and Liu 2020),for instance, constructs stochastic input ensembles and utilizes statistical properties ofensembles for classifying testing samples. In RanMASK (Zeng et al. 2023), a few inputtokens are randomly substituted using [MASK] for fine-tuning, and testing samplesare also masked at different locations to generate multiple masked versions. The finalprediction is determined by a majority vote from the ensemble of these masked versions.RSMI (Moon et al. 2023) is a two-stage framework that utilizes randomized smoothingand masked inference. In the first stage, stochastic smoothing is employed to establish asmooth classifier. In the second stage, tokens with significant loss gradients are chosenfor masking using multiple Monte-Carlo sampling. The final prediction is obtained byaveraging predictions from all these masked samples."}, {"title": "3. Proposed method", "content": "This section introduces a simple yet effective algorithm designed to enhance the modelresilience against adversarial attacks, termed Defensive Dual Masking (DDM). Theproposed method is characterized by strategically injecting [MASK] tokens into inputsequences during both training and inference stages. The workflow of the proposedDDM is shown in Fig. 2."}, {"title": "3.1 Defensive Dual Masking", "content": "The proposed method involves two primary stages. In the training stage, DDM followsthe standard fine-tuning process, utilizing the identical network architecture and train-ing loss as the vanilla model. However, it introduces a unique step of randomly inserting[MASK] tokens into input sequences. In the inference stage, our method substitutes po-tentially adversarial tokens with [MASK] before forwarding the sequence for prediction.Compared to existing defense methods, our approach does not alter the vanilla model(neither its architecture nor training loss) nor necessitates complex strategies for gen-erating noisy variants of original samples. Additionally, our method does not requiremasking input sequences multiple times for an ensemble-based learning approach, asin the randomized smoothing approach.\nSpecifically, considering the tokenized (clean) input sequence $x$ (i.e., $x = [CLS]x_1x_{|x|}[SEP]$), where $x_i$ represents the i-th token from $x$. In the context of textclassification, the goal is to optimize an encoder $Enc(\u00b7)$ and a Multilayer Perceptron$(MLP)$ layer $F(\u00b7)$ to map $x$ to a desired label $y$, i.e., $F(Enc(x)) \u2192 y$. Furthermore, let $b_M$be the predefined masking budget (or the fraction of masked tokens).\nDuring training, $DDM$ injects $M$ consecutive masks after $[CLS]$ within $x$ to createa masked sequence, denoted as\n$x'= [CLS] [MASK]_1 ...[MASK]_M x_1 ... x_{|x|} [SEP]$,\nwhere $M$ is determined as $[|x| * b_M]$. Subsequently, only $x'$ (instead of $x$) is utilized fortraining, and a standard (Cross-Entropy) loss function, denoted as $L(F(Enc(x')), y)$, isutilized.\nDuring inference, when presented with an unseen sequence $\\bar{x}$, our method initiallycomputes a potentially adversarial score for each input token. Subsequently, in a de-scending manner, tokens with higher scores from $\\bar{x}$ (say $i$ and $j$) are successivelyreplaced by $[MASK]$ until the desired number $M$ of masked tokens is reached. This"}, {"title": "3.2 Analysis on DDM", "content": "Our approach leverages [MASK] tokens during both the training and inference phases,each with distinct objectives. During training, [MASK] tokens are inserted at the be-ginning of samples to introduce perturbations that deviate from the original dataset,effectively acting as a form of noise. This use of [MASK] as a placeholder helps themodel learn to generalize by exposing it to incomplete or partially obscured data, thusenhancing its robustness against unseen or adversarial inputs. In the inference phase,[MASK] tokens are strategically employed to replace potentially adversarial tokens,enabling the model to reduce the influence of adversarial perturbations while preserv-ing the integrity of the underlying context. The subsequent analysis demonstrates theadvantages of replacing adversarial tokens with [MASK], highlighting its contributionto improved model robustness.\nLet $a$, $r$, $S$, and $m$ represent the victim token (being attacked), the replaced token (af-ter the attack), the remaining unchanged tokens, and the $[MASK]$ token, respectively\u00b9,and the hidden dimension is $d$. Unchanged tokens, i.e., tokens that are not subjected toadversarial attacks, can be collapsed into a single contracted point. The rationale behindis rooted in the attention mechanism of a Transformer model. Consider three tokens, $x$,$s$, and $p$, with their corresponding projections resulting from the linear transformationsapplied within the attention mechanism:\n$x_i =xW_i, i = 1,2,3,$\nwith similar projections (i.e., $s_{(1,2,3)}$ and $p_{(1,2,3)}$) for the other two tokens. The recon-structed token $x$ after the attention mechanism then is expressed as:\n$x = \\frac{exp(x_1x_2)x_3 + exp(x_1s_2)s_3 + exp(x_1p_2)p_3}{exp(x_1x_2) + exp(x_1s_2) + exp(x_1p_2)} = \\frac{x_3 + w'p_3}{1 + w'}$\nwhere\n$x_3 = \\frac{exp(x_1x_2)x_3 + exp(x_1s_2)s_3}{exp(x_1x_2) + exp(x_1s_2)}, w' = \\frac{exp(x_1p_2)}{exp(x_1x_2) + exp(x_1s_2)}$.\nThat is, $x_3$ is independent of $p$, and can be regarded as a contraction of $x$ and $s$. Due to the flexibility of $W_2$, we have $w' \\in R^+$, i.e., $w'$ can take any non-negative real value."}, {"title": "4. Experiments", "content": "To evaluate the effectiveness of DDM, we conducted experiments on highlycompetitive text classification benchmark datasets: AGNews (Zhang, Zhao, and LeCun2015) and MR (Pang and Lee 2005). Table 2 provides statistics on these datasets."}, {"title": "4.1 Setup", "content": "Attacking Algorithms. To demonstrate that our strategy can handle different adversar-ial attacks, we employed the well-known TextAttack framework (Morris et al. 2020)and utilized four different attack strategies. The details of specific attack methods areas follows:\nTextFooler (Jin et al. 2020) introduces word-level perturbations byreplacing original words with their synonyms.\nBERT-Attack (Li et al. 2020) applies word-level perturbations byleveraging a pre-trained masked language model to substitute targetwords.\nDeepWordBug (Gao et al. 2018) focuses on character-level perturbations,including substitutions, deletions, insertions, and letter swaps withinwords.\nTextBugger (Li et al. 2019) combines both symbol- and word-levelperturbations, utilizing techniques like inserting spaces, replacing words,"}, {"title": "Defense Algorithms.", "content": "The proposed DDM is evaluated against state-of-the-art methodsacross three categories, as outlined below:\nData Augmentation: FreeLB++ (Li et al. 2021), RMLM (Wang et al. 2023),Adv-Purification (Li, Song, and Qiu 2023), MVP (Raman et al. 2023), andFAT (Yang, Liu, and He 2024).\nModel Enhancements: InfoBERT (Wang et al. 2021), Flooding-x (Liu et al.2022), RobustT (Zheng et al. 2022),ATINTER (Gupta et al. 2023),SIWCon (Zhan et al. 2023), LLMPM (Moraffah et al. 2024), andROIC-DM (Yuan, Yuan, and HE 2024).\nRandom Smoothing: RanMASK (Zeng et al. 2023), RSMI (Moon et al.2023), and MI4D (Hu et al. 2023),Text-RS (Zhang et al. 2024).\nAmong these methods, RanMASK, RMLM, RSMI, Adv-Purification, MVP, RobustTand MI4D also utilize the masking approach. All contender methods are reviewed inSection 2.2, with their results directly drawn from the respective original papers."}, {"title": "Implementation.", "content": "For our main experiments (Section 4.2), we employ the BERT-basemodel (Devlin et al. 2019) as the encoder. Training is conducted using a batch sizeof 32 sequences, each with a maximum length of 128 tokens. We reserve 10% of thetraining set for validation, and early stopping is applied if the validation accuracy doesnot improve within one epochs or after reaching a maximum of 10 epochs. A dropoutrate of 0.1 is applied across all layers. We utilize the Adam optimizer with a learningrate that warms up to 2e-5 over the first 10,000 steps and then decays linearly to le-6following a cosine annealing schedule. Gradient clipping is enforced within the range(-1, 1). During inference, DDM masks potentially adversarial tokens by employing theWord Frequency-based (FGWS) strategy (Mozes et al. 2021) to estimate the perturbationprobability of each candidate token. Specifically, we calculate the occurrence frequencyof each candidate token within the training dataset, and the perturbation probabilityis estimated as the ratio of the token frequency to the total (training) corpus size. Themasking budget for training and testing is set as 30%. All experiments are repeatedfive times using different random seeds, with the results averaged to ensure reliability.Finally, all computations are performed on an NVIDIA A100 GPU server."}, {"title": "Evaluation Metrics.", "content": "We follow the experimental setup outlined in (Wang et al. 2021;Zhang et al. 2022; Zeng et al. 2023). That is, we select 1,000 samples that were suc-cessfully attacked, originally drawn at random from the testing dataset, to evaluatethe model robustness against adversarial attacks. The following three key metrics areemployed: (1) CLA%, which represents the classification accuracy of the model onthe original, clean data; (2) CAA%, denoting the classification accuracy under spe-cific adversarial attacks. A higher CAA% reflects better defense performance; and(3) SUCC%, which measures the success rate of adversarial attacks, defined as theproportion of examples successfully misclassified out of the total attack attempts. ALower SUCC% indicates greater robustness of the model."}, {"title": "4.2 Main results", "content": "The average results from five trials for adversarial defense performance are sum-marized in Table 3. The following key observations can be made: (1) On classificationaccuracy. The proposed method does not compromise the classification accuracy onclean testing data (CLA%). Specifically, it maintains performance on par with regularfine-tuning and does not introduce additional data, as seen in data augmentation tech-niques. As a result, the CLA% remains consistent across all datasets. (2) In terms ofdefense accuracy. Our approach demonstrates superior results compared to existingmethods across all datasets. For instance, against the TextFooler attack, our methodachieves excellent performance on two datasets, where it achieves a CAA% of 82.3%and 55.2%. For DeepWordBug, DDM outperforms all other methods, achieving thehighest reported CAA% values of 85.8% and 68.7% across all datasets, demonstrating itsrobustness against this attack. (3) Different Masking strategy. We further explore theutilization of masking strategies across various defense methods, including our own.From the results presented in Table 3, incorporating masking into Data Augmentationis more effective against word-substitution attacks such as TextFooler and BERT-Attack,while integrating masking into Random Smoothing proves more effective againstcharacter-level attacks like DeepWordBug and TextBugger. This can be attributed tothe fact that masking in Data Augmentation allows the model to reconstruct overallcontextual semantics rather than relying on specific words, directly countering word-substitution strategies. In contrast, masking in Random Smoothing conceals regions"}, {"title": "5. Conclusion", "content": "In this study, we introduced a novel adversarial defense method that leverages thestrategic insertion/replacement of [MASK] tokens both during classifier training and asa defensive action in response to attacks. This approach capitalizes on the vulnerabilityof less frequent tokens, which are often the adversarial manipulations. By training theclassifier with samples that begin with [MASK] tokens and replacing the least frequenttokens in adversarial samples with [MASK], we strengthen the classifier's ability todetect and invalidate threats that traditional methods might miss. Our experimentalresults across various datasets and attack models have demonstrated that this methodconsistently outperforms existing defense approach, highlighting its effectiveness inenhancing the robustness of NLP applications against adversarial attacks. Additionally,the application of our method to Large Language Models has shown a significantimprovement in their robustness, suggesting that this approach is not only effectivebut also adaptable to different NLP frameworks and applications."}]}