{"title": "Swarm Behavior Cloning", "authors": ["Jonas N\u00fc\u00dflein", "Maximilian Zorn", "Philipp Altmann", "Claudia Linnhoff-Popien"], "abstract": "In sequential decision-making environments, the primary approaches for training agents are Reinforcement Learning (RL) and Imitation Learning (IL). Unlike RL, which relies on modeling a reward function, IL leverages expert demonstrations, where an expert policy \\(\\pi_e\\) (e.g., a human) provides the desired behavior. Formally, a dataset D of state-action pairs is provided: \\(D = \\{(s, a = \\pi_e(s))\\}\\). A common technique within IL is Behavior Cloning (BC), where a policy \\(\\pi(s) = a\\) is learned through supervised learning on D. Further improvements can be achieved by using an ensemble of N individually trained BC policies, denoted as \\(E = \\{\\pi_i(s)\\}\\_{1\\le i \\le N}\\). The ensemble's action a for a given state s is the aggregated output of the N actions: \\(a = \\frac{1}{N}\\Sigma\\pi_i(s)\\). This paper addresses the issue of increasing action differences the observation that discrepancies between the N predicted actions grow in states that are underrepresented in the training data. Large action differences can result in suboptimal aggregated actions. To address this, we propose a method that fosters greater alignment among the policies while preserving the diversity of their computations. This approach reduces action differences and ensures that the ensemble retains its inherent strengths, such as robustness and varied decision-making. We evaluate our approach across eight diverse environments, demonstrating a notable decrease in action differences and significant improvements in overall performance, as measured by mean episode returns.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning (RL) is a widely recognized approach for training agents to exhibit desired behaviors by interacting with an environment over time. In RL, an agent receives a state s, selects an action a, and receives feedback in the form of a reward, learning which behaviors are beneficial (high reward) and which are detrimental (low reward) based on the reward function provided by the environment (Sutton and Barto, 2018). While RL has shown great promise, designing an effective reward function can be highly challenging. A well-designed reward function must satisfy several criteria: (1) the optimal behavior should yield the maximum possible return \\(R^*\\) (the sum of all rewards in an episode); (2) suboptimal behaviors must be penalized, resulting in a return \\(R < R^*\\), ensuring that shortcuts or unintended strategies are discouraged; (3) the reward function should be dense, providing informative feedback at every step of an episode rather than just at the end; (4) the reward should support gradual improvement, avoiding overly sparse rewards such as those that assign 1 to the optimal trajectory and 0 to all others, which can hinder exploration and learning (Eschmann, 2021; Knox et al., 2023).\nDue to the inherent complexity of crafting such reward functions, the field of Imitation Learning (IL) has emerged as an alternative approach (Zheng et al., 2021; Torabi et al., 2019b). Instead of relying on an explicitly defined reward function, IL uses expert demonstrations to model the desired behavior. This paradigm has proven effective in various real-world applications, such as autonomous driving (Bojarski et al., 2016; Codevilla et al., 2019) and robotics (Giusti et al., 2015; Finn et al., 2016). A prominent method within IL is Behavior Cloning (BC), where supervised learning is applied to a dataset of state-action pairs \\(D = \\{(s,a = \\pi_e(s))\\}\\), provided by an expert policy \\(\\pi_e\\). Compared to other IL methods like Inverse Reinforcement Learning (Zhifei and Meng Joo, 2012; N\u00fc\u00dflein et al., 2022) or Adversarial Imitation Learning (Ho and Ermon, 2016; Torabi et al., 2019a), BC has the advantage of not requiring further interactions with the environment, making it particularly suitable for non-simulated, real-world scenarios.\nA straightforward extension of BC is the use of an ensemble of N individually trained policies. In this approach, the ensemble action is computed by aggregating the N predicted actions as \\(a = \\Sigma_i \\pi_i(s)\\). Al-"}, {"title": "2 BACKGROUND", "content": "2.1 Reinforcement Learning\nReinforcement Learning (RL) problems are often modeled as Markov Decision Processes (MDP). An MDP is represented as a tuple \\(M = (S, A, T, r, p_0, \\gamma)\\) where S is a set of states, A is a set of actions, and \\(T(s_{t+1}| s_t, a_t)\\) is the probability density function (pdf) for sampling the next state \\(s_{t+1}\\) after executing action \\(a_t\\) in state \\(s_t\\). It fulfills the Markov property since this pdf solely depends on the current state \\(s_t\\) and not on a history of past states \\(s_{t<t}\\). \\(r : S \\times A \\rightarrow R\\) is the reward function, \\(p_0\\) is the start state distribution, and \\(\\gamma \\in [0;1)\\) is a discount factor which weights later rewards less than earlier rewards (Phan et al., 2023).\nA deterministic policy \\(\\pi : S \\rightarrow A\\) is a mapping from states to actions. Return \\(R = \\Sigma_{t=0}^{\\infty} \\gamma^t\\cdot r(s_t, a_t)\\) is the (discounted) sum of all rewards within an episode. The task of RL is to learn a policy such that the expected cumulative return is maximized:\n\\(\\pi^* = \\underset{\\pi}{\\text{argmax }} J_{p_0} (\\pi, M) = \\underset{\\pi}{\\text{argmax }} E \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) | \\pi \\right]\\)\nActions \\(a_t\\) are selected following policy \\(\\pi\\). In Deep Reinforcement Learning the policy \\(\\pi\\) is represented by a neural network \\(f_\\theta(s)\\) with a set of trainable parameters \\(\\theta\\) (Sutton and Barto, 2018)."}, {"title": "2.2 Imitation Learning", "content": "Imitation Learning (IL) operates within the framework of Markov Decision Processes, similar to Reinforcement Learning (RL). However, unlike RL, IL does not rely on a predefined reward function. Instead, the agent learns from a dataset of expert demonstrations consisting of state-action pairs:\n\\(D = \\{(s_i, a_i = \\pi_e(s_i))\\}\\_i\\)\nwhere each \\(a_i\\) represents the expert's action \\(\\pi_e(s_i)\\) in state \\(s_i\\). IL is particularly useful in situations where demonstrating the desired behavior is easier than designing a corresponding reward function.\nIL can be broadly divided into two main categories: Behavior Cloning (BC) and Inverse Reinforcement Learning (IRL). Behavior Cloning focuses on directly mimicking the expert's actions by training a policy through supervised learning on the provided dataset D (Torabi et al., 2018). In contrast, Inverse Reinforcement Learning seeks to infer the underlying reward function \\(r_e(s, a)\\) that would make the expert's behavior optimal, using the same dataset D.\nThe key advantage of BC over IRL is that BC does not require further interactions with the environment during training. This makes BC more applicable to real-world (non-simulated) scenarios, where collecting new trajectories can be costly, time-consuming, or even dangerous due to the exploratory actions involved (Zheng et al., 2021; Torabi et al., 2019b).\nIn addition to BC and IRL, there are adversarial approaches to IL that do not neatly fit into these two categories. These methods also necessitate environment rollouts for training. The core idea behind adversarial methods is to frame the learning process as a zero-sum game between the agent and a discriminator. The discriminator's objective is to distinguish between state-action pairs generated by the agent and those produced by the expert, while the agent tries to generate actions that fool the discriminator (Ho and Ermon, 2016; Torabi et al., 2019a)."}, {"title": "3 PROBLEM ANALYSIS: ACTION DIFFERENCE IN ENSEMBLE BEHAVIOR CLONING", "content": "When training an ensemble of N policies, denoted as \\{\\pi_i\\}\\_{1\\le i \\le N}\\), on a given dataset \\(D = \\{(s_t, a_t)\\}\\_t\\) consist-"}, {"title": "Definition 1 (Mean Action Difference).", "content": "Let \\(E = {\\pi_i}\\)\\_{1<i<N}\\) represent an ensemble of N policies, and let s be a given state. For the ensemble E, we can compute N action predictions, denoted as \\(A = {a_i = \\pi_i(s)}\\_{1<i<N}\\). The mean action difference, d, is defined as the average pairwise L2-norm between the actions in A. Formally, the mean action difference d is given by:\n\\[d=\\frac{2}{N(N-1)}\\sum_{i<j}||a_i - a_j||\\]\nThis measure quantifies the average difference between the action predictions of the ensemble members for a particular state s. A higher value of d indicates greater divergence in the actions predicted by the policies, whereas a lower value suggests that the ensemble members are more consistent in their predictions.\nIn Figure 2 (left), we illustrate the mean action difference across an entire episode within the LunarLander-continuous environment. The ensemble in this experiment consisted of six policies trained on a dataset D derived from a single expert demonstration. The expert used for this demonstration was a fully-trained Soft-Actor-Critic (SAC) model from"}, {"title": "4 RELATED WORK", "content": "Imitation Learning is broadly divided into Behavior Cloning (BC) and Inverse Reinforcement Learning (IRL) (Zheng et al., 2021; Torabi et al., 2019b). While in Behavior Cloning the goal is to learn a direct mapping from states to actions (Bain and Sammut, 1995; Torabi et al., 2018; Florence et al., 2022), IRL is a two-step process. First, the missing Markov Decision Process (MDP) reward function is reconstructed from expert data, and then a policy is learned with it using classical reinforcement learning (Arora and Doshi, 2021; Ng et al., 2000). Besides BC and IRL, there are also adversarial methods such as GAIL (Ho and Ermon, 2016) or GAIFO (Torabi et al., 2019a). BC approaches cannot be adequately compared to IRL or adversarial methods, since the latter two require access to the environment for sampling additional episodes. Therefore, we compare our approach only against other Behavior Cloning approaches: BC (Bain and Sammut, 1995) and Ensemble BC (Yang et al., 2022).\nBesides these approaches, other Behavior Cloning algorithms exist which, however, require additional inputs or assumptions. In (Brantley et al., 2019) the algorithm Disagreement-regularized imitation learning is presented that first learns a standard BC ensemble. In the second phase, another policy is learned by using a combination of BC and RL, in which the reward function is to not drift into states where the variance of the ensemble action predictions is large. The policy therefore has two goals: (1) it should act similarly to the expert (2) the policy should only perform actions that ensure the agent doesn't leave the expert's state distribution. However, this approach also requires further interactions with the environment making it inappropriate to compare against a pure Behavior Cloning algorithm. The similarity to our approach Swarm BC is that both algorithms try to learn a policy that has a low mean action difference. Our algorithm can therefore be understood as a completely offline version of Disagreement-regularized imitation learning.\nIn (Smith et al., 2023) the proposed algorithm"}, {"title": "5 SWARM BEHAVIOR CLONING", "content": "In this section, we introduce our proposed approach, Swarm Behavior Cloning (Swarm BC), which aims to reduce the divergence in action predictions among ensemble policies by encouraging them to learn similar hidden feature representations.\nWe assume that each of the N policies in the ensemble \\(E = {\\pi_i}\\)\\_{1<i<N}\\) is modeled as a standard Multilayer Perceptron (MLP). The hidden feature activations of policy \\(\\pi_i\\) at hidden layer k, given input state s, are represented as \\(h_{i,k}(s) \\in R^m\\), where m is the number of neurons in that hidden layer. These hidden activations form the basis of the ensemble's predictions.\nConsider a training data point \\((s,a) \\in D\\), where s is the state and a is the expert's action. In standard Behavior Cloning (BC), each policy in the ensemble is trained individually using a supervised learning loss function. The goal is to minimize the difference between each policy's predicted action \\(\\pi_i(s)\\) and the corresponding expert action a. The standard loss for training a BC ensemble is given by:\n\\[L(s,a) = \\sum_i (\\pi_i(s) - a)^2\\]\nThis formulation treats each policy independently, which can lead to divergence in their predicted actions, especially in underrepresented states, resulting in a high mean action difference.\nThe core idea behind Swarm BC is to introduce an additional mechanism that encourages the policies to learn more similar hidden feature activations, which in turn reduces the variance in their predicted actions. This is achieved by modifying the standard loss function to include a regularization term that penalizes large differences in hidden feature activations between policies. The adjusted loss function is:\n\\[L(s,a) = \\sum_i (\\pi_i(s) - a)^2 + \\tau \\sum_k \\sum_{i<j} (h_{i,k}(s) - h_{j,k}(s))^2\\]\nThe first term is the standard supervised learning loss, which minimizes the difference between the predicted action \\(\\pi_i(s)\\) and the expert action a. The second term introduces a penalty for dissimilarity between the hidden feature activations of different policies at each hidden layer k. The hyperparameter \\(\\tau\\) controls the balance between these two objectives: reducing action divergence and maintaining accuracy in reproducing the expert's behavior.\nBy incorporating this regularization term, the individual policies in the ensemble are encouraged to align their internal representations of the state space, thereby reducing the mean action difference. At the same time, the diversity of the ensemble is preserved to some extent, allowing the individual policies to explore different solution paths while producing more consistent outputs.\nThe final action of the ensemble, known as the ensemble action, is computed as the average of the actions predicted by the N policies, following the standard approach in ensemble BC:"}, {"title": "Algorithm 1 Swarm Behavior Cloning", "content": "Algorithm 1 Swarm Behavior Cloning\nInput: expert data \\(D = \\{(s, a = \\pi_e(s))\\}\\)\nParameters:\n\\(\\tau\\) (regularization coefficient)\nN (number of policies in the ensemble)\nOutput: trained ensemble \\(E = {\\pi_i}\\)\\_{1<i<N}\\). Predict an action a for a state s using formula (3)\n1: initialize N policies \\(E = {\\pi_i}\\)\\_{1<i<N}\n2: train ensemble E on D using loss (2)\n3: return trained ensemble E"}, {"title": "6 EXPERIMENTS", "content": "In this experiments section, we want to verify the following two hypotheses:\n\u2022 Using our algorithm Swarm Behavior Cloning we can reduce the mean action difference as defined in Definition 3.1 compared to standard Ensemble Behavior Cloning.\n\u2022 Swarm Behavior Cloning shows a better performance compared to baseline algorithms in terms of mean episode return.\nFor testing these hypotheses we used a large set of eight different OpenAI Gym environments (Brockman et al., 2016): HalfCheetah, BipedalWalker, LunarLander-continuous, CartPole, Walker2D, Hopper, Acrobot and Ant. They resemble a large and diverse set of environments containing discrete and continuous action spaces and observation space sizes ranging from 4-dim to 27-dim.\nTo examine if Swarm BC improves the test performance of the agent we used a similar setting as in (Ho and Ermon, 2016). We used trained SAC- (for continuous action spaces) and PPO- (for discrete action spaces) models from Stable-Baselines 3 (Raffin et al., 2021) as experts and used them to create datasets D containing x \u2208 [1,8] episodes. Then we trained our approach and two baseline approaches until convergence. We repeated this procedure for 5 seeds. The result is presented in Figure 3.\nFor easier comparison between environments, we scaled the return. For this, we first determined the mean episode return following the expert policy \\(R_{expert}\\) and the random policy \\(R_{random}\\). Then we used the formula \\(R_{scaled} = (R \u2013 R_{random})/(R_{expert} - R_{random})\\) for scaling the return into the interval [0,1]. 0 represents the performance of the random policy and 1 of the expert policy.\nThe solid lines in Figure 3 show the mean test performance for 20 episodes and 5 seeds. The shaded areas represent the standard deviation.\nThe main conclusion we can draw from this experiment is that Swarm BC was nearly never worse than BC or Ensemble BC and in larger environments significantly better. In HalfCheetah, for example, the agent achieved a mean scaled episode return of 0.72 using Swarm BC for datasets D containing 8 expert episodes and just 0.17 using Ensemble BC. Ensemble BC still performed better than single BC in most environments.\nTo test whether our approach does reduce the mean action difference as introduced in Definition 3.1 we used the same trained models from the previous experiment and calculated the mean action difference for each timestep in the test episodes. The x-axes in Figure 4 represent the timestep and the y-axes represent the mean action difference. The plots show the average for 20 episodes and 5 seeds. Swarm Behavior Cloning did reduce the mean action difference, but not always to the same extend. In the BipedalWalker environment it was reduced by almost 44% while in Ant it was only reduced by 11%.\nNevertheless, we can verify the hypothesis that Swarm BC does reduce the mean action difference.\nSwarm BC shows significantly better performance compared to baseline algorithms with nearly no computational overhead. The main disadvantage however is the introduction of another hyperparameter \\(\\tau\\). To test the sensitivity of this parameter we conducted an ablation study regarding \\(\\tau\\) and also about N (the number of policies in the ensemble E). The results are plotted in Figure 5. For this ablation study, we used the Walker2D environment and again scaled the return for better comparison. For \\(\\tau\\) we tested values within \\(\\{0.0, 0.25, 0.5, 0.75, 1.0\\}\\). The main conclusion for the ablation in \\(\\tau\\) is that too large values can decrease the performance. \\(\\tau = 0.25\\) worked best so we used this value for all other experiments in this paper.\nFor the ablation on N we tested values in the set \\(\\{2, 4, 6, 8\\}\\). For N = 2 the test performance was significantly below the test performance for N = 4. For larger ensembles (N = 6 and N = 8) the performance did not increase significantly anymore. But since the training time scales linearly with the number of policies N we chose N = 4 for all experiments."}, {"title": "7 THEORETICAL ANALYSIS", "content": "Let \\((s, a) \\in D\\) be a random but fixed state-action tuple out of the training dataset D. Let \\(E = {\\pi_i}\\)\\_{1<i<N}\\) be an ensemble of N policies, each represented as an MLP containing K hidden layers. For state s the ensemble E produces N hidden feature activation \\(h_{i,k}\\) for each layer k \u2208 [1, K].\nThe basic idea of our approach Swarm BC is to train an ensemble E that produces similar hidden features:\n\\[\\forall (i, j) \\in [1,N]^2, k \\in [1,K] : h_{i,k} \\approx h_{j,k}\\]\nBy doing so the ensemble tries to find features \\(h_{i,k}\\) that can be transformed by at least N different transformations to the desired action a since we have no restriction regarding the weights W, b:\n\\[h_{i,k+1} = \\sigma(W_{i,k}h_{i,k} + b_{i,k})\\]\nTraining a single neural net \\(f_{\\phi}\\) with parameters \\(\\phi\\) on D with some fixed hyperparameters Q corresponds to sampling from the probability density function (pdf):\n\\[\\phi \\sim p(\\phi | D, Q)\\]\nSince D and Q are fixed we can shorten this expression to \\(p(\\phi)\\). The pdf for hidden features \\(h_k\\) for state s corresponds to the integral over all weights that produce the same feature activations:\n\\[p(h_k | s) = \\int 1 [f_{\\phi}(s) = h_k] : p(\\phi)\\]\nFor a fixed state s we can shorten this expression to \\(p(h_k)\\).\nWe now show that training an ensemble with similar feature activations corresponds to finding the global mode of the pdf \\(p(h_k)\\).\nIf we are training a standard ensemble, we are sampling N times independently from the pdf \\(p(h_k)\\). But the pdf for sampling N times the same hidden feature activation corresponds to:\n\\[p^N(h_k) = \\frac{p(h_k)^N}{\\int p(\\phi) d\\phi}\\]\nwith\n\\[p(h_k) = \\prod_{i=1}^N p(h_k)\\]"}, {"title": "8 CONCLUSION", "content": "Behavior Cloning (BC) is a crucial method within Imitation Learning, enabling agents to be trained safely using a dataset of pre-collected state-action pairs provided by an expert. However, when applied in an ensemble framework, BC can suffer from the issue of increasing action differences, particularly in states that are underrepresented in the training data \\(D = (s_t, a_t)\\_t\\). These large mean action differences among the ensemble policies can lead to suboptimal aggregated actions, which degrade the overall performance of the agent.\nIn this paper, we proposed Swarm Behavior Cloning (Swarm BC) to address this challenge. By fostering greater alignment among the policies while preserving the diversity of their computations, our approach encourages the ensemble to learn more similar hidden feature representations. This adjustment effectively reduces action prediction divergence, allowing the ensemble to retain its inherent strengths such as robustness and varied decision-making\u2014while producing more consistent and reliable actions.\nWe evaluated Swarm BC across eight diverse OpenAI Gym environments, demonstrating that it effectively reduces mean action differences and significantly improves the agent's test performance, measured by episode returns.\nFinally, we provided a theoretical analysis showing that our method approximates the hidden feature activations with the highest probability density, effectively learning the global mode \\(h = \\text{argmax}\\_h p(h_k|\\phi; D)\\) based on the training data D. This theoretical insight further supports the practical performance gains observed in our experiments."}]}