{"title": "BanglaQuAD: A Bengali Open-domain Question Answering Dataset", "authors": ["Md Rashad Al Hasan Rony", "Sudipto Kumar Shaha", "Rakib Al Hasan", "Sumon Kanti Dey", "Amzad Hossain Rafi", "Amzad Hossain Rafi", "Ashraf Hasan Sirajee", "Jens Lehmann"], "abstract": "Bengali is the seventh most spoken language on earth, yet considered a low-resource language in the field of natural language processing (NLP). Question answering over unstructured text is a challenging NLP task as it requires understanding both question and passage. Very few researchers attempted to perform question answering over Bengali (natively pronounced as Bangla) text. Typically, existing approaches construct the dataset by directly translating them from English to Bengali, which produces noisy and improper sentence structures. Furthermore, they lack topics and terminologies related to the Bengali language and people. This paper introduces BanglaQuAD, a Bengali question answering dataset, containing 30,808 question-answer pairs constructed from Bengali Wikipedia articles by native speakers. Additionally, we propose an annotation tool that facilitates question-answering dataset construction on a local machine. A qualitative analysis demonstrates the quality of our proposed dataset.", "sections": [{"title": "1. Introduction", "content": "Question answering (QA) over unstructured text is challenging, as the system requires understanding of a wide range of vocabulary and question types. Machine reading comprehension (MRC) is an extractive question answering task that focuses on detecting a continuous answer span within a passage for a given question. Dataset plays a key role in developing such QA systems. A dataset that covers a wide range of vocabulary, question types, and variable answer lengths can facilitate the system with more textual patterns to learn.\nAlthough question answering over English text is a widely studied topic (Rajpurkar et al., 2016, 2018; Rony et al., 2022), only a few research works focus on Bengali question answering (Tahsin Mayeesha et al., 2021; Bhattacharjee et al., 2022). The lack of a high-quality Bengali QA dataset is one of the primary reasons for this. Existing Bengali QA systems use neural translators (Devlin et al., 2019) to transform an English QA dataset into Bengali to obtain training data (Bhattacharjee et al., 2022; Aurpa et al., 2022). These types of data sets suffer from three major issues. Firstly, the meaning of the translated Bengali text is often incorrect due to the use of a neural translator. Secondly, they often contain grammatically and structurally incorrect text. Thirdly, they lack terminologies from the Bengali domain (i.e., topic, place, and individuals), because English datasets are built from articles largely related to the English speaking world.\nAddressing the shortcomings, we present an open-domain Bengali question answering dataset, BanglaQuAD. BanglaQuAD contains 30,808 high-quality human-annotated question-answer pairs.\nSpecifically, we select 658 articles from more than 12,000 Bengali Wikipedia articles based on importance and frequently encountered topics. We follow the official Wikipedia categories of articles and dive deep one level into sub-categories to further widen the scope of the dataset. This selection process ensures a broader coverage of topics, resulting in a wide range of vocabulary from different topics. Human annotators are employed to construct the question-answer pairs from passages of the curated articles. This ensures that the data is of the best possible quality. To diversify and extend the understanding of QA systems, BanglaQuAD includes diverse question types, unanswerable questions, answers with variable lengths. Furthermore, we introduce a Bengali annotation tool, BnAnno, by extending the cdQA \u00b9 tool to construct a Bengali question answering dataset. BnAnno supports unstructured text and converts them into the popular SQUAD dataset format after the annotation. The SQuAD dataset format is widely adopted for developing QA systems (Bhattacharjee et al., 2022). The user interface of BnAnno is developed in Bengali to assist native Bengali annotators. The contributions of this paper are summarized as follows:\n\u2022 A high quality human-annotated Bengali question-answer dataset, BanglaQuAD, suitable for machine reading comprehension and information retrieval tasks. BanglaQuAD contains 30,808 question-answer pairs, annotated by native Bengali speakers.\n\u2022 An annotation tool, BnAnno, to construct question-answer pairs from unstructured text data,"}, {"title": "2. BanglaQuAD", "content": "BanglaQuAD can be used as a benchmark for developing question answering and information retrieval systems on Bengali text. We made the dataset and annotation tool publicly available 2."}, {"title": "2.1. Dataset Generation Workflow", "content": "To construct BanglaQuAD, first, we assign three human annotators to shortlist 658 Wikipedia articles depending on their importance and the likeliness of a question being asked by a Bengali speaking person. To eliminate noisy text, the curated articles are passed through a cleaning process before starting the annotation process. A group of annotators construct question answer pairs based on the assigned articles using BnAnno (\u00a72.2). The data set construction pipeline is depicted in Figure 1. The following subsections provide a brief description of the dataset construction process."}, {"title": "Data Collection:", "content": "BanglaQuAD is developed based on Bengali Wikipedia articles\u00b3. We divided Bengali Wikipedia articles into 14 categories and 114 subcategories following the official topic types. Annotators directly copied the content of the article into the annotation tool to start the annotation process. To eliminate noisy text references (i.e., [3] and [3,5]) and an English texts are removed from the text."}, {"title": "Data Annotation:", "content": "We instruct seven native Bengali-speaking human annotators (four from Computer Science (CS) and three from non-CS background) to first construct questions from a set of passages and then select an answer span in the passage that answers the question. We uniformly distributed 658 articles among the annotators. The articles are selected based on their importance and frequency. Next, the annotators utilized our annotation tool to construct question-answer pairs by following the annotation guidelines listed below:\n\u2022 Please go through each sentence in a given passage and create only factual question-answer pairs whenever possible. If there exist multiple facts in a given sentence, then please create multiple question-answer pairs for each fact in that.\n\u2022 Do not ask questions that require a summary of the text to answer. Only ask questions where the answer is a continuous span in the given passage.\n\u2022 At least construct five unanswerable questions from each article.\nA JavaScript Object Notation (JSON) file containing the annotated data in SQuAD format is generated by the annotation tool for each article upon completion. Finally, we combine all the generated JSON files into a single file to obtain the final version of BanglaQuAD."}, {"title": "2.2. BnAnno: A Bengali Annotation Tool", "content": "In this work, we extended the cdQA annotation tool to support Bengali language. The standard cdQA tool requires the users to pre-process the data into SQUAD-like format in order to use the tool. In contrast, in BnAnno the user can copy any text document to the tool and start the annotation right away without having to think about pre-processing and the output format. This saves a lot of time and"}, {"title": "2.3. Dataset Statistics:", "content": "BanglaQuAD covers a wide range of general topics besides the Bengali domain, depicted in Figure 4. Furthermore, Table 1 shows core dataset statistics. On average, about 47 questions were asked from each article, indicating the depth and coverage of various types of facts and related questions. Figure 2 depicts the distribution of question and answer lengths. A wide range of question and answer lengths makes the dataset very challenging. Our intuition is that it would stress the capabilities of QA systems. Furthermore, BanglaQuAD has a vocabulary size of 87,482, spanning 14 major topics that include life and Bengali language related domains: Physical Science and Mathematics, Engineering and Technology, Health and Medicine, Life, Earth and Geography, Society and Social Sciences, Business and Economics, Religion and Philosophy, Language and Literature, Art, Sports and Recreation, History, Women and feminism, Biography, and India. This would enable the QA system to handle a wide range of contexts. We split the dataset into 80-20 train-test split to obtain 24,646 train and 6,162 test data."}, {"title": "3. Experiments and Analysis", "content": "Experimental Setup: We select baseline models that are specifically trained on Bengali language: BanglaBERT (Bhattacharjee et al., 2022) and IndicBERT (Kakwani et al., 2020). BanglaBERT is a pre-trained BERT (Devlin et al., 2019) model trained on Bengali text where IndicBERT is a multilingual model trained on Indian language. The baseline models are trained with the recommended hyper-parameters.\nAs benchmark dataset we select publicly available Bengali QA datasets: UDDIPOK (Aurpa et al., 2023) and TYDI QA (Clark et al., 2020). UDDIPOK is a crowd sourced question answering dataset collected from newspaper, textbook and exam questions. The dataset contains 2,908 training and 728 test samples. TYDI QA is a non-translation based crowd sourced multilingual question answering dataset. It contains 10,768 training and 334 test data. We develop BanglaQuAD a machine reading comprehension dataset constructed based on Bengali Wikipedia article. The dataset contains 24,646 training and 6,162 test data."}, {"title": "Quantitative Analysis:", "content": "Table 2 reports the results in Exact Match (EM) and F1 score. In both cases on the BanglaQuAD dataset the baseline models obtained the lowest score. This result exhibits that pre-trained models fine-tuned on BanglaQuAD find it difficult to understand diverse range of topics and question types. This characteristics of BanglaQuAD would stress the comprehension capabilities of QA models and help develop robust QA systems."}, {"title": "Qualitative Analysis:", "content": "Figure 5 demonstrates three example question-answer pairs based on a given passage. The examples of QA pairs include three types of Wh-questions (what, why, and where), leading to answers with variable lengths. To challenge the development of QA systems, we include questions where the answers do not exist in the given paragraph. The second example question exhibits such a scenario. BanglaQuAD contains nine types of Wh-questions. The distribution is as follows: what (13%), when (12%), where (12 %), who (7%), whom (6%), which (6%), whose (5 %), why (6%), and how (4%). The rest of the 29% questions start with a keywords or names. A QA system trained on a wide variety of question types would improve the systems' language understating capabilities and question-answering performance.\nTo assess the quality of the annotation process, we randomly chose 100 data points and asked two annotators to select an answer span, given a question and a passage. The inter-annotator agreement (Cohen's kappa k) of the answer span selection experiment is 0.79)."}, {"title": "4. Related Work", "content": "Machine reading comprehension is a widely adopted form of question answering over unstructured text (Chen et al., 2017; Guu et al., 2020; Yang et al., 2019; Raffel et al., 2020). Recent large-scale MRC datasets contributed to the rapid development of MRC systems (Xiong et al., 2019; Jing et al., 2019; Xiong et al., 2019; Rajpurkar et al., 2016; Rony et al., 2022). The most popular one is SQUAD (Rajpurkar et al., 2018) introduces 100,000 open-domain English question-answer pairs. In a different work, an investigation (Dzendzik et al., 2021) on 60 English MRC datasets reveals that Wikipedia is commonly used as a source of unstructured data. Furthermore, they suffer from a deficiency of question types beginning with why, when, and where.\nBengali MRC systems typically translate English data into Bengali for developing question answering systems (Bhattacharjee et al., 2022; Aurpa et al., 2022; Saha et al., 2021). Very recently, (Bhattacharjee et al., 2022) trained a BERT (Devlin et al., 2019) model on a translated SQUAD dataset to develop an open domain QA system. In a different work, (Aurpa et al., 2022) translated 3,675 question-answer pairs from a COVID-19-related English MRC dataset into Bengali. A non-translation based Multilingual TYDI QA (Clark et al., 2020) is also proposed where only 11,430 Bengali QA-pairs are available. In contrast to the existing datasets, we focus on maintaining quality in addition to quantity. We select articles based on importance and based on the Bengali domain (i.e., topic, place, and individuals). We instruct human annotators to ask diverse type of questions, leading to factual and variable-length answers."}, {"title": "5. Conclusion", "content": "We have presented BanglaQuAD, a Bengali question answering dataset containing 30,808 high-quality human-annotated question-answer pairs. Furthermore, we developed an annotation tool to construct question-answer pairs from unstructured text data. We made the dataset and annotation tool publicly available to promote research on the Bengali language and text. In the future, we plan extend the dataset and create a question answering public leader-board based on BanglaQuAD to encourage further research on open-domain question answering."}, {"title": "6. Ethical Considerations", "content": "We carefully checked and confirm that the work conducted in this paper does not violate any ethical considerations."}]}