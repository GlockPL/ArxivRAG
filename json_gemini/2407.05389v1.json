{"title": "Image-Conditional Diffusion Transformer for Underwater Image Enhancement", "authors": ["Xingyang Nie", "Su Pan", "Xiaoyu Zhai", "Shifei Tao", "Fengzhong Qu", "Biao Wang", "Huilin Ge", "Guojie Xiao"], "abstract": "Underwater image enhancement (UIE) has attracted much attention owing to its importance for underwater operation and marine engineering. Motivated by the recent advance in generative models, we propose a novel UIE method based on image-conditional diffusion transformer (ICDT). Our method takes the degraded underwater image as the conditional input and converts it into latent space where ICDT is applied. ICDT replaces the conventional U-Net backbone in a denoising diffusion probabilistic model (DDPM) with a transformer, and thus inherits favorable properties such as scalability from transformers. Furthermore, we train ICDT with a hybrid loss function involving variances to achieve better log-likelihoods, which meanwhile significantly accelerates the sampling process. We experimentally assess the scalability of ICDTs and compare with prior works in UIE on the Underwater ImageNet dataset. Besides good scaling properties, our largest model, ICDT-XL/2, outperforms all comparison methods, achieving state-of-the-art (SOTA) quality of image enhancement.", "sections": [{"title": "I. INTRODUCTION", "content": "UNDERWATER imaging has been widely used in underwater archaeology, underwater robotics, marine detection, and other fields [1]\u2013[3]. However, wavelength- and distance-dependent light attenuation and scattering cause the problem of low contrast and color deviation in underwater images. These degraded images not only lead to unsatisfactory visual experience for humans but also affect the performance in computer vision tasks like object detection, image classification, and semantic segmentation.\nTo enhance the quality of underwater images, scholars have proposed a series of effective measures, promoting the development of underwater image enhancement (UIE). Conventional UIE methods [4]\u2013[9] (e.g., white balance [4], [5] and histogram equalization [6]) depend on assumptions or a priori knowledge, models, or design guidelines to enhance underwater images. Although conventional methods are easy to execute, they are not adaptable to different water environment and lighting conditions.\nThe rapid development of deep learning brings new insights and tools for UIE. By leveraging large-scale underwater image datasets to learn the patterns and features, deep-learning-based UIE methods can achieve intelligent, automated, and end-to-end underwater image enhancement. Deep-learning-based UIE methods are typically divided into two primary categories: 1) convolutional neural network (CNN)-based [10]\u2013[15] and 2) generative adversarial network (GAN)-based [16]\u2013[21]. The CNN-based UIE methods train deep CNNs to learn the mapping relationship from the degraded image to the high-quality reference image [10], which is robust to different underwater scenes. The GAN-based UIE can also accomplish the conversion from the degraded underwater image to the corresponding ground truth image and achieve good performance [16]. However, GANs often suffer from unstable training process and mode collapse. Therefore, it is necessary to develop more stable and diversified methods to further improve the effect and quality of UIE.\nDenoising diffusion probabilistic models (DDPM) have begun to attract extensive attention for its good convergence properties and relatively stable training process. DDPMs have emerged as a new state-of-the-art (SOTA) baseline in the field of image generation [22], [23]. As scholars continue to explore diffusion models, their potential for image-to-image generation is continually developed [24]. In image restoration, image coloring, and image super-resolution tasks, diffusion model-based approaches have produced superior results than GAN models [22]\u2013[24]. Although DDPMs can yield excellent results, an acknowledged challenge of DDPMs is the long inference time, which leads to poor real-time performance.\nIn this article, a generative approach for UIE based on conditional DDPM is proposed, which uses the degraded image as the conditional input. As for the model architecture, the conventional U-Net [25] backbone in a diffusion model is replaced with a transformer, which we call image-conditional diffusion transformer (ICDT). Diffusion transformer adheres to many good practices of vision transformers (ViT) [26], perhaps one of the most important factors is the excellent scalability properties compared to traditional convolutional networks."}, {"title": "II. RELATED WORK", "content": "Conventional UIE methods depend on assumptions or a priori knowledge, models, or design rules to enhance underwater images. These methods are based on physical models and utilize image degradation priors to inversely solve degradation models. Drews et al. [7] introduced a UIE method using underwater dark channel prior (DCP), which utilizes the statistical prior of images obtained in outdoor natural scenes and applies it only in the blue and green channels to improve enhancement quality. Li et al. [6] minimized the information loss of the enhanced underwater images to obtain the transmission map and improved the brightness and contrast of underwater images based on natural image histogram distribution prior. Wang et al. [8] proposed the maximum attenuation identification method for UIE based on a simplified underwater light propagation model. Ancuti et al. [4] combined the Laplacian pyramid and white balance to produce two enhanced results, which were then fused via weight mapping. Peng et al. [9] generalized the common DCP method to image restoration. They estimated scene transmission through calculating the difference between the ambient light and the observed intensity. In addition, they incorporated adaptive color correction into the image formation model to remove color casts and restore contrast.\nAlthough these methods have specific theoretical support, they may still suffer from under-enhancement or over-enhancement under challenging water environment and lighting condition. One reason is that the underwater environment is complex and changeable, and physical parameters are difficult to get; the other reason is that the underwater image degradation model is different from that on land and difficult to establish."}, {"title": "B. Deep-Learning-Based UIE Method", "content": "With tremendous progress of deep learning in computer vision, deep-learning-based methods have emerged as the baseline for UIE tasks in recent years. The deep learning-based methods trained with different types of underwater images can break through the limitation of conventional methods. Deep-learning-based methods mainly consist of two categories: CNN-based and GAN-based.\nCNN extracts hierarchical high-level image features based on convolution and pooling, enabling automatic learning of effective features in underwater images. Among them, Sun et al. [11] proposed a deep pixel-to-pixel network with an encoder-decoder framework for UIE. It uses convolution layers as encoder for noise filtering, while uses deconvolution layers as decoder for detail recovery and image refinement, achieving self-adaptive data-driven image enhancement without considering the physical environment. Li et al. [12] proposed a gated fusion network called Water-Net for UIE, which fuses the input images into the enhanced result according to the predicted confidence maps. The predicted confidence maps learnt by Water-Net determine the most significant features of input images left in the enhanced result. Li et al. [13] presented a light-weight CNN (UWCNN) based on underwater scene prior, which is trained with the corresponding data to accommodate various underwater scenes. Naik et al. [14] proposed a shallow neural network architecture (Shallow-UWnet) for UIE, which has fewer parameters than the state-of-art models while preserving good performance. Li et al. [15] utilized CNN coupled with attention mechanism to extract the most discriminative features from multiple color spaces and adaptively integrate and highlight them, which improves the visual quality of underwater images effectively. The above methods benefit from the efficacy and generalization ability of CNN, which brings considerable improvement to UIE. However, the finite receptive field restricts CNN-based methods from modeling long-range pixel dependence, and the fixed"}, {"title": "C. Denoising Diffusion Probabilistic Models", "content": "DDPMs [31] have made great success in the field of image generation [32]\u2013[36], which outperforms GANs, the previous SOTA method, in many tasks. DDPM is a class of simplified diffusion model which applies variational inference for modeling and reparameterization trick for sampling. In general, the DDPM includes two processes: the forward noising process (i.e., diffusion process) and the reverse denoising process. The forward noising process gradually injects Gaussian noise to the clean image, making it more and more random and blurry until it approximates an isotropic Gaussian distribution. The reverse denoising process reconstructs the original image from a Gaussian-distributed noisy image by gradually removing noise. The reverse process trains a neural network to estimate the conditional probability distribution at each step, more specifically, the data distribution of the previous step given the current data. DDPM has demonstrated the advantage of more stable training process and higher quality and diversity of synthesis.\nRecent advances of DDPMs have been largely due to reformulating DDPMs to estimate noise rather than pixels [31], utilizing cascaded DDPM pipelines in which low-resolution base diffusion models are trained in concurrence with upsamplers [32], [37], and improved sampling techniques [31], [38], [39]. For example, Nicholas et al. [27] introduced improved DDPMs (IDDPM), which can obtain better log-likelihood and high-quality samples. Saharia et al. [37] presented a class-conditional diffusion model named cascaded diffusion models (CDM) for high-fidelity image generation, which utilizes category labels as input conditions to produce images of the corresponding category. Saharia et al. [23] also proposed super-resolution via repeated refinement (SR3). SR3 is a conditional DDPM that uses a low-resolution image as input condition and generates the corresponding high-resolution image through an iterative stochastic denoising process.\nCurrently, DDPMs have been applied to image reconstruction tasks [35], [40], [41]. Kawar et al. [42] introduced denoising diffusion restoration models (DDRMs) which utilizes a pre-trained denoising diffusion generative model to solve a linear inverse problem since image restoration can usually be represented as a linear inverse problem. DDRM denoises a sample to the desired output gradually and stochastically, conditioned on the inverse problem and measurements. In this way a variational inference objective is employed to learn the posterior distribution, from which images are produced efficiently. Lu et al. [43] first presented a DDPM-based UIE method (UW-DDPM), which uses two U-Net networks for image distribution transformation and image denoising, effectively enhancing the underwater images. However, that work does not address the issue of slow sampling speed of DDPMS and its application scenarios are substantially limited. Lu et al. [44] further proposed SU-DDPM for UIE, which reduces the sampling step and changes the initial sampling distribution to accelerate the sampling inference process and achieve real-time enhancement. Additionally, SU-DDPM combines the reference image with the degraded image in the diffusion process, effectively solving the issue of color deviation and improving the quality of underwater images. Convolutional U-Nets [25] are presently the prevailing choice for backbone architecture of DDPMs. Concurrent work [45] introduced a new, efficient DDPM architecture which is based on attention mechanisms. Inspired by these diffusion models mentioned above, we propose ICDT to enhance underwater images."}, {"title": "III. IMAGE-CONDITIONAL DIFFUSION TRANSFORMER", "content": "Conditional diffusion models take supplemental information as input. For UIE task, we use degraded image as the extra conditional input. Additionally, a few simple modifications are done following [27] to make DDPMs achieve better log-likelihoods and sample much faster without sacrificing sample quality."}, {"title": "A. Image-Conditional Denoising Diffusion Probabilistic Mod-els", "content": "DDPMs [31] are a kind of generative models which progressively transforms a Gaussian noise distribution into the distribution of data on which the model is trained. The forward noising process is a Markov process which gradually corrupts\n$q(x_{t}|x_{t-1}) = \\mathcal{N}(x_{t}; \\sqrt{1 - \\beta_{t}}x_{t-1}, \\beta_{t}I).$\nGiven sufficiently large T and an appropriate schedule of $\u03b2_t$, $x_T$ approximately obeys an isotropic Gaussian distribution. Thus, if the reverse distribution $q(x_{t-1}|x_t)$ is known, we can start from $x_T \\sim \\mathcal{N}(0, I)$ and run the reverse process to obtain a sample from $q(x_0)$.\nDuring training we sample a data pair $(x_0,x)$ where x is a degraded image and $x_0$ is the corresponding ground truth image. Conditional diffusion models use a neural network to learn the reverse process defined by the conditional joint distribution\n$p_{\\theta}(x_{0:T}|\\overline{x}) = p(x_{T}) \\prod_{t=1}^{T} p_{\\theta}(x_{t-1}|x_{t}, \\overline{x}),$\n$p_{\\theta}(x_{t-1}|x_{t}, \\overline{x}) = \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_{t}, \\overline{x}, t), \\Sigma_{\\theta}(x_{t}, \\overline{x}, t)),$\nso that the sampled image in the diffusion process has high fidelity to the data distribution conditioned on $\\overline{x}$ (cf. Fig. 1). Here $\\mu_{\\theta}(x_{t}, \\overline{x}, t)$ and $\\Sigma_{\\theta}(x_{t}, \\overline{x}, t)$, the statistics of $p_{\\theta}$, are estimated by the neural network where $\\overline{x}$ is provided as input.\nThe loss function which the model is trained with is a variational lower bound (VLB) [46] of the negative log likelihood $E_{q(x_{0})}[-\\log p_{\\theta}(x_{0})] \\leq L_{vlb}$, and the VLB can be written as [31], [32]\n$L_{vlb} = E_{q}[D_{KL}(q(x_{T}|x_{0})||p(x_{T})) - \\log p_{\\theta}(x_{0}|X_{1},\\overline{x})\\n+ \\sum_{t>1} D_{KL}(q(x_{t-1}|x_{t}, x_{0})||p_{\\theta}(x_{t-1}|x_{t}, \\overline{x}))].$\nThe loss can be efficiently optimized through stochastic gradient descent over uniformly sampled $L_{t-1}$ terms [31], taking into account that we can sample intermediate $x_t$ from an arbitrary step of the diffusion process with the marginal\n$q(x_{t}|x_{0}) = \\mathcal{N}(x_{t}; \\sqrt{\\overline{a}_{t}}x_{0}, (1 - \\overline{a}_{t})I),$"}, {"title": "IV. EXPERIMENTAL SETUP", "content": "We first explore the scaling properties of our ICDT models. In this paper, most of the model complexity analysis is from the perspective of theoretical FLOPs which is widely used in the architecture design literature. We gauge the scalability of our ICDT models from the standpoint of forward pass complexity measured by FLOPs. The models are briefly notated with their configurations and input latent patch sizes p in the following; for instance, ICDT-XL/2 means the XLarge configuration and $p = 2$. We provide three choices for p (i.e., $p = 2, 4, 8$). Choosing smaller p leads to longer sequence length T and thus more FLOPs."}, {"title": "A. Datasets", "content": "We use the Underwater ImageNet [16] dataset, a subset of the enhancement of underwater visual perception (EUVP) [18] dataset. This dataset covers a wide scope of scene/main object categories such as coral and marine life. The EUVP dataset contains separate sets of unpaired and paired image samples of poor and good perceptual quality to facilitate supervised training of UIE models, which also offers a platform to evaluate the performance of different UIE methods. The Underwater ImageNet dataset includes 3700 paired underwater images. We randomly select 3328 pairs of the images from it to create the training set, while the remaining 372 pairs constitute the test set."}, {"title": "V. EXPERIMENTS", "content": "We train 12 models covering four model configurations (S, B, L, XL) and three different choices of patch size (8, 4, 2). Three top subfigures in Fig. 3 show how PSNR changes with model size when patch size is kept constant. Throughout all four configurations, considerable improvements in PSNR are attained over all training stages by using deeper and wider transformer. Similarly, four bottom subfigures in Fig. 3 demonstrate PSNR as patch size is changing and model size is fixed. We again observe significant PSNR improvements across training by merely scaling the number of tokens input to the models, holding parameters roughly constant. In summary, we discover that increasing model size and decreasing patch size can substantially improve the image enhancement performance of our diffusion models.\nThe results of Fig. 3 indicate that parameter counts is not the unique factor which determines the image enhancement quality of our models. As model size remains unchanged and patch size decreases, the overall amount of the transformer's parameters is almost constant (slightly decreased actually), while only FLOPs increases. The results suggest that scaling model FLOPs is crucial to the performance of our models. To clarify this further, we plot the PSNR versus model FLOPS in Fig. 4. The results illustrate that different models have comparable performance in terms of PSNR if their total FLOPs are similar (e.g., ICDT-B/4 and ICDT-S/2). We observe a significant positive correlation between PSNR and model FLOPs, indicating that increased model computation is the key ingredient to improve the models' performance. We observe the same trend in Fig. 1 of Supplementary Materials for other metrics such as SSIM.\nIn Fig. 5, we plot PSNR against total training compute for all models, where training compute is approximately estimated as model FLOPs batch size training iterations \u00b73 (the factor of 3 approximates the backwards pass to be twice as compute-intensive as the forward pass). We find that models with small FLOPs (small model size or large patch size), despite being trained longer, eventually have inferior performance relative to models with large FLOPs trained for fewer iterations. For example, L/2 is outperformed by XL/2 after roughly $6.6 \u00d7 10^8$ GFLOPs. After sufficient training, computational intensive models behavior better even when controlling for total training FLOPs; that is to say, they are more compute-efficient.\nWe illustrate the impact of scaling on sample quality in Fig. 6. At 90K training iterations, we sample an image from each of the 12 models using the same initial noise $x_T$ and sampling noise. This visually demonstrates that increasing either model size or the number of tokens results in considerable improvements in visual quality."}, {"title": "B. Comparison with Other UIE Methods", "content": "After analyzing the scalability, we compare the most computational intensive model, ICDT-XL/2, with SOTA UIE methods. These methods includes five CNN-based methods (Water-Net [12], UWCNN [13], Shallow-UWnet [14], UIEC^2-Net [58], and RAUNE-Net [59]) as well as two GAN-based methods (FUnIEGAN [18] and MLFCGAN [19]).\nWe visualize several results in Fig. 7. In Fig. 7, the proposed ICDT effectively remits color casts and removes the haze on the underwater images, producing visually pleasing results. By contrast, the competing methods induce low brightness and contrast (e.g., UWCNN [13], UIEC^2-Net [58], and FUnIEGAN [18]) or introduce blurring (e.g., UWCNN [13]), unexpected colors (e.g., MLFcGAN [19]), and artifacts (e.g., RAUNE-Net [59] and FUnIEGAN [18]).\nTable II reports the results of quantitative comparisons with SOTA UIE methods in terms of PSNR, SSIM, LPIPS, and UIQM. The full-reference image quality metrics PSNR, SSIM, and LPIPS are obtained through comparison between the result of each method and the corresponding reference image. DiT-XL/2 outperforms all competing methods, achieving the highest PSNR and SSIM as well as the lowest LPIPS. In addition, we observe that our DiT-XL/2 achieves the highest UIQM score of all prior UIE methods, indicating DiT-XL/2 also performs best in terms of non-reference image quality assessment.\nQualitative and quantitative comparisons demonstrate the effectiveness of our ICDT method."}, {"title": "VI. CONCLUSION", "content": "We introduce a novel ICDT model for UIE, which uses the degraded underwater image as the extra conditional input. ICDT employs a transformer backbone instead of the U-Net which is commonly used in DDPMs. As a result, ICDT inherits good scalability properties from the transformer model, enabling a trade-off between image enhancement quality and"}]}