{"title": "DIFFERENTIAL PRIVACY OVERVIEW AND FUNDAMENTAL TECHNIQUES", "authors": ["Ferdinando Fioretto", "Pascal Van Hentenryck", "Juba Ziani"], "abstract": "This chapter is meant to be part of the book \u201cDifferential Privacy in Artificial Intelligence: From Theory to Practice\" and provides an introduction to Differential Privacy. It starts by illustrating various attempts to protect data privacy, emphasizing where and why they failed, and providing the key desiderata of a robust privacy definition. It then defines the key actors, tasks, and scopes that make up the domain of privacy-preserving data analysis. Following that, it formalizes the definition of Differential Privacy and its inherent properties, including composition, post-processing immunity, and group privacy. The chapter also reviews the basic techniques and mechanisms commonly used to implement Differential Privacy in its pure and approximate forms.", "sections": [{"title": "Introduction", "content": "Data is continuously harvested from nearly every facet of our lives by corporations, service providers, and public institutions. Whether through smartphones, social media interactions, internet use, healthcare visits, or financial transactions, information is continuously gathered, shaping the foundation of the modern economy. Private companies leverage this vast pool of data to evaluate loan candidates, optimize transportation networks, improve supply chains, personalize services, and predict market demands, all to enhance decision making. Similarly, public policies and government initiatives rely heavily on this data, guiding resource distribution, monitoring public health crises, and driving urban development and sustainability efforts.\nHowever, these datasets also contain a large array of sensitive information, including health, financial, or location data. Major privacy violations and breaches are commonplace, and can have severe negative impacts, not only on consumers and online users, but also on entire organizations and governments. For instance, the 2017 Equifax data breach [Wikipedia, 2024a] exposed the personal information of 147 million individuals, including social security numbers, birth dates, and addresses, leaving millions vulnerable to identity theft, fraud, and long-term financial harm. Similarly, the 2016 Facebook-Cambridge Analytica scandal [Wikipedia, 2024b], in which the personal data of up to 87 million Facebook users was harvested without consent for political advertising purposes, raised concerns about its possible influence on the outcome of the 2016 presidential election.\nPrivacy concerns have become central in today's society, driving significant changes in government policy. Various regulatory frameworks have been established, with the United States and Europe leading efforts toward stronger privacy practices. In Europe, the General Data Protection Regulation (GDPR) sets strict standards for data management, focusing on consent and data minimization [Parliament and of the European Union, 2016], while in the U.S., regulations like Title 13 [Bureau, 1998] govern the handling of census data and laws like the Health Insurance Portability and Accountability Act (HIPAA) and the California Consumer Privacy Act (CCPA) offer protections for health and consumer data [Centers for Medicare & Medicaid Services, 1996, Legislature, 2018]. This movement was further emphasized in October 2023 when the Biden administration issued an Executive Order on AI, ensuring the enforcement of consumer protection laws and introducing safeguards against privacy violations in AI systems. Government actions, such as the release of the AI Bill of Rights Blueprint [House, 2023] in the US, underscore the increasing focus on privacy in both policy and technology.\nPublic policy has also devoted extensive research into technical solutions for privacy. Over the past three decades, this research has explored a wide range of privacy definitions and techniques, but one has emerged as a pivotal framework:"}, {"title": "A Historical Perspective on Privacy", "content": "This section begins by posing a fundamental question: What are the key desiderata and properties that a robust privacy definition must guarantee? To address this, it examines historical failures of previous and current privacy definitions, highlighting the necessity for well-defined and formal guarantees. This section first outlines the main properties satisfied by Differential Privacy\u2014these properties will be formally detailed later in this chapter. It then delves into specific examples of major privacy breaches over the past 30 years, identifying for each how adherence to certain privacy desiderata could have prevented the failure.\nA central argument of this book is the importance of well-defined and formal privacy guarantees. A major weakness in many privacy techniques arises when the protections themselves are poorly specified, particularly when they fail to clearly define the classes of attacks they are designed to resist. Over the past three decades, numerous privacy attacks have exploited such ambiguities, often by applying privacy notions beyond their intended use cases. To address these challenges, this chapter focuses on four main desiderata that a strong privacy definition should satisfy:\n1. Desiderata 1: Compositionality. A good privacy definition should ensure that its protections gracefully degrade when applied multiple times, whether across several datasets or through repeated private data analyses. In a data-driven world, where datasets are frequently analyzed multiple times and may contain overlapping information about individuals, composition is crucial. Without it, repeated analyses can cumulatively erode privacy safeguards and ultimately compromise individual privacy.\n2. Desiderata 2: Post-processing immunity. Once data has been privatized using a privacy-preserving mechanism, any further data analyses should not degrade its privacy guarantees, provided that the original, non-privatized data remains inaccessible. This property assures that subsequent steps or transformations applied to the privatized output cannot compromise privacy. Post-processing immunity offers a strong guarantee that allows data analysts to abstract away potential attack models, effectively providing future-proof protection against privacy violations.\n3. Desiderata 3: Group privacy. Group privacy aims at controlling how privacy guarantees degrade when considering groups of individuals rather than single individuals. It ensures that a privacy mechanism does not arbitrarily fail to protect privacy beyond the individual level when data from multiple users is combined. While it is inevitable that privacy guarantees weaken as group sizes increase, since more information is encoded about them, the degradation should be controlled and quantifiable.\n4. Desiderata 4: Quantifiable privacy-accuracy trade-offs. There is no free lunch in privacy: releasing accurate information about a group of people must necessarily and statistically encode some information about individuals. As privacy protection increases the accuracy of insights derived from the data may decrease. A good privacy definition should provide quantifiable trade-offs, allowing data analysts, decision-makers, and model builders to measure how much accuracy is sacrificed for a given level of privacy. This enables them to balance privacy and utility according to specific needs."}, {"title": "Data Anonymization", "content": "A standard technique for privacy protection in various domains is anonymization. It involves the removal or masking of any identifying details to prevent the recovery of personal identities. Anonymization has been employed in areas such as the release of medical datasets under the Health Insurance Portability and Accountability Act (HIPAA) standards. In the mid-1990s, the Massachusetts Group Insurance Commission (GIC), a government agency responsible for purchasing health insurance for state employees, sought to promote medical research by releasing anonymized health data. The GIC approach involved removing what they considered \"explicit\" identifiers such as names, addresses, and social security numbers, while retaining hundreds of other attributes deemed non-identifiable. Supported by then-Governor William Weld, this initiative aimed at balancing data utility with privacy protection. However, in 1997, Dr. Latanya Sweeney, then a graduate student at MIT, set out to challenge the effectiveness of this anonymization. Using publicly available information, she re-identified Governor Weld's medical records within the dataset and sent them to his office, starkly demonstrating the vulnerability of supposedly anonymized data.\nHow was Dr. Sweeney able to uncover Governor Weld's personal medical information from the GIC's released data? One might assume that such an attack required sophisticated techniques and significant resources. In reality, her de-anonymization attack cost only $20 and limited time. The GIC's dataset included three crucial attributes for each individual: sex, zip code, and date of birth. Dr. Sweeney purchased voter registration records from Cambridge, Massachusetts, which contained names, addresses, zip codes, and dates of birth. By cross-referencing these two datasets, she found that only six people in Cambridge shared Governor Weld's birth date. Of those, only three were male, and just one resided in his zip code-uniquely identifying his medical records. This type of attack, known as a linkage attack, re-identifies individuals by linking anonymized data with external public records. In a subsequent report [Sweeney, 2000], Dr. Sweeney demonstrated that her attack extended far beyond a single high-profile individual. She found that \"87% of the population in the United States had reported characteristics that likely made them unique based only on 5-digit ZIP, gender, date of birth.\u201d Even at broader geographic levels, significant portions of the population could be uniquely identified with minimal information. \"About half of the U.S. population are likely to be uniquely identified by only place, gender, date of birth, where place indicates the city, town, or municipality in which the individual resides.\"\nWhy did anonymization fail? Anonymization failed because it lacked formal privacy guarantees. Dr. Sweeney's attack was remarkably simple, yet unanticipated due to the absence of a precise attack model. The lack of post-processing immunity meant that, once the anonymized data was released, combining it with other publicly available datasets could reveal more information than intended. If the privacy mechanism had been robust to post-processing, additional analyses or data combinations would not have compromised individual privacy beyond what was already publicly accessible. This example motivates the need for formal privacy definitions that account for all potential avenues of data exploitation."}, {"title": "K-Anonymity", "content": "At this point, one might argue that the previous example does not represent a fundamental failure of anonymization as a privacy technique, but rather a misapplication in that specific instance. Is it possible to thwart de-anonymization attacks by simply withholding more attributes? For instance, would not releasing someone's zip code, date of birth, or gender resolve the issue? However, a significant challenge emerges in determining which combinations of publicly available attributes could uniquely identify an individual. As the number of features in a dataset grows, it becomes practically impossible for modern computing to predict and guard against all potential attack vectors. Moreover, sensitive attributes often correlate statistically with non-sensitive ones, rendering anonymization susceptible to statistical attacks that can probabilistically reconstruct sensitive information through these correlations\u2014for a particularly sensitive example involving genomic data, refer to Homer et al. [2008].\nDespite decades of deployment, anonymization has consistently failed to provide robust privacy protection. Other high-profile failures include the AOL search data release [Barbaro et al., 2006], the Netflix Prize dataset [Narayanan and Shmatikov, 2006], and studies demonstrating that individuals can be uniquely identified using just a few mobile phone location points [De Montjoye et al., 2013]. So, what is the next step? Can the concept of anonymization be refined to address its shortcomings? A promising strategy might be to release only partial information about each attribute. For example, in demographic or medical analyses, knowing that an individual falls within a certain age range, such as \"between 18 and 35,\" might suffice. By revealing less precise information, can re-identification attacks be made more difficult?"}, {"title": "Any Perfectly Accurate and Deterministic Privacy Notion Must Fail", "content": "Various strategies have been proposed to address the shortcomings k-anonymity without significantly reducing the utility of data for demographic and population-level analyses. One such method is data swapping, which involves exchanging parts of dataset entries among individuals to ensure that no single row corresponds directly to one person, while still preserving overall demographic counts like the \u201cnumber of people in dataset X that have property Y.\" This technique was employed in the release of U.S. Census data products prior 2020. Another approach is data minimization, which focuses on collecting as little data as necessary and discarding it after it has served its purpose. Despite these efforts, the challenge of ensuring that privacy guarantees degrade gracefully and predictably under repeated queries remains unresolved. To address this, it is important to highlight a fundamental property that must be satisfied by any robust privacy definitions, helping us narrow down the search for effective solutions. Specifically, the claim is that no perfectly accurate and deterministic privacy technique can satisfy our requirements, and that randomization is essential for privacy.\nThis crucial point can be illustrated with a simple example where the lack of randomness leads to a failure in composition. Imagine a hypothetical company named Gluble, which has 25 employees. Gluble publicly announces that the average salary of its employees is $500,000, perhaps to attract top talent with its competitive compensation. After hiring a 26th employee named Rick, the company updates its public average salary to $505,000. From these two pieces of information, one can deduce Rick's salary. Using basic arithmetic, Rick's salary x is obtained by solving $(x + 25 \\times 500,000) = 505,000$, i.e., $x = $630,000. This amount is significantly higher than his colleagues' salaries. This scenario shows a failure of composition: while each individual data release seems innocuous, combining them allows an adversary to infer sensitive information about an individual. Even with access to just two queries, a differential attack reconstructed private data. Although this example is simplified, Dinur and Nissim [2003] have shown that such differential attacks can be executed in far more complex settings, even when the query language is restricted. Importantly, the attack used no information about how the data was privatized. This vulnerability arises because the average salary at Gluble was released deterministically and exactly. What would happen if noise was added to Gluble's salary reports? Suppose that the average salary before hiring Rick was reported as approximately $500,000, and after hiring, it was approximately $505,000. It is no longer clear question whether the change is due to Rick's salary or simply a result of the added randomness. After all, the introduction of noise creates uncertainty, preventing exact inference of individual salaries. This concept of adding randomness to data releases is a cornerstone of Differential Privacy.\nObserve that providing Differential Privacy is more complex than \u201cjust\u201d adding noise. At a high level, the more noise is added, the better our privacy guarantees are going to be; however, adding too much noise is undesirable, as it destroys the utility of privately-released datasets and statistics. Therefore, noise must be carefully calibrated to balance privacy protection with data utility, enabling us to provide formal and provable privacy guarantees alongside precise"}, {"title": "A Side Note: Other Types of Privacy Breaches", "content": "The discussion above highlights the importance of our privacy desiderata and illustrates how previous techniques that failed to meet these criteria have led to significant privacy failures. So far, the presentation relied on simple examples involving variants of anonymization techniques and the challenges associated with privatizing and releasing datasets. However, with the advent of increasingly complex models and large-scale machine learning applications, privacy failures have begun to emerge in more intricate and subtle ways-even when privatized datasets are never directly released. In particular, recent research has demonstrated that privacy can be compromised not only through released statistics but also via the models themselves. A notable example is Federated Learning (FL) [Li et al., 2020]. The goal of FL frameworks is to protect privacy through decentralization: each user retains their data on their local device, performs computations locally, and only transmits aggregated updates (such as gradient information) to a central server. The intent is that no central entity ever accesses individual user data, thereby preserving privacy. Yet, recent work has shown that this is insufficient: the gradient updates themselves often encode sufficient information to be able to guess the original user data with high accuracy [Zhu et al., 2019].\nThis issue is not confined to the training of machine learning models. Even after a model is trained and the original data is ostensibly deleted, the released models can still encapsulate information about the training data. This can lead to privacy breaches where models inadvertently memorize and reproduce parts of their training datasets. For instance, large language models trained on extensive text corpora have been found to occasionally output verbatim snippets from the training data when prompted in specific ways [Carlini et al., 2021]. A real-world example involves a South Korean AI company Scatter Lab [Dobberstein, 2021]. Scatter Lab used text and messaging data from users on South Korea's biggest test messaging company, KakaoTalk, to train a chatbot service. Despite efforts to remove personally identifiable information, the chatbot reproduced memorized conversations from the training data when users interacted with it, inadvertently disclosing private and sensitive information about KakaoTalk users. These examples illustrate that privacy breaches can occur even without direct access to the underlying datasets. Thus there is a need for privacy-preserving techniques that extend beyond data anonymization and address the inherent risks in modern machine learning practices. Differential Privacy offers a framework to mitigate these risks by providing formal guarantees that limit the potential for information leakage, even when models are trained on sensitive data and released publicly. Part II of this book explores how Differential Privacy can be applied to machine learning and optimization tasks to safeguard individual privacy for increasingly complex data analysis."}, {"title": "What Protections Does Differential Privacy Provide?", "content": "This section examines and defines what Differential Privacy does and does not protect against. It considers the scenario of an analyst or data curator who aims at collecting and aggregate personal and sensitive data for release in a privacy-preserving manner. This release can take various forms, such as a synthetic version of the dataset that masks private information, a set of sensitive population-level statistics about individuals in the dataset, or a model trained on the sensitive data. The common objective in all these cases is to release data that carefully conceal sensitive attributes at the individual and group level while retaining sufficient information to provide useful statistics or models at the population level. For example, an analyst might wish to determine the fraction of a population with a particular disease or calculate the average salary of employees in a company. In these instances, the data pertaining to each individual is private and sensitive, and individuals may prefer to keep it confidential.\nFirst attempt: No information leakage. Ideally, no information about any specific individual should be leaked through the data release, i.e., \"nobody can learn any information about a specific individual from the privatized computation.\" Achieving this level of privacy is theoretically straightforward: simply do not collect or use any data at all. However, this is impractical, as it precludes any meaningful data analysis. Herein lies a fundamental tension highlighted earlier in this chapter: using more data enhances the accuracy and usefulness of the models and statistics but potentially compromises individual privacy."}, {"title": "What Does Differential Privacy Promise?", "content": "Implementing Differential Privacy requires careful consideration of the context in which privacy guarantees are applied, particularly regarding the underlying trust model. The degree of trust placed in data curators or aggregators significantly influences the design and effectiveness of privacy-preserving mechanisms. This book examines the two primary frameworks within privacy-preserving ecosystems: the centralized model and the distributed (or local) model, each with distinct characteristics and implications for privacy management.\nIn a centralized framework, all data collection, storage, and processing occur at a single, central location managed by a trusted data curator. This central entity has direct access to the raw data and is responsible for implementing and monitoring all privacy-preserving mechanisms. The assumption here is that the data curator will faithfully protect individual privacy and handle data responsibly. This setup represents the central model in Differential Privacy, as illustrated in Figure 1 (left). Conversely, a distributed framework keeps data decentralized, residing at its point of origin-such as personal devices or local databases. Privacy-preserving algorithms are executed locally by the data contributors themselves, and only essential, processed information is communicated to a central authority. For instance,"}, {"title": "Differential Privacy: Formal Definition, Techniques, and Properties", "content": "Differential Privacy is a mathematical framework for measuring and bounding the individuals' privacy risks in a computation. The concept, first introduced in 2006 by Dwork et al. in Dwork et al. [2006b], informally states that the presence or absence of any individual record in a dataset should not significantly affect the outcome of a mechanism. In this book, a mechanism is defined as any computation that can be performed on the data. Differential Privacy deals with randomized mechanisms, and a mechanism is considered differentially private if the probability of any outcome occurring is nearly the same for any two datasets that differ in only one record.\nIn this context, an adversary is any entity attempting to infer sensitive information about individuals from the output of a data analysis. Remarkably, the privacy guarantee of Differential Privacy holds even if the adversary possesses unlimited computing power and complete knowledge of the algorithm and system used to collect and analyze the data. Thus, even if the adversary were to develop new and sophisticated methods, including the attack methods discussed earlier, as well as new attacks that do not yet exist today, or even if new additional external information becomes available, Differential Privacy provides the exact same level of protection. In this sense, Differential Privacy is considered future-proof."}, {"title": "Differential Privacy, Formally", "content": "Prior to defining Differential Privacy formally, this section formalizes what this privacy notion aims at protecting (dataset) and the means by which an analyst interacts with data (queries).\nDatasets and queries. A dataset D is a multi-set of elements in the data universe U. The set of every possible dataset is denoted D. The data universe U is a cross product of multiple attributes $U_1, ..., U_n$ and has dimension n. For example, Figure 2, illustrates a dataset D with three attributes: city, age, and gender. If C is the set of all cities considered, the interval $A = [0, 100]$ the set of all ages considered, and G = {M, F, other}, then $U = C \\times A \\times G$. A numeric query is a function $f : D \\rightarrow \\mathbb{R} \\subseteq \\mathbb{R}^n$ that maps a dataset in some real vector space. For instance, the query f(D) could be an SQL statement that counts the number of male individuals over the age of 18 in dataset D, as illustrated in Figure 2.\nThe concept of adjacency is fundamental in DP. It frames the unit of change that Differential Privacy seeks to protect against, ensuring that the presence or absence of any single individual's data does not significantly alter the outcomes of data analysis. There are two common ways to define adjacency in the context of Differential Privacy, reviewed next.\nDefinition 4.1 (Add/remove adjacency). Two datasets D and D' are said adjacent under the add/remove notion, denoted as D ~ D', if |DAD'| = 1, where \u25b3 is the symmetric difference of two sets."}, {"title": "Differential Privacy", "content": "In other words, two datasets are defined as adjacent if one can be obtained from the other by either adding or removing the data of a single individual. This model is particularly relevant when considering the impact of an individual's participation or absence in the dataset.\nDefinition 4.2 (Exchange adjacency). Two datasets D and D' are said adjacent under the exchange notion, denoted as D ~ D', if D' is obtained from D by successively removing one record and then adding a (possibly different) record. That is, there exist elements d \u2208 D and d' \u2208 U such that: D' = (D \\ {d}) \u222a {d'}. This implies that |D| = |D'| and |DAD'| = 2.\nIn this notion, adjacent datasets differ in the data of exactly one individual but have the same size. This definition is suited to scenarios where the alteration of data within a constant-size dataset is the primary concern, and can be viewed as the removal followed by the addition of one individual.\nThe choice between add/remove or exchange adjacency has some implications for how Differential Privacy is applied as it directly affects the computation of global sensitivity, introduced next, which measures the maximum change in the output of a function for adjacent datasets. This chapter, and generally the book unless specified otherwise, adhere to the add/remove notion of adjacency.\nGlobal sensitivity. The impact of a single individual's data on the overall analysis is measured through the concept of global sensitivity. Formally, the global sensitivity of a function $f : D \\rightarrow \\mathbb{R}$ is defined as the maximum difference in the output of f over all pairs of adjacent datasets $D \\sim D' \\in D$, measured with respect to the $l_p$ norm:\n$\\Delta_p f = \\max_{D \\sim D'} ||f(D) - f(D')||_p.$\\nIn simpler terms, it measures how much the output of a function can change when an individual's data is added or removed from the dataset. This measurement provides a basis for determining the amount of noise that needs to be added to the function's output to achieve privacy. For example, the query considered in Figure 2 that counts the number of individuals satisfying a certain property in a dataset has global sensitivity 1, since adding or removing a single individual in the dataset can affect the final count by at most 1. Suppose instead that the task is to compute the average age of all individuals in the dataset. Then the global sensitivity of this average function would be\n$\\Delta_f = \\frac{max(A) - min(A)}{|D|} = \\frac{100}{|D|},$ where A represents the range of possible ages (assuming ages range from 0 to 100). In this chapter, the $l_1$-sensitivity $\\Delta_1 f$ is denoted with $\\Delta f$.\nDifferential Privacy. These examples illustrate how a single individual's data can influence the output of a function applied to a dataset. This influence is central to the concept of Differential Privacy. The impact of adding or removing an individual's data varies depending on the type of function in question\u2014whether it's calculating sums, averages, or any other measure of the data. This sensitivity measurement tells us how much the output of the target function need to be adjusted in order to protect an individual's privacy. Differential Privacy achieves this by adding noise to the function's output, by an amount calibrated to the function sensitivity. This approach ensures that the presence or absence of any single individual's data does not significantly alter the output, thereby masking their participation.\nDefinition 4.3 (Differential Privacy [Dwork et al., 2006b]). A randomized mechanism $M : D \\rightarrow \\mathbb{R}$ with domain D and range R is $(\\epsilon, \\delta)$-differentially private if, for any event $S \\in \\mathbb{R}$ and any pair D, D' \u2208 D of adjacent datasets:\n$\\Pr[M(D) \\in S] \\leq \\exp(\\epsilon) \\Pr[M(D') \\in S] + \\delta,$ where the probability is calculated over the randomness of M.\nA differentially private mechanism maps a dataset to a distribution over the possible outputs because, e.g., it adds random noise or makes randomized choices. The released DP output is a single random sample drawn from this distribution. The level of privacy is controlled by the parameter $\\epsilon \\geq 0$, called the privacy loss, with values close to 0 denoting strong privacy, and a secondary parameter $\\delta$ which can be loosely interpreted as a margin of error.\nFirst, observe that the inequality:\n$\\Pr[M(D) \\in S] \\leq \\exp(\\epsilon) \\Pr[M(D') \\in S],$ where \\Delta is the symmetric difference of two sets."}, {"title": "Formal Properties of Differential Privacy", "content": "This section formalizes the properties guaranteed by Differential Privacy, and how they match the desirata described in Section 2. The composition, group privacy, and post-processing properties are derived directly from the direction of Differential Privacy, and do not assume a specific mechanism like Randomized Response. As such, composition. group privacy, and post-processing hold for any differentially private mechanism, i.e. any mechanism that satisfies requirement (2).\nComposition. Composition ensures that a combination of differentially private mechanisms (whether the mechanisms release privatized data, statistics on data, or learning models) preserves Differential Privacy. Composition is a key concept that enables the construction of complex algorithms by combining simpler primitives. It facilitates privacy accounting, the rigorous analysis of the overall privacy loss of a composite and potentially complex algorithm by aggregating the privacy guarantees of individual primitives. More formally, it can be stated as follows [Dwork and Roth, 2014]:"}, {"title": "The Laplace Mechanism", "content": "The Laplace Distribution with 0 mean and scale b has a probability density function $Lap(x|b) = \\frac{1}{2b} e^{-\\frac{|x|}{b}}$. The Laplace mechanism is a differentially private mechanism based on the Laplace distribution for answering numeric queries [Dwork et al., 2006b]. It is a fundamental building block for many DP algorithms described in this book, and it functions by simply computing the output of the query f and then perturbing each coordinate with noise drawn from the Laplace distribution. The scale b of the noise is calibrated to the query sensitivity $\\Delta f$ divided by $\\epsilon$:\nDefinition 4.4 (The Laplace Mechanism). Let $f : D \\rightarrow \\mathbb{R} \\subseteq \\mathbb{R}^d$ be a numerical query, with d being a positive integer. The Laplace mechanism is defined as $M_{Lap}(D; f, \\epsilon) = f(D) + Z$ where $Z \\in \\mathbb{R}$ is a vector of i.i.d. samples drawn from $Lap(\\frac{\\Delta f}{\\epsilon})$.\nThe Laplace mechanism adds random noise drawn from the Laplace distribution independently to each of the d dimensions of the query response.\nTheorem 4.4 (Differential Privacy of The Laplace Mechanism). The Laplace mechanism, $M_{Lap}$, achieves $(\\epsilon, 0)$-Differential Privacy."}, {"title": "Answering Private Queries in Practice", "content": "Next, we present two examples to illustrate how the Laplace Mechanism can be applied in practice.\nExample 1: Computing the average age. Consider a dataset containing the ages of 10, 000 individuals, with ages ranging from 0 to 100 years. The task is to compute the average age while ensuring differential privacy. A practical procedure follows the following steps:\n1. Determine the query function and its sensitivity. In this task the query function is the average age,\n$f(data) = \\frac{1}{n} \\sum_{i=1}^n age_i,$ where n is the number of individuals in the dataset. The global sensitivity $\\Delta f$ of the average function is the maximum change in the output when one individual is added or removed. Since the age can vary between 0 and 100, adding the data about a single individual can affect the sum by at most 100 units. Therefore, the sensitivity is:\n$\\Delta f = \\frac{max age - min age}{n} = \\frac{100 - 0}{10,000} = 0.01.$\n2. Apply the Laplace Mechanism. The next step is to select the privacy parameter $\\epsilon$ and add noise drawn from the Laplace distribution with scale parameter $\\frac{\\Delta f}{\\epsilon}$. Selecting $\\epsilon = 0.5$ to obtain a strong privacy guarantee adds the following noise:\nnoise ~ Lap($\\frac{\\Delta f}{\\epsilon}$) = Lap($\\frac{0.01}{0.5}$) = Lap(0.02).\nThe private query thus reports f (data) + noise.\n3. Analyze the error bound. Additionally, by setting a confidence level $\\beta = 0.05$ (meaning that one is 95% confident in the error bound), the error bound can be computed as,\nError Bound = $\\frac{\\Delta f}{\\epsilon} ln(\\frac{1}{\\beta}) = \\frac{0.01}{0.5} ln(\\frac{1}{0.05}) \u2248 0.06$ years.\nThis means that, with 95% confidence, the noisy average age returned by the Laplace Mechanism will differ from the true average age by no more than approximately 0.06 years. If the privacy parameter is set to $\\epsilon = 1$, allowing for slightly less privacy in exchange for greater accuracy, the error bound decreases to about 0.03 years. Thus, selecting $\\epsilon$ and $\\beta$ appropriately ensures that the released data remains both useful and privacy-preserving.\nExample 2: Releasing a histogram. Suppose a statistical agency wants to release a histogram showing the number of individuals in different age groups, segmented by gender and region, from a dataset containing a large number of respondents. The age groups could be categorized in intervals (e.g., 0\u20139, 10\u201319, ..., 90+). The goal is to release this histogram while ensuring differential privacy. Note that this is different from the previous task where a single quantity was released. The procedure again follows the the three same steps:\n1. Determine the query function and its sensitivity. The query function is the count of individuals in each combination of age group, gender, and region. For count queries, the global sensitivity $\\Delta f$ is 1 because adding or removing one individual can change the count in one category by at most 1.\n2. Apply the Laplace Mechanism. The next step consists in selecting a privacy parameter $\\epsilon = 0.5$ for each count in the histogram and adding independent Laplace noise to each cell (i.e., each combination of age group, gender, and region) in the histogram. Let $C_{i,j,k}$ be the true count for age group i, gender j, and region k, and $\\tilde{C}_{i,j,k}$ is the private counterpart to be released. The counts are linked by the following formula:\n$\\tilde{C}_{i,j,k} = C_{i,j,k} + Noise_{i,j,k},$ where $Noise_{i,j,k}  Laplace(\\frac{\\Delta f}{\\epsilon}) = Laplace(2).$\n3. Post-processing to ensure valid counts. Notice that the application of real-valued noise to each count may render the resulting privacy-preserving counterpart negative or non-integers, thus producing invalid ouputs. These issues can be corrected by applying a post-processing step, that set any negative noisy counts to zero and round the noisy counts to the nearest integer. Such post-processing steps do not alter the privacy guarantees of the original release and are commonly applied in deployments Cohen et al. [2021].\n4. Analyze privacy and utility. Each count is $\\epsilon$-differentially private with $\\delta = 0.5$. Since each individual's data affects only one count, and the counts are disjoint, the overall privacy guarantee remains $\\epsilon = 0.5$. The added Laplace noise has a mean of zero and a scale of 2 and thus the expected absolute error for each count is 2. For categories with large counts, this noise has a relatively small impact. However, for categories with small counts, especially in less populated age groups or regions, the noise can significantly affect the accuracy. For a further analysis of disparate impacts of Differential Privacy on different subpopulations, we refer the reader to the survey Fioretto et al. [2022]."}, {"title": "Approximate Differential Privacy", "content": "The discussion in the previous section focused on pure Differential Privacy and the mechanisms and guarantees associated with it. The case where \u03b4 > 0 for $(\\epsilon, \\delta)$-DP constitutes a variant of Differential Privacy known as Approximate Differential Privacy. Recall that \u03b4 \u2208 (0,1) is the failure probability of the privacy loss bound in the relaxed variant of pure DP, and is meant to be a cryptographically low quantity\u2014that is, so small it is considered negligible for practical purposes, often much less than $\\frac{1}{N^2}$ where N is the dataset size. This allows practitioners to apply other mechanisms that yield better utility than the Laplace mechanism in exchange for a marginal failure probability. Importantly, approximate Differential Privacy retains the composition, group privacy, and post-processing immunity properties provided by pure Differential Privacy."}, {"title": "The Gaussian Mechanism", "content": "The canonical mechanism for $(\\epsilon", "2014": ".", "relationship": "for a vector $x \\in \\mathbb{R}^d, ||x||_2 \\leq ||x||_1 \\leq \\sqrt{d} ||x||_2$. Thus, the $l_2$ sensitivity can be up to a factor $\\sqrt{d}$ less than the $l_1$ sensitivity. The Gaussian distribution with 0 mean and standard deviation \u03c3 has the probability density function\n$N(x | \u03c3) = \\frac{1}{\u03c3\\sqrt{2\u03c0}"}]}