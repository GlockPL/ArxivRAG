{"title": "Speech Boosting: Low-Latency Live Speech Enhancement for TWS Earbuds", "authors": ["Hanbin Bae", "Pavel Andreev", "Azat Saginbaev", "Nicholas Babaev", "Won-Jun Lee", "Hosang Sung", "Hoon-Young Cho"], "abstract": "This paper introduces a speech enhancement solution tailored\nfor true wireless stereo (TWS) earbuds on-device usage. The\nsolution was specifically designed to support conversations in\nnoisy environments, with active noise cancellation (ANC) acti-\nvated. The primary challenges for speech enhancement models\nin this context arise from computational complexity that limits\non-device usage and latency that must be less than 3 ms to pre-\nserve a live conversation. To address these issues, we evaluated\nseveral crucial design elements, including the network archi-\ntecture and domain, design of loss functions, pruning method,\nand hardware-specific optimization. Consequently, we demon-\nstrated substantial improvements in speech enhancement qual-\nity compared with that in baseline models, while simultaneously\nreducing the computational complexity and algorithmic latency.", "sections": [{"title": "1. Introduction", "content": "Recently, true wireless stereo (TWS) earbuds have been suc-\ncessfully popularized along with mobile phones, thereby in-\ncreasing convenience for many users. In line with this, compa-\nnies developing earbuds have introduced a variety of functions\nto maximize user experience. Active noise cancellation (ANC),\nwhich blocks almost all sounds around the user, is a core feature\nof the TWS earbud. This function enhances various experiences\nin everyday life, such as listening to music, making calls, or\nfocusing on work by removing background noise.\nThe need for additional technology becomes apparent when\na person wearing earbuds and applying ANC wants to have con-\nversations with nearby people. Currently, to clearly hear the\nvoice of a nearby person, the user must turn off the ANC func-\ntion or remove the earbuds altogether. If there is a technology\nthat can enhance the voice of a nearby person while reducing\nambient sounds through ANC, this reduces several inconve-\nniences, such as missing a few words, delaying conversation,\nand increasing the risk of losing earbuds. In this study, we aim\nto apply a speech enhancement solution focused on advanc-\ning the noise suppression capabilities of earbuds, particularly\nin noisy environments where ANC is in operation. We aim to\nensure that suppression does not hinder conversations. This ne-\ncessitates the development of advanced low-latency speech en-\nhancement models capable of ensuring a balance between noise\nreduction and critical sound preservation.\nTo successfully implement an effective speech enhance-\nment model for the aforementioned scenario, two key criteria\nmust be satisfied. First, the algorithmic latency of the speech\nenhancement module should be maintained at a maximum of\n3 ms or less. This is a critical factor in the context of remote\ncommunication, where users are noticeably more sensitive to\ndelays. This sensitivity arises from the fact that users interact in\nthe same space, making any inconvenience caused by the spec-\ntral coloration of the comb-filtering effect [1] from the super-\nposition of the direct and delayed speech more disruptive. Sec-\nond, the use of computing resources must be minimized. This\nis particularly crucial in real-time on-device applications, such\nas ours, where the efficient use of resources can significantly\nimpact the performance and user experience.\nTo meet these requirements, we explored several design\nchoices to achieve efficient low-latency speech enhancement.\n1. We compared the efficiencies of a state-of-the-art frequency-\ndomain network and a time-domain baseline and discovered\nthat the time-domain baseline was more effective when al-\nlocated comparable computational resources and algorithmic\nlatency.\n2. We investigated whether modern structured state space-based\nmodels [2, 3] could replace our conventional Wave-U-Net +\nLSTM baseline structure. Despite the encouraging results for\nlong-context modeling tasks, these models were unable to\noutperform our simple baseline in a low-latency speech en-\nhancement setup.\n3. We evaluated the efficiency of adversarial losses, a common\ntool for training contemporary speech enhancement mod-\nels [4-6], in a low-latency setup and noted its propensity\nfor speech oversuppression. To counter this effect, we sug-\ngested two-stage training that combines Phone-Fortified Per-\nceptual Loss (PFPL) [7], adversarial [4], UTokyo-sarulab\nMean Opinion Score (UTMOS) [8], and Perceptual Evalu-\nation Speech Quality (PESQ) [9] losses, that we believe can\nenhance speech intelligibility and minimize artifacts.\n4. We assessed the performance of the magnitude pruning\nmethod against that of the novel Sparsity Profiles via DY-\nnamic programming search (SPDY) + Optimal Brain Com-\npression (OBC) method [10, 11]. We observed that the SPDY\n+ OBC method significantly improved the quality of the\npruned models.\nOverall, the combination of these techniques delivered low-\nlatency speech enhancement models with a 3 ms algorithmic\nlatency and 0.21 GMAC complexity (or 291 MCPS after being\nported on-device), making it suitable for on-device usage while\noutperforming the baselines with less latency and complexity."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Low-latency speech enhancement", "content": "Recently, low-latency speech enhancement has attracted sig-\nnificant research interest. Numerous studies [12, 13] have ex-\nplored time-domain causal neural architectures, such as ConvTasNet [14] and Wave-U-Net + LSTM [15], for this task.\nAndreev et al. [15] suggested a novel training procedure\nfor low-latency models, termed iterative autoregression (IA).\nThis method was used to train a time-domain Wave-U-Net +\nLSTM model for low-latency speech enhancement, and the ben-\nefits of IA were demonstrated in a comparative comparison\ncategorical rating study. In the current study, we examine ef-\nficient neural architectures, training losses, and pruning meth-\nods. As such, the IA is distinct from the improvements pro-\nposed in the IA paper, and we defer the application of the IA\nto our model for future studies. Another line of works [16-19]\nutilizes time-frequency domain architectures using asymmet-\nric analysis synthesis pairs for windows of short-time Fourier\ntransform or future frame prediction. For example, the iNeuBe-\nX framework [18] incorporates the TF-GridNet model [20] for\nlow-latency speech enhancement. The approach based on this\narchitecture won the first place in the 2nd Clarity challenge [21],\na competition for low-latency speech enhancement systems for\nhearing aids. This method implements low-latency speech en-\nhancement in the time-frequency domain by predicting future\nshort-time fourier transform (STFT) frames, thus reducing the\nlatency imposed by the STFT windows. In the present study,\nwe challenged the TF-GridNet architecture against our time-\ndomain baseline."}, {"title": "2.2. On-device speech enhancement", "content": "Several studies have been conducted to optimize speech en-\nhancement models for different devices. For TinyLSTMs [22],\nThe authors demonstrated that structural pruning and 8-bit INT\nquantization could be jointly applied to two LSTM layers.\nThe compressed model achieved a 11.9x reduction in model\nsize and a 2.9\u00d7 reduction in operations. For the DEMUCS-\nMobile [23], The authors demonstrated that batch normaliza-\ntion pruning utilizing GLU activation could be applied to DE-\nMUCS [24] model. The compressed model achieved a 10x re-\nduction in model size. However, these models had an algorith-\nmic latency of more than 20 ms that prevented its use in our\nscenario. Notably, the authors did not study the effect of loss\nfunctions on perceptual quality and measured the quality using\nSI-SDR alone that was poorly correlated with perceptual qual-\nity [4]. We also note that with our pruning technique, we man-\naged to reduce the number of operations in our model by 10\ntimes while preserving the high quality, in contrast to 3 times\nclaimed in [22]. The size of our model achieved less than 1 Mb,\nin contrast to the 8.9 Mb claimed in [23]."}, {"title": "3. Speech Boosting", "content": ""}, {"title": "3.1. Methodology", "content": "Data In all our experiments, we considered additive noise\nas the distortion to be removed from speech recordings. We\nemployed the VoiceBank-DEMAND dataset [25], a standard\nbenchmark for speech-denoising systems. The training set con-\nsisted of 28 speakers with four signal-to-noise ratios (SNR) (15,\n10, 5, and 0 dB) and contained 11572 utterances. The test set\n(824 utterances) consisted of two speakers unseen by the model\nduring training with four SNRs (17.5, 12.5, 7.5, and 2.5 dB).\nEvaluation We used the state-of-the-art objective speech qual-\nity metrics UTMOS [8] and WV-MOS [4]. In our internal ex-\nperiments, we observed that these metrics had the best corre-\nlation among well-known objective metrics (for example, SI-\nSDR, DNSMOS, and PESQ) with human-assigned MOSes for\nspeech enhancement tasks. We also used 5-scale MOS tests for\nsubjective quality evaluation, following the procedure described\nin [4].\nBaseline We began with the Wave-U-Net + LSTM base-\nline introduced in [15]. We used the adversarial loss function\nwith MSD discriminators because this loss had better percep-\ntual properties than regression-type losses [4, 6, 15]. We used a\nthree-layer architecture with 4, 4, and 2 strides and 32, 64, and\n128 channels, as shown in Figure 1. The model operated at a 16\nkHz sample rate. It used a chunk size of 32 timesteps and a look-\nahead of 16 timesteps. This look-ahead is achieved by duplicat-\ning the input waveform into several channels, each containing\na shifted waveform. Consequently, the total algorithmic latency\nwas 48 timesteps, that is equivalent to 3 ms. The computational\ncomplexity of this model was approximately 2 GMAC/s. In all\nthe experiments, the batch size was 16, segment size was set to\n2 s, and Adam optimizer was used with a learning rate of 0.0002\nand betas 0.8 and 0.9."}, {"title": "3.2. Time domain versus frequency domain", "content": "Many studies have considered time-frequency domain architec-\ntures for low-latency speech enhancement. Therefore, our first\nchallenge is to decide whether time or time-frequency domain-based approach is more effective for low-latency speech en-\nhancement. For this purpose, we implemented a state-of-the-art\ntime-frequency domain TF-GridNet architecture [20] for com-\nparison with our Wave-U-Net + LSTM baseline. Because we\nhad to understand whether this time-frequency architecture had\nbenefits over the time-domain baseline, given similar computa-\ntional constraints, we chose the parameters of the TF-GridNet\narchitecture to match the algorithmic latency to 4 ms and the\ncomputational complexity to 2.6 GMAC/s by decreasing the\nnumber of channels and TF-GridNet blocks within the network\nto 4 and 16, respectively. We trained the TF-GridNet model us-\ning the same losses as those in the baseline case. The results\nare summarized in Table 1. TF-GridNet appeared to be worse\nthan our baseline. This is likely owing to the high computational\ncomplexity of the original TF-GridNet model. The complex-\nity of the original model was approximately 36 GMAC/s, and\nits performance was significantly degraded when its parameters\nwere reduced, making it impractical for our usage scenario."}, {"title": "3.3. Model architecture", "content": "In our search for alternative time-domain architectures, we used\nthe recently proposed structured state-space layers (S4) [3].\nGiven the exceptional performance of S4 in sequence model-\ning tasks, particularly those involving long ranges, we believed\nthat this block could serve as a replacement for the LSTM bot-"}, {"title": "3.4. Loss functions", "content": "We observed that the adversarial loss function has two main dis-\nadvantages. First, training with this loss is considerably slow be-\ncause of the training of the discriminators. Second, low-latency\nmodels trained with adversarial loss tend to oversuppress the\nspeech content within the recording. This obstacle is expected\nbecause the adversarial loss promotes outputs of the model to\nbe within the speech recording distribution rather than preserv-\ning the speech content; thus, it may sacrifice some of the speech\ncontent to increase the distribution credibility of the generated\nspeech.\nThus, as an alternative, we trained the model using the\nPFPL [7]. The PFPL is a regression-type loss formulated by\ncombining the time-domain L1 loss and the Wasserstein dis-\ntance between the wav2vec2.0 [26] features of the generated\nand reference (clean) waveforms.\nUsage of the PFPL in the initial training stage offers two\nsignificant benefits: (1) This loss impeccably retains speech\ncontent during the noise suppression process. This is likely ow-\ning to the incorporation of wav2vec2.0 features, known for their\nproficiency in extracting speech content. (2) Training with the\nPFPL is considerably faster than with its adversarial counterpart\nowing to the absence of discriminator training.\nHowever, the use of the PFPL during training sometimes\nleads to the emergence of background squeak artifacts, a typical\nphenomenon associated with regression-type losses. To address\nthis, we implemented a second stage of training (fine-tuning)\nthat integrated the adversarial [4], UTMOS [8], and PESQ [27,\n28] losses with 1, 50 and 5 weights, respectively.\nWe applied adversarial loss with MSD discriminators as an\neffective solution for squeak artifacts. This ensured a correla-\ntion between the distributions of the clean and generated sig-\nnals, thereby correcting any distributional discrepancies. Con-\ncurrently, UTMOS and PESQ augmented speech intelligibil-\nity by incorporating insights gleaned from human preference\nstudies. We utilized the official implementation of the UTMOS\nscore and the PyTorch implementation of the PESQ metric.\nBoth metrics are differentiable with respect to their inputs and\ncan therefore be applied as loss functions (multiplied by nega-\ntive constants). Owing to the initial stage of PFPL training, we\nonly had to fine-tune the models with second-stage losses for\na few epochs, thereby saving time and preserving the speech\ncontent captured by the PFPL training.\nTo verify the efficacy of the proposed training pipeline,\nwe compared it with vanilla adversarial training and vanilla\nPFPL training. As summarized in the results in Table 3, the pro-\nposed two-stage training procedure considerably outperformed\nthe baselines according to human opinion."}, {"title": "3.5. Pruning", "content": "The original Wave-U-Net + LSTM model had a complexity of\napproximately 2 GMACs, rendering it unsuitable for on-device\ndeployment. We implemented block-structured pruning to opti-\nmize the model in terms of performance and storage.\n1. For convolutional layers, we applied kernel pruning that en-\nforces sparsity in such a way that if $W$ represents a weight\nin a convolutional layer, then for certain input channel $i$ and\noutput channel $j$, $W[i, j, :] = 0$. We only stored the indices\nof the non-zero kernels and computed the outputs based on\nthese kernels.\n2. The LSTM layers were pruned using block sparsity. For each\nnon-zero block, we recorded its coordinates within the fully\nconnected LSTM layers and performed computations only\nfor these non-zero blocks. The blocks measured 16 x 1.\nOur pruning pipeline followed an iterative prune + fine-\ntune strategy. At each pruning iteration, 10% of the remain-\ning weights were pruned and the model was fine-tuned for 50\nepochs. The procedure was continued until the total sparsity\nof the model ~90% (complexity-wise). The key is determin-\ning the weights required to prune at each iteration. We handled\nthis problem by using the SPDY + OBC pruning strategy. This\nstrategy decomposes the pruning process into layer-wise local\npruning (OBC) [11] and search for layer sparsity distributions\n(SPDY) [10].\nIn the first step of SPDY + OBC, we used the OBC for\npruning each layer independently to optimally reconstruct lo-\ncal activations using the mean squared error criterion, given the\nsparsity constraint. This approach is based on the exact realiza-\ntion of the classical optimal brain surgeon framework applied\nto local layer pruning. Using the OBC, we obtained a bank of\nweights for each layer that satisfied different sparsities.\nSubsequently, the SPDY search was employed to deter-\nmine layer sparsities such that the total model sparsity was suit-\nable for the current computational budget while maximizing the\nmodel performance on the calibration data. The algorithm as-\nsumed a linear dependency of the model quality on the log-"}, {"title": "3.6. HiFi4 DSP simulation", "content": "We implemented the 0.21 GMAC pruned model in the native C\ncode. Running this code on on a system with Cadence Tensil-\nica HiFi4 DSP core [29] provided 2031 million clocks per sec-\nond (MCPS) for this model. This is the total number of clocks,\nincluding the instructions to load and store each variable re-\nquired for the calculations through the data memory interface.\nThe clock frequencies supported by the micro control units are\ntypically approximately 300-600 MHz. Because the process-\ning time is longer than the algorithmic latency, delays are in-\nevitable. Therefore, single instruction multiple data (SIMD) op-\nerations, such as the 16-bit four-way SIMD operation of HiFi4\nDSP for fixed-point numbers, have to be used to reduce the to-\ntal MCPS. We converted the inputs and parameters of each layer\ninto fixed-point numbers using Q format. In this case, the input\nvalues were converted to Q12 as a 32-bit integer variable. The\nweights and biases of the convolutional layers were converted\nto Q13 and Q25 as 16-bit short and 32-bit integer variables, re-\nspectively. The weights and biases of the LSTM layers were\nconverted to Q13 as 16-bit short variables. Subsequently, we\nreplaced the calculation expressions of the convolutional and\nLSTM layers with SIMD operations of the HiFi4 DSP. The fi-\nnal optimized model had 291 MCPS and around 800 kB size."}, {"title": "4. Results", "content": ""}, {"title": "4.1. Comparison with existing approaches", "content": "We compared the resulting speech-boosting models with Wave-\nU-Net + LSTM trained using IA [15] and a non-causal DE-\nMUCS denoiser [24]. For the IA baseline, we used Wave-U-Net\n+ LSTM with the K, N, and C parameters set to 7, 4, and [16,\n24, 32, 48, 64, 96, 128], respectively. This configuration corre-\nsponds to 8 ms of algorithmic latency and 2.0 GMAC complex-\nity. The DEMUCS denoiser was used in a non-causal configura-\ntion with the H parameter set to 64. Both these baseline models\nhad considerably higher computational complexity and algo-\nrithmic latency than our pruned model, while delivering com-\nparable perceptual quality according to the MOS score."}, {"title": "4.2. Limitations and future work", "content": "Figure 2 shows the enhanced speech samples of all the process\nmodels, where the input sample is a male voice mixed with\nsubway noise at an SNR of 2.5 dB. Samples mixed with bab-\nbble noise is shown in the left segment, and samples mixed with\nthe additional harmonic noise of the alarm sound is shown in\nthe right segment. As observed, the babble noise was well re-\nmoved, whereas the harmonic noise remained in small amounts\nin the utterance segments of the enhanced speech. This obser-\nvation suggests that Wave-U-Net + LSTM struggles to filter out\nharmonic signals in the noisy speech adequately.\nOwing to the inherent complexities of speech signals and\nnoise characteristics, accurately estimating and removing noise\nwhile preserving the speech components can be a delicate bal-\nance. In addition, variations in harmonic gains and fluctuations\nin the denoising process can contribute to the generation of har-\nmonic noise artifacts, making it challenging to achieve a clean\nand natural sounding output [30]. To alleviate this issue, Wave-\nU-Net + LSTM should be improved to capture harmonic re-\nlationships in noisy speech. One promising avenue for future\nresearch in this area includes hybrid architectures that operate\nsimultaneously in time and frequency domains [4, 31, 32]."}, {"title": "5. Conclusion", "content": "This work advances low-latency, on-device speech enhance-\nment by reevaluating several critical design choices. We ex-\namine different model architectures, training losses, and prun-\ning techniques, selecting the optimal scenario for efficient low-latency speech enhancement. The resulting model achieves a\nremarkable balance between performance and resource utiliza-\ntion. It is suitable for on-device usage, exhibits low algorithmic\ndelay, and delivers a quality comparable to models with signifi-cantly higher algorithmic latency. The experimental results will\npave the way for future advancements in speech enhancement\ntechnology for TWS earbuds that will be co-operated with vari-ous audio processing modules such as ANC and Beamforming."}]}