{"title": "Guided Synthesis of Labeled Brain MRI Data Using Latent Diffusion Models for Segmentation of Enlarged Ventricles", "authors": ["Tim Ruschke", "Jonathan Frederik Carlsen", "Adam Espe Hansen", "Ulrich Lindberg", "Amalie Monberg Hindsholm", "Martin Norgaard", "Claes N\u00f8hr Ladefoged"], "abstract": "Deep learning models in medical contexts face challenges like data scarcity, inhomogeneity, and privacy concerns. Synthetic data can help, but often underperforms compared to real data. This study focuses on improving ventricular segmentation in brain Magnetic Resonance Imaging (MRI) images, particularly for cases with enlarged ventricles, using synthetic data. We employed two latent diffusion models (LDMs): a mask generator trained using 10,000 masks, and a corresponding SPADE image generator optimized using 6,881 scans to create an MRI conditioned on a 3D brain mask. Conditioning the mask generator on ventricular volume in combination with classifier-free guidance enabled the control of the ventricular volume distribution of the generated synthetic images. Next, the performance of the synthetic data was tested using three nnU-Net segmentation models trained on a real (N=1,000), augmented (N=1,512) and entirely synthetic data (N=1,000), respectively, where the synthetic data contains a more uniform distribution of ventricular volumes compared to the real data. The resulting models were tested on a completely independent hold-out dataset of patients with enlarged ventricles (N=42), with manual delineation of the ventricles used as ground truth performed by a trained neuroradiologist. The model trained on real data showed a mean absolute error (MAE) of 9.09 \u00b1 12.18 mL in predicted ventricular volume, while the models trained on synthetic and augmented data showed MAEs of 7.52 \u00b1 4.81 mL and 6.23 \u00b1 4.33 mL, respectively. Both the synthetic and augmented model also outperformed the state-of-the-art model SynthSeg, which due to limited performance in cases of large ventricular volumes, showed an MAE of 7.73\u00b112.12 mL with a factor of 3 higher standard deviation. The model trained on augmented data showed the highest Dice score of 0.892 \u00b10.05, slightly outperforming SynthSeg (0.874\u00b10.06) and on par with the model trained on real data (0.891\u00b10.05). The synthetic model (0.872\u00b10.06) performed similar to SynthSeg. In summary, we provide evidence that guided synthesis of labeled brain MRI data using LDMs improves the segmentation of enlarged ventricles and outperforms existing state-of-the-art segmentation models.", "sections": [{"title": "1. Introduction", "content": "The success of machine learning largely depends on the quantity and quality of available training data. As models grow increasingly large, they require immense datasets, which has culminated in some of the most successful models trained on so many instances of data they need not see the same sample twice (Brown et al. (2020)). While such abundance can be found in natural images or natural language processing tasks, this wealth of data is not universally accessible. In medical imaging, data scarcity, variability, and privacy constraints present significant challenges to building similarly large models.\nPatients with normal pressure hydrocephalus (NPH) exemplify this issue, displaying an abnormal buildup of cerebrospinal fluid in the brain's ventricle system, consequently leading to unique and abnormally enlarged ventricles. Contemporary state-of-the-art models for brain segmentation of magnetic resonance imaging (MRI) data (Billot et al. (2023a)) struggle to accurately segment the enlarged ventricle systems of NPH patients. Furthermore, obtaining a large training set for these patients is difficult due to their uniqueness and the diverse nature of the disease.\nGenerating synthetic data offers a promising alternative to the prohibitively costly process of collecting real data. Modern image generation models, such as diffusion models, initially seem to provide limitless, highly customizable data with perceptual quality nearly indistinguishable from real images (Podell et al. (2023)). However, extensive research has shown that purely synthetic training data cannot match the effectiveness of real training data (Marwood et al. (2023); Fernandez et al. (2022, 2024)). Instead, synthetic data is best used to supplement real data to enhance model performance (Azizi et al. (2023)).\nIn the field of MRI, several studies (Pinaya et al. (2022); Han et al. (2023)) have demonstrated the remarkable perceptual quality of synthetic images generated using diffusion models. Although these synthetic images still do not match the quality of real data, Fernandez et al. (2022) made significant progress in narrowing the performance gap between real and synthetic training data for brain MRI segmentation models. Additionally, conditioning the models on scalar values can produce increasingly realistic and morphology-preserving images. For example, Pinaya et al. (2022) trained a latent diffusion model (LDM) on a large sample of 3D T1-weighted MRI data (N=31,740) from the UK Biobank dataset (Sudlow et al. (2015)) while conditioning on several parameters such as ventricular volume, thus being able to generate synthetic data with varying ventricle sizes. Achieving remarkable vi-"}, {"title": "2. Methods", "content": ""}, {"title": "2.1 Data", "content": ""}, {"title": "2.1.1 LDM100K DATASET", "content": "Pinaya et al. (2022) generated a publicly available2 large synthetic dataset of 100,000 3D T1-weighted brain MRI images using an LDM trained on UK biobank data (Sudlow et al. (2015)) with isotropic 1 mm\u00b3 voxel size and a resolution of 160 x 224 x 160. In this work, to reduce computational constraints, we first downsampled the data to a spacing of 1.67 x 1.75 x 1.67 mm\u00b3 and center-cropped to a resolution of 96 x 128 x 96. We also only used the first 10,000 images in the data to keep the size in line with our other dataset (MS-FLAIR Dataset), assuming a valid representation of the full dataset. The main purpose of this dataset is two-fold, 1) serving as the training data for our mask diffusion model due to its balanced spread of ventricular volumes, and 2) also being used for transfer learning. To generate segmentation labels for the data, we used the SynthSeg model (Billot et al. (2023a)). In addition to the segmentation, the model also estimates the size of the volumes for different parts of the brain. To obtain the conditioning value c, we divided the ventricular volume by the intracranial volume to get the relative size of the ventricles. We subsequently normalized these values between 0 and 1 over the subset of the data, and the outcome was then parsed into the models for conditioning. Lastly, we used a randomized 80-20 schema, splitting the data into training and hold-out validation datasets."}, {"title": "2.1.2 MS-FLAIR DATASET", "content": "The dataset consists of T2-weighted 3D FLAIR images acquired at the Copenhagen University Hospital - Rigshospitalet, Copenhagen, Denmark. The data comprises 2,927 patients with multiple sclerosis (MS) who were scanned multiple times as part of their treatment, resulting in a total of 6,881 scans. A more thorough description of the data is available in Hindsholm et al. (2023). All patient-specific data was handled in compliance with the Danish data protection agency act no. 502. The collection of the retrospective dataset"}, {"title": "2.2 Diffusion Models", "content": "Diffusion models are generative models that can be used to generate synthetic images by iteratively denoising a noisy sample, starting from simple gaussian noise. This is called the backward process, modeling the reverse of the forward process which iteratively adds noise to an image until reaching complete noise. Given enough time steps in the procedure, there is a mathematical justification (Sohl-Dickstein et al. (2015)) that if the forward process follows a gaussian distribution, so does the backward process. The gaussian distribution modeling the backward process is implemented by means of a neural network, indicated by parameters \u03b8. As each step is independent from the previous one, leveraging the ensuing markov property results in the entire backward trajectory given by\n$$p_\\theta(x_{0:T}) = p(x_T) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t)$$\nwhere T is the number of timesteps, $p(x_T) = N(x_T; 0, I)$, $x_0$ a sample from the data distribution, and $p_\\theta(x_{t-1}|x_t) = N(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t,t))$. The forward process is modeled analogously but with a known distribution based on a specific noise schedule $\u03b2_t$.\nFor training, Sohl-Dickstein et al. (2015) derived the evidence lower bound (ELBO) for the log likelihood (intractable to calculate directly), which has since been simplified and improved by Ho et al. (2020). After making a few minor assumptions about the model, rewriting the objective in terms of predicting the noise instead of the the target image, and simplifying as much as possible, the final objective function is given by\n$$\\frac{\\beta_t^2}{2 \\sigma \\tau a_t (1 - \\bar{a}_t)} ||\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar{a}_t}x_0 + \\sqrt{1 - \\bar{a}_t} \\epsilon, t)||^2$$\nwhich compares the noise added to the original image with the noise predicted by the neural network using mean squared error (MSE), scaled by a factor depending on the choice of noise schedule $\u03b2_t$ (from which $a_t$ and $\\bar{a}_t$ are derived, see Ho et al. (2020)). The training algorithm (see Ho et al. (2020)) is derived from the expectation over the data and over the time of the learning objective, where we then simply sample from the training data, sample a timestep $t \\sim [1,T]$ uniformly at random, and then adjust the model by the gradient according to the loss calculated."}, {"title": "2.3 Conditioning Diffusion Models", "content": "The most simple case of conditioning a given distribution is to simply add a conditioning input c at each step as an input to the model, as given by\n$$p(x_{0:T}/c) = p(x_t) \\prod_{t=1}^T p_\\theta(x_{t-1}|x_t, c)$$\nwhich simply corresponds to $e_\\theta(x_t,t,c)$ getting an additional parameter. In practice, this can take the form of concatenating the conditioning information with the input image or using an attention mechanism (Rombach et al. (2022); Vaswani et al. (2017)).\nGuidance (Dhariwal and Nichol (2021)) has been suggested as a means to amplify the effect of conditioning. Guidance uses the gradient of a classifier (corresponding to $p(c|x_t)$) to influence the prediction step when sampling. As this mechanism requires a separately trained classifier, classifier-free guidance (Ho and Salimans (2022)) is a convenient way to forgo the classifier, instead training both a conditional and unconditional model (which can even be the same model training for both cases) and combining the predictions with a weight G:\n$$\\epsilon_\\theta = G\\epsilon_{p_\\theta(x|c)} + (1 \u2013 G)\\epsilon_{p_\\theta(x)}$$\nDue to the scaling by G in both terms, similarities in the cases are independent from G while differences will be emphasized in favor of the conditional case as G increases beyond 1."}, {"title": "2.4 Mask Generator Latent Diffusion Model", "content": "In this work, the first step in our model development pipeline to generate synthetic labeled data is to generate masks. For this, we used a diffusion model, specifically an LDM (Rombach et al. (2022)) due to the large computational requirements imposed by working in the 3D image space. The autoencoder architecture and hyperparameters were based on Pinaya et al. (2022), but given the different target of mask reconstruction instead of images, we made several changes based on an informal hyperparameter search. First, our reconstruction loss consisted of a cross-entropy loss. As labels cannot be trivially used with common perceptual losses and we found no benefit in using an adverserial loss, we used no other additional loss to train the model except for KL-regularization to a normal distribution with a weight of 1e-6. Next, instead of feeding the labels directly to the model, we used an embedding layer, trained end-to-end, before the first layer, with a feature dimension of 64. We trained the autoencoder for 40 epochs using the LDM100k Dataset. For the diffusion model, we followed the basic denoising model and training algorithm from Ho et al. (2020). The backbone of the noise-prediction model was a U-Net with an additional attention mechanism closely resembling Pinaya et al. (2022). We used conditioning on the normalized ventricular volume ratio c \u2208 [0,1] (see subsubsection 2.1.1) via attention. Further, we used classifier-free guidance (Ho and Salimans (2022)) by training the model on the conditional and unconditional case at the same time, adding an additional binary conditioning parameter and randomizing c in the unconditional case. The extra parameter was needed to ensure c maintained a smooth distribution which the model can learn more easily. During training, there was a 20% chance for any dataset entry to be transformed to the unconditional case. The diffusion model was trained for 5 epochs. The low number of epochs was motivated by Dar et al. (2024), who found that longer training times are associated with reproducing patient data and thus less diversity. For more details, please see Figure A.1 in the appendix."}, {"title": "2.5 Image Generator Latent Diffusion Model", "content": "Now having defined a mask generation LDM (previous subsection), we used the output of that model to condition the image generation (Figure 2), following the same general 3D LDM approach. Specifically, we used a KL-autoencoder similar to the Mask Generator case, but the decoder used the segmentation mask like a SPADE model (Park et al. (2019)) to enable precise alignment of images with the mask. We used a similar loss combination as in Pinaya et al. (2022), using a combination of perceptual loss, adverserial loss, pixel-based L1 loss, and KL-loss as in the mask generator case. For the adverserial loss, we used a patch-based discriminator based on Wang et al. (2018). We trained the autoencoder on the LDM100k Dataset for 20 epochs first to utilize the large and diverse dataset, and then fine-tuned the autoencoder for another 10 epochs on the MS-FLAIR Dataset, since the test set used for the downstream segmentation task was acquired using the T2-weighted FLAIR sequence. The diffusion model closely matched the mask generator model architecture. Like the decoder of the autoencoder, the upsampling part of the U-Net backbone also employed SPADE connections. We trained the diffusion model for 20 epochs using the LDM100k Dataset and fine-tuned for 5 epochs on the MS-FLAIR Dataset. For an overview of the general training architecture, please see Figure A.2 in the appendix."}, {"title": "3. Experiments", "content": ""}, {"title": "3.1 Generating large ventricles with guidance and conditioned on out of distribution values", "content": "We trained our model on the LDM100k Dataset and compared the generated distribution with the ground truth values (masks generated using SynthSeg (Billot et al. (2023b))) from the validation set. Model values were determined by generating synthetic masks in equal number with the same c values as present in the validation set. With matching c values, we should expect similar mean volumes in the generated data, assuming the model generalizes well. We repeated this for G \u2208 {1, 2, 3, 4, 5}, with G = 1.0 corresponding to the conditioned case without guidance. Due to the long generation time of an LDM, we used a DDIM scheduler (Song et al. (2022)) with 50 inference timesteps to speed up the process. For values of c > 1.0, we sampled 50 images per bucket, each prompted with the middle point of the bucket (1.05, ...).\nWe start by studying the behaviour of the generated distribution of ventricular volumes when varying the conditioning and guidance parameters, reported in Figure 3. In Figure 3, the ground truth curve follows a roughly linear shape, which is to be expected given that c"}, {"title": "3.2 Image generation conditioned on synthetic masks", "content": "Figure 4 shows representative samples of synthetic images, when varying the conditioning parameter c at G = 4.0. We generally observed degeneration of the synthetic data (Figure 4, right) for c > 1.4 or G > 4.0, but precise thresholds varied across models. The transition took the form of a gradual increase in probability of degeneration, as we rarely observed coherent samples for large c and G and very infrequently for out of distribution values that were still somewhat close to the training distribution, like c = 1.25, G = 4.0. However, it is non-trivial to develop a reliable mechanism for detecting degenerate masks to quantitatively analyze the rate, a topic which is left for future research.\nWe report quantitative metrics for assessing the image quality of the generated images for the diffusion model fine-tuned on the MS-FLAIR Dataset compared against the hold-out validation set. The metrics serve mostly for comparison in future work, as they can be difficult to interpret across datasets. We used the Fr\u00e9chet Inception Distance (FID) (Heusel et al. (2018)) as a measure of realism, where a small value indicates that the distribution of the generated images are similar to the distribution of the real images. We also measured the diversity of the generated images by computing the Structural Similarity Index Measure (SSIM) and the Multi Scale SSIM (MS-SSIM) between 500 pairs of synthetic images, with low values indicating high diversity. The model had an FID of 0.34, a pairwise SSIM of 0.31, and a pairwise MS-SSIM of 0.45.\nIn addition to the quantitative metrics, we also performed a qualitative analysis by having an experienced neuroradiologist assess the realism and quality of a smaller subset of the generated images. For this, we generated thirteen synthetic images with a guidance value of 4.0 and c\u2208 [0,1.3] in intervals of 0.1, and further 10 with guidance 1.0 and c\u2208 [0,1.0]. All generated images used a DDPM scheduler with 1,000 timesteps. The generated images were visually inspected by a neuroradiologist, reporting that the images were generally realistic, particularly noting the correct displacement of surrounding tissue caused by enlarged ventricles. However, larger guidance values were generally associated with less realistic images."}, {"title": "3.3 Can synthetic data improve segmentation performance on enlarged ventricles?", "content": "To test whether synthetic data can improve the segmentation performance on enlarged ventricles, we created three datasets, $D_{real}$, $D_{syn}$, and $D_{aug}$. $D_{real}$ consisting of 1,000 real images with labels from SynthSeg randomly sampled from the previously used FLAIR training data (Figure 5A). $D_{syn}$ consisted of paired synthetic labels and synthetic images, generated using the mask generator and image generator SPADE model. The data was systematically generated to have a more uniform distribution of ventricular volumes compared to the real data, including out of distribution values. Specifically, we created 600 images using a guidance value of 4.0 and a conditioning value $c \\in [0, 1.3]$ with intervals of size 0.1 containing the same number of images. The exact value of c in an interval was sampled uniformly at random. Motivated by enhancing diversity as well as robustness of the images, we generated another 400 images using the same procedure but with guidance $G = 1.0$ and $c \\in [0, 1.0]$. Lastly, $D_{aug}$ consisted of a subset of $D_{syn}$ (200 with $G = 1.0$, 312 with $G = 4.0$) as well as the entirety of $D_{real}$, comprising 1,512 samples in total. The roughly 2:1 ratio of real to synthetic data was a conservative estimate motivated by previous research, showing that too much synthetic data can be detrimental (Azizi et al. (2023); Fernandez et al. (2024)).\nIn order to investigate the impact of the three training datasets (real, synthetic, and augmented), we subsequently trained several models for a downstream segmentation task. Specifically, we used the original nnU-Netv2 segmentation model (Isensee et al. (2021)) with no further modifications to the model or training other than the data, as this model has been shown to achieve state-of-the-art performance on many segmentation tasks. We used the nnU-Net training procedure algorithm out of the box, but only used the 3D full resolution configuration as our resolution was already quite small. We trained the model with mirroring removed from the data augmentation pipeline because otherwise, the model would not have been able to properly distinguish between left and right anatomical structures.\nTo evaluate the performance, we ran inference of our three models on NPH-FLAIRtest, which consisted of 42 patients with enlarged ventricles imaged with the T2-weighted FLAIR sequence. The ground truth ventricle labels were manually delineated by an expert. A large proportion of these images used a 2D T2w-FLAIR acquisition instead of 3D, and thus differ from the training data of both the generator models as well as the nnU-Net models. Finally, we also compared the trained nnU-Net models to the performance of SynthSeg (Billot et al. (2023b)).\nIn Table 1, the segmentation performance is shown for each model. On Dice and IoU, nnUaug and nnUreal perform equally well and both outperform nnUsyn and SynthSeg by a small margin while also displaying less standard deviation. SynthSeg performs similarly to nnUsyn. While the segmentation metrics of Dice and IoU provide a good measure of how well the voxels of the generated segmentation align with the ground truth, they only indirectly measure whether the ventricular size was predicted correctly, and particularly at this low resolution can be quite sensitive to voxel shifts. As such, we additionally compared the ventricular volume between predicted and ground truth masks. As we are interested in its performance on outliers with enlarged ventricles, which we assume to be more difficult to segment correctly, we measured not only mean absolute error (MAE) but also mean"}, {"title": "4. Discussion", "content": "The prospect of training a model for a given task solely on synthetic data is a tempting one. Between a potential abundance of training data and reduced (albeit not removed (Dar et al. (2024))) concern for its privacy, this approach appears to address several pervasive problems with machine learning in a medical context.\nIn this work, building on previous successful 3D synthesis of brain MRI (Pinaya et al. (2022)), we developed a separate LDM mask generator and SPADE LDM image generator to create paired synthetic images and labels. While other approaches cast the mask reconstruction as an image reconstruction problem (Fernandez et al. (2022)), we employed a more simple approach where the mask was encoded via an end-to-end trained embedding and used a simple cross-entropy loss commonly used in segmentation problems. Our own hyperparameter search on this problem was only cursory, and given the label in our architecture dictated the semantics and shape of the final image, the role in final image diversity is likely very large. As such, exploration with other architectures may be beneficial. While there is comparably little research on mask compression, there are still some noteworthy alternative approaches. Gansbeke and Brabandere (2024) found success in mask compression using a shallow architecture for 2D panoptic segmentation in combination with a bit-encoding of the mask. They also point out RGB-encoding as a viable option.\nWe show that conditioning, particularly in combination with guidance, can be an effective way of controlling the ventricular volume of generated images. While still subject to considerable variance that does not guarantee precise control over individual images, the distribution of synthetic data respected the parameters we chose at generation time. As guidance increased and conditioning values exceeded the training interval, the models exhibited an increased tendency of degeneration that rapidly reduced realism. When generated with parameters only slightly outside of distribution, synthetic images were judged by an expert neuroradiologist to be realistic (although not on the level of real images) both from a general perspective and from the perspective of displacement caused by enlarged ventricles. Nevertheless, since optimization of image quality was not a focus of this work, we believe there is significant potential for improvements here.\nThe results of the downstream task indicated that the synthetic data does indeed help the segmentation models perform better on cases of enlarged ventricles. The models trained with synthetic data were notably more consistent than nnUreal, even more so than SynthSeg. In binary segmentation, the synthetic variants both outperformed SynthSeg in ventricular volume prediction error and nnUaug achieved better Dice and IoU scores, while being trained on far less data. As nnUreal was doubtlessly trained on more realistic images, we mainly attribute this to the more even distribution of ventricular volumes. However, to determine the precise reason for this may require further ablation experiments. Finally, we also point out that our test sets are on the smaller side, and having a much larger dataset would provide a considerably more precise and robust estimate of the true performance gap in the models.\nLimitations of this work include that we have spent limited resources on optimizing the diffusion model part of the mask generator. Given its role in generating a latent representation to be decoded by the autoencoder, it likely plays a significant role in the diversity of masks being generated. While we attempted to promote diversity by training for a shorter duration, a proper analysis on the success of this approach for masks remains the subject of future work. Another obvious improvement of this work that needs attention in the future involves the scheduler. In this work, we used the original linear scaling from Ho et al. (2020), but there are compelling arguments (Chang et al. (2023)) for the use of other schedules like cosine-based ones, as well as other samplers for generation. Furthermore, given the propensity of diffusion models to replicate training data (Dar et al. (2024)) and their well-documented lack of diversity compared to real data in a natural image context (Marwood et al. (2023)), the quality of the synthetic dataset as a whole may still offer room for improvement. While we found that individual images appeared distinct and diverse enough according to an expert evaluation, the only indication we have for the quality of the generated synthetic datasets as a whole are their performance on downstream tasks, which may not be representative. Lastly, we point out some general limitations of our models. Working with 3D medical data is challenging due to the cubic scaling of the data. We largely circumvented this issue by universally downscaling the data and using an LDM approach. Depending on the application, however, higher resolution data with more detail may be necessary. Other work have attempted to tackle this issue by separating the mask generation task into a sequential generation of slices conditioned on previous slices (Han et al. (2023)), though this involves a significantly more complicated architecture. Another possibility may be the use of a separate super-resolution model in order to upsample low-resolution synthetic images.\nThe success of synthetic data in this current work raises some interesting questions. Generally, purely synthetic data is known to perform considerably worse than real data. While Fernandez et al. (2022) found comparative success in similar segmentation tasks with synthetic data, its performance still falls short of real data, which is only echoed by an increasing volume of research from the natural image domain (Azizi et al. (2023); Marwood et al. (2023)). While we have speculated about potential reasons for this, we believe this work offers a compelling starting point for future work to further investigate the success of purely synthetic data.\nIn conclusion, we have proposed a framework for generating synthetic data and labels for brain MRI with varying ventricular volumes. As a proof-of-concept, we demonstrated its effectiveness in augmenting a dataset of real patients, particularly those with few or no examples of large ventricles. Additionally, we showed that incorporating these synthetically generated images and labels during training improved performance in the downstream segmentation task tested on a data set of patients with abnormally enlarged ventricles."}]}