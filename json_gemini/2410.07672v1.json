{"title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization", "authors": ["Yougang Lyu", "Lingyong Yan", "Zihan Wang", "Dawei Yin", "Pengjie Ren", "Maarten de Rijke", "Zhaochun Ren"], "abstract": "As large language models (LLMs) are rapidly advancing and achieving near- human capabilities, aligning them with human values is becoming more urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong alignment problem where we need to effectively align strong student LLMs through weak supervision generated by weak teachers. Existing alignment methods mainly focus on strong-to-weak alignment and self-alignment settings, and it is impractical to adapt them to the much harder weak-to-strong alignment setting. To fill this gap, we propose a multi-agent contrastive preference optimization (MACPO) framework. MACPO facilitates weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar negative ones. To get this, we devise a mutual positive behavior augmentation strategy to encourage weak teachers and strong students to learn from each other's positive behavior and further provide higher quality positive behavior for the next iteration. Additionally, we propose a hard negative behavior construction strategy to induce weak teachers and strong students to generate familiar negative behavior by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human judgments, demonstrate that MACPO simultaneously improves the alignment performance of strong students and weak teachers. Moreover, as the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance through more iteration optimization rounds.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have helped to make rapid progress in diverse domains [4, 44, 51], making it important to align them with human values and preferences [1, 2, 14]. Two widely used algorithms for aligning LLMs with human values are reinforcement learning from human feedback [RLHF, 44] and direct preference optimization [DPO, 52]. The core idea of these algorithms is to train LLMs to reinforce desirable positive behavior and penalize negative behavior. These algorithms mainly adhere to the strong-to-weak alignment setting, i.e., trying to effectively align weak student LLMs by using high-quality supervision from humans or stronger teacher LLMs [3, 32, 76]. As LLMs have been shown to potentially outperform humans on certain tasks [5, 6, 19], we are facing a weak-to-strong alignment problem, where strong student LLMs need to be aligned by weak teachers"}, {"title": "2 Related Work", "content": "LLM alignment. Alignment plays a crucial role in shaping the behavior of large language models (LLMs) to human values and preferences [44, 2, 6]. The widely used algorithms for aligning LLMs with human values are RLHF [44] and DPO [52], which align LLMs by reinforcing positive desirable behavior and penalizing negative behavior. However, collecting large-scale human preferences for LLM behavior is expensive. To mitigate this, several works have explored using LLMs to construct synthetic preferences [3, 79, 60]. One line is strong-to-weak alignment, which usually uses strong LLMs to provide feedback or construct preference pairs for aligning smaller models [32, 54, 41]. Bai et al. [3] propose reinforcement learning from AI feedback (RLAIF) methods to use powerful off-the-shelf LLMs to annotate helpfulness and harmlessness scores. [76] introduce reinforcement"}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 Problem Formulation", "content": "To study the weak-to-strong alignment problem, following Burns et al. [5], we consider a simple analogy setting that replaces weak human supervisors with weak model supervisors for training strong students. Specifically, given an original alignment training dataset D = {(xi, Yi)}^2N_{i=1}, we split it equally into two parts D\u2081 and D2. Then, by fine-tuning, we initialize weak supervisors Mw on D1 with golden labels. Next, we filter queries Qw2s = {xi}_{i=1} of the held-out dataset D2 and use weak supervisors to generate weak labels for questions Qw2s. Finally, we use these weak labels to initialize strong students Ms. Note that weak teachers and strong students can only access the questions Qw2s during the subsequent weak-to-strong alignment process."}, {"title": "3.2 Alignment Training", "content": "Alignment training of LLMs usually contains two stages, supervised fine-tuning and preference optimization [15, 75, 74]. Next, we present the loss functions for supervised fine-tuning (SFT) and preference optimization in detail.\nSupervised fine-tuning. SFT aims to train pre-trained LLMs to understand and answer natural language questions. Formally, given a dataset D = {(xi, Yi)}^N_{i=1}, where xi and yi denotes a question and a corresponding answer. The training objective of SFT is to minimize the following loss:\n$L_{sft} = \\sum_{i=1}^{N} \\log P_{\\pi_{\\theta}} (y_{i,j}|y_{i,<j}, x_i)$"}, {"title": "4 Multi-Agent Contrastive Preference Optimization", "content": "In this section, we introduce a framework for weak-to-strong alignment named multi-agent contrastive preference optimization (MACPO). The main idea underlying MACPO is to facilitate weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behaviors and penalizing familiar negative behaviors. MACPO includes two complementary strategies: (i) mutual positive behavior augmentation, and (ii) hard negative behavior construction. For the mutual positive behavior augmentation strategy, weak teachers and strong students engage in mutual learning, where each learns unfamiliar positive behavior from the other. The process is iterative: in each round, weak teachers and strong students improve by adopting the positive behavior learned in the previous round, thereby enhancing alignment performance and providing higher-quality behavior for subsequent iterations. For the hard negative behavior construction strategy, we induce weak teachers and strong students to generate familiar negative behavior by fine-tuning on negative behavioral data. We hypothesize that, since weak teachers and strong students have different knowledge, self-induced negative behavior is more familiar to them. We describe these strategies and the iterative training process in more detail below. Figure 1 provides an overview of the framework."}, {"title": "4.1 Mutual Positive Behavior Augmentation", "content": "To learn from reinforcing unfamiliar positive behavior, we encourage positive weak teachers and positive strong students to learn from each other's behavior, thereby enhancing the quality of positive behavior iteratively. First, we assume there are K weak teachers {Mw,k}_{k=1} and one strong student Ms in our framework. For strong students, since behavior generated by weak teachers may contain negative noise, we filter high-quality positive behavior of weak teachers. Specifically, we first ask K"}, {"title": "4.2 Hard Negative Behavior Construction", "content": "To learn from penalizing familiar negative behavior, we induce negative weak teachers and the negative strong student to generate familiar negative behaviors. Similar to the initialization of positive weak teachers and positive strong students, we initialize negative weak teachers {Mnegw,k}_{k=1} on negative behavioral data with gold labels, and then fine-tune the negative strong student Mneg using weak labels generated by negative weak teachers on the held-out question set Qw2s. Then, we ask the strong student to generate familiar negative behavior for itself:\nGnegMs = {yi | yi ~ Mneg(xi) \u2227 xi \u2208 Qw2s}.\nMoreover, we ask each negative teacher to generate familiar negative behaviors for itself as follows:\nGnegMw.k{yi | yi ~ Mnegw,k(xi) \u2227 xi \u2208 Qw2s}"}, {"title": "4.3 Iterative Training Process", "content": "Our overall procedure trains a series of K positive weak teachers {M0w,k, ..., MDw,k}Kk=1 and one positive strong student {M0s,..., MTs}, where each successive model t+1 uses contrastive preference data created by the t-th positive weak teachers and the t-th positive strong student. Note that we only iteratively optimize the positive agents and the negative agents remain unchanged after initialization.\nIn our experiments, we define positive weak teachers and the strong student, and the contrastive preference data as follows:\nInitialization positive agents {M0w,k}Kk=1 and M0s: Base multiple weak teachers and a strong student, we initialize weak teachers by fine-tuning on ground truth labels D\u2081, and initialize the strong student on weak labels generated by weak teachers for the held-out question set Qw2s.\nFirst iteration positive agents {M1w,k}Kk=1 and M1s: Initialized with {M0w,k}Kk=1 and M0s, then trained with {DC1Mw,k}Kk=1 and DC1Ps, respectively, using Lpo.\nSecond iteration positive agents {M2w,k}Kk=1 and M2s: Initialized with {M1w,k}Kk=1 and M1s, then trained with {DC2Mw,k}Kk=1 and DC2Ps, respectively, using Lpo.\nThird iteration positive agents {M3w,k}Kk=1 and M3s: Initialized with {M2w,k}Kk=1 and M2s, then trained with {DC3Mw,k}Kk=1 and DC3Ps, respectively, using Lpo."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Research Questions", "content": "We aim to answer the following research questions in our experiments: RQ1: Does MACPO outperform state-of-the-art (SOTA) methods on the weak-to-strong alignment setting? RQ2: How does the number of weak teachers influence the weak-to-strong alignment performance and iterative training process? RQ3: How does the alignment performance of weak teachers evolve during the iterative training process? RQ4: What impact do different strategies have on the weak-to-strong alignment performance of MA\u0421\u0420\u041e?"}, {"title": "5.2 Datasets", "content": "We conduct experiments using two helpfulness and harmlessness alignment datasets:\nHH-RLHF [2] consists of conversations between humans and LLM assistants. Each sample contains a pair of conversations, with human annotators marking one conversation as preferred. The dataset includes a helpful subset (denoted as HH-Helpful) and a harmless subset (denoted as HH-Harmeless). We randomly filter samples from each subset to conduct experiments on weak-to-strong alignment, respectively.\nPKU-SafeRLHF [11] consists of conversation comparisons. Each comparison is annotated with two labels: a preference label indicating the human's choice between two responses and a harmless label associated with the preferred response, confirming whether it complies with safety standards. Following [66, 55], we filter samples to ensure that each sample includes both preference labels and the preferred conversation fits safety standards."}, {"title": "5.3 Baselines", "content": "To evaluate the effectiveness of MACPO, we compare it against a variety of methods, which can be categorized into three groups:\nStrong-to-weak alignment methods: RLAIF [3] uses LLMs to annotate helpfulness or harmless-ness scores for candidate answers, constructing comparison sets based on these scores. RLCD [76] simulates pairwise helpfulness or harmlessness preferences using a positive prompt and a negative prompt, aiming to amplify the differences between outputs.\nSelf-alignment methods: SPIN [9] uses a self-play mechanism, where a main LLM player is iteratively fine-tuned to distinguish its responses from those of the previous iteration's opponent. Self-rewarding [79] prompts an LLM to assign rewards to its own generated responses for constructing preference pairs.\nWeak-to-strong alignment methods: Naive SFT [5] represents vanilla fine-tuning the strong student backbone on weak labels generated by weak teachers according to Eq. 1. Confident loss [5] combines weak teacher predictions with those of the strong student, to reinforce the student's confidence in its own predictions."}, {"title": "5.4 Evaluation Metrics", "content": "We present our experimental results using two evaluation metrics: automatic evaluation and human-based evaluation. For automatic evaluation metrics, following [52, 57], we use a third-party reward model to assess automatic helpfulness and harmlessness scores\u00b2. In addition, since recent studies indicate that GPT-4 can effectively evaluate the quality of LLM answers [84, 16, 17], we also conduct pairwise evaluation on helpfulness and harmlessness aspects using GPT-4. We also employ human judgments as the gold standard for assessing the quality of answers. Human evaluators conduct pairwise comparisons of the top-performing models identified by the automatic evaluations."}, {"title": "5.5 Implementation Details", "content": "Our framework MACPO employs multiple weak teacher models and one strong student model. For the weak teacher LLM backbones, we employ Llama2-7b-base [66], Mistral-7b-v0.1-base [30] and Llama3-8b-base [15]. For the strong student LLM backbone, we employ Llama2-70b-base [66]. During the training phase, weak teachers and strong students are initialized with SFT for 3 epochs, and then these models are trained with DPO for 1 epoch at each iteration. More details of the implementation are in Appendix E."}, {"title": "6 Experimental Results and Analysis", "content": "To answer our research questions, we conduct weak-to-strong alignment experiments on helpfulness and harmlessness, investigate the impact of varying the number of weak teachers, evaluate the performance of weak teachers during iterations, and conduct ablation studies. Additionally, we introduce case studies to further assess the effectiveness of MACPO."}, {"title": "6.1 Weak-to-Strong Alignment Results (RQ1)", "content": "Automatic evaluation. Table 1 and Table 2 present the third-party reward model and GPT-4 evaluation results for the helpfulness and harmlessness alignment datasets. Across all metrics, MACPO consistently outperforms baseline methods on the HH-helpful, HH-harmless and PKU-SafeRLHF datasets. Based on these results, we have three main observations:\nMACPO consistently outperforms strong-to-weak alignment baselines in terms of helpfulness and harmlessness, across HH-Helpful, HH-Harmless and PKU-SafeRLHF test sets. Strong-to-weak alignment methods RLAIF and RLCD assume teachers are stronger than students and only require students to learn from teachers. However, in the weak-to-strong alignment setting, without continuous alignment ability improvement of weak teachers, weak teachers inevitably introduce noise. It indicates the importance of iterative mutual learning of weak teachers and strong students in the weak-to-strong alignment setting.\nDuring the multi-round iterative optimization process, MACPO consistently outperforms self-alignment methods without collapse, in helpfulness and harmlessness. As shown in Table 1, the alignment performance of SPIN and Self-rewarding starts to decrease after the first and second iteration, respectively, while MACPO continues to improve the alignment performance through three rounds iteration. This finding aligns with Shumailov et al. [56] and Wenger [69]: self-alignment methods use self-generated data to continually train LLMs, leading to collapse during multiple iterative optimization rounds. This underscores the effectiveness and necessity of encouraging weak teachers and strong students to learn from each other to reinforce unfamiliar positive behaviors."}, {"title": "6.2 Effect of Different Numbers of Weak Teachers (RQ2)", "content": "We conduct experiments to evaluate the effect of varying the number of weak teachers in MACPO, as shown in Figure 2. As the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance and iterates more rounds without collapse. Specifically, when MACPO contains only one weak teacher, the alignment performance of the strong student starts to degrade after the second round across all datasets. In contrast, when we scale the number of weak teachers to three, MACPO displays improvements over more iterations and achieves better weak-to-strong alignment performance. Bringing more weak teachers in MACPO can improve the diversity of positive behavior to mitigate the model collapse problem [21]."}, {"title": "6.3 Alignment Performance of Weak Teachers (RQ3)", "content": "We conduct experiments to evaluate the alignment performance of weak teachers of MACPO during the iterative training process, as illustrated in Figure 3. Weak teachers improve alignment per-formance over iterations, and outperform state-of-the-art baselines of strong students. The alignment performance of all weak teachers (Llama2-7b, Mistral-7b, and Llama3-8b) improves"}, {"title": "6.4 Ablation Studies (RQ4)", "content": "In Figure 4, we compare MACPO with several ablative variants. The variants are: (i) -MP: we remove the mutual positive behavior augmentation strategy, and use self-generated positive behavior of strong students; (ii) -HN: we remove the hard negative behavior construction strategy of strong students, and use negative behavior of weak teachers; and (iii) -IW: we remove the iterative training process of weak teachers, and freeze weak teachers after initialization. Our findings are as follows:\nRemoving the mutual positive behavior augmentation. We observe that removing mutual positive behavior augmentation (-MP) and using self-generated positive behavior decreases the alignment performance of helpfulness and harmlessness. Specifically, using self-generated data during iterative training leads to strong student collapse and the alignment performance decrease from the second iteration round. This indicates that collecting unfamiliar positive behavior from weak teachers for strong students is more effective for improving weak-to-strong alignment.\nRemoving the hard negative behavior construction. The absence of hard negative behavior con-struction (-HN) results in substantial performance degradation on the helpfulness and harmlessness alignment datasets. As a result, although strong students are still penalizing negative behavior during the alignment process, penalizing unfamiliar negative behavior of weak teachers leads to poor alignment performance.\nRemoving the iterative training process of weak teachers. We observe that removing the iterative training process of weak teachers (-IW) decreases the performance of helpfulness and harmlessness. This demonstrates that freezing weak teachers during the iterative training process results in their inability to improve the quality of positive behavior, which eventually reduces the alignment performance of strong students."}, {"title": "6.5 Case Study", "content": "We conduct several case studies and find that MACPO is more effective at generating answers that are more specific and more in line with the requirements of helpfulness and harmlessness than baselines. More details of our case study results are in Appendix F."}, {"title": "7 Conclusions", "content": "In this paper, we focus on the weak-to-strong alignment task, which aligns strong students with human values using weak labels generated by weak teachers. We have proposed MACPO to encourage weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behavior and penalizing familiar negative behavior. To learn from reinforcing unfamiliar positive behavior, we have proposed a mutual positive behavior augmentation strategy. To learn from penalizing familiar negative behavior, we have proposed a hard negative behavior construction strategy. We have conducted comprehensive experiments on the HH-RLHF and PKU-SafeRLHF datasets, demonstrating that MACPO simultaneously improves the alignment performance of strong students and weak teachers, through automatic and human evaluations. Furthermore, as the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance through more iteration optimization rounds. Overall, our findings provide evidence that encouraging weak teachers and strong students to learn from each other is a promising direction for achieving weak-to-strong alignment. Our code and dataset are available at https://anonymous.4open.science/r/ MACPO-61E6."}, {"title": "Limitations", "content": "In this study, MACPO has only been evaluated to improve weak-to-strong alignment in helpfulness and harmlessness. We plan to expand the assessment of MACPO and adopt it to other challenging tasks such as mathematical reasoning [39, 72, 77] and code programming tasks [40, 35]. Another limitation is that we have only considered fine-tuning on negative behavioral data as a way of inducing negative behavior of LLMs. We plan to explore more jailbreaking attack methods to induce diverse negative behavior, such as adversarial prompting [86] and adversarial decoding [29, 82] for this purpose."}]}