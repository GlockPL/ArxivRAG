{"title": "Fast Convergence of Softmax Policy Mirror Ascent", "authors": ["Reza Asad", "Reza Babanezhad", "Issam Laradji", "Nicolas Le Roux", "Sharan Vaswani"], "abstract": "Natural policy gradient (NPG) is a common policy optimization algorithm and can be viewed as mirror ascent in the space of probabilities. Recently, Vaswani et al. [2021] introduced a policy gradient method that corresponds to mirror ascent in the dual space of logits. We refine this algorithm, removing its need for a normalization across actions and analyze the resulting method (referred to as SPMA). For tabular MDPs, we prove that SPMA with a constant step-size matches the linear convergence of NPG and achieves a faster convergence than constant step-size (accelerated) softmax policy gradient. To handle large state-action spaces, we extend SPMA to use a log-linear policy parameterization. Unlike that for NPG, generalizing SPMA to the linear function approximation (FA) setting does not require compatible function approximation. Unlike MDPO, a practical generalization of NPG, SPMA with linear FA only requires solving convex softmax classification problems. We prove that SPMA achieves linear convergence to the neighbourhood of the optimal value function. We extend SPMA to handle non-linear FA and evaluate its empirical performance on the MuJoCo and Atari benchmarks. Our results demonstrate that SPMA consistently achieves similar or better performance compared to MDPO, PPO and TRPO.", "sections": [{"title": "1 Introduction", "content": "Policy gradient (PG) methods [Williams, 1992; Sutton et al., 1999; Konda and Tsitsiklis, 2000; Kakade, 2001] have been critical to the achievements of reinforcement learning (RL). Although the PG objective is non-concave, recent theoretical research [Agarwal et al., 2021; Mei et al., 2020, 2021; Bhandari and Russo, 2021; Lan, 2023; Shani et al., 2020; Liu et al., 2024; Lu et al., 2024; Alfano and Rebeschini, 2022; Yuan et al., 2023] has analyzed PG methods in simplified settings and demonstrated their global convergence to an optimal policy. While such simplified analyses are helpful in understanding the underlying optimization issues, the resulting methods are rarely used in practice. On the other hand, while methods such as TRPO [Schulman, 2015], PPO [Schulman et al., 2017], MDPO [Tomar et al., 2020] are commonly used in deep RL, their theoretical analysis in the function approximation setting is quite limited. In particular, existing work either (i) analyzes these methods only in the impractical tabular setting [Tomar et al., 2020; Shani et al., 2020] or (ii) modifies these algorithms to make them more amenable to theoretical analysis [Liu et al., 1906; Zhong and Zhang, 2024]. Unfortunately, these modified algorithms are quite different from the original variants and are not systematically benchmarked on standard environments. Consequently, there exists a large gap between PG methods that have theoretical guarantees in realistic settings versus those which are implemented in practice. To make matters worse, it has been demonstrated that code-level implementation details impact the empirical performance more than the underlying algorithm [Engstrom et al., 2019].\nDesigning theoretically principled PG algorithms that simultaneously have good empirical performance on the standard set of benchmarks is the main motivation behind this work. To that end, we leverage an algorithm first proposed by Vaswani et al. [2021], which we modify to remove the need for normalization. We coin this refinement Softmax Policy Mirror Ascent (referred to as SPMA). We show that SPMA has comparable convergence guarantees as existing theoretical techniques [Lu et al., 2024; Yuan et al., 2023] in the tabular and function approximation settings, while achieving comparable practical performance as PPO, TRPO and MDPO, without additional algorithmic modifications. In particular, we make the following contributions."}, {"title": "2 Problem Formulation", "content": "We consider an infinite-horizon discounted Markov decision process (MDP) [Puterman, 2014] defined by M = (S,A,P,r, \u03c1, \u03b3), where S and A represent the states and actions, P : S \u00d7 A \u2192 \\Delta S is the transition probability function, r : S \u00d7 A \u2192 [0, 1] is the reward function, \u03c1 \u2208 \\Delta S is the initial state distribution, and \u03b3 \u2208 [0, 1) represents the discount factor. In this paper, we exclusively consider the setting where the number of states and actions is finite, but potentially large.\nGiven s \u2208 S, the policy \u03c0 induces a probability distribution \u03c0(.|s) over the actions. The action-value function Q:S \u00d7 A \u2192 R induced by \u03c0 is defined as $Q^\\pi(s,a) := E[\\sum_{t=0}^\\infty \\gamma^t r(s_t,a_t)|s_0 = s, a_0 = a]$ where $s_t \\sim p(.|s_{t-1}, a_{t-1})$, and $a_t \\sim \\pi(.|s_t)$ for t > 1. The value function corresponding to $Q^\\pi$ starting from state s is defined as $V^\\pi(s) := E_{a \\sim \\pi(.|s)}[Q^\\pi(s, a)]$ with J(\u03c0) := $V^\\pi(\u03c1) := E_{s \\sim \u03c1}[V^\\pi(s)]$ representing the expected discounted cumulative reward. Furthermore, the advantage function A : S\u00d7A \u2192 R is defined as $A^\\pi (s,a) := Q^\\pi(s,a) - V^\\pi(s)$. The policy also induces a discounted state-occupancy measure $d^\\pi(s) := (1 \u2212 \u03b3) \\sum_{t=0}^\\infty Pr [s_t = s|s_0 \\sim \u03c1]$ over the states. The objective is to find an optimal policy \u03c0* that maximizes the expected reward J(\u03c0), i.e. \u03c0* = arg max J(\u03c0). As a special case, in the bandit setting, |S| = 1, |A| = \u039a, \u03b3 = 0, and J(\u03c0) = <\u03c0,r>, with K representing the number of arms."}, {"title": "3 Softmax Policy Mirror Ascent: Tabular Parameterization", "content": "Softmax policy mirror ascent (referred to as SPMA) represents the policy using the softmax function h:$\\mathbb{R}^A$ \u2192 \u0394\u0391 i.e. \u03c0(s) = h(z(s,.)) s.t. for all (s,a) \u2208 S \u00d7 A, $\\pi(a|s) = \\frac{exp(z(s,a))}{\\sum_{a'} exp(z(s,a'))}$, where the logits z are SA-dimensional vectors and AA is the A-dimensional simplex. We first focus on the tabular parameterization where the number of parameters scales with the number of states and actions, and aim to learn the logits corresponding to an optimal policy. With some abuse of notation, we use J(z) to refer to J(\u03c0) where \u03c0(s) = h(z(s,.)) and state the objective as: $\\max_{z\\in\\mathbb{R}^{SA}} J(z)$.\nAs the name suggests, SPMA uses mirror ascent (MA) to maximize J(z). For a differentiable, strictly convex mirror map \u03a6, \u039c\u0391 [Beck and Teboulle, 2003; Bubeck et al., 2015] is an iterative algorithm whose update at"}, {"title": "3.1 Bandit Setting", "content": "It is instructive to first instantiate the SPMA update for the bandit setting where J(\u03c0) = <\u03c0,r>. In this setting, \u2207zJ(z) \u2208 RA s.t. $[\\nabla_z J(z)](a) = \\pi(a) [r(a) \u2212 \\langle \\pi,r\\rangle]$. Following Vaswani et al. [2021], we use the log-sum-exp mirror map i.e. \u03a6(z) = $\\log( \\sum_a \\exp(z(a))$. Since $[\\nabla\\Phi(z)](a) = \\frac{\\exp(z(a))}{\\sum_{a'} \\exp(z(a'))} = [h(z)](a) = \\pi(a)$, the logit and probability spaces are dual to each other, and the \u2207\u03a6 map can be used to move between these spaces. Given this, the SPMA update can be written as:"}, {"title": "3.2 MDP Setting", "content": "In order to extend SPMA to the MDP setting, we use a (state-wise) weighted log-sum-exp mirror map, i.e. for a logit z \u2208 RSA, we define $\\Phi(z) := \\sum_{s} w(s) \\phi(z(s, \u00b7)) = \\sum_{s} w(s) \\log(\\sum_{a} \\exp(z(s, a))$ where w(s) are the per-state weights. Following the proof of Vaswani et al. [2024, Lemma 11], the resulting Bregman divergence is given as: $D_{\\Phi}(z,z') = \\sum_{s}w(s) KL(\\pi'(\\cdot|s)||\\pi(\\cdot|s))$ where \u03c0 and \u03c0' are the policies corresponding to logits z and z'. At iteration t of SPMA, we choose w(s) = $d^\\pi_t(s)$ and use the policy gradient theorem [Sutton et al., 1999]"}, {"title": "3.3 Theoretical Results", "content": "In this section, we prove convergence guarantees for SPMA in the multi-armed bandit and tabular MDP settings. We first establish linear convergence for SPMA for multi-armed bandits for any constant \u03b7 \u2264 1."}, {"title": "4 Handling Function Approximation", "content": "Handling large MDPs requires function approximation (FA) techniques to share information between states and actions. For example, given a set of state-action features X \u2208 RSA\u00d7d where d << SA, the log-linear policy parameterization [Agarwal et al., 2021; Alfano and Rebeschini, 2022; Yuan et al., 2023] considers policies of the form: $\\pi(a|s) = \\frac{exp( \\langle x(s,a), \\theta \\rangle)}{\\sum_{a'} exp( \\langle x(s,a'), \\theta \\rangle)}$, where \u03b8 \u2208 Rd is the parameter to be learned. Hence, the log-linear policy parameterization can handle large state-action spaces while learning a compressed d-dimensional representation. We interpret the log-linear"}, {"title": "4.1 Comparison to Existing Approaches", "content": "Comparison to NPG: A principled extension of NPG to handle FA is via the compatible function approximation [Kakade, 2001; Agarwal et al., 2021]. An example of such an algorithm, Q-NPG involves solving a quadratic surrogate at each iteration t: $\\hat w_t = \\arg \\min_{w} \\sum_s d_t(s) \\sum_a \\pi_t(a|s) (f_w(s, a) - Q^\\pi_t (s, a))^2$. The policy parameters are updated using $\\hat w_t$ which corresponds to the natural gradient direction. While this approach results in theoretical guarantees (see Section 4.2 for details); for a general parameterization, the resulting algorithm involves changing the representation of the critic at every iteration. Consequently, solving the surrogate is expensive, limiting the practicality of the method.\nComparison to MDPO: A more practical extension of NPG is mirror descent policy optimization [Tomar et al., 2020] (MDPO). Similar to SPMA, MDPO can be interpreted as projected (onto the feasible set of policies) mirror ascent in the space of probabilities [Vaswani et al., 2021]. The resulting surrogate (as a function of the policy parameters) is given by: $\\sum_s d_t(s) KL (\\pi_t(\\cdot|s) || h (f_{\\theta_t} (s, \u00b7) \\exp(\\eta Q^{\\pi_t} (s, \u00b7))))$. Unlike the surrogate in Eq. (4), the MDPO surrogate is non-convex even when using a tabular softmax parameterization for the policy, and consequently does not have any theoretical guarantees. However, MDPO results in good empirical performance, and we compare to it in Section 5.\nComparison to TRPO: As explained in Vaswani et al. [2021], the surrogate in Eq. (4) is closely related to TRPO. In particular, the TRPO update consists of solving the following optimization problem: $\\arg \\max_\\theta \\mathbb{E}_{s\\sim d_t}[\\sum_a \\pi_t(a|s) A^{\\pi_t} (s, a) \\frac{\\pi_t(a|s)}{\\pi_{\\theta_t}(a|s)}]$, such that $\\mathbb{E}_{s\\sim d_t} [KL(\\pi_t(\\cdot|s) || \\pi_{\\theta_t}(\\cdot|s))] < \\delta$. SPMA (i) uses $\\sum_s d_t(s) \\sum_a \\pi_t(a|s) A^{\\pi_t} (s, a) \\log \\frac{\\pi_t(a|s)}{\\pi_{\\theta_t}(a|s)}$, i.e. the logarithm of the importance sampling ratio, making the resulting update more stable [Vaswani et al., 2021] and (ii) enforces the proximity between policies via a regularization rather than a constraint. Enforcing the trust-region constraint in TRPO requires additional hyper-parameters, code-level optimizations and computation [Engstrom et al., 2019]. In contrast, SPMA is more computationally efficient and simpler to implement in practice. In the next section, we study the theoretical properties of Algorithm 1."}, {"title": "4.2 Theoretical Guarantee", "content": "For rewards in [0,1] and for a general policy parameterization, Vaswani et al. [2021] prove that, for \u03b7 \u2264 1 - \u03b3, Algorithm 1 results in monotonic improvement, i.e. $J(\\pi_{t+1}) \u2265 J(\\pi_t)$ and hence converges to a stationary point at an O(1/6) rate. Since J is non-convex and can have multiple stationary points, the result in Vaswani et al. [2021] does not provide sufficient evidence for the good empirical performance of Algorithm 1. In this section, we prove that, under reasonable assumptions similar to existing works, Algorithm 1 can converge to the neighbourhood of the optimal value function at a linear rate. The size of"}, {"title": "5 Empirical Evaluation", "content": "We evaluate SPMA on three types of problems: (i) tabular MDPs with access to exact policy gradients, (ii) MDPs with continuous states but discrete actions, using inexact policy gradients, and (iii) MDPs with continuous state-actions spaces and inexact gradients. For tabular MDPs, we use the tabular parameterization and compare SPMA against NPG and constant step-size SPG [Mei et al., 2020]. For these environments, we also consider log-linear policies and compare SPMA to MDPO and SPG. For non-tabular environments, we consider PPO, TRPO and MDPO as baselines. We consider two variants of TRPO TRPO-constrained, the standard optimized variant in Raffin et al. [2021] and"}, {"title": "6 Discussion", "content": "We developed SPMA, a PG method that corresponds to mirror ascent in the dual space of logits. We believe that our paper bridges the gap between theoretical PG methods and practical objectives by presenting a method that offers strong theoretical convergence guarantees while delivering competitive practical performance (compared to PPO, TRPO, MDPO), without relying on additional heuristics or algorithmic modifications. In the future, we aim to develop techniques for adaptively tuning the step-size and avoiding expensive grid-searches. We also plan to develop and analyze an off-policy variant of SPMA."}, {"title": "Supplementary Material", "content": "Organization of the Appendix\nA Multi-armed Bandit Proofs\nB MDP Proofs\nC Tabular MDP Experiments\nD Additional Details for Stable Baselines Experiments"}, {"title": "A Multi-armed Bandit Proofs", "content": "Theorem 1. The SPMA update in Eq. (2) with (i) a constant step-size \u03b7 < 1, and (ii) uniform initialization i.e. $\\pi_0(a) = \\frac{1}{K}$ for all a converges as:\n$r(a*) - \\langle \\pi_T,r\\rangle \\le (1-\\frac{\\eta \\Delta_{min}}{K})^T$,\nwhere T is the number of iterations, a* is the optimal arm i.e. $a* = \\arg \\max_a r(a)$ and $\u2206_{min} := \\min_{a\\neq a*} \u2206(a*, a) = r(a*) \u2013 r(a) is the gap.\nProof.. As in equation (2), we can write the update for arm a as following where \u2206(a, a') = r(a) \u2013 r(a'),"}, {"title": "A.1 Super-linear Rate for Bandits", "content": "In order to achieve the desired fast rate of convergence, we modify the update in Eq. (2) to use a set of $\\{\\eta_{a,a'}\\}_{a,a'\\in[K]}$ constant gap-dependent step-sizes. The new update can be written as:\n$\n\\pi_{t+1}(a) = \\pi_t(a) [1 + \\sum_{a'\\neq a}\\pi_t(a') \\eta_{a,a'} \\Delta(a, a')]\n$\nThe following theorem shows that the above update results in super-linear convergence."}, {"title": "B MDP Proofs", "content": "B.1 Tabular Setting\nLemma 1. For any policy \u03c0t we have\n$\\sum_a \\pi_t(a|s) [A^{\\pi_t}(s,a)]^2 \\ge C_t \\max_a A^{\\pi_t} (s, a)$\nwhere $C_t := \\min_s \\{\\pi_t(\\overline A_t(s)|s) \\Delta_t(s)\\}$, $\\overline A_t(s) := \\arg \\max_{a\\in A} Q^{\\pi_t}(s,a)$, $\\pi_t(\\overline A_t(s)|s) = \\sum_{a\\in \\overline A_t(s)} \\pi_t(\\overline a_t(s)|s)$ and $\u2206_t(s) := \\max_{a\\in A} Q^{\\pi_t} (s, a) \u2013 \\max_{a \\notin \\overline A} Q^{\\pi_t} (s, a)$.\nProof. Recall $\\overline A_t(s) := \\arg \\max_{a\\in A} A_t(s, a)$ i.e. $\\overline A_t(s)$ is a set containing actions with maximum advantage for state s. Let's define $\\pi_t(\\overline A_t(s)|s) = \\sum_{a \\in \\overline A_t(s)} \\pi_t(\\overline a_t(s)|s)$. We can split the sum on the LHS of the above over $\\overline A_t(s)$:"}, {"title": "B.2 Function Approximation With Exact Advantage", "content": "Recall the definitions of lt and lt\n$\\ell_t(\\theta) = \\sum_s d_t (s) KL(\\pi_{t+1/2}(\\cdot|s) || \\pi_{\\theta}(\\cdot|s))$\n$\\hat \\ell_t(\\theta) = \\sum_{s \\sim \\tau}KL (h(f_{\\theta_t} (s, \u00b7))(1 + \\eta A^{\\pi_t}(s, \u00b7)) || h(f_{\\theta}(s, \u00b7)))$"}, {"title": "B.3 Function Approximation With Inexact Advantage", "content": "In practice computing the Art at every iteration is costly. In this section, we assume that we access an oracle such that at each iteration t, it gives us $ \\hat A^{\\pi_t}$ an approximation of $A^{\\pi_t}$.\nAssumption 4. Valid Approximation. For all iterations t and (s, a) \u2208 S \u00d7 A, | $ \\hat A^{\\pi_t} (s, a)$| \u22641.\nAssumption 5. Approximation Error. For all iterations t and s s\u2208 S, $| |A^{\\pi_t} (s,.) - $ \\hat A^{\\pi_t} (S,.) | | \\le \\epsilon_{approx}.$\nUsing this inexact advantage function, we define the following update and functions\n$\\pi_{t+1/2}(a|s) = \\pi_t(a|s)(1 + \\eta \\hat A^{\\pi_t} (s,a))$\n$\\ell_t(\\theta) = \\sum_s d_t (s) KL(\\pi_{t+1/2}(\\cdot|s) || \\pi_{\\theta}(\\cdot|s))$\n$\\hat \\ell_t(\\theta) = \\sum_{s \\sim \\tau}KL (h(f_{\\theta_t} (s,-))(1 + \\eta \\hat A^{\\pi_t}(s, \u00b7)) || h(f_{\\theta}(s, \u00b7)))$"}, {"title": "C Tabular MDP Experiments", "content": "In this section, we empirically evaluate SPMA on tabular MDP environments. For these experiments, we use Cliff World [Sutton, 2018] and Frozen Lake [Brockman, 2016] following the setup in Vaswani et al. [2024]. In subsection, C.1 we examine the case where the policy is parametrized using softmax tabular representation. In subsection, C.2 we investigate the function approximation setting as described in Section 4, where the policy is parametrized using a linear model."}, {"title": "C.1 Softmax Tabular Representation", "content": "For this parametrization we initialize z \u2208 RS\u00d7A uniformly, i.e., $\\pi_0(a|s) = \\frac{1}{A}$ for each a and s. Furthermore, for each algorithm, we set \u03b7 using a grid search and pick the step-sizes that result in the best area under the curve (AUC). The tabular MDP results suggest SPMA and NPG achieve similar performance and they both outperform SPG [Sutton et al., 1999; Schulman et al., 2017] (see Fig. 3). To analyze the sensitivity of each algorithm to the choice of \u03b7, we examine each optimizer across different values of \u03b7. The results in Fig. 4 suggest that overall SPG (in green) is more sensitive to different values of \u03b7 compared to SPMA (blue) and NPG (red)."}, {"title": "C.2 Linear Functional Approximation (Linear FA)", "content": "For the Linear FA setting, we use a log-linear policy parametrization: $\\pi(a|s) = \\frac{exp( \\langle X(s,a), \\theta \\rangle)}{\\sum_{a'} exp( \\langle x(s,a'), \\theta \\rangle)}$, with X \u2208 RSAxd and \u03b8 \u2208 Rd representing the features and parameters. We use constant initialization for 0 and following Vaswani et al. [2024], use tile-coded features for X. As in the previous section, we set \u03b7 for SPMA and MDPO via grid search and report results based on the best AUC. For the inner loop optimization (e.g., minimizing Eq. (5)), we use Armijo line search [Armijo, 1966], avoiding an additional grid search for the step-size. For SPG we use the update from Mei et al. [2020] where Armijo line search is used to set n."}, {"title": "D Additional Details for Stable Baselines Experiments", "content": "In subsection D.1, we provide additional details on the hyper-parameters used for the results in Section 5. Next, we present an ablation study on the number of inner loop optimization steps (m) in subsection D.2."}, {"title": "D.1 Atari and Mujoco Details", "content": "In the Atari experiments, we use the default hyper-parameters for each method from stable baselines [Raffin et al., 2021]. This choice is motivated by two factors: i) following the work of Tomar et al. [2020], we aim to evaluate the effectiveness of different surrogate losses without conducting an exhaustive search over numerous hyper-parameters; ii) the CNN-based actor and critic networks make grid searching over many hyper-parameters (e.g., framestack, GAE \u03bb, horizon length, discount factor) computationally infeasible. For a complete list of hyper-parameters used in the Atari experiments, see Table 1.\nIn the MuJoCo experiments, we use the default hyper-parameters from stable baselines for each method, but perform a grid search on the Adam inner loop step size for PPO and TRPO-constrained (best among [3 \u00d7 10-3,3 \u00d7 10\u22124,3 \u00d7 10\u22125]) and the probability ratio clipping parameter in PPO (best from [0.1, 0.2, 0.3]). For the regularized surrogates (i.e., the remaining methods: SPMA, MDPO, and TRPO-regularized), we avoid a grid search for the inner loop step size by using a full batch (i.e., the horizon length) along with the Armijo line search [Armijo, 1966]. See Table 2 for the full list of hyper-parameters used in the MuJoCo experiments.\nTo set \u03b7 for the regularized surrogates, we perform a grid search over five fixed values ([0.3, 0.5, 0.7, 0.9, 1.0]). Although Tomar et al. [2020] anneals \u03b7 from 1 to 0 during training, we observe that using a constant step size results in better performance. Our grid search strategy for all stable baselines experiments is consistent: we run the experiments for 2 million iterations, select the hyper-parameters that yield the best AUC, and then use these hyper-parameters for an additional 8 million iterations."}, {"title": "D.2 Ablation Study on the Number of Inner Loop Optimization Steps", "content": "In this subsection, we investigate the effect of varying the number of inner loop optimization steps (m) in the stable baselines experiments. Consistent with Tomar et al. [2020], we observe that using m = 1 results in poor performance, so we focus on larger values of m. In the MuJoCo experiments, increasing m from 5 to 10 and 15 consistently improves the performance of SPMA (see Figure 9). Specifically, for larger m, SPMA becomes comparable to TRPO-constrained on Hopper and Ant, while outperforming it on HalfCheetah (see Figure 7).\nFor the Atari experiments, we observe that increasing m does not necessarily improve the results across methods (see Figure 10). We conjecture that this is a side-effect of using a constant tuned step-size (for m = 5) in the inner-loop. In the future, we plan to run the full grid-search for the inner-loop step-size for each value of m. Alternatively, we plan to investigate an adaptive way of setting the inner-loop step-size."}]}