{"title": "CLMIA: Membership Inference Attacks via Unsupervised Contrastive Learning", "authors": ["Depeng Chen", "Xiao Liu", "Jie Cui", "Hong Zhong"], "abstract": "Since machine learning model is often trained on a limited data set, the model is trained multiple times on the same data sample, which causes the model to memorize most of the training set data. Membership Inference Attacks (MIAs) exploit this feature to determine whether a data sample is used for training a machine learning model. However, in realistic scenarios, it is difficult for the adversary to obtain enough qualified samples that mark accurate identity information, especially since most samples are non-members in real world applications. To address this limitation, in this paper, we propose a new attack method called CLMIA, which uses unsupervised contrastive learning to train an attack model without using extra membership status information. Meanwhile, in CLMIA, we require only a small amount of data with known membership status to fine-tune the attack model. Experimental results demonstrate that CLMIA performs better than existing attack methods for different datasets and model structures, especially with data with less marked identity information. In addition, we experimentally find that the attack performs differently for different proportions of labeled identity information for member and non-member data. More analysis proves that our attack method performs better with less labeled identity information, which applies to more realistic scenarios.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the development of artificial intelligence (AI) has been in full swing. Machine learning (ML) is the core part of AI, and it has a wide range of applications in many fields, such as image recognition, natural language processing, and recommendation systems. With the continuous development of ML technologies, especially deep learning, massive amounts of data can be utilized more fully and efficiently. This is because t raining ML models require a large amount of user data, which often involves personal privacy content, such as financial information[7], medical data information[43], location information[31] and other personally sensitive data.\nPrior studies[3], [36], [28], [17], [34] have been shown that ML models largely remember the data in the training set, leading to the disclosure of private information in the training set. A membership inference attack (MIA) is an attack that determines whether data samples are used to train an ML model, and this attack leads to the disclosure of private information about the ML model.\nIn the last few years, to test the leakage of membership information of the ML models, most existing MIAs[36], [15], [26], [33] have focused on improving attack methods through more fine-grained analysis or reducing the background knowledge and computing power required to conduct an attack. Specifically, most of MIA methods train attack models to perform inference attacks based on the posterior of the target model, which has significantly different outputs for members and non-member data, mainly because ML models are trained on a limited dataset, so the models are trained multiple times on the same data sample. And this feature gives the model sufficient ability to remember the data samples in the training set, resulting in the model performing differently against the training set data and the non-training set data. Therefore, for these different performances, MIA can detect the difference in performance and infer member and non-member data, resulting in the disclosure of private information on the training dataset. As shown in Figure 1, We can easily see that the target model has significantly different output results for member and non-member data, which leads the adversary to effectively distinguish whether the data samples are in the training set of the target model. However, this approach tends to misclassify non-member data as member data, leading to a high false- positive rate (FPR). Carlini et al. [2] pointed out that M\u0399\u0391 should reduce the occurrence of high FPR and follow the true-positive rate (TPR) at the Low FPR strategy. Besides, this distinction between members and non-members requires the data identified to be labeled in advance to train the attack model text, which is difficult in a realistic scenario where a large amount of data with general membership information is available[16]. Therefore, in this paper, we propose an attack method to train the attack model using unsupervised contrastive learning, which requires less data with labeled membership information and reduces the adversary's a priori knowledge.\nThis paper proposes a novel membership inference at-"}, {"title": "II. BACKGROUND", "content": "A. Membership Inference Attack\nMembership inference attacks lead to the disclosure of data sample privacy in the training datasets by determining whether the data samples are used for training ma- chine learning models. Specifically, MIA detects whether the data sample $x_i$ comes from the training dataset $D_{train}$ = {$(x_1,y_1), (x_2,y_2), ..., (x_n, y_n)$ } of the target model $f_\\theta$ by high-precision prediction, thereby threatening the privacy of data providers. The following function defines a typical MIA:\n$A : K, f_\\theta, x \\rightarrow {0 or 1}$                                   (1)\nWhere A is the attacker, K is the degrees of the target model the attacker possesses, i.e., the attacker's prior knowledge. x is the data sample waiting for inference, and $f_\\theta$ is the target model. If the output is 0, it is judged that the sample x does not belong to the training data set $D_{train},i.e.x \\notin D_{train}$, otherwise $x \\in D_{train}$.\nShadow model based MIA. Shadow model-based MIA was first proposed by [36], which is implemented by training multiple shadow models with the same structure and training data set as the target model. This scheme imitates the behavior of the target model through multiple shadow models to achieve the privacy of the target model training set without direct access to the target model. Still, the attack performance of this method relies on the transferability of the shadow model and the target model and requires training multiple shadow models, which increases the attack overhead. To reduce the problem of high attack overhead due to introducing multiple shadow models, [34] proposes MIA based on a single shadow model, where attacker A only needs to train one shadow model, which not only saves computational attack overhead but also achieves the same attack performance as multiple shadow models.\nIn addition, [28], [17], [25], [33], [30], [38], [22] conducted a more fine-grained analysis of it, as well as being applied in different scenarios, such as industrial IoT [5], recommender systems [42], semantic segmentation [14], [41] and speech models [35], [29] etc.\nThreshold-based MIA. In threshold-based comparison MIA, most of the proposed methods do not use shadow models, but obtain the output of the target model directly, followed by statistical analysis of it, and predict the membership re- lationship by comparing it with the threshold value set in advance. Li and Zhang [24] propose an attack schema that by comparing the cross-entropy loss $CELoss$, the method considers the loss in training set smaller than in the test set. Therefore when the $CELoss$ is smaller than the threshold $T$ set in advance, it is judged as a member. Otherwise, it is a non-member. Furthermore, Rahimian et. al [32] present a sampling attack to generate perturbed samples of data samples x by a perturbation function pert(), which argues that at a specific level of perturbation, member data do not easily change their labels compared to non-member data because training data are further away from the decision boundary than non-training data samples. Besides, [10] pointed out to gener- ate adversarial samples by adding perturbed noise to change their original prediction labels and, finally, to determine the membership relationship by measuring the size of the added noise. However, this one-size-fits-all decision approach only applies to simple decision boundaries, and no effective attack can be implemented for target models with complex decision boundaries. Therefore, it is essential to propose a more fine- grained attack to evaluate each data sample's impact on the dataset's distance by differential comparison [16]. Specifically,"}, {"title": "III. ATTACK METHODOLOGY", "content": "In this section, we introduce the methodology of CLMIA. We first define the threat model based on the attacker's knowledge of the target model and then introduce the design intuition of our attack method. And the details of our attack method CLMIA are proposed at last.\nA. Threat Model\nIn this paper, we set the attacker's knowledge as a black box, which does not know the structure of the target model and only has access to the posterior probabilities of the target model. The attacker only has an unlabeled target dataset $D_t$, which infers the membership of its dataset and a few labeled datasets $D_l$ that have identified members and non-members. This is realistic because the attacker can get a target dataset for reasoning about the membership in the dataset and has a small amount of data with annotated membership information.\nB. Design Intuition\nExisting MIAs use a supervised approach to train the attack model and require a large dataset with identified member states for training the attack model, which essentially limits the generalization ability of the attack model. In realistic scenarios, it is difficult to obtain such a large number of datasets with labeled members and non-members. To mitigate this obstacle, we propose a contrastive learning framework\u2013 CLMIA, which has good generalization capability through contrastive learning and does not require labeled data during the training phase of the attack model and only uses a small amount of labeled data when fine-tuning the attack model.\nOur attack method distinguishes between members and non-members only by the posterior probability of the target model output. For the output posterior probability of a data sample, two similar positive samples are generated using data augmentation. Then the model is trained in an unsupervised manner by using a loss function for contrast learning. By contrast learning, we expect to make member data closer to each other and different types of data further away from each other. Thus, the attack model can distinguish between the member and non-member data efficiently.\nC. Attack Method\nInspired by contrastive learning not requiring labeled data and the trained model having better generalization ability, we propose a new membership inference attack called contrastive learning MIA, CLMIA. To the best of our knowledge, this is the first time unsupervised learning has been applied to train an attack model. In CLMIA, the attacker uses a black-box approach to access the target model. The attacker does not need to know the structure and internal parameters of the target model but only the output of the last layer of the target model. This is the same way as the current mainstream MIA approach [27] to access the target model. Besides, the attacker has a target dataset $D_l$ for which membership information has not been inferred and a limited number of datasets $D_i$ for which membership information has been annotated. This is all the information the adversary has about the target model, which is easier to obtain in a real scenario.\nIn addition, the positive and negative samples are needed to train the model using contrastive learning. To solve this issue, We adopt the dropout layer. Specifically, the adversary first adds the dropout layer to the output of the last layer of the target model as the positive samples $x_i$ and $x_j$ generated from the data sample x. By doing this, the adversary can obtain positive samples and train the attack model using contrastive learning. After obtaining positive samples, the adversary can fine-tune the attack model obtained from the above training based on a small number of labeled datasets $D_r$. Finally, the attack model can infer the target dataset's membership state only by the output of the last layer of the target model.\nTo summarize, the overall pipeline of CLMIA is illustrated in Figure 2. It consists of five stages: training the target model, training the shadow model, training the attack model, fine-tuning the attack model, and membership inference. The detailed implementation of each specific phase is as follows:\nTraining the Target Model. First, several users form a data"}, {"title": "IV. EVALUATION", "content": "A. Experimental Setup\nIn this section, we implement extensive experiments using multiple model structures and datasets to evaluate our attack method CLMIA and compare it with other representative schemes.\nDatasets. we conduct our experiments on the following three public image datasets:\nCIFAR-10: The dataset includes 50,000 training data and 10,000 test data, and the images are 3-channel 32*32 color RGB images divided into 10 classes, including aircraft, birds, cats, frogs, and other classes.\nCIFAR-100: The dataset consists of 50,000 training data and 10,000 test data. The images are 3-channel\n32*32 color RGB images divided into 100 classes, and these 100 classes are divided into 20 superclasses, each containing 5 more fine-grained labels.\nSTL-10: The dataset consists of 5000 training data and 8000 test data with 3-channel 96*96 color RGB images, divided into 10 classes, each with fewer labeled training examples than CIFAR-10.\nModels. For the choice of model structure, we exploit the following three models to test the performance of CLMIA, including simple CNN, VGG-19, and Resnet-18 for the target model. The performance of different model structures on different data sets, i.e., the model's training accuracy and testing accuracy, is expressed as the degree of overfitting of the model. As shown in Table I.\nMetrics. We evaluate the effectiveness of our approach through the following metrics:\nBalanced Accuracy. This metric means that there are an equal number of member and non-member data samples in the test dataset. This gives an accuracy rate of 50% if the attacker is guessing at random.\nF\u2081 Score. It is used to measure the performance of the classification model, taking into account both the precision and recall of the classification model.\nROC. Receiver Operating Characteristic (ROC) curve is exploited to compare the ratio of true-positives to false-positives to evaluate the classifier's performance.\nAttack Baselines. In this part, we show the effectiveness of our CLMIA by comparing with five existing representative MIA methods [21], [34], [37], where the attack methods are as follows:\nPrediction correctness. Lein et al. [21] point out making the sample a member when the model outputs the correct predictive label and a non-member oth- erwise. This attack evaluates existing models against MIAs well but performs poorly against attacks on models with powerful generalization capabilities.\nNN attack. Salem et al. [34] propose to train the attack model by using a single shadow model combined with the posterior of the target model."}, {"title": "V. ABLATION STUDY", "content": "In this section, we analyze several essential factors that affect the performance of the attack. First, we pass through only the fully Connected layers (FC layers) as a comparison to demonstrate that learning, by contrast, enables the target model to learn the distinction between members and non- members. We then explore the performance of the attack when the adversary knows only a limited amount of membership information. Finally, we also analyze the effect of dropout rates and different temperature parameters in the loss function on the attack performance."}, {"title": "VI. RELATED WORK", "content": "A. Membership Inference Attacks\nMembership inference attack (MIA) is a new type of attack that leads to the disclosure of user privacy during machine learning and has attracted extensive academic attention in recent years. Shokri et al. [36] first propose deep learning classification tasks for MIAs, which use multiple shadow models to simulate the behavior of the target model. And the output posterior of the shadow model is used to train the attack model. But this requires a significant computational overhead of attack. To reduce the computational overhead, Salem et al. [34] present a novel method to merge multiple shadow models into a single shadow model and gradually relax the adversary knowledge assumptions.\nIn Addition, other researchers propose not to train the attack model but only by comparing the relationship between the size of the threshold 7 set in advance and the output of the target model. Li et al. [24] present a transfer attack method by comparing the cross entropy loss of the target model output. To solve the weakness of attack based on prediction entropy, Song et al. [37] a modified prediction entropy, combining the properties of monotonically decreasing with the correct label while monotonically increasing with the incorrect label. Salem et al. [34] present an inference method by comparing the maximum posterior probability of the target model output. And Yeom et al. [40] propose to infer the membership state by comparing the prediction loss of the samples. However, such MIAs that require only thresholds for comparison need the adversary to know the difference between the distribution of members and nonmembers in advance to design the optimal threshold for effective MIAs.\nB. Defense Mechanism\nThe literature [36], [28], [40], [23] point out that overfitting is one of the leading causes of the vulnerability of machine learning models to membership inference attacks. Therefore, in the research of defense against MIA, preventing machine learning models from overfitting during training is a defense below MIAs. For example, this can be achieved by $L_1$ or $L_2$ regularization [23], Dropout [20], and the robustness of the model through data augmentation [19]. Chen et al. [4] propose a gradient ascent strategy to reduce the generalization error while guaranteeing the accuracy of the target model.\nIn addition, there are studies to fool adversaries by per- turbing the output and internal parameters of the model, thus hiding the accurate model parameters and output. Defenders can use the differential privacy stochastic gradient descent (DP-SGD) mechanism [1], which perturbs the data- dependent objective function by adding noise to the gradient during the model's training, which replaces the normal gradient descent and mitigates the inference attack. Besides, Jia et al. [18] inspired by the adversarial sample, and propose the MemGuard mechanism, which achieves the purpose of fooling the attacker by adding perturbation noise to the output probability of the target model, so that each inference attack by the attacker is close to a random guess, thus protecting the privacy informa- tion of the training set in the target model.\nC. Contrastive Learning\nThe success of many supervised models relies on a large amount of labeled data behind them, and obtaining labeled data is often costly or difficult to achieve. Contrastive learning is a new unsupervised learning approach that focuses on learning standard features among similar samples and distinguishing differences among non-similar samples. Instead of focusing on the tedious details of samples, contrast learning only needs to learn to distinguish data on the feature space at the abstract semantic level. Hence, the model and its optimization become more straightforward, and its generalization ability is more robust. The goal of contrastive learning is to learn an encoder that encodes similar data of the same kind and makes the encoding results of different classes of data as different as possible. Wu et al. [39] propose to achieve viewing each sample as a class by using a memory bank to store negative samples. He et al. [13] propose using momentum contrast to do unsupervised visual representation learning. Chen et al. [6] propose a simple visual representation contrast learning framework SimCLR, which generates negative samples in contrast learning by data augmentation techniques. All the above contrastive learning approaches require using both pos- itive and negative samples for training models. In addition, some studies propose contrastive learning approaches that do not need negative samples. Grill et al. [12] predicted the data samples after data augmentation without using negative samples. Chen et al. [9] proposed a more straightforward contrastive learning framework designed by siamese networks without using negative samples."}, {"title": "VII. ETHIC STATEMENT", "content": "This work aims to promote the understanding of mem- bership inference attacks by identifying vulnerabilities in ma- chine learning models and contributing to the development of stronger defenses. While the proposed method, CLM\u0399\u0391, introduces an improved attack mechanism, it is intended solely for ethical purposes, such as evaluating and enhancing model privacy and security. All experiments were conducted on publicly available datasets containing no personally identifiable information. Misuse of this work to compromise real-world systems or violate data privacy would contradict its ethical objectives. We encourage researchers and practitioners to use this work responsibly and in alignment with privacy-preserving practices."}, {"title": "VIII. CONCLUSION", "content": "In this paper, we propose an unsupervised contrastive learning approach to train an attack model for membership inference attacks. We demonstrate that training the attack model with an unsupervised contrastive learning approach has better generalization ability. Specifically, we propose a new attack method, CLMIA, which uses the output of the target model and statistically computes the features of the data iden- tity information, and then uses the contrast learning approach to train the attack model in a black-box scenario to obtain the attack model. Our extensive experiments have confirmed that CLMIA has better attack performance, especially for the case of less marked data identity information, and CLMIA has better generalization ability. In addition, we also analyze some important factors that affect the performance of CLMIA"}]}