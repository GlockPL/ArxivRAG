[{"title": "Towards Understanding Extrapolation: a Causal Lens", "authors": ["Lingjing Kong", "Guangyi Chen", "Petar Stojanov", "Haoxuan Li", "Eric P. Xing", "Kun Zhang"], "abstract": "Canonical work handling distribution shifts typically necessitates an entire target\ndistribution that lands inside the training distribution. However, practical scenarios\noften involve only a handful of target samples, potentially lying outside the training\nsupport, which requires the capability of extrapolation. In this work, we aim to\nprovide a theoretical understanding of when extrapolation is possible and offer\nprincipled methods to achieve it without requiring an on-support target distribution.\nTo this end, we formulate the extrapolation problem with a latent-variable model\nthat embodies the minimal change principle in causal mechanisms. Under this\nformulation, we cast the extrapolation problem into a latent-variable identification\nproblem. We provide realistic conditions on shift properties and the estimation\nobjectives that lead to identification even when only one off-support target sample is\navailable, tackling the most challenging scenarios. Our theory reveals the intricate\ninterplay between the underlying manifold's smoothness and the shift properties.\nWe showcase how our theoretical results inform the design of practical adaptation\nalgorithms. Through experiments on both synthetic and real-world data, we validate\nour theoretical findings and their practical implications.", "sections": [{"title": "1 Introduction", "content": "Extrapolation necessitates the capability of generalizing beyond the training distribution sup-\nport, which is essential for the robust deployment of machine learning models in real-world\nscenarios. Specifically, given access to a source distribution \\(D_{src} := p(x_{src}, y_{src})\\) with support\n\\(X_{src}:=\\text{Supp}(P_{src}(x))\\) and one or a few out-of-support samples \\(x_{tgt} \\notin X_{src}\\), the goal of extrapolation\nis to predict the target label \\(y_{tgt}\\). For example, if the training distribution includes dog images, we\naim to accurately classify dogs under unseen camera angles, lighting conditions, and backgrounds.\nWhile intuitive for humans, machine learning models can be brittle to minor distribution shifts [1-4].\nAddressing distribution shifts has garnered significant attention from the community. Unsupervised\ndomain adaptation under covariate shifts addresses the shift of the marginal distribution \\(p(x)\\) across\ndomains. However, canonical techniques such as importance sampling and re-weighting [5\u20139]\nare predicated on the assumptions of overlapping supports \\(\\text{Supp}(P_{tgt}(x)) \\subset \\text{Supp}(p_{src}(x))\\) and the\navailability of the entire target marginal distribution \\(P_{tgt}(x)\\). Similarly, domain generalization [10\u201312]\nassumes access to multiple source distributions \\(p_{src}(x, y)\\) whose supports jointly cover the target\ndistribution. In addition to these methods, test-time adaptation (TTA) [13\u201316] is particularly relevant\nto our discussion of extrapolation. TTA addresses out-of-distribution test samples at the individual\nsample level. Canonical methods include updating the source model with entropy-based or self-\nsupervised losses on target samples. However, most TTA research focuses on empirical aspects,\nwith limited theoretical formalization [17]. Most related to our work, Kong et al. [18] and Li et al."}, {"title": "2 Related Work", "content": "Extrapolation. Out-of-distribution generalization has attracted significant attention in recent years.\nUnlike our work, the bulk of the work is devoted to generalizing to target distributions on the same\nsupport as the source distribution [22, 23, 8]. Recent work [24-27] investigates extrapolation in the\nform of compositional generalization by resorting to structured generating functions (e.g., additive,\nslot-wise). Another line of work [28\u201330] studies extrapolation in regression problems and does not\nconsider the latent representation. Saengkyongam et al. [31] leverage a latent variable model and\nlinear relations between the interventional variable and the latent variable to handle extrapolation.\nIn this work, we formulate extrapolation as a latent variable identification problem. Unlike the\nsemi-parametric conditions in prior work, our conditions do not constrain the form of the generating\nfunction and are more compatible with deep learning models and tasks.\nLatent-variable identification for transfer learning. In the latent-variable model literature, one\noften assumes latent variables \\(z\\) generate the observed data \\(x\\) (e.g., images, text) through a generating\nfunction. However, the nonlinearity of deep learning models requires the generating function to be\nnonlinear, which has posed major technical difficulty in recovering the original latent variable [32].\nTo overcome this setback, a line of work [33\u201336] assumes the availability of an auxiliary label \\(u\\) for\neach sample \\(x\\) and under different \\(u\\) values, each component \\(z_i\\) of \\(z\\) experiences sufficiently large\nshift in its distribution. Since this framework assumes all latent components' distributions vary over\ndistributions indexed by \\(u\\), it does not assume the existence of some shared, invariant information\nacross distributions, which is often the case for transfer learning tasks. To address this issue, recent\nwork [18, 19] introduce a partition of \\(z\\) into an invariable variable \\(c\\) and an changing variable \\(s\\) (i.e.,\n\\(z := [c, s]\\)) such that \\(c\\)'s distribution remains constant over distributions. They show both \\(c\\) and \\(s\\) can\nbe identified and one can directly utilize the invariant variable \\(c\\) for domain adaptation. However,\ntheir techniques crucially rely on the variability of the changing variable \\(s\\), mandating the availability\nof multiple sufficiently disparate distributions (including the target) and their overlapping supports.\nThese constraints make them unsuitable for the extrapolation problem. In comparison, our theoretical\nresults give identification of the invariant variable \\(c\\) (the on-support variable in the extrapolation\ncontext) with only one source distribution \\(p_{src}(x)\\) and as few as one out-off support target sample \\(x_{tgt}\\)\nthrough mild assumptions on the generating function, directly tackling the extrapolation problem."}, {"title": "3 Extrapolation and Latent-Variable Identification", "content": "Given the labeled source distribution \\(p(x_{src}, y_{src})\\), our goal is to make predictions on a target sample\n\\(x_{tgt}\\) outside the source support (\\(x_{tgt} \\notin X_{src}\\)). While more target samples would provide better\ninformation about the distribution shift, in practice, we often have only a handful of samples to work\nwith. Therefore, we focus on the challenging scenario where only one target sample \\(x_{tgt}\\) is available.\nMaking reliable predictions on out-of-support samples \\(x_{tgt}\\) is infeasible without additional structure.\nReal-world problems where humans successfully extrapolate often follow a minimal change principle:\nthey involve sparse, non-semantic intrinsic shifts despite complex raw data changes. For example, a\nperson who has only seen a cow on a pasture can recognize the same cow on a beach, even if the\nbackground pixels change significantly. Here, the cow corresponds to the part of the latent variable\nthat remains within the support of the source data, which we call the invariant variable \\(c\\) (\\(C_{tgt} \\in C_{src}\\)),\nwhile the background change corresponds to the complement that drifts off the source support, which\nwe call the changing variable \\(s\\) (\\(S_{tgt} \\neq S_{src}\\)). Clearly, extrapolation is impossible if the intrinsic shift\nis dense (i.e., all dimensions change, \\(z = s\\)) or semantic (i.e., \\(y\\) is a function of \\(s\\)). For instance, if\nthe variable \\(s\\) also alters the cow's appearance drastically, making it unrecognizable, extrapolation\nfails. We define the data-generating process to encapsulate this minimal change principle, as follows:\n\n\n\n\\(c \\sim p(c), s \\sim p(s_{c});\\)\n\\(x = g(z), y = g_y(c).\\)\n\n\n\nIn this process, the latent space \\(z \\in Z \\subset \\mathbb{R}^{d_z}\\) comprises two subspaces: the invariant variable\n\\(c \\in C \\subset \\mathbb{R}^{d_c}\\) and the changing variable \\(s \\in S \\subset \\mathbb{R}^{d_s}\\). We define \\(Z := Z_{src} \\cup \\{Z_{tgt}\\}\\) as\nthe source support augmented with the target sample \\(z_{tgt}\\) and similarly \\(X\\) and \\(S\\). The invariant\nvariable \\(c\\) encodes shared information between the source distribution \\(p(x_{src})\\) and the out-of-support\ntarget sample \\(x_{tgt}\\), while the changing variable \\(s\\) describes the shift from the source support \\(X_{src}\\).\nHence, \\(C_{tgt} \\in C_{src}\\) and \\(S_{tgt} \\notin S_{src}\\). The variables \\(z := [c, s]\\) jointly generate the observed variable\n\\(x \\in X \\subset \\mathbb{R}^{d_x}\\) through an invertible generating function \\(g : \\mathbb{R}^{d_z} \\rightarrow \\mathbb{R}^{d_x}\\). Furthermore, we assume\nthat the label \\(y\\) originates from the invariant variable \\(c\\). This assumption reflects the reality that\nfactors such as camera angles and lighting do not affect the object's class in an image.\nOur latent-variable model adheres to the minimal change principle in two key ways: (1) the target\nsample's out-of-support nature arises from only a subset of latent variables \\(s\\), and (2) these changing\nvariables \\(s\\) are non-semantic, thus not influencing the label \\(y\\).\nExtrapolation and identifiability. Under this framework, extrapolation is possible if we can identify\nthe true invariant variable \\(c\\) in both the source distribution \\(p_{src}(x)\\) and the target data \\(x_{tgt}\\). This\nallows us to learn a classifier \\(f_{cls}: c \\rightarrow y\\) on the labeled source distribution \\(p_{src}(x, y)\\). Since the\ntarget sample's invariant variable falls within the source support (\\(C_{tgt} \\in C_{src}\\)), this classifier \\(f_{cls}\\) can\nbe directly applied to the target sample \\(c_{tgt}\\). Thus, the task of extrapolation reduces to identifying the\ninvariant variable \\(c\\) in both the source distribution \\(p(x_{src})\\) and the target sample \\(x_{tgt}\\). In Section 4,\nwe explore the conditions for identifying the invariant variable \\(c\\).\nGiven the above reasoning, we define identifiability in Definition 3.1 (i.e., block-wise identifiabil-\nity [37, 24]) which suffices for extrapolation.\nDefinition 3.1 (Identifiability of the Invariant Variable \\(c\\)). For any \\(x_1\\) and \\(x_2\\), their true invariant\nvariables \\(c_1, c_2\\) are equal if and only if the estimates \\(\\hat{c}_1, \\hat{c}_2\\) are equal: \\(c_1 = c_2 \\leftarrow \\hat{c}_1 = \\hat{c}_2\\)."}, {"title": "4 Identification Guarantees for Extrapolation", "content": "In this section, we provide two sets of conditions on which one can identify the invariant variable \\(c\\)\nand discuss the intuition and implications.\nAs discussed in Section 3, we need to identify the target sample \\(x_{tgt}\\) with source samples \\(x_{src}\\) that\nshare the same invariant variable values with the target sample, i.e., \\(c_{src} = c_{tgt}\\). This enables us to\nobtain the label of \\(x_{tgt}\\) by assigning the label of such \\(x_{src}\\). The shift between the source distribution"}, {"title": "4.1 Dense-shift Conditions", "content": "We begin by investigating scenarios where there are no constraints on the number of dimensions of \\(x\\)\n(i.e., the number of pixels) influenced by the changing variable \\(s\\), i.e., potentially large \\(|I_s(z)|\\), which\nwe term as dense shifts. For images, these shifts encompass global transformations such as changes\nin camera angles and lighting conditions that could potentially affect all pixel values (Figure 1b).\nUnderstanding the problem. As dense shifts could influence all the dimensions of \\(x\\), every\ndimension could be out of the source support and there might not be dimensions of \\(x\\) that solely\ncontain the information of \\(c\\). Consequently, relying on any subset of \\(x\\) dimensions to infer the original\n\\(c\\) becomes untenable. For instance, consider a scenario where the source distribution contains frontal-\nview images of a cat, while the target sample portrays the same cat from a side view (Figure 1b). The\nmodel cannot recognize these two images as the same cat (the same \\(c\\)) by matching a specific part of\nthe side view, say the cat's nose, to samples in the source distribution because this cat's nose only\nshows up as a front view and can be vastly different in terms of the pixel region and values. The\nmodel cannot match specific features such as the cat's nose, between the side-view target and the\nsource distribution, as the pixel region and values for the nose drastically differ.\nOur approach. For the reasons above, we need to constrain such dense changes so that even\nwhen all dimensions are affected, the target sample adheres to some intrinsic structure determined\nby the underlying \\(c_{tgt}\\) and remains distinguishable from samples of \\(c \\neq c_{tgt}\\) In many real-world\ndistributions, we can interpret \\(c\\) as the embedding vector of classes or other categories, with each \\(c\\)\nvalue indexing a manifold \\(g(c, \\cdot)\\) over \\(s\\). If manifolds are smooth and sufficiently separable from each\nother, they should exhibit limited variations in the adjacent region to the training support, avoiding\nconfusion between distinct categories. For example, there exists a noticeable distinction between\ncats and lions, such that moderate illumination changes would not cause confusion until illumination\nsignificantly obscures distinguishing features. In the following, we formalize these structures by\nassuming a finite cardinality of \\(c\\) and constraining the distance of \\(S_{tgt}\\) to the support \\(S_{src}\\).\nAdditional notations. We denote with \\(J_u\\) an upper bound of the Jacobian spectrum norm: \\(||J_g(z)|| \\leq\nJ_u\\) on the support. In Appendix A2, we show \\(J_u < \\infty\\) due to Assumption 4.1-i and Assumption 4.1-\nii. We denote with \\(D(c_1, c_2)\\) the \\(l_2\\) distance between two manifolds on the support boundary:\n\\(D(c_1, c_2) := \\inf_{s_1,s_2 \\in Bd(S_{src})} ||g(c_1, s_1) - g(c_2, s_2)||\\), where we denote the boundary of source\nsupport with \\(Bd(S_{src})\\). We denote with \\(D(s, S_{src})\\) the minimal \\(l_2\\) distance between \\(s\\) and the source\nsupport \\(S_{src}\\), i.e., \\(D(s, S_{src}) := \\inf_{s_{src} \\in S_{src}} ||s - s_{src}||\\).\nAssumption 4.1 (Identification Conditions under Global Shifts).\ni [Smoothness & Invertibility]: The generating function \\(g\\) in Equation 1 is a smooth invertible\nfunction with a smooth inverse everywhere.\nii [Compactness]: The source data space \\(X_{src} \\subset \\mathbb{R}^{d}\\) is closed and bounded."}, {"title": "4.2 Sparse-shift Conditions", "content": "We now examine cases where the changing variable \\(s\\) influences only a subset of dimensions of \\(x\\),\ni.e., a limited \\(|I_s(z)|\\), which we refer to as sparse shifts. For image distributions, these shifts include\nlocal corruptions or background changes that do not alter foreground objects (Figure 1c).\nAdditional notations. We define the index set \\(I_c(z)\\) under the influence of \\(c\\) and the indices under\nthe the exclusive influence of \\(c\\) as \\(I_{c\\setminus s}(z) := I_c(z) \\setminus I_s(z)\\).\nUnderstanding the problem. In contrast to the dense-shift scenario, here we have a non-trivial\nsubset of dimensions \\([x]_{I_{c\\setminus s}(z)}\\) that are unaffected by the changing variable \\(s\\). Consequently, if these\ndimensions carry sufficient information about \\(c\\), we can exploit them to directly recover the true \\(c\\),\nregardless of the distance \\([x]_{I_s(z)}\\) deviates from the support. In contrast, in the dense-shift scenario,\nwe need to constrain the out-of-support distance of \\(s\\) and assume the discreteness of \\(c\\). Consider"}, {"title": "4.3 Implications for Practical Algorithms", "content": "Generative adaptation. Our theoretical framework, inherently a generative model, can be imple-\nmented through auto-encoding over the source distribution and the target. Akin to our estimation\nframework, MAE-TTT [20] trains a masked auto-encoding model (\\(f_{enc}\\) and \\(f_{dec}\\)) on the source\ndistribution and adapts to target samples through the auto-encoding objective. Consequently, we have"}, {"title": "5 Synthetic Data Experiments", "content": "In this section, we conduct synthetic data experiments on classification to directly validate the\ntheoretical results in Section 4. We present additional experiments on regression in Section A4.2.\nExperimental setup. We generated the synthetic data following the generative process in Equation 1,\nwith \\(d_c = 4\\) and \\(d_s = 2\\). We focus on binary classification and sample class embeddings \\(c_1\\) and\n\\(c_2\\) from \\(\\mathcal{N}(0, I_c)\\) and \\(\\mathcal{N}(2, I_c)\\) respectively. We sample \\(s_{src}\\) from a truncated Gaussian centered\nat the origin and sample \\(s_{tgt}\\) at multiple distances from the origin. For the dense-shift case, we\nconcatenate \\(c\\) and \\(s\\) and feed them to a well-conditioned 4-layer multi-layer perceptron (MLP) with\nReLU activation to obtain \\(x\\). For the sparse-shift case, we pass \\(c\\) to a 4-layer MLP to obtain a 4-d\nvector. We duplicate 2 dimensions of this vector and add \\(s\\) to it. The final \\(x\\) is the concatenation\nof the 4-d vector and the 2-d vector. We sample 10k points for the source distribution and 1 target\nsample for each run. We perform 50 runs for each configuration and compute the accuracy on the\ntarget samples. More details can be found in Appendix A4.\nResults and discussions. We compared our method with iMSDA [18] and a model trained only\non source data. The results in both dense and sparse shift settings are summarized in Table 1. Our\nmethod consistently outperforms both baseline methods (nearly random guesses) by a large margin\non all sub-settings, validating our theoretical results. The results on iMSDA suggest that directly\napplying domain-adaptation methods to the extrapolation task may result in negative effects for lack\nof the target distribution in their training."}, {"title": "6 Real-world Data Experiments", "content": "We provide real-world experiments to validate our theoretical insights for practical algorithms\n(Section 4.3) and theoretical results (Section 4.2). More results can be found in Appendix A5."}, {"title": "6.1 Generative Adaptation with Entropy Minimization", "content": "As discussed in the first implication in Section 4.3, we incorporate an entropy-minimization loss to\nMAE-TTT and compare it with the original MAE-TTT.\nExperimental setup. We conduct experiments on ImageNet-C [45] and ImageNet100-C [46] with\n15 different types of corruption. For the baseline, we utilize the publicly available code of MAE-TTT.\nIn our approach, we do not directly integrate the entropy-minimization loss into the MAE-TTT\nframework. This is because the training process of self-supervised MAE relies on masked images,\nwhereas entropy-minimization requires the classification of the entire image. To address this, we\nintroduce additional training steps with unmasked images and apply the entropy-minimization loss\nduring these steps. Specifically, the training process for each test-time iteration is split into two stages.\nWe first follow the MAE-TTT approach by inputting masked images and training the model using\nreconstruction loss. In this stage, only the encoder is updated. Then, we input full images (32 in a\nbatch) and optimize the model with the entropy minimization loss following SHOT [43]. In this stage,\nboth the encoder and classifier are optimized. The learning rates for both stages are set the same."}, {"title": "6.2 Sparsity Regularization", "content": "As suggested by the second implication in Section 4.3, we integrate sparsity constraints into the state-\nof-the-art TTA method, TeSLA/TeSLA-s [21]. Although our theoretical results rely on a generative\nmodel, we demonstrate that our implications are also applicable to discriminative models.\nExperimental setup. We conduct experiments on the CIFAR10-C, CIFAR100-C, and ImageNet-C\ndatasets [45], following the protocols outlined for TeSLA and TeSLA-s [21], with and without\ntraining data information. In the pre-train stage, we apply the ResNet50 [47] as the backbone network\nand follow prior work [14, 44] to pre-train it on the clean CIFAR10, CIFAR100, and ImageNet\ntraining sets, with joint contrastive and classification losses. In the test-time adaptation process, we\nadopt the sequential TTA protocol as outlined in TTAC [44] and TeSLA [21]. This protocol prohibits\nthe change of training objectives throughout the test phase. To encourage sparsity, we add low-rank\nadaptation (LoRA) modules [48] to the backbone network, which limits the adaptation to low intrinsic\ndimensions. Beyond LoRA, we further implement a masking layer with corresponding sparsity\nconstraint (\\(l_1\\) loss) to filter out redundant changes. More details can be found in Appendix A5.\nResults analysis. The average error rates under 15 corruption types for all CIFAR10-C, CIFAR100-\nC, and ImageNet-C datasets are summarized in Table 2. We can observe that sparsity constraints\nconsistently improve performance over the current SOTA method, TeSLA/TeSLA-s, across all\nthree datasets. The lightweight nature of the sparsity constraint and its consistent performance\nenhancements make it a valuable addition. This demonstrates the potential of sparsity constraints as a\nversatile, plug-and-play module for enhancing existing TTA methods."}, {"title": "6.3 Shift Scope and Severity", "content": "To investigate the trade-off between the shift\nscope (dense vs. sparse) and severity, we sim-\nulate different levels of corruption severity and\ncorrupted region sizes and evaluate a classical\nTTA method TENT [15] on these configurations.\nFollowing [45], we inject impulse noise to the\nCIFAR10 dataset, with noise levels ranging from\n1 to 10 to simulate various severity levels. To\ncontrol the shift's scope, we crop regions of var-\nious sizes and introduce corruption only to this\nregion.\nwe can observe that classification errors rise with increasing noise levels and region sizes.\nNotably, for large block sizes (dense shifts), the performance dramatically declines and even collapses\nas the severity level rises, whereas the performance remains almost constant over all severity levels in\nthe sparse shift regime, verifying the theoretical conditions for Theorem 4.2 and Theorem 4.4."}, {"title": "7 Conclusion and Limitations", "content": "In this work, we characterize extrapolation with a latent-variable model that encodes a minimal\nchange principle. Within this framework, we establish clear conditions under which extrapolation\nbecomes not only feasible but also guaranteed, even for complex nonlinear models in deep learning.\nOur conditions reveal the intricate interplay among the generating function's smoothness, the out-of-\nsupport degree, and the influence of the shift. These theoretical results provide valuable implications\nfor the design of practical test time adaptation methods, which we validate empirically.\nLimitations: On the theory aspect, the Jacobian norm utilized in Theorem 4.2 only considers the\nglobal smoothness of the generating function and thus may be too stringent if the function is much\nmore well-behaved/smooth over the extrapolation region of concern. Therefore, one may consider\na refined local condition to relax this condition. On the empirical side, our theoretical framework\nentails learning an explicit representation space. Existing methods without such a structure may still\nbenefit from our framework but to a lesser extent. Also, our framework involves several loss terms\nincluding reconstruction, classification, and the likelihood of the target invariant variable. A careful\nre-weighting of these terms may be needed during training."}, {"title": "A1 Related Work", "content": "In this section, we discuss some related topics including extrapolation, latent-variable identification,\nand test-time adaptation.\nExtrapolation. Out-of-distribution generalization has attracted significant attention in recent years.\nUnlike our work, the bulk of the work is devoted to generalizing to target distributions on the same\nsupport as the source distribution [22, 23, 8]. Recent work [24\u201327] investigates extrapolation in the\nform of compositional generalization by resorting to structured generating functions (e.g., additive,\nslot-wise). Another line of work [28-30] studies extrapolation in regression problems and does\nnot consider the latent representation. Saengkyongam et al. [31] leverage a latent variable model\nand assumes a linear relation between the intervention variable and the latent variable to handle\nextrapolation. In this work, we formulate extrapolation as a latent variable identification problem.\nUnlike the semi-parametric conditions in prior work, our conditions do not constrain the form of the\ngenerating function and are more compatible with deep learning models and tasks. We demonstrate\nthat our conditions naturally lead to implications benefiting practical deep-learning algorithms.\nLatent-variable identification for transfer learning. Identifying latent variables in a causal model\nhas become one canonical paradigm to formalize and understand representation learning in the deep\nlearning regime. Typically, one would assume some latent variables \\(z\\) generate the observed data\n\\(x\\) (e.g., images, text) through a generating function. However, the nonlinearity of deep learning\nmodels requires the generating function to be nonlinear, which has posed major technical difficulty in\nrecovering the original latent variable [32]. To overcome this setback, a line of work [33\u201336] assumes\nthe availability of an auxiliary label \\(u\\) for each sample \\(x\\) and under different \\(u\\) values, each component\n\\(z_i\\) of \\(z\\) experiences sufficiently large shift in its distribution. This condition leads to component-\nwise identification of \\(z\\), i.e., each estimate \\(\\hat{z}_i\\) is equivalent to \\(z_{\\pi(i)}\\) up to an invertible mapping\nfor a permutation function \\(\\pi : [d_z] \\rightarrow [d_z]\\). Since this framework assumes all latent components'\ndistributions vary over distributions indexed by \\(u\\), it doesn't assume the existence of some shared,\ninvariant information across distributions, which is often the case for transfer learning tasks. To\naddress this issue, recent work [18, 19] introduce a partition of \\(z\\) into an invariable variable \\(c\\) and an\nchanging variable \\(s\\) (i.e., \\(z := [c, s]\\)) such that \\(c\\)'s distribution remains constant over distributions.\nThey show both \\(c\\) and \\(s\\) can be identified and one can directly utilize the invariant variable \\(c\\) for\ndomain adaptation. However, their techniques crucially rely on the variability of the changing variable\n\\(s\\), mandating the availability of multiple sufficiently disparate distributions (including the target) and\ntheir overlapping supports. These constraints make them unsuitable for the extrapolation problem.\nIn comparison, our theoretical results give identification of the invariant variable \\(c\\) (the on-support\nvariable in the extrapolation context) with only one source distribution \\(p_{src}(x)\\) and as few as one\nout-off support target sample \\(x_{tgt}\\) through mild assumptions on the generating function, which\ndirectly tackles the extrapolation problem.\nTest-time adaptation. Test-time Adaptation (TTA) aims at adapting models trained on a source\ndomain to align with the target domain during testing [49\u201355]. It is broadly classified based on\nwhether the training objective is modified. Test-time Training (TTT) methods [13, 14, 44, 56, 57],\nincluding TTT [13] and TTT++ [14], proficiently adjust models to target domains by implementing\nsimilar self-supervised learning strategies on both training and testing data. In contrast, Sequential\nTest-Time Adaptation [15, 54, 55, 58\u201363] (sTTA) garners significant interest due to its practicality,\nnotably its one-pass sequential inference and no training objective access. Research in sTTA primarily\nconcentrates on two facets: the selection of model parameters for adaptation and the refinement of\npseudo-labeling techniques for enhanced efficiency. For instance, TENT [15] fine-tunes the Batch\nNormalization (BN) layers by minimizing entropy, SHOT [16] adjusts the backbone network while\nmaintaining a static classifier, and T3A [64] updates the classifier prototype. Moreover, a burgeoning\nline of research [65, 21, 44, 50, 15, 16] focuses on deriving more robust self-training signals through\nimproved pseudo labeling strategies. For example, TTAC [44] employs clustering techniques to\nextract more accurate pseudo labels. Despite the prominent recent development, these algorithms tend\nto be brittle and sensitive to hyper-parameter tuning [66] and limited in theoretical understanding [17].\nOur work offers formalization and understanding to fill in this gap. We show that insights inferred\nfrom our theory can indeed benefit existing TTA algorithms, which hopefully will serve as the first\nstep to bridge the theory and practice for TTA algorithms."}, {"title": "A2 Proof for Theorem 4.2", "content": "Assumption 4.1 (Identification Conditions under Global Shifts).\ni [Smoothness & Invertibility", "Compactness": "The source data space \\(X_{src"}, "subset \\mathbb{R}^{d}\\) is closed and bounded.\niii [Discreteness"], "set": "C = \\{c_k\\"}, {"Continuity]": "The probability density function \\(p(s|c)\\) is continuous over \\(s \\in S_{src"}, ["Out-of-support Distance"], {"constrained": "inf_{s \\in S_{src"}, {"objective": "n\nsupp(\\(\\hat{p"}, {"to": "hat{p}(x) = p(x), \\forall x \\in X_{src}; \\hat{s}_{tgt} \\in arg \\inf_{\\hat{s}} D(\\hat{s}, S_{src})\\). (2)\n\nUnder Assumption 4.1, the estimated model can attain the identifiability in Definition 3.1.\nProof for Theorem 4.2. Lemma A1 shows that the discrete invariant variable \\(c\\) is identifiable on the\nsource distribution.\nIn the following, we show that the target's invariant variable \\(c_{tgt}\\) is identifiable if \\(s_{tgt}\\) does not drift\ntoo far away from the source support \\(S_{src}\\). Suppose that \\(x_{tgt}\\) resides on both manifolds \\(g(c_k, \\cdot)\\)\nand \\(g'(c_{k'}, \\cdot)\\) where \\(k \\neq k'\\). The generating function \\(g' \\in G\\) belongs to the generating function\nclass and behaves exactly the same as \\(g\\) on the source support"}]