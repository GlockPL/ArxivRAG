{"title": "Towards Understanding Extrapolation: a Causal Lens", "authors": ["Lingjing Kong", "Guangyi Chen", "Petar Stojanov", "Haoxuan Li", "Eric P. Xing", "Kun Zhang"], "abstract": "Canonical work handling distribution shifts typically necessitates an entire target distribution that lands inside the training distribution. However, practical scenarios often involve only a handful of target samples, potentially lying outside the training support, which requires the capability of extrapolation. In this work, we aim to provide a theoretical understanding of when extrapolation is possible and offer principled methods to achieve it without requiring an on-support target distribution. To this end, we formulate the extrapolation problem with a latent-variable model that embodies the minimal change principle in causal mechanisms. Under this formulation, we cast the extrapolation problem into a latent-variable identification problem. We provide realistic conditions on shift properties and the estimation objectives that lead to identification even when only one off-support target sample is available, tackling the most challenging scenarios. Our theory reveals the intricate interplay between the underlying manifold's smoothness and the shift properties. We showcase how our theoretical results inform the design of practical adaptation algorithms. Through experiments on both synthetic and real-world data, we validate our theoretical findings and their practical implications.", "sections": [{"title": "1 Introduction", "content": "Extrapolation necessitates the capability of generalizing beyond the training distribution sup- port, which is essential for the robust deployment of machine learning models in real-world scenarios. Specifically, given access to a source distribution Dsrc := p(xsrc, ysrc) with support Xsrc:= Supp(Psrc(x)) and one or a few out-of-support samples xtgt \u2209 Xsrc, the goal of extrapolation is to predict the target label ytgt. For example, if the training distribution includes dog images, we aim to accurately classify dogs under unseen camera angles, lighting conditions, and backgrounds. While intuitive for humans, machine learning models can be brittle to minor distribution shifts [1-4].\nAddressing distribution shifts has garnered significant attention from the community. Unsupervised domain adaptation under covariate shifts addresses the shift of the marginal distribution p(x) across domains. However, canonical techniques such as importance sampling and re-weighting [5\u20139] are predicated on the assumptions of overlapping supports Supp(Ptgt(x)) C Supp(psrc(x)) and the availability of the entire target marginal distribution Ptgt(x). Similarly, domain generalization [10\u201312] assumes access to multiple source distributions psrc (x, y) whose supports jointly cover the target distribution. In addition to these methods, test-time adaptation (TTA) [13\u201316] is particularly relevant to our discussion of extrapolation. TTA addresses out-of-distribution test samples at the individual sample level. Canonical methods include updating the source model with entropy-based or self- supervised losses on target samples. However, most TTA research focuses on empirical aspects, with limited theoretical formalization [17]. Most related to our work, Kong et al."}, {"title": "2 Related Work", "content": "Extrapolation. Out-of-distribution generalization has attracted significant attention in recent years. Unlike our work, the bulk of the work is devoted to generalizing to target distributions on the same support as the source distribution [22, 23, 8]. Recent work [24-27] investigates extrapolation in the form of compositional generalization by resorting to structured generating functions (e.g., additive, slot-wise). Another line of work [28\u201330] studies extrapolation in regression problems and does not consider the latent representation. Saengkyongam et al. [31] leverage a latent variable model and linear relations between the interventional variable and the latent variable to handle extrapolation. In this work, we formulate extrapolation as a latent variable identification problem. Unlike the semi-parametric conditions in prior work, our conditions do not constrain the form of the generating function and are more compatible with deep learning models and tasks.\nLatent-variable identification for transfer learning. In the latent-variable model literature, one often assumes latent variables z generate the observed data x (e.g., images, text) through a generating function. However, the nonlinearity of deep learning models requires the generating function to be nonlinear, which has posed major technical difficulty in recovering the original latent variable [32]. To overcome this setback, a line of work [33\u201336] assumes the availability of an auxiliary label u for each sample x and under different u values, each component zi of z experiences sufficiently large shift in its distribution. Since this framework assumes all latent components' distributions vary over distributions indexed by u, it does not assume the existence of some shared, invariant information across distributions, which is often the case for transfer learning tasks. To address this issue, recent work [18, 19] introduce a partition of z into an invariable variable c and an changing variable s (i.e., z := [c, s]) such that c's distribution remains constant over distributions. They show both c and s can be identified and one can directly utilize the invariant variable c for domain adaptation. However, their techniques crucially rely on the variability of the changing variable s, mandating the availability of multiple sufficiently disparate distributions (including the target) and their overlapping supports. These constraints make them unsuitable for the extrapolation problem. In comparison, our theoretical results give identification of the invariant variable c (the on-support variable in the extrapolation context) with only one source distribution psrc(x) and as few as one out-off support target sample xtgt through mild assumptions on the generating function, directly tackling the extrapolation problem."}, {"title": "3 Extrapolation and Latent-Variable Identification", "content": "Given the labeled source distribution p(xsrc, ysrc), our goal is to make predictions on a target sample xtgt outside the source support (xtgt & Xsrc). While more target samples would provide better information about the distribution shift, in practice, we often have only a handful of samples to work with. Therefore, we focus on the challenging scenario where only one target sample xtgt is available.\nMaking reliable predictions on out-of-support samples xtgt is infeasible without additional structure. Real-world problems where humans successfully extrapolate often follow a minimal change principle: they involve sparse, non-semantic intrinsic shifts despite complex raw data changes. For example, a person who has only seen a cow on a pasture can recognize the same cow on a beach, even if the background pixels change significantly. Here, the cow corresponds to the part of the latent variable that remains within the support of the source data, which we call the invariant variable c (Ctgt \u2208 Csrc), while the background change corresponds to the complement that drifts off the source support, which we call the changing variable s (Stgt \u2260 Ssrc). Clearly, extrapolation is impossible if the intrinsic shift is dense (i.e., all dimensions change, z = s) or semantic (i.e., y is a function of s). For instance, if the variable s also alters the cow's appearance drastically, making it unrecognizable, extrapolation fails. We define the data-generating process to encapsulate this minimal change principle, as follows:\nc ~ p(c), s ~ p(sc);\nx = g(z), y = gy(c). (1)\nIn this process, the latent space z \u2208 Z C Rdz comprises two subspaces: the invariant variable c\u2208 CC Rde and the changing variable s \u2208 S C Rds. We define Z := Zsrc \u222a {Ztgt} as the source support augmented with the target sample ztgt and similarly X and S. The invariant variable c encodes shared information between the source distribution p(xsrc) and the out-of-support target sample xtgt, while the changing variable s describes the shift from the source support Xsrc. Hence, Ctgt \u2208 Csrc and Stgt & Ssrc. The variables z := [c, s] jointly generate the observed variable x \u2208 X C Rdx through an invertible generating function g : Rdz \u2192 Rdx. Furthermore, we assume that the label y originates from the invariant variable c. This assumption reflects the reality that factors such as camera angles and lighting do not affect the object's class in an image.\nOur latent-variable model adheres to the minimal change principle in two key ways: (1) the target sample's out-of-support nature arises from only a subset of latent variables s, and (2) these changing variables s are non-semantic, thus not influencing the label y.\nExtrapolation and identifiability. Under this framework, extrapolation is possible if we can identify the true invariant variable c in both the source distribution psrc(x) and the target data xtgt. This allows us to learn a classifier fcls: cy on the labeled source distribution psrc (x, y). Since the target sample's invariant variable falls within the source support (Ctgt \u2208 Csrc), this classifier fcls can be directly applied to the target sample ctgt. Thus, the task of extrapolation reduces to identifying the invariant variable c in both the source distribution p(xsrc) and the target sample xtgt. In Section 4, we explore the conditions for identifying the invariant variable c.\nGiven the above reasoning, we define identifiability in Definition 3.1 (i.e., block-wise identifiabil- ity [37, 24]) which suffices for extrapolation.\nDefinition 3.1 (Identifiability of the Invariant Variable c). For any x\u2081 and x2, their true invariant variables c1, c2 are equal if and only if the estimates \u01091, \u01082 are equal: c\u2081 = C2 \u2190 C1 = C2."}, {"title": "4 Identification Guarantees for Extrapolation", "content": "In this section, we provide two sets of conditions on which one can identify the invariant variable c and discuss the intuition and implications.\nAs discussed in Section 3, we need to identify the target sample xtgt with source samples xsrc that share the same invariant variable values with the target sample, i.e., csrc = ctgt. This enables us to obtain the label of xtgt by assigning the label of such xsrc. The shift between the source distribution"}, {"title": "4.1 Dense-shift Conditions", "content": "We begin by investigating scenarios where there are no constraints on the number of dimensions of x (i.e., the number of pixels) influenced by the changing variable s, i.e., potentially large |Is(z)|, which we term as dense shifts. For images, these shifts encompass global transformations such as changes in camera angles and lighting conditions that could potentially affect all pixel values (Figure 1b).\nUnderstanding the problem. As dense shifts could influence all the dimensions of x, every dimension could be out of the source support and there might not be dimensions of x that solely contain the information of c. Consequently, relying on any subset of x dimensions to infer the original c becomes untenable. For instance, consider a scenario where the source distribution contains frontal- view images of a cat, while the target sample portrays the same cat from a side view (Figure 1b). The model cannot recognize these two images as the same cat (the same c) by matching a specific part of the side view, say the cat's nose, to samples in the source distribution because this cat's nose only shows up as a front view and can be vastly different in terms of the pixel region and values. The model cannot match specific features such as the cat's nose, between the side-view target and the source distribution, as the pixel region and values for the nose drastically differ.\nOur approach. For the reasons above, we need to constrain such dense changes so that even when all dimensions are affected, the target sample adheres to some intrinsic structure determined by the underlying ctgt and remains distinguishable from samples of c \u2260 ctgt In many real-world distributions, we can interpret c as the embedding vector of classes or other categories, with each c value indexing a manifold g(c, .) over s. If manifolds are smooth and sufficiently separable from each other, they should exhibit limited variations in the adjacent region to the training support, avoiding confusion between distinct categories. For example, there exists a noticeable distinction between cats and lions, such that moderate illumination changes would not cause confusion until illumination significantly obscures distinguishing features. In the following, we formalize these structures by assuming a finite cardinality of c and constraining the distance of Stgt to the support Ssrc.\nAdditional notations. We denote with Ju an upper bound of the Jacobian spectrum norm: ||Jg(z)|| \u2264 Ju on the support. In Appendix A2, we show Ju < \u221e due to Assumption 4.1-i and Assumption 4.1- ii. We denote with D(c1, c2) the l2 distance between two manifolds on the support boundary: D(C1, C2) := infs1,s2\u2208Bd(Ssrc) ||g(C1, S1) - g(C2, S2)||, where we denote the boundary of source support with Bd(Ssrc). We denote with D(s, Ssrc) the minimal l2 distance between s and the source support Ssrc, i.e., D(s, Ssrc) := infssrc\u2208Ssrc ||S - Ssrc||.\nAssumption 4.1 (Identification Conditions under Global Shifts).\ni [Smoothness & Invertibility]: The generating function g in Equation 1 is a smooth invertible function with a smooth inverse everywhere.\nii [Compactness]: The source data space Xsrc C Rd is closed and bounded.\niii [Discreteness]: The invariant variable c takes on values from a finite set: C = {ck}k\u2208[K]\u00b7\niv [Continuity]: The probability density function p(s|c) is continuous over s \u2208 Ssrc, for all c \u2208 C.\nv [Out-of-support Distance]: The target sample's out-support components Stgt's distance from the source support Ssrc is constrained: infs\u2208Ssre ||Stgt - S|| \u2264 mincec\\{ctgt} D(Ctgt,c)\n2Ju"}, {"title": "4.2 Sparse-shift Conditions", "content": "We now examine cases where the changing variable s influences only a subset of dimensions of x, i.e., a limited |Is(z)|, which we refer to as sparse shifts. For image distributions, these shifts include local corruptions or background changes that do not alter foreground objects (Figure 1c).\nAdditional notations. We define the index set Ic(z) under the influence of c and the indices under the the exclusive influence of c as Ic\\s(z) := Ic(z) \\ Is(z).\nUnderstanding the problem. In contrast to the dense-shift scenario, here we have a non-trivial subset of dimensions [x]Ze\\s(z) that are unaffected by the changing variable s. Consequently, if these dimensions carry sufficient information about c, we can exploit them to directly recover the true c, regardless of the distance [x]1\u300f(z) deviates from the support. In contrast, in the dense-shift scenario, we need to constrain the out-of-support distance of s and assume the discreteness of c. Consider"}, {"title": "4.3 Implications for Practical Algorithms", "content": "Generative adaptation. Our theoretical framework, inherently a generative model, can be imple- mented through auto-encoding over the source distribution and the target. Akin to our estimation framework, MAE-TTT [20] trains a masked auto-encoding model (fenc and fdec) on the source distribution and adapts to target samples through the auto-encoding objective. Consequently, we have"}, {"title": "5 Synthetic Data Experiments", "content": "In this section, we conduct synthetic data experiments on classification to directly validate the theoretical results in Section 4. We present additional experiments on regression in Section A4.2.\nExperimental setup. We generated the synthetic data following the generative process in Equation 1, with dc = 4 and ds = 2. We focus on binary classification and sample class embeddings c\u2081 and c2 from N(0, Ic) and N (2, Ic) respectively. We sample ssrc from a truncated Gaussian centered at the origin and sample stgt at multiple distances from the origin. For the dense-shift case, we concatenate c and s and feed them to a well-conditioned 4-layer multi-layer perceptron (MLP) with ReLU activation to obtain x. For the sparse-shift case, we pass c to a 4-layer MLP to obtain a 4-d vector. We duplicate 2 dimensions of this vector and add s to it. The final x is the concatenation of the 4-d vector and the 2-d vector. We sample 10k points for the source distribution and 1 target sample for each run. We perform 50 runs for each configuration and compute the accuracy on the target samples. More details can be found in Appendix A4.\nResults and discussions. We compared our method with iMSDA [18] and a model trained only on source data. The results in both dense and sparse shift settings are summarized in Table 1. Our method consistently outperforms both baseline methods (nearly random guesses) by a large margin on all sub-settings, validating our theoretical results. The results on iMSDA suggest that directly applying domain-adaptation methods to the extrapolation task may result in negative effects for lack of the target distribution in their training."}, {"title": "6 Real-world Data Experiments", "content": "We provide real-world experiments to validate our theoretical insights for practical algorithms (Section 4.3) and theoretical results (Section 4.2)."}, {"title": "6.1 Generative Adaptation with Entropy Minimization", "content": "As discussed in the first implication in Section 4.3, we incorporate an entropy-minimization loss to MAE-TTT and compare it with the original MAE-TTT.\nExperimental setup. We conduct experiments on ImageNet-C [45] and ImageNet100-C [46] with 15 different types of corruption. For the baseline, we utilize the publicly available code of MAE-TTT. In our approach, we do not directly integrate the entropy-minimization loss into the MAE-TTT framework. This is because the training process of self-supervised MAE relies on masked images, whereas entropy-minimization requires the classification of the entire image. To address this, we introduce additional training steps with unmasked images and apply the entropy-minimization loss during these steps. Specifically, the training process for each test-time iteration is split into two stages. We first follow the MAE-TTT approach by inputting masked images and training the model using reconstruction loss. In this stage, only the encoder is updated. Then, we input full images (32 in a batch) and optimize the model with the entropy minimization loss following SHOT [43]. In this stage, both the encoder and classifier are optimized. The learning rates for both stages are set the same."}, {"title": "6.2 Sparsity Regularization", "content": "As suggested by the second implication in Section 4.3, we integrate sparsity constraints into the state- of-the-art TTA method, TeSLA/TeSLA-s [21]. Although our theoretical results rely on a generative model, we demonstrate that our implications are also applicable to discriminative models.\nExperimental setup. We conduct experiments on the CIFAR10-C, CIFAR100-C, and ImageNet-C datasets [45], following the protocols outlined for TeSLA and TeSLA-s [21], with and without training data information. In the pre-train stage, we apply the ResNet50 [47] as the backbone network and follow prior work [14, 44] to pre-train it on the clean CIFAR10, CIFAR100, and ImageNet training sets, with joint contrastive and classification losses. In the test-time adaptation process, we adopt the sequential TTA protocol as outlined in TTAC [44] and TeSLA [21]. This protocol prohibits the change of training objectives throughout the test phase. To encourage sparsity, we add low-rank adaptation (LoRA) modules [48] to the backbone network, which limits the adaptation to low intrinsic dimensions. Beyond LoRA, we further implement a masking layer with corresponding sparsity constraint (l1 loss) to filter out redundant changes. More details can be found in Appendix A5.\nResults analysis. The average error rates under 15 corruption types for all CIFAR10-C, CIFAR100- C, and ImageNet-C datasets are summarized in Table 2. We can observe that sparsity constraints consistently improve performance over the current SOTA method, TeSLA/TeSLA-s, across all three datasets. The lightweight nature of the sparsity constraint and its consistent performance enhancements make it a valuable addition. This demonstrates the potential of sparsity constraints as a versatile, plug-and-play module for enhancing existing TTA methods."}, {"title": "6.3 Shift Scope and Severity", "content": "To investigate the trade-off between the shift scope (dense vs. sparse) and severity, we sim- ulate different levels of corruption severity and corrupted region sizes and evaluate a classical TTA method TENT [15] on these configurations. Following [45], we inject impulse noise to the CIFAR10 dataset, with noise levels ranging from 1 to 10 to simulate various severity levels. To control the shift's scope, we crop regions of var- ious sizes and introduce corruption only to this region. Figure 3 displays classification error curves under various shift severity levels and re- gion sizes. We can observe that classification errors rise with increasing noise levels and region sizes. Notably, for large block sizes (dense shifts), the performance dramatically declines and even collapses as the severity level rises, whereas the performance remains almost constant over all severity levels in the sparse shift regime, verifying the theoretical conditions for Theorem 4.2 and Theorem 4.4."}, {"title": "7 Conclusion and Limitations", "content": "In this work, we characterize extrapolation with a latent-variable model that encodes a minimal change principle. Within this framework, we establish clear conditions under which extrapolation becomes not only feasible but also guaranteed, even for complex nonlinear models in deep learning. Our conditions reveal the intricate interplay among the generating function's smoothness, the out-of- support degree, and the influence of the shift. These theoretical results provide valuable implications for the design of practical test time adaptation methods, which we validate empirically.\nLimitations: On the theory aspect, the Jacobian norm utilized in Theorem 4.2 only considers the global smoothness of the generating function and thus may be too stringent if the function is much more well-behaved/smooth over the extrapolation region of concern. Therefore, one may consider a refined local condition to relax this condition. On the empirical side, our theoretical framework entails learning an explicit representation space. Existing methods without such a structure may still benefit from our framework but to a lesser extent. Also, our framework involves several loss terms including reconstruction, classification, and the likelihood of the target invariant variable. A careful re-weighting of these terms may be needed during training."}, {"title": "A1 Related Work", "content": "In this section, we discuss some related topics including extrapolation, latent-variable identification, and test-time adaptation.\nExtrapolation. Out-of-distribution generalization has attracted significant attention in recent years. Unlike our work, the bulk of the work is devoted to generalizing to target distributions on the same support as the source distribution [22, 23, 8]. Recent work [24\u201327] investigates extrapolation in the form of compositional generalization by resorting to structured generating functions (e.g., additive, slot-wise). Another line of work [28-30] studies extrapolation in regression problems and does not consider the latent representation. Saengkyongam et al. [31] leverage a latent variable model and assumes a linear relation between the intervention variable and the latent variable to handle extrapolation. In this work, we formulate extrapolation as a latent variable identification problem. Unlike the semi-parametric conditions in prior work, our conditions do not constrain the form of the generating function and are more compatible with deep learning models and tasks. We demonstrate that our conditions naturally lead to implications benefiting practical deep-learning algorithms.\nLatent-variable identification for transfer learning. Identifying latent variables in a causal model has become one canonical paradigm to formalize and understand representation learning in the deep learning regime. Typically, one would assume some latent variables z generate the observed data x (e.g., images, text) through a generating function. However, the nonlinearity of deep learning models requires the generating function to be nonlinear, which has posed major technical difficulty in recovering the original latent variable [32]. To overcome this setback, a line of work [33\u201336] assumes the availability of an auxiliary label u for each sample x and under different u values, each component zi of z experiences sufficiently large shift in its distribution. This condition leads to component- wise identification of z, i.e., each estimate 2\u2081 is equivalent to \u0396\u03c0(i) up to an invertible mapping for a permutation function \u03c0 : [dz] \u2192 [dz]. Since this framework assumes all latent components' distributions vary over distributions indexed by u, it doesn't assume the existence of some shared, invariant information across distributions, which is often the case for transfer learning tasks. To address this issue, recent work [18, 19] introduce a partition of z into an invariable variable c and an changing variable s (i.e., z := [c, s]) such that c's distribution remains constant over distributions. They show both c and s can be identified and one can directly utilize the invariant variable c for domain adaptation. However, their techniques crucially rely on the variability of the changing variable s, mandating the availability of multiple sufficiently disparate distributions (including the target) and their overlapping supports. These constraints make them unsuitable for the extrapolation problem. In comparison, our theoretical results give identification of the invariant variable c (the on-support variable in the extrapolation context) with only one source distribution psrc(x) and as few as one out-off support target sample xtgt through mild assumptions on the generating function, which directly tackles the extrapolation problem.\nTest-time adaptation. Test-time Adaptation (TTA) aims at adapting models trained on a source domain to align with the target domain during testing [49\u201355]. It is broadly classified based on whether the training objective is modified. Test-time Training (TTT) methods [13, 14, 44, 56, 57], including TTT [13] and TTT++ [14], proficiently adjust models to target domains by implementing similar self-supervised learning strategies on both training and testing data. In contrast, Sequential Test-Time Adaptation [15, 54, 55, 58\u201363] (sTTA) garners significant interest due to its practicality, notably its one-pass sequential inference and no training objective access. Research in sTTA primarily concentrates on two facets: the selection of model parameters for adaptation and the refinement of pseudo-labeling techniques for enhanced efficiency. For instance, TENT [15] fine-tunes the Batch Normalization (BN) layers by minimizing entropy, SHOT [16] adjusts the backbone network while maintaining a static classifier, and T3A [64] updates the classifier prototype. Moreover, a burgeoning line of research [65, 21, 44, 50, 15, 16] focuses on deriving more robust self-training signals through improved pseudo labeling strategies. For example, TTAC [44] employs clustering techniques to extract more accurate pseudo labels. Despite the prominent recent development, these algorithms tend to be brittle and sensitive to hyper-parameter tuning [66] and limited in theoretical understanding [17]. Our work offers formalization and understanding to fill in this gap. We show that insights inferred from our theory can indeed benefit existing TTA algorithms, which hopefully will serve as the first step to bridge the theory and practice for TTA algorithms."}, {"title": "A2 Proof for Theorem 4.2", "content": "Assumption 4.1 (Identification Conditions under Global Shifts).\ni [Smoothness & Invertibility]: The generating function g in Equation 1 is a smooth invertible function with a smooth inverse everywhere.\nii [Compactness]: The source data space Xsrc C Rd is closed and bounded.\niii [Discreteness]: The invariant variable c takes on values from a finite set: C = {ck}k\u2208[K]\u00b7\niv [Continuity]: The probability density function p(s|c) is continuous over s \u2208 Ssrc, for all c \u2208 C.\nv [Out-of-support Distance]: The target sample's out-support components Stgt's distance from the source support Ssrc is constrained: infs\u2208Ssre ||Stgt - S|| \u2264 mincec\\{ctgt} D(ctgt,c)\n2Ju\nWe first present Lemma A1 from Kong et al. [38] which establishes the discrete information on the source support and serves as the starting point in the proof of Theorem 4.2.\nLemma A1 (Source discrete subspace identification [38]). Assuming a generating process in Equa- tion 1, we estimate the distribution with model (\u011d, \u00f4(\u0109), \u00ee(\u015d)). Under Assumption 4.1 i,ii,iii,iv, it follows that the estimated variable \u0109 takes on values from {\u0109k}k_1 where each value corresponds uniquely to one value of the true variable c, i.e., c = ck \u21d4 \u0109 = \u0109k.\nTheorem 4.2 (Extrapolation under Dense Shifts). Assuming a generating process in Equation 1, we estimate the distribution with model (\u011d, p(\u0109), \u00ee(\u015d)) with the objective:\nsupp(\u0109tgt), Subject to: p(x) = p(x), \u2200x \u2208 Xsrc; $tgt \u2208 arg inf D(s, Ssrc). (2)\nUnder Assumption 4.1, the estimated model can attain the identifiability in Definition 3.1.\nProof for Theorem 4.2. Lemma A1 shows that the discrete invariant variable c is identifiable on the source distribution.\nIn the following, we show that the target's invariant variable ctgt is identifiable if stgt does not drift too far away from the source support Ssrc. Suppose that xtgt resides on both manifolds g(ck,.) and g'(ck',) where k \u2260 k'. The generating function g' \u2208 G belongs to the generating function class and behaves exactly the same as g on the source support, i.e., g' = g over C \u00d7 Ssrc. We define the minimal distance D(ck, ck') between the two manifolds on support boundaries, i.e., D(ck, ck') := mins1,s2\u2208Bd(Ssrc) ||9(ck, S\u2081) - g(ck', S2)|| > 0. Since xtgt lives on both manifolds g(ck,\u00b7) and g'(ck',\u00b7), we can express it as xtgt = g(ck, Stgt) = g'(ck', Stgt). We define ssrc \u2208 arg mins\u2208Ssrc ||s - Stgt|| and ssrc \u2208 arg mins\u2208ssre ||s - Stgt || as two closest points on the source support to Stgt and sgt respectively. It follows that\nxtgt - g(ck, Ssrc) =(S10 Jg(ck,.) (Ssrc+t. h)dth;) h,xtgt-g\u2032(ck\u2032,Ssrc)=(S10 Jg(ck\u2032,.) (Ssrc+th\u2032)dth) h\u2032. (4)"}, {"title": "A3 Proof for Theorem 4.4", "content": "Assumption 4.3 (Identification Conditions under Local Shifts).\ni [Smoothness & Invertibility]: The generating function g in Equation 1 is invertible and differen- tiable, and its inverse is also differentiable.\nii [Invariant Variable Informativeness]: The dimensions under c's exclusive influence is uniquely determined: for a fixed c \u2208 C, [X]Te\\s(c,S1) \u2260 [X]1c\\s(c*,s2) for any c* \u2260 c, S1 \u2208 S, and $2 \u2208 S.\niii [Sparse Influence]: At any z \u2208 Z, the changing variable s influences at most ds dimensions of x, i.e., Is(z)| \u2264 ds. Alternatively, the two variables c and s do not intersect on their influenced dimensions Ic(z) \u2229 Is (z) = (0.\niv [Mechanistic Dependence]: For all z, any nontrivial partition P1, P2 of the dimensions Ic\\s(z) yields dependence between the sub-matrices of the Jacobian J\u300f(z): rank([Jg(Z)]Ic\\s (Z)) < rank([Jg(z)]P\u2081(z)) + rank([Jg(z)]P2 (Z)).\nTheorem 4.4 (Extrapolation under Sparse Shifts). Assuming a generating process in Equation 1, we estimate the distribution with model (\u011d, \u00ee(\u0109), \u00ee(\u015d)) with the objective:\nsupp(\u0109tgt), Subject to: p(x) = p(x), \u2200x \u2208 Xsrc. (3)"}, {"title": "A4 Synthetic Data Experiments", "content": "We employ a variational auto-encoder [67] whose encoder and decoder are both 4-layer MLP with 32 dimensions and leaky ReLu (a = 0.2). Following Equation 2 and Equation 3, we implement reconstruction loss, KL loss on the source distribution, the likelihood loss on the target sample, and a classification loss on the source data. For the dense case, we implement an additional distance loss to minimize the l2 distance of \u015dtgt to the center of the source support (which is the origin in our case). The source-only baseline is trained only with classification loss. The iMSDA implementation is adopted directly from the source code of Kong et al. [18]. We train all methods with Adam [68] and learning rate 2e - 3 for 25 epochs. We fix the loss weights Xcls = 1, drecons = 0.1, Atgt_likelihood = 0.1, and As_distance 0.01 (for dense shifts) overall distance configurations. We only tune AKL from the interval {1e \u2013 1, 1e \u2013 2, 1e \u2013 3}. We run synthetic data experiments on one Nvidia L40 GPU and each run consumes less than 2 minutes."}, {"title": "A4.2 Regression Task Evaluation", "content": "In addition to the classification experiments, we evaluate our model on regression in this section."}, {"title": "A4.2.1 Implementation", "content": "Data generation. The regression target y is generated from a uniform distribution U(0,4). We sample 4 latent invariant variables c from a normal distribution N(y, Ic). Two changing variables in the source domain ssrc are sampled from a truncated Gaussian centered at the origin. In the target domain, changing variables stgt are sampled at multiple distances (e.g., {18, 24, 36}) from the origin. For dense shifts, observations x are generated by concatenating c and s and feeding them to a 4-layer MLP with ReLU activation. For sparse shifts, only two out of six dimensions of x are influenced by the changing variable s. We generate 10k samples for training and 50 target samples for testing (one target sample accessed per run).\nModel. We make two modifications on the classification model in Section 5. First, we substitute the classification head with a regression head (the last linear layer). Second, we replace the cross-entropy loss with MSE loss. We fix the loss weights of MSE loss and KL loss at 0.1 and 0.01 for all settings, respectively, and keep all other hyper-parameters the same as in the classification task. We use MSE as the evaluation metric."}, {"title": "A5 Real-world Data Experimental Details", "content": "The datasets used in our experiments include CIFAR10-C, CIFAR100-C, ImageNet-C [45", "69": "designed to evaluate model robustness against visual corruptions, featuring 10 and 100 classes respectively, each with 50,000 clean training samples and 10,000 corrupted test samples. ImageNet-C, on the other hand, scales this concept up with 1,000 classes, providing 50,000 test samples of each of 15 corruption types. ImageNet-100 [46", "46": "from ImageNet-C [45"}]}