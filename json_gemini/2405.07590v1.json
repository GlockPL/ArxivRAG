{"title": "Evaluating the Explainable AI Method Grad-CAM for Breath Classification on Newborn Time Series Data", "authors": ["Camelia Oprea", "Mike Gr\u00fcne", "Mateusz Buglowski", "Lena Olivier", "Thorsten Orlikowsky", "Stefan Kowalewski", "Mark Schoberer", "Andr\u00e9 Stollenwerk"], "abstract": "With the digitalization of health care systems, artificial intelligence becomes more present in medicine. Especially machine learning shows great potential for complex tasks such as time series classification, usually at the cost of transparency and comprehensibility. This leads to a lack of trust by humans and thus hinders its active usage. Explainable artificial intelligence tries to close this gap by providing insight into the decision-making process, the actual usefulness of its different methods is however unclear. This paper proposes a user study based evaluation of the explanation method Grad-CAM with application to a neural network for the classification of breaths in time series neonatal ventilation data. We present the perceived usefulness of the explainability method by different stakeholders, exposing the difficulty to achieve actual transparency and the wish for more in-depth explanations by many of the participants.", "sections": [{"title": "1. INTRODUCTION", "content": "Mechanical ventilation is a vital therapy for neonates with respiratory failure. Modern ventilators are able to not just deliver breaths with predetermined settings but can also detect the patient's breathing efforts and effectively support these. Time series data of flow and pressure thus exhibit specific patterns for different types of breaths. These allow the categorization of breaths, for example as spontaneous or mechanical. The classification of breaths can be of interest for clinical and medical engineering applications, for instance during the weaning process of patients (Sangsari et al. (2022)). However, the classification of breaths by hand is a time-intensive task, requiring domain knowledge. Hence, an automated procedure is desirable. Chong et al. (2021) introduced a rule-based approach for breath detection and division of breaths into phases for neonatal patients. The different breath types were however not the focus.\nThe neonatal breath data used in this work is recorded as multivariate times series. The utilization of artificial intelligence (AI) for time series classification constitutes a promising approach, considering its achievements for this task (Ismail Fawaz et al. (2019); Ruiz et al. (2021)). In fields such as medicine, especially in the intensive care unit, the result of an AI-based system may have significant consequences. Thus, the comprehensibility of the underlying methodology plays an important part in the assessment of accountability and the user's confidence in the system. Achieving greater comprehensibility is the motivation behind the field of explainable artificial intelligence (XAI). Widespread xAI methods (Ribeiro et al. (2016); Lundberg and Lee (2017); Selvaraju et al. (2017)) have already been applied in medicine (Fauvel et al. (2021); Raab et al. (2023)), their actual use in enhancing the explainability of the underlying method is however still questioned (Dur\u00e1n and Jongsma (2021)). To better asses the perceived explainability of xAI, the different stakeholders, that interact with the system, need to be regarded (Arrieta et al. (2020)). These may have different demands and requirements with respect to the delivered explanations, from developers, aiming to improve the system, to end-users such as domain experts, aiming to successfully apply the system.\nThis work presents the results of a user study based evaluation of a Grad-CAM implementation. As working example, the convolutional neural network (CNN) by Fauvel et al. (2021) is applied to breath classification in neonatal respiratory data. To evaluate the usefulness of the resulting visual explanations, two stakeholders are regarded: developers and domain experts."}, {"title": "2. DATA DESCRIPTION", "content": "The used data was collected by the RWTH Aachen Uni- versity Hospital and annotated by a senior neonatologist (M. Schoberer). The records comprise ventilation data and vital parameters from the neonatal intensive care unit. From these, the flow and pressure measurements during invasive mechanical ventilation are used for the proposed method of classifying single breaths. In total, 18 patients are selected for the study. Both considered parameters are provided at a frequency of 125 Hz in each patient and the illustrative format of the data streams is depicted in Fig. 1.\nThe utilized ventilator, while displaying some information on breath types, does not store the information about the classification of individual breaths. Hence, a zero crossing detection is used on the flow measurement to divide the time series data into time segments, representing potential breaths. This workflow is based on the fact, that a positive flow reflects air flowing into the lungs, whilst a negative flow represents air flowing out of the lungs. Thus, flow crossings from a negative to a positive value depict a potential start of an inspiration and, in the used definition of this work, also the end of the previous breath. Artefacts, such as flow oscillating around zero or single inspirations without accompanying expiration, do not represent an actual breath and should be recognized and distinguished by the proposed model as such. This is realised by including the artefacts in the annotation phase to be consequentially learned by the neural network. The selected time segments are annotated and classified by the assisting neonatologist. For each of the 18 patients, five minutes per patient were extracted for annotation, resulting in a total of 6304 potential breaths to be used for training and evaluation. Each potential breath, consisting of flow and pressure data, can either be classified into one of four breath types or can be annotated as an artefact and therefore not be categorized as a breath. In our definition, the following breath types exist:\n\u2022 Spontaneous: the breath is fully carried out by the patient without the support of the ventilator.\n\u2022 Mechanical: the ventilator initiates and sustains the breath.\n\u2022 Triggered: the patient's inspiratory effort, creating positive airflow, triggers the ventilator.\n\u2022 Unclassifiable: time segments, which are identified as breaths by the senior neonatologist, but exhibit characteristics, that prevent a clear assignment to the other classes."}, {"title": "3. CLASSIFICATION MODEL AND RESULTS", "content": "This chapter describes the trained network and gives an overview of its results, while chapter 4 details the explain- ability aspect, which is the main focus of this paper. The foundation for the proposed breath classification method is a state-of-the-art explainable convolutional neural network for multivariate time series classification (XCM) proposed by Fauvel et al. (2021). The network uses three convolu- tional layers, which bring a two-fold advantage. On the one hand, it enables the network to process the input data both separately and as the combination of all observed variables. This results in the extraction of more discrimi- native features. On the other hand, the approach allows a model-specific explanation, which can be applied to each of the underlying convolutional layers separately."}, {"title": "4. EXPLAINABILITY METHOD AND EVALUATION", "content": "In the context of XCM, Grad-CAM (Selvaraju et al. (2017)) can be applied to each of the convolutional layers to obtain a layer-specific explanation. Grad-CAM allows generating heatmaps for the current inputs based on the trained weights and the resulting feature maps of a con- volutional layer. In the use case of breath classification, this enables to determine and visualize the importance of each considered flow and pressure data point for the current classification, in separate or as combination. For the proposed model, we decided to apply Grad-CAM on the two-dimensional and the concluding one-dimensional convolutional layer of XCM, as shown in Fig. 3. The ob- tained heatmaps thus comprise information about the sep- arate variables, flow and pressure, in the two-dimensional layer and the combination of the two input variables in the concluding layer. The latter convolutional layer encodes higher-level features than the preceeding one-dimensional convolution, therefore we regard its expla- nation as the most appropriate for practical usage. The obtained heatmap of the separately explained inputs is made optional to the user. It does not necessarily represent important information for the final classification due to the design of XCM.\nFor the evaluation, a self-developed data viewer is used, in which both emerging explanations are displayed, as depicted in Fig. 4. While the selected breath is processed by the network with a zero padding, the plotting of leading and trailing zeros leads to a confusing visualization. There- fore, in the data viewer the breath is visually padded with the first and last available value. The input's combined explanation of the final convolutional layer is presented as a heatmap in the background. The input's separate importance is visualized by the color intensity of the variable's corresponding graph. The classification of the network is textually displayed above the plot together with the related confidence of the network. The latter value is intended to further support the assessment of the classification of the current breath.\nThe usefulness of the proposed method is evaluated with the help of a self-developed questionnaire. The application- specific questions are based on relevant aspects of explain- able artificial intelligence, as proposed by Arrieta et al. (2020). For this paper, the following facets are of special interest:\n\u2022 Trustworthiness: the model acts as intended when facing a problem.\n\u2022 Causality: the explanation states correlations, which expectantly have a causal justification.\n\u2022 Transferability: the explanations provide an assess- ment of the models generalisability limits.\n\u2022 Informativeness: the explanations give enough in- formation to grasp the models functionality.\n\u2022 Confidence: the model and its explanations act robust and stable.\n\u2022 Fairness: the model has not trained a bias.\nThe questionnaire is divided into three sections, which are conducted sequentially. First, the performance of the method is evaluated, followed by the proposed explana- tions and finally, the overall practicability is assessed. The three sections aim to cover the above mentioned x\u0391\u0399 aspects. A precise assignment between a question and an aspect is however difficult. The perceived performance may cover trustworthiness, confidence but also fairness. The explanation evaluation may target the causality and informativeness, while the overall practicability might hint at the transferability of the chosen method. Each included question features a numerical answer ranging from 1 (worst result) to 6 (best result), which, by providing an even amount of possible answers, aims to prevent a tendency to the middle. This paper brings an evaluation on the explainability of a machine learning method with appli- cation in the field of neonatal ventilation. Therefore, the participants are chosen to cover both the medical and computer science field to include possible end users as well as developers of such algorithms. From the twelve partic-"}, {"title": "5. DISCUSSION", "content": "Regarding the performance of the model, the assessment is worse in the group of medical professionals in every considered category and especially the predictability of the classifications is assessed poorly, as seen in Fig. 5. One possible interpretation is, that people with less experience in the given task, might be more prone to trusting the explanations, accepting them as valid and even trying to detect and learn a reasoning from it. On the other hand, the domain experts might use their own knowledge to challenge the delivered explanation and classification.\nA major point of criticism in the provided feedback, is the misclassification of spontaneous breaths. A spontaneous breath is performed by the patient, without the venti- lator's support and thus is lacking a significant rise in pressure, see Fig. 2. This is a feature which was easily understood by both groups as a straightforward logic to distinguish spontaneous from triggered and mechanical breaths. Therefore, a breath classified as spontaneous, despite distinct indications of ventilator intervention, had an intensified misleading effect, due to the clarity of the human reasoning. Furthermore, the classification of arte- facts as spontaneous breaths is rated as a problem of the proposed model. From the developers perspective it is troubling, that the algorithm is not capable to distinguish between random noise and a structured form. From the medical perspective, an excessive detection of spontaneous breaths could lead to a false interpretation of the pa- tient's activity. Lastly, the explanations were criticized for highlighting meaningful segments but yielding the wrong classification. This was especially the case for mechanical breaths wrongly classified as triggered. These observations are coherent with the results of the performance evaluation in chapter 3. In this case of repeated misclassification, there was no clear evidence whether the explanations de- livered any clarity.\nConcerning the explanations, the assessments of develop- ers and domain experts do not vary substantially. The visualisations are generally considered as useful, the ratio- nale of the explanations and the relation between the two provided visualisations are however criticized. The latter is reasoned to yield uncertainties in the interpretation of the results. The influence of the separate explanations on the final classification was not clear and thus abstruse to most users. The most prominent criticism of the proposed explanations and the underlying model is the seemingly ar- bitrariness of important regions in some breaths. In many breaths, the participants assessed the identified important regions as reasonable. However, in some cases regions, such as the required data padding, were identified as important. These might have a rational explanation in the used model, but are not coherent with the reasoning process of the par- ticipants. Lastly, the importance of the provided separate visualisations is emphasized, while the meaningfulness in this model is doubted and the missing relation to the joint classification is criticized.\nGenerally, the method was assessed to not being useful for clinical usage in the current form, while most participants see a vast potential, especially in the context of research. One important note is the desire for a more explicit expla- nation by most of the medical professionals, indicating the insufficiency of the given explanations for the application of breath classification in neonatal intensive care.\nLimitations. The choice for a CNN-based classification brings the constraint of a fixed length input. To overcome this limitation, we used zero padding. This, however, led to more error room for the classifier and the explanations, as they sometimes focused on the padding, which is not deemed as reasonable and confused the participants. Further, due to the rather low number of participants, which were further split into two groups, there was no easily discernible rating trend within a group."}, {"title": "6. CONCLUSION", "content": "This work underlines the many factors that influence a human-based evaluation of explainable AI. The knowledge of the task performed by the AI as well as knowledge on AI itself may affect the perceived usability of the expla- nation methods. Domain knowledge may affect the trust- worthiness, as exemplified here by the medical experts. The self-assessed AI-expertise, however, had no clear over- all correlation with the model's practicability assessment. Further, misclassifications accompanied by unhelpful ex- planations reduce the informativeness and, hence, lower the confidence in the model. This is especially the case for classifications, which are straightforward for a human, the model and explanations however are wrong or misleading, and thus lower the trust. Lastly, the choice of visualizing the importance of the input data once separately and once together was assessed as a useful concept. However, it also raised more questions about the model's workings, affecting its perceived comprehensibility. While such conclusions can be drawn from both the numerical assessment and the given remarks, these leave room for interpretation and should rather be used as groundwork for a more in-depth evaluation than definitive findings. Further user studies should cover data from multiple subjects. Additionally, a larger participant group is desired, to better cover the in- terplay of a person's understanding of both the AI method and the medical application."}]}