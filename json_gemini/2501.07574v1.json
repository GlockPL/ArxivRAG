{"title": "UnCommon Objects in 3D", "authors": ["Xingchen Liu", "Piyush Tayal", "Jianyuan Wang", "Jesus Zarzar", "Tom Monnier", "Konstantinos Tertikas", "Jiali Duan", "Antoine Toisoul", "Jason Y. Zhang", "Natalia Neverova", "Andrea Vedaldi", "Roman Shapovalov", "David Novotny"], "abstract": "We introduce Uncommon Objects in 3D (uCO3D), a new\nobject-centric dataset for 3D deep learning and 3D genera-\ntive AI. uCO3D is the largest publicly-available collection\nof high-resolution videos of objects with 3D annotations\nthat ensures full-360\u00b0 coverage. uCO3D is significantly\nmore diverse than MVImgNet and CO3Dv2, covering more\nthan 1,000 object categories. It is also of higher quality, due\nto extensive quality checks of both the collected videos and\nthe 3D annotations. Similar to analogous datasets, uCO3D\ncontains annotations for 3D camera poses, depth maps and\nsparse point clouds. In addition, each object is equipped\nwith a caption and a 3D Gaussian Splat reconstruction. We\ntrain several large 3D models on MVImgNet, CO3Dv2, and\nuCO3D and obtain superior results using the latter, show-\ning that uCO3D is better for learning applications.", "sections": [{"title": "1. Introduction", "content": "The primacy of data has been the defining characteristic of\nthe last decade of machine learning, alongside deep learn-\ning. The most powerful models in language, speech and\ncomputer vision are simple but large deep networks trained\non massive amounts of data, and then further fine-tuned on\na high-quality subset of that data. One should expect this\nparadigm to extend to any application of machine learning,\nincluding 3D computer vision.\nHowever, 3D training data is much harder to come by\nthan data for text, audio and image processing.\nSeeking training data for large 3D neural networks such\nas the LRM of [27], many have turned to synthetic datasets\nlike Objaverse [13]. However, synthetic data is a poor sub-\nstitute for real data in applications like digital twinning,\nwhich aims to create 3D models of real-life objects. This\nis why many photorealistic reconstruction networks [6, 23,\n57, 58, 63, 67] are trained using real object-centric datasets\nlike CO3D [47], MVImgNet [76], GSO [15] and OmniOb-\nject3D [68]. Using real data is also crucial for generaliza-\ntion, as demonstrated by DUSt3r [64] for point map pre-\ndiction and DepthAnything [74] for depth prediction, both\nof which are trained on numerous real datasets. Even non-\ncurated image datasets like the billion-scale LAION [52]\nare applicable to 3D vision. For instance, text-to-3D gen-\nerators [35, 45, 53, 54] build on large text-to-image mod-\nels [5, 11], which are pre-trained on such data.\nGiven the importance of 3D datasets, but also their rela-\ntive scarcity, in this paper we ask what is the next step for\nreal data in 3D vision. To answer this question, we note that,\nwhile the size of a dataset is crucial, in most cases its quality\nis just as important. For example, the multi-view image gen-\nerators built into current text-to-3D models [35, 55] are no-\ntoriously sensitive to the quality of the fine-tuning data, and\nthey are only trained on a tiny fraction of best-looking mod-\nels (e.g., Instant3D [35] uses only about 1% of Objaverse).\nWe conclude that simply contributing more low-quality data\nis insufficient; instead, we need a high-quality dataset.\nBased on this observation, we argue that there is a gap in\nthe real object-centric 3D datasets that are currently avail-\nable, as none strikes the optimal balance between quality\nand scale. For example, the 3D object scans in OmniOb-\nject3D [68] and GSO [15] provide very accurate geometry\nand textures, but only count a few thousand objects. Con-\nversely, datasets like CO3D [47] and MvImgNet [76] con-\ntain orders-of-magnitude more objects, but lack reliable 3D\nscans. Instead, they provide many views of the objects to-\ngether with lower-quality 3D cameras and point clouds re-\nconstructed with structure-from-motion (SfM).\nIn this paper, we address this gap with a new dataset,\nUncommon Objects in 3D (uCO3D), which better balances\ndata quality and siz (Tab. 1). Similar to CO3D, it com-\nprises full-360\u00b0 crowd-sourced videos capturing objects"}, {"title": "3. Uncommon Objects in 3D", "content": "In this section, we introduce uCO3D, our new dataset of\nreal-life 3D objects. uCO3D comprises 360\u00b0 turn-table-\nlike videos of objects, crowdsourced and annotated with 3D\ncameras, point clouds, 3D Gaussians, and textual captions.\nCompared to older datasets like CO3Dv2 [47], uCO3D\ncomes with many improvements. First, uCO3D is much\nlarger and more diverse than CO3Dv2: it contains more\nthan 1k different categories and more than 170k objects,\ncompared to the 50 and 38k of CO3Dv2. While CO3Dv2's\ncategories are taken from MS COCO [38], the categories\nin uCO3D are taken from the LVIS [21] taxonomy. Hence,\nwe inherit the LVIS focus on covering the long-tail of the\nvisual-category distribution. To simplify data analysis, we\ngrouped the 1k+ LVIS categories to 50 super-categories,\neach containing approximately 20 subcategories. Figure 2\nshows the number of videos collected per super-category,\nand the LVIS category distribution.\nSecond, uCO3D improves quality significantly com-\npared to CO3Dv2, ensuring that videos are of high reso-\nlution, cover each object well, and that the 3D annotations\nare accurate. uCO3D also contains rich textual descriptions\nof each object, missing in other datasets, and useful to train\nlarge generative models. It also comes with additional 3D\nGaussian Splat reconstructions of all objects, each rigidly\naligned to a canonical object-centric reference, which make\nit possible to re-render the dataset from a fixed, canonical\nset of cameras, simulating synthetic data acqiusition, which"}, {"title": "Dataset collection.", "content": "Videos of objects were captured by\nworkers on Amazon Mechanical Turk. To ensure high video\nquality, workers were required to submit videos of a suffi-\ncient resolution. As a result, more than 60% of videos in\nuCO3D are of 1080p resolution or higher, compared to 33%\nin CO3Dv2. Furthermore, to aid the 3D reconstruction,\nworkers followed a sine-wave capture trajectory instead of\nthe plain circular trajectory of CO3Dv2, ensuring varying\ncamera elevations (c.f. Fig. 4). Finally, each video was in-\ndividually manually assessed to make sure that it adheres to\nthese requirements, a process more rigorous than the rough\neyeballing used in CO3Dv2 [47]."}, {"title": "3D annotation with VGGSfM.", "content": "For each video, we use the\nstate-of-the-art VGGSfM [62] Structure from Motion (SfM)\nsystem to estimate the parameters of the cameras (intrin-\nsic and extrinsic) for 200 frames sampled uniformly. VG-\nGSfM also outputs a sparse 3D point cloud, and its denser\nversion obtained by triangulating additional 3D points from\nVGGSfM's tracker. Examples of sparse and densified SfM\npoint clouds are shown in Fig. 3."}, {"title": "Scene alignment.", "content": "While the coordinate system of VG-\nGSfM cameras is defined only up to a rigid transformation,\nit is crucial for applications like generation and reconstruc-\ntion to train on a dataset of rigidly aligned objects. We thus\nalign all objects so they have a horizontal ground plane,\nsimilar scale, centring, and orientation. Details of the scene\nalignment procedure are in the supplementary material."}, {"title": "Gaussian Splat reconstruction.", "content": "Sparse and even dense\nSfM point clouds provide an accurate but still quite sparse\n3D reconstruction of the scene's surface. To further densify\nit, uCO3D provides a 3D Gaussian Splat reconstruction [29]\nfor each scene, fitted using gsplat [75]."}, {"title": "Scene captioning.", "content": "uCO3D also provides textual captions\nfor all scenes, useful for generative modelling. Motivated\nby Cap3D [41], we first caption each view separately using\na vision-language model, and then summarise these into a\nsingle scene caption using LLAMA3 [16]."}, {"title": "4. Applications", "content": "In this section, we demonstrate uCO3D's merit on three\npopular 3D learning tasks: feedforward sparse-view 3D re-\nconstruction (Sec. 4.1), new-view synthesis using diffusion\n(Sec. 4.2), and text-to-3D (Sec. 4.3)."}, {"title": "4.1. Few-view 3D Object Reconstruction", "content": "Traditionally, multi-view 3D-annotated datasets such as\nCO3D or MVImgNet have been used to supervise few-\nview 3D reconstructors. In this section, we train Light-\nplaneLRM [6], an evolution of the seminal LRM [27], and\nshow that doing so on uCO3D leads to better performance\nthan training on alternative datasets.\nLRM is a large transformer [61] that accepts few input\nimages of an object and predicts a 3D representation of the\nlatter. The transformer, conditioned on the tokens of the ob-\nserved images via cross attention, converts a set of learnable\ninput tokens to a 3D representation. The 3D representa-\ntion is a triplane [7] supporting an opacity/radiance implicit\nshape. LightplaneLRM improves LRM by adding so called\n\"splatting layers\" and a memory-efficient renderer.\nDuring training, LightplaneLRM receives four random\nsource frames from a training uCO3D video sequence, and\nrenders the predicted triplane into held-out target views.\nLearning minimizes the photometric loss between the ren-\nders and the corresponding ground-truth targets. Both\nsource and target views are masked using the extracted seg-\nmentation masks to make sure that LightplaneLRM only re-\nconstructs the foreground object. Training uses the Adam"}, {"title": "4.2. Novel-view synthesis using diffusion", "content": "We now consider application of uCO3D to training new-\nview image diffusion generators. These generators can,\ngiven one or a few views of an object and a target camera\npose, output new arbitrary views as observed from the target\ncamera, hallucinating missing details based on a statistical\nprior they learn. They can thus complement and integrate\nthe feed-forward reconstruction models of the previous sec-\ntion, which are deterministic and thus unable to deal with\nambiguity well. To this end, we train a diffusion model\nsimilar to the recent CAT3D [19], but reimplement it from\nscratch given lack of source code (see details in the supple-\nmentary). We call this model CAT3D-like.\nEvaluation protocol. As in Sec. 4.1, we compare ver-\nsions of CAT3D-like trained using uCO3D, MVImgNet,\nand CO3Dv2 and test them on held-out datasets. A feature\nof CAT3D is the ability to reconstruct both the principal\nobject in the images as well as the background. We thus\nbenchmark the method using new-view synthesis datasets\nthat do contain background, namely DTU [28], contain-\ning structured light scans of various objects, LLFF [43],\ncontaining scenes captured from fronto-parallel camera tra-\njectories, RealEstate10k [79], containing real-estate walk-\nthroughs, and Mip-NeRF 360 [3] with complex indoor and\noutdoor scenes. For evaluation, we take three known views\nas input and use the model to predict a new view. We report"}, {"title": "4.3. Photorealistic Text-to-3D", "content": "Next, we show that uCO3D allows to train a photorealis-\ntic text-to-3D generators. Methods like CAT3D and oth-\ners [39, 42, 53] generate several views of the object first, and\nthen fit a 3D model, such as a NeRF or 3DGS, via optimiza-\ntion. This can work well, but it is not particularly robust\nor fast. An alternative, popularized by Instant3D [35] and\nfollow-ups [54, 72], is to use a feedforward reconstructor in\nthe second step, similar to LightplaneLRM from Sec. 4.1,\nwhich is faster and more robust. However, these mod-\nels require canonical views of the objects for example,\nInstant3D considers 4 orthogonal viewpoints, covering all\n'sides'. The requirement of such training canonical views\ncomplicates training on real data, where viewpoints are ar- \nbitrary, and explains why such models are usually trained\non synthetic data, limiting realism.\nImaging 3DGS from canonical views. Our new idea is to\n're-shoot' the 3DGS reconstructions provided with uCO3D\nfrom canonical viewpoints, making our data compatible"}, {"title": "Dataset Difficulty", "content": "4.  Conclusions"}, {"content": "We have introduced uCO3D, a new object-centric 3D\ndataset of real-life objects. uCO3D strikes a balance be-\ntween size and quality, ensuring the quality of the collected\nturntable-like videos and of the 3D annotations, while at the"}, {"title": "A. Appendix", "content": "A.1. CAT3D-like model details\nThis section provides more details for the CAT3D-like\nmodel used in Sec. 4.2. CAT3D [19] is a diffusion model\nwhich takes as input a set of $N_{tgt}$ target cameras and a set\nof $N_{src}$ source views with cameras, and aims at generat-\ning the views associated to the target cameras. Since the\ncode is not available, we follow the implementation de-\ntails of [19] to reproduce a model with similar capabilities.\nSpecifically, starting from a pretrained text-to-image latent\ndiffusion model similar to [48], we first modify all 2D self-\nattention layers in the decoder part of the denoising UNet\nsuch that 2D self-attention is performed across all the views\nin the batch. First proposed by [53], this cross-view atten-\ntion allows each image token to attend to tokens of all views\nin the batch, thus improving multi-view consistency. Then,\nwe modify the architecture with zero-initialized channel ex-\npansion such that it can take as input the latent features con-\ncatenated with mask maps indicating source views and cam-\nera maps in the form of Pl\u00fccker rays. Different from [19],\nwe use the v-prediction / v-loss parametrization [49] and the\nzero terminal SNR noise scheduling recommended by [37]\nas we found it to work better than the original CAT3D\nrecipe. For all experiments, we use $N_{src}=3$, $N_{tgt}=5$ and\ntrain for 100k iterations using Adam [30] optimizer with a\nconstant learning rate of 1e-5 and a global batch size of 64.\nFor evaluation, we follow standard practices [19, 67] and\nreport results on common out-of-distribution NVS datasets\n(RealEstate10K, LLFF, DTU, Mip-NeRF 360) using the\nsame test splits. We evaluate novel-view synthesis in the\n3 view input setting and report LPIPS and PSNR metrics."}, {"title": "A.2. Text-to-3D model details", "content": "The text-to-image stage of the Instant3D-like model from\nSec. 4.3 is based on an internal text-to-image model archi-\ntecturally similar to Emu [11]. Starting from a model pre-\ntrained on a dataset of image-caption pairs, we fine-tune the\nmodel on 4-view canonical-render grids of uCO3D objects.\nIn all our experiments we use the Adam optimizer [31] with\na batch size of 160 and a constant learning rate of le-5.\nWe distribute the training across 32 NVIDIA A100 gpus\nfor a total of 20k steps. During inference, we use a Diffu-\nsion Probabilistic Model (DPM) Sampler [40] and denoise\nover 60 steps. The 4-view-to-3D stage of the Instant3D-\nlike model is based on LightplaneLRM [6]. For the Light-\nplaneLRM model which reconstructs both the central ob-\nject and the scene background, we use the coordinate con-\ntraction of MERF [46] which non-linearly maps the distant\nparts of the scene so they always fall into the [-1,1] bound-\ning cube of the utilizied triplane representation. The rest of\nthe training procedure follows the LightplaneLRM protocol\ndescribed in Sec. 4.1.\nWe train three different versions of both the Instant3D-\nlike model and LightplaneLRM, each version correspond-\ning to a different dataset. Specifically, we train on a dataset\nof synthetic assets similar to Objaverse [13], and on two\nversions of uCO3D, one that contains background and one\nwhere the background information is masked. For evalua-\ntion, we report the FID metric [25] for the models trained on\ndatasets without background information (Tab. 4). The eval-\nuation sets corresponding to the Surreal and Real prompts\nare created by randomly selecting 50 image frames for ev-\nery scene / prompt pair. We center the objects of the uCO3D\nimages using the per-frame mask information for consistent\nevaluation across all datasets. The generated 3D objects of\nthe text-to-3D model are rendered from sampled cameras\ndrawn from the camera distribution of each individual eval-\nuation set. The qualitative results presented in Fig. 8 are\nextracted using the model variants trained with background\ninformation."}, {"title": "A.3. Rigid scene alignment", "content": "In Sec. 3, we described a procedure that estimates a rigid\ntransform for each object to align it to dataset-wide object-\ncentric reference. Here, we provide additional details.\nWe start with finding the gravity axis by making sure the\nroll of the cameras is close to 0, following [57]. Then, we\ntranslate and scale the densified point cloud ((b) in Fig. 3)\nso that the median locations along the horizontal axes are 0,\nand so that the STD of its points' coordinates is 1. Then,\nwe normalize the 2D rotation in the horizontal plane by\naligning the principal components, and, finally, shift the ob-\nject vertically to make the ground plane's elevation zero.\nAs shown in the experiments, this normalisation allows to\nrender each object from 4 canonical viewpoints defined in\nthe object-centric reference, which eventually enables the\nInstant3D-like text-to-3D model training."}]}