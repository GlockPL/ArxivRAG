{"title": "Improving Retrieval Augmented Language Model with Self-Reasoning", "authors": ["Yuan Xia", "Jingbo Zhou", "Zhenhui Shi", "Jun Chen", "Haifeng Huang"], "abstract": "The Retrieval-Augmented Language Model (RALM) has shown remarkable performance on knowledge-intensive tasks by incorporating external knowledge during inference, which mitigates the factual hallucinations inherited in large language models (LLMs). Despite these advancements, challenges persist in the implementation of RALMs, particularly concerning their reliability and traceability. To be specific, the irrelevant document retrieval may result in unhelpful response generation or even deteriorate the performance of LLMs, while the lack of proper citations in generated outputs complicates efforts to verify the trustworthiness of the models. To this end, we propose a novel self-reasoning framework aimed at improving the reliability and traceability of RALMs, whose core idea is to leverage reasoning trajectories generated by the LLM itself. The framework involves constructing self-reason trajectories with three processes: a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We have evaluated our framework across four public datasets (two short-form QA datasets, one long-form QA dataset, and one fact verification dataset) to demonstrate the superiority of our method, which can outperform existing state-of-art models and can achieve comparable performance with GPT-4, while only using 2,000 training samples.", "sections": [{"title": "1 Introduction", "content": "The Retrieval-Augmented Language Model (RALM) has become a critical enhancement for Large Language Models (LLMs) by integrating external knowledge during inference. Despite their advanced capabilities in language understanding and generation (Brown et al., 2020; Touvron et al., 2023), LLMs are prone to producing hallucinated and inaccurate content, particularly in knowledge-intensive tasks (Ji et al., 2023). Augmenting LLMs with relevant information obtained from external sources like Wikipedia and search engines has proven effective in reducing these inaccuracies (Guu et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022; Asai et al., 2024). This method has proven effective in mitigating the factual hallucinations inherent in LLMs (Kwiatkowski et al., 2019; Petroni et al., 2021; Ram et al., 2023).\nNevertheless, there are still limitations associated with RALMs, particularly in terms of reliability and traceability. First, the reliability of the retrieved information remains a significant concern. Previous studies have shown that noisy retrieval can adversely affect the performance of an LLM (Menick et al., 2022; Li et al., 2023), as irrelevant data can lead to misguided responses and disturb the model's ability to leverage its intrinsic knowledge effectively. Second, the interpretability and traceability of outputs generated by RALMs need to be improved. While RALMs incorporate retrieved documents during both training and inference phases, they may fail to cite these documents explicitly, thus complicating the process of tracing and verifying the claims made by LLMs.\nTo improve the retrieval robustness, recent studies have explored incorporating natural language inference (NLI) models (Honovich et al., 2022) and document summarization models during inference (Yoran et al., 2023; Xu et al., 2023a). However, the effectiveness of these external NLI and summarization models largely affects the overall performance of RALMs. The training and optimization of these auxiliary models require additional costs. Consequently, determining the most appropriate training and selection methods for these NLI and summarization models presents a further challenge in the application of such approaches.\nTo address the above limitations, we propose a novel end-to-end SELF-REASONING framework to improve the performance of RALMs. Our intuition is that the explicit self-reasoning trajectory crafted by LLMs can improve the retrieval robustness and accuracy of question answering. During the pre-training phase, an LLM primarily focuses on knowledge acquisition, yet it does not learn to reason from retrieved documents to generate answers. To address this, a feasible approach is to incorporate reasoning trajectories into a post-training phase. Such an approach could potentially teach the model to reason and distinguish relevant and irrelevant documents, improving its response accuracy to queries. An example of how our SELF-REASONING framework generates reasoning trajectories is illustrated in Figure 1. In contrast, as shown in the middle part of Figure 2, the conventional RALM methods gather all documents in a non-selective manner, leading to the distraction of the LLM by irrelevant content and consequently resulting in the generation of erroneous answers.\nOur framework comprises three self-reasoning processes: 1) a Relevance-Aware Process (RAP), which instructs the LLM to judge the relevance between the retrieved documents and the question, 2) an Evidence-Aware Selective Process (EAP), which directs the LLM to choose and cite relevant documents, and then automatically select snippets of key sentences as evidence from the cited documents, 3) a Trajectory Analysis Process (TAP), which requires the LLM to generate a concise analysis based on all gathered self-reasoning trajectories generated by previous two processes and subsequently provide the final inferred answer. Furthermore, we propose a gradual training method by employing stage-wise masking strategies to enhance the performance of our framework.\nWe summarize our contributions as follows:\n\u2022 We propose a novel end-to-end SELF-REASONING framework that improves the robustness of RALMs by leveraging reasoning trajectories generated by the LLM itself, without the need for external models or tools.\n\u2022 We carefully design three processes to enhance the interpretability and traceability of RALMs by requiring LLMs to explicitly generate snippets and citations from documents, and further explain the reason why cited documents can help answer the question.\n\u2022 We evaluate our framework on four public datasets (two short-form QA, one long-form QA, and one fact verification), demonstrating that our method surpasses existing state-of-art models in performance, achieving this with only using 2,000 training samples."}, {"title": "2 Related Work", "content": "2.1 Retrieval-augmented LMs\nMany studies have investigated augmenting the performance of LLMs with externally retrieved information (Izacard et al., 2022; Guu et al., 2020; Borgeaud et al., 2022) and some of them pre-train language models with retrieved passages. For works focusing on RALMs with citations, Menick et al. (2022); Nakano et al. (2021) instruct or train an LLM to answer questions with retrieved documents while providing citations. Gao et al. (2023b) proposes an end-to-end system to retrieve supporting evidence and generate answers with citations, while only focusing on prompting without updating their model weights. Other works instruct or fine-tune LLMs to use external tools to retrieve dynamically (Schick et al., 2023; Yao et al., 2023; Jiang et al., 2023), which offers an adaptive method of when and what to search. Gao et al. (2023a) improves the attribution and factuality of language models by taking outputs of LLMs and applying a post-process retrieve-and-edit approach.\n2.2 Robustness for RALMS\nTo improve the robustness of RALMs, Yoran et al. (2023) use a natural language inference model to filter out irrelevant documents before RALMs, and Xu et al. (2023a) use a memorization model to filter out or compress retrieved documents before using them to prompt an LLM. The concurrent work (Yu et al., 2023) generates summaries of documents as notes before RALMs, and Baek et al. (2023) uses a separate small language model as a verifier to detect and correct errors in LLMs during retrieval for generating factually correct outputs. Different from the above works, ours identifies key sentences and cites relevant documents through an end-to-end framework that eliminates irrelevant ones, without dependence on external inference models.\nThe most similar work to ours is presented by Asai et al. (2024), who developed a method that teaches models to retrieve information using designed reflection tokens. However, this approach needs to train extra critic models and generator models to predict the reflection tokens, which requires tens of thousands of extra training samples. In contrast, our method does not rely on special tokens. Instead, we construct reasoning trajectories and then use them to directly enhance the performance of LLMs, offering a more efficient and scalable solution. More related works on LLMs for reasoning are discussed in the Appendix \u00a7A.1."}, {"title": "3 Preliminary", "content": "We formally define the problem of retrieval augmented generation with self-reasoning. Given a query q and a corpus of documents D, an LLM-generated answer with m statements and n tokens can be defined as $y = (s_1,s_2,\\dots,s_m) = (w_1, w_2, \\dots,w_n)$, where $s_i$ is the i-th statement and $w_j$ is the j-th token in the generated answer. In addition, for long-form QA settings, each statement $s_i$ should cite a list of documents $C_i = {c_i^{(1)}, c_i^{(2)}, ...}$, where $c_i^{(k)} \\in D$. In our work, we train an LLM (e.g. LLaMA2) to first generate reasoning trajectories \u03c4 through self-reasoning and then to generate answers y* (including long-form answers and short-form answers) on condition of \u03c4. The model output is $y = concat(\u03c4, y*)$, which is the concatenation of \u03c4 and y*. Note that the generations of \u03c4 and y* are done in a single pass within our SELF-REASONING framework."}, {"title": "4 Method", "content": "In this section, we provide a detailed implementation of the self-reasoning process and the method to construct reasoning trajectories.\nIn this work, we propose a novel framework which involves three processes: 1) a Relevance-Aware Process (Sec \u00a74.1), 2) an Evidence-Aware Selective Process (Sec \u00a74.2), and 3) a Trajectory Analysis Process (Sec \u00a74.3). An illustration of our SELF-REASONING framework for improving the RALMs is shown in Figure 2. In Sec 4.4, we demonstrate the process of data generation and quality control, and Sec 4.5 presents the details of model training.\n4.1 Relevance-Aware Process\nIn this work, we choose the DPR (Karpukhin et al., 2020) and the Contriever (Izacard et al., 2021) as the default retriever R to recall the top-k relevant documents. When presented with a question and a set of documents, people can determine whether the question is relevant to the retrieved documents. Therefore, we first instruct the model to judge the relevance between the retrieved documents D and the given question q. We further request the model to explicitly generate reasons explaining why given documents are identified as relevant. The output should include two fields as relevant and relevant reason, as depicted in Figure 2. Noting that if all of the retrieved documents are irrelevant, the model should provide an answer based on the internal knowledge acquired during its pre-training phase. We define the self-reasoning trajectories generated by RAP as Tr.\n4.2 Evidence-Aware Selective Process\nWhen required to answer the question, people generally will first identify the crucial sentences from the provided documents, and then cite or highlight them as key points. This process of citing the document facilitates reading comprehension and can serve as a technique for combining multiple short answers to address various aspects. While people may carry out this selective process and citation instantaneously, LLMs need to formulate the self-reasoning trajectories explicitly.\nIn our work, we require the LLM to explicitly state the reason why the selected sentence is supportive and plausible in answering the question. We define the selected sentence as evidence in our paper. Specifically, after retrieving the top-k documents, the self-reasoning method for Evidence-Aware Selective Process can be formulated as follows: First, we instruct the LLM to choose relevant documents and automatically select snippets of key sentences for the selected documents. Then, we request the LLM to output the reason why the selected snippets can answer the question. The intermediate output is a list containing multiple contents, each content should include two fields, as cite content and reason for cite, which is illustrated in Figure 2. We define the self-reasoning trajectories generated by EAP as Te.\n4.3 Trajectory Analysis Process\nFinally, we consolidate all the self-reasoning trajectories (Tr and Te) in the previous processes together to form a chain of reasoning snippets, thereby enhancing the overall performance of the retrieval augmentation generation. Specifically, we ask the LLM to analyze the reasoning trajectories within itself and ultimately to output a concise analysis and a short answer. We instruct the LLM to output content with two fields as analysis and answer, which is shown in Figure 2. We define the self-reasoning trajectories generated by TAP as ta. In this work, the analysis output is defined as a long-form answer, and the answer output is defined as a short-short answer. In Section 5.2, we further explored the performance of long-form and short-form QA settings.\n4.4 Data Generation and Quality Control\nTraining Data Generation. For the Relevance-Aware Process data generation, as manually labeling the relevant and irrelevant documents is label-intensive, we request the GPT-4 (OpenAI, 2023) to generate answers as ground truth. Specifically, we instruct the GPT-4 to generate labels regarding irrelevant fields, and further to output the reasons why the given documents cannot answer the question. We concatenate the given question and the retrieved documents as positive samples. For negative samples, we randomly select a different question from the training set and retrieve the top-k documents related to it. These documents are then concatenated with the initial question to form negative samples. To avoid order bias in the training data, we shuffle the order of the documents.\nFor the EAP and TAP data generation, manually annotating the citation of the document and writing the self-reasoning process for each question is not feasible in practice. Therefore, we follow a similar process to RAP, we first instruct the GPT-4 to generate a snippet of selected documents and subsequently output the reasoning process as trajectories. The method to construct the EAP training data is the same as RAP except that the instructions for the GPT-4 are different. The details of the instructions are shown in Appendix \u00a7A.2.\nData Quality Control. For training data generation, correct and comprehensive reasoning trajectories are very important. When training an LLM, the quality of the training samples is more important than the quantity (Zhou et al., 2023). As we cannot guarantee the correctness of self-reasoning trajectories and citations by the GPT-4, we develop two efficient methods to control the quality of data generation: 1) The first method is to use the off-the-shelf tools \u00b9 in Gao et al. (2023b) to automatically verify the performance of data generation for document citations. We calculate the citation precision and recall score for each training sample and filter out scores lower than our pre-defined thresholds \u03b4p and \u03b4r, for citation precision and recall, respectively. 2) Second, though the validation of self-reasoning trajectories and citations generated by GPT-4 is challenging, verifying the correctness of the final answer is straightforward. Therefore, we filter out the trajectories that lead to the incorrect answers and only keep the correct ones. We totally generate 10,000 training samples by GPT-4, after the filtering strategy by quality control, we finally keep 2,000 training samples with high quality.\n4.5 Model Training\nWe train our self-reasoning generation model \u03a6 by our constructed corpus which is augmented with self-reasoning trajectories \u03c4 using the standard language modeling objective, maximizing likelihood:\n$max \\mathbb{E}_{(q,\\tau,y) \\sim D_{sr}} log p_\\Phi(y | \\tau, q) p_\\Phi(\\tau | q)$   (1)\nwhere $\\tau = \\tau_r \\oplus \\tau_e \\oplus \\tau_a$ are the self-reasoning trajectories, $\\oplus$ is the concatenation operator, $\\tau_r, \\tau_e, \\tau_a$ are trajectories generated by above three processes respectively. q is the provided question, and y is the model output, including the intermediate reason trajectories and the final answer. $D_{sr}$ is the training corpus augmented with self-reasoning trajectories. During training, we observed that it is more challenging to ensure the correctness of an LLM with 13B parameters when generating long reasoning trajectories than short ones. We hypothesize that an LLM's effective reasoning length is limited and exceeding this limit might lead to error accumulation during the inference stage. Therefore, we propose a gradual training method by employing stage-wise masking strategies to gradually learn to generate long trajectories.\nSpecifically, we propose a stage-wise training process while we train the LLM stage by stage. In the first stage, we mask the trajectories produced by the next two stages (EAP and TAP) and train the model with a learning rate ra. Then in the second stage, we only mask the trajectories generated by TAP and train the model with a learning rate rb. Finally, we concatenate the reasoning trajectories from all stages and put them into a self-reasoning LLM for end-to-end training with a learning rate rc. Hyper-parameters for training are described in Appendix \u00a7A.4."}, {"title": "5 Experiments", "content": "5.1 Datasets and Settings\nTo demonstrate the effectiveness of our proposed SELF-REASONING framework, we conduct an extensive experimental evaluation on two short-form QA datasets (NaturalQuestion (Kwiatkowski et al., 2019) and PopQA (Mallen et al., 2023)), one long-form QA dataset (ASQA (Stelmakh et al., 2022)), and one fact verification dataset (FEVER (Thorne et al., 2018)). Detailed descriptions of the datasets can be found in Appendix \u00a7A.3. We explore off-the-shelf retrievers. We use the DPR (Karpukhin et al., 2020) and the Contriever-MS MARCO (Izacard et al., 2021) to retrieve the top five documents from Wikipedia.\nBy default, we use the DPR as a retriever for the NQ, as the DPR has been fine-tuned on the high-quality NQ data. On the PopQA, where question and answer pairs are created based on Wikipedia in 2022, therefore, for the PopQA, we use the December 2020 preprocessed Wikipedia corpus provided by (Izacard et al., 2022) and use the Contriever as a retriever. For the ASQA dataset, we use GTR (Ni et al., 2022) as a retrieval that corresponds to the experimental settings in (Gao et al., 2023b). More settigns can be found in Appendix \u00a7A.4.\n5.2 Evaluation Metrics\nWe use different evaluation metrics for short-form QA, long-form QA, and fact verification tasks.\nShort-form QA metrics. We report accuracy for short-form QA tasks, which is based on whether ground-truth answers are included in the model predictions instead of strictly requiring exact matching, following Mallen et al. (2023); Schick et al. (2023).\nLong-form QA metrics. For long-form QA tasks, we report the EM recall as a correctness metric, and the citation recall and the citation precision for citation quality, which are the same as the metrics in (Gao et al., 2023b).\nFact verification metrics. For the fact verification task, we report the accuracy as a metric, which is a three-class classification accuracy, following Thorne et al. (2018).\n5.3 Baseline Models\nBaseline models without retrieval. We evaluate strong open-source pre-trained LLMs as baseline models. For basic LLMs, we test LLaMA2-7B, LLaMA2-13B (Touvron et al., 2023) and its instruction-tuned chat version LLaMA2-Chat-7B, LLaMA2-Chat-13B.\nBaseline models with retrieval. To assess the performance of the retrieval augmented LMs, we use retrievers described in Section 5.1. First, we benchmark the models using the LLaMA2 and the Vicuna (Chiang et al., 2023) series models for baselines. Additionally, for a fair comparison, we also include LLaMA2-FT, where LLaMA2 is fine-tuned on all the training samples generated by GPT-4 except the self-reasoning trajectories. To establish strong baselines, we compare our method against RECOMP (Xu et al., 2023a), ReAct (Yao et al., 2023), and Self-RAG (Asai et al., 2024), all of which are trained with extra GPT-4 generated samples or external tools. We also compare our framework with GPT-4 (OpenAI, 2023). We include categorical comparisons with the baseline models in the Appendix \u00a7A.5.\n5.4 Main Results\nTable 1 shows the performance comparisons with different methods on the four public datasets. For short-form QA evaluations, the performance of LLMs with augmented retrieval is consistently better than that of basic ones, affirming the effectiveness of the augmented approach. Notably, under the same order of magnitude parameters, our SELF-REASONING framework outperforms most of the strong baseline LLMs. Specifically, compared to the Self-RAG, our framework is an end-to-end system trained with only 2,000 self-reasoning trajectory samples. In contrast, the Self-RAG requires training additional critic LMs to predict reflection tokens using an additional 46,000 instances generated by GPT-4. This efficiency not only simplifies the training process but also significantly reduces resource consumption.\nIn the context of long-form QA evaluations, for the metrics of EM recall, it needs to comprehend multiple documents and merge answers. The EAP and TAP are specifically designed for multi-"}, {"title": "6 Analysis", "content": "6.1 Ablation Study\nWe conduct an ablation study on two short-form QA datasets and a fact verification dataset to analyze the individual contributions of each process within our proposed SELF-REASONING framework. We further explore the effectiveness of the gradual learning (GL) method and the quality control (QC) of data generation (a detailed analysis described in Appendix \u00a7A.7). The main ablation study results are shown in Table 2.\nEffectiveness of Relevant-Aware Process\nFirst, we evaluate the effect of the Relevant-Aware Process (RAP). The removal of the RAP causes the overall performance to drop in two short-form QA datasets and a fact verification dataset, suggesting that preliminary consideration of the relevance between questions and retrieved documents can help improve performance. We notice that the performance declines most significantly in the FEVER dataset. Detecting irrelevant documents is critical in the fact-verification task. Our model will immediately output NotEnoughInfo if it detects that all documents are irrelevant.\nEffectiveness of Evidence-Aware Process\nThen we evaluate the effect of the Evidence-Aware Selective Process (EAP). Removing the EAP causes the overall performance of the average accuracy to decline from 60.9 to 56.3 in three short-form QA datasets. This reduction indicates that snippets of key sentences and document citations generated through self-reasoning are instrumental in boosting accuracy.\nEffectiveness of Trajectory Analysis Process\nFinally, we evaluate the effect of the Trajectory Analysis Process (TAP). When excluding the TAP, we can observe a performance decline on all three datasets, demonstrating that self-analysis based on two previous processes generated trajectories can also improve the performance of LLMs. Note that the analysis content generated by TAP is indispensable for the long-form QA evaluation.\n6.2 Retrieval Robustness Analysis\nAs retrievers are not perfect and past work has shown that noisy retrieval can have negative effects on the performance of LLMs (Petroni et al., 2020; Li et al., 2023). In this section, we design two kinds of settings to validate the robustness of RALMs. In the first setting, we test whether the order of the retrieved documents will affect the performance of the RALMs. Specifically, after retrieving the top-k documents using retrievals (such as the DPR) with a descending relevance score, we randomly shuffle the order of the retrieved documents and then input them to an LLM. In the second setting, we test how noisy documents impact the performance of LLMs. When retrieving the top-k documents from the given question, we randomly replace 50% of the retrieved documents with other documents sampled from a different question in the dataset.\nFigure 3 shows the noise robustness experiment results on three datasets. Our SELF-REASONING framework consistently outperforms the Self-RAG and Vicuna models. We observe that random shuffling of retrieved documents has a minimal impact on the performance of RALMs. If the provided documents are supportive, it is trivial for a RALM to determine the correct answer. However, when presented with noisy documents, all models experience a decline in performance. The performance drop for our self-reasoning framework is relatively minimal, which demonstrates that our proposed method is robust even when dealing with noisy documents.\n6.3 Citation analysis\nAs the automatic evaluation by the NLI model cannot detect partially supported citations, we discuss the analysis of citations with human evaluation in this section. Similarly to Liu et al. (2023), we conduct a human evaluation on two dimensions: 1) citation recall: annotators are given a statement and all documents that the statement refers to and are asked to judge whether the documents fully"}, {"title": "7 Conclusion", "content": "RALMs can effectively enhance the performance of LLMs in handling knowledge-intensive tasks. Despite their effectiveness, notable concerns about their reliability and traceability persist. To address these limitations, we propose a novel SELF-REASONING framework to improve the performance of RALMs by using reasoning trajectories generated by the LLM itself. It is comprised of a relevance-aware process, an evidence-aware selective process, and a trajectory analysis process. We conduct extensive experiments on four public datasets to demonstrate the superiority of our framework over existing state-of-the-art models."}, {"title": "8 Limitations", "content": "In this work, we mainly focus on improving the performance of RALMs with a self-reasoning framework on the task of open domain question answering and fact verification. Although we believe our framework can involve the distribution of real-world user questions, as we have evaluated in four public datasets, we acknowledge that we have not explored more challenging scenarios, such as multi-hop reasoning, code generation, and arithmetic reasoning. In future work, more challenging reasoning tasks, such as arithmetic reasoning, should be explored for the self-reasoning framework. We believe that our framework can effectively mitigate factual hallucinations in LLMs and improve the robustness of RALMs. However, there is still a risk that our method might generate hallucinations."}, {"title": "A Appendix", "content": "A.1 More Related Work of LMs for Reasoning\nOne of the most well-known methods of using LLMs for reasoning is the Chain-of-Thought (CoT) (Wei et al., 2022), which demonstrates the capability of LLMs to create their thinking process for problem-solving. Zhou et al. (2022) proposes a least-to-most prompting for solving complex tasks. Wang et al. (2022) introduces a method to reason with self-consistency. Press et al. (2023) proposes a method to further improve the chain of thought by reasoning explicitly instead of implicitly.\nRecent works have extended beyond the internal reasoning ability of LLMs to include interactions with external tools (e.g., search engines or retrievers) for solving complex tasks. The ReAct (Yao et al., 2023) presents an iterative paradigm to combine reasoning and acting with LLMs for tackling language reasoning and decision-making tasks. Xu et al. (2023b) introduces a framework to enable information retrieval and LLMs to interact with each other effectively with chain-of-query decomposition. Pan et al. (2024) proposes a novel framework named Chain-of-Action (CoA), which integrates a reasoning retrieval method to decompose complex questions into chains of configurable actions.\nDifferent from the above works, which are mostly based on relatively large LLMs (e.g., Chat-GPT), our proposed method focuses on enhancing smaller LLMs (e.g., LLaMA2) using only a limited number of samples to achieve high robustness and interpretability through single-step interaction.\nA.2 Instructions\nThe instructions for GPT-4 to generate self-reasoning trajectories are shown in Figure 5 (the short-form and long-form QA tasks) and Figure 6 (the fact verification task). The words in the orange font are key fields that need to be generated.\nA.3 Datasets Description\nWe conducted an extensive experimental evaluation of two short-form QA datasets, one long-form QA dataset, and a fact verification dataset.\nNaturalQuestion (NQ) (Kwiatkowski et al., 2019) contains real user questions issued to the Google search and answers found from Wikipedia by the annotators. NQ is created to train and evaluate automated question answering systems.\nPopQA (Mallen et al., 2023) is a large-scale open-domain question answering dataset, consisting of entity-centric QA pairs. Each question is made by converting a knowledge triplet retrieved from Wikidata using a template. In this work, we use PopQA to evaluate performance in long-tail settings.\nASQA (Stelmakh et al., 2022) is a long-form factoid dataset, and most questions can be answered by Wikipedia. Each question originates from AmbigQA (Min et al., 2020) and represents an ambiguous query that requires multiple short answers to cover various aspects. The dataset provides a long-form answer that contains all short answers.\nFEVER (Thorne et al., 2018) is a fact verification dataset that contains claims generated by rewriting sentences extracted from Wikipedia and subsequently verified without knowledge of the sentence from which they were derived. The claims are classified as Supported, Refuted, or NotEnoughInfo.\nA.4 Experiment settings\nTraining settings. During gradual learning, we fine-tune the LLaMA-2 (Touvron et al., 2023) model with our self-reasoning framework for 3 epochs with a batch size set to 32, leveraging the DeepSpeed library (Rasley et al., 2020) and the ZeRO optimizer, and we use parameter partitioning ZeRO stage 3 with float16 precision. The learning rate ra for the first stage is set to 5e-5, the learning rate rs for the second stage is set to 3e-5, and the learning rate rc for the final stage is set to le-5. Our SELF-REASONING 13B model is trained on the NVIDIA Tesla 8 \u00d7 V100 32GB GPU for 4 hours, while the 7B model is trained for 2 hours.\nInference settings. We use the vLLM 2 framework (Kwon et al., 2023) to accelerate the inference speed during inference. The codes follow the Apache-2.0 license agreement. We use greedy decoding in all experiments to ensure deterministic generations. We test the temperature within a range of {0.2, 0.4, 0.6, 0.8, 1.0}, finally we set the temperature to 0.2, as we observed lower temperature results in better performance in the open-domain question answering task. The maximum generation length is set to 2048 for our model. All baseline models are tested with zero-shot settings for short-form QA datasets, and with one-shot settings for\nA.5 Categorical Comparisons\nWe differentiate our method from existing strong baseline models by categorizing and comparing it across six dimensions, as presented in Table 3. As illustrated in the table, our method can stand out in several key aspects.\nFirst, our SELF-REASONING method is the only end-to-end framework among existing methods that can improve performance without relying on external models or tools. Second, our method eliminates the need for external modules during both the training and inference phases. In practical applications, our framework does not need to call multiple tools or modules. Third, our framework requires a significantly smaller dataset for training the LLM compared to other methods, needing only 2,000 samples with self-reasoning trajectories. This efficiency in training drastically lowers the resources and time needed, making our method both cost-effective and scalable for practical applications.\nA.6 Case Study\nIn our case study, as illustrated in Figure 7, we compare the responses generated by the raw LLM, the standard RALM (e.g. LLaMA2 with retrievals), and our SELF-REASONING method. The challenge involves reconciling information from multiple retrieved documents to provide a correct answer as the retrieved documents contained noisy data.\nThe response from the raw LLM (e.g., LLaMA2) suggested that the film was made in 2000, based on its inherited knowledge. However, this answer is incorrect and is a hallucination generated by the LLM. The standard RALM approach yielded 1989 as the production date. This answer was based on unrelated details from the retrieved documents, showing a lack of context-specific understanding and robustness for noisy retrieved documents.\nOur SELF-REASONING framework provided a comprehensive approach by assessing the relevance and context of retrieved documents. First, in the relevance-aware process, the documents were identified as relevant based on their content regarding the production dates and events surrounding the film. Second, in the evidence-aware selective process, the model retrieved the first documents, which highlighted the original start date as January 2002, with filming commencing in February 2002 (highlighted in green in the figure). This information was crucial in establishing the timeline for the film's production. The model can also understand of the difference between the production date and the release date in the third retrieved document (highlighted in red in the figure). In the trajectory analysis process, the correct timeline was deduced by piecing together self-generated trajectories, leading to the conclusion that the film Catch Me If You Can was indeed produced in 2002. As the case illustrated above, by leveraging relevant documents and focusing on contextual evidence, our SELF-REASONING framework can achieve a precise and well-supported answer, highlighting its utility and robustness in complex information retrieval tasks.\nA.7 More Analysis on Ablation Study\nEffectiveness of Gradual Learning\nFurther, we validate the effect of gradual learning. Rather than training an LLM with a stage-by-stage approach, we initially concatenate the reasoning trajectories from all stages and put them into the LLM for end-to-end training. As shown in Table 2, the performance decline can be observed in three datasets, suggesting that gradual learning can help improve performance.\nEffectiveness of Quality Control\nThe effect of quality control on data generation is also evaluated in our work. Instead of using the filtered high-quality training samples, we randomly sampled 2,000 unfiltered training samples generated by GPT-4. As shown in Table 2, the substitution of unfiltered training data leads to the degradation of the model results.\nA.8 Human Evaluation\nWe randomly sample 100 examples from the ASQA dataset and annotate the outputs of selected models. Each sample is then assigned to two people for annotation. Each annotator is required to verify the citation recall and citation precision acorrding to our provided scheme. The annotation scheme is inspired by (Gao et al., 2023b) as follows:\nCitation Recall. The annotators are shown the question q, the statement si, and all of its citations Ci, and they assess if the set of citations fully support the statement (recall=1) or if they do not support all the claims (recall=0). We calculate the overall recall score for the model by averaging the recall scores of all statements.\nCitation Precision The annotators are shown the question q and a statement si and one of its citation ck) \u2208 Ci. We ask the annotator if the citation fully supports, partially supports, or does not support the generated claims in s\u2081. Citation ck) has a citation precision of 1 if si has a recall of 1, and c(ki) fully or partially supports si. Finally, we calculate the overall precision score for the model by averaging the precision scores of all statements."}]}