{"title": "Character-level Tokenizations as Powerful Inductive Biases for RNA Foundational Models", "authors": ["Adri\u00e1n Morales-Pastor", "Raquel V\u00e1zquez-Reza", "Mi\u0142osz Wiecz\u00f3r", "Cl\u00e0udia Valverde", "Manel Gil-Sorribes", "Bertran Miquel-Oliver", "\u00c1lvaro Ciudad", "Alexis Molina"], "abstract": "RNA is a vital biomolecule with numerous roles and functions within cells, and interest in targeting it for therapeutic purposes has grown significantly in recent years. However, fully understanding and predicting RNA behavior, particularly for applications in drug discovery, remains a challenge due to the complexity of RNA structures and interactions. While foundational models in biology have demonstrated success in modeling several biomolecules, especially proteins, achieving similar breakthroughs for RNA has proven more difficult. Current RNA models have yet to match the performance observed in the protein domain, leaving an important gap in computational biology. In this work, we present ChaRNABERT, a suite of sample and parameter-efficient RNA foundational models, that through a learnable tokenization process, are able to reach state-of-the-art performance on several tasks in established benchmarks. We extend its testing in relevant downstream tasks such as RNA-protein and aptamer-protein interaction prediction. Weights and inference code for ChaRNABERT-8M will be provided for academic research use. The other models will be available upon request.", "sections": [{"title": "1. Introduction", "content": "RNA is known to be a pivotal molecule in molecular biology and research on its functions has significantly transformed our understanding of gene expression, regulation, and therapeutic potential. Unlike DNA, which primarily serves as a stable repository of genetic information, RNA is dynamic and versatile, taking on multiple roles within cells. It acts not only as the messenger that conveys genetic instructions from DNA to the protein synthesis machinery (mRNA) but also as a regulator of gene expression through molecules like microRNAs (miRNAs) and small interfering RNAs (siRNAs) (Shang et al., 2023). Additionally, some RNAs function as catalysts in biochemical reactions, exemplified by ribozymes (Weinberg et al., 2019). These unique structural and functional properties enable RNA to influence nearly every aspect of cellular biology, making it essential to both fundamental life processes and modern medical applications.\nRecent breakthroughs in RNA biology have opened new avenues for therapeutic interventions, turning RNA into a powerful tool for combating diseases previously considered undruggable by conventional treatments. RNA therapeutics, such as RNA interference (RNAi) for gene silencing and mRNA vaccines that instruct the immune system, offer immense potential. The rapid development and deployment of mRNA vaccines during the COVID-19 pandemic showcased the speed, flexibility, and effectiveness of RNA technology in addressing global health crises (Chaudhary et al., 2021). Beyond vaccines, RNA-based therapies are being explored to treat a wide range of conditions, including genetic disorders, cancer, neurodegenerative diseases, and metabolic syndromes. Techniques like antisense oligonucleotides (ASOs), siRNAs, and RNA aptamers leverage RNA molecules to modulate gene expression, inhibit harmful proteins, or even repair genetic mutations (Zhu et al., 2022). Furthermore, RNA-guided genome editing tools, such as CRISPR-Cas systems, represent a new frontier in precision medicine by targeting RNA directly and offering unparalleled specificity in gene modulation without permanently altering DNA (Fellmann et al., 2017).\nHowever, despite these advancements, challenges remain particularly in enhancing RNA stability, improving delivery mechanisms, and reducing off-target effects (Saw & Song, 2024). As RNA-mediated therapies advance, there is an increasing need for methodologies that not only rationalize RNA behavior and properties but also scale effectively to meet the demands of the field. Computational biology plays a critical role in predicting RNA behavior prior to experimental validation. Several software tools have been developed to predict RNA features, with ViennaRNA (Lorenz et al., 2011) being one of the most prominent. While the primary focus of RNA tool development has been on predicting properties, molecular simulations have played a crucial role in studying RNA's dynamic behavior and validating its secondary and tertiary structures (Sponer et al., 2018). However, these simulations are often resource-intensive, requiring significant engineering expertise and deep knowledge of the modeled systems. Moreover, they may not always provide the resolution needed to capture the full complexity of RNA behavior.\nIn contrast, the emergence of artificial intelligence in RNA research holds the promise of expanding the field, offering appealing opportunities that surpass traditional computational methods. AI models, especially at the inference stage, are dramatically more efficient to run and can tackle a vast array of tasks, many of which can be learned directly from RNA sequence data alone. While protein language models (pLMs) have revolutionized our understanding of protein folding, function, and design with models like Evolutionary Scale Modeling (ESM) (Lin et al., 2023) excelling in applications ranging from drug discovery to protein engineering, AI applications for RNA have developed more gradually. Protein models have rapidly expanded their use cases, whereas RNA-focused models have often specialized in narrower tasks, such as predicting RNA splicing patterns or optimizing codon usage, limiting their broader impact.\nRecent efforts to create general-purpose RNA models show promise but still lag behind the advances seen in protein modeling. Even the most advanced RNA models, which have achieved state-of-the-art performance in areas like secondary structure prediction (Peni\u0107 et al., 2024), have yet to achieve the transformative success that AI has brought to protein science.\nThis work aims to take a step toward establishing a robust RNA foundation model, ChaRNABERT, capable of performing a wide range of downstream tasks. Our approach is built on two core concepts: first, utilizing learnable tokenization to move beyond human-curated motif-selection, which may not be optimal for a biomolecule like RNA; and second, training on a diverse set of RNA types to ensure the model's ability to generalize across different types and tasks. To assess the ChaRNABERT's performance in relevant scenarios, we evaluate its performance on an standardized benchmark, expand the benchmark with additional tasks, and test its capabilities in high-impact applications such as aptamer interaction prediction."}, {"title": "2. Related Work", "content": "The rapid advancement of RNA language models has tried to mirror the transformative impact of language models in protein science, aiming to decode the \"language\" of RNA sequences. These models endeavor to capture the underlying patterns, structural motifs, and functional elements inherent in RNA, thereby facilitating breakthroughs in structure prediction, functional annotation, and therapeutic design. The progression of these models reflects a concerted effort to overcome the unique challenges posed by RNA's structural diversity and functional versatility.\nEarly pioneers in this field, such as RNA-FM (Chen et al., 2022) and RNABERT (Akiyama & Sakakibara, 2022), laid the foundational groundwork for RNA language modeling. RNA-FM was one of the first general-purpose models designed for non-coding RNA (ncRNA) sequences. RNA-FM, a 100M parameter model, was trained on a previous RNA-central release, encompassing 23 million samples. This model demonstrated the potential of language models to learn directly from RNA sequences, enabling tasks like secondary structure prediction and functional annotation. Concurrently, RNABERT emerged with a focus on structural alignment and clustering of ncRNA. By incorporating partial multiple sequence alignments from RNAcentral and the Rfam 14.3 dataset, totaling over 762 thousand sequences, RNABERT leveraged evolutionary information to enhance its ability to discern structural similarities among RNA molecules. This integration of evolutionary data marked a step toward understanding RNA structure-function relationships with a special focus over clustering and alignment.\nBuilding on these foundations, models like UNI-RNA (Wang et al., 2023) sought to scale up both in model complexity and dataset size. UNI-RNA featured 400M parameters and was trained on an expansive dataset of 1 billion sequences from RNAcentral, the Nucleotide Collection (nt), and Genome Warehouse (GWH). Aiming to be a universal RNA model, UNI-RNA endeavored to capture a broad spectrum of RNA types and functions, enabling the modeling of very long RNA sequences without truncation.\nApplication specific models also made significant contributions. RNA-MSM (Zhang et al., 2023) introduced a novel approach by directly utilizing evolutionary information from multiple sequence alignments to model ncRNA sequences, benchmarking a diverse array of dowstream tasks. Splice-BERT (Chen et al., 2024b) addressed the critical aspect of RNA splicing in precursor messenger RNA (pre-mRNA), aiding in the prediction of splice sites and alternative splicing events. These advancements underscored the importance of specialized models in tackling specific biological questions.\nModels like CodonBERT, UTR-LM, and 3UTRBERT (Ren et al., 2024b; Chu et al., 2024; Yang et al., 2024) focused on different regions of mRNA, capturing codon usage patterns and post-transcriptional regulation mechanisms mediated by untranslated regions (UTRs). CodonBERT, concentrated exclusively on the coding sequences (CDS) of mRNA, employing codon-level tokenization to capture patterns crucial for gene expression optimization. UTR-LM and 3UTRBERT specialized in the 5' and 3' UTRs, respectively, enhancing our understanding of mRNA expression, translational efficiency, and gene regulation mediated by UTRs.\nBigRNA (Celaj et al., 2023) diverged from sequence-based models by integrating genomic context and utilizing thousands of genome-matched datasets. This approach underscored the importance of multi-omics data in capturing the complexity of RNA regulation in different cellular contexts, moving beyond sequence information to include expression patterns and regulatory interactions.\nDespite these advancements, a noticeable gap remained when compared to the transformative impact of language models in protein science. Many existing RNA models were specialized or limited in scope, hindering their generalizability and broader applicability. Addressing this challenge, RINALMO (RNA Integrated Language Model Optimization) (Peni\u0107 et al., 2024) emerged as a notable milestone in RNA language modeling. RiNALMo was designed to bridge this gap by providing a comprehensive and versatile framework capable of capturing the full complexity of RNA sequences and structures. This model employed a deep transformer-based architecture with attention mechanisms tailored specifically for RNA.\nOne of the key introductions of RiNALMo was its pre-training strategy. The model was trained on an extensive and diverse dataset that included a wide array of RNA sequences from databases such as RNAcentral, as well as experimentally derived structural data. This multimodal training approach allowed RiNALMo to learn rich representations that encapsulate both linear sequence information and three-dimensional conformations of RNA, bridging the gap between sequence and structure. Moreover, RiNALMo introduced a modification in comparison to standard tokenization methods that went beyond simple nucleotide or codon representations. By utilizing k-mer embeddings and incorporating secondary structure annotations, the model could understand folding patterns and motifs crucial for RNA function.\nIn practical applications, RiNALMo set new benchmarks across multiple RNA-related tasks. It achieved state-of-the-art results in secondary and tertiary structure prediction, surpassing previous models in accuracy and reliability. Additionally, RiNALMo demonstrated exceptional capabilities in predicting RNA-protein and RNA-RNA interactions, key for understanding cellular processes and developing RNA-based therapeutics.\nRNA language modeling currently lacks a foundational model that can handle a wide range of tasks through a straightforward token-masking framework without depending on task-specific data or dedicated pre-processing. Such a model would excel at efficiently learning from the intrinsic structure of RNA sequences.\nTo move away from imposed biases, we propose a new tokenization strategy. Instead of relying on single nucleotides, codons, or static k-mers, which each bring arbitrary assumptions and fixed nucleotide groupings, we introduce a learnable tokenization scheme that adapts to capture sequence details at multiple levels of granularity. Paired with a BERT-like transformer optimized for contextual understanding, this approach achieves competitive or superior performance relative to larger, task-specific RNA models while substantially reducing parameter demands."}, {"title": "3. Methods", "content": null}, {"title": "3.1. ChaRNABERT architecture", "content": "The ChaRNABERT (CRB) architecture is designed to be able to capture both fine-grained nucleotide details and broader contextual relationships efficiently, optimized for understanding the complex structures of RNA. At its core, CRB employs a modified Gradient-Based Subsequence Tokenization (GBST) (Tay et al., 2022), paired with a bidirectional BERT encoder (Devlin et al., 2019). This combination allows the model to dynamically identify and emphasize biologically relevant subsequences without the constraints of a predefined vocabulary. Simultaneously, it captures the long-range dependencies and bidirectional context crucial for accurately modeling RNA structures and functions."}, {"title": "3.2. Character-level tokenization", "content": "To effectively model subsequence information directly from nucleotide-level inputs, we employ a \"soft\" subword tokenization approach from character-level inputs. The original idea behind this approach is to allow the model to learn latent subsequence segmentations by dynamically selecting the most appropriate subsequence block at each character position during training procedure.\nThis key idea is extended through the enumeration of offsets in the sequences in a sliding window manner, as to model the equivalent of open reading frames (ORFs). The learnable combination of both approaches allows us to dynamically select the best tokenization possible for each of the nucleotides and whether or not to take into account the local environment of the sequence.\nWe also differ from the original implementation in the removal of the downsampling procedure, as single nucleotide resolution is highly desirable for several downstream applications."}, {"title": "3.2.1. CONSTRUCTING CANDIDATE SUBSEQUENCE EMBEDDINGS", "content": "Given an input sequence of nucleotides, they are embedded as a tensor $X \\in R^{L \\times d}$, where L is the sequence length and d is the nucleotide embedding dimension. To this individual representation GBST applies a one-dimensional depthwise convolution of kernel size equal to the maximum block size M, that acts as a smoothing operation, encouraging block level representations and allowing model to consider small shifts in the starting positions of blocks and the influence of non-adjacent nucleotides to some extent.\n$X = \\text{1D DWConv} (X)$,\nAfterwards, we generate candidate subsequence blocks by enumerating contiguous and overlapping spans of nucleotides up to a maximum block size M. For each block size b (where 1 < b < M), we construct subsequence blocks $X_{b,i}$ starting at position i by applying a pooling function over the embeddings of the nucleotides in the block:\n$X_{b,i} = F(X_{i:i+b})$,\nwhere $F: R^{b \\times d} \\rightarrow R^{d}$ is a non-parametric pooling function, in our case a sum pooling, that aggregates the embeddings within the selected subsequence into a single vector.\nThis procedure is also repeated for b \u2013 1 offsets o of 1 for each of the block sizes in a sliding window manner. For example, for block size of one it obtains single nucleotides representations, whereas for block size two it is able to extract information from each pair of nucleotides starting from position 0 and position 1. This is continued up to the maximum block size M.\n$X_{b,i,o} = F (X_{i+o:i+b+o})$,"}, {"title": "3.2.2. FORMING LATENT SUBSEQUENCE REPRESENTATIONS", "content": "To determine the most suitable subsequence block and offset at each nucleotide position, the approach introduces a scoring network $F_R : R^d \\rightarrow R$. This network computes a scalar score $p_{b,i}$ for each candidate $X_{b,i,o}$, reflecting the model's confidence in selecting that representation:\n$p_{b,i,o} = F_R (X_{b,i,o})$.\nWe then compute a softmax over the scores for all representation sizes at position i, producing a probability distribution $P_i$ over the candidate blocks and offsets:\n$P_i = \\text{softmax} ([p_{1,i}, p_{2,i}, ..., p_{M,i}])$.\nThis probabilistic weighting allows the model to softly select among the candidate representations based on their scores.\nTo enhance the model capabilities of capturing global context in the initial representation selection, we decided to incorporate the position-wise score calibration procedure from the original implementation. This layer computes a pseudo self-attention score between the different positions, encouraging the model to learn consensus among representation selection across the entire sequence. Specifically, updates are applied to the representation scores P using a self-attention mechanism without additional projections:\n$P = \\text{softmax} (P P^T) P$,\nwhere $P \\in R^{L \\times M}$ is the matrix of block probabilities, and $P$ is the consensus probability matrix.\nThe latent subsequence representation at position i is obtained by computing a weighted sum of the candidate representation embeddings, using the probabilities from the modified probability matrix as weights:\n$X_i = \\sum_{b=1}^{M} \\sum_{o=0}^{M-1} P_{b,i,o} X_{b,i,o}$.\nThis operation effectively allows the model to learn a soft subsequence segmentation, where each nucleotide position contributes to the final representation based on the likelihood of various block sizes and offsets. This soft selection mechanism ensures that the entire process is differentiable, enabling data-driven changes in the tokenization scheme during the training procedure and end-to-end training of the model.\nOverall, by integrating GBST into our model, we leverage the strengths of subword representations while maintaining the flexibility, adaptability and resolution of nucleotide-level processing."}, {"title": "3.3. Bidirectional BERT encoder", "content": "As the main architecture we employ a BERT-based model with a few improvements. This model is a transformer encoder that enables bidirectional context learning through a self-attention mechanism and pre-training objectives. Each input token is mapped to an embedding and tokenized, combined with a positional encoding to maintain token order, and passed through multiple layers of the encoder. The core mechanism in each layer is multi-head self-attention (multi-heads are omited from all equations for clarity), where for each token i, its attention with all tokens j in the sequence is computed as:\n$\\text{Attention}(Q, K, V) = \\text{softmax} (\\frac{QK^T}{\\sqrt{d_k}}) V$\nwhere $Q = XW_Q$, $K = XW_K$, and $V = XW_V$ are linear projections of the input X, i.e. token embeddings, and $d_k$ is the dimensionality of the keys/queries.\nWe introduce a few other common architectural modifications, namely SwiGLU's non-linearities (Shazeer, 2020), Rotary Positional Encodings (ROPE) (Su et al., 2024), Query Key Normalization (QKNorm) (Dehghani et al., 2023) and Flash Attention 2 (Dao, 2024).\n$\text{SwiGLU}(x) = \\sigma(xW_1) \\odot \\text{swish}(xW_2)$ where the Swish function is defined as:\n$\\text{swish}(x) = x \\cdot \\sigma(x)$ and o is the standard sigmoid non-linearity.\n$\\sigma(x) = \\frac{1}{1+ e^{-x}}$\nSwiGLU non-linearities are a combination of the Swish and Gated Linear Unit (GLU) non-linearities that have shown improved performance over its individual parts or classical functions like ReLU.\nWe additionally move away from absolute positional encodings and introduce ROPE to our model, which has been shown to increase performance and length generalization capabilities in comparison with absolute and other relative positional encodings. This approach is based upon a rotation mechanism, where positions in the sequence are represented as rotations in the embedding space.\n$f_{\\{q,k\\}} (x_m, m) = \\begin{pmatrix} \\frac{W_{\\{q,k\\}}^{(11)} x_m^{(1)} + W_{\\{q,k\\}}^{(12)} x_m^{(2)}}{W_{\\{q,k\\}}^{(21)} x_m^{(1)} + W_{\\{q,k\\}}^{(22)} x_m^{(2)} } \\end{pmatrix} \\begin{pmatrix} \\text{cos } m\\theta \\\\  -\\text{sin } m\\theta  \\\\ \\text{sin } m\\theta  \\\\ \\text{cos } m\\theta \\\\  \\end{pmatrix} X $\nwith to the following values for $\u03b8$ to add the the long-term decay property between the relative positions to the positional encoding.\n$\\theta_i = 10000^{-2i/d_k}$\nMoreover, to reduce the amount of training instabilities and loss spikes we decide to introduce QKNorm, effectively reducing the growth of the attention logits, which we found was a cause of instability in our training procedure. This mechanism applies a LayerNorm (LN) to the output of the Query and Key linear transformations.\n$\\text{softmax} [\\frac{1}{\\sqrt{d_k}} LN(XW_Q)(LN(XW_K))^T]$\nLastly, we include the hardware-aware Flash Attention 2 algorithm, allowing for an efficient increase of our context"}, {"title": "4. Pretraining", "content": null}, {"title": "4.1. Masking strategies", "content": "Typically, BERT's pre-training uses a masked language model (MLM) objective, where a random subset of tokens is replaced with a special mask token. The model is trained to predict the original tokens based on both preceding and following context, forcing it to encode bidirectional information. The prediction of the masked token is computed as:\nP(token Xmasked) = softmax(Wohi)\nwhere hi is the hidden state of token i after passing through multiple self-attention layers, and W\u3002 is a learned output projection matrix.\nLearning from more complex corruption schemes can enhance a model's ability to capture complex patterns and dependencies, therefore we chose to incorporate the UL2 (Unifying Language Learning) (Tay et al., 2023) paradigm into our training regimen. UL2 is a pre-training framework that unifies various language modeling objectives to create a more versatile and robust language model.\nIt introduces a novel masking strategy that combines different types of denoising objectives. These include short-span masking (S-denoising), extreme-span masking (X-denoising), and retrieval-augmented masking (R-denoising). S-denoising is similar to BERT's MLM objective, where individual tokens or short spans are randomly masked within the input sequence, and the model learns to predict these masked tokens using bidirectional context. X-denoising involves masking longer contiguous spans of text, which forces the model to understand and reconstruct larger chunks of information, thereby enhancing its ability to handle longer dependencies. R-denoising trains the model in an autoregressive fashion, predicting future tokens based on past context, akin to models like GPT.\nUL2 employs a shared g-masked token between the strategies to replace the masked spans, providing a unified way for the model to identify and reconstruct the missing information regardless of the span length. These strategies are selected based on a predefined sampling strategy, exposing the model to the different denoising strategies along the training process.\nThis process involves preparing the input by selecting a mode (S, X, or R) according to a series of specified probabilities and masking the input text accordingly. The model processes the masked input to generate hidden states for each token, and for the masked positions, it predicts the original tokens using the surrounding context. The training objective is to minimize the cross-entropy loss between the model's predictions and the actual masked tokens across all modes. Therefore the prediction for each masked token i can be computed in the same way as the MLM objective (see Eq.15).\nBy leveraging UL2 masking, the model benefits from enhanced context understanding, versatility, and improved generalization. First, training on both short and long spans allows the model to comprehend and generate text over varying lengths, improving its understanding of context and long-range dependencies. Second, the combination of bidirectional and autoregressive objectives enables the model to perform well on a wide range of tasks.Last, exposure to different types of denoising tasks helps the model generalize better to unseen data and tasks."}, {"title": "4.2. RNA datasets", "content": "For our study, we employed RNAcentral (Consortium, 2020) as the primary source for non-coding RNA sequences in our training dataset. RNAcentral is an extensive repository that consolidates non-coding RNA data from multiple expert databases, providing a unified and comprehensive resource. The dataset encompasses a diverse range of RNA families, including but not limited to: microRNAs (miRNAs), small nuclear RNAs (snRNAs), small nucleolar RNAs (snoRNAs), transfer RNAs (tRNAs), ribosomal RNAs (rRNAs), long non-coding RNAs (lncRNAs), Piwi-interacting RNAs (piRNAs), and small interfering RNAs (siRNAs). In total, RNAcentral contributed approximately 31 million non-coding RNA sequences to our dataset, offering a rich and varied collection for training our models.\nTo enable a comprehensive analysis that includes both non-coding and coding sequences, we expanded our dataset by incorporating coding sequences from RefSeq (Reference Sequence) database at the National Center for Biotechnology Information (NCBI) (O'Leary et al., 2016). Specifically, we added 31 million coding sequences to our dataset. This augmentation resulted in a balanced and extensive dataset comprising both non-coding and coding sequences, which is crucial for training robust models capable of distinguishing between the two types."}, {"title": "4.3. Model sizes", "content": "To assess the impact of model size on performance and explore scalability in RNA sequence analysis, we trained models with parameter counts mostly aligned with the main ESM models, specifically developing models with approximately 8M, 33M, 50M, 100M, 150M, and 650M parameters.\nOur exploration of scaling effects, both with and without GBST, involved training these models to examine the combined impact of character-level tokenization with the BERT encoder. This investigation not only focuses on MLM/UL2 loss performance but also evaluates the models' effectiveness on downstream tasks and generalization capabilities.\nDespite we investigate scaling parametrically (see Section 5.3), we chose to train models at these specific sizes as a baseline, even though it may not represent the optimal approach for all scenarios. This decision allows us to better analyze the interaction between model size and GBST. Our objective is to understand how these factors influence not only loss metrics but also broader performance across downstream applications.\nFor a thorough analysis, all model sizes were trained on two datasets: 31 million non-coding RNA sequences from RNAcentral and the combination of this dataset and the 31 million coding sequences from RefSeq.\nAll the models are trained in BF16 precision using Distributed Data Parallel with the DeepSpeed (Rasley et al., 2020) and ZeRO (Rajbhandari et al., 2020) frameworks for maximum optimization of computational resources."}, {"title": "5. Experiments", "content": "In this section, we show several analyses on the factors that influence the performance of ChaRNABERT in order to identify the settings that provide the best performance of the model as well as to identify the optimal trade-off between performance and computational cost.\nAdvances in large language models (LLMs) have been driven by scaling up parameters, enhancing their applications in natural language processing (NLP), as detailed by the studies of (Kaplan et al., 2020) and (Hoffmann et al., 2022). While scaling studies have also progressed in fields like pLMs (Serrano et al., 2024), with research investigating model size effects, a comprehensive analysis of LLM scaling applied to RNA remains underexplored. Works such as RINALMO (Peni\u0107 et al., 2024) have examined performance differences under variations of the parameter count, yet a detailed investigation of RNA-specific scaling laws is still absent. This analysis aims to bridge that gap, using ChaRNABERT as an initial model to guide future large-scale RNA LLM research.\nFirst, we explore how various learning rates and context window sizes affect ChaRNABERT's performance across model sizes, aiming to understand the impact of key hyperparameters. Next, we assess the model's efficiency with datasets of different sizes to evaluate the effects of data scaling. Finally, we analyze computational efficiency by measuring the impact of increased floating-point operations (FLOPs) on model improvements. Throughout, we follow (Hoffmann et al., 2022)'s scaling principles and compare tokenization strategies, highlighting the performance gains of using GBST over embeddings alone."}, {"title": "5.1. Impact of learning rate and context window", "content": "We analyzed the impact of using different learning rates and context window sizes on three models of different sizes: 5 million, 50 million, and 100 million parameters, using a dataset composed of 31 million non-coding sequences extracted from RNACentral (Consortium, 2020).\nFor each model size, three learning rates were tested, selected on the basis of the number of parameters in the model. Specifically, for the 5M and 50M parameter models, the learning rates tested were 5e-4, 1e-4, and 5e-5. For the 100M parameter model, the learning rates tested were 1e-4, 5e-5, and 1e-5. We choose different learning rates per model size in order to avoid training instabilities. The aforementioned tests permitted an evaluation of the influence of the learning rate on convergence.\nIn general, it was observed that in smaller models (Table 2), such as the 5 million parameter model, higher learning rates, 5e-4 or 1e-4, achieved a slightly lower loss compared to lower rates like 5e-5. Nevertheless, these gains, while present, were not substantial. In the middle models, such as the 50M parameter model (Table 3), learning rates such as 1e-4 or 5e-5 achieved lower losses compared to higher learning rates like 5e-4 which resulted in high instability. For larger models, such as 100M model (Table 4), higher learning rates, 1e-4 and 5e-5, similarly resulted in a lower loss compared to 1e-5. However, upon examining the convergence curves, lower learning rates helped to avoid instability in mid and large-sized models.\nTo explore how context window size affects performance, tests were conducted with window sizes of 8192, 4096, 2048 (Figure 2). For RNA language models, the context window is especially relevant due to the biological nature of the sequences processed by these models. Interactions, such as base pairing and secondary structures, can be scattered throughout the sequence, making it essential that the model is able to capture a context wide enough to identify relevant patterns."}, {"title": "5.2. Varying token counts", "content": "We also aimed to identify the impact of token count in model performance. We retained the three model sizes, 5M, 50M and 100M parameters, and generated datasets of varying sizes: 15M, 66M, 100M and 150M sequences which by the average number of tokens per sequence correspond to 2.4B, 10.9B, 16.5B, and 24.8B tokens respectively.\nSince the amount of non-coding sequences is limited, we included both coding and non-coding, extracted from the MARS (Chen et al., 2024a) sequence database. The sequences were randomly selected. In Figure 3, the loss obtained per model size and per dataset for both tokenization methods, GBST and EM, is shown.\nFrom the results displayed in Figure 3, it can be observed that the size of the dataset has a visible but not substantial impact on the overall performance of the model. In both GBST and EM tokenization experiments, the losses decreased slightly as the dataset size increased, but these improvements were not substantial enough to drastically improve the performance of the models. Furthermore, for both tokenization techniques, a clear U-shaped trend is still observed in the plots, suggesting that model performance can saturate at around 30M to 50M parameters, regardless of the size of the dataset used.\nBased on the analysis conducted, it is clear that while learning rate and context window size have some impact on performance, the factor that most significantly affects ChaRNABERT's performance is the size of the model. Larger models tend to perform better up to a certain point, after which performance gains become marginal, and in some cases, loss even increases slightly. In the following section, we will further explore this by conducting a detailed study on model scaling, with the aim of understanding how the number of parameters influences performance and identifying the optimal scaling strategies for ChaRNABERT."}, {"title": "5.3. Increasing model size", "content": "We follow Hoffmann et al. (2022) to fit a parametric loss function. Using the data collected during the experimental phases, we fit the power laws to establish the relationships $N_{opt} \\propto C^a$ and $D_{opt} \\propto C^b$, where $N$ is the model size, $D$ is the number of tokens, and $C$ represents the computational budget in FLOPs\u00b9. The exponents a and b were determined based on the fitted parameters, of the scaling law:\n$\\hat{L}(N, D) \\approx E + \\frac{A}{N^\\alpha} + \\frac{B}{D^\\beta}$\nHere, N is the number of model parameters, D is the size of the data set (in tokens), and E captures the natural entropy of the text (ideal loss). The terms with A and B reflect the deviation of the model from the ideal loss, due to the limited size of the model and data. The exponents \u03b1 and \u03b2 determine the impact of model and dataset size on the loss\u00b2.\nWe fit the expression in Eq. 16 following (Hoffmann et al., 2022)\u00b3. In particular, we utilized the most optimal results obtained for each model size from the experiment described in Section 5.2. This analysis was intended to capture the effects of both model size and dataset size on the model performance.\nWe found that for both GBST and EM, the final loss decreases predictably as the model parameters increase (Figure 11), following a general trend of improvement with larger models. However, in both cases, improvements plateau or even cease to be significant once a certain parameter threshold is exceeded, particularly around 30 to 50 million parameters.\nFrom the obtained parameters we derived the power-law exponents for model size and token count as a function of compute $N_{opt} \\propto C^{0.2279}$ and $D_{opt} \\propto C^{0.7720}$. A key finding is that the optimal model size scales sublinearly with the compute budget. Specifically, the relationship $N_{opt} \\propto C^{0.2279}$ indicates that the model size grows at a slower rate than the compute budget. In contrast, the optimal number of training tokens scales superlinearly with compute. The relationship $D_{opt} \\propto C^{0.7720}$ shows that, as compute grows, the number of training tokens increases more rapidly than the model size.\nWe observe that model performance improves rapidly with increased compute at first, but after a certain threshold, approximately 10\u00b9\u2076 FLOPs, the improvements in training loss begin to plateau (Figure 4). This trend suggests that, while scaling up model size and compute provides significant"}, {"title": "6. Assessing CRB's performance in downstream applications", "content": null}, {"title": "6.1. BEACON benchmark", "content": "Implementing BEACON benchmark. We leverage BEACON (BEnchmArk for COmprehensive RNA tasks and language models) to assess the performance of the CRB models in downstream tasks (Ren et al., 2024a). BEACON is the first comprehensive benchmark designed to evaluate deep learning methods for RNA analysis, encompassing 13 tasks across structural analysis, functional studies, and engineering applications. BEACON evaluates both traditional models such as CNNs, ResNets, and LSTMs, as well as advanced RNA foundation models like RNA-FM and RNA-BERT.\nFor each downstream task in the BEACON benchmark, we integrated a head module and trained it alongside the CRB architecture. The choice of the head module was based on the best benchmark available, either from BEACON or RINALMo, depending on the type of task."}]}