{"title": "AMBER - Advanced SegFormer for Multi-Band\nImage Segmentation: an application to\nHyperspectral Imaging", "authors": ["Andrea Dosi", "Massimo Brescia", "Stefano Cavuoti", "Mariarca D'Aniello", "Michele Delli Veneri", "Carlo Donadio", "Adriano Ettari", "Giuseppe Longo", "Alvi Rownok", "Luca Sannino", "Maria Zampella"], "abstract": "Deep learning has revolutionized the field of hyperspectral image (HSI) analysis,\nenabling the extraction of complex and hierarchical features. While convolu-\ntional neural networks (CNNs) have been the backbone of HSI classification, their\nlimitations in capturing global contextual features have led to the exploration\nof Vision Transformers (ViTs). This paper introduces AMBER, an advanced\nSegFormer specifically designed for multi-band image segmentation. AMBER\nenhances the original SegFormer by incorporating three-dimensional convolu-\ntions to handle hyperspectral data. Our experiments, conducted on the Indian\nPines, Pavia University, and PRISMA datasets, show that AMBER outperforms\ntraditional CNN-based methods in terms of Overall Accuracy, Kappa coefficient,\nand Average Accuracy on the first two datasets, and achieves state-of-the-art\nperformance on the PRISMA dataset.", "sections": [{"title": "1 Introduction", "content": "Hyperspectral imaging (HSI) captures detailed spectral information across a wide\nrange of wavelengths, offering a unique and rich data source for various remote\nsensing applications, including environmental monitoring, agriculture, and mineral\nexploration. The classification of hyperspectral images involves identifying the mate-\nrial composition of each pixel, a task that requires complex algorithms capable of\nhandling the high dimensionality and complexity of the data.\nIn recent years, deep learning algorithms, particularly convolutional neural networks\n(CNNs), have become prominent in HSI classification due to their ability to learn\nhierarchical features from raw data. CNN-based models have shown considerable\nsuccess in extracting spectral-spatial features. However, the inherent limitation of\nCNNs in capturing spectral-spatial long-range dependencies has spurred interest in\ntransformer-based models.\nVision Transformers (ViTs)[1], with their self-attention mechanism, have demon-\nstrated remarkable success in various image-processing tasks. They are adept at\ncapturing global contextual information, which is crucial for HSI classification. Many\ndeep learning models have adopted the transformer architecture for hyperspectral\ndata, and have achieved good results.\nThis paper presents AMBER, an advanced version of SegFormer [2] tailored for hyper-\nspectral image segmentation. By integrating three-dimensional convolutions, AMBER\neffectively processes multi-band images, preserving spatial dimensions and enhancing\nfeature extraction. We evaluate AMBER on the Indian Pines, Pavia University, and\nPRISMA datasets, demonstrating its superior performance compared to traditional\nmethods."}, {"title": "2 Related Works", "content": "Deep learning algorithms have transformed the landscape of hyperspectral image\n(HSI) pixel classification by learning complex and meaningful features hierarchically.\nAmong these, convolutional neural networks (CNNs) have become a cornerstone for\nexploring spectral-spatial features within local windows, owing to their robustness\nand scalability ([3])."}, {"title": "3 Methodology", "content": "This section introduces AMBER, a new and advanced SegFormer for Multi-Band\nImage Segmentation. As the original SegFormer [2], AMBER consists of two main\nmodules: a hierarchical Transformer encoder which generates high-resolution coarse\nfeatures and low-resolution fine features; and (2) a lightweight All-MLP decoder used\nto fuse multi-level features together. The final layer of the decoder performs dimen-\nsionality reduction to generate a bi-dimensional semantic segmentation mask from a\nthree-dimensional input image. Unlike the original SegFormer, AMBER is suited to\ncope with multi-band images thanks to its three dimensional convolution. Moreover,\nAMBER preserves the input spatial dimensions H and W: this in order to maximize\naccuracy at the price of alight increase in the number of trainable parameters. For an\noverview of the architecture, see Fig. 1."}, {"title": "3.1 Hierarchical Transformer Encoder", "content": "We create a series of Mix Transformer encoders (MiT) specifically designed for\nsemantic segmentation of multi-band images, such as hyperspectral images."}, {"title": "Hierarchical Feature Representation.", "content": "Given an input image with a resolution of $D \\times H \\times W$, we perform patch merging\nto obtain a hierarchical feature map $F_i$ with a resolution of $\\frac{D}{2^{i+1}} \\times \\frac{H}{2^{i+1}} \\times \\frac{W}{2^{i+1}} \\times C_i$,\nwhere $i \\in 1, 2, 3, 4$, and $C_{i+1}$ is larger than $C_i$."}, {"title": "Overlapped Patch Merging.", "content": "We utilize merging overlapping patches to avoid the need for positional encoding. To\nthis end, we define K, S, and P, where K is the three dimensional kernel size (or patch\nsize), S is the stride between two adjacent patches, and P is the padding size. Unlike\nthe original SegFormer, in our experiments, we set K = 3, S = 1, P = 1 ,and K = 3,\nS = 2, P = 2 to perform overlapping patch merging. The patch size is intentionally\nkept small to preserve image details and avoid parameter explosion. S = 1 preserves\nthe original image spatial dimensions H and W, avoiding the reduction of spatial\ndimensions by 1/4."}, {"title": "Efficient Self-Attention.", "content": "In the multi-head self-attention process, each of the heads Q, K, V have the same\ndimensions NX C, where N = D \u00d7 H \u00d7 W is the length of the sequence, the self-\nattention is estimated as:\n$Attention(Q, K, V) = Softmax(\\frac{Q K^T}{\\sqrt{d_{head}}})V$.\n(1)\nWe utilize the same approach as in the original paper to simplify the self-attention\nmechanism from a complexity of O(N2) to O(2). In our experiments, we set R to\n[64, 16, 4, 1] from stage-1 to stage-4."}, {"title": "Mix-FFN.", "content": "We introduce Mix-FFN [2] which considers the effect of zero padding using a 3\u00d73\u00d73\nConv in the feed-forward network (FFN). Mix-FFN can be formulated as:\n$X_{out} = MLP(GELU(Conv_{3\\times3\\times3}(MLP(X_{in})))) + X_{in}$\n(2)\nwhere $x_{in}$ is the feature from the self-attention module. Mix-FFN mixes a 3\u00d73\u00d73\nconvolution and an MLP into each FFN"}, {"title": "3.2 Lightweight All-MLP Decoder", "content": "The All-MLP decoder consists of five main steps. First, multi-level features from the\nMiT encoder go through a MLP layer to unify the channel dimension. Then, in a\nsecond step, features are up-sampled to the original three-dimensional image and con-\ncatenated together. Third, a MLP layer is adopted to fuse the concatenated features.\nFourth, a convolutional layer is used with a kernel (D\u00d7 1 \u00d7 1) to reduce the image\ndimension to a bi-dimensional prediction. Finally, a two-dimensional convolutional"}, {"title": "4 Experimental and analysis", "content": "In order to test the proposed architecture, we utilize hyperspectral imaging, which\nrecords a detailed spectrum of light for each pixel and provides an invaluable source\nof information regarding the physical nature of the different materials, leading to the\npotential of a more accurate classification. Experiments were conducted firstly, on\ntwo of the most widely used public dataset - Indian Pines [13] and Pavia University\n[13] hyperspectral datasets - and, finally, on a hyperspectral image acquired from the\nPRISMA satellite. Here is an overview of the datasets."}, {"title": "4.1 Data and experimental environment description", "content": "The Indian Pines and Pavia University data sets consists of images with the absorption\nband caused by water mist removed. The Indian Pines image is obtained from an\nAVIRIS spectrometer in a forest area in Northwestern Indiana, USA. The image size\nis 145 \u00d7 145 pixels, the wavelength is 0.40 \u2013 2.50 \u00b5m, and the number of bands is\n200, the spatial resolution is 20 m. The Pavia University image is obtained by ROSIS\nspectrometer at the University of Pavia, Italy. The image size is 610 \u00d7 340 pixels, the\nwavelength is 0.43 \u2013 0.86 \u00b5m, the number of bands is 103, and the spatial resolution is\n1.3 m. The two datasets are divided into 16 categories and 9 categories respectively.\nTheir false-color images and ground-truth images are shown in Figs. 2 and 3. The\nground-truth classes and their respective samples number are shown in Table 1 and\nTable 2"}, {"title": "PRISMA dataset", "content": "PRISMA, a space-borne hyperspectral sensor developed by the Italian Space Agency\n(ASI), can capture images in a continuum of 231 spectral bands ranging between 400\nand 2500 nm, at a spatial resolution of 30 m. The method was applied to a PRISMA\nBottom-Of-Atmosphere (BOA) reflectance scene of the Quadril\u00e1tero Ferr\u00edfero area\n(Minas Gerias State, Brazil), located in the central portion of the Minas Gerais State\n(Northern Brazil).\nThe PRISMA hyperspectral image shows a reflectance scene at the Bottom-Of-\nAtmosphere (BOA) of the Quadril\u00e1tero Ferr\u00edfero mining district. It was acquired on\nMay 21, 2022, at 13:05:52 UTC.\nLevel 2C PRISMA hyperspectral imagery was accessed from the mission website\nhttp://prisma.asi.it/. The VNIR and SWIR datacubes were pre-processed using tools\navailable in ENVI 5.6.1 (L3Harris Technologies, USA). Errors in absolute geoloca-\ntion were corrected using the Refine RPCs (HDF-EOS5) task, included within the\nPRISMA Toolkit in ENVI. Only 193 out of 231 bands were used for the final image\ndue to atmospheric absorption affecting the excluded bands.\nThe ESA WorldCover-Map, available at https://esa-worldcover.org/en, was used in\nconjunction with the Global-scale mining polygons (Version 1) [14] to establish the\nground truth for the Brazilian region. In order to avoid redundant and noisy informa-\ntion, only the following features were selected: vegetation (including dense, sparse, and\nvery sparse vegetation as well as crops), water areas, mining areas, and build-up areas.\nAs it can be seen from Table 3, the vegetation class in the image is over-represented.\nTo address this issue, a random selection of 700,000 vegetation pixels are set to 0\n(undefined class). This adjustment ensures that the number of vegetation class pixels\nis comparable with the other classes. This is illustrated in Fig. 4."}, {"title": "4.2 Implementation details", "content": "We trained AMBER and made inferences using the IBiSCo Data Center (Infrastruc-\nture for Big Data and Scientific Computing). Information about the Data Center is\navailable at https://ibiscohpc-wiki.scope.unina.it/. Specifically, AMBER was trained\nusing four Tesla V100. Unlike the original SegFormer[2], training was performed from\nscratch without pre-training the encoder and randomly initializing the decoder. Dur-\ning the training phase, we applied random flipping to the patches and excluded\nthe undefined class from the computation of the loss function. We set crop size to\nD \u00d7 32 \u00d7 32 (where D is the spectral dimension of the datasets) on all the three\ndatasets. We trained the model using an SGD optimizer for all experiments. We used\na batch size of 3 for experiments on the Indian Pines dataset, 10 for those on the"}, {"title": "4.3 Evaluation metrics", "content": "In order to effectively assess the performance of this method in the experiment, three\nindicators, Overall Accuracy (OA), Kappa Coefficient (Kappa) and Avarage Accuracy\n(AA), are used to evaluate the classification performance of the model.\nOA is calculated as the sum of correctly classified pixels divided by the total number\nof pixels. The number of correctly classified pixels is found along the diagonal of the\nconfusion matrix, and the total number of pixels is equal to the total number of pixels\nof all real reference sources. The formula for OA is as follows:\n$OA = \\frac{\\sum_{i=1}^n h_{ii}}{N}  \\times 100/%$\n(3)\nWhere:\n\u2022\n$(h_{ii})$ is the number of correctly classified pixels distributed along the diagonal of\nthe confusion matrix.\n\u2022 N is the total number of samples.\n\u2022\nn is the number of categories.\nThe Kappa coefficient is a measure of consistency in testing. It is calculated by mul-\ntiplying the total number (N) of all real reference pixels by the sum of the diagonal\n($h_{kk}$) of the confusion matrix. Then, the number of real reference pixels in each cat-\negory and the classified pixels ($h_{ik}, h_{jk}$) are subtracted from the product of the total\nnumber. This result is divided by the square of the total number of pixels minus the\nproduct of the total number of true reference pixels in each category and the total\nnumber of classified pixels in the category. The kappa coefficient comprehensively con-\nsiders the various factors in the confusion matrix and provides a more comprehensive\nreflection of the accuracy of the overall classification. A larger value of the Kappa\ncoefficient indicates higher accuracy of the corresponding classification algorithm. The\ngeneral formula is as follows:\n$Kappa = \\frac{N\\sum_k h_{kk} \u2013 \\sum_k (\\sum_j h_{kj} \\sum_i h_{ik})}{N^2 \u2013 \\sum_k (\\sum_j h_{kj} \\sum_i h_{ik})}$\n(4)"}, {"title": "4.4 Data Preprocessing", "content": "We constructed the datasets using only random patches, unlike other papers that\nemploy various data balancing strategies (for example, see [15]). The purpose was\nboth to assess AMBER's ability to learn from an unbalanced dataset, and to create\na dataset with a consistent number of patches. The center of each patch is a pixel\nrandomly selected from pixels that are not located on the edges of the images. For the\nPavia University dataset and the Indian Pines dataset, the center is a pixel that does\nnot belong to the undefined class to avoid building patches with only undefined pixels.\nAfter creating the dataset, we addressed the well-known issue of pixel overlap between\nthe training and test sets (effect of the random patches strategy), which can lead to\ninformation leakage [16, 17]. To mitigate this, we designated 20% of the patches for\ntraining and 80% for testing for the Indian Pines and Pavia University datasets, and\n10% for training and 90% for testing for the PRISMA dataset.\nBefore the training phase, it is common to perform dimensionality reduction on the\nspectral dimension of hyperspectral images (e.g., PCA, SVD, or Gabor Filter [18, 19])\nto address information redundancy. Conversely, our strategy was to keep the patches\nwith the full spectral dimension, favoring a fine-grained classification, and leaving the\ntask of extracting relevant features to AMBER's encoder."}, {"title": "5 Results", "content": "To evaluate the effectiveness of this method in classifying hyperspectral remote sens-\ning images, we made a comparison with four mainstream methods: SVD/Unet [19],\nHSI-CNN [20], 3D-CNN [21] and contextual deep CNN [22]. It's important to note\nthat not all experiments involve pixel-by-pixel classification (or semantic segmenta-\ntion). Only SVD/Unet, contextual deep CNN, and AMBER use this approach. On\nthe other hand, HSI-CNN and 3D-CNN perform pure image classification, where the\nclasses are determined by the centroid of the patch. This results in an increase in the\nmetrics. Despite this, AMBER shows better results compared to the other methods\non the Indian Pines dataset and Pavia University dataset and comparable results on\nthe PRISMA dataset.\nIn the SVD/Unet method, the spectral dimension of the image is first reduced by a\nSingular Value Decomposition, considering only the first three singular values, and\nthen a Unet architecture is used to classify pixels. This method has shown promising"}, {"title": "5.1 Classification results", "content": "results in the semantic segmentation of PRISMA hyperspectral images.\nIn the HSI-CNN, the spectral-spatial features are first extracted from a target pixel\nand its neighbors. Next, several one-dimensional feature maps are obtained by per-\nforming a convolution operation on the spectral-spatial features, and these maps are\nthen stacked into a two-dimensional matrix. Finally, this two-dimensional matrix,\ntreated as an image, is input into a standard CNN.\nThe 3D-CNN architecture is a Multiscale 3D deep convolutional neural network (M3D-\nDCNN) which could jointly learn both 2D Multi-scale spatial feature and 1D spectral\nfeature from HSI data in an end-to-end approach.\nThe contextual deep CNN first concurrently applies multiple 3-dimensional local con-\nvolutional filters with different sizes jointly exploiting spatial and spectral features of\na hyperspectral image. The initial spatial and spectral feature maps obtained from\napplying the variable size convolutional filters are then combined together to form a\njoint spatio-spectral feature map. The joint feature map representing rich spectral and\nspatial properties of the hyperspectral image is then fed through fully convolutional\nlayers that eventually predict the corresponding label of each pixel vector.\nEvery neural network mentioned above was trained on the same patches using the\nconfiguration described in the corresponding papers [19] [20] [21] [22]. \"Same patches\u201d\nin this context refers to patches that have identical centroids. AMBER was trained\non patches of dimension D \u00d7 32 \u00d7 32 where D is the spectral dimension, as previously\nmentioned. SVD/UNet was trained on patches of 3 \u00d7 32 \u00d7 32 where 3 is the number\nof singular values. HSI-CNN was trained using patches of D \u00d7 3 \u00d7 3, 3D-CNN using\nD\u00d7 7 \u00d7 7 patches, and contextual deep CNN using D \u00d7 5\u00d75 patches (D is, again, the\nspectral dimension). For every method, we conducted 5 experiments for each dataset.\nThe classification accuracy of different methods on the three data sets is presented in\nTables 4, 5, and 6, with the highest values highlighted in bold font. A comprehensive\ncomparative analysis is presented to draw conclusions."}, {"title": "6 Conclusions", "content": "In this paper, we presented AMBER, an advanced SegFormer designed and fine tuned\nfor hyperspectral image segmentation. By incorporating three-dimensional convolu-\ntions and preserving spatial dimensions, AMBER effectively captures both local and\nglobal features in multi-band images. Our extensive experiments on the Indian Pines,\nPavia University, and PRISMA datasets show that AMBER in most cases outper-\nforms traditional CNN-based methods and other state-of-the-art models in terms of\nOverall Accuracy, Kappa coefficient, and Average Accuracy.\nThe results indicate that AMBER not only improves classification accuracy but\nalso exhibits robust generalization capabilities across different types of hyperspec-\ntral images. This demonstrates the potential of transformer-based architectures in\nadvancing the field of hyperspectral image analysis. Future work could explore further\noptimizations and applications of AMBER in various remote sensing tasks, potentially\nleveraging additional data augmentation techniques and transfer learning strategies to\nenhance performance even further. Additionally, AMBER could be applied to astro-\nnomical data to extend its utility and effectiveness in analyzing complex and diverse\ndatasets."}]}