{"title": "COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences", "authors": ["Yixin Liu", "Argyris Oikonomou", "Weiqiang Zheng", "Yang Cai", "Arman Cohan"], "abstract": "Many alignment methods, including reinforcement learning from human feedback (RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to capture the full range of general human preferences. To achieve robust alignment with general preferences, we model the alignment problem as a two-player zero-sum game, where the Nash equilibrium policy guarantees a 50% win rate against any competing policy. However, previous algorithms for finding the Nash policy either diverge or converge to a Nash policy in a modified game, even in a simple synthetic setting, thereby failing to maintain the 50% win rate guarantee against all other policies. We propose a meta-algorithm, Convergent Meta Alignment Algorithm (COMAL), for language model alignment with general preferences, inspired by convergent algorithms in game theory. Theoretically, we prove that our meta-algorithm converges to an exact Nash policy in the last iterate. Additionally, our meta-algorithm is simple and can be integrated with many existing methods designed for RLHF and preference optimization with minimal changes. Experimental results demonstrate the effectiveness of the proposed framework when combined with existing preference policy optimization methods.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have fundamentally transformed the fields of natural language processing and artificial intelligence. They excel in tasks ranging from text generation and translation to complex question answering and interactive dialogue systems. As these models become more integrated into daily life, a key challenge is ensuring they achieve high levels of alignment with human values and preferences.\nOne of the most widely adopted approaches to addressing this challenge is Reinforcement Learning from Human Feedback (RLHF) [Christiano et al., 2017, Ouyang et al., 2022]. This framework consists of two steps: first, learning a reward model from a dataset containing human preferences, and second, optimizing the LLM using the proximal policy optimization (PPO) algorithm [Schulman et al., 2017]. Recently, Rafailov et al. [2024] observed that the first step can be bypassed, proposing the direct preference optimization (DPO) algorithm, directly optimizing the LLM from the dataset."}, {"title": null, "content": "However, the aforementioned approaches crucially rely on the assumption that human preferences can be expressed using the Bradley-Terry (BT) model [Bradley and Terry, 1952]. Unfortunately, the BT model is too restrictive to capture the richness and complexity of human preferences. Specifically, the BT model can only induce transitive preferences \u2013 i.e., if more people favor A over B, and B over C, then more people must favor A over C. Such transitivity may not hold in the presence of diverse populations and is also incompatible with evidence from human decision-making [May, 1954, Tversky, 1969].\nTo overcome this limitation, recent research has begun to explore alignment under general preferences. Munos et al. [2024] formulate this alignment problem as a symmetric two-player zero-sum game, where both players' strategies are LLMs, and their payoffs are determined by the win rate against the opponent's LLM according to the preference model. The objective is to identify a Nash equilibrium policy that guarantees at least a 50% win rate against any other policy [Munos et al., 2024, Swamy et al., 2024, Azar et al., 2024, Calandriello et al., 2024], a property we refer to as robust alignment. However, all the proposed algorithms either diverge or converge to the Nash policy of a modified game, thereby failing to maintain the 50% win rate guarantee against all other policies."}, {"title": null, "content": "Our Contribution. We introduce a novel meta-algorithm, Convergent Meta Alignment Algorithm (COMAL), inspired by the conceptual prox-method, a convergent algorithm for solving two-player zero-sum games [Nemirovski, 2004]. Our first observation is that many existing algorithms, including PPO [Schulman et al., 2017], DPO [Rafailov et al., 2024], IPO [Azar et al., 2024], SPPO [Wu et al., 2024], REBEL [Gao et al., 2024], DRO [Richemond et al., 2024], INPO [Zhang et al., 2024], etc., can be interpreted as implementations of the Prox operator [Nemirovski, 2004]. COMAL employs the Prox operator as its fundamental building block and provably converges to the Nash equilibrium policy in the last iterate, assuming the Prox operator can be computed exactly, thus achieving robust alignment. This approach allows us to leverage many existing algorithms in a black-box manner. While several algorithms, e.g., IPO, SPPO, etc., in the literature demonstrate average-iterate convergence to the Nash equilibrium policy, they all diverge in the last iterate. Unfortunately, iterate averaging can be cumbersome, particularly when deep-learning components are involved, as it may not be feasible to average the outputs of LLMs. For the more desirable last-iterate convergence [Munos et al., 2024, Zhang et al., 2024], existing algorithms only guarantee convergence to a KL-regularized Nash equilibrium, which does not have the robust alignment property. Compared to these algorithms, COMAL is the first to provably converge to a Nash equilibrium policy in the last iterate, thus guaranteeing robust alignment.\nIn addition to our theoretical analysis, we validate the effectiveness of COMAL through both synthetic and LLM-based experiments."}, {"title": null, "content": "Synthetic experiments. We construct a 3 \u00d7 3 two-player zero-sum preference game and compare COMAL with a wide range of algorithms proposed in the literature. The result clearly shows that COMAL is the only algorithm that converges to the Nash equilibrium of the game in the last iterate.\nLLM-based experiments. Furthermore, we evaluate the performance of COMAL against existing preference optimization algorithms under a practical setting, where a pre-trained LLM, Qwen2-1.5B [Yang et al., 2024] is fine-tuned using different algorithms on the UltraFeedback [Cui et al., 2023] dataset, which is commonly used for alignment fine-tuning of LLMs. We run iterative algorithms up to 42 iterations and compare both the best and the last checkpoints. Our experimental results demonstrate the advantages of"}, {"title": "2 Background", "content": "We use \\( \\Delta(Z) \\) to denote a distribution over a set \\( Z \\). We denote \\( x \\in \\mathcal{X} \\) as an instruction where \\( \\mathcal{X} \\) is the instruction set. We assume a fixed distribution \\( \\rho \\in \\Delta(\\mathcal{X}) \\) over the instruction set. We denote \\( \\mathcal{Y} \\) as the response set and \\( y \\in \\mathcal{Y} \\) as one response. Given any instruction \\( x \\in \\mathcal{X} \\), an LLM policy \\( \\pi \\) specifies the output distribution \\( \\pi(\\cdot | x) \\in \\Delta(\\mathcal{Y}) \\). For distributions \\( p, q \\in \\Delta(Z) \\), the Kullback-Leibler (KL) divergence is defined as \\( \\text{KL}(p||q) := \\sum_{z \\in Z} p(z) \\log \\frac{p(z)}{q(z)} \\). The sigmoid function is \\( \\sigma(x) := \\frac{e^x}{1+e^x} \\). We use \\( \\text{supp}(p) \\) to denote the support of a distribution \\( p \\).\nPreference Models In this paper, we focus on general preference models."}, {"title": "Definition 1 (General Preference Model).", "content": "A general preference model \\( \\mathcal{P} : \\mathcal{X} \\times \\mathcal{Y} \\times \\mathcal{Y} \\rightarrow [0, 1] \\) satisfies\n\\[\\mathcal{P}(y_1 > y_2 | x) = 1 - \\mathcal{P}(y_2 > y_1 | x).\\]\nWhen we query \\( \\mathcal{P} \\) with \\( (x, y_1, y_2) \\), it outputs 1 with probability \\( \\mathcal{P}(y_1 > y_2 | x) \\) meaning \\( y_1 \\) is preferred over \\( y_2 \\), and it outputs 0 otherwise."}, {"title": null, "content": "We define \\( \\mathcal{P}(\\pi_1 > \\pi_2) := \\mathbb{E}_{x \\sim \\rho}[\\mathbb{E}_{y_1 \\sim \\pi_1, y_2 \\sim \\pi_2} [\\mathcal{P}(y_1 > y_2 | x)]] \\) as the win rate of \\( \\pi_1 \\) over \\( \\pi_2 \\) under preference model \\( \\mathcal{P} \\). We denote the preference distribution \\( \\Lambda_{\\mathcal{P}}(y, y') \\) as a binary distribution:\n\\[\\Lambda_{\\mathcal{P}}(y, y') = \\begin{cases}\n    (y, y') & \\text{with probability } \\mathcal{P}[y > y'] \\\\\n    (y', y) & \\text{with probability } 1 - \\mathcal{P}[y > y']\n\\end{cases} \\tag{1}\\]\nA special case of the general preference model is the Bradley-Terry (BT) model, which assumes a reward function parameterizes the preference."}, {"title": "Definition 2 (Bradley-Terry Model).", "content": "A preference model \\( \\mathcal{P} \\) satisfies the Bradley-Terry (BT) assumption if there exists a reward function \\( r^* : \\mathcal{X} \\times \\mathcal{Y} \\rightarrow \\mathbb{R} \\) such that\n\\[\\mathcal{P}(y_1 > y_2 | x) = \\sigma(r^*(x, y_1) - r^*(x,y_2)) = \\frac{\\exp (r^*(x, y_1))}{\\exp (r^*(x, y_1)) + \\exp (r^*(x, y_2))}.\\]"}, {"title": "2.1 Alignment under the Bradley-Terry Model Assumption", "content": "RLHF The canonical formulation of Reinforcement Learning from Human Feedback (RLHF) is to first learn a reward function \\( r \\) under the BT model and then find the optimal KL regularized policy \\( \\pi^* \\) with respect to the learned reward function \\( r \\):\n\\[\\pi^* := \\arg \\max_{\\pi} \\mathbb{E}_{x \\sim \\rho, y \\sim \\pi(\\cdot|x)} [r(x, y) - \\eta^{-1} \\text{KL}(\\pi(\\cdot | x)||\\tau_{\\text{ref}}(\\cdot | x))], \\tag{2}\\]\nwhere \\( \\eta^{-1} > 0 \\) controls the regularization, and \\( \\tau_{\\text{ref}} \\) is the initial reference model, usually the policy \\( \\pi_{\\text{SFT}} \\) obtained from pre-training and supervised fine-tuning."}, {"title": "2.2 Robust Alignment with General Preference Models", "content": "The BT model assumption is insufficient to capture the full range of general human preferences [Munos et al., 2024, Swamy et al., 2024]. To achieve robust alignment with general preferences, we model the policy optimization problem as a two-player zero-sum game with the objective function as follows:\n\\[J(\\pi_1, \\pi_2) := \\mathcal{P}(\\pi_1 > \\pi_2) = \\frac{1}{2} \\mathbb{E}_{x \\sim \\rho}[\\mathbb{E}_{y_1 \\sim \\pi_1, y_2 \\sim \\pi_2} [\\mathcal{P}(y_1 > y_2 | x)]] - \\frac{1}{2}. \\tag{4}\\]\nIn this game, the max-player controls \\( \\pi_1 \\) and tries to maximize \\( J(\\pi_1, \\pi_2) \\) while the min-player controls \\( \\pi_2 \\) and tries to minimize \\( J(\\pi_1, \\pi_2) \\). We focus only on policies with \\( \\Pi := {\\pi : \\text{supp}(\\pi) \\subseteq \\text{supp}(\\pi_{\\text{SFT}})} \\) in the support of the initial SFT policy. A Nash equilibrium policy \\( (\\pi^*, \\pi^*) \\) satisfies\n\\[\\pi^*, \\pi^* \\in \\arg \\max_{\\pi_1 \\in \\Pi} \\arg \\min_{\\pi_2 \\in \\Pi} J(\\pi_1, \\pi_2), \\quad J(\\pi^*, \\pi^*) \\leq J(\\pi_1, \\pi^*) \\leq J(\\pi_1, \\pi_2), \\forall \\pi_1, \\pi_2 \\in \\Pi.\\]\nSince \\( J(\\pi_1, \\pi_2) \\) is symmetric, the game has a symmetric Nash equilibrium \\( (\\pi^*, \\pi^*) \\). Moreover, the Nash equilibrium policy \\( \\pi^* \\) guarantees that for any other policy \\( \\pi \\), its win rate is at least \\( \\mathcal{P}(\\pi^* > \\pi) > \\mathcal{P}(\\pi^* > \\pi^*) = 50\\% \\). We call this property robust alignment. Our goal is to find a policy with robust alignment.\nExisting online iterative preference optimization methods designed for or applicable to the original game, including iterative IPO [Azar et al., 2024] and SPPO [Wu et al., 2024], are based on Multiplicative Weights Update (MWU, definition in Section 3.2), and thus diverge in the last iterate as we show in Section 4.4 There is also a line of works including Nash-MD [Munos et al., 2024, Ye et al., 2024], Online IPO [Calandriello et al., 2024], INPO [Zhang et al., 2024] aim to find the Nash equilibrium of a modified KL-regularized game:\n\\[\\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) := J(\\pi_1, \\pi_2) - \\tau \\mathbb{E}_{x \\sim \\rho}[\\text{KL}(\\pi_1(\\cdot | x)||\\pi_{\\text{ref}}(\\cdot | x))] + \\tau \\mathbb{E}_{x \\sim \\rho}[\\text{KL}(\\pi_2(\\cdot | x)||\\pi_{\\text{ref}}(\\cdot | x))]. \\tag{5}\\]\nThe additional KL regularization terms in the objective are introduced for training stability. However, the Nash equilibrium of the modified game no longer achieves robust alignment, i.e., it has a win rate of at least 50% against any competing policy. We present comparison of these algorithms in Table 1.\nMoreover, most existing theoretical convergence guarantees only hold for the average iterate, i.e., the uniform mixture of training iterates, which is not used in practice. We focus on designing algorithms with provable last-iterate convergence to Nash equilibrium, which aligns with practice and is more space-efficient [Munos et al., 2024].\nAs we show in the next section, our meta-algorithm COMAL can also be implemented with black-box access to algorithms that solve the regularized game \\( \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\)."}, {"title": "3 A Convergent Meta-Algorithm for Alignment", "content": "We propose a simple meta-algorithm, Convergent Meta Alignment Algorithm (COMAL, Algorithm 1), for robustly aligning LLMs with general preferences by solving the unregularized game \\( J(\\pi_1, \\pi_2) \\) (4). In Section 3.1 and 3.2, we present the theoretical foundations of COMAL and analyze its convergence properties. Section 3.3 describes its practical implementation that integrates COMAL with existing preference learning methods."}, {"title": "3.1 COMAL", "content": "COMAL (Algorithm 1) is an online iterative algorithm inspired by the classic conceptual prox-method [Nemirovski, 2004] first introduced in the optimization theory community. This method has recently been applied to finding a Nash equilibrium in zero-sum games [Perolat et al., 2021, Abe et al., 2024] and has had notable success in training advanced game AI models [Perolat et al., 2022]."}, {"title": null, "content": "Algorithm 1: Convergent Meta Alignment Algorithm (COMAL) for solving \\( J(\\pi_1, \\pi_2) \\) (4)\nInput: Initial policy \\( \\pi_{\\text{sft}} \\), preference oracle \\( \\mathcal{P} \\), regularization \\( \\tau > 0 \\), number of iterations \\( T > 1 \\)\nOutput: Optimized policy \\( \\pi^T \\)\nInitialize \\( \\pi^1, \\pi_{\\text{ref}} \\leftarrow \\pi_{\\text{sft}} \\)\nfor \\( t = 1, 2, ..., T - 1 \\) do\n\\( \\pi^{t+1} \\leftarrow \\arg \\max_{\\pi_1} \\min_{\\pi_2} \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\) using Algorithm 2 (discussed in Section 3.2)\n\\( \\pi_{\\text{ref}} \\leftarrow \\pi^{t+1} \\)\nreturn \\( \\pi^T \\)"}, {"title": null, "content": "Update Rule of COMAL In each iteration \\( t \\), COMAL uses a regularized game solver (Algorithm 2) to update the next-iteration policy \\( \\pi^{t+1} \\) as the Nash equilibrium policy of a regularized game \\( \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\) using the current policy as reference \\( \\pi_{\\text{ref}} = \\pi^t \\). We defer further discussion of Algorithm 2 to Section 3.2 for clarity. The rationale behind COMAL is simple: update the reference policy when no further progress can be made, which occurs when the algorithm reaches the Nash equilibrium of the regularized game."}, {"title": "3.2 Solving a Regularized Game", "content": "We present Mirror Descent (MD) in Algorithm 2 to compute a Nash equilibrium of the regularized game \\( \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\). MD uses the prox operator as building blocks and we later show how to implement the prox operator using existing policy optimization algorithms. For simplicity, we consider policy \\( \\pi \\in \\Delta(\\mathcal{V}) \\) and omit the dependence on the instruction \\( x \\). All discussions can be extended to the contextual setting in a straightforward way."}, {"title": null, "content": "Algorithm 2: Regularized game solver for \\( \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\) -- \\( \\arg \\max_{\\pi_1} \\min_{\\pi_2} \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\)\nInput: Reference policy \\( \\pi_{\\text{ref}} \\), preference oracle \\( \\mathcal{P} \\), regularization \\( \\tau > 0 \\), step size \\( \\eta > 0 \\), number of iterations \\( K > 1 \\)\nOutput: Regularized Nash equilibrium policy \\( \\mu_K \\)\nInitialize \\( \\mu^1 \\leftarrow \\pi_{\\text{ref}} \\)\nfor \\( k = 1, 2, ..., K - 1 \\) do\n\\( g \\leftarrow \\nabla_{\\mu} (\\mathcal{P}(\\mu > \\mu^k) - \\tau \\text{KL}(\\mu||\\pi_{\\text{ref}})) = \\mathcal{P}(\\cdot > \\mu^k) - \\tau (\\log \\frac{\\mu}{\\pi_{\\text{ref}}} + 1) \\)\n\\( \\mu^{k+1} \\leftarrow \\text{Prox}(\\mu^k, \\eta g) \\)\nreturn \\( \\mu_K \\)"}, {"title": null, "content": "Mirror Descent and Multiplicative Weights Update Mirror Descent (MD) is a classical family of optimization algorithms [Nemirovskij and Yudin, 1983]. An important member of this family is the Multiplicative Weights Update (MWU) algorithm [Arora et al., 2012], which is MD with negative entropy regularization. For a maximization problem \\( \\max_{\\pi} f(\\pi) \\), given an existing policy \\( \\pi^t \\), MWU computes the update \\( \\pi^{t+1} \\) as follows:\n\\[\\pi^{t+1} := \\arg \\max_{\\pi} (\\nabla f(\\pi^t), \\pi) - \\eta^{-1} \\cdot \\text{KL}(\\pi||\\pi^t). \\tag{6}\\]"}, {"title": null, "content": "Note that RLHF in (2) is equivalent to MWU if we interpret \\( f(\\pi) \\) as the expected reward under \\( \\pi \\) \\( \\mathbb{E}_{y \\sim \\pi}[r(y)] \\), and the gradient \\( \\nabla f \\) corresponds directly to \\( r \\).\nWe note that the update rule of MWU can be succinctly expressed using the prox operator as shown in Algorithm 2. Therefore, our analysis will consider the general case of the Prox Operator."}, {"title": "Prox operator.", "content": "Fix a 1-strongly convex function \\( \\phi : \\mathcal{Z} \\rightarrow \\mathbb{R} \\) over a closed convex set \\( \\mathcal{Z} \\subset \\mathbb{R}^n \\). The Bregman divergence induced by \\( \\phi \\) is\n\\[\\begin{aligned}\nD_{\\phi}(\\cdot|\\cdot) : \\mathcal{Z} \\times \\mathcal{Z} &\\rightarrow \\mathbb{R}_{\\geq 0},\\\\\nD_{\\phi}(z||z') := \\phi(z) - \\phi(z') - \\langle \\nabla \\phi(z'), z - z' \\rangle.\n\\end{aligned}\\]\nGiven a reference point \\( z \\in \\mathcal{Z} \\) and a vector \\( g \\in \\mathbb{R}^n \\), the prox operator \\( \\text{Prox}(z, g) \\) generalizes the notion of a gradient ascent step from \\( z \\) in the direction of \\( g \\)."}, {"title": "Definition 3 (Prox Operator).", "content": "For a strongly convex regularizer \\( \\phi \\), the prox operator is defined as\n\\[\\text{Prox}(z, g) := \\arg \\max_{z'} \\langle g, z' \\rangle - D_{\\phi}(z'||z) = \\arg \\max_{z'} \\langle g + \\nabla \\phi(z'), z' \\rangle - \\phi(z').\\]"}, {"title": null, "content": "When \\( \\phi(z) = ||z||_2^2 \\) is the \\( \\ell_2 \\) regularizer, the prox operator \\( \\text{Prox}(z, g) = \\Pi_{\\mathcal{Z}}[z + g] \\) is the exactly the projected gradient ascent step. In this paper, without additional notes, we choose \\( \\phi = \\sum_{i=1}^n z[i] \\ln z[i] \\) as the negative entropy regularizer and the corresponding Bregman divergence \\( D_{\\phi} \\) is the KL divergence. The update rule of MWU in (6) is equivalent to \\( \\pi^{t+1} = \\text{Prox}(\\pi^t, \\eta \\nabla f(\\pi^t)) \\)."}, {"title": "Exponentially Fast Convergence", "content": "Denote \\( \\pi_{\\tau}^* \\) the Nash equilibrium of the KL regularized game \\( \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\), which is \\( \\tau \\)-strongly monotone. We can apply existing results to show that MWU (Algorithm 2) achieves linear last-iterate convergence rate: the KL divergence to the Nash equilibrium \\( \\pi_{\\tau}^* \\) decreases exponentially fast."}, {"title": "3.3 Practical methods for computing the prox operator", "content": "We show how to implement COMAL in practical large-scale applications like LLM alignment by computing the prox operator. Specifically, we observe that many existing algorithms designed for RLHF and preference optimization with neural network parameters can be adapted to solve the prox operator \\( \\text{Prox}(\\pi, \\eta g) \\) (\\( \\eta > 0 \\) is the step size). These algorithms include RL algorithms like PPO [Schulman et al., 2017] and loss-minimization algorithms like, DPO [Rafailov et al., 2024], IPO [Azar et al., 2024], SPPO [Wu et al., 2024], REBEL [Gao et al., 2024], DRO [Richemond et al., 2024], INPO [Zhang et al., 2024]. Each of them may be preferred in certain settings. Due to space limit, we only present IPO and INPO here but defer discussion of other methods to Appendix D.\nOur contribution here is not proposing new algorithms but unifying existing diverse preference methods through the perspective of computing the prox operator. This perspective opens the possibility of applying other algorithms from online learning and optimization to robust LLM alignment. We include implementations for two other last-iterate convergent algorithms, the Mirror-Prox algorithm [Nemirovski, 2004] and the Optimistic Multiplicative Weights Update algorithm [Rakhlin and Sridharan, 2013, Syrgkanis et al., 2015], in Appendix E."}, {"title": "IPO for computing Prox for unregularized preferences", "content": "Before we provide the a practical implementation of Algorithm 2, we first show that the IPO loss could be used to solve \\( \\pi_{\\theta} = \\text{Prox}(\\pi, \\eta g_{\\mu}) \\) where \\( g \\) is the unregularized win-rate against a reference policy \\( \\mu \\) such that \\( g_{\\mu}(y) = \\mathcal{P}[y > \\mu] := \\mathbb{E}_{y' \\sim \\mu}[\\mathcal{P}[y > y']] \\). Given a dataset of win-lose pairs sampled from \\( \\mu \\): \\( {y_w, y_l \\sim \\mu} \\), the (population) IPO loss [Azar et al., 2024] is\n\\[\\ell_{\\text{IPO}}(\\theta) := \\mathbb{E}_{(y_w,y_l) \\sim \\mu} [ \\mathbb{E}_{(y^+,y^-) \\sim (y_w,y_l)} \\left( \\log \\frac{\\pi_{\\theta}(y^+)}{\\pi_{\\mu}(y^+)} - \\frac{\\eta}{2} \\right) ].\\]"}, {"title": null, "content": "Azar et al. [2024] have shown that the minimizer of the \\( \\ell_{\\text{IPO}}(\\theta, \\mu) \\) satisfies\n\\[\\pi_{\\theta}(y) \\propto \\pi(y) \\exp (-\\eta \\mathcal{P}[y > \\mu]) \\Leftrightarrow \\pi_{\\theta} = \\text{Prox}(\\pi, \\eta g_{\\mu}).\\]\nThus we can compute the prox operator \\( \\text{Prox}(\\pi, \\eta g_{\\mu}) \\) where \\( g_{\\mu} = \\mathcal{P}(\\cdot > \\mu) \\) by minimizing the IPO loss against policy \\( \\mu \\).\nINPO for computing Prox for regularized preferences The Iterative Nash Policy Optimization (INPO) loss [Zhang et al., 2024] is a generalization of the IPO loss to the regularized preference setting. We show that INPO could be used to compute \\( \\text{Prox}(\\mu, \\eta g_{\\tau}) \\), where \\( g_{\\tau} := \\nabla_{\\pi} J_{\\tau}(\\pi, \\mu, \\pi_{\\text{ref}}) = \\mathcal{P}(\\cdot > \\mu) - \\tau \\log \\frac{\\pi}{\\pi_{\\text{ref}}} \\) is the gradient of the regularized objective (5). Given a win-loss pair data set \\( {y_w, y_l \\sim \\mu} \\), the INPO loss is\n\\[\\ell_{\\text{INPO}}(\\pi) := \\mathbb{E}_{(y_w,y_l) \\sim \\mu} [ \\mathbb{E}_{(y^+,y^-) \\sim \\Lambda_{\\mathcal{P}}(y_w,y_l)} \\left( \\log \\frac{\\pi(y^+)}{\\mu(y^+)} - \\eta \\tau \\log \\frac{\\pi_{\\text{ref}}(y^+)}{\\mu(y^+)} - (1 - \\eta \\tau) \\log \\frac{\\pi}{\\mu(y^-)} - \\frac{\\eta}{2} \\right) ].\\]"}, {"title": null, "content": "It has been proved that the minimizer of the INPO loss is \\( \\text{Prox}(\\mu, \\eta g_{\\tau}) \\) [Zhang et al., 2024]. Thus we can use INPO in Algorithm 2 as a regularized game solver, as we show in Algorithm 3.\nAlgorithm 3: INPO [Zhang et al., 2024] for solving \\( \\mathcal{J}_{\\tau}(\\pi_1, \\pi_2, \\pi_{\\text{ref}}) \\)\nInput: Reference policy \\( \\pi_{\\text{ref}} \\), regularization \\( \\tau > 0 \\), step size \\( \\eta > 0 \\), number of rounds \\( K > 1 \\), preference oracle \\( \\mathcal{P} \\).\nOutput: Approximate regularized Nash equilibrium policy \\( \\mu^K \\)\nInitialize \\( \\mu^1 \\leftarrow \\pi_{\\text{ref}} \\)\nfor \\( k = 1, 2, ..., K - 1 \\) do\nGenerate response pairs \\( {y_1^{(i)}, y_2^{(i)} \\} \\), where \\( y_1, y_2 \\sim \\mu^k \\)\nQuery preference oracle \\( \\mathcal{P} \\) to get preference data \\( \\mathcal{D}_k = \\{y_w^{(i)}, y_l^{(i)} \\} \\)\nCompute \\( \\mu^{k+1} = \\arg \\min_{\\pi \\in \\Pi} \\mathbb{E}_{(y_w,y_l) \\sim \\mathcal{D}_k} \\ell_{\\text{INPO}}(\\pi) \\) where\n\\[\\ell_{\\text{INPO}}(\\pi) := \\mathbb{E}_{(y_w,y_l) \\sim \\Lambda_{\\mathcal{P}}(y_w,y_l)} \\left[ \\log \\frac{\\pi(y^+)}{\\mu(y^+)} - \\eta \\tau \\log \\frac{\\pi_{\\text{ref}}(y^+)}{\\mu(y^+)} - (1 - \\eta \\tau) \\log \\frac{\\pi}{\\mu(y^-)} - \\frac{\\eta}{2} \\right] \\]\nreturn \\( \\mu^K \\)"}, {"title": null, "content": "Practical Implementation of COMAL We present an implementation of COMAL in Algorithm 4 using the INPO [Zhang et al., 2024] as a subgame solver. We remark that COMAL can also be implemented using PPO or many other preference learning algorithms, as we show in Section 3.3 and Appendix D. Given the implementation of these existing methods, our meta-algorithm requires minimal change but achieves last-iterate convergence to a Nash equilibrium."}, {"title": "4 Synthetic Experiments", "content": "We conduct experiments on a simple bandit problem with \\( y = {y_a, y_b, y_c} \\) and non-BT preference model over \\( \\mathcal{Y} \\). Specifically, we set \\( \\mathcal{P}[y_b > y_a] = \\mathcal{P}[y_c > y_b] = 0.9 \\) and \\( \\mathcal{P}[y_a > y_c] = 0.8 \\). Observe that the preference is intransitive and exhibits a preference cycle \\( y_c > y_b > y_a > y_c \\)."}, {"title": "Experiments using noiseless gradient", "content": "We present numerical results of mirror-descent (MD) algorithms (equivalent to MWU) and COMAL (Algorithm 1) in Figure 1. We can see that the MD algorithm diverges from the unique Nash equilibrium and suffers a large equilibrium gap, while COMAL achieves fast last-iterate convergence to the Nash equilibrium, aligned with our theoretical results."}, {"title": "Experiements using preference samples", "content": "Since the popular iterative DPO algorithm does not contain a gradient step, we also conduct experiments with only Oracle query access to the preference model. We compare the performance of various algorithms, including iterative DPO, iterative IPO, SPPO, and INPO and present results in Figure 2. The sample-only setting is also more aligned with what happens in practice. We use a sufficient number of samples in each iteration for every algorithm. As a result, the COMAL performs the same as in the noiseless gradient setting, while the iterative IPO algorithm becomes equivalent to the MD algorithm."}, {"title": "Iterative DPO:", "content": ""}]}