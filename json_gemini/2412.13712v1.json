{"title": "An Algebraic Notion of Conditional Independence, and Its Application to Knowledge Representation (full version)", "authors": ["Jesse Heyninck"], "abstract": "Conditional independence is a crucial concept supporting adequate modelling and efficient reasoning in probabilistics. In knowledge representation, the idea of conditional independence has also been introduced for specific formalisms, such as propositional logic and belief revision. In this paper, the notion of conditional independence is studied in the algebraic framework of approximation fixpoint theory. This gives a language-independent account of conditional independence that can be straightforwardly applied to any logic with fixpoint semantics. It is shown how this notion allows to reduce global reasoning to parallel instances of local reasoning, leading to fixed-parameter tractability results. Furthermore, relations to existing notions of conditional independence are discussed and the framework is applied to normal logic programming.", "sections": [{"title": "Introduction", "content": "Over the last decades, conditional independence was shown to be a crucial concept supporting adequate modelling and efficient reasoning in probabilistics (Pearl, Geiger, and Verma, 1989). It is the fundamental concept underlying network-based reasoning in probabilistics, which has been arguably one of the most important factors in the rise of contemporary artificial intelligence. Even though many reasoning tasks on the basis of probabilistic information have a high worst-case complexity due to their semantic nature, network-based models allow an efficient computation of many concrete instances of these reasoning tasks thanks to local reasoning techniques. Therefore, conditional independence has also been investigated for several approaches in knowledge representation, such as propositional logic (Darwiche, 1997; Lang, Liberatore, and Marquis, 2002), belief revision (Kern-Isberner, Heyninck, and Beierle, 2022; Lynn, Delgrande, and Peppas, 2022) and conditional logics (Heyninck et al., 2023). For many other central formalisms in KR, such a study has not yet been undertaken.\nDue to the wide variety of formalisms studied in knowledge representation, it is often beneficial yet challenging to study a concept in a language-independent manner. Indeed, such language-independent studies avoid having to define and investigate the same concept for different formalisms. In recent years, a promising framework for such language-independent investigations is the algebraic approximation fixpoint theory (AFT) Denecker, Marek, and Truszczy\u0144ski (2003), which conceives of KR-formalisms as operators over a lattice (such as the immediate consequence operator from logic programming). AFT can represent a wide variety of KR-formalisms; such as propositional logic programming Denecker, Bruynooghe, and Vennekens (2012); Heyninck (2024); Heyninck and Bogaerts (2023a), default logic Denecker, Marek, and Truszczy\u0144ski (2003), autoepistemic logic Denecker, Marek, and Truszczy\u0144ski"}, {"title": "Background and Preliminaries", "content": "In this section, we recall the necessary basics of logic programming, abstract algebra, and AFT. We refer to the literature for more detailed introductions on logic programming semantics based on four-valued operators (Fitting, 1991; Denecker, Bruynooghe, and Vennekens, 2012), order theory Davey and Priestley (2002) and AFT Bogaerts (2015)."}, {"title": "Logic Programming", "content": "We assume a set of atoms A and a language L built up from atoms, conjunction A and negation not. A (propositional normal) logic program P (a nlp, for short) is a finite set of rules of the form\np \u2190 p1 \u2227... pn not q1\u06f8\u06f0\u06f0\u06f0\u06f8 not qm, where p, p1,..., pn, q1,..., qm are atoms that may include the propositional constants T (representing truth) and I (falsity). A rule is positive if there are no negations in the rule's bodies, and a program is positive if so are all its rules. We use the four-valued bilattice consisting of truth values U, F, T and C ordered by \u2264i by: U \u2264i F\u2264i C and U \u2264i T \u2264i C,\nand ordered by \u2264t by: F <t C\u2264t T and F <t U \u2264t T.\nWe also assume a \u2264t-involution on \u2264t (i.e, F = T, \u2212T = F, \u2212U = U and -C = C). A four-valued interpretation of a program P is a pair (x, y), where x \u2286 Ap is the set of the atoms that are assigned a value in {T, C} and y \u2286 Ap is the set of atoms assigned a value in {T, U}. Somewhat skipping ahead to section 2.2, the intuition here is that x (y) is a lower (upper) approximation of the true atoms. Interpretations are compared by the information order \u2264i, where (x, y) \u2264i (w, z) iff x \u2286 w and zy (sometimes called \u201cprecision\" order), and by the truth order \u2264t, where (x, y) \u2264t (w, z) iff x \u2286 w and y\u2286 z (increased \u2018positive' evaluations). Truth assignments to complex formulas are then recursively defined as follows:"}, {"title": "Lattices and sub-lattices", "content": "We recall some necessary preliminaries on set theory and (sub-)lattices. A lattice is a partially ordered set L = (L, \u2264) where every two elements x, y \u2208 L have a least upper xy and a greatest lower bound \u0445\u0443. We will often also refer to a lattice (L, \u2264) by its set of elements L. A lattice is complete if every set X C L has a least upper (denoted || X) and a greatest lower bound (denoted \u220fX). (24, \u2286) is an example of a complete lattice. x is a fixpoint of O if x = O(x), and the least fixpoint of O is denoted lfp(O).\nWe now provide background on how to (de)compose lattices into sub-lattices, following Vennekens, Gilis, and Denecker (2006). Let I be a set, which we call the index set, and for each i \u2208 I, let Li be a set. The product setiel Li is the following set of functions:\n\u2297i\u2208I Li = { f | f : I \u2192 \u222a Li s.t. \u2200i \u2208 I : f(i) \u2208 Li}.\nThe product set iei Li contains all ways of selecting one element of every set Li. E.g. for the sets L\u2081 = {0, {p}} and L2 = {0, {q}}, i\u2208 {1,2} Li contains, among others, f and f' with f(1) = f(2) = () and f'(1) = \u00d8 and f\u2032(2) = {q}. For finite I = {1, ..., n}, the productiel Li is (isomorphic to) the cartesian product L\u2081 \u00d7 ... \u00d7 Ln. We will also denote i\u2208 {1,2} Li by L1 \u25ca L2 to avoid clutter.\nIf each Lj is partially ordered by some \u2264j, this induces the product order \u2264\u00ae on \u2297j\u22081 Lj: for all x, y \u2208 \u2297j\u22081 Lj, x \u2264\u00ae y iff for all j \u2208 I, x(j) \u2264j y(j). We will sometimes denote the product"}, {"title": "Approximation Fixpoint Theory", "content": "We recall basic notions from approximation fixpoint theory (AFT) by Denecker, Marek, and Truszczy\u0144ski (2000).\nGiven a lattice L = (L, \u2264), we let L\u00b2 = (C2, <i, \u2264t) be the structure (called bilattice), in which L2 = L \u00d7 L, and for every x1, Y1, X2, Y2 \u2208 L,\n\u2022 (X1,Y1) \u2264i (X2,Y2) if x1 \u2264 x2 and Y1 \u2265 Y2,\n\u2022 (X1,Y1) \u2264t (X2,Y2) if x1 \u2264 Xx2 and y1 \u2264 Y2.\nAn approximating operator O : L2 \u2192 C\u00b2 of an operator O : L \u2192 L is an operator that maps every approximation (x, y) of an element z to an approximation (x', y') of another element O(z), thus approximating the behavior of the approximated operator O. As an example, ICp is an approximation operator of ICP.\nDefinition 2. Let Oc : L \u2192 L and O : L\u00b2 \u2192 L\u00b2. (1) O is \u2264i-monotonic, if when (x1,Y1) \u2264i (X2,Y2), also O(X1,Y1) \u2264i O(X2,Y2); (2) O is approximating, if it is \u2264i-monotonic and for any x \u2208 L, (O(x,x))1 = (O(x,x))2;1 (3) O is an approximation of Or, if it is \u2264i-monotonic and O extends O, that is: (O(x,x))\u2081 = (O(x,x))2 = \u039f\u00a3(x).\nTo avoid clutter, we denote (O(x, y))1 by O\u0131(x, y) and (O(x, y))2 by Ou(x, y) (for lower and upper bound).\nThe stable operator, defined next, is used for expressing the semantics of many non-monotonic formalisms.\nDefinition 3. Given a complete lattice L = (L, \u2264), let O : L\u00b2 \u2192 L2 be an approximating operator. \u039f\u03b9(\u00b7,y) = \u03bb\u03b1.\u039f\u03b9(x,y), i.e.: \u039f\u03b9(\u00b7,y)(x) = \u039f\u03b9(x,y) (and similarly for the upper bound operator Ou).\nThe stable operator for O is: S(O)(x,y) = (lfp(Or(., y)), lfp(Ou(x, .)).\nThe components lfp(Or(., y)) and lfp(Ou(x, .) of S(O) will be denoted by C(O\u2081)(y) respectively C(Ou)(x).\nStable operators capture the idea of minimizing truth, since for any \u2264i-monotonic operator O on L2, fixpoints of the stable operator S(O) are \u2264t-minimal fixpoints of O (Denecker, Marek, and Truszczy\u0144ski, 2000, Theorem 4). Altogether, we obtain the following notions: Given a complete lattice L = (L, \u2264), let O : L2 \u2192 L2 be an approximating operator, (x, y) is\n\u2022 a Kripke-Kleene fixpoint of O if (x, y) = lfp<\u2081(O(x, y));\n\u2022 a three-valued stable fixpoint of O if (x, y) = S(O)(x, y);\n\u2022 a two-valued stable fixpoints of O if (x, y) = S(O)(x, y); and x = y;\n\u2022 the well-founded fixpoint of O if it is the \u2264i-minimal (three-valued) stable model fixpoint of O."}, {"title": "Conditional Independence", "content": "Conditional independence in an operator-based setting is meant to formalize the idea that for the application of an operator to a lattice consisting of three sub-lattices L1, L2 and L3, full information about L3 allows us to ignore L2 when applying O to L1L3. In more detail, it means that the operator O overi\u2208 {1,2,3} Li can be decomposed in two operators O1,3 and O2,3 over the sub-lattices L1 \u25ca L3 respectively L2 L3.\nDefinition 4. Let O be an operator \u2297i\u2208{1,2,3} Li. The lattices L\u2081 and L2 are independent w.r.t. L3 according to O, in symbols: L1 \u315b0 L2 | L3, if there exist operators\nOi,3: LiL3\u2192 LiL3 for i = 1,2\ns.t. for every x1 \u2208 L1, x2 \u2208 L2, X3 \u2208 L3 it holds that:\nO(X1 X2\nO(X1 X2\nX3)|1,3 = 01,3(X1 & X3) and\nX3)|2,3 = O2,3(X2 X3)\nThus, two sub-lattices L\u2081 and L2 are independent w.r.t. L3 according to O if, once we have full information about L3, information about L2 does not contribute anything in the application of O when restricted to L\u2081 (and vice versa). Where L\u2081 10 L2 L3, we will also call L3 the conditional pivot, and will refer to members of L3 as such as well.\nExample 1. Consider the logic program P using atoms for infected, vaccinated and contact:\nr\u2081: inf(b) \u2190 inf(a), cnct(a, b), not vac(b).\nr2: inf(c) \u2190 inf(a), cnct(a, c), not vac(c).\nr3: inf(a)., r4: cnct(a, b)., r5: cnct(a, c).\nNotice that, as soon as we know that inf(a). is the case, we can decompose the search for models into two independent parts, as can also be seen in the dependency graph in figure 1.\nAs a product lattice consisting of power sets of sets A1, ..., A3 is isomorphic to the powerset of the union of these sets A\u2081 U... U A3, we shall use them interchangeably. We let:\nAb= {inf(b), cnct(a, b), vac(b)}\nAc= {inf(c), cnct(a, c), vac(c)}\nAa= {inf(a)}\nWe see that 2A | ICP 2Ac | 2Aa, by observing that:\nICA, Aa = ICPAAa and ICA,Aa = ICpac,Aa\nwhere PAb,Aa = {r1,r3,r4} and PAc,Aa = {r2, r3,r5}. It is easily verified that for every x1Ux2Ux3 \u2286 Ap, it holds that ICp(xb U xc U xa) \u2229 (Ai \u222a Aa) = ICPA,Aa (Xi U xa) for any i = b, c."}, {"title": "Conditional Independence and AFT", "content": "The notion of conditional independence is immediately applicable to approximation operators. In this section, we derive results on the modularisation of AFT-based semantics based on the results from the previous section.\nAs observed by Vennekens, Gilis, and Denecker (2006), the bilattice L\u00b2 of a product lattice L =\niel Li is isomorphic to the product lattice of bilattices i\u22081 L?, and we move between these two constructs without further ado.\nAs an approximation operator is a \u2264i-monotonic operator, we immediately obtain that the search for fixpoints, including the Kripke-Kleene fixpoints, can be split on the basis of conditional independence of an approximator:\nProposition 4. Let an approximation operator O over a bilattice of the product lattice Di\u2208{1,2,3} Li be given s.t. L\u315b0 L2 | L3. Then the following hold:\n\u2022 (x, y) is the Kripke-Kleene fixpoint of O iff (x|i,3, Y|i,3) is the Kripke-Kleene fixpoint of Oi,3 for i = 1,2.\n\u2022 (x,y) is a fixpoint of O iff (x|i,3,Y|i,3) is a fixpoint of Oi,3 for i = 1,2.\nWe now turn our attention to the stable operators. We first investigate the relation between an approximation operator and the lower and upper-bound component of this operator when it comes to respecting independencies. It turns out that the component operators Or and Ou respect conditional independencies, and, vice-versa, that the respect of the two component operators of conditional independencies implies respect of these independencies by the approximator:\nProposition 5. Let an approximation operator O over a bilattice of the product lattice i\u2208 {1,2,3} Li be given. Then L? Do L3 | L3 iff L\u00b2 110, L2 | L3 and L\u00b2 Ho\u201e L\u00b2 | L3.2\nStable operators respect the conditional independencies respected by the approximator from which they derive:\nProposition 6. Let an approximation operator O over a bilattice of the complete product lattice i\u2208{1,2,3} Li be given s.t. L? Lo L3 | L3. Then L1 \u315bc(0\u2081) L2 | L3 and L1 \u315bc(Ou) L2 | L3.\nThis allows us to derive another central result, stating that search for stable fixpoints, including the well-founded one, can be split up on the basis of conditional independencies.\nProposition 7. Let an approximation operator O over a bilattice of the product lattice i\u2208{1,2,3} Si be given s.t. C 10 L2L3. Then:"}, {"title": "Parametrized Complexity Results Based on Conditional Independence", "content": "In this section, we show how the notion of conditional independence can be made useful to break up reasoning tasks into smaller tasks that can be solved in parallel. We do this by introducing conditional independence trees, and illustrate its usefulness by showing that the size of the induced modules serves as a parameter for the fixed parameter tractability of calculating the well-founded fixpoint."}, {"title": "Preliminaries on Parametrized Complexity", "content": "In this section, we recall the necessary background on (parametrized) complexity. For more detailed references, we refer to Downey and Fellows (2013). We assume familiarity with basics on the polynomial hierarchy. An instance of a parametrized problem Lis a pair (I, k) \u2208 \u03a3* \u00d7 N for some finite alphabet \u2211, and we call I the main part and k the parameter. Where |I| denotes the cardinality of I, L is fixed-parameter tractable if there exists a computable function f and a constant c such that (I,k) \u2208 L is decidable in time O(f(k)|I|c). Such an algorithm is called a fixed-parameter tractable algorithm. Thus, the intuition is that when f(k) remains sufficiently small, the problem remains tractable no matter the size of I."}, {"title": "Conditional Independence Trees", "content": "How can conditional independence be used to make reasoning more efficient? Conditional independence allows to split up the application of an operator over lattice elements to parallel reasoning over elements of the sub-lattices. These sub-lattices thus define sub-modules for parallel reasoning. Conditional independence only allows us to split up reasoning into two modules, thus only splitting the search space in half at best. However, the operator allowing, these modules can again be split up into smaller modules, leading to a tree structure of nested modules, which we call conditional independence trees:\nDefinition 5. Let \u2297i\u22081 Li be a product lattice. We call a binary labelled tree (V, E, v) a conditional independence tree for O (in short, CIT) if the following holds:\n\u2022\u03bd: V \u2192 2\u2032 \u00d7 2 \u00d7 21,\n\u2022 the root is labelled (I1, I2, I3) where I1, I2, I3 is a partition of I,\n\u2022 for every (v1, v2), (v1, v3) \u2208 E, where v(vi) = (I1, I2, I3) for i = 1,2,3, I} \u222a I} = I{+1 U I\u2082+1 U\n1+1 for j = 1,2.\n\u2022 E11 Li 011 U12 U13 iEI2 Li | i\u2208I3 Li for every v \u2208 V with v(v) = (I1, I2, I3).\nThus, a CIT is a tree where each vertex is labelled with a partition (I1, I2, I3) of a sub-lattice iEI1UI2UI3 Li S.t. \u00d0i\u220811 Li is independent w.r.t. \u00d8ieI2 Li given \u2297i\u220813 Li according to O, and such that the labels of the leaves of a node compose the sub-lattices mentioned in the the labels of the parent node. Finally, the root of the tree should be a decomposition of the original lattice."}, {"title": "Application to Logic Programs", "content": "In this section, we illustrate the theory developed in the previous section by applying it to normal logic programs. We can avoid clutter with a slight abuse of notation by writing A1 Ip A2 | A3 to denote 2A1 ICP 2A2 | 2A3."}, {"title": "Related Work", "content": "In this section, we provide a summary of related work. For most of the comparisons, we provide more formal comparisons in the appendix. In the context of classical logic, a notion of conditional independence was proposed by Darwiche (1997). Darwiche assumes a database \u2206 (i.e. a set of propositional formulas), which is used as a background theory for inferences. The idea behind conditional independence is then that a database A sanctions the independence of two sets of atoms 21 and 22 conditional on a third set of atoms x3 if, given full information about 13, inferences about x1 are independent from any information about 22. As shown in the appendix, our notion of conditional independence implies Darwiche's notion.\nA concept related to conditional independence studied in approximation fixpoint theory is that of stratification (Vennekens, Gilis, and Denecker, 2006). This work essentially generalizes the idea of splitting as known from logic programming, where the idea is to divide a logic program in layers such that computations in a given layer only depend on rules in the layer itself or layers below. For example, the program {q \u2190 not r; r \u2190 not s; s \u2190 not p} can be stratified in the layers {p}, {s,r}, {q}. This concept was formulated purely algebraically by Vennekens, Gilis, and Denecker (2006). Our study of conditional independence took inspiration from this work in using product lattices as an algebraic tool for dividing lattices, and many proofs and results in our paper are similar to those shown for stratified operators (Vennekens, Gilis, and Denecker, 2006). Conceptually, stratification and conditional independence seem somewhat orthogonal, as conditional independence allows to divide a lattice \"horizontally\" into independent parts, whereas stratification allows to divide a lattice \u201cvertically\u201d in layers that incrementally depend on each other. However, we show in the appendix that conditional independence can be seen as a special case of stratification.\nA lot of work exists on the parametrization of the computational complexity of various computational tasks using treewidth decompositions as a parameter (Gottlob, Scarcello, and Sideri, 2002). These results show that the computational effort required in solving a problem is not a function of the overall size of the problem, but rather of certain structural parameters of the problem, i.e. the treewidth of a certain representation of the problem. These techniques have been applied to answer set programming (Fichte et al., 2017). In these works, the treewidth of the tree decomposition of the dependency graph DP(P) and incidence graph (which also contains vertices for rules) of a logic program are used as parameters to obtain fixed-parameter tractability results. In the appendix, we show conditional independence and treewidth are orthogonal. Furthermore, our results go beyond the answer set semantics, e.g. partial stable, supported and well-founded (ultimate) semantics.\nOther operator-based formalisms have been analysed in terms of treewidth decompositions (Fichte, Hecher, and Schindle-2022; Dvo\u0159\u00e1k, Pichler, and Woltran, 2012). A benefit of our operator-based approach is that all results are purely algebraic and therefore language-independent, which means that applications to specific formalisms are derived as straightforward corollaries. Furthermore, the results for AFT-based semantics, which subsume many KR-formalisms (an overview is provided by Heyninck and Bogaerts (2023b)), are not restricted to the total stable fixpoints, but also apply to partial stable and well-founded semantics, in contrast to many studies on fixed-parameter tractability.\nConditional independence has been investigated in several other logic-based frameworks, such as (iterated) belief revision (Lynn, Delgrande, and Peppas, 2022; Kern-Isberner, Heyninck, and Beierle,"}, {"title": "Conclusion", "content": "In this paper, the concept of conditional independence, well-known from probability theory, was formulated and studied for operators. This allows to use this concept to a wide variety of formalisms for knowledge representation that admit an operator-based characterisation. As a proof-of-concept, we have applied it to normal logic programs.\nThere exist several fruitful avenues for future work. Firstly, we want to investigate related notions of independence, such as context-specific independence Boutilier et al. (1996). A second avenue for future work is a more extensive application of the theory to concrete formalisms, both in breadth (by applying the theory to further formalisms) and in depth (e.g. by investigating more syntactic methods to identify conditional independencies, and by evaluating the computational gain experimentally)."}, {"title": "Proofs and Supplementary Material for Section 3 (Conditional Independence)", "content": "Fact 1. Let an operator O on Di\u2208{1,2,3}Li s.t. L1 Lo L2 | L3 be given. Then for any x1 & X2 / X3 \u2208 i\u2208{1,2,3} Li, it holds that:\nO(X1 X2 X3) = O1,3(X1 X3) O2,3(X2 X3)|2\n= 01,3(X1 X3)|1 O2,3(X2 X3).\nFurthermore, for any i, j = 1,2, i \u2260 j, xi \u2208 Li, xj,x'; \u2208 Lj and x3 \u2208 L3 it holds that:\nO(XiXjX3)|i,3 = O(xi & X'; & X3)|i,3\nProof. Immediate in view of the definition of conditional independence.\nRemark 1. We now demonstrate that our conditional independence also shows some differences with conditional independence as known from probability theory. For example, not all semi-graphoid-properties (Pearl, Geiger, and Verma, 1989) are satisfied. In more detail, whereas symmetry (i.e. L1 L2 L3 implies L2 \u315bo L1 | L3) is obviously satisfied, the properties of decomposition (i.e. L1 Lo L2 L3 | \u00d8 implies L\u2081 Lo L2 | \u00d8) and weak union (i.e. L\u2081 \u315bo L2 \u2297 L3 | \u00d8 implies L10 L2 L3) are not satisfied. Regarding decomposition, it should be noted that this property is undefined as we assume conditional independence over decompositions of the complete lattice. A violation of weak union is illustrated in the following example:\nExample 5. Consider the logic program P = {a \u2190; b \u2190 not c; c \u2190 not b}. Note that 2{a} ||ICP 2{b,c} | \u00d8. Yet it does not hold that 2{a} || ICp 2{c} | 2{b}, as:\nICp({a}) \u2229 {a, c}= {a,c} \u2260\nICp({a,b}) \u2229 {a, c} = {a}\nThe reason for the failure of weak union is that we are not only interested in the behaviour of the operator O w.r.t. the conditionally independent sub-lattices L\u2081 and L2, but also take into account the conditional pivot L3. Contrast this with probabilistic conditional independence where the defining condition p(x1 | X3) = P(x1x2,x3) only talks about L\u2081. The reason that conditional pivots are taken into account in our account of conditional indepdence is that we are usually interested in fixpoints of an operator. It might be interesting to look at a weaker notion of conditional independence w.r.t. operators that does not consider the conditional pivot in the output of the operator (and indeed, it is not hard to see that weak union is satisfied for such a notion), but due to our focus on fixpoints, we restrict attention to the stronger notion.\nLemma 1. Let an operator O on Di\u2208{1,2,3}Li s.t. L1 |0 L2 | L3 be given. Then O2,3(X2 X3)|3 =\nO1,3(X1 X3) 13 for every X1 X2 X3 \u2208 i\u2208 {1,2,3} Li.\nProof. As L1 Lo L2 | L3, for any X1 X2 X3 \u2208 \u00d0i\u2208{1,2,3} Li, O(X1 X2 X3) = 01,3(X1\nO2,3(X2 X3) 12 = 01,3(X1X3)|1 O2,3(X2X3), which implies O1,3(X1X3)\nX3), which implies O1,3(X1\nX3)|3 = O2,3(X2 X3)|3.\nProposition 1. Let an operator O on \u2297i\u2208{1,2,3} Li s.t. L1 Do L2 | L3 be given. Then x1\nX2 X3 = O(X1 X2 X3) iff X1 X3 = 01,3(x1x3) and x2 x3 = O2,3(x2\nX3) (for any X1 X2 X3 \u2208 i\u2208{1,2,3} Li).\nProof. For the \u21d2-direction, suppose that x1 X2 X3 = O(X1 X2 X3). Since L\u2081 Lo L2 | L3,\nOi,3(Xi X3) = O(x)|i,3 = X|i,3 = Xi & X3 (for i = 1,2). For the <-direction, suppose that X1 X3 =\nO(x1x3) and x2 x3 = O(x2 x3). As L1 Lo L2 | L3, O(x1 X2 X3) = 01,3(X1 X3) O2,3 (X2 X3)|2 =\nX1 X2 / X3 = X."}, {"title": "Proofs and Supplementary Material for Section 4 (Conditional Independence and AFT)", "content": "Proposition 4. Let an approximation operator O over a bilattice of the product lattice i\u2208 {1,2,3} Li\nbe given s.t. C 10 L2 L3. Then the following hold:\n\u2022 (x,y) is the Kripke-Kleene fixpoint of O iff (x|i,3, Y|i,3) is the Kripke-Kleene fixpoint of Oi,3 for\ni = 1,2.\n\u2022 (x,y) is a fixpoint of O iff (x|i,3,Y|i,3) is a fixpoint of Oi,3 for i = 1,2.\nProof. This is an immediate consequence of Propositions 1, 2 and 3.\nProposition 5. Let an approximation operator O over a bilattice of the product lattice \u00d0i\u2208{1,2,3} Li\nbe given. Then L? Do L3 | L3 iff L\u00b2 110, L2 | L3 and L\u00b2 Ho\u201e L\u00b2 | L3.3"}, {"title": "Proofs and Supplementary Material for Section 5 (Parametrized Complexity Results Based on Conditional Independence)", "content": "Proposition 8. Let an operator O over \u2297ie1 Li and CIT T = (V,E,v) be given with V\u2081 the leafs of\nT. Then:\n1. lfp(O) = (11,12,13)\u2208V\u2081 lfp(OI\u2081UI3) lfp(O12U13),\n2. x is a fixpoint of O iff for every (I1, I2, I3) \u2208 V and i = 1,2, X|I\u00bfUI3 is a fixpoint of OI\u00bfUI3\u00b7\nProof. We show this by induction on the size |E| of CIT T = (V, E,\u03bd) The base case is |E| = 0 is\ntrivial. For the inductive case, suppose the two statements hold for any CIT T' = (V', E', v') with\n|E' \u2264 n and consider the CIT T = (V, E, v) with |E| = n + 2. We show the two claims for T:\n1. Let T' = (V', \u0395',\u03bd') where V' = V \\ VE' = E\u2229 (V' \u00d7 V'), and v'(v) = v(v) for every v \u2208 V'.\nNotice that T' is also a CIT, and as E' CE, |E'| \u2264 n. Thus, with the inductive hypothesis, (\u2020):\nlfp(O) = U(11,12,13)\u2208V\u2032 lfp(011013) lfp(O12013), where V' are the leaf nodes of V'. Consider\nnow some v \u2208 V' with children nodes v1 and v2 and v(v) = (I1, I2, I3). By definition of a CIT,\nwhere v(v1) = (I1, I2, I3) and v(v1) = (1, 12, 13), ieri LilOrjururie Li | Dier Li for\nj = 1,2, which means (with Proposition 3), that Ifp(O\u0e1a\u0e27\u0e35\u0e48\u0e1a\u0e32\u0e23\u0e39\u0e49) = lfp(Ori\u0e1a\u0e23\u0e39\u0e49) lfp(01\u0e23\u0e39\u0e1a\u0e32) for\nj = 1,2. As I\u00a6 UI U I} = I\u2081 U I3 and I U IZ UI} = I2 U I3 and \u2297i\u220811 LiOI1UIQUI3iEI2 Li |\nieI3 Li, we conclude that (\u2020): lfp(OI\u2081UI2UI3) = 1fp(01\u2081U13) lfp(OI2UI3) = lfp(OIUIUI)"}, {"title": "Proofs and Supplementary Material for Section 6 (Application to Logic Programs)", "content": "Proposition 11. Let a nlp P be given for which Ap is partitioned into A\u2081 U A2 U A3 s.t. A1 \u0428\u0440\nA2 A3. Then IC = ICPA; (for i", "somer": "p \u2190\nP1,..., Pn, not q1,..., not qm \u2208 P s.t. p\u03b9 \u2208 Xi Ux"}]}