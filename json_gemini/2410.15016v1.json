{"title": "Transit Pulse: Utilizing Social Media as a Source for Customer\nFeedback and Information Extraction with Large Language Model", "authors": ["Jiahao Wang", "Amer Shalaby"], "abstract": "Users of the transit system flood social networks daily with messages that contain valuable\ninsights crucial for improving service quality. These posts help transit agencies quickly identify\nemerging issues. Parsing topics and sentiments is key to gaining comprehensive insights to\nfoster service excellence. However, the volume of messages makes manual analysis impractical,\nand standard NLP techniques like Term Frequency-Inverse Document Frequency (TF-IDF) fall\nshort in nuanced interpretation. Traditional sentiment analysis separates topics and sentiments\nbefore integrating them, often missing the interaction between them. This incremental approach\ncomplicates classification and reduces analytical productivity. To address these challenges,\nwe propose a novel approach to extracting and analyzing transit-related information, including\nsentiment and sarcasm detection, identification of unusual system problems, and location data\nfrom social media. Our method employs Large Language Models (LLM), specifically Llama\n3, for a streamlined analysis free from pre-established topic labels. To enhance the model's\ndomain-specific knowledge, we utilize Retrieval-Augmented Generation (RAG), integrating\nexternal knowledge sources into the information extraction pipeline. We validated our method\nthrough extensive experiments comparing its performance with traditional NLP approaches on\nuser tweet data from the real world transit system. Our results demonstrate the potential of LLMS\nto transform social media data analysis in the public transit domain, providing actionable insights\nand enhancing transit agencies' responsiveness by extracting a broader range of information.", "sections": [{"title": "1. Introduction", "content": "Understanding user feedback is crucial for public transit agencies. Collecting and analyzing this feedback improves\nservice quality, improves operational efficiency, builds community trust, and supports informed, data-driven decisions.\nThrough user feedback, agencies gain insights into passengers' perceptions of the transit service, identifying satisfactory\nand unsatisfactory aspects, pinpointing when and where issues occur, and understanding overall needs. This process\nenables agencies to adapt to changing demands, address safety concerns, and maintain transparency and accountability.\nUltimately, this leads to increased ridership and higher customer satisfaction.\nHowever, collecting user feedback is a challenging process. Traditional methods, such as physical or online surveys,\nwhile effective, have several limitations. These methods have a limited reach and can be costly, requiring significant\ninvestments in money, time, and labor. In addition, respondents often experience survey fatigue, which can reduce the\nquality of feedback. Feedback collection through surveys is typically delayed, often conducted annually or subannually,\nwhich is useful for long-term analysis and decision making but impractical for understanding user perceptions in\nreal-time [21].\nPeriodic surveys tend to capture opinions about the overall system or a broad network, making it difficult for agencies\nto obtain specific information about when, where, and what issues occurred. This lack of timely, detailed feedback\nhampers the ability of transit agencies to address immediate concerns and make prompt improvements to their services.\nIn contrast, collecting feedback through social media posts offers a more dynamic and cost-effective and relatively\nmore real-time alternative [23]. Social media platforms allow agencies to reach a broader audience and collect crowd-\nsourced information from diverse users. This method is not only cost-effective, but also provides near-real-time feedback,\nwhich is crucial for timely responses and adjustments. Furthermore, social media offers access to a massive dataset,\nwhich can be seen as the foundation of applying complex analysis tools [31]."}, {"title": "2. Literature Review", "content": "This section provides a brief overview of widely used methods for information extraction from social media."}, {"title": "2.1. Manual Identification", "content": "Manual identification is a common method used to extract useful information from social media. For instance,\nauthors in [26] and [24] manually identified geo-information from tweets mentioning station or route names. Although\nthis method guarantees accuracy, it is inefficient for large volumes of tweets. In [6], 1,624 tweets were identified for\nroute information extraction, while [26] involved manually labeling 3,454 tweets for spatial information. While human\nlabeling is valuable for building benchmark datasets or lexicons, it is highly costly and impractical for frequent data\nanalysis on fast-updating social media datasets."}, {"title": "2.2. Lexicon-Based Approaches", "content": "One of the most prominent methods for information extraction is the lexicon-based approach. This method identifies\ninformation, such as sentiment, using a collection of tokens with predefined scores. The overall sentiment of a sentence\nis determined based on the scores of these tokens [17]. Lexicon-based approaches are common in sentiment analysis,\nwhere tokens are assigned scores of -1, 0, or 1, representing negative, neutral, or positive sentiments, respectively.\nThe sentence score is then calculated accordingly [14, 7]. To enhance the performance of lexicon-based methods,\nit is often necessary to generate specific lexicons tailored to particular classification tasks. For instance, in [16], a\nlexicon-based method was used to identify nine problematic topics in a tweet dataset related to the Calgary transit system.\nSimilarly, [26] employed 435 predefined terms to classify tweets into four topics: punctuality, comfort, breakdowns,\nand overcrowding. In another study, [3] introduced a human-involved lexicon-building process to identify tweet topics,\nsuch as bus-related issues, sentiment, and sarcasm.\nDespite its popularity, the lexicon-based approach has inherent drawbacks for social media data analysis. Firstly,\ndividing sentences into tokens can lead to the loss of contextual information. For example, irony or sarcasm can render\nindividual tokens' meanings opposite to their intended sentiment in the sentence. Additionally, the informal writing\nhabits prevalent on social media make it challenging to build comprehensive lexicon datasets that accommodate informal\nword usage."}, {"title": "2.3. Machine Learning-Based Approaches", "content": "Another notable method for information extraction is the machine learning (ML)-based approach. In this method,\nML or deep learning (DL) models, such as logistic regression (LR), support vector machines (SVM), and decision trees\n(DT) [38], are trained on well-structured datasets for specific topic classification tasks. These models are commonly\nused in sentiment analysis. Additionally, transformer models with large structures, such as BERT, have been applied to\ntopic classification tasks for social media analysis [26]. A well-constructed dataset with relevant labels and similar\ncontent is crucial for ML-based methods. Transfer learning can alleviate this limitation by using models trained on\ndatasets built for similar tasks in different domains, then applying them to current tasks [25]. For example, in [19], the\nauthor used BERT to build a model for extracting transit topics from social media. Similarly, in [9], the BERT model\nwas used for classiy pedestrian maneuver types. The model was trained on an online customer feedback dataset with\n11 different topics and then used for further classification tasks related to the Washington Metropolitan Area Transit\nAuthority.\nHowever, even with transfer learning, ML-based approaches are still constrained by the scope of the training dataset\nand cannot identify topics not present in the dataset. Moreover, ML-based classification tasks are typically single-aspect.\nFor instance, a model trained for sentiment analysis only performs sentiment analysis. This approach becomes costly\nwhen multiple aspects of information need to be extracted from the target text."}, {"title": "2.4. Large Language Model-Based Approaches", "content": "Large Language Models (LLMs) are a type of deep learning model built using multi-layer Transformer archi-\ntectures [36], containing vast numbers of parameters and typically pre-trained on large-scale corpora [?]. Through\npre-training, LLMs acquire general knowledge, common sense, and the ability to understand and generate text. Due\nto their exceptional capabilities, LLMs are increasingly adopted for information extraction tasks. Beyond traditional\nnatural language processing (NLP) tasks such as sentiment analysis [43] and semantic analysis [41], LLMs are widely\napplied in downstream tasks like knowledge reasoning, question answering, relation extraction, and event extraction,\noften outperforming traditional NLP models [39].\nIn the transportation domain, LLMs have been used to answer transportation-related questions such as those\nconcerning transportation economics and driver characteristics [32]. Additionally, LLMs are applied in areas such as\ntransportation infrastructure planning and design [30], project management [1], operations and maintenance [29], and\nsafety control [13]. Multimodal LLMs are also used for tasks like object detection in transportation [4].\nAs noted in [27, 35], high-quality data ecosystems are essential for effective LLM applications in specific domains.\nFor instance, efforts to benchmark LLM performance in transportation have resulted in datasets like TransportBench [32],\ndesigned to assess reasoning abilities in transportation problems, and various question-answer datasets [42], which\nevaluate decision-making, complex event causality reasoning, and human driving exam performance. Moreover, [28]\nprovides an image dataset for evaluating LLMs' ability to detect transportation-related issues like cracks or congestion.\nThe GTFS-related dataset in [12] evaluates LLM performance in semantic understanding and information retrieval.\nHowever, as discussed in [13], while LLMs are powerful, they face challenges when applied to domain-specific tasks\ndue to knowledge gaps. Fine-tuning is a potential solution, as demonstrated in [37], where an open-source multimodal\nLLM, VisualGLM, was fine-tuned with 12.5 million textual tokens to enhance its transportation domain knowledge.\nSimilarly, in [44], LLaMA 3 was fine-tuned with a traffic safety dataset to improve its performance in that domain.\nFine-tuning pre-trained LLMs requires large amounts of high-quality training data and significant computational\nresources. A lightweight alternative is the Retrieval-Augmented Generation (RAG) system, which retrieves domain-\nspecific information from external databases, as proposed in [40]. However, this framework remains largely conceptual,\nwith limited real-world applications.\nTo address the gap in applying LLMs to transportation information extraction, particularly in user feedback analysis,\nwe introduce Transit Pulse. This is one of the first attempts to provide a lightweight solution for semi-automatic\ninformation extraction from social media posts, offering insights for transit monitoring and control."}, {"title": "3. Methodology", "content": "This paper addresses two user feedback analysis tasks: traditional classification and information extraction &\nsummarizing. For the traditional multi-class classification task, we cover sentiment classification, sarcasm detection,\nand transit problem topic classification. This section introduces the classification and information extraction pipelines\nused in our experiments."}, {"title": "3.1. Traditional Classification Task", "content": "As shown in Fig. 1, the traditional classification process involves data processing, vectorization, model training\nor fine-tuning, and model evaluation. We use both traditional NLP methods-Term Frequency-Inverse Document\nFrequency (TF-IDF) with machine learning (ML)-based classification\u2014and large language model (LLM)-based\nclassification methods."}, {"title": "3.1.1. TF-IDF", "content": "TF-IDF is a statistical measure that evaluates the importance of a word in a document relative to a collection of\ndocuments (corpus). It is widely used in information retrieval and text mining.\n\u2022 Term Frequency (TF) measures how frequently a term occurs in a document, normalized to prevent bias towards\nlonger documents:\n$tf(t, d) = \\frac{\\text{Number of times term t appears in document d}}{\\text{Total number of terms in document d}}$\n\u2022 Inverse Document Frequency (IDF) measures the importance of a term across the corpus, reducing the weight\nof frequently occurring terms:\n$idf(t, D) = log(\\frac{N}{\\{d \\in D : t \\in d\\}\\}})$\nwhere N is the total number of documents in the corpus D, and |{d \u2208 D : t \u2208 d}| is the number of documents\nin which the term t appears.\n\u2022 TF-IDF score for a term t in a document d is the product of its term frequency and inverse document frequency:\n$tf-idf(t, d, D) = tf(t, d) \\times idf(t, D)$\nTF-IDF provides a numerical statistic reflecting the importance of a word to a document in a corpus, enabling\neffective text analysis and retrieval."}, {"title": "3.1.2. BERT Tokenizer and BERT Model", "content": "In contrast to TF-IDF, the BERT tokenizer, part of a pre-trained LLM (the embedding layer in BERT), tokenizes and\nembeds text into dense vectors with contextual meaning. The BERT tokenizer splits text into tokens using WordPiece,\nthen processes these tokens through bidirectional transformers pre-trained on large corpora to capture contextual\ninformation.\nThe tokenized input is further processed by the pre-trained BERT model [10], which uses self-attention to learn\ncontextual information. The BERT model, thanks to its bidirectional structure, understands word meanings from both\ndirections in a sentence and from distant words. Fine-tuning the BERT model for classification involves adding a dense\nlayer that takes the model's output as a high-level feature vector for final classification. Pre-trained on massive datasets,\nthe BERT model's general understanding of language aids the classification layer in making accurate classifications."}, {"title": "3.2. Information Extraction", "content": "Traditional classification methods are easy to deploy and generally reliable but face limitations. They are restricted\nby the quality and scope of the training data set and can only classify predefined topics. Models trained for specific\ntasks, such as sentiment classification, cannot be directly used for other tasks like sarcasm detection. Additionally,\nperformance can degrade when applied to scenarios different from the training dataset.\nTo overcome these limitations, we introduce an information extraction pipeline based on the powerful open-source\nLLM, Llama 3 by Meta [2]. Llama 3 excels in over 150 benchmark tasks, including answering science / domain-specific\nquestions and common sense reasoning [5]. Its power comes from high-quality training data and a large model structure.\nLlama 3 was pre-trained with more than 15 trillion tokens, covering all high-quality open data available until December\n2023. The model's 70 billion parameters enable it to learn from this massive dataset through supervised fine-tuning\n(SFT) and reinforcement learning with human feedback (RLHF), resulting in accurate and human-preferred outputs.\nAs illustrated in Fig. 2, the information extraction pipeline begins by embedding target tweets into a structured\nprompt to guide the LLM in extracting the desired information. The prompt defines the LLM's role and tasks, including\nextracting and summarizing information related to the transit agency tweeted about (in our case the Toronto Transit\nCommission, TTC for short), such as station name, sentiment, sarcasm, and problem topic. The output is JSON-like text,\nwhich is processed through an information aggregation step. This step involves segregating text chunks into key-value\npairs, filtering unuseful text with regex, and constructing a structured JSON dataset. The consensus mechanism\ndetermines the most common answer from multiple LLM responses to mitigate performance variations."}, {"title": "3.3. Retrieval Augmented Generation (RAG) System", "content": "Despite Llama 3's strengths in context understanding and NLP tasks, it has limitations with unfamiliar data or\ndomain-specific knowledge. For instance, it may misinterpret station names in TTC-related tweets, especially when\nnames are abbreviated or misspelled, as shown in Fig. 3.\nTo address this, we implemented a Retrieval Augmented Generation (RAG) system [20]. The RAG system\nsupplements the LLM with external knowledge to improve accuracy in domain-specific information extraction.\nAs shown in Fig. 4, the RAG process starts by embedding the external knowledge base using a pre-trained LLM\nembedding model in a vector space. Each document is represented as a point in this high-dimensional space. When a\nquery (e.g., tweet content) is embedded into the same space, we compare it to the knowledge base to find the closest\nmatches. These matches are relevant external documents selected based on distance metrics like Euclidean Distance,\ncosine similarity, or maximum inner product (MIP). The retrieval process typically outputs more candidates than needed\nfor re-ranking by a more complex embedding system or LLM-driven prompts. The retrieved information is then added\nto the information extraction pipeline, enhancing accuracy and comprehensiveness.\nIn summary, our methodology combines advanced LLM techniques and the RAG system, to effectively classify and\nextract information from social media data related to public transit."}, {"title": "4. Experiments", "content": "This section presents our experiments on using LLM-based pipeline for traditional classification problems and\ninformation extraction & summarization. We begin by introducing the experimental setup, including datasets, models,\nand hardware configuration. Then, we discuss model performance on each dataset and the information extracted by\nour pipeline. Additionally, we provide a case study showcasing the practical application of our information extraction\nsystem for real-time public transit service monitoring."}, {"title": "4.1. Datasets", "content": "We used three datasets to test model performance on multi-class classification tasks for sentiment analysis, sarcasm\ndetection, and transit problem classification.\nFor sentiment analysis, we used the Sentiment Analysis on Movie Reviews dataset [8], containing 156,060 training\nrecords and 66,292 testing records across five sentiment groups: negative, somewhat negative, neutral, somewhat\npositive, and positive.\nFor sarcasm detection, we used the Tweets with Sarcasm and Irony dataset [22], which includes four classes: irony,\nsarcasm, regular, and figurative. The dataset has 54,618 training records and 7,861 testing records.\nThese datasets are publicly available and specifically designed for sentiment analysis and sarcasm detection. We\ndid not use our TTC tweets dataset for performance testing due to the lack of human-labeled sentiment and sarcasm\ninformation.\nFor the transit-related problem classification task, we used tweets related to the Toronto Transit Commission (TTC),\nwhich also serves as our information extraction dataset. The data was collected from February 5th, 2015, to December\n31st, 2015, from two main TTC X (formerly Twitter) accounts (as shown in Fig. 6): TTC Service Alerts and TTC\nCustomer Service. The TTC operates 192 bus routes, 11 streetcar routes, and 3 subway lines, serving over 1.4 million\nriders daily at its peak in 2023 [33].\nThe TTC-related tweets dataset includes 631,691 records. After removing duplicates and retweets with minimal\nadditional information, 27,312 tweets remained for analysis. Figure 7 shows the number of tweets posted at different\ntimes of the day, peaking during peak hours.\nThe dataset categorizes tweets into 10 problem categories: maintenance, capacity availability, interaction with staff,\ntravel time, ride quality, winter maintenance, temporal availability, safety and security, accessibility, and communication\n(Table 1). Initially, a group of keywords for each problem topic was manually identified from a subset of the entire\ndataset. Subsequently, a lexicon-based method was employed to label the remaining tweets based on the frequency of\nkeyword occurrences, assigning each tweet to the most relevant problem category.\nTo evaluate model performance with and without the RAG system, we utilized a GTFS-specific question dataset [11]\ndesigned to assess the understanding of GTFS standards and the retrieval of information from structured GTFS data.\nThis dataset comprises 195 questions across six categories: term definitions, common reasoning, file structure, attribute"}, {"title": "4.2. Models and Environment Setup", "content": "We implemented various models for classification tasks, including logistic regression, KNN, FFNN, and RNN, to\ncompare with the relatively small LLM: BERT. As discussed in the methodology section, traditional classification\nmethods using ML/DL are based on TF-IDF for text embedding and trained from scratch. In contrast, BERT is fine-tuned\nfor each task using corresponding datasets, showcasing the power of LLM structures.\nFor the information extraction pipeline, we used LLama-3 as the core for text understanding, information extraction,\nand summarization.\nThe experiments were conducted in the following environment:\n\u2022 System: Pop OS\n\u2022 Processor: 2.4 GHz 8-Core Intel Core i9\n\u2022 GPU: Nvidia 4090\n\u2022 Programming Language: Python\n\u2022 Large Language Model (for classification): BERT\n\u2022 Large Language Model (for information extraction): LLama-3 8B\n\u2022 Prompt Framework: LangChain"}, {"title": "4.3. Experiment Results", "content": ""}, {"title": "4.3.1. Classification Tasks", "content": "Table 2 presents the accuracy results of various models across three different tasks: Sentiment Analysis, Sarcasm\nDetection, and Problem Topic Classification.\nThe LLM model demonstrates the highest accuracy across all three tasks, substantially outperforming other models,\nespecially in Sentiment Analysis and Sarcasm Detection. For Problem Topic Classification, BERT (LLM) achieves\n91.5% accuracy, highlighting its robustness in handling complex language understanding tasks compared to traditional\nand other neural network models."}, {"title": "4.3.2. GTFS Understanding and Retrieval", "content": "This section presents the performance of the LLM for GTFS understanding and retrieval tasks, with and without the\nRAG system. Figure 10 shows the prompts for the LLM answering multiple choice questions on GTFS understanding,\nboth with and without the retrieved information from the RAG system. The external documents in the RAG system\nwere built using the official GTFS documentation\u00b9.\nThe results of these experiments are shown in Figure 11. For multiple choice questions across six types, the LLM\nwith RAG consistently achieves a higher number of correct answers. Notably, for Categorical Mapping questions,\nthe model with RAG scores 51 correct answers compared to 27 without RAG. Overall, the accuracy of answering\nGTFS-related multiple choice questions increases from 57.43% to 73.85%. For programming questions, accuracy\nimproves from 24.14% (21/87) to 52.87% (46/87), demonstrating the enhanced capability of RAG in improving LLM\nperformance for domain-specific questions."}, {"title": "4.3.3. Station Name Extraction with RAG", "content": "The prompts used in the RAG system during the post-retrieval phase to enhance the accuracy and formality of\nstation name extraction are shown in Fig. 12. To guide the language model in selecting the most relevant station name\nfrom the retrieved list, we employed the Chain-of-Thoughts (CoT) and Few-Shot techniques. These methods instruct\nthe model to think step-by-step and provide reference examples for making decisions.\nFigure 13 shows three examples of using the RAG system for station extraction. As illustrated, the RAG system\nenables the language model to extract more formal station information, avoiding issues related to using abbreviations or\nmisspellings. Additionally, the model can better handle instances where \"TTC\" is incorrectly identified as a station\nname."}, {"title": "4.3.4. Information Extraction with LLM", "content": "This section demonstrates information extraction from the TTC-related tweets dataset using LLM, compared with\ntraditional NLP methods. A notable advantage of the LLM is its ability to generate comprehensive outputs, including\nstation information, sentiment, sarcasm, and problem summarization in a single response."}, {"title": "4.3.5. Case Study: System Monitoring with LLM Powered Information Extraction Pipeline", "content": "This section demonstrates how the LLM-powered information extraction pipeline can help transit agencies respond\npromptly and efficiently to system issues."}, {"title": "5. Conclusion and Future Work", "content": "The LLM-powered information extraction pipeline has demonstrated significant advantages in analyzing social\nmedia data for transit systems. This pipeline enhances sentiment analysis by simultaneously detecting sentiment and\nsarcasm, providing a more accurate understanding of public opinions.\nAdditionally, the pipeline facilitates actionable insights for transit agencies by transitioning from system-level to\nstation-level analysis. By extracting station-specific information and summarizing potential issues mentioned in social\nmedia posts, it enables targeted interventions and improvements.\nMoreover, using this LLM-based information extraction pipeline reduces the dependency on pre-identified labels in\ndatasets. By adjusting the prompts and incorporating relevant external guidance documents, the pipeline can extract\nmore useful information with less human effort, broadening the scope of analysis.\nHowever, there are limitations and areas for future research. The TTC-related tweets dataset used in this study\nlacks comprehensive human annotations for sentiment classification and sarcasm detection. Additionally, station\ninformation was not pre-extracted, and there was no human review of labeling results. Building a more robust dataset\nwith high-quality annotations is crucial. This would provide ground truth for classification and extraction tasks and\nbenefit future LLM studies in the transit domain by offering a reliable dataset for training and fine-tuning models,\nserving as a benchmark for LLM applications in public transit.\nFurthermore, refining the prompts used in the LLM is necessary to ensure a more consistent output format, which\nwould enhance the information aggregation process and minimize the loss of relevant information.\nLastly, the LLM used for information extraction, Llama 3, is a high-performance model with 8 billion parameters,\nrequiring substantial computing resources. Future work will focus on employing techniques such as Knowledge\nDistillation to transfer knowledge from larger models to smaller ones, thereby improving performance and efficiency for\nhandling a relatively limited range of tasks."}]}