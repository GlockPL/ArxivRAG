{"title": "A Cycle Ride to HDR: Semantics Aware Self-Supervised Framework for\nUnpaired LDR-to-HDR Image Translation", "authors": ["Hrishav Bakul Barua", "Kalin Stefanov", "Lemuel Lai En Che", "Abhinav Dhall", "KokSheik Wong", "Ganesh Krishnasamy"], "abstract": "Low Dynamic Range (LDR) to High Dynamic Range\n(HDR) image translation is an important computer vision\nproblem. There is a significant amount of research uti-\nlizing both conventional non-learning methods and mod-\nern data-driven approaches, focusing on using both single-\nexposed and multi-exposed LDR for HDR image recon-\nstruction. However, most current state-of-the-art methods\nrequire high-quality paired {LDR,HDR} datasets for model\ntraining. In addition, there is limited literature on using\nunpaired datasets for this task where the model learns a\nmapping between domains, i.e., LDR \u2194 HDR. To address\nlimitations of current methods, such as the paired data con-\nstraint, as well as unwanted blurring and visual artifacts\nin the reconstructed HDR, we propose a method that uses\na modified cycle-consistent adversarial architecture and\nutilizes unpaired {LDR,HDR} datasets for training. The\nmethod introduces novel generators to address visual arti-\nfact removal and an encoder and loss to address seman-\ntic consistency, another under-explored topic. The method\nachieves state-of-the-art results across several benchmark\ndatasets and reconstructs high-quality HDR images.", "sections": [{"title": "1. Introduction", "content": "High Dynamic Range (HDR) [4, 57] images capture a\nbroader range of intensity values compared to Low Dy-\nnamic Range (LDR) or Standard Dynamic Range (SDR)\ncounterparts, which have a pixel bit depth of only 28 inten-\nsity levels. From human vision perspective, LDR images\nare often not visually pleasing, while from computer vision\napplications perspective, they hold limited information.\nMost commercial devices produce and display LDR con-\ntent and some specialized hardware can support HDR, e.g.,\nHDR cameras and sensors [37], which can capture images\nwith more than 256 intensity levels and HDR displays [12],\nwhich can display HDR intensity levels. Given that special-\nized hardware that captures HDR is expensive, researchers\nhave been investigating approaches for accurate HDR re-\nconstruction from LDR (i.e., inverse tone-mapping [78])\nusing conventional non-learning methods and modern data-\ndriven approaches. For display purposes, however, one of-\nten needs to tone-map [27] the HDR content to LDR to fit\nthe intensity range supported by standard hardware.\nEarly attempts on HDR reconstruction address tasks\nsuch as image enhancement, e.g., recovering missing in-\nformation due to extreme lighting conditions [62] or low\nlighting conditions [47, 84] and compression [11]. Current\napplications of HDR reconstruction span multiple domains,\nincluding, robotics/machine vision [83], media and enter-\ntainment [29], gaming, mixed reality and novel view syn-\nthesis [31,53,72,75], as well as medical imaging [31].\nEarly data-driven methods utilize Convolutional Neu-\nral Networks (CNN) [74], Transformers [52, 73] and Gen-\nerative Adversarial Networks (GAN) [63, 67]. More re-\ncent approaches are based on Diffusion Models [15, 23],\nNeural Radiance Fields (NeRF) [31], and Gaussian Splat-\nting [75]. Some methods use multi-exposed LDR [6, 18, 24,\n25, 45, 48, 51, 69, 71, 90], while others use single-exposed\nLDR [5, 9, 88] paired with an HDR for supervised training.\nAlthough these methods achieve excellent results in re-\nconstructing HDR images, most of them require proper\n{LDR,HDR} paired datasets [7, 19,51] for training. Conse-\nquently, the quality of the state-of-the-art data-driven HDR\nreconstruction methods depends on the quality of the avail-\nable paired {LDR,HDR} datasets. There is a research gap\nin the field; on the one hand, the literature on unpaired HDR\nreconstruction is extremely limited [49], and on the other,\nonly a few approaches utilize semantic and contextual infor-\nmation to guide the reconstruction process [22,23,52,77]."}, {"title": "2. Related Work", "content": "To address this gap, we propose a method that leverages\ncycle consistency [14, 89] objective using unpaired data,\nwhere the model learns a mapping between domains, i.e.,\nLDR \u2194 HDR. In addition, the method ensures semantic\nconsistency between the LDR and reconstructed HDR by\nutilizing CLIP [66] embeddings and loss based on semantic\nsegmentation. The proposed method reconstructs artifact-\nfree and visually impressive HDR from single-exposed\nLDR images. It also outperforms the most recent state-of-\nthe-art which predominantly uses paired datasets for train-\ning. Our work makes the following contributions:\n\u2022 We introduce the first semantic and cycle consistency\nguided self-supervised learning method for unpaired\n{LDR,HDR} data which addresses both the inverse\ntone-mapping (i.e., LDR \u2192 HDR) and tone-mapping\n(i.e., HDR \u2192 LDR) tasks (cf., Section 3).\n\u2022 We propose novel generators based on modified U-Net\narchitecture [70] that incorporates ConvLSTM-based\nfeedback mechanism [30, 50] to mitigate visual arti-\nfacts in the HDR reconstruction (cf., Section 3.1).\n\u2022 We propose a CLIP embedding encoder for contrastive\nlearning to minimize the semantic difference between\nLDR and reconstructed HDR pairs, while maximizing\nthe difference between LDR and other {LDR,HDR}\npairs (cf., Section 3.1).\n\u2022 We propose a novel loss function based on the Mean\nIntersection of Union (mIOU) metric to further en-\nsure semantic consistency between the LDR and re-\nconstructed HDR (cf., Section 3.2).\n\u2022 We perform thorough experimental validation for the\ncontribution of all proposed components, both qualita-\ntively and quantitatively (cf., Section 5)."}, {"title": "2.1. Non-Learning Approaches", "content": "The most popular approach in this category consists of\nmulti-exposed LDR fusion to achieve high dynamic range\nin the output image [35]. This approach involves image fea-\nture alignment, calculating weights for feature mapping on\nthe basis of image characteristics, and fusing the images on\nthe basis of the weights to get the most appropriate expo-\nsures for each part of the image. Another approach is us-\ning histogram equalization [16] that allows for contrast and\nbrightness levels re-adjustment in the over/underexposed\nregions of the image. Gradient domain manipulation [44]\ntechniques enhance the granular details of an LDR image\nand expand its dynamic range. Pyramid-based image fu-\nsion [38] is a technique where image features are extracted\ninto Laplacian pyramids [38] and then fused for the HDR.\nSome methods use Retinex-based [40, 58] tuning to ef-\nfectively produce and display HDR images on consumer\nscreens [40, 58]. Intensity Mapping Function based ap-\nproaches [17, 65, 85] use transformations on the image pix-\nels either by using Logarithmic mapping or Gamma correc-\ntion to stretch the intensity range or enhance lower inten-\nsity pixels. The methods in this category, although time-\nefficient, face some issues including ghosting effects, arti-\nfacts, and blurring in the reconstruction, and they do not\ngeneralize well for variety of input LDR images."}, {"title": "2.2. Learning-Based Approaches", "content": "The methods in this category either use single-exposed\nLDR as input [18, 24, 25, 45, 48, 51, 71, 90] or alternatively,\nmulti-exposed LDR [6, 9, 63, 69, 88]. Barua et al. [5] har-\nnessed the concept of histogram equalization of input LDR\nimages, in addition to the original LDR, to overcome the\ncontrast and hue miss-representation in over/underexposed\nareas of the LDR image. Cao et al. [10] proposed a\nchannel-decoupled kernel-based approach which combines\nthe output HDR with the output of another architecture\nin a pixel-wise fashion. Luzardo et al. [54] proposed a\nmethod to enhance the artistic intent of the reconstructed\nHDR by enhancing the peak brightness in the reconstruc-\ntion process. In addition, recently, Neural Radiance Fields\n(NeRF) [31, 53, 59, 60], Diffusion Models [15, 23], and\nGaussian Splatting [75] have also been used to improve the\nreconstruction, both in terms of space and time complexity.\nSome methods use weakly-supervised [45], self-\nsupervised [88] and unsupervised [62] approaches for HDR\nreconstruction. Le et al. [45] proposed an indirect approach\nfor HDR reconstruction from multi-exposed LDR using\nweak supervision. The method first outputs a stack of multi-\nexposed LDR, which are then merged using the state-of-the-\nart tool Photomatix [28] to obtain an HDR image. Zhang et\nal. [88] proposed a self-supervised approach for HDR re-\nconstruction. The method requires multi-exposed LDR to\nlearn the reconstruction network during training without the\nneed for HDR counterparts. Nguyen et al. [62] proposed an\nunsupervised approach to recover image information from\noverexposed areas of LDR images. This approach does not\nrequire any HDR for supervision and instead uses pseudo\nground truth images. Lee et al. [46] proposed a method with\nlimited supervision for multi-exposed LDR generation con-\nsisting of a pair of networks that generate LDR with higher\nand lower exposure levels. Then the two exposure levels are\ncombined to generate the HDR image.\nGAN-Based Approaches. Generative Adversarial Net-\nworks (GAN) [3,21] architectures are well-explored in this\ndomain. Niu et al. [63] proposed a GAN-based method\nthat can handle images with foreground motions by fusing\nmulti-exposed LDR and extracting important information\nfrom the over/underexposed areas of the LDR. Raipurkar et"}, {"title": "3. Method", "content": "al. [67] proposed a conditional GAN architecture that adds\ndetails to the saturated regions of the input LDR using a\npre-trained segmentation model to extract exposure masks.\nGuo et al. [24] proposed a two-stage pipeline that extracts\nthe over/underexposed features with high accuracy. GAN\nwith attention mechanism first generates the missing infor-\nmation in those extreme exposure areas, and in the sec-\nond stage, a CNN with multiple branches fuses the multi-\nexposed LDR from the previous stage to reconstruct the\nHDR. Nam et al. [61] proposed a GAN method that uses ex-\nposure values for conditional generation of multi-exposure\nstack that adapts well to varying color and brightness levels.\nLi et al. [49] proposed a GAN-based method for un-\npaired multi-exposure LDR to HDR translation. The\nmethod introduces modified GAN loss and novel discrim-\ninator to tackle ghosting artifacts caused from the misalign-\nment in the LDR stack and HDR. Wu et al. [82] proposed\nCycleGAN-like architecture for low-light image enhance-\nment task using unpaired data for training. The method\nfuses the Retinex theory with CycleGAN concept to en-\nhance the lighting conditions globally, recover color and re-\nduce noise in the output HDR image.\nSemantic and Knowledge-Based Approaches. Some\nmethods attempt to use semantic/context information in the\nimage or image formation process for HDR reconstruction.\nWang et al. [77] proposed a method which approximates the\ninverse of the camera pipeline. Their knowledge-inspired\nblock uses the image formation knowledge to address three\ntasks during HDR reconstruction: missing details recov-\nery, image parameters adjustment, and image noise reduc-\ntion. Goswami et al. [23] proposed a method to recover the\nclipped intensity values of an LDR/SDR image due to the\ntone-mapping process in the camera. The proposed method\nworks in two stages - first it uses a semantic graph-based\nguidance to help the diffusion process with the in-painting\nof saturated image parts, and second, the problem is for-"}, {"title": "", "content": "mulated as HDR in-painting from SDR in-painted regions.\nLiu et al. [52] proposed a vision transformer approach with\ncontext-awareness to remove ghosting effects in the output\nHDR. The model is a dual-branch architecture to capture\nboth local and global context in the input image that enables\nthe generation process to remove unwanted information in\nthe output HDR and avoid artifacts.\nLimitations. Our analysis reveals that methods based on\nsingle-exposed LDR fail to preserve image characteristics\nsuch as color hue and saturation, color contrast, and inten-\nsity brightness levels in the HDR. On the other hand, meth-\nods based on multi-exposed LDR produce unwanted visual\nartifacts, missing details and ghosting effects. While most\nmethods produce excellent results in terms of visual quality,\nthe main limitations of these approaches are the requirement\nof paired {LDR,HDR} datasets for training, and the lack\nof semantic and contextual knowledge guidance in the re-\nconstruction process which could significantly improve the\nquality of output HDR images. Table 1 summarizes a com-\nparison between recent state-of-the-art and our method in\nterms of various parameters."}, {"title": "3. Method", "content": "This section describes the proposed method - the first se-\nmantic and cycle consistency guided self-supervised learn-\ning approach for unpaired {LDR,HDR} data which ad-\ndresses both the inverse tone-mapping (i.e., LDR \u2192 HDR)\nand tone-mapping (i.e., HDR \u2192 LDR) tasks."}, {"title": "3.1. Architecture Modules", "content": "We adopt a CycleGAN [89] architecture as the basis of\nour method depicted in Figure 1. The network includes two\ngenerators and two discriminators (cf., Supplementary for\nbrief background on cycle consistency). Let us denote the\ntwo domains as X for LDR images and Y for HDR images.\nFurthermore, let us denote the generator used in the for-\nward cycle that maps images from LDR to HDR as Gy and\nthe one used in the backward cycle that maps images from\nHDR to LDR as Gx. Finally, let us denote the discrimina-\ntor that discriminates between the reconstructed LDR and\nreal LDR images as Dx and the one that discriminates be-\ntween the reconstructed HDR and real HDR images as Dy.\nThe images from the two domains can be represented as\n{x}=1 where xi \u2208 X and {y;}}11 where yj \u2208 Y. The\ndata distribution of the two domains can be represented as\nx ~ Pdata(x) and y ~ Pdata(y).\nGenerators. The generators Gy and Gx are based on U-\nNet architecture [70] that includes an encoder and a de-\ncoder block with skip connections from each level of the\nencoder to the decoder. In our generators, we propose a\nfeedback mechanism [30, 86] between the encoder and de-\ncoder block. The rationale behind the feedback is to refine\nthe features extracted from the encoder (during the first iter-"}, {"title": "", "content": "ation of the feedback) to guide the decoder for better output\nimage reconstruction over the rest of the iterations. Hence,\nthe feedback block not only iterates over its own output but\nalso re-runs the decoder situated ahead of it in each iter-\nation while keeping the encoder frozen until the feedback\niteration completes. The feedback is implemented with a\nConvLSTM [30,50] network and the number of iterations is\nfixed to 4 based on an ablation experiment testing 2, 3, and\n4 iterations setups (cf., Supplementary for visual depiction\nof the proposed generators and implementation details).\nDiscriminators. The discriminators Dx and Dy are based\non [32,49] (cf., Supplementary for implementation details).\nEncoders. We also introduce a CLIP embeddings en-\ncoder E. Specifically we use the pre-trained Vision Trans-\nformer (ViT) [2] CLIP encoder from [66] which helps in\nextracting image embeddings with both local and global se-\nmantic context. For the forward cycle consistency we use\nE(x) and E(y). Then we add the embeddings from the en-\ncoder and feed them back to the bottleneck layer of the Gy\ndecoder. Similarly, for the backward cycle consistency we\nadd the embeddings from E(y) and E(x) and feed them\nback to the bottleneck layer of the Gx decoder."}, {"title": "3.2. Loss Functions", "content": "We use five loss functions to train the proposed method.\nTwo are standard CycleGAN loss functions, i.e., adversarial\nloss [20] and cycle consistency loss [89]. Inspired from [89]\nwe also use an identity loss [76]. Finally, we introduce\ntwo novel loss functions for HDR reconstruction, i.e., con-\ntrastive and semantic segmentation loss.\nThe loss is calculated on tone-mapped versions of the\nreconstructed and real HDR images. The tone-mapping is\nperformed on the basis of the \u00b5-law [34]. This is done to\navoid the high intensity pixels of HDR images that can dis-\ntort the loss calculation. The tone-mapping operator T can\nbe represented as follows:\n$$T(y_j) = \\frac{\\log(1 + \\mu y_j)}{\\log(1 + \\mu)},$$\nwhere the amount of compression \u03bc is 5000 following [39].\nAdversarial Loss. We apply adversarial loss to both map-\npings. The mapping from LDR to HDR domain, i.e., Gy :\nX \u2192 Y with the discriminator Dy, can be expressed as:\n$$L_{GAN} (G_Y, D_Y, X,Y) = \\mathbb{E}_{y\\sim P_{data}(y)} [\\log D_Y (y)] + \\mathbb{E}_{x\\sim p_{data}(x)} [\\log(1 \u2013 D_Y(G_Y(x)))],$$\nwhere Gy generates images that look similar to images\nfrom domain Y while Dy distinguishes between gener-\nated samples \u0177 and real samples y. Gy aims to minimize\nthis objective against Dy that aims to maximize it, i.e.,\nmingy Max Dy LGan(Gy, Dy, X, Y).\nThe mapping from HDR to LDR domain, i.e., Gx :\nY \u2192 X with the discriminator Dx, can be expressed as:\n$$L_{GAN}(G_X, D_X, Y, X) = \\mathbb{E}_{x\\sim P_{data}(x)} [\\log D_X(x)] + \\mathbb{E}_{y\\sim P_{data}(y)} [\\log(1 \u2212 D_X(G_X(y)))],$$\nwhere Gx generates images that look similar to images\nfrom domain X while Dx distinguishes between generated\nsamples \u00ee and real samples x. Similar to above, Gx aims to\nminimize this objective against Dx that aims to maximize\nit, i.e., mingx maxDx LGAN(Gx, Dx,Y,X).\nCycle Consistency Loss. The adversarial loss does not\nguarantee learning without contradiction i.e., the forward\nGy: X \u2192 Y and backward Gx : Y \u2192 X map-\npings might not be consistent with each another. Hence,\nwe also incorporate a cycle consistency loss to prevent mu-\ntual contradiction of the learned mappings Gy and Gx.\nFor each image xi from the LDR domain, the cycle of\nreconstruction i.e., from LDR to HDR and then back to"}, {"title": "", "content": "LDR must result back in the original image xi. Hence,\nwe can define the forward cycle consistency as: xi \u2192\nGy(xi) \u2192 Gx (Gy(xi)) \u2248 xi. Similarly, the backward\ncycle consistency can be represented as: yj \u2192 Gx(yj) \u2192\nGy(Gx(yj)) \u2248 yj. The loss can be formulated as:\n$$L_{cyc}(G_Y, G_X) = \\mathbb{E}_{x\\sim P_{data}(x)} [||G_X (G_Y(x)) - x||_1] + \\mathbb{E}_{y\\sim P_{data}(y)} [||G_Y (G_X(y)) \u2013 y||_1],$$\nwhere || ||1 is the L1 norm (cf., Supplementary for visual\ndepiction of the cycle consistency loss).\nIdentity Loss. For LDR \u2194 HDR translation tasks, we\nalso find that adversarial and cycle consistency formulations\nalone cannot preserve the color and hue information. This\nis due to incorrect mapping of color shades from LDR to\nHDR domains by the generators stemming from the under-\nlying difference in dynamic ranges. Therefore, we force the\ngenerators to replicate an identity mapping by providing tar-\nget domain images. This loss can be expressed as:\n$$L_{id}(G_Y, G_X) = \\mathbb{E}_{y\\sim P_{data}(y)} [||G_Y (y) - Y||_1] + \\mathbb{E}_{x\\sim P_{data}(x)} [||G_X (x) - x||_1] .$$\nContrastive Loss. This loss is based on embeddings ex-\ntracted using a CLIP encoder and ensures semantic informa-\ntion preservation across domains. Here, we do not directly\nextract the embeddings from LDR images but instead, use\na histogram-equalized version processed using the OpenCV\nfunction equalizeHist [8]. Histogram equalization im-\nproves the pixel visibility in extreme lighting areas or areas\nwith shadows/darkness of an image by re-adjusting the con-\ntrast and saturation levels. This is done by spreading out the\nfrequent pixel intensity values across 256 bins. Equaliza-\ntion often leads to revealing hidden semantic information\nor non-perceivable objects in an image. For the image em-\nbedding from E(x) and y from E(y) we first define the\ncosine similarity between them as follows:\n$$sim(x, y) = \\frac{x \\cdot y}{||x|| ||y||},$$\nwhere \u2022 represents the dot product between the two embed-\ndings and || || represents the norm of the embeddings. We\nformulate the contrastive loss for an input batch as positive\npairs of images (e.g., LDR and the corresponding recon-\nstructed HDR images) and negative pairs of images (e.g.,\neach LDR with the rest of the LDR as well as the rest of the\nreconstructed HDR images) as follows:"}, {"title": "", "content": "$$L_{con} =  \\frac{1}{N} \\sum_{i=1}^{N}  \\log  \\frac{exp(sim(x_i, Y_i)/\\tau)}{\\sum_{j=1}^{N}(exp(sim(x_i, x_j)/\\tau) + exp(sim(x_i, Y_j)/\\tau))}$$\nwhere N is the batch size, exp represents the exponential\nfunction, and 7 represents the temperature parameter which\ncontrols the amount of emphasis given in distinguishing be-\ntween positive and negative pairs. This loss is calculated\nin both the forward and backward cycles with the input and\noutput being swapped. This loss replicates the contrastive\nlearning paradigm of CLIP for {image,image} instead of\n{image,text} pairs (cf., Supplementary for visual depiction\nof the contrastive loss and an ablation experiment for T).\nSemantic Segmentation Loss. This loss is based on seg-\nmentation masks. The Mean Intersection of Union (mIoU)\nmetric measures the amount of overlap between ground\ntruth and predicted segmentation masks. Similar to the pre-\nvious loss function, we use histogram-equalized versions\nof the LDR images processed using the OpenCV function\nequalizeHist [8]. We choose equalized images instead\nof original LDR because segmentation on low-light or ex-\ntreme brightness images does not yield good results. We\nuse Segment Anything (SAM) [42] to generate segmen-\ntation classes in the histogram-equalized LDR and recon-\nstructed tone-mapped HDR images (cf., Supplementary for\nvisual depiction of the semantic segmentation loss). This\nloss component helps in mitigating differences in boundary\nand edge pixels between the LDR and HDR images. We\ncan define the IoU metric as:\n$$IoU_c = \\frac{X_c \\cap Y_c}{X_c \\cup Y_c}$$\nwhere xc and yc represent the segmentations for class c in\nimage x and y, respectively. \u2229 represents the overlapping\narea of predicted and ground truth pixels while \u222a represents\nthe total area covered by predicted and ground truth pixels\nfor class c. The mean IoU over all segmentation classes can\nbe formulated as:\n$$mIoU = \\frac{1}{C} \\sum_{c=1}^C IoU_c.$$\nWe represent the semantic segmentation loss as:\n$$L_{sem} = 1 - mIoU.$$\nFinal Loss. The full objective is expressed in Eq. (11),\nwhere \u03bb scales the relative importance of the cycle consis-"}, {"title": "", "content": "tency and identity loss. A is set to 10 in our experiments and\n$$L_{full} = L_{GAN}(G_Y, D_Y, X, Y) + L_{GAN}(G_X, D_X,Y, X) + \\lambda (L_{cyc}(G_Y,G_X) + 0.5 \u00d7 L_{id}(G_Y,G_X))+ \u03b1L_{con} + \u03b2L_{sem}$$\nthe weight for identity loss is 0.5 inspired from the setup\nin the original cycle consistency work [89]. a and \u03b2 are\nweights for the contrastive and semantic segmentation loss,\nrespectively. Both values are set to 2 (cf., Supplementary\nfor ablation experiments for a and \u03b2)."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Implementation", "content": "The proposed method is implemented in PyTorch\u00b9 on\nUbuntu 20.04.6 LTS workstation with Nvidia Quadro\nP5000 GPU with 16GB memory, Intel\u00ae Xeon\u00ae W-2145\nCPU at 3.70GHz with 16 CPU cores, 64GB RAM, and\n2.5TB SSD. We used Adam optimizer [41] to train the mod-\nels for 170 epochs. Batch size of 1 is expected to work well\nin cycle consistency models [89] however, since we are us-\ning contrastive loss on positive and negative pairs, the batch\nsize is set to 4. We used learning rate of 4 \u00d7 10-4 for the\ngenerators and 2 \u00d7 10-4 for the discriminators. The learn-\ning rate is kept constant for the first 100 epochs and subse-\nquently set to decay linearly to 0. For all models, we resized\nthe input images to 512 \u00d7 512. The HDR images used in\nthe text are tone-mapped with Reinhard's operator [68]."}, {"title": "4.2. Datasets", "content": "For the primary comparison of our method with the state-\nof-the-art, we consider the HDRTV [13], NTIRE [64], and\nHDR-Synth & HDR-Real [51] paired datasets. HDRTV has\n1235 samples for training and 117 for testing. The NTIRE\ndataset consists of approximately 1500 training, 60 valida-\ntion, and 201 testing samples. HDR-Synth & HDR-Real\ndataset consists of 20537 samples. Other paired datasets\nused in our evaluations are DrTMO [19] (1043 samples),\nKalantari [36] (89 samples), HDR-Eye [43] (46 samples),\nand LDR-HDR Pair [33] (176 samples). We choose these\ndatasets because they consist of a balance between real\nand synthetic images as well as image and scene diver-\nsity. We perform 80/20 split for training and testing un-\nless specified otherwise. For methods working with single-\nexposed LDR inputs, we use only one LDR from datasets\nwith multi-exposed LDR. For methods working with multi-\nexposed LDR inputs, we generate the required exposures\nusing the OpenCV function convertScaleAbs [8] for\ndatasets with only single-exposed LDR images."}, {"title": "4.3. Metrics", "content": "We use four metrics to report the results. High Dynamic\nRange Visual Differences Predictor (HDR-VDP-2) [55] or\nMean Opinion Score Index (Q-Score) is used for evalua-\ntion replicating the human vision model. Structural Sim-\nilarity Index Measure (SSIM) [79-81] is used to compare\nblock-wise correlations on the basis of structural similarity,\nluminance, and contrast information. Peak Signal-to-Noise\nRatio (PSNR) [26] (in dB) is used for a pixel-to-pixel com-\nparison for noise in the signals2. Learned Perceptual Image\nPatch Similarity (LPIPS) [87] is used to compare between\nthe high-level features in the images, such as the seman-\ntic entities present in the scene, and evaluates our semantic\nand contextual knowledge based contributions. This metric\naligns with the perceptual judgement of humans."}, {"title": "5. Results", "content": ""}, {"title": "5.1. HDR Reconstruction", "content": "For an extensive comparison we choose a combination\nof methods that utilize different learning paradigms in-\ncluding, HDRCNN [18], DrTMO [19], ExpandNet [56],"}, {"title": "5.3. Ablation Results", "content": "Generators. We propose a novel generator that modifies\nthe U-Net architecture by introducing a feedback mecha-\nnism (cf., Section 3.1). The feedback helps in avoiding\nartifacts in image-to-image translation tasks where huge\namounts of training data might make the process of halluci-\nnating details difficult. To test this hypothesis, we first re-\nplaced the original U-Net used in the SingleHDR(W) [45]\nmethod with our U-Net generator. We see that leads to im-\nprovement in all metrics as shown in Table 5. The table also\ndemonstrates the improvement in our model when we use\nthe feedback U-Net instead of the original U-Net of Sin-\ngleHDR(W) (cf., Supplementary for qualitative results in\nsupport of this quantitative evaluation).\nArchitecture. Our method uses the cycle consistency con-\ncept of CycleGAN and introduces many new components\nto address the LDR \u2194 HDR translation task. The most sig-\nnificant components are the CLIP embedding encoder and\nfeedback U-Net generators (cf., Section 3.1). We perform a\nthorough ablation on each of the components to highlight\nthe contributions. Table 6 summarizes the results where\nwe add each component to the architecture as we go down.\nThe first row lists the results from the original CycleGAN\nmethod. The second row is the CycleGAN utilizing the U-\nNet generator from SingleHDR(W) [45]. The third row is\nthe CycleGAN utilizing our feedback U-Net generator. This\nresults in a significant improvement in SSIM, LPIPS, and\nHDR-VDP-2. The fourth row lists the results when we add\nthe CLIP embedding encoder to the architecture. This re-\nsults in an improvement in all metrics. The fifth and sixth\nrows depict the improvements after adding contrastive and\nsemantic segmentation objectives in the architecture (cf.,\nSection 3.2). The seventh row depicts the results with all\ncomponents in our architecture but with the original U-Net\nfrom SingleHDR(W). Finally, the last row is the full archi-\ntecture with all components which achieves the best perfor-\nmance on all metrics.\nLoss Functions. We also study the loss functions and their\ncontribution towards the overall results keeping the archi-\ntectural components constant. Table 7 summarizes the re-\nsults. The first row is the results from the standard adversar-\nial and cycle consistency loss. In the second row we add the\nidentity loss which improves all metrics (cf., Supplemen-\ntary for qualitative results in support of Lid). In the third and\nforth rows we add the contrastive and semantic segmenta-\ntion loss. This results in an improvement in all metrics (cf.,\nSupplementary for qualitative results in support of Leon and\nLsem). Finally, adding all loss functions achieves the best\nresults on all the metrics.\nWe also perform separate tests on contrastive and se-\nmantic segmentation loss to examine the contribution of\nhistogram-equalized LDR compared to original LDR im-\nages. We summarize these results in Tables 8 and 9. In both"}, {"title": "6. Conclusions", "content": "cases the loss calculated between the histogram-equalized\nversions of the LDR (instead of original the LDR) and the\nrecnstructed HDR is better. This is due to the fact that\nhistogram-equalized LDR can reveal semantic information\nin extremely over/underexposed areas of the original LDR\nimages which in turn improves the extracted CLIP embed-\ndings for Lcon and segmentation maps for Lsem.\nMost current state-of-the-art methods for HDR re-\nconstruction (inverse tone-mapping) require high-quality\npaired {LDR,HDR} datasets for model training. Conse-\nquently, the quality of the state-of-the-art in inverse tone-\nmapping depends on the quality of the available paired\n{LDR,HDR} datasets. Additionally, only a few approaches\nutil"}]}