{"title": "On-chip Hyperspectral Image Segmentation with Fully\nConvolutional Networks for Scene Understanding in Autonomous\nDriving", "authors": ["Jon Guti\u00e9rrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe", "M. Victoria Mart\u00ednez", "Unai Martinez-Corra", "\u00d3scar Mata-Carballeira", "In\u00e9s del Campo"], "abstract": "Most of current computer vision-based advanced driver assistance systems (ADAS) perform\ndetection and tracking of objects quite successfully under regular conditions. However, under adverse\nweather and changing lighting conditions, and in complex situations with many overlapping objects, these\nsystems are not completely reliable. The spectral reflectance of the different objects in a driving scene\nbeyond the visible spectrum can offer additional information to increase the reliability of these systems,\nespecially under challenging driving conditions. Furthermore, this information may be significant enough\nto develop vision systems that allow for a better understanding and interpretation of the whole driving\nscene. In this work we explore the use of snapshot, video-rate hyperspectral imaging (HSI) cameras in\nADAS on the assumption that the near infrared (NIR) spectral reflectance of different materials can\nhelp to better segment the objects in real driving scenarios. To do this, we have used the HSI-Drive\n1.1 dataset to perform various experiments on spectral classification algorithms. However, the information\nretrieval of hyperspectral recordings in natural outdoor scenarios is challenging, mainly because of deficient\ncolour constancy and other inherent shortcomings of current snapshot HSI technology, which poses some\nlimitations to the development of pure spectral classifiers. In consequence, in this work we analyze to\nwhat extent the spatial features codified by standard, tiny fully convolutional network (FCN) models can\nimprove the performance of HSI segmentation systems for ADAS applications. In order to be realistic from\nan engineering viewpoint, this research is focused on the development of a feasible HSI segmentation system\nfor ADAS, which implies considering implementation constraints and latency specifications throughout the\nalgorithmic development process. For this reason, it is of particular importance to include the study of\nthe raw image preprocessing stage into the data processing pipeline. Accordingly, this paper describes the\ndevelopment and deployment of a complete machine learning-based HSI segmentation system for ADAS,\nincluding the characterization of its performance on different embedded computing platforms, including a\nsingle board computer, an embedded GPU SoC and a programmable system on chip (PSoC) with embedded\nFPGA. We verify the superiority of the FPGA-PSoC over the GPU-SoC in terms of energy consumption\nand, particularly, processing latency, and demonstrate that it is feasible to achieve segmentation speeds\nwithin the range of ADAS industry specifications using standard development tools.", "sections": [{"title": "1 Introduction", "content": "Today, thanks to the availability of small-size, portable, snapshot hyperspectral cameras, it is possible\nto set up HSI processing systems on moving platforms. The use of drones for precision agriculture\nand ecosystem monitoring is probably one of the most active and mature application domains of\nthis technology [20]. The research into how hyperspectral information can be used to develop more\ncapable and robust ADAS is, on the contrary, in its infancy. HSI provides rich information about\nhow materials reflect the light of different wavelengths (spectral reflection), and this can be used to\nidentify and classify surfaces and objects in a scene [12,52]. In fact, RGB images tend to suffer from\nmetamerism (two objects with different reflection spectra but having the same colour under a certain\nlight source), so HSI can help to resolve this phenomenon and become a powerful solution for object\nsegmentation [25]. Thus, with the application of appropriate information processing techniques, HSI\ncan help to enhance the accuracy and robustness of current ADAS for object identification and tracking\nand, eventually, can be used for scene understanding, which is a step forward in the achievement of\nmore capable and intelligent ADAS/ADS (Autonomous Driving Systems) [43, 56].\nHSI segmentation of real driving scenes is, however, very challenging for a variety of reasons that\nare relative to both the lack of control over the environment (lighting, weather, distance of objects etc.)\nand the technology of portable snapshot HSI cameras, i.e. the use of spatial mosaic and tiled spectral\nfilters, the limited spectral range of CMOS sensors and the lack of a photometer-based exposure-time\nsetting. First, acquiring HSI outdoors implies working with varying illumination and environmental\nconditions and dealing with the presence of moving objects and changing backgrounds, which strongly\nconditions the setup of the acquisition system. In consequence, acquired images need to undergo a\ntime-consuming preprocessing stage to transform a 2D image with radiance data into a 3D cube of\nreflectance values which, under these variable conditions and changing camera setups, can hardly\nguarantee the required robustness for the subsequent information processing. Secondly, the spectral\nsignatures of the different objects of interest for ADAS/ADS in a driving scene may show a weak\nspectral separability when acquired under the conditions described herein. Thirdly, the extraction of\nadditional spatial features that could help to improve the segmentation of items with similar spectral\nreflectance is problematic as a result of the enormous diversity of shapes, view angles, distances\nand scales in an image. Finally, ADS/ADAS development implies executing image processing and AI\nalgorithms onboard and in real-time (the latter meaning with a sufficiently low latency), and this must\nbe performed on resource constrained hardware platforms with tight power consumption restrictions.\nThis entails not only evaluating the performance indexes (recall/precision) of the inferred classification\nor segmentation maps but carefully analyzing the computational complexity of the algorithms involved\nand how to efficiently implement them on the target devices.\nIn this work, which is an extended version of [21], we describe an ML approach based on FCN for the\nsegmentation of HSI video recorded in real driving conditions as part of a research project which aims at\nexploring the applicability of small-size snapshot hyperspectral sensor technology to the enhancement\nof vision-based ADAS. This is a multidisciplinary work that includes investigating in an integrated\nmanner HSI processing techniques, machine learning and AI algorithms and models, and processor and\nadvanced SoC design techniques to develop a functional real-time image segmentation system that can\nrealistically be implemented in an embedded processing system. We present two application examples\nof the design of a HSI segmentation system that could be applied to ADS/ADAS. These examples\nare used throughout the paper to show the system development process, from the algorithmic design,\n(i.e. ML model training and optimization), to the final system prototyping and testing on different"}, {"title": "2 Related work", "content": "There is very limited prior work on ADAS segmentation in HSI, so we first summarize related work\non segmentation with HSI outside ADAS and deep learning-based ADAS segmentation in RGB data."}, {"title": "2.1 Deep learning-based segmentation with RGB data in ADAS", "content": "Semantic segmentation of urban scenes has been attracting the attention of researchers since the\npublication of widely known databases such as KITTI Stereo and Flow Benchmark [2] (200 annotated\ntraining images and 200 test images of urban environments), CamVid [8,9] (700 cityscape images),\nCityscapes [13] (5000 finely-annotated and 20 000 weakly-annotated images), although there are also\nother databases that focus on other environments such as [30], which provides 1200 annotated images\nfrom highway scenarios. In most cases, these databases are used to compare different models according\nto various metrics that are usually focused on accuracy results without paying attention to either\nnetwork complexity or energy consumption, neglecting their possible deployment in an embedded\napplication.\nIn fact, when analyzing the architectures that report remarkable results in the Cityscapes database,\nit can be seen that most of them contain tens of millions of parameters and require billions of float-\ning point operations (GFLOPS) at inference. For instance, Deeplab [11], a deep convolutional neural\nnetwork which includes atrous convolution for dense feature extraction and field-of-view enlargement,\ncontains tens of millions of parameters. SwiftNet [40], another deep convolutional encoder-decoder\nnetwork which is based on an interleaved pyramid fusion model, contains, depending on the config-\nuration, from 2.4M to 24.7M parameters and performs from 41 to 218 GFLOPs when run on an\nNVIDIA GTX 1080Ti GPU. STDC [17], a convolutional model which features various short-term\ndense concatenate modules (STDC) to fuse outputs from convolutions with different receptive fields\nand ends with a fully connected layer, contains, depending on the architecture, from 8.44M to 813M\nparameters and performs from 0.01 to 1.45 GFLOPS when run on an NVIDIA GTX 1080Ti. Finally,\nTinyHMSeg [31] is an encoder-decoder network which instantiates MobileNetV2 as the lightweight"}, {"title": "2.2 RGB versus HSI for image segmentation", "content": "The hyperspectral reflectance data of different material surfaces outside the visible spectrum can\nprovide very valuable information to improve intelligent vision tasks and, particularly, for image seg-\nmentation. Outside ADAS, HSI is being applied in various fields such as remote sensing for geoscience\napplications, food assessment and biomedical image analysis. With regards to remote sensing, in [19],\nto identify tree species in mixed-conifer forests, high spatial resolution airborne hyperspectral imagery\nand a convolutional neural network are used. The most relevant aspect of this article is that the au-\nthors show that the HSI CNN model outperformed the RGB CNN model by 23% on average, taking\ninto account all metrics and species. This indicates that the additional spectral information provided\nby HSI is essential for increasing model performance. In the field of food assessment, in [47], they\nshow the greater potential of HSI compared with conventional RGB imaging for predicting L-value of\nmushrooms, a method for mushroom quality grading. Finally, concerning biomedical image analysis,\nespecially interesting is the work presented in [45] where they compare five deep learning models,\nvarying both the spatial (from pixel-based segmentation to image-based segmentation) and spectral\n(RGB, domain-specific tissue parameter images (TPI) and HSI) granularities. They show that unpro-\ncessed HSI data (not TPI) offers a huge benefit compared to RGB or TPI data for organ segmentation,\nalthough this superiority is reduced when using more complex networks such as FCNs, which take\nadvantage not only of the richness of the spectral signature of the materials but also of the spatial\nproperties of the images."}, {"title": "2.3 Deep learning-based segmentation with HSI data in ADAS", "content": "The main hypothesis of this line of research is that spectral information beyond the visible range\ncan improve the accuracy and robustness of current RGB-based ADAS for object identification and\ntracking and, eventually, can be used for scene understanding. Leaving aside the studies carried out\nwith thermal cameras (far infrared), there is not much previous work published in the field [7]. Never-\ntheless, there are some researchers that have started to explore the applicability of HSI cameras to the\nfield of ADS. In [33], a hyperspectral image dataset for road segmentation in urban and rural scenes\nis introduced and in [21] the authors explore the use of FCNs to segment HSI applied to ADAS. The\nunderlying idea is that the incorporation of richer spectral information can provide a distinct spectral\nfingerprint for each entity in the image, helping to achieve more precise and robust detection systems.\nIn the study conducted by [12], the authors characterize the reflection of a wide variety of road\nobjects, such as a work zone signaling cone, a road reflector, an asphalt pavement, a dark blue fleece\nsweater, a tyre and an exhaust pipe in the 350 to 2450nm range using a spectroradiometer. They show\nthat there is a great difference among the spectral signatures of all the measured materials and remark"}, {"title": "3 Experimental Setup", "content": ""}, {"title": "3.1 The Dataset", "content": "As is reported in [6], there are very few HSI datasets aimed at developing ADAS and ADS applications.\nIn [6], the authors present a dataset named HSI-Drive, which was specifically conceived to provide\nthe research community with data obtained at real driving scenarios with a small size, portable,\nsnapshot hyperspectral camera capable of recording 25-band HSI at video rates. The current version\nof the dataset, i.e. HSI Drive v1.1, contains 276 manually labelled images recorded while driving a\ncar in urban, road and highway scenarios under diverse weather (sunny, cloudy, rainy and foggy) and\nlighting (dawn, midday, sunset) conditions. The recordings were performed during Spring (121 images)\nand Summer (155 images). The recording setup included a single Photonfocus camera featuring an\nImec 25-band VIS-NIR (535nm-975nm) mosaic spectral filter over a CMOSIS CMV200 image wafer\nsensor. The resolution of the raw images was 1088 x 2048 pixels with 5\u00b5m x 5\u00b5m size. However, as\nthe spectral bands were extracted from a mosaic formed by 5x5 pixel window Fabri-Perot filters, the\noutput resolution of the HSI cubes was actually reduced to 216 x 409 x 25 [41]. Indeed, the use of\nthis technology implies performing an image preprocessing stage prior to the processing pipeline to\nget the spectral cubes from the recorded radiance raw images, an issue that is thoroughly addressed\nin Subsection 5.2 of this paper.\nThe original labelling of the scene images in the dataset comprises 10 different classes related\nmostly to the expected differences in the surface reflectance signatures of various materials: metal,\nvegetation, concrete, tarmac etc. Available ground truth masks however are not fully dense, since the\nlabelling method was aimed at favoring the accuracy in the spectral information provided for each\nclass by reducing spectral mixing. This means, for instance, that contour pixels of many items and\nthe surfaces on the background that cannot be precisely distinguished have not been labeled (Figure\n1). While this method of labelling is not an issue for non-convolutional ML models, it may have\nsome adverse effects on the performance of CNN-based segmentation models. In fact, the training\nof convolutional neural networks with \"weak supervision\" on sparsely labelled datasets is a line of\nresearch itself (see for instance [51] and references therein)."}, {"title": "3.2 Data Partitioning and Application Examples", "content": "In order to investigate the potential performance of segmentation systems based on the training of\nML models with this dataset, throughout this work we use two application examples that can be\nemployed in the development of ADAS/ADS. The first example focuses on segmenting images into\nthree classes: road (tarmac), road marks and non-drivable areas, the latter including the remaining\nclasses in the labelled masks. This low-complexity segmentation example would be aimed at developing\na system for the discrimination of drivable and non-drivable zones, together with a lane-keeping aid\nfor ADAS or even a trajectory planning system for ADS. In the second example, two additional classes\nhave been added to the model training (vegetation and sky), which show quite satisfactory spectral\nseparability indexes (see Table 1) and add relevant information for a scene understanding application."}, {"title": "3.3 Reference Baseline Spectral Classifiers", "content": "In a first approach to the problem, baseline neural classifiers were trained relying only on spectral\ndata. The results obtained, in addition to exposing the foreseeable limitations of segmentation based\non pure, non-processed spectral features, will be useful for assessing fairly the performance results of\nthe convolutional models presented in the following sections.\nWe trained medium-depth, fully connected ANNs with three hidden layers to perform image\nsegmentation on the per-pixel classification basis. First, spectral reflectance values were normalized"}, {"title": "4 FCNs for HSI Image Segmentation", "content": "It is well known that incorporating spatial information to the spectral data in HSI can be an effective\nmeans to overcome the limits of the spectral imaging technology for image segmentation. The use of\nconvolutional filters with tunable parameters in CNNs to extract spatial information has shown excel-\nlent performance in many applications. In particular, FCNs are neural computing architectures specif-\nically aimed at image segmentation. However, when convolutional models are designed and trained\nwith disregard for the computational constraints imposed by the target processing devices and the\nrequirements of the application (power, latency), the results obtained may prove to be impractical. In\nthis section we describe the development of an FCN that enhances the accuracy and robustness of the\nspectral classifiers while keeping model architecture in the range of tens of thousands of parameters\nfor efficient embedded processing."}, {"title": "4.1 Model development", "content": "U-Net is an FCN-based architecture intended for accurate image segmentation. This model is of the\ntype encoder-decoder, which means that after an encoding stage, there is a decoding or \"deconvolu-\ntion\" up-sampling stage that re-projects the extracted characteristics, and recovers the original image\nresolution to directly produce segmentation maps. Intermediate skip connections enable the fusion of\nlow- and high-level features. U-Net was originally intended for biological image segmentation [44] but\nhas been successfully used in other segmentation tasks, such as precision agriculture [51], food quality"}, {"title": "4.2 Segmentation Results", "content": "Having first selected the best model architecture, we have trained U-Net on the complete training\nset. The training has been performed on a NVIDIA GeForce RTX 3090-24GB GPU with an Adam\noptimizer (0.9 gradient decay factor and 0.999 squared gradient decay factor), a mini batch size\nof 128, an initial learning rate of 0.005 with drop period and weighted cross-entropy loss function,\namong other parameters, for, at most, 60 epochs.\nshows how the use\nof overlapping patches improves the segmentation performance, especially the precision, compared to\nthe non overlapping reconstruction. This is because the FCNs tend to fail to correctly predict the\nclasses on the patch contours since they lack part of the contextual information for the contour pixels."}, {"title": "5 System Prototyping and Characterization", "content": "As stated above, there are two main processes involved in the implementation of this HSI segmentation\nsystem: the preprocessing of the raw images provided by the snapshot camera and the FCN-based AI.\nIn this section we describe in detail how both processing stages have been developed, implemented and\ntested on three different embedded processing systems: a low-cost single board computer, an embedded\nGPU SOC and a PSoC with an embedded FPGA. We also give details of the benchmarking setup used\nfor the characterization of all three implementations and we analyze the results obtained."}, {"title": "5.1 Benchmark Setup", "content": "Proper hardware selection is essential for the successful deployment of AI in an edge device. In the\nparticular case of the ADAS/ADS, this decision is conditioned by the need to satisfy strict specifica-\ntions in terms of processing latency, cost and energy consumption without compromising the overall\nperformance of the algorithm (safety criticality considerations aside). Most current edge-AI deploy-\nments rely on SoCs including AI-oriented coprocessors (domain specific or even application specific).\nHowever, depending on the complexity of the AI model (number of parameters and FLOPS) and the\nconstraints associated with the specific application, the use of cheaper, lower-end devices could provide\nsatisfactory results. Consequently, we have selected three different prototyping platforms to explore\nthe performance of our application: a Raspberry Pi 4B, a NVIDIA Jetson Nano Development Kit and\nan AMD-Xilinx ZCU104 development board.\nThe Raspberry Pi 4 Model B (Pi4B) is the leading single-board computer of the Raspberry Pi\nfamily with significant enhancement in CPU, GPU and I/O performance. This single-board computer\nconsists of a quad-core Cortex-A72 (ARM v8) 64-bit SoC 1.5GHz with 8GB of LPDDR4 SDRAM,\nwhich includes a recent upgrade that lets the cores work in turbo-boost mode and reach a peak\nfrequency of 1.8GHz safely [42]. The NVIDIA Jetson Nano is a small, powerful computer for embedded\nAI applications with 4 GB 64-bit LPDDR4 SDRAM. The SoC combines a Quad-core ARM Cortex-\nA57 MPCore processor (1.43GHz of maximum theoretical frequency) with a Maxwell architecture\nNVIDIA GPU containing 128 CUDA cores [37]. Finally, the ZCU104 development board includes\na Zynq UltraScale+ MPSOC with a quad-core ARM Cortex-A53 processor (1.5GHz of maximum\ntheoretical frequency) and a dual-core Cortex-R5 real-time processor in the Processing System (PS)\nconnected to a 16nm FinFET Programmable Logic (PL) with access to a 2 GB 64-bit wide DDR4\nexternal memory [58].\nAlthough all three devices include ARM v8-A architecture Quad-Core Cortex processors (A53,\nA57 and A72), they do not only differ in CPU frequency but also in other aspects, such as L1 I/D\nCache Size, L2 Cache Size, execution order, number of pipeline stages and so on. Cortex A53 is said\nto be a power-efficient processor when compared to the more powerful A57, while A72, which is the\ndirect successor of the A57, was designed to improve its predecessor in the PPA metric: performance,\npower and area [3]."}, {"title": "5.2 Raw image preprocessing", "content": "Recording in real driving conditions implies that the setup of the hyperspectral camera cannot be\nkept invariable. Moreover, lighting conditions cannot be controlled due to changes in the scenarios,\nweather conditions and daylight. Additionally, the use of a moving platform (the vehicle) and the\npresence of moving objects severely constrain the exposition time and, in consequence, the amount of\nlight that can reach the sensor in each situation. The HSI-drive database contains images acquired in\nvery diverse conditions that require different setups of the camera and optics. This diversity makes\nit more difficult to effectively preprocess the raw images in order to generate hyperspectral cubes\nwith equivalent characteristics that can be used to successfully train AI models. In addition, this\npreprocessing has to be replicated on-the-fly during the inference phase, so the computational aspects\nof the preprocessing pipeline have also to be addressed.\nThe processing stages involved in the pipeline of the raw image preprocessing to obtain the cor-\nresponding reflectance hyperspectral cubes depend on the type of application. In our context, the\nprogrammed preprocessing algorithm consists of the following four steps: crop and clip, reflectance\ncorrection, band extraction, and translation to center (partial demosaicing).\nAs reported in Section 3, the CMOS sensor has a standard size of 1088x2048. However, as the\nmultispectral filter array (MSFA) does not cover the entire surface of the sensor, the resulting ac-\ntive area has a lower resolution (1080x2045) and so the acquired raw images need to be cropped\nand clipped. After that, the acquired radiance is normalized by taking a white reference frame as\nthe highest response from the sensor, resulting in a reflectance signal which is assumed to be robust\nunder different acquisition conditions. In order to eliminate the static noise of the sensor, bias cor-\nrection is previously performed by subtracting a dark reference frame from both the image and the\nwhite reference frames. The next step, known as band extraction, transforms the 2D representation\nobtained from the mosaic-shaped MSFA\ninto a hyperspectral cube. As a consequence of\nthe snapshot camera technology, each of the 25 pixels of the 5x5 mosaic contains spectral information"}, {"title": "5.2.1 Embedded Software Programming", "content": "The raw image preprocessing algorithm has been pro-\ngrammed in C and compiled to be executed as an embedded Linux application in the microprocessors\nof the benchmark devices as part of the HW/SW codesign for the implementation of the system.\nTo reduce its latency, we have combined thread-level parallelism (OpenMP pragmas) with data-level\nparallelism (Single instruction multiple data, SIMD, via Neon). OpenMP is an API to develop parallel\napplications on shared-memory processors, such as embedded systems and accelerators, in a flexible\nmanner [39]. OpenMP grants the programmer control of the thread creation, workload distribution\namong the threads (not only the scheduling type but also the restriction of a certain task to one\nthread), thread synchronization and variable attributes (i.e. which variables are shared among threads\nand which are kept private to avoid data incoherence and race conditions). The way of working with\nOpenMP is by adding compiler directives or pragmas and modifying environment variables that con-\ndition both the compile and runtime behaviour of the program in a fork-join model. OpenMP can be\ncombined with Neon (ARM Advanced SIMD architecture) and Floating Point technologies which are\nfully integrated into the processor and share the processor resources for integer operation, loop control\nand caching, significantly reducing the area and power cost [4]. SIMD instructions are carried out in\nthe 32 128-bit SIMD/floating-point registers that the AArch64 architecture includes.\nTo implement those techniques in our software design, on the one hand, some compiler directives\nhave been added to the code after analyzing how to parallelize the code execution among the different\nthreads. The analysis consisted of identifying parallel regions (coarse-grained parallelism) and then\nfocusing on the parallelizable loops (fine-grained parallelism). In order to parallelize loops, we have\nstudied the dependency between loop indexes or data inside the loops. Besides this, as all threads\ncan modify and access all variables, we have also examined when to change the scope of one of the"}, {"title": "5.2.2 Test Results", "content": "Table 10 shows the mean latency over 1000 iterations of the raw image pre-\nprocessing pipeline running on the Cortex A-72 (Raspberry Pi 4B), Cortex A-57 (Jetson Nano), and\nCortex A-53 (ZCU104) quad-core processors. According to these figures and if no pipeline schemes\nare implemented, this processing stage would limit the reachable throughput to 19.10 FPS, 40.56 FPS\nand 19.84 FPS respectively."}, {"title": "5.3 FCN Inference", "content": "The implementation of the FCN has been carried out in several phases, starting with the study of the\neffect of different quantization levels on the ARM v8 quad-core Cortex processors and ending with the\nimplementation of the optimized models on the AI hardware accelerators, when available, such as the\nNVIDIA GPU and the AMD-Xilinx DPUs on the FPGA."}, {"title": "5.3.1 Implementation Workflow and Tools", "content": "The design, training and validation of the FCN has\nbeen performed with MATLAB's Deep Learning Toolbox. While the most recent version of MATLAB\nincludes tools to directly export deep models, including segmentation FCNs, to other frameworks such\nas TensorFlow, and even provide tools for the automatic deployment of deep models in FPGAs [36],\nthis was not possible with the Matlab version used in this project (2021b)."}, {"title": "6 Conclusions", "content": "HSI contains relevant spectral information that can help to develop more capable intelligent vision\nsystems. In the field of ADAS and ADS, HSI can provide sufficient information to produce the correct\nsegmentation of relevant items in driving scenarios and thus provide the vehicles with meaningful\ndata for scene understanding. However, ADAS and ADS require capturing and processing HSI at video\nrates, which posses an additional challenge to the task of image segmentation. Nowadays, hyperspectral\nimaging at video rates in small formats is possible mainly by using snapshot cameras that incorporate\nstandard CMOS imaging sensors with filter-on-chip technology. Filter-array technology constrains,\nto some extent, the quality of the captured spectral and spatial information, especially in outdoor\nuncontrolled environments, as is the case in real driving situations. Consequently, to achieve good\nclassification performance and robustness, it is necessary to apply additional techniques beyond the\nmere use of raw spectral information.\nIt is known that the use of spatial information via convolution operations allows for obtaining a\nmore robust segmentation of images by applying deep-learning techniques, but it is at the cost of a\nhuge increase in the complexity of the models. However, the requirements of ADAS applications impose\ntight constraints on the complexity of the AI models, since they must be deployed in compliance with\nthe low cost, low latency and low power requirements of embedded computing platforms. In this regard,\nwe demonstrate that satisfactory segmentation results can be obtained for low/medium-complexity\nuse cases when combining spectral information from a snapshot camera with the spatial information\nobtained with light encoder-decoder FCN models. In particular, we provide evidence that with a\ncareful design and a hyperparameter optimization process, it is possible to achieve good performance\nwith FCNs of sizes in the range of tens of thousands of parameters.\nWe provide a benchmark of designed segmentation systems by the deployment on three significative\nembedded computing platforms: a multicore microprocessor, a SoC featuring an embedded GPU,\nand a PSoC containing an FPGA. We show that the FPGA-based PSoC approach is superior in\nterms of both latency and energy efficiency. The PSoC implementation allows for achieving almost\n20 fps of processing throughput with a simple two-stage processing pipeline (preprocessing stage and\ninference stage), which meets the latency requirements of the ADAS industry standards. In fact, it\nis important to highlight that, unlike other works published in this field of application, our tested\nprototypes incorporate not only the acceleration of the neural network inference via the AI dedicated\ncoprocessor, but also the necessary preprocessing of the acquired raw HSI which, in fact, reveals itself\nto be a bottleneck that needs to be taken into account in the image segmentation pipeline. In this\nsense, we also show that accelerating the preprocessing stage by taking advantage of both the thread-\nlevel and the data-level parallelism of modern microprocessor architectures in the writing of the code,\nmakes the difference in terms of throughput."}]}