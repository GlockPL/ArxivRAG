{"title": "MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map", "authors": ["Yuhong Chou", "Man Yao", "Kexin Wang", "Yuqi Pan", "Ruijie Zhu", "Yiran Zhong", "Yu Qiao", "Jibin Wu", "Bo Xu", "Guoqi Li"], "abstract": "Various linear complexity models, such as Linear Transformer (LinFormer), State Space Model (SSM), and Linear RNN (LinRNN), have been proposed to replace the conventional softmax attention in Transformer structures. However, the optimal design of these linear models is still an open question. In this work, we attempt to answer this question by finding the best linear approximation to softmax attention from a theoretical perspective. We start by unifying existing linear complexity models as the linear attention form and then identify three conditions for the optimal linear attention design: i) Dynamic memory ability; ii) Static approximation ability; iii) Least parameter approximation. We find that none of the current linear models meet all three conditions, resulting in suboptimal performance. Instead, we propose Meta Linear Attention (MetaLA) as a solution that satisfies these conditions. Our experiments on Multi-Query Associative Recall (MQAR) task, language modeling, image classification, and Long-Range Arena (LRA) benchmark demonstrate that MetaLA is more effective than the existing linear models. Code: https://github.com/BICLab/MetaLA", "sections": [{"title": "1 Introduction", "content": "Transformer with softmax attention [1] benefits from efficient parallel training and exhibits impressive performance on deep learning applications [2, 3, 4, 5, 6, 7], but it suffers from the quadratic growth of computation cost to the input length [8]. Linear recurrent models, such as LinFormer [9], SSM [10], and LinRNN [11, 12], are expected to achieve linear substitution of Transformer. The original intention of LinFormer is to replace softmax attention, which exploits the kernel approach to decompose softmax operation; typical work includes TransNormer [13, 14], RetNet [15], GLA [16]. On the other hand, SSMs, such as S4 [10] and Mamba [17], are models inspired by the classical state-space approach, which enjoys sub-quadratic training and inference like either a recurrence or convolution. In contrast, LinRNN is a revival of traditional RNNs, including RWKV-4 [18], Griffin [19], LRU [20], etc., which solves the training difficulties of traditional RNNs due to nonlinear dependencies between hidden states. It is natural to think that they are different types of models, since these LinFormer/SSM/LinRNN models have different origins and forms.\nThis work breaks this perception and abstracts existing LinFormer/SSM/LinRNN models into a unified linear attention form, which has the following significance: i) Facilitates understanding"}, {"title": "2 Background", "content": "For notations in this work, we use bold upper-case letters for matrices (e.g., Q, K), bold lower-case letters for row vectors (e.g., qt, kt), and italic upper-case for learnable parameter matrices (e.g.,WQ). We generally use the same alphabet to show the rows of a matrix, e.g., qt is the t-th row of Q.\nSoftmax Attention first calculates an attention map SoftAttMap (Q, K) through Q (Query), K (Key), and use the attention map to weight different tokens V (Value) later:\n$0 = SoftAttMap (Q, K) V = softmax(\\frac{Q K^T}{\\sqrt{d_k}} \\odot M) V \\in R^{n \\times d_v},$ (1)\n$Q, K = XW_Q, XW_K \\in R^{n \\times d_k}; V=XW_V  \\in R^{n \\times d_v},$ (2)\nwhere $W_Q, W_K \\in R^{d \\times d_k}, W_V \\in R^{d \\times d_v}$ are learnable matrices, n, d, dk, dv are sequence length, model dimension, Key/Query and Value dimension, respectively. $X \\in R^{n \\times d}$ refers to the input. $M \\in R^{n \\times n}$ is a mask matrix in autoregressive tasks to prevent a token from attending to future tokens. The t-th row of SoftAttMap (Q, K) is a probability distribution that represents the attention scores between token vt to others. Softmax attention in Eq. (1) enables efficient parallel training, but suffers from O(n\u00b2) time and memory complexity [9]. It uses the recurrent form during inference:\n$o_t = \\frac{\\sum_{s=1}^{t} exp(q_t k_s)v_s}{\\sum_{s=1}^{t} exp(q_t k_s)} \\in R^{1 \\times d_v},$ (3)\n$q_t, k_t = x_t W_Q, x_t W_K \\in R^{1 \\times d_k}; v_t = x_t W_V  \\in R^{1 \\times d_v}.$ (4)\nAt each time t, token mix is computed between query qt and all the keys, values before ks, vs(s \u2264 t). This \"KV cache\" results in O(n) time and memory complexity per token during inference."}, {"title": "3 General Form of LinFormer/SSM/LinRNN Mechanisms", "content": "Observing Eq. (3), Eq. (5), Eq. (7), and Eq. (10), we find that their recurrent forms during inference can all be understood from the view of maintaining hidden states. Softmax attention maintains an unlimited hidden state (KV cache). By contrast, LinFormer/SSM/LinRNN have limited hidden states: linear attention with $\\phi^T (k_t)v_t \\in R^{d_k \\times d_v}$, SSM with $h_t \\in R^{N \\times d}$, linear RNNs with $h_t \\in R^{1 \\times d}$, where dk > N > 1 in usual. Inspired by this fact, we unify LinFormer/SSM/LinRNN mechanisms in the form of linear attention, formally containing Query, Key, and Value matrices (see Fig. 1)."}, {"title": "4 Optimal Linear Approximation to the Softmax Attention Map", "content": "We here discuss the optimal approximation of LinAttMap to SoftAttMap based on its general form. The function of softmax attention is two-fold: i) Memorizing information, all the current and historical information can be stored in KV cache; ii) Modeling relationships, softmax attention can calculate arbitrary attention scores of stored information. Unfortunately, such a powerfully expressive attention map generated by softmax attention requires infinite hidden states. By contrast, linear attention expects to exploit limited hidden states to achieve the same functionality as softmax attention.\nSome existing linear models, such as Performer[31], RFA[32], etc., optimize the model with the goal of approximating the value of SoftAttMap. In contrast, this work investigates the functional approximation of SoftAttMap, which is the basis for value approximation. Specifically, we here attempt to answer two key questions: i) Can linear attention realize the function of softmax attention? ii) If it can be achieved, what kind of linear attention approximation is better? To achieve this goal, we first give the definition of necessary conditions of optimal linear approximation. Then we categorize the existing linear models based on the conditions of the optimal linear approximation.\nDefinition 4.1. Necessary Conditions of Optimal Linear Approximation to Softmax Attention Map. A function f (xt, xs|0) : R1\u00d7d \u00d7 R1\u00d7d \u2192 R, used to compute attention score between any xt and xs (tokens), with parameters 0, is an optimal linear approximation to softmax attention map if it satisfies: i) Linear complexity. Attention map can be computed in linear time, i.e., O(n) space and time complexity during training and O(1) space and time complexity during inference. ii) Dynamic memory ability. When handling inputs sequentially, f (xt, xs|0) with limited hidden states should be able to store the most important information adaptively while forgetting unimportant ones. iii) Static approximation ability: For an arbitrarily given softmax attention map P with scores pts, there must exists bounded 0 such that f(xt, xs|0) = pts, Vt, s = 1,\u2026, n. iv) Least parameter approximation: On the premise that the first three conditions are met, use as few parameters as possible to achieve approximation to softmax attention map.\nIn definition 4.1, Condition 0 (C0) underlines computational and memory efficiency. Conditions 1 (C1) and 2 (C2) consider memory and modeling ability of linear attention. Due to limited state size d, linear attention can only memorize the history of most important d tokens without information loss and precisely model arbitrary attention map of those d tokens. Condition 3 (C3) is our expectation to seek the least parameters on the premise that previous three conditions are met.\nTheoretical Analysis for Optimal Linear Approximation. For the C1 condition, suppose the information about Vt1,..., Vtd is successfully stored in St (t1,..., tdk \u2264 t), we will check whether the model can substitute unimportant vt\u2081 when the new important input vt+1 arrives.\nFor the C2 condition, Eq. (18) illustrates the LinAttMap only relate to qt = fq(xt, 0q), kt = fk(xt, 0k), at = fa(xt, \u03b8\u03b1). Denote decay At = diag(at). Assuming the inputs are good enough"}, {"title": "5 MetaLA Transformer", "content": "Transformer is stacked by a series of Encoder/Decoder blocks. Generally, each block is composed of two modules in sequence: token mixer and channel mixer [35, 36]. Softmax attention plays the role of the token mixer. In this work, we follow the Transformer architecture as a whole but use our proposed MetaLA module as the token mixer. Due to space constraints, the architecture of MetaLA Transformer is given in detail in appendix A3. Here we focus on describing the three enhancements of MetaLA relative to the general linear attention in Sec. 3 (see Fig. 2): i) The Key matrix is not used, which is based on our theoretical analysis. ii) Self-augmentation and iii) Short convolution are two other optional techniques to further enhance the modeling ability of our model.\ni) The Key matrix is not used. We exploit 1 at to replace kt, based on theoretical analysis in Sec. 4 and appendix A2, i.e., dynamic decay At is the key mechanism to achieve dynamic memory and static approximation, and K is not required. As shown in Tab. 3, compared with Eq. (13), the main improvement is:\n$S^h_t = diag(a_t)S^h_{t-1} + (1 - a_t)q_t v_t \\in R^{d\\times d}$ (21)\nwhich can be trained in a parallel form in Eq. (16). The only difference is that K is replaced by B and Bt,: = 1-at. With usage of At and Q, MetaLA can achieve linear approximation to SoftAttMap. Without usage of K (WK), we can allocate more parameters and utilize full-rank matrix Wa to produce dynamic decay rather than low-rank matrix used by GLA, such that we do not sacrifice expression capacity of fa and approximation performance of MetaLA.\nii) Self-augmentation can enhance a token's attention to itself, avoiding attention dilution [13]:\n$o^t = q_t S_t^h + \\sigma(q_t (w_{aug} (1 - a_t))v_t ) \\in R^{1\\times d}.$ (22)\nWithout changing the hidden state S in Eq. (21), the proposed self-augmentation (the second term on the right side of the equation) is only added on the output process, with a learnable parameter Waug \u2208 R1\u00d7dk. The proposed design has two advantages (more analysis in appendix A3.2): First, it maintains parallel computing; Second, it augments the information of current token itself and does not affect future output through inner state.\niii) Short convolution. An additional short convolution can be inserted before entering the MetaLA layer to enhance local interaction further, motivated by Mamba [17] and Griffin [19]."}, {"title": "6 Experiments", "content": "We conduct a comprehensive evaluation of MetaLA to validate its"}]}