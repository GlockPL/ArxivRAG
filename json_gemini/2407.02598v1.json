{"title": "AutoSplat: Constrained Gaussian Splatting for Autonomous Driving Scene Reconstruction", "authors": ["Mustafa Khan", "Hamidreza Fazlali", "Dhruv Sharma", "Tongtong Cao", "Dongfeng Bai", "Yuan Ren", "Bingbing Liu"], "abstract": "Realistic scene reconstruction and view synthesis are essential for advancing autonomous driving systems by simulating safety-critical scenarios. 3D Gaussian Splatting excels in real-time rendering and static scene reconstructions but struggles with modeling driving scenarios due to complex backgrounds, dynamic objects, and sparse views. We propose AutoSplat, a framework employing Gaussian splatting to achieve highly realistic reconstructions of autonomous driving scenes. By imposing geometric constraints on Gaussians representing the road and sky regions, our method enables multi-view consistent simulation of challenging scenarios including lane changes. Leveraging 3D templates, we introduce a reflected Gaussian consistency constraint to supervise both the visible and unseen side of foreground objects. Moreover, to model the dynamic appearance of foreground objects, we estimate residual spherical harmonics for each foreground Gaussian. Extensive experiments on Pandaset [36] and KITTI [12] demonstrate that AutoSplat outperforms state-of-the-art methods in scene reconstruction and novel view synthesis across diverse driving scenarios. Our project page is at: https://autosplat.github.io/", "sections": [{"title": "1 Introduction", "content": "View synthesis and scene reconstruction from captured images are fundamental challenges in computer graphics and computer vision [14, 25, 26], crucial for autonomous driving and robotics. Reconstructing detailed 3D scenes from sparse sensor data on moving vehicles [12, 36] is especially challenging at high speeds, where both the ego-vehicle and surrounding objects are in motion. These techniques enhance safety by simulating realistic driving scenarios, particularly for costly or hazardous corner cases.\nThe advent of Neural Radiance Fields (NeRFs) [18] transformed view synthesis and reconstruction by implicitly representing a scene using a multi-layer perceptron (MLP). Numerous efforts have addressed NeRF's challenges, such"}, {"title": "2 Related Work", "content": "Implicit Representations and Neural Rendering Volumetric rendering techniques, notably NeRF, have significantly advanced 3D reconstruction and novel view synthesis. However, NeRF encounters challenges including slow training and rendering, high memory usage, and imprecise geometry estimation, particularly with sparse viewpoints [18, 19, 21]. To address the slow training speed different approaches such as voxel grids [10,29], tensor factorization [5,6] as well as hash encoding [19, 32], have been explored. For improving the rendering latency, FasterNeRF [11], devised a graphic-inspired factorization to compactly cache a deep radiance map at each position in space, while efficiently querying that map using ray directions. MobileNeRF [8] and BasedSDF [40], achieve fast rendering speed by transforming implicit volumes into explicit textured meshes. To tackle the low-quality rendering of NeRF, Mip-NeRF [1], efficiently renders anti-aliased, conical frustums instead of rays. Mip-NeRF 360 [2], addresses the inherent ambiguity of large (unbounded) scenes from a small set of images by employing a non-linear scene parameterization, online distillation, and a distortion-based regularizer.\nUrban Scene Reconstruction with NeRF Modeling city-scale scenes is challenging due to managing thousands of images with varied lighting conditions, each capturing only a fraction of the scene, posing significant computational demands. MegaNeRF [31] and BlockNeRF [30] partition the scene into blocks and train separate NeRF models for each block. However, these approaches do not model dynamic objects conventionally found in autonomous driving scenarios. NSG [22] and MARS [35] perform dynamic scene modeling by incorporating a scene graph. Unlike NSG, SUDS [32] addresses reconstruction during ego-vehicle motion, utilizing LiDAR data for improved depth perception and optical flow to alleviate the stringent demand for object labeling. EmerNeRF [37] learns spatial-temporal representations of driving scenarios by stratifying scenes and using induced flow fields to enhance rendering precision of dynamic objects."}, {"title": "3 Method", "content": "3.1 Prerequisites\n3DGS [16] explicitly represents a scene using anisotropic 3D Gaussians initialized from a set of 3D points. It is defined as:\n$$G(x) = e^{-(x-\\mu)^T \\Sigma^{-1}(x-\\mu)}$$\nwhere, $\\mu\\in R^3$ and $\\Sigma\\in R^{3\\times3}$ represent the center vector and covariance matrix of each 3D Gaussian, respectively. Moreover, in 3DGS [16], each Gaussian is assigned an opacity o and color c attributes, where the latter is represented using spherical harmonic coefficients $f_{SH}$. For ease of optimization, the covariance matrix $\\Sigma$ is decomposed into a scaling matrix S and a rotation matrix R:\n$$\\Sigma = RSST RT$$\nFor differentiable rendering, the 3D Gaussians are splatted onto the image plane by approximating their projected position and covariance in 2D. By sorting the Gaussians according to their depth in camera space, each attribute of the Gaussian is queried and the final rasterized color C of a pixel is computed by blending the contributions of N overlapping Gaussians as:\n$$C = \\sum_{i=1}^{N} \\sigma_{i} \\prod_{j=1}^{N-1} (1 - o_{j}), \\hspace{0.2cm} \\alpha_{i} = o_{i}G(x')$$"}, {"title": "3.2 Overview", "content": "Given sequentially captured and calibrated multi-sensor data, which comprises a series of N images $(I_i)$ taken by a camera with its corresponding intrinsic $(K_i)$ and extrinsic $(E_i)$ matrices, along with the 3D LiDAR point clouds $L_i$ and corresponding dynamic objects trajectories $T_i$, our objective is to leverage 3DGS to reconstruct the 3D scene and synthesize novel views at any camera pose with new object trajectories. The overview of our proposed method is shown in Fig. 2. We begin by reconstructing a geometry-aware, static background. Then, from a 3D template, foreground objects are reconstructed, ensuring consistency between visible and unseen regions while modeling their dynamic appearances. Finally, we fuse the foreground and background Gaussians to produce a refined and unified representation."}, {"title": "3.3 Background Reconstruction", "content": "Autonomous driving scenes are large and unbounded, while sensor observations are sparse. Naively using 3DGS to represent the background from these limited observations is insufficient for realistic reconstruction and simulation. Furthermore, the Gaussians reconstructing the road and sky regions suffer from being geometrically incorrect and produce floater artifacts. While these Gaussians are capable of reconstructing the scene from ground-truth views, their incorrect geometry produces distortions evident when simulating novel scenarios such as laterally shifting the ego-vehicle as shown in Fig. 1.\nTo address these issues, the background training in our framework is conducted in two phases. In the first phase, the road and sky regions are decomposed from the rest of the background using semantic masks obtained from an off-the-shelf, pre-trained segmentation model [9]. By projecting LiDAR points to the"}, {"title": "3.4 Foreground Reconstruction", "content": "Foreground reconstruction in autonomous driving scenes is crucial for realistic simulation despite challenges like occlusions and dynamic appearances. Here, we introduce novel strategies to tackle these complexities in the 3DGS paradigm.\nConstructing Template Gaussians 3DGS faces challenges in reconstructing foreground objects due to its dependence on SfM techniques tailored for static scenes and its lack of motion modeling capabilities. To overcome these limitations, we need an alternative approach to initialize the Gaussians representing these foreground objects and optimize their properties. This can be done by leveraging randomly initialized points, accumulated LiDAR scans, or using single or few-shot 3D reconstruction methods [4, 20, 23]. Although LiDAR captures detailed geometry, it has limitations including blind spots and sparse surface"}, {"title": "3.5 Scene-Level Fusion", "content": "Scene-level fusion comprises blending the foreground and background Gaussians. When separately optimized, these two sets of Gaussians exhibit distortions when rasterized together, particularly evident near the foreground objects' borders.\nTo address these distortions, both the foreground and background Gaussians are fine-tuned together and supervised on the whole image. This will result in a fused foreground-background image, in which the distortions of both components are alleviated. Moreover, to address the noisy object trajectories, we optimize a transformation correction per object, comprising rotation and translation offsets. These are applied to foreground object tracks to overcome noisy 3D bounding boxes. The final loss term is obtained as:\n$$L = L_{BG} + L_{FG}$$"}, {"title": "4 Experiments", "content": "We present the experimental setup in Sec. 4.1, followed by a comparative evaluation of our approach against SOTA methods using publicly available datasets in Sec. 4.2. Additionally, we conduct a comprehensive examination of the proposed strategies to elucidate their effectiveness and potential advantages in Sec. 4.3."}, {"title": "4.1 Experimental Setup", "content": "Dataset We experimented with two open-source self-driving datasets, KITTI [12] and Pandaset [36]. For KITTI, our approach closely followed existing methods. Pandaset includes 103 urban driving scenarios in San Francisco, each with 80 image frames and corresponding LiDAR point clouds. We selected 10 challenging sequences with various dynamic scenes, including multiple foreground objects as well as day and night scenarios.\nEvaluation Metric When synthesizing novel views with ground-truth images, we use standard metrics such as PSNR, SSIM [34], and LPIPS [41] for quantitative evaluations. However, for novel view synthesis with lateral ego-vehicle trajectory adjustments, we report FID [15].\nImplementation Details Our implementation is based on the 3DGS framework [16]. Instead of SfM points, we use accumulated LiDAR point clouds for background initialization. Following 3DGS, our background training includes 30K iterations split into two phases of 15K iterations each. Foreground training comprises 5K iterations, followed by 10K iterations for scene fusion, where both foreground and background Gaussians are fine-tuned together. During fusion, attributes of the foreground object Gaussians are fined-tuned while for the background Gaussians adjustments are solely made to the opacity, as their appearance and geometry are established during the background training phase. More details are provided in the supplementary materials."}, {"title": "4.2 Main Results", "content": "In Pandaset experiments, we benchmark our method against SOTA methods for scene reconstruction and novel view synthesis tasks. Unlike novel view synthesis, where 10% of frames are excluded, all frames are used during training for scene reconstruction, representing upper-bound results. These results are presented in Tab. 1. Our method shows significant superiority over alternatives across various evaluation metrics, notably excelling in SSIM and LPIPS. While achieving similar PSNR to EmerNeRF, our method notably outperforms SUDS in novel view synthesis, with slightly reduced scene reconstruction performance. Additionally, our method offers faster execution speed, highlighting its efficiency for real-world applications. This evidence demonstrates the efficacy and robustness of our approach, making it a compelling solution for high-quality reconstructions and realistic synthesis. The qualitative novel view synthesis results on different scenes are shown in Fig. 4. Comparing the results, it can be seen that our method demonstrates exceptional capacity to generate more realistic renderings.\nSimulating lane changes is a crucial aspect of replicating real-world scenarios in autonomous driving environments. We comprehensively evaluated our approach against competitors by assessing scene realism across varying degrees of lateral shift for the ego-vehicle, using FID. Results in Tab. 2 show that our method consistently outperforms alternatives, demonstrating superior synthesis quality. These findings highlight our approach's efficacy in capturing complex"}, {"title": "4.3 Ablation Studies", "content": "We validate our method's design decisions through Pandaset experiments, encompassing diverse and challenging autonomous driving scenarios, thoroughly evaluating the impact of the proposed primary components.\nBackground Geometry Constraints For this investigation, we tried our method with and without the proposed background geometry constraints on all scenes and measured the average FID based on different amounts of lateral shift. As seen in Fig. 6 (a), with the proposed geometric constraints, the synthesized sequences exhibit lower FID, indicating better visual quality. Furthermore, the difference between the FID values in these two cases varies from 4.8 to 18.9 points as the amount of lateral shift increases from 1 meter to 3 meters. In Fig. 6 (b), the qualitative results depicting various degrees of lateral shift are presented. It is evident that, in the absence of geometric constraints, lateral camera displacement leads to significant distortion of the road and the disappearance of lane markings. Additionally, unregulated positioning of the sky Gaussians, while acceptable in ground-truth views, occludes the background during lateral shifts.\nForeground Initialization To validate the effect of using a 3D template for foreground Gaussian initialization, we compared its performance with random initialization in object 3D boxes. Average FID scores of the foreground objects"}, {"title": "5 Limitation", "content": "Our method is constrained to reconstructing rigid dynamic foreground objects, such as vehicles, and cannot accommodate non-rigid objects including pedestrians, cyclists, etc. Future work could explore more complex dynamic scene modeling methods to address this limitation. Furthermore, our approach depends on ground-truth 3D boxes and adapts a transformation offset for each object to rectify inaccuracies in their poses. Exploring alternative methods to reduce this dependency, such as leveraging motion information for foreground object identification and trajectory estimation, presents an intriguing avenue for further investigation."}, {"title": "6 Conclusion", "content": "This paper introduces AutoSplat, a novel approach for accurately reconstructing and synthesizing dynamic autonomous driving scenes. By constraining road and sky Gaussians, we achieve realistic novel view synthesis during ego-vehicle"}, {"title": "A. Implementation Details", "content": "Our approach builds upon the 3DGS framework [16], opting for accumulated LiDAR point clouds over conventional SfM points for background initialization. Aligned with the methodology of 3DGS, our background training unfolds across two distinct phases, each spanning 15K iterations and collectively summing up to 30K iterations. Background reconstruction requires masks for the road, sky, and other remaining background regions (excluding foreground objects). We acquire these masks through a pre-trained Mask2Former model [9]. Throughout the two phases of background training, we maintain a fixed positioning of road and sky Gaussians, ensuring their stability and preventing inadvertent reconstruction of other background regions.\nForeground training spans 5K iterations, during which we utilize masked ground-truth images to supervise the synthesis of foreground objects. To enforce the reflected Gaussian consistency constraint, Gaussians for each foreground object are reflected according to their respective reflection planes every alternate iteration. Subsequently, the rasterized foreground objects with reflected Gaussians undergo supervision using the corresponding masked ground-truth image. The foreground training is followed by an additional 10K iterations for scene fusion, wherein both foreground and background Gaussians are fine-tuned together. During fusion, attributes of the foreground object Gaussians are fined-tuned while for background Gaussians the adjustments are solely made to the opacity, as their appearance and geometry are established during the background training phase.\nAdditionally, our loss terms are configured with \u03b2 and \u03b3 values set to 1000 and 1, respectively. We increased the grad threshold to 0.001, as a lower threshold significantly increases unnecessary points. All experiments are conducted on a single NVIDIA Tesla V100 GPU with 32 GB of memory."}, {"title": "B. More Results", "content": "The qualitative results of the novel view synthesis (test frames) conducted on Pandaset are visually depicted in Fig. 11. The comparative analysis reveals the efficacy of our proposed method, showcasing superior synthesis quality in both background and foreground elements. This demonstrates the robustness and precision of our approach in accurately reproducing complex scenes with intricate details. It is worth noting, while we reconstruct the background with a high degree of realism, our method falls short in accurately modelling details in the sky regions, such as clouds.\nIn Fig. 12 more results on ego-vehicle lateral shift are shown. Through the utilization of the background geometry constraint embedded in our approach, we excel in generating backgrounds of exceptional fidelity, preserving intricate elements like road markings. Conversely, competing methods fall short of faithfully reproducing such features, frequently leading to a loss of detail or inconsistent"}]}