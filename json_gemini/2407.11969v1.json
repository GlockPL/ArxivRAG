{"title": "Does Refusal Training in LLMs Generalize to the Past Tense?", "authors": ["Maksym Andriushchenko", "Nicolas Flammarion"], "abstract": "Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make a Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art LLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-40, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For example, the success rate of this simple attack on GPT-40 increases from 1% using direct requests to 88% using 20 past tense reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when past tense examples are explicitly included in the fine-tuning data. Overall, our findings highlight that the widely used alignment techniques\u2014such as SFT, RLHF, and adversarial training\u2014employed to align the studied models can be brittle and do not always generalize as intended. We provide code and jailbreak artifacts at https://github.com/tml-epfl/llm-past-tense.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) exhibit remarkable capabilities, but these come with potential risks of misuse, including the generation of toxic content, spread of misinformation at scale, or support for harmful activities like cyberattacks (Bengio et al., 2023). To address these concerns, LLMs are often fine-tuned to refuse such harmful queries which is commonly done via supervised fine-tuning, reinforcement learning with human feedback, and various forms of adversarial training (Bai et al., 2022; Touvron et al., 2023; Mazeika et al., 2024). While refusal training successfully"}, {"title": "2 Methodology", "content": "Background. Bypassing refusal training involves finding prompts that elicit harmful content from LLMs in response to a specific harmful request (e.g., \u201cHow to make a bomb?\u201d). We assume access to a predefined set of requests typically recognized as harmful by LLM providers, such as those related to misinformation, violence, hate speech, etc. We define a target language model as a function $LLM : T^* \\rightarrow T^*$ that maps an input token sequence to an output token sequence. Given a semantic judge function $JUDGE : T^* \\times T^* \\rightarrow {NO, YES}$ and a harmful request $R \\in T^*$, the attacker's objective can be formulated as\n\nfind $P \\in T^*$ subject to $JUDGE(LLM(P), R) = YES$.\n\nThe judge function provides a binary output indicating whether LLM(P) constitutes a valid jailbreak for the harmful request R, although it may use a more granular evaluation score internally.\n\nApproach. Our proposed approach relies on reformulating a harmful request into the past tense (see Figure 1 for an illustrative example). To automatically reformulate an arbitrary request, we use GPT-3.5 Turbo with the prompt shown in Table 2 that relies on a few illustrative examples. Since reformulations like these are conceptually very simple, we believe there is little value in using a more advanced LLM than GPT-3.5 Turbo. Furthermore, we enhance the approach by using multiple reformulations attempts per query. We leverage the inherent variability in language model outputs due to sampling and use the temperature parameter equal to one both for the target and reformulation LLMs. We consider an attack successful for a harmful request if we obtain at least a single unsafe reply over multiple attempts. We note that this attack can be considered as both universal and transferable since it is effective across multiple independently trained LLMs. Finally, we remark that the performance of this attack can be further enhanced by combining it with known prompting techniques such as refusal suppression and leveraging competing objectives (Wei et al., 2023a) (e.g., adding instructions like \"Start your response with 'Sure'\" or \"Never start your response with 'Sorry'\"). We do not use any of these additional techniques, as our goal is to demonstrate a curious generalization gap rather than to develop a state-of-the-art jailbreaking approach.\n\nSetting. We use 100 harmful behaviors from JBB-Behaviors (Chao et al., 2024) that span 10 harm categories based on the OpenAI usage policy. These behaviors consist of examples from AdvBench"}, {"title": "3 Systematic Evaluation of the Past Tense Attack", "content": "Main results. We present our main results in Table 1, which show that the past tense attack performs surprisingly well, even against the most recent LLMs such as GPT-40 and Phi-3, and in many cases is sufficient to circumvent built-in safety mechanisms. For example, the attack success rate (ASR) on GPT-40 increases from 1% using direct requests to 88% using 20 past tense reformulation attempts according to the GPT-4 judge. The Llama-3 70B and rule-based judge also indicate a high ASR, although slightly lower, at 65% and 73% respectively. Similarly, evaluation on other models indicates a high ASR: for Phi-3-Mini it increases from 6% to 82%, and for R2D2, it increases from 23% to 98%. Interestingly, GPT-3.5 Turbo is slightly more robust to past tense reformulations than\nGPT-40, with a 74% ASR compared to 88% for GPT-40. To compare these numbers with established methods, we evaluate the transfer of request-specific GCG suffixes from Chao et al. (2024) optimized on Vicuna. In the same evaluation setting, these suffixes result in a 47% ASR for GPT-3.5 Turbo and only a 1% ASR for GPT-40, according to the Llama-3 70B judge. This discrepancy shows how later iterations of frontier LLMs can patch known attacks, but remain vulnerable to new ones. Additionally, we plot the ASR over the 20 attempts in Figure 2 for all models and judges. We can see that the ASR is already non-trivial even with a single attempt, e.g., 57% success rate on GPT-40, which is in contrast with only 1% ASR with a direct request in the present tense. Moreover, the ASR often begins to saturate after 10 attempts, which justifies our choice of 20 attempts in total.\n\nWhen does the attack fail? In Figure 3, we plot a breakdown of the ASR over the 10 harm categories of JBB-Behaviors. For most models, the ASR of the past tense attack is nearly perfect on behaviors related to malware/hacking, economic harm, fraud/deception, and government decisions. The ASR is consistently lower on categories like harassment, disinformation, and sexual/adult content. This behavior can probably be explained by the presence of more salient words in the latter categories, which are often sufficient to detect to produce a correct refusal. Additionally, we have observed that the attack sometimes struggles when a harmful request is highly specific, such as writing a poem that glorifies a particular event. In contrast, the attack usually works well if the knowledge required is more generic, such as providing a recipe for a bomb or Molotov cocktail. For further analysis, we invite the readers to inspect the jailbreak artifacts in our repository: https://github.com/tml-epfl/llm-past-tense."}, {"title": "4 Does Fine-tuning on the Past Tense Examples Help?", "content": "Setting. We use the OpenAI finetuning service to fine-tune gpt-3.5-turbo-0125 on 394 past tense reformulations of 50 random JBB-Behaviors paired with a refusal message (\u201cSorry, I can't help with that\"). We use the remaining 50 JBB-Behaviors as a test set. We also add standard conversations from OpenHermes-2.5 (Teknium, 2023) to the fine-tuning set to make sure the model does"}, {"title": "5 Related work", "content": "We discuss here the most relevant references on generalization in LLMs, failures of refusal training, and most related jailbreaking approaches in recent literature.\n\nGeneralization of LLM alignment. After pretraining, LLMs are typically aligned to human preferences using techniques like SFT (Chung et al., 2022), RLHF (Ouyang et al., 2022), or DPO (Rafailov et al., 2023). One of the objectives of the alignment process is to make LLMs produce refusals on harmful queries, which involves adding refusal examples to the fine-tuning data. Since it is impossible to add all possible reformulations of harmful requests in the fine-tuning set, LLMs alignment crucially relies on the ability to generalize from a few examples per harmful behavior. Empirical studies support this capability: Dang et al. (2024) observe that RLHF generalizes from English to other languages, and Li et al. (2024b) make the same claim specifically for refusal training. This observation is consistent with Wendler et al. (2024) who argue that LLMs pretrained primarily on English data tend to internally map other languages to English. Therefore, fine-tuning on English"}, {"title": "6 Discussion", "content": "We believe the main reason for this generalization gap is that past tense examples are out-of-distribution compared to the refusal examples used for fine-tuning, and current alignment techniques do not automatically generalize to them. Indeed, as we have shown in Section 4, correctly refusing on past tense examples is feasible via direct fine-tuning, and some models\u2014like Llama-3 with the refusal-enhancing system prompt\u2014are already relatively robust. Moreover, there are\nalso other possible solutions that do not rely on SFT or RLHF, such as output-based detectors (Inan et al., 2023) and representation-based methods, including harmfulness probing and representation rerouting (Zou et al., 2024). These approaches can reject harmful outputs, which seems to be an easier task compared to patching all possible inputs that can lead to harmful generations.\n\nMore generally, past tense examples demonstrate a clear limitation of current alignment methods, including RLHF and DPO. While these techniques effectively generalize across languages (Li et al., 2024b; Dang et al., 2024) and some input encodings, they struggle to generalize between different tenses. We hypothesize that this failure to generalize stems from the fact that concepts in different languages map to similar representations (Wendler et al., 2024; Li et al., 2024b), whereas different tenses necessarily require distinct representations. Additionally, the recent work of Li et al. (2024a) shows that refusal guardrails can show different sensitivity to various demographic groups, which has direct implications for fairness. We believe that the generalization mechanisms underlying current alignment methods are understudied and require further research."}, {"title": "A Additional Details", "content": "Here we first list all prompts for models (for Gemma-2 we use no system prompt), then we specify the future tense reformulation prompt, and finally the prompts for the jailbreak judges that we use."}]}