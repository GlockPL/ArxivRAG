{"title": "Does Refusal Training in LLMs Generalize to the Past Tense?", "authors": ["Maksym Andriushchenko", "Nicolas Flammarion"], "abstract": "Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or ille-gal outputs. We reveal a curious generalization gap in the current refusal training approaches:simply reformulating a harmful request in the past tense (e.g., \u201cHow to make a Molotov cocktail?\u201dto \u201cHow did people make a Molotov cocktail?\u201d) is often sufficient to jailbreak many state-of-the-artLLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo, Gemma-2 9B,Phi-3-Mini, GPT-40, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For ex-ample, the success rate of this simple attack on GPT-40 increases from 1% using direct requeststo 88% using 20 past tense reformulation attempts on harmful requests from JailbreakBenchwith GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the futuretense are less effective, suggesting that refusal guardrails tend to consider past historical ques-tions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when pasttense examples are explicitly included in the fine-tuning data. Overall, our findings highlightthat the widely used alignment techniques\u2014such as SFT, RLHF, and adversarial training\u2014employed to align the studied models can be brittle and do not always generalize as intended.We provide code and jailbreak artifacts at https://github.com/tml-epfl/llm-past-tense.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) exhibit remarkable capabilities, but these come with potentialrisks of misuse, including the generation of toxic content, spread of misinformation at scale, orsupport for harmful activities like cyberattacks (Bengio et al., 2023). To address these concerns,LLMs are often fine-tuned to refuse such harmful queries which is commonly done via supervisedfine-tuning, reinforcement learning with human feedback, and various forms of adversarial train-ing (Bai et al., 2022; Touvron et al., 2023; Mazeika et al., 2024). While refusal training successfully"}, {"title": "2 Methodology", "content": "Background. Bypassing refusal training involves finding prompts that elicit harmful content fromLLMs in response to a specific harmful request (e.g., \u201cHow to make a bomb?\u201d). We assume ac-cess to a predefined set of requests typically recognized as harmful by LLM providers, such asthose related to misinformation, violence, hate speech, etc. We define a target language model asa function \\(LLM : T^* \\rightarrow T^*\\) that maps an input token sequence to an output token sequence. Givena semantic judge function \\(JUDGE : T^* \\times T^* \\rightarrow \\{NO, YES\\}\\) and a harmful request \\(R \\in T^*\\), theattacker's objective can be formulated as\nfind \\(P \\in T^*\\) subject to \\(JUDGE(LLM(P), R) = YES\\).\nThe judge function provides a binary output indicating whether LLM(P) constitutes a valid jailbreakfor the harmful request R, although it may use a more granular evaluation score internally.\nApproach. Our proposed approach relies on reformulating a harmful request into the past tense(see Figure 1 for an illustrative example). To automatically reformulate an arbitrary request, we useGPT-3.5 Turbo with the prompt shown in Table 2 that relies on a few illustrative examples. Since\nreformulations like these are conceptually very simple, we believe there is little value in using amore advanced LLM than GPT-3.5 Turbo. Furthermore, we enhance the approach by using mul-tiple reformulations attempts per query. We leverage the inherent variability in language modeloutputs due to sampling and use the temperature parameter equal to one both for the target andreformulation LLMs. We consider an attack successful for a harmful request if we obtain at least asingle unsafe reply over multiple attempts. We note that this attack can be considered as both uni-versal and transferable since it is effective across multiple independently trained LLMs. Finally, weremark that the performance of this attack can be further enhanced by combining it with knownprompting techniques such as refusal suppression and leveraging competing objectives (Wei et al.,2023a) (e.g., adding instructions like \"Start your response with 'Sure'\" or \"Never start your responsewith 'Sorry'\"). We do not use any of these additional techniques, as our goal is to demonstrate acurious generalization gap rather than to develop a state-of-the-art jailbreaking approach.\nSetting. We use 100 harmful behaviors from JBB-Behaviors (Chao et al., 2024) that span 10 harm\ncategories based on the OpenAI usage policy. These behaviors consist of examples from AdvBench"}, {"title": "3 Systematic Evaluation of the Past Tense Attack", "content": "Main results. We present our main results in Table 1, which show that the past tense attack per-forms surprisingly well, even against the most recent LLMs such as GPT-40 and Phi-3, and in manycases is sufficient to circumvent built-in safety mechanisms. For example, the attack success rate(ASR) on GPT-40 increases from 1% using direct requests to 88% using 20 past tense reformulationattempts according to the GPT-4 judge. The Llama-3 70B and rule-based judge also indicate a highASR, although slightly lower, at 65% and 73% respectively. Similarly, evaluation on other modelsindicates a high ASR: for Phi-3-Mini it increases from 6% to 82%, and for R2D2, it increases from23% to 98%. Interestingly, GPT-3.5 Turbo is slightly more robust to past tense reformulations than\nGPT-40, with a 74% ASR compared to 88% for GPT-40. To compare these numbers with estab-lished methods, we evaluate the transfer of request-specific GCG suffixes from Chao et al. (2024)\noptimized on Vicuna. In the same evaluation setting, these suffixes result in a 47% ASR for GPT-3.5\nTurbo and only a 1% ASR for GPT-40, according to the Llama-3 70B judge. This discrepancy shows\nhow later iterations of frontier LLMs can patch known attacks, but remain vulnerable to new ones.\nAdditionally, we plot the ASR over the 20 attempts in Figure 2 for all models and judges. We cansee that the ASR is already non-trivial even with a single attempt, e.g., 57% success rate on GPT-40,\nwhich is in contrast with only 1% ASR with a direct request in the present tense. Moreover, theASR often begins to saturate after 10 attempts, which justifies our choice of 20 attempts in total.\nWhen does the attack fail? In Figure 3, we plot a breakdown of the ASR over the 10 harm cat-egories of JBB-Behaviors. For most models, the ASR of the past tense attack is nearly perfecton behaviors related to malware/hacking, economic harm, fraud/deception, and government de-cisions. The ASR is consistently lower on categories like harassment, disinformation, and sex-ual/adult content. This behavior can probably be explained by the presence of more salient wordsin the latter categories, which are often sufficient to detect to produce a correct refusal. Addi-tionally, we have observed that the attack sometimes struggles when a harmful request is highlyspecific, such as writing a poem that glorifies a particular event. In contrast, the attack usuallyworks well if the knowledge required is more generic, such as providing a recipe for a bomb orMolotov cocktail. For further analysis, we invite the readers to inspect the jailbreak artifacts in ourrepository: https://github.com/tml-epfl/llm-past-tense."}, {"title": "4 Does Fine-tuning on the Past Tense Examples Help?", "content": "Setting. We use the OpenAI finetuning service to fine-tune gpt-3.5-turbo-0125 on 394 pasttense reformulations of 50 random JBB-Behaviors paired with a refusal message (\u201cSorry, I can'thel p with that\"). We use the remaining 50 JBB-Behaviors as a test set. We also add standard conver-sations from OpenHermes-2.5 (Teknium, 2023) to the fine-tuning set to make sure the model does"}, {"title": "5 Related work", "content": "We discuss here the most relevant references on generalization in LLMs, failures of refusal training,and most related jailbreaking approaches in recent literature.\nGeneralization of LLM alignment. After pretraining, LLMs are typically aligned to human pref-erences using techniques like SFT (Chung et al., 2022), RLHF (Ouyang et al., 2022), or DPO (Rafailovet al., 2023). One of the objectives of the alignment process is to make LLMs produce refusals onharmful queries, which involves adding refusal examples to the fine-tuning data. Since it is impos-sible to add all possible reformulations of harmful requests in the fine-tuning set, LLMs alignmentcrucially relies on the ability to generalize from a few examples per harmful behavior. Empiricalstudies support this capability: Dang et al. (2024) observe that RLHF generalizes from English toother languages, and Li et al. (2024b) make the same claim specifically for refusal training. Thisobservation is consistent with Wendler et al. (2024) who argue that LLMs pretrained primarily onEnglish data tend to internally map other languages to English. Therefore, fine-tuning on English"}, {"title": "6 Discussion", "content": "We believe the main reason for this generalization gap is that past tense examples are out-of-distribution compared to the refusal examples used for fine-tuning, and current alignment tech-niques do not automatically generalize to them. Indeed, as we have shown in Section 4, correctlyrefusing on past tense examples is feasible via direct fine-tuning, and some models\u2014like Llama-3 with the refusal-enhancing system prompt\u2014are already relatively robust. Moreover, there are"}, {"title": "A Additional Details", "content": "Here we first list all prompts for models (for Gemma-2 we use no system prompt), then we specifythe future tense reformulation prompt, and finally the prompts for the jailbreak judges that we use."}]}