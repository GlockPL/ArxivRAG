{"title": "Investigating the Robustness of Deductive Reasoning with Large Language Models", "authors": ["Fabian Hoppe", "Filip Ilievski", "Jan-Christoph Kalo"], "abstract": "Large Language Models (LLMs) have been shown to achieve impressive results for many reasoning-based Natural Language Processing (NLP) tasks, suggesting a degree of deductive reasoning capability. However, it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust on logical deduction tasks. Moreover, while many LLM-based deduction methods have been proposed, there is a lack of a systematic study that analyses the impact of their design components. Addressing these two challenges, we propose the first study of the robustness of LLM-based deductive reasoning methods. We devise a framework with two families of perturbations: adversarial noise and counterfactual statements, which jointly generate seven perturbed datasets. We organize the landscape of LLM reasoners according to their reasoning format, formalisation syntax, and feedback for error recovery. The results show that adversarial noise affects autoformalisation, while counterfactual statements influence all approaches. Detailed feedback does not improve overall accuracy despite reducing syntax errors, pointing to the challenge of LLM-based methods to self-correct effectively.", "sections": [{"title": "Introduction", "content": "Deriving new knowledge from existing knowledge, as in deductive reasoning, is a key human cognitive skill necessary for various applications, including complex question-answering and decision-making. Deductive reasoning can be intensive for humans (e.g., taking a lot of time), require specific expertise (e.g., logicians), and lead to incorrect conclusions (e.g., due to biases). This promotes deductive reasoning over Natural Language (NL) as a key objective of human-centric AI. Automatic deduction engines aim to support humans by providing certifiable reasoning chains, avoiding invalid inferences, and accelerating the process. To provide effective support for deductive reasoning, AI must be able to formalise knowledge and rules provided in NL robustly.\nPerforming logical reasoning has received much interest in AI. In the early days, symbolic methods were aimed at transforming specific parts of language into logical statements [Pereira, 1982]. Recently, LLMs have been shown to achieve impressive results for many reasoning-based NLP tasks, suggesting a degree of deductive reasoning capability [Srivastava et al., 2023]. In particular, generating informal reasoning chains via Chain-of-thought (CoT) prompting achieves good reasoning performance on many benchmarks [Wei et al., 2022]. Contrary to symbolic methods, LLMs can answer a deductive reasoning task without providing a formal intermediate reasoning chain. Nevertheless, these informal reasoning chains do not need to follow truth-preserving proof rules, thus leading to reasoning chains that are hard to verify. Recent work shows that many informal reasoning chains suffer from lack of faithfulness [Ye and Durrett, 2022; Tanneru et al., 2024].\nAddressing these challenges, autoformalisation approaches [Pan et al., 2023; Olausson et al., 2023] use LLMs to translate NL input into a logical form, and a deterministic symbolic solver to perform the deductive reasoning. Autoformalisation is thus a hybrid approach, which aims to provide a faithful and verifiable reasoning chain while leveraging the linguistic manipulation skills of LLMs. Autoformalisation faces two key challenges: First, since they translate rich NL into a limited grammar of symbols and operations, it is critical to leverage a syntax with an optimal tradeoff between translation accuracy and expressivity. Second, while autoformalisation chains provide an opportunity for syntactic and semantic validation and error analysis, designing an effective and efficient terror recovery mechanism is non-trivial. While prior autoformalisation systems leverage multiple syntaxes and error recovery mechanisms, no systematic study has investigated their impact on autoformalisation accuracy.\nMeanwhile, a key requirement of LLM-based reasoning methods is their robustness to noise [Ebrahimi et al., 2018] and out-of-distribution [Hendrycks et al., 2021] inputs. Given the strong performance of LLMs across many domains and benchmarks [Sarlin et al., 2020], dealing with noisy data in reasoning tasks has been considered more important [Sourati et al., 2024]. Most evaluations have focused on adversarial noise (e.g., lexical perturbations) [Sarlin et al., 2020], while a recent study has also experimented with counterfactual statements [Liu et al., 2023]. While robustness evaluations for NLP tasks have yielded mixed results [Wang and Zhao, 2024; Liu et al., 2023], it remains unclear to which extent LLMs, in both informal and autoformalisation methods, are robust in logical deduction tasks.\nWe address these two challenges by investigating the ro-bustness of LLM-based deductive reasoning methods. Our overall approach is summarized in Figure 1. Our study makes three contributions. First, following standard practices in evaluating robustness, we devise a robustness framework for logical deduction with two families of perturbations: adversarial noise, where the model needs to preserve its label in the face of added irrelevant information, and counterfactual perturbations, where a single alteration in the context flips the label of the question. The combinations of the perturbations produce seven variants from a given dataset. Second, we synthesize the landscape of existing LLM-based logical deduction methods into a methodological framework with three dimensions: reasoning format, grammar syntax, and error recovery mechanism. For each of these dimensions, we incorporate representative approaches in the literature. Third, We perform extensive experiments with seven LLMs on eight perturbed variants of a recent modular benchmark. Our findings provide nuanced insights into the robustness of LLM-based methods on logical deduction."}, {"title": "Related Work", "content": "This section gives an overview of LLM-based reasoning methods and studies that evaluate their robustness."}, {"title": "Methods for LLM-based Reasoning", "content": "Informal reasoning. Scaling up the size of LLMs enables strong performance in many NLP tasks by few-shot prompting [Brown et al., 2020], which suggests inherent reasoning capabilities. CoT combines few-shot prompting with generating intermediate informal reasoning chains [Wei et al., 2022]. These informal reasoning skills motivated more elaborate prompting techniques, like Zero-Shot CoT [Kojima et al., 2022] or self-consistency by generating multiple chains [Wang et al., 2022b], as well as more complex structures than chains, such as Tree of Thoughts [Yao et al., 2023] and Graph of Thoughts [Besta et al., 2024].\nThese methods use an LLM to generate intermediate steps and evaluate the output through self-refinement. Similarly, ProofWriter [Tafjord et al., 2021] improves multi-hop reasoning by adding the intermediate results to the reasoning context [Tafjord et al., 2021]. A key benefit of informal reasoning chains is their flexibility, but this comes at the expense of guaranteeing faithfulness. Consequently, methods combining LLMs with formal reasoning have been suggested.\nAutoformalisation. Instead of informal reasoning, another way combines an LLM with a deterministic symbolic solver, e.g., a theorem prover. Here, the prover guarantees faithful and deterministic reasoning. This combination is known as autoformalisation. One of the first autoformalisation approaches using formal reasoning chains in combination with prompting is PAL [Gao et al., 2023]. The authors generate Python snippets alongside informal steps and generate the final response by executing the generated code snippets. Logic-LM [Pan et al., 2023] prompts LLMs to generate multiple task-specific formalisations (logic programming, first-order logic, satisfiability modulo theories and constraint satisfaction), which are solved by dedicated solvers. They report higher robustness for longer reasoning chains compared to CoT reasoning. The extension Logic-LM++ [Kirtania et al., 2024] tries to avoid new syntax errors by integrating a self-refinement mechanism. The LINC [Olausson et al., 2023] approach uses the idea of self-consistency from CoT and generates multiple formalisations to avoid formalisation errors. Autoformalisation models achieve high accuracy for many deductive reasoning benchmarks, showing clear benefits for complex reasoning.\nComparison. Our work is the first to explore the robustness of LLM reasoning approaches from prior work: direct few-shot prompting, CoT, and autoformalisation. Another contribution of our work is consolidating these methods with their syntax and error recovery choices into a coherent methodological framework."}, {"title": "Robustness Evaluation of LLM Reasoning", "content": "Evaluating deductive reasoning. The improvements of LLM-based reasoning have inspired the development of benchmarks investigating capabilities for deductive reasoning. The synthetic PrOntoQA [Saparov and He, 2022] dataset is built on modus ponens multi-hop reasoning and confirms reasoning capabilities for the largest models. However, they noted issues with proof planning and selecting correct proof steps for longer reasoning chains. The FOLIO [Han et al., 2022] and AR-LSAT [Zhong et al., 2022] benchmarks confirmed these errors for more complex reasoning and more naturalistic language. LogicBench [Parmar et al., 2023] is a recent benchmark that systematically studies the performance of LLM-based reasoners across multiple inference rules (e.g., modus ponens), reporting a good model performance for predicate logic and first-order logic grammars. A vital challenge identified by these works is unfaithful reasoning, i.e., the intermediate steps are not used to infer the final result [Ye and Durrett, 2022; Lanham et al., 2023; Tanneru et al., 2024].\nRobustness studies. A key requirement of LLM-based reasoning methods is their robustness to noise [Ebrahimi et al., 2018] and out-of-distribution [Hendrycks et al., 2021] inputs. Given the strong performance of LLMs across many domains and benchmarks [Sarlin et al., 2020], dealing with noisy data in reasoning tasks has been considered more important [Sourati et al., 2024], including adversarial and counterfactual perturbations. A variety of robustness tests have therefore been designed [Wang et al., 2022a]. These robustness benchmarks rely on the original problem's perturbations through paraphrasing and distractions on character, word, and sentence levels [Sourati et al., 2024; Sarlin et al., 2020]. Many works try to generate adversarial examples to better understand the generalization capabilities of LLMs, which are shown to significantly decrease LLM performance [Wang et al., 2022a]. RUPbench is a recent robustness study on logical reasoning of LLMs [Wang and Zhao, 2024], covering many reasoning datasets and all three types of perturbations. Another category involves semantic changes in the input texts. Semantic changes can also be performed purely on a linguistic or logical level. Recent approaches use these logic-based perturbations [Nakamura et al., 2023], which aligns with our robustness framework. Meanwhile, counterfactual studies of LLM robustness have been less common. One exception is RECALL [Liu et al., 2023], a benchmark based on external knowledge bases, whose study reveals that LLMs are generally susceptible to external knowledge with counterfactual information and that simple mitigation strategies cannot significantly alleviate this challenge.\nComparison. We conduct the first investigation of robustness differences between formal and informal LLM-based logical deduction methods. For this purpose, we base our experiments on LogicBench [Parmar et al., 2023], which systematically includes nine inference rules mapped to natural language situations. We develop seven additional variants of LogicBench incorporating adversarial noise and counterfactual statements, in line with prior work like [Nakamura et al., 2023] that studies LLM robustness on other reasoning tasks."}, {"title": "Framework for Evaluating Robustness in Logical Deduction Tasks", "content": "Task definition. Deductive reasoning is commonly formatted as binary Question Answering (QA) over a given context.\nThe task input consists of a NL question q and a set of logical premises transformed into an NL context c. The output of the task a is one of the two possible answers: true or false, indicating whether the context supports or refutes the question. Formally, an NL deductive reasoning task defines a function $f : (c, q) \\rightarrow \\{true, false\\}$.\nRobustness types. Following prior work on investigating the robustness of LLMs [Sarlin et al., 2020; Liu et al., 2023], we conceptualize two families of context perturbations. First, inspired by adversarial robustness studies [Nakamura et al., 2023], we posit that a reasoning system must be robust to adversarial noise (distractions), i.e., including irrelevant information in the context should not alter its prediction. Noise can shift the focus of a text, making it more difficult for an LLM to capture the relevant context. Second, a reasoning system must be robust to counterfactual shifts in the context. Namely, the system must be faithful to the provided context without including biases from implicit world models [Liu et al., 2023]. If the context states that men are immortal, the reasoning must overwrite its belief of men as mortal. By introducing counterfactual perturbations contradicting common sense, we investigate whether LLM-based logical deduction methods use reasoning shortcuts or perform genuine reasoning. We focus our perturbations on the NL context c rather than the question q because it corresponds to background knowledge sources, which tend to vary significantly in real-world applications (e.g., compare reasoning based on a research article to reasoning over a list of facts). The perturbed task can be formalised as $f' : (c', q) \\rightarrow \\{true, false\\}$.\nNext, we detail our framework for evaluating the robust-ness of LLM-based deduction methods to noise and counter-factual shifts."}, {"title": "Adversarial Noise", "content": "Formalisation. As monotonic reasoning, deductive reasoning must remain invariant to newly added irrelevant information, i.e., additional text that does not change the semantics of the premises used to derive the conclusion from the context. Let us consider a perturbed context that includes noisy information: $c' = d_1 ... d_kc$, where each di denotes a noisy sentence concatenated to c for $k \\in \\{1,2,4\\}$. The task function $f'$ must resolve to the same output as the original function f for its proof to remain valid. To avoid any distractions that might change the original semantics (e.g., by breaking inter-sentence co-references), we append distractions only to the beginning of the context.\nDesign of adversarial noise. We define three types of noise relevant to logical deduction tasks, which vary in their degree of referential content, formalisation complexity, and depth of logical reasoning. All noise sentences are sampled in a way that guarantees they do not impact the semantics of the context and the original proof. Figure 2 (bottom-left) shows examples of the three noise types.\n1.  Encyclopedic (E) perturbations are NL sentences expressing factual information such as It is 21 km from Karimnagar, on the highway from Karimnagar to Peddapa. They express real-world information following pragmatic principles of language [Grice, 1975]. Encyclopedic sentences are often difficult or even impossible to formalize in first-order logic. At the same time, encyclopedic facts are not connected by complex logical relations and lack the linguistic structure typical for reasoning contexts. In summary, they represent world information, have a high formalisation complexity, and have low logical reasoning depth.\n2.  Logical (L) statements provide a typical structure of reasoning contexts and contain only knowledge that can be natively formalised, such as All dresses are clothes. Logical sentences include information about the world, albeit in a fictional form. The required formalisation usually requires more complex reasoning, like multi-hop inferences. Thus, logical perturbations introduce limited world information, have low formalisation complexity, and high reasoning depth.\n3.  Tautological (T) perturbations are easily recognisable general statements, e.g., True is not false. They may include negations and a small number of disjunctions or conjunctions. As such, they contain no referential information and require simple reasoning. However, their formalisation is often difficult because many first-order logic syntaxes do not consider predefined truth constants, like 1, T. In summary, tautologies in NL contain no referential information, have high formalisation complexity, and have low reasoning depth."}, {"title": "Counterfactual Shifts", "content": "Formalisation. We consider common sense contradictions by altering original statements in c into counterfactual ones. The inclusion of counterfactual statements is motivated by the requirement for faithful reasoning. Namely, the validity of a deduction depends only on the structure of the logical form, which in turn should follow the original task description in the context and the question. To introduce counter-factual premises, we do not add new sentences; instead, we negate sentences from the original context c. Since original premises often state common sense knowledge, their negation naturally contradicts world knowledge. For example, in the modus ponens inference in Figure 1, we negate mortal(X), stating that All men are immortal. The altered context c' is formalized as follows: c' = neg(c), where neg(c) negates one of the terms in the original context. The label of the resulting function f' is opposite from that of the original function f.\nDesign of counterfactual perturbations. We assume a deductive reasoning dataset where the natural language form corresponds to a logical formula. Then, we introduce counterfactual perturbations by altering each logical formula using predefined rules that negate the formula (see full list of rules in the Appendix). For example, we negate the consequent $q(a)$ for the first implication of a constructive dilemma and adapt the inference to $\u00acq(a) \\lor s(a)$. Then, the negation to create the context c' is manually added before the relevant terms in c, thus guaranteeing high data quality. Since the inference results in the opposite label for f', we adapt the target label accordingly. Figure 2 (top-right) shows an example of such a contradiction. Importantly, all counterfactual perturbations can be combined with noise perturbations, leading to counterfactual versions of the original (Oc), encyclopedic (Ec), logical (Lc), and tautological (LT) sets."}, {"title": "Methodological Framework", "content": "We consider three key design dimensions: reasoning format, syntax, and error recovery mechanism to systematically analyse robust deductive reasoning capabilities for LLM-based methods. We describe each of these dimensions and their representative approaches studied in this work."}, {"title": "Reasoning Format", "content": "LLM-based methods can either: a) operate without any explicit formalisation as informal reasoning systems and generate the answer based on their internal representation, or b) generate a formal representation of the given input, which is fed into a theorem prover to find a valid proof.\nThe informal reasoning method instructs an LLM to answer q with Yes or No based on a context. We employ few-shot prompting using three manually engineered in-context examples. To make the model more robust, two of the three in-context examples include distractions with irrelevant information. We explicitly note in the instruction part of the prompt that the provided contexts may contain irrelevant details. The model is evaluated in two modes: direct prompting, where it answers directly, and CoT prompting, where it generates step-by-step reasoning in natural language before generating an answer. To support CoT prompting, we manually created in-context examples with fine-grained natural language reasoning steps designed to improve performance [Wei et al., 2022]. The model's answers are extracted using a regular expression (details in Appendix E).\nWe include an autoformalisation method combining a symbolic theorem prover with an LLM. This approach follows Logic-LM [Pan et al., 2023] to formalise the context and query into symbolic representations, which are then deterministically evaluated. The process is divided into two sub-tasks: First, the model generates formal representations of the context and query, referred to as CLF (context logical form) and qLF (query logical form). Second, a symbolic theorem prover evaluates these logical forms to determine whether qLF can be derived from CLF, producing a true or false outcome. This approach allows for a transparent and verifiable reasoning process grounded in logical consistency. In practice, we prompt the LLM to create the logical forms using the same three in-context examples as in the informal reasoning approach. The prompt is extended with instructions describing the formalisation syntax, following the methodology of Logic-LM. The resulting logical forms are parsed and combined into an Abstract Syntax Tree (AST), which provides the input for a theorem prover."}, {"title": "Formalisation Syntax", "content": "While the reasoning performance should be independent of the particular formalisation syntax, models may perform better with specific syntaxes, e.g., because of their frequency in the training data [Razeghi et al., 2022]. It is an open question whether the choice of syntax for formal reasoning impacts the model's translation performance and the robustness of its reasoning. Although all the syntaxes we consider represent First-Order Logic (FOL), their surface form variations may influence the formalisation ability of LLMs. The three evalu-"}, {"title": "Error Recovery Mechanism", "content": "To make autoformalisation more robust, we synthesise strategies for handling syntactic and semantic errors. Syntactic errors occur when the logical forms generated by LLMs do not follow the required syntax. Syntactic errors are easy to detect as logical forms cannot be parsed if they violate grammatical rules. In contrast, semantic errors, such as incomplete context representation, are more challenging to identify and resolve. We apply task-specific heuristics to identify suspicious constructs that lead to semantic errors and generate warnings. Unknown predicates as part of QLF are one example of such a construct, because they indicate an incomplete context. We consider four strategies for handling these errors:\nNo recovery. The baseline approach does not attempt to correct errors. Instead, we predict a random value, true or false, as a fallback strategy. We avoid introducing an evaluation bias associated with more complex strategies, such as CoT-based refinement, which is commonly done in prior work [Pan et al., 2023; Kirtania et al., 2024], as this would blur the comparison with other methods.\nError type feedback. The LLM is prompted to refine the logical form using a generic parsing error message, such as 'parsing error'. This type of message does not need a parser with an error handler, though it fails to point to specific errors in the logical form. Prior work has shown the effectiveness of this simple feedback method [Pan et al., 2023].\nError message feedback. A more detailed approach where the LLM is given specific feedback, highlighting the exact parts of the logical form that violate the syntax. Creating this kind of feedback necessitates an error-handling strategy for the parser. For example, the missing argument in man / mortal(Socrates) results in the error message: mismatched input '^' expecting '('. Using error messages from parsers as feedback to improve LLMs performance when synthesizing a formal language has shown promising results in code generation [Zhang et al., 2023; Jiang et al., 2024].\nWarning feedback. This strategy extends the error message with warnings generated from heuristics to recognize semantic errors, inspired by the \"soft\" critics in the LLM-modulo method [Kambhampati et al., 2024], where it has been shown to enhance robustness. Notably, soft critics have not been incorporated in prior LLM-based methods for logical deduction."}, {"title": "Experimental Setup", "content": "Dataset details. The robustness evaluation is based on the FOL part of the LogicBench (Eval) [Parmar et al., 2023] dataset, containing 520 samples with 180 unique contexts. LogicBench systematically covers nine different inference rules. The samples are automatically generated by prompting GPT-3.5 and manually verified. 340 of the total of 520 samples are negative examples constructed by negating the conclusion. If the conclusion is a disjunction, each part of the disjunction is negated, resulting in a slight class imbalance. The authors report a mean accuracy of around 90% for three human annotators. By applying our robustness framework from \u00a73, we obtain seven perturbed variants of LogicBench.\nPerturbations. The noise sentences are randomly sampled from a source s. We sample encyclopedic perturbation sentences from 10,000 abstracts of Wikipedia articles gathered via its API. As a logical reasoning source, we use sentences from 1001 contexts of the deduction QA benchmark FOLIO [Han et al., 2022]. As tautologies, we manually write 22 sentences. All sentences use negations and, at most, one disjunction or conjunction. A complete list can be found in Appendix B. We randomly sample noise perturbations and add them to each sample. We do not alter the class distribution, i.e., we keep LogicBench's original class imbalance.\nWe consider unique contexts for eight out of the nine logical forms to create counterfactual statement perturbations, resulting in 160 samples. We create a balanced dataset by altering between valid and invalid queries from LogicBench.\nMetrics. We use accuracy as a standard metric for classification tasks. We report execution rate as the fraction of parsable texts and valid accuracy as the accuracy on these parsable samples in the Appendix A due to space limitations.\nLLMs. We test GPT 40-mini, as well as a smaller and a larger variant for the three open-source LLM families: Gemma-2 (9b and 27b), Mistral (7b and Small), and Llama 3.1 (8b and 70b)."}, {"title": "Evaluation", "content": "We provide three sets of insights into this section, organised as findings (F*). We quantitatively study the effect of the adversarial and counterfactual perturbations on the performance of informal reasoners and autoformalisation methods. Then, we dive deeper into method variants. Finally, we analyse the nature of formalisation errors made by the models."}, {"title": "Robustness Analysis", "content": "F1: Noise perturbations have a stronger effect on formal-isation methods than informal LLM reasoners. Table 1 shows that, on average, the accuracy of both direct and CoT informal reasoning remains between 73% and 74% in the face of added noise. While the autoformalisation method performs similarly to informal reasoners on the original dataset, its performance decreases between 4% and 11%. The accuracy drops especially with logical (L) and tautological (T) distractions, whose logical language formats trick the LLM into formalizing the noisy clauses. On the other hand, the linguistically complex and more natural sentences of encyclopedic distractions show a minor effect, suggesting that LLMs successfully avoids formalizing the more complicated sentences.\nF2: All LLM-based reasoning methods suffer a drop for counterfactual perturbations. Table 1 shows that counterfactual statements cause a significant decrease in performance for both the informal reasoners and autoformalisation methods of between 12% and 13% on average. Moreover, this observation also holds for all tested models, i.e., none are robust towards counterfactual perturbations across every evaluated dimension. Even the strongest model, GPT 40-mini, yields a performance of 63-68%, which is relatively close to the random performance of 50%. The high impact of counterfactual statements (the single \"not\" inserted) could be due to the inability of LLMs to overwrite prior knowledge with explicitly stated information or memorization of the answers. We study the error sources further in \u00a76.3.\nF3: Introducing multiple noise sentences has an effect only for logical distractions. We show the impact of introducing between one and four sentences for the two top-performing autoformalisation models in Figure 4. The figure shows similar trends with and without counterfactual perturbations. As additional logical distractions are introduced, the model performance consistently decreases. Tautological (T) distractions lead to a decline in accuracy with a single disruptive sentence, yet adding more noise does not worsen the outcome. The tautological corpus introduces truth constants for all sentences as a persistent unseen logical construct. Given that this leads only to a decrease for a single occurrence, we can assume that a model can consistently handle the same unseen logical construct. In contrast, the logical corpus increases the chance of adding text, requiring new, previously unseen reasoning constructs for each added sentence. The impact of encyclopedic noise remains negligible, generalising F1 to k sentences. Similarly, counterfactual perturbations remain much more effective for all settings, generalising F2."}, {"title": "Impact of Method Design", "content": "F4: CoT prompting is most impactful when both noise and counterfactual perturbations are applied. The accuracies for the individual LLMs in Table 1 show that the impact of CoT is negligible for noise-only datasets (first four columns). Meanwhile, the benefit from CoT is most pronounced in the datasets that combine noise and counterfactual perturbations. The better-performing informal prompting strategy for a model remains stable for all types of distractions. Still, the decline in performance due to counterfactuals leads to a less consistent preference for a specific prompting style.\nF5: The best-performing grammar differs per model and is unstable across data versions. The evaluation of different logical forms for formal LLM-based reasoning in Table 2 shows the preference of some models for specific syntactic formats. Llama 3.1 70B has a considerable improvement of"}, {"title": "Error Analysis", "content": "F7: Autoformalisation increases syntax errors for noise perturbations. The low performance for noise perturbations correlates with more syntax errors for all models and distraction categories (cf. execution rates in Table 4). The three worst-performing models (both Mistral models, Gemma-2 9b) generate, at best, for 37% and, at worst, for only 4% of the samples, a valid logical form. Gemma-2 9b and Llama3.1 8b produce more syntax errors than the larger counterparts, suggesting that larger models are more robust towards noise perturbations. The accuracy of syntactically valid samples is higher than the informal reasoning methods for most distractions (Table 5), motivating informal reasoning as a backup strategy for formal reasoning. The error message feedback reveals two common syntax errors: 1) errors by models with an initial low execution rate exhibit issues with the template structure, including using incorrect keywords or adding conversational phrases; 2) perturbation-related errors, the most common of which is using undefined truth constants as part of tautological distractions.\nF8: Autoformalisation increases semantic errors for counterfactuals. Unlike the introduced noise, counterfactual perturbations do not lead to more syntax errors. The execution rate in Table 4 is stable or improves for counterfactuals. However, we see a drop in accuracy for the counterfactual column Oc in Table 1 and can conclude that the number of logical forms with semantic errors has to increase. This suggests that the introduced negation is not correctly formalised. Looking at the warnings generated by the feedback mechanism, for GPT 40-mini, 161 warning messages are generated on the unperturbed data. 54 of these were fixed with a single iteration. Not considering predicates and individuals as part of the context is the most frequent warning across all models."}, {"title": "Conclusion", "content": "We presented the first study of the robustness of LLM-based deductive reasoning methods by introducing two types of perturbations: adversarial noise and counterfactual statements. These perturbations were used to examine the methodological aspects of LLM reasoners based on their format, syntax, and feedback mechanism for error recovery. While adversarial noise only affects autoformalisation approaches, counterfactual statements remain a significant challenge for all variants of the tested method. While feedback strategies may lead to fewer syntax errors in autoformalisation methods, the refined formalisations tend to be semantically incorrect, failing to increase accuracy. We call on future work to devise more advanced mechanisms for detecting, reporting, and incorporating semantic errors. We also anticipate generalizing the study in this paper to other logical deduction datasets."}, {"title": "Experiments", "content": ""}, {"title": "Tautology corpus", "content": "\u2022 False is not true.\n\u2022 True is not false.\n\u2022 Not false is true.\n\u2022 Not true is false.\n\u2022 False and true is not true.\n\u2022 False and not true is false.\n\u2022 False and not false is false.\n\u2022 Not true and true is false.\n\u2022 Not true and false is false.\n\u2022 True and false is not true.\n\u2022 True and true is not false.\n\u2022 True and not false is true.\n\u2022 Not false and true is true.\n\u2022 Not false and false is false.\n\u2022 True or not true is true.\n\u2022 True or true is true.\n\u2022 False or not false is true.\n\u2022 False or not true is false.\n\u2022 Not true or false is not true.\n\u2022 Not true or true is true.\n\u2022 Not false or false is true.\n\u2022 Not false or true is not false."}, {"title": "Inference rules for counterfactual statements", "content": "Bidirectional dilemma\n\u2200x (p(x) \u21d2 q(x)) \u2227 (r(x) \u21d2 s(x)) \u2227 (p(a)\u2228 \u00acs(a)) = (\u00acq(a) \u2228 \u00acr(a))\nNegated: \u2200x (p(x) \u21d2 \u00acq(x)) \u2227 (r(x) \u21d2 s(x)) \u2227 (p(a) \u2228 \u00acs(a)) = (\u00acq(a) \u2228 \u00acr(a))\nConstructive dilemma\n\u2200x ((p(x)\u21d2q(x))\u2227(r(x) \u2192 s(x)))\u2227(p(a)vr(a)) = (q(a) Vs(a))\nNegated: \u2200x ((p(x) \u2190 \u00acq(x)) \u2227 (r(x) \u21d2 s(x))) \u2227 \u039b (p(a) Vr(a)) = (\u00acq(a) Vs(a))\nDisjunctive syllogism\n\u2200x (p(x) \u2228 q(x)) \u2227 \u00acp(a) = q(a)\nNegated: \u2200x (p(x) \u2228 \u00acq(x)) \u2227\u00acp(a) = q(a)\nExistential generalization\np(a) =\u2203x p(x)\nNegated: p(a) = \u2203xp(x)\nHypothetical syllogism\n\u2200x (p(x) \u21d2 q(x)) \u2227 (q(x) \u21d2 r(x)) = (p(a) \u21d2 r(a))\nNegated: \u2200x (\u00acp(x) \u21d2 q(x)) \u2227 (q(x) \u21d2 r(x)) = (p(a)\u21d2 r(a))\nModus ponens\n\u2200x (p(x) q(x)) \u2227 p(a) = q(a)\nNegated: \u2200x (p(x) \u21d2 \u00acq(x)) \u2227p(a) = \u00acq(a)\nModus tollens\n\u2200x (p(x) \u21d2 q(x)) \u2227 \u00acq(a) = \u00acp(a)\nNegated: \u2200x (\u00acp(x) \u21d2 q(x)) \u2227\u00acq(a) = p(a)\nUniversal instantiation\n\u2200xp(x) = p(a)\nNegated: \u2200 x \u00acp(x) = \u00acp(a)"}, {"title": "Implementation details", "content": "If available, the specific instruction-tuned variant has been used for the tested LLMs. Except for GPT 40-mini, all models are provided from hugging-face and used with 4-bit quantization.\nThe Vampire theorem prover processes the formal method's input, generating proofs by refutation to check whether the query logically follows from the context.\nOur FOL variant is adapted from the ANTLR grammars-v4 repository. For each grammar in \u00a7 4.2, we generate a parser using ANTLR4 to decouple the logical forms from the input requirements of the theorem prover.\nFor all error recovery strategies, the LLM prompt includes three in-context examples. When recovery is attempted, refinement is limited to three iterations. If the error persists, the random fallback strategy is applied. This setup allows us to systematically compare the robustness of these recovery methods in the autoformalisation process.\nThe generated warnings use three heuristics to identify semantic errors. First, checking for predicates and individuals only mentioned in the query to avoid an incomplete context. Second, ensuring that all predicates with the same identifier use the same amount of parameter (checking for narity). Finally, avoiding similar named predicates and individuals based on Levenshtein distance."}, {"title": "Prompts", "content": ""}, {"title": "Direct prompt", "content": "Given the context and"}]}