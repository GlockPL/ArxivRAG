{"title": "Learning Symbolic Task Decompositions for Multi-Agent Teams", "authors": ["Ameesh Shah", "Niklas Lauffer", "Thomas Chen", "Nikhil Pitta", "Sanjit A. Seshia"], "abstract": "One approach for improving sample efficiency in cooperative multi-agent learning is to decompose overall tasks into sub-tasks that can be assigned to individual agents. We study this problem in the context of reward machines: symbolic tasks that can be formally decomposed into sub-tasks. In order to handle settings without a priori knowledge of the environment, we introduce a framework that can learn the optimal decomposition from model-free interactions with the environment. Our method uses a task-conditioned architecture to simultaneously learn an optimal decomposition and the corresponding agents' policies for each sub-task. In doing so, we remove the need for a human to manually design the optimal decomposition while maintaining the sample-efficiency benefits of improved credit assignment. We provide experimental results in several deep reinforcement learning settings, demonstrating the efficacy of our approach. Our results indicate that our approach succeeds even in environments with codependent agent dynamics, enabling synchronous multi-agent learning not achievable in previous works.", "sections": [{"title": "1 INTRODUCTION", "content": "Using a single reward signal for a team of agents in multi-agent reinforcement learning (MARL) can make it challenging for agents to understand how their individual behavior impacts the overall reward. This challenge in MARL is known as the credit assignment problem [1] and can severely limit the effectiveness of naive reinforcement learning approaches in the multi-agent setting [23, 35].\nOne method for addressing the credit assignment problem is to formulate the task as a symbolic concept that can be precisely decomposed into sub-tasks for assignment to individual agents [21, 33]. By using the reward signal from each sub-task, agents are credited for completing the specific sub-task they are assigned, even if the overall task is not achieved.\nPrevious literature has established sufficient conditions for \"valid\" decompositions of multi-agent reward machines [37], an automaton-based symbolic task structure, where satisfaction of the sub-tasks provably satisfies the overall task [21]. Human-designed decompositions that satisfy these validity conditions help the agents learn to accomplish the task more quickly in tabular settings. Further work demonstrates that valid decompositions can be automatically generated based on human-designed heuristics [33].\nThese prior approaches to reward machine decomposition are limited when many possible decompositions exist. In these cases, the optimal decomposition of a task must be carefully designed and selected by a human. Selecting a meaningful task decomposition amongst many possible options often requires prior knowledge of the environment that is not assumed in standard RL, making the process tedious at best and impossible at worst. If a task decomposition is selected arbitrarily, the decomposition may not be compatible with the specifics of the environment to effectively break down the task.\nMotivating example. Consider the \"Repairs\" environment in Figure 2, where a team of three robot agents must visit a number of communication stations in their environment to make repairs. First, any two of the agents must meet at headquarters (HQ, denoted by the control tower symbol), at which point a ready signal is sent out to both stations (red and yellow) to inform the stations that agents will be visiting each station to perform repairs. The agents are then tasked to visit these stations in any order. The agents can independently move around in grid in any of the four cardinal directions, or remain still. We can formulate this task in the form of a Reward Machine (RM), an automaton that encodes the objective over high level 'events' which occur in specific states of the environment (visualized in Figure 2). RMs offer a precise notion of task completion and can easily capture temporal dependencies required in objectives (i.e., visit the stations after HQ).\nIn the Repairs environment, there exists a hazardous region, encoded in orange, that prevents more than one agent from entering the region at a time. The location and existence of this hazardous region is not known to the agents a priori, which prevents this constraint from being encoded in the reward machine as part of the task description.\nThis task, even with no prior knowledge of the environment, can naturally be decomposed in a number of ways. For example, Agent 1 and Agent 2 can be tasked with meeting at HQ, and then Agent 2 can visit the yellow station and leave the hazardous region while Agent 3 visits the red station. Alternatively, Agents 2 and 3 can meet at HQ, and then Agent 1 can visit both the red and yellow stations. Although many plausible decompositions exist, knowing which decomposition of the overall task leads to the most efficient completion of the task (i.e., is optimal) largely depends on the dynamics of the underlying environment. In our case, Agent 3 happens to be closest to the red station, which might mean that assigning that station to them would be optimal. However, without knowledge of the layout of the environment, as is standard in a model-free setting, this would be impossible to know ahead of time.\nOur Contributions. In this work, we address the aforementioned limitations by introducing an approach to automatically find optimal decompositions of an overall task into sub-tasks for a team of agents. Our method is a lightweight extension of standard MARL and is applicable to model-free settings with no prior information about the environment or individual capabilities of the agents.\nWe summarize our approach briefly as follows. At the beginning of training, we generate possible candidate decompositions of our overall task as assignments of sub-tasks for our agents to accomplish. Then, during training, we explore selecting different decompositions for our team of agents and observe their performance as they attempt to learn a goal-conditioned policy that can achieve the variety of possible sub-tasks captured by our candidates. We record the value of agents' performances for different sub-tasks, and use this information to intelligently select subsequent decompositions for our agent team. This allows us to simultaneously learn (1) the optimal decomposition for our task and (2) policies for each agent that optimize said decomposition. We leverage selection strategies popularized in the multi-armed bandit algorithm literature [4, 6, 17] to balance exploring different candidate decompositions with exploiting the value our agents achieve as they learn sub-tasks during training. We visualize our approach in Figure 1.\nIn addition to our decomposition selection strategy, we introduce a novel training setup for RM-driven MARL that rectifies issues caused by dependent agent dynamics. Prior approaches [21, 33] to RM-driven MARL make the assumption that the dynamics of each agent are independent of other agents, so that each agent can be trained individually in an environment absent from other agents. This assumption is often impractical: it is inefficient to train individual agents independently, and in many cases, multi-agent dynamics are codependent. For example, an agent that has completed their sub-task may obstruct another agent from the completion of their own sub-task. Our setup gives each agent in the team a global view of the overall task along with their sub-task, enabling agents to learn policies that accomplish their individual sub-tasks and help facilitate the completion of other agent's sub-tasks as well."}, {"title": "2 PRELIMINARIES", "content": null}, {"title": "2.1 MDPs and Labeled Markov Games", "content": "A Markov Decision Process (MDP) $M = (S, A, T_M, d_0, \\gamma)$ is a tuple that consists of a state space $S$, an action space $A$, a transition function $T_M: S \\times A \\rightarrow S$, an initial state distribution $d_0 \\in \\Delta(S)$, and a discount factor $0 < \\gamma < 1$. A (stationary) policy $\\pi : S \\rightarrow \\Delta(A)$ in $M$ produces a distribution over actions given a state. Following standard RL notation, we define $\\pi(a_t|s_t)$ as the probability of taking action $a_t$ in state $s_t$ at timestep $t$. A policy takes an action in a given state in $M$, transitions to a new state via $T_M$, and repeats, generating a trajectory of length $t$ as $(s_0, a_0, ... s_\\tau, a_\\tau)$.\nTo generalize to the MARL setting, we extend $M$ to a cooperative Markov game with homogeneous action spaces. We define a cooperative Markov game with $n$ agents as $G = (S, A, T_G, d_{01} ... d_{0n}, \\gamma, L_G)$, which corresponds to the joint set of states $S = S_1 \\times \\cdot\\cdot\\cdot \\times S_n$, the joint set of actions $A = A_1 \\times \\cdot\\cdot\\cdot \\times A_n$, a joint transition operator $T_G : S \\times A \\rightarrow S$, a set of independent initial distributions for each agent $d_{01} \\times \\cdot\\cdot\\cdot \\times d_{0n}$, and a discount factor $\\gamma$ that remains unchanged. We consider the centralized training, decentralized execution [7] setting of MARL, where each agent receives independent observations and deploys their individual policies $\\pi_i : S_i \\rightarrow A(A_i)$ at execution time, but are jointly trained. The joint policy $\\pi : S \\rightarrow \\Delta(A)$ executes all individual policies simultaneously at each timestep, and successive states are dictated by $T_G$, generating joint trajectories $((s_{01}, a_{01}, ... s_{0n}, a_{0n}), ... (s_{\\tau 1}, a_{\\tau 1}, ... s_{\\tau n}, a_{\\tau n}))$.\nIn addition to the standard components of our Markov Game, we assume access to a known labeling function $L_G : S \\rightarrow 2^\\Sigma$ that maps states in $S$ to a set of environment events, denoted by $\\Sigma$. Each environment event $e \\in \\Sigma$ is represented by a variable that takes on a Boolean truth value (True or False). For example, in Figure 2, if Agent 1 is in the yellow station, Agent 2 is in the red station, and Agent 3 has not moved from its initial position, the labeling function for this joint state would return {$Y_s, R_s$}."}, {"title": "2.2 Task completion Reward Machines", "content": "In this work, we consider task completion reward machines (RMs) [21, 33, 44] as our team objective. A task completion RM is specified by a tuple $R = (U, u_{-1}, \\Sigma, \\delta, U^*)$ consisting of a set of states $U$, an alphabet of events $\\Sigma$ that trigger transitions in $R$ via the transition function $\\delta: U \\times \\Sigma \\rightarrow U$, an initial state $u_{-1}$, and a set of goal states $U^*$. There are no outgoing transitions from any goal state $u^* \\in U^*$. Task completion RMs additionally define an output scoring function $\\sigma : U \\times U \\rightarrow \\mathbb{R}$, where $\\sigma(u, u') = 1$ whenever $u \\notin U^*$ and $u' \\in U^*$, and 0 otherwise. Task completion RMs are Mealy machines [20] where reaching the goal state represents a valid completion of the task. We remark that $\\delta$ and $\\sigma$ are partial functions defined on subsets of $U \\times \\Sigma$ and $U \\times U$. The transition operator for a task completion RM takes in single events $e \\in \\Sigma$ as input; when multiple events occur simultaneously, the events are passed in sequence to the RM in arbitrary order."}, {"title": "2.3 Using task completion RMs in MARL", "content": "Recall our problem setting of centralized training, decentralized execution: each agent will receive independent observations during execution. In other words, each agent will view the cooperative Markov game as an MDP, where the presence and actions of other agents are captured by the dynamics of their respective environments. To train our agent team, we can use the acceptance condition of a task completion RM as a reward function for MARL. This objective can be naturally compiled down to a reward function expressed over a product MDP for individual agent policies $\\pi_1 ... \\pi_n$. Concretely, a product MDP synchronizes $M$ and $R$ so that an agent may learn a policy over the joint space by coupling the reward machine state $u_t$ during a trajectory with the MDP's state $s_t$, producing an action conditioned on both: $\\pi(a_t| s_t, u_t)$. If the agents transition to a goal state in $R$, they will receive a reward of one; all other transitions will receive a reward of zero. Existing deep RL algorithms, such as PPO [31], have shown success in learning performant policies for RMs in a variety of single-agent settings by learning policies over the product MDP [19, 41].\nHowever, in a MARL setting, providing the full task completion RM for each agent creates difficulties in learning. If all agents see the same states and transitions in R, then all agents will receive credit when the task is completed, even if one or more agents did not contribute to completion of the task. Recall our running example in Figure 2. Suppose Agent 1 and Agent 2 visit HQ at the same time, and then Agent 2 visits the yellow and red stations, while Agent 3 remains stationary. Because the task is completed, all agents will receive equal reward for that episode. As a result, Agent 3 will think that its stationary behavior is desirable and be encouraged to act similarly in future episodes.\nExisting work addresses this shortcoming by introducing the notion of task decomposition for a given task completion RM [21]. A decomposition of a task completion RM creates sub-tasks in the form of $n$ smaller RMs $R_1... R_n$, derived from the original, that can then be assigned to each agent. Each agent is only concerned with accomplishing their own sub-task encoded by the RM assigned to them. In order to compute a decomposition, a practitioner provides Local Event Sets (LES) for each agent. An LES is a subset of events $\\Sigma_i \\in \\Sigma$ that is deemed relevant to agent $i$'s individual sub-task and restricts agent $i$ to only observing events in $\\Sigma^i$. A task completion RM is then projected onto these local event sets to create an individual's sub-task reward machine $R_i = (U_i, u_{-1}^i, \\Sigma_i, \\delta_i, U^*_i)$. The states, initial state, and goal states of $R_i$ are sets of equivalence classes over states in $R$ based on an equivalence relation that subsumes states whose transitions do not contain any event in $\\Sigma_i$. The transition function is a projection of the original $\\delta$ where a transition between two states $u_j$ and $u_k$ in $R_i$ exists only if an event in $\\Sigma_i$ triggered a transition between two distinct states in $R$ that were subsumed by $u_j$ and $u_k$ respectively. For the exact procedure of this projection, see [21].\nFor each agent's sub-task RM $R_i$, we define an individual labeling function $L_i$ that projects the set of environment events returned by $L$ to the events belonging to an agent's local event set $\\Sigma_i$. We say an event $e$ is a shared event if it belongs to more than one local event set. For example, the event \"Signal\" in Figure 2 is a shared event. In the case of shared events, agents' sub-task RMs must synchronize on this event. This means that a synchronized event must trigger a transition for all agents' RMs that share the event (i.e. all sub-task RMs must be in the appropriate state for the synchronized event to cause a transition), or no transition is taken from encountering that event for any agent's RM.\nThe decomposition approach outlined in [21] relies on a practitioner manually designing the local event sets to assign to each agent. In order to help guide the search for an appropriate assignment of local event sets, [21] provides a notion of validity for a given decomposition: a decomposition is valid if and only if the parallel composition of all reward machines $R_1 ... R_n$ is bisimilar to the original $R$. If a decomposition is valid, then for any trajectory of events $\\xi$, $R$ accepts $\\xi$ if and only if all sub-task RMs $R_1 ... R_n$ accept $\\xi$. In other words, a trajectory of events that accomplishes all decomposed sub-tasks encoded by $R_1,..., R_n$ is guaranteed to solve the overall task."}, {"title": "3 METHODOLOGY", "content": "In practice, multiple valid decompositions often exist for a given RM. However, a valid decomposition may not be feasible under the dynamics of a given MDP; that is, the resulting learned policies may not be able to achieve the individual tasks prescribed by a decomposition. Moreover, when multiple feasible decompositions do exist, we do not know which decomposition most efficiently achieves the task or provides the best credit assignment for learning. Recall the Repairs environment and task in Figure 2. A feasible decomposition of the task would be for Agent 1 and Agent 2 to meet at HQ, then for Agent 2 to visit the yellow, then red stations in that order. However, this decomposition is less efficient than a decomposition where Agents 1 and 3 visit HQ, then Agent 2 visits the yellow station and leaves the hazardous region while Agent 3 visits the red station (assuming Agents complete their sub-tasks with optimal efficiency).\nRecent work attempts to automate the search for RM decompositions by leveraging additional information provided by a practitioner [33]. In this work, subsets of events in $\\Sigma$ can be provided that either require or forbid an event to belong to a specific agent's local event set $\\Sigma_i$, along with a utility function that quantifies how valuable an event would be to a specific agent. This information is then used to find a subset of events in $\\Sigma$ that still leads to a valid decomposition of $R$, if one exists. However, the aforementioned approach does not leverage knowledge gained about the MDP during training and therefore cannot ensure that the decomposition generated from their method is feasible or optimally efficient. In what follows, we will introduce our approach, which aims to find the optimal decomposition by learning a policy on-the-fly for many possible sub-tasks during training.\nOur approach can be broken down into three primary components: (1) automatically generating a set of possible (candidate) decompositions for our task, (2) using a task-conditioned policy to generalize learning across multiple decompositions, and (3) employing the Upper Confidence Bounds (UCB) strategy [5] to balance exploration and exploitation of candidates throughout training."}, {"title": "3.1 Generating candidate decompositions", "content": "Approaches to procedurally generate decompositions of reward machines and similar automaton-based task specifications have been explored in the literature [18, 33]. Such methods can be used to generate a finite set of candidate decompositions $D = \\{(R_1^1 ... R_n^1), ... (R_1^{|D|} ... R_n^{|D|})\\}$ from which the optimal decomposition can be found. We will denote an individual decomposition in our set as $d \\in D$. In our work, we will use the decomposition approach for task completion RMs introduced in [33]. In this approach, all possible valid decompositions are generated given $\\Sigma$, and each one is assigned a score based on three factors: (1) minimizing the average number of events assigned to each agent's local event set, (2) maximizing the similarity amongst the sizes of all local event sets, and (3) maximizing the average 'utility' of each agent's local event set, based on a practitioner-provided utility function that maps the assignment of an event to an agent to a scalar value. In our work, we assume that no utility function has been provided. We will therefore enumerate the 'top-k' valid candidate decompositions based on a weighted sum of scores pertaining to factors (1) and (2), where k is a hyperparameter that sets the number of decompositions we will consider, i.e. $k = |D|$. Our objective is to find the decomposition $d^* \\in D$ that leads to a learned policy which will most efficiently achieve the original task $R$."}, {"title": "3.2 Task-conditioned policies", "content": "We consider each agent policy in our team as sharing a task-conditioned policy. Instead of learning separate policies for individual sub-tasks, we learn a single policy that outputs actions conditioned on a specific sub-task RM belonging to a decomposition generated in section 3.1. We can then deploy separate copies of our policy on each agent during execution time.\nPolicy Architecture. Our policy is represented by a feedforward neural network that is shared and learned across all agents. The neural policy receives four inputs, depending on the individual agent:\n*   an observation from the underlying MDP (different for each agent);\n*   an encoding of the selected decomposition $d_j \\in D$ for the current execution episode (same for each agent);\n*   the current state in their sub-task $R_i \\in d_j$ (different for each agent);\n*   an encoding of the overall task and the current state in the overall task (same for each agent).\nWe visualize these inputs in Figure 3 for an example decomposition of our Repairs task. Learning a task-conditioned policy allows us to distinguish between different sub-tasks within different decompositions while exploiting the generalization capabilities of multi-task learning [24, 38] as well as the natural curriculum learning inherent in decomposition exploration. Let us return to the running example from Figure 2. Imagine that as part of a decomposition selected during training, agent $A_1$ is only assigned the task of meeting at HQ and nothing else. Even if this decomposition is not optimal, succeeding in this sub-task teaches $A_1$ the valuable skill of how to reach HQ. This experience will be useful in the future when $A_1$ is tasked with more complicated sub-tasks, such as \"first go to HQ and then the red station\".\nConditioning on the overall task allows us to relax the assumption of independent dynamics as we will describe in Section 3.4. In our experiments, we use one-hot encodings of the selected sub-task and the current position in the sub-task during the rollout. However, the embeddings could in principle be generated in any way, such as learned embeddings from a graph neural network that encode the structure of an RM [45]."}, {"title": "3.3 Selecting decompositions during training", "content": "A naive approach to our objective of finding an optimal ordered decomposition would be to learn a task-conditioned policy for each $d \\in D$ independently, and then choose the decomposition that performs the best after a certain amount of training. As the number of candidate decomposition increases, this approach quickly becomes intractable. We seek to improve the efficiency of this process by simultaneously learning both the most efficient decomposition and the corresponding policies that achieve this decomposition.\nA key insight of our work is that we can use rewards from previous executions of a sub-task $R_i$ to estimate the satisfaction likelihood of that sub-task. These estimates, which we call value estimates, are used as a heuristic in decomposition selection to assess how well a policy is performing on a specific sub-task. Our approach computes value estimates as an exponential weighted moving sum of previous rewards, as more recent rewards are typically a more accurate reflection of the performance of the current policy.\nWe will denote the value estimates for a sub-task $R_i$ as $V_{R^i}$. On episode $H$ of training on decomposition $j$, we can compute a sub-task's value estimate as $V_{R^i_j} = \\frac{\\sum_{h=1}^H \\alpha^{H-h} r_h}{\\sum_{h=1}^H \\alpha^{H-h}}$. Here, $r_h$ represents the reward achieved by agent $i$ under sub-task $R_i$ from the $h$-th execution and $\\alpha$ is a hyperparameter defining the decay rate.\nWith value estimates in hand for each agent policy in our team, on each sub-task, we can consider a number of heuristic approaches to utilize them. In this work, we use the Upper Confidence Bound (UCB) algorithm [5], which balances exploring different decompositions with exploiting higher scoring decomposition via a hyperparameter $\\beta$. The score assigned to each decomposition $d_j$ is an average of the decomposition's current value estimates for each sub-task $\\{V_{R_1^j},..., V_{R_n^j}\\}$.\nWe note that the value estimates early in training may be arbitrarily inaccurate due to the lack of progress made in learning the policy for different sub-tasks by each agent. When inaccuracy is high early in training, exploration is critical so that agents can sufficiently optimize towards achieving each candidate sub-task before the value estimates are exploited to converge on the optimal decomposition. Once the optimal decomposition is converged upon, additional training will further optimize the policy conditioned on its corresponding sub-tasks."}, {"title": "3.4 Dealing with dependent dynamics", "content": "Prior work in RM-guided MARL [21, 33] assumes that the underlying dynamics of the MDP are independent between agents. This prevents the use of an MDP with dynamics that model collisions or interactions between agents unless the interaction is explicitly modeled by the task's reward machine, which requires knowing about the interaction a priori.\nConsider the motivating example visualized in Figure 2. The example includes dependent dynamics between the agents in the form of the hazardous region. Only one agent can occupy the region at a time, meaning that an agent's ability to enter the region is dependent on the other agents' positions and actions.\nPrior works [21, 33] require the assumption of independent dynamics so that the completion of an individual agent's reward machine is independent of the behavior of other agents. With dependent dynamics, this is rarely the case. For example, imagine that Agent 3 is assigned to reach the yellow target and Agent 2 is assigned to reach the red target. If Agent 3 enters the hazardous region and reaches the yellow target, it accomplishes its sub-task. Now, completion of the overall task only requires Agent 2 to reach the red target. Since the red target is located in the hazardous region, this requires Agent 3 to exit the hazardous region. However, Agent 3 has already accomplished its sub-task (and has no knowledge of Agent 2's task), so it has no incentive to leave the hazardous region.\nWe are able to relax the assumption of independent dynamics made in previous work [21, 33] and handle environments such as the one in Figure 2 by allowing agents to condition not only on the status of their sub-task, but on the status of the overall task as well. In addition, agents are given a small reward for the completion of the overall task during training along with their primary reward for completing their assigned sub-task. This reward structure incentivizes agents to condition on the status of other agent's tasks (e.g., that Agent 2 needs to go to red) so that the overall task can be completed (e.g., Agent 3 is incentivized to leave the hazardous region)."}, {"title": "4 EXPERIMENTS", "content": "We evaluate our proposed approach, which we refer to as LOTaD (Learning Optimal Task Decompositions), in a variety of MARL settings with varying task complexity. Our code is available at https://github.com/thomasychen/LOTaD.\nIn our experiments, we seek to answer the following research questions: (RQ1) Does LOTaD outperform existing approaches to RM-guided MARL that are not learning-informed, including approaches that do not perform task decomposition? (RQ2) Can LOTaD learn to avoid pitfalls in learning that may occur due to dependent dynamics across agents? (RQ3) How does varying the number of candidate decompositions $k = |D|$ affect the learning performance of LOTaD?"}, {"title": "4.1 Environments and tasks", "content": "Our environments include the Repairs environment and RM task presented in Figure 2. The environment is instantiated as a 7x12 grid in which agents can move in any of the cardinal directions, or take a no-op action where no movement occurs. We make the Repairs environment stochastic by giving a small chance of an agent \"slipping\" and taking a random action rather than the action selected by their policy. Each agent observes only their position in the world at each timestep.\nIn addition to the Repairs environment, we also include two \"Buttons\" environments that require a team of agents to press a series of buttons in a particular order. We instantiate two environment-task pairs: First, we use Cooperative Buttons, a task from [21] where any agent must reach a specified goal location, but in order to do so, must traverse regions that can only be crossed once a corresponding button has been pressed. Second, we use Four-Buttons, a task where two agents must press four buttons (yellow, green, blue, and red) in an environment, with an ordering constraint that the yellow button must be pressed before the red button. Both of these environments are represented as 10x10 grids with the same observation space, action space, and \"slipping\" dynamics as the Repair environment. Visualizations of these environments are presented in our Appendix.\nLastly, we evaluate LOTaD on two environments from the popular multi-agent benchmark Overcooked [9]. In both environments, we provide the same simple task of delivering a soup to the delivery station, which requires putting three onions in a pot, plating, and delivering the soup. Agents must coordinate depending on the dynamics of their environment to efficiently cook and deliver the soup. We use a Cramped-Corridor environment, where two agents must navigate around one another to reach the pot at the end of a small corridor in a cramped room, and an Asymmetric-Advantages environment, where two agents are in separate rooms, but have access to an asymmetric set of resources in their respective rooms. We visualize both Overcooked environments in our Appendix. The observation space and action space for these environments are the same as those provided in the Overcooked implementation from [27] with the adjustment that we do not expose the number of onions in the pot or whether the soup has finished cooking in order to ensure that no information provided by the labeling function is redundant in the agents' observations.\nIn all environments, we apply a discount factor $\\gamma < 1.0$ to the reward offered by our RM to incentivize our agents to accomplish the task as efficiently as possible. We generate $k = 10$ decomposition candidates in all experiments using the generation method described in Section 3.1 for LOTaD to search amongst. We provide additional information regarding each environment and task in our Appendix."}, {"title": "4.2 Baselines", "content": "To evaluate LOTaD, we compare against a baseline that selects a decomposition using the ATAD method [33]. This approach selects a set decomposition prior to learning based on the scoring method described in Section 3.1. We break ties between top scoring decompositions arbitrarily. In addition to this baseline, we also compare against a baseline approach that assigns each agent the overall task. We call this baseline Monolithic and use it to evaluate how MARL would perform if no decomposition of the RM was used. We use PPO [31] with a Gaussian policy over the action space for each agent as our RL algorithm in every environment."}, {"title": "4.3 Results", "content": "We plot training curves for all experimental domains in Figure 4. In these curves, we report the current best discounted reward achieved by LOTaD amongst all decomposition candidates under consideration. We find that LOTaD outperforms both the monolithic and ATAD baselines across all environments, answering (RQ1) in the affirmative. In many of our environments, learning decompositions with LOTaD allows for agents to explicitly parallelize their contributions towards achieving the task. For example, in the Four-Buttons environment, a candidate decomposition allows for one agent to visit the yellow and green buttons while the other agent visits the blue button.\nInterestingly, LOTaD-learned decompositions are useful even when explicit parallelization is not possible, such as the Overcooked RM task. Decompositions of this task can still facilitate multi-agent learning: for example, one agent may be tasked with putting all three onions in the pot, while the other agent must wait for the soup to be cooked before plating and delivering the soup. In this decomposition, the agent tasked with plating and delivering the soup may fetch a plate and stand near the pot so that the soup can be quickly delivered upon completion of cooking.\nIn comparison to LOTaD, the ATAD baseline is unable to consistently find performant task decompositions. Although ATAD selects a decomposition based on the same scoring method by which we select D, there exist many possible decompositions tied for the highest achievable score in a given task. As a result, ATAD may select the optimal decomposition when the optimal decomposition is also the highest scoring, but performs suboptimally when this is not the case. In addition to lower reward, this leads to a higher variance in performance by ATAD, as evidenced by Figure 4. The Monolithic baseline consistently achieves the lowest reward due to the sparsity of the overall task reward. Similar to ATAD, we notice that policy learning with the Monolithic baseline is unreliable and tends to converge more slowly than LOTaD.\nWe find that LOTaD is able to successfully accomplish the task in the Repairs and Overcooked Cramped-Corridor environments, which both involve dependent dynamics amongst agents. LOTaD is indeed able to avoid issues when training agent teams in environments with dependent dynamics, affirming (RQ2). To further investigate our hypothesis of whether a global task view alleviates these issues, we ran LOTaD without an encoding of the overall task in the Repairs and Cramped-Corridor environment. We present the training curves from this ablation study in Figure 5. We find that without the overall task encoding, LOTaD is largely unable to accomplish the Repairs task, and is slower and less stable in accomplishing the Cramped-Corridor task. This suggests that the overall task encoding enables our agents to learn policies that progress towards completion of the overall task even when an agent's individual sub-task is already achieved. In our other environment settings, where codependent agent dynamics do not exist, removing the overall task encoding does not significantly affect training.\nTo answer (RQ3), we perform ablation studies where we vary the size of D. These training curves are visualized in Figures 6. We see that values of k that are either very small or very large make the learning problem more challenging for LOTaD. We reason that if k is too large, more exploration is required, and LOTaD may struggle to find the optimal decomposition amongst many candidates. If k is too small, there is a lower chance of \"good\" decompositions appearing in the set D, which inherently limits the performance potential of LOTaD. We note that the inherent randomness of LOTaD, as well as the choice of environment, task, and the amount of exploration prioritized by our UCB hyperparameter $\\beta$, may confound results."}, {"title": "5 RELATED WORK", "content": "Automaton-based task specifications in RL. An extensive body of work has explored automaton-based task specification for RL agents. Previous efforts have proposed RL approaches to policy learning for reward machines [8, 14, 37", "41": "by augmenting the state space of the MDP. These efforts focus primarily on single-agent settings and are not designed to handle the MARL case with shared objectives or formally decompose the automaton.\n(Symbolic) task decomposition for multi-agent teams. Symbolic structures have been leveraged to facilitate efficient multi-agent learning in a variety of settings. A number of these approaches rely on a known dynamics model [16, 29, 30", "33": ".", "43": "or decompose traditional (non-Markovian) reward functions [34", "39": "is a"}]}