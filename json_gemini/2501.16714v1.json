{"title": "Separate Motion from Appearance: Customizing Motion via Customizing Text-to-Video Diffusion Models", "authors": ["Huijie Liu", "Jingyun Wang", "Shuai Ma", "Jie Hu", "Xiaoming Wei", "Guoliang Kang"], "abstract": "Motion customization aims to adapt the diffusion model (DM) to generate videos with the motion specified by a set of video clips with the same motion concept. To realize this goal, the adaptation of DM should be possible to model the specified motion concept, without compromising the ability to generate diverse appearances. Thus, the key to solving this problem lies in how to separate the motion concept from the appearance in the adaptation process of DM. Typical previous works explore different ways to represent and insert a motion concept into large-scale pretrained text-to-video diffusion models, e.g., learning a motion LoRA, using latent noise residuals, etc. While those methods can encode the motion concept, they also inevitably encode the appearance in the reference videos, resulting in weakened appearance generation capability. In this paper, we follow the typical way to learn a motion LoRA to encode the motion concept, but propose two novel strategies to enhance motion-appearance separation, including temporal attention purification (TAP) and appearance highway (AH). Specifically, we assume that in the temporal attention module, the pretrained Value embeddings are sufficient to serve as basic components needed by producing a new motion. Thus, in TAP, we choose only to reshape the temporal attention with motion LoRAs so that Value embeddings can be reorganized to produce a new motion. Further, in AH, we alter the starting point of each skip connection in U-Net from the output of each temporal attention module to the output of each spatial attention module. Extensive experiments demonstrate that compared to previous works, our method can generate videos with appearance more aligned with the text descriptions and motion more consistent with the reference videos.", "sections": [{"title": "1. Introduction", "content": "Text-to-video diffusion models [3, 5] have made remarkable progress. With just a text input, these models can generate high-quality videos that faithfully reflect the prompt provided. However, relying solely on text descriptions, pretrained text-to-video diffusion models struggle to generate high-fidelity videos for highly customized motion concepts. To address this issue, researchers explore various ways to realize motion customization of text-to-video diffusion models.\nFormally, motion customization aims to adapt the diffusion model (DM) to generate videos with the motion specified by a set of video clips (i.e., reference videos) with the same motion concept. To realize this goal, the adaptation of DM should be possible to model the specified motion concept, without compromising the ability to generate diverse appearance. Thus, the key to solving this problem lies in how to separate the motion concept from the appearance in the adaptation process of diffusion models. Typical previous works [18, 24, 47] explore different ways to represent and insert a motion concept into large-scale pretrained text-to-video diffusion models. For example, [47] trains motion LoRAs to encode the motion concept from the reference videos. Additionally, [47] trains appearance LoRAS"}, {"title": null, "content": "to encode the appearance of reference videos. At inference, the appearance LoRAs are abandoned to avoid overfitting to the appearance of reference videos. Other works like [18] utilize residual vectors between consecutive frames to represent a motion. While those methods can encode the motion concept, there exists no explicit constraint that appearance information should not be learned or encoded into the motion representations. Actually, they inevitably encode the appearance of the reference videos (termed as \u201cappearance leakage\"), resulting in weakened appearance generation ability. As shown in the Fig. 1 (first row), the background of the reference videos includes some \"window\", which has nothing to do with the motion concept. Previous methods (second row), while fitting the motion, also mistakenly fit the appearance of the \"window\". As a result, the generated video's background also includes \"window\", which is not aligned with the text description.\nIn this paper, we follow the typical way to learn motion LoRAs to encode the motion concept but propose two novel strategies to enhance motion-appearance separation, including temporal attention purification (TAP) and appearance highway (AH). We choose to adopt the typical text-to-video diffusion model (T2V-DM) ZeroScope [31] or ModelScope [34] to perform the motion customization. Specifically, we assume that in the temporal transformers of text-to-video diffusion models, the pretrained Value embeddings are sufficient to serve as basic components needed to produce a new motion. Thus, in TAP, we choose only to reshape the temporal attention with motion LoRAs so that Value embeddings can be reorganized to produce a new motion. As TAP may not perfectly avoid encoding appearance information in motion LoRAs, we propose AH to maintain the appearance generation ability of original T2V-DM and further avoid appearance leakage issue. Specifically, we alter the starting point of each skip connection in spatial-temporal U-Net [8, 14] from the output of each temporal transformers to the output of each spatial transformers. The underlying logic of AH is that as the skip connection of U-Net mainly conveys high-frequency appearance information [28], the starting point alternation can provide a \"highway\" for the hidden states of non-adapted spatial transformers and cut the shortcut from the hidden states of LoRA-adapted temporal transformers. We empirically find that through AH, the hidden states from the decoding branch of spatial-temporal U-Net can encode appearance information more similar to the vanilla T2V-DM and motion information more similar to the T2V-DM adapted with TAP. This means AH largely maintains the appearance generation ability of vanilla T2V-DM, without harming the motion concept learning from reference videos. With the proposed TAP and AH, our method can largely mitigate the appearance leakage issue and generate videos with customized motion and high-fidelity appearance well aligned with the\""}, {"title": null, "content": "text descriptions, as shown in Fig. 1 (last row). Additionally, we point out that motion modeling occurs in the early steps of the denoising process and propose phased LORA integration (PIL) to maximize the T2V-DM's appearance modeling capability. Extensive experiments demonstrate that our method effectively separates motion from appearance and outperforms previous state-of-the-arts.\nIn a nutshell, our contributions are summarized as\n\u2022 We carefully examine the attention mechanism of typical T2V-DMs and propose Temporal Attention Purification (TAP) to reduce the appearance information encoded in the motion LoRAs, without disturbing the motion concept learning.\n\u2022 We propose Appearance Highway (AH) strategy, which alters the starting point of skip connection in U-Net from the hidden states of LoRA-adapted temporal transformers to those of non-adapted spatial transformers, to maintain the appearance generation ability of vanilla T2V-DM and further avoid appearance leakage issue.\n\u2022 We conducted extensive qualitative and quantitative experiments, demonstrating that our method outperforms previous state-of-the-arts."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Text-to-Video generation.", "content": "Text-to-video generation is a task that involves generating realistic videos based on text prompts. Early research primarily focused on Generative Adversarial Networks (GAN) [2, 27, 32, 33], autoregression transformers[10, 15, 21, 42]. Recently, diffusion models have emerged as the most popular paradigm in text-to-video generation [1, 3, 5, 9, 16, 44\u201346]. Make-A-Video [29] is first trained on labeled images and then on unlabeled videos to address the issue of the lack of paired video-text data. Imagen Video[13] generates high-quality videos by using a text-to-video model combined with spatial and temporal super-resolution models. Magic Video[48] and VideoLDM [4] adapts the Latent Diffusion Model's [26] architecture for video generation. In contrast to these methods, Text2Video-Zero[19], AnimateDiff[12] leverage the prior knowledge of image generation models to generate videos. Recently, open-source T2V-DM, ModelScope [34] and ZeroScope [31] has attracted considerable attention."}, {"title": "2.2. Motion customization generation.", "content": "To generate motion-customized videos, several video editing methods [6, 11, 22, 43], including Control-A-Video [7] and VideoComposer [35], transfer motion from a reference video to new appearances. These approaches primarily modify the appearance without fully capturing the essence of motion, which limits appearance diversity. Tune-A-Video [38] extends text-to-image models into the video domain, enabling customized video generation. VMC [18]"}, {"title": "3. Method", "content": "In this paper, we aim to adapt the text-to-video diffusion model (T2V-DM) to perform motion customization, i.e., we need to adapt T2V-DM with the reference videos containing one specific motion concept to generate new high-fidelity videos with the same motion concept. The general framework of our method is illustrated in Fig. 2. Our method is based on typical pre-trained T2V DMs (e.g., ZeroScope [31] or ModelScope [34]) whose architecture is a 3D U-Net. The basic components for such a 3D U-Net include spatial transformer and temporal transformer, which encode the spatial and temporal information respectively. Skip connections are utilized to connect the encoders and decoders of the U-Net.\nFollowing MotionDirector, we adopt a dual-path LORA adaption for training (Sec. 3.1), which tunes the model with"}, {"title": null, "content": "low-rank adaptions (LoRAs) while keeping the pre-trained parameters fixed, and finally models the motion information in the LoRAs of temporal transformers. However, such a strategy still inevitably encodes the appearance of the reference videos into temporal LoRAs, resulting in weakened appearance generation ability. Thus, we propose two effective modifications. Firstly, instead of injecting LoRAs into all linear layers of the attention mechanism in temporal transformers, we propose a Temporal Attention Purification (TAP) strategy (Sec. 3.2). Secondly, at the inference stage, we modify the skip connection between the encoder and decoder of spatial-temporal U-Net with an Appearance Highway strategy (Sec. 3.3), which transfers the hidden states from the spatial transformers rather than the temporal transformers of the encoder to the decoder. Finally, during inference, we adopt a phased LoRA integration trick where we utilize our modified framework to denoise for a few steps and then use the original spatial-temporal U-Net for the latter steps of denoising (Sec. 3.4)."}, {"title": "3.1. Baseline: Dual-Path LoRA Adaption", "content": "MotionDirector [47] utilizes a spatial-temporal U-Net [8, 14], which takes visual latent code $z_t \\in \\mathbb{R}^{b \\times f \\times h \\times w \\times c}$ (b, f, h, w, c represents batch size, frame, height, width and channel dimensions, respectively) at each time step t and textual feature y as input. MotionDirector adapts ZeroScope [31] to perform motion customization.\nMotionDirector adopts a dual-path LoRA adaption: For the spatial path, MotionDirector injects spatial LoRAs into self-attention layers of spatial transformers to reconstruct"}, {"title": null, "content": "the appearance of training data, supervised by a random frame from the reference video. For the temporal path, MotionDirector keeps the spatial LoRAs frozen while injecting temporal LoRAs into temporal transformers to model the motion information with the following loss, supervised by the entire reference videos:\n$\\min_{\\theta_{tp}} E_{z_0, y, t, \\epsilon} [||\\epsilon - \\epsilon_{\\theta_{tp}}(z_t, y, t)||_2^2],$ (1)\nwhere $\\epsilon$ denotes the noise added in $z_t$ and $\\epsilon_{\\theta_{tp}}$ denotes the U-Net $\\epsilon_{\\theta}$ with temporal LoRAs $\\theta_{tp}$. As a result, MotionDirector effectively models the motion concept within the trained temporal LoRAs. At inference, it generates videos simply utilizing the trained temporal LoRAs.\nHowever, we observe MotionDirector tends to overfit the appearance of the reference videos, suggesting that the trained temporal LoRAs still encode the appearance information. It is reasonable as with explicit regularization, the dual-path LoRA adaptation framework cannot guarantee the temporal LORA does not encode the appearance information. In this paper, we carefully examine the architecture of typical T2V DMs and propose two novel strategies to constrain the temporal LoRA learning, including the temporal attention purification (TAP) and appearance highway (AH)."}, {"title": "3.2. Temporal attention purification", "content": "In this section, we propose Temporal Attention Purification strategy (TAP) to separate the motion encoding from appearance encoding in adapting temporal transformer block with temporal LoRAs.\nSpecifically, we assume that in temporal attention module, the pretrained Value embeddings $W_V$ are sufficient to serve as the basic components needed for depicting a motion. To generate a new motion, what we need to adapt is the temporal attention, which determines how we combine the Value embeddings in temporal attention module. Thus, in TAP, we choose to utilize temporal LoRAs only to adapt the Query or the Key embeddings to reshape the temporal attention to produce a new motion, without disturbing the Value embeddings. As the temporal attention is only utilized to combine the Value embeddings, we expect reshaping it will not introduce appearance information.\nWe empirically find that our assumption is reasonable. As shown in Tab. 1, we evaluate the effects of adapting different parts of a temporal transformer block. By adapting different parts of temporal transformer block with LoRAs, we have different generation results. We utilize an off-the-shelf motion classifier [41] to classify the generated videos. Higher accuracy (denoted as \"motion\" in the table) means higher quality of motions generated to an extent. We also utilize CLIP to evaluate the appearance alignment of generated videos with the text descriptions of users (denoted as \"App. Align.\") and those of reference videos (denoted as \"App. Leak.\"). Higher \"App. Align.\" and lower"}, {"title": null, "content": "\"App. Leak.\" means better appearance quality of generated videos. From the Tab. 1, we observe that compared to the T2V-DM baseline, adapting Query, Key, Value and the feed-forward network (FFN) of temporal transformer (i.e., MotionDirector) do improves the motion quality, but scarifies the appearance quality very much. Adapting Value and the feed-forward network also harm the appearance modeling. In contrast, reshaping the temporal attention (including reshaping Query, Key or both) yields high-quality videos considering both motion quality and appearance quality. All those results align well with our basic assumption.\nBased on our observations, we choose to only adapt the Key embedding of a temporal transformer block with temporal LoRA, which is formulated as\n$\\min_{\\theta_{\\kappa}} E_{z_0, y, t, \\epsilon} [||\\epsilon - \\epsilon_{\\theta_{\\kappa}}(z_t, y, t)||_2^2].$ (2)\nCompared with Eq. 1, the difference is that we only insert temporal LORA to adapt Key embeddings."}, {"title": "3.3. Appearance Highway", "content": "As TAP may not perfectly avoid encoding appearance information in motion LoRAs, we propose using Appearance Highway (AH) strategy to maintain the appearance generation ability of the original T2V-DM and further avoid appearance leakage issues. Specifically, we alter the starting point of each skip connection in spatial-temporal U-Net [8, 14] from the output of each temporal transformers to the output of each spatial transformers.\nThe underlying logic of AH is that as the skip connection of U-Net mainly conveys high-frequency appearance information [28], the starting point alternation can provide a \"highway\" for the hidden states of non-adapted spatial transformers and cut the shortcut from the hidden states of LoRA-adapted temporal transformers. As shown in Fig. 3, the videos are generated conditioned on \"A monkey is playing golf on a field full of flowers\". The first row of Fig. 3 illustrates the videos generated with AH. We further show how the appearance changes as we enhance the effect of hidden states from the spatial transformers by varying scales. Compared with adapted T2V-DM with vanilla skip connection, AH yields videos with appearance more aligned with text descriptions of the target. Moreover, as"}, {"title": null, "content": "the effect of hidden states from the spatial transformers is enhanced by increasing scales (see first row of Fig. 3), the appearance of generated videos becomes more aligned with the text descriptions. In contrast, with vanilla skip connection, no matter how we enhance the temporal transformer's output, the appearance cannot match the text descriptions.\nFurther, we find that AH may largely maintain the appearance generation ability of vanilla T2V-DM, without harming the motion concept learning from the reference videos. We conduct experiments to verify this. As changing the starting point of skip connection directly affects the decoding results, we extract the hidden states from the decoding part after projecting the hidden states combined from the starting point of skip connection and the decoder input (the pink part illustrated in Fig. 2(c)).\nWe compare hidden states extracted from four kinds of"}, {"title": null, "content": "models: (1) vanilla T2V-DM with high appearance generation ability; (2) T2V-DM with spatial-LoRA (SL) which customizes the appearance of reference videos; (3) T2V-DM with TAP (TAP); (4) TAP combined with AH (AH).\nAs shown in Fig. 4 (a), we compare the similarity score between AH and T2V-DM and that between TAP and T2V-DM. We observe that with AH, the hidden states of decoder is more similar to T2V-DM compared to TAP, which indicates AH may further improves the appearance generation ability of adapted T2V-DM. We also compare the similarity score between AH and SL and that between TAP and SL in Fig. 4 (b). We observe that with AH, the hidden states of decoder is more dissimilar to SL compared to TAP. All those results indicate AH may enhance the appearance modeling ability beyond TAP.\nWe extract optical flows from TAP's hidden states as positive samples and those from T2V-DM's hidden states as negative samples to train a motion classifier. Then we use this classifier to evaluate the hidden states from AH. Fig. 4 shows most of the hidden states from AH can be classified into TAP rather than vanilla T2V-DM in a high accuracy and with high confidence. These results show that utilizing AH may not disturb the motion generation ability of TAP.\nWe multiply the starting point of AH, i.e., hidden states of spatial transformer by a constant $\u03b2$ to strengthen its effect. AH can be used during both training and inference, or exclusively during inference. Our experiments show that AH utilized during training and AH utilized during inference yield comparable results. For simplicity, we only use it during the inference process (see Sec. 4)."}, {"title": "3.4. Phased LoRA Integration", "content": "We find that as denoising proceeds, the model tends to learn more fine-grained appearance information and the risk of overfitting to the appearance of the reference videos (appearance leakage) becomes higher. Thus, we propose utilizing a phased LoRA integration strategy during inference to mitigate the risk of appearance leakage. Specifically, in the early steps of denoising, we utilize T2V-DM adapted with TAP and AH, and in the remaining steps of denoising, we utilize the vanilla T2V-DM. The formula of phased LORA integration is\n$z_{t-1} = \\frac{z_t}{\\sqrt{\\alpha_t}} + (\\frac{1}{\\sqrt{\\alpha_t}} - 1) \\cdot \\epsilon_{\\theta_{\\kappa}} (z_t, t) + \\sigma_t, t \\leq T$\n$\\frac{z_t}{\\sqrt{\\alpha_t}} + (\\frac{1}{\\sqrt{\\alpha_t}} - 1) \\cdot \\epsilon (z_t, t) + \\sigma_t, t > T$\nwhich $\\alpha$ is a constant controlling the noise strength. (3)"}, {"title": "4. Experiment", "content": null}, {"title": "4.1. Experimental Setup", "content": "Datasets For evaluations on a single video, we first utilize LOVEU-TGVE-2023 dataset [39]. The dataset includes 76"}, {"title": "4.2. Evaluations on One-Shot", "content": "Quantitative results. For automatic evaluations in Tab. 2, our method significantly improves text alignment and Vi-CLIP score, demonstrating that it suppresses appearance leakage. Our method also demonstrates superiority in other dimensions. Human evaluations confirm the same results.\nQualitative results. In the example on the left of Fig. 5, previous methods overfitted the \u201cgrey sharks\", while ours generate a realistic \u201csea dragons\u201d. In the example on the right, our method generated a vividly \u201csandwich\u201d, while the \"sandwich\" produced by other methods resembled the \"pizza\" from the reference video."}, {"title": "4.3. Evaluation on Few-Shot", "content": "Quantitative results. As shown in 3, the automatic evaluation shows our method surpasses previous approaches across multiple dimensions. The human evaluations show that while our temporal consistency may not surpass that of DreamVideo and MotionDirector, our appearance diversity and motion fidelity are superior.\nQualitative Results. Fig. 6 illustrates that MotionDirector occasionally exhibits appearance leakage from the training set. In the example on the left, DreamVideo generates a teddy bear holding an object resembling a suitcase and wearing a tie. It occurs because the training set includes humans carrying suitcases and wearing ties. MotionDirector generates a background featuring vertical reflective structures, similar to the background in the second reference video. In the right-hand case, the backgrounds generated by previous methods lack a distinctly \"cyberpunk\" aesthetic and instead resemble those of the reference videos. In contrast, our method effectively avoids this issue."}, {"title": "4.4. Ablation", "content": "Effect of Our Sub-Contributions. in Tab. 4, we evaluate the capabilities of each contribution: temporal attention"}, {"title": null, "content": "purification (TAP), appearance highway (AH), and phased LORA integration (PLI). TAP, AH, and PLI each significantly enhance text alignment. AH improves text alignment and aesthetic score but slightly compromises temporal consistency. However, combined with PLI, it not only further enhances text alignment and aesthetic score but also mitigates the loss in motion consistency caused by AH.\nEffect of motion customization with different T2V-DM. Tab. 4 demonstrates that our method is effective across various foundation models using a spatial-temporal U-Net. We select ZeroScope [31] and ModelScope [34] as the foundation models for verifying. The effectiveness of each component is confirmed in both models.\nTraining with AH vs. Post-processing with AH. The appearance highway can be used during both training and inference, or exclusively during inference. We compare the differences between these two usage methods. As shown in Tab. 5, the results of the two methods are comparable. For simplicity, we use it with post-processing as our method."}, {"title": "5. Conclusion", "content": "In this paper, we propose two novel strategies to separate motion from appearance during adapting T2V-DM to perform motion customization task, including Temporal Attention Purification (TAP) and Appearance Highway (AH). In TAP, we utilize LoRA only to adapt the temporal attention and in AH, we alter the starting point of skip connection from LoRA-adapted temporal transformer block to non-adapted spatial transformer block. Extensive experiments show that our method performs favorably against previous state-of-the-arts."}]}