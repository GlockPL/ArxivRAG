{"title": "Operator Learning for Reconstructing Flow Fields from Sparse Measurements: an Energy Transformer Approach", "authors": ["Qian Zhang", "Dmitry Krotov", "George Em Karniadakis"], "abstract": "Machine learning methods have shown great success in various scientific areas, including fluid mechanics. However, reconstruction problems, where full velocity fields must be recovered from partial observations, remain challenging. In this paper, we propose a novel operator learning framework for solving reconstruction problems by using the Energy Transformer (ET), an architecture inspired by associative memory models. We formulate reconstruction as a mapping from incomplete observed data to full reconstructed fields. The method is validated on three fluid mechanics examples using diverse types of data: (1) unsteady 2D vortex street in flow past a cylinder using simulation data; (2) high-speed under-expanded impinging supersonic jets impingement using Schlieren imaging; and (3) 3D turbulent jet flow using particle tracking. The results demonstrate the ability of ET to accurately reconstruct complex flow fields from highly incomplete data (90% missing), even for noisy experimental measurements, with fast training and inference on a single GPU. This work provides a promising new direction for tackling reconstruction problems in fluid mechanics and other areas in mechanics, geophysics, weather prediction, and beyond.", "sections": [{"title": "1 Introduction", "content": "Machine learning methods have been widely applied in various scientific problems, e.g., physics [1-11], chem- istry [12-14], biology [15, 16], epidemiology [17,18], etc. This great success is due to the rapid simultaneous development of algorithms and computation power, which enable transforming the tasks that are difficult or expensive to solve by traditional methods into optimization tasks, which may be easier to tackle.\nIn this paradigm, operator learning is a prominent approach that has gained increasing interest recently [14, 19-22]. In traditional methods, time-consuming and computationally expensive algorithms need to be run every time for a new input condition. On the other hand, operator learning aims to learn the mapping from general input conditions or distributions to solutions, i.e. the solution operator, by a neural network. Hence, neural operators can be used as surrogates to the traditional methods as the forward pass of a neural network usually is much faster than the conventional algorithms. Some recent progress in architectures like DeepONet [19], Fourier neural operator (FNO) [23], LNO [14] and ViTO [24] have demonstrated the great potential of operator learning in diverse applications. More specifically, DeepONet is a mesh-free method, allowing solutions to be evaluated at any point in the domain. FNO leverages the Fourier transform to efficiently model long-range dependencies and capture global structures in data. LNO is based on Laplace transform and excels in handling transient responses and non-periodic signals. ViTO utilizes the Vision Transformer to learn the operator from data on a regular mesh, offering a fast and efficient approach.\nHowever, the situation is more challenging for reconstruction problems, i.e. full flow fields, from sparse and sporadic measurements. A common example is jet flow reconstruction, where measurement limitations often"}, {"title": "2 Method", "content": "result in only partial access to the flow field. Although obtaining high-resolution flow fields is desirable, it can be costly. Therefore, the primary goal in these scenarios is to reconstruct the complete data from the observed, incomplete data. In addition, the objective of the operator learning method for reconstruction problems is to learn the mapping between the observed data and the full data with limited example pairs. Clearly, the incomplete and noisy nature of the data pose great challenges for the generalization ability of operator learning methods, because the test data is probably out of the distribution of the training data. For instance, the physics-informed diffusion model (PIDM) [25] effectively reconstructs flow fields from sparse measurements. However, this approach relies on measurements taken at regular mesh points, which limits its applicability to more general and irregular scenarios. Expanding operator learning methods to handle such cases remains an open area of research. For non-operator learning methods, Physics-Informed Neural Networks (PINNs) [26] have shown some success in reconstruction problems in fluid mechanics [4,27-32], because PINNs can incorporate the prior knowledge (physical laws) into the loss function. However, they are still limited by the requirement of the explicit form of the governing equations, relatively high computational cost and lack of ability to generalize.\nHerein, we propose to use the Energy Transformer [33] for solving reconstruction problems in the operator learning framework. The Energy Transformer is inspired by recent developments in Dense Associative Memory models [34,35], which are high memory storage capacity extensions of Hopfield networks [36]. Energy Transformer reconstructs the patterns from partial data and fill in the missing parts, which concurs with the objective of reconstruction problems. This makes Energy Transformer an ideal model for reconstruction operator. More specifically, the patterns are stored as the local minima of the energy function learned by the Energy Transformer. Then in reconstruction, the full data are inferred by minimizing the energy function starting from the observed data.\nIn this paper, we formulate the reconstruction problems in an operator learning framework with the Energy Transformer. We present three diverse examples in fluid mechanics using multimodal data, including real problems with noisy data, to demonstrate the effectiveness of the method."}, {"title": "2.1 Operator learning framework", "content": "In this section, we formulate an operator learning framework for the reconstruction problem and introduce the proper notation. We denote a data sample as a collection of position-value pairs $P_a = \\{(x_a^i,v_a^i)\\}_{i=1}^{M_a}$, where $x_a^i$ represents the position, $v_a^i$ denotes the data value at position $x_a^i$, and $M_a$ is the number of position-value pairs in sample a. This collection $P_a$ is referred to as a full data sample. Meanwhile, an observed data sample is defined as $\\tilde{P_a} = \\{(x_a^j,v_a^j)\\}_{j \\in O_a}$, where $O_a$ indexes the observed position-value pairs. A dataset is defined as a collection of full data samples $D = \\{P_a\\}_{a=1}^M$, where M is the number of samples. The goal of operator learning for the reconstruction problem is to learn a mapping R from the observed data sample to the full data sample, i.e.,\n$v_a^i = R(x_a^i; \\tilde{P_a}), \\forall i' \\notin O_a$. \nIn certain reconstruction problems, we have access to adequate full data samples, making a data-driven approach possible. This involves representing the reconstruction operator with a neural network $R_\\Theta$. The observed data for training can be easily obtained by selecting some pairs from the full data samples. Furthermore, for new observed data, the neural network can predict the full data directly without running complex reconstruction algorithms, which is the advantage of operator learning compared to PINNs reconstruction or any other method [29].\nWe assume that the full data sample can be divided into patches of the same size, with the observed data consisting of some patches from the full data (i.e., the remaining patches are masked). Therefore, the positions x become the patch indices. The model is expected to learn to fill in the masked patches based on the observed data. The key idea is to use the Energy Transformer (introduced in detail in the next section) to serve as a memory model to store the patterns of the full data, and retrieve the missing parts by querying the memory with the observed data.\nThe workflow of the proposed method is illustrated in Figure 1. The patcher and depatcher are fixed operations while the Energy Transformer, tokenizer and detokenizer are trainable neural networks. In the training stage,"}, {"title": "2.2 Energy Transformer", "content": "We now introduce the structure of the Energy Transformer and how it is employed [33]. The Energy Transformer is built on the ET block (depicted in Figure 2), which consists of three components: layer normalization, energy attention layer, and Hopfield network. The energy attention layer can learn large-scale global patterns from the interaction between the input tokens, while the Hopfield network can learn small-scale local patterns within each token.\nLayer Normalization. Each token $x \\in R^D$ passed to Energy Transformer is normalized by layer normalization. This enhances the model stability and generalization ability. The layer normalization is defined as:\n$\\hat{x} = \\gamma \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\delta$,\nwhere \u03bc and \u03c3 are the mean and standard deviation of the input x, \u03b3 and \u03b4 are learnable parameters, and \u03b5 is a small constant to avoid division by zero. Furthermore, the layer normalization can be defined as the partial"}, {"title": "Multi-head Energy Attention.", "content": "The main contribution of the Energy Transformer is the multi-head energy attention layer. Similar to the conventional attention mechanism, energy attention layer learns the interaction between tokens. However, in this case, each token generates query and key vectors, and there is no value vector. The output of the energy attention layer is a scalar energy value, where the minimization of this energy indicates that the queries of the masked tokens are aligned with the keys of the observed tokens. This alignment results in the output of the energy attention layer capturing the global structure of the full data with limited observations. The energy function of the multi-head energy attention layer is defined as:\n$E^{ATT} = -\\frac{1}{H} \\sum_{h=1}^H \\sum_{C=1}^N log\\bigg(\\frac{exp(\\beta A_{hBC})}{\\sum_{B \\neq C} exp(\\beta A_{hBC})}\\bigg)$,\nwhere H is the number of heads, N is the number of tokens, \u03b2 is a constant, and $A_{hBC}$ is the attention matrix between the B-th and C-th tokens in the h-th head. More specifically, the attention matrix is defined as:\n$A_{hBC} = \\sum_{\\alpha=1}^Y K_{hB}^{\\alpha}Q_{hC}^{\\alpha}, A \\in R^{Y \\times N \\times N}$\n$K_{hB}^{\\alpha} = \\sum_{l=1}^D W_{ahl}^K g_{Bl}, K \\in R^{Y \\times H \\times N}$,\n$Q_{hC}^{\\alpha} = \\sum_{l=1}^D W_{ahl}^Q g_{Cl}, Q \\in R^{Y \\times H \\times N}$,\nwhere $W^K, W^Q \\in R^{Y \\times H \\times D}$ are learnable parameters, $g_B$ and $g_C$ are the normalized vectors of the B-th and C-th tokens, D is the dimension of each token, and Y is the internal dimension of the K-Q contraction."}, {"title": "Hopfield Network.", "content": "The Hopfield network is employed to learn the local patterns within each token, ensuring that the token representations are consistent with real data. The energy function of the Hopfield network is defined as:\n$E^{HN} = - \\sum_{B=1}^N \\sum_{\\mu=1}^K \\bigg(\\sum_{j=1}^D G\\big(\\xi_{\\mu j} g_{Bj}\\big)\\bigg)$,\nwhere K is the number of hidden units in the Hopfield network, \u03be \u2208 $R^{K \\times D}$ is a learnable matrix, which corresponds to \"memories\" in simple Hopfield networks, and G(\u00b7) is the integral of the activation function. The Hopfield network functions analogously to the feedforward MLP in traditional transformer models, and it is applied to each token individually."}, {"title": "Forward Pass.", "content": "The patches are initially tokenized by an embedding layer. The input tokens x are then normalized by a layer normalization layer. Both the multi-head energy attention layer and the Hopfield network take the normalized g and output a scalar energy. The energy of the ET block, denoted as E, is the summation of these two energies. Then, the output of the ET block is the local minimizer of the energy function E, which is obtained by iteratively updating the tokens according to the rule: $x^{t+1} = x^t \u2013 a\\nabla_x E(g(x))$, where a is the step size constant. The tokens are initialized as $x^0 = x$ and iterated until convergence. In practice, the convergence is achieved after a certain number of iterations. Finally, the tokens are detokenized to obtain the reconstructed data."}, {"title": "3 Results", "content": "In this section, we demonstrate the effectiveness of the proposed method for three different examples in fluid mechanics. These examples validate the ability of the Energy Transformer to learn the reconstruction operator for different types of data with small errors. In all these examples, two-layer MLPs with ReLU activation are used for the tokenizer and detokenizer. The patches are flattened to be tokenized. The hyperparameters for the model are in Table 1."}, {"title": "3.1 2D Vortex Street", "content": "In flow past bluff bodies the von Karman vortex street is formed in the wake above a certain Reynolds number, e.g. $Re \u2248 50$ for flow past a circular cylinder [39]. Here, we will use simulated data to demonstrate the ET-based flow field reconstruction. The spatial domain is [-4, 12] \u00d7 [-4, 4], with the cylinder diameter $D = 1$ and the Reynolds number is 400. The flow is simulated by a high-order spectral element method and we obtain 100 snapshots [39]. The training data is the first 80 time steps and the model is tested on the rest 20 time steps. The data shape of each snapshot is [128, 256, 4], which is the stack of the velocity field, pressure field and the temperature field. Each patch is of shape [8, 8, 4] due to the multi-modality of the data. The observed data is randomly masked with 90% of the patches, in other words, only 10% of the patches are observed. The model is trained based on the relative mean squared error (RMSE) loss for 10,000 epochs with learning rate $10^{-4}$. The training loss history is"}, {"title": "3.2 2D Jet Flow", "content": "The jet impinging experiments provide detailed data about the interaction between fluid and surface. In this example, the dataset is captured using Schlieren imaging techniques, hence represents the gradient of the density of the fluid [40]. The experiments are conducted in the following way: the air is compressed and the temperature"}, {"title": "3.3 3D Turbulent Jet Flow", "content": "Turbulent jet flow experiments facilitate the understanding of the potential core of the jet and the breakdown to turbulence downstream. In this example, the data is collected from experiments conducted at TU Delft, where"}, {"title": "4 Comparison with Other Methods", "content": "The Energy Transformer presents a unique approach to reconstruction problems when compared to other operator learning methods like PINNs, DeepONet, and FNO. PINNs, which are used in Artificial Intelligent Velocimetry, can incorporate physical laws into reconstruction process. Due to their minimization nature, they can handle observed data at random positions. However, the inference cost is high as the process requires training the model to minimize the physics equation loss and data loss, which are highly non-convex. Furtheremore, PINNs suffer from poor generalization ability for extrapolation tasks.\nOn the other hand, DeepONet and FNO are purely data-driven methods. Their inference cost is much lower because the inference does not include minimization and is just a feedforward pass. However, they cannot handle observed data at random positions as covering all possible configurations is prohibitively expensive.\nThe Energy Transformer integrates the benefits of these approaches while addressing their limitations. Un- like other neural operators, Energy Transformer learns a memory-like energy function, whose minimizer is the reconstruction result. The inference is a minimization process on a relatively simple function, which is more com- putationally efficient than PINNs. And due to the strong inductive bias of the simple form of energy function, it can handle random observed data positions effectively without exhaustive search over all possible configura- tions. This unique capability makes the Energy Transformer a powerful tool for reconstruction tasks, offering low computational costs with flexibility for irregular sparse data measurements."}, {"title": "5 Summary", "content": "In this paper, we have proposed an operator learning framework with the Energy Transformer for reconstruction problems in fluid mechanics, where the goal is to recover the full flow field from sparse observed data. We demonstrated the effectiveness of the proposed method by three examples in fluid mechanics. The results show that the Energy Transformer can learn the reconstruction operator for different types of data with tolerable errors, even for the real problem with noisy data. Furthermore, its training and inference costs are relatively low compared with PINNs and the artificial intelligent velocimetry (AIV) method. In practice, sparse flow measurements can be accomplished in different parts of the domain, e.g. with small windows of particle image velocimetry (PIV), to cover the spatially varying dynamics across the domain. It is also possible to use direct numerical simulation for the training stage and build foundation models based on the Energy Transformer framework we presented herein, then the observed data will be obtained using selective experiments.\nAs a memory model, the Energy Transformer performs well when the test data resembles the training data, a scenario often encountered in fluid mechanics. However, for general evolution problems, the Energy Transformer may not perform as effectively, as it learns to complete individual snapshots rather than understand the underlying evolution rules. But even in this case, since the energy attention mechanism can learn large-scale structures in flow field, the Energy Transformer can still provide an averaged representation of the training data, which can be useful for limited extrapolation. Our work provides a new perspective for solving reconstruction problems in fluid mechanics and offers a promising direction for other engineering and scientific applications, e.g., in solid mechanics using digital impage correlation (DIC), in geophysics, in weather prediction, and more."}]}