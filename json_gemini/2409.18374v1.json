{"title": "Adaptive Learning of the Latent Space of Wasserstein Generative Adversarial Networks", "authors": ["Yixuan Qiu", "Qingyi Gao", "Xiao Wang"], "abstract": "Generative models based on latent variables, such as generative adversarial networks (GANs) and variational auto-encoders (VAEs), have gained lots of interests due to their impressive performance in many fields. However, many data such as natural images usually do not populate the ambient Euclidean space but instead reside in a lower-dimensional manifold. Thus an inappropriate choice of the latent dimension fails to uncover the structure of the data, possibly resulting in mismatch of latent representations and poor generative qualities. Towards addressing these problems, we propose a novel framework called the latent Wasserstein GAN (LWGAN) that fuses the Wasserstein auto-encoder and the Wasserstein GAN so that the intrinsic dimension of the data manifold can be adaptively learned by a modified informative latent distribution. We prove that there exist an encoder network and a generator network in such a way that the intrinsic dimension of the learned encoding distribution is equal to the dimension of the data manifold. We theoretically establish that our estimated intrinsic dimension is a consistent estimate of the true dimension of the data manifold. Meanwhile, we provide an upper bound on the generalization error of LWGAN, implying that we force the synthetic data distribution to be similar to the real data distribution from a population perspective. Comprehensive empirical experiments verify our framework and show that LWGAN is able to identify the correct intrinsic dimension under several scenarios, and simultaneously generate high-quality synthetic data by sampling from the learned latent distribution.", "sections": [{"title": "1 Introduction", "content": "Unsupervised generative models receive great attentions in the machine learning community nowadays due to their impressive performance in many fields (Kingma and Welling, 2014;\nGoodfellow et al., 2014; Li et al., 2015; Dinh et al., 2016; Gao et al., 2020; Qiu and Wang,\n2021). Given a random sample from a p-dimensional random vector X \u2208 X C R\u00ba with\nan unknown distribution Px, the goal is to train a generative model that can produce\nsynthetic data that look similar to the observed samples from X. While there are several\nways of quantifying the similarity, the most common approach is to directly employ some\nof the known divergence measures, such as the Kullback-Leibler (KL) divergence and the\nWasserstein distance, between the real data distribution and the synthetic data distribution.\nThere are two influential frameworks for generative models: generative adversarial net-\nworks (GANs, Goodfellow et al., 2014) and variational auto-encoders (VAEs, Kingma and\nWelling, 2014). They are latent variable models through a latent variable Z\u2208 Z C Rd\ndrawn from a simple and accessible prior distribution Pz, such as the standard multivari-\nate normal distribution Pz = N(0, Ia). Then the synthetic data are generated by either a\ndeterministic transformation G : Z \u2192 X or a conditional distribution p(x|z) of X given Z.\nGAN and WGAN. Training GANs is like a two-player game, where two networks, a\ngenerator and a discriminator, are simultaneously trained to allow the powerful discrimi-"}, {"title": null, "content": "nator to distinguish between real data and generated samples. As a result, the generator\nis trying to maximize its probability of having its outputs recognized as real. This leads to\nthe following minimax objective function,\n$\\inf _{G \\in \\mathcal{G}} \\sup _{F \\in \\mathcal{F}} \\mathbb{E}_{X} [\\log(f(X))] + \\mathbb{E}_{Z} [\\log (1 - f(G(Z)))]$,                                                                                                                                                                                (1)\nwhere f \u2208 F is a discriminator and G\u2208 G is a generator. Optimizing (1) is equivalent to\nminimizing the Jensen-Shannon divergence between the generation distribution and real\ndata distribution. GANs can generate visually realistic images, but suffer from unstable\ntraining and mode collapsing.\nThe Wasserstein GAN (WGAN, Arjovsky et al., 2017) is an extension to the vanilla\nGAN that improves the stability of training by leveraging the 1-Wasserstein distance be-\ntween two probability measures. Denote by PG(z) the generation distribution measure, and\nthen the 1-Wasserstein distance between Px and PG(z) is defined as\n$W_{1}(P_{X}, P_{G(Z)}) = \\inf_{\\pi \\in \\Pi(P_X, P_Z)} \\mathbb{E}_{(X, Z) \\sim \\pi} ||X - G(Z)||$,                                                                                                                                                                                                            (2)\nwhere || || represents the l\u2082-norm and I(Px, Pz) is the set of all joint distributions of (X, Z)\nwith marginal measures Px and Pz, respectively. It is hard to find the optimal coupling \u03c0\nthrough this constrained primal problem. However, thanks to the Kantorovich-Rubinstein\nduality (Villani, 2008), WGAN can learn the generator G by minimizing a dual form of\n(2),\n$W_{1}(P_{X}, P_{G(Z)}) = \\sup_{f \\in \\mathcal{F}} {\\mathbb{E}_{X} f(X) - \\mathbb{E}_{Z}f(G(Z))}$,                                                                                                                                                                                (3)\nwhere f is called the critic function, and F is the set of all bounded 1-Lipschitz functions.\nWeight clipping (Arjovsky et al., 2017) and gradient penalty (Gulrajani et al., 2017) are\ntwo common strategies to maintain the Lipschitz continuity of f. Weight clipping utilizes\na tuning parameter c to clamp each weight parameter to a fixed interval [-c, c] after each"}, {"title": null, "content": "gradient update, but this method is very sensitive to the choice of the parameter c. Instead,\ngradient penalty adds a regularization term, $\\mathbb{E}_{X} {(\\nabla#f(X)|| - 1)^{2}}$, to the loss function\nto enforce the 1-Lipschitz condition, where X is sampled uniformly along the segment\nbetween pairs of points sampled from Px and PG(z). This is motivated by the fact that the\noptimal f has unit gradient norm on the segment between optimally coupled points from\nPx and PG(Z).\nVAE and WAE. A VAE defines a \u201cprobabilistic decoder", "probabilistic encoder": "z|x) with the unknown pa-\nrameter & is defined to approximate the posterior distribution $p_{\\theta}(z|x) = p_{\\theta}(x|z)p_z(z)/p_{\\theta}(x)$.\nThe objective of VAE is to maximize a lower bound of the log-likelihood log po(x), which\nis called the evidence lower bound (ELBO):\nELBO = Eq\u2084(z|x) [log po(x|z)] \u2013 KL (q\u2084(z|x)||pz(z)),\nwhere the first term can be efficiently estimated by the Monte Carlo sampling, and the\nsecond term has a closed-form expression when q\u00f8 is Gaussian. VAEs have strong theoretical\njustifications and typically can cover all modes of the data distribution. However, they often\nproduce blurry images due to the normal approximation of the true posterior.\nThe Wasserstein auto-encoder (WAE, Tolstikhin et al., 2018) makes two modifications\nto VAE. It uses a deterministic encoder Q : X \u2192 Z to approximate the conditional\ndistribution of Z given X, and a deterministic generator G : Z \u2192 X to approximate the\nconditional distribution of X given Z. In addition, WAE adopts the 1-Wasserstein distance\nbetween the real data distribution Px and the generation distribution PG(z), rather than\nthe KL divergence used in VAEs, to train the model. Let PQ(x) denote the aggregated"}, {"title": null, "content": "posterior distribution measure, and then WAE minimizes the following reconstruction error\nwith respect to the generator G,\n$\\inf_{Q \\in \\mathcal{Q}} \\mathbb{E}_{X} ||X - G(Q(X))|| + \\lambda D(P_{Q(X)}, P_Z)$,                                                                                                                                                                                              (2)\nwhere D is any divergence measure between two distributions PQ(x) and Pz, and x > 0 is\na regularization coefficient. The regularization term forces the aggregated posterior PQ(x)\nto match the prior distribution Pz.\nThere are several limitations for the generative models above. It is a requirement for\ncurrent approaches of training generative models to pre-specify the dimension of the latent\ndistribution Pz and treat it as fixed during the training process. For example, the latent\ndimensions for VAEs and GANs are pre-specified by users. Another type of generative\nmodel called normalizing flows (Dinh et al., 2016) keeps the latent dimension the same as the\ndimension of the data. This is because normalizing flows approximate the data distribution\nby a deterministic invertible mapping G such that X = G(Z). Since many observed data\nsuch as natural images lie on a low-dimensional manifold embedded in a higher-dimensional\nEuclidean space, an inappropriate choice of the latent dimension could cause a wrong latent\nrepresentation that does not populate the full ambient space (Rubenstein et al., 2018).\nHence, the wrongly specified latent dimension fails to uncover the structure of the data,\nand the corresponding generative models may suffer from mode collapsing, under-fitting,\nmismatch of representation learning, and poor generation qualities. Furthermore, although\nthere are many interesting works taking advantages of both VAEs and GANs (Larsen et al.,\n2016; Dumoulin et al., 2017; Donahue et al., 2017; Chen et al., 2021), it remains unclear\nwhat principles are underlying the framework combining the best of WAEs and WGANS\nwhen the latent dimension is unknown.\nTo handle the aforementioned drawbacks, we propose a novel framework, called the\nlatent Wasserstein GAN (LWGAN), to identify the intrinsic dimension of a data distribution"}, {"title": null, "content": "that lies on a topological manifold, and then improve the quality of generative modeling as\nwell as representation learning. We have performed two major modifications to the current\nGAN and VAE frameworks. First, we change the latent distribution from N(0, Ia) to a\ngeneralized normal distribution N(0, A) with A being a diagonal matrix with entries taking\nvalues 0 or 1. Therefore, the rank of A allows us to characterize the intrinsic dimension\nof the latent space. This modification has been adopted for the flow model to reduce the\ndimension of the latent space (Zhang et al., 2023), but it has not been applied to GAN\nor VAE models. Second, we combine WGAN and WAE in a principled way motivated\nby the primal-dual iterative algorithm. We utilize a deterministic encoder Q : X \u2192 Z\nto learn an informative prior distribution Pz ~ N(0, A). On the other hand, a generator\nG: Z \u2192 X is combined with Q to generate images that look like the real ones using the\nlatent code Z from Pz. We theoretically guarantee the existence of such a generator G and\nan encoder Q. To get rid of possible invalid divergences, we focus on the 1-Wasserstein\ndistance to measure the similarities between two distributions, which applies to any pair\nof distributions as long as they can be sufficiently sampled. Note that the KL divergence\nis not well-defined when the supports of two probability measures do not overlap, which is\nvery common for high-dimensional data.\nThe rest of the paper is organized as follows. Section 2 investigates the phenomenon\nof dimension mismatch between the latent distribution and data distribution. Section 3\npresents the new LWGAN framework that provides a feasible way to estimate the encoder,\ngenerator, and intrinsic dimension. Theoretical analyses are given in Section 4, including\nresults on generalization error bounds, estimation consistency, and intrinsic dimension con-\nsistency. Section 5 demonstrates extensive numerical experiments under different settings\nto verify that the LWGAN is able to detect the intrinsic dimensions for both simulated\nexamples and real image data. Finally, Section 6 concludes this article. Proofs of theorems\nand additional numerical results are provided in the supplementary materials."}, {"title": "2 Issues of Latent Dimension Mismatch", "content": "Throughout this article we use X C R\u00ba and Z C Rd to denote the spaces of observed\ndata points and latent variables, respectively. To precisely describe the structure of high-\ndimensional data with a low latent dimension, we first make the following definition of a\ntopological manifold.\nDefinition 1 (Topological manifold, Lee, 2013). Suppose that M is a topological space.\nM is a topological manifold of dimension r if M is a second-countable Hausdorff space,\nand for each x \u2208 M, there exist an open subset U C M containing x, an open subset\nVCR', and a homeomorphism 4 between U and V. A homeomorphism 6 : U Vis a\ncontinuous bijective mapping with a continuous inverse 6-1.\nIn this article, all manifolds are referred to as topological manifolds unless otherwise\nnoted. Typically, M is a subset of some Euclidean space R\u00ba, in which case the Hausdorff\nand second-countability properties in Definition 1 are automatically inherited from the\nEuclidean topology. To exclude overly complicated cases, we moderately strengthen the\nqualification of the homeomorphism 4 in Definition 1 to make it a global one:\nAssumption 1. X is an r-dimensional manifold, and there exists a homeomorphism 4\nbetween X and Rr.\nIn what follows, the symbol y is used to denote one homeomorphism between X and R\u201d.\nThen we can define a continuous distribution supported on the manifold X that satisfies\nAssumption 1.\nDefinition 2. A random vector X \u2208 R\u00ba is said to have a continuous distribution Px\nsupported on X, if its image \u03c6(X) follows a continuous distribution on R\u201d.\nLet X \u2208 X C R\u00ba be the observed data with a continuous distribution Px supported on\nX, where X satisfies Assumption 1. We define the intrinsic dimension of the data distribu-"}, {"title": null, "content": "tion Px as the dimension of the manifold X, denoted by InDim(Px) = r, and its ambient\ndimension as the dimension of the enclosing Euclidean space, denoted by AmDim(Px) = p.\nBy Theorem 1.2 of Lee (2013), InDim(Px) must be unique, and it cannot be larger than\nAmDim(Px).\nIn most existing deep generative models, the latent variable Z is selected as ad-\ndimensional standard normal distribution N(0, Ia), so InDim(Pz) = AmDim(Pz) = d. The\ndimension d is typically predetermined to be a number that is smaller than p. In GAN-based\nmodels, if the generator G is a continuous function, then the synthetic sample G(Z) will be\nsupported on a manifold of dimension at most InDim(Pz). When InDim(Pz) < InDim(Px),\nforcing PG(z) to be close to Px with unmatched intrinsic dimensions is a challenging task.\nOn the other hand, in auto-encoder-based models, similar phenomenon of dimension mis-\nmatch occurs for the encoded distribution PQ(x). For example, it is difficult to enforce PQ(x)\nto be close to Pz if InDim(Px) < InDim(Pz), as filling a plane with a one-dimensional curve\nis hard.\nTo highlight this phenomenon and to motivate our proposed model, we first employ\na toy example to provide intuitions for the effects and consequences of different intrinsic\ndimensions of the model and data distributions. Consider a 3D S-curve dataset as shown\nin Figure 1(a), where each data point X = (X1, X2, X3) is generated by\nX\u2081 = sin(3\u03c0(U \u2013 0.5)), X2 = 2V, X3 = sign(3\u03c0(U \u2013 0.5)) cos(3\u03c0(U \u2013 0.5)),\nfor U ~ Unif(0,1) and V ~ N(0,1). This example results in AmDim(Px) = 3 and\nInDim(Px) = 2. We first choose the latent distribution Pz to be a one-dimensional normal\ndistribution N(0, 1), and then the generated sample from WGAN is plotted in Figure 1(b).\nTo minimize the 1-Wasserstein distance between the real distribution Px and the genera-\ntion distribution PG(z), WGAN learns an outer contour of the S-curve, but it cannot fill\npoints on the surface. Instead, if we choose a three-dimensional standard normal N(0, I3)"}, {"title": null, "content": "as the latent distribution, then WAE is forced to reconstruct the images well, but at the\nsame time it tries to fill the three-dimensional latent space evenly by a distribution sup-\nported on a two-dimensional manifold. The only way to do this is by curling the manifold\nup in the latent space as shown in Figure 1(d). This disparity between Pz and PQ(x) in\nthe latent space induces a poor generation of PG(z) in Figure 1(c)."}, {"title": "3 The Latent Wasserstein GAN", "content": "A natural solution to the mismatch problem described in Section 2 is to select a latent\ndistribution Pz whose intrinsic dimension is the same as that of the data distribution Px.\nHowever, InDim(Px) is typically unknown, so one option is to learn it from the data. When\nboth the continuous generator G and the continuous encoder Q are combined in an auto-\nencoder generative model, PG(z) = Px and PQ(x) = Pz cannot be satisfied simultaneously\nunless InDim(Px) = InDim(Pz) according to our previous discussion. This motivates us\nto search for an encoder Q and a corresponding generator G, such that Q(X) reflects the\nlatent space supported on an r-dimensional manifold, and generated samples using the\nlatent variables are of high quality. To be concrete, we need an auto-encoder generative\nmodel that satisfies the following three goals at the same time: (a) the latent distribution\nPz is supported on an r-dimensional manifold; (b) the distribution of G(Z) is similar to\nPx; (c) the difference between X and its reconstruction G(Q(X)) is small."}, {"title": "3.1 Existence of optimal encoder-generator pairs", "content": "Unlike conventional generative models that use a fixed standard normal distribution as\nthe latent distribution, we consider a latent distribution whose intrinsic dimension could\nbe less than d, i.e., the latent variable Z \u2208 Z C Rd can have a distribution supported\non some manifold Z. This idea is realized by the generalized definition of the normal\ndistribution (Zhang et al., 2023). In particular, let As = diag(1,...,1,0,...,0) be a\ndiagonal matrix whose first s diagonal elements are one and whose remaining (d - s)\ndiagonal elements are zero, and Zo be a random vector following standard multivariate\nnormal distribution N(0, Ia). Then clearly, the random vector Z = A5Zo is supported on an\ns-dimensional manifold Z, and its distribution Pz = PA&zo has dimensions InDim(Pz) = s\nand AmDim(Pz) = d. For convenience, we use the classic notation N(0, As) to denote this\ndistribution, although As is a degenerate covariance matrix.\nChoosing Pz = N(0, As), where s is a parameter to estimate, enables us to solve the\ndimension mismatch problem in Section 2. If s = r, then the latent variable Z can be\nmapped to G(Z) supported on an r-dimensional manifold, and meanwhile, Pz and the\nencoded distribution PQ(x) can have matched intrinsic dimensions. Formally, Theorem\n1 states that for any data distribution Px defined by Definition 2, there always exist a\ncontinuous encoder that guarantees meaningful encodings on an r-dimensional manifold,\nand a continuous generator G\u00ba that generates samples with the same distribution as Px,\nusing those latent points encoded by Q\u00ba.\nTheorem 1. If d > r, then there exist two continuous mappings Q\u00ba: X Z and G\u00b0:\nZ \u2192 X such that Q\u00b0(X) ~ N(0, Ar) and X = G\u00b0(Q\u00b0(X)).\nIn such cases, we call (Q\u00b0, G\u00b0) an optimal encoder-generator pair for the data distribu-\ntion Px, and note that (Q\u00ba, G\u00ba) may not be unique. On the other hand, Corollary 1 below\nshows that if the ambient dimension of Pz is insufficient, then the auto-encoder structure is"}, {"title": null, "content": "unable to recover the original distribution of X, which justifies the finding in Figure 1(b).\nCorollary 1. Suppose that d < r. Then for any continuous mappings Q : RP Rd and\nG: Rd \u2192 RP, we have Ex ||X - G(Q(X))|| > 0."}, {"title": "3.2 The proposed model", "content": "Theorem 1 shows the possibility to identify the dimension of the data manifold X by\nlearning a latent distribution with the same intrinsic dimension via the encoder Q. In\nthis section, we realize this idea through our new auto-encoder generative model, LWGAN,\nwhich takes advantages of both WGAN and WAE. LWGAN is capable of learning Q, G,\nand r simultaneously to accomplish all of our three goals. For brevity, we abbreviate the\nsubscript s in the matrix As when no confusion is caused.\nThere are three probability measures involved in our problem: the real data distribution\nPx, the generation distribution PG(Azo), and the reconstruction distribution PG(Q(X)). Our\ngoal is to ensure that all three measures are similar to each other in a systematic way. To\nthis end, we propose the following distance between Px and PG(AZ0) with given G and A:\n$\\tilde{W}_{1}(P_{X}, P_{G(AZ_0)}) = \\inf_{Q \\in \\mathcal{Q}} \\sup_{F \\in \\mathcal{F}^0} \\mathcal{L}_A(G, Q, f)$,                                                                                                                                                                                (4)\n$\\mathcal{L}_A(G, Q, f) = \\mathbb{E}_{X} ||X - G(Q(X))|| + \\mathbb{E}_{X} [f(G(Q(X)))] - \\mathbb{E}_{Z_0} [f(G(AZ_0))]$,\nwhere F is the set of all bounded 1-Lipschitz functions, and Q\u00ba is the set of continuous\nencoder mappings. The term Ex ||X \u2013 G(Q(X))|| can be viewed as the auto-encoder re-\nconstruction error in WAE, and also a loss to measure the discrepancy between Px and\nPG(Q(X)). The other term Ex [f(G(Q(X)))] \u2013 Ezo [f(G(AZo))] quantities the difference be-\ntween PG(Q(X)) and PG(AZ0). Theorem 2 below shows that, under some mild conditions, (4)\nachieves its minimum as the 1-Wasserstein distance W\u2081(Px, PG(Azo))."}, {"title": null, "content": "Theorem 2. The W\u2081 distance defined in (4) has the following representation:\n$W_{1}(P_{X}, P_{G(AZ_0)}) = \\inf_{Q \\in \\mathcal{Q}} {W_{1}(P_{X}, P_{G(Q(X))}) + W_{1}(P_{G(Q(X))}, P_{G(AZ_0)})}$.                                                                                                                                                                                (5)\nTherefore, W\u2081(Px, PG(AZo)) \u2264 W1(Px, PG(Azo)), and the equality holds if there exists an\nencoder Q \u2208 Q\u00basuch that Q(X) has the same distribution as AZo.\nRemark 1. Theorem 1 shows that there exists some optimal encoder-generator pair (Q\u00ba, G\u00b0)\nsuch that Q\u00b0(X)=A,Zo and X = G\u00b0(Q\u00b0(X)). Therefore, Q\u00ba is an optimal solution to\n(5) for A = Ar, and hence the equality W\u2081(Px, PG(ArZo)) = W1(Px, PG(ArZo)) holds. This\nindicates that W\u2081 is a tight upper bound for W\u2081. Furthermore, with G = G\u00b0, we have\nW1(Px, PG\u00b0(ArZo)) = 0, which reaches its global minimum.\nRemark 2. The condition Q(X) AZo is sufficient but not necessary for W\u2081 = W\u2081 to hold.\nFor example, using (Q\u00ba, G\u00ba) in the proof of Theorem 1, we can show that Q\u00b0(X) ArZo\nbut W\u2081(PX, PGO(AsZ0)) = W1(Px, PG\u00b0(AsZ0)) = 0 for any s such that r < s < d.\nIn our framework, we represent the encoder, generator, and critic using deep neural\nnetworks, G = G(\u00b7;0G), Q = Q(\u00b7;0q), f = f(\u00b7;0f), where 0 = (0, 0, 0f) are the network\nparameters. We restrict the three components of 0 to compact sets \u04e8\u0434, \u04e8q, and Of,\nrespectively, and further define \u0113f = {0f \u2208 Of : || f(\u00b7; 0f)||\u2081 \u2264 1}, where ||g||L stands for the\nLipschitz constant of a function g. Then we define the parameter space \u04e8 = \u04e8\u04ab \u00d7 \u04e8Q \u00d7 Of\nand function spaces G = {G(\u00b7;\u03b8G) : \u03b8\u03b1 \u2208 \u0398G}, Q = {Q(\u00b7; \u03b8q) : dq \u2208 \u04e8q}, F = {f(\u00b7;0f) :\nOf \u2208 f}. Accordingly, hereafter we replace the spaces Q and F\u00ba in (4) with Q and F\nrespectively for the definition of W1(Px, PG(Azo)).\nIn practice, we only have the empirical versions of Px and PG(AZ0). Suppose we have\nobserved an i.i.d. data sample X1, ..., Xn, and have simulated an i.i.d. sample of N(0, Id),"}, {"title": null, "content": "Z0,1,..., Zo,n, where X and Zo samples are independent. Then we define\nL(x, z; 0) = ||x \u2212 G(Q(x;\u03b8\u2084);0G)|| + f(G(Q(x;0q);0G); 0f) \u2212 f(G(z; \u03b8G); \u03b8f),\nl(0, A) = Ex\u00aezo [L(X, AZ\u03bf, \u03b8)], \u00cen (\u03b8, \u0391) = \u03a3(\u03a7\u03af, \u0391\u0396\u03bf,\u03af, \u03b8),\nwhere Exozo means taking the expectation of independent X and Zo. Clearly,\nW1(PX, PG(AZo)) = inf sup L(G, Q, f, A) = inf sup l(0, A),\nQEQ FEF\ntho \\in \\Theta_G \\theta_Q \\in \\Theta_\\theta\\varepsilon\\Theta_f\nand we denote its empirical version as W\u2081(Px, PG(Azo)) = infeqeeq suposees ln(0, A).\nRemark 1 of Theorem 2 motivates us to estimate the generator G and the rank-revealing\nmatrix A based on the W\u2081 distance, but Remark 2 suggests that purely minimizing W\u2081 is\nnot enough, since a matrix A with a rank larger than r can still drive W\u2081 to zero, the global\nminimum value. Therefore, we also need to introduce a penalty term to regularize the rank\nof A. Since A is uniquely determined by its rank s, below A and s are used interchangeably\nto represent the rank parameter. Define the rank-regularized objective function as\n\u03b2\u03b7(\u03b8\u03c2, A) = W\u2081(Px, PG(Azo)) + \u03bb\u03b7\u00b7 rank(A),\nwhere An is a deterministic sequence satisfying An 0 and n1/2\u03bb\u03b7 \u2192 \u221e, which will be\njustified in Theorem 5. Then the generator G and the matrix A are estimated by\n$(\\hat{\\theta}_G, \\hat{r}) = \\underset{\\theta_G \\in \\Theta_G, 1 < s < d}{\\arg \\min} \\hat{l}_n (\\theta_G, A_s)$.                                                                                                                                                                                (6)\nWhen the optimal points are not unique, I can be chosen arbitrarily from the solution\nset, and \u0155 is taken as the smallest one among all the optimal points."}, {"title": "3.3 Computational algorithm", "content": "The optimization problem (6) can be solved by computing the \u201crank score\u201d\n$\\hat{l}_n(s) = \\underset{\\theta_G, \\theta_Q} \\min \\underset{\\theta_f}{\\max} \\hat{l}_n (\\theta, A_s) + \\lambda_n s$                                                                                                                                                                                (7)\nfor each s = 1, . . ., d, and then we have r = arg mins \u00een (s). Equivalently, we need to solve\n$\\underset{G_1, Q_1} \\min \\underset{f_1}{\\max} \\frac{1}{n} \\sum_{i=1}^n [||X_i - G_1(Q_1(X_i))|| + f_1(G_1(Q_1(X_i))) - f_1(G_1(A_1Z_{o,i}))", "f_d(G_d(A_dZ_{o,i}))": "lambda_n \\cdot d$\nby fitting d different sets of neural networks (Gs, Qs, fs), s = 1,...,d, which may be\ntime-consuming. Instead, we propose a practical and efficient algorithm based on the idea\nthat encoder and critic functions under different ranks can share network parameters. We"}]}