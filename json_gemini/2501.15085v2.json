{"title": "DATA CENTER COOLING SYSTEM OPTIMIZATION\nUSING OFFLINE REINFORCEMENT LEARNING", "authors": ["Xianyuan Zhan", "Xiangyu Zhu", "Peng Cheng", "Xiao Hu", "Ziteng He", "Hanfei Geng", "Jichao Leng", "Huiwen Zheng", "Chenhui Liu", "Tianshun Hong", "Yan Liang", "Yunxin Liu", "Feng Zhao"], "abstract": "The recent advances in information technology and artificial intelligence have\nfueled a rapid expansion of the data center (DC) industry worldwide, accompanied\nby an immense appetite for electricity to power the DCs. In a typical DC, around\n30~40% of the energy is spent on the cooling system rather than on computer\nservers, posing a pressing need for developing new energy-saving optimization\ntechnologies for DC cooling systems. However, optimizing such real-world in-\ndustrial systems faces numerous challenges, including but not limited to a lack of\nreliable simulation environments, limited historical data, and stringent safety and\ncontrol robustness requirements. In this work, we present a novel physics-informed\noffline reinforcement learning (RL) framework for energy efficiency optimization\nof DC cooling systems. The proposed framework models the complex dynam-\nical patterns and physical dependencies inside a server room using a purposely\ndesigned graph neural network architecture that is compliant with the fundamental\ntime-reversal symmetry. Because of its well-behaved and generalizable state-action\nrepresentations, the model enables sample-efficient and robust latent space offline\npolicy learning using limited real-world operational data. Our framework has been\nsuccessfully deployed and verified in a large-scale production DC for closed-loop\ncontrol of its air-cooling units (ACUs). We conducted a total of 2000 hours of short\nand long-term experiments in the production DC environment. The results show\nthat our method achieves 14~21% energy savings in the DC cooling system, with-\nout any violation of the safety or operational constraints. We have also conducted a\ncomprehensive evaluation of our approach in a real-world DC testbed environment.\nOur results have demonstrated the significant potential of offline RL in solving a\nbroad range of data-limited, safety-critical real-world industrial control problems.", "sections": [{"title": "1 INTRODUCTION", "content": "With the surge of demands in information technology (IT) and artificial intelligence (AI) in recent\ndecades, data centers (DCs) have quickly emerged as crucial infrastructures in modern society. Along\nwith the rapid growth of the DC industry, comes immense energy and water consumption. In 2022,\nthe global DC electricity consumption was estimated to be 240~340 TWh, accounting for around\n1~1.3% of global electricity demand (International Energy Agency, 2023). It is forecasted that\nby 2026, the DC energy consumption in the US will rise to approximately 6% of the country's\ntotal power usage (International Energy Agency, 2024). To deal with the considerable amount of\nheat generated from servers and achieve temperature regulation, cooling systems typically account\nfor about 30~40% of total energy consumption in large-scale DCs (Van Heddeghem et al., 2014).\nCompared to server-side energy consumption that is primarily spent on computational tasks, reducing\ncooling energy consumption offers greater practical value for energy saving. How to improve the\nenergy efficiency of DC's cooling systems while ensuring thermal safety requirements has become a\ncritical problem for the DC industry, which has great economic and environmental impacts."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "Data center cooling control optimization. The cooling loop of typical DCs consists of water-side\nand air-side sub-systems. The former cools water with chillers and evaporative cooling towers, while\nthe latter circulates the cold water to ACUs on the server floors. Through air-water heat exchange,\nthe cooled air is blown out from the ACUs, regulating the air temperature in the server room. The\ngenerated warm water is then sent back to the chillers and cooling towers for re-cooling. In this\nstudy, we focus on the air-side cooling in the server room (also called floor-level cooling (Lazic et al.,\n2018)), where the primary goal is to optimize the fan speed (control the airflow) and valve opening\n(control the amount of cold water supply) in multiple ACUs, in order to achieve energy saving while\nmeeting the room temperature requirements and ensuring thermal safety.\nTraditional air-side cooling control methods include the local PID control (Durand-Estebe et al.,\n2013), the two-stage method (Lazic et al., 2018; Mirhoseininejad et al., 2021; Ogawa et al., 2013;\nGarcia-Gabin et al., 2018), and expert-based control (Gao & Jamidar, 2014). Specifically, local PID\ncontrol relies on local sensor feedback to regulate the fan speed and valve opening of each individual\nACU based on PID controllers, which is only applicable to small-scale control problems and unable\nto jointly optimize numerous ACUs. Two-stage methods first build a mechanism model and then\napply optimization methods (such as MPC or linear quadratic control) to solve the cooling control\nproblem based on the model. Both local PID and two-stage methods lack sufficient expressive power\nto capture complex state-action and dynamics patterns, and do not scale effectively with increasing\nproblem size. Expert-based control leverages the experience and expertise of human operators to\nmanage ACU cooling, requiring significant human labor and lacking transferability to different DC\ncooling systems. Recently, there have been many attempts to use online reinforcement learning\n(RL) to solve the DC cooling optimization problem (Chen et al., 2019; Ran et al., 2022b;a; Mahbod\net al., 2022; Wang et al., 2022; Li et al., 2019; Chervonyi et al., 2022; An et al., 2023). However,\nthese studies are restricted to simulation-based policy learning and validation. For a safety-critical\nindustrial control scenario like DC cooling, it is nearly impossible to interact with real systems during\npolicy training, and building a high-fidelity simulator can be very costly and impractical. This makes\nthe previous online RL methods hardly have any success in real-world deployment.\nOffline reinforcement learning. Offline RL aims to solve a sequential decision-making problem\nformulated by a Markov Decision Process (MDP), solely using a fixed offline dataset D. The MDP is\ntypically defined by a tuple $(S, A, T, r, \\gamma)$ (Sutton & Barto, 2018), where S and A denote the state\nand action spaces, respectively. $T(S_{t+1}|S_t, a_t)$ denotes the transition dynamics. $r(s_t, a_t)$ denotes the\nreward function. y is the discount factor. Our goal is to learn an optimized policy $\\pi^*(s)$ based on\ndataset D to maximize the discounted cumulative return, i.e., $R(\\pi) = E[\\Sigma_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$.\nUnder the offline setting, evaluating the RL value function in OOD regions can produce falsely\noptimistic values. Such exploitation errors can quickly build up during Bellman updates, eventually\nleading to severe value overestimation and misguiding policy learning. Hence most offline RL\nmethods adopt various forms of data-related regularization schemes to stabilize policy learning,\nsuch as adding explicit or implicit behavioral constraints (Kumar et al., 2019; Fujimoto et al., 2019;\nFujimoto & Gu, 2021; Li et al., 2022; 2023; Mao et al., 2024b), value regularization (Kumar et al.,\n2020; Xu et al., 2022c; Bai et al., 2021; Zhan et al., 2022b; Lyu et al., 2022; Niu et al., 2022), or\nadopt strict in-sample learning (Kostrikov et al., 2022; Xu et al., 2022a;b; Wang et al., 2024; Mao\net al., 2024a). However, due to the exclusive use of strict data-related regularization, existing offline\nRL methods often suffer from over-conservatism and poor OOD generalization performance (Li\net al., 2022; Cheng et al., 2023), which greatly restricts their usability in most data-limited real-"}, {"title": "3 METHODOLOGY", "content": "In this study, we develop a physics-informed offline RL framework and a system to solve the DC\ncooling control optimization problem. We first mathematically formulate the problem into a standard\nMDP, with a specifically designed safety-aware reward function to ensure thermal-safe cooling\ncontrol. The core component of our framework is a T-symmetry enforced Thermal Dynamics Model\n(TTDM), with a specifically designed GNN architecture to embed domain knowledge of spatial and\ncontrol dependencies among sensors and ACUs. This model provides well-behaved and generalizable\nrepresentations, enabling data-efficient and robust offline policy learning in the latent space. Based on\nthe proposed offline RL framework, we also built an ACU control system that successfully deployed\nit in real-world DC environments. The overall framework of our method is illustrated in Figure 2."}, {"title": "3.1 PROBLEM FORMULATION AND REWARD DESIGN", "content": "As illustrated in Figure 1a, typical DC floor-level cooling systems involve several rows of server\nracks, flanked by two air handling rooms (AHRs) on each side, each containing 5 to 6 ACUs that blow\ncold air into the server rooms. The server racks are arranged with hot and cold aisles, utilizing hot (or\ncold) aisle containment. Temperature and humidity sensors are distributed throughout the aisles for\noverheating monitoring. The fan speed and valve opening can be controlled for each ACU to achieve\ndesirable temperature regulation. However, commercial DCs have strict temperature requirements,\nimproper control could cause cold aisle temperatures to exceed the safety threshold and negatively\nimpact server operations. To solve this problem, we formulate it into a MDP with states, actions, and\na reward function designed as follows:\nStates. The states of our problem $s = \\{s_s, s_a, s_e\\}$ contain three types of sensor inputs, including\ntemperature and humidity sensor readings within the hot and cold aisles, and server rack temperature\nsensor readings, denoted as $s_s$; the working states of ACUs, such as leaving water temperature (LWT),\nleaving air temperature (LAT), and entering air temperature (EAT), denoted as $s_a$; and lastly, as"}, {"title": "Safety-aware reward function", "content": "We design a reward function to balance energy saving and\ntemperature regulation, taking into account both the operational parameters of the ACUs and the\nenvironmental factors within the cooling system. For an actual ACU m, high fan speed $f_m$ directly\nincreases its power consumption, as the fan power consumption is proportional to the cube of the\nfan speed. On the other hand, a large valve opening $o_m$ could also marginally increase the energy\nconsumption on the water-side sub-system. However, increasing fan speed and valve opening also\nimproves the ACU's cooling effect, hence there is a complex trade-off. In terms of temperature\nsafety constraints, our primary concern is whether the cold aisle temperature (CAT) $T_n$ monitored\nby the corresponding temperature sensor n violates the safety threshold $\\rho_T$. Additionally, for safety\nconsiderations and in consultation with on-site engineers' experience, we also regulate the LAT $T_m^l$\nof ACU m below a predefined threshold $\\rho_L$. The resulting reward function is thus designed as:\n$r = r_0 - \\beta_1 \\sum_{m=1}^{M} f_m - \\beta_2 \\sum_{n=1}^{N} ln(1 + exp(T_n - \\rho_T)) - \\beta_3 \\sum_{m=1}^{M} o_m - \\beta_4 \\sum_{m=1}^{M} ln(1 + exp(T_m^l - \\rho_L))$   (1)\nwhere $r_0$ is a bias constant to keep the reward positive, M is the number of ACUs, and N is the\ntotal number of temperature sensors in the cold aisles. Positive coefficients $\\beta_1, \\beta_2, \\beta_3, \\beta_4$ are used\nto weight the respective terms in the reward function, balancing energy saving optimization and\ntemperature regulation within the cooling system."}, {"title": "3.2 T-SYMMETRY ENFORCED THERMAL DYNAMICS MODEL", "content": "To extract robust and generalizable representations conducive to sample-efficient offline policy\nlearning, we construct a special T-symmetry enforced thermal dynamics model (TTDM) to model and\nexplain the fundamental thermal dynamics patterns inside the server room. More specifically, we start\nby abstracting the cooling system into two coupled graph structures with corresponding adjacency\nmatrices as illustrated in Figure 2b. In this graph, each green node represents the ACU features $s_a$\nand a, corresponding to its working states and controllable actions (fan speeds and valve openings),\nwhile each orange node represents sensor measurements $s_s$. As nearby sensor readings often have\nstrong spatial correlation, while sensor readings themselves also have control dependencies on nearby\nACUs, hence we connect these two types of nodes with orange and green edges to reflect the spatial\nand control dependencies respectively based on domain knowledge.\nThe detailed encoder-decoder architecture of TTDM is shown in Figure 2c. We design a state-action\nencoder $\\phi(s, a)$ that contains a pair of GCN blocks (Kipf & Welling, 2017), to capture the spatial\ndependencies between sensor nodes (orange) and control dependencies across sensor nodes and ACU\nnodes (green). The external factors $s_e$ are integrated through a two-layer MLP to derive the final\nembedded representations. From the encoder $\\phi(s, a)$, we can obtain the latent representations of\nthe current state, action and next state $z_s, z_a, z_{s'}$ from data. To further enhance the reliability and\ngeneralizability of the learned representations, we introduce a pair of ODE latent forward dynamics\n$f(z_s, z_a) = \\dot{z}_s$ and reverse dynamics $g(z_{s'}, z_a) = -\\dot{z}_s$ to enforce the T-symmetry consistency\n($f(z_s, z_a) = -g(z_{s'}, z_a)$). To learn this model, we design the following loss terms:\nReconstruction loss. As mentioned above, the state-action encoder $\\phi(s, a) = (z_s, z_a)$ takes state-\naction pairs as input and outputs their corresponding latent representations. We then use a pair of\nstate and action decoders $\\Psi_s(z_s)$ and $\\Psi_a(z_a)$ to ensure that the learned representations can be mapped\nback to the original data space:\n$\\ell_{rec}(s, a) = ||s - \\Psi_s(z_s)||^2 + ||a - \\Psi_a(z_a)||^2$  (2)\nLatent ODE forward and reverse dynamics. We utilize the similar approach in Cheng et al.\n(2023) and Champion et al. (2019), embedding a discrete-time first-order ODE system to capture"}, {"title": "3.3 SAMPLE-EFFICIENT OFFLINE POLICY OPTIMIZATION", "content": "We construct a highly sample-efficient offline RL algorithm for energy-efficient DC cooling control\nby integrating the properties of the learned TTDM. The most notable benefit of leveraging TTDM\nin offline policy learning lies in the well-behaved compact data representations produced by its\nstate-action encoder $\\phi(s, a)$, which are both information-rich (capturing fundamental dynamics\ninformation) and robust (well-regularized and T-symmetry preserving). This can greatly enhance\noffline policy learning and generalization on OOD areas, crucial for the small-sample learning setting.\nConsequently, instead of learning the action-value function in the original data space as in typical RL\nalgorithms, we learn our action-value function within the latent space (i.e., $Q(z_s, z_a)$). This provides\nmore reliable value estimates even with limited offline data. Specifically, we update our Q-function\nusing the following objective with the safety-aware reward function defined in Eq. (1):\n$Q = \\underset{Q}{argmin} E_{(s, a, s') \\sim D} \\Big[ \\big( r(s, a) + \\gamma \\varphi(\\phi(s', \\pi(s')) - Q(\\phi(s, a)) \\big)^2 \\Big]$\nFor policy optimization, we adopt a similar treatment as in TD3+BC (Fujimoto & Gu, 2021), where\nwe maximize the value function Q but in the latent space, and constrain the policy output actions\ncloser to actions within the dataset. However, solely adding the regularization to offline behavioral\ndata is insufficient to ensure reasonable generalization performance. Hence we further regularize\nthe T-symmetry consistency of policy-induced samples $(s, \\pi(s))$ using the T-symmetry consistency\nloss $\\ell_{T-sym}$ as in Cheng et al. (2023). This enforces the policy to generate actions that are compliant\nwith T-symmetry, even in OOD areas, thereby greatly enhancing the generalization performance and\nsample efficiency of policy learning. The final policy optimization objective is presented as follows:\n$\\pi = \\underset{\\pi}{argmax} E_{(s, a) \\sim D} \\Big[ \\lambda Q(\\phi(s, \\pi(s))) - (\\pi(s) - a)^2 - \\ell_{T-sym}(\\phi(s, \\pi(s))) \\Big]$\n(9)\nwhere we follow TD3+BC and use $\\lambda_a = \\alpha / [\\sum_{s_i, a_i} |Q(\\phi(s, a))| / N]$ as the normalization term to\nbalance the strength of value maximization and policy regularization (N is the number of samples in\na training batch). We tuned the scale parameter a in the range of [2.5, 10] during our experiments."}, {"title": "4 REAL-WORLD EXPERIMENTS", "content": "To validate our proposed physics-informed offline RL framework, we develop a deployment-friendly\nsoftware system to support the close-loop control of ACUs using the learned policy. We successfully\ndeployed our system and conducted a series of experiments (from January to December 2024) in a\nlarge-scale commercial data center in China, controlling up to 4, 6, and all (10 or 11) ACUs in two\nof its server rooms (referred as Room A and B in the later content). Our method has been operated\neffectively and safely for over 2000 hours in total. As conducting experiments in a production\nenvironment suffers lots of restrictions, to further validate our method, we also built a real-world\nsmall-scale DC testbed to conduct more comprehensive comparative experiments and model ablations.\nThe testbed contains 22 servers and an ACU, and supports testing a wide range of server load settings.\nMore information about the two real-world DC testing environments and the collected historical\noperational datasets can be found in Appendix B. Throughout this study, we train and validate our\nmodel on real-world data and environments, with completely no simulation involved."}, {"title": "4.1 VALIDATION ON REAL-WORLD DATA CENTER", "content": "Comparison with conventional control. We first compare our DC cooling optimization method\nwith the default ACU PID controllers on two server rooms in the real-world commercial data center.\nAs our experiments are conducted in the real production environment, we are only allowed by the\nDC operator to control 4 out of 11 ACUs in the room in the early stages of the experiment, once\nthe effectiveness was validated, we proceeded with experiments controlling 6 ACUs and then all\nACUs in a server room. To ensure a fair comparison, we select several time periods (lengths from\n5.5 to 7.5 hours) that have similar server load patterns for comparison. Table 1 shows the results on\nenergy consumption metrics, including the average electric power and total energy consumption of\nservers and the ACU cooling system. We use the Air-side Cooling Load Factor (ACLF) to analyze\nthe floor-level cooling system's energy efficiency, which is widely adopted by the DC industry. It is\ncalculated as the ratio of the ACU system's energy consumption to the servers' energy consumption\nduring the test period. Lower ACLF indicates higher energy efficiency. In the tested two server rooms,\nour method improves the cooling system's energy efficiency by 14% to 21% compared to the default\nPID controllers. Throughout our experiment, we observed no thermal safety violations and regulated\nthe cold aisle temperature (CAT) well below the required operational threshold.\nControl quality. We also conducted consecutive 48-hour experiments to compare the control\nbehaviors of our method and the PID controllers in Server Room B with fluctuating server loads.\nThe results are presented in Figure 3, where we compare the same 4 controlled ACUs and the\ntemperature variation patterns of the directly impacted hot and cold aisles. As shown in Figure 3a,\nduring the periods controlled by the PID controllers and our method, the total server load fluctuated"}, {"title": "C ADDITIONAL RESULTS", "content": "C.1 PERFORMANCE UNDER DRASTIC SERVER LOAD FLUCTUATION\nTo further evaluate the adaptability and load-awareness of our method, we tested on a specific scenario\nwith drastic server load fluctuations in Server Room B. We compare the control strategy of two ACUS\nwith one controlled by the default PID controller and the other by our method. Experimental results\nare presented in Figure 11. The PID controller demonstrates limited adaptability in this scenario, with\nno adjustments to fan speeds and only marginal changes in valve opening percentage. In contrast, our\noffline RL approach was able to promptly adapt to external changes, resulting in a more optimal and\nenergy-efficient strategy. These results underscore the effectiveness and adaptability of our approach\nin highly dynamic DC service conditions.\nC.2 ADDITIONAL ABLATIONS ON REWARD FUNCTION DESIGN\nWe considered both the control parameters of the ACUs and environmental factors within the cooling\nsystem to design a reasonable reward function for RL policy learning. For the weight coefficient\n$\\beta_1, \\beta_2, \\beta_3, \\beta_4$ in the reward function Eq. (1), we set their values as the reciprocal of the mean of\nthe corresponding reward term calculated based on the preprocessed dataset. This ensures each\nreward term has a similar scale. For the first constant term $r_0$ in the reward function, to keep the\nreward positive, we calculate the sum of the other terms in the reward function for each record in the\npreprocessed dataset and take their maximum value plus 1 as the value of $r_0$.\nTo further investigate the robustness of our reward function design, we also conducted additional\nexperiments on the testbed by varying the relative scale of the third term ($\\beta_2 \\sum_{n=1}^{N} ln(1+\\exp(T_n - \\rho_T))$)\nin Eq. (1), which controls the strength of CAT violation penalty. Specifically, we test the\ndefault value of $\\beta_2$ as well as multiply it by 5 and 10, to test the impact of prioritizing more on safety\nconstraint satisfaction. We train three models with different $\\beta_2$ values and use the resulting models\nto control the ACU for 6 hours under low and high server load conditions on the testbed. In all\nthese experiments, the CAT was controlled below the predefined threshold. Moreover, as reported\nin Table 2, the energy-saving performances of the models under different $\\beta_2$ weight coefficients\nconsistently achieve comparable and low ACLF values. This shows our designed reward function\nis robust and does need much tuning to ensure good practical performance, which is particularly\ndesirable for real-world deployments."}, {"title": "C.3 ANALYSIS OF IMPACTS ON THE UPSTREAM WATER-SIDE COOLING SYSTEM", "content": "To evaluate the potential impact of optimizing the air-side cooling system using our method on the\nupstream water-side cooling system, we conducted additional analysis on the water-side related states\nthrough two before-and-after tests. We select two time periods with relatively stable server loads in\nthe two server rooms (November 10-11 for Server Room A and October 29-30 for Server Room B)\nto compare the chilled water pump frequency (CWP freq) and the ACUs' entering water temperature\n(EWT) before and after using our offline RL policy for control. The CWP freq. and EWT are key\nstates that reflect the working conditions of the water-side cooling system, which are external factors\nto the air-side cooling systems. Figure 12 shows the experimental results during the full control of\nall ACUs in Server Room A and Server Room B. The Figure 12a and 12b, the dashed vertical lines\nindicate the time points when our method took over the control. In both Server Room A and Server\nRoom B, before and after our method began controlling, the average EWT of the ACUs remained\nstable. Additionally, the chilled water pump frequency of the water-side cooling system also did not\nexhibit significant variations. However, comparing the results before and after adopting our control\nmethod, the ACLF values in both rooms significantly decreased. These results demonstrate that\nalthough our method effectively reduces the air-side cooling system's energy consumption, it does\nnot have a noticeable impact on the upstream water-side cooling system. Moreover, as also shown in\nSection 4.1, Figure 3 and 4, our offline RL policy enables much better temperature regulation and\nforms a more stable temperature field for the hot aisles, due to smartly coordinating the control of all\nACUs based on the dynamic temperature patterns in the server rooms. This effectively decreases the\noscillation in the conventional control approach, which often results in frequent overshoots during\ntemperature control and causes higher ACU energy consumption. This partly explains why our\nmethod can have lower energy consumption but achieve the same or better cooling effect."}, {"title": "D IMPLEMENTATION DETAILS", "content": "D.1 PRACTICAL IMPLEMENTATIONS OF OUR PROPOSED METHOD\nData preprocessing. We preprocessed the DC raw data to facilitate model training. Min-max\nnormalization was applied to both states and actions using the following formulas: $s = \\frac{(S-S_{min})}{(S_{max} - S_{min})}$\nand $a = \\frac{(a - a_{min})}{(a_{max} - a_{min})}$. $S_{max}$, $S_{min}$, $a_{max}$, $a_{min}$ are maximum and minimum normalization bound-\naries for state and action features. For actions, we set $a_{min} = 0$ and $a_{max} = 100$ as both fan speed\nand valve openings are percentage values. For the states, as described in Section 3.1, there exist\ndifferent types of sensor inputs: s = {Ss, Sa, Se}, and each type of sensor reading has distinct scales."}, {"title": "E REAL-WORLD DATA ANALYSIS", "content": "In the real-world data center, due to the use of PID group control for ACUs throughout the historical\noperation, and infrequent adjustments to the PID-related temperature setpoints, the action patterns of\nthe ACUs system (fan speed and water valve opening) are narrowly distributed. Additionally, the\ndistributions of other state features are mostly concentrated with a single peak. All these factors\npose significant challenges to offline RL policy learning, requiring models with strong generalization\ncapability to effectively learn and optimize control strategies. Figure 13 shows the historical dataset\ndistributions collected from our real-world testbed, in which we collect system operational data\nfrom more diverse server load and control settings, resulting in relatively broader state-action space\ncoverage. This actually makes the task more manageable for existing offline RL algorithms like CQL,\nIQL, and FISOR. However, as we have shown in Figure 6, our proposed method still outperforms the\nbaseline methods in the testbed experiments, and more importantly, achieves good performance in\nthe much more challenging production DC environment."}, {"title": "F LIMITATIONS AND FUTURE WORKS", "content": "In this study, we only tested in a single large-scale commercial DC facility and a small-scale real-\nworld testbed. For future works, we plan to further expand our experiments to multiple DC facilities\nwith different air-side cooling system configurations. Also, our approach models the safety constraints\nby incorporating them as penalty terms inside the RL reward function, which adds complexity to\nreward design and may not be sufficient to ensure safety under certain special conditions. Future\ninvestigations can be conducted to expand our method to a safe offline RL framework, with dedicated\nconsideration of constraint satisfaction, which would provide more safety guarantees in practice.\nFurthermore, it is also meaningful to explore the joint optimization of both cooling and server-side\nsystems, which can fully maximize the potential for DC energy saving."}, {"title": "G LEARNING CURVES", "content": "Figure 14 reports the learning curves of the proposed TTDM and offline policy learning method. As it\nis not possible to directly interact with the real DC environment and evaluate the policy's performance\nduring offline RL training, hence we report the Q-function learning loss and policy loss for different\ntraining steps. Both our proposed TTDM and the RL policy learning scheme enjoy stable model\nconvergence during training."}, {"title": "A SYSTEM DEPLOYMENT", "content": "We have developed a full-function software system to facilitate the deployment and validation of our\nproposed physics-informed offline RL framework. We successfully deployed our system in a large-scale commercial data center for production environment performance validation and the small-scale\nDC testbed for more comprehensive model evaluation and ablation. The overall deployed system\narchitecture is illustrated in Figure 8, which consists of two main phases: offline training and online\ndeployment. In the offline training phase, the historical operational data of the floor-level cooling\nsystems is exported from the DC log management system. The exported data undergoes automated\ndata processing and feature engineering processes and is stored in a historical dataset. Subsequently,\nbased on the processed offline dataset, we train the T-symmetry enforced thermal dynamics model,\nfollowed by a sample-efficient offline policy learning module to obtain the optimized floor-level\ncooling control policy. In the online deployment phase, the learned policy is deployed in a local\npolicy server within the data center to provide control services. Real-time data from the cooling\nsystems is retrieved by the management system API, processed, and stored in a real-time database.\nThe system then forwards the real-time data to the policy server, which outputs optimized ACU\ncontrol actions. These optimized control actions are directly written into the ACUs via the Modbus\nprotocol for closed-loop control.\nOur developed system is deployment-friendly and broadly applicable to various DC floor-level cooling\nsystems with different configurations, exhibiting great flexibility and transferability. Moreover,\nas environmental and server load conditions in the data center frequently change over time, the\ncompletely data-driven design of our system offers extra advantages. As it allows for re-collection of\nnew historical data every few months, and uses the new data to retrain and fine-tune the ACU control\npolicy accordingly. This endows our system with high adaptability, providing an evolvable control\noptimization solution to a slowly changing industrial system."}, {"title": "B REAL-WORLD TESTING ENVIRONMENTS AND EXPERIMENT SETUPS", "content": "B.1 PRODUCTION DATA CENTER ENVIRONMENT\nFigure 9 presents some photographs and the layout illustration of our real-world data center testing\nenvironment. In this large-scale commercial data center, we are granted permission to conduct\nexperiments in two designated server rooms. These server rooms host the real IT loads of a large\nvideo-sharing website in China. Specifically, in Server Room A, the average total server load is\naround 550 kW, with an overall ACU power consumption of around 25 kW; in Server Room B, the\naverage total server load is around 610 kW, and the overall ACU power consumption is about 37\nkW. In the early stages of the experiment, we are only allowed to control 4 ACUs in each server\nroom (ACU 1-6, 1-5, 2-5, and 2-4 on the left side in Server Room A; ACU 1-1, 1-2, 2-1, and 2-2 on\nthe right side in Server Room B). These ACUs are arranged in pairs on opposite sides of the room,\ndirectly influencing two cold aisles and one hot aisle. The remaining ACUs continue to operate under\nPID control. After verifying the effectiveness of the experiments, we further used the model to control"}, {"title": "B.2 REAL-WORLD TESTBED", "content": "To thoroughly assess the performance of our proposed method, we also constructed a real-world\ntestbed environment, which contains 22 servers and an inter-column air conditioner as the ACU\n(located between Rack 1 and Rack 2). This is a compressor-based ACU, which is smaller than the\ntypical ACUs in commercial data centers that use the cold water from chillers and cooling towers as\nthe cold source. Therefore the fans and the compressor inside the ACUs are the primary contributors\nto the ACU's energy consumption. For the testbed environment, temperature regulation is achieved\nby adjusting the entering air temperature (EAT) setpoint of the ACU to ensure the CAT remains\nbelow the predetermined threshold. We installed 6 sets of temperature and humidity sensors (24 in\ntotal) to monitor the internal temperature field inside our DC testbed environment. Moreover, we\nalso have access to the interior temperature sensor readings from each server, which provides even\nfiner-grained monitoring of the thermal dynamics inside the testbed. Figure 10 provides a detailed\ndepiction of the testbed environment configuration.\nTo support testing with a wide variety of server loads, we also developed a software framework to\nassign servers with different load patterns that mimic real-world IT tasks. The software employs a\nKubernetes (k8s) cluster architecture and is implemented under the CentOS Stream 9 operating system.\nThe ACU control is implemented through the Modbus protocol, which regulates the setpoint of the\nEntering Air Temperature (EAT) of the ACU, thereby indirectly adjusting the fan and compressor of\nthe ACU. In our experiments, the control policies calculate and output the EAT setpoint every two\nminutes and control the ACU accordingly. The experimental server loads in our testbed range from\napproximately"}]}