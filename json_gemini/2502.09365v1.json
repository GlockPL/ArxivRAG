{"title": "SIMPLE PATH STRUCTURAL ENCODING FOR GRAPH TRANSFORMERS", "authors": ["Louis Airale", "Antonio Longa", "Mattia Rigon", "Andrea Passerini", "Roberto Passerone"], "abstract": "Graph transformers extend global self-attention to graph-structured data, achieving notable success in graph\nlearning. Recently, random walk structural encoding (RWSE) has been found to further enhance their predictive\npower by encoding both structural and positional information into the edge representation. However, RWSE\ncannot always distinguish between edges that belong to different local graph patterns, which reduces its\nability to capture the full structural complexity of graphs. This work introduces Simple Path Structural\nEncoding (SPSE), a novel method that utilizes simple path counts for edge encoding. We show theoretically\nand experimentally that SPSE overcomes the limitations of RWSE, providing a richer representation of\ngraph structures, particularly for capturing local cyclic patterns. To make SPSE computationally tractable,\nwe propose an efficient approximate algorithm for simple path counting. SPSE demonstrates significant\nperformance improvements over RWSE on various benchmarks, including molecular and long-range graph\ndatasets, achieving statistically significant gains in discriminative tasks. These results pose SPSE as a powerful\nedge encoding alternative for enhancing the expressivity of graph transformers.", "sections": [{"title": "Introduction", "content": "Graphs are pervasive across diverse domains, representing com-\nplex relationships in areas such as social networks[Otte and\nRousseau, 2002], molecular structures[Quinn et al., 2017], and\ncitation graphs[Radicchi et al., 2011]. Recent advances in graph\nneural networks (GNNs) have driven significant progress in\nlearning from graph-structured data [Kipf and Welling, 2017,\nVeli\u010dkovi\u0107 et al., 2018, Bodnar et al., 2022, Lachi et al., 2024,\nFerrini et al., 2024, Duta and Li\u00f2, 2024], yet these models of-\nten face challenges in capturing long-range dependencies and\nstructural patterns due to their reliance on localized message\npassing [Alon and Yahav, 2021, Topping et al., 2022].\nInspired by their success in vision and sequence learning\ntasks [Vaswani, 2017, Dosovitskiy, 2020], transformers have\nbeen extended to graph learning problems [Yun et al., 2019,\nDwivedi and Bresson, 2020, Ying et al., 2021, Kreuzer et al.,\n2021]. Unlike traditional GNNs, graph transformers lever-\nage global self-attention, allowing each node to attend to all\nothers within a graph, regardless of distance. This flexibility\novercomes the limitations of message-passing approaches but\nintroduces new challenges, particularly in designing suitable\npositional and structural encodings that capture the inherent\nirregularities of graphs.\nFor directed acyclic graphs (DAGs), positional encodings (PEs)\nbased on partial orderings can be directly applied [Dong et al.,\n2022, Luo et al., 2024a, Hwang et al., 2024]. However, for\ngeneral undirected graphs, successful PEs often rely on eigen-\ndecompositions of the graph Laplacian, drawing inspiration\nfrom sinusoidal encodings in sequence transformers [Dwivedi\nand Bresson, 2020, Mialon et al., 2021]. These approaches en-\ncode node-level information but fail to capture the full structural\ncomplexity of edge patterns in node neighborhoods.\nTo address this limitation, several graph transformer architec-\ntures incorporate initial message-passing steps to encode local\nsubstructures [Wu et al., 2021, Mialon et al., 2021, Chen et al.,\n2022]. While effective, these methods focus solely on node\nrepresentations and do not exploit the potential of injecting\npairwise structural encodings directly into the self-attention\nmechanism. Recent studies have explored structural edge en-\ncodings in pure transformer architectures, based for instance\non Laplacian eigenvectors, heat kernels, or shortest path dis-\ntances [Kreuzer et al., 2021, Ying et al., 2021, Chen et al., 2023].\nDespite their utility, these encodings are limited in expressivity,\nparticularly for capturing local cyclic patterns or higher-order\nsubstructures.\nA promising alternative is random walk structural encoding\n(RWSE), which encodes richer structural information by con-\nsidering random walk probabilities as edge features. RWSE has\nshown substantial improvements in the performance of state-\nof-the-art graph transformers [Menegaux et al., 2023, Ma et al.,\n2023]. However, RWSE struggles to differentiate between\ndistinct graph structures in certain cases. Meanwhile, paral-\nlel research on message-passing GNNs has demonstrated the\nbenefits of simple paths (or self-avoiding walks) over random\nwalks in enhancing model expressivity [Michel et al., 2023,\nGraziani et al., 2023]. These findings motivate the exploration\nof simple paths as a structural encoding mechanism in graph\ntransformers.\nIn this work, we introduce Simple Path Structural Encod-\ning (SPSE), a novel method for structural edge encoding that\nreplaces RWSE in graph transformers. SPSE encodes graph\nstructure by counting simple paths of varying lengths between\nnode pairs, capturing richer structural information than random\nwalks. To address the computational challenges of simple path\ncounting, we propose an efficient algorithm based on succes-\nsive DAG decompositions using depth-first search (DFS) and\nbreadth-first search (BFS). This approach avoids the exponen-\ntial memory costs of path enumeration, enabling scalability to\nlong path lengths.\nWe validate SPSE on extensive benchmarks, including molecu-\nlar datasets from Benchmarking GNNs [Dwivedi et al., 2023],\nLong-Range Graph Benchmarks [Dwivedi et al., 2022], and\nLarge-Scale Graph Regression Benchmarks [Hu et al., 2021].\nSPSE consistently outperforms RWSE in graph-level and node-\nlevel tasks, demonstrating significant improvements in molecu-\nlar and long-range datasets. We also characterize limit cases of\nthe algorithm, and identify situations in which the performance\nmight be more sensitive to approximate path counts.\nThe remainder of this paper is structured as follows. Section 2\nintroduces key concepts and notations. Section 3 analyzes\nthe limitations of RWSE and motivates SPSE. The proposed\npath-counting algorithm and encoding method are detailed in\nSection 4. Finally, experimental results and related works are\ndiscussed in Sections 5 and 6, respectively."}, {"title": "2 Preliminaries", "content": "2.1 Graph Theory\nLet $G = (V,E,X)$ be a graph, where V is the set of nodes,\n\u03b5 \u2286 V \u00d7 V is the set of edges, and X \u2208 R|V|\u00d7d represents\nthe node features of dimension d. The adjacency matrix A \u2208\n{0,1}|V|\u00d7|V| is a square matrix that represents the connectivity\nof the graph, i.e. Aij is one if there is an edge between nodes\ni and j, and zero otherwise. The diagonal degree matrix D\u2208\nR|V|\u00d7|V| is a square matrix where the diagonal element Di,i\nrepresents the degree of node i. Formally, Di,i = \u2211j Ai,j and\nDi,j = 0 for i \u2260 j.\nDefinition 2.1 (Walk). Given a graph G, a walk is a finite\nsequence of nodes vo, U1,..., Um, where each consecutive pair\nof nodes (vi, Vi+1) is connected by an edge, i.e., (Vi, Vi+1) \u2208 E.\nThe number of edges in a walk is referred to as the walk length.\nDefinition 2.2 (Simple Path). A simple path (here indifferently\ncalled path), is a walk in which all nodes are distinct. The\nnumber of edges in a simple path is called the simple path\nlength (or path length). Paths themselves constitute graphs\ncalled path graphs. The distance between two nodes is the\nlength of the shortest path between these two nodes."}, {"title": "2.2 RWSE encoding in Graph Transformers", "content": "Pure graph transformers, which do not use any MPNN layer,\ntypically encode structural and positional information directly\nin the self-attention layer. To compute the RWSE matrix\nErw, all k-hop random walk matrices up to a maximum walk\nlength K are concatenated into a matrix P = [P1,\u2026, Pk] \u2208\n[0,1]|V|\u00d7|V|\u00d7K, which is then encoded through a shallow neu-\nral network \u03a60 that maps K to a (usually larger) dimension\nd [Menegaux et al., 2023, Ma et al., 2023]: ERW = \u03a60(P) \u2208\nR|V|\u00d7|V|\u00d7d. Erw both acts as a relative positional encoding,\nsince it contains the shortest path distance between nodes, and\nas a structural edge encoding, since walks encode information\nabout the visited sub-structures. As a standard positional encod-\ning, it biases the pair-wise alignment between nodes given by\nthe attention matrix. Very generally, the resulting self-attention\nlayer can be written as follows, where the precise implementa-\ntion of $1 and 2 varies among different methods:\naij = $1(WQxi, WKxj, (Erw)ij)  (1)\nAij = \\frac{exp(a_{ij})}{\\sum_{k}exp(a_{ik})} (2)\nYi = \u03a3\u03b1ij$2(WVxj, (Erw)ij)  (3)\nj\nwith xi and x; the features of nodes i and j, WQ, WK and\nWV the query, key and value matrices, and y\u2081 the output from\nthe self-attention layer for node i."}, {"title": "3 Theoretical Properties of RWSE and SPSE", "content": "In this section, we highlight the limitations of RWSE in distin-\nguishing different graph structures, as it can assign identical\ntransition probabilities to edges in distinct topologies, poten-\ntially leading to suboptimal performance. Conversely, we show\nthat SPSE naturally captures cycle-related information. In par-\nticular, SPSE enables cycle counting, which is crucial in appli-\ncations such as molecular chemistry (e.g., identifying functional\ngroups like aromatic rings) [May and Steinbeck, 2014, Ag\u00fandez\net al., 2023], social network analysis (e.g., detecting commu-\nnities) [Radicchi et al., 2004, Dhilber and Bhavani, 2020], and\ncircuit design (e.g., analyzing feedback loops) [Horowitz et al.,\n1989]"}, {"title": "3.1 Limitations of RWSE for Edge Encoding", "content": "Random walk probabilities possess the great advantage of being\ncomputable in closed form. However, their use introduces cer-\ntain ambiguities, as illustrated in Figure 1. In these two exam-\nples, Ga and Gc are cycle graphs, GB and GD are path graphs,\nand yet the following two relations hold (see Appendix C for a\ngraphical proof up to a depth of 5):\n(0,1)9A =Rw (0,1)9\u0432,\n(0,1)9C =Rw (0,1)GD.\nAlthough the edges belong to clearly different graphs, RWSE\nassigns them the same transition probabilities in the cycle and\npath graphs. These examples are in reality two special cases\nof the following result which links RWSE edge encodings in\neven-length cycle graphs and linear graphs (all proofs can be\nfound in Appendix B):\nProposition 1. Let G = (V,E) be an even-length cycle graph,\ni.e. |V| = 2n for some n \u2208 N*, and G' = (V',E') a path\ngraph such that |V'| = 2n + 1. Then given any pair of nodes\n(i, j) in G, there exits a pair of nodes (i', j') in G' such that\n(i, j) =rw (i', j')''.\nEach pair of nodes in an even-length cycle graph is thus equiva-\nlent, under RWSE encoding, to another pair of nodes in a linear\ngraph. In other words, random walks transition probabilities\ncannot be used to distinguish between even-length cycles and\npaths when considering single node pairs. On the other end,\nit is easy to show that this does not apply to SPSE encoding"}, {"title": "3.2 SPSE through the Prism of Cycle Counting", "content": "The relation between path and cycle counting has been well\nstudied [Perepechko and Voropaev, 2009, Graziani et al., 2023].\nThe following result, which was introduced by Perepechko and\nVoropaev [2009], connects SPSE encoding with cycle counting\nfor adjacent nodes:\nProposition 3. Let (i, j) be two adjacent nodes in a graph\nG = (V,E), i.e. (i,j) \u2208 E, and Sk the k-hop simple path\nmatrix of G for any k \u2208 N*, such that (Sk)ij = mk \u2208 N. Then\nfor k\u2265 2, there are exactly mk cycles of length k + 1 in G that\nadmit (i, j) as an edge.\nThe case of k = 1 simply corresponds to the number of parallel\nedges between nodes i and j. Note that this is not the same"}, {"title": "4 Simple Path Structural Encoding", "content": "The superior expressivity of paths over walks in\nMPNNS [Michel et al., 2023, Graziani et al., 2023] has\nbeen studied, where message-passing follows short paths stored\nin memory. However, due to the combinatorial complexity\nof enumerating all paths, storing these sequences becomes\nimpractical. Results from Section 3 indicate that counting\ndistinct paths between nodes, while requiring significantly less\nmemory, still possesses theoretical advantages over random\nwalk probabilities as an edge structural encoding method.\nThis approach, however, introduces two challenges. First,\nwhile existing path-counting algorithms efficiently handle\nshort paths [Perepechko and Voropaev, 2009, Giscard et al.,\n2019], the graph topologies and path lengths considered here\n(15-23) necessitate approximate methods. Second, since the\nnumber of paths between two nodes can grow exponentially\n(bounded by \\frac{(|V|-2)!}{(k-1)!} for length-k paths in a complete\ngraph), an appropriate encoding function is required. This\nsection addresses both challenges."}, {"title": "4.1 Simple Path Counting", "content": "The path counting method followed here relies on the extraction\nof node orderings from an input undirected graph. Any such or-"}, {"title": "4.2 Path Count Encoding", "content": "Compositions of logarithm functions are used to map the ob-\ntained path count matrix S = [S1,\uff65\uff65\uff65, Sk] to a manageable\nvalue range for subsequent neural networks, as total counts can\ngrow very large. Using superscript to denote the composition\nof a function with itself, we use the following mapping f for a\ntotal count x:\nf:xagn(x) + \u03b2, (4)\nwith g :x ln(1+x), and a, \u1e9e and n being hyperparame-\nters to be adjusted for different graph collections. Normalized\npath counts f (S) can then be used in place of the random walk\nmatrix P as input to the edge encoding network of graph trans-\nformer models, yielding the SPSE matrix ESP which replaces\nERW in equations 1 and 3."}, {"title": "5 Experiments", "content": "We first validate Proposition 3 experimentally through a syn-\nthetic experiment (Section 5.1), and demonstrate the empirical\nsuperiority of SPSE on real-world datasets (Section 5.2). We\nthen present an ablation study on the algorithm parameters\n(Section 5.3) and discuss limitations (Section 5.4)."}, {"title": "5.1 Cycle Counting Synthetic Experiment", "content": "Synthetic Dataset. To validate Proposition 3, we design a\nsynthetic dataset consisting of 12,000 graphs. Each graph is\ngenerated by randomly adding cycles of lengths between 3 and\n8 until the total count for each cycle length reaches a value\nbetween 0 and 14. This process results in graphs with an\naverage of 149 nodes and 190 edges. The dataset is split into\ntraining (10,000 graphs), validation (1,000 graphs), and test\n(1,000 graphs) sets. The objective is to determine the number\nof cycles for each of the six cycle lengths. We frame this as\nsix simultaneous multiclass classification tasks and evaluate\nperformance using mean accuracy. Examples of the generated\ngraphs are provided in Appendix E.\nModels. We build upon two state-of-the-art graph transformer\nmodels that use RWSE as an edge encoding method: GRIT [Ma\net al., 2023] and CSA [Menegaux et al., 2023]. SPSE can\nseamlessly replace RWSE in these models by substituting the\nencoding matrix Esp, which captures path counts, for Erw\nin equations 1 and 3. To evaluate performance on the cycle\ncounting task, we train these models using three hyperparameter\nconfigurations adopted from [Menegaux et al., 2023]. These\ncorrespond to the setups used for ZINC (config #1), PATTERN\n(config #2), and CIFAR10 (config #3), covering a range of\nmodel complexities from 40 to 280 gigaflops. This provides\na comprehensive assessment of the impact of the two edge\nencoding methods across diverse settings.\nResults. The test accuracy for both model architectures across\nthe three training configurations is reported in Figure 4, along\nwith standard deviations computed over 10 runs with different\nrandom seeds. In all but one case, SPSE encoding achieves\nsignificantly higher cycle counting accuracy than RWSE. All\nmodels learn to count cycles almost perfectly under the third\ntraining configuration, which suggests that deep architectures\ncan compensate for expressivity limitations in the edge en-\ncoding matrix (see Appendix E for details on configurations).\nHowever, SPSE still performs significantly better than RWSE\nfor CSA in this setting.\nThese results empirically validate the superior ability of simple\npaths to characterize cycles when used as an edge encoding\nmethod in graph transformers."}, {"title": "5.2 Real-World Benchmarks", "content": "Datasets. We conduct experiments on graph datasets from\nthree distinct benchmarks, covering both node- and graph-level\ntasks. These include ZINC, CLUSTER, PATTERN, MNIST,\nand CIFAR10 from Benchmarking GNNs [Dwivedi et al.,\n2023], Peptides-functional and Peptides-structural from the\nLong-Range Graph Benchmark [Dwivedi et al., 2022], and\nthe 3.7M-sample PCQM4Mv2 dataset from the Large-scale\nGraph Regression Benchmark [Hu et al., 2021]. We shall see\nin Section 5.3 how constraints imposed by graph complexity\nand dataset sizes impact the path count precision in molecular\ndatasets (ZINC, the two Peptides & PCQM4Mv2), image su-\nperpixel (MNIST & CIFAR10) and Stochastic Block Model\n(SBM) (PATTERN & CLUSTER) benchmarks, justifying a\ndifferenciated treatment.\nExperimental Setup. As before we replace RWSE with\nSPSE in GRIT and CSA. We also explore adding SPSE to\nthe well-known GraphGPS model [Ramp\u00e1\u0161ek et al., 2022], in\nwhich case the edge encoding is restricted to the elements of\nE, and is only used to produce node-level positional encodings\nin the MPNN layer. In all cases, for better result robustness,\nwe retrain the original model and the SPSE version on ten\nrandom seeds (one seed for the large PCQM4Mv2) using the\nreleased training configurations, with two exceptions: the un-\nstable CSA learning rate for ZINC is reduced by a factor 2, and\nconfiguration files are added to train CSA on Peptides.\nIt is important to emphasize that no hyperparameter tuning is\nperformed. This decision ensures a fair and unbiased compari-\nson, isolating the contribution of SPSE as a drop-in replacement\nfor RWSE, and demonstrating its effectiveness across different\narchitectures without the need for task-specific adjustments.\nNote that replacing walks by path count is done at no addi-\ntional cost as the number of trainable parameters remains un-\nchanged. Two-sided Student's t-tests are conducted to assess\nthe significance of the obtained results. Finally, we compare\nwith the following GNN methods: GCN [Kipf and Welling,\n2016], GIN [Xu et al., 2019], GAT [Velickovic et al., 2017],\nGatedGCN [Bresson and Laurent, 2017] and GIN-AK+ [Zhao\net al., 2021], along with the graph transformers: SAN [Kreuzer\net al., 2021], Graphormer [Ying et al., 2021], EGT [Hussain\net al., 2022], GraphViT [He et al., 2023], Exphormer [Shirzad\net al., 2023] and Drew [Gutteridge et al., 2023]. Results are\nreported in Table 1.\nResults and Discussion Replacing RWSE with SPSE im-\nproves performance in 21 out of 24 cases (underlined in Ta-\nble 1), with variations depending on the benchmark and model.\nThe most notable gains are observed for CSA and GRIT on\nmolecular graphs, where SPSE achieves statistically significant\nimprovements in 5 out of 6 cases (marked with \"*\"). Im-\nprovements are also evident on superpixel benchmarks, with\ntwo cases showing statistically significant differences, on both\nCSA and GRIT. These results indicate that leveraging the full\nstructural encoding matrix in self-attention layers, rather than\nrestricting it to & as done in GPS, is an effective strategy for\nenhancing performance. The limited improvement for GPS is\nhowever also likely due to the constraint of leaving the total\nnumber of parameters unchanged. In practice, this amounts to\nreducing the dimensionality of existing edge embeddings when\nusing SPSE, limiting its potential benefit. This explanation is"}, {"title": "5.3 Path Count Sensitivity to Hyperparameters", "content": "In this section, we examine how the number of paths discov-\nered by Algorithm 1 is affected by variations in hyperparam-\neters across different benchmarks. Those parameters are the\nproportion of root nodes R, the maximum DFS depth DDFS\nand the number of trials N (path length K is fixed to walk\nlengths). ZINC, PATTERN, and MNIST are used as represen-\ntative benchmarks for molecular graphs, SBM, and superpixels,\nrespectively. We measure the average proportion of discovered\npaths relative to a canonical configuration (reported in Ap-\npendix A) and the computation time per sample while varying\na single hyperparameter. Results are reported in Figure 5, with\npath count proportions as solid lines (left y-axis) and compu-\ntation time as dashed lines (right y-axis). The main parameter\ncontrolling the path count precision for ZINC and other molec-\nular datasets is the root node proportion R which can be set\nto its maximum value (R = 1) due to its low computational\ncost. In contrast, DFS depth and the number of trials have little\nto no impact on path counts and can be kept at moderate or\nlow values. MNIST benefits almost equally from increases\nin all three hyperparameters, though adjusting DDFS offers the\nlowest computational overhead. Notably, the trade-off setting\nof N = 2, imposed by the large size of the dataset, results\nin approximately 40% fewer discovered paths compared to\nN = 8, potentially leading to suboptimal learning accuracy.\nFor the PATTERN dataset, we prioritize increasing N over\nother hyperparameters, as it provides the most computationally\nefficient way to improve path discovery. In this case, however,\nwe expect a non-negligible fraction of paths to be missed due\nto the highly connected nature of SBM graphs, which sets a\nlimit on the number of discoverable paths (see Section 5.4 for\nfurther discussion)."}, {"title": "5.4 Limitations", "content": "In the proposed algorithm, the path count between two nodes is\nupdated by comparing the value provided by each new DAG\nwith the total currently stored in memory. The latter is then\nreplaced by the maximum of the two values. This approach\nis necessary since paths are not enumerated because of mem-\nory constraints. The obtained counts therefore constitute lower\nbounds on the exact number of paths (see line 16 of Algorithm 3\nin Appendix D). Failure cases can however arise because indi-\nvidual paths are not distinguished. For instance, consider the\ninput graph illustrated in Figure 6. The total number of paths\nof length 4 between nodes 0 and 1 is two (blue and green). The\nonly directed graphs that would allow to discover these paths\nsimultaneously are not acyclic: our algorithm can simply count\nthem one at a time.\nThis limitation is especially true in high-density graphs: in fact\nour method does not yield any statistically significant improve-\nment on the dense CLUSTER dataset."}, {"title": "6 Related Work", "content": "Graph Transformers. Graph transformers are a class of mod-\nels designed to process graph-structured data by leveraging self-\nattention, which provides a graph-wide receptive field [Vaswani,\n2017, Ying et al., 2021], thereby avoiding common limitations\nof message-passing GNNs [Topping et al., 2022]. Despite their\nadvantages, graph transformers have not yet become the dom-\ninant architecture in graph learning, primarily due to the lack"}, {"title": "7 Conclusion", "content": "This work introduced Simple Path Structural Encoding (SPSE),\na novel structural encoding method for graph transformers that\nleverages path counts instead of random walk probabilities. By\nproviding a more expressive and structurally informative edge\nencoding, SPSE improves performance across various graph\nlearning benchmarks. Our theoretical and experimental study\nshows that SPSE mitigates the limitations of random walk-\nbased encodings while maintaining computational feasibility\nthrough an efficient approximation algorithm. While promis-\ning, SPSE's applicability to extremely large-scale graphs and\nits interaction with different transformer architectures would\nrequire further exploration. Future research directions may also\ninclude optimizing its computational efficiency and extending\nits applicability to broader domains such as knowledge graphs\nand large social networks."}, {"title": "Impact Statement", "content": "This paper presents Simple Path Structural Encoding (SPSE), a\nnovel structural encoding method for graph transformers, aimed\nat enhancing the expressivity of self-attention mechanisms by\ncapturing richer structural patterns than existing random walk-\nbased encodings. By improving edge encodings with simple\npath counts, SPSE contributes to the broader field of graph\nrepresentation learning, offering a more robust method for tasks\ninvolving molecular graphs, social networks, and long-range\ndependencies.\nThe societal impact of this work aligns with the broader ad-\nvancements in Machine Learning and Graph Neural Networks,\nwith potential applications in drug discovery, recommendation\nsystems, and scientific knowledge extraction. However, as with\nany machine learning model, the deployment of SPSE-based\narchitectures should be approached with considerations for fair-\nness, interpretability, and robustness, particularly in high-stakes\napplications.\nWe do not foresee any immediate ethical concerns or risks asso-\nciated with our contributions beyond those generally applicable\nto graph machine learning. Future work should consider po-\ntential biases in training data and energy efficiency of graph\ntransformers, as large-scale models can have computationally\nintensive requirements."}]}