{"title": "Class-RAG: Content Moderation with Retrieval Augmented Generation", "authors": ["Jianfa Chen", "Emily Shen", "Trupti Bavalatti", "Xiaowen Lin", "Yongkai Wang", "Shuming Hu", "Harihar Subramanyam", "Ksheeraj Sai Vepuri", "Ming Jiang", "Ji Qi", "Li Chen", "Nan Jiang", "Ankit Jain"], "abstract": "Robust content moderation classifiers are essential for the safety of Generative AI systems. Content moderation, or safety classification, is notoriously ambiguous: differences between safe and unsafe inputs are often extremely subtle, making it difficult for classifiers (and indeed, even humans) to properly distinguish violating vs. benign samples without further context or explanation. Furthermore, as these technologies are deployed across various applications and audiences, scaling risk discovery and mitigation through continuous model fine-tuning becomes increasingly challenging and costly. To address these challenges, we propose a Classification approach employing Retrieval-Augmented Generation (Class-RAG). Class-RAG extends the capability of its base LLM through access to a retrieval library which can be dynamically updated to enable semantic hotfixing for immediate, flexible risk mitigation. Compared to traditional fine-tuned models, Class-RAG demonstrates flexibility and transparency in decision-making. As evidenced by empirical studies, Class-RAG outperforms on classification and is more robust against adversarial attack. Besides, our findings suggest that Class-RAG performance scales with retrieval library size, indicating that increasing the library size is a viable and low-cost approach to improve content moderation.", "sections": [{"title": "Introduction", "content": "Recent advances in Generative AI technology have enabled new generations of product applications, such as text generation OpenAI (2023); Anthropic (2023); Dubey (2024), text-to-image generation Ramesh et al. (2021); Dai et al. (2023); Rombach et al. (2022), and text-to-video generation Meta (2024). Consequently, the pace of model development must be matched by the development of safety systems which are properly equipped to mitigate novel harms, ensuring the system's overall integrity. This is important to prevent the use of Generative AI products from being exploited by bad actors to disseminate misinformation, glorify violence, and proliferate sexual content Foundation (2023).\nTraditional model fine-tuning approaches are often employed to this end, and classifiers learning patterns from labeled content moderation text data are leveraged as guardrails for these deployed systems OpenAI (2023). However, there are many challenges associated with automating the content moderation task with a traditional model fine-tuning approach. First, content moderation is a highly subjective task, meaning that inter-annotator agreement in labeled data is low, due to, potentially, many different interpretations of policy guidelines, especially on borderline cases Markov et al. (2023). Second, it is impossible to enforce a universal taxonomy of harm, not only due to the subjectivity of the task, but due to the impact of systems scaling to new locales, new audiences, and new use cases, with different guidelines and different gradients of harm defined on those guidelines Shen et al. (2024). Training a robust content moderation classifier is already challenging due to the subjective nature of the task and the limitations of structured labeled data in capturing this subjectivity. Additional challenges arise when classifiers must be continuously fine-tuned to adapt to an evolving landscape of safety risks.\nTo address these challenges of subjectivity and inflexibility as a result of scale, we propose a Classification approach to content moderation which employs Retrieval-Augmented Generation (Class-RAG) to add context to elicit reasoning for content classification. While RAG Lewis et al. (2020) is often used for knowledge-"}, {"title": "Related Work", "content": "Content moderation Much work has been done in the last decade to mitigate the dissemination of undesired content in the wake of innovations in communication technologies. Machine learning approaches have been proposed to address sentiment classification Yu et al. (2017), harassment Yin et al. (2009), hate speech detection Gamb\u00e4ck and Sikdar (2017), abusive language Nobata et al. (2016), and toxicity Adams et al. (2017). General improvements in deep learning have also accelerated the field of content moderation. WPIE, or Whole Post Integrity Embeddings, is built with BERT and XLM on top of advances in self-supervision, and obtains a holistic understanding of a post through a pretrained universal representation of content Schroepfer (2019). Even more recent advances in Generative AI have also spurred the question of whether or not LLMs could potentially be used as content moderators Huang (2024).\nGenerative AI safety With the advent of Generative AI also comes a proliferation of harm types beyond hate speech or toxicity detection whose mitigations and benchmarks warrant further research and exploration. A comprehensive AI harm taxonomy encompasses such harm categories like academic dishonesty, unauthorized privacy violations, and non-consensual nudity Zeng et al. (2024). As the approaches to mitigating Generative AI safety proliferate, so too do benchmarks which establish baselines for the efficacy of existing classifiers, such as UnsafeBench Qu et al. (2024). Benchmarks such as I2P Schramowski et al. (2023a), which includes approximately 4.5k real-world examples of English text-to-image prompts, and P4D Chin et al. (2024), which employs red-teaming strategies for automated detection of unsafe outputs, have provided valuable datasets to evaluate harmful categories like self-harm, illegal activity, sexual content, and graphic violence.\nRAG and its applications While the base capabilities of LLMs OpenAI (2023); Anthropic (2023); Dubey (2024) are powerful, LLMs have a known tendency to hallucinate Huang et al. (2023), a lack of ability to provide interpretable explanations for their generations, and are constrained by the cutoff date of their training data. Retrieval Augmented Generation (RAG) Lewis et al. (2020) mitigates some of these problems by augmenting the base capabilities of large pre-trained language models with a retrieval mechanism to explicit non-parametric memory Zhao et al. (2024). RAG has often been used to extend the capabilities of LLMs, particularly for knowledge-intensive tasks Gao et al. (2024). The flexibility of RAG-based approaches allows for applications that do not require additional in-domain finetuning, such as adapting detector models in the field of computer vision Jian et al. (2024). For example, RAFT improves the model's ability to answer questions in open-book in-domain settings Zhang et al. (2024)."}, {"title": "System Architecture", "content": "Class-RAG is a quadripartite system consisting of an embedding model, a retrieval library, a retrieval module, and a fine-tuned LLM classifier. When a user inputs a prompt, an embedding is computed on the prompt via the embedding model. The embedding of the user input is compared against an index of embeddings which are mapped to positive and negative examples of prompts in the retrieval library. Using Faiss, a library for efficient similarity search Douze et al. (2024), k nearest reference examples are retrieved against the embedding of the user input prompt, and the reference examples and input prompt are then sent to the fine-tuned LLM for classification. We leverage the CoPro dataset Liu et al. (2024) to train and evaluate our model."}, {"title": "Embedding Model", "content": "We leverage the DRAGON ROBERTa Lin et al. (2023) context model as our primary embedding model. DRAGON is a bi-encoder dense retrieval model that utilizes a dual-encoder architecture to embed both queries and documents into dense vector representations, facilitating efficient retrieval of relevant information. In this study, we specifically employ the context encoder component of the DRAGON model. To investigate the impact of alternative embedding models on our approach, we also evaluate a variant of WPIE (Whole Post Integrity Embedding) Meta (2021). The WPIE model we test is a 4-layer XLM-R Conneau et al. (2020) model that has been pre-trained on content moderation data, yielding two distinct outputs: an unsafe probability estimation and a prompt embedding representation."}, {"title": "Retrieval Library", "content": "Our retrieval library is comprised of two distinct sub-libraries: a safe library and an unsafe library. Each entry in the retrieval library is represented by a quadruplet of attributes, including: (1) prompt, (2) label, (3) embedding, and (4) explanation. The construction of the retrieval library is described in detail in the Data Preparation section."}, {"title": "Retrieval Module", "content": "Given the selected embedding, we leverage the Faiss library for similarity search Douze et al. (2024) to efficiently retrieve the two nearest safe examples and the two nearest unsafe examples from the retrieval library. Specifically, we utilize the L2 distance metric to compute the similarity between the input embedding and the embeddings stored in the retrieval library, allowing us to identify the most relevant examples."}, {"title": "LLM Classifier", "content": "Inspired by LlamaGuard Inan et al. (2023), the classifier is fine-tuned on top of the OSS Llama-3-8b checkpoint Dubey (2024)."}, {"title": "Data Preparation", "content": "We leverage the CoPro dataset Liu et al. (2024) to train and evaluate our model. The CoPro dataset consists of an in-distribution (ID) test set and an out-of-distribution (OOD) test set, both generated by a large language model. The OOD test set is generated with unseen seed inputs, providing a more challenging evaluation scenario. In addition to CoPro, we use the Unsafe Diffusion (UD) Qu et al. (2023) and I2P++ Liu et al. (2024) datasets to evaluate our model's generalizability capabilities. I2P Schramowski et al. (2023b) consists of unsafe prompts only, which we combine with captions in the COCO 2017 validation set Lin et al. (2015) (assuming all captions are safe) to create the I2P++ dataset. We split I2P++ and UD into validation and test sets with a ratio of 30/70. The sizes of the source datasets are summarized in Table 1."}, {"title": "Robustness Test Set Construction", "content": "To assess our model's robustness against adversarial attacks, we augment all test sets with 8 common obfuscated techniques using the Augly library Papakipos and Bitton (2022). These techniques include:\n\u2022 change_case: Hello world \u21d2 HELLO WORLD\n\u2022 insert_punctuation_chars: Hello world \u21d2 He'll'o 'wo'rl'd\n\u2022 insert_text: Hello world \u21d2 PK Hello world\n\u2022 insert_whitespace_chars: Hello world Hello world\n\u2022 merge_words: Hello world \u2192 Helloworld\n\u2022 replace_similar_chars: Hello world \u21d2 Hell[] world\n\u2022 simulate_typos: Hello world Hello worls\n\u2022 split_words: Hello world Hello worl d"}, {"title": "Retrieval Library Construction", "content": "In-Distribution Library Construction We constructed the in-distribution (ID) library by leveraging the CoPro training set, where each prompt is associated with a specific concept. The ID library comprises two distinct sub-libraries: one for safe examples and one for unsafe examples. To populate the safe library, we employed K-Means clustering to group safe examples into 7 clusters per concept, and selected the centroid examples from each cluster for inclusion in the safe sub-library. We applied the same clustering approach to collect unsafe examples. This process yielded a total of 3,484 safe examples and 3,566 unsafe examples, which collectively form the in-distribution retrieval library. To further enhance the library's utility for model reasoning, we utilized the Llama3-70b model Dubey (2024). to generate explanatory text for each example (Figure 2). Each entry in the retrieval library is represented by a quadruplet of attributes: prompt, label,"}, {"title": "Training Data Construction", "content": "Our training data construction process involves three key steps, which are applied to each input prompt in the CoPro training set. First, we retrieve reference examples from the in-distribution retrieval library using the Faiss index Douze et al. (2024). Specifically, we retrieve 4 reference examples for each input prompt, including 2 nearest safe reference examples and 2 nearest unsafe reference examples. Next, we generate a reasoning process for each input prompt using the Llama-3-70b model Dubey (2024). This process takes into account the input prompt, label, and 4 reference examples (2 safe and 2 unsafe), and aims to provide a clear reasoning process for the model to learn (Figure 3). Finally, we enrich the input text by incorporating a specific format of instructions, including the retrieved reference examples and the generated reasoning process. This enriched prompt is then used as input for our model training (Figure 4).\nWe construct the training data for LLAMA3, the Llama-3-8b baseline model following the methodology outlined in the Llama Guard paper Inan et al. (2023). A detailed example of this process can be found in Figure 6. In this paper, we focus on illustrating the construction of Class-RAG training and evaluation data."}, {"title": "Evaluation Data Construction", "content": "We construct the evaluation data using the same approach as the training data, with two key exceptions. Firstly, the retrieval library used for evaluation may differ from the one used for training. Secondly, the response and reasoning content are excluded from the evaluation data (Figure 5). This allows us to assess the model's performance in a more realistic setting, while also evaluating its ability to generalize to new, unseen data."}, {"title": "Experiments", "content": "We conducted a comprehensive experimental evaluation to assess the performance of our proposed model. To provide a thorough comparison, we selected two baseline models: WPIE (a 4-layer XLM-R) and LLA\u041c\u0410\u0417 (Llama-3-8b), with the latter configured according to the settings outlined in Llama Guard Inan et al. (2023). Our experimental evaluation consisted of seven distinct components, which are detailed in the following sections.\nThe experimental setup is described in Section 5.1. We then present the results of our evaluation, which examined six key aspects of our model's performance: (1) classification performance and robustness to adversarial attacks (Section 5.2); (2) adaptability to external data sources (Section 5.3); (3) ability to follow instructions (Section 5.4); (4) scalability of performance with respect to retrieval library size (Section 5.5); (5) impact of reference example numbers on performance (Section 5.6); and (6) effect of different embedding models on performance (Section 5.7)."}, {"title": "Experimental Setup", "content": "For training and evaluation, we enrich the input text with additional information by adding system instruction and reference prompts to both training and evaluation data. For training data specifically, we also include the reasoning process to enable our model to learn from the context and explanations provided.\nTraining Configuration We developed both LLAMA3 and Class-RAG models on top of the Llama-3-8b model Dubey (2024). The training setup for both models was identical, with the following hyperparameters: training on a single machine equipped with 8xA100 80GB GPUs, batch size of 1, model parallelism of 1, and a learning rate of 2 \u00d7 10-6. We trained both models for a single epoch with less than 3.5 GPU hours.\nModified Chain-of-Thought During training, our models learned to assess the input text by leveraging retrieved reference examples. We employed a modified Chain-of-Thought (CoT) Wei et al. (2023) approach. CoT has been shown to improve the response quality of large language models. In contrast to the typical CoT setup, where answers are derived by the reasoning process, we opted to place the answer before the reasoning process to minimize latency. Specifically, we enforced the first token to be the answer, followed by a citation and a reasoning section (Figure 4). The citation indicates which reference examples were used to inform the assessment, while the reasoning section provides an explanation for the induced assessment. At evaluation time, we only output a single token and use the probability of the \"unsafe\" token as the unsafe probability.\nEvaluation Metrics We adopted the area under the precision-recall curve (AUPRC) as our primary evaluation metric for all experiments. We chose AUPRC because it focuses on the performance of the positive class, making it more suitable for imbalanced datasets."}, {"title": "Classification and Robustness", "content": "We conducted a comprehensive evaluation of Class-RAG, comparing its performance to two baseline models, WPIE and LLAMA3, on the CoPro in-distribution (ID) test set and out-of-distribution (OOD) test set. To assess the robustness of our model against adversarial attacks, we augmented the test sets with 8 common obfuscation techniques using the Augly library Papakipos and Bitton (2022). The results, presented in Table 3, demonstrate that Class-RAG outperforms both baseline models. Notably, both LLAMA3 and Class-RAG achieved an AUPRC score of 1 on the in-distribution and out-of-distribution test sets, indicating excellent classification performance. However, Class-RAG (DRAGON ROBERTa) exhibits superior robustness to LLAMA3 against adversarial attacks, highlighting its ability to maintain performance in the presence of obfuscated inputs."}, {"title": "Adaptability to External Data", "content": "One of the key benefits of incorporating Retrieval-Augmented Generation (RAG) into Class-RAG is its ability to adapt to external data without requiring model retraining. To facilitate this adaptability, new reference examples are added to the retrieval library, allowing the model to leverage external knowledge. We evaluated the adaptability of Class-RAG on two external datasets, I2P++ and UD, using the retrieval libraries constructed as described in the Data Preparation section. Specifically, we utilized the in-distribution"}, {"title": "Instruction Following Ability", "content": "The instruction following ability of a large language model (LLM) refers to its capacity to comprehend and accurately respond to given instructions. In this section, we investigate the ability of Class-RAG to follow the guidance from reference examples and generate responses consistent with these examples. To evaluate this, we utilized the ID test set with a flipped ID library, which contains the same examples as the original ID library but with flipped labels (\"unsafe\" \u2192 \"safe\", \"safe\" \u2192 \"unsafe\") and removed explanations. The results, presented in Table 5, demonstrate that Class-RAG possesses a strong instruction following ability. Notably, the predicted labels of 99.49% of ground-truth safe examples were successfully flipped from \"safe\" to \"unsafe\", while the predicted labels of 12.29% of ground-truth unsafe examples were flipped from \"unsafe\" to \"safe\". This disparity in flipping ratios between ground-truth safe and unsafe examples can be attributed to the safety fine-tuning of the Llama3 model, which has been designed to prevent generating harmful responses"}, {"title": "Performance Scalability with Retrieval Library Size", "content": "We conducted an investigation to examine the impact of retrieval library size on the performance of Class-RAG, with results presented in Table 4. Specifically, we utilized the in-distribution (ID) library collected from the CoPro training set and the external (EX) library collected from the validation sets of I2P++ and UD. To assess the effect of library size on performance, we created downscaled versions of the external library, denoted as EX (1/8), EX (1/4), and EX (1/2), which were constructed by re-clustering the full external library to 1/8, 1/4, and 1/2 of its original size, respectively.\nOur results demonstrate that model performance consistently improves with increasing retrieval library size. On the I2P++ dataset, we observed AUPRC scores of 0.235, 0.501, 0.554, 0.577, and 0.677 when adding 0, 1/8, 1/4, 1/2 and the full size of the external library, respectively. Similarly, on the UD dataset, AUPRC scores increased from 0.914 to 0.959, 0.963, 0.964, and 0.974 with the addition of 0, 1/8, 1/4, 1/2, and the full size of the external library, respectively.\nNotably, our findings suggest that performance scales with the size of the retrieval library, indicating that increasing the library size is a viable approach to improving Class-RAG performance. Furthermore, as the retrieval library only incurs the cost of storage and indexing for retrieval, which is relatively inexpensive"}, {"title": "Performance Scalability with Reference Example Number", "content": "We conducted a further investigation to examine the impact of the number of reference examples on the performance of Class-RAG. Specifically, we evaluated the model's performance when adding 0, 2, 4, 6, and 8 reference examples, with an equal number of safe and unsafe examples added in each case. The results, presented in Table 6, demonstrate that the performance of Class-RAG consistently improves with the addition of more reference examples. On the I2P++ dataset, we observed average AUPRC scores of 0.303, 0.632, 0.677, 0.715, and 0.721 when using 0, 2, 4, 6, and 8 reference examples, respectively. Similarly, on the UD dataset, average AUPRC scores increased from 0.932 to 0.965, 0.974, 0.978, and 0.980 with the addition of 0, 2, 4, 6, and 8 reference examples, respectively.\nWhile our results indicate that performance improves with the number of reference examples, we also observe that this improvement becomes saturated at around 8 reference examples. Furthermore, adding more reference examples incurs a higher computational cost compared to scaling up the retrieval library size. Therefore, while increasing the number of reference examples can enhance performance, it is essential to balance this with the associated computational expense."}, {"title": "Performance with Different Embedding Models", "content": "The choice of embedding model is crucial for retrieving relevant content in our proposed approach. In this section, we investigate the impact of two different embedding models on the performance of Class-RAG: DRAGON ROBERTa Lin et al. (2023) and WPIE. DRAGON is a bi-encoder dense retrieval model that embeds both queries and documents into dense vectors, enabling efficient search for relevant information from a large number of documents. We utilize the context encoder component of DRAGON in our experiments. In contrast, WPIE is a 4-layer XLM-R Conneau et al. (2020) model that has been pre-trained on safety data, producing two outputs: an unsafe probability and a prompt embedding."}, {"title": "Conclusion", "content": "We introduce Class-RAG, a modular framework integrating an embedding model, a retrieval library, a retrieval module, and a fine-tuned large language model (LLM). Class-RAG's retrieval library can be used in production settings as a flexible hot-fixing approach to mitigate immediate harms. By employing retrieved examples and explanations in its classification prompt, Class-RAG offers interpretability into its decision-making process, fostering transparency in the model's predictions. Exhaustive evaluation demonstrates that Class-RAG substantially outperforms baseline models in classification tasks and exhibits robustness against adversarial attacks. Moreover, our experiments illustrate Class-RAG's ability to effectively incorporate external knowledge through updating the retrieval library, facilitating efficient adaptation to novel information. We also observe a positive correlation between Class-RAG's performance and the size of the retrieval library, as well as the number of reference examples. Notably, our findings indicate that performance scales with library size, suggesting a novel, cost-effective approach to enhancing content moderation. In summary, we present a robust, adaptable, and scalable architecture for detecting safety risks in the Generative AI domain, providing a promising solution for mitigating potential hazards in AI-generated content."}, {"title": "Future Work", "content": "Several future research avenues are promising. Firstly, we aim to extend Class-RAG's capabilities to multi-modal language models (MMLMs), enabling the system to effectively process and generate text in conjunction with other modalities. Secondly, our analysis in Section 5.4 reveals that Class-RAG excels at following the guidance of unsafe reference examples, but struggles with safe examples. To address this, we plan to investigate methods to enhance its instruction-following abilities for safe examples. Additionally, we intend to explore the use of more advanced embedding models, evaluate Class-RAG's multilingual capabilities, and develop more effective approaches for constructing the retrieval library. These directions hold significant potential for further improving the performance and versatility of Class-RAG."}, {"title": "Limitations", "content": "We acknowledge the potential risks and limitations associated with our Classification approach employing Retrieval-Augmented Generation (Class-RAG) for robust content moderation.\n\u2022 Our classifier may produce false positives or false negatives, leading to unintended consequences.\n\u2022 We rely on open-source English datasets, which may contain biases that can skew moderation decisions. These biases can be demographic, cultural, or reflect stereotypes. For example, our model may disproportionately block content from certain groups or unfairly moderating certain types of content.\n\u2022 Our model's common sense knowledge is limited by its base model and training data, and it may not perform well on out-of-scope knowledge or non-English languages.\n\u2022 There is a risk of misuse, such as over-censorship or targeting certain user groups unfairly.\n\u2022 Our model may generate unethical or unsafe language if used in a chat setting or be susceptible to prompt injection attacks."}, {"title": "Ethics Disclosure", "content": "Class-RAG was neither trained nor evaluated on any data containing information that names or uniquely identifies private individuals. Though Class-RAG can be an important component of an AI safety system, it should not be used as the sole or final arbiter in making content moderation decisions without any other checks or balances in place. We believe in the importance of careful deployment and responsible use to mitigate these risks, and emphasize that model-only approaches to ensuring content moderation will never be fully robust and must be used in conjunction with human-assisted strategies in order to mitigate bias. Ultimately, we stress the importance of ongoing evaluation and model development to address potential and future biases and limitations. To communicate our ideas more effectively, sections of original text in this paper were refined and synthesized with the help of Meta AI, though the original writing, research and coding is our own."}]}