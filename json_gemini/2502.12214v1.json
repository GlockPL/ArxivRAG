{"title": "Zero Token-Driven Deep Thinking in LLMs:\nUnlocking the Full Potential of Existing Parameters via Cyclic Refinement", "authors": ["Guanghao Li", "Wenhao Jiang", "Li Shen", "Ming Tang", "Chun Yuan"], "abstract": "Resource limitations often constrain the parameter counts of Large Language Models (LLMs),\nhindering their performance. While existing\nmethods employ parameter sharing to reuse the\nsame parameter set under fixed budgets, such approaches typically force each layer to assume\nmultiple roles with a predetermined number of\niterations, restricting efficiency and adaptability.\nIn this work, we propose the Zero Token Transformer (ZTT), which features a head-tail decoupled parameter cycling method. We disentangle\nthe first (head) and last (tail) layers from parameter cycling and iteratively refine only the intermediate layers. Furthermore, we introduce a ZeroToken Mechanism, an internal architectural component rather than an input token, to guide layerspecific computation. At each cycle, the model retrieves a zero token (with trainable key values) from a Zero-Token Pool, integrating it alongside\nregular tokens in the attention mechanism. The\ncorresponding attention scores not only reflect each layer's computational importance but also\nenable dynamic early exits without sacrificing overall model accuracy. Our approach achieves superior performance under tight parameter budgets, effectively reduces computational overhead via early exits, and can be readily applied to fine-tune existing pre-trained models for enhanced efficiency and adaptability.", "sections": [{"title": "1. Introduction", "content": "n recent years, it has been widely acknowledged that the\nperformance of Large Language Models (LLMs) improves\nwith an increasing number of parameters (Rae et al., 2021;\nConsequently, scaling up parameter\ncounts has become a common strategy for enhancing model\nperformance (Leviathan et al., 2023; Xu et al., 2024; Pope\net al., 2023). However, this approach is often infeasible\nfor users with limited computational resources. A critical\nchallenge, therefore, is to achieve better performance under\na fixed parameter budget (Zhou et al., 2024).\nA variety of model compression techniques, including quantization (Lin et al., 2024; Liu et al., 2023), pruning (Ma\net al., 2023; Sun et al., 2023), and distillation (Latif et al.,\n2023; Shum et al., 2024), have been proposed to shrink large\nmodels to smaller ones. In parallel, another line of research\nhas investigated ways to leverage additional computation\nwithin a fixed number of parameters, thereby unlocking\ndeeper or more iterative reasoning (Dehghani et al., 2018;\nLan, 2019). A common strategy here is parameter shar-\ning, where model layers reuse the same parameters across\nmultiple computational cycles, sometimes referred to as\n\"parameter cycling.\" Rather than maintaining a separate set\nof parameters for each layer, models recurrently apply a\ncompact parameter set, reducing memory requirements and\npotentially increasing depth of reasoning.\nDespite its potential, parameter cycling raises three core\nchallenges: (1) Which parameters should be reused across\niterative cycles? (2) How can these shared parameters\nbe managed to avoid functional conflicts and performance\ndegradation? (3) When should the model decide that no further reasoning is necessary, thus saving computational cost\nwithout truncating essential inference steps prematurely?\nExisting works partially address one or two of these ques-\ntions. For example, Solar (Kim et al., 2023) reuses parame-\nters from intermediate layers (which parameters), while the\nRelaxed Recursive Transformer (Bae et al., 2024) focuses on\nhow to manage the recurring layer through LoRA (Hu et al.,\n2021). Palbert (Balagansky & Gavrilov, 2022), combining\nPonderNet (Banino et al., 2021) with ALBERT, explores\nwhen to stop via a dynamic pondering mechanism. However,\nnone of these approaches provide a comprehensive solution\nthat systematically addresses all three dimensions\u2014which\nparameters to cycle, how to apply them, and when to termi-\nnate reasoning."}, {"title": "2. Related Work", "content": "Parameter sharing has long been explored in early deep\nlearning architectures, such as Convolutional Neural Net-\nworks (CNNs) (Eigen et al., 2013; Savarese & Maire, 2019)\nand Recurrent Neural Networks (RNNs) (Graves, 2016;\nSherstinsky, 2020), effectively reducing model complex-\nity while preserving performance. The Universal Trans-\nformer (Dehghani et al., 2018) later extended this idea to\nthe Transformer architecture, demonstrating that reusing\nparameters in a cyclical manner across layers can substan-\ntially enhance efficiency. Subsequently, various studies\nhave investigated which Transformer components should be\nshared. Some focus on parameter reuse within individual\nlayers (Dabre & Fujita, 2019), tying encoder and decoder\ncomponents (Milbauer et al., 2023; Xia et al., 2019), or\nadopting partial expert networks (Liu et al., 2024). Others\noptimize how parameter sharing is organized, for instance\nby stacking parameters in specific orders (Takase & Kiyono,\n2021) or applying factorized embeddings (Lan, 2019) to im-\nprove performance. A critical aspect of parameter cycling is\ndetermining when to repeat computations. Methods such as\nACT (Chowdhury & Caragea, 2024; Graves, 2016; Csord\u00e1s\net al., 2024; Tan et al., 2023) and PonderNet (Banino et al.,\n2021; Balagansky & Gavrilov, 2022) introduce adaptive\nrecursion, allowing the model to decide how many cycles\nof computation are needed for deeper reasoning. However,\nmost of these studies focus on training from scratch rather\nthan fine-tuning large pre-trained models, limiting their prac-\nticality in real-world scenarios.\nRecent work has begun to address parameter cycling within\npre-trained Large Language Models. For instance, So-\nlar (Kim et al., 2023) improves performance by reusing\nparameters from the middle layers of the Llama model (Tou-\nvron et al., 2023), illustrating which layers to cycle. Re-\nlaxed Recursive Transformers (Bae et al., 2024) integrate\nLORA (Hu et al., 2021) modules to alleviate performance\ndegradation caused by repetitive parameter usage. Despite\nthese advances, the question of when to stop recurrent pro-\ncessing remains underexplored. Although Relaxed Recur-\nsive Transformers consider varying numbers of cycles, they\nrely on fixed computation paths rather than a genuinely\ndynamic mechanism. Meanwhile, research on early exit-\ning (Chen et al., 2023; Pan et al., 2024) focuses primarily\non non-recurrent models, leaving open the question of how\nmany cycles a recurrent model should undergo. In contrast,\nour approach comprehensively addresses the three central\nquestions of what to cycle, how to manage recurrent parame-\nters, and when to terminate reasoning. We propose a method\napplicable both to training from scratch and to fine-tuning\npre-trained LLMs, offering a practical and efficient solution\nfor enhancing model performance under tight parameter\nbudgets."}, {"title": "3. Zero Token Transformer", "content": "In this section, we introduce our Zero-Token Transformer\n(ZTT), a novel approach that combines Head-Tail Decoupled\nCyclic Architecture and a Zero-Token Mechanism with a gat-\ning strategy. We begin by examining preliminary observa-\ntions from a Basic Cyclic Transformer (BCT) and highlight\nthe challenges that arise when scaling up. We then describe\nhow our head-tail decoupling addresses these issues, and\nhow the Zero Token and gating mechanism further enhance\nperformance and enable dynamic early exiting."}, {"title": "3.1. Basic Cyclic Transformer (BCT) and Motivation", "content": "We first consider a simple form of cyclic Transformer, fol-\nlowing (Bae et al., 2024; Takase & Kiyono, 2021), where\na single Transformer layer (or a small stack of layers) is\nrepeatedly applied for N cycles. Formally, let $H_l^n$ be the\noutput of layer l at cycle n:\n$H_l^n = H_l^{n-1} + FFN(LN(H_l^{n-1}), \\theta^l)$,\n$H_l^{A,n} = H_l^{F,n-1} + MultiHead(H_{l-1}^{F,n}, \\Phi^l)$,\n$l \\in \\{1,2,..., L\\}, n \\in \\{1,2,..., N\\}$,\nwhere \u03a6 and \u03b8 are trainable parameters, and the same parameters are reused at each cycle. For simplicity, $H_1^0$ is the\nembedding of the input tokens. When N = 1, this reduces\nto a standard (vanilla) L-layer Transformer.\nFigure 2(a) shows perplexities on WikiText-2 under the\nsame total computational cost (number of layers \u00d7 number\nof cycles). For instance, a 1-layer BCT run for 12 cycles\nachieves a lower perplexity than a 1-layer vanilla Trans-\nformer (i.e., N = 1), but it is still worse than a standard\n12-layer Transformer with the same total computational bud-\nget. Similarly, adding classification heads after each cycle\nfor early exiting (\"Early exit\" in Figure 2(a)) further de-\ngrades performance, indicating that requiring intermediate\noutputs imposes extra burdens on the cyclic layers.\nEmpirical Comparison. These findings highlight a fun-\ndamental issue with basic cyclic Transformers: each cyclic\nblock assumes significantly greater responsibilities than a\nvanilla Transformer layer. First, cyclic layers must fulfill\nthe roles of multiple layers by providing enhanced interme-\ndiate representations for subsequent computations, while\nalso producing high-quality outputs for the classification\nhead. Moreover, there is no explicit guidance on the specific\nrole a given layer should perform at each iteration. This\nlack of role distinction may lead to confusion and compu-\ntational conflicts, ultimately affecting overall performance.\nConsequently, even when the current representation is well-\noptimized, it is recalculated in subsequent cycles, potentially\noverwriting previously learned representations.\nIncreasing Cycles Alone Is Insufficient. Reusing the\nsame layer(s) for multiple cycles indeed improves perfor-\nmance compared to a single-layer Transformer. However, as\nwe increase the total layer count or require early-exit outputs\nat each cycle, the performance gap between Basic Cyclic\nTransformers and an equivalently sized vanilla Transformer\nwidens. We hypothesize that this occurs because each cyclic\nblock in BCT must simultaneously compute refined internal\nrepresentations and produce final outputs for classification\nat each cycle, without any guidance on how to specialize.\nAs cycles accumulate, these conflicting objectives can lead\nto overwriting or \u201cconflicting\u201d representations.\nBased on the observations above, we identify three key\nissues that motivate our design:\nIssue 1: No separation of specialized layers. The first and\nlast layers in a Transformer typically have distinct\nroles (e.g., mapping raw inputs or producing log-"}, {"title": "3.2. Head-Tail Decoupled Cyclic Architecture", "content": "Recent analyses (Sun et al., 2024) suggest that intermediate\nTransformer layers often exhibit functionally similar rep-\nresentations, whereas the head (first) and tail (last) layers\nspecialize in tasks such as raw feature encoding and output\nmapping. Pruning or drastically altering these boundary lay-\ners tends to have an outsized impact on overall performance.\nHence, we preserve the original first and last layers as fixed\n(non-cyclic) and let only the middle layers reuse parameters\nacross N cycles, as shown in Figure 1(right). Concretely,\nif the Transformer has L layers, we exclude layers 1 and L\nfrom parameter cycling:\n$l \\in \\{2, 3, ..., L \u2013 1\\}, n \\in \\{1,..., \u039d\\}$. (2)\nBy doing so, the distinct responsibilities of the head and\ntail layers remain intact, mitigating conflicts. Figure 2(a)\nshows that under the same total computational budget, our\nHead-Tail Decoupled Cyclic Transformer (HDT) achieves\nbetter perplexity than a straightforward \u201call-layer cycling\u201d\ndesign, confirming that preserving specialized boundary\nlayers helps alleviate the performance gap (Issue 1)."}, {"title": "3.3. Zero-Token Mechanism", "content": "Even with head-tail decoupling, intermediate layers in a\ncyclic setup can still redundantly process representations. If\nthe current representation is already high-quality, repeatedly\nrefining it may overwrite earlier features or create conflicts.\nWe address this by introducing a small, trainable \"Zero\nToken\" into each attention layer-acting like a prompt that\n\"signals\" whether the model should refine or skip a cycle."}, {"title": "3.4. A Dynamic Mechanism for Determining the Number of Cyclic Iterations", "content": "While the Zero Token's attention score is the main indicator\nfor deciding whether to halt additional cycles, we introduce\na lightweight gating mechanism around the feed-forward\nnetwork (FFN) to provide finer-grained computational con-\ntrol. Even if the model has not yet triggered an early exit,\nsome cycles may only require partial FFN computation.\nWe modify the FFN in Eq. 1 by adding:\n$H_l^{F,n} = H_l^{F,n-1} + [FFN(LN(H_l^n),\\theta^l)] \\cdot gate (LN(H_l^n))$,\ngate() \u2208 [0, 1]. (4)\nWhen gate() is close to 1, the full FFN transform is applied;\nwhen near 0, the FFN is mostly bypassed, saving computation. We observe that the gating value often correlates with\nthe Zero Token's attention: if the model pays high attention\nto the Zero Token, it implies less refinement is needed, and\nthe gate decreases accordingly.\nEarly Exit Criterion. The Zero Token's attention score\n(\u00a73.3) remains the primary criterion for early exit. Once it\nexceeds a threshold (e.g., 0.9), we terminate further cycles\nand output the final representation. The gating function sim-\nply provides a smoother transition for intermediate cycles,\nreducing unnecessary FFN computation before the final exit\ntrigger.\nDiscussion. We do not gate the attention module itself\nbecause attention integrates token representations, includ-\ning the Zero Token. By gating the FFN, we allow partial\nskipping of the more expensive transformations without in-\nterfering with the Zero Token's signaling. Hence, the Zero\nToken attention decides when to stop entirely, while the gate\nrefines how much computation to apply until that point.\nOverall, by combining head-tail decoupling (to preserve spe-\ncialized boundary layers), a Zero Token (to guide cycle-level\ncomputation), and a gating-based mechanism (to smooth\npartial skips and enable early exit), our Zero-Token Trans-\nformer effectively addresses the three major issues outlined\nin \u00a73.1."}, {"title": "4. Experiments", "content": "In this section, we present our experimental setup and results\nto demonstrate the effectiveness of our proposed Zero-Token\nTransformer approach under a fixed parameter budget. We\nevaluate both training from scratch and fine-tuning scenar-\nios, using a decoder-only Transformer architecture."}, {"title": "4.1. Experimental Setup", "content": "Models. We consider two main training settings:\n\u2022 Training from Scratch: We base our architecture on\nGPT-2 (Radford et al., 2019) but restrict each layer to\naround 10M parameters. The total number of layers is\nL = 6. To maintain a fixed computational budget, we\ndefine a total of 6 \u201cnetwork computation cycles\", where\neach layer in a standard setting (i.e., without parameter\nsharing) is counted as one cycle. All models in this setting\nare pre-trained on the C4 English subset (Raffel et al.,\n2020) using a causal next-token prediction objective for\n10B tokens.\n\u2022 Fine-Tuning Pre-Trained Models: We also fine-tune\nwidely used checkpoints such as GPT-2 and OPT (Zhang\net al., 2023) to show that our approach can be applied to\nlarge pre-trained models with minimal modification.\nBaselines. We compare several approaches, all based on\ndecoder-only Transformers. One variant, referred to as\nearly exit, adds classification heads at each cycle, facilitat-\ning intermediate predictions. These intermediate exits are\nrepresented as E.\n\u2022 Vanilla (V): A standard Transformer with L distinct lay-\ners. The total computation cost is effectively L cycles.\""}, {"title": "A. Experimental Setup", "content": "A.1. Evaluation Details\nTo assess the effectiveness of our proposed method, we evaluate models on a diverse set of well-established NLP benchmarks.\nThese benchmarks span four key reasoning tasks: commonsense physical reasoning, multiple-choice question answering,\nlong-term context recall, and natural language inference. For all datasets, we report accuracy (ACC) as the primary\nevaluation metric.\nA.1.1. REASONING: PIQA\nDataset: The Physical Interaction Question Answering (PIQA) dataset (Bisk et al., 2020) evaluates a model's ability to\nreason about everyday physical interactions. It consists of multiple-choice questions that require an understanding of how\nobjects and tools function in real-world scenarios.\nTask Objective: Given a short natural language query, the model must select the most plausible solution from two candidate\nanswers. This task assesses the model's ability to infer practical physical knowledge beyond simple memorization.\nEvaluation Metric: Accuracy (ACC), measuring the proportion of correctly predicted answers.\nA.1.2. MULTIPLE-CHOICE QUESTION ANSWERING: ARC CHALLENGE AND ARC EASY\nDataset: The AI2 Reasoning Challenge (ARC) (Clark et al., 2018) is a standardized multiple-choice QA benchmark designed\nto evaluate a model's ability to answer science-related questions. It consists of two subsets:\n\u2022 ARC Challenge: A more difficult subset requiring complex reasoning and deeper knowledge retrieval.\n\u2022 ARC Easy: A simpler subset containing factual questions that can often be answered with surface-level understanding.\nTask Objective: The model is provided with a science-related question and four answer choices, from which it must\nselect the correct one. The dataset requires a combination of commonsense reasoning, logical inference, and scientific\nknowledge to achieve high performance.\nEvaluation Metric: Accuracy (ACC), computed as the percentage of correctly answered questions.\nA.1.3. LONG-TERM CONTEXT RECALL: LAMBADA\nDataset: The LAMBADA dataset (Paperno et al., 2016) is specifically designed to assess a model's capability for long-range\ncontext comprehension. Unlike standard language modeling tasks, LAMBADA requires a model to retain and process\ninformation over an extended passage to predict a crucial missing word.\nTask Objective: Given a long contextual passage, the model must predict the final missing word of the last sentence. The\ndifficulty arises from the fact that the target word is nearly impossible to guess without understanding the full passage.\nEvaluation Metric: Accuracy (ACC), where a prediction is considered correct if the entire target word matches the ground\ntruth exactly.\nA.1.4. NATURAL LANGUAGE INFERENCE: HELLASWAG\nDataset: The HellaSwag dataset (Zellers et al., 2019) is an advanced benchmark designed to evaluate commonsense\ninference and story continuation. It builds on the SWAG dataset by incorporating adversarial filtering, making it more\nchallenging for models to rely on surface-level heuristics.\nTask Objective: Given an incomplete story or event description, the model must select the most logical next step from\nfour possible continuations. This requires strong contextual understanding and the ability to anticipate real-world event\nprogressions.\nEvaluation Metric: Accuracy (ACC), measuring how often the model correctly predicts the most plausible continuation."}, {"title": "A.2. Training and Fine-Tuning Settings", "content": "In this section, we describe the training settings for both pre-training from scratch and fine-tuning of pre-trained models.\nThe fine-tuning stage is required for all models before final evaluation, while models trained from scratch undergo both\npre-training and fine-tuning. The fine-tuning hyperparameters are kept consistent across both settings.\nA.2.1. PRE-TRAINING FROM SCRATCH\nFor models trained from scratch, we first conduct pre-training on the C4 English dataset (Raffel et al., 2020). The pre-training\nprocess follows these configurations:\nPre-Training Protocol\n\u2022 Dataset: The C4 (Colossal Clean Crawled Corpus) English subset.\n\u2022 Computing Resources: We utilize an A800 GPU cluster for training.\n\u2022 Batch Size per GPU: 80, with gradient accumulation to maintain a global batch size of 256.\n\u2022 Training Steps: The model is trained for a total of 10B tokens.\n\u2022 Optimizer: AdamW (?) with a weight decay of 0.01.\n\u2022 Learning Rate: A linear warmup is applied for the first 1% of total steps, followed by a cosine decay schedule.\n\u2022 Precision: Training is performed in half-precision (FP16) to optimize memory efficiency.\nAfter pre-training, the models proceed to the fine-tuning stage before being evaluated on downstream tasks.\nA.2.2. FINE-TUNING SETTINGS\nBefore evaluating on the test datasets, we fine-tune our models using the corresponding training sets. For pre-trained\nmodels, only fine-tuning is performed, while models trained from scratch undergo both pre-training and fine-tuning.\nThe fine-tuning process is conducted under the same computational settings as pre-training.\nFine-Tuning Protocol\n\u2022 Fine-Tuning Epochs: Each dataset is fine-tuned for 3 epochs.\n\u2022 Batch Size per GPU: 20, with gradient accumulation ensuring an effective batch size of 80.\n\u2022 Optimizer: AdamW with a 0.01 weight decay.\n\u2022 Learning Rate: The default Hugging Face Trainer API learning rate is used.\n\u2022 Prompt Engineering: We utilize prompt templates from promptsource to better adapt models to the task format.\n\u2022 Computing Resources: The same A800 GPU cluster is used as in pre-training.\n\u2022 Training Framework: Fine-tuning is implemented with Hugging Face's Trainer API.\nDataset-Specific Fine-Tuning Details Fine-tuning is performed on the following datasets before model evaluation. The\ndetails of each dataset, including the number of training examples, are presented in Table 6."}, {"title": "A.3. Early-Exit Training Settings", "content": "To ensure effective intermediate predictions when early-exit mechanisms are applied, we implement additional training\nfor intermediate classifier heads. This helps maintain meaningful intermediate outputs, preventing degradation in\nperformance due to premature exits.\nA.3.1. CLASSIFIER PLACEMENT\n\u2022 Simple Cycling: The classification head is placed only at the final output layer.\n\u2022 Head-Tail Separation: The classification head is placed at both the final layer and the last shared layer before\ncycling begins.\nA.3.2. TRAINING STRATEGY FOR EARLY-EXIT MODELS\nTo optimize models for early exits, we introduce additional supervision at intermediate layers. Instead of relying solely on\nthe final output, we ensure that multiple exit points are trained effectively.\n\u2022 Intermediate Supervision: The model is trained to produce meaningful predictions at designated early-exit points.\n\u2022 Exit Point Optimization: Models with multiple cycling blocks undergo training to align their intermediate outputs\nwith final predictions, improving robustness across different exit depths.\n\u2022 Gradual Refinement: The early-exit heads are optimized using the same fine-tuning data, ensuring consistency across\nall prediction layers.\nBy integrating these early-exit classifiers and fine-tuning them separately, we ensure that models can gracefully exit\nat earlier layers without sacrificing predictive accuracy. This design allows our method to maintain efficiency while\npreserving strong performance across different computational budgets."}]}