{"title": "Correcting Negative Bias in Large Language Models through\nNegative Attention Score Alignment", "authors": ["Sangwon Yu", "Jongyoon Song", "Bongkyu Hwang", "Hoyoung Kang", "Sooah Cho", "Junhwa Choi", "Seongho Joe", "Taehee Lee", "Youngjune L. Gwon", "Sungroh Yoon"], "abstract": "A binary decision task, like yes-no questions or\nanswer verification, reflects a significant real- world scenario such as where users look for\nconfirmation about the correctness of their de- cisions on specific issues. In this work, we ob- serve that language models exhibit a negative bias in the binary decisions of complex rea- soning tasks. Based on our observations and the rationale about attention-based model dy- namics, we propose a negative attention score (NAS) to systematically and quantitatively for- mulate negative bias. Based on NAS, we iden- tify attention heads that attend to negative to- kens provided in the instructions as answer can- didate of binary decisions, regardless of the question in the prompt, and validate their as- sociation with the negative bias. Additionally, we propose the negative attention score align- ment (NASA) method, which is a parameter- efficient fine-tuning technique to address the extracted negatively biased attention heads. Ex- perimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their gener- alization abilities.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models\n(LLMs) have enabled complex reasoning tasks exe- cuted by understanding user instructions (Ouyang\net al., 2022; Touvron et al., 2023; Achiam et al.,\n2023; Jiang et al., 2023). As the capabilities of\nLLMs have expanded rapidly, research has inten- sified to analyze their characteristics and inherent\nissues. One of the major issues is the generation\nof factually incorrect content, known as \u201challucina- tion\", which significantly degrades the reliability of\nLLM-based services (Zhang et al., 2023; Xu et al.,\n2024; Huang et al., 2023). The hallucination prob- lem in LLMs can be attributed to factors such as\nparametric knowledge, overconfidence, and biases\n(Zhang et al., 2023). Studies have been conducted\nto understand the decision-making mechanisms in\nLLMs from a parameter perspective. For instance,\nthe \"logit lens\" technique allows for the interpre- tation of the model's reasoning process along the layers, exploiting the hidden representations from intermediate layers (Belrose et al., 2023; Ferrando\net al., 2023). This technique has been utilized to\nanalyze model characteristics in various scenar- ios such as in-context learning. On the other hand,\nYuan et al. (2024) focus on analyzing hallucina- tion phenomenon based on attention heads. How- ever, the causes and mechanisms of hallucination in LLMs vary by type of task, indicating that exten- sive further study is still necessary.\nIn this paper, we aim to identify biases that\nemerge when LLMs respond to questions requir- ing a binary decision such as affirmation or nega- tion. Specifically, in yes-no question-answering\n(QA) tasks that demand complex reasoning, such\nas mathematical or logical reasoning, we observe a\nnegative bias in LLMs. This negative bias, which\nimplies a discrepancy between the model's reason- ing ability and accuracy, potentially decreases the\nreliability of model predictions. From the rationale\nthat the model allocates higher attention to the neg- ative candidates among the binary answer options\npresented in the user prompt, we propose a negative\nattention score (NAS) to systematically probe these\nnegatively biased attention heads in large language\nmodels (LLMs). Based on NAS, we detect attention\nheads that attend to tokens associated with nega- tion in an instruction. Our findings also confirm the\""}, {"title": "2 Related Work", "content": "LLMs are demonstrating capabilities not only in\ntypical natural language processing tasks but also\nin tasks that require complex thought processes. In\nparticular, the use of emergent abilities, such as\nthe chain-of-thoughts, has significantly enhanced\nthe performance of complex reasoning (Wei et al.,\n2022; Kojima et al., 2022; Wang et al., 2022;\nMadaan et al., 2023). However, the increased com- plexity of tasks and pipelines has made the hallu- cination problem in complex reasoning tasks chal- lenging to analyze and resolve, which remains an\nactive area of research (Zhang et al., 2023). With\nthe advent of benchmarks vulnerable to hallucina- tions, recent LLMs are laying the groundwork for\nanalyzing the causes of LLM hallucinations based\non substantial data. For instance, StrategyQA (Geva\net al., 2021) and MuSiQue (Trivedi et al., 2022) in- volve queries and contexts that demand multi-step\nreasoning from LLMs, while GSM8k (Cobbe et al.,\n2021) involves queries that require mathematical\nreasoning skills."}, {"title": "2.2 Analysis on Roles and Knowledge in\nModel Parameters", "content": "As the capacity and complexity of LLMs increase,\nthe need to understand and interpret these models\nis becoming more crucial. In particular, there is ac- tive research into analyzing parametric knowledge\nand model behavior that influence the phenomenon\nof model hallucination. Meng et al. (2022) pro- pose a framework to specify the location where the\nknowledge of the model is stored, while Belrose\net al. (2023) and Yang et al. (2024) research to in- terpret the inference process of the model. Yuan\net al. (2024) suggest a method to detect model pa- rameters involved in the false premise hallucination\nphenomenon of LLMs and address this issue.\nSimilarly, we demonstrate that the model's neg- ative bias is related to the attention heads. Specifi- cally, we explore the existence and characteristics\nof query-agnostic negative bias in attention heads. Furthermore, we demonstrate that this can be sig- nificantly addressed through a parameter-efficient\nfine-tuning method."}, {"title": "3 Negative Bias in Large Language\nModels", "content": "A binary decision task, such as yes-no QA or an- swer verification, represents a major real-world\nscenario where users seek confirmation on whether\ntheir decisions regarding specific problems are\ncorrect. This acts as the basic block of the high- dimensional reasoning tasks that need step-by-step\nsub-tasks. Despite the importance of the binary de- cision task, unfortunately, many of the complex rea- soning benchmark sets are not in the form of binary decisions. Therefore, in this section, we first trans- form existing complex reasoning tasks into yes-no\nbinary decision tasks. We then conduct statistical\nobservations and quantitative analyses on how vari- ous large language models perform in these binary\ndecision scenarios."}, {"title": "3.1 Statistical Observation on Complex\nReasoning Tasks", "content": "Transformation to binary decision task In this\nwork, we employ two techniques to transform gen- eral QA reasoning tasks into binary decision tasks. We modify each sample into a positive or nega- tive sample where the answer is \u201cYes\u201d or \u201cNo\u201d,\nrespectively. In this section, we focus on the case\nof short-answer QA datasets. We refer to Appendix\nC for the case of multiple choice datasets.\nFor the positive sample, we perform a rule-based\ntransformation on general queries by appending a confirmatory query. For example, the query \"What\nis 1+1?\" is transformed into \"What is 1+1? Is the\nanswer 2?\" This approach is straightforward but is limited to cases where the correct answer is \"Yes\".\nFormally, given the question and its label, we trans- form the prompt for the binary decision task whose\ntarget answer is \u201cYes\u201d:"}, {"title": "Prompt I (Binary Decision Transformation)", "content": "You are given a question and you MUST an-\nswer Yes or No. Question: {question} Is the\nanswer {label}? Answer:\nTo transform into the negative sample, a wrong\nanswer is necessary based on the query and answer which will be placed in label in Prompt I. For this,"}, {"title": "3.2 Formulation of Negative Bias", "content": "In this section, we formulate the previously discov- ered negative bias from the perspective of model\nintrinsic properties. Specifically, we focus on the internal attention patterns of the model. Generally, to obtain an answer for a binary decision, a user provides answer candidates like \u201cYes\u201d or \u201cNo\u201d in\nthe form of instructions to the model before pos- ing the query prompt. The model follows these instructions and responds to the given query in the manner provided in the instructions. During this response process, the model attends to the given\ninstructions, and due to the operational characteris- tics of attention-based models, the candidate with\nthe larger attention weight generally appears in the response. In this context, the negative bias of the LLM can be seen as originating from assigning greater attention weight to negative answer candi- dates during the reasoning process."}, {"title": "3.2.1 NAS: Negative Attention Score", "content": "To validate\nthis rationale, we define the following Negative At- tention Score (NAS) using the attention weights applied to negative answer candidate tokens like\n\"No\", and positive answer candidate tokens like\n\"Yes\", observed in a attention head.\nFormally, let Lp be the length of the prompt x, tyes and tNo the positions of the \u201cYes\u201d and \u201cNo\u201d tokens within the instruction, and L\u2081 the length of the instruction. For the attention weight inferred by the hth attention head in the lth layer, denoted as\n$A^{l,h} \\in R^{L_P \\times L_P}$, the NAS is defined as:\n$NAS^{l,h} = \\sum_{i=L_I}^{L_P} ( A_{i,t_{No}}^{l,h} + A_{i,t_{Yes}}^{l,h} ) * log( \\frac{A_{i,t_{No}}^{l,h}}{A_{i,t_{Yes}}^{l,h}} )$\nwhere we omit the input x in the attention function\nfor brevity. In the i-th row, each NAS term consists"}, {"title": "3.2.2 Empirical Studies", "content": "To demonstrate that the previously defined NAS\nis an effective indicator of negative bias, we mea- sure the correlation between NAS and the nega- tive confidence observed in model responses. For this measurement, we compile a test set consisting of a total of 1,500 samples, drawing 500 samples each from StrategyQA, rephrased GSM8K, and AR-LSAT. This validates that NAS is an effective indicator of a model's negative bias.\nAdditionally, we conduct a study on negative\nheads defined through the Negative Attention Score (NAS). For the QA dataset composed of three dif- ferent domains that we previously created, we ex- plore negative attention heads in three cases based\non each domain subset. We designate as negative attention heads for a domain subset those heads among the top 200 heads with the highest single head NAS per sample that appear commonly in"}, {"title": "4 NASA: Negative Attention Score\nAlignment", "content": "Motivated by previous observations, we aim to quantify the negative bias of LLMs from the per- spective of attention heads. To observe the negative bias in attention heads, we start by constructing a negative attention head probing set. To construct\nthe probing set, we first select samples that can\nbe answered by the model's parametric knowledge then convert them into binary decision making for- mat using Prompt I. Note that in the probing set, the label is uniformly set to \"Yes\" to effectively reveal the model's negative bias.\nWe suppose that the probing samples must meet\nthe condition: the question should be answerable using the model's parametric knowledge. To deter- mine whether the question in the sample meets the condition, we construct the prompt as follows:"}, {"title": "Prompt II (Inquiring Parametric Knowledge)", "content": "You MUST answer shortly the given ques- tion based on your knowledge. Question:\n{question} Answer:\nThis prompt effectively selects samples that be- long to the model's parametric knowledge while minimizing the influence of the model's negative bias. We assume that samples meet the condition if the model returns the correct answers for the prompt, and we use them as the probing set. We uti- lize GPT-4 to verify whether the prediction matches to label. A detailed explanation of the parametric sample selection can be found in Appendix D."}, {"title": "4.2 Negative Attention Head Probing", "content": "We probe negatively biased attention heads using\nthe constructed probing set. First, we measure the\nNAS of each attention head when the prompt from\neach probing sample is input into the model. We\nthen identify the top $k'$ attention heads with the highest NAS for each probing sample. Among\nthese extracted heads, we select those that are con- sistently extracted in over 90% of the probing set samples. Finally, the top $k$ attention heads, sorted\nby their single head NAS values, are targets for further addressing. In this paper, we set $k$ to 100. At this point, we further categorize the probing set into cases where the model correctly answers\n(i.e., the true positive set) and cases where it does not (i.e., the false negative set). During the fine- tuning phase of the model (See Section 4.3), we\nuse the false negative set as the training set. This\napproach is aimed at inducing the model to gener- ate positive answers for samples that it incorrectly fails to answer due to a negative bias in paramet- ric knowledge. Meanwhile, the true positive set is used to determine the criteria for early halting of"}, {"title": "4.3 Head-wise Incremental Tuning", "content": "To address the identified attention heads as speci- fied in Section 4.2, we propose an additional super- vised fine-tuning method. Our fine-tuning method\nis parameter efficient as it focuses only on fine- tuning the query and key projection weights of the selected negative bias attention heads."}, {"title": "4.3.1 Dataset", "content": "We utilize the data from the attention head prob- ing set for fine-tuning (i.e., subset of rephrased HotpotQA). Because the model already possesses parametric knowledge for answering questions in the probing set, we construct the fine-tuning data to encourage the model to generate positive an- swers in response to Prompt I. As the queries in the probing set contain parametric knowledge, we do not provide contexts containing related facts to the model."}, {"title": "4.3.2 Training Objective", "content": "Similar to a standard supervised fine-tuning objec- tive, we target only the answer token (e.g., \u201cYes\u201d)\nfor training. Since our dataset contains only short answers, training the model to generate the special token that indicates the end of a sequence could in- troduce a bias towards short responses. To prevent this side effect, we only use the target answer token in our loss calculation."}, {"title": "4.3.3 Training Strategy", "content": "We propose to further fine-tune each attention head sequentially in order of decreasing NAS which is measured during the attention head probing stage. Specifically, we only train the query and key pro- jection modules of the target attention head.\nSince we train with data where the target answer is positive, there is a risk of inducing a positive bias. Furthermore, updating a single attention head might change the NAS of another attention head. To prevent this, we set aside a portion of the dataset as a validation set and apply early stopping and up- date cancellation schemes for each attention head tuning, and early halting scheme for the whole fine- tuning process.\nStrategy of attention head tuning subprocess we set early stopping criteria based on the model NAS and single head NAS. For the former criteria, we check for a decrease in the value each epoch and halt training if there is an increase. For the lat- ter one, we verify that i) the value decreases each epoch and ii) remains above a specific threshold. Training is also stopped if either of the two condi- tions is not met.\nAfter fine-tuning each attention head, we calcu- late the difference between the single NAS and the"}, {"title": "5 Experiments", "content": "We conduct experiments using LLaMA3, Mistral,\nGemma, and Qwen2. We use those instruct models"}, {"title": "5.2 Main Results", "content": "In this section, we examine whether the negative bias observed and analyzed in Section 3 has been effectively mitigated by NASA. Generally, we observe an improvement in accuracy and a significant re- duction in the gap between precision and recall. As mentioned in Section 3, high precision cou- pled with low recall indicates that the model is ex- cessively cautious in providing positive responses. This, in turn, leads to a degradation in the trustwor- thiness of negative responses.\nFrom this perspective, reducing the gap between precision and recall while maintaining accuracy at an equivalent or higher level is a way to mitigate negative bias, balancing the trustworthiness of the model's responses, and ultimately improving the model. Additionally, based on the improvement in the F1 score, which is the harmonic mean of preci- sion and recall, it is demonstrated that the balance between precision and recall has been achieved in a positive direction of improving the model capa- bility for reasoning. Regarding the NAS metric, we observe that our method effectively reduces NAS to an appropriate level.\nMeanwhile, for the mathematical reasoning task, recall surpasses precision. This indicates that this task is somewhat more sensitive compared to other tasks and suggests that additional regularization, considering the task's sensitivity, could enhance the effectiveness of our method. Future research"}, {"title": "6 Analysis", "content": "In this section, we evaluate the performance of\na large language model with NASA applied to general QA reasoning tasks. We assess the gen- eral reasoning performance on benchmarks such\nas MuSiQue, GSM8K, and AR-LSAT using the LLaMA3-8B model both with and without NASA. After verifying whether each model-generated pre- diction includes an answer in these benchmarks, we utilize GPT-4 to double-check whether the pre- dictions that contain answers indeed correspond\nsemantically to the actual answers. Moreover, the slight decrease in precision might\nsuperficially appear as a performance decline. How- ever, it can be interpreted as an enhancement of the\nmodel's trustworthiness by adjusting the alignment\nbetween the model's actual knowledge and its re- sponses. This means that precision might slightly decrease because some samples that were previ- ously incorrectly categorized as true negatives due to negative bias might shift to false positives. For instance, even if the model makes incorrect infer- ences or remains uncertain about a question, in a binary decision scenario, negative bias could lead to it being classified as a true negative."}, {"title": "6.1\nPreservation of General Reasoning\nAbilities", "content": "However, the slight decrease in precision might\nsuperficially appear as a performance decline. How- ever, it can be interpreted as an enhancement of the\nmodel's trustworthiness by adjusting the alignment\nbetween the model's actual knowledge and its re- sponses. This means that precision might slightly decrease because some samples that were previ- ously incorrectly categorized as true negatives due\nto negative bias might shift to false positives. For\ninstance, even if the model makes incorrect infer- ences or remains uncertain about a question, in a\nbinary decision scenario, negative bias could lead to it being classified as a true negative."}, {"title": "6.2 Generalization to Universal Binary\nDecision", "content": "We analyze the generalization ability for binary\ndecisions by replacing the positive and negative to- kens used in our fine-tuning set. Our fine-tuning set consists of \"Yes\" and \"No\", and in this experiment, we test with the tokens \"True\" and \"False\", as well\nas \"Correct\u201d and \u201cWrong\u201d. As shown\nin the table, ours fine-tuned using NASA achieves\nsemantic-level generalization without overfitting to specific tokens."}, {"title": "6.3 Transferability across Various\nInstructions", "content": "In this work, we maintain a fixed instruction for\nbinary decision-making in yes-no QA throughout the application of our methodology. In this section, we evaluate the robustness of our method when presented with instructions in the inference stage"}, {"title": "6.4\nEnhanced Model Calibration", "content": "As Fig 2 illustrates, the negative bias of LLMs is\nalso associated with the model's confidence in its predictions. Research on the calibration between\nprediction confidence and accuracy is necessary to\nenhance the trustworthiness of LLMs. Table 8 shows the expected calibration error (He et al., 2022) when NASA is applied to Mistral-7B. In this paper, we focus on calibration in a complex reasoning yes-no QA dataset. Across five datasets, NASA consistently demonstrates improved calibra- tion. This indicates that NASA effectively mitigates model bias, resulting in better alignment between prediction confidence and accuracy."}, {"title": "7 Conclusion", "content": "We identify a critical issue where large language\nmodels exhibit negative bias in binary decision\ntasks requiring complex reasoning. To address this issue, we propose a negative attention score for for- mulation and employ it to discover query-agnostic negative heads. By performing parameter-efficient tuning on these heads, we introduce the NASA method, which effectively mitigates the bias prob- lem. Our method not only enhances the perfor- mance of the model but also serves as a useful analytical framework from the perspective of inter- pretability."}, {"title": "Limitations", "content": "In this work, we focus on understanding the rela- tionship between negative biases exhibited in fine- tuned LLMs and attention heads. However, further research is still needed to comprehend the causes and mechanisms behind the occurrence of nega- tive biases in LLMs, and we anticipate that our observations and experimental results will lay the groundwork for future work. Additionally, we have adopted a scheme of fine-tuning a small number of attention heads individually. In future work, it may be possible to explore more time-efficient training methods. While this study focuses on understand- ing the characteristics of negative bias attention heads, future work could involve a more integrated research approach connecting various elements."}, {"title": "4.1 Probing Set Construction", "content": "Prompt II (Inquiring Parametric Knowledge)\nYou MUST answer shortly the given question based on your knowledge. Question: {question} Answer:\nThis prompt effectively selects samples that belong to the model's parametric knowledge while minimizing the influence of the model's negative bias. We assume that samples meet the condition if the model returns the correct answers for the prompt, and we use them as the probing set. We utilize GPT4 to verify whether the prediction matches to label."}, {"title": "G Ablation Study", "content": "NASA updates the parameters related to the inference of attention weights of negative attention heads (i.e., query and key projection weights). To demonstrate the effectiveness of NASA, we conduct two ablation studies: i) updating only the query projection weight and ii) random attention head tuning."}, {"title": "G.1 Freezing Key Projection Weight", "content": "We perform fine-tuning using the same pipeline as before, except that we freeze the key projection weight. Note that strategies such as early stopping and halting, as well as other hyperparameters, re- main the same as those used in the NASA settings."}]}