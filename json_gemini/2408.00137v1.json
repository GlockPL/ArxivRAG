{"title": "Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment", "authors": ["Sangwon Yu", "Jongyoon Song", "Bongkyu Hwang", "Hoyoung Kang", "Sooah Cho", "Junhwa Choi", "Seongho Joe", "Taehee Lee", "Youngjune L. Gwon", "Sungroh Yoon"], "abstract": "A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities. Our codes are available at https://github.com/ysw1021/NASA.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in large language models (LLMs) have enabled complex reasoning tasks executed by understanding user instructions (Ouyang et al., 2022; Touvron et al., 2023; Achiam et al., 2023; Jiang et al., 2023). As the capabilities of LLMs have expanded rapidly, research has intensified to analyze their characteristics and inherent issues. One of the major issues is the generation of factually incorrect content, known as \u201challucination\", which significantly degrades the reliability of LLM-based services (Zhang et al., 2023; Xu et al., 2024; Huang et al., 2023). The hallucination problem in LLMs can be attributed to factors such as parametric knowledge, overconfidence, and biases (Zhang et al., 2023). Studies have been conducted to understand the decision-making mechanisms in LLMs from a parameter perspective. For instance, the \"logit lens\" technique allows for the interpretation of the model's reasoning process along the layers, exploiting the hidden representations from intermediate layers (Belrose et al., 2023; Ferrando et al., 2023). This technique has been utilized to analyze model characteristics in various scenarios such as in-context learning. On the other hand, Yuan et al. (2024) focus on analyzing hallucination phenomenon based on attention heads. However, the causes and mechanisms of hallucination in LLMs vary by type of task, indicating that extensive further study is still necessary.\nIn this paper, we aim to identify biases that emerge when LLMs respond to questions requiring a binary decision such as affirmation or negation. Specifically, in yes-no question-answering (QA) tasks that demand complex reasoning, such as mathematical or logical reasoning, we observe a negative bias in LLMs. This negative bias, which implies a discrepancy between the model's reasoning ability and accuracy, potentially decreases the reliability of model predictions. From the rationale that the model allocates higher attention to the negative candidates among the binary answer options presented in the user prompt, we propose a negative attention score (NAS) to systematically probe these negatively biased attention heads in large language models (LLMs). Based on NAS, we detect attention heads that attend to tokens associated with negation in an instruction. Our findings also confirm the existence of attention heads that are predominantly involved in manifesting this negative bias.\nTo address the identified attention heads, we introduce a parameter-efficient fine-tuning technique named NAS alignment (NASA). In NASA, the model is incrementally fine-tuned, starting with the most biased attention head. The method includes periodic monitoring of NAS which is used for automatic scheduling of early stopping and update cancellation. Experimental results demonstrate that our method significantly reduces negative bias while maintaining general reasoning performance. Additionally, we observe an improvement in calibration for yes-no QA tasks in LLMs.\nOur contributions can be summarized as follows:\n\u2022 We observe that LLMs exhibit a negative bias in binary decisions requiring complex reasoning and demonstrate its association with attention heads.\n\u2022 We propose NAS, a metric and framework that allows for systematic probing of attention heads involved in negative bias.\n\u2022 Based on NAS, we introduce NASA, a parameter-efficient fine-tuning technique that successfully addresses negative bias."}, {"title": "2 Related Work", "content": "LLMs are demonstrating capabilities not only in typical natural language processing tasks but also in tasks that require complex thought processes. In particular, the use of emergent abilities, such as the chain-of-thoughts, has significantly enhanced the performance of complex reasoning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Madaan et al., 2023). However, the increased complexity of tasks and pipelines has made the hallucination problem in complex reasoning tasks challenging to analyze and resolve, which remains an active area of research (Zhang et al., 2023). With the advent of benchmarks vulnerable to hallucinations, recent LLMs are laying the groundwork for analyzing the causes of LLM hallucinations based on substantial data. For instance, StrategyQA (Geva et al., 2021) and MuSiQue (Trivedi et al., 2022) involve queries and contexts that demand multi-step reasoning from LLMs, while GSM8k (Cobbe et al., 2021) involves queries that require mathematical reasoning skills.\nIn this paper, we aim to understand hallucinations from the perspective of model parameters, using the currently employed complex reasoning datasets. Specifically, we focus on binary decision tasks involving affirmation and negation to identify new characteristics of the model."}, {"title": "2.2 Analysis on Roles and Knowledge in Model Parameters", "content": "As the capacity and complexity of LLMs increase, the need to understand and interpret these models is becoming more crucial. In particular, there is active research into analyzing parametric knowledge and model behavior that influence the phenomenon of model hallucination. Meng et al. (2022) propose a framework to specify the location where the knowledge of the model is stored, while Belrose et al. (2023) and Yang et al. (2024) research to interpret the inference process of the model. Yuan et al. (2024) suggest a method to detect model parameters involved in the false premise hallucination phenomenon of LLMs and address this issue.\nSimilarly, we demonstrate that the model's negative bias is related to the attention heads. Specifically, we explore the existence and characteristics of query-agnostic negative bias in attention heads. Furthermore, we demonstrate that this can be significantly addressed through a parameter-efficient fine-tuning method."}, {"title": "3 Negative Bias in Large Language Models", "content": "A binary decision task, such as yes-no QA or answer verification, represents a major real-world scenario where users seek confirmation on whether their decisions regarding specific problems are correct. This acts as the basic block of the high-dimensional reasoning tasks that need step-by-step sub-tasks. Despite the importance of the binary decision task, unfortunately, many of the complex reasoning benchmark sets are not in the form of binary decisions. Therefore, in this section, we first transform existing complex reasoning tasks into yes-no binary decision tasks. We then conduct statistical observations and quantitative analyses on how various large language models perform in these binary decision scenarios."}, {"title": "3.1 Statistical Observation on Complex Reasoning Tasks", "content": ""}, {"title": "3.1.1 Setup for Binary Decision Task", "content": "Transformation to binary decision task In this work, we employ two techniques to transform general QA reasoning tasks into binary decision tasks. We modify each sample into a positive or negative sample where the answer is \u201cYes\u201d or \u201cNo\u201d, respectively. In this section, we focus on the case of short-answer QA datasets. We refer to Appendix C for the case of multiple choice datasets.\nFor the positive sample, we perform a rule-based transformation on general queries by appending a confirmatory query. For example, the query \"What is 1+1?\" is transformed into \"What is 1+1? Is the answer 2?\" This approach is straightforward but is limited to cases where the correct answer is \"Yes\". Formally, given the question and its label, we transform the prompt for the binary decision task whose target answer is \u201cYes\u201d:\nYou are given a question and you MUST answer Yes or No. Question: {question} Is the answer {label}? Answer:\nTo transform into the negative sample, a wrong answer is necessary based on the query and answer which will be placed in label in Prompt I. For this, we obtain suitable incorrect answers by prompting GPT-4. Subsequently, we use the aforementioned rule-based method to convert the original QA samples into binary decision samples where the answer is \"No\". Details of GPT-4 prompts can be found in Appendix C."}, {"title": "Datasets and models", "content": "We use StrategyQA, MuSiQue, subsets of MetaMATH (GSM8k-Rephrased and MATH-Rephrased; Yu et al., 2023), and AR-LSAT for evaluation, covering three domain tasks: multi-hop open-domain QA, mathematical reasoning, and logical reasoning. StrategyQA is a yes-no QA dataset, MuSiQue and the rephrased versions of GSM8k and MATH are short-answer QA datasets, and AR-LSAT is a multiple-choice dataset. Examples of each dataset can be found in Table 12.\nTo assess reasoning capabilities in binary decision tasks, we convert these datasets into binary decision sets using the aforementioned techniques, except for StrategyQA. Specifically, AR-LSAT is converted to the straightforward yes-no QA formats, while the others are rephrased for answer verification. Details on the conversion to binary decision datasets can be found in Appendix C.\nWe sample a portion of the entire dataset, and detailed statistics can be found in Appendix A. For the search space of LLMs, we explore the dynamics of LLaMA3-8B-Instruct, Mistral-7B-Instruct-v0.3,"}, {"title": "3.1.2 Observation of Negative Bias", "content": "Fig 1 presents the precision and recall results for models across various datasets. In most cases, a significant drop in recall compared to precision is observed. As defined, high precision indicates that a high frequency of the model's positive responses (TP+FP) are positive cases (TP). High recall means that among all the positive cases (TP+FN), the model frequently responds correctly with positive answers (TP). Therefore, intuitively, the phenomenon where existing models exhibit high precision but low recall can be explained to that the model is overly cautious in outputting positive responses. Conversely, this implies that the model outputs negative responses indiscriminately, indicating that the trustworthiness of negative responses and positive responses is not balanced.\nFig 2 shows a histogram of the response confidence of the LLaMA3 model in mathematical and logical reasoning tasks. The X-axis denotes confidence for all lines. Each line represents FP (false positive), FN (false negative), TP (true positive), and TN (true negative). We observe that the overall frequency and confidence of negative responses, including FN and TN, are higher than those of positive responses, including FP and TP. This indicates that the model tends to output negative responses more frequently and with greater confidence.\nIn summary, through statistical observation, we find that LLMs typically show 1) over-cautiousness to the positive response, and 2) more frequent and more confident negative response. Thus, it is evident that large language models possess a bias towards negative responses in the binary decision-making process of complex reasoning tasks. Detailed case study of the negative responses about the negative bias can be found in Appendix E."}, {"title": "3.2 Formulation of Negative Bias", "content": ""}, {"title": "3.2.1 NAS: Negative Attention Score", "content": "In this section, we formulate the previously discovered negative bias from the perspective of model intrinsic properties. Specifically, we focus on the internal attention patterns of the model. Generally, to obtain an answer for a binary decision, a user provides answer candidates like \u201cYes\u201d or \u201cNo\u201d in the form of instructions to the model before posing the query prompt. The model follows these instructions and responds to the given query in the manner provided in the instructions. During this response process, the model attends to the given instructions, and due to the operational characteristics of attention-based models, the candidate with the larger attention weight generally appears in the response. In this context, the negative bias of the LLM can be seen as originating from assigning greater attention weight to negative answer candidates during the reasoning process. Fig 3 illustrates an example of such a \"negative head\". To validate this rationale, we define the following Negative Attention Score (NAS) using the attention weights applied to negative answer candidate tokens like \"No\", and positive answer candidate tokens like \"Yes\", observed in a attention head.\nFormally, let $L_p$ be the length of the prompt $x$, $t_{\\text{yes}}$ and $t_{\\text{No}}$ the positions of the \u201cYes\u201d and \u201cNo\u201d tokens within the instruction, and $L_I$ the length of the instruction. For the attention weight inferred by the $h$th attention head in the $l$th layer, denoted as $A^{l,h} \\in \\mathbb{R}^{L_P \\times L_P}$, the NAS is defined as:\n$\\text{NAS}^{l,h} = \\sum_{i=L_I}^{L_P} (A_{i,t_{\\text{yes}}}^{l,h} + A_{i,t_{\\text{No}}}^{l,h}) * \\log (\\frac{A_{i,t_{\\text{No}}}^{l,h} / \\sum_{i=L_I}^{L_P} A_{i,t_{\\text{No}}}^{l,h}}{A_{i,t_{\\text{yes}}}^{l,h} / \\sum_{i=L_I}^{L_P} A_{i,t_{\\text{yes}}}^{l,h}})$ (1)\nwhere we omit the input $x$ in the attention function for brevity. In the i-th row, each NAS term consists"}, {"title": "3.2.2 Empirical Studies", "content": "To demonstrate that the previously defined NAS is an effective indicator of negative bias, we measure the correlation between NAS and the negative confidence observed in model responses. For this measurement, we compile a test set consisting of a total of 1,500 samples, drawing 500 samples each from StrategyQA, rephrased GSM8K, and AR-LSAT. Table 1 shows the Pearson correlation and Spearman's rank correlation coefficient measured between NAS and negative response confidence for each model. In all cases, we confirm that NAS indeed has a positive correlation with negative confidence. This validates that NAS is an effective indicator of a model's negative bias.\nAdditionally, we conduct a study on negative heads defined through the Negative Attention Score (NAS). For the QA dataset composed of three different domains that we previously created, we explore negative attention heads in three cases based on each domain subset. We designate as negative attention heads for a domain subset those heads among the top 200 heads with the highest single head NAS per sample that appear commonly in over 90% of the subset samples. Keeping the model type constant, we extract such negative heads from each subset and then measure the extent of their overlap. The overlapping rates are displayed in Table 2, and surprisingly, we find that negative heads extracted through different domain subsets significantly overlap. This indicates the existence of query-agnostic negative attention heads that represent a common underlying cause of the model's negative bias."}, {"title": "4 NASA: Negative Attention Score Alignment", "content": ""}, {"title": "4.1 Probing Set Construction", "content": "Motivated by previous observations, we aim to quantify the negative bias of LLMs from the perspective of attention heads. To observe the negative bias in attention heads, we start by constructing a negative attention head probing set. To construct the probing set, we first select samples that can be answered by the model's parametric knowledge then convert them into binary decision making format using Prompt I. Note that in the probing set, the label is uniformly set to \"Yes\" to effectively reveal the model's negative bias.\nWe suppose that the probing samples must meet the condition: the question should be answerable using the model's parametric knowledge. To determine whether the question in the sample meets the condition, we construct the prompt as follows:\nYou MUST answer shortly the given question based on your knowledge. Question: {question} Answer:\nThis prompt effectively selects samples that belong to the model's parametric knowledge while minimizing the influence of the model's negative bias. We assume that samples meet the condition if the model returns the correct answers for the prompt, and we use them as the probing set. We utilize GPT-4 to verify whether the prediction matches to label. A detailed explanation of the parametric sample selection can be found in Appendix D."}, {"title": "4.2 Negative Attention Head Probing", "content": "We probe negatively biased attention heads using the constructed probing set. First, we measure the NAS of each attention head when the prompt from each probing sample is input into the model. We then identify the top $k'$ attention heads with the highest NAS for each probing sample. Among these extracted heads, we select those that are consistently extracted in over 90% of the probing set samples. Finally, the top k attention heads, sorted by their single head NAS values, are targets for further addressing. In this paper, we set k to 100.\nAt this point, we further categorize the probing set into cases where the model correctly answers (i.e., the true positive set) and cases where it does not (i.e., the false negative set). During the fine-tuning phase of the model (See Section 4.3), we use the false negative set as the training set. This approach is aimed at inducing the model to generate positive answers for samples that it incorrectly fails to answer due to a negative bias in parametric knowledge. Meanwhile, the true positive set is used to determine the criteria for early halting of the fine-tuning process. In other words, during the training process aimed at reducing the model NAS, if the NAS on the validation set (i.e., false negative subset) falls below the minimum model NAS of true positive set, training is halted to minimize the bias towards positives."}, {"title": "4.3 Head-wise Incremental Tuning", "content": "To address the identified attention heads as specified in Section 4.2, we propose an additional supervised fine-tuning method. Our fine-tuning method is parameter efficient as it focuses only on fine-tuning the query and key projection weights of the selected negative bias attention heads."}, {"title": "4.3.1 Dataset", "content": "We utilize the data from the attention head probing set for fine-tuning (i.e., subset of rephrased HotpotQA). Because the model already possesses parametric knowledge for answering questions in the probing set, we construct the fine-tuning data to encourage the model to generate positive answers in response to Prompt I. As the queries in the probing set contain parametric knowledge, we do not provide contexts containing related facts to the model."}, {"title": "4.3.2 Training Objective", "content": "Similar to a standard supervised fine-tuning objective, we target only the answer token (e.g., \u201cYes\u201d) for training. Since our dataset contains only short answers, training the model to generate the special token that indicates the end of a sequence could introduce a bias towards short responses. To prevent this side effect, we only use the target answer token in our loss calculation."}, {"title": "4.3.3 Training Strategy", "content": "We propose to further fine-tune each attention head sequentially in order of decreasing NAS which is measured during the attention head probing stage. Specifically, we only train the query and key projection modules of the target attention head.\nSince we train with data where the target answer is positive, there is a risk of inducing a positive bias. Furthermore, updating a single attention head might change the NAS of another attention head. To prevent this, we set aside a portion of the dataset as a validation set and apply early stopping and update cancellation schemes for each attention head tuning, and early halting scheme for the whole fine-tuning process.\nStrategy of attention head tuning subprocess we set early stopping criteria based on the model NAS and single head NAS. For the former criteria, we check for a decrease in the value each epoch and halt training if there is an increase. For the latter one, we verify that i) the value decreases each epoch and ii) remains above a specific threshold. Training is also stopped if either of the two conditions is not met.\nAfter fine-tuning each attention head, we calculate the difference between the single NAS and the whole NAS before and after training the attention head. If one or both of these values have increased compared to before training, we revert the parameter update of that attention head. This is to exclude modules that reinforce the negative bias of other attention heads from the training target.\nStrategy of the whole tuning process In Section 4.2, we measure the NAS of each sample within the true positive probing set to establish the minimum value as the early halting threshold, \u03c4. We hypothesize that if the NAS value in a false negative sample is lower than in a true positive, it may induce a positive bias. After training each negative bias attention head, we measure the NAS across all attention heads in the validation set. If this value falls below T, training is halted to prevent falling into local optima."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experimental Setup", "content": "We conduct experiments using LLaMA3, Mistral, Gemma, and Qwen2. We use those instruct models before further NASA fine-tuning as baselines. We use LLaMA-Factory (Zheng et al., 2024)\u00b9 and HuggingFace Transformers (Wolf et al., 2020)\u00b2 for all the experiments. The models are evaluated using yes-no QA datasets, which are described in Section 3. For these datasets, we measure accuracy, precision, recall, F1, and the sum of NAS values across all attention heads in the attention head probing set. For early stopping, we halt training if two NAS values (refer to the last paragraph in Section 4.3) cease to decrease or if the NAS for the target attention head on the validation set drops below 0.5. The initial learning rate is set to 1e-6, the batch size to 32, and the maximum number of epochs to 30. We reference the Alpaca repository\u00b3, setting the weight decay to 0 and the warmup ratio to 0.03."}, {"title": "5.2 Main Results", "content": "In this section, we examine whether the negative bias observed and analyzed in Section 3 has been effectively mitigated by NASA. Tables 3 and 4 present the performance of our method across various binary decision tasks. Generally, we observe an improvement in accuracy and a significant reduction in the gap between precision and recall. As mentioned in Section 3, high precision coupled with low recall indicates that the model is excessively cautious in providing positive responses. This, in turn, leads to a degradation in the trustworthiness of negative responses.\nFrom this perspective, reducing the gap between precision and recall while maintaining accuracy at an equivalent or higher level is a way to mitigate negative bias, balancing the trustworthiness of the model's responses, and ultimately improving the model. Additionally, based on the improvement in the F1 score, which is the harmonic mean of precision and recall, it is demonstrated that the balance between precision and recall has been achieved in a positive direction of improving the model capability for reasoning. Regarding the NAS metric, we observe that our method effectively reduces NAS to an appropriate level. In Appendix G, we demonstrate that the components of our method contribute effectively to consistently tackling NAS during the training process.\nMeanwhile, for the mathematical reasoning task, recall surpasses precision. This indicates that this task is somewhat more sensitive compared to other tasks and suggests that additional regularization, considering the task's sensitivity, could enhance the effectiveness of our method. Future research based on our work could delve deeper into this aspect.\nMoreover, the slight decrease in precision might superficially appear as a performance decline. However, it can be interpreted as an enhancement of the model's trustworthiness by adjusting the alignment between the model's actual knowledge and its responses. This means that precision might slightly decrease because some samples that were previously incorrectly categorized as true negatives due to negative bias might shift to false positives. For instance, even if the model makes incorrect inferences or remains uncertain about a question, in a binary decision scenario, negative bias could lead to it being classified as a true negative. A detailed analysis of the shifts in negative responses due to NASA can be found in Appendix F."}, {"title": "6 Analysis", "content": ""}, {"title": "6.1 Preservation of General Reasoning Abilities", "content": "In this section, we evaluate the performance of a large language model with NASA applied to general QA reasoning tasks. We assess the general reasoning performance on benchmarks such as MuSiQue, GSM8K, and AR-LSAT using the LLaMA3-8B model both with and without NASA. After verifying whether each model-generated prediction includes an answer in these benchmarks, we utilize GPT-4 to double-check whether the predictions that contain answers indeed correspond semantically to the actual answers. To generate predictions from the model, we utilize the previously mentioned Prompt II (Inquiring Parametric Knowledge). The content of the GPT-4 prompt for prediction verification can be found in Table 13.\nAs shown in Table 5, the model with NASA applied demonstrates improved results, showing enhanced or maintained general reasoning abilities while effectively tackling negative bias. Since negative bias is a task-specific factor contributing to hallucination in binary decision tasks, its influence on general reasoning QA, like short-answer QA, is relatively low compared to other factors contributing to hallucination such as parametric knowledge. Therefore, improving reasoning ability in binary decision tasks does not necessarily enhance reasoning capabilities in general reasoning QA. This means that addressing bias in binary decision tasks while maintaining performance in general reasoning QA can be considered an advancement in the model's overall reasoning capability, not overfitted to the binary decision task."}, {"title": "6.2 Generalization to Universal Binary Decision", "content": "We analyze the generalization ability for binary decisions by replacing the positive and negative tokens used in our fine-tuning set. Our fine-tuning set consists of \"Yes\" and \"No\", and in this experiment, we test with the tokens \"True\" and \"False\", as well as \"Correct\u201d and \u201cWrong\". Table 6 presents the results of experiments with LLaMA3-8B on various types of complex reasoning tasks. As shown in the table, ours fine-tuned using NASA achieves semantic-level generalization without overfitting to specific tokens."}, {"title": "6.3 Transferability across Various Instructions", "content": "In this work, we maintain a fixed instruction for binary decision-making in yes-no QA throughout the application of our methodology. In this section, we evaluate the robustness of our method when presented with instructions in the inference stage that are different forms of content. We assess the binary decision performance using the LLaMA3-8B model with and without NASA on benchmarks: MuSiQue, GSM8K, and AR-LSAT for paraphrased instruction types A and B. As shown in Table 7, our methodology with NASA applied still demonstrates superior performance compared to the baseline in both prompt types A and B. This supports our claim of instruction robustness for our methodology. Note that datasets in Table 6 and 7 are rephrased version. Fig 7 in Appendix B shows the content of instructions."}, {"title": "6.4 Enhanced Model Calibration", "content": "As Fig 2 illustrates, the negative bias of LLMs is also associated with the model's confidence in its predictions. Research on the calibration between prediction confidence and accuracy is necessary to enhance the trustworthiness of LLMs (Kadavath et al., 2022).\nTable 8 shows the expected calibration error (He et al., 2022) when NASA is applied to Mistral-7B. In this paper, we focus on calibration in a complex reasoning yes-no QA dataset. Across five datasets, NASA consistently demonstrates improved calibration. This indicates that NASA effectively mitigates model bias, resulting in better alignment between prediction confidence and accuracy."}, {"title": "7 Conclusion", "content": "We identify a critical issue where large language models exhibit negative bias in binary decision tasks requiring complex reasoning. To address this issue, we propose a negative attention score for formulation and employ it to discover query-agnostic negative heads. By performing parameter-efficient tuning on these heads, we introduce the NASA method, which effectively mitigates the bias problem. Our method not only enhances the performance of the model but also serves as a useful analytical framework from the perspective of interpretability."}, {"title": "Limitations", "content": "In this work, we focus on understanding the relationship between negative biases exhibited in fine-tuned LLMs and attention heads. However, further research is still needed to comprehend the causes and mechanisms behind the occurrence of negative biases in LLMs, and we anticipate that our observations and experimental results will lay the groundwork for future work. Additionally, we have adopted a scheme of fine-tuning a small number of attention heads individually. In future work, it may be possible to explore more time-efficient training methods. While this study focuses on understanding the characteristics of negative bias attention heads, future work could involve a more integrated research approach connecting various elements."}, {"title": "K Licenses", "content": "StrategyQA, MuSiQue, MetaMATH, and AR-LSAT are under the license of MIT license, CC-BY-4.0 license, MIT License, and MIT license, respectively. LLaMA 3, Mistral, Gemma, Qwen, and GPT-4 are under the license of META LLAMA 3 COMMUNITY LICENSE AGREEMENT, Apache License 2.0, Gemma, Apache License 2.0, and OpenAI, respectively."}, {"title": "LUsage of AI Writing Assistance", "content": "This paper received linguistic assistance from the AI assistant GPT-4, which provided services including paraphrasing, spell-checking, and refinement of the original content authored. No additional help was utilized beyond this support."}, {"title": "D Details of Parametric Sample Selection", "content": "As mentioned in Section 4.1, we construct the probing and fine-tuning sets using the HotpotQA dataset, which is a short-answer QA dataset. From the HotpotQA dataset, we use samples that can be correctly answered solely with parametric knowledge, without any context, as our probing samples. During the probing sample selection process, we first input the question into the target model to extract a prediction. Since the HotpotQA dataset can contain various forms of answers, we use GPT-4 to determine if the prediction corresponds to the label. Specifically, we input the prompt from Table 13 into GPT-4 and only include samples in the probing set if the response is \u201cYes\u201d."}, {"title": "E Case Study for Negative Responses", "content": "In this section, we classify negative responses based on the following criteria:\nFirstly, in a general QA reasoning task, a model's response to a specific question can be broadly categorized into below two cases:\n\u2022 Deterministic Case (Det): The model provides a certain answer regardless of its correctness.\n\u2022 Non-Deterministic Case (Non-Det): The model fails to provide a certain answer, indicating that the question is unanswerable due to insufficient information from the given context or its knowledge base.\nAlso, the deterministic case can be further classified based on whether the certain answer provided by the model is correct (True) or incorrect (False). Considering this, a model's response to a general question requiring a short answer can be categorized into the following three types:\n\u2022 True Deterministic (Det-T): The model provides a correct answer.\n\u2022 False Deterministic (Det-F): The model provides an incorrect answer.\n\u2022 Non-Deterministic (Non-Det): The model indicates that the question is unanswerable.\nAdditionally, beyond the extrinsic responses of the model, we can classify the model's responses based on the intrinsic factor of confidence. Specifically, if the model exhibits low confidence in the response, it can be considered closer to not knowing the answer to the question. We calculate the entropy of the output distribution for the first token of the model's response and use the median value of these entropies across all responses to classify them as high-confidence (low-entropy) or low-confidence (high-entropy). This classification is performed based on the original model's responses before applying NASA method.\nWe apply this classification to cases where the model gives a negative response in a binary decision task. Negative responses can be classified as true negative (TN) or false negative (FN), with our focus in this discussion on FN. In this, an FN response classified as high-confident Det-T is the case where the model knows the answer is correct but says \"no\", while an FN response classified as Non-Det is the case where the model doesn't know the correct answer and says \"no\". For high-confident Det-F responses, the model does not indicate unanswerability but rather holds incorrect knowledge. This issue falls outside the scope of our methodology, which focuses on handling negative bias in binary decision tasks rather than general knowledge-intensive tasks.\nAlthough it is clear that the case where the model knows the answer is correct but says \"no\" is the most problematic, we argue that in a binary decision task, the case where the model doesn't know the correct answer and says \"no\" can be related to the issue of negative bias. In a binary decision task where the model must answer \"Yes\" or \"No,\" instances where the model is uncertain about the correct answer should result in an equal frequency of positive (yes) and negative (no) responses. However, our analysis indicates that existing models tend to output negative responses more confidently and frequently (as shown in Figures 1 and 2 in the paper). This suggests that when the model is uncertain, it predominantly outputs negative responses, which negatively impacts the trustworthiness of these negative responses. Therefore, for models exhibiting such negative bias, Non-Det and low-confident responses should also be partially adjusted."}, {"title": "F Analysis of Negative Responses Shifting in NASA", "content": "In this section, we analyze the ratios of samples which are originally responded to as negative by the original model and subsequently changed to positive by the NASA model. We categorize the ratio cases by each classification group explained in section E.\nTable 15 illustrates the ratios of samples shifted from FN to TP. This demonstrates that the Det-T corresponding to the case where the model knows the answer is correct but says \"no\" generally has the highest rate of change. This finding indicates that our methodology does not indiscriminately add positive bias to the model but rather enhances the model's binary decision reasoning ability effectively.\nTable 16 illustrates the ratios of samples shifted from TN to FP. As discussed in the section on main results, some samples that are incorrectly categorized as TN due to negative bias might shift to FP (Det-F), and this case does not correspond to the model degradation. Actually, among the TN samples, those classified as Deterministic-T with high confidence exhibit a lower shift ratio compared to their Deterministic-F counterparts. This indicates that the model tends to maintain predictions for samples that it can accurately classify. In conclusion, it can be interpreted that NASA has improved reasoning capability in binary decision tasks while maintaining general reasoning capability."}, {"title": "G Ablation Study", "content": "NASA updates the parameters related to the inference of attention weights of negative attention heads (i.e., query and key projection weights). To demonstrate the effectiveness of NASA, we conduct two ablation studies: i) updating only the query projection weight and ii) random attention head tuning."}, {"title": "G.1 Freezing Key Projection Weight", "content": "We perform fine-tuning using the same pipeline as before, except that we freeze the key projection weight. Note that strategies such as early stopping and halting, as well as other"}]}