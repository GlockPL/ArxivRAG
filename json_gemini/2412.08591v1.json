{"title": "RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation", "authors": ["Mingfei Han", "Liang Ma", "Kamila Zhumakhanova", "Ekaterina Radionova", "Jingyi Zhang", "Xiaojun Chang", "Xiaodan Liang", "Ivan Laptev"], "abstract": "Vision-and-Language Navigation (VLN) suffers from the limited diversity and scale of training data, primarily constrained by the manual curation of existing simulators. To address this, we introduce RoomTour3D, a video-instruction dataset derived from web-based room tour videos that capture real-world indoor spaces and human walking demonstrations. Unlike existing VLN datasets, RoomTour3D leverages the scale and diversity of online videos to generate open-ended human walking trajectories and open-world navigable instructions. To compensate for the lack of navigation data in online videos, we perform 3D reconstruction and obtain 3D trajectories of walking paths augmented with additional information on the room types, object locations and 3D shape of surrounding scenes. Our dataset includes ~100K open-ended description-enriched trajectories with ~200K instructions, and 17K action-enriched trajectories from 1847 room tour environments. We demonstrate experimentally that RoomTour3D enables significant improvements across multiple VLN tasks including CVDN, SOON, R2R, and REVERIE. Moreover, RoomTour3D facilitates the development of trainable zero-shot VLN agents, showcasing the potential and challenges of advancing towards open-world navigation.", "sections": [{"title": "1. Introduction", "content": "Over the past years, Vision-and-Language Navigation (VLN) [2, 21, 24, 32, 42, 54] has largely relied on human-designed simulators and annotated trajectories. R2R [2] established a benchmark for language-guided navigation in simulated indoor settings, while CVDN [51], REVERIE [42], and SOON [66] expanded VLN to dialogue-based and object-focused tasks. However, these manually curated simulations lack scene diversity and fail to capture real-world complexity.\nTo address limited diversity, recent methods propose the use of richer and more varied training data. AirBERT [15] combines discrete Airbnb images for panoramic views, which lack consistency and naturalistic context of an indoor scene. ScaleVLN [55] utilizes laboriously curated 3D scenes [45, 58], but suffers from reconstruction quality and scalability. More recently, YTB-VLN [34] attempts to use video frames to compose panoramic views and organize instructions with predefined templates, yet overlooks object variety and geometry structure. NaVid [60] constructs sequential single-view trajectories from MatterPort3D [4] and R2R [2] annotations, paired with general video data to train a sim-to-real agent. None of these approaches simultaneously achieves scalability in scene diversity, openness in object variety, or comprehensive geo-perception in spatial representations, each of which is critical to training effective and open-world navigation agents.\nTo address the challenge, we introduce RoomTour3D, a novel dataset that provides a geometry-aware, spatially enriched training environment for VLN agents. Built upon easily accessible room tour videos from the Internet, RoomTour3D captures continuous movement through real estates with a hand-held camera from a first-person perspective. Each frame presents a realistic, agent-centric view and showcases a rich array of indoor items. The continuous flow of these frames captures multiple views of the environment, presenting diverse room layouts and inherently embedding the geometric properties of the spaces. To unleash the power of these videos, we propose an automatic and extendable pipeline to obtain open-ended geometry-aware human walking trajectories, spatially contextualized textual instructions using open vocabularies.\nTo better model the navigation scenario, we take advantage of the continuous walk-through trajectories and densely sample frames from room tour videos. Then, we use COLMAP [46, 47] to reconstruct 3D scenes of real-estates to obtain the geometric information. With access to camera locations and orientations, we sample \"decision-"}, {"title": "2. Related Work", "content": "2.1. Vision-and-Language Navigation\nLearning to navigate unseen indoor environments with natural language instructions is vital for enabling embodied agents to assist humans. Various scenarios have been explored, including fine-grained instruction following (R2R [2]) and dialogue-based navigation (CVDN [51]), object localization from instructions (SOON [66], REVERIE [42]), and embodied question answering through active 3D exploration [10, 56]. While substantial work focuses on task-specific models [1, 6, 7, 14, 16-20, 25, 30, 37, 39, 43, 44, 52, 62, 66], they often lack generalization across tasks. In this context, NaviLLM [63] introduces an embodied generalist model that simultaneously addresses multiple tasks through a single framework, demonstrating strong generalization ability.\n2.2. Data-Centric Methods for VLN\nThe scarcity of VLN training data remains a critical issue and results in poor generalization of VLN agents to unseen environments. Most of existing VLN datasets such as R2R [2], RxR [24], CVDN [51] and SOON [66] are produced in simulators, which constrains data scalability due to the high labor costs involved. To tackle the problem, data augmentation [12, 13, 23, 27, 28, 35, 49] and self-exploration in simulator environments [31, 53] have been investigated.\nVLN-BERT [17] and AirBERT [15] attempt to use web-based image-caption pairs for pre-training, however, resulting trajectories often fail to mimic realistic navigation. Similarly, automatic dataset generation pipelines [8, 22], including ScaleVLN [55], rely on manually curated 3D scenes or synthetic environments, which are costly to produce and lack the photorealism needed for robust real-world generalization. PanoGen [26] enhances VLN training by generating diverse text-conditioned panoramic environments using text-to-image diffusion models and recursive outpainting. While this approach addresses the scarcity of training environments, it relies on synthetic panoramas and may not generalize well to real environments. YTB-VLN [34] advances scalability by leveraging YouTube room tour videos to generate path-instruction pairs but omits explicit path geometry, essential for robotic navigation.\nIn our work, we address the limitations by designing RoomTour3D with properties: (i) free-form and open-vocabulary path annotations instead of template-based instructions, (ii) extraction of open-ended trajectories from sequential video clips, and (iii) inclusion of turning points and spatially close frames as navigable candidate actions, moving beyond panoramic nodes. Furthermore, our approach integrates 3D reconstruction of indoor videos to retrieve trajectory geometry and employs an LLM to generate detailed, object-aware instructions with enhanced spatial understanding.\n2.3. Zero-shot Navigation\nGiven the substantial semantic variations in complex real-world scenarios, fully-supervised VLN models often struggle to generalize across diverse navigation scenes. Zero-shot VLN has thus gained attention as it eliminates prior knowledge of environments and instructions, effectively mitigating environmental biases.\nCommercial model based methods utilize advanced LLMs and robust frameworks for seamless solutions. MapGPT [5] incorporates a map-based prompting system with global spatial reasoning and adaptive path planning. DiscussNav [39] employs a multi-expert framework where LLMs specialize in subtasks like instruction analysis and vision perception. NavGPT [64] focuses on explicit reasoning by combining commonsense reasoning with visual observations. As to non-commercial methods, LangNav [41] uses language as the primary perceptual space, while Nav-CoT [33] introduces parameter-efficient training to allow LLMs to autonomously reason and act.\nWe show that usage of our action-enriched data for navigation tuning results in superior zero-shot performance over all non-commercial methods and reaches comparable results to commercial approaches based on GPT-3.5."}, {"title": "3. RoomTour3D", "content": "In this section, we present the automatic data curation pipeline of RoomTour3D. We detail the process of annotations, from sampling open-ended human walking trajectories to generating corresponding descriptions with open-world object variety and spatial awareness. Enabled by reconstructed 3D scenes, we further sample navigable trajectories with actions. The overall pipeline of our data generation is illustrated in Figure 1. Please refer to Appendix A for details about video collection.\n3.1. Description-Enriched Trajectories\nIn this subsection, we detail the process of generating controllable descriptions for open-ended trajectories. We start by generating human-walking trajectories by uniformly sampling frames at a rate of one frame every two seconds, which aligns with the average human walking speed of 1.42 meters per second [57], typically slower in indoor environments. Subsequently, to annotate these trajectories, as shown in Figure 2, we employ expert models such as BLIP-2 [29], RAM [61], Grounding-DINO [38], and Depth-Anything [59] to gather extensive information on object variety, spatial positions, and depth measurements. Finally, we integrate this information into GPT4 [40] to generate detailed and coherent traje.\nObject Variety and Spatial Awareness. In order to harness object variety and enable spatial awareness, we compose three expert models and design a textual template, i.e., \"There is a object tag to the spatial position of current spot in relative distance\", to organize the multi-source information to ease GPT generation. Firstly, we used RAM [61] to annotate the object categories within the frames. Based on these category tags, we employed Grounding DINO [38] to locate the objects in the frames. Subsequently, we used Depth-Anything [59] to predict the depth maps corresponding to the frames.\nUsing this data, we identify the spatial locations and distances of objects relative to the current camera position. By analyzing object bounding box center positions and depth map locations, we can generate frame captions, as illustrated in Figure 2. Finally, objects in different frames can be easily correlated and capture the progression across different frames. More details in the spatial awareness data generation are provided in Appendix C.\nRoom Location Annotation in Videos. To determine the camera of each frame, w.r.t. the room category, we used BLIP2 [29] in visual question-answering mode, posing the question, \"Which room am I in?\" A predefined list of 16 common room types (e.g., bedroom, bathroom, kitchen) was used as possible answers. This list was curated by analyzing 10 randomly sampled long videos, using BLIP2 in generative mode to identify and rank the most frequently mentioned room types. For frame-level predictions, we switched to BLIP2's discriminative mode and applied temporal smoothing to denoise outputs.\nWe validated this approach by manually annotating 50 video clips, achieving an accuracy of 85%. The use of BLIP2 leverages its open-world knowledge, while limiting room types to 16 categories for discriminative selection simplifies outputs and reduces ambiguity. Any loss in open-vocabulary flexibility is addressed during GPT-based trajectory summarization.\nControllable Instruction Generation. To generate descriptions that accurately capture human-walking trajectories and reflect the environment, we integrate frame-level room locations with frame captions composed of object descriptions. We then employ GPT-4-Turbo [40] for controllable instruction generation, leveraging the multi-source information contained in the composed captions. As depicted in Figure 2, we organize the prompt using the \"Task instruction - In-context examples - Prediction\" scheme. This approach defines our instruction generation task as describing object progression along the moving trajectories, and includes two examples to ensure GPT produces instruction-style texts only. As shown in Figure 2 (c), captions of frames along the trajectory are embedded into the prompt and input into GPT.\n3.2. Action-Enriched Trajectories\n3D Environment Reconstruction. To obtain the geometric information of trajectories within RoomTour3D, we employ COLMAP [46, 47] for 3D reconstruction. This process allows us to infer the 3D layout of environments in the videos, providing a detailed geometric context for navigation tasks. Specifically, we sample the videos at 3 frames per second to balance accuracy and execution time. To further improve time efficiency, we split the videos into 100-second video clips with 10-second overlaps between adjacent clips and"}, {"title": "4. Vision-and-Language Navigation Model", "content": "In this section, we introduce a practice to use our data to train a generalist embodied agent. To start with, we first provide a concise introduction to the state-of-the-art VLN model, NaviLLM [63], which is an LLM-based navigation agent. Then, we introduce two tasks that are adapted for our RoomTour3D data, i.e., vision-instruction summarization task for pretraining and action-instruction navigation task for finetuning.\n4.1. Revisiting NaviLLM\nNaviLLM is a SOTA LLM-based model for embodied navigation, excelling on benchmarks like CVDN and SOON. It processes panoramic inputs by encoding environmental views and integrating them with navigation instructions. Specific tokens are defined for different types of inputs: <hist> for historical observations and <cand> for candidate views at each navigational step.\nDuring training, NaviLLM receives instructions and a sequence of candidate views. At each step, the candidate observations are input into the model along with the instructions. The model predicts the next action by selecting the appropriate view from the candidates, and this selected view is then cached as a <hist> token, updating the model's internal state for future decisions. At the last step of the navigation task, the model summarizes all <hist> tokens as a separate training task to ensure comprehensive understanding and retention of the navigational history. For testing, the model similarly uses historical observations, i.e., <hist> tokens, accumulated during navigation, and candidate views, i.e., <cand>, at each step to decide the next"}, {"title": "5. Tasks and Experiments", "content": "This section outlines our experimental setup and presents the results. Detailed implementation information can be found in Appendix E.\nDatasets. During pretraining, we follow practice from NaviLLM [63] and perform teacher-forcing training on the combined dataset from our video-instruction data from RoomTour3D, together with CVDN [51], SOON [66], R2R [2], REVERIE [42] and ScanQA [3], and augmented data from R2R and REVERIE. In the multi-task fine-tuning stage, we alternate between teacher forcing and student forcing on the combined data from our action-instruction data from RoomTour3D, together with CVDN, SOON, R2R, REVERIE, ScanQA and LLaVA-23k [36].\nTo evaluate the impact of our data on navigation agent training, we test on CVDN, SOON, R2R, and REVERIE. CVDN requires navigating towards a target by understanding dialog history, linking dialogue comprehension to actions. SOON tasks the agent with locating objects without bounding boxes, emphasizing semantic-visual alignment. R2R involves following step-by-step instructions, requiring dynamic progress tracking and precise alignment with navigational history. REVERIE focuses on localizing distant objects based on concise instructions, aided by ground truth bounding boxes at waypoints.\nEvaluation Metrics. For the navigation tasks, we follow the evaluation methodology from [2] using the following navigation metrics: Success Rate (SR), which measures whether the agent reaches the target location within a set distance threshold; Success Rate Weighted by Path Length (SPL), which is the SR adjusted by the ratio of the ground truth path length to the actual path traveled; Goal Progress (GP), the advancement in meters towards the goal. GP is utilized for the CVDN dataset, whereas SR and SPL are the metrics for other datasets.\n5.1. Comparison on Supervised Tasks\nAs shown in Table 1, we performed a one-time fine-tuning on the four tasks in a fully supervised manner. To begin, our experiments reiterate the superiority of multitask training over single-task training. Also, incorporating our RoomTour3D data into the pre-training process led to consistent improvements across all metrics on Val-U, achieving state-of-the-art results in the GP metric in the CVDN dataset.\nNotably, finetuning with our action-enriched data results in state-of-the-art performance on both Val-U and Test sets across SOON, R2R and REVERIE tasks. While the improvement on the CVDN and SOON datasets is modest, the most significant boost compared to the reproduced baseline is observed in R2R Val-U and REVERIE Val-U, with gains of approximately 5.7% and 6%, respectively. The improvement in R2R is largely driven by enhanced spatial awareness, stemming from the inclusion of proximity data, which helps the model better understand object distance and position. Similarly, gains in REVERIE are attributed to a combination of open-vocabulary tags, spatial awareness, and the addition of room type data, which encourages the model to infer the layout of environments, thereby boosting its spatial reasoning capabilities. Moreover, our use of open-ended instructions allows the model to adapt flexibly to diverse"}, {"title": "5.2. Comparison on Zero-shot Task", "content": "To further demonstrate the substantial indoor knowledge contained in our data and its effectiveness for embodied action and language instructions, we conduct zero-shot experiments on embodied action prediction, as shown in Table 3. We removed all action and geometric data from the training datasets and retrained NaviLLM with and without our RoomTour3D dataset. Without action prediction data, NaviLLM lacked the ability to learn effective navigable action selection. However, with the inclusion of our action-enriched trajectories, NaviLLM achieved a 14.33% SR and a 10.86% SPL, outperforming open-source models built on LLaMA-7B and reaching results comparable to NavGPT [64], which leverages GPT-3.5. These improvements validate the effectiveness of our 3D trajectories mined from room tour video reconstructions and emphasize the value of our action-enriched trajectories. This highlights the significant contribution of our dataset to advancing open-world navigation.\n5.3. Ablation study\nEffect of open-world semantics and spatial awareness. As shown in Table 2, we analyzed the impact of various information types on instruction generation. Adding object"}, {"title": "5.4. Data correctness verification", "content": "We evaluated the correctness of our automated data-generation pipeline by manually rating 100 randomly sampled trajectory descriptions on a 4-point relevance scale: 1 for \"totally irrelevant\", 2 for \"partially relevant\", 3 for \"mostly relevant\" and 4 for a \"perfect match\". The evaluation yielded an average score of 3.08, with 74% of descriptions rated as \"mostly relevant\" or \"perfect match,\" demonstrating the method's effectiveness in generating meaningful, visually aligned descriptions.\n5.5. Navigation Case Visualization\nAs shown in Figure 4, selecting the correct action is critical at specific decision points, such as when a left turn is required to follow the instruction accurately. In the example, at step 2, both the rooms to the left and right could satisfy the latter part of the instruction, \u201cpass the bed and go into the bathroom.\u201d However, the baseline method incorrectly chooses a right turn at the designated left-turn point, causing it to deviate from the intended path. Once this error occurs, even with scene graph history, the model struggles to realign with the correct trajectory. This challenge is particularly common in household environments, where bedroom layouts often appear similar. It further demonstrates the effectiveness of our data alignment in improving adherence to action-based instructions."}, {"title": "6. Conclusion", "content": "In this paper, we present RoomTour3D, a novel dataset automatically curated from room tour videos for VLN tasks. By leveraging the rich, sequential nature of video data and incorporating object variety and spatial awareness, we generate 200K navigation instructions and 17K action-enriched trajectories from 1847 room tour scenes. Additionally, we produce navigable trajectories from video frames and reconstructed 3D scenes, which significantly boost the performance and set new state-of-the-art results on the SOON and REVERIE benchmarks. This approach also enables the development of a trainable zero-shot navigation agent, demonstrating the effectiveness and scalability of RoomTour3D in advancing VLN research."}, {"title": "A. Room Tour Video Collection", "content": "To enable more diversity for indoor scenes, we leveraged the rich variety and volume of room tour videos available on YouTube. These videos, recorded with hand-held cameras from a first-person perspective, offer a realistic and dynamic view of indoor environments. We curated a dataset from 1847 YouTube room tour videos, in total 243 hours. Our data collection approach builds on the video list from YTB-VLN [34], which we further filtered and expanded to enhance diversity and quality.\nTo ensure high-quality data, we prioritize continuous videos with least transitions, such as human interviews or abrupt cutting into close-ups, for better 3D reconstruction. We applied a title-description-based filtering process by using GPT-4 [40] and excluded videos shorter than three minutes. Additionally, we detected abrupt video transitions, retaining videos with at least nine continuous shots occupying over 80% of the video duration. We further extended our dataset by continuously updating high-quality channels (e.g., NavaRealtyGroup, Open House 24, Sona Visual) with new videos, resulting in our current 1847 room tour scenes.\nTo process this data, we spatially downscale the resolution to shorter side 360 and temporally downsample the frame rate to 3 frames per second. All the following processing are performed on this downsampled data."}, {"title": "B. Navigable points generation", "content": "To inject open-world knowledge from room tour videos into navigation agents, we propose navigating agents using video frames. Each frame in a human walking demonstration can be treated as having two next actions: move forward or stop. However, at significant view-change points instances of distinct view shifts within a close radius we sample frames with varied orientations as candidate actions to enhance the agent's training. Unlike YTB-VLN [34], which composes panoramic images at room nodes, our approach involves taking every significant view-change point and its neighboring frames that meet specific criteria as candidate actions.\nFirst, we detect significant view-change points along person's trajectory. By reconstructing the 3D scene, we can determine the camera orientation difference and distance between frames. There are instances where the person may revisit a nearly identical location, resulting in varied views within almost the same spatial region. Additionally, turning points with notable view changes in close proximity are essential to capture. Identifying these view-change points is useful for producing diversified navigable action data, especially when panorama images are not available.\nTo find these points, for each point along the trajectory we calculated pairwise cosine similarity. We then applied a threshold of 45 degrees to retain only frames that demonstrate a substantial change in view. Afterwards, non-maximum suppression is performed along the trajectory to isolate local maxima in angular change to highlight the most significant view changes.\nTo account for the points that are close in proximity, but have different views due to an intersection in the walking trajectory, we performed DBSCAN clustering [11] of the points that were retained after Non-Maximum Suppression. This clustering step ensures a diverse set of navigable actions is maintained, even without the availability of panoramic images.\nFinally, as shown in Figure 5, to extract varied navigable action candidates, we post-processed the clusters by identifying the distinct walking paths of the person within each cluster. In cases where paths intersect, the cluster may encompass two separate routes. For each walking path, we select the most recent frame on the walking path as a positive candidate, while a negative candidate is chosen as the frame within the cluster that exhibits the highest angular difference in view with the positive candidate."}, {"title": "C. Instruction Generation", "content": "In this section, we detail the process of transforming spatial awareness and object variety information into textual captions for use with GPT. This involves extracting multi-source data using models such as RAM (Swin-L) [61], Grounding DINO [38], and Depth-Anything [59], and then organizing this information into structured text inputs.\nObject variety into texts. Web videos offer a rich, open-world setup, capturing diverse items, arrangements, room functionalities, and layouts, which are critical for training open-world navigation agents. To fully utilize this diversity and ensure a controllable generation of instructions, we use RAM [61] (Swin-L) to extract object tags in each frame. For each frame, we filter out the resulting entries indicating room types in order to be consistent with the identified room locations from BLIP-2. Then these object tags are used for grounding objects within the frames, for further integration of spatial awareness information.\nSpatial awareness into texts. Navigation agents are frequently tasked with approaching or obtaining objects, making it crucial for them to sense object locations and dynamics during movement. To achieve this, we jointly use Grounding DINO [38] and Depth-Anything [59] models to gather detailed spatial information. The reason why we used Depth-Anything over the depth derived from COLMAP [46, 47] reconstructions is its ability to directly extract reliable depth without relying on long-range frames or structure-from-motion processes, which are prone to errors in complex video reconstructions. This spatial awareness information is then transformed into text inputs suitable for GPT, enabling effective training.\nWe start by using Grounding DINO to spatially localize objects within the video frames. We define spatial locations relative to the capturing spot: to the left of the current spot, in the middle, and to the right of the current spot. Specifically, the center 40% of the frame is considered the middle, the leftmost 30% as the left, and the rightmost 30% as the right. For depth perception, we categorize distances into three ranges: in the near distance (closest 30%), in closer distance (next 40%), and in a further distance (remaining 30%).\nFollowingly, we integrate spatial location and depth estimation by measuring the overlap ratio between objects and the defined distance ranges. For example, if a carpet overlaps with the near-distance area by more than 30%, we consider the carpet to be in the near distance to the capturing spot. Large objects that span multiple distance categories, such as a carpet visible in both near, closer, and further distances, are annotated accordingly to reflect their extended presence within the scene.\nThis structured approach ensures that our instructions capture the relative positioning and depth of objects, providing comprehensive context for navigation tasks. These"}, {"title": "D. Room Reconstruction", "content": "To obtain complete geometric information, we adopt COLMAP [46, 47] for indoor reconstruction. In this subsection, we detail the procedure of reconstructing room tour scenes, which further facilitates sampling navigable frames.\nReconstruction of video clip. To reconstruct video clips, we start by sampling videos at 3 frames per second to balance accuracy and execution time. This frame rate provides sufficient detail for accurate 3D reconstruction while maintaining manageable processing times. Each video is divided into 100-second clips with a 10-second overlap between ad-"}, {"title": "E. Implementation details", "content": "Following the practice from NaviLLM [63], we fine-tune the multi-view fusion module and the LLM. The multi-view fusion module consists of a 2-layer transformer encoder with a hidden size of 1024, and the LLM is built upon Vicuna-7B-v1.1 [9]. The ViT in the scene encoder is EVA-CLIP-02-Large, which remains frozen during training. Our training follows a two-stage strategy using the Adam optimizer with a learning rate 3e-5. The model is trained for 2500 steps in the pre-training stage and 1250 steps in the multi-task fine-tuning stage, with a batch size 256. The training process utilizes 4\u00d78 Nvidia A100 GPUs. During testing, we employ a sampling strategy with a temperature of 0.01 for the SOON and REVERIE tasks to encourage exploration, while a greedy strategy is used for other tasks. This approach ensures robust performance across various evaluation scenarios."}, {"title": "F. Qualitative Results", "content": "This section presents qualitative results to demonstrate the effectiveness of our model trained with the RoomTour3D dataset. The model was evaluated on unseen scenes using the R2R dataset, focusing on its performance in following navigation instructions. As shown in Figure 8, we tested the model on an unseen scene, 8194nk5LbLH, with trajectory ID 4332. Experimented with two different instructions, the agent trained our data shows its flexibility in following the instructions. For example, in (a), the agent moves straight to the bar, then reaches the three tables with chars, and finally stops near the couch. In (b), the agent directly moves towards the window, following the instructions, then moves towards the far coach and stops. These results demonstrate the instruction-following navigation ability of the agent, which further highlights the effectiveness of our video-instruction data."}, {"title": "G. Data Sample Visualization", "content": "In this section, we present visualizations of data samples from the RoomTour3D dataset, as shown in Figure 9. These visualizations highlight the rich variety of indoor scenes, the spatial awareness embedded in the data, and the detailed annotations used for training navigation agents.\nData correctness verification. We provide part (14 out of 100) of manual check trajectories in Figure 10 and Figure 11. For each trajectory, sampled frames and generated descriptions are provided, along with the manual check scores. The score ranges from 1 to 4, representing \"totally irrelevant\", \"partially relevant\u201d, \u201cmostly relevant\" and \"perfect match\" respectively. Most of the sampled trajectories gain scores 3 and 4, which shows the convincing quality of our automatically generated descriptions."}, {"title": "H. Broader Impact", "content": "Data Limitations and Ethical Considerations. We provide downsampled video frames instead of the original videos. Users can also download these from the original sources. Additionally, our meticulous filtering process ensures that the video frames and annotations contain only indoor rooms and houses, containing no personally identifiable information or offensive content. The authors will take responsibility for long-term maintenance.\nScope of Conclusions. It is important to recognize that experiments and data, including ours, might only represent a subset of universal realities. Nevertheless, given the wide range of room tour scenes covered in our videos, we believe our conclusions offer a robust understanding applicable to indoor embodied navigation. While specific to our dataset and results, these findings provide significant insight into the broader field of embodied navigation.\nUsage of Language Models and Simulators. Our use of the LLaMA model\u00b9 from Meta, use of MatterPort3D data [4] is authorized for research purposes. Those intending to use our model post-release should ensure they have the necessary permissions and adhere to usage restrictions. We express deep respect for the work of developers and contributors, recognizing their integral role in advancing language modeling and data collection.\nFuture Research and Development. Aligned with our commitment to the research community, we released our code and dataset. This is intended to encourage further research and enable others to build upon our work. Although our current experiments require up to 8\u00d74 A100-80G GPUs for pretraining and 8 A100-80G for multi-task tuning, we are aware this may be a limitation. Consequently, we plan to focus future efforts on adapting these experiments to be compatible with parameter-efficient tuned LLMs. It's important to note that fitting the experiments within an 8 GPU or fewer framework is not the primary focus of this paper. Still, we consider it a crucial step towards making our research more accessible and inclusive for various research groups.\nAlso, it would be interesting to investigate the usefulness of our data for grounded question-answering for 3D environments, particularly on the ScanQA dataset [3]."}]}