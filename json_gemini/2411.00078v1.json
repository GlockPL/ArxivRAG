{"title": "How Good Are We? Evaluating Cell AI Foundation Models in Kidney Pathology with Human-in-the-Loop Enrichment", "authors": ["Junlin Guo", "Siqi Lu", "Can Cui", "Ruining Deng", "Tianyuan Yao", "Zhewen Tao", "Yizhe Lin", "Marilyn Lionts", "Quan Liu", "Juming Xiong", "Yu Wang", "Shilin Zhao", "Catie Chang", "Mitchell Wilkes", "Mengmeng Yin", "Haichun Yang", "Yuankai Huo"], "abstract": "Training AI foundation models has emerged as a promising large-scale learning approach for addressing real-world healthcare challenges, including digital pathology. While many of these models have been developed for tasks like disease diagnosis and tissue quantification using extensive and diverse training datasets, their readiness for deployment on some arguably simplest tasks, such as nuclei segmentation within a single organ (e.g., the kidney), remains uncertain. This paper seeks to answer this key question, \"How good are we?\", by thoroughly evaluating the performance of recent cell foundation models on a curated multi-center, multi-disease, and multi-species external testing dataset. Additionally, we tackle a more challenging question, \"How can we improve?\", by developing and assessing human-in-the-loop data enrichment strategies aimed at enhancing model performance while minimizing the reliance on pixel-level human annotation. To address the first question, we curated a multicenter, multidisease, and multispecies dataset consisting of 2,542 kidney whole slide images (WSIs). Three state-of-the-art (SOTA) cell foundation models-Cellpose, StarDist, and CellViT\u2014were selected for evaluation. To tackle the second question, we explored data enrichment algorithms by distilling predictions from the different foundation models with a human-in-the-loop framework, aiming to further enhance foundation model performance with minimal human efforts. Our experimental results showed that all three foundation models improved over their baselines with model fine-tuning with enriched data. Interestingly, the baseline model with the highest F1 score does not yield the best segmentation outcomes after fine-tuning. This study establishes a benchmark for the development and deployment of cell vision foundation models tailored for real-world data applications.", "sections": [{"title": "1 Introduction", "content": "AI foundation models trained on massive, diverse datasets are widely applied across numerous fields, including healthcare.1,2 Their versatility enables these models to tackle a variety of downstream tasks, with one of the most prominent applications being digital pathology. 3-6 Among many tasks in digital pathology, cell instance segmentation is often serving as the initial step in extracting biological signals crucial for accurate disease diagnosis and treatment planning. 7-12 The accuracy\n1"}, {"title": null, "content": "of the cell or cell nuclei segmentation forms the foundation for various subsequent biological or medical analyses, including cell type classification, 13 specific cell counting, 14 and cell phenotype analysis. 15 It is also considered as the stepping stone for whole slide image (WSI) analysis in any biological and biomedical applications.16\nIn recent years, several cell foundation models17-20 trained on large and diverse datasets encompassing various cell types, imaging techniques (e.g., fluorescence, bright-field, phase contrast), and experimental conditions\u2014have demonstrated promising results. While many of these models have been developed for tasks such as disease diagnosis and tissue quantification, their readiness for deployment on some of the arguably simpler tasks, such as nuclei segmentation within a single organ (e.g., the kidney), remains uncertain. This leads us to pose the first question: Is nuclei segmentation on whole slide images (WSIs) within a single organ, like the kidney, a solved problem using current cell foundation models in histopathology?\nTo answer the preceding question, we have chosen the kidney as the evaluation organ for this study due to its diverse cell types. According to, 21 the kidney comprises at least 16 specialized epithelial cell types, along with various endothelial, immune, and interstitial cells. Additionally, kidney whole slide images and their primary staining type, Periodic Acid-Schiff (PAS), are un- derrepresented in the training of current cell nuclei foundation models, underscoring a gap in the research field. We evaluated the performance of current cell nuclei foundation models in kidney pathology within a multi-center, multi-staining, multi-species setting. As shown in Fig. 1a, b, and c, we construct a diverse evaluation dataset that includes kidney nuclei data from 2,542 kidney whole slide images (WSIs) sourced from humans and rodents across both public and in-house datasets. To our knowledge, the scale of this study's kidney WSIs surpasses all publicly available labeled nuclei datasets that include the kidney, as illustrated in Fig. 1a. We conducted a com-\n2"}, {"title": "2 Related work", "content": "2.1 Image Processing-Based Nuclei Instance Segmentation\nA key challenge in nuclei instance segmentation is separating overlapping nuclei. 23-30 Solutions to this problem range from traditional image processing techniques to deep learning approaches. Traditional methods rely on image processing for feature extraction, using intensity, shape, texture, and morphological patterns to identify nuclei. For instance, traditional methods such as marker- controlled watershed segmentation have been widely adopted for their effectiveness in separating overlapping or touching nuclei by utilizing predefined markers and adaptive thresholding tech- niques. 23,26 However, these methods often struggle with over segmentation or under segmentation when dealing with highly heterogeneous or densely clustered nuclei. To address these limita- tions, region-based active contour models and shape-prior integration have been developed, which incorporate geometric shape information to improve boundary delineation and handle object oc- clusions. 28 Additionally, multi-pass segmentation strategies, like the Multi-Pass Fast Watershed (MPFW) method, have demonstrated superior performance by employing iterative segmentation passes and combining gradient-based and shape-based techniques, making them particularly ef- fective for complex cytological samples such as cervical cell images. 25 However, these image processing-based methods remain heavily reliant on hand-crafted features, requiring domain ex- pertise and being sensitive to hyperparameters. 31,32\n2.2 Deep Learning-Based Nuclei Instance Segmentation\nTo address the limitations of hand-crafted feature-based methods, deep learning (DL) has enabled automatic feature extraction. As outlined in, 33 current DL approaches for nuclei instance segmen- tation can be broadly categorized into two-stage and one-stage approaches. Two-stage methods,\n5"}, {"title": null, "content": "such as Mask-RCNN, 34 first detect and localize cell nuclei using bounding boxes before refining the segmentation in a subsequent stage. Similarly, 35 uses the first stage to generate nuclei pro- posals and the second stage to refine nuclei boundaries. However, two-stage methods often suffer from non-standardized training and high computational costs. Additionally, overlapping nuclei can lead to incorrect segmentation being passed from the first to the second stage, requiring further postprocessing.\nIn contrast, one-stage methods include DL network and postprocessing into an end to end training and inference framework. For example, Micro-Net 36 modifies the U-Net architecture 37 by incor- porating multi-resolution input images, enabling better performance for nuclei of different sizes. The Dist model 38 introduces an additional decoder branch to predict distance maps from nucleus boundaries to their center of mass, aiding watershed-based postprocessing. HoVer-Net, 31 a SOTA method, leverages horizontal and vertical distance maps to detect nuclei edges through the Sobel operator. StarDist 18 and CPP-Net 32 use a polygon-based approach, which both show comparable results with HoVer-Net. Cellpose 17 incorporates a U-Net architecture combined with gradient flow tracking to create an end-to-end model for nuclei instance segmentation. Similarly, network archi- tecture in CellViT 19 uses a U-shaped vision transformer (ViT) 39 combined with HoVer-Net post- processing. Overall, one-stage models offer a balance between accuracy and efficiency, making them more suitable for practical deployment where computational resources are limited. Leverag- ing the ease of standardization and training offered by one-stage methods, this work also incorpo- rates foundation models from this branch.\n6"}, {"title": "2.3 Human-in-the-loop", "content": "Another key factor in improving nuclei instance segmentation is data labeling, as training instance segmentation models require pixel-level annotations for every object in an image. 20 However, creating these annotations can be expensive (10-2 to 100 USD/label), 9,40,41 which highlights the need to reduce the marginal cost of labeling. A recent improvement in labeling involves a multi- phase human-in-the-loop (HITL) process, where pathologists refine model errors to train founda- tion models sequentially. 9,41,42 Once the model achieves human-level performance, it is used to fully automate annotation of new images without further human correction. For example, 41 uses a two-phase HITL strategy to sequentially train models for cell segmentation and tracking by al- ternating between model predictions and human corrections.9 uses a three-phase HITL strategy to build a large training dataset through crowdsourced correction and expert validation. Cellpose 2.0 42 implements a flexible HITL pipeline for fine-tuning pretrained models with minimal new annotations. This process reduces the annotation load to 100\u2013200 ROIs (region of interest) while still achieving high-quality segmentations. However, these approaches do not leverage multiple foundation models with HITL. Motivated by this limitation, in our work, we utilized multiple foundation models with a HITL design to scale up the training dataset and further enhance the performance of the foundation models.\n3 Method\nThis section details the performance evaluation and HITL data enrichment framework (as shown in Fig. 2) used in this study. First, we describe the construction of a diverse, large-scale dataset for evaluating foundation models. Then, we present the HITL design that utilizes evaluations of multiple foundation models to enrich the training dataset. Lastly, the continuous fine-tuning of\n7"}, {"title": "3.1 Curate a Diverse Large-Scale Dataset", "content": "To provide a comprehensive assessment of their performance in segmenting kidney nuclei, we constructed a diverse evaluation dataset. Our dataset comprises both public and private kidney data, with a total of 2,542 whole-slide images (WSIs). As illustrated in Fig.1b, 57% (1,449 WSIs) come from publicly available sources, including the Kidney Precision Medicine Project (KPMP), 43 NEPTUNE, 44 and HUBMAP, 45,46 while the remaining 43% (1,093 WSIs) originate from an inter- nal collection at Vanderbilt University Medical Center. To increase the diversity of our dataset, we incorporated WSIs from both human and rodent samples. As depicted in Fig. 1c, these WSIs were stained using Hematoxylin and Eosin (H&E), Periodic acid-Schiff methenamine (PASM), and Pe- riodic acid-Schiff (PAS), with PAS being widely used in kidney pathology but less frequently in other organs. We randomly extracted four 512\u00d7512-pixel image patches from each kidney WSI at 40\u00d7 magnification. Patches that were contaminated, of low imaging quality, from dead tissue, or with incorrect staining methods were discarded. This process resulted in an evaluation dataset of 8,818 image patches.\n3.2 Data Enrichment with Multiple Foundation Models and HITL\nAs shown in Fig. 2a, nuclei instance segmentation was performed on these kidney nuclei image patches using three cell foundation models: Cellpose, 17 StarDist, 18 and CellViT.19 Then, instead of directly correcting their predicted instance masks, we use a rating-based system to evaluate the pre- dictions from the three foundation models (as shown in Fig. 2b). This curation process involves two rounds of input from human experts. In the first round of image patch curation, two pathologist- trained students evaluated the predicted masks, categorizing them as \u201cgood,\u201d \u201cmedium,\u201d or \u201cbad\u201d based on a rating standard set by a renal pathologist with 20 years of experience. \u201cGood\u201d predic-\n9"}, {"title": null, "content": "tions captured approximately 90% of the nuclei in a patch, \u201cbad\u201d predictions captured less than 50%, and the rest were classified as \u201cmedium.\u201d In the second round, the experienced renal pathol- ogist manually reviewed and validated the uncertain samples from the first round. We used this rating system to both qualitatively and quantitatively evaluate each model's predictions within our dataset. Each rating reflects the segmentation quality of the predicted mask for an image patch. In this study, employing multiple foundation models reduces bias in image patch curation by having each patch evaluated by three models trained with different architectures and settings. This ap- proach offers diverse perspectives, helping to identify image patches that highlight gaps between general and specific (e.g., kidney) domain performance. As a result, curated predictions from these models can be used in the next stage of the HITL process to further refine their performance in segmenting nuclei in kidney pathology.\nIn this work, we leveraged the image patch curation results to reduce the pixel-level labeling effort in the HITL design. Specifically, we directly utilized the \u201cgood\u201d samples (shown as Fig. 2d) for the next round of model refinement. Including these samples and their predictions as image and pseudo-label pairs enhances the training dataset at scale. Additionally, for each foundation model, the \"good\" samples reflect its strength and pretrained knowledge e(curation cost: 10 seconds per image patch). Pathologists manually corrected or annotated a small set of these \"bad\" image patches, which can be used for the next round of training for the foundation models (annotation cost: 10 seconds per image patch).\n3.3 Data-Enriched Foundation Models Fine-Tuning\nBenefiting from the data enrichment provided by multiple foundation models' curation, the models can be fine-tuned using either \u201cgood\u201d image patches (foundation model-generated pseudo labels),\n10"}, {"title": null, "content": "\u201cbad\u201d image patches (pathologist-corrected), or both, to enhance kidney nuclei segmentation per- formance. In this section, we first introduce the three foundation models used for image patch curation and continuous fine-tuning. A key consideration in fine-tuning, particularly when using both \"good\" and \"bad\" image patches, is the imbalance between these two classes. To address this, we applied a customized weighted oversampling method.\n3.3.1 Cell Foundation Models\nThis section outlines the three cell foundation models used in this study. Details for each founda- tion model can be found in Cellpose, 17 StarDist, 18 and CellViT.19\n1. Cellpose: Cellpose 17 is a generalist segmentation model that utilizes a U-Net backbone to predict the horizontal and vertical gradients of topological maps, as well as a binary map. It was trained on a diverse dataset containing over 70,000 segmented objects from various microscopy modalities, including fluorescence, brightfield, and other specialized types. The training dataset primarily consists of fluorescently labeled cytoplasm, with or without an ad- ditional nuclear channel. Additionally, 30 H&E images from MoNuSeg 47 were included in the nucleus dataset. These H&E-stained histology images contain many small nuclei, and their polarity was inverted to make them resemble fluorescence images more closely. This variety of image sources, covering cells with diverse morphologies and modalities, allowed Cellpose to generalize well across a broad range of cell types without requiring retraining or parameter adjustments. To segment individual cells, Cellpose employs a gradient track- ing algorithm that clusters pixels based on their convergence to a common center, enabling precise delineation of cellular boundaries even in challenging cases with dense packing or complex morphologies.\n11"}, {"title": null, "content": "2. StarDist: StarDist 18 employs a unique star-convex polygon representation for nuclei seg- mentation, which is particularly effective for roundish objects such as cell nuclei. The model is built on a U-Net backbone and predicts object probabilities and radial distances from the center of each nucleus to its boundary, forming star-convex polygons. StarDist was originally developed for fluorescence microscopy images. To enhance its applicability to histopathol- ogy images, the model was further trained on the Lizard dataset, 48 which includes 4,981 histopathology images, each of size 256 \u00d7 256 \u00d7 3, containing six different cell types (neu- trophil, epithelial, lymphocyte, plasma, eosinophil, and connective tissue cells). During the post-processing stage, non-maximum suppression (NMS) is used to remove redundant poly- gons, ensuring that only unique instances are retained. Additionally, test-time augmentations and model ensembling were employed to further enhance segmentation performance, mak- ing StarDist a robust solution for diverse microscopy image types.\n3. CellViT: CellViT 19 employs a U-Net shaped hierarchical encoder-decoder Vision Trans- former (ViT) backbone, designed specifically for nuclei segmentation and classification in histopathology images. The encoder utilizes pre-trained weights from a ViT trained on 104 million histological images (ViT256), 49 a model that demonstrated superior performance in cancer subtyping and survival prediction tasks. CellViT is further trained on the PanNuke dataset, 50 which includes 189,744 annotated nuclei across 19 tissue types, grouped into five clinically relevant classes: neoplastic, inflammatory, epithelial, dead, and connective. The images in this dataset are captured at 40\u00d7 magnification with a resolution of 0.25 \u00b5m/pixel, making it a challenging benchmark due to its diversity and class imbalance. CellViT's post- processing pipeline follows the HoVer-Net methodology, 31 using gradient maps of horizontal\n12"}, {"title": null, "content": "and vertical distances for accurate boundary delineation. Additionally, the network benefits from extensive data augmentation techniques and transfer learning strategies to overcome the scarcity of annotated medical data, achieving SOTA performance on both the PanNuke and MoNuSeg datasets.\n3.3.2 Class-wise Weighted Oversampling\nIn this work, we addressed the imbalance between human-annotated image patches and pseudo labels from foundation models \u201cgood\u201d predictions (with a ratio of about 1:50) by applying over- sampling. Specifically, we implemented a customized class-based weighted oversampling method, as used in. 19 For a training dataset of NTrain samples, the sampling weight for each training sample i was calculated based on its curated class (\u201cgood\u201d or \u201cbad\" image patches), as outlined below:\nWClass (i, Ys) = \\frac{NTrain}{Ys (\\sum_{j\\in[1,NTrain]}l_{j=l; 1}) + (1 \u2212 Ys)NTrain} \\tag{1}\nAs denoted, WClass(i, s) represents the sampling weight assigned to image patch i based on its curated class. Higher weights will be assigned to underrepresented classes, ensuring they are sam- pled more frequently during training. \u2208 [0,1] is the oversampling parameter that controls the strength of oversampling. When y = 0, there is no oversampling, and all patches are sampled uniformly. When y = 1, the maximum balancing is applied, and patches from underrepresented classes are given the highest weights. In this work, we followed the default setting of s = 0.85 in.19 \u03a3j\u2208[1,NTrain][lj=l; 1, this sum counts the number of patches in the training dataset that belong to the same class as patch i. Specifically, l\u00a1 and li represent the class labels of patches j and i, respec\u0441- tively. The numerator NTrain ensures that the weights are normalized based on the total number of\n13"}, {"title": null, "content": "training patches. The expression Ys (Ej\u2208[1,N'Train])|lj=l; 1) + (1 \u2212 ys) Train controls the balance be- tween classes. When ys is large, equation (1) gives more weight to underrepresented classes (with fewer samples), while for smaller s, equation(1) reduces the effect of class imbalance, leading to more uniform sampling. In summary, this oversampling approach ensures that human annotated samples, such as those with challenging nuclei or lower image quality, are sampled more frequently during training, which helps improve the model's ability to generalize across all cell nuclei in the kidney.\n4 Experiments\n4.1 Dataset\nEvaluation Dataset. As described in the previous section, the evaluation dataset in this study is highly diverse, comprising 2,542 whole-slide images (WSIs) from both public and private sources. It includes samples from both human and rodent tissue, stained with H&E, PASM, and PAS. After discarding contaminated or poor-quality patches, we randomly extracted four 512 \u00d7 512-pixel patches from each WSI at 40\u00d7 magnification, resulting in an evaluation dataset of 8,818 image patches. As illustrated in Fig.1b, 57% (1,449 WSIs) were collected from publicly available sources, including the Kidney Precision Medicine Project (KPMP), 43 NEPTUNE, 44 and HUBMAP, 45, 46 while the remaining 43% (1,093 WSIs) were acquired from an internal collection at Vanderbilt University Medical Center. To increase the diversity of our dataset, we incorporated WSIs from both human and rodent samples.\nFine-tuning Dataset. Building on the evaluation dataset construction described above, three foun- dation models were used for inference and curation on each 512 \u00d7 512 image patch. The \"good\" prediction masks from all three foundation models, along with their corresponding images, were\n14"}, {"title": null, "content": "retrieved as image-pseudo label pairs and added to the fine-tuning dataset. Additionally, a small set of 198 images, rated as \u201cbad\u201d by all foundation models, were manually annotated by pathologists and included in the fine-tuning dataset. This resulted in a total of 12,005 image-label pairs. For the training and validation split, 11,807 pairs were used for training, while a diverse set of 100 pairs was reserved for validation. Lastly, an additional 185 images, sampled from all kidney WSI sources, were annotated by pathologists and designated as the hold-out testing set to evaluate the performance of the fine-tuned foundation models. Each 512 \u00d7 512 image patch contains 50 to 300 cell nuclei, providing substantial signals for effective fine-tuning.\n4.2 Experimental Design\n4.2.1 Evaluation of Foundation Model Performance\nThe segmentation of nuclei instances from the three foundation models was evaluated as \"good,\u201d \"medium,\u201d or \u201cbad,\u201d following a standard established by a renal pathologist with more than 20 years of experience. We applied this rating system to quantitatively assess each cell foundation model's predictions in our evaluation dataset. The specific definitions are as follows:\n\u2022 Good: Predictions capture approximately 90% of the nuclei in an image patch.\n\u2022 Medium: Predictions capture between 50% and 90% of the nuclei in an image patch.\n\u2022 Bad: Predictions capture less than 50% of the nuclei in an image patch.\nSingle Model Performance Evaluation. First, we evaluated the performance of each individual foundation model by analyzing the distribution of rating assignments across our evaluation dataset. This analysis provided insights into each cell foundation model's performance and behavior by\n15\""}, {"title": null, "content": "examining the frequency of prediction classes (\u201cgood,\u201d \u201cmedium,\u201d \u201cbad\u201d) for each foundation model.\nFused Model Performance Evaluation. Building on the evaluation of each individual cell foun- dation model's performance, we conducted a joint model performance evaluation. The rating as- signment for each image patch was determined by the fusion of ratings from the three foundation models. Specifically, an image patch was assigned to the \u201cgood\u201d prediction class if any model rated it as \"good.\" Conversely, an image patch was categorized as \u201cbad\u201d only if all foundation models were assigned a \u201cbad\u201d rating. The remaining image patches were categorized as \u201cmedium.\u201d Simi- larly, the distribution of rating classes was analyzed. The results of this analysis indicate the upper bound of applying multiple foundation models to our domain task and highlight the potential for fine-tuning these models through our data enrichment strategies. Furthermore, a taxonomy of com- mon errors made by all cell foundation models in this study can be derived from the fused \"bad\" image patches.\nCross-Model Performance Evaluation. In this work, we also evaluated cross-model performance on our kidney nuclei dataset by conducting an agreement analysis among the foundation models. The steps are as follows:\n1. Agreement Matrix. We computed the percentage of agreement between each pair of models to assess how often they assigned the same rating to image patch predictions. This matrix highlights the consistency among the foundation models in their rating assignments.\n2. Class-Specific Agreement. Image patches were categorized into three groups: All Three Models Agree, where all models assigned the same rating; Two Models Agree, where any two models assigned matching ratings; and No Agreement, where none of the models as-\n16"}, {"title": null, "content": "signed the same rating to the image patch.\n4.2.2 Data-Enriched Fine-Tuning with Multiple Foundation Models\nTo improve kidney nuclei segmentation performance, we leveraged data enrichment from multiple foundation models' curation outcomes and fine-tuned the models under three different settings. Each foundation model was fine-tuned using either \"good\" image patches (foundation model- generated pseudo labels), \u201cbad\u201d image patches (pathologist-corrected), or both. As previously explained, fine-tuning on corrected \u201cbad\u201d image patches aims to narrow the knowledge gap be- tween domains by focusing on samples where all foundation models fail. \u201cGood\u201d image patches enable mutual knowledge distillation across foundation models, enhancing the training of each individual model. The details of the fine-tuning experiments are shown in the Table. 1. For each fine-tuning setting, we used different scales of the training dataset (25%, 50%, 75%, and 100%) to assess the consistency of the model's fine-tuning. Specifically, each entry in the \u201cIncremental Training Dataset Settings\" represents the size of the scaled training dataset used for fine-tuning experiments. For example, when fine-tuning with only \u201cgood\u201d image patches derived from foun- dation models' predictions, we conducted experiments using 2,952 (25%), 5,901 (50%), 8,854 (75%), and 11,807 (100%) \u201cgood\u201d predictions. Similar experiments were performed for all three fine-tuning strategies across the three foundation models: Cellpose, StarDist, and CellViT.\n4.3 Evaluation Metrics\nTo assess the instance segmentation performance of the foundation models after fine-tuning, we used Recall (equation(2)), Precision (equation(3)), and F1 score (equation(4)) as evaluation met- rics.\n17\""}, {"title": null, "content": "Recall = \\frac{True Positives}{True Positives + False Negatives} = \\frac{TP}{TP+FN} \\tag{2}\nPrecision = \\frac{True Positives}{True Positives + False Positives} = \\frac{TP}{TP+FP} \\tag{3}\nF1 = 2 \u00d7 \\frac{Precision \u00d7 Recall}{Precision + Recall} =  \\frac{2 \u00d7 TP}{2\u00d7TP+FP+FN} \\tag{4}\nMoreover, the Intersection over Union (IoU) threshold is set to 0.5 to consider a detection as a True Positive (TP). |TP|,|FP|,and |FN| are the number of True Positives, False Positives, and False Negatives, respectively.\n18"}, {"title": null, "content": "\u2022 True Positives (TP): Matched pairs of nuclei, i.e., correctly detected nuclei.\n\u2022 False Positives (FP): Unmatched predicted nuclei, i.e., predicted nuclei without matching ground truth (GT) nuclei.\n\u2022 False Negatives (FN): Unmatched GT nuclei, i.e., GT nuclei without matching predicted nuclei.\n4.4 Implementation Details\nInference: For a simple implementation of model inference using each foundation model's re- leased pretrained weights with GPU support, our previous work 51 provides customized object- oriented python modules. Models were trained using Python 3.9 and PyTorch 2.0.1, with GPU acceleration provided by CUDA 11.7. The experiments were conducted on an NVIDIA RTX A5000 GPU with 24GB of memory.\nTo maintain consistency in experimental settings and enable better comparison of fine-tuned per- formance, we applied the same oversampling factor y = 0.85 across all experiments that used both \"good\" and", "good": "nd \u201cbad\u201d image patches, the number of training epochs was set to 50 for all fine-tuning experiments involving \u201cgood\u201d image patches, while experiments using only", "bad": "mage patches were trained for 25 epochs. The batch size was consistently set to 16 across all experiments.\nCellpose Training: For training Cellpose using our true color images, we first converted the im- ages into the format required for training the Cellpose nucleus model. Each image was converted into a 2-band format, where the first band represents the grayscale image and the second band con- sists entirely of zeros. This follows the default Cellpose nucleus model inference format, where the\n19"}, {"title": null, "content": "first channel is used for nuclei segmentation. Due to different loss convergence patterns in train- ing Cellpose, the base learning rate was set to 0.1 for fine-tuning with only \"bad\" image patches and 0.001 for fine-tuning that includes \"good\" image patches. Other experimental settings are unchanged according to .17\nStarDist Training: The learning rate was maintained at 0.0003 (default), with all other settings unchanged as in.18 In this work, the fine-tuned StarDist model can be converted and loaded into QuPath extension format for easy validation and labeling purposes.\nCellViT Training: The base learning rate was set to 0.0003, with a scheduling factor of 0.85 to gradually reduce the learning rate during training. HoVer-Net post-processing with equal weight- ing on each loss objective (excluding nuclei classification and tissue classification) was applied during training. The pretrained ViT256 encoder and decoder were trained end-to-end, with no layers frozen. All other hyperparameters remained the same as in the original setting. 19\n5 Results\n5.1 Evaluation of Foundation Model Performance\nIndividual Model Performance Evaluation. First, we present the evaluation of each individual foundation model's performance. We analyzed the distribution of human-rated predictions for each foundation model across our evaluation dataset. From the barplot in the upper panel of Fig. 3, we can observe the distribution of the rated predictions for each foundation model in this study. For all three foundation models (Cellpose, StarDist, and CellViT), at least 40% of predictions are rated as \"bad\"\u201d or \u201cmedium.\u201d at least 40% of predictions are \u201cgood.\u201d This indicates that, despite not being specifically trained on kidney pathology nuclei data, the models retain some transferable knowledge applicable to our task. However, none of the foundation models is perfect, and the 40%\n20"}, {"title": null, "content": "performance gap highlights the ongoing need for more specialized foundation models tailored specifically to kidney pathology and their challenging cell nuclei segmentation cases. Among the evaluated cell foundation models, Cellpose and StarDist have similar amounts of \"good\" rated predictions, while CellViT shows a notably higher number of \u201cgood\u201d predictions of 5,609 (63%), suggesting its potentially superior performance and adaptability in kidney pathology.\nFused Model Performance Evaluation. As shown in Fig. 3, building on the evaluation of each individual cell foundation model's performance, we conducted a fused model performance evalu- ation through a data enrichment strategy. The rating assignment for each image patch was deter- mined by the fusion of ratings from the three foundation models. This evaluation aims to demon- strate the potential of applying multiple foundation models together in our domain task. As shown, after data enrichment, the fused model performance indicates an increase in the number of \u201cgood\u201d image patches (6,001 \u201cgood\u201d image patches, representing 68% of the dataset) and a decrease in the number of \"bad\" image patches (534 image patches, representing 6% of the dataset). Conse- quently, the \u201cgood\u201d image patches surpass those of each foundation model, while the \u201cmedium\u201d and \"bad\" image patches fall below all three models. This highlights the expanded capacity of the foundation models and supports subsequent continual learning using data-enriched prediction outcomes.\nFurthermore, as illustrated in the lower panel of Fig. 3, a taxonomy of common errors made by all cell foundation models is summarized from the fused set of \"bad\" image patches. It is evident that certain types of nuclei and tissue conditions diminish the segmentation performance of foundation models. Specifically, long and flat nuclei, nuclei with blurred boundaries, and densely distributed nuclei within glomeruli are particularly difficult to segment accurately. Additionally, slides with faint staining, those containing fatty tissues, or slides with an excessive number of red blood cells\n21"}, {"title": null, "content": "can result in a higher incidence of model failures.\nCross-Model Performance Agreement. Next, we move beyond evaluating the performance of\n22"}, {"title": null, "content": "individual foundation models and assess their collective behavior by examining the agreement and disagreement among their predictions. Fig. 4a shows the consistency between model predictions.\nAs illustrated, no pair of models reaches over 90% agreement or complete disagreement. The high- est agreement occurs between Cellpose and StarDist (0.76), while the lowest is between Cellpose and CellViT (0.67). These non-extreme values, ranging from 0.67 to 0.76, suggest that the current SOTA nuclei foundation models in digital pathology do not generalize exceptionally well to large- scale, diverse kidney datasets. Each model exhibits its own strengths, highlighting the potential for combining multiple SOTA foundation models to improve performance in downstream kidney nuclei tasks. Fig. 4b shows the percentages of image patches where all three models agree, two models agree, or no models agree for each rated prediction class (\u201cgood\u201d, \u201cmedium\u201d, \u201cbad\u201d). It is evident that \u201cbad\u201d samples exhibit the highest inter-model reliability, with over 70% agreement among all three models, indicating the nature of our design that prioritizes human annotation effort for consensus \"bad\" image patches.\n23"}, {"title": "5.2 Data-Enriched Foundation Models Fine-Tuning", "content": "To enhance model performance while minimizing reliance on pixel-level human annotation, we further fine-tuned the foundation models using our data enrichment strategies, which ensemble the evaluation outcomes of multiple foundation models. For each foundation model, the pretrained weights served as the baseline method. First, we evaluated the baseline performance of each cell foundation model using our hold-out testing dataset. The results of assessing the baselines for\n25"}, {"title": null, "content": "work. Each column represents the fine-tuning results for an individual foundation model, while each row compares an instance segmentation performance metric (F1 score, Precision, or Recall) across the models. Different colors represent distinct fine-tuning strategies or settings. The X-axis in each plot corresponds to the incremental percentage (25%, 50%, 75%, and 100%) of the training dataset used for each fine-tuning strategy, and the Y-axis shows the performance metrics. From the fine-tuning performance trends illustrated in Fig. 5, both the Cellpose and StarDist models demonstrate significant improvements in F1 score compared to their baseline models. These gains highlight the effectiveness of the data enrichment strategies employed during fine-tuning. The details of instance segmentation performance gains are provided below with the evaluation metric values referenced in Table. 2.\n1. For the Cellpose model"}]}