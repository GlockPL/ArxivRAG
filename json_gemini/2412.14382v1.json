{"title": "BALANS: Multi-Armed Bandits-based Adaptive Large Neighborhood Search for Mixed-Integer Programming Problems", "authors": ["Junyang Cai", "Serdar Kad\u0131o\u011flu", "Bistra Dilkina"], "abstract": "Mixed-Integer Programming (MIP) is a powerful paradigm for modeling and solving various important combinatorial optimization problems. Recently, learning-based approaches have shown potential to speed up MIP solving via offline training that then guides important design decisions during search. However, a significant drawback of these methods is their heavy reliance on offline training, which requires collecting training datasets and computationally costly training epochs yet offering only limited generalization to unseen (larger) instances. In this paper, we propose BALANS, an adaptive meta-solver for MIPs with online learning capability that does not require any supervision or apriori training. At its core, BALANS is based on adaptive large-neighborhood search, operating on top of a MIP solver by successive applications of destroy and repair neighborhood operators. During the search, the selection among different neighborhood definitions is guided on the fly for the instance at hand via multi-armed bandit algorithms. Our extensive experiments on hard optimization instances show that BALANS offers significant performance gains over the default MIP solver, is better than committing to any single best neighborhood, and improves over the state-of-the-art large-neighborhood search for MIPs. Finally, we release BALANS as a highly configurable, MIP solver agnostic, open-source software.", "sections": [{"title": "Introduction", "content": "Mixed-Integer Programming (MIP) can model many important combinatorial optimization problems, and hence, improving the efficiency of MIP solving is of great practical and theoretical interest. Broadly, solving MIPs falls under two main classes of algorithms: exact and meta-heuristic methods. For exact methods, branch-and-bound (Land and Doig 2010), and its extensions, are at the core of solving MIPs to optimality. When proving optimality is beyond reach, meta-heuristics, such as Large Neighborhood Search (LNS), offer an attractive alternative for finding good solutions and increasing scalability.\nIncorporating learning-based methods into MIP solving has shown great potential to improve exact and heuristic methods. For exact branch-and-bound based methods, these include learning to branch (Bengio, Lodi, and Prouvost 2021; Lodi and Zarpellon 2017; Khalil et al. 2016; Kad\u0131oglu, Malitsky, and Sellmann 2012; Liberto et al. 2016), and node selection (He, Daume III, and Eisner 2014). For meta-heuristics, these include predicting and searching for solutions (Ding et al. 2020; Khalil, Morris, and Lodi 2022), integrating machine learning into LNS on top of MIP solvers (Song et al. 2020; Huang et al. 2023b), and learning to schedule heuristics within the branch-and-bound search tree (Khalil et al. 2017; Chmiela et al. 2021; Hendel 2022).\nThe significant drawback of these learning-guided methods is their heavy dependency on offline training. Training is computationally costly, depends on carefully curating training datasets with desired properties and distributions, and has limited generalization. Moreover, training might even depend on using exact solvers in the first place to create the supervised datasets, which defeats the purpose of improving solving for very hard instances. Adapting offline learning-based methods to new distributions and domains remains a challenge, and hence, on-the-fly or online learning approaches to MIP solving are a much needed alternative. This is exactly what we study in this paper.\nIn this paper, we focus on solving MIPs via online adaptive methods that do not depend on any offline training. Recent research has shown the potential of meta-solvers that embed a MIP solver within LNS (Song et al. 2020) and consequently achieve better performance than default MIP via LNS(MIP). The state-of-the-art in this line of research is based on a single neighborhood definition, referred to as local branching relaxation (Huang et al. 2023a). However, the previous work showing that offline learning can improve LNS (Song et al. 2020; Nair et al. 2020; Huang et al. 2023b) suffers from the above-mentioned limitations. We take the idea of machine learning guided LNS for MIP further and propose a meta-solver based on Adaptive LNS (ALNS) operating on a MIP solver, i.e., ALNS(MIP), composed of a diverse set of neighborhood definitions that are driven by multi-armed bandits online learning policy.\nThis is, however, non-trivial since online learning is hard, but given the significant drawbacks of offline learning, it must be tackled on the fly, adaptively for the specific instance at hand. We show how to cast this as a multi-armed bandit problem that treats adaptive neighborhoods as different arm choices with unknown reward distributions to be estimated during search. Bringing these components together, we propose BALANS, an online meta-solver for MIPs using multi-armed Bandits-based Adaptive Large Neighborhood Search operating across a diverse set of neighborhoods."}, {"title": "Background", "content": "Mixed-Integer Programming (MIP): We deal with MIP solving that formulates optimization problems of the form:\n$f(x) = min{c^x | Ax \u2264 b, x \u2208 R\u2033, xi \u2208 {0,1} di \u2208 B, xj \u2208 Z\\{0,1\\} Vj \u2208 I}$\nwhere $f(x)$ is the objective function, and $A \u2208 R^{m\u00d7n},b\u2208 R^m, c\u2208 R^n$. Let $V = [1,2,...,n]$ denote the list of all variable indices, $B$ the list of binary indices, $I$ the list of integer indices, and $D = B \\cup I$ the discrete indices.\nWhen $V = I$, we deal with Integer Programs (IPs), and when $V = B$, we deal with Binary Programs (BP). The Linear Programming (LP) relaxation of a MIP $X_{lp}$ is obtained by relaxing integer variables to continuous variables, i.e., by replacing the integer constraint $x_j \u2208 Z$ $\u2200 j \u2208 I$ to $x_j\u2208 R$ $\u2200 j \u2208 I$. The LP relaxation is an essential part of the branch-and-bound (BnB) algorithm, and many destroy operators used in large-neighborhood search.\n(Adaptive) Large-Neighborhood Search (LNS / ALNS): LNS is a meta-heuristic that starts with an initial solution and then iteratively destroys and repairs a part of the solution until hitting a stopping condition (Pisinger and Ropke 2019). When applied to MIPs, LNS(MIP), we first generate an initial solution, typically found by running BnB for a short time. This becomes the starting point of LNS. In iteration $t \u2265 1$, given the solution of the previous state, $x_{t-1}$, a sub-MIP is created by a destroy heuristic. The re-optimized solution to this sub-MIP, $x_t$, then becomes the candidate for next state, decided by an accept criterion. Adaptive LNS (ALNS) is defined by multiple destroy heuristics to choose from at each iteration, thereby extending LNS(MIP) to ALNS(MIP).\nMulti-Armed Bandits (MAB): MAB algorithms (Vermorel and Mohri 2005) is a class of reinforcement learning aimed at solving online sequential decision-making problems. In MAB, each arm defines a decision that an agent can make, generating either a deterministic or stochastic reward. At each step, the agent faces a decision whether to utilize an arm with the highest expected reward (\u201cexploit\u201d) or to try out new arms to learn something new (\u201cexplore\u201d). The agent's goal is to maximize the cumulative reward in the long run, for which it must balance exploration and exploitation. The reward of each arm is estimated from past decisions using a learning policy (Kuleshov and Precup 2014).\nBALANS: Online Meta-Solver for MIPS\nAs illustrated in Figure 1, our BALANS approach brings the complementary strengths of MIP, ALNS, and MAB together to create an online meta-solver for tackling hard combinatorial optimization problems."}, {"title": "Online Learning", "content": "We identify two types of exploration that are crucial for effective online learning. The first exploration decision stems from state exploration, which decides whether the search should continue with the next state or discard the move. The second exploration decision stems from neighborhood exploration, which decides the destroy neighborhood operator to apply at each state. Distinguishing the two different needs of exploration and addressing them separately in a principled manner is key to our good performance.\nState Exploration: As shown in Figure 1, the acceptance criterion governs the search space among states. We consider two complementary acceptance criteria: Hill Climbing (HC) and Simulated Annealing (SA). HC mostly exploits yet allows the search to progress to the next state when the objective value is the same. On the other hand, SA offers more exploration capacity and allows the search to move to worsening next states.\nNeighborhood Exploration: We employ multi-armed bandits for choosing among different neighborhoods. To apply MAB, there are three important design decisions: the definition of arms, the reward mechanism, and the learning policy.\nAs shown in Figure 1, in BALANS, we treat every pair of destroy and repair operators as a single arm. When an arm is selected, the neighborhood operation is atomic; it destroys the current state and then immediately repairs it to obtain the next state. In this paper, given that we only have a single repair operator, which re-optimizes the sub-MIP obtained from applying destroy, the list of arms can be treated identically with our destroy operators. If desired, users of BALANS can introduce additional repair operators.\nOur reward mechanism for MAB is designed specifically for the ALNS(MIP) framework. We consider four distinct rewards aligned to the possible outcomes of the acceptance criterion as part of state exploration. These four distinct outcomes are: the next state is the best solution found so far, the next state is better than the previous state, the next state is neither the best nor better, but it is accepted as a move, or the next state is rejected. Each outcome is associated with a specific reward value, as later listed in our experiments.\nFinally, MAB learns from historical arm choices associated with their observed rewards based on its learning policy. For learning policy, we consider three strategies: e-Greedy (Auer, Cesa-Bianchi, and Fischer 2002) and Softmax (Luce 1977) with numeric rewards and Thompson Sampling (Thompson 1933) with binary rewards. For numeric rewards, we experiment with linear and exponential settings.\nTo summarize our overall algorithm, once the initial solution is found, the main ALNS loop starts, which is an interplay between LNS and MAB. First, MAB selects an arm among the different neighborhoods and provides it to LNS. Then, LNS applies the operation and observes the outcome. Based on the resulting state, its solution quality, and the acceptance criterion, LNS provides the reward feedback to MAB. MAB updates the reward estimate of the selected arm according to its learning policy. This process repeats until hitting the stopping condition, and at the end, the best solution found during the search is returned."}, {"title": "Experiments", "content": "What remains to be seen is the performance of BALANS in practice. For that purpose, our main research questions are:\nQ1: What is the performance comparison between the default MIP, LNS(MIP) that commits to a single neighborhood, the state-of-the-art LNS(MIP), and our ALNS(MIP) using BALANS? Can BALANS achieve good performance without any offline training and explore states and neighborhoods simultaneously by adapting to the instance at hand on the fly via bandit-based ALNS(MIP)?\nQ2: How is arm selection among the portfolio of neighborhoods distributed in our bandit strategy? Does BALANS depend on the single best neighborhood, or can it improve over the single best by applying weaker operators sequentially in an adaptive fashion?\nDatasets: In our experiments, we use the recently introduced Distributional MIPLIB (D-MIPLIB) (Huang et al. 2024) and the commonly used MIPLIB2017-Hard (H-MIPLIB) (Gleixner et al. 2021). For the former, we randomly select 10 instances from Multiple Knapsack, Set Cover, Maximum Independent Set, Minimum Vertex Cover, and Generalized Independent Set Problem, yielding 50 instances. For the latter, we consider a subset that permits a feasible solution within 20 seconds, yielding 44 instances. SCIP cannot solve any of these instances to optimality within 1 hour, ensuring the hardness of our benchmarks.\nApproaches: For comparison, we consider the following:\nMIP: We use SCIP, the state-of-the-art open-source MIP solver with default settings (Bolusani et al. 2024).\nState-of-the-art LNS(MIP): We use lb-relax thanks to the original implementation from (Huang et al. 2023a). As discussed earlier, this algorithm selects the neighborhood with the local branching relaxation heuristic.\nSingle Neighborhood LNS(MIP): All the operators given in Table 1 are implemented and readily available in BALANS to serve in LNS(MIP). That said, in our experiments, we found DINS and Random Objective to perform poorly, hence we focus on the remaining six destroy operators. By varying the parameters of these operators, we obtain 16 different destroy operators from 6 unique neighborhood definitions. These are: crossover, lb_10/25/50 (local branching), mutation_25/50/75, proximity_05/15/30, rens_25/50/75, and rins_25/50/75. For the accept criterion, we use Hill Climbing (HC) and Simulated Annealing (SA) with initial temperature set to 20 and end temperature set to 1 with step size of 0.1.\nBALANS ALNS(MIP): Given the 16 different single destroy operators used in LNS(MIP), we build BALANS for ALNS(MIP) with a portfolio that includes all of the 16 operators. We again apply HC and SA as the acceptance criterion. For the learning policy, we use e-Greedy and Softmax with numeric rewards and Thompson Sampling (TS) with binary rewards. For numeric rewards, we use a linear ([3, 2, 1, 0]) and an exponential setting ([8, 4, 2, 1]) corresponding to best, better, accept, and reject outcomes. For binary rewards, we consider [1, 1, 0, 0], where accept reward is same as reject, and [1, 1, 1, 0], where accept is better than reject."}, {"title": "Conclusions", "content": "In this paper, we proposed BALANS, a multi-armed Bandits-based Adaptive Large Neighborhood Search for mixed-integer programming. Our experiments have shown that all configurations of our online approach, without any offline training and almost zero tuning, significantly improve over 1) the default BnB solver, and 2) any single large neighborhood search, including the state-of-the-art, on hard optimization problems. We release BALANS solver as open-source software with its high-level interface, modular and extendable architecture, and configurable design.\nFor future work, our research opens the door for interesting directions. These include improving the performance of BALANS further by careful algorithm configuration and portfolio construction (Kadioglu et al. 2010, 2011), and exploring hybrid ALNS(MIP), where existing offline training methods are introduced as additional arms in our portfolio."}, {"title": "Related Work", "content": "Primal heuristics play a crucial role in the rapid discovery of feasible solutions to improve the primal bound. As such, there is a rich literature on learning-based methods for improving heuristics. Examples in this line of research include IL-LNS (Sonnerat et al. 2021), which learns to select variables by imitating LB, RL-LNS (Wu et al. 2021), which uses a similar framework but trained with reinforcement learning, and CL-LNS (Huang et al. 2023b) uses contrastive learning to learn to predict variables. Our work complements these approaches as an online method that does not require any offline training to generate good quality solutions on challenging optimization instances, as shown in our experiments.\nThere is also a body of work that focuses on primal heuristic operations of BnB within-the-solver. These include offline learning methods, such as (Khalil et al. 2017), which builds a mapping between BnB nodes and a binary decision for running a given primal heuristic, and (Chmiela et al. 2021), which constructs a schedule that assigns priority and computational budget to each heuristic.\nWhen considering online methods (Chmiela et al. 2023; Hendel 2022) are the closest to our work. As in our paper, both are examples of online learning, focus on LNS neighborhoods, use SCIP, and are based on multi-armed-bandits. The crucial difference is that BALANS is a meta-solver that operates on top of a MIP solver in a solver-agnostic manner, whereas the previous work operates within the BnB search of SCIP, where LNS is invoked at internal search nodes. As a result, these works remain specific extensions of SCIP and are white-box, i.e., require access to solver internals. Contrarily, BALANS is an integration technology that offers a modular architecture to leverage best-in-class open-source software dedicated to their specific domain. We leverage MABWISER (Strong, Kleynhans, and Kadioglu 2019, 2021) for bandits, ALNS library (Wouda and Lan 2023) for adaptive large-neighborhood search, and SCIP (Bolusani et al. 2024) for MIP solving. Future advances in these distinct fields, realized independently within each software, propagate to our meta-solver with compounding effects. This unique integration also makes BALANS highly configurable. Beyond the settings used here, many more configurations are available through the parameters of ALNS and MABWiser. Moreover, our design enables the introduction of new destroy and repair operators with ease. Extensions become available as new parameters that plug and play with existing options. In the supplemental material, we present a quick start usage example of BALANS showcasing its ease-of-use and high-level API design.\nIt is important to note that the default configuration of SCIP applies ALNS within-the-solver (Hendel 2022). As our experimental results have shown, BALANS significantly outperforms default SCIP; hence, our ALNS(MIP) stands superior to MIP(ALNS). Similarly, while (Huang et al. 2023a) presents the state-of-the-art single neighborhood performance of lb-relax based on LNS(MIP), it is outperformed by BALANS by a large margin.\nA detailed performance comparison between offline trained methods and our online approach requires access to trained artifacts and is beyond the scope of this paper. We take results of offline learning approaches from (Huang et al. 2023b) as-is and compare their results directionally with BALANS on the same D-MIPLIB distributions. We find BALANS perform comparable with offline learning approaches, relying only on online learning. Full table of results and analysis are in the supplementary material.\nFinally, let us acknowledge the successful applications of MAB in other relevant domains. Among those, (Phan et al. 2024) uses MAB with ALNS for multi-agent path finding, (Almeida et al. 2020) uses MAB with hyper-heuristics for multi-objective flow shop problems, (Yu, Kveton, and Mengshoel 2017) uses Thompson Sampling for SAT via local search, and (Zheng et al. 2022) uses MAB for MaxSAT. Beyond combinatorial problems, MAB is heavily used in recommender systems (Kadioglu and Kleynhans 2024) and game-playing agents (Schaul et al. 2019)."}]}