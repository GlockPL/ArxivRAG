{"title": "CONTEXT-AUGMENTED CODE GENERATION USING PROGRAMMING KNOWLEDGE GRAPHS", "authors": ["Iman Saberi", "Fatemeh Fard"], "abstract": "Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly improved code generation, but, they frequently face difficulties when dealing with challenging and complex problems. Retrieval-Augmented Generation (RAG) addresses this issue by retrieving and integrating external knowledge at the inference time. However, retrieval models often fail to find most relevant context, and generation models, with limited context capacity, can hallucinate when given irrelevant data. We present a novel framework that leverages a Programming Knowledge Graph (PKG) to semantically represent and retrieve code. This approach enables fine-grained code retrieval by focusing on the most relevant segments while reducing irrelevant context through a tree-pruning technique. PKG is coupled with a re-ranking mechanism to reduce even more hallucinations by selectively integrating non-RAG solutions. We propose two retrieval approaches-block-wise and function- wise-based on the PKG, optimizing context granularity. Evaluations on the HumanEval and MBPP benchmarks show our method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art models by up to 34% on MBPP. Our contributions include PKG-based retrieval, tree pruning to enhance retrieval precision, a re-ranking method for robust solution selection and a Fill-in-the- Middle (FIM) enhancer module for automatic code augmentation with relevant comments and docstrings.", "sections": [{"title": "INTRODUCTION", "content": "Large Language Models (LLMs) have significantly improved the performance of tasks related to code, such as code generation (Huang et al., 2023; Roziere et al., 2023a; Li et al., 2023; Wang et al., 2023). As code-related models continue to emerge rapidly (Chen et al., 2021; Li et al., 2023; 2022; Roziere et al., 2023a; Zhu et al., 2024), most of these models rely on a natural language-to-code (NL-to-Code) paradigm, which often lacks the ability to leverage existing contextual information (Wang et al., 2024). Generating a solution from scratch, without access to supplementary context, poses significant challenges (Wang et al., 2024), even for humans (Zhong et al., 2024). Retrieval-Augmented Generation (RAG) enables retrieving and integrating relevant context from external knowledge sources during the inference time (Guu et al., 2020; Lewis et al., 2020), minimizing the necessity of embedding all knowledge within the model's parameters (Asai et al., 2024).\nRAG-based approaches can enhance accuracy across different scenarios (Izacard et al., 2022), without the need for further training of the model (Mallen et al., 2022; Ram et al., 2023). RAG-methods for code generation were previously proposed for retrieving information from library documentation (Zhou et al., 2022) and file repositories (Zhang et al., 2023). Wang et al. (2024) explored the impact of different retrieved chunk sizes or including the entire data cells during the retrieval for code generation; showing that both factors have a negative effect on the performance of code generation tasks by introducing irrelevant data. They identified two main challenges in retrieval for code generation. First, accurately identifying and retrieving helpful documents, and second, the limited context capacity of models that can lead to hallucinations when given irrelevant data. Our work aims to alleviate these challenges through two main contributions."}, {"title": "METHODOLOGY", "content": "Our approach is explained in three distinct steps: (1) PKG Generation, as illustrated in Figure 2, where we describe the process of generating PKG; (2) Information Retrieval from PKG, shown in Figure 3, where we outline the retrieval of relevant information from the PKG; and (3) Solution Re-ranking, where we detail the process of re-ranking the retrieved solutions."}, {"title": "PKG GENERATION", "content": "In this section, we will explain how to generate PKG in 6 steps as explained below.\nStep (1) Programming Dataset: We generate a PKG from a given dataset that contains text and code contents. In our experiments we have used PythonAlpaca dataset (Petit, 2024) as it consists of conversational question-answers in general python programming problems (Step 1 in Figure 2).\nStep 2 Fuction Extraction: We aim to extract the question-answer samples that solve a unique problem. To this end we used our developed FunctionAnalyzer tool to extract python functions from the output section of the dataset (Step 2 in Figure 2).\nStep (3) Code Block Extraction: In our approach, each code block is represented as a node corresponding to specific Python constructs, such as if, for, with, or try blocks. The FunctionAnalyzer is responsible for extracting the context-flow graph (CFG) of each function, and subsequently identifying the code blocks, which are represented as individual nodes. Each function consists of three types of nodes: 'function name', 'function implementation', and 'extracted code blocks'. The relationships between these nodes are captured as structural edges in the PKG. Specifically, each function is represented by a 'function name' node, which is connected to a node representing the complete implementation of the function. This implementation node is connected to its corresponding sub-block nodes, reflecting the hierarchical structure of the code (as shown in Step 3 of Figure 2).\nHere is the mathematical formulation of the Code Block Extraction process, let F represent a function. C(F) be the set of code blocks extracted from F. GF = (VF, EF) represents the graph for the function F, where VF is the set of nodes and EF is the set of edges representing the relationships between the nodes. The nodes VF can be defined as:\nVF = {nameF, vimplF} \u222a {vblockiF | i = 1, 2, ..., |C(F)|}  (1)\nwhere nameF is the node representing the \u2018function name', vimplF is the node representing the full implementation of function F, vblockiF represents the i-th code block extracted from F. The edges EF capture the hierarchical relationships between the nodes:"}, {"title": null, "content": "EF = { (nameF, vimplF ) U (vimplF, blockiF))}{(blockiF, blockjF) |i, j \u2208 {1,2,..., |C(F)|} |}\nThe edge (nameF, vimplF) represents the relationship between the function name and its complete implementation. The edge (vimplF, blockiF) represents the relationships between the function implementation and its largest constituent code block and the relations between code blocks are denoted by (blockiF, blockjF). Block-wise retrieval retrieves from Vblock while function-wise retrieval only search over Vimpl nodes. When we encounter a function call within a retrieved function or code block, we perform a search over the Vname nodes in the knowledge graph. This search allows us to find function calls bodies, enabling us to provide relevant contextual information that makes the retrieved content self-contained.\nStep (4) Enhance PKG: We have developed a module named FunctionEnhancer, specifically designed to enrich the representation of function implementations within the PKG. This enhancement process leverages a fill-in-the-middle (FIM) objective, applied at different locations of the implementation. The FIM technique enables the generation of explanations for code components by placing the [#<fim_suffix>.] anywhere we want to generate a one-line comment and [\"\"\"<fim_suffix>\"\"\"] after function signature where we want to generate its docstring. In particular, we focus on augmenting functions with detailed docstrings, which will enhance the implementation nodes' content. These nodes provide valuable metadata, including input parameters, output values, and descriptions of the overall functionality of each function. By incorporating such comprehensive documentation into the PKG, we achieve a more accurate and meaningful representation of the behavior and purpose of functions, thereby improving the system's overall ability to interpret and generate code (as shown in Step 4 of Figure 2). For this module, we utilize StarCoder2-7b as the underlying model (Li et al., 2023). To the best of our knowledge, this is the first application of the FIM technique for code enhancement.\nStep (5) Encode PKG: The primary objective of this step is to enable semantic search over the PKG. To achieve this, each node within the graph will be encoded. Previous research, such as the experiments conducted by Wang et al. (2024), has explored various embedding models for code-RAG methods. Based on these findings, we have selected the VoyageCode2 model2, which is recognized as one of the most effective embedding models for code representation (Step 5 of Figure 2).\nStep (6) Neo4j Graph Generation: Once all nodes, along with their corresponding embeddings and relationships have been defined, we construct a Neo4j vector graph. This graph will enable efficient knowledge retrieval through the use of graph indexing and semantic search functionalities."}, {"title": "RETRIEVAL FROM PKG", "content": "To retrieve relevant information for a given query from the PKG, we first obtain the query's embeddings using our embedder model (Step 1 in Figure 3). Let q represent the user query. Embed(q) \u2208 Rd be the query's embedding in a d-dimensional space, generated by an embedder model &, i.e., Embed(q) = E(q). Similarly, for each node v in the PKG, let Embed(v) \u2208 Rd represent the embedding of the content of node v.\nWe perform a semantic vector search to identify the node vbest in the PKG that is most similar to the query. This is done by computing the cosine similarity between the query's embedding and each node's embedding (Step 2 in Figure 3):\nSim(q, v) = Embed(q) \u00b7 Embed(v)/ ||Embed(q)|| \u00b7 ||Embed(v)||\nWe propose two code-retrieval approaches on the PKG: block-wise retrieval and function-wise retrieval. Block-wise Retrieval: Retrieval will be performed on the code blocks as a granular retrieval setting, denoted as vblock, with the results labeled as 'Block-PKG'. This method aims to capture the most relevant context by focusing on related blocks of code within the graph. Function-wise"}, {"title": null, "content": "Retrieval: Here, the retrieval will be performed on the implementation nodes, denoted as vimpl, and the results will be referred to as 'Func-PKG'. The entire function is returned as the relevant context, ensuring that the retrieved information is tightly focused on functional code units.\nAt each setting, the node nbest that maximizes this similarity is chosen :\nnbest = arg max Sim(q, n)\nNext, we refine the selected node nbest by removing branches that are irrelevant to the query (Step 3 in Figure 3). The node nbest is modeled as a Directed Acyclic Graph (DAG) Gnbest = (Vibest, Enbest), where each node represents a code-block or sub-function, and edges represent child dependencies between them. For branch pruning, let Gi nbest represent the pruned graph where the i-th branch (subgraph) is removed from Gnbest.\nWe compute the embedding Embed (Grest) for each pruned version of the function. The best pruned version Gpruned is selected by maximizing the cosine similarity between the query embedding and the pruned graph embeddings:\nGpruned arg max Sim (q, Grbest)\ni\nQuery Augmentation (Step 4 in Figure 3): After identifying the most relevant pruned version of the node, we augment the original query q with the pruned graph content (i.e., npruned):\nQaugmented Augment (q, npruned)\nwhere Augment is a function that combines the query with the npruned content.\nFor instance, as illustrated in Figure 3, if the user's prompt is to generate code that counts the total number of 'boring' sentences starting with 'I', the knowledge graph may initially return a function that counts both 'boring' and 'exciting' sentences. By removing the \u2018exciting' sentence branch, we refine the function to better align with the query (Step 3 in Figure 3). In the final step, we augment the query with the retrieved function and send it to the model for code generation."}, {"title": "SOLUTION RE-RANKING", "content": "In our results, we demonstrate that even with access to a powerful PKG or other retrieval sources, the model can still hallucinate when provided with additional information in certain scenarios. This highlights the necessity of incorporating a re-ranking mechanism to effectively select the best solution from multiple approaches. \nTo address this issue, we implemented a simple yet effective re-ranking approach consisting of three key steps. First, the solution candidates are passed through AST analysis to filter out those with syntactical errors. In the second step, we execute the remaining candidates to eliminate any solutions containing runtime issues, such as undefined variables. Finally, we perform a semantic similarity check by comparing the embeddings of the remaining candidates with the query embedding, returning the solution with the highest similarity score. This multi-step process ensures the selection of a robust and valid solution, significantly improving the reliability of the model's output."}, {"title": "RELATED WORK", "content": ""}, {"title": "PROGRAM GENERATION USING LLMS", "content": "The generation of code using LLMs and CLLMs has been widely studied, as highlighted in recent works (Dubey et al., 2024; Lozhkov et al., 2024; Zhu et al., 2024; Roziere et al., 2023a). These studies primarily assess performance using the pass@k metric (Chen et al., 2021), which measures the success rate of generating correct code within a set number of attempts. Models are trained with various objective functions, including code infilling (Roziere et al., 2023a), handling long input contexts (Roziere et al., 2023a), fill-in-the-middle techniques (Li et al., 2023), and instruction fine-tuning (Li et al., 2023; Roziere et al., 2023a; Zhu et al., 2024). While knowledge is embedded within the model's parameters during training, our approach stores code-specific domain knowledge separately in a graph structure and retrieves it during code generation when relevant prompts are encountered."}, {"title": "RETRIEVAL AUGMENTED GENERATION", "content": "RAG approaches have been extensively explored in the domain of general text generation (Guu et al., 2020; Lewis et al., 2020; Jiang et al., 2023; Gao et al., 2023). These approaches can be categorized into three types (Gao et al., 2023): (1) Naive RAG, which uses a simple dataset and retriever to fetch content similar to the input prompt; (2) Advanced RAG, which incorporates additional steps such as query rewriting before retrieval and solution re-ranking after retrieval to refine the results; and (3) Modular RAG, which combines multiple RAG strategies and selects the most relevant documents from different retrieval methods. Our framework fits into the Modular RAG category, as it utilizes multiple retrieval cores composed of both naive and advanced RAG components."}, {"title": "RAG FOR CODE GENERATION", "content": "The use of RAG in code-related tasks remains underexplored (Wang et al., 2024). Previous studies, such as Parvez et al. (2021), have experimented with smaller code language models like Code-BERT (Feng et al., 2020) and GraphCodeBERT (Guo et al., 2020), focusing on tasks like code summarization and generation. Unlike their work, which involved fine-tuning the retriever module to extract relevant data, our approach applies RAG during inference time without requiring any model fine-tuning. While (Wang et al., 2024) presents a more similar approach to ours by comparing the performance of LLMs and CLLMs across various data sources and retrieval methods, they highlight challenges with retrievers extracting similar content and models' limited capacity for additional context. Our work differs by representing knowledge in a granular way, allowing retrievers to more accurately extract relevant information and prompting models with only useful content to reduce hallucinations."}, {"title": "EXPERIMENTAL SETUP", "content": "Retrieval Approaches: We utilized two retrieval methods based on a comparative analysis of various code retrieval models, as described by Wang et al. (2024). For dense retrieval, we selected the Voyage-Code-2 model, recognized as one of the top-performing dense retrievers for code. Embeddings were obtained through API calls to this model. For sparse retrieval, we employed the BM25 algorithm, implemented using the rank-bm25 Python library\u00b3, which exhibited the strongest performance among sparse retrieval techniques.\nDataset and PKG Generation: We used the PythonAlpaca dataset (Petit, 2024), which contains 143,000 general Python question-answer pairs. After preprocessing, we extracted 115,000 Python functions from the dataset. This extraction enabled us to construct a PKG comprising 425,058 nodes and 434,518 relations. The graph was generated using Neo4J version 5.20.0, optimized for handling large-scale graphs and supporting semantic search over the stored content.\nCode Generation Models: We conducted our experiments on four well-known CLLMs: CodeLlama-7B (Roziere et al., 2023b), CodeLlama-13B (Roziere et al., 2023b), StarCoder2-7B"}, {"title": "RESULTS", "content": "In this section we carry out experiments to answer the following research questions. The questions and their results are explained in the following."}, {"title": "RQ1: Does PKG improve code generation?", "content": "In this research question, we aim to explore the potential of leveraging graph-based retrieval-augmented methods to improve code generation task. Specifically, we will investigate how the relevant context retrieved from PKG can enhance the performance of LLMs and CLLMs in generating accurate code.\nThe proposed approach retrieves relevant information related to the programming problems from the PKG and integrates it into the code generation process. We evaluated our method against several baselines, which are detailed in Table 1 and Table 2 for HumanEval and MBPP benchmarks, respectively. The tables outline different retrieval and augmentation settings: 1) None: No retrieval-augmented generation is applied. 2) BM25: This baseline applies the BM25 algorithm to the entire dataset without any pre-processing. 3) VoyageEmb: In this setting, embeddings for each question-answer pair in the dataset are extracted and used for retrieval. 4) Func-BM25: This involves applying BM25 on functions extracted by the FunctionAnalyzer module we developed, ignoring all parts of data except python functions. 5) Func-PKG: Semantic search is performed over function-related nodes in PKG. These nodes are enhanced by the FunctionEnhancer module, which enriches their contextual information. 6) Block-PKG: A more granular retrieval is conducted by performing semantic search over specific code blocks in PKG, providing a deeper context for code generation. 7) Reranked: A re-ranking method selects the best candidate output from the retrieval settings (None, Func-BM25, Func-PKG, Block-PKG). 8) Ideal Re-ranker: This setting demonstrates an upper bound for the re-ranker model, simulating ideal conditions. It assumes a perfect re-ranker that always selects the correct candidate, showing the maximum possible accuracy.\nAs demonstrated in Table 1 and 2, our approach outperforms NoRAG and other RAG approaches across most CLLMs, under identical environmental conditions. This ensures that all methods have equal access to the same data source, providing a fair comparison. However, Deepseek-Coder benefits less from others in HumanEval. This aligns with observations from a related study by (Wang et al., 2024), where it exhibited similar behavior. Based on these findings, we hypothesize that DeepSeek-Coder may not be effectively utilizing additional contextual information during training.\nFigure 1 illustrates the motivation for the necessity of a re-ranking algorithm. While applying RAG can lead to solving additional problems, it also introduces a downside: providing external context can degrade some of the previously correct solutions.\nOur re-ranking algorithm addresses this issue by selecting the best candidate solution from the different approaches, thereby optimizing the overall performance. The impact of this re-ranking process is reflected in the \"Reranked\" column in Tables 1 and 2, which shows that when PKG coupled with our re-ranker, consistently outperforms both benchmarks across all baseline CLLMs and LLM models. In conclusion, our approach significantly improves the Pass@1 accuracy for both HumanEval and MBPP benchmarks."}, {"title": "RQ2: Which knowledge representation method is most effective in optimizing context retrieval for code generation tasks?", "content": "In this research question, we evaluate the performance of RAG by exploring different knowledge representation approaches. Specifically, we investigate three types of representations: (1) Question-Answering (Q&A) representation for entire rows, (2) Function-wise (FW) representation, and (3) Block-wise (BW) representation. Additionally, we use two types of retrievers: BM25 as a sparse retriever (SR) and Voyage-Code-2 as a dense retriever (DR).\nTo analyze the results, we first compare the BM25 and Func-BM25 columns in Tables 1 and 2. This comparison shows the detrimental effects of including low-quality question-answering data in the prompts (represented by the BM25 column) when compared to a cleaned, function-extracted version (represented by the Func-BM25 column). BM25 performs noticeably worse than Func-BM25 across both benchmarks, highlighting the importance of using cleaner, more relevant data for improved code generation accuracy and demonstrating the limited context capacity of generative models on ignoring noisy data.\nA similar trend is observed when comparing VoyageEmb (Voyage-Code-2 embeddings applied to question-answer pairs) with Func-PKG (Voyage-Code-2 embeddings applied to extracted functions). Despite using the same embedder model, the difference in content highlights the detrimental impact of augmenting irrelevant data when using dense retrieval methods.\nNext, the comparison between Func-BM25 and Func-PKG highlights that dense retrieval methods, like Func-PKG, consistently outperform sparse retrievers, such as Func-BM25, when applied to the same underlying content. This result underscores the effectiveness of dense retrievers in capturing more nuanced semantic relationships within the data.\nFinally, when comparing Func-PKG to Block-PKG, the results demonstrate that leveraging more granular data, particularly at the block level, significantly enhances model accuracy. Block-PKG enhances precision by retrieving relevant individual code blocks instead of entire functions. This approach involves pruning irrelevant branches from the DAG associated with the selected blocks, ensuring that only the most pertinent contextual information is leveraged. By focusing on finer-grained code structures, Block-PKG achieves superior performance across most models, offering a more targeted and efficient retrieval process."}, {"title": "RQ3: Which problem topics benefit more from RAG, and which benefit less?", "content": "This research question explores the performance of RAG across various problem categories. To address this, we employ the DeepSeek-Coder-7B model to extract the main topics from the MBPP (Austin et al., 2021) dataset, as it offers a larger and more diverse problem-set than Hu-"}, {"title": "RQ4: What types of errors can be reduced or introduced by applying RAG?", "content": "While the previous research questions focus on evaluating correct solutions generated using the RAG framework, this research question shifts the focus to incorrect solutions. Specifically, it aims to investigate the behavior of models with and without the application of RAG, identifying the types of errors that are mitigated and those that may arise due to the integration of the RAG approach. The error analysis is conducted on three models: StarCoder-7B, CodeLlama, and DeepSeekCoder through the execution traces of MBPP benchmark as it has more diverse and complex problems, providing insights into the error dynamics introduced or reduced by RAG in code generation tasks.\nAs shown in Table 3, StarCoder-7B+PKG reduces assertion errors by 51 compared to its baseline version. However, the application of RAG introduces 18 indentation errors that were absent in the baseline. For CodeLlama7B+PKG, RAG reduces name errors by 73 but increases type errors by 9 compared to the baseline, so it means RAG can mitigate assertion errors significantly but it introduces other errors such as indentation errors or name errors due to the additional context. In the case of DeepSeekCoder7B, despite being provided the same data as the other models, it generates more assertion errors, name errors, type errors, and other miscellaneous errors. We hypothesize that"}, {"title": "CONCLUSION", "content": "We introduced PKG for code generation task and evaluated our approach using standard Python benchmarks, HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). PKG enables us to retrieve code at a fine-grained level, focusing on highly relevant segments. Meanwhile, our re-ranker is designed to ignore suboptimal solutions, ensuring that only high-quality code is selected. The key findings from our experiments are: 1) PKG-based approaches significantly outperform other RAG and non-RAG approaches for code generation tasks. 2) Both LLMs and CLLMs are highly vulnerable to irrelevant data, which can negatively affect performance. 3) The inclusion of a code re-ranker is essential for optimizing RAG-based approaches for code generation. 4) Different types of problems benefit differently from RAG-based approaches, indicating that problem-topic specificity is an important factor. As future work, more advanced techniques are needed during instruction-tuning to enable models to learn more effectively from additional context. Additionally, the lack of code re-ranker models remains a notable gap in the current literature."}, {"title": "APPENDIX", "content": "In this section, we provide a thorough analysis of the experimental results from the CodeLlama-7B, StarCoder2, and DeepSeek-Coder-7B models. For each model, we detail the specific prompt templates employed during the experiments, ensuring reproducibility and clarity. We also include radar charts that visually represent the accuracy of each model across different problem topics, allowing for easy comparison of their topic-specific performance.\nAdditionally, we analyze the distribution of solved and unsolved MBPP problems across various topics, comparing two scenarios: one without RAG (NoRAG) and another using our proposed approach. This comparison highlights the impact of our method on problem-solving effectiveness.\nFinally, we present case studies of specific problems where the NoRAG approach fails, but our method succeeds. These examples provide concrete evidence of the advantages of our approach in addressing challenging tasks."}, {"title": "CODELLAMA7B", "content": ""}, {"title": "PROMPTS:", "content": "The prompts we have used for CodeLlama7B model is provided in Code 7.1.1:"}, {"title": "TOPIC-SPECIFIC APPROACH COMPARISON:", "content": "Figure 5 presents the Pass@1 accuracy for each method\u2014NoRAG, PKG, BM25, and the re-ranked approach across various programming topics. Similar to the performance observed with the StarCoder2-7B model, the re-ranker struggles to correctly prioritize solutions in the 'Optimization Techniques,' 'Mathematics,' and 'Algorithm' categories. However, in other topic areas, the re-ranker demonstrates superior performance compared to the other methods. Notably, for this model, PKG"}, {"title": "TOPIC-BASED ACCURACY DISTRIBUTION", "content": "Figure 6 illustrates the distribution of MBPP problems on a two-dimensional plot, where the embedding dimensions have been reduced to two for visualization purposes. The different problem topics are represented by distinct shapes, while the correctness of the solutions is indicated by color. Problems that were solved incorrectly are shown in orange, and those solved correctly are shown in green. The legend for each topic separates the total number of correct solutions from the incorrect ones using a slash (\"/\"). Figure 7 shows the distribution of correct and incorrect problems when we apply our approach."}, {"title": "STARCODER2-7B", "content": ""}, {"title": "PROMPTS:", "content": "The prompts we have used for StarCoder2-7B model is provided in Code 7.2.1:"}, {"title": "TOPIC-BASED ACCURACY DISTRIBUTION", "content": "Figure 8 presents the distribution of MBPP problems on a two-dimensional plot, with the embedding dimensions reduced for visualization. Each problem topic is represented by a unique shape, while solution correctness is color-coded. Problems incorrectly solved by StarCoder2-7B are highlighted in orange, whereas correctly solved problems are shown in green. The legend for each topic indicates the total number of correct versus incorrect solutions using a \"correct/incorrect\" format.\nAdditionally, Figure 9 visualizes the same distribution but reflects the accuracy after applying our proposed approach, showcasing improvements in solution correctness across topics."}, {"title": "DEEPSEEK-CODER-7B", "content": ""}, {"title": "PROMPTS:", "content": "The prompts we have used for DeepSeek-Coder-7B model is provided in Code 7.3.1:"}, {"title": "TOPIC-SPECIFIC APPROACH COMPARISON", "content": "Figure 10 illustrates the Pass@1 accuracy for each evaluation method: NoRAG, PKG, BM25, and the re-ranked approach, across a range of programming topics. The performance trends observed with the DeepSeek-Coder-7B model are echoed here. Specifically, the re-ranking method shows difficulty in accurately prioritizing solutions within the categories of 'Optimization Techniques,' 'Mathematics,' and 'Algorithms.' Despite these challenges, the re-ranked approach excels in other topic areas, demonstrating superior performance compared to the other methods.\nNotably, the PKG method achieves higher accuracy across most topics evaluated. However, it does face competition in the 'String Manipulation' and 'Data Structures' categories, where it is outperformed by NoRAG approach. We have observed the same behaviour for the previous models."}, {"title": "TOPIC-BASED ACCURACY DISTRIBUTION", "content": "Figure 11 displays the distribution of problems from the MBPP dataset in a two-dimensional plot, achieved by reducing the embedding dimensions for improved visualization. Each distinct shape in the plot corresponds to a specific problem topic, while the correctness of the solutions is indicated by color coding. Problems that were solved incorrectly are represented in orange, whereas those that were solved correctly are shown in green. The legend accompanying each topic delineates the total number of correct solutions from the incorrect ones, separated with a slash (\"/\").\nIn addition, Figure 12 presents a similar distribution of problems, highlighting the outcomes after applying our novel approach. This figure further distinguishes between correct and incorrect solutions, allowing for a comparative analysis of the effectiveness of our method."}, {"title": "EXAMPLES:", "content": "In this section, we present two selected samples from the HumanEval benchmark. We provide the responses generated by StarCoder-2-7B and DeepSeek-Coder-7B models. Each model's output is displayed in two scenarios: first, without using RAG, and second, utilizing our PKG approach. These examples illustrate how incorporating additional context can enhance the models' ability to solve complex problems more effectively."}]}