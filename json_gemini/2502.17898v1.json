{"title": "VeriPlan: Integrating Formal Verification and LLMs into End-User Planning", "authors": ["Christine P Lee", "David Porfirio", "Xinyu Jessica Wang", "Kevin Zhao", "Bilge Mutlu"], "abstract": "Automated planning is traditionally the domain of experts, utilized in fields like manufacturing and healthcare with the aid of expert planning tools. Recent advancements in LLMs have made planning more accessible to everyday users due to their potential to assist users with complex planning tasks. However, LLMs face several application challenges within end-user planning, including consistency, accuracy, and user trust issues. This paper introduces VeriPlan, a system that applies formal verification techniques, specifically model checking, to enhance the reliability and flexibility of LLMs for end-user planning. In addition to the LLM planner, VeriPlan includes three additional core features\u2014a rule translator, flexibility sliders, and a model checker\u2014that engage users in the verification process. Through a user study (n = 12), we evaluate VeriPlan, demonstrating improvements in the perceived quality, usability, and user satisfaction of LLMs. Our work shows the effective integration of formal verification and user-control features with LLMs for end-user planning tasks.", "sections": [{"title": "1 INTRODUCTION", "content": "Automated planning-the search for sequences of actions that guide an autonomous agent from an initial state to a goal state [44]- has traditionally been the domain of experts. Planning has been applied in professional settings, including production planning in manufacturing, medical resource planning in healthcare, project planning in construction, and route and fleet planning in transportation [8, 52, 53]. Automated planning is inherently complex, as the problem space involves managing numerous contingencies, constraints, and variables such as resource limitations, timing dependencies, and evolving preferences or changing conditions. Given the complexity and critical nature of these tasks, entire research communities and industries have dedicated themselves to building and utilizing planning tools (e.g., [18, 38, 73]) that support foresight, decision-making, and the intricate coordination required for effective outcomes.\nWhile these planning tools have traditionally been designed for expert use in professional settings, people increasingly need similar planning support in their everyday lives. People often manage multiple complex planning tasks in their everyday lives, such as coordinating pickup schedules for three children's school and after-school activities, hosting a family dinner party, preparing multiple meals simultaneously, and still setting aside time for personal tasks like writing a book and working out. Despite this need, they often lack effective tools to assist them, relying instead on manual methods or basic calendar apps. Traditional planning tools are often inaccessible for everyday users, as they require expertise in low-level planning languages, complex semantics, or detailed domain specifications for the task environment. Recent advances in artificial intelligence (AI), particularly large language models (LLMs), present an opportunity to bridge this gap. By understanding context, adapting flexibly, managing constraints, and automating decision-making, LLMs can make complex planning support more accessible and effective for everyday users.\nDespite their potential, end-users have yet to utilize LLMs effectively for such planning tasks. First, it is unclear whether LLMs can, out of the box, offer users solutions that adhere to user expectations, especially in highly constrained planning problems. Existing work has shown that, despite the increasing attention LLMs are receiving as planning tools, they are insufficient for planning and self-verification, particularly in the planning domain [30, 36, 77, 105, 106, 111]. Recent research has also highlighted several challenges, including difficulties with prompt input and navigation, limitations of text-only interfaces, and issues with evaluating LLMs' consistency and accuracy in meeting user needs [39, 85, 99, 102]. Finally, LLMs are prone to \"hallucinations\"-coherent but incorrect information-that undermine user trust, usability, and satisfaction [51]. These technical limitations and user-centered barriers make it difficult for end-users to rely on LLMs for effective planning.\nTo address these challenges and enable the effective use of LLMs in end-user planning, LLM-based planning systems must not only be designed to be reliable, but the user-LLM interaction must also be designed to support correction when the system produces incorrect or unacceptable output-a core principle of human-centered AI [2, 49]. To these ends, LLM-based planning systems must be designed to be (1) verifiable and (2) to keep the user in the loop during verification. Achieving these design principles necessitates combining interaction design with formal verification, a set of techniques grounded in mathematical and logical principles to ensure that a system's behavior meets predefined specifications.\nIn this work, we apply formal verification to LLMs, in order to enable their use as effective end-user planning tools. Specifically, we leverage model checking, a formal verification technique, to verify LLM outputs against user-defined constraints. Crucially, we explore how to involve users in the verification process and support user control and flexible adaptation to their needs. Based on this goal, we pose the following research questions:\n(1) How can formal verification methods, specifically model checking, be effectively applied to LLMs?\n(2) How can we engage humans in the process of model checking to improve (1) the quality of outputs from LLMs and (2) the user's experience?\n(3) At what stage of the model-checking process should users be engaged to maximize the effectiveness of integrating verification approaches with LLMs?\nTo address these research questions, we present VeriPlan, which integrates a formal verification-based approach to verifying plans generated by LLMs. VeriPlan consists of three key features: a rule translator, flexibility sliders, and a model checker, which enables user control throughout the verification process. To evaluate our system, we conducted a user study that ablates different features to assess its effectiveness and impact on users. Our findings indicate that model checking improves the user experience with LLMs in planning tasks, particularly in terms of perceived output quality, user control, and transparency. Additionally, user control over constraint verification enforces rigidity in LLMs, while control over the strictness of constraints enables flexibility and creativity in planning. Finally, we offer design implications for integrating verification methods and user control features into LLM design to make them more useful and applicable for everyday planning tasks. Our work makes the following contributions:\n(1) System contributions: We present VeriPlan, a verification-based approach involving the use of model checking against LLM outputs with multiple user control features. VeriPlan includes three key features: rule translator, flexibility adjuster, and model checker.\n(2) Empirical contributions: We evaluate VeriPlan through a user study (n = 12) to understand its effectiveness and the specific contributions of its key features.\n(3) Conceptual contributions: We present a template-based approach to categorizing temporal constraints for verifying LLM outputs, instantiated and validated within a finite set of scenarios.\n(4) Design implications: Based on our findings, we present design insights on how to integrate formal verification techniques and user control into the design of LLMs for effective application for end-user planning."}, {"title": "2 RELATED WORKS", "content": "In this section, we provide background on automated planning for end-users and discuss the challenges they face when using LLMs. Next, we review existing verification approaches for LLMs, both broadly and within the context of automated planning. Finally, we provide background on model checking and its use in our verification approach."}, {"title": "2.1 Automated Planning for End-users", "content": "Automated planning refers to automated techniques that decide what an agent does, namely the steps that it takes to achieve a goal, rather than how it performs each step [19]. Numerous languages and libraries exist that enable users to interact with planning algorithms, such as the Planning Domain Definition Language (PDDL) [18], the GTPyhop planner [73], and the extensive Unified Planning library [38], to name a few examples. Although planning tools are typically intended for expert users, recent work has engaged novice users in the planning process through visualization [13] and plan creation [80]. However, these planning tools pose significant challenges for end-users due to their reliance on complex formal languages and abstract logic formulas [16, 31, 88], which are difficult to learn and apply. The technical interfaces often lack intuitiveness, providing rigid workflows and low-level feedback [25, 79, 89]. Moreover, users must invest significant effort in creating detailed system models, specifying states, transitions, and probabilities [81, 82, 86]-tasks that demand technical expertise and are highly time-consuming. Designed with a focus on theoretical rigor and correctness, these tools often neglect practical usability, leaving them to fall short in addressing the dynamic and high-level goals of end-users.\nLLMs possess great potential to further increase the accessibility of automated planning for novice users. Given a natural language prompt or set of prompts, LLMs are demonstrably capable planners [62, 94, 97] without requiring the user to directly interact with low-level planning languages or libraries. Still, LLMs are insufficient as standalone planners, requiring external support to verify and improve planning output [37]. To this end, Gundawar et al. [21] contributes an LLM-Modulo Framework that checks LLM-produced plans against a set of critics, which provide feedback to the LLM to iterate. In our work, we envision the novice user as a critical component of the verification-feedback loop, akin to recent work in human-LLM interaction for text annotation tasks [109]. For planning tasks, there is a research gap on designing systems to engage novice users in the verification-replanning process, which this work aims to address."}, {"title": "2.2 End-user Challenges with LLMs", "content": "As LLMs are increasingly deployed in everyday applications and engage directly with end-users, they demonstrate great potential but also present significant human-centered challenges, particularly in terms of usability and reliability.\nUsability remains a critical issue as users frequently struggle with crafting effective prompts and engaging with systems beyond the input stage. Studies highlight the difficulty users face in formulating prompts that elicit desired responses [39, 60, 99, 102, 120]. Additionally, the cognitive demands placed on users\u2014such as monitoring and deciding on strategies for prompting and interaction\u2014exacerbate these challenges [99, 102]. Another usability barrier is users' difficulty understanding how prompts influence outputs and building accurate mental models of the system's behavior and the reasoning behind it [7, 101, 107]. In response to these challenges, engaging users during the interaction process to steer the LLM's behavior, and support user's understanding of the reasoning has gained increasing attention. Strategies like co-creation, where users and AI collaboratively refine outputs, have been proposed to expand engagement and improve interaction intuitiveness [87]. Similarly, interactive environments with user-controllable parameters enable experimentation, helping users build a better understanding of LLM capabilities [34, 61, 64, 100]. In addition, approaches like enhancing explainability and introducing customizable interaction options aim to reduce cognitive load and improve user experience [102, 103].\nWhile engaging users and providing control to address usability challenges is a promising direction, further work is needed to understand how and when to involve users throughout the interaction process with LLMs. Such exploration can reveal ways to gather direct input and feedback that help LLMs accommodate evolving preferences and more effectively meet diverse user needs.\nThe reliability of the output is another significant challenge. LLMs are prone to generating text that appears structurally coherent but contains factual inaccuracies or nonsensical information, a phenomenon known as hallucination [6, 32, 68, 83]. The lack of interpretability further complicates users' safe reliability, as users often struggle to understand the reasoning behind the output of the LLM [60, 66, 71, 116, 125]. These issues are especially concerning in safety or mission-critical domains, such as healthcare or military applications, where reliance on incorrect outputs can have severe consequences [42, 50, 84]. These issues can further lead to risks of users over-relying on LLM-generated outputs without sufficient critical evaluation, underscoring the need for mechanisms that support users' safe and reliable interactions with LLMs [32, 68]."}, {"title": "2.3 Verification Approaches for LLMs", "content": "The advancements in LLMs have unlocked unprecedented capabilities in sense-making, language use, and interaction, enabling precise inference of user needs and applications across diverse domains [40, 70, 126]. As these systems advance, ensuring their safety, reliability, trustworthiness, and alignment with user needs has become a pressing focus. To address this, a substantial body of work has emerged on verifying LLM outputs, which we broadly categorize into two directions.\nThe first direction focuses on enhancing user trust through explanations and interface design. Existing approaches generate explanations to support users in understanding and trusting LLM outputs [29, 45, 54, 65, 113]. Others have explored designing interfaces and tools that help users deconstruct textual components, evaluate LLM outputs, and act upon them effectively [34, 64, 100].\nThe second direction focuses on ensuring the validity of LLM outputs. One notable direction includes using LLMs for evaluation [14, 124, 127] or orchestrating multi-agent systems to verify outputs [9, 24, 56, 72]. These methods have been applied to complex tasks such as mathematical reasoning [55, 114, 122], semantic reasoning [10, 57, 74], and data annotation [110]. Additionally, other approaches involve humans in evaluating and correcting outputs [91, 110]. Finally, a growing area of research incorporates constraint-based approaches, such as applying constraints to planning in robotics [117], creating datasets with constraints for evaluation [123], or generating plans that adhere to multiple constraints [115]. However, constraint-based approaches often utilize predefined datasets and can suffer from the lack of mechanisms for dynamically incorporating user preferences, needs, or evolving contexts.\nDespite recent advancements, challenges persist in relying on LLMs for verification. Using LLMs to verify their own outputs risks critical flaws. Studies highlight their deficiencies in error detection, correction mechanisms, and adherence to constraints, as well as their tendency to hallucinate or retrieve inaccurate context [33, 58, 118]. For instance, in the planning domain, despite extended context windows and few-shot learning, Xie et al. [115] and Chen et al. [11] demonstrate that LLMs struggle to generate plans and feedback for complex scenarios or adhere to predefined constraints. Similarly, Valmeekam et al. [105] reports that GPT-4 achieves an average success rate of 12% in planning tasks, highlighting the inadequacy of LLMs in handling intricate requirements independently. Other works have highlighted how utilizing LLMs for evaluation can suffer from bias based on the order, appearance, or length of the content, aspect-specific evaluation, scalability, and effectiveness in diverse contexts [28, 43, 78, 96, 108]. These limitations have led to heuristic and modular approaches as verification mechanisms to address such shortcomings [36, 106]. Moreover, LLM reasoning and explanations, such as chain-of-thought reasoning, can be influenced by biased contexts, raising further caution about their reliability [104]. Consequently, developing methods to verify LLM outputs without relying on LLMs is critical to ensure validity, particularly for high-stakes, real-world applications."}, {"title": "2.4 Model Checking in Formal Verification and LTL constraints", "content": "Model checking is a formal verification technique used to determine whether a software or hardware system satisfies requirements expressed in formal logic [4]. By systematically exploring all possible states that a system may encounter or produce, model checking exhaustively examines system behavior against these requirements, making it essential for proving the behavior of highly complex systems. Linear Temporal Logic (LTL) is a commonly used representation to express requirements, or properties, in domains such as assistive robotics [15] and autonomous navigation [59]. LTL allows users to specify and compose temporal constraints in the form of sequencing (i.e., \"event A must occur before event B\"), eventuality (i.e., \"event C must eventually happen.\"), and safety (i.e., \"event D will never occur\"), to name a few examples. This expressiveness makes LTL suitable for real-world tasks such as scheduling, safety protocols, and workflow management, where the timing and the order of actions are critical.\nIn summary, our work builds on existing approaches to verify and validate LLM outputs, with a particular focus on constraint-based methods. We extend these methods by directly involving human engagement to define and refine constraints that align with users' needs and preferences. Our features for human engagement are designed to support varying levels of user control and involvement, for users to effectively guide the LLM's behavior. We leverage the significant potential of LLMs as end-user planning tools while addressing their shortcomings and user challenges through the implementation of an external verification approach using model checking."}, {"title": "3 TECHNICAL APPROACH", "content": "This section introduces the technical approach of VeriPlan, illustrating how it utilizes model checking on LLM outputs. We begin by outlining the three core features of VeriPlan, followed by a detailed explanation of the technical approach for each feature, accompanied by an illustrative example. All LLM agents used in our implementation are powered by GPT-4 [1]. Specific information on prompts used for LLM agents and the source code for our implementation can be found in the supplementary materials.\n3.1 Patient Navigation Planning Scenario\nThroughout this section, we use the scenario of a user using an LLM to plan patient navigation for a counseling session while following conflict-prevention rules to illustrate how VeriPlan assists with complex planning tasks.\nYou (P1) are a family counselor preparing to hold a family therapy session. You are aware that certain family members have deeper conflicts with some more than others. You believe that a group session could be beneficial, allowing you to use established procedures to help heal family tensions. However, to avoid conflict before the group session begins, you decide to escort each member separately to the counseling room (L2) based on the severity of their conflicts. All family members are currently in the waiting room (L1) with you. Due to hospital safety protocols, all family members (P2, P3, P4) must be escorted by you, and only one person can be escorted at a time. However, because of ongoing tensions, P2 and P3 cannot be left alone together, and similarly, P3 and P4 cannot be left alone together.\nUsing this scenario, we demonstrate how VeriPlan assists the user in iteratively solving the navigation planning task using its three features for model checking-rule translator, flexibility slider, and model checker-until a successful planning solution is reached."}, {"title": "3.2 Features", "content": "The verification approach implemented in VeriPlan includes the following features: the (1) LLM planner; (2) rule translator; (3) flexibility slider; (4) model checker; and (5) refined LLM planner.\nLLM Planner. The LLM receives the initial user input in the form of a natural language prompt, which includes the user's request, context, and constraints. Based on this input, the single-agent LLM will attempt to create a plan according to the provided prompt.\nRule Translator. The rule translator converts the user's initial natural language input into formal language properties that are interpretable for the model checker to use during verification. The translation is then translated back into natural language and presented to the user, who provides feedback to verify whether the translation is accurate.\nFlexibility Slider. Once the correctness of the rules is verified, the user can adjust the strictness of each rule using the flexibility sliders, defining the level of enforcement. This strictness determines the extent to which the model checker will insist on adhering to the rules during model checking.\nModel Checker. VeriPlan employs an external verification process, using a formal verification technique called model checking (see \u00a73.2 for more). For model checking, we use an off-the-shelf probabilistic model checker, to systematically inspect every state within the system to confirm whether a set of behavioral properties are satisfied. The model checker uses the user-defined constraints to evaluate the LLM planner's planning attempts, ensuring they align with the specified requirements. After completing the evaluation, the model checker provides feedback to the user and LLM on whether the plan is valid or which constraints are violated.\nRefined LLM Planner. Once feedback is provided, the LLM planner will iteratively regenerate a plan based on this feedback until it either reaches a valid solution or the maximum number of iterations specified in the program. At the end of the iterations, based on feedback from the model checker, the user can adjust the constraints using the rule translator or flexibility sliders before rerunning the LLM planner to reach a satisfying solution."}, {"title": "3.3 LLM Planner", "content": "The front-end interface of VeriPlan is shown in Figure 2. In the example scenario, the user inputs their full planning requests and constraints through the input panel (depicted as step 1), and the request is reflected on the interface (step 2\n3.3.1 How It Works. The beginning of the pipeline for VeriPlan, including the LLM planner, is presented in Figure 3. At the start of the interaction, as the user inputs their prompt (step a), an LLM agent generates an initial plan based on the user's request (step f). This plan is then later to be checked by the model checker, using the constraints defined by the rule translator and the flexible slider features discussed below."}, {"title": "3.4 Rule Translator", "content": "The role of the rule translator is to extract constraints from the user's prompt that a correct plan must follow. The rule translator presents the extracted results to the user, allowing them to review the extracted constraints and either confirm them or request regeneration. For confirmation, the user selects the correct version of the constraint using the check box (step 3). If the presented constraints are unsatisfactory, the user can ask the rule translator to regenerate translations for the constraints using the input panel.\n3.4.1 How It Works. The pipeline of the rule translator is shown in Figure 4. Receiving the user prompt (step a) which includes the user's planning request and desired constraints, an LLM-based mapping agent extracts content from the prompt and maps it to the appropriate categories in the temporal constraint template described below (step b). The mapping agent is bound to select from the seven categories and has been prompt-engineered with examples for mapping accuracy.\nTemplate of Temporal Constraints. To ensure that the rule translator can accurately convert user input into rules for model checking, it uses a predefined temporal constraint template. For the model checker to function, the rules must be specified in LTL logic. However, since users input rules in natural language, manually translating them into LTL formulas is challenging. Unlike fixed algorithms that require rigid input formats, LLMs can interpret and categorize variable natural language inputs into temporal categories by understanding context and intent, guided by examples from prompt engineering. This adaptability allows complex or unconventional rules to be mapped to predefined LTL constraint templates, reducing the need for extensive manual refinement in rule translation.\nTo address this, we developed a template of LTL properties which are fed into an LLM for translation, covering six temporal categories: (1) fixed time blocks, (2) sequential order, (3) concurrent events, (4) conditional constraints, (5) exclusive constraints, and (6) global constraints. Each category includes a template for converting natural language into LTL properties, which are fed into the LLM. In the constraint template, LTL provides modal operators to formalize such statements. The global operator, $G$, specifies conditions that must hold in every state. The future operator, $F$, checks for events that must occur at some point in the future. The until operator, $U$, specifies that an event $\\phi$ must remain true until another specified event $\\psi$ occurs, and that $\\psi$ must indeed happen. The detailed templates are provided in Table 1.\nOnce the mapping is complete, it is sent to the LTL translator (stepc). The LLM-based LTL translator uses the template to convert the mapped outputs into LTL properties, guided by prompt engineering to determine the appropriate conditionals for each constraint. The translator then generates an LTL formula for the constraint.\nThese LTL translations are then sent to the LLM-based PRISM translator, for converting the LTL properties into an interpretable format for the model checker (step d). Our verification approach utilizes the PRISM Model Checker [47] (discussed in detail in \u00a73.6) to format LTL properties, which requires that properties be expressed in the PRISM language. While an algorithmic approach could perform this translation, an LLM was chosen for its seamless integration and demonstrated feasibility during system design. Our PRISM translator utilizes manual examples for prompt engineering"}, {"title": "3.5 Flexibility Sliders", "content": "As shown in Figure 2, once users have verified the correctness of the constraints, they can specify the strictness of each constraint using the flexibility sliders (step 4). In the given example, the user initially believes that all the rules should be treated as hard constraints, as they pertain to hospital protocols and are crucial for avoiding conflicts among patients. Consequently, they set the sliders to 100% for each rule and submitted the adjustments. After the first few attempts fail, the user decides to set the strictness of rule four to 50%, reasoning that P1 might be able to travel with both P2 and P4. Throughout the interaction, users can freely modify the strictness of individual rules after reviewing the outputs from the LLM and model checker. Once the strictness levels are finalized, the complete set of constraints, verified and customized by the user, is sent to the model checker.\n3.5.1 How It Works. Constraints that are verified by the user from the rule translator are then sent to the flexibility sliders. These sliders allow users to adjust the strictness of each rule, where strictness defines how rigidly the model checker will enforce the rule. Strictness includes both \"soft\" and \"hard\" constraints: hard constraints must be satisfied for a plan to be valid, and any plan that violates a hard constraint is immediately rejected. Soft constraints, while preferred, are not strictly necessary and their violation does not invalidate the plan. If a soft constraint is violated, unlike hard constraints, the plan will not be immediately rejected. Instead, the plan with the violated soft constraint will be marked as valid, and the user will be notified of the violation. Constraints are then weighted based on hardness, and VeriPlan samples from the weighted constraints, with lower-weighted constraints (corresponding to \"softer\" constraints) being less likely to be sampled. The model checker then checks the plan against the sampled constraints."}, {"title": "3.6 Model Checker", "content": "Once the correctness and strictness of the rules are defined by the user, the model checker uses these rules to check the initial plan generated by the LLM agent. In the interface, the user can view the initial planning attempt generated by the LLM (step 5). The model checker then performs model checking on this plan, comparing each state against the specified constraints. Based on the verification, the model checker provides feedback, which includes a list of broken rules or confirmation of the plan's validity (step 6). This feedback is then sent to the user to explain the system's status and to the LLM for regenerating the plan based on the feedback.\n3.6.1 How It Works. The pipeline of the model checker is shown in Figure 5. Similar to LTL translation, the initial plan generated by the LLM agent based on the user's request (depicted as step f) is also translated into the PRISM language format for the model checker to process (stepg). In this work, we use the PRISM Model Checker [47] and Stormpy for verification. Stormpy is a Python API for Storm [26] that enables model checking and property verification within a Python environment. At this point, since the model checker has (a) a set of LTL-expressed rules, and (b) the LLM-generated plan expressed in the PRISM language, it evaluates the plan against these rules (steph). During verification, the model checker examines each state of the plan for rule violations. Any rule violations will result in an invalid plan. The validity of the plan, along with any rules that were violated are sent to both the user and the LLM agents to refine their future solutions (step i"}, {"title": "3.7 Refined LLM Planner", "content": "Receiving the feedback from the model checker, the process of the LLM regenerating a plan and the model checker verifying it against the user-defined rules is iterated two additional times, allowing for a total of three iterations, as defined by the system parameters. Between iterations, the user can adjust the strictness of the constraints to explore different planning solutions (step 4). Once all iterations are complete, the user can choose to inquire about aspects such as the constraints, the decision-making procedure, the logic of the model checker, or the system status through the input panel (step 1). Additionally, the user can modify the constraints using the flexibility sliders (step 4), or modify the constraints through the rule translator through the input panel before initiating a new interaction (step 1).\n3.7.1 How It Works. Upon receiving feedback from the model checker, this information is provided as updated requirements to the LLM, which is then asked to regenerate a plan. The regenerated plan is checked by the model checker for rule violations using the user-defined rules. If no violations are found and a correct plan is generated, the interaction ends. If a correct plan is not generated by the end of the iterations, the system prompts the user to adjust the constraints or their strictness for additional iterations."}, {"title": "4 USER STUDY", "content": "4.1 Scenarios\nWe design three scenarios that incorporate the temporal constraints illustrated in Table 1. One of these scenarios is the \"patient navigation in hospital\" example discussed in \u00a73.1. Below, we describe the remaining two scenarios.\nOptimizing Cooking Procedures. The user is hosting a dinner party on Wednesday at 6:00 PM with multiple guests, requiring the preparation of various dishes to accommodate different dietary preferences, such as vegetarian and gluten-free. The user plans to make spaghetti and meatballs as the main dish and cheesecake for dessert, with meat, vegetarian, and gluten-free versions of each. The user must plan how to cook these dishes simultaneously, ensuring they are ready on time while optimizing the cooking process.\nScheduling Multiple Events. The user is trying to schedule multiple events for the week. These include three hour writing blocks for her book, a dinner party on Wednesday at 6:00 PM, meetings with colleagues on Tuesdays and Wednesdays, tennis lessons on Fridays at 3:00 PM, child pickup and playtime, household chores, and personal routines (e.g., listening to music while writing or having coffee in the morning). Every Sunday evening, she creates a weekly plan to organize and fit all these tasks into her schedule.\n4.2 Study Design\nThis study aimed to understand the importance and impact of VeriPlan's verification approach and user control features, specifically evaluating how these elements influenced user reliance, usability, satisfaction, and the perceived performance of LLM outputs. We conducted an ablation study using a within-subjects design, where different ablation conditions served as the within-subjects variable. In Condition 1, participants engaged with VeriPlan, which included the rule translator, flexibility sliders, and model checker. Condition 2 removed the flexibility slider, leaving only the rule translator and model checker. Condition 3 removed the rule translator, including only the flexibility slider and model checker. In Condition 4, all three features, including the rule translator, flexibility slider, and model checker, were removed as neither the rule translator nor the flexibility sliders can function without the model checker. For consistency, we denote these conditions with C1 (Full), C2 (\u00acSlider), C3 (\u00acTranslator), C4 (None) in the remainder of the paper. During the study, participants were randomly assigned to two of the three scenarios. In each scenario, participants engaged in all four conditions in a randomized order. After each condition, participants completed the quantitative scales. At the end of their interaction with each scenario, semi-structured interviews were conducted. The entire study lasted 1.5 hours. Questionnaires used during the study can be found in the supplementary materials.\n4.3 Measures\nTo evaluate the participants' experiences with the system, we employed the Usefulness, Satisfaction, and Ease (USE) scale [63] to measure three key dimensions: usefulness (Cronbach's $\\alpha$ = 0.94), ease of use (Cronbach's $\\alpha$ = 0.83), and satisfaction (Cronbach's $\\alpha$ = 0.95). We also used the performance questionnaire from the fairness, accountability, transparency, and explainability (FATE) scale developed by Shin [92] to measure participants' perceived quality of the LLM's output (Cronbach's $\\alpha$ = 0.91). Both scales were placed on a seven-point Likert scale.\n4.4 Participants\n12 participants were recruited for our user study. Participants were required to be in the United States, fluent in English, and at least 18 years old. All participants were recruited through university mailing lists. While our sample size is not large, the within-subjects study design achieves an acceptable level of statistical power for significant results [5]. Participants age ranged from 19-48 (M = 25, SD = 7.9). 50% of the participants identified as female and 50% as male. 50% of our participants were White, 41.6% were Asian, and 8.4% were American Indian or Alaska Native. After the study, participants were compensated $15.00 per hour. We refer to participants as P1-P12, using the notation Pi to indicate participants, where i indicates participant ID number. In the recruitment survey, we also collected participants' experiences with LLMs, asking them to select a category that best described their familiarity: \u201cnot familiar or none,\u201d \u201coccasional use,\u201d or \u201cregular use.\u201d Five participants (P7-P11) selected \"not familiar or none,\" four (P1, P4, P6, P12) selected \"occasional use,\" and three (P2, P3, P5) selected \"regular use.\" Those who reported occasional or regular use mentioned using LLMs for tasks such as brainstorming, search engines, writing assistance, and planning tools (e.g., scheduling assistance, task management, project coordination, and itinerary planning.)\n4.5 Analysis\nFor the quantitative data, we conducted a Dunnett test to compare the means of the ablation groups (C2, C3, C4) to the mean of the full system (C1). Dunnett's test compares the mean of several experimental conditions to a control condition, in which for our study, the full VeriPlan system (C1) is considered to be the control. The test was performed with an alpha level of 0.05.\nFor qualitative data, we conducted a Thematic Analysis (TA) on the interviews. The coding of the responses was conducted by deriving representative themes from transcriptions [12, 69]. During open coding, the first author coded for significant concepts in the data."}, {"title": "5 RESULTS", "content": "Our analysis aimed to understand the impact of our verification-based approach and its features on the effectiveness of and user experience with LLMs in planning tasks. The results of our quantitative data are shown in Figure 6. Overall, the Dunnett test revealed that the full system (C1) significantly outperformed the C3 (\u00acTranslator) (p = .0011) and C4 (None) (p = .0013) conditions; was significantly more useful than the C2 (\u00acSlider) (p = .047), C3 (\u00acTranslator) (p = .009), and C4 (None) (p = .0257) conditions; and was significantly more satisfying than the C3 (\u00acTranslator) (p = .007) and C4 (None) (p = .0101) conditions.\nBelow, we present our findings in four key themes that emerged in our analysis. For the first three themes, we present quantitative findings first, followed by qualitative insights that reveal differences in use patterns and user perceptions across conditions, providing a deeper understanding of our system's impact. For the fourth theme, we present findings derived from the qualitative analysis, focusing on participants' interaction experiences with VeriPlan.\n5.1 Rule Translator Improved Performance, Satisfaction, and Usefulness of LLMs\nParticipants' scores in C3 (\u00acTranslator) were significantly lower than those in C1 (Full) in measures of performance, usefulness, and satisfaction. Our qualitative analysis provides further insight into these"}]}