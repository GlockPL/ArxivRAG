{"title": "Machine Learning and Constraint Programming for Efficient Healthcare Scheduling", "authors": ["Aymen Ben Said", "Malek Mouhoub"], "abstract": "Solving combinatorial optimization problems involve satisfying a set of hard constraints while optimizing some objectives. In this context, exact or approximate methods can be used. While exact methods guarantee the optimal solution, they often come with an exponential running time as opposed to approximate methods that trade the solution's quality for a better running time. In this context, we tackle the Nurse Scheduling Problem (NSP). The NSP consist in assigning nurses to daily shifts within a planning horizon such that workload constraints are satisfied while hospital's costs and nurses' preferences are optimized. To solve the NSP, we propose implicit and explicit approaches. In the implicit solving approach, we rely on Machine Learning methods using historical data to learn and generate new solutions through the constraints and objectives that may be embedded in the learned patterns. To quantify the quality of using our implicit approach in capturing the embedded constraints and objectives, we rely on the Frobenius Norm, a quality measure used to compute the average error between the generated solutions and historical data. To compensate for the uncertainty related to the implicit approach given that the constraints and objectives may not be concretely visible in the produced solutions, we propose an alternative explicit approach where we first model the NSP using the Constraint Satisfaction Problem (CSP) framework. Then we develop Stochastic Local Search methods and a new Branch and Bound algorithm enhanced with constraint propagation techniques and variables/values ordering heuristics. Since our implicit approach may not guarantee the feasibility or optimality of the generated solution, we propose a data-driven approach to passively learn the NSP as a constraint network. The learned constraint network, formulated as a CSP, will then be solved using the methods we listed earlier.", "sections": [{"title": "1 Introduction", "content": "Combinatorial optimization problems play a significant role in various industry applications. Solving these problems involves finding the optimal solution among feasible solutions for many real-world problems. Leveraging combinatorial optimization methods in applications like scheduling can effectively optimize"}, {"title": "2 Literature Review", "content": "Numerous approaches have been proposed to solve the NSP [8,9,10,11]. In addition to solving the NSP using exact methods, researchers have been proposing evolutionary methods based on meta-heuristics that works by eliciting candidate"}, {"title": "3 Implicit Solving of the NSP", "content": "We propose an implicit solving approach based on ML methods (Association Rules Mining, High Utility Item-set Mining, Naive Bayes, and Bayesian Network) and historical data without any prior knowledge. The aim of our solving"}, {"title": "3.1 Frequent Pattern Mining via Apriori", "content": "Association rules mining [39] is an unsupervised learning method for discovering frequent patterns and associations between items in transactional databases. Association rules mining was mainly introduced in the market basket analysis to analyze and learn the itemsets that are frequently purchased together in the form of association rules such as \"item1 & item2 \\rightarrow item3\" which translates to: if item1 and item2 occur together, item3 is highly likely to also occur. The items in the NSP context are the nurses, therefore, we adopt the Apriori algorithm [39,40] to learn associations between nurses. More precisely, we apply Apriori on historical scheduling solutions to extract the frequent assignments of nurses using interestingness measures; support and confidence, under user-defined thresholds. The Apriori algorithm relies on a powerful property to prune the infrequent itemsets and minimize the search space. The Apriori property states that all the subsets of a frequent item-set should be frequent, and if an item-set is not frequent, then all its super-sets will be infrequent [40]. The generated rules implicitly represent the assignments that could be enforced by the constraints and/or preferences, and are utilized to simulate new scheduling scenarios."}, {"title": "3.2 High Utility Itemset Mining via Two-Phase", "content": "High Utility Item-set Mining (HUIM) [39] is derived from the mining framework. Given that Apriori is limited to only considering the frequency of itemsets rather than the utility, we rely on the Two-Phase algorithm [41] to overcome this limitation and learn the itemsets that maximize the utility in addition to the frequency. By doing so, we aim to extract more relevant itemsets and construct more accurate scheduling solutions. The input of the Two-Phase algorithm consist of workload coverage and the nurses' average preferences in the form of external utility table. Note that the utility table in the context of the NSP are the preference costs related to nurses working in specific shifts which we obtain from the NSPLib instances [42]. Similar to Apriori, Two-Phase uses a min-utility threshold to elicit the high utility itemsets. Setting the min-utility parameter is very challenging as stated in literature [43] and may require domain knowledge. For simplicity, we set a high value for this parameter with respect to the number of generated high utility itemsets since we are interested in item-sets of high utilities."}, {"title": "3.3 Naive Bayes Classifier", "content": "Naive Bayes (NB) classifiers are supervised machine learning methods derived from the Bayes Law and used for classification tasks. NB classifies new data observations of data using labeled data-set and relies on the independence assumption (i.e. naive assumption) of variables given the target class which implies that each attribute contribute independently to the target class. In the context of solving the NSP, the input schedules are expected to be partially completed and the target is to fill out the missing assignments, more precisely, we use NB to predict unknown shift assignments based on prior shift observations.\nThe following is a formulation of the NSP problem according to the Bayes Law. Let i = {1, .., n} be the nurse index, j = {1, .., m} is the shift index.\n$$N_{ij} =\\begin{cases} 1, \\text{ if Nurse i is assigned shift j} \\\\ 0, \\text{ otherwise} \\end{cases}$$\n$$P(N_{i'j'}|N_{ij}) = \\frac{P(N_{ij} \\cap N_{i'j'})}{P(N_{ij})}$$\n$$P(N_{ij}| N_{i'j'}) = \\frac{P(N_{ij}\\cap N_{i'j'})}{P(N_{i'j'})}$$\n$$P(N_{ij} \\cap N_{i'j'}) = P(N_{ij}|N_{i'j'}) P(N_{i'j'})$$\n$$\\Rightarrow P(N_{i'j'}|N_{ij}) = \\frac{P(N_{ij}|N_{i'j'}) P(N_{i'j'})}{P(N_{ij})}\\quad(1)$$"}, {"title": "3.4\nBayesian Network (BN)", "content": "A Bayesian Network (BN) can be described as a Directed Acyclic Graph (DAG). The nodes represent random variables and the directed edges are causal relationships between these variables. A probabilistic graphical model provides a compact and intuitive representation of the joint probability distribution across a"}, {"title": "3.5 Experimentation", "content": "Data The NSPLib benchmark library [42] provide various dataset settings, including workload coverage requirements for different number of nurses and different planning horizons, as well as preference costs related to daily shifts. For our experimentation, we use instances with the following setting; 25 nurses, 7 days, and 4 shifts/day and further reduce the number of nurses to 5 for simplicity. Note that the coverage requirements is randomly distributed among nurses to introduce fairness.\nEvaluation metrics and Quality Performance Measures The main goal of our implicit solving approach is to automate the generation of scheduling solutions while conserving the properties of the input schedule, in other words, we aim to capture and extract frequent patterns from input schedules, and leverage this knowledge to generate new solutions that inherit the learned patterns. Therefore, we evaluate the quality of the solutions by computing the distance between the input schedule and the generated schedules using the Frobenius Norm [46]. Let M and N be two matrices, mij and nij their respective entries, the Frobenius Norm quantifies the element-wise average error between two matrices as depicted in Equation 3. Regarding the NB method, the quality of the solution is achieved by measuring the model accuracy (i.e. comparing the predicted class labels with the ground truth).\n$$||M - N||_F = \\sqrt{\\frac{1}{xy} \\sum_{i=1}^x\\sum_{j=1}^y (M_{ij} - N_{ij})^2}\\quad(3)$$"}, {"title": "3.6 Results and Discussion", "content": "The experimental results of all our proposed methods are reported in Table 8. The Apriori, Two-Phase, and Bayesian Network methods are evaluated using the Frobenius Norm, and Naive Bayes is evaluated based on accuracy. The results reveal low average error in all the methods, while the Naive Bayes accuracy is promising given the limited data."}, {"title": "4 Explicit Solving of the NSP", "content": "The CSP is a powerful framework for modeling and solving real-world constraint satisfaction problems such as Map Coloring, N-Queen, TSP, Knapsack, etc. Modeling such problems using the CSP framework consists of formulating it in terms of variables X\u2081 = {X1, ..., Xn}, defined on a set of non-empty domains of possible values Di = {dom(x1), ..., dom(xn)}, and a set of constraints C = {C1, ..., Ck } restricting variables' assignment combinations. The goal of solving a CSP is to find a consistent assignment of values to the variables from their domains such that all the constraints are satisfied. The WCSP is an extension of the CSP which considers violation costs related to soft constraints or weights associated with the domain values. In addition to finding a solution that satisfies all the constraints, the target of a WCSP is to optimize the solution's cost. Solving CSPs and WCSPs usually involves exact and approximate methods (e.g. backtracking, local search, SLS, etc) that work based on global and local search respectively, to elicit candidate solutions. Exact methods guarantee the optimal solution but come with a heavy running time cost while approximate methods often trade the quality of the solution for a better running time. Approximate methods iterate through the search space to improve the most recent solution while exact methods explore the entire search space to find the optimal one. Given the fact that the exact solving techniques require exponential running time to find the optimal solution, researchers usually rely on various constraint propagation techniques (e.g. NC, AC, GAC, etc), and Variables/values ordering heuristics in the scope of CSPs and WCSPs to minimize the search space which consequently optimizes the running time. In this context, we rely on the WCSP framework to model the NSP in terms of variables, domains, and constraints, and solve it using explicit methods. For the solving task, we propose a variant of the B&B and SLS methods as exact and approximate solvers for the"}, {"title": "4.1 WCSP Problem Formulation", "content": "The WCSP is defined by the tuple (X, D, C, K), where X, D, and C represent the variables, domains, and constraints, respectively. K represent the largest numerical value for the cost decision variable Cij.\nVariables: X = {X1, ..., Xn} is the set of nurses.\nDomain: D = the set of all possible shift patterns.\nConstraints: C = {const1, ..., const4} is the set of NSP constraints\nTo model the NSP constraints, we rely on function A(i, j, k, s) as described below.\n$$A(i, j, k, s) = \\begin{cases} 1, & \\text{if nurse i (Xi) is assigned shift pattern j, and j covers shift s on day k} \\\\ 0, & \\text{Otherwise} \\end{cases}$$\nOur WCSP problem formulation consist of global and unary constraints: const1 is a global constraint (involving all the variables) and constraints const2, const3, const4 are unary constraints (involving individual variables). The following parameters and indices are used to elicit the constraints."}, {"title": "Hard Constraints:", "content": "1. Minimum and Maximum number of nurses per shift\n$$const1 : Psk \\leq \\sum_{i=1}^n A(Xi, aj, k, s) \\geq qsk, \\forall k, s, aj; \\in D\\quad(4)$$\n2. Maximum number of shifts for a given nurse during the schedule\n$$const2: \\sum_{k=1}^7\\sum_{s=1}^3A(Xi, aj, k,s) \\leq hi,\\forall Xi,aj\u2081 \\in D\\quad(5)$$\n3. Maximum number of consecutive shifts (night shifts followed by morning shifts)\n$$const3 : \\sum_{k=1}^6A(Xi, aj\u2081, k, 3) + A(Xi, aji, k + 1,1) \\leq y,\\forall Xi, aj\u2081 ED\\quad(6)$$\n4. Maximum number of night shifts\n$$const : \\sum_{k=1}^7A(Xi, aj, k,3) \\leq bi,\\forall Xi, aji \\in D\\quad(7)$$\nSoft Constraint:\n$$fi : aji \\in D \\rightarrow Ciji\\quad(8)$$\nObjective: hospital costs to minimize\n$$Minimize(\\sum_{i=1}^n(Ciji)) \\quad ajz \\in D\\quad(9)$$"}, {"title": "4.2 Branch & Bound (B&B)", "content": "B&B uses the Depth First Search (DFS) strategy to explore all candidate solutions. This process involves generating sub-branches and applying the pruning concept using the Lower Bound (LB) and Upper Bound (UB) parameters. The UB and LB are used in the pruning process to verify in advance if the exploration of a branch would result in getting a better solution or not before actually traversing the branch, preventing the exploration of branches that do not lead to an optimal solution. In our minimization problem (minimizing hospital's costs), the LB overestimates the best possible solution. The UB represents the best solution found so far and it is updated whenever a new solution with a better"}, {"title": "4.3 Constraint Propagation (CP)", "content": "CP relies on local consistency techniques [50,51] to eliminate some \u201clocally\" inconsistent domain values and consequently reduce the search space scope. Enforcing CP may lead to two outcomes; either resulting in an empty domain, which indicates the inconsistency of the WCSP, or leaving multiple values in some domains, which requires the use of B&B to solve the problem and find the optimal solution. Various techniques are employed to enforce local consistency, including Arc Consistency, Path Consistency, etc. Considering the the types of constraints in our WCSP (unary and global constraints), we utilize two local consistency algorithms: Node Consistency (NC) and Generalized Arc Consistency (GAC). In addition to local consistency, we rely on variables ordering heuristics to help find the optimal solution faster following the \"fail first principle\", prioritizing the variables that most likely lead to a dead-end [52,53], and ordering the domain values according to their weights. Node consistency is used to reduce variables' domain size by eliminating all the values that violate the unary constraint. A WCSP is node consistent if all its variables are node consistent [54] (i.e. every value in every variable' domain satisfies the unary constraint(s)). In addition to"}, {"title": "4.4 Stochastic Local Search (SLS)", "content": "While exact methods guarantee the optimal solution in solving the NSP, they come with exponential time complexity. As a result, researchers often explore strategies to address the exponential time cost associated with exact methods,"}, {"title": "4.5 Experimentation", "content": "To asses the efficiency of our proposed methods in practice, and to asses the effect of CP when combined with B&B and SLS, we have conducted multiple experiments against metaheuristic methods (variants of WOA and hybrid GA methods [7]) using multiple NSP instances. The parameters of the NSP instances used in the experiments are presented in Table 9. The experiments were performed on a personal computer with the following specifications: Intel\u00ae Core\u2122 i5-6200U CPU @ 2.3 GHz and 8 GB of RAM. Table 10 presents the experimental results, including the best solutions returned (BS) and their corresponding running times (RT). Additionally, Figure 13 illustrates the running times for all methods. Our experimentation also involved a solution quality analysis of the approximate methods based on 20 runs, and the result of this analysis covering the best, average, and standard deviation metrics is provided in Table 11.\nThe experimental results are very promising as they reveal the efficiency of B&B in returning the optimal solution, while SLS offers a reasonable trade-off"}, {"title": "5 Data-driven Learning for the NSP", "content": "Constraint Programming (CP) and Operation Research (OR) focus on solving real-world combinatorial optimization problems, where the modeling task is fundamental to both fields. Modeling a given combinatorial optimization problem involves representing the constraints that must be satisfied, as well as the objectives to be optimized. These constraints and objectives are then provided to a solver to find a solution a given problem instance. A major challenge in solving optimization problems, lies in the modeling phase, because the method used in the modeling phase can influence the solving process (such as selecting the solver). In general, constraints are often actively and manually learned from domain experts or software systems. This learning process may also involve further verification against relevant contract agreements and past solutions, which may be tedious due to the huge volume of data available that may need to be analyzed. Additionally, obtaining constraints from hospitals may be challenging in practice due to security and privacy limitations. All these facts motivates the automatic modeling approach where the learning of the constraints may be done actively or passively using historical data (past solutions). The challenge with the ML implicit approach we have proposed in Section 3 is that it comes with uncertainty related to the quality of the solution, as the implicitly learned patterns do not concretely reveal the satisfied constraints and the optimized objectives. Furthermore, there is no practical method to quantify this uncertainty without explicit knowledge of the constraints. In contrast, our CSP approach"}, {"title": "5.1 Passive Learning of CSP model via Matrix Slicing", "content": "Our approach to learning a CSP model is based on the assumption that the constraints to be learned are identified, but their bounds, as well as whether they are satisfied, remain unknown. A typical real-world scenario would be collecting the set of constraints applied in a hospital during a specific planning period. In this context, we adopt the NSP constraints related to the WCSP formulation from [5]. The objective of our passive matrix slicing learning method is to determine the bounds of the constraints and also learn whether certain constraints hold within historical data. Consequently, the learned constraints that constitute a CSP model can be fed as an input to our explicit solving approach, and solved using our B&B and SLS solvers. To achieve this goal, we assume that the historical data is represented as a 2D array (i.e. matrix), where the rows correspond to nurses, the columns represent the planning horizon, and the array entries are binary entries where 1 indicates a nurse is assigned to a shift, and 0 indicates they are not as illustrated in Table 12. The CSP model can be learned as follows; the number of nurses corresponds to the number of rows, and the domain is the set of all possible shift patterns based on the number of columns. The domain"}, {"title": "5.3 Non-Negative Matrix Factorization (NMF)", "content": "Non-Negative Matrix Factorization (NMF) is an unsupervised ML technique mainly applied in topic modeling and dimensionality reduction tasks [61,62]. NMF may also be leveraged in various fields including recommender systems, signal processing, document clustering, etc. The goal of NMF is to decompose multivariate data into a user-specified number of features. Given a non-negative matrix X, NMF decomposes X into two lower-rank non-negative matrices H and\nW. The product of H and W serves as an approximation of X (X \u2248 W \u00b7 H). Consider a rank-k matrix X of dimensions m\u00d7n with no negative entries. There exist two rank-k matrices W and H of dimensions mxr and r\u00d7n, respectively, where r is smaller than both m and n). Note that k is the number of features that may be set by the user. For instance, in the context of NSP, k correspond to the number of shifts per day. Finding an accurate approximation of X using NMF involves solving a minimization problem. NMF works by initially setting matrices H and W with random values, and then iteratively tuning these matrices to minimize the distance between their product and X relying on a distance metric. This process uses the multiplicative update equations [63] as illustrated in Equation 10 and Equation 11, respectively. Finally, the algorithm stops once the average error (with respect to the distance metric) converges, or once a maximum number of iterations is reached.\n$$H_{ij}^{n+1} \\leftarrow H_{ij}^n + H_{ij}^n \\frac{((W^n)^TX)_{ij}}{((W^n)^TW^nH)_{ij}}\\quad(10)$$\n$$W_{ij}^{n+1} \\leftarrow W_{ij}^n \\frac{(X(H^{n+1})^T)_{ij}}{(W^nH^{n+1}(H^{n+1})^T)_{ij}}\\quad(11)$$\nTo evaluate the quality of NMF factorization, an error function needs to be used to measure how accurately the approximation represents the original data. Therefore, the Frobenius Norm is used to quantify the error rate at each iteration of the algorithm, as shown in Equation 12. The stopping criterion is determined either when a specified number of iterations is reached or when the quantified FN error convergences.\n$$Minimize(||X_{ij} - (WH)_{ij}||_F)\\quad(12)$$\nExample 5 To illustrate how NMF is applies to learn the NSP constraints and preferences, we use a scheduling example that shows the number of worked days during a week, as shown in Table 14. NMF requires the entries in the input matrix to be non-negative, which is satisfied in the context of NSP, since each"}, {"title": "6 Conclusion and Future Work", "content": "To conclude, given the challenge that comes from modeling the NSP manually or automatically, we have proposed an implicit solving approach relying on ML methods and historical data without any prior knowledge. In addition to evaluating the ML methods individually, we further conducted an experiment against an explicit method named COUNT-OR which learns constraints from historical data based on predefined metrics and then applies them to generate new solutions. The experiment involved quantifying the average error by calculating the distance between the generated solutions and historical data using the Frobenius Norm, which demonstrated that our methods outperformed COUNT-OR. Given that our proposed ML implicit approach comes with uncertainly related to the quality of the solution, we have proposed an explicit solving approach to model and solve the NSP relying on the CSP framework. In this context, we have formulated the NSP as a WCSP and further proposed new variants of B&B and SLS methods as exact and approximate solvers, respectively. Furthermore, we have applied CP techniques to minimize the exponential time cost related our solvers (B&B in particular). To evaluate the performance of our proposed solving methods, we considered additional metaheuristic methods, and conducted an experiment to evaluate the quality of the solutions and the running time for each method. While the experimental results demonstrated that the approximate methods, particularly SLS, provide a good trade-off between solution quality and the corresponding running time, the results also show that B&B still suffer from the exponential running time regardless of applying CP. In addition to solving the NSP implicitly and explicitly, we have also tackled the limitations that comes from manual modeling, by proposing two learning methods that passively and automatically learn NSP constraints from historical data. These two learning methods may serve as an initial step to model the NSP for our explicit solving approach. Given the promising results obtained in scope of this paper, in the near future, we plan to consult with hospitals to evaluate our proposed solving approaches using real-world NSPs. Moreover, we plan to develop nature-inspired techniques to solve NSPs, modeled as a WCSP [64,2,65]. Additionally, we plan to investigate an NSP variant that involve ordinal nurses' preferences by extending our WCSP model to conditional qualitative preferences [66]. Furthermore, since fairness is a significant part of solving the NSP, we plan to address it more precisely by relying on the Multi-objective optimization framework to model additional constraints and objectives that maximize nurse preferences while ensuring equal workload distribution. Finally, we will also investigate tackling NSPs in a dynamic environment, where schedules need to be adjusted in real-time, due to unexpected changes in demand or sudden nurse absences (the COVID-19 pandemic is a good example of such scenarios). Building on previous research work [67,68], the target is to develop a solving approach that can generate new scheduling solutions with minimal perturbation (in terms of re-assignments) within an acceptable timeframe."}]}