{"title": "Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis", "authors": ["Sakhinana Sagar Srinivas", "Chidaksh Ravuru", "Geethan Sannidhi", "Venkataramana Runkana"], "abstract": "Semiconductors, crucial to modern electronics, are generally under-researched in foundational models. It highlights the need for research to enhance the semiconductor device technology portfolio and aid in high-end device fabrication. In this paper, we introduce SLAVA, a small-scale vision-language assistant tailored for semiconductor manufacturing, with a focus on electron microscopy image analysis. It addresses challenges of data scarcity and acquiring high-quality, expert-annotated data. We employ a teacher-student paradigm, using a foundational vision-language model like GPT-4 as a teacher to create instruction-following multimodal data for customizing the student model, SLAVA, for electron microscopic image analysis tasks on consumer hardware with limited budgets. Our approach allows enterprises to further fine-tune the proposed framework with their proprietary data securely within their own infrastructure, protecting intellectual property. Rigorous experiments validate that our framework surpasses traditional methods, handles data shifts, and enables high-throughput screening.", "sections": [{"title": "1. Introduction", "content": "The semiconductor multi-step fabrication process is highly complex and involves specialized firms. Fabless chip designers like Apple, Qualcomm, and NVIDIA create complex integrated circuit designs but outsource manufacturing to foundries like TSMC and Samsung. Foundries use expensive, high-tech fabrication techniques, including photolithography and chemical vapor deposition, to produce intricate integrated circuits (ICs) on silicon wafers. The chips then undergo rigorous quality assurance, followed by packaging and assembly into devices such as microprocessors and memory chips. These semiconductor devices are then integrated into various electronic systems, such as consumer electronics and automotive technologies. Sub-7nm technology marks a significant leap in chip miniaturization, enabling the creation of smaller, more powerful, and efficient devices. However, the industry faces challenges in achieving this miniaturization, such as strictly adhering to the design specifications and tolerances to consistently produce reliable, high-performance sub-7nm chips with minimal variation and high precision. Overcoming these challenges requires thorough testing using sophisticated imaging techniques and analysis to achieve high-quality, large-scale production of semiconductor chips. Advanced microscopy tools like Scanning Electron Microscopy (SEM) and Transmission Electron Microscopy (TEM) generate electron micrographs (nanoscale images), critical for quality control, failure analysis, and subsequent process optimization or design adjustments to help mitigate defects and ensures chips conform to specifications. Current deep learning methods for characterizing materials are insufficient for the semiconductor industry's specialized needs for accurately analyzing electron micrographs. More effective technology is critical to support ongoing technological innovations. Recent advancements in Artificial Intelligence (AI), such as Large Multimodal Models (LMMs) like GPT-4 Turbo with Vision (OpenAI, 2023), Google Gemini(Team et al., 2023) have the potential to impact semiconductor manufacturing by accurately analyzing microscopic images for various tasks, including zero/few-shot classification, image captioning, and visual question answering (VQA) tasks. GPT-4's combination of advanced natural language processing, image processing capabilities, and logical reasoning abilities could enable it to interpret and answer natural language questions about the microscopic images being analyzed. The insightful responses generated for end-user questions would allow human users to better evaluate the rationale behind GPT-4's image analysis and, consequently, trust its responses. Using proprietary multimodal vision-language models raises legitimate data privacy concerns, as intellectual property leaks could potentially undermine the cutting-edge technological portfolio of semiconductor firms and jeopardize future innovation. Additionally, their large size and complexity limit the adaptability to tailor them for specialized tasks. On the other hand, open-source, smaller models like"}, {"title": "2. Experiments And Results", "content": "Our study utilized the extensive SEM dataset (Aversa et al., 2018) containing over 21,000 electron micrographs across 10 categories of nanomaterials to generate a diverse set of instruction-following multimodal data by GPT-4. We trained our framework for task-specific customization using this machine-generated data only, without relying on any human-annotated data. Unlike previous research (Modarres et al., 2017) that used only a subset of the data, we leveraged the publicly available entire dataset, enabling broader and more robust model training. Since the dataset curator did not provide predefined train/validation/test splits, we randomly divided the dataset into 70%, 10%, and 20% portions for training, validation, and testing, respectively. Rigorous benchmarking against baseline algorithms demonstrated significant improvements across tasks for the proposed framework, highlighting its effectiveness. Additionally, we tested our framework's generalizability on other open-source material datasets, demonstrating its effectiveness in similar thematic areas. For a detailed discussion on additional benchmark datasets, please refer to the appendix."}, {"title": "2.1. Datasets"}, {"title": "2.2. Experimental Studies", "content": "We evaluated our framework on various tasks involving microscopic images, including multi-class classification, image captioning, and open-ended VQA, in order to gain a better understanding of the nanomaterials depicted in the electron micrographs. We also explored VQA tasks to evaluate intra-class dissimilarity, inter-class similarity, and spatial heterogeneity, enriching our insights into the nanomaterials depicted in electron micrographs."}, {"title": "2.3. Results", "content": " presents the experimental results on the image captioning task in terms of evaluation metrics like BLEU, METEOR, and ROUGE, comparing the framework-generated captions with ground-truth captions. Our proposed framework SLAVA surpasses contemporary baseline models, InstructBLIP (Dai et al.), LLaVA (Liu et al., 2023), and MiniGPT-4 (Zhu et al., 2023) on the image captioning task.  shows representative electron microscope images with their true captions and framework-generated captions, including evaluation metric scores. The experimental results for zero/few-shot classification, open-ended VQA tasks, and others are discussed in the technical appendix."}, {"title": "3. Conclusion", "content": "Our research introduces a novel approach to electron micrograph analysis and presents a small-scale, instruction-tuned language-and-vision assistant, customized by a multimodal dataset generated with GPT-4 and optimized for consumer hardware with performance on-par with proprietary LMMs. The pre-trained framework can be further fine-tuned with proprietary data, all without compromising sensitive information to third-party LMMs, making it ideal for secure, efficient, and economically viable enterprise applications."}, {"title": "4. Technical Appendix", "content": "Low-Rank Adaptation (LoRA, (Hu et al., 2021)) is a parameter-efficient technique that enables the efficient fine-tuning of large foundational models on consumer hardware (low-cost GPUs). LoRA injects and adapts these additional parameters while keeping the original pre-trained weights frozen, allowing for task-specific customization without full-parameter fine-tuning. LoRA dramatically reduces memory and computational requirements for fine-tuning foundational models with task-specific corpus without increasing inference latency. LoRA serves as a plug-and-play solution for tailoring general-purpose large-scale foundational models to specialized tasks, retaining parametric knowledge acquired from the vast training corpus and mitigating catastrophic forgetting of pre-training knowledge while effectively learning new information. LoRA incorporates a lightweight, trainable pair of low-rank matrices (adapter modules) into each pre-trained model layer. LoRA updates these ancillary parameters while keeping the original pre-trained weights fixed, achieving performance comparable to that of traditional full-parameter fine-tuning but with enhanced resource efficiency. Large-scale pretrained models (Vaswani et al., 2017) benefit from LoRA's ability to incorporate low-rank adapter modules into their linear layers, enhancing performance on specialized tasks. These ubiquitous layers hold a significant portion of the parameters and directly influence learning, making them ideal targets for efficient fine-tuning. In LoRA, updates to the linear layer are achieved by introducing new trainable parameters, denoted as AW, that capture task-specific information without altering the original pre-trained weight matrix represented as Wo. AW is linearly added to Wo to achieve task-specific adaptation while keeping the original weights frozen. The low-rank adaptation of the linear layer, with input X and output Y, can be mathematically described as follows:\n\nY = (Wo + AW)X = W0X + (AB)X\n\nHere, Y \u2208 Rbxdout and X \u2208 Rbxdin. The dimensions of the input and output are denoted by din and dout, respectively, with b representing the batch size. The original weight matrix Wo \u2208 Rdin\u00d7dout holds the pretraining knowledge, preserving the foundational model's general capabilities, while the low-rank addition AW to Wo captures task-specific information during fine-tuning. A \u2208 Rdin\u00e6r is a projection-down weight matrix, and B \u2208 Rrxdout is a projection-up weight matrix. The rank of the decomposition, denoted as r, is a hyperparameter notably smaller than both din and dout, expressed as r < din, dout. The value of r is a critical parameter that optimizes the trade-off between model adaptability, efficiency, and generalization. The scaling factor a is typically set to. During the fine-tuning, the trainable weight matrices A and B are updated, while Wo remains constant. Fine-tuning foundational models involves computing parameter gradients through a task-specific loss function, updating trainable pa- rameters using Adam (Kingma & Ba, 2014) or SGD (Robbins & Monro, 1951) optimizers, and storing additional meta-data such as momentum and adaptive learning rates. Fine-tuning foundational models demands significant memory for model parameters, gradients, and optimizer states. LORA reduces this memory overhead by decreasing the number of trainable parameters through low-rank adaptation. Consequently, LoRA requires fewer computational resources compared to full-parameter fine-tuning, offering a more efficient method for adapting foundational models to specialized tasks. While LoRA reduces memory usage due to fewer trainable parameters, it still requires significant memory to hold large intermediate input activations (X \u2208 Rb\u00d7din; refer to Equation 4.1) during the computation of gradients for low-rank weights, A and B, during back- propagation. This high activation memory demand limits scalability, especially under resource constraints. Methods like selective LoRA (Hu et al., 2021) or activation recom- putation (Chen et al., 2016) could help mitigate this issue, but they may impact efficiency. Thus, the high demand for activation memory remains a challenge in efficiently adapting large-scale foundational models with LoRA, pos- ing a significant limitation. To overcome the aforemen- tioned limitations, LoRA with Frozen-A (LoRA-FA) (Zhang et al., 2023)\u2014a variant of LoRA-reduces activation mem- ory footprint by avoiding the storage of full-rank input activations, enabling efficient fine-tuning of foundational models on limited resources without compromising perfor- mance. LoRA-FA accomplishes this through freezing both the original pre-trained weights, Wo, and the projection- down weight, A, while only updating the projection-up weight, B, which is typically initialized to zero. The frozen projection-down weight matrix A, sampled from a normal distribution, maps the high-dimensional input X into a reduced r-dimensional space (AX \u2208 Rb\u00d7r, where r < din). This low-dimensional mapping further reduces the activation memory requirements for gradient computation of B during back-propagation. In essence, LoRA-FA effectively decreases the number of trainable parameters and also reduces the activation memory usage, making it an efficient technique for fine-tuning large-scale foundational models without increasing inference latency. We propose a novel approach that combines the advantages of the Mixture of Experts (MoEs) framework with Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA-FA. We refer to this innovative method as Mixture of Parameter-Efficient Experts (MoPEs) (Zadouri et al., 2023). This method adapts the MoE approach to be more parameter-efficient by integrating LORA-FA adapters. We employ MoPEs technique to"}, {"title": "4.1. Dynamic Adaptation of Mixture of Quantized Parameter-Efficient Experts (DyA-MoQPEs)", "content": null}, {"title": "4.2. Fine-Tuning, Pretrained Large Language Models(LLMs)", "content": "The Llama-2 (Touvron et al., 2023), a sophisticated auto-regressive, language-optimized transformer architecture tailored specifically for various natural language processing tasks, leverages supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) optimized for chat applications and natural language generation tasks. The core strength of Llama-2 lies in its ability to process and generate text for end-user questions that closely resembles human language, making it highly suitable for complex language processing tasks. The Llama-2's architecture, an auto-regressive decoder, excels at open-ended conditional text generation, particularly suited for interpreting natural language questions. Its advanced architectural features include RMSNorm pre-normalization(Zhang & Sennrich, 2019), SwiGLU activation functions inspired by PaLM(Chowdhery et al., 2022), and rotary positional embeddings(Shaw et al., 2018). To extend its context comprehension, Llama-2 leverages a grouped-query attention mechanism(Ainslie et al., 2023), allowing it to process a significant number of 2048 tokens. The architecture, consisting of 32 layers, 32 attention heads, and a hidden size of 4096, efficiently handles batch sizes of up to 32 for sequences of up to 2048"}, {"title": "4.3. Generation of MultiModal Instruction-Tuning Data", "content": "We leverage GPT-4 Turbo with Vision, a state-of-the-art multimodal large language model (MLLM), to create a customized and comprehensive dataset of image-question answer pairs specifically designed for fine-tuning small multimodal models (SMMs) for visual question answering (VQA) tasks on electron micrographs. GPT-4 first generates challenging and contextually relevant questions that interpret and analyze these micrographs. Simultaneously, it utilizes knowledge distillation to produce corresponding answers using its internal knowledge representation grounded in the visual content of the microscopic images. These generated answers are enriched with domain-specific insights, ensuring accurate responses to open-ended user questions about electron micrographs. Our approach addresses the scarcity of high-quality vision-language datasets for analyzing microscopic images. By training SMMs using the generated vision-language instruction-following dataset, we enable them to acquire domain-specific adaptation abilities through transfer learning. This allows them to perform comparably to proprietary large multimodal models (LMMs) on VQA tasks without incurring excessive computational costs. Our approach offers a methodology for developing a highly efficient, accurate, and domain-specific framework to interpret complex microscopic images. It leverages multimodal intelligence, encompassing vision, language, and reasoning, to address these challenges. The compact multimodal models facilitate interaction between multiple modalities through joint representation learning. This process implicitly aligns semantic concepts across vision and language, enabling the smaller models to contextually understand and reason about these multimodal inputs in order to answer visual questions. This establishes a clear, concise, and relevant foundation for SMMs, allowing them to grasp the visual representation of concept-based instructions and their corresponding answers. GPT-4 crafts questions to guide a comprehensive and thorough investigation of diverse facets, including fundamental characteristics like the size, distribution, and morphology of nanomaterials depicted in microscopic images, such as:\n\nPrompt 1: **Basics** - This image depicts a nanomaterial. What specific type of nanomaterial is it? Additionally, what is the scale or resolution - that is, what real-world length does one unit of measurement in the image correspond to?. Prompt 2: **Morphology and Structure** - Can you describe the overall shape and morphology of the nanomaterials depicted in the image? - Are there any visible layers, phases, or distinct domains within the nanomaterials? Do the nanomaterials exhibit a consistent size and shape throughout, or do they display variability in these aspects?. Prompt 3: **Size and Distribution** - Can you estimate the approximate size or size range of the individual nanostructures depicted in the image? - Additionally, how are the nanomaterials distributed - are they evenly spaced, clustered, or randomly placed? - Finally, is there any visible evidence of aggregation or bundling among the nanostructures?\u201d. Prompt 4: **Surface Characteristics** - When examining the nanomaterials in the image, what are their surface textures like - are they predominantly smooth, rough, or do they possess distinct textures? - Additionally, are there any noticeable imperfections, such as defects, pores, or impurities, visible on the surfaces of these nanomaterials?. Prompt 5: **Composition and Elements** - In the provided image, can we identify any evidence of compositional variations, such as changes in color, brightness, or contrast that might indicate different components? - Additionally, are there any discernible labels or markers within the image that specifically point to the presence of certain elements or compounds?. Prompt 6: **Interactions and Boundaries** - Describe how the individual nanostructures visually interact with one another. For example, do they appear to be touching, fused together, or fully separate? - Examine the boundaries between nanostructures. Can you clearly distinguish boundaries between different structures or phases? - Or do they blend together without defined borders?."}, {"title": "4.4. Sampling Strategies for Instruction Tuning Dataset Generation", "content": "To generate instruction-tuning data using GPT-4 Turbo with vision for few-shot image classification tasks (refer to Figure 5), and to discover key insights into high intra-class dissimilarity, high inter-class similarity, and spatial heterogeneity in electron micrographs (refer to Figures 6 - 8), we follow the strategies outlined below. Given an input image I as a 3D tensor with dimensions H \u00d7 W \u00d7 C (height, width, and number of channels, respectively), we divide it into non-overlapping patches of size P \u00d7 P \u00d7 C. This results in n = ( HW ) patches. Each patch of size PPC is then encoded into a 1D vector, resulting in an encoded patch matrix I' \u2208 Rn\u00d7d, where d represents the embedding dimension. To incorporate spatial information, we add positional embeddings to these patch embeddings. Additionally, we introduce a classification token, <cls>, to represent the global image characteristics. The augmented patch sequence, including the classification token, is processed by the Vision Transformer (ViT (Dosovitskiy et al., 2020)), which refines the patch representations through multiple encoder layers. We update the trainable parameters through a supervised learning task, aiming to minimize cross-entropy loss and maximize multiclass classification accuracy. Consequently, the output embedding hcls corresponding to the < cls > token encapsulates a comprehensive representation of the"}, {"title": "4.5. Loss Functions", "content": "The ITM loss is fundamental to multimodal learning. It utilizes binary cross-entropy loss to enhance the alignment of image and text representations within a shared embedding space. Minimizing the LM loss allows the image-grounded text encoder to effectively determine whether an image and text pair are a match, thereby improving the alignment between image and text representations. For each image-text pair, a ground truth label, (yi), is assigned, where (y\u2081 = 1) indicates a match (the image and text are relevant to each other), and (y = 0) signifies a non-match. The encoder predicts the probability, (pi), that each pair is a match. This probability is computed from the output of the encoder's final linear layer through a sigmoid function. The ITM loss is calculated using the binary cross-entropy loss as follows:\n\nLITM = b \u22121 \u2211 [yi log(pi) + (1 \u2212 yi) log(1 \u2212 pi)]\n\ni=1\nwhere b represents the batch size. This approach ensures a balanced consideration of both matching and non-matching pairs in the loss calculation. It penalizes the encoder for incorrect predictions, thereby guiding it towards more precise representations for image-text matching pairs."}, {"title": "4.5.1. IMAGE-TEXT MATCHING LOSS (ITM)", "content": null}, {"title": "4.5.2. LANGUAGE MODELING LOSS (LM)", "content": "In VQA and image captioning tasks, minimizing LM loss is crucial. It ensures the image-grounded text decoder generates accurate and descriptive textual descriptions of the visual content, tailored to the corresponding end-user questions. Optimizing for LM loss in VQA and image captioning tasks discourages the model from relying solely on the question's linguistic patterns. This prevents language bias and promotes the inclusion of relevant visual information in the generated descriptions. This ensures the decoder learns the correct grammatical structure and vocabulary for answer sentences, resulting in texts that are not only coherent but"}, {"title": "4.6. Additional Information", "content": "We train small-scale vision-language models on the electron micrographs analysis using an instruction-following dataset generated by GPT-4 Turbo with Vision through a teacher-student strategy. The robustness and effectiveness of these small-scale models depend on the composition and design of the training dataset, particularly the comprehensiveness and detail of the image-question-answer pairs. In this work, we propose a novel approach that leverages a balanced combination of concise, summarized answers and more comprehensive, detailed responses in training datasets for the same end-user questions. This method optimizes the performance of small-scale vision-language models across a range of tasks, from image captioning to complex visual question answering (VQA) tasks. Utilizing training data of varied lengths in the small-scale model training offers numerous advantages. It enhances flexibility and adaptability by exposing the small-scale model to diverse sentence structures and visual complexities, thus improving its ability to handle real-world scenarios with varying levels of detail. This approach improves generalizability and prevents overfitting to specific data patterns. Moreover, it challenges the small-scale model's reasoning and attention mechanisms, promoting a deeper understanding of the relationships between visual features and textual descriptions. These benefits lead to improved performance in tasks such as image"}, {"title": "4.7. Experimental Setup", "content": "In this work, we propose a novel framework utilizing a teacher-student paradigm. A large multimodal model like GPT-4 Turbo with vision acts as a teacher to generate instruction-following data to train a smaller, specialized student model, called SLAVA, specifically designed for zero/few-shot image classification, image captioning, and VQA tasks in electron microscopy image analysis. It leverages the vision-language instruction-tuning approach to efficiently transfer knowledge from the larger to smaller model, enabling the student model to perform comparably to larger models in terms of generating accurate and contextually relevant responses to end-user questions based on input images. Additionally, SLAVA is better suited for on-premises enterprises adoption, ensuring data privacy and security. The SLAVA framework is a small-scale, visually conditioned autoregressive language generation model designed for micrograph analysis. It consists of a vision encoder that analyzes microscopic images, while a text encoder interprets end-user questions. The cross-attention mechanism in the image-grounded text encoder enables the small-scale model to effectively align multimodal information, facilitating accurate answer generation. The small-scale model then leverages this integrated multimodal understanding to generate accurate and contextually relevant answers or image captions. The generated text is not only factually accurate but also contextually aligned with the specifics of the electron microscopy images. The small-scale model focuses on both zero/few-shot settings, using multi-modal prompts as inputs consisting of a microscopic image, supplementary image information, and the natural language question for precise analysis and response. The framework adopts a bi-objective approach, optimizing both understanding-based and generation-based goals to improve performance in microscopic image-based analysis on the image captioning and VQA tasks. We trained the SLAVA framework using the tailored image-question-answer pairs dataset generated by GPT-4 based on the SEM dataset (Aversa et al., 2018), a collection of high-resolution images (1024 \u00d7 768 \u00d7 3) showcasing diverse nanomaterials. For preprocessing, we resized the images to 224 \u00d7 224 \u00d7 3 and applied data standardization to normalize the mean and variance across channels to 0.5, constraining values between -1 and 1. To effectively capture local features, we divided the resized images (224 \u00d7 224\u00d73) into the 32-pixel patches, representing each micrograph as a sequence of patches with an embedding dimension of 64. This patch-wise approach enabled the model to learn local features while retaining contextual information through patch sequences. This ultimately enhances the proficiency of the SLAVA framework to understand and analyze complex nanomaterial images. Parameter-efficient fine-tuning"}, {"title": "4.8. Evaluation Metrics", "content": "Image Captioning and VQA tasks combine computer vision and natural language processing to answer image-based questions. Evaluating the accuracy of these answers is challenging, but evaluation metrics assess linguistic similarity, grammatical correctness, and semantic relevance between the ground-truth and generated answers, driving the framework towards more human-like and accurate responses. Here's an overview of some key metrics:\n\n\u2022 BLEU Score (Bilingual Evaluation Understudy): The BLEU Score measures the quality of machine- generated text by comparing it with a reference transla- tion (ground-truth). It analyzes the frequency of over- lapping word sequences (n-grams) between the two texts. The focus of BLEU is on precision; it counts matching n-grams while preventing an overemphasis on repeated phrases. BLEU scores range from 0, in- dicating no overlap, to 1, indicating a perfect match. Higher scores signify a greater shared vocabulary and similarity in phrasing.\n\n\u2022 METEOR (Metric for Evaluation of Translation with Explicit Ordering): The METEOR metric evaluates machine-generated text by comparing it to the ground truth, focusing on word similarity. It considers syn- onyms, paraphrases, and variations of words. ME- TEOR prioritizes exact matches, lemmas, stems, and semantic similarities, capturing both recall and pre- cision on a 0-1 scale. Higher scores indicate greater similarity to the reference translation. While BLEU focuses on how often short phrases (n-grams) appear in the translation, METEOR provides a more compre- hensive evaluation by including fluency, grammar, and semantic matching. This allows it to correlate better with human judgment of translation quality.\n\n\u2022 ROUGE Score (Recall-Oriented Understudy for Gist- ing Evaluation): ROUGE measures the quality of machine-generated text by comparing its lexical over- lap with ground-truth. ROUGE-N, the basic metric, counts matching n-grams between the candidate and reference texts. Variants like ROUGE-L, ROUGE- W, and ROUGE-S focus on longest common subse- quences, word sequences, and skip-bigrams, respec- tively. Scores range from 0, indicating no overlap, to 1 for complete lexical identity. Higher scores suggest better quality, indicating content similar to human ref- erences. While ROUGE primarily evaluates lexical similarity, variants such as ROUGE-L correlate well with human judgments of linguistic quality and coher- ence."}, {"title": "4.9. Empirical Insights into Nanomaterial Classification", "content": "Our research thoroughly evaluated the proposed framework SLAVA for classifying electron micrographs of diverse nano- materials. These complex materials vary in composition, morphology, structure, and other properties, which is evident in their electron micrographs. The framework achieved high accuracy on the imbalanced SEM dataset(Aversa et al., 2018) using metrics like precision, recall, and F1-score, demonstrating its effectiveness in categorizing nanomateri- als with different patterns in a zero-/few-shot setting. Table 9 reports the experimental results. The multi-metric approach provided a detailed analysis, highlighting SLAVA's"}, {"title": "4.10. Additional Results", "content": "The Figures 4, 5, 6, 7, and 8 illustrate the small-scale, language-and-vision assistant (SLAVA). SLAVA belongs to a family of small multimodal models (SMMs) that take electron micrographs and supporting image information as input and produce free-form text output in response to end-user questions. Figures 4 and 5 show variants of the SLAVA framework on the zero/few-shot classification task. Tables 7 and 8 show the experimental results on the zero/few- shot multiclass classification task, comparing the accuracy of our proposed framework to several baseline algorithms. Table 3 shows the framework's performance on the open- ended VQA task. Unlike closed-ended VQA, which requires choosing the correct answer from a set of predefined options, open-ended VQA tasks require the small-scale model to generate its own free-form responses to end-user questions. Table 10 displays electron microscope images with their true captions and small-scale model generated captions. It additionally includes BLEU-2, ROUGE-L, and METEOR scores that evaluate the similarity of the small-scale model's generated captions to the correct captions. Tables 11 to 20 display samples from the instruction-tuning Q&A pairs dataset, which was generated by GPT-4 Turbo with Vision for training the smaller multimodal model, SLAVA. Figure 6, 7, and 8 show variants of the SLAVA framework for the VQA task, addressing high intra-class dissimilarity, high inter-class similarity, and spatial heterogeneity in electron micrographs, respectively. Tables 4, 5, and 6 summarize the performance of various methods on the aforementioned VQA task."}, {"title": "4.11. Related Work", "content": "Large Language Models (LLMs) like Open AI Chat- GPT(OpenAI, 2023), Google Gemini(Team et al., 2023) have significantly advanced natural language processing by demonstrating remarkable abilities in understanding and generating human-like text. Building on this progress, Multi- modal Large Language Models (MLLMs) like MiniGPT- 4(Zhu et al., 2023), LLaVA(Liu et al., 2023), and Instruct- BLIP(Dai et al.) have emerged, integrating visual under- standing with linguistic capabilities. These MLLMs, often based on open-source LLMs like LLaMA(Touvron et al., 2023) and Qwen(Bai et al., 2023), can process and interpret both text and images, leading to a more holistic compre- hension of complex questions that require analysis of both modalities. InstructBLIP(Dai et al.) is an advanced vision- language model that utilizes instruction tuning and compo-"}, {"title": "4.12. Additional datasets and Experimental results", "content": "To assess the robustness and applicability of our framework, we conducted a comprehensive evaluation using a diverse set of open-source benchmark datasets. We carefully selected datasets that were relevant to our research domain and encompassed a broad spectrum of applications, ensuring a generalizable evaluation process. This rigorous approach not only verified the effectiveness of our framework on these established datasets but also demonstrated its adaptability to a wide range of scenarios. This is particularly significant because our framework extends beyond the SEM dataset(Aversa et al., 2018) for which it was initially developed, showcasing its potential for real-world use cases."}, {"title": "4.12.1. NEU-SDD((DESHPANDE ET AL., 2020))", "content": "To rigorously evaluate our proposed framework's performance on zero/few-shot label prediction and VQA tasks for steel material surface defects, we leveraged the comprehensive NEU-SDD dataset\u00b9. The diverse dataset encompasses a variety of surface defect types, making it well-suited for assessing the generalizability of the proposed framework's performance. The dataset includes an extensive collection of 1,800 electron microscopy images depicting surface defects on hot-rolled steel plates, providing a comprehensive resource for evaluating our framework's ability to understand complex visual information and answer insightful questions about the surface defects. The NEU-SDD dataset comprises grayscale images, each having a dimension of 200 \u00d7 200 pixels, and is carefully classified into six distinct defect types, with 300 representative images for each category. These categories depict a diverse range of surface imperfections, including pitted surfaces, scratches, rolled- in scale, crazing, patches, and inclusion defects. Figure 9 provides illustrative images from each defect category. The NEU-SDD dataset is a valuable benchmark for developing and testing algorithms that can answer questions about images of surface defects. Its large size, diversity of defect types depicted, and high-quality images make it a demanding and representative dataset for evaluating VQA methods"}, {"title": "4.12.2. CORROSION MONITORING INSPECTION(CMI)", "content": "The CMI dataset\u00b2 contains 600 detailed electron micrographs of corroded panels, carefully curated by corrosion experts. This collection of images vividly captures deterioration across varying severity levels of corrosion damage. The images are classified according to the ASTM-D1654 standards, with individual scores ranging from 5 to 9 (with higher scores indicating less corrosion severity), with 120 unique micrographs per score. Each high-resolution micrograph, measuring 512 \u00d7 512 pixels, provides a granular view of the corrosion damage. We used the CMI dataset (as shown in Figure 10 with representative images from each scoring category) to conduct experimental studies evaluating the effectiveness of our proposed framework for zero/few- shot prediction and VQA tasks."}, {"title": "4.12.3. \u039aTH-TIPS", "content": "The KTH-TIPS dataset\u00b3, a seminal benchmark in texture analysis, comprises an extensive collection of 810 high- resolution electron micrographs. Each image, having a dimension of 200 \u00d7 200 pixels, has been meticulously cate- gorized into one of ten distinct material classes, showcasing a rich diversity of textures. Included are materials such as sponge, orange peel, styrofoam, cotton, cracker, linen, crust, sandpaper, aluminum foil, and corduroy. The microscopic images capture each texture under varying real-world conditions, such as differences in lighting, orientation, and scale. This versatility makes the KTH-TIPS dataset challenging and comprehensive for evaluating texture recognition and analysis methods. Figure 11 presents illustrative samples from each of the ten material categories."}]}