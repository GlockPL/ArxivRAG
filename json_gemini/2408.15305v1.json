{"title": "Parameter-Efficient Quantized Mixture-of-Experts Meets Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis", "authors": ["Sakhinana Sagar Srinivas", "Chidaksh Ravuru", "Geethan Sannidhi", "Venkataramana Runkana"], "abstract": "Semiconductors, crucial to modern electronics, are generally under-researched in foundational models. It highlights the need for research to enhance the semiconductor device technology portfolio and aid in high-end device fabrication. In this paper, we introduce SLAVA, a small-scale vision-language assistant tailored for semiconductor manufacturing, with a focus on electron microscopy image analysis. It addresses challenges of data scarcity and acquiring high-quality, expert-annotated data. We employ a teacher-student paradigm, using a foundational vision-language model like GPT-4 as a teacher to create instruction-following multimodal data for customizing the student model, SLAVA, for electron microscopic image analysis tasks on consumer hardware with limited budgets. Our approach allows enterprises to further fine-tune the proposed framework with their proprietary data securely within their own infrastructure, protecting intellectual property. Rigorous experiments validate that our framework surpasses traditional methods, handles data shifts, and enables high-throughput screening.", "sections": [{"title": "1. Introduction", "content": "The semiconductor multi-step fabrication process is highly complex and involves specialized firms. Fabless chip designers like Apple, Qualcomm, and NVIDIA create complex integrated circuit designs but outsource manufacturing to foundries like TSMC and Samsung. Foundries use expensive, high-tech fabrication techniques, including photolithography and chemical vapor deposition, to produce intricate integrated circuits (ICs) on silicon wafers. The chips then undergo rigorous quality assurance, followed by packaging and assembly into devices such as microprocessors and memory chips. These semiconductor devices are then integrated into various electronic systems, such as consumer electronics and automotive technologies. Sub-7nm technology marks a significant leap in chip miniaturization, enabling the creation of smaller, more powerful, and efficient devices. However, the industry faces challenges in achieving this miniaturization, such as strictly adhering to the design specifications and tolerances to consistently produce reliable, high-performance sub-7nm chips with minimal variation and high precision. Overcoming these challenges requires thorough testing using sophisticated imaging techniques and analysis to achieve high-quality, large-scale production of semiconductor chips. Advanced microscopy tools like Scanning Electron Microscopy (SEM) and Transmission Electron Microscopy (TEM) generate electron micrographs (nanoscale images), critical for quality control, failure analysis, and subsequent process optimization or design adjustments to help mitigate defects and ensures chips conform to specifications. Current deep learning methods for characterizing materials are insufficient for the semiconductor industry's specialized needs for accurately analyzing electron micrographs. More effective technology is critical to support ongoing technological innovations. Recent advancements in Artificial Intelligence (AI), such as Large Multimodal Models (LMMs) like GPT-4 Turbo with Vision (OpenAI, 2023), Google Gemini(Team et al., 2023) have the potential to impact semiconductor manufacturing by accurately analyzing microscopic images for various tasks, including zero/few-shot classification, image captioning, and visual question answering (VQA) tasks. GPT-4's combination of advanced natural language processing, image processing capabilities, and logical reasoning abilities could enable it to interpret and answer natural language questions about the microscopic images being analyzed. The insightful responses generated for end-user questions would allow human users to better evaluate the rationale behind GPT-4's image analysis and, consequently, trust its responses. Using proprietary multimodal vision-language models raises legitimate data privacy concerns, as intellectual property leaks could potentially undermine the cutting-edge technological portfolio of semiconductor firms and jeopardize future innovation. Additionally, their large size and complexity limit the adaptability to tailor them for specialized tasks. On the other hand, open-source, smaller models like"}, {"title": "Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis", "content": "LLaVA(Liu et al., 2023) and MiniGPT-4(Zhu et al., 2023) offer the benefits of customizable and interpretable microscopic image analysis of nanomaterials, but they may not match the reasoning and generalization capabilities of larger closed-source proprietary models. Creating secure, on-premises small-scale vision-language models for electron micrograph analysis offers several advantages for enterprise adoption, such as improved efficiency, data privacy, cost-effectiveness, interpretability, and customizability. However, this approach also presents significant challenges. Firstly, the scarcity and high cost of high-quality datasets tailored for customizing small-scale multimodal models(SMMs) on electron micrograph analysis for Visual Question Answering (VQA) tasks hinder the collection of necessary data. Secondly, annotating these microscopic images demands expert knowledge and specialized tools, resulting in a resource-intensive and time-consuming process. Finally, the diverse characteristics and representations of microscopic images generated by different imaging techniques pose the most significant obstacle to developing a versatile multimodal model that performs effectively across various electron micrograph-based datasets. In addition, electron micrograph-based zero/few-shot multi-class classification, image-captioning, and, VQA tasks offer powerful insights despite facing challenges. These challenges, highlighted in Figure 1, include: (1) high intra-class dissimilarity, (2) high inter-class similarity, and (3) the presence of multi-scale visual intricacies (spatial heterogeneity). These factors complicate both accurate image understanding and question answering. To address these limitations, we propose a novel framework that utilizes a unique teacher-student paradigm. In this paradigm, a pre-trained foundational large multimodal model (LMM), such as GPT-4, serves as a robust 'teacher' to generate instruction-tuning data (image-question-answer pairs) for customizing a 'student' \u2013 a small-scale, autoregressive, language-and-vision assistant (SLAVA) (hereafter referred to as a small-scale multimodal model or SMM) \u2013 to perform various zero/few-shot multimodal tasks (such as multi-class classification, image captioning, or VQA) for electron microscopy image analysis. Building upon this instruction-following dataset, we introduce vision-language instruction tuning for the smaller multimodal models (SMMs) designed for electron micrograph analysis, thereby eliminating the need for high-quality, human-annotated question-answer pairs for domain-specific customization. Our method efficiently transfers knowledge from a large teacher model to a smaller student model, enhancing its grounded language generation and visual reasoning capabilities to understand the visual content and generate natural language descriptions or responses that accurately reflect the visual information for the end-user question. By distilling the teacher's knowledge, the student achieves performance on par with the original, large-scale proprietary models, demonstrating the efficacy of our approach. Enterprises can further fine-tune our pre-trained language-and-vision assistant, specifically trained for micrograph analysis tasks, using their proprietary data within their own infrastructure. This empowers them with a secure, on-premises solution for electron micrograph analysis, offering enhanced data privacy, sovereignty, and security, thereby democratizing access to advanced micrograph analysis capabilities. Overall, it accelerates adoption and fosters innovation in semiconductor manufacturing. The proposed small-scale vision-language framework is a visually conditioned autoregressive language generation model with an encoder-decoder architecture, designed for zero-shot or few-shot multiclass classification, image captioning, and VQA tasks. The multimodal model takes as input an interleaved multimodal prompt containing a target microscopic image, supplementary image information, and an end-user question. It then process and aligns the complementary multimodal information to achieve integration of knowledge and semantics, ultimately outputting an open-ended text response grounded in the visual content of the microscopic image. In zero-shot settings, it relies on the domain-specific knowledge acquired during pre-training to answer user questions on unseen images. For few-shot settings, it additionally requires a small set of examples involving microscopic images and the corresponding question-answer pairs (input-output mappings) to tailor its responses for interpreting new, unseen images. SLAVA, a small-scale multimodal model that integrates image processing with language modeling, can answer questions about specific microscopic image characteristics. SLAVA includes the following components: (a) The vision encoder is implemented with a vision transformer(Dosovitskiy et al., 2020) to capture the long-range dependencies between microscopic image regions with an expanded receptive field. Consequently, the vision encoder captures salient and global information of the microscopic"}, {"title": "Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis", "content": "image, effectively highlighting more relevant visual regions along with their contextual relationships to understand and ground the questions within visual concepts. We incorporate a <cls> token to attend to and aggregate information from all image regions. The higher-level visual semantic representation of the global (<cls>) token represent the summary of the input image. (b) The text encoder plays a crucial role in analyzing and interpreting user input to understand the nature of the question. It leverages supporting text descriptions associated with the image to extract key details and provide accurate and relevant answers. We insert <image> token at the image location in the interleaved multimodal input. We append a <Encode> token to the tokenized text to facilitate multimodal integration, with its output embedding representing the fused image-text representation. To better capture the nuances of language and context, the text encoder leverages a pre-trained language model, Llama-2-7b(Touvron et al., 2023), to compute a high-level representation that captures the semantic meaning and relationships within the end-user question. We fine-tune Llama-2-7b using Dynamic Adaptation of Mixture of Quantized Parameter-Efficient Experts (DyA-MOQPEs) technique (details in the technical appendix) using the instruction-following dataset generated by GPT-4. This Parameter-Efficient Fine-Tuning (PEFT) technique integrates quantization-aware low-rank adaptation (QLORA) with Mixture of Experts (MoEs) and employs dynamic rank sampling. This approach enhances our ability to interpret natural language questions. Both unimodal encoders play a crucial role in interpreting an end-user question (textual input) regarding the target microscopic image and then analyzing the target microscopic image (visual input) to aid in generating answers that are not only factually accurate but also consistent with the context of the visual information in the microscopic image. (c) The image-grounded text encoder facilitates cross-modal learning to bridge the gap between visual content and linguistic end-user questions by pairing textual descriptions with visual patterns through a cross-attention mechanism. This allows the encoder to focus on relevant image regions and integrate visual information directly into text understanding, resulting in a contextually relevant text representation grounded in the microscopic image's visual content. We minimize the binary cross-entropy loss to align positive image-text pairs. (d) The image-grounded text decoder leverages multimodal representations to generate accurate and contextually relevant answers, bridging the gap between visual perception and language understanding. To demarcate the generated text sequence, we insert a special <Decode> token at the beginning and an end-of-sequence (<EOS>) token at the end, acting as brackets for the output. The decoder, trained to ground its text generation in visual information, generates contextually relevant descriptions closely aligned with the"}, {"title": "Vision-Language Instruction Tuning for Semiconductor Electron Micrograph Analysis", "content": "image content. This bridging of visual perception and language generation is achieved through a language modeling loss, ensuring the output accurately captures the image's essence. To achieve robust comprehension and accurate answer generation, our proposed multimodal learning framework employs a two-pronged learning approach. First, we minimize positive image-text pair matching losses to ensure a deep understanding of both visual and textual content. Second, minimization of language modeling loss fosters the generation of accurate and contextually grounded answers. We jointly optimize these objectives through the vision-language instruction tuning of our proposed model, SLAVA, using a multimodal dataset of image-question-answer pairs generated by GPT-4. This enables SLAVA to achieve remarkable expertise in the challenging domain of microscopic image-based question-answering tasks. As illustrated in Figure 2, the proposed framework, SLAVA, is applied for the zero-shot image captioning task. For other tasks, such as zero/few-shot multi-class classification and open-ended VQA, technical details are discussed in the appendix. In summary, the framework outputs free-form text answers to open-ended image-related questions."}, {"content": "2. Experiments And Results"}, {"title": "2.1. Datasets", "content": "Our study utilized the extensive SEM dataset (Aversa et al., 2018) containing over 21,000 electron micrographs across 10 categories of nanomaterials to generate a diverse set of instruction-following multimodal data by GPT-4. We trained our framework for task-specific customization using this machine-generated data only, without relying on any human-annotated data. Unlike previous research (Modarres et al., 2017) that used only a subset of the data, we leveraged the publicly available entire dataset, enabling broader and more robust model training. Since the dataset curator did not provide predefined train/validation/test splits, we randomly divided the dataset into 70%, 10%, and 20% portions for training, validation, and testing, respectively. Rigorous benchmarking against baseline algorithms demonstrated significant improvements across tasks for the proposed framework, highlighting its effectiveness. Additionally, we tested our framework's generalizability on other open-source material datasets, demonstrating its effectiveness in similar thematic areas. For a detailed discussion on additional benchmark datasets, please refer to the appendix."}, {"title": "2.2. Experimental Studies", "content": "We evaluated our framework on various tasks involving microscopic images, including multi-class classification, image captioning, and open-ended VQA, in order to gain a better understanding of the nanomaterials depicted in the electron micrographs. We also explored VQA tasks to evaluate intra-class dissimilarity, inter-class similarity, and spatial heterogeneity, enriching our insights into the nanomaterials depicted in electron micrographs."}, {"title": "2.3. Results", "content": "Table 1 presents the experimental results on the image captioning task in terms of evaluation metrics like BLEU, METEOR, and ROUGE, comparing the framework-generated captions with ground-truth captions. Our proposed framework SLAVA surpasses contemporary baseline models, InstructBLIP (Dai et al.), LLaVA (Liu et al., 2023), and MiniGPT-4 (Zhu et al., 2023) on the image captioning task. Table 2 shows representative electron microscope images with their true captions and framework-generated captions, including evaluation metric scores. The experimental results for zero/few-shot classification, open-ended VQA tasks, and others are discussed in the technical appendix."}, {"title": "3. Conclusion", "content": "Our research introduces a novel approach to electron micrograph analysis and presents a small-scale, instruction-tuned language-and-vision assistant, customized by a multimodal dataset generated with GPT-4 and optimized for consumer hardware with performance on-par with proprietary LMMs. The pre-trained framework can be further fine-tuned with proprietary data, all without compromising sensitive information to third-party LMMs, making it ideal for secure, efficient, and economically viable enterprise applications."}, {"title": "4. Technical Appendix", "content": ""}, {"title": "4.1. Dynamic Adaptation of Mixture of Quantized Parameter-Efficient Experts (DyA-MoQPEs)", "content": "Low-Rank Adaptation (LoRA, (Hu et al., 2021)) is a parameter-efficient technique that enables the efficient fine-tuning of large foundational models on consumer hardware (low-cost GPUs). LoRA injects and adapts these additional parameters while keeping the original pre-trained weights frozen, allowing for task-specific customization without full-parameter fine-tuning. LoRA dramatically reduces memory and computational requirements for fine-tuning foundational models with task-specific corpus without increasing inference latency. LoRA serves as a plug-and-play solution for tailoring general-purpose large-scale foundational models to specialized tasks, retaining parametric knowledge acquired from the vast training corpus and mitigating catastrophic forgetting of pre-training knowledge while effectively learning new information. LoRA incorporates a lightweight, trainable pair of low-rank matrices (adapter modules) into each pre-trained model layer. LoRA updates these ancillary parameters while keeping the original pre-trained weights fixed, achieving performance comparable to that of traditional full-parameter fine-tuning but with enhanced resource efficiency. Large-scale pretrained models (Vaswani et al., 2017) benefit from LoRA's ability to incorporate low-rank adapter modules into their linear layers, enhancing performance on specialized tasks. These ubiquitous layers hold a significant portion of the parameters and directly influence learning, making them ideal targets for efficient fine-tuning. In LoRA, updates to the linear layer are achieved by introducing new trainable parameters, denoted as AW, that capture task-specific information without altering the original pre-trained weight matrix represented as Wo. AW is linearly added to Wo to achieve task-specific adaptation while keeping the original weights frozen. The low-rank adaptation of the linear layer, with input X and output Y, can be mathematically described as follows:\nY = (Wo + AW)X = WoX + (AB)X                                             (1)\nHere, Y \u2208 Rbxdout and X \u2208 Rbxdin. The dimensions of the input and output are denoted by din and dout, respectively, with b representing the batch size. The original weight matrix Wo \u2208 Rdin\u00d7dout holds the pretraining knowledge, preserving the foundational model's general capabilities, while the low-rank addition AW to Wo captures task-specific information during fine-tuning. A \u2208 Rdin\u00e6r is a projection-down weight matrix, and B \u2208 Rrxdout is a projection-up weight matrix. The rank of the decomposition, denoted as r, is a hyperparameter notably smaller than both din and dout, expressed as r < din, dout. The value of r is a critical parameter that optimizes the trade-off between model adaptability, efficiency, and generaliza-"}, {"title": "Technical Appendix", "content": "tion. The scaling factor a is typically set to . During the fine-tuning, the trainable weight matrices A and B are updated, while Wo remains constant. Fine-tuning foundational models involves computing parameter gradients through a task-specific loss function, updating trainable parameters using Adam (Kingma & Ba, 2014) or SGD (Robbins & Monro, 1951) optimizers, and storing additional meta-data such as momentum and adaptive learning rates. Fine-tuning foundational models demands significant memory for model parameters, gradients, and optimizer states. LORA reduces this memory overhead by decreasing the number of trainable parameters through low-rank adaptation. Consequently, LoRA requires fewer computational resources compared to full-parameter fine-tuning, offering a more efficient method for adapting foundational models to specialized tasks. While LoRA reduces memory usage due to fewer trainable parameters, it still requires significant memory to hold large intermediate input activations (X \u2208 Rb\u00d7din; refer to Equation 4.1) during the computation of gradients for low-rank weights, A and B, during back-propagation. This high activation memory demand limits scalability, especially under resource constraints. Methods like selective LoRA (Hu et al., 2021) or activation recom-putation (Chen et al., 2016) could help mitigate this issue, but they may impact efficiency. Thus, the high demand for activation memory remains a challenge in efficiently adapting large-scale foundational models with LoRA, posing a significant limitation. To overcome the aforementioned limitations, LoRA with Frozen-A (LoRA-FA) (Zhang et al., 2023)\u2014a variant of LoRA\u2014reduces activation memory footprint by avoiding the storage of full-rank input activations, enabling efficient fine-tuning of foundational models on limited resources without compromising performance. LoRA-FA accomplishes this through freezing both the original pre-trained weights, Wo, and the projection-down weight, A, while only updating the projection-up weight, B, which is typically initialized to zero. The frozen projection-down weight matrix A, sampled from a normal distribution, maps the high-dimensional input X into a reduced r-dimensional space (AX \u2208 Rb\u00d7r, where r < din). This low-dimensional mapping further reduces the activation memory requirements for gradient computation of B during back-propagation. In essence, LoRA-FA effectively decreases the number of trainable parameters and also reduces the activation memory usage, making it an efficient technique for fine-tuning large-scale foundational models without increasing inference latency. We propose a novel approach that combines the advantages of the Mixture of Experts (MoEs) framework with Parameter-Efficient Fine-Tuning (PEFT) techniques, such as LoRA-FA. We refer to this innovative method as Mixture of Parameter-Efficient Experts (MoPEs) (Zadouri et al., 2023). This method adapts the MoE approach to be more parameter-efficient by integrating LORA-FA adapters. We employ MoPEs technique to"}, {"title": "Technical Appendix", "content": "instruction-tune pretrained foundational models, thereby improving their performance on niche, domain-specific tasks while minimizing resource usage. In the MoPE architecture, a set of specialized experts, known as LoRA-FA adapters, are trained to address different aspects of the fine-tuning data. This targeted approach allows each expert to focus on specific data aspects, significantly enhancing the performance of pretrained decoder-only foundational models on complex downstream tasks. These multiple experts are activated based on a gating mechanism denoted as router R, designed for conditional computation. We represent the set of K experts as Bo = E(X; \u03b80), . . ., \u0392\u03ba = E(X; \u03b8\u03ba), where each Bk corresponds to the weight matrix of the k-th expert, which is learned during the fine-tuning based on the downstream task. Here, E represents a parameterized function, and \u03b8k denotes the trainable parameters specific to expert k. The router R typically takes the form of another feed-forward network, producing a k-dimensional vector that indicates the routing probabilities for each expert.\nK\nY = (W+AW)X = W0X+A(BX), B = \u2211R(X)kBk\nk=1\nHere, B represents a composite weight matrix obtained by combining the contributions of multiple expert weight matrices, with each matrix weighted by its respective routing probability. We implement a top-k routing strategy for soft merging, where only the k experts with the highest routing probabilities contribute to the composite matrix. This effectively reduces computational complexity. While MoPEs slightly increases trainable parameters compared to LoRA-FA due to conditional computation, the reduced activation memory usage makes it an economical choice for fine-tuning on consumer-grade hardware with improved performance than LoRA-FA. While MoPEs effectively reduce memory usage without compromising their fine-tuning performance, they are not without limitations. Carefully tuning rank r is crucial, as it balances model complexity and learning complex data patterns. A static rank, however, could limit adaptability to data distribution shifts. To address these limitations, which stem from a fixed rank size and require exhaustive searches for the optimal rank, we introduce 'Dynamic low-rank adaptation with MoPEs' (denoted as DyA-MoPEs). Specifically, DyA-MoPEs can adapt across various ranks within the range from rmin to rmax, where rmin and rmax are introduced as hyperparameters during training. This approach eliminates the need for multiple training iterations to determine the optimal singular rank. Dynamic low-rank adaptation offers significant advantages by allowing dynamic rank adjustments during training for effective performance across a broad range of ranks. Moreover, DyA-MoPEs can adapt their rank based on the task, making them suitable for continuous learning scenarios or contexts with frequent data distribution shifts. During fine tuning, we dynamically sample a rank b from a pre-defined categorical distribution, b ~ PB(Range[rmin, max]) and the pair of low-rank matrices are truncated as follows:\nB = B[1 : b, :]\nA+6 = A[:, 1 : 6]\nY = WoX + A+b(B+X)\nThis truncation keeps the first b rows of B and the first b columns of A, resulting in matrices with a lower rank. Consequently, the output Y is computed using these lower-rank matrices, allowing for dynamic adjustment of model complexity during training. We compute gradients for these truncated matrices and apply updates accordingly. To manage the increased computational complexity, we utilize custom gradient accumulation. This technique enables more stable and efficient learning by accumulating gradients over multiple iterations or steps. Additionally, we implement rank normalization to equalize the influence of different ranks on the model's learning process. By scaling gradients or updates according to the rank size, this method helps stabilize training and ensures fair contributions from all ranks. To reduce their memory footprint, we quantize the pre-trained weights or base weights (W0) of the Llama 2-7B model from a 16-bit format into a lower precision format (e.g., 8-bit quantization (Dettmers et al., 2023; Xu et al., 2023)). During inference, the product of low-rank adapter parameters, Ab and B, is combined with these quantized weights to approximate the original full-precision model."}, {"title": "Technical Appendix", "content": "4.2. Fine-Tuning, Pretrained Large Language Models(LLMs)\nThe Llama-2 (Touvron et al., 2023), a sophisticated auto-regressive, language-optimized transformer architecture tailored specifically for various natural language processing tasks, leverages supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) optimized for chat applications and natural language generation tasks. The core strength of Llama-2 lies in its ability to process and generate text for end-user questions that closely resembles human language, making it highly suitable for complex language processing tasks. The Llama-2's architecture, an autoregressive decoder, excels at open-ended conditional text generation, particularly suited for interpreting natural language questions. Its advanced architectural features include RMSNorm pre-normalization(Zhang & Sennrich, 2019), SwiGLU activation functions inspired by PaLM(Chowdhery et al., 2022), and rotary positional embeddings(Shaw et al., 2018). To extend its context comprehension, Llama-2 leverages a grouped-query attention mechanism(Ainslie et al., 2023), allowing it to process a significant number of 2048 tokens. The architecture, consisting of 32 layers, 32 attention heads, and a hidden size of 4096, efficiently handles batch sizes of up to 32 for sequences of up to 2048"}, {"title": "Technical Appendix", "content": "tokens. We fine-tune Llama-2 using Parameter-Efficient Fine-Tuning (PEFT) methods, specifically employing the Dynamic Adaptation of Mixture of Quantized Parameter-Efficient Experts (DyA-MoQPEs) technique. This approach enhances Llama-2's performance on visual question answering (VQA) tasks related to electron micrograph analysis through Vision-Language Instruction Tuning. As a result, Llama-2 efficiently adapts its extensive language understanding capabilities to the specific context and nuances of niche domain topics, such as interpreting complex natural language queries related to electron micrographs. The fine-tuned Llama-2 model demonstrates a deeper understanding of end-user questions, effectively handling ambiguity and complex language for accurate image-text correspondence in VQA tasks, connecting textual concepts and entities with their visual counterparts in microscopic images. Our work integrates QDyA-MoPEs adapter modules into each linear layer of the grouped-query attention mechanism layers in the Llama 2-7B model architecture. These layers analyze different aspects of language understanding, with earlier layers focusing on fundamental syntactic elements and subsequent layers exploring more complex semantic connections. This integration allows for task-specific customization to effectively interpret natural language questions related to microscopic image analysis."}, {"title": "4.3. Generation of MultiModal Instruction-Tuning Data", "content": "We leverage GPT-4 Turbo with Vision, a state-of-the-art multimodal large language model (MLLM), to create a customized and comprehensive dataset of image-question answer pairs specifically designed for fine-tuning small multimodal models (SMMs) for visual question answering (VQA) tasks on electron micrographs. GPT-4 first generates challenging and contextually relevant questions that interpret and analyze these micrographs. Simultaneously, it utilizes knowledge distillation to produce corresponding answers using its internal knowledge representation grounded in the visual content of the microscopic images. These generated answers are enriched with domain-specific insights, ensuring accurate responses to open-ended user questions about electron micrographs. Our approach addresses the scarcity of high-quality vision-language datasets for analyzing microscopic images. By training SMMs using the generated vision-language instruction-following dataset, we enable them to acquire domain-specific adaptation abilities through transfer learning. This allows them to perform comparably to proprietary large multimodal models (LMMs) on VQA tasks without incurring excessive computational costs. Our approach offers a methodology for developing a highly efficient, accurate, and domain-specific framework to interpret complex microscopic images. It leverages multimodal intelligence, encompassing vision, language, and reasoning, to address these challenges. The compact multimodal models facilitate interaction between multiple modalities through"}, {"title": "Technical Appendix", "content": "joint representation learning. This process implicitly aligns semantic concepts across vision and language, enabling the smaller models to contextually understand and reason about these multimodal inputs in order to answer visual questions.\nThis establishes a clear, concise, and relevant foundation for SMMs, allowing them to grasp the visual representation of concept-based instructions and their corresponding answers. GPT-4 crafts questions to guide a comprehensive and thorough investigation of diverse facets, including fundamental characteristics like the size, distribution, and morphology of nanomaterials depicted in microscopic images, such as:\nPrompt 1: **Basics** - This image depicts a nanomaterial. What specific type of nanomaterial is it? Additionally, what is the scale or resolution - that is, what real-world length does one unit of measurement in the image correspond to?. Prompt 2: **Morphology and Structure** - Can you describe the overall shape and morphology of the nanomaterials depicted in the image? - Are there any visible layers, phases, or distinct domains within the nanomaterials? Do the nanomaterials exhibit a consistent size and shape throughout, or do they display variability in these aspects?. Prompt 3: **Size and Distribution** - Can you estimate the approximate size or size range of the individual nanostructures depicted in the image? - Additionally, how are the nanomaterials distributed - are they evenly spaced, clustered, or randomly placed? - Finally, is there any visible evidence of aggregation or bundling among the nanostructures?\u201d. Prompt 4: **Surface Characteristics** - When examining the nanomaterials in the image, what are their surface textures like - are they predominantly smooth, rough, or do they possess distinct textures? - Additionally, are there any noticeable imperfections, such as defects, pores, or impurities, visible on the surfaces of these nanomaterials?. Prompt 5: **Composition and Elements** - In the provided image, can we identify any evidence of compositional variations, such as changes in color, brightness, or contrast that might indicate different components? - Additionally, are there any discernible labels or markers within the image that specifically point to the presence of certain elements or compounds?. Prompt 6: **Interactions and Boundaries** - Describe how the individual nanostructures visually interact with one another. For example, do they appear to be touching, fused together, or fully separate? - Examine the boundaries between nanostructures. Can you clearly distinguish boundaries between different structures or phases? - Or do they blend together without defined borders?."}, {"title": "Technical Appendix", "content": "Prompt 7: **External Environment** - In the provided image, can you identify any signs of interaction between the nanomaterials and their surrounding environment or matrix, which might include solvents, polymers, or other materials? - Additionally, are there any discernible structures or objects present in the image that are not nanomaterials? If so, please describe these elements?. Prompt 8: **Image Technique and Modifications** - Can you identify the specific imaging technique, such as Scanning Electron Microscopy (SEM) or Transmission Electron Microscopy (TEM), used to capture this image of nanomaterials? - Additionally, were there any post-processing techniques or modifications applied, including but not limited to false coloring or 3D rendering?. Prompt 9: **Functional Features** - Can you identify any specific functional elements in the image, like active sites or regions with distinct properties? - Additionally, does the image depict any dynamic processes taking place within the subject, or is it primarily a static representation?. Prompt 10: **Context and Application** What is the primary intended use or application of the nanomaterials as depicted in the image, and is the representation of these nanomaterials based on actual experimental samples, or are they theoretical or simulation-based representations?"}, {"title": "4.4. Sampling Strategies for Instruction Tuning Dataset Generation", "content": "To generate instruction-tuning data using GPT-4 Turbo with vision for few-shot image classification tasks (refer to Figure 5), and to discover key insights into high intra-class dissimilarity, high inter-class similarity, and spatial heterogeneity in electron micrographs (refer to Figures 6 - 8), we follow the strategies outlined below. Given an input image I as a 3D tensor with dimensions H \u00d7 W \u00d7 C (height, width, and number of channels, respectively), we divide it into non-overlapping patches of size P \u00d7 P \u00d7 C. This results in n = (\u8056) patches. Each patch of size P\u00baC is then encoded into a 1D vector, resulting in an encoded patch matrix I' \u2208 Rn\u00d7d, where d represents the embedding dimension. To incorporate spatial information, we add positional embeddings to these patch embeddings. Additionally, we introduce a classification token, <cls>, to represent the global image characteristics. The augmented patch sequence, including the classification token, is processed by the Vision Transformer (ViT (Dosovitskiy et al., 2020)), which refines the patch representations through multiple encoder layers. We update the trainable parameters through a supervised learning task, aiming to minimize cross-entropy loss and maximize multiclass classification accuracy. Consequently, the output embedding hels corresponding to the < cls > token encapsulates a comprehensive representation of the"}, {"title": "Technical Appendix", "content": "microscopic image. We propose a similarity-driven sampling approach for the few-shot image classification task, based on the hypothesis that training on demonstrations resembling the target image's data distribution promotes model adaptability and accuracy. This method utilizes cosine similarity of the hels token embeddings to select the top-K most similar images to the target image from the training set. We follow the same strategy for sampling highly similar images across different nanomaterial categories to generate question-answer pairs, aiming to gain insights into high inter-class similarity. Conversely, we employ the inverse strategy for generating question-answer pairs for each target image, extracting insights on high intra-class dissimilarity by sampling highly dissimilar images within the same nanomaterial category."}, {"title": "4.5. Loss Functions", "content": ""}, {"title": "4.5.1. IMAGE-TEXT MATCHING LOSS (ITM)", "content": "The ITM loss is fundamental to multimodal learning. It utilizes binary cross-entropy loss to enhance the alignment of image and text representations within a shared embedding space. Minimizing the LM loss allows the image-grounded text encoder to effectively determine whether an image and text pair are a match, thereby improving the alignment between image and text representations. For each image-text pair, a ground truth label, (yi), is assigned, where (y\u2081 = 1) indicates a match (the image and text are relevant to each other), and (y = 0) signifies a non-match. The encoder predicts the probability, (pi), that each pair is a match. This probability is computed from the output of the encoder's final linear layer through a sigmoid function. The ITM loss is calculated using the binary cross-entropy loss as follows:\nLITM = 1/b \u03a3[yi log(pi) + (1 \u2212 yi) log(1 \u2212 pi)]\ni=1\nwhere b represents the batch size. This approach ensures a balanced consideration of both matching and non-matching pairs in the loss calculation. It penalizes the encoder for incorrect predictions, thereby guiding it towards more precise representations for image-text matching pairs."}, {"title": "Technical Appendix", "content": "4.5.2. LANGUAGE MODELING LOSS (LM)\nIn VQA and image captioning tasks, minimizing LM loss is crucial. It ensures the image-grounded text decoder generates accurate and descriptive textual descriptions of the visual content, tailored to the corresponding end-user questions. Optimizing for LM loss in VQA and image captioning tasks discourages the model from relying solely on the question's linguistic patterns. This prevents language bias and promotes the inclusion of relevant visual information in the generated descriptions. This ensures the decoder learns the correct grammatical structure and vocabulary for answer sentences, resulting in texts that are not only coherent but"}, {"title": "Technical Appendix", "content": "also contextually aligned with both the image and the question. The framework is trained to enhance word prediction accuracy in a text sequence. It considers both preceding words and the visual context to refine its ability to interpret and respond to image-based queries. This involves minimizing the negative log-likelihood of the actual words, based on the predicted probabilities from the decoder, ultimately leading to grammatically correct, semantically coherent, and"}]}