{"title": "UNIGUARD: Towards Universal Safety Guardrails for Jailbreak Attacks on Multimodal Large Language Models", "authors": ["Sejoon Oh", "Yiqiao Jin", "Megha Sharma", "Donghyun Kim", "Eric Ma", "Gaurav Verma", "Srijan Kumar"], "abstract": "Multimodal large language models (MLLMs) have revolutionized vision-language understanding but are vulnerable to multimodal jailbreak attacks, where adversaries meticulously craft inputs to elicit harmful or inappropriate responses. We propose UNIGUARD, a novel multimodal safety guardrail that jointly considers the unimodal and cross-modal harmful signals. UNIGUARD is trained such that the likelihood of generating harmful responses in a toxic corpus is minimized, and can be seamlessly applied to any input prompt during inference with minimal computational costs. Extensive experiments demonstrate the generalizability of UNIGUARD across multiple modalities and attack strategies. It demonstrates impressive generalizability across multiple state-of-the-art MLLMs, including LLaVA, Gemini Pro, GPT-4V, MiniGPT-4, and InstructBLIP, thereby broadening the scope of our solution.", "sections": [{"title": "Introduction", "content": "The rapid development of multimodal large language models (MLLMs), exemplified by models like GPT-4V (OpenAI, 2023), Gemini (Reid et al., 2024), and LLaVA (Liu et al., 2023b,a), has revolutionized vision-language understanding but introduced new risks. One of the most pressing concerns is the vulnerabilities of MLLMs to adversarial attacks or jailbreaks (Qi et al., 2023; Shayegani et al., 2023; Niu et al., 2024; Deng et al., 2024), which leverages inherent weaknesses of models to bypass their safety mechanisms, raising concerns about their secure deployment.\nChallenges. Ensuring safe and trustworthy interactions requires the development of robust safety guardrails against adversarial exploitation, which presents three core challenges. 1) Multimodal Effectiveness. Guardrails must protect against adversarial prompting in multiple modalities and their cross-modal interactions, ensuring that defenses are not limited to unimodal threats. 2) Generalizability Across Models. Safety mechanisms should be adaptable to both open-source and proprietary models. 3) Robustness across diverse attacks. Effective guardrails must withstand a wide range of attack strategies, including constrained attacks that subtly modify inputs while maintaining visual similarity, and unconstrained attacks that introduce noticeable changes (Qi et al., 2023). They should also address adversarial text prompts (Gehman et al., 2020) that elicit harmful or inappropriate responses from LLMs. Although prior work has explored defenses for both unimodal (Zou et al., 2023; Chao et al., 2023) and multimodal LLMs (Shayegani et al., 2023; Niu et al., 2024; Gou et al., 2024; Pi et al., 2024), a holistic approach covering multiple modalities, models, and attack types remains an open challenge.\nThis Work. We introduce UNIGUARD, a novel defense mechanism that provides robust, Universally applicable multimodal Guardrails against adversarial attacks in both visual and textual inputs. As shown in Figure 1, the core idea is to create specialized safety guardrail for each modality while accounting for their cross-modal interactions. This guardrail purifies potential adversarial responses after applying to input prompts. Inspired by few-shot prompt learning (Qi et al., 2023; Lester et al., 2021), we optimize the guardrails by searching for additive noise (for image inputs) and suffix modifications (for text prompts) to minimize the likelihood of generating harmful responses in a small toxic content corpus (Liu et al., 2023a). We conduct comprehensive experiments on both adversarial and benign input prompts. Our results demonstrate that UNIGUARD significantly improves robustness against various adversarial attacks while maintaining high accuracy for benign inputs. For example, UNIGUARD effectively reduces the attack success rate on LLAVA by nearly 55%, with a small performance-safety trade-off in visual question-answering. The safety guardrails developed for one model such as LLAVA (Liu et al., 2023a) is transferable to other MLLMs, including both open-source models like MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023), as well as proprietary models like Gemini Pro (Team et al., 2023) and GPT-4V (OpenAI, 2023), highlighting the generalizability of our approach across different models and architectures.\nContributions. Our major contributions are:\n1.  Effective Defense Strategy. We propose UNIGUARD, a pioneering multimodal defense mechanism that effectively enhances MLLM robustness against jailbreak attacks;\n2.  Novel Methodology. We introduce a novel optimization technique that generates multimodal safety guardrails using a small corpus of harmful content and an open-source MLLM;\n3.  Comprehensive Evaluation. We conduct comprehensive evaluation showing that UNIGUARD effectively enhances the robustness of multiple models, including both open-source MLLMs (LLAVA, MiniGPT-4, and InstructBLIP.) and proprietary models (Gemini Pro and GPT-4V)."}, {"title": "Proposed Method: UNIGUARD", "content": "2.1 Overview\nWe consider a conversational setup where an MLLM responds to user prompts containing images, text, or both. Adversarial attackers may attempt to manipulate the MLLM to produce harmful content or include specific phrases in the output (Bailey et al., 2023). We focus on defending against jailbreak attacks, where carefully crafted prompts cause the MLLM to generate offensive or inappropriate output. These attacks can use unrelated image-text combinations, such as white noise paired with a toxic text prompt. While simple safety guardrails such as blurring image or random perturbation of text can be used as the first line of defense, special safety guardrails optimized to make the MLLM generate less harmful content might be more effective defense. Thus, our objective is to optimize safety guardrails for each modality (e.g., image and text), specifically tailored to mitigate jailbreak attacks on aligned MLLMs. Figure 2 summarizes the safety guardrail optimization process of UNIGUARD.\n2.2 Image Safety Guardrail\nFew-shot prompt learning (Qi et al., 2023; Lester et al., 2021) has demonstrated that using a few in-context task-related examples can achieve performance similar to full fine-tuning of LLMs. Inspired by this method, we aim to find an additive"}, {"title": "Image Safety Guardrail", "content": "noise (i.e., the safety guardrail) via optimization that, when added to the adversarial image, minimizes the likelihood of generating harmful sentences (e.g., racism or terrorism) of a predefined corpus C. These harmful sentences serve as few-shot examples, helping the MLLM recognize jailbreak attacks and making the optimized noise transferable across different attack scenarios. The harmful corpus C can be small and sourced from existing adversarial prompt datasets (Qi et al., 2023; Zou et al., 2023) or webscraping. Formally, the image safety guardrail $v_{sg}$ is defined as:\n$v_{sg} = argmin_{v_{noi}} \\sum_{i=1}^{|C|}logp(C_i|{x_{sys}, v_{adv} + v_{noi}}),$\nwhere $c_i$ indicates the i-th harmful sentence from C, $x_{sys}$ is the MLLM's system prompt, $v_{adv}$ indicates an adversarial image, $v_{noi}$ is an additive noise applied to the image that satisfies $||v_{noi}||_\\infty \\leq \\epsilon$. $p(\u00b7|\u00b7)$ indicates the generation probability of MLLM given the input text and image. The hyperparameter $\\epsilon \\in [0, 1]$ is a distance constraint that controls the noise magnitude.\nWe optimize the safety guardrail with respect to unconstrained attack images $v_{adv}$ (Qi et al., 2023), which can be seen as the worst-case scenario an MLLM can encounter in the real world as it is the most effective attack, allowing any pixel values in $v_{adv}$ after normalization. This optimization ensures robustness against both unconstrained and suboptimal (e.g., constrained) attacks.\nSince the additive noise $v_{noi}$ in Eq. (1) is continuous and the loss function is differentiable with respect to $v_{noi}$, we employ the Projected Gradient Descent (PGD) algorithm (Madry et al., 2018;"}, {"title": "Text Safety Guardrail", "content": "While the optimization in Eq. 1 addresses adversarial images, vulnerabilities in the text prompt can still compromise the MLLM. To ensure full robustness, we jointly optimize a text safety guardrail $x_{sg}$. Unlike image-based optimization, finding $x_{sg}$ requires discrete optimization. We adapt the gradient-based top-K token search algorithm (Shin et al., 2020; Qi et al., 2023) and begin by initializing $x_{sg}$ with random tokens of a fixed-length L. Subsequently, for each token $x_{sg}^i \\in x_{sg}$, we identify the top-K candidate tokens V as per reducing the generation probability of harmful content from the MLLM:\n$V := TopK_{w \\in V}[ \\sum_{i=1}^{|C|} w \\nabla_{w}log p(C_i|x_{default})]$", "where": "V indicates a pre-defined set of tokens\u00b9, and w denotes an embedding of w, and the gradient is taken with respect to the embedding of the i-th token $x_{sg}^i \\in X$. We note that no no image tokens are used in Eq. (2), i.e., the optimization is solely based on the text data. The final step is to replace $x_{sg}^i$ with a token in V one by one and find the best token for a replacement as per reducing the loss. A single optimization step comprises updating all the tokens in $x_{sg}$, and we repeat this process for multiple epochs (e.g., 50 times). The final $x_{sg}$ is appended at the end of the input text ($X_{input}$) to act as a safety guardrail and robustify the MLLM against the jailbreak attack.\nAlongside this optimized method, we also consider a separate method that instead sets $x_{sg}$ to a simple, human-written, pre-defined text: \"DO NOT include harmful content in your response\". We retain this method alongside our proposed optimized method for its simplicity and interpretability."}, {"title": "Evaluation", "content": "We evaluate UNIGUARD for its defense against adversarial prompts and its performance on visual question answering (VQA) accuracy using a benign Q&A dataset.\nDataset. To obtain benign and adversarial images, we use the validation set of COCO 2017 (Lin et al., 2014), following Schwenk et al., which includes 1,000 images and corresponding text questions. Adversarial images are generated using the state-of-the-art visual jailbreak attack (Qi et al., 2023), with one image for guardrail creation and the rest for evaluation. Additionally, we apply constrained attacks with $\\epsilon = \\frac{64}{255}$ on sampled images from COCO for evaluation, where $\\epsilon \\in [0, 1]$ represents the perturbation magnitude. For adversarial text, we use the RealToxicityPrompts (RTP) (Gehman et al., 2020) dataset, which contains subtly adversarial prompts that induce the LLM to generate offensive and inappropriate responses. We use 574 harmful strings from AdvBench\u00b2 in (Zou et al., 2023) as the corpus C.\nMLLMs. We start with using LLAVA (Liu et al., 2023a) as the base model due to its wide adoption in user-facing applications like online dialogue systems (Oshima et al., 2023), advertisements (Feizi et al., 2023), and social media content moderation (Jin et al., 2024). To evaluate generalizability, we incorporate additional models, including both open-source models like MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023), as well as proprietary models like Gemini Pro (Team et al., 2023) and GPT-4V (OpenAI, 2023). Details about the models are in Appendix C.\nBaseline Defenses. We compare UNIGUARD with several baseline defense methods. BLURKERNEL and COMP-DECOMP apply image-only defenses by using small average convolution kernels or reduce image quality to diminish the adversarial features. DIFFPURE (Nie et al., 2022) introduces minor noise to the adversarial image through diffusion and purifies it via reverse generation. SMOOTHLLM (Robey et al., 2023) is a text-only defense that applies random perturbations to multiple copies of input text. The toxicity is measured using the average toxicity of multiple responses derived from the text and image. To the best of our knowledge, no defense jointly offers safety guardrails for multiple modalities.\nEvaluation Metrics. We use the Perspective API3 to systematically quantify abusive comments in the generated response. The API evaluates each LLM response across five attributes: toxicity\u2074, identity attack, threat, profanity, and sexually explicit content. The scores reflect the negative impact an LLM response may have in online conversations. For each attribute, we calculate the proportion of generated texts with scores over 0.5. A jailbreak attack is considered successful if any attribute surpasses this threshold.\nImplementation Details. We implemented UNIGUARD in PyTorch (Paszke et al., 2019) and performed all experiments on a Linux server with 5 NVIDIA A100 GPUs. For image safety guardrail generation, we use 5,000 epochs, a batch size of 8, a step size a of $\\frac{8}{255}$, and a distance constraint $\\epsilon$ to $\\frac{64}{255}$. For text safety guardrail generation, we use 100 epochs, a batch size of 8, a maximum sequence length of 16, and a candidate token number of 100. The inference uses a token number between 128 and 1024. We set top-p to 0.9, and set the temperature to 0.6 and 0.9 for adversarial and benign input prompts, respectively."}, {"title": "Overall Performances", "content": "Effectiveness Against Jailbreak Attacks. Table 1 and 4 present the robustness results against unconstrained and constrained visual attacks (Qi et al., 2023), respectively, along with the RTP text prompt (Gehman et al., 2020). The original model exhibits an attack success ratio of over 80%, highlighting the risks of deployment without safeguards."}, {"title": "Sensitivity Analysis", "content": "Figure 5 presents the sensitivity analysis under unconstrained visual attacks (Qi et al., 2023) and RTP (Gehman et al., 2020) adversarial text prompts, focusing on two major hyperparameters: the distant constraint \u03f5 for image safety guardrails and the maximum token length L for text safety guardrails. We observe a trade-off between model robustness and performance: increasing \u03f5 generally reduces the attack success ratio for both types of guardrails but may compromise accuracy on benign tasks (e.g., $\\epsilon \\geq \\frac{128}{255}$). A balance can be achieved at $\\epsilon = \\frac{64}{255}$. For the text guardrail, a medium length L = 16 is preferred, as shorter guardrails may not have enough protective power, whereas longer ones can lead to low-quality LLM responses."}, {"title": "Ablation Studies", "content": "We investigate the usefulness of multimodal safety guardrails in UNIGUARD by selectively disabling the guardrail for one modality while retaining the other. Table 3 presents the ablation results against unconstrained visual attack (Qi et al., 2023) and RTP (Gehman et al., 2020) adversarial text. UNIGUARD with multimodal safety guardrails improve robustness with a lower attack success ratio compared to UNIGUARD with unimodal guardrails. While both improve robustness, the image guardrails has greater contribution to model robustness than the text guardrail. Between pre-defined and optimized text guardrails, the optimized version reduces attack success ratio but increases perplexity.\nGeneralizability. We demonstrate the generalizability of our safety guardrails when using other MLLMs as the base model. Figure 3 shows the results of MiniGPT-4, InstructBLIP, GPT-4V, and Gemini Pro towards unconstrained visual attacks. The full results are in Table 5-8.\nAcross all MLLMs, UNIGUARD shows the lowest attack success ratio among all defenses. Similar to LLAVA 1.5, UNIGUARD with the pre-defined text guardrail shows similar or better performance than the optimized one.\nOn MiniGPT-4, the pre-defined and optimized text guardrails significantly reduced the attack success ratio from 37.20% to 25.88% and 24.98%, respectively, a 13.2% improvement over the best baseline defense. On GPT-4V, where a strict content filtering algorithm pre-filters about 30% of adversarial prompts, only 10% of the remaining ones lead to successful jailbreaks. Regardless, UNIGUARD still enhances the robustness of GPT-4V. Unlike GPT-4V, the jailbreak attack is successful on Gemini Pro as we turn off its safety filter. We observe remarkable robustness improvement when UNIGUARD with image & pre-defined text safety patch is used. Due to the proprietary nature of GPT-4V and Gemini Pro and limited resources, we do not test baseline defenses on them."}, {"title": "Related Work", "content": "4.1 Multimodal Large Language Models\nLLMs have demonstrated exceptional capabilities in instruction following (Lou et al., 2024), and text generation (Zhao et al., 2024; Xiao et al., 2024; Li et al., 2024). These models are characterized by billion-scale parameters, enormous training data (Jin et al., 2023; Xiong et al., 2024), and emergent reasoning capabilities (Wei et al., 2022). Multimodal LLMs (MLLMs) extend LLMs by integrating visual encoders to enable general-purpose visual and language understanding, exemplified by open-source models such as Pixtral (AI, 2024), LLAVA (Liu et al., 2023b,a), MiniGPT-4 (Zhu et al., 2023), InstructBLIP (Dai et al., 2023), and OpenFlamingo (Awadalla et al., 2023), as well as proprietary models like GPT-4V (OpenAI, 2023) and Gemini (Reid et al., 2024). This work primarily focus on open-source models, as their accessible fine-tuning data and weights enable researchers to develop more efficient protocols and conduct comprehensive evaluation.\n4.2 Adversarial Attacks and Defenses on LLMs and MLLMs\nThe versatility of LLMs has made them susceptible to adversarial attacks, which exploit the models' intricacies to bypass their safety guardrails or elicit undesirable outcomes such as toxicity and bias (Chao et al., 2023; Yu et al., 2023; Zhang et al., 2023; Nookala et al., 2023; Dan et al., 2024). For example, Qi et al. demonstrated that a single visual adversarial example can universally jailbreak an aligned model, leading it to follow harmful instructions beyond merely replicating the adversarial inputs. In response, various defense strategies have emerged. Among these, DiffPure (Nie et al., 2022) applies diffusion models to purify adversarial examples. However, the extensive time requirement for the purification process, which is in proportion to the diffusion timestep, coupled with the method's sensitivity to image colors, limits its applicability in scenarios demanding real-time responses and diminishes its effectiveness against color-related corruptions. SmoothLLM (Robey et al., 2023) enhances the model's ability to detect and resist adversarial attempts by randomly perturbing and aggregating predictions from multiple copies of an input prompt. In this work, we propose a pioneering multimodal safety guardrails for MLLMs to improve their adversarial robustness against jailbreak attacks."}, {"title": "Conclusion", "content": "We introduced UNIGUARD, the first multimodal defense framework to enhance the robustness of multimodal large language models (MLLMs) against jailbreak attacks. UNIGUARD optimizes multimodal safety guardrails that reduce the likelihood of harmful content generation by addressing adversarial features in input data, leading to safer outputs from MLLMs."}, {"title": "Limitations", "content": "Despite the effectiveness of UNIGUARD, there remain areas for further enhancement. First, although UNIGUARD demonstrates noticeable transferability across MLLMs, tailoring safety guardrails to specific models could improve defenses, though at the cost of additional computational resources. Developers may need to balance the choice between universal and model-specific safety guardrails based on their specific requirements. Second, UNIGUARD is currently designed to safeguard MLLMs with image and text inputs. Expanding UNIGUARD capabilities to support additional modalities, such as audio and video, would increase its applicability and make it more effective across a broader range of tasks, such as content moderation in multimedia environments. In addition, we identify a trade-off between reducing the toxicity of model outputs and maintaining model performance. Future research could explore this balance in greater depth and refine strategies that preserve both safety and model efficacy. Finally, training approaches can be further improved for the fluency of responses produced using the optimized text guardrail, and prompt engineering can be done to improve the performance of the pre-defined text guardrail."}, {"title": "Ethical Considerations", "content": "Ethical Data Usage. UNIGUARD optimizes a safety guardrail using a small harmful corpus, which poses risks of misuse and potential leakage of toxic information. Researchers should implement strong safeguards to prevent unintended exploitation or exposure.\nEvolving Adversarial Threats. While UNIGUARD addresses state-of-the-art adversarial attacks across multiple modalities, the rapid evolution of attack techniques means few defense strategies can guarantee complete coverage. Relying solely on one system risks exposure to novel forms of adversarial attacks, particularly as attack strategies evolve within different social and cultural contexts. Thus, continuous refinement of defense strategies is necessary.\nBias and Content Filtering. Overly restrictive content filters could suppress legitimate or creative outputs, introducing biases that misclassify benign inputs as harmful. This may reduce the flexibility of MLLMs, limiting their effectiveness in applications like satire, artistic expression, or nuanced conversations.\nBroader Impact. The multimodal safety guardrail offered by UNIGUARD can significantly enhance the robustness of MLLMs, which can positively impact numerous fields reliant on large language models, including education, content moderation, and customer service. The deployment of such models with robust defenses could lead to safer online environments by minimizing the risk of harmful content generation. This has broad societal implications, potentially reducing the spread of misinformation, hate speech, and other malicious outputs generated by AI models. On the other hand, the implementation of safety measures like UNIGUARD could introduce new challenges. There is a potential for bias to be embedded in the safety guardrails themselves, depending on the nature of the training data and the optimization processes used. In particular, marginalized communities may be disproportionately affected if their language patterns or content are more frequently flagged as harmful due to cultural or linguistic misunderstandings in the model."}, {"title": "Appendix", "content": "Additional Experimental Results\nTables 5, 6, 7, 8 show the robustness test results on the other two state-of-the-art MLLMs, MiniGPT-4 and InstructBLIP, against both unconstrained and constrained visual attacks (Qi et al., 2023) and RTP (Gehman et al., 2020) adversarial text. Figure 4 summarizes all the robustness test results on MiniGPT-4 and InstructBLIP MLLMs. In all tables, UNIGUARD lowers the attack success ratio the most compared to all defense baselines, which demonstrates the transferability and usefulness of multimodal safety guardrails of UNIGUARD.\nDetails About Model Selection\nWe leverage five state-of-the-art MLLMs.\n\u2022 LLAVA 1.5 (Liu et al., 2023a) effectively bridges the visual encoder CLIP (Radford et al., 2021) with the language encoder LLaMA-2 (Touvron et al., 2023) through a novel cross-modal connector;\n\u2022 Gemini Pro (Team et al., 2023) is a highly scalable and efficient MLLM trained with Google's Tensor Processing Units (TPUs). Gemini Pro offers an adjustable safety filter, where a user can control the probability threshold for blocking unsafe responses.\n\u2022 GPT-4V (OpenAI, 2023) is an extension of GPT-4 to analyze the image input provided by the user, along with the text input. GPT-4V has a strong content filter and refusal system against the jailbreak attack to ensure the safety of the model's output.\n\u2022 MiniGPT-4 (Zhu et al., 2023) employs a novel alignment strategy, integrating a frozen visual encoder EVA-CLIP (Fang et al., 2023) with a frozen Vicuna (Chiang et al., 2023) model via a projection layer.\n\u2022 InstructBLIP (Dai et al., 2023) introduces an innovative Q-Former to extract instruction-aware visual features from the output embeddings of the frozen image encoder.\nDetails About Model Selection\nWe use GPT-40 to improve the writing of our manuscript."}]}