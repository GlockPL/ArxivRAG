{"title": "Not Every AI Problem is a Data Problem: We Should Be Intentional About Data Scaling", "authors": ["Tanya Rodchenko", "Natasha Noy", "Nino Scherrer", "Jennifer Prendki"], "abstract": "While Large Language Models require more and more data to train and scale, rather than looking for any data to acquire, we should consider what types of tasks are more likely to benefit from data scaling. We should be intentional in our data acquisition. We argue that the topology of data itself informs which tasks to prioritize in data scaling, and shapes the development of the next generation of compute paradigms for tasks where data scaling is inefficient, or even insufficient.", "sections": [{"title": "Isn't Scaling All You Need?", "content": "Scaling laws for LLMs have been driving our thinking on how to build optimal models almost since the inception of transformers (Vaswani, 2017). In 2020, OpenAI researchers demonstrated the rela-tionships between compute budget, model size, and dataset size for an optimal model (Kaplan et al., 2020). Subsequently, researchers from Google DeepMind investigated the compute-optimal growth of model and dataset size under a fixed compute budget, demonstrating that the model size and train-ing data should be scaled equally (Hoffmann et al., 2022). The scaling narrative suggests that this path could continue to yield improvements indefinitely, allowing LLMs to get better and better, and addressing more and more use cases over time.\nHowever, most scaling laws rely on the inherent assumption that we have an infinite supply of qual-ity data. In reality, quality data, and in particular human-generated quality data, is of course fi-nite. As models grow into hundreds of billions of parameters, we tend to run out of quality data (Villalobos et al., 2022; Edwards, 2024).\nLow quality data contains both irrelevant information like duplicates and diluted content, as well as unreliable or meaningless data. Some amount of low-quality data assuming we can reliably identify"}, {"title": "Where Data-Driven Scaling Thrives and Stumbles", "content": "Let's consider the advancements that large models are catalyzing in robotics (Vanhoucke, 2024), drug discovery (Jumper et al., 2021), and machine translation (Zhu et al., 2023). What are the common threads among these success stories?\nConsider Machine Translation (MT). Early deterministic AI translation models struggled with con-textual nuances of language (American Translators Association, 2018), such as capturing culturally specific elements. Now transformer-based language models excel in this area due to several factors. First, the relatively static nature of language, with its abstractable rules and gradual vocabulary evo-lution, offered a stable foundation for model training. Second, high quality translation data, often sourced from reputable publishers and professional translations, further enabled effective training. Expansive datasets, including non-parallel corpora, enhanced the naturalness and contextual under-standing of these models. Finally, LLM's vastly superior capacity to understand contexts (e.g., an entire document) led to significant improvements in translation quality.\nAt the other end of the spectrum is one of AI's most formidable challenges\u2014robust and reliable reasoning, where AI systems often fail surprisingly at tasks that are easy for humans. At the time of writing, Google's Gemini 2.02 and OpenAI's o13 have just raised the bar on reasoning benchmarks. And yet, OpenAI's o1 still fails on simple alterations of existing benchmarks (Mirzadeh et al., 2024; Gulati et al., 2024) or more complex math problems (Glazer et al., 2024).4 In our discussions with various field experts, they almost unanimously agreed that this challenge is primarily rooted in model architecture and learning algorithms, rather than data-driven scaling."}, {"title": "Predictive Power of Data Shape", "content": "Successful use cases offer clues on where data scaling will be helpful under the current learning paradigm. We find the framework of Topological Data Analysis (Carlsson, 2009), which aims to identify the intrinsic dimensions and patterns within datasets, particularly useful. While scientists discussed the concept of 'shape of data' as early as 2009, it still remains relatively underexplored (Uchendu and Le, 2024)."}, {"title": "Data Acquisition as Another Predictor", "content": "Beyond data shape, the feasibility of data-driven scaling is largely determined by the nature of the data-acquisition process. Intuitively, if quality data is available and accessible, the potential for scaling increases significantly. For example, as we continue to collect sensor data from autonomous cars, their reliability and performance will continue to improve.\nThe specific type of data to collect also plays a crucial role. For example, training on step-by-step-style content that embodies \"procedural knowledge\" leads to significant performance improvement in the current learning paradigm (Ruis et al., 2024; Yu et al., 2024a). The abundance of such data in domains like math and coding, coupled with the emergence of strong evaluation metrics, has fueled rapid progress in these areas. However, acquiring or creating procedural knowledge for other tasks remains a significant challenge.\nAs we discuss acquiring quality data, it is crucial to note that the very definition of data quality is nuanced. For starters, quality data needs to be accurate, reliable, and complete. Additionally, training data must be dense with useful information and offer novel insights compared to the data already available for training. High quality data needs to be at the right level of granularity given the use case, timely and diverse enough to provide meaningful insights. In other words, the definition of high quality data is not universal; the quality is connected to a use case and the value that the trained model delivers to the user.\nFinally, we cannot fully assess the impact of data without critically looking at today's evaluation frameworks. Our evaluation approaches need to better reflect the nuances and needs of real-world users, and focus less on relatively simplistic single-turn benchmarks. Current benchmarks test limited aspects of performance which do not necessarily translate to user value. The next generation of evalu-ation approaches needs to consider how AI models handle real-world complexities, measure stochastic performance, and reflect user satisfaction and economic value.\nUnderstanding the data-acquisition challenges is a critical component of making an informed decision about a potential scaling initiative, including an ultimate assessment of whether the benefits of scaling outweigh the costs."}, {"title": "The Promise of Intentional Data Scaling", "content": "We should be intentional in data-driven scaling. By focusing on use cases with a strong hypothesis about efficacy of scaling, and by collecting fit-for-purpose data based on the needs of these use cases, we can make model training more efficient and reduce the volume of data that we need. Even with existing data, intentional filtering and selection are crucial to ensure that a larger fraction of training mixture is of high quality.\nAs we continue to learn how to define the shape of data, and how these dimensions impact model performance, an evolution of this approach could play a role in active learning (Mindermann et al., 2022; Evans et al., 2024), where models prioritize the right type of data during training via human-in-the-loop and model-in-the-loop, potentially accelerating progress even further. Moreover, the relation between topological dimensions of data and model performance is likely to provide us crucial pointers on where current learning paradigms fail, and hence inform the next generations of learning paradigms as well as relative value of various datasets.\nBy adopting a more intentional approach, we can build a more focused and efficient AI-powered future, using resources efficiently and paving the way for tackling complex AI challenges that require more than just data and scale."}]}