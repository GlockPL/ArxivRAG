{"title": "Contrastive representations of high-dimensional, structured treatments", "authors": ["Oriol Corcoll", "Athanasios Vlontzos", "Ciar\u00e1n M. Gilligan-Lee", "Michael O'Riordan"], "abstract": "Estimating causal effects is vital for decision making. In standard causal effect estimation, treatments are usually binary- or continuous-valued. However, in many important real-world settings, treatments can be structured, high-dimensional objects, such as text, video, or audio. This provides a challenge to traditional causal effect estimation. While leveraging the shared structure across different treatments can help generalize to unseen treatments at test time, we show in this paper that using such structure blindly can lead to biased causal effect estimation. We address this challenge by devising a novel contrastive approach to learn a representation of the high-dimensional treatments, and prove that it identifies underlying causal factors and discards non-causally relevant factors. We prove that this treatment representation leads to unbiased estimates of the causal effect, and empirically validate and benchmark our results on synthetic and real-world datasets.", "sections": [{"title": "1 Introduction", "content": "Estimating the causal effect of a treatment is crucial for actionable decision-making Richens et al. [2020], Vlontzos et al. [2023], Gilligan-Lee [2020], Pearl [2009], Jeunen et al. [2022], Van Goffrier et al. [2023], Corcoll et al. [2022], Reynaud et al. [2022], Zeitler et al. [2023], O'Riordan and Gilligan-Lee [2024], Van Goffrier et al. [2023] In standard effect estimation, treatments are usually binary- or continuous-valued. However, in many cases of real-world importance, treatments correspond to complex, structured, high-dimensional objects, such as text, audio, images, graphs, or products in an online market place, to name a few. This setting provides a challenge to traditional causal estimation methods that must be overcome if we are to understand cause and effect in real-world settings.\nWhile leveraging shared structure across different treatments can help generalize to unseen treatments at test time, and improve data efficiency, we show in this paper that using such structure blindly leads to biased causal effect estimation. Indeed, in most cases, the outcome is actually caused by underlying causal variables corresponding to latent aspects of the complex treatment object we observe the tone of a text, for instance. The object we use to characterise the treatment can be thought of as a high-dimensional proxy for these underlying causal latent variables. Importantly, the treatment object can also be proxies for other latent variables which do not causally impact the outcome, such as the style of a text. We show that when such non-causal latent variables are correlated with confounding variables in a given setting, then directly using the high-dimensional, structured treatment for causal effect estimation leads to bias-even when all confounders are observed."}, {"title": "2 Background and definitions", "content": "We adopt the Structural Causal Model (SCM) framework as introduced by Pearl [2009].\nDefinition 1 (Structural Causal Model). A structural causal model (SCM) specifies a set of latent variables U = {u1,..., un} distributed as P(U), a set of observable variables X = {X1,..., Xm}, a directed acyclic graph (DAG) G, called the causal structure of the model, whose nodes are the variables UUX, a collection of functions F = {f1,..., fn}, such that Xi = fi(PA(Xi), ui), for i = 1,..., n, where PA denotes the parent observed nodes of an observed variable.\nA (hard) intervention on variable T is denoted by do(T = t), and it corresponds to removing all incoming edges in the causal graph and replacing its structural equation with a constant.\nThe main causal quantity of interest in this work is the conditional average treatment effect (CATE), which corresponding to the change in outcome for different treatments T, T' at covariate value x:\n\u03c4(T, T', x) := E (Y | do(T), X = x) \u2013 E (Y | do(T'), X = x)\nWhen confounders are observed and d-separate treatment and outcome, the CATE can be estimated via back-door adjustment Pearl [2009] as follows:\n\u03c4(T,T', x) = E (Y | T, X = x) \u2013 E (Y | T', X = x)"}, {"title": "3 The problem", "content": "In this paper, we consider a setting where the object describing the treatment is generated by some collection of latent variables which can causally interact with one another. These could be, for instance, latent aspects of a piece of text, such as tone or style, a collection features representing a video, or the structure of the bonds in a molecule. We denote the causally relevant latent variables by Tc = {T_c^1,...,T_c^n} and the non-causally relevant latent variables by Tnc = {T_{nc}^1,...,T_{nc}^n}.\nIn the general case, we may not be given direct access to the latents themselves, but some function\nIn this setting, a given Te is mapped to a set of T values, indexed by the Tnc latents: Tc \u2192 Trc = {T = f(Tc,Tnc)}Tnc. For causal effect estimation using treatment T to be unbiased, we require that the CATE using T must reproduce the correct CATE with Tc. That is:\n\u222b (\u03c4(Tc,TC, X) \u2013 \u0442(T,T', X))\u00b2 P(X)dX = 0, \u2200T\u2208 TTC and T' \u2208 TT'C\nThe standard approach to estimating the causal effect of a treatment, T, on outcome, Y, with confounders, X, is to estimate Y given T and X-known as back-door adjustment Pearl [2009]. We now show that this approach can result in an unbiased estimate of the causal effect when the treatment T is a mixture of causal and non-causal latents.\nTheorem 1. Consider treatment T from the DAG of Figure 1, with structural equations as given at the start of Section 3. Back-door adjustment directly using T leads to biased causal effect estimation.\nProof. To show back-door adjustment with T does not suffice for causal effect estimation, we just need to construct at least one example where it fails. Consider the following data generation process:\nX = ex, Tc = \u03b1X + \u20acTC, Tnc = BX + \u20ac\u03c4\u03b7\u03c2, Y = pTc + \u03b4X + \u20acy, and T = [\\frac{Tc}{Tnc}]\nwith \u20ac\u00bf ~ \u039d(0, \u03c3\u03b5). This provides us with a joint distribution P(Y, X, T).\nIn order to show backdoor adjustment fails, we need to show that regressing Y onto T and X does not always result in unbiased estimates of the causal effect of Tc on Y. That is, we need to show the existence of a model where E(Y | T, X) is equal to E(Y | Tc, X) from the above data generation process, but where \u03c4(T, T', X) from this model does not equal t(Tc, T'C, X) from the data generation process, above, where T \u2208 TTC and T' \u2208 Tr\u02b9. In this case, E(Y | T, X) does not identify E(Y | do(Tc), X).\nConsider the following model for Y:\nY = [\u03b1/2\u03b2/2] Tc/Tnc + X + \u20acx = [\u03b1/2\u03b2/2] T + X + \u20acy\nAs Tnc = BX + \u20acTnc it follows that the expected values for Y given T and X, E(Y | T, X), generated by this model is equivalent to E(Y | Tc, X) from the data generation process, above. As E(Y | T, X) = E(Y | Tc, X), this model is a possible solution to regressing Y on T and X. However, when we intervene on Tnc, we break the relationship between X and Tnc, which reveals"}, {"title": "we have not learned the correct causal model. To see this, consider T", "content": "= [\\frac{Tc}{Tnc}] and T' = [\\frac{Tc}{T'nc}]\nwith Tnc \u2260 T'nc. Here, r(Tc, Tc, X) = 0, but \u315c(T, T', X) = 2\u03b2/2(Tnc \u2013 T'nc) \u2260 0.\nWhy does this happen? Well the Tnc are proxies for the confounders, X. So using them in our estimation can make it look like we have suitably controlled for the confounders using the Tnc, but when we intervene on the Tnc we break the link between Tnc and X and reveal that we have not appropriately controlled for the true confounders.\nIn the experiments section we investigate this empirically on both synthetic and real data, and find that back door adjustment of T leads to biased effect estimation.\nIn order to estimate an unbiased causal effect, we should not directly use the high-dimensional treatment itself. Instead, in the example from Theorem 1, had we been able to use a representation of T that does not contain any information about the non-causal latents, then the backdoor adjustment with this representation would have identified the correct causal effect. This motivates using a representation of the treatment (T) to estimate causal effects. If we learn a representation of T that doesn't depend on the non-causal latents, it seems intuitive that effect estimation will be unbiased in general. We now prove this is necessary and sufficient.\nTheorem 2. Causal effect estimation is unbiased if and only if a representation of T is used that contains no information about the non-causal latents.\nProof. Assume first that \u03c8(T) contains no information about Tnc. Then it must map all T\u2208 TT_c to the same value. That is, (T) is just a reparametrization of Tc, as Tc are in one-to-one correspondence with TTC. This moreover means that (.) preserves interventions on Tc, which implies it preserves CATE.\nTo show the other direction, that unbiased CATE implies (T) contains no information about Tnc, consider the following. For the CATE to be unbiased we require that\n\u222b (\u03c4(TC, TC, X) - (T, T', X)\u00b2 P(X)dX = 0, \u2200T\u2208 TTC and T' \u2208 TTC.\nIn particular, this means that for T, T' \u2208 TT\u0108 with T \u2260 T', we have that:\n0 = \u222b (\u03c4(TC, TC, \u0425) \u2013 \u0442(T,T', X))\u00b2 P(X)dX = \u222b (\u03c4(T,T', X))\u00b2 P(X)dX\nAs all terms in the integral are positive, for it to be equal to zero we must have each term equal to zero. But P(X) is positive on its support set, hence we have that for all X\n0 = \u03c4(T, T', X) = E(Y|\u03c8(T), X) \u2013 E(Y|\u03c8(T'), X)\n\u21d2 E(Y|\u03c8(T), X) = E(Y|\u03c8(T'), X), \u2200X and T,T' \u2208 TTC\nThis tells us that \u03c8(T) and \u03c8(T') are interventionally equivalent from the point of view of Y. As the only difference between T and T' are their non-causal latents, then the representation (.) must map all T\u2208 TTo to the same value. Hence it must disregard information about Tnc."}, {"title": "4 A contrastive algorithm for learning causally relevant treatment representations", "content": "How do we learn a representation that has no information about Tnc? To build intuition, consider the structural equations from Section 3, with f(.) in Y = f(Tc, X) an invertible function. Suppose we have two data points where the X and Y values are the same, but the T's are different: [T, x, y], [T', x, y]. As Y = f(Tc, X), we have that f(Tc,C) = f(T'c, X). As this function is invertible, the causal components of T and T' are the same. However, data points where the X values are the same, but the Y values are different must have different causal components.\nThis observation suggests a contrastive algorithm with positive and negative pairs as below should push T with similar Te together, and different Te apart Oord et al. [2018], Tingey et al. [2021].\nPositive pairs: [T, x, y], [T', x, y] such that T \u2260 T', and X = X', and Y = Y'\nNegative pairs: [T, x, y], [T', x, y] such that T \u2260 T', and X = X', and Y \u2260 Y'\nWe now show this provably block identifies the causal components of T. The resulting representation (T) contains all and only information about Tc: there exists an invertible \u03c6: \u03c8(T) = $(Tc).\nTheorem 3. Assume a structural causal model with DAG from Figure 1 and equations X = l(ex), Tc = g(X,\u20acTc), TnC = h(X,\u20acTnc), T = m(Tc,Tnc) and Y = f(Tc, X), with all functions smooth and invertible with smooth inverses, with noise terms drawn i.i.d. e\u00bf ~ P(ei) from smooth distributions that have P(\u20ac\u00a1) > 0 almost everywhere. Then the contrastive approach outlined above yields a representation of T that block-identifies the causal latents.\nProof. Theorem 4.2 from Von K\u00fcgelgen et al. [2021] can be applied to help us prove that our con-trastive learning approach with positive and negative pairs as above yields a treatment representation that identifies the latents in T that Y causally depends on. This Theorem states that when we have data involving two classes of variables, if we can create pairs of data points with one of the pair being the original view and the other an augmented view, such that a subset of one class is different to the original view, then we can block identify the class of variables that remains the same. The theorem holds as long as the underlying data generating process consists of smooth, invertible functions with smooth inverses, and smooth distributions that are non-zero almost everywhere.\nWe are going to use the above described theorems to prove we can block identify Tc. To do this, we need to show that we can take an observation T = (Tc,Tnc) and \u201caugment\u201d it to get (Tc, Thc), where To is the same but (possibly some subset of) Tnc is not.\nConsider two data points where the X and Y values are the same, but the T's are possibly different: [T, x, y], [T', x, y]. We have that y = f(Tc,x) = f(T\u0106, x). As f is invertible we have that Tc = TC. What this means is that the causally relevant components of T and T' are the same when the values of Y and X are the same. But we need to also show that our augmentations have different Tnc components. That is, these augmentations leave To invariant, but change (some subset of) Tnc.\nIf there exists different T, T' that occur with the same values of X and Y, then Tnc must be different, as Te is the same. But does there exist at least two different T's for some values of X and Y? If there doesn't then this means that The only depends on X and not the noise term ETC, which is a contradiction as we assumed at the start that P(\u20acTnc) has non-trivial support. Thus choosing data augmentations in this fashion ensures Te is invariant between augmentations, but Tnc is not. Hence we can apply Theorem 4.1 from Von K\u00fcgelgen et al. [2021]to conclude the proof.\nGiven high-dimensional covariates X and continuous outcome Y, the approach to constructing positive pairs from the start of this section is impractical. Instead of demanding equality X = X' and Y = Y' between samples to find positive pairs, one could instead impose thresholds \u03b4, \u20ac and consider X, X' and Y, Y' \u201cclose\u201d if |X \u2013 X'\\ < \u03b4 and |Y \u2013 Y'| \u2264 6. Additionally, one could also first learn a low-dimensional representation g(.) of X and consider X, X' close if |g(X) \u2013 g(X')| \u2264 \u03b4. Indeed, for continuous g(.), if g(X), g(X') are close, so too are X, X'. In this setting, samples [X, T, Y], [X', T', Y'] with: |g(X) \u2013 g(X')| \u2264 d and |Y \u2013 Y'| < \u0454 also have similar Te and Te. Indeed we have | f (Tc, X)-f(T'c, X')| = |Y \u2212Y'| < \u0454. For continuous g(.) with |g(X)-g(X')| \u2264 \u03b4, there exists a p such that we have X \u2248 X' + p. f(T'c,X') = f(T'c, X + p) \u2248 f(T', X) by Taylor expanding smooth f(.) with small p. This implies |f(Tc, X) \u2212 f(T', X)|\u0454,\u2200X. For smooth f(.) we have that Tc and T\u0107 are close. If, instead we had |Y \u2013 Y'| > \u20ac, then Tc and T'e would not be close, which provide negative samples. Hence a contrastive approach with such positive and negative pairs should still intuitively push T's with similar Te's together, and dissimilar Te's apart.\nAlgorithm 1 describes this practical contrastive approach to learn representations of high-dimensional treatments, which we empirically validate on synthetic and real data in Section 6"}, {"title": "5 Related work", "content": "While many works have approached the task of invariance and disentanglement through contrastive learning (for example Wang et al. [2021], Li et al. [2021], Liu et al. [2024]), few, to the best of our"}, {"title": "6 Experiments", "content": "The contrastive approach presented in Section 4 aims to make a effect estimation model more robust to non-causal information present in high-dimensional treatments. Non-causal information presents a crucial risk to machine learning models. A model fails to discard non-causal information due to two types of errors: irreducible and reducible errors. In other words, error due imperfect information among covariates and treatment; or due to the inability of the learning mechanism to model the problem correctly. Regardless of the error type, causal models should discard non-causal information.\nDatasets A common characteristic among each of the datasets used in the experiments is that, similarly to Fig. 1, multidimensional treatments are constructed from causal and non-causal information. The goal is to evaluate that the model is able to discard non-causal information and retain causal information. To add complexity through irreducible error, we use a Synthetic dataset, as in Fig. 1. This synthetic dataset has 1000 samples (70% for training and 30% for evaluation); the treatment variable has 10 dimensions, 5 are causal and 5 are non-causal, both highly correlated with the covariates; the outcome is causally determined by the covariates, the causal part of the treatment and random noise. On the other hand, to introduce complexity through reducible error we use the Molecule dataset Ramakrishnan et al. [2014], Weinstein et al. [2013] and the Coat recommender dataset Schnabel et al. [2016]. These two datasets have more complex causal relations than the Synthetic dataset and large part of the error should be reducible by the model. See Appendix A.\nModels The contrastive method is applied to a classical CATE model (see Fig. 2a). The contrastive loss chosen en these experiments is the Triplet loss Schroff et al. [2015]. Positive and negative pairs are selected using a simple clustering method, referred as g in Section 4, where each component of the variable is bucketed, thus converting continuous variables into discrete ones where the method described is easily applicable. The contrastive CATE model is compared to two baselines; the exact"}, {"title": "6.1 Irreducible error", "content": "A common source of error in ML models is due to the lack of information in their inputs. Problems with imperfect information can make the model rely on correlations instead of the true causal relations between treatment, covariates and outcome. This experiment aims to study if the contrastive method proposed makes the model more robust to this kind of errors. To this end, the Synthetic dataset is perturbed by adding additional noise to the outcome before training. The added noise comes from a normal distribution with mean zero and its standard deviation increases linearly in steps of 0.1 starting at 0.0 up to 1.0. Note that due to the learning mechanism used, a model may incorrectly pick on correlations between the treatment and covariates, even without intervening these variables.\nAn ideal model would predict the same outcome regardless of the non-causal information in the treatment (tnc), since this information does not causally influence the outcome. What the experiment in Fig. 2b shows is the difference in predictions (the effect) between a sample (x, t, y) and a perturbed version of the treatment (x, t', y) where t' only has its tnc' component changed. We can see that the contrastive method achieves, to a reasonable degree, that effect; but the CATE and SIN models fail to"}, {"title": "6.2 Reducible error", "content": "Another source of error is in the intrinsic complexity of the problem, the more complex the problem the harder it is to perform well, requiring larger models or larger datasets. Even when the data has the right information to discard the non-causal information, it may be difficult for a learning mechanism to do so. This experiment tests the ability of each model to discard non-causal information when the irreducible error does not change (it is intrinsic to the data) but where reducible error is introduced in the form of noise on the non-causal information at test time.\nWe would expect for the model to be able to completely ignore the non-causal information but as shown in Fig. 3 the SIN and CATE models are less robust, having larger differences in effect when perturbed the non-causal component of the treatment. Moreover, Table 2 again shows how all models have good performance on the problem but that only the contrastive one has low PEHE and thus, it is\n\u00b9after extensive hyperparameter search on SIN, it does not achieve similar performance to the CATE models"}, {"title": "7 Conclusion", "content": "In this paper we investigated a challenging setting for causal inference with importance for real-world applications: causal effect estimation when treatments are high-dimensional, structured objects. We showed that using the shared structure across different treatments blindly can lead to biased causal effect estimation. To address this challenge we devised a novel contrastive approach that learns a representation of the high-dimensional treatment which provably identifies the underlying causal latents and discards the non-causal ones. We also proved that using this treatment representation provides unbiased causal effect estimation, and empirically validated our results on synthetic and real-world datasets. Lastly, we demonstrated that previous work on causal effect estimation with high-dimensional treatments does not result in unbiased estimation of causal effects."}, {"title": "Appendices", "content": ""}, {"title": "A Dataset", "content": "Synthetic dataset: the following (python) pseudo-code describes how data is generated for the Synthetic dataset. The dataset generate for experiments in Sec. 6 has 1K samples with 5 causal dimensions and 5 non-causal dimensions.\ndef synthetic_dataset(n: int, y_noise_std: float)\nx = Normal(0,1,n)\nt_causal = x[:, :causal_dimensions] + Normal(0,1,n)\nt_non_causal = x[:, causal_dimensions:] + Normal(0, 1, n)\nt = concat(t_causal, t_non_causal, axis=-1)\nmask_t = zeros_like(t)\nmask_t[:, :causal_dimensions] = 1\ny_noise = Normal(0, y_noise_std, n)\ny = sum(mask_t * t + x + y_noise, axis=-1)\nreturn y\nCoat recommender dataset: the coat recommender dataset is a real-world dataset generated using ratings of users to coats. All components of the treatment/coat are determined causal and additionally we add a 8 dimensional vector to the treatment that is correlated with the covariates/users. The dataset has 10K samples with 33 causal dimensions and 8 non-causal dimensions.\nMolecule dataset the molecules dataset is another real-world dataset used in the experiments with 5K samples, 8 causal dimensions and 8 non-causal dimensions. Note that we use the PCA covariates of the dataset and properties as treatments."}, {"title": "B Model", "content": "CATE model: Fig. 2a shows the architecture of the model. The covariates sub-network has one layer and treatment sub-network has two layers of hidden size 32. The common sub-network has two layers of size 64 and 32 each. It is optimized using Adam optimizer, learning rate of le 4 and Huber loss.\nContrastive model: is the same as the CATE model but with a weighted contrastive loss. The triplet loss is weighted by 0.1 for the Synthetic dataset and 1 for the Molecule and Recommender datasets. The margin hyperparameter of the loss is set to 30 for the Synthetic dataset and 100 for the Molecule and Recommender datasets.\nSIN: uses the same hyperparameters as in the original paper but with 3 layers and reduced to 32 the hidden size of the sub-networks. Additionally the GNN is replaced with an MLP due to the adaptations of the dataset to have non-causal dimensions."}, {"title": "C Compute", "content": "All our experiments run on a CPU machine with 16 cores and 64GB of memory."}]}