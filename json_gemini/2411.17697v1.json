{"title": "StableAnimator: High-Quality Identity-Preserving Human Image Animation", "authors": ["Shuyuan Tu", "Zhen Xing", "Xintong Han", "Zhi-Qi Cheng", "Qi Dai", "Chong Luo", "Zuxuan Wu"], "abstract": "Current diffusion models for human image animation struggle to ensure identity (ID) consistency. This paper presents StableAnimator, the first end-to-end ID-preserving video diffusion framework, which synthesizes high-quality videos without any post-processing, conditioned on a reference image and a sequence of poses. Building upon a video diffusion model, StableAnimator contains carefully designed modules for both training and inference striving for identity consistency. In particular, StableAnimator begins by computing image and face embeddings with off-the-shelf extractors, respectively and face embeddings are further refined by interacting with image embeddings using a global content-aware Face Encoder. Then, StableAnimator introduces a novel distribution-aware ID Adapter that prevents interference caused by temporal layers while preserving ID via alignment. During inference, we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to further enhance the face quality. We demonstrate that solving the HJB equation can be integrated into the diffusion denoising process, and the resulting solution constrains the denoising path and thus benefits ID preservation. Experiments on multiple benchmarks show the effectiveness of StableAnimator both qualitatively and quantitatively.", "sections": [{"title": "1. Introduction", "content": "Diffusion models [8, 16\u201318, 31, 37, 41, 42, 45, 51, 54\u201356] have achieved remarkable success in image/video generation, significantly inspiring researches in image animation [22, 34, 47, 57, 64, 66]. In particular, human image animation explores generative models [22, 34, 38, 39, 47, 50, 57, 64, 66] to animate a reference image conditioned on a sequence of poses through synthesizing controllable human animation videos, offering diverse applications in entertainment content creation and virtual reality experiences. However, when dealing with pose sequences that exhibit significant motion variation, current approaches suffer from significant distortions and inconsistencies, particularly in facial regions destroying identity information.\nTo address this issue, there are a number of approaches"}, {"title": "2. Related Work", "content": "Diffusion for Video Generation. Renowned for the capacity in diversity and high-fidelity, diffusion models [8, 16\u201318, 31, 32, 37, 41, 42, 45, 53] have demonstrated significant success in the video generation. Compared with image generation, video generation requires additional temporal smoothness and temporal consistency. Current video generation models [4, 13, 40, 43, 44, 48, 52] tend to add temporal layers to pre-trained image generation diffusion models for joint spatio-temporal modeling. Some researchers replace the diffusion U-Net with the transformer [1, 19, 30, 33, 58, 62] for facilitating generative performance. Inspired by previous image animation models [34, 64], we utilize Stable Video Diffusion (SVD [3]) as the backbone.\nID Consistency Image Generation. Studies have explored ID preservation in the image domain. LoRA [21] applies a few additional weights for customized dataset training, but it requires individual training for each character, restricting its flexibility. IP-Adapter-FaceID [61] attempts to directly separate the cross-attention layers for text features and face features, which potentially introduces the misalignment among features. PhotoMaker [29], FaceStudio [59], and InstantID [46] present hybrid ID preservation mechanisms for refining face embeddings. ConsistentID [23] designs a facial prompt generator for capturing facial details. PuLID [14] introduces contrastive alignment loss and accurate ID loss, ensuring ID fidelity. However, these models cannot be directly integrated into video diffusion models, as the temporal layers may alter the spatial distribution, resulting in domain mismatching with diffusion latents. This conflict between video fidelity and ID consistency ultimately degrades the quality of animations. By contrast, our StableAnimator can integrate ID information into video diffusion models via a distribution-aware ID Adapter, effectively resolving the above conflict."}, {"title": "3. Method", "content": "Illustrated in Fig. 2, StableAnimator is based on the commonly used SVD [3] following previous works [34, 64]. A reference image is processed through the diffusion model via three pathways: (1) Transformed into a latent code by a frozen VAE Encoder [27]. The latent code is duplicated to match video frames, then concatenated with main latents. (2) Encoded by the CLIP Image Encoder [36] to obtain image embeddings, which are fed to each cross-attention block of a denoising U-Net and our Face Encoder, respectively, to modulate the synthesized appearance. (3) Input to Arcface [7] to gain face embeddings, which are subsequently refined for further alignment via our Face Encoder. Refined face embeddings are then fed to the denoising U-Net. More details are described in Sec. 3.1. A PoseNet with a similar architecture as AnimateAnyone [22] extracts the features of the pose sequence, which are then added to the noisy latents.\nWe replace the original input video frames with random noise during inference, while the other inputs stay the same. We propose a novel HJB-equation-based face optimization to enhance ID consistency and eliminate reliance on third-party post-processing tools. It integrates the solution process of the HJB equation into the denoising, allowing optimal gradient direction toward high ID consistency as detailed in Sec. 3.2."}, {"title": "3.1. ID-preserving During Training", "content": "Global Content-aware Face Encoder. Our goal is to animate the reference image under the guidance of the pose sequence while preserving the ID of the reference image. Directly feeding face embeddings into the U-Net can enrich the diffusion model with face-related information, but lacks awareness of the global context (layout and background) in the reference image before being injected into the U-Net. As a result, ID-irrelevant elements in the reference image bring noise to face modeling, degrading the overall quality of animations. To address this, we propose a Global Content-Aware Face Encoder, in which the face embeddings go through multiple cross-attention blocks to interact with the reference image embeddings as shown in Fig. 2.\nDistribution-aware ID Adapter. The outputs of the Face Encoder are then fed to our ID Adapter for further alignment to avoid the distortion of spatial features occurring when directly incorporating image-domain ID-preserving methods [14, 23, 38, 46] into video diffusion model. Feature distortion describes the misalignment between face embeddings and spatial diffusion latents, caused by distribution shifts when temporal layers are added at each denoising step. Image-domain ID-preserving methods rely heavily on a stable spatial distribution of diffusion latents, but temporal layers often alter this distribution, leading to instability in ID preservation. Such distortion causes a conflict between maintaining high video fidelity and preserving ID consistency. Thus, animated videos often suffer from noticeable blurring effects and can even lose background details. The Distribution-aware ID Adapter modifies each spatial layer of the U-Net, as shown in Fig. 2 (b). Before each temporal modeling, our ID Adapter aligns refined face embeddings with diffusion latents based on their feature distributions, effectively avoiding feature distortion.\nConcretely, following the standard operation of spatial layers in the diffusion model, we first apply spatial self-attention on latents \\(z_i\\). The latents of the U-Net perform cross-attention with image embeddings \\(emb_{img}\\) and refined face embeddings \\(emb_{face}\\), respectively:\n\\begin{equation}\n\\begin{split}\nz_i &= \\mathcal{SAttn}(z_i), \\\\\nz_{img} &= \\mathcal{CAttn}(z_i, emb_{img}), \\\\\nz_{face} &= \\mathcal{CAttn}(z_i, emb_{face}),\n\\end{split}\n\\end{equation}\nwhere \\(\\mathcal{SAttn}()\\) and \\(\\mathcal{CAttn}()\\) refer to self-attention and cross-attention operations. To align \\(z_{img}\\) and \\(z_{face}\\), we enforce \\(z_{img}-\\mu_{img}\\over{\\sigma_{img}}\\) = \\(z_{face}-\\mu_{face}\\over{\\sigma_{face}}\\, where \\(\\mu_{img/face}\\) and \\(\\sigma_{img/face}\\) refer to the mean and standard deviation of \\(z_{img/face}\\), respectively. If the equation above holds, the feature distributions on both sides are basically in the same domain. Thus, the aligned \\(z^{face}\\) is element-wise added to \\(z_{img}\\) for maintaining ID consistency:\n\\begin{equation}\nz_i = z^{face} + z_{img} = z_{face}  \\times \\frac{\\sigma_{img}}{\\sigma_{face}} + \\mu_{img},\n\\end{equation}\nThe outputs of our ID Adapter \\(z_i\\) are further fed to temporal layers for temporal modeling. When spatial distribution is altered by temporal layers, the aligned \\(z^{face}\\) remains in the same domain as \\(z_{img}\\), enabling the original \\(z^{face}\\) to reduce reliance on the unstable spatial distribution. Thus, subsequent temporal modeling does not impede the injection of ID information into the U-Net."}, {"title": "3.2. ID-preserving During Inference", "content": "To improve ID consistency, the latest animation works [34, 64] use a third-party face-swapping tool FaceFusion [15] for post-processing faces. However, animations suffer from overall quality degradation due to excessive reliance on post-processing tools. The reason is that post-processing tools can disrupt the original pixel distribution, as faces generated by third-party tools are clearly not aligned with the domain of original animations. To address this issue, inspired by the HJB equation [2, 5, 35], we propose the HJB Equation-based Face Optimization. The HJB equation guides optimal variable selection at each moment in a dynamic system to maximize the cumulative reward. In our setting, this reward refers to ID consistency, which we aim to enhance by integrating the HJB equation with the diffusion denoising process. The variable refers to the predicted sample by the diffusion model at each denoising iteration.\nWe first introduce the process of our face optimization and then demonstrate its rationale.\nIn particular, we optimize the predicted sample \\(x_{pred}\\) by minimizing the face similarity distance between \\(x_{pred}\\) and the reference before employing denoising (EDM [26]) at each step. The details are in the Algorithm 2, following the structure of the Algorithm 2 in the EDM paper [26]."}, {"title": "3.3. Training", "content": "As illustrated in Fig. 2, we use the reconstruction loss to train our model, with trainable components including a U-Net, a FaceEncoder, and a PoseNet. We introduce face masks \\(M\\), extracted by ArcFace [7] from the input video frames to enhance the modeling of face regions:\n\\begin{equation}\n\\mathcal{L} = \\mathbb{E}(\\| (z_{gt} - z_{\\varepsilon}) \\odot (1 + M)\\|^2),\n\\end{equation}\nwhere \\(z_{gt}\\) and \\(z_{\\varepsilon}\\) refer to diffusion latents and denoised latents in Fig. 2, respectively."}, {"title": "4. Experiments", "content": "4.1. Implementation Details\nSince previous works do not open-source their training datasets, we collect 3K videos (60-90 seconds long) from the internet to train our model. We utilize DWPose [60] and Arcface [7] to extract skeleton poses and face embeddings/masks. Following previous works [22, 47, 50, 57, 66], we evaluate our model on TikTok dataset [25]. We conduct"}, {"title": "4.2. Comparison with State-of-the-Art Methods", "content": "Quantitative results. We compare with recent human image animation models, including GAN-based models (MRAA [39]) and diffusion-based models (DisCo [47], AnimateAnyone [22], MagicAnimate [57], Champ [66], Unianimate [50], MimicMotion [64], ControlNeXt [34]). Based on previous studies that assess quantitative results using the self-driven and reconstruction approach, we perform quantitative comparisons with the above competitors on the TikTok dataset [25] and Unseen100, comprising complex motion and appearance information. Notably, all competitors are trained on our dataset before evaluating on Unseen100 to ensure a fair comparison.\nQualitative Results. The qualitative results are shown in Fig. 3. Notably, qualitative results in the paper are in the cross-ID setting [66]. Disco [47], MagicAnimate [57], AnimateAnyone [22], and Champ [66] exhibit face/body distortion and clothing changes, while Unianimate [50] accurately modifies the reference motion, and MimicMotion [64] and ControlNeXt [34] effectively preserve clothing details. However, all competitors struggle to maintain reference identities. In contrast, our StableAnimator accurately animates images based on the given pose sequences while preserving reference identities, highlighting the superiority of our model in identity retention and in generating precise, vivid animations."}, {"title": "4.3. Ablation Study", "content": "ID Consistency Preservation. We conduct an ablation study to demonstrate the contributions of core components in StableAnimator, as shown in Table 2 and Fig. 4. Notably, all quantitative ablation studies are on the Unseen100 dataset. We can see that removing the core components significantly degrades performance, particularly in face-related regions (CSIM), highlighting that our components significantly enhance both video fidelity and single-frame quality while preserving high ID consistency.\nWe further conduct an ablation study regarding current face enhancement approaches, as shown in Table 3 and Fig. 5. We replace our components with the commonly used IP-Adapter and FaceFusion. By analyzing the results, we can gain the following observations: (1) IP-Adapter can improve the ID consistency, while the video fidelity and single-frame quality dramatically degrade. The plausible reason is that directly inserting the IP-Adapter hinders its ability to adapt to spatial representation distribution variations during temporal modeling, thereby deteriorating the capacity of the video diffusion model. (2) The third-party post-processing face-swapping tool FaceFusion refines the face quality but relatively degrades the video fidelity. The underlying reason is that the third-party post-processing operates in a different domain from the diffusion model, leading to a loss of semantic details and disrupting video fidelity. (3) StableAnimator can significantly refine the face quality while maintaining high video fidelity since our model remains in the same domain as the video diffusion model due to the distribution-aware end-to-end pipeline.\nFeature Alignment. We conduct a comparison between"}, {"title": "4.4. Applications and User Study", "content": "Long Animation. We conduct a qualitative comparison between our StableAnimator and current animation models in long animation generation. Inspired by MimicMotion [64], we follow the same pipeline for synthesizing long videos. The results are shown in Sec. A.4 of the Supp. Each pose sequence contains over 300 frames with complex motion, while the references include intricate details of appearances and backgrounds. The results show that competitors suffer from blurry noises and face distortion. By contrast, our model can effectively handle long human image animation in high fidelity while preserving identities.\nMulti-Person Animation. We experiment on multi-person"}, {"title": "5. Conclusion", "content": "In this paper, we proposed StableAnimator, a video diffusion model with dedicated modules for training and inference to generate high-quality, ID-preserving human image animations. StableAnimator first used off-the-shelf models to gain image and face embeddings. To capture the global context of the reference, StableAnimator introduced a Face Encoder to refine face embeddings. StableAnimator further designed an ID-Adapter, which applied alignment to mitigate the interference from temporal modeling, enabling seamless face embedding integration without video fidelity loss. During inference, to further enhance face quality, StableAnimator incorporated the HJB equation alongside diffusion denoising for face optimization. It ran in parallel with denoising, creating an end-to-end pipeline that eliminates the need for third-party face-swapping tools. Experimental results across various datasets demonstrated the superiority of our model in producing high-quality ID-preserving human animations."}, {"title": "A. Supplementary Material", "content": "A.1. Evaluation Metrics\nFollowing previous human image animation evaluation settings, we implement numerous quantitative evaluation metrics, including L1, PSNR, SSIM, LPIPS, FVD, and CSIM, to compare our StableAnimator with current state-of-the-art animation models. The details of the above metrics are described as follows:\n(1) L1 refers to the average absolute difference between the corresponding pixel values of two images. It measures the typical magnitude of prediction errors without considering their direction, making it a valuable tool for quantifying the extent of discrepancies.\n(2) PSNR measures the ratio between the maximum possible power of a signal (in this case, the original image) and the power of corrupting noise that affects the fidelity of its representation. PSNR is expressed in decibels (dB), with higher values indicating better quality.\n(3) SSIM refers to the similarity between two images based on their luminance, contrast, and structural information.\n(4) LPIPS measures the similarity between images by analyzing the feature representations of their patches, reflecting human visual perception effectively.\n(5) FVD evaluates the disparity between the feature distributions of real and generated videos, considering both spatial and temporal dimensions. FVD is often used to measure the video fidelity.\n(6) CSIM refers to the cosine similarity between the facial embeddings of two face images. The facial embeddings are extracted by ArcFace.\n\nA.2. Preliminaries\nThe diffusion model includes a forward diffusion process and a reverse denoising process. In the forward process, the Gaussian noise is progressively added to the data sample \\(x_0\\) ~ \\(\\mathbb{P}_{data}\\) from the particular data distribution \\(\\mathbb{P}_{data}\\):\n\\begin{equation}\nq(x_t|x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{\\alpha_t}x_{t-1}, (1 - \\alpha_t)\\mathbb{I}).\n\\end{equation}\nThe data sample \\(x\\) is ultimately converted into Gaussian noise \\(x_T\\) ~ \\(\\mathcal{N}(0, 1)\\) after T diffusion forward steps. \\(\\alpha_t\\) is a constant noise schedule. In the reverse process, the diffusion model \\(\\varepsilon_{\\theta}(x, t)\\) tends to recover \\(x_0\\) from \\(x_T\\) by predicting the noise \\(\\varepsilon\\) based on the current sample \\(x_t\\) and time step t. The MSE loss is applied to train \\(\\varepsilon(\\cdot)\\):\n\\begin{equation}\n\\mathcal{L} = \\mathbb{E}_{x_0, \\varepsilon, t}(\\|\\varepsilon - \\varepsilon_{\\theta}(x_t, t)\\|^2).\n\\end{equation}\nMoreover, the denoising process can be regarded as a continuous process (reverse-SDE):\n\\begin{equation}\ndX_t = [f(X_t, t) - g^2(X_t, t)\\nabla \\log \\mathbb{P}(X_t, t)]dt + g(X_t, t)dW_t,\n\\end{equation}\nwhere \\(W_t\\) and \\(\\nabla \\log \\mathbb{P}(X_t, t)\\) refer to the standard Brownian motion and score function. \\(f(X_t, t)\\) and \\(g(X_t, t)\\) are\n\nA.3. Details of Testing Dataset\nWe select 100 unseen videos (10-20 seconds long) from the internet to construct the testing dataset Unseen100. Some examples are shown in Fig. 7. The first row refers to five frames of a video, while the following rows represent individual frames of different videos. The sources of videos come from numerous social media platforms, including YouTube, TikTok, and BiliBili. These videos showcase individuals across ethnicities, genders, portrayed in full-body, half-body, and close-up shots against varied indoor and outdoor settings. In contrast to existing open-source testing datasets (TikTok dataset), our Unseen100 contains relatively complicated motion information and intricate protagonist appearances. Moreover, positions and facial expressions in some Unseen100 videos dynamically change, such as shaking heads, making it more challenging to maintain identity consistency.\n\nA.4. Long Animation\nWe conduct several comparison experiments of our StableAnimator and SOTA human image animation models, as shown in Fig. 8, Fig. 9, and Fig. 10. Each video contains more than 300 frames, featuring complex appearances of the protagonists, complicated motion sequences, and intricate background information. The results highlight the superiority of our StableAnimator in generating long animations while competing methods experience dramatic distortion of human bodies and identities."}, {"title": "Algorithm 2 HJB Equation-based Face Optimization (\\(\\tau(t) = t\\) and \\(s(t) = 1\\))", "content": "Input: A diffusion model \\(D_{\\theta}(x; \\sigma)\\), Timesteps \\(t_i\\)\u2208{0,...,N}, Pre-defined factors \\(\\gamma_i\\)\u2208{0,...,N\u22121}, A reference image y\nSample \\(x_0\\) ~ \\(\\mathcal{N}(0, t\\)\nFor i \u2208 {0, . . ., N \u2013 1} do\n\\(\\gamma_i\\) = 0\nif \\(t_i\\)\u2208 [\\(St_{min}\\), \\(St_{max}\\)]:\n\\(\\gamma_i\\) = min (\\(S_{churn}\\), \\(\\sqrt{2}\u22121\\))\nSample \\(\\epsilon_i\\) ~ \\(\\mathcal{N}(0, \\mathbb{S}_{noise} \\mathbb{I}\\)\n\\(t_i\\) = \\(t_i\\) + \\(\\gamma_i t_i\\)\n\\(x_i\\) = \\(x_i\\) + \\(\\sqrt{t_i^2 - t_i^2}\\)\n\\(x_{pred}\\) = \\(D_{\\theta}(x_i; t_i)\\)\n\\(x_{op}\\) = \\(x_{pred}\\).clone().detach()\nop = Adam([\\(x_{op}\\)], \\(\\eta\\))\n\\(x_{op}\\).requires_grad = True\nFor k \u2208 {1,2,..., 10} do\n\\(f_{pred}\\) = Decoder(\\(x_{op}\\))\nloss = (1 - Cos(Arc(\\(f_{pred}\\)),Arc(y))).abs().mean()\nop.zero_grad()\nloss.backward(retain_graph=True)\nop.step()\n\\(x_{pred}\\) = \\(x_{op}\\)\n\\(d_i\\) = (\\(x_i\\) - \\(x_{pred}\\))/\\(t_i\\)\n\\(x_{i+1}\\) = \\(x_i\\) + (\\(t_{i+1}\\) - \\(t_i\\))\\(d_i\\)\nif \\(t_{i+1}\\) \\(\\neq\\) 0:\n\\(d^{\\prime}\\) = (\\(x_{i+1}\\) - \\(D_{\\theta}(x_{i+1}; t_{i+1})\\))/\\(t_{i+1}\\)\n\\(x_{i+1}\\) = \\(x_i\\) + (\\(t_{i+1}\\) - \\(t_i\\)) (\\(d_i\\) + \\(d^{\\prime}\\))\nreturn \\(x_N\\)"}, {"title": "A.5. Multiple Person Animation", "content": "To demonstrate the robustness of our StableAnimator, we experiment on a particular video involving multiple protagonists, as shown in Fig. 11. We can see that our StableAnimator is also capable of handling multiple-person animations while preserving the original identity and achieving high video fidelity."}, {"title": "A.6. Optimization Details", "content": "We present a more detailed HJB Equation-based Face Optimization in Algorithm 2. Notably, the basic structure of our algorithm closely resembles Algorithm 2 in the EDM paper. In the main paper, \\(\\gamma_1\\) = -\\(r \\cdot (X_1 - x_1)\\) is derived from Eq.4 and Eq.5. In particular, this formula is obtained by calculating the transversality condition of Eq. 4 at the terminal time."}, {"title": "A.7. Additional Face Discussion", "content": "We further conduct a comparison between our StableAnimator and other facial restoration models (GFP-GAN and CodeFormer). The results are shown in Fig. 12. w/o Face refers to the baseline model of our StableAnimator without incorporating any face-related components. It is noticeable that our StableAnimator has the best identity-preserving capability compared with other competitors, demonstrating the superiority of our StableAnimator regarding identity consistency. By contrast, GFP-GAN and CodeFormer suffer from serious facial distortion and over-sharpening. The plausible reason is that w/o Face cannot synthesize the precise facial layout, which in turn undermines the effectiveness of subsequent facial restoration processes. This represents a fundamental limitation of post-processing-based face enhancement strategies."}, {"title": "A.8. Identity-Preserving Loss", "content": "In the image-domain identity-preserving methods, they often incorporate the ArcFace ID loss into the training process, which calculates the cosine similarity between the ArcFace face embeddings of the denoised result and the groundtruth. By contrast, during training, we introduce face masks extracted by Arcface to the conventional reconstruction MSE loss to improve modeling of face-related regions. The reason is that applying the ArcFace ID loss requires employing a VAE Decoder to convert the denoised latents into pixel level. The reason is that applying the ArcFace ID loss requires using a VAE Decoder to convert the denoised"}, {"title": "A.9. Additional Comparison Results", "content": "Fig. 13 and Fig. 14 show additional comparison results. The provided pose sequences encompass complex motion information, and the initial poses of the reference images are two categories: one with the protagonist facing directly toward the camera, and another with the protagonist's profile turned toward the camera. We can observe that our StableAnimator can accurately modify the motion of the reference images and maintain the original identity, while other competitors encounter varying degrees of human body distortion and loss of facial details."}, {"title": "A.10. Animation Results", "content": "We show our animation results in Fig. 15. We can see that our StableAnimator can perform a wide range of human image animation while simultaneously preserving the protagonist's appearance, background, and identity. Fig. 16, Fig. 17, and Fig. 18 show additional animation results generated by our StableAnimator. Each cases contain complex protagonist's appearance and intricate motion information. For example, in the reference image in the fifth row of Fig. 16, the protagonist's closed eyes make it particularly challenging for the human animation model to preserve ID consistency. It is noticeable that our StableAnimator can accurately manipulate motion in the reference image while preserving high-quality identity consistency, even in specific cases involving significant motion variations, such as head shaking and body rotation. Even when the head of the protagonist is continuously shaking and the angle facing the camera is constantly changing during the animation process, StableAnimator can still maintain a high level of identity consistency in the animation results without sacrificing details of the protagonist and the background."}, {"title": "A.11. Additional Ablation Study", "content": "To validate the contribution of our proposed components, We conduct a more comprehensive qualitative ablation study on different diffusion backbones, as shown in Fig. 19. ControlNeXt and MagicAnimate are based on Stable Video Diffusion (SVD) and Stable Diffusion (SD), respectively. We can see that our proposed components can significantly facilitate the performance of different backbone-based models, particularly in the facial regions. Notably, our proposed HJB Equation-based Face Optimization can still enhance the overall quality of animations to some extents, even when the backbone models lack any face-related encoders or adapters. The plausible reason is that our proposed HJB Equation-based Face Optimization can update the diffusion latents based on the face embedding similarity at each denoising step, thereby progressively refining the overall quality of denoised results without introducing any explicit face-related components."}, {"title": "A.12. Limitation and Future Work", "content": "Fig. 20 shows one failure case of our StableAnimator. In the given reference image, the girl's hand covers most of her face. Our StableAnimator struggles to fill in the obscured face regions, thereby degrading the quality of the synthesized face. One potential solution is introducing an additional face-aware inpainting adapter to the diffusion backbone for refining the face quality of given reference images. This part is left as future work."}, {"title": "A.13. Ethical Concern", "content": "Our StableAnimator can animate the given reference image based on the given pose sequence, which can be implemented in various fields, including virtual reality and digital human creation. However, the potential misuse of this model, particularly for creating misleading content on social media platforms, is a concern. To mitigate this, it is essential to use sensitive content detection algorithms."}]}