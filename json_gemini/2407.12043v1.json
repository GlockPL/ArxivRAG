{"title": "The Art of Saying No: Contextual Noncompliance in Language Models", "authors": ["Faeze Brahman", "Sachin Kumar", "Vidhisha Balachandran", "Pradeep Dasigia", "Valentina Pyatkin", "Abhilasha Ravichander", "Sarah Wiegreffe", "Nouha Dziri", "Khyathi Chandu", "Jack Hessel", "Yulia Tsvetkov", "Noah A. Smith", "Yejin Choi", "Hannaneh Hajishirzi"], "abstract": "Chat-based language models are designed to be helpful, yet they should not comply with every user request. While most existing work primarily focuses on refusal of \"unsafe\" queries, we posit that the scope of noncompliance should be broadened. We introduce a comprehensive taxonomy of contextual noncompliance describing when and how models should not comply with user requests. Our taxonomy spans a wide range of categories including incomplete, unsupported, indeterminate, and humanizing requests (in addition to unsafe requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30% of requests. To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.", "sections": [{"title": "1 Introduction", "content": "With language models now serving millions of users via chat interfaces, there is an increasing need for such models not to comply with every user request. The majority of prior work on refusal, or, more generally, noncompliance, has focused on AI safety with methods developed to prevent models from generating content that carries the risk of harm, such as generating offensive language, providing dangerous (mis)information, or violating privacy, among others [5, 29, 24, 106, 10, 61]. To measure model safety along these dimensions, several benchmarks have been introduced [91, 98, 86, 87].\nIn this work, we broaden the scope of noncompliance beyond the prior focus of safety. We consider a wide range of contextual nuances and out-of-scope requests that models ought to be aware of and"}, {"title": "2 A Contextual Noncompliance Taxonomy", "content": "In this work, we aim to broaden the scope of noncompliance beyond its previous focus on safety. To achieve this, we develop a taxonomy of contextual noncompliance for language models, drawing inspiration from previous research in real-world content moderation, user safety and experience [88, 9, 71], and AI safety and risks [103, 113]. We outline the taxonomy in Figure 2 with examples: it comprises five major categories, each of which includes several fine-grained sub-categories. Note that the categories are not exclusive, that is, one request may belong to several categories. While we aim to be comprehensive, not all categories may be relevant for all models. We note this wherever applicable.\nIncomplete Requests. Requests that are not answerable with the provided information. They include:\na) Underspecified: requests that are missing crucial information required to appropriately respond. For example, \"who was the Prime Minister in 1956?\" without specifying the country [62]."}, {"title": "3 CoCoNoT: A Noncompliance Training and Evaluation Resource", "content": "We first describe how to create our dataset, COCONOT (for \u201cContextually, Comply Not\") based on the proposed taxonomy (\u00a72) and then propose an evaluation framework to measure contextual noncompliance (\u00a73.3). Our dataset contains (1) noncompliance queries and (2) a contrast query set that should be complied with. Each group is split into a human-verified evaluation set and a training set (with responses). We use the former to assess (\u00a74) and latter to enhance noncompliance (\u00a75).\n3.1 Collecting Noncompliance Queries and Responses\nWe create a set of queries that should elicit noncompliance either by curating examples from existing datasets or synthetically generating them using GPT models [76, 77]. We then split these queries into an evaluation and training set and generate noncompliance responses for the latter. We apply several filters on both and manually verify the evaluation split to ensure minimal overlap and high quality.\nStep 1: Collecting queries For synthetic query generation, we, the authors, handcraft a seed set of ~10 queries for each subcategory in our taxonomy. Inspired by SELF-INSTRUCT [100], we augment this set by iteratively prompting different LMs to generate new queries. For each subcategory, given a pool of N seed requests, we prompt an LM with instructions to generate a new noncompliance request followed by k demonstrations randomly sampled from the pool. We add the generated output back to the pool (which improves diversity) and repeat this process until a desired number of queries have been generated (prompts and LMs used for each category are described in Appendix A). We obtain all \"underspecified\u201d queries from SituatedQA [108], and part of \u201crequests with safety concerns\u201d from WildChats [111]. With all other queries synthetically generated, we obtain an initial set of 25K queries equally spread across all subcategories 10% of which is used to create the evaluation set.\nStep 2: Generating responses For each query, we use GPT-4 (gpt-4-1106-preview) to generate noncompliant responses. We provide as input the name of the subcategory, its definition, and the"}, {"title": "3.2 Collecting Contrast Sets for Measuring and Mitigating Exaggerated Noncompliance", "content": "Prior work has shown that models trained for noncompliance can overfit to refuse benign queries that superficially resemble those requiring noncompliance [86]. We thus create a contrastive version of our data [75, 30, 84] to study this exaggerated behavior (\u00a74) and potentially mitigate it (\u00a75).\nStep 1: Generating contrastive prompts Not all the categories in our taxonomy have the potential of having contrastive counterparts. We thus create such queries only for \"incomplete requests\" (specifically, false presuppositions and underspecified queries), \u201cunsupported requests\u201d (specifically, modality limitations), and \"requests with safety concerns.\" We follow a similar SELF-INSTRUCT procedure iteratively asking an LM to generate contrastive queries with an instruction and optional demonstrations of either transforming noncompliance queries to contrastive queries or generating both together. This is followed by quality filtering and manual verification done by authors. For underspecified contrastive requests, we leverage the construction mechanism of SituatedQA detailed in Appendix A.2. We hold 10% of this data (379 queries) to create our contrast evaluation set.\nStep 2: Contrastive preference data For each query in the train set, we generate a compliant and a noncompliant response to create preference data where compliance is preferred over noncompliance.5 We generate the former using GPT-4 and the latter using a combination of open-source models that have varying levels of overrefusal tendencies [86]. Specifically, we generate outputs from all Llama 2 models [96] and all Tulu 2 models [40] and use various heuristics to filter compliant responses (see Appendix A.2). For all queries for which at least one model response is noncompliance, we create a preference instance with GPT-4 response as the preferred response and one of the randomly sampled model responses as the dispreferred one. This results 927 training instances called COCONOT-PREF."}, {"title": "3.3 Evaluating Contextual Noncompliance", "content": "Noncompliant responses for queries in COCONOT can take many forms across categories from completely refusing to answer, to asking clarification questions, to providing approximate answers which complicates surface-level automated evaluation. We, therefore, opt for LM-based evaluation specifically using GPT-3.5.6 We report compliance rate as our final metric, i.e., the percentage"}, {"title": "4 Benchmarking models with COCONOT", "content": "In this section, we aim to answer RQ1-how well state-of-the-art language models perform when presented with noncompliance requests in CoCONOT?\nModels to Test. We evaluate a variety of proprietary and open-source model families and sizes trained with different datasets and objectives. These include GPT models (gpt-40, gpt-4, gpt-4-1106-preview, gpt-3.5-turbo ) [76, 77], Claude 3 (sonnet) [4], Llama-2 Chat (7B, 13B, 70B) [95], Llama-3 Instruct, Vicuna (13B) [19], Tulu-2 (SFT and DPO models; 7B, 13B, 70B) [40], Gemma (7B Instruct) [93], Mistral (7B Instruct V0.2) [43], Mixtral (8x22B Instruct) [44].\nInput Format. We evaluate using two input formats: one where the model receives only the prompt from our evaluation set, and another where we provide an additional system prompt instructing the model not to comply with requests defined in our taxonomy (exact prompt is provided in Figure 6).\n4.1 Results and Findings\nWe report aggregated compliance rates for each category in our taxonomy as well as our contrast set in Table 2 (see Appendix C for all models we evaluate on). We detail our main findings:"}, {"title": "5 Training Strategies To Improve Noncompliance with COCONOT", "content": "Our evaluation showed significant compliance rates of models across several categories. In this section, we aim to answer RQ2-How can we train models towards closing this gap?\nBaselines. We conduct all training experiments using Llama-2 7B as our base LM (limited by our compute budget). We compare our trained models with models trained on top of Llama-2 via SFT on Tulu2Mix (T2M), a general purpose instruction tuning dataset containing 300K instances [40]. Tulu2Mix is sourced from several public sources and contains some unmarked refusal-related"}, {"title": "5.1 Results and Findings", "content": "We report our main results in Table 3 and discuss our findings below (detailed results in Appendix D).\nSFT from scratch shows mixed results. We find that including our training set in Tulu2Mix and finetuning Llama-2 results in significantly improved noncompliance rates over baselines with minimal decline in general capabilities. However, on both contrast sets (i.\u0435., XSTB and our COCONOT-Contrast), we see a decline in compliance suggesting the model overfits to refusing benign requests.\nFurthermore, supervised finetuning of a pretrained model is computationally inefficient and requires access to the original instruction-following data, which may not always be available.\nLoRa finetuning finds a good balance. Continued finetuning of all parameters of Tulu2 models on COCONOT results in a significant reduction in general capabilities. Including a matched-sized subset of Tulu2Mix at this stage helps slightly but is unable to recover the original performance. On the other hand, finetuning with LoRA not only significantly improves noncompliance across the board but also maintains general task performance on top of both Tulu-2 and Tulu-2-no-refusal. This finding is in line with recent work which shows that LoRa finetuning learns less but forgets less [11]. The improvements in noncompliance is not as drastic as training from scratch, however, it also performs much better on contrastive test sets. Finally, inspired by Huang et al. [38], we merge the adapter learned by training on Tulu-2-no-refusal with Tulu-2 and found that to perform better than both LoRA tuned models even outperforming GPT-4 compliance rates on CoCoNot.\nIt is important to note that while GPT-4 performs well on safety metrics, it still exhibits limitations when tested on COCONOT.\nPreference tuning on contrast data reduces overrefusals. Finally, DPO on our contrast training set which finetunes the model to prefer compliances for benign queries helps improve compliance rates on the contrast sets while maintaining other metrics, resulting in overall superior performance.\nImpact of training data size. We investigate the impact of training data sizes on the noncompliance behavior of the resulting model. For this experiment, we continue LoRA finetuning of Tulu 2 7B model using using 10%, 25%, 50%, 75%, and 100% of the COCONOT training data (11,477 instances). Results are shown in Figure 4. We observe that training on more data almost consistently improves noncompliance for some categories but not all including incomplete and unsupported requests, and requests with safety concerns (Figure 4a). However, this comes with increased compliance rate on the contrast set which is not ideal (Figure 4b)."}, {"title": "6 Related Work", "content": "LM Safety Language models have been extensively studied for their propensity to learn and amplify societal biases and pose safety risks to users [103]. This category has seen the most attention when studying model noncompliance. Prior works have proposed taxonomies [103, 113, 54, 57, 26, 85], evaluation frameworks [110, 101, 67, 13, 14, 39, 15, 68, 61] and training methodologies [112, 10, 58] to align models to refuse unsafe requests. Based on this line of work, most recent models have incorporate safety training in their pipelines [12, 8, 94]. While in our work, we include many of the categories and findings from LM safety research, our aim is to expand the understanding of model noncompliance beyond safety to include other aspects which can impact user experience and trust.\nIncomplete, Unsupported, and Indeterminate Requests. These categories in our taxonomy are inspired from early reading comprehension datasets which introduced notions of unanswerable queries that models should identify [60, 42, 20, 80]. Other follow-up works have looked at ambiguous questions [74, 51, 105], quantifying uncertainty in the face of unspecified or ambiguous user inputs [7], and abstaining from answering model unknowns [28]. A common approach for dealing with such ambiguous or underspecified inputs is to ask clarifying questions in return [81, 69, 78, 109, 56].\nEpistemology and Language Models Orthogonal to our work are prior epistemology-of-LM works that aim to characterize epistemic versus aleatoric uncertainty [35, 49, 36, 2, 52]; and works that measure calibration of language models on benchmark tasks (for which humans achieve high accuracy) [27, 46, 47, 97, 73]. We instead measure LLM self-identification rates for cases where they should exhibit epistemic uncertainty, i.e., cases where an LLM (by our prescription) cannot express a justified belief (in the Platonic sense of \u201cjustified true belief", "particular": "the incomplete, unsupported, indeterminate, and humanizing subsets) is a necessary (but not sufficient) condition for language models to exhibit \u201cepistemic responsibility"}, {"title": "7 Conclusion and Future Work", "content": "In this work, we propose to broaden the scope of noncompliance in chat-based language models to a diverse range of scenarios beyond only safety. We introduce a taxonomy of requests that text-based LMs should selectively not comply with. Based on this taxonomy, we create COCONOT which consists of an evaluation benchmark for testing model noncompliance capabilites and a sythentically generated training data to induce noncompliance. We find that several popular models, both proprietary and open-source, show significant level of compliance on our evaluation set. Our training explorations show that continued finetuning with parameter efficient methods (LoRA) can be helpful in improving performance while maintaining general capabilities of the model. This work opens several avenues for future work. For example, how can we utilize a model's own epistemic awareness? Are our training methodologies robust to jailbreaking tactics? Can LoRA continued finetuning be a viable approach for addressing catastrophic forgetting in general? Finally, we believe that much future research remains to be done in identifying how to create improved experiences for users interacting with language models, and how to increase user trust.\nLimitations\nCOCONOT is limited by a few factors. First, the entire dataset, except for a specific subset, is generated synthetically-both prompts and responses and may be noisy, although we manually validate the evaluation sets. Furthermore, while our taxonomy provides a wide coverage of categories and subcategories which informs our dataset, the scope of requests within each subcategory is extremely large and our dataset may not have covered all of it. Lastly, We also note that while we provide prescriptive norms of noncompliance for our benchmark, as we discuss in \u00a72, not every subcategory demands noncompliance for every language model. Hence, performing poorly on certain categories such as humanizing requests does not necessarily demerit the model.\nEthical Considerations\nOur training set, both prompts and responses, are generated synthetically. Although we take measures to filter out prompts with compliant generated responses, the heuristics are not perfect, and it is conceivable that the training sets might contain some harmful requests with compliant responses. However, this proportion is likely to be small given that training with this dataset improves downstream noncompliance behavior. We also note that while training for noncompliance can mitigate many types of risks, it does not guarantee that the models are 100% safe to use and deploying such systems to real users needs additional precautions to ensure user safety. Further, since several requests in our dataset concern model safety and we show that many existing LMs show poor compliance rates on many of the categories, malicious players may use such responses to build models to instigate harm. To prevent misuse, we plan to gate the COCONOT release behind a content warning and terms agreement limiting usage to research and model noncompliance improvement."}, {"title": "A COCONOT: Dataset Creation Details", "content": "A.1 Noncompliance Data Generation\nWe list the data sources for categories with existing datasets and models used for synthetic data generation in Table 7 and the prompt used to generate synthetic examples in Figure 7. We provide the seed set of prompts we use for augmentation in the supplementary material."}, {"title": "A.2 Contrastive Data Generation", "content": "Incomplete requests For underspecified requests, we leverage crowdworker edits from the SituatedQA dataset [108], which disambiguate the earlier geographically-ambiguous question by specifying a location. For false presuppositions, we construct contrastive prompt templates where the assumption in the question is true, as shown in Table 9\nUnsupported requests (modality limitations) Examples of the instructions used to prompt GPT-4 for contrastive examples, as well as some generated contrastive queries are given in Figure 9 and 10. We validate that GPT-4 itself does not refuse to answer any of these queries. Additional validation by the authors finds that this process is largely successful- out of 102 inspected generated queries, 93 are valid contrastive queries that should not be refused as modality limitations, despite mentioning the same themes or topics of the original query. Of the remaining 9, 8 of those involve translation (GPT-4 supports translation, and so fails to follow the instruction to only generate requests involving English). However, these are generally quite easy to filter out from the resulting dataset.\nRequests with safety concerns To encourage the construction of meaningful harmless queries that maintain topical overlap with the harmful ones, we generate the contrastive examples by prompting GPT-4 to generate both a harmful query and its harmless counterpart simultaneously. We validate that GPT-4 itself does not refuse to answer any of the contrastive queries. The instruction used to prompt GPT-4, as well as some generated constrative examples, are given in Figure 11 and Table 8, respectively.\nContrastive preference data For each query in the train set, we generate a compliant and a noncompliant response to create a preference data where compliance is preferred over noncompliance. We generate the former using GPT-4 and latter using a combination of open-source models that have varying levels of overrefusal tendencies [86]. Specifically, we generate outputs from all Llama 2 models [96] and all Tulu 2 models [40] and use different heuristics to filter compliant responses. Specifically, we use the function described in figure 8 to mark noncompliance responses. For all queries for which at least one model response is noncompliance, we create a preference instance with GPT-4 response as the preferred response and one of the randomly sampled model responses as the dispreferred one. This results 927 training instances called CoCONOT-PREF."}, {"title": "B Evaluation Details", "content": "B.1 Human Agreement with GPT Judgment of Compliance\nTo ensure the accuracy of GPT-based evaluation, we manually verify 300 randomly selected model outputs generated for prompts in COCONOT using two models, GPT-4 (one of the largest models) and Llama-2-Chat 7B (one of the smallest we evaluate). Each sample is marked by three annotators, the authors, with a binary label indicating if the GPT evaluation is correct according to the guidelines. We find that 93% of the outputs are verified as accurate by majority of annotators with 63% Fleiss Kappa IAA. The 63% Fleiss kappa IAA shows significant agreement between annotators regarding the accuracy (93%) of GPT-based evaluation.\nB.2 Description of Evaluation Suite\nB.2.1 General Capabilities\nWe adopt most of the evaluation suite from Open-Instruct codebase [99, 40] for evaluating the general capabilities of safety-trained models. In addition, we evaluate models with AlpacaEval V2 with length control that was not previously included in Open-Instruct.\nMMLU The Massive Multitask Language Understanding task [34] consists of 57 diverse multiple-choice tasks drawn from areas in the hard sciences, humanities, social sciences. The test set consists of 14,079 questions. We use the Open-Instruct implementation of this evaluation, and the reported metric is average accuracy.\nGSM GSM8k [22] consists of 8.5k grade school math word problems. We use the Open-Instruct framework, which conducts this evaluation in chain-of-thought form, with eight few-shot examples. The reported metric is average accuracy.\nBBH BIG-Bench Hard Suzgun et al. [92] is a collection of 23 challenging multiple choice or exact match tasks from among the BIG-Bench evaluations Srivastava et al. [90], on which previous LM performance did not exceed average human performance. The benchmark contains 6,511 evaluation items, and we use the Open-Instruct framework, which conducts the evaluation in chain-of-thought form, using the provided prompts which contain three few-shot examples. The reported metric is average accuracy.\nTydiQA TydiQA [21] is a question-answering dataset spanning 11 typologically diverse languages, with a test set consisting of 18,751 QA pairs. We use the Open-Instruct implementation, which conducts this evaluation in a one-shot setting in which the gold passage is provided along with the question. The reported metric is F1.\nCodex-Eval We use the Open-Instruct evaluation, which uses the HumanEval set from Chen et al. [17], consisting of 164 programming problems prompting models to complete a Python function given a docstring. The reported metric is pass@10, sampling with a temperature of 0.8.\nTruthfulQA TruthfulQA [65] consists of 818 questions crafted such that humans may answer incorrectly due to common misconceptions. We use Open-Instruct, using these questions to prompt models to generate answers, providing six few-shot examples. Reported metrics are percentage of outputs classified as Truthful and percent of outputs classified as Informative, as judged by two GPT-based classifiers.\nAlpacaEval V1 AlpacaEval [64] V1 consists of 805 instruction prompts and uses GPT-4 to compare model responses against responses from Davinci-003. We use Open-Instruct, and the reported metric is percentage of model responses preferred by GPT-4 over those of Davinci-003.\nB.2.2 Safety Capabilities\nToxiGen ToxiGen [32] consists of toxic and benign statements about 13 minority groups. We use Open-Instruct, which selects 500 toxic prompts for each group, and instructs models to complete the prompt. The reported metric is percentage of outputs classified as toxic by the RoBERTa-Large toxicity classifier fine-tuned by Hartvigsen et al. [32]."}, {"title": "C Complete Baseline Results", "content": "We provide the complete baseline compliance rates for COCONOT on all existing models we compare with in Tables 10, 11, 12, 13, and 14."}, {"title": "D Training Results", "content": "D.1 Training Details\nFollowed the experimental setup and hyperparameters used in Ivison et al. [40], for supervised fine-tuning (and continue full fine-tuning), we used the following:\n\u2022 Precision: BFloat16\n\u2022 Epochs: 2 (1 for continue FT)\n\u2022 Weight Decay: 0\n\u2022 Warmup ratio: 0.03\n\u2022 Learning rate: 2e-5\n\u2022 Max Seq. length: 2,048\n\u2022 Effective batch size: 128\nFor LoRA training, we used the following:\n\u2022 Precision: BFloat16\n\u2022 Epochs: 1\n\u2022 Weight Decay: 0\n\u2022 Warmup ratio: 0.03\n\u2022 Learning rate: le-5\n\u2022 Learnin rate scheduler: cosine\n\u2022 Max Seq. length: 2,048\n\u2022 Effective batch size: 128\n\u2022 Lora rank: 64\n\u2022 Lora alpha: 16\n\u2022 Lora dropout: 0.1\nFor DPO, we used the following:\n\u2022 Precision: BFloat16\n\u2022 Epochs: 1\n\u2022 Weight Decay: 0\n\u2022 Warmup ratio: 0.1\n\u2022 Learning rate: 5e-7\n\u2022 Max Seq. length: 2,048\n\u2022 Effective batch size: 128\nWe conduct all our experiments on NVIDIA A100-SXM4-80GB machines.\nD.2 Complete Training Results on all Benchmarks\nHere, we present full results of of our finetuned models on all benchmarks including those evaluating general capabilities (Table 15) and safety (Table 16). Conclusions made in the main paper also holds true here, i.e., continued finetuning of all parameters of Tulu models results in a significant reduction in general capabilities and lead to exaggerated safety behaviors on the contrast sets. Including a subset of Tulu2Mix at this stage helps slightly but is unable to recover the original general performance."}]}