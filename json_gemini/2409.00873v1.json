{"title": "Equitable Skin Disease Prediction Using Transfer Learning and Domain Adaptation", "authors": ["Sajib Acharjee Dip", "Kazi Hasan Ibn Arif", "Uddip Acharjee Shuvo", "Ishtiaque Ahmed Khan", "Na Meng"], "abstract": "In the realm of dermatology, the complexity of diagnosing skin conditions manually necessitates the expertise of dermatologists. Accurate identification of various skin ailments, ranging from cancer to inflammatory diseases, is paramount. However, existing artificial intelligence (AI) models in dermatology face challenges, particularly in accurately diagnosing diseases across diverse skin tones, with a notable performance gap in darker skin. Additionally, the scarcity of publicly available, unbiased datasets hampers the development of inclusive AI diagnostic tools. To tackle the challenges in accurately predicting skin conditions across diverse skin tones, we employ a transfer-learning approach that capitalizes on the rich, transferable knowledge from various image domains. Our method integrates multiple pre-trained models from a wide range of sources, including general and specific medical images, to improve the robustness and inclusiveness of the skin condition predictions. We rigorously evaluated the effectiveness of these models using the Diverse Dermatology Images (DDI) dataset, which uniquely encompasses both underrepresented and common skin tones, making it an ideal benchmark for assessing our approach. Among all methods, Med-ViT emerged as the top performer due to its comprehensive feature representation learned from diverse image sources. To further enhance performance, we conducted domain adaptation using additional skin image datasets such as HAM10000. This adaptation significantly improved model performance across all models.", "sections": [{"title": "Introduction", "content": "Skin diseases encompass a wide spectrum of conditions, and some pose significant health risks if not identified and treated promptly. The diagnosis of these diseases is predominantly a manual process performed by dermatologists through visual inspection and clinical judgment. As the prevalence of skin diseases increases worldwide, the need for an efficient and accurate diagnosis becomes increasingly pressing. Al has emerged as a promising solution to assist in the triage and preliminary identification of skin conditions, potentially leading to early intervention and better pa-"}, {"title": "Related Work", "content": "Prior studies have validated the efficacy of machine learning (ML) and deep learning (DL) in the classification and diagnosis of dermatological conditions, achieving levels of performance comparable to or exceeding that of board-certified dermatologists in cases of skin cancer , eczema , psoriasis , and onychomycosis . In particular, Emam et al. reported an AUC of up to 0.95 for the discontinuation of biological treatments using a variety of models, including deep learning techniques. Similarly, Wang et al. and Roffman et al. focused on predicting non-melanoma skin cancer with AUCs of 0.89 and 0.81, respectively. The work of Khozeimeh et al. presented a distinction in the response to wart treatment methods between cryotherapy and immunotherapy, with respective accuracies of 80% and 98%. Furthermore, Tan et al. investigated the complexity of reconstructive surgery after excision of periocular basal cell carcinoma, applying Bayesian and other methodologies to achieve AUC values greater than 0.83. Each of these investigations used data sets that encompass 7 to 20 clinically relevant patient characteristics, underscoring the importance of comprehensive data for model training and validation. Egorov et al. evaluated three advanced models: ModelDerm , DeepDerm , and HAM10000 , which showed commendable results in the datasets on which they were trained, but experienced a decrease in performance when applied to the DDI . Therefore, we can say, existing dermatological diagnostic algorithms lacks robustness and generalizability. Our research seeks to address this gap."}, {"title": "Methods and Materials", "content": "In our methodological framework, the collection of data sets is significant, as it underpins the training of our AI model. The primary dataset utilized in this study is an assembly of skin disease images with a focus on inclusive skin tone representation. These images were meticulously curated from pathology reports archived at the Stanford Clinic over a decade from 2010 to 2020. To ensure the reliability and clinical applicability of the dataset, each image has been annotated by a duo of board-certified dermatologists, providing"}, {"title": "Dataset Collection", "content": "a substantial foundation for the subsequent AI-driven analysis. The data set embraces the Fitzpatrick Skin Type (FST) classification system, a globally recognized schema for categorizing human skin tones. This stratification allows for a detailed and nuanced approach to the representation of diverse skin types within our dataset. In total, the data set comprises 656 images depicting conditions of 570 unique patients. These images are distributed across the FST spectrum as follows: 208 images from FST categories I-II, including 159 benign and 49 malignant cases; 241 images from FST III-IV, encompassing 167 benign and 74 malignant cases; and 159 images from FST V-VI, with 159 benign and 48 malignant cases.\nTo augment our primary dataset and enhance the robustness of our transfer learning approach, we have incorporated additional datasets renowned for their extensive collection of images of skin disease. DeepDerm provides a vast repository with 129,450 images, and Ham10000 complements this with an additional 10,015 images. These datasets serve as a foundation for the initial adaptation phase of our pre-trained model, enabling it to acclimate to the domain of dermatological imagery before fine-tuning with the more focused but less voluminous DDI dataset. The strategic amalgamation of these datasets is designed to foster a comprehensive learning environment that allows the extraction of generalizable features, which are then refined to discern subtle nuances across diverse skin types."}, {"title": "Transfer learning", "content": "In our transfer learning approach, we begin with a pre-trained model on retinal images, using it as a foundation to adapt to dermatological tasks with the DeepDerm and HAM10000 datasets. We then finetune the model's weights on the DDI dataset to refine its ability for skin disease classification, ensuring specificity and accuracy in our predictions."}, {"title": "Model Selection", "content": "We select DeepDerm and HAM10000  as open-source pre-trained models trained on skin images for dermatology applications. While selecting models, we focuses on their adaptability for skin cancer classification and reliable accuracy. DeepDerm is trained end-to-end from images and disease labels and performs comparable to board-certified dermatologists in the classification of skin lesions. It shows potential for enhanced diagnosis using deep convolutional neural networks (CNNs) and a dataset of 129,450 clinical images. Furthermore, HAM10000 overcomes the challenge of diversity in dermatoscopic image datasets with 10,015 images, facilitating machine learning research and comparisons with human experts in diagnosing pigmented skin lesions, with more than 50% of the lesions confirmed by pathology.\nIn selecting our model, we focus on the domain-specific RETFound, pre-trained on ImageNet-1k and MEH-MIDAS datasets, as depicted in Figure 1. For benchmarking, we use the generalist"}, {"title": "General Medical Image Pre-trained Model", "content": "There exist many general-purpose Medical Imaging models. These models are commonly used in various downstream tasks through fine-tuning. These models are trained on diverse types of medical images representing different body parts and organs. We choose MedViT as our pre-trained model. It introduces a hybrid CNN-Transformer model that merges the CNN's locality with the vision Transformer's global connectivity. MedViT stands out for its focus on learning smoother decision boundaries to increase resilience against adversarial attacks. This is achieved by augmenting shape information within the high-level feature space. In particular, this model demonstrates high robustness and generalization capabilities while managing to reduce computational complexity. Its performance sets a new benchmark in medical image analysis. For our specific classification task, we adapted and fine-tuned the MedViT model using skin datasets."}, {"title": "General Image Pre-trained Model", "content": "In our exploration of the transfer learning approach, we extend our scope to include general purpose vision models. YOLOv8 is known as a leading contender in this category. The model achieves state-of-the-earth results in various real-time object detection and image segmentation capabilities. First introduced in 2015, YOLO (You Only Look Once) quickly gained acclaim for its exceptional speed and accuracy. We use the latest version of the model. To fit YOLO as an effective classifier model between benign and malignant cancers images, we replace it's final layers with a classification layer and fine tune the whole model with our curated skin dataset."}, {"title": "Data Preprocessing", "content": "In the data processing phase of the study, we meticulously curated the Diverse Dermatology Images (DDI) dataset to ensure a comprehensive and balanced evaluation of all models involved. This dataset, sourced from Stanford Clinic Pathology reports spanning from 2010 to 2020, comprises images labeled by two boards of certified dermatologists, providing a robust foundation for our research.\nOne of the unique characteristics of the DDI dataset is its representation of diverse skin tones, classified according to the Fitzpatrick Skin Type (FST) scheme. We categorized the dataset into three distinct skin tone groups:\n\u2022 Dark Skin Tone (FST I-II): This group encompasses individuals with darker skin tones, represented by FST categories I and II.\n\u2022 Medium Skin Tone (FST III-IV): Individuals with medium skin tones fall under FST categories III and IV.\n\u2022 White Skin Tone (FST V-VI): FST categories V and VI represent individuals with lighter skin tones.\nEach skin tone group contains images labeled with two categories: benign and malignant. These labels are essential for training and evaluating our models' performance in accurately diagnosing skin diseases."}, {"title": "Data Preprocessing", "content": "To ensure a fair and balanced evaluation, we partitioned the dataset into training and testing sets in an 80:20 ratio. We took care to maintain an equitable distribution of benign and malignant labels within both the training and testing subsets. Additionally, we stratified the data over skin tones, ensuring that each set (training and testing) includes samples from all three skin tone categories shown in Table 1. This approach prevents bias and guarantees that our evaluation dataset is representative of the entire spectrum of skin tones encountered in clinical practice.\nFurthermore, as part of our preprocessing pipeline, we resized all images to a standard size of 224 pixels and performed common preprocessing techniques to enhance model performance. These preprocessing steps ensure uniformity and facilitate effective model training and testing.\nBy adhering to these rigorous data processing procedures, we established a robust evaluation framework that enables us to assess the performance of various pretrained models accurately. This approach not only enhances the reliability and reproducibility of our findings but also ensures the inclusivity and fairness of our analysis across diverse skin tone populations."}, {"title": "Domain Adaptation", "content": "As we utilize pre-trained models, it's important to note that these models are originally trained on datasets from different domains. For instance, RETFound is trained on retinal datasets. Another reason for domain adaptation is the presence of a small dataset for fine-tuning. Our diverse skin dataset is relatively small. Training or fine-tuning with such a small dataset poses its own challenges. It might lead to suboptimal results. Alternatively, if the model has large weights, it may overfit, resulting in low validation accuracy. This motivates us to add domain adaptation as a prerequisite step for fine-tuning with the DDI dataset. In the domain adaptation step, we use the HAM10000 image dataset, which consists of 10,015 skin images. Our expectation is that domain adaptation will increase the accuracy compared to fine-tuning with DDI only across all benchmarks."}, {"title": "Fine-tuning", "content": "Fine-tuning involves adapting a pre-trained model to improve its performance on a specific task by training it further on a task-specific dataset. The tasks utilizing the pre-trained model, RETFound as an example are shown in Figure 1. For optimal performance on the DDI dataset, we do fine-tuning, which is both effective and resource efficient. We create two datasets one including HAM10000 samples and another excluding them. Each data set is used to fine-tune our can-"}, {"title": "Hyperparameter Selection", "content": "Selecting appropriate hyperparameters is crucial for optimizing the performance of our fine-tuned models. We experimented with various configurations and settled on the following parameters based on their impact on model convergence and accuracy:\n\u2022 Batch size: We use a batch size of 16, which provides a balance between training speed and model stability.\n\u2022 Base learning rate (blr): The initial learning rate is set to 5 \u00d7 10-3. This rate is chosen to ensure fast convergence without overshooting the minima.\n\u2022 Layer decay: A layer decay of 0.65 is applied to adjust the learning rates of deeper layers, effectively preventing overfitting.\n\u2022 Weight decay: We apply a weight decay of 0.05 to regularize the model and reduce the likelihood of overfitting.\n\u2022 Drop path rate: A drop path rate of 0.2 is utilized to introduce regularization by randomly dropping paths during training. This enhance the model's generalization.\n\u2022 Number of classes: Our models are configured to distinguish between two classes, benign and malignant.\n\u2022 Input size: The input size for our models is set to 224 \u00d7 224 \u00d7 3, aligning with common practice for image-based models to capture sufficient detail while managing computational load.\nThese hyperparameters were fine-tuned through iterative training and validation, leading to optimized performance on both the training and testing datasets."}, {"title": "Evaluation metrics", "content": "In our study, we used accuracy, macro-average F1 score and weighted average F1 score to evaluate the performance of various models. Accuracy measures the overall correctness of the model and is defined as the ratio of true predictions (both true positives and true negatives) to the total number of cases examined. The formula for Accuracy is:\n$\\Accuracy = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$\nThis metric is straightforward, but may not always provide a complete picture, especially in imbalanced datasets where one class may dominate the others. The F1 score is a harmonic mean of precision and recall, providing a balance between these two metrics. It is particularly useful when"}, {"title": "Evaluation metrics", "content": "dealing with imbalanced datasets. The formula for F1-Score is:\n$\\F1-Score = 2x\\frac{\\text{Precision }\\times \\text{ Recall}}{\\text{Precision + Recall}}$\nThe macro-average method calculates the F1 score independently for each class but does not take class imbalance into account. Each class is given equal weight. The formula for Macro-average F1-Score is:\n$\\Macro\\text{-average F1} = \\frac{\\sum(F1\\text{-Score of each class})}{\\text{Number of classes}}$\nThe weighted-average F1 score calculates the F1 score for each class but gives them a weight depending on their support. This method accounts for class imbalance by weighting the F1-score of each class by the number of true instances in each class. The formula for the weighted average F1 score is:\n$\\Weighted\\text{-average F1} = \\sum(\\frac{\\text{Support of class}}{\\text{Total samples}} X \\text{F1-Score of class})$\nUsing both macro-average and weighted-average F1 scores and accuracy enables a comprehensive and nuanced evaluation of model performance in classifying skin disease images. The macro-average F1-score highlights the model's consistency across different conditions, emphasizing its capability to handle rare diseases effectively. In contrast, the weighted average F1 score provides insight into the model's accuracy in diagnosing more common diseases, reflecting its practical utility in a typical clinical environment. This"}, {"title": "Evaluation metrics", "content": "layered approach ensures that the evaluation captures both overall accuracy and detailed performance across various class distributions, facilitating a balanced comparison of models tailored to the specific needs of healthcare applications. These metrics provide a comprehensive view of model performance, highlighting strengths in handling the overall dataset and specific classes, particularly useful when dealing with medical data like skin diseases where some conditions may be rarer than others. The evaluation results are presented in Table 2 and Table 3."}, {"title": "Results and Discussions", "content": "Table 2 illustrates the initial performance metrics of various models fine-tuned solely on the DDI dataset without domain adaptation. The results show a noticeable variance in performance across different models. Generally, models pretrained on larger, diverse datasets demonstrated superior performance compared to those trained specifically on skin image datasets. For instance, both DeepDerm and HAM10000 exhibited subpar F1 scores, with HAM10000 reaching a relatively high accuracy of 0.74, whereas DeepDerm lagged with an accuracy of 0.59. RETFound and YOLOv8 outperformed these models, showing better accuracy and F1 scores. Specifically, YOLOv8 models trained on a broader range of data outperformed those trained exclusively on chest images or the YOLOv8-x variant.\nThe enhanced adaptability of YOLOVN to skin images might come from its more effective feature extraction and"}, {"title": "Results and Discussions", "content": "augmentation techniques, which are crucial to handling nuanced variations in skin texture and color. This model also appears to better generalize across the smaller, more specialized datasets typical in dermatology, potentially reducing overfitting issues seen in YOLOv8-x.\nAmong all models evaluated, MedVIT stood out, probably due to its medical imagery-optimized transformer architecture that can ably integrate multiscale features and fine-grained details essential for accurate skin condition classification. This design is particularly adept at utilizing sparse annotations prevalent in medical datasets, thereby boosting its learning efficiency. Moreover, within the MedVIT series, the base model distinguished itself by achieving the highest accuracy at 0.74, surpassing both the small and large versions. This superior performance is attributed to its balanced complexity, which effectively prevents overfitting, and its focused and efficient feature learning capabilities."}, {"title": "Enhanced Model Performance Through Domain Adaptation", "content": "As detailed in Table 3, post-domain adaptation-which involved fine-tuning models on a combined dataset of HAM10000 and DDI-significant improvements were noted in both accuracy and F1 scores across most models. This process leveraged the larger HAM10000 dataset, which comprises approximately 10,000 samples, significantly more than the DDI dataset. This considerable dataset size helped bridge the domain gap and enhanced the models' ability to capture and learn from diverse skin image features more effectively.\nHowever, the performance of YOLOv8-N slightly decreased, likely due to the model overfitting when exposed to the large volume of domain-specific data. In contrast, other models demonstrated substantial enhancements due to domain adaptation: DeepDerm showed notable increases of 15% in accuracy and 16.7% in macro-average F1 score, indicating that additional training on skin images significantly bolstered its capability for efficient feature learning. RET-Found also displayed improved performance, with gains of 1.4% in accuracy and 4.7% in macro-average F1 score. Furthermore, MedViT-base saw its accuracy rise from 0.74 to 0.76, along with improvements of 2.7% in accuracy and 5% in macro-average F1 score. These results underscore that incorporating more domain-specific training data can significantly enhance model robustness and accuracy, making them more adept at classifying various skin diseases."}, {"title": "Discussions", "content": "As shown in Fig 3, the results of our experiments prove the effectiveness of transfer learning approach. First, the improvement in model performance due to domain adaptation is evident from the comparison between the DDI-only models and those fine-tuned on combined Ham10000+DDI datasets. Models pre-trained on skin images, such as Deep-Derm and HAM10000, demonstrate a notable increase in accuracy when further adapted to specific dermatological tasks. This underscores the benefit of using domain-specific training data, which enhances the model's ability to generalize from learned dermatological features. Secondly, general"}, {"title": "Discussions", "content": "medical image models, such as MedViT, often outperform domain-specific models. This can be attributed as their training on diverse medical imagery, enabling them to learn more robust and generalizable features. MedViT-base achieves higher accuracy compared to more specialized models like DeepDerm and RETFound. The data also reveals that larger versions of models, such as MedViT-large, do not always equate to better performance. In some instances, these models exhibit a decline in accuracy compared to their base or smaller counterparts, likely due to overfitting on the training data. The larger models, while potentially more powerful, might be too complex for the amount of training data available, leading to worse generalization on unseen data."}, {"title": "Conclusion", "content": "In this article, we showed a study that represents a significant advancement in the field of dermatological AI, addressing critical challenges in the diagnosis of skin diseases and paving the way for more inclusive and accurate diagnostic tools. Through comprehensive experimentation and analysis, we have demonstrated the effectiveness of pre-trained models, including RETFound, MedViT, and YOLOv8-Chest, in accurately predicting skin diseases in various skin tones. Our research underscores the importance of leveraging transfer learning techniques and domain adaptation to harness the wealth of knowledge encapsulated in pre-trained models, thereby enhancing their performance on underrepresented skin tones. By benchmarking these models on the Diverse Dermatology Images (DDI) dataset, we have provided valuable insights into their strengths and limitations, allowing clinicians and researchers to make informed decisions regarding model selection and deployment. Furthermore, our meticulous data processing procedures, including stratification by skin tone and label balance, ensure the fairness and reliability of our evaluation framework. This approach not only improves the generalizability of our findings, but also underscores our commitment to inclusivity and equity in dermatological AI research. Looking ahead, our findings lay the foundation for future research efforts aimed at further improving the accuracy and inclusiveness of skin disease diagnosis. By continuing to refine and expand upon our methodologies, we can drive innovation in the development of AI-driven diagnostic tools that benefit patients of all skin tones."}, {"title": "Future works", "content": "In our current research, we've utilized pre-trained models from various domains and perspectives to enhance the performance of our model. For instance, we have leveraged pretrained models designed for analyzing retinal images, chest images, and medical images such as MedVit, which specializes in medical image analysis. Using transfer learning techniques, we adapt these models to work with skin images, broadening the scope of our diagnostic capabilities.\nLooking ahead, our future work will involve evaluating the effectiveness of SkinGPT , a generative model specifically designed to generate prompts to diagnose and describe skin conditions. We plan to evaluate Sk-"}]}