{"title": "Kernel Looping: Eliminating Synchronization Boundaries for Peak Inference Performance", "authors": ["David Koeplinger", "Nathan Sheeley", "Reid Goodbar", "Angela Wang", "Darshan Gandhi", "Matheen Musaddiq", "Matthew Shaffer", "Mingran Wang", "Pushkar Nandkar", "Leon Zhang", "Han Wang", "Raghu Prabhakar"], "abstract": "Token generation speed is critical to power the next wave of Al inference applications. GPUs significantly underperform during token generation due to synchronization overheads at kernel boundaries, utilizing only 21% of their peak memory bandwidth. While recent dataflow architectures mitigate these overheads by enabling aggressive fusion of decoder layers into a single kernel, they too leave performance on the table due to synchronization penalties at layer boundaries. This paper presents kernel looping, a specialized global op- timization technique which exploits an optimization oppor- tunity brought by combining the unique layer-level fusion possible in modern dataflow architectures with the repeated layer structure found in language models. Kernel looping eliminates synchronization costs between consecutive calls to the same kernel by transforming these calls into a single call to a modified kernel containing a pipelined outer loop. We evaluate kernel looping on the SambaNova SN40L Re- configurable Dataflow Unit (RDU), a commercial dataflow accelerator for AI. Experiments demonstrate that kernel loop- ing speeds up the decode phase of a wide array of powerful open-source models by up to 2.2\u00d7 on SN40L. Kernel looping allows scaling of decode performance over multiple SN40L sockets, achieving speedups of up to 2.5x. Finally, kernel looping enables SN40L to achieve over 90% of peak perfor- mance on 8 and 16 sockets and achieve a speedup of up to 3.7\u00d7 over DGX H100. Kernel looping, as well as the mod- els evaluated in this paper, are deployed in production in a commercial Al inference cloud.", "sections": [{"title": "Introduction", "content": "In recent years, the landscape of open-source language mod- els has evolved significantly, with state-of-the-art (SOTA) models becoming smaller while achieving very high accu- racy [14, 16, 19, 35, 40]. These compact models, often pre- trained or distilled from larger ones over vast datasets [16], have achieved remarkable accuracy through advancements in model architecture and training techniques. As model con- text length continues to grow (sometimes exceeding 128,000 tokens [16]), more sophisticated prompt engineering and in-context learning techniques [13] have emerged, shifting focus from training to inference.\nInference involves generating a sequence of output to- kens, given a sequence of input tokens. Output tokens are generated in two distinct phases: \"prefill\" and \"decode\". Pre- fill is the process of producing the first output token, while decode is the process of producing all subsequent output tokens in an autoregressive manner. The time spent in the prefill phase is called Time to First Token (TTFT). The time taken to produce one token in the decode phase is called Time per Output Token (TPOT). Decoding performance is often reported as a throughput: tokens per second [1]. The prefill phase is often compute-bound on modern AI accelera- tors [11] due to the high operational intensity in its compute graph. Conversely, the decode phase is memory-bound due to low operational intensity as shown in Table 1. The ratio of input:output tokens also varies widely based on the task. For instance, summarization tasks have a high input:output token ratio, while math and coding questions can have a low input:output token ratio.\nWith access to powerful open-source models, the ma- chine learning community has produced advanced infer- ence techniques that drastically increase the number of out- put tokens for an input prompt. Chain-of-Thought (CoT) prompting [15, 30, 39] performs complex reasoning tasks by breaking down problems into a series of logical steps. However, this method generates multiple intermediate to- kens during inference, significantly increasing the overall time spent in the decode phase. Similarly, test-time scaling techniques [12, 25, 32] have demonstrated that inference ef- ficiency can be improved by increasing the output tokens generated, leveraging the model's ability to iterate and refine its outputs. Such iterative methods rely on a large context window that holds all tokens generated at each step. While a naive deployment of such solutions can negatively impact TTFT, innovations like context caching [2, 9, 10] mitigate this impact by preserving KV cache values across multiple turns. Context caching accelerates prefill by reducing the compute required during this phase in multi-turn conversa- tions. With context caching, optimization needs shift back to the decode phase. This brings optimizations like specula- tive decoding [20, 23] to the forefront. Speculative decoding relies on smaller draft models to have high decode through-put to be effective. This way, the smaller model generates tokens rapidly which are then checked and corrected by a larger model at a chosen cadence [20]. Consequently, fast token generation on both large and small models is the most important enabler for modern inference.\nThe decode phase of inference is memory-bound on mod- ern Al accelerators like the DGX H100 [6] due to its low operational intensity. Table 1 shows the operations, bytes, and the operation intensities to generate one token from various Llama3.1 models under different batch sizes and se- quence lengths. In this table, we estimate the FLOPs/byte by assuming that generating a single token requires reading all parameters and KV cache values once. The reported compute FLOPs are obtained by adding all matrix multiply operations in all layers. Streaming operations like softmax and RM- SNorm account for a much smaller fraction of the compute and do not change the overall insight below. DGX H100 has a peak BF16/FP16 compute capability of 1000 TFLOPs (ex- cluding structured sparsity), and a peak HBM bandwidth of 3 TB/s [5]. Any configuration with an operation intensity less than 1000/3 = 333 will therefore be memory bandwidth- bound. As shown in Table 1, all Llama 3.1 model config- urations are memory bandwidth-bound. Note that for the combination of large sequence length and large batch size,"}, {"title": "", "content": "the FLOPs/byte goes down as sequence length increases. Un- like batch size, which linearly increases both compute FLOPs and KV cache size, sequence length only impacts a subset of the operators during decode. Consequently, the KV cache size grows faster than the compute FLOPS, leading to lower operational intensity.\nBatching is a common technique to increase operation intensity and improve the overall Performance/TCO. How- ever, batch sizes cannot be made arbitrarily large. Both prefill TFLOPs (which is already compute-bound) and KV cache size increase linearly with batch size. Consequently, higher batch sizes increase TTFT which impacts the end-user SLA and interactivity. Larger KV cache sizes can exceed the limited HBM capacity available on accelerators. Finally, the number of output tokens produced can widely vary for each query in the batch. This variability in output tokens can utilize hardware inefficiently and increase overall queuing delays. As a result, many inference applications, such as chatbots or LLM agents, often limit batch size to ensure a reasonable turnaround time for user queries. [22] [41].\nFigure 1 plots the decode speed for Llama 3.1 8B with TensorRT-LLM [8] on 2, 4, and 8 H100 GPUs (green bars). The last two orange bars, in comparison, are the contribution of this paper. In Figure 1b, a value of 100% corresponds to the performance obtained if the available HBM bandwidth were fully utilized to transfer weights and the KV cache from memory, with everything else happening in its shadow. We can observe that GPUs operate well below their roofline performance for decode. Furthermore, decode performance scales poorly with increasing numbers of GPUs. Much of this inefficiency stems from forced synchronization overheads at kernel call boundaries. Rigidity in the GPU's on-chip mem- ory and interconnect, limited on-chip SRAM, and program- ming model constraints limit the number of operators that can be fused and executed asynchronously in a kernel [27].\nReconfigurable Dataflow Architectures (RDAs) have re- cently emerged as promising commercial alternatives to GPUs [27]. Dataflow architectures provide a much more flexible hardware substrate that enables \"layer-level fusion\", or fusing entire layers into a single kernel call [26, 27], sig- nificantly reducing the number of kernel calls compared to GPUs. However, we quantify and argue that dataflow archi- tectures incur synchronization overheads at kernel call boundaries too. Furthermore, synchronization costs can be nontrivial: reprogramming a dataflow architecture at kernel boundaries involves loading a new configuration bitstream onto all on-chip units, which is a heavier operation than a typical instruction fetch. These synchronization overheads can quickly wipe out the performance upside from higher operator fusion. Note that synchronization points at kernel call boundaries are not fundamental to the algorithm itself. Rather, it is imposed by the limitations of the underlying hardware platform and/or programming model."}, {"title": "", "content": "This paper proposes a compiler optimization to eliminate unnecessary synchronization boundaries. We build on the benefits of layer-level fusion in dataflow architectures and exploit repetitive layers found in the most powerful model architectures to date. We propose kernel looping, a global op- timization technique that rewrites groups of repeated kernel calls into a single kernel call with a pipelined outer loop. Ker- nel looping has two key benefits: (i) eliminating synchroniza- tion between calls to the same kernel, and (ii) overlapping the compute and communication across kernels. We evalu- ate the benefits of kernel looping on the SambaNova SN40L Reconfigurable Dataflow Unit (RDU) [26, 27], a commer- cial RDA amenable to aggressive layer-level fusion. Figure 1 shows that with kernel looping, Llama3.1 8B can run at 78% of roofline performance on the SN40L, outperforming DGX H100 by 2x. Kernel looping also helps scale performance across more SN40L sockets, achieving 76% of roofline perfor- mance on 16 sockets. Kernel looping and all models reported in this paper have been deployed in production, and are currently in use by several customers and developers.\nThis paper makes the following key contributions:"}, {"title": "Decoder Performance Characterization", "content": "In this section, we use Llama3.1-8B as a driving example to characterize token generation performance on two plat- forms: DGX H100 [5] and SN40L. Here, SN40L-8 corresponds to a system with 8 SN40L sockets, similar to DGX H100 with 8 GPUs. Specifically, we highlight and quantify the ineffi- ciencies on both platforms due to synchronization at kernel call boundaries, thus motivating the need for the techniques described in Section 3. We chose Llama3.1-8B because it is"}, {"title": "", "content": "a very popular open-source model. But more importantly, Llama3.1-8B plays an important role as a draft model to ac- celerate larger models like Llama3.1-70B and Llama3.1-405B using speculative decoding [20]. While we use a specific model to illustrate concepts, the insights apply broadly to a larger variety of model architectures.\nFigure 2 and Figure 3 depict the structure of Llama3.1-8B and its execution on both DGX H100 and SN40L. Figure 2 shows that Llama3.1-8B has 4 major types of blocks: one Embedding layer, 32 Decoder layers, one Classifier layer, and Sampling. While Sampling is not part of the model, we include it here as it is executed once for every output token, similar to the other layers. Figure 2 also shows the key operators comprising a decoder. As decoder layers dominate token generation time, the rest of this section will focus on how a single decoder layer is executed on multiple sockets.\nEach decoder layer is parallelized across multiple sockets to extract higher performance. In a DGX H100, a decoder is parallelized across 8 H100 GPUs. While several paralleliza- tion strategies exist in literature, we study the Tensor-Parallel (TP) mapping here as it is a commonly deployed mechanism. TP mapping involves sharding weights and input tensors along one or more dimensions, and distributing each shard"}, {"title": "", "content": "to a different socket. Sharding introduces distributed reduc- tion and broadcast operations across sockets like allreduce.\nFigure 2 shows the execution of a decoder on DGX H100 and SN40L-8. On both platforms, a \u201ckernel\u201d is a unit of work dispatched to the accelerator, which executes one or more operators in a fused fashion. Kernels act as forced synchro- nization barriers on both platforms. Specifically, operators be- longing to different kernels are not executed asynchronously. For a TP-mapped kernel, a kernel call boundary executes an implicit synchronization barrier across all sockets. The key difference between the two platforms is the number of operators fused into a single kernel call. On H100, a decoder is broken up and executed as 10 kernel calls, labeled in green in Figure 2 as K1 through K10. In contrast, the SN40L-8 ex- ecutes all operators in the decoder layer as a single kernel call, labeled in orange in Figure 2 as K0.\nThe kernel makeup for H100 is obtained by using TensorRT- LLM [8] to run Llama3.1-8B and by extracting kernel call information using the NVIDIA Nsight Systems profiler [7]."}, {"title": "", "content": "The kernel makeup on the SN40L-8 is similar to prior litera- ture [26] performing layer-level fusion. Collective commu- nication operators introduced by TP mapping like allreduce are shown explicitly as a separate operator in Figure 2. On H100, allreduce is implemented by calling into the NVIDIA Collective Communications Library [4]. On the SN40L, allre- duce is implemented as a cross-chip dataflow operator using the chip-to-chip peer-to-peer (P2P) protocol.\nWe first characterize the performance of a decoder layer on DGX H100. Profiling data is obtained using benchmarking scripts included with TensorRT-LLM after a few warm up iterations, as recommended by the documentation [8]. Using this methodology, we observe that Llama3.1-8B generates tokens at over 300 tokens/s on 8xH100. This is competitive with the optimized implementations of several commercial API providers [1], suggesting that our methodology is repre- sentative of a real world deployment that includes key GPU optimizations. Figure 3(i) shows the kernel schedule for a single decoder on DGX H100, comprising K1 through K10.\nFigure 4 shows the time breakdown of the Time Per Output Token (TPOT) for Llama3.1-8B on 2, 4, and 8 H100 sockets. All kernels studied in this scenario are memory bandwidth- bound. From Figure 4, we see that the performance of all kernels does not scale by the same amount with increasing socket count. GEMM performance scales better than other operators, but not linearly. Operators like RMSNorm, SiLU, and SDPA perform at a significantly lower efficiency from their theoretical peak despite having more aggregate HBM bandwidth, indicating that kernel warm up and synchroniza- tion costs inhibit scaling. The cost of allreduce and other non-decoder operators account for a larger portion of the overall time per output token for 8xH100.\nWe next characterize the mapping and performance of a decoder layer on SN40L. Figure 3(ii) shows the kernel sched- ule to produce one output token. Unlike H100, SN40L in- vokes a single kernel Ko to execute a decoder. This speaks to a well-studied advantage of dataflow architectures over more traditional data-parallel mappings: the number of op- erations that can be fused into a single kernel is very flexible. On one extreme, a single logical operation can be spread across the entire chip using data parallelism. On the other extreme, hundreds of operations can be fused into a single kernel via aggressive tiling and pipelining transformations. As we later show in Section 4, this characteristic of dataflow architectures, and SN40L in particular, makes aggressive ker- nel fusion applicable across a wide range of model sizes and complexities.\nBy reducing the number of kernel calls, the single decoder mapping reduces the number of synchronization required. However, Figure 5 shows that a single decoder kernel by it- self is insufficient to extract peak performance on SN40L. Synchronization overheads at the end of each call to Ko ac- count for more than 30% of the TPOT, nullifying the perfor- mance upside of layer fusion."}, {"title": "System Overview", "content": "In this section, we describe the implementation details of the kernel looping optimization and provide several simplified examples to help explain its behavior."}, {"title": "Pattern Matching", "content": "The first step in kernel looping is to pattern match against a compatible contiguous list of kernel calls. This is done as a dataflow analysis which determines whether all kernel pa- rameters fit strict patterns of usage across these calls. Given a kernel with M parameters $[p_0,..., p_{M-1}]$, a candidate set of N calls to this kernel $C = [c_0, . . ., c_{N-1}]$, where arguments to the i'th parameter $p_i$ are $A_i = [a_{i,0},..., a_{i,N-1}]$, relevant usage properties are defined in Figure 6a. We assume basic usage analysis (Reads, Writes) has been run within the ker- nel body and that a basic liveness analysis (LiveAfter) has already been run across kernel calls."}, {"title": "", "content": "The Local property is used to match on chained calls like the chained decoder case shown in Figure 3. As we will dis- cuss in Section 3.2, this pattern can be used to promote inter- mediate off-chip tensors to an on-chip buffer. However, doing so requires that several restrictive conditions are met. First, the parameters must be uniquely chained. That is, within C, the chained output tensor must only be used by the next call, and the output parameter must be consistently used by the same input parameter. Additionally, the write address space (WriteSpace) over all writes must entirely cover the read address space (ReadSpace) to ensure that the read is only dependent on the preceding call. 1\n$P(p) = Const(p) \\lor In(p) \\lor Local(p) \\lor Live(p) \\lor Out(p)$\nEquation 1 defines the set of usage properties P for a parameter p. In this equation, the function F(p) yields the singleton usage property set {F} if the corresponding condi- tion in Figure 6a holds and \u00d8 otherwise. The meet function V is defined by the lattice in Figure 6b. The candidate call list C then pattern matches successfully if Equation 2 holds:\n$Match(C) = \\forall i \\in [0, M): P(p_i) < T$\nWe require that P(p) is strictly less than the top element (T), as the top element is considered to be a match failure. All other elements in the lattice are successful matches.\nThe matching function in Equation 2 can be used to build an analysis over all kernel calls in the program. In our exper- iments described in Section 4, we found that a basic greedy algorithm was sufficient to fuse and pipeline chains of de- coder calls in the targeted language models. In applications with more complex kernel call interactions and orderings, it may be necessary to do a cost-driven search to maximize the number of optimized contiguous calls."}, {"title": "Transformation", "content": "Figure 7 summarizes the transformation rules when ap- plying kernel looping on a kernel k over a list of N pattern matched calls. As shown in rule 7(1), this transformation cre- ates a new kernel with a loop, where the number of iterations is equal to the number of calls being optimized.\nIn order to use this loop to iterate over arguments across calls, we can view the number of calls N as a new, outermost dimension created by concatenating and reshaping the tensor arguments. We refer to this transformation as \u201cgrouping\u201d, where arguments to a parameter should become a group if Equation 3 holds. Correspondingly, rule 7(2) alters parameter shapes, assumed here to be encoded in the type system.\n$Group(p) = Live \\in P(p)U(Local \\notin P(p) \\cap Const \\notin P(p))$\nDepending on the level of abstraction used by the intermediate represen- tation, proving this condition in general may require polyhedral analysis of access patterns. A compiler using a sufficiently high level domain specific intermediate representation of tensor operations, however, can match on common cases with relative ease."}, {"title": "Evaluation", "content": "We evaluate kernel looping across some of the most powerful open source models to date. First, we evaluate its generality, or how broadly applicable the optimization is. Second, we evaluate its scalability over increasing numbers of SN40L sockets. Lastly, we compare performance between SN40L with kernel looping and DGX H100."}, {"title": "Methodology", "content": "Table 2 describes the benchmark configurations studied in this paper. Table 3 describes the compute TFLOPS and mem- ory bandwidths of the hardware platforms being evaluated. As this paper focuses only on the decode stage of inference, all evaluations in this section measure and report the decode throughput. All benchmarks are run and measured on real hardware. The throughput number is obtained by measuring"}, {"title": "Generality", "content": "Figure 9 shows the speedups obtained on SN40L-16 with ker- nel looping enabled, over a baseline without kernel looping. Kernel looping speeds up a broad variety of model archi- tectures, parameter sizes, batch sizes, and sequence lengths, with a geomean speedup of 1.6\u00d7. We highlight that kernel looping is not limited only to smaller models. Figure 9 shows that larger models like Qwen2.5-72B observe speedups close to 2x even at large batch sizes, highlighting the nontrivial"}, {"title": "Scalability", "content": "Next, we quantify the impact of kernel looping on scaling performance over multiple sockets. Figure 10 shows the speedups of both three configurations over a baseline SN40L- 8: SN40L-8 with kernel looping, and SN40L-16 with and without kernel looping. We observe that the optimization enables performance scaling from 8 to 16 SN40L sockets, with a geometric mean speedup of 2.5\u00d7 over an SN40L-8 baseline without kernel looping. We also observe that the performance of models suffer significantly on SN40L-16, run- ning slower than SN40L-8 due to the out-sized impact of synchronization overheads at the end of each decoder. Ker- nel looping eliminates these overheads."}, {"title": "Performance vs. DGX H100", "content": "Figure 11 shows the speedups achieved by SN40L-8 and SN40L-16 over 8xH100. SN40L-8 matches or outperforms"}, {"title": "", "content": "DGX H100 by up to 2x in spite of having only 50% of the DGX H100's peak HBM bandwidth. A detailed breakdown of performance scaling issues on DGX H100 can be found in Section 2. While we only compare four model configurations against the DGX H100, we observe that SN40L with kernel looping achieves similar speedups on the other models, based on publicly benchmarked decode performance numbers from several inference API providers [1].\nFigure 12 plots the performance as a percentage of peak achievable performance. Here, 'peak' (100%) on each plat- form is calculated as the time taken to stream the weights"}, {"title": "Related Work", "content": "We discuss one class of related work by casting Kernel loop- ing as a logical composition of two related transformations - loop rerolling and kernel fusion. Loop (re)rolling is done in CPU compilers to reduce code size to target memory- limited environments like embedded devices. The transfor- mation may be performed in an isolated fashion, such as in ROLAG [29] and RollBin [17] or to condense intermediate or final code size in a larger software pipelining optimiza- tion [34]. Loop rerolling is also used to \u201crediscover\u201d unrolled loops when decompiling binary code [33] and hardware netlists [31]. In the latter case, rerolling serves to both de- crease artifact size and speed up hardware simulation time. However, this speedup happens because it exploits optimiza- tions in the targeted hardware simulator related to simulat- ing rolled loops in the hardware description language. In contrast, Kernel looping is agnostic to code size, and aims primarily to improve the performance of the model.\nKernel fusion is a commonly known optimization which combines multiple smaller kernels into a single larger ker- nel. Fusion has been shown to improve both GPU energy efficiency [38] and performance [24, 37], particularly for memory-bound kernels. While kernel fusion typically refers to the vertical fusion of chained, data-dependent kernel calls, horizontal fusion across independent GPU kernel calls has also been proposed [21] and shown to help improve perfor- mance by hiding instruction latencies.\nSeveral methods have been proposed to reduce the run- time overhead of kernel calls. Ismayilov, et. al., [18] present an execution model which offloads synchronization and or- chestration of GPU kernels in a multi-GPU environment to the GPUs themselves, thus improving performance and reducing communication latency. CUDA Streams [28] and CUDA Graphs [3] enable concurrent kernel execution and minimizes host involvement in launching multiple kernels on GPUs. In contrast, kernel looping eliminates the need for multiple calls and overlaps the execution of operations."}, {"title": "Conclusions", "content": "This paper presented kernel looping, a novel compiler opti- mization to improve inference performance by eliminating synchronization at kernel call boundaries. Kernel looping leverages the repetitive structure of layers in LLMs to trans- form multiple consecutive kernel calls into a single call with a pipelined outer loop, thereby minimizing synchronization points and improving compute-memory overlap. Evaluation on SN40L, a commercial reconfigurable dataflow accelerator, shows significant performance improvements with a 3.7\u00d7 speedup over DGX H100 with TensorRT-LLM, and scaling efficiently to larger configurations with up to 90% of peak bandwidth utilization on multi-socket setups. Kernel looping achieves a geometric mean speedup of 1.6\u00d7 over a variety of models, demonstrating general applicability. Additionally, kernel looping achieved a 2.5\u00d7 geometric mean speedup when scaling from 8 to 16 SN40L sockets. Kernel looping and the models evaluated in this paper are deployed in a com- mercial Al inference cloud. Kernel looping establishes a new performance ceiling for inference on dataflow accelerators like SN40L. The work lays a foundation for further com- piler and hardware co-design to fully exploit the potential of modern Al inference platforms."}]}