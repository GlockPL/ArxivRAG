{"title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation", "authors": ["Ziyang Luo", "Haoning Wu", "Dongxu Li", "Jing Ma", "Mohan Kankanhalli", "Junnan Li"], "abstract": "Large multimodal models (LMMs) with advanced video analysis capabilities have recently garnered significant attention. However, most evaluations rely on traditional methods like multiple-choice question answering in benchmarks such as VideoMME and LongVideoBench, which are prone to lack the depth needed to capture the complex demands of real-world users. To address this limitation\u2014and due to the prohibitive cost and slow pace of human annotation for video tasks\u2014we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS Chatbot Arena's framework, designed to automatically assess LMMs' video analysis abilities. VideoAutoArena utilizes user simulation to generate open-ended, adaptive questions that rigorously assess model performance in video understanding. The benchmark features an automated, scalable evaluation framework, incorporating a modified ELO Rating System for fair and continuous comparisons across multiple LMMs. To validate our automated judging system, we construct a \"gold standard\" using a carefully curated subset of human annotations, demonstrating that our arena strongly aligns with human judgment while maintaining scalability. Additionally, we introduce a fault-driven evolution strategy, progressively increasing question complexity to push models toward handling more challenging video analysis scenarios. Experimental results demonstrate that VideoAutoArena effectively differentiates among state-of-the-art LMMs, providing insights into model strengths and areas for improvement. To further streamline our evaluation, we introduce VideoAutoBench as an auxiliary benchmark, where human annotators label winners in a subset of VideoAutoArena battles. We use GPT-4O as a judge to compare responses against these human-validated answers. Together, VideoAutoArena and VideoAutoBench offer a cost-effective, and scalable framework for evaluating LMMs in user-centric video analysis.", "sections": [{"title": "1. Introduction", "content": "Recently, large multimodal models (LMMs) with advanced video understanding capabilities, such as GPT-4O, Gemini-1.5-Pro, Aria, Qwen2-VL, and LLaVa-Video, have garnered significant attention within the multimodal community. These models represent a major shift in the capabilities of artificial intelligence by extending beyond traditional image-based LMMs to encompass complex video inputs. Unlike their predecessors, which primarily focused on static visual data, these models are designed to handle dynamic, time-variant data, making them well-suited for processing lengthy and intricate video sequences. By leveraging language-based instructions from users, these LMMs can efficiently conduct a range of video analysis tasks, from understanding fine-grained scene details to comprehending overarching narrative structures.\nTo better evaluate the video analysis capabilities of these models, recent works have introduced several widely used benchmarks, including MVBench, VideoMME, and LongVideoBench. These benchmarks typically share a common feature: they pre-define core video analysis skills, such as object recognition in a single frame and action reasoning across a sequence of frames. They adopt an ability-centric approach, often using multiple-choice questions to assess performance. While these benchmarks have significantly contributed to the development of LMMs, they place limited emphasis on the types of questions real users might ask when seeking assistance with video analysis. In contrast, practical video analysis scenarios are far more complex and diverse in their requirements.\nTo address this gap, we can draw inspiration from the evaluation methods used for large language models (LLMs). Traditional language benchmarks, such as MMLU, IFEval, HumanEval, and GSM8k, also suffer from limited alignment with real user interactions. To mitigate this issue, platforms like LMSYS Chatbot Arena have been introduced, providing an open, crowdsourced platform for evaluating LLMs based on human preferences. Chatbot Arena employs a pairwise comparison approach, collecting feedback from a diverse set of users, ensuring that evaluation questions are generated by real users. This approach overcomes many of the challenges posed by previous benchmarks and has become one of the most widely adopted methods for evaluating LLMs.\nOne straightforward solution is to adapt this idea directly to LMMs for video analysis. Indeed, recent work, such as WildVision Arena, has attempted to do so. However, an analysis of the WildVision Video Arena leaderboard reveals certain challenges with this approach. The video arena has received only 256 votes across 11 models, with each model participating in an average of just 23 battles since its release approximately 6 months ago. This limited number of battles can likely be attributed to the increased complexity involved in formulating questions for video-based tasks. Unlike language and image data, where questions can be quickly generated or verified within seconds, videos are typically longer and contain richer, more complex contexts. Annotators must spend significantly more time watching and understanding the video content before formulating high-quality questions. In our preliminary attempts, we hired annotators to complete this task, and the time cost proved significant, with a maximum output of only 7 samples annotated per hour. This time investment hinders the scalability of generating high-quality questions for LMMs, a crucial factor for ensuring the effectiveness of arena-style evaluations.\nTo address the limitations of existing video analysis benchmarks, we propose VideoAutoArena, a fully automated, arena-style evaluation method for LMMs. Unlike human-driven platforms, VideoAutoArena leverages LMM agents for user simulation and preference selection, eliminating the need for costly human annotators and enabling scalable, efficient evaluations. The framework also integrates fault-driven hard prompt evolution, which generates progressively challenging questions based on model performance, ensuring more rigorous testing. By simulating real-world user behavior, VideoAutoArena bridges the gap between ability-centric evaluations and practical application demands. Our human preference experiments show that 84.20% of the time, questions in VideoAutoArena better mimic real-world user question styles compared to VideoMME and LongVideoBench. Additionally, 87.29% of the time, our automatic judging aligns with human preference selection.\nExperiments on 11 well-known proprietary and open-source LMMs reveal that open-source models still lag behind the SOTA closed-source model GPT-4O in video analysis, with a significant performance gap (-385.7). This gap is notably larger than those observed in traditional multiple-choice question-answering benchmarks. The disparity becomes even more pronounced as video length increases or the difficulty of the questions rises. Furthermore, when focusing on user-background relevance and helpfulness, the performance gap widens further. These findings highlight how our benchmark offers a user-centric perspective, providing valuable insights for the development of LMMs.\nTo complement VideoAutoArena, we also introduce VideoAutoBench, a streamlined benchmark designed for faster, more accessible evaluation of LMMs in video analysis. VideoAutoBench leverages a curated subset of battles from VideoAutoArena, where human annotators have selected the winning model responses. Using GPT-4O for automatic judging, VideoAutoBench compares model answers"}, {"title": "2. Related Work", "content": "LMMs with advanced video understanding capabilities have garnered significant research attention. For the closed-source models, GPT-40 and Google's Gemini-1.5 demonstrate SOTA video analysis performance. Meanwhile, the open-source community has made notable strides. Notably, the LLaVa series has been updated to the LLaVa-Video and LLaVa-OneVision models, along with the release of all training data. The Qwen-VL model has also been upgraded to the Qwen2-VL version, and the first open-source multimodal mixture-of-experts (MoE) model, Aria, has recently been introduced. These contributions have significantly narrowed the gap between closed-source and open-source models in video understanding. To accelerate the development of LMMs in video analysis, the establishment of more comprehensive benchmarks is essential.\nIn the early phase, researchers primarily relied on benchmarks featuring short videos, such as MSVD-QA, MSRVTT-QA, NExT-QA, and MVBench. However, these benchmarks have limitations due to their short video durations, averaging less than 50 seconds. This brevity restricts their ability to comprehensively evaluate the temporal understanding capabilities of LMMs, thereby hindering further advancements in LMMs development. To address these limitations, benchmarks like ActivityNet-QA and EgoSchema have extended video durations to approximately 180 seconds on average. More recently, research has introduced even more comprehensive benchmarks. For instance, MovieChat-1K assesses LMMs using movie videos with an average duration of 500 seconds, while LongVideoBench focuses on long-context interleaved evaluation with an average duration of 473 seconds. Additionally, MLVU presents a LMMs' ability-centric benchmark, featuring substantial extensions of video lengths. Furthermore, VideoMME introduces a highly comprehensive benchmark that includes short, medium, and long videos, further enhancing the evaluation of LMMs' temporal understanding abilities. As discussed in the introduction, most current benchmarks are limited by multiple-choice questions that diverge from real user interaction. To address this, we introduce VideoAutoArena, which evaluates LMMs through open-ended, simulated human questions."}, {"title": "3. VideoAutoArena", "content": "As illustrated in Figure 1, the VideoAutoArena pipeline consists of four core components: user simulation, peer battles, automatic judging, and fault-driven evolution. Initially, an agent reviews a video to identify user personas likely to be interested in the content. Adopting one of these personas, the agent formulates a relevant question about the video. Two randomly selected models then engage in peer battles to respond to the question. A judging agent determines which model provides the better response, while an analysis agent evaluates the responses, performs fault analysis, and generates progressively challenging questions to further assess the models' capabilities. To demonstrate the effectiveness of our VideoAutoArena, we use 2,881 videos from LongVideoBench, with an average duration of 479 seconds. These videos are categorized into four duration ranges (8s, 15s], (15s, 60s], (180s, 600s], (900s, 3600s] and 10 categories: Movie, Life Vlogs, Geography, History, News Programs, Art, STEM, Computer Science, Cooking Recipes, and Travel Guides. The data distribution are provided in Figure 2. Our benchmark is not restricted to specific videos, and new videos can be easily incorporated into the evaluation pipeline."}, {"title": "3.2. User Simulation", "content": "Ideally, real user queries would offer the direct evaluation of LMM performance in real life video analysis. However, given the complex contextual demands of video content, human annotation is expensive and time-cost. To address this, VideoAutoArena adopts user simulation with role-play, with SOTA LMMs acting as agents to generate realistic, user-centeric questions and preferences, enabling a more practical evaluation. In language-based roly-play, there is typically no need to consider external context, such as videos, allowing role-play to freely generate diverse questions. In video analysis, however, questions are constrained by the video content. To address this, we introduce a novel role-play method called video content-constrained persona generation. Given a video, agents first identify the types of users likely to be interested in it, defining three user types: (1) users with backgrounds highly relevant to the video; (2) users with moderately relevant backgrounds; and (3) users with unrelated backgrounds who encounter the video by chance. This relevance-based categorization aims to emulate real users with varied backgrounds seeking assistance from LMMs for video analysis. Once user types are established, agents adopt these personas to generate persona-specific questions for detailed video analysis. This user-centric process sets our evaluation method apart from previous ability-centric benchmarks."}, {"title": "3.3. Peer Battles and Fault-Driven Evolution", "content": "Once the role-play agent generates a question for a specific video, we initiate peer battles between two anonymous models, following the Chatbot Arena style. Two models are randomly chosen from the model pool, presented with the same video and question, and asked to generate responses. Our goal is to fairly compare the quality of these responses to determine which LMM provides a better answer.\nSimilar to the concept of Hard Prompts in Chatbot Arena, incorporating more challenging questions can further push the boundaries of evaluating the abilities of current LMMs in video analysis. Thus, we aim to create a harder question set for evaluation purposes. Unlike Chatbot Arena, which can directly source hard prompts from millions of real user queries, our approach is limited to the prompts generated by a user simulator. To derive harder questions, we draw inspiration from the famous instruction synthesis method, Evol-Instruct, which evolves existing questions into more complex ones using predefined heuristic strategies. However, because Evol-Instruct generates questions based on a similar prompt structure for each evolution, it encounters limitations in question diversity. To address this, we introduce a fault-driven question evolution strategy that iteratively increases question complexity. Rather than relying on isolated prompts, each new evolution generates questions based on the results of the previous model battle. This approach creates a more adaptive and progressively challenging environment for the models, pushing them to respond to increasingly complex questions.\nIn this framework, a response analysis agent initially reviews responses from two competing models, identifying specific faults and performance weaknesses. Based on this assessment, role-play agents then generate tailored questions aimed at probing these weaknesses, making the question synthesis process progressively more fault-driven. After a new question is generated, a complexity evaluation agent assesses its difficulty. If the new question receives a higher overall complexity score than the previous one, it is retained for subsequent model battles. This iterative approach establishes a rigorous testing environment, challenging models with increasingly complex and contextually nuanced tasks, thus providing a deeper evaluation of each model's video understanding capabilities."}, {"title": "3.4. Judging and Ranking", "content": "A key aspect of arena-style evaluation is determining the winner in each model comparison. In Chatbot Arena, human annotators directly express their preferences, but in VideoAutoArena, human annotation is costly and difficult to scale due to the time-intensive nature of video analysis. To address this, we aim to automate the judgment process. Drawing inspiration from automated judging benchmarks like Arena-Auto-Hard and MT-Bench, we first define our judging standards as follows:\n1. Instruction Following: The response should closely adhere to the user's instructions, ensuring it directly addresses the specified task.\n2. Accuracy: The response must utilize information from the video accurately, avoiding fabrication or misquotation. It should maintain factual correctness, avoid hallucinations, and demonstrate contextual coherence with precise terminology and knowledge.\n3. Relevance: The response should consider the user's background information and needs, providing a comprehensive, detailed answer that addresses the question directly without straying off-topic. Responses should be thorough, offering multiple perspectives where relevant.\n4. Helpfulness: The response should provide valuable information to aid the user in understanding or solving their issue, avoiding irrelevant or vague content.\nBased on these standards, we adopt the LMM-as-a-Judge paradigm to automatically determine the better"}, {"title": "3.5. Experiments", "content": "Setup. To demonstrate the effectiveness of VideoAutoArena, we evaluate 11 SOTA LMMs, including GPT-4O/mini, Gemini-1.5-Pro/Flash, Aria, Qwen2-VL-72B/7B, LLaVa-Video-72B/7B, and LLaVa-OneVision-72B/7B, all of which have shown strong performance on the VideoMME. For response generation, each video was uniformly sampled to provide 64 frames as input. Since most of these LMMs do not support audio, the audio track was converted to subtitles and combined with the question as input. For automatic judging, each video was sampled to provide 128 frames and combined with subtitles. Additional experimental details are provided in the Appendix B.\nUser Simulation and Diversity. In our experiments, we generated an average of three personas per video across 2.9k videos, resulting in about 8.6k unique personas. Each persona includes motivations or reasons for their interest in the video, enabling more persona-specific question generation."}, {"title": "4. VideoAutoBench", "content": "While VideoAutoArena offers a novel approach to evaluating LMMs in video analysis, it is less immediately user-friendly than multiple-choice benchmarks like VideoMME and LongVideoBench, which provide straightforward scores based on model responses. In contrast, VideoAutoArena requires the target model to engage in comparative battles with other models to generate results. To streamline this evaluation process, we introduce VideoAutoBench, which combines the user-centric assessment strengths of VideoAutoArena with the simplicity and speed of traditional benchmarks. In our automated judging experiments, we included human annotators to label winners for a subset of battles, using these questions and non-"}, {"title": "5. Conclusion", "content": "We introduce VideoAutoArena, an automated arena-style benchmark that addresses the limitations of traditional multiple-choice video QA benchmarks. Using user simulation, peer battles, automated judging, and fault-driven evolution, VideoAutoArena enables a scalable, user-centric evaluation for complex video analysis tasks. Alongside, we present VideoAutoBench, a streamlined evaluation comparing model responses to human-labeled answers. Experiments on 11 SOTA LMMs reveal a notable performance gap between closed and open-source models, particularly for long videos, challenging questions, and scenarios involving user background relevance and response helpfulness."}, {"title": "A. Prompts", "content": "In Figure 10 and 11, we include the prompts for video content-constrained persona generation and persona-constrained video question asking."}, {"title": "A.2. Fault-Driven Evolution", "content": "Figure 12 includes the prompt for the fault-driven evolution."}, {"title": "A.3. Automatic Judging", "content": "Figure 13 includes the prompt for the automatic judging. Our VideoAutoBench adopts the same prompt for judging."}, {"title": "A.4. Difficulty Level Evaluation", "content": "Figure 14 includes the prompt for the complexity evaluation."}, {"title": "B. Experimental Details", "content": "In VideoAutoArena, Model A wins 5,620 battles, Model B wins 5,941 battles, and 918 ties. VideoAutoBench consists of 255 samples corresponding to 244 unique videos, with an average duration of 478.5 seconds. The duration distribution includes 62 videos for 8-15s, 62 for 15-60s, 60 for 180-600s, and 60 for 900-3,600s. The samples span 10 categories: 29 from Movies, 50 from Life Vlogs, 9 from Geography, 13 from History, 12 from News Programs, 9 from Art, 6 from STEM, 8 from Computer Science, 55 from Cooking Recipes, and 53 from Travel Guides."}, {"title": "B.2. LMMs Selection", "content": "In our experiments, we evaluate 11 SOTA LMMs:"}, {"title": "B.3. Hyperparameters", "content": "For user simulation, fault-driven evolution, automatic judging, and difficulty level evaluation, each video is uniformly sampled into a maximum of 128 frames, while response generation uses up to 64 frames. Each frame is resized to 512 x 512. For API-based LMMs, the max_tokens parameter is set to 4096, with other settings using default values. For open-source LMMs, the temperature is set to 0.7, and max_new_tokens is limited to 2048."}, {"title": "C. Examples", "content": ""}, {"title": "C.1. Persona", "content": "As shown in Figure 15, we include 15 examples of different personas for 5 different videos."}, {"title": "C.2. Question", "content": "As shown in Figure 15, we include 15 examples of different questions for 5 different videos."}, {"title": "C.3. Responses and Judging", "content": "As shown in Figure 16, 17, 18, and 19, we include the battle examples between different models."}, {"title": "D. Human Annotations", "content": ""}, {"title": "D.1. Question Ranking", "content": "In Figure 20, we include our guideline for the question ranking annotation."}, {"title": "D.2. Judging", "content": "In Figure 21, we include our guideline for the response judging annotation."}, {"title": "E. Limitation", "content": "VideoAutoArena and VideoAutoBench currently lack evaluations for multi-turn and non-English interactions, primarily due to the limited multi-turn conversational capabilities and restricted non-English proficiency of current open-sourced LMMs. Moreover, the automatic judging system tends to favor detailed responses, a preference also observed in human evaluations. While detailed responses are often more helpful to users, this introduces challenges in evaluating LMMs."}]}