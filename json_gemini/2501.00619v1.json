{"title": "A Study on Context Length and Efficient Transformers for Biomedical Image Analysis", "authors": ["Sarah Hooper", "Hui Xue"], "abstract": "Biomedical images are often high-resolution and multi-dimensional, presenting computational challenges for deep neural networks. These computational challenges are compounded when training transformers due to the self-attention operator, which scales quadratically with context length. Recent works have proposed alternatives to self-attention that scale more favorably with context length, alleviating these computational difficulties and potentially enabling more efficient application of transformers to large biomedical images. However, a systematic evaluation on this topic is lacking. In this study, we investigate the impact of context length on biomedical image analysis and we evaluate the performance of recently proposed substitutes for self-attention. We first curate a suite of biomedical imaging datasets, including 2D and 3D data for segmentation, denoising, and classification tasks. We then analyze the impact of context length on network performance using the Vision Transformer and Swin Transformer. Our findings reveal a strong relationship between context length and performance, particularly for pixel-level prediction tasks. Finally, we show that recent attention-free models demonstrate significant improvements in efficiency while maintaining comparable performance to self-attention-based models.", "sections": [{"title": "1. Introduction", "content": "Biomedical and clinical imaging modalities often produce high-resolution, multi-dimensional images that contain rich and detailed information. These large image sizes present computational challenges for deep neural networks, such as increased memory requirements and long processing times. \nThe popularity of transformers has compounded the computational difficulties of training neural networks on medical images. Central to transformers is the self-attention operator, which scales quadratically with context length. This quadratic scaling can be prohibitive when training models on medical images, where capturing fine-grained details in high-resolution, multi-dimensional images is critical.\nIn natural language processing (NLP), recent efforts have improved the efficiency of self-attention or have investigated replacing it all together. These works aim to design operators that match the performance of self-attention while scaling more favorably with context length, enabling models to process longer inputs. Such advances have gained popularity in NLP, driving new innovation and capabilities. While such long-context models also hold promise for biomedical image analysis\u2014potentially making transformers more efficient and effective when applied to high-resolution images\u2014a systematic study on this topic is lacking.\nIn this work, we investigate long-context models for biomedical imaging. We ask two questions: do medi-"}, {"title": "2. Related Work", "content": "Vision Transformers. The transformer, initially introduced for NLP, has been widely adapted and applied to vision tasks. ViT showed that a transformer architecture nearly identical to those used in NLP achieved strong performance on image recognition. Follow-on works adapted the transformer for specific vision tasks. For example, Swin introduced a shift-and-merge windowing scheme, wherein image patches only attended to local windows, reducing computational complexity and improving performance on pixel-level prediction. Similarly, PVT and Segformer introduced hierarchical transformer architectures designed for dense prediction tasks. Finally, work like DeiT introduced training and distillation strategies to improve the data efficiency of vision transformers.\nEfficient Attention. While transformers achieve strong performance, their self-attention operator scales quadratically with context length, leading to prohibitive computational demands for processing long-context inputs. In response, many works have proposed approaches to improve attention's efficiency. Flash attention is a popular approach that is an exact, hardware-aware implementation of attention, reproducing attention but with subquadratic scaling. Other approaches propose approximations to attention, including sparse and local attention, linear attention, and others. These approaches are more efficient than self-attention, but typically trade-off speed with expressivity and performance."}, {"title": "Alternatives to Attention", "content": "An alternative approach to making attention more efficient is to replace it entirely. This class of approaches tries to construct operators that maintain attention's performance while scaling more favorably with context length. For example, the Hyena operator leverages long convolutions to match self-attention's ability to capture global dependencies but with an operation that scales subquadratically with context length. Other approaches include state space models (SSMs), which take inspiration from traditional signal processing models. Gu and Dao (2023) recently proposed the selective SSM in a model called Mamba, which increases the expressivity of SSMs and achieves promising performance on NLP and audio tasks.\nSome of these alternatives have been evaluated for vision tasks. For example, early SSM models were adapted to image classification, Hyena showed proof-of-principal on ImageNet, and Mamba has been adapted for natural image processing. Similarly, related work has proposed new architectures leveraging some of these efficient operators for medical applications, however these applications typically focus on a single task and architecture instead of a systematic evaluation over many operators, tasks, and data types."}, {"title": "Image Resolution and Context Length", "content": "There is a growing body of evidence that context length and image resolution play key roles in the quality of representations learned by transformers. While not synonymous, image resolution and context length are closely linked, as smaller patches used to tokenize the image better preserve image resolution at the expense of increased context length.\nFor example, a study on masked autoencoding showed improved performance for increasing context length. Diffusion models have shown improved performance with decreased patch size. A recent work showed competitive performance tokenizing images at the pixel-level, a finding consistent with the results of this work and which further motivates our exploration of efficient alternatives to attention. Recent work in multimodal pretraining have found improved performance with higher-resolution images. A few studies have looked at the impact of ViT patch size on classification, finding improved performance with smaller patches. Finally, prior work has explored conceptually similar questions using CNNs. For example, several studies have highlighted the importance of preserving image resolution to achieve high CNN performance, and some work has suggested larger convolutional filter sizes improve CNN performance.\nSummary. While significant progress has been made improving transformer efficiency for long-context inputs in NLP, a systematic evaluation of the relationship between context length, efficiency, and performance in biomedical imaging is lacking. Further, many efficient operators have not been tested in common medical imaging settings (e.g., with 3D data, for improving image quality). We aim to fill these gaps by investigating the impact of context length and the performance of efficient attention alternatives on diverse biomedical imaging datasets, offering insights into the development of more efficient deep learning models for biomedical applications."}, {"title": "3. Approach", "content": "We begin with background on self-attention and the alternative operators we evaluate. We then discuss model architectures, our approach to changing context length, and our evaluation datasets."}, {"title": "3.1. Background: Attention and Alternatives", "content": "Self-Attention We show the standard transformer block in Figure 2, which is traditionally powered by self-attention. For an input sequence $X \\in \\mathbb{R}^{n \\times d}$, where n is the sequence length and d is the sequence dimension, self-attention maps this sequence to $Y \\in \\mathbb{R}^{n \\times d}$ using the set of trainable parameters $W_q \\in \\mathbb{R}^{d \\times d}$, $W_k \\in \\mathbb{R}^{d \\times d}$, $W_v \\in \\mathbb{R}^{d \\times d}$. First, the query, key, and value matrices are computed as $Q = XW_q$, $K = XW_k$, and $V = XW_v$. The softmax dot-product self-attention operation is then defined as:\n$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right) V$"}, {"title": "Alternatives to Attention", "content": "Many alternative operators have been proposed to enable longer context processing. To do a thorough analysis across tasks, datasets, and context lengths, we carefully selected which alternatives to evaluate. We selected operators that showed proof-of-principal performance on imaging tasks and outperformed similar baselines. Further, we selected operators that could be swapped out for attention in existing architectures, enabling a direct comparison between operators without confounding influences from other architectural changes.\nHyena. We selected the Hyena operator as the first attention alternative to evaluate (Poli et al., 2023) (Figure 2). Hyena uses long convolutions to achieve subquadratic scaling with respect to context length, while still maintaining token-level precision and global context. Hyena further introduces element-wise gating to inject data dependence into the operator, mimicking the data dependence property of self-attention. The computational complexity of Hyena is $O(n \\log_2(n))$.\nWe selected Hyena because it maintains two characteristics of attention\u2014token-level precision and global context\u2014that we hypothesized would help maintain performance on both sparse and dense image analysis tasks. Additionally, Hyena has shown"}, {"title": "3.2. Model Architectures", "content": "We evaluated two widely used architectures for vision: ViT and Swin . ViT closely mirrors transformers used"}, {"title": "3.3. Changing Context Length", "content": "Consistent with most transformers for computer vision, both ViT and Swin begin with a patch embedding layer that partitions the image into non-overlapping patches, which are then embedded and used as tokens. The context length of the self-attention operator is defined by how many tokens are processed concurrently. Thus, longer context lengths occur when attending to more image patches.\nWe can vary context length by (i) changing the patch size, thereby increasing the number of tokens per image region; or (ii) changing the size of the attention window, enabling attention among a greater portion of the image. We explore both in this work.\nTo change the context length in ViT, we swept the patch size used in the patch embedding layer. We evaluated 32-, 16-, 8-, and 4-pixel isotropic patches. Reducing the patch size increases context length and computational complexity, but results in a higher resolution representation of the input image . For Swin, we fixed the embedding patch size to 2-pixel isotropic patches while we varied the size of the local attention window. We evaluated 4-, 8-, and 16-token isotropic windows. Larger windows increase context length and computational complexity, but enable the network to use a greater portion of the image to inform each token's representation. In the Appendix, we also evaluate the impact of the patch size on Swin performance."}, {"title": "3.4. Dataset and Task Selection", "content": "We selected diverse biomedical imaging tasks to evaluate the impact of context length and self-attention. We included segmentation to evaluate the networks' ability to identify pixel-level features. We included image denoising as a task that requires models to restore high-fidelity details. Finally, we included classification to evaluate the networks' ability to aggregate global information and predict image-level labels. For each task type, we included 2D and 3D data from different imaging modalities. This comprehensive evaluation allowed us to analyze how context length and different operators influence performance across many datasets as well as tasks that require fine-grained precision and global understanding.\nOur tasks are visualized in Figure 3 and described below, with additional details in the Appendix.\n\u2022 2D Retinal Vessel Segmentation. This public fundus photograph dataset contains 800 images, each of shape 2048 \u00d7 2048 pixels with three channels. Each image has pixel-wise annotations of retinal vessels.\n\u2022 3D Abdominal CT Organ Segmentation. This public dataset contains 945 images, each with nine organs segmented. We resized each axial slice to 256 \u00d7 256 pixels and cropped to 64 axial slices per volume.\n\u2022 2D Microscopy Denoising. This public fluorescence microscopy dataset contains 360 images, each of shape 1024 \u00d7 1024. Each sample contains a paired high- and low-SNR image.\n\u2022 3D Cardiac MRI (CMR) Denoising. This private dataset contains 13,964 retro-gated cines, each with 32 frames and center cropped to 128 x 128 pixels. Each sample contains a paired high- and low-SNR image.\n\u2022 2D Pneumothorax Classification. This public chest x-ray dataset contains 18,887 chest x-rays, each of 1024 x 1024 pixels. 15% of the images contain a pneumothorax.\n\u2022 3D Pulmonary Embolism Classification. This public CT dataset contains 7,205 images, 32% positive for pulmonary embolism. We resized each axial slice to 256 \u00d7 256 pixels and cropped to 64 axial slices per volume."}, {"title": "4. Experiments", "content": "We first describe our experimental setup, then evaluate task performance and training efficiency as a function of context length."}, {"title": "4.1. Experimental Setup", "content": "We split the datasets randomly by patient into 60% train, 20% validation, and 20% test, except for the vessels dataset which had pre-defined splits. We tuned the learning rate for each experiment; final learning rates are given in the Appendix.\nWe trained the classification and segmentation tasks using the cross entropy loss and the denoising tasks using the sum of the mean squared error loss, Charbonnier loss, and Gaussian loss. We used an affine transform and brightness jitter as training augmentations for all tasks except CMR denoising, where we only used an affine transform. We did not use brightness jitter on CMR denoising since the pixel values are representative of the SNR.\nOther training parameters were kept constant for all experiments. We used the Adam optimizer with a one cycle learning rate scheduler and no weight decay. All experiments were run for 250 epochs on eight 80GB NVIDIA A100s using Python 3.11. Models were checkpointed using the minimum validation loss."}, {"title": "4.2. Task Performance", "content": "We next report the task performance for each network with changing context lengths and operators, as shown in Figures 4 and 5. We evaluated segmentation performance using the Dice coefficient, denoising performance using the structural similarity index measure (SSIM), and classification performance using the area under the receiver operating curve (AUROC). We computed 95% confidence intervals by bootstrapping over the test set.\nPatch Size Strongly Impacts ViT Performance. In Figure 4, we observe a strong relationship between patch size and performance. Using self-attention, the best performance across all tasks was achieved by the smallest patch size.\nWe notice a particularly strong correlation for pixel-level prediction, with all operators consistently"}, {"title": "4.3. Training Efficiency", "content": "We next evaluate training efficiency. While smaller patches can improve performance, they also increase computational complexity due to increased context length. For example, when training a self-attention-based ViT on our datasets, using 16- or 8-pixel patches increased the time required for a forward and backward pass by 252.90% and 2,335.48% compared to using 32-pixel patches, respectively. This drastic increase in computation with longer context lengths motivates the use of more efficient operators.\nTo assess the efficiency of each model, we evaluated the time required to perform a forward and backward pass as well as the maximum memory allocated. We provide results for all runs in the Appendix and summarize key findings in Tables 3 and 4, where we report"}, {"title": "5. Discussion and Conclusion", "content": "In this study, we evaluated the impact of context length on the performance and efficiency of transformers for biomedical image analysis. We further investigated two alternatives to self-attention\u2014Hyena and Mamba Vision\u2014on diverse imaging tasks.\nKey Findings. Our results indicate a strong relationship between patch size and task performance, particularly for pixel-level prediction tasks. Smaller patch sizes, which correspond to longer context lengths, consistently yielded better performance. This finding underscores the importance of preserving high-resolution information in biomedical images, which often contain critical fine-grained details necessary for accurate predictions.\nIn contrast, Swin's window size did not strongly impact performance, although denoising tasks showed some performance gains with larger windows. This suggests that while local context is crucial, Swin's hierarchical design may already provide a sufficient balance between local and global information for many tasks. In this case, dedicating more context length to preserving image resolution may be more impactful than extending context length to achieve larger attention windows.\nWe found both Hyena and Mamba Vision to be promising alternatives to self-attention that enable smaller patches and greater attention windows. For ViT pixel-level prediction tasks, we found that both operators could exceed the performance achieved"}, {"title": "Appendix A. Training Details", "content": "A.1. Hyperparameters\nWe tuned the learning rate for each experiment from {1e - 5, 1e - 4, 1e \u2013 3, 1e \u2013 2}. Selected learning rates are given in Table 5 and Table 6. We set batch size to maximize GPU memory. We required a minimum batch size of two to fit on the GPU to enable batch normalization layers.\nA.2. Data Preprocessing\nFor the retinal vessel segmentation dataset (Jin et al., 2022), we directly used the public data with no additional preprocessing. When training the Swin models, we resized the images to 1024 \u00d7 1024 to fit onto the GPU.\nFor the abdominal CT organ segmentation dataset, we used the images supplied by Antonelli et al. (2022) and segmentation masks supplied by Qu et al. (2024) for the aorta, gall bladder, kidneys, liver, pancreas, postcava, spleen, and stomach. We windowed the CT with a window level of 50 and window width of 400. We resized each axial image using linear interpolation to 256 x 256 and center cropped to 64 axial slices.\nFor the microscopy denoising dataset (Zhou et al., 2020), we treated each of the three supplied channels in the public dataset as different images. We selected a single frame from the widefield images as our low-SNR image and normalized each to zero mean and unit variance. We used the structured-illumination microscopy image as our paired high-SNR image, and scaled the high-SNR image using a least squares fit.\nFor the cardiac MR denoising dataset, we used images reconstructed in SNR units, meaning the amplitude of the signal in the reconstructed images is representative of its SNR. We added realistic MRI noise using an MRI noise model, reducing the SNR by a ratio selected from a uniform distribution between [1,40]. We center cropped each cine to 128 \u00d7 128 pixels and 32 frames.\nFor the pneumothorax dataset (Feng et al., 2021), we normalized each image between [0, 1].\nFor the pulmonary embolism dataset (Colak et al., 2021), we windowed the CT with a window level of 100 and window width of 700. We cropped around the lung region then resized each axial slice to 256 \u00d7 256 and center cropped the axial slices to 64 slices, ensuring the embolism was captured in the cropped region."}, {"title": "A.3. Model Implementation", "content": "We used the ViT and Swin implementations from Monai. We used the MambaVision implementation provided by the authors of the MambaVision paper, which calls code provided by the authors of the original Mamba paper. We used the Hyena implementation from a study on efficient language models , which provides a simple implementation of the method proposed in the Hyena paper."}, {"title": "A.4. Model Parameter Count", "content": "As discussed in Section 3, changing the patch size in ViT and local attention window in Swin changes the initial patch embedding parameters and task head parameters; otherwise, the backbone parameterization is largely unchanged. We report the number of parameters in the model for each experiment in Tables 7 and 8. An X in these tables indicates the configuration could not be run due hardware constraints."}, {"title": "Appendix B. Additional Results", "content": "B.1. Efficiency\nB.1.1. TRAINING TIMING\nTo assess runtime efficiency, we timed a forward and backward pass on a single NVIDIA A100 using a batch size of one. We only timed the backbone models (i.e., we did not include the linear, UNETR, or UPerNet task heads). We took the average of ten runs as the runtime reported in this work. We plot the runtime for each dataset and model configuration in Figures 6 and 7. Note that the abdominal CT dataset and chest CT embolism dataset have approximately the same runtime and the chest x-ray pneumothorax dataset and the microscopy denoising dataset have approximately the same runtime due to these pairs of datasets having the same image sizes. For Swin, the vessels dataset also has the same runtime as the microscopy and chest x-ray datasets since it was resized to train the Swin models.\n\u0392.1.2. \u039cAXIMUM MEMORY ALLOCATED\nTo assess memory efficiency, we recorded the maximum memory allocated on a single NVIDIA A100 using a batch size of one. We only assessed the backbone models (i.e., we did not include the linear, UN-"}, {"title": "B.2. Additional Results on Swin", "content": "B.2.1. SWIN PATCH SIZE\nIn the main text, we discussed how context length can be varied by either changing the patch size or attention window. We varied patch size on ViT, while we kept the patch size constant for Swin and instead varied the attention window. In this section, we evaluate the impact of patch size on Swin performance. Specifically, we investigated tokenizing the image with 4-pixel patches instead of 2-pixel patches (as used in the main text). We evaluated performance on all tasks using self-attention with a window size of eight and report the results in Table 9. For segmentation, we report Dice; for denoising, we report SSIM; and for classification, we report AUROC. 95%"}]}