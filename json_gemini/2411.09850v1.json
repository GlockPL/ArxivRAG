{"title": "ENHANCING DIFFUSION POSTERIOR SAMPLING\nFOR INVERSE PROBLEMS BY INTEGRATING CRAFTED\nMEASUREMENTS", "authors": ["Shijie Zhou", "Huaisheng Zhu", "Rohan Sharma", "Ruiyi Zhang", "Kaiyi Ji", "Changyou Chen"], "abstract": "Diffusion models have emerged as a powerful foundation model for visual genera-\ntion. With an appropriate sampling process, it can effectively serve as a generative\nprior to solve general inverse problems. Current posterior sampling based methods\ntake the measurement (i.e., degraded image sample) into the posterior sampling to\ninfer the distribution of the target data (i.e., clean image sample). However, in this\nmanner, we show that high-frequency information can be prematurely introduced\nduring the early stages, which could induce larger posterior estimate errors dur-\ning the restoration sampling. To address this issue, we first reveal that forming\nthe log posterior gradient with the noisy measurement ( i.e., samples from a dif-\nfusion forward process) instead of the clean one can benefit the reverse process.\nConsequently, we propose a novel diffusion posterior sampling method DPS-CM,\nwhich incorporates a Crafted Measurement (i.e., samples generated by a reverse\ndenoising process, compared to random sampling with noise in standard methods)\nto form the posterior estimate. This integration aims to mitigate the misalignment\nwith the diffusion prior caused by cumulative posterior estimate errors. Exper-\nimental results demonstrate that our approach significantly improves the overall\ncapacity to solve general and noisy inverse problems, such as Gaussian deblur-\nring, super-resolution, inpainting, nonlinear deblurring, and tasks with Poisson\nnoise, relative to existing approaches.", "sections": [{"title": "1 INTRODUCTION", "content": "Diffusion models (Ho et al., 2020) have achieved remarkable generative performance on images\n(Amit et al., 2021; Baranchuk et al., 2021; Brempong et al., 2022), videos (Singer et al., 2022; Wu\net al., 2023), audios (Popov et al., 2021; Yang et al., 2023a), natural language (Austin et al., 2021;\nHoogeboom et al., 2021; Li et al., 2022) and molecular generation (Hoogeboom et al., 2022; Jing\net al., 2022). Besides its strong modeling capacity for complex and high dimensional data, diffusion\nmodels have exhibited a strong generative prior to form the diffusion conditional sampling (Song\net al., 2020) that can be harnessed for diffusion posterior sampling. In the context of noisy inverse\nproblems, this sampling process effectively approximates precise data distributions from noisy and\ndegraded measurements. Noisy inverse problems, such as super-resolution, inpainting, linear and\nnonlinear deblurring, are targeted to restore an unknown image x from its noise-corrupted mea-\nsurement y given the corresponding forward measurement operators A(\u00b7) : Rn \u2192 Rm. Recently,\nDiffusion models have been extensively utilized for these tasks (Zhu et al., 2023; Song et al., 2022;\nWang et al., 2022), offering a robust framework for reconstructing high-quality images from de-\ngraded measurements.\nCurrent diffusion-based methods generally employ two distinct strategies to solve inverse problems.\nThe first strategy is to train problem-specialized diffusion models (Saharia et al., 2022; Whang et al.,\n2022; Luo et al., 2023; Chan et al., 2023) given measurements and clean image pairs. In contrast,\nmethods of the second strategy only capitalize on problem-agnostic pre-trained diffusion models to\nbenefit the zero-shot diffusion restoration sampling by posterior estimate (Song et al., 2023a; Rout"}, {"title": "2 BACKGROUND", "content": ""}, {"title": "2.1 DIFFUSION MODELS", "content": "Diffusion models (Ho et al., 2020; Song et al., 2020) comprise two processes: a forward noising pro-\ncess in which the noise is progressively injected into the sample and a reverse denoising process for\ngeneration. Specifically, the forward process of Denoising Diffusion Probabilistic Models (DDPM)\n(Ho et al., 2020) can be formulated by the variance preserving stochastic differential equation (VP-"}, {"title": "3 METHOD: DIFFUSION POSTERIOR SAMPLING WITH CRAFTED\nMEASUREMENTS", "content": "We propose DPS-CM, which performs diffusion posterior sampling from p (xt yt) with Crafted\nMeasurement yt belonging to another diffusion reverse trajectory {y}=0 instead of the vanilla"}, {"title": "3.1 DPS AMPLIFY POSTERIOR SAMPLING ERRORS", "content": "input y. Specifically, applying Eq.4 on \u00e6t and yt, we incorporate the tractable p (yo | 20) as the\nlikelihood approximation of p (yt | xt) to enable the posterior sampling from p(xt | yt). DPS-\nCM can be interpreted as pushing the bond between two noisy approximations \u011do and 20 as the\nmeasurement model \u0177o = A (20). While \u011do is progressively recovered to the input measurement y\nvia its diffusion denoising process, 20 is expected to develop into the target ground truth \u00e6 gradually\nduring the synchronous reverse process with the measurement bond \u0177o = A (20). In Section 3.1,\nwe explore the advantage of posterior sampling utilizing noisy measurements yt sampled from the\ndiffusion forward process over using the clean input y in reducing intermediate generation errors in\ndiffusion posterior sampling. In Section 3.2, we formulate the DPS-CM in detail.\nOur claim is that solving inverse problems in the posterior sampling manner follows similar fre-\nquency dynamics with the regular denoising process in that it recovers the low-frequency (structural,\ndomain-independent) components at first and the high-frequency (details, domain-dependent) com-\nponents later. The posterior sampling from pt (xt | y) in DPS is inevitable adding high-frequency\nsignals from the gradient of the approximate log-likelihood \u2207 log pt (y | 20) during the early\nstages due to the frequency and noise discrepancy between the sharp measurement y and interme-\ndiate noisy reconstruction 20. It will create a distorted frequency dynamic which leads to a larger\nposterior sampling error.\nTo verify our assumption, we conduct frequency dynamic analysis of different posterior sampling\nmethods for solving inverse problems in Fig.1a. Afterward, the posterior sampling error is illustrated\nvia the visualization of the e-prediction error and intermediate reconstruction error dynamics in\nFig.1b and Fig.1c respectively. We conduct the diffusion posterior sampling from pt (xt | yt) with\nnoisy measurement DPSy\u0165 in comparison with DPS to illustrate the advantage of the likelihood\napproximation with less high-frequency elements.\nGiven the noisy measurement\nconstructed by the diffusion forward process starting from the clean measurement y as in Eq.\n3, we replace y with yt to form DPSy\u2081 with the posterior sampling from pt (xt | yt). The corre-\nsponding likelihood approximation pt (Yt | 20) \u2243 Pt (Yt | xt) involves yt and \u00e6t with the similar\nfrequency pattern and noise level.\nWe verify our assumption on frequency\ndynamic of diffusion posterior sampling by transforming the gradient of log posterior (i.e.,\n\u221ax+log p (xt | y) in the case of DPS) into spectral space as the gradient of log posterior serves\nas the main update for each posterior sampling step in Eq.5. We split the low-frequency and high-\nfrequency areas in the spectral space of the gradient with the low-high cut-off frequency of 32Hz\nand visualize the mean magnitude ratio between them of each t shown in Fig.la.\ne-prediction error and intermediate reconstruction error. Since there exists no ground truth for\nthe intermediate generation of diffusion posterior sampling for inverse problems, we measure the\nposterior sampling error implicitly by computing e-prediction error and intermediate reconstruction"}, {"title": "3.2 ENHANCING POSTERIOR SAMPLING BY INTEGRATING CRAFTED NOISY\nMEASUREMENTS", "content": "DPSy introduced in Section 3.1 fits the diffusion model e\u0473(\u00b7,t) adaptively and produces smaller\ne-prediction errors, but fails to recover high-frequency components (details, domain-dependent sig-\nnals) during the late denoising stage. Besides the random noise obstructing the generation of real\ndetails, this failure also origins from the intractable likelihood term p (yt | xt) in Eq.7:\nApparently, there exists no explicit dependency between the noisy measurement yt and \u00e6t nor 20\napplying Eq.4 given the measurement model. Thus, V\u00e6\u2081 log p (yt | 20) is an inadequate estimate\nfor V\u00e6 log p (xt | yt). In order to leverage the advantage of DPSy\u2081 while exploiting the tractable"}, {"title": "4 EXPERIMENTS", "content": ""}, {"title": "4.1 EXPERIMENTAL SETTINGS", "content": "Evaluation datasets and baselines. We evaluate our proposed posterior sampling method DPS-CM\non the validation set of FFHQ 256\u00d7256 and ImageNet 256\u00d7256 datasets by applying DDPM sam-\npling with 1000 timesteps. For the diffusion prior, the pre-trained diffusion models for FFHQ and\nImageNet are taken from DPS (Chung et al., 2022a) and ablated diffusion model (ADM) (Dhariwal\n& Nichol, 2021) respectively. Detailed hyperparameter {$t, wt, \u03bc} settings of DPS-CM are shown\nin Appendix A.1. We compare our method with several recent state-of-the-art approaches including\nDPS (Chung et al., 2022a), Denoising Diffusion Restoration Models (DDRM) (Kawar et al., 2022),\nDiffPIR (Zhu et al., 2023), Optimal Posterior Covariance (OPC) (Peng et al., 2024), and also latent\ndiffusion-based methods: Posterior Sampling with Latent Diffusion (PSLD) (Rout et al., 2024) and\nReSample (Song et al., 2023a). We also compare DPS-CM with FPS-SMC (Dou & Song, 2023)\nand LGD-MC (Song et al., 2023b) for ablation study in Section.4.3 as they also improve posterior\nsampling with augmented yt or x. Although FPS-SMC relies on the separable Gaussian kernel for"}, {"title": "4.2 QUANTITATIVE RESULTS", "content": "The quantitative results of DPS-CM and baselines on these noisy linear and nonlinear inverse prob-\nlems are shown in Table 1, 2, and Table 3. Besides, the performance of DPS-CM for measure-\nments with Poisson noise is shown in Appendix C. We can observe that DPS-CM can achieve the\noverall best performance over four metrics when solving deblurring and super-resolution problems\nand significantly outperform baselines on random/box inpainting problems. While the performance\nand stability of DPS will significantly drop when performing Gaussian/Motion deblurring on the\nImageNet dataset, DPS-CM with the crafted measurements can instead form more precise and ro-\nbust posterior sampling reflected from the consistent performance over these two datasets. Optimal\nPosterior Covariance (OPC), an improved posterior estimate by constructing the optimal posterior\ncovariance instead of improving over the expectation, is no better than DPS-CM but still achieves\ncomparable performance with our method. For latent diffusion-based baselines PSLD and Resam-\nple, It is interesting that they can handle the more challenging inverse problems on the ImageNet\nbetter than the FFHQ dataset, which exhibits the advantage of latent diffusion capturing on more\ncomplex visual distributions and semantic patterns. Remarkably, our method exceeds baselines with\ndifferent diffusion priors when solving motion deblurring problems, shown in examples of Figure\n3b. Besides, DPS-CM outperforms DPS in solving ill-posed problems, such as nonlinear blurring\nand inverse problems with Poisson noise as shown in Table 3 and Appendix C. In Fig.3, 4, and 5a,\ncompared with baselines on different noisy linear inverse problems, DPS-CM shows high-quality\nreconstructions with mild distortion and precise detail recovery. For example, DPS-CM has better"}, {"title": "4.3 ABLATION STUDIES", "content": "Influence of hyperparameter \u03bc. Here, we want to investi-\ngate the influence of hyperparameter \u03bc\u2208 [0, 1], which controls\nthe proportion of proposed \u2207xt log pt (yt | xt) in the sampling\nof DPS-CM. When \u03bc = 0.0, it falls into DPS. Specifically,\nwe can observe the performance (PSNR) of DPS-CM on box\ninpainting on ImageNet with different u in Figure 6. DPS-\nCM achieves the best results when \u03bc = 0.5 which indicates\nthe benefits of the interplay between V\u00e6r log pt (yt | xt) and\nx log pt (yxt). Besides, pure DPS-CM with \u03bc = 0\nleads to a better log posterior gradient estimate and outper-\nforms DPS-CM with \u03bc = 0.0.\nCompare crafted measurements with other augmented posterior estimate methods. Here, we\nfurther validate the effect of our crafted measurements {yt}t_r compared with DPSy\u2081, where yt\nin the posterior is an i.i.d sample from the forward process p (yt | Yo) same as Eq.3, LGD-MC\n(Song et al., 2023b), which is a loss guided diffusion with \u00e6(2) i.i.d. sampled from p (xo | xt) for the"}, {"title": "5 CONCLUSION", "content": "This work introduces Diffusion Posterior Sampling with Crafted Measurements to solve general and\nnoisy inverse problems. DPS-CM leverages an additional diffusion reverse process to form a crafted"}]}