{"title": "ENSEMBLE TOT OF LLMS AND ITS APPLICATION TO AUTOMATIC GRADING SYSTEM FOR SUPPORTING SELF-LEARNING", "authors": ["Yuki Ito", "Qiang Ma"], "abstract": "Providing students with detailed and timely grading feedback is essential for self-learning. While existing LLM-based grading systems are promising, most of them rely on one single model, which limits their performance. To address this, we propose Ensemble Tree-of-Thought (ToT), a framework that enhances LLM outputs by integrating multiple models. Using this framework, we develop a grading system. Ensemble ToT follows three steps: (1) analyzing LLM performance, (2) generating candidate answers, and (3) refining them into a final result. Based on this, our grading system first evaluates the grading tendencies of LLMs, then generates multiple results, and finally integrates them via a simulated debate. Experimental results demonstrate our approach's ability to provide accurate and explainable grading by effectively coordinating multiple LLMs.", "sections": [{"title": "1 Introduction", "content": "Since the release of ChatGPT in 2022, generative AI has gained significant popularity, with many students increasingly adopting it as a learning aid for various purposes. A survey on students' use of AI for learning identified several common applications, such as brainstorming, generating ideas, summarizing readings, and analyzing large datasets [Chan and Hu, 2023]. Among these, AI's ability to provide personalized and immediate learning support is particularly valued. Notably, some students in the survey reported using AI to obtain grading and its explanations on their completed homework. They found these grading comments helpful for enhancing their depth of thinking and understanding. This highlights the importance of providing students with detailed, point-by-point feedback on their homework to facilitate self-learning.\nHowever, generative AI responses are not always reliable, raising concerns about the potential for misleading feedback. While feedback from human teachers is ideal for guiding student self-learning, it is often impractical for teachers to provide individualized and immediate support to every student. In this context, automatic grading systems emerge as a practical solution. These systems evaluate student answers and deliver timely grading comments, bridging the gap without placing additional burdens on teachers.\nBefore this work, several studies have proposed automatic grading methods. For example, Gobbo et al. introduced GradeAid, a framework that evaluates student answers by extracting both lexical and semantic features to determine grading scores [del Gobbo et al., 2023]. Similarly, Huang et al. developed a system to grade reading comprehension answers [Huang et al., 2023]. Their system not only evaluates student answers but also provides auto-generated hints to guide students toward correct answers."}, {"title": "2 Related Work", "content": "Recent advancements in large language models (LLMs) have explored strategies to leverage the collective strengths of multiple models, enhancing performance and reliability across various tasks.\nFor example, Zhang et al. introduce the Chain-of-Agent (CoA) framework, which utilizes multi-agent collaboration in natural language [Zhang et al., 2024]. In CoA, multiple LLM agents communicate to process different segments of text. Then, a manager agent synthesizes their contributions into the final output. With each agent processing short contexts, CoA can handle longer contextual input more effectively than conventional methods. It has demonstrated significant improvements (up to 10%) in tasks such as question answering, summarization, and code completion, when compared to strong baselines like retrieval augmented generation (RAG), full-context models, and multi-agent LLMs.\nThe LLM-Synergy framework [Yang et al., 2023] proposes techniques like boosting-based weighted majority voting and cluster-based dynamic model selection to enhance accuracy in medical QA datasets. This approach achieves state-of-the-art results, including 38.01% accuracy on MedMCQA, 96.36% on PubMedQA, and 38.13% on MedQA-USMLE, outperforming other methods across all three datasets.\nAdditionally, the Formal Debate framework (FORD) [Xiong et al., 2023] introduces a structured three-stage debate process to facilitate effective collaboration among LLMs. While the integrated results are not always optimal, the study demonstrates the potential of debates to improve performance even when there are inconsistencies between LLMs' outputs.\nThese methods highlight the potential of multi-LLM collaboration in addressing challenges related to performance and reliability.\nBased on the insights provided by these studies, our study explores a new approach to achieving multi-LM cooperation. Unlike the aforementioned frameworks, our system introduces two key elements: pseudo-learning, which pre-evaluates the performance trends of LLMs, and debate integration, which synthesizes LLMs' outputs through discussions. Through these mechanisms, our system aims to effectively address performance and reliability challenges."}, {"title": "2.2 Automated Short Answer Grading", "content": "The application of a LLM in education has also received significant attention. Automated short answer grading (ASAG) field is no exception. ASAG targets the grading of student answers to open-ended questions, which are typically a few sentences long. In this section, we introduce such technologies, by categorizing existing grading methods into two primary approaches: score or label prediction and detailed feedback generation."}, {"title": "2.2.1 Grading Methods for Scores and Labels Prediction", "content": "Numerous studies have explored grading methods that focus on assigning scores or labels to student answers. For example, Gobbo et al. propose GradeAid, a framework to grade short student answers. It gives numeric scores to student answers by extracting lexical and semantic features [del Gobbo et al., 2023] from them. Lexical features are calculated using TF-IDF, while semantic features are derived using a BERT-Cross Encoder. These features are concatenated into a single vector and processed through a regression model to predict the final grading score. GradeAid demonstrates effective grading across various datasets, while its performance consistency depends on the specific questions and datasets used. Another approach utilizes ensembles of pre-trained BERT models for scoring tasks [Ormerod, 2022]. This study shows that combining multiple models can improve score prediction accuracy.\nWhile BERT models are used in various grading tasks, GPT models are also becoming popular these days. For example, Chang et al. employ GPT-3.5 and GPT-4 for scoring and label classification (pass/fail) of Finnish student answers [Chang and Ginter, 2024]. This research found that GPT-4 performs well in the grading tasks, especially in the one-shot setting. But they also admit the necessity of further performance improvement before introducing this kind of technology to the real education situation.\nAutomated grading can reduce not only the burden on teachers but also the subjectivity inherent in manual grading. Gobrecht et al. indicate that automated scoring methods can enhance fairness and equity in evaluations [Gobrecht et al., 2024]. They fine-tuned a transformer-based scoring system and evaluated student answers which already assigned grades by human graders. Then, human graders grade the same answers again. Finally, these results are compared with the past grading to evaluate the grading consistency. This experiment reveals that the model has less deviation from the past grading results than the human re-graders."}, {"title": "2.2.2 Grading Methods for Detailed Feedback Generation", "content": "Some methods generate detailed grading reasons alongside scores and labels to enhance transparency and usability. For example, Huang et al. proposes a method that automatically evaluates answers using a fine-tuned GPT model and generates feedback to guide students to the correct answers [Huang et al., 2023]. We have also proposed a technique to automatically generate grading labels and reasons by combining a single LLM and a grammatical structure analysis technique [Ito and Ma, 2025].\nIn addition, Lee et al.'s study [Lee et al., 2024] uses a prompt construction method called Chain-of-Thought (CoT) to perform reasoned automatic grading with GPT-3.5 and GPT-4 [Lee et al., 2024]. CoT prompting is a way to improve LLM outputs by encouraging step-by-step reasoning. In this system, the LLM generates evidence that supports its grading decisions before assigning a label (e.g., Proficient, Developing, or Beginning). This approach enables GPT-4 to provide explanations for its assigned labels, improving interpretability. Another study investigated GPT-4's potential for grading middle school level answers [Jiang and Bosch, 2024]. Their method generates both grading scores and rationale paragraphs, by referring to the question, student answer, scoring rubrics, and additional information where applicable. GPT-4 achieved a quadratic weighted kappa of 0.677 across 10 questions. While this experimental result shows a high potential of auto-grading by the model, it is also mentioned that grading outcomes varied across subjects (e.g., biology, English), highlighting the instability.\nSome studies propose grading frameworks that can work with various types of LLMs. For example, Jordan et al. developed an automated grading framework: FreeText [Matelsky et al., 2023]. It gets a question, grading criteria, and a student answer, then generates personalized feedback for the student. In this framework, users can freely select a grading model. Fateen et al. propose another system, ASAS-F-RAG, which integrates retrieval-augmented generation (RAG) with automated grading [Fateen et al., 2024]. When grading student answers, this method retrieves the top 3 to 5 most similar examples from past grading data using the ColBERT retriever. These examples are given to the LLMs as grading examples. This method achieves accurate generation of scores, labels (e.g., Correct, Partially Correct, or Incorrect), and feedback. They also employ several LLMs in their experiments, indicating that their system achieves accurate grading without depending on a specific LLM."}, {"title": "2.2.3 Dataset Contributions for Automated Grading", "content": "While many studies focus on label and score prediction, fewer studies address the generation of grading reasons. This discrepancy stems from the limited availability of datasets that include detailed grading reasons. To address this, several works have introduced new datasets to support research on ASAG:\nSAF Dataset [Filighera et al., 2022]: Filighera et al. construct the SAF dataset collected from college-level network communication classes. It includes questions, reference answers, student answers, grading labels, scores, and grader feedback. They demonstrated the dataset's utility by fine-tuned models like T5-base and mT5-base to jointly predict scores, labels, and feedback.\nEngSAF Dataset [Aggarwal et al., 2024]: Aggarwal et al. construct another dataset: EngSAF. It has a similar structure to the SAF, but it expands the scope with a larger data size and broader topics across various courses. Experiments using EngSAF involved both fine-tuned models (e.g., LLaMA-2-13B-chat, Mistral-7B-Instruct) and zero-shot evaluations with GPT-3.5-turbo. These models generated output labels and feedback by comparing student answers to reference answers.\nThese datasets are crucial for advancing the development of systems that prioritize reason generation alongside grading."}, {"title": "2.2.4 Comparison to Our Approach", "content": "Tables 1 and 2 summarize the features and techniques of the reviewed methods, contrasting them with our system.\nAs shown in Table 1, our system focuses on label prediction and reason generation, excluding score prediction. This design choice aligns with our ultimate goal: providing effective feedback to support student self-learning rather than determining official grades.\nTable 2 highlights how our system distinguishes itself from prior methods. Unlike approaches that rely on a single model, our method leverages multiple LLMs to enhance grading accuracy. This is achieved through a combination of ensemble learning and the Tree of Thought (ToT) approach. By collaborating multiple LLMs, we can compensate for the weaknesses of individual models. This results in more consistent and balanced grading outcomes, without relying on a specific LLM."}, {"title": "3 Proposed Framework and System", "content": "This section first introduces the core idea, Ensemble ToT. Then, it defines the problem that the GET system is designed to solve. Finally, it provides a detailed explanation of the system's overall structure."}, {"title": "3.1 Ensemble ToT", "content": "Inspired by ensemble learning and Tree-of-Thought (ToT) [Yao et al., 2023], we propose a novel framework, Ensemble ToT, for utilizing multiple LLMs.\nEnsemble learning is a machine learning technique that combines the predictions from multiple models to improve accuracy [Ibomoiye and Sun, 2022]. One common method, Stacking [Wolpert, 1992], involves training multiple models on the same dataset and then training a new model to combine their outputs. This new model generally produces higher-quality results by leveraging the strengths of the individual models.\nIn contrast, ToT is a framework designed to help LLMs solve problems by breaking them down into a series of intermediate thought steps. For each step, ToT guides the LLM to generate multiple candidate solutions. These candidates are then evaluated using heuristics or voting mechanisms to select the most promising options. By iteratively exploring multiple reasoning paths, ToT organizes the problem-solving process into a tree-like structure, allowing LLMs to explore various paths simultaneously.\nEnsemble ToT builds upon Stacking and ToT. First, it identifies the performance tendencies of LLMs on tasks similar to the target problem. This phase mirrors the training phase in Stacking. However, we do not actually train the LLMs, since they have large size, and fine-tuning them is resource-intensive. To address this, we design an alternative process called pseudo-learning, as described in Section 3.3.\nSecond, based on the ToT framework, Ensemble ToT generates multiple candidate solutions using several LLMs. While the original ToT uses a single LLM to generate multiple candidates, Ensemble ToT utilizes different LLMs for each candidate. This increases the diversity of the candidates, leading to a wider range of solutions. In the GET system, this is implemented in the multi-LLM grading phase, detailed in Section 3.4.\nFinally, Ensemble ToT combines the candidate solutions while considering the identified performance tendencies of the LLMs. This process corresponds both to ensemble learning, which combines the outputs of multiple models through a newly trained model, and to ToT, which merges thought candidates using heuristics or voting mechanisms. In Ensemble ToT, an LLM combines the performance tendencies and candidate solutions to select the best possible solution. The GET system implements this process in the debate integration phase, as explained in Section 3.5.\nThe following section provides a detailed explanation of the GET system, which implements the Ensemble ToT framework."}, {"title": "3.2 Problem Definition", "content": "The GET system's objective is to grade short student answers. To define this problem, the symbols used are explained in Table 3.\nThe problem addressed by the GET system is to grade student answers based on the question content and the corresponding reference answers. Specifically, for the input tuple $(q_i, r_i, s_{i_j})$, the system predicts a pair consisting of a grading label and a grading reason, $(gl_{i_j}, gr_{i_j})$.\nMathematically, this task can be represented as follows:\n$(gl_{i_j}, gr_{i_j}) = GET(q_i, r_i, s_{i_j})$\nThe GET system tackles this task through three steps: pseudo-learning, multi-LLM grading, and debate integration."}, {"title": "3.3 Pseudo-Learning", "content": "Pseudo-learning aims to evaluate the capabilities and grading tendencies of LLMs employed in the system. It is conducted before grading student answers. The process comprises two steps: grading past-data and tendencies identification."}, {"title": "3.3.1 Grading Past-Data", "content": "In the grading past-data step, each LLM independently grades answers from an existing dataset $D'$, such as historical student answers with grading results. Then it computes the LLMs' performance metrics.\nFormally, each LLM predicts grading label and reason pairs $(gl'_{i_j}, gr'_{i_j})$ based on the input tuples $(q'_i, r'_i, s'_{i_j}) \\in D'$. The predicted grading labels (denoted as $pl'_{i_j}$) are then evaluated against ground truth $gl'$, using performance metrics, including accuracy and macro F1-score.\nAccuracy is defined as the proportion of accurately classified samples to the total number of samples:\n$Accuracy = \\frac{Number \\ of \\ Accurate \\ Predictions}{Total \\ Number \\ of \\ Predictions}$\nThe macro F1-score is calculated as the arithmetic mean of F1-scores for all three grading labels in GL, ensuring equal weight for each class, regardless of its frequency. For each label $gl$, the F1-score is defined as:\n$F1_{gl} = \\frac{2 \\cdot Precision_{gl} \\cdot Recall_{gl}}{Precision_{gl} + Recall_{gl}}$\nwhere:\n$Precision_{gl} = \\frac{TP_{gl}}{TP_{gl} + FP_{gl}} \\quad Recall_{gl} = \\frac{TP_{gl}}{TP_{gl} + FN_{gl}}$\nTrue Positives ($TP_{gl}$): The number of samples predicted as belonging to grading label $gl$ and having $gl$ as their true label.\nFalse Positives ($FP_{gl}$): The number of samples predicted as belonging to grading label $gl$ but having true labels from other classes.\nFalse Negatives ($FN_{gl}$): The number of samples with a true label of $gl$ but predicted as belonging to another grading label.\nThe macro F1-score is then computed as:\n$Macro \\ F1 = \\frac{1}{|GL|} \\sum_{gl \\in GL} F1_{gl}.$\nThroughout this study, these metrics are calculated using Scikit-learn [Pedregosa et al., 2011]."}, {"title": "3.3.2 Tendencies Identification", "content": "Tendencies identification analyzes the strengths, weaknesses, and grading biases of each LLM based on the performance metrics calculated in the previous step. The analysis is conducted from two perspectives:\nLabeling tendencies compared to other LLMs (e.g., an LLM excels at identifying errors but is overly strict).\nOutput pattern analysis for the most likely label prediction (e.g., if LLM1 and LLM2 assign Correct while only LLM3 assigns Partially correct, the ground truth is Partially correct in many cases).\nFor labeling tendencies, an LLM interprets the numeric metrics and explains its findings in natural language. The prompt is shown in Fig. 3. Through this process, numerical data are transformed into text form. LLMs are generally more effective at processing natural language than numerical data. This transformation is expected to ensure the results of pseudo-learning can be utilized more effectively in the subsequent processes."}, {"title": "3.4 Multi-LLM Grading", "content": "The multi-LLM grading is the phase to generate multiple grading result candidates of the actual student answers.\nThis task can be formally defined as follows: Given a question, reference answer, and student answer $(q_i, r_i, s_{i_j})$ as the input, each LLM predicts the grading label and reason $(gl_{i_j}, gr_{i_j})$. This process finally generates three patterns of the predicted label and reason.\nMulti-LLM grading involves two steps: few-shot selection and independent grading, as shown in Fig. 4. First, few-shot selection chooses three grading examples from the dataset $D'$. In independent grading, three LLMs separately grade the student answers based on these examples. Finally, the grading results are forwarded to the debate integration phase (illustrated in Fig. 5 and detailed in Section 3.5)."}, {"title": "3.4.1 Few-shot Selection", "content": "We employ a three-shot prompt (instructions provided to LLMs with three output examples) for the grading task. This approach is motivated by the findings of Fateen et al. [Fateen et al., 2024], who demonstrated that selecting appropriate examples for each student answer using ColBERT [Khattab and Zaharia, 2020] significantly enhances grading performance. Based on their study, the GET system incorporates a retrieval phase to search for suitable grading cases. This process involves the following steps:\nRetrieve relevant grading examples:\nIf the question $q_i$ has been posed to students in $D'$, retrieve the corresponding grading examples: $(q_i, r_i, s'_{i_j}, gl'_{i_j}, gr'_{i_j})$.\nOtherwise, gets all data $(q_k, r_k, s'_{k_1}, gl'_{k_1}, gr'_{k_1}) \\in D'$.\nPrepare data for embedding: Concatenate all items in each grading example into a single string.\nGenerate embeddings:"}, {"title": "3.4.2 Independent Grading", "content": "After selecting examples, three different LLMs independently grade the student answer $s_{i_j}$ by providing both a grading label and reason.\nThe grading prompt is illustrated in Fig. 6 and Fig. 7. We created this prompt by referring to the GitHub code published by Fateen et al [Fateen et al., 2024]. The prompt is almost similar to that used in the grading past-data of the pseudo-"}, {"title": "3.5 Debate Integration", "content": "After multi-LLM grading by three LLMs, one of them integrates the results into a unified grading conclusion. For this task, the LLM with the highest macro F1 score in the pseudo-learning phase, designated as the Skilled Expert Grader, is utilized.\nThe debate integration process has three steps: initial debate, quality validation, and debate retry, as shown in Fig. 5"}, {"title": "3.5.1 Initial Debate", "content": "In the initial debate, the selected LLM simulates a debate among the graders by completing a template as shown in Fig. 8. The debate follows four stages: Ice Break, Divergence, Conversion, and Voting. This debate construction is inspired by the work of Dong et al. who propose an effective way to facilitate debates using LLMs [Dong et al., 2024].\nThe statements made by the three graders during the debate are not the actual outputs of three separate LLMs. Instead, they are generated by a single LLM. This single LLM generates the statements based on the grading results produced during the multi-LLM grading stage and the grading tendencies identified in the pseudo-learning phase.\nAdditionally, if the question $q_i$ exists in the past dataset $D'$, the three examples selected during the few-shot selection stage are also included as the input. Conversely, if $q_i$ is not present in $D'$, no examples are provided. This approach prevents the LLM from over-relying on past data when grading new questions, thereby ensuring high-quality performance across diverse datasets."}, {"title": "3.5.2 Quality Validation", "content": "The quality validation phase ensures the quality and correctness of the results integrated in the previous step. The same LLM used in the debate is employed for this validation.\nAs the input, it takes $q_i, r_i, s_{i_j}$, the most likely label for $s_{i_j}$ (same as passed in the debate integration phase), and the integrated grading label and reason to be evaluated. Additionally, if the question $q_i$ exists in the historical dataset $D'$, the grading result generated by it during independent grading is also provided as a grading example. Conversely, if $q_i$ is not found in $D'$, no examples are given. This is because the grading performance of each LLM can vary significantly depending on the question. Notably, the best-performing LLM in pseudo-learning does not necessarily excel when grading a novel question $q_i$."}, {"title": "3.5.3 Debate Retry", "content": "The debate retry phase is executed only if the integrated grading result is deemed invalid in the quality validation phase. In this phase, the LLM compares the first debate result and revised grading from the quality validation. This process is also done by simulating debate among three graders. The debate template for this is shown in Fig. 9. The output from this phase is determined as the final conclusion."}, {"title": "4 Experimental Setup", "content": "We evaluated our system's grading performance through automated assessments. These assessments measured its ability to predict grading labels and provide valid reasoning. This section outlines the experimental settings."}, {"title": "4.1 Dataset", "content": "We used the Short Answer Feedback (SAF) Dataset [Filighera et al., 2022] for evaluation. This dataset targets a college-level communication network course and contains questions, reference answers, student answers, grading labels (Correct, Partially correct, Incorrect), and grader feedback.\nIt is divided into four subsets: train, validation, test-unseen-answers (UA), and test-unseen-questions (UQ). The subsets contain 1700, 427, 375, and 479 entries, respectively. The train set and UA set contain the same questions but include different student answers. In contrast, the train set and UQ set have neither overlapping questions nor student answers. We used the train set for pseudo-learning and the UA and UQ sets to evaluate system performance."}, {"title": "4.2 Evaluation Methods", "content": "We evaluated grading label and reason performance using statistical metrics, following previous studies."}, {"title": "4.2.1 Statistic Evaluation", "content": "Since grading label prediction is a task of classifying student answers into three categories (Correct, Partially correct, and Incorrect), accuracy and macro F1 score were used as evaluation metrics. The definitions of these metrics are shown in Eq. 2 and Eq. 5.\nFor the generation of grading reasons, the grader feedbacks included in the SAF dataset were used as a reference, and the results were evaluated using BLEU [Papineni et al., 2002], ROUGE-2 [Lin, 2004], and BERTScore [Zhang et al., 2020]. However, BLEU and ROUGE measure word similarity rather than semantic similarity, which may not fully reflect the quality of the grading reasons [Fateen et al., 2024]. BERTScore also uses word embeddings, so it tends to be higher when the LLM reproduces the same phrases as the dataset's grading reasons. Therefore, these metrics are shown for reference only, and the quality of the grading reasons will be mainly evaluated using LLMs, as described below."}, {"title": "4.2.2 LLM-Based Evaluation", "content": "To address the limitations of statistical evaluation for grading reasons, we introduced an automated evaluation process using LLMs. This approach is inspired by Jurenka et al. [Jurenka et al., 2024], who utilized LLMs to validate the quality of hints provided to students. Building on this concept, we designed a new process to assess the validity of the grading reasons generated by our system and baselines.\nThe evaluation is conducted as follows:\nEach of the three LLMs used in the GET system individually assesses the validity of the grading reasons.\nThe percentage of grading reasons rated as valid by each LLM is calculated.\nThe final evaluation result is obtained by averaging these percentages."}, {"title": "4.3 Model Selection and Parameter Settings", "content": "To evaluate the GET system, we selected LLMs based on usability and cost-effectiveness to ensure practical implementation in real educational environments."}, {"title": "4.3.1 Models in Pseudo-Learning", "content": "To describe label tendencies using classification metrics, we employed Gemini 1.5 Flash [Reid et al., 2024]. Among the three models used in multi-LLM grading (introduced in the following section), Gemini 1.5 Flash was chosen due to its superior performance on mathematics-related tasks, as reported in benchmarks [Meta], [Google DeepMind], [Mistral AI] (Accessed: December 26, 2024). Since the task involves analyzing numerical metrics such as accuracy and F1 scores, Gemini's capabilities align closely with the requirements of this analysis."}, {"title": "4.3.2 Models in Multi-LLM Grading", "content": "For grading tasks, we selected Meta-Llama-3-8B-Instruct [AI@Meta, 2024], Gemini 1.5 Flash [Reid et al., 2024], and Mixtral-8x22b [Jiang et al., 2024]. These models were chosen because they either offer free-tier APIs or are open-source, making them cost-effective and accessible for integration into the GET system."}, {"title": "4.3.3 Parameters", "content": "In pseudo-learning, the output pattern analysis (Algorithm 1) needs the threshold to decide the most likely label for each output combination. In the experiment, it is set to 1.2 based on the results of trials with several values.\nThree LLMs also have some parameters. We set the temperature of them to 0.7. The number of maximum tokens which newly generated is set to 8192, which is the same as the maximum length of ColBERT's input. Other parameters of each LLM are left as default."}, {"title": "4.4 Pseudo-learning on the selected LLMS", "content": "Before the main experiments, we conducted a pseudo-learning phase. The results of grading past-data are summarized in Table 4.\nBased on their macro F1 scores, role names were assigned to each LLM: Mixtral-8x22b was designated as the Skilled Expert Grader, Gemini 1.5 Flash as the University Teacher, and Llama-3-8B-Instruct as the Student TA.\nThe results of the tendencies identification phase are presented in Fig. 11 (Labeling tendencies analysis) and Table 5 (Output pattern analysis)."}, {"title": "4.5 Baseline Models", "content": "To evaluate the grading performance of the GET system, we compared it against several baseline systems. These baselines fall into two main categories: grading methods for comparison study and ablation study."}, {"title": "4.5.1 Grading Methods for Comparison Study", "content": "These methods provide a benchmark to compare the performance of our proposed system against existing approaches:\nMajority Voting Baseline: Grading labels are determined based on the majority vote among three graders. The grading reason is randomly selected from the graders who align with the majority label. This follows the same flow of multi-LLM grading to the GET system, but is different in using the majority voting instead of debate integration.\nCriteria-Based Grading System [Ito and Ma, 2025]: A system we introduced at ICETC 2024. It decomposes each question into specific grading criteria based on predefined grammatical rules. A fine-tuned language model (Zephyr-7B-Beta) then evaluates the student answers based on the criteria to generate grading labels and their reasons.\nASAS-F-RAG [Fateen et al., 2024]: A system by Fateen et al., which is constructed from an LLM with RAG. We implemented this approach using their publicly available GitHub repository for replication."}, {"title": "4.5.2 Grading Methods for Ablation Study", "content": "These variants are designed for an ablation study, helping us assess the impact of individual components in the GET system:"}, {"title": "5 Experimental Results and Discussion", "content": "We evaluated the proposed system and baseline methods on two subsets of the SAF dataset: UA and UQ. First, we present the comparison results between the GET system and other grading methods. This is followed by the results of an ablation study using partial variants of GET."}, {"title": "5.1 Comparison Study", "content": "This subsection presents the results of the GET system compared to other grading methods."}, {"title": "5.1.1 Grading Label Prediction Analysis", "content": "The results of the statistical evaluation of grading label predictions are presented in Tables 6. All indicators are rounded to the fifth decimal place."}, {"title": "5.1.2 Grading Reason Generation Analysis", "content": "The results of the LLM-based evaluation are summarized in Table 7.\nThe proposed system achieved the highest average proportion of valid grading reasons for the UQ dataset and in the combined average across both datasets. These results indicate that the GET system generates semantically accurate grading reasons more effectively than baseline systems. This supports the effectiveness of the Ensemble ToT framework for both label prediction (classification) and grading reason generation (text generation)."}, {"title": "5.2 Ablation Study", "content": "This subsection discusses the results of the ablation study with partial variants of the GET system."}, {"title": "5.2.1 Grading Label Prediction Analysis", "content": "The evaluation results for grading label prediction are presented in Tables 9."}, {"title": "5.2.2 Grading Reason Generation Analysis", "content": "The results of the LLM-based evaluation are shown in Table 10. In this evaluation, our proposed system gets the highest scores on both the UA and UQ sets, indicating it has the best performance on generating grading reasons.\nWhile the variant without pseudo-learning partially surpassed the full system in grading label prediction, it under-performed in grading reason quality. This suggests that pseudo-learning has a greater impact on integrating grading reasons than on label prediction."}, {"title": "5.3 Limitations", "content": "While this study demonstrates promising results, there are several limitations. First, the quality of the output of the GET system needs further improvement, as we have discussed above. Second, the system's performance was evaluated using only three specific LLMs, leaving its compatibility and effectiveness with other models untested. Third, this study has not investigated actual students' perceptions and evaluations of the grading results generated by GET. Addressing these limitations could provide valuable insights into the usability and accuracy of our system."}, {"title": "6 Case Study", "content": "This chapter supplements the statistical and LLM-based evaluations from the previous chapter with case studies of grading results. We analyzed three student answers to a question from the UQ subset of the SAF dataset. The target question and its reference answer are shown in Figure 12. We present the GET system's outputs for three categories of student answers: Correct, Partially correct, and Incorrect."}, {"title": "6.1 Correct Student Answer", "content": "This case examines a correct student answer (Figure 13). In the multi-LLM grading phase (Figure 14), the Skilled Expert Grader and University Teacher accurately assigned grades, while the Student TA mistakenly marked the answer as partially correct.\nThen, in the initial debate phase (the first step of debate integration), the Skilled Expert Grader facilitated a simulated debate (Figure 15). The debate began at Step 3 (Conversion), as Steps 1 (Ice Break) and 2 (Divergence) were pre-filled based on the outputs from multi-LLM grading and pseudo-learning. During the debate, the Student TA revised the grade to align with the others.\nFinally, the quality validation phase re-checked the integrated grading (Figure 16), confirming the initial debate result and finalizing the GET system's output."}, {"title": "6.2 Partially Correct Student Answer", "content": "This case examines a partially correct student answer (Figure 17). In the multi-LLM grading phase (Figure 18), only the University Teacher's grade aligned with the human evaluator, marking the answer as partially correct. The other two graders marked it as incorrect. However, all graders identified the lack of drawbacks in the student's answer as a key issue.\nThe initial debate phase (Figure 19) successfully resolved the discrepancy, with"}]}