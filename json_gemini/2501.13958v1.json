{"title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models", "authors": ["Qinggang Zhang", "Shengyuan Chen", "Yuanchen Bei", "Zheng Yuan", "Huachi Zhou", "Zijin Hong", "Junnan Dong", "Hao Chen", "Yi Chang", "Xiao Huang"], "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs), like the GPT series [1], have surprised the world with their remarkable ability across a wide range of tasks, achieving breakthroughs in text comprehension [2], question answering [3], and content generation [4]\u2013[6]. Despite their effectiveness, LLMs are always criticized for their limited ability to handle knowledge-intensive tasks, especially when faced with questions requiring domain expertise [7]. Specifically, the application of LLMs to specialized domains remains challenging for three fundamental reasons, including\nKnowledge limitations: LLMs' pre-trained knowledge is broad but shallow in specialized fields. Their training data primarily consists of general-domain content, leading to insufficient depth in professional domains and potential inconsistencies with current domain-specific standards and practices. Reasoning complexity: Specialized domains require precise, multi-step reasoning with domain-specific rules and constraints. LLMs often struggle to maintain logical consistency and professional accuracy throughout extended reasoning chains, particularly when dealing with technical constraints or domain-specific protocols. Context sensitivity: Professional fields often involve context-dependent interpretations where the same terms or concepts may have different meanings or implications based on specific circumstances. LLMs frequently fail to capture these nuanced contextual variations, leading to potential misinterpretations or inappropriate generalizations.\nTo adapt LLMs for specific or private domains, initial strategies involved fine-tuning LLMs with specialized datasets [8]. This method enhances performance by adding a limited number of parameters while fixing the parameters learned in the pre-training [9]. However, the significant distribution gap between the domain-specific dataset and the pre-training corpus makes it challenging for LLMs to integrate new knowledge without compromising their existing understanding [10]. A recent study by Google Research further highlighted the risks associated with using supervised fine-tuning to update knowledge, particularly in cases where new knowledge conflicts with pre-existing information; acquiring new knowledge through supervised fine-tuning can lead to the model generating new hallucinations and even experiencing severe catastrophic forgetting [11].\nRetrieval-Augmented generation (RAG) offers a promising solution to customize LLMs for specific domains [12]. Rather than retraining LLMs to incorporate updates, RAG enhances these models by leveraging external knowledge from text corpora without modifying their architecture or parameters. This approach enables LLMs to generate responses by leveraging not only their pre-trained knowledge but also real-time retrieved domain-specific information, thereby providing more accurate and reliable answers. The naive RAG systems operate through three key steps: knowledge preparation, retrieval, and integration. During knowledge preparation, external sources such as documents, databases, or webpages are divided into manageable textual chunks and converted into vector representations for efficient indexing. In the retrieval stage, when a user submits a"}, {"title": "II. OVERVIEW", "content": "This survey provides a comprehensive analysis of GraphRAG, detailing its taxonomy, mechanisms, challenges, and future research directions, which is organized into seven main sections that progress from foundational concepts to practical implementations. Specifically, we begin in Section 2 II by establishing the foundational framework, tracing GraphRAG's evolution from traditional RAG systems, examining RAG's limitations in handling structured knowledge, and introducing GraphRAG's core concepts and advantages in complex reasoning tasks. The following three sections systematically explore the key components of GraphRAG systems: two primary paradigms for structured knowledge organization, including knowledge carrier graphs and index graphs (Section 3 IV), retrieval techniques for extracting query-relevant factual information from structured knowledge bases (Section 4 V), and knowledge integration methods for effectively incorporating retrieved knowledge into LLMs (Section 5 VI). Moving toward practical applications, Section 6 VIII addresses implementation aspects by providing detailed guidelines, reviewing open-source projects, and presenting domain-specific case studies supported by comprehensive datasets and evaluation benchmarks. Finally, Section 7 VII concludes by identifying future research directions and discussing potential challenges in knowledge quality, retrieval efficiency, system generalization, and security, alongside practical guidance for building domain-specific GraphRAG systems. Throughout the survey, we balance fundamental concepts, the current state of the art, and practical implementations, making it valuable for researchers advancing the field and practitioners developing GraphRAG applications in real-world scenarios.\nOur survey advances beyond the existing survey [28]\u2013[30] through a more systematic and comprehensive approach to GraphRAG systems. While the previous survey offered a basic workflow description covering Graph-based Indexing, Graph-guided Retrieval, and Graph-enhanced Generation, we introduce a more sophisticated and comprehensive taxonomy that clearly categorizes GraphRAG approaches into three distinct categories (Knowledge-based, Index-based, and Hybrid GraphRAG), providing a more nuanced understanding of the field. Our survey features a more systematic six-section structure that progresses logically from theoretical foundations to practical implementations, offering a more detailed exploration of each component, including knowledge organization paradigms, retrieval techniques, and integration methods. Unlike the previous survey, we provide extensive practical guidance through detailed review of open-source projects, and domain-specific case studies supported by comprehensive datasets and evaluation benchmarks. We also offer a more thorough analysis of challenges and solutions across multiple dimensions, including knowledge quality, retrieval efficiency, system generalization, and security concerns. Finally, while the"}, {"title": "III. WHAT IS GRAPHRAG", "content": "This section provides an overview of RAG with LLMs, including the traditional RAG pipeline, the GraphRAG, and the advantages GraphRAG offers over traditional RAG systems.\nA. Traditional RAG Pipeline\nA RAG framework begins by retrieving relevant information from pre-constructed external knowledge bases based on the query. This information is then used to prompt LLMs, guiding them in constructing credible reasoning chains. As a result, RAG enables LLMs to generate more substantiated and accurate content, effectively minimizing hallucinations and inconsistencies. The traditional RAG pipeline typically comprises three core components: knowledge organization, knowledge retrieval, and knowledge integration.\n1) Knowledge organization: In traditional RAG systems, knowledge organization involves structuring and preparing external knowledge repositories to facilitate rapid and relevant retrieval when provided with a query. A common strategy is to split the large-scale text corpus into manageable chunks. These chunks are then transformed into embeddings using an embedding model, where the embeddings serve as keys of original text chunks in a vector database [81]\u2013[83]. This setup enables efficient look-up operations and retrieval of relevant content via distance-based search in the semantic space.\nAs a crucial step in the pre-retrieval process, several methods [84], [85] have been proposed to optimize knowledge organization, focusing on two main aspects: granularity optimization and indexing optimization. Granularity optimization aims to balance relevance and efficiency, as coarse-grained units provide richer context but risk redundancy and distraction, while fine-grained units may lack semantic integrity and increase the retrieval burden [86], [87]. To control granularity, chunking strategies are employed to split documents into chunks based on token limits. Methods such as recursive splits, sliding windows, and Small-to-Big [85], [88] strive to maintain semantic completeness while optimizing context length. Indexing optimization seeks to improve the structure and quality of content for retrieval. Metadata-addition techniques, which attach chunk text with metadata like titles, timestamps, categories, and keywords, enable filtering and re-ranking operations during the post-retrieval process [85]. Another type of technique is hierarchical indexing, which organizes files into parent-child relationships with summaries at each node, facilitating faster and more efficient data traversal while reducing retrieval errors [89]. Such tree-like indexing methods represent early attempts at structured knowledge organization and have inspired successors to harness the power of graph structures for knowledge organization, i.e., GraphRAG.\nIn summary, knowledge organization is foundational to the retrieval process. By carefully constructing the knowledge resources, RAG systems can ensure the fidelity and reliability of retrieved content.\n2) Knowledge retrieval: The knowledge retrieval stage encompasses various methods and strategies designed to efficiently access and retrieve the necessary knowledge from pre-organized repositories, ensuring the selection of relevant information that can enhance the quality of generated outputs. Current RAG works usually involve retrieval methods such as k-nearest neighbor retrieval (KNN), term frequency-inverse document frequency (TF-IDF), and best matching 25 (BM25) to retrieve the relevant content. RETRO [81] employs KNN to extract approximate relevant neighbors from the conducted key-value database by calculating the L2 distance. RETRO-prompt [90] extends this approach to construct a few-shot"}, {"title": "B. Limitations of traditional RAG", "content": "Although researchers have extensively explored traditional RAG, there are still some unresolved limitations due to the constraints of the data structure itself.\n1) Complex query understanding: Traditional RAG faces significant challenges in precisely answering complex queries, mainly due to the intrinsic limitation of its knowledge organization (vector database). Given a query, these RAG methods can only retrieve information from chunks containing anchor entities and are incapable of multi-hop reasoning. This limitation becomes more pronounced as granularity decreases, making it challenging to handle domain knowledge, as domain knowledge and complex logic often require multi-hop reasoning for effective understanding. Some traditional RAG methods have attempted to improve the complex query understanding, such as enhancing queries before retrieval [91], enhancing multiple query candidates for re-ranking [92], and using related metadata to provide richer information [85]. However,"}, {"title": "C. GraphRAG", "content": "To address the limitations of traditional RAG, a novel paradigm known as Graph Retrieval-Augmented Generation (GraphRAG) has been introduced. Leveraging structured knowledge, GraphRAG provides an efficient and accurate solution for organizing and retrieving information with structural databases, enhancing the performance and reliability of RAG systems.\n1) Definition of GraphRAG: GraphRAG can be formally defined as a subclass of RAG framework that leverage graph structure to organize and retrieve knowledge. Unlike traditional RAG methods, which rely on vector databases for knowledge organization, GraphRAG employs structural databases where graphs are used to model dependencies among knowledge pieces. This approach enhances the accuracy and efficiency of information retrieval, enabling more effective augmented generation for LLMs.\nSpecifically, GraphRAG systems can utilize graphs as either the carrier of knowledge or the indexing tools for efficient retrieval from chunked textual data. By modeling dependencies between nodes, GraphRAG enables the discovery of related knowledge pieces centered around a topic or anchor entity, ensuring comprehensive knowledge retrieval. Moreover, these connections support efficient search by navigating through relevant pathways and meanwhile pruning irrelevant information during the retrieval process.\n2) Workflow of GraphRAG: Like traditional RAG, the workflow of GraphRAG can be divided into three key stages: knowledge organization, knowledge retrieval, and knowledge integration. However, due to the structured nature of GraphRAG, they have some special strategies compared to traditional RAG:\nKnowledge Organization The knowledge organization stage structures external information using graph-based methods, either through explicit knowledge representation (graphs as knowledge carriers) or indexing mechanisms (graphs for knowledge indexing). These approaches enable efficient, context-aware information retrieval.\nKnowledge Retrieval. GraphRAG models employ graph-based planners (either learnable planners or graph algorithm-based planners) to retrieve relevant information based on the input query. These retrieval techniques not only consider the semantic similarity between the query and each text chunk, but also the logical coherence between the query type and the retrieved subgraph.\nKnowledge Integration. Once the relevant knowledge is retrieved, the GraphRAG model integrates it with"}, {"title": "D. Traditional RAG vs GraphRAG", "content": "GraphRAG provides several key advantages over traditional RAG systems, enhancing the capabilities of AI-driven information retrieval and generation. Below, we discuss these benefits and illustrate the comparison between traditional RAG and GraphRAG in Figure 4.\n1) Enhanced Knowledge Representation: GraphRAG utilizes graph structures to represent knowledge, capturing complex relationships between entities and concepts. This approach allows for a more nuanced and contextual understanding of information compared to the flat document-based representation used in traditional RAG. The graph structure in GraphRAG can represent hierarchies, associations, and multi-hop relationships, providing a richer semantic context for queries and revealing non-obvious connections between different pieces of information. This capability can lead to new insights and discoveries, making GraphRAG particularly valuable in research and analysis applications. By representing multiple possible interpretations or relationships in the graph, GraphRAG can better handle ambiguous queries. It can explore different semantic paths and provide responses that account for various possibilities, offering a more nuanced understanding of complex topics.\n2) Flexibility in Knowledge Sources: GraphRAG systems can adapt to and integrate various knowledge sources, including structured databases, semi-structured data (like JSON or XML), and unstructured text. This versatility allows organizations to leverage their existing data infrastructure while benefiting from the advanced capabilities of GraphRAG. The system can connect data types, providing a unified view of an organization's knowledge. GraphRAG can incorporate different types of data (text, images, numerical data) into a single graph structure. This capability allows for more comprehensive knowledge representation and the ability to answer queries that span multiple data modalities.\n3) Efficiency and Scalability: GraphRAG systems built on fast graph databases can handle large-scale datasets efficiently. Graph databases are optimized for relationship-based queries, allowing for quick traversal of complex data structures. This efficiency translates to faster response times, especially for queries that require exploring multiple relationships. Research has shown that GraphRAG systems can generate LLM responses using 26% to 97% fewer tokens compared to traditional methods, indicating significant improvements in both speed and resource utilization. GraphRAG systems can more easily accommodate updates to the knowledge base. New information can be added as nodes or edges in the graph without requiring a complete reindexing of the entire knowledge base. This feature"}, {"title": "IV. KNOWLEDGE ORGANIZATION", "content": "A major distinction of GraphRAG techniques compared to traditional RAG methods lies in their ability to leverage graph structures for efficient knowledge organization, enhancing the effectiveness of query responses. In this setup, the LLM functions as an intelligent agent, while the graph structures support its ability to organize and integrate information more comprehensively and precisely. In real-world applications, external knowledge sources may include extensive text corpora, document collections, search results, historical user data, or interaction logs. Properly organizing these sources is essential, as direct retrieval can be prone to irrelevant results and missing contexts, leading to information overload, knowledge conflicts, and compromised comprehension.\nTo address this, GraphRAG methods employ a two-step process: first constructing a graph structure to organize knowledge, then retrieving and integrating information relevant to the query. The organization of knowledge varies by task and source type, with three primary paradigms: (1) Graphs for Knowledge Indexing for index-based graphRAG: Text chunks are organized by symbolizing each chunk as a node within a graph, within which edges between nodes signify query-oriented relationships, making it easier to localize relevant knowledge. Considering knowledge hierarchy, further work builds multi-level index graphs, involving bottom-up knowledge summarization and top-down knowledge localization to enable coarse-to-fine knowledge indexing. (2) Graphs as Knowledge"}, {"title": "V. KNOWLEDGE RETRIEVAL", "content": "Based on the well-built knowledge base with graph-based organization strategies, we need to conduct retrieval from the knowledge base. Specifically, given a user query and a graph knowledge base with dense information, retrieving factual information relevant to the given query from the knowledge base is very important in developing effective and efficient GraphRAG systems. This retrieval process forms the cornerstone of a robust GraphRAG architecture, directly impacting its overall performance and utility. In this section, we will introduce current knowledge retrieval methods for graph-based retrieval-augmented generation in detail.\nA. Overall Pipeline of Knowledge Retriever\nThe knowledge retrieval process in GraphRAG focuses on extracting relevant background knowledge from a graph database in response to a given query. This process follows three distinct and sequential steps that transform raw graph data into usable, contextual knowledge. The overall process of knowledge retrieval is shown in Figure 5.\n1) Query/Graph Preprocessing: The preprocessing stage operates simultaneously on both the query and graph databases to prepare them for efficient retrieval. For query preprocessing, the system transforms the input question into a structured representation through vectorization or key term extraction [34], [50], [51]. These representations serve as search indices for subsequent retrieval operations. On the graph side, the graph database undergoes more comprehensive processing where pre-trained language models transform graph elements (entities, relations, and triples) into dense vector representations that serve as retrieval anchors [23], [33], [52], [60]. Additionally, some advanced retrieval models apply graph neural networks (GNNs) on the graph database to extract high-level structural features, while a few methods even adopt rule mining algorithms to generate rule banks as rich, searchable indexes of the graph knowledge [22], [31], [54], [57].\n2) Matching: The matching stage establishes connections between the preprocessed query and the indexed graph database. This process compares the query representations against the graph indices to identify relevant knowledge fragments. The matching algorithm considers both semantic similarity and structural relationships within the graph. Based on the matching score, the system retrieves connected components and subgraphs that demonstrate high relevance to the query, creating an initial set of candidate knowledge.\n3) Knowledge Pruning: The pruning stage refines the initially retrieved knowledge to improve its quality and relevance. This refinement process addresses the common challenge of retrieving excessive or irrelevant information, particularly when dealing with complex queries or large graph databases. The pruning algorithm applies a series of refinement operations to consolidate and summarize the retrieved knowledge [7], [32], [52]. Specifically, the system first removes clearly irrelevant or noisy information. It then consolidates related knowledge fragments and generates concise summaries of complex graph knowledge. The purpose of knowledge summarization is to facilitate comprehensive knowledge synthesis, which is vital"}, {"title": "VI. KNOWLEDGE INTEGRATION", "content": "The knowledge retrieval phase is crafted to gather pertinent documents from various external sources in alignment with a specified query. Following this, the integration phase focuses on seamlessly synthesizing documents obtained from knowledge retrieval into a cohesive prompt, simultaneously setting appropriate training goals for the purpose of optimization. Recognizing that LLMs form a sturdy and foundational framework, the exploration during the integration phase predominantly steers clear of modifying the internal structure of individual layers or necessitating a complete reinitialization of LLMs for training. In this section, we delve into the comprehensive pipeline, relevant technologies, and strategies for enhancing knowledge integration.\nA. Overall Pipeline of Knowledge Integration\nIntegrating graph-retrieved knowledge into LLMs mainly includes two main ways: fine-tuning and in-context learning. The overall pipeline of these integration methods can be summarized as follows.\nFine-tuning. To directly leverage information retrieved from graph searches to enhance open-source LLMs, fine-tuning offers a straightforward solution for the integration, such as LoRA-based tuning [8], [161] and other data-efficient fine-tuning strategies [162], [163]. It injects the retrieved knowledge directly into the LLMs, focusing on graph-retrieved information at three knowledge levels: node-level knowledge, path-level knowledge, and subgraph-level knowledge for model tuning. In this way, graph information from different levels enhances the different capabilities of LLMs.\nIn-context Learning. While numerous open-source LLMs have been released to date, many state-of-the-art LLMs remain closed-source in practice. The integration of closed-source LLMs is constrained since it is not feasible to jointly train or fine-tune closed-source LLMs in an end-to-end manner. Thus, in-context learning provides an indirect strategy for knowledge integration, which can be roughly decomposed into two steps: prompt format choice and LLM response optimization. (i) Prompt format choice: The choice of prompt format is crucial for knowledge integration. This is because LLMs are highly sensitive to the prompt format. For example, the order of examples in in-context learning can lead to different responses [164], [165]. (ii) LLM response optimization: Then the appropriate prompt format joins the retrieved content and the questions as input to prompt the LLMs for response generation or further optimization.\nB. Integration Techniques\n1) Fine-tuning Techniques: The fine-tuning process, leveraging various graph information, can be delineated into three distinct categories based on the granularity of the input target: (i) Node-level Knowledge: Focusing on individual nodes within the graph. (ii) Path-level Knowledge: Concentrating on the connections and sequences between nodes. (iii) Subgraph-level Knowledge: Considering larger structures composed of multiple nodes and their interconnections. We will explore each of these aspects in detail."}, {"title": "D. Discussions", "content": "The true magic of graph-based knowledge resources lies in their supernatural ability to connect data while being flexible and scalable during retrieval. Graphs can seamlessly integrate various data sources, including structured and unstructured data. This fusion creates a unified view of information, often revealing hidden connections and patterns. Advanced graph knowledge bases incorporate ontologies and semantic schemas,"}, {"title": "VII. LIMITATIONS AND FUTURE OPPORTUNITIES", "content": "In this section, we systematically analyze several critical limitations of existing GraphRAG systems regarding knowledge quality, knowledge conflict, data privacy, and efficiency"}, {"title": "VIII. OPEN-SOURCE PROJECTS AND APPLICATIONS", "content": "The advent of GraphRAG has opened new avenues for enhancing information retrieval and generation processes through the integration of graph-based methodologies. As researchers and developers explore the potential of GraphRAG, a variety of open-source projects and applications have emerged, demonstrating its versatility across different domains. This section delves into the real-world implementation of GraphRAG systems, including benchmark datasets, that serve as evaluation"}]}