{"title": "One-Layer Transformer Provably Learns One-Nearest Neighbor In Context", "authors": ["Zihao Li", "Yuan Cao", "Cheng Gao", "Yihan He", "Han Liu", "Jason M. Klusowski", "Jianqing Fan", "Mengdi Wang"], "abstract": "Transformers have achieved great success in recent years. Interestingly, transformers have shown particularly strong in-context learning capability \u2013 even without fine-tuning, they are still able to solve unseen tasks well purely based on task-specific prompts. In this paper, we study the capability of one-layer transformers in learning one of the most classical nonparametric estimators, the one-nearest neighbor prediction rule. Under a theoretical framework where the prompt contains a sequence of labeled training data and unlabeled test data, we show that, although the loss function is nonconvex when trained with gradient descent, a single softmax attention layer can successfully learn to behave like a one-nearest neighbor classifier. Our result gives a concrete example of how transformers can be trained to implement nonparametric machine learning algorithms, and sheds light on the role of softmax attention in transformer models.", "sections": [{"title": "Introduction", "content": "Transformers have emerged as one of the most powerful machine learning models since its introduction in Vaswani et al. [2017], achieving remarkable success in various tasks, including natural language processing [Devlin et al., 2018, Achiam et al., 2023, Touvron et al., 2023], computer vision [Dosovitskiy et al., 2020, He et al., 2022, Saharia et al., 2022], reinforcement learning [Chen et al., 2021, Janner et al., 2021, Parisotto et al., 2020], and so on. One intriguing aspect of transformers is their exceptional In-Context Learning (ICL) capability [Garg et al., 2022, Min et al., 2022, Wei et al., 2023, Von Oswald et al., 2023, Xie et al., 2021, Aky\u00fcrek et al., 2022]. It has been observed that transformers can effectively solve unseen tasks solely relying on task-specific prompts, without the need for fine-tuning. However, the underlying mechanisms and reasons behind the exceptional in-context learning capability of transformers remain largely unexplored, leaving a significant gap in our understanding of how and why transformers can be pretrained to exhibit such remarkable performance.\nSeveral recent studies have attempted to understand in-context learning (ICL) through the lens of learning specific function classes. Notably, Garg et al. [2022] proposed a well-defined approach: the training data includes a demonstration prompt, consisting of a sequence of labeled data and a new unlabeled query. The in-context learning performance of a transformer is then evaluated based on its ability to successfully execute a machine-learning algorithm to predict the query data label using the prompt demonstration (i.e., the context). Based on such definition, several works such as Zhang et al. [2023], Huang et al. [2023], Chen et al. [2024] investigated ICL the optimization dynamics of transformers under in-context learning from a theoretical lens, but their studies are limited to linear regression prediction rules, which is a significant simplification of the transformer in-context learning task. Another line of work including Bai et al. [2024], Aky\u00fcrek et al. [2022] investigated the expressiveness of transformers in context, but no optimization result is guaranteed. Whether transformers can handle more complicated ICL tasks under regular gradient-based training is still, in general, unknown.\nIn this paper, we examine the ability of single-layer transformers to learn the one-nearest neighbor prediction rule. Our major contributions are as follows:\n\u2022 We establish convergence guarantees as well as prediction accuracy guarantees of a single-layer transformer in learning from examples of one-nearest neighbor classification. Utilizing the softmax attention layer, we demonstrate that the training loss can be minimized to zero despite the highly non-convex loss function landscapes. We further justify our results with numerical simulations.\n\u2022 Based on the optimization results, we further establish a behavior guarantee for the trained transformer, demonstrating its ability to act like a 1-NN predictor under data distribution shift. Our result thus serves as a concrete example of how transformers can learn nonparametric methods, surpassing the scope of previous literature focusing on linear regression.\n\u2022 In our technical analysis, we make the key observation that although the transformer loss is highly nonconvex when learning from one-nearest neighbor, its optimization process can be controlled by a two-dimensional dynamic system when choosing a proper initialization. By analyzing the behavior of such a system, we establish the convergence result despite the curse of nonconvexity.\nTo summarize, our result gives a concrete example of how transformers can be trained to implement nonparametric machine learning algorithms and sheds light on the role of softmax attention in transformer models. To our knowledge, this is the first paper that establishes a provable result in both optimization and consecutive behavior under distribution shift for a softmax attention layer beyond the scope of linear prediction tasks."}, {"title": "Preliminaries", "content": "In this section, we introduce the in-context learning data distribution based on the one-nearest neighbor data distribution and the setting of one-layer softmax attention transformers. Then, we discuss the training dynamics of transformers based on gradient descent."}, {"title": "In-Context Learning Framework: One-Nearest Neighbor", "content": "In an In-Context Learning (ICL) instance, the model is given a prompt {$(x_i, y_i)_{i\\in[N]}$} ~ $P_{prompt}$ and a query input $x_{N+1}$ ~ $P_{query}$ from some data distributions $P_{prompt}$ and $P_{query}$, where {$(x_i)_{i\\in[N]}$} are the input vectors, {$(y_i)_{i\\in[N]}$} $C$ R are the corresponding labels (e.g. real-valued for regression, or {+1,-1}-valued for binary classification), and $x_{N+1}$ is the query on which the model is required to make a prediction. Given a prompt {$(x_i, y_i)_{i\\in[N]}$}, the prediction task is to predict an ground truth model $f(x_{N+1}; {(x_i, y_i)}_{i\\in[N]})$ that maps the query token $x_{N+1}$ to a real number.\nFor a prompt {$(x_i, y_i)_{i\\in[N]}$} of length N and a query token $x_{N+1}$, we consider use the following embedding:\nH = [$h_1$, $h_2$, ..., $h_{N+1}$] = $\\begin{bmatrix} x_1 & x_2 & ... & x_N & x_{N+1} \\ y_1 & y_2 & ... & y_N & 0 \\ 0 & 0 & ... & 0 & 1 \\ \\end{bmatrix}$$\\in R^{(d+2)\\times(N+1)}$. (2.1)\nWe use the notation of $h_j$ = [$x_j$, $y_j$, 0] for $j \\leq N$, and $h_{N+1}$ = [$x_{N+1}$,0,1]. Here, {$(x_i)_{i\\in[N]}$} represents the input vectors, each associated with a corresponding label {$(y_i)_{i\\in[N]}$}, where $y_i$ $C$ R is the label. Throughout this paper, the sequence {$(x_i, y_i)_{i\\in[N]}$} are referred to as the context or prompt exchangeably. The (d + 2)-th row serves as the indicator for the training token, which equals to 0 value for i $C$ [N] and 1 for i = N + 1, analogous to a positional embedding vector. Such an indicator allows the model to distinguish the query token from the context. Similar models have been"}, {"title": "One-Layer Softmax Attention Transformers", "content": "We consider a simplified version of the one-layer transformer architecture [Vaswani et al., 2017] that processes any input sequence H defined by Eq. (2.1) and outputs a scalar value:\n$H_W$ = H. softmax(HWWH), (2.2)\nwhere softmax(A) applies softmax operator on each column of the matrix A, i.e. [softmax(A)]ij = exp($A_{ij}$)/$\\sum_{j}$ exp($A_{ij}$). Our model is slightly different from the standard self-attention transformers, as we consider a frozen value matrix. However, we also claim that such practice is common in deep learning theory Fang et al. [2020], Lu et al. [2020], Mei et al. [2018]. We also merge the query and key matrices into one matrix denoted as W, which is often taken in recent theoretical frameworks [Zhang et al., 2023, Huang et al., 2023, Jelassi et al., 2022, Tian et al., 2023]. The output of the model is defined by the (d + 1)-th element of the last column of $H_W$, with a closed form:\n$\\hat{y}_W(x_{N+1}; {(x_i, y_i)}_{i\\in[N]}) := [H_W]_{(d+1,N+1)}$ = $\\frac{\\sum_{i=1}^N y_i exp(h_i^T Wh_{N+1})}{\\sum_{j=1}^{N+1} exp(h_j^T Wh_{N+1})}$, (2.3)\nwhich is the weighted mean of $y_1$,..., $y_N$. Here and after, we may occasionally suppress dependence on {$(x_i, y_i)_{i\\in[N]}$} and write $\\hat{y}_W(x_{N+1}; {(x_i, y_i)}_{i\\in[N]})$ as $\\hat{y}_W(x_{N+1})$. Since the prediction takes only one entry of the token matrix output by the attention layer, actually only parts of W affect the prediction. To see this, we denote\nW = $\\begin{bmatrix} W_{11} & W_{12} & W_{13} \\ W_{21} & W_{22} & W_{23} \\ W_{31} & W_{32} & W_{33} \\ \\end{bmatrix}$, (2.4)\nwith $W_{11}$ $C$ $R^{d\\times d}$, $W_{21}$ $C$ $R^{1\\times d}$, $W_{31}$ $C$ $R^{1\\times d}$, $W_{12}$ $C$ $R^{d\\times 1}$, $W_{13}$ $C$ $R^{1\\times d}$, $W_{22}$, $W_{23}$, $W_{32}$ and $W_{33}$ $C$ R. Then by Eq. 2.3, it is easy to see that $W_{i2}$ does not affect $\\hat{y}_W$ for i $C$ [3], which means we can simply take all these entries as zero in the following sections. Notably, for a fixed prompt-query pair {$(x_i, y_i)_{i\\in[N]}$} and {$x_{N+1}$}, such an architecture allows an arbitrarily close approximation to the 1-NN model: consider $W_{11}$ = $g^*I_d$ with $g^*$ goes to positive infinity, $W_{33}$ = $g$ such that $g e^{-g}$ converges to infinity, with the rest of $W_{ij}$ bounded, then $\\hat{y}_W (x_{N+1})$ converges to $y_{i^*}$ as $g$ goes to infinity."}, {"title": "Training Dynamics", "content": "To train the transformer model over the 1-NN task, we consider the Mean-Square Error (MSE) loss function. Specifically, the loss function is defined by\nL(W) = $\\frac{1}{2} E_{\\{(x_i, y_i)}_{i\\in[N]}, x_{N+1}\\} [(\\hat{y}_W(x_{N+1}) - y_{i^*})^2]$. (2.5)\nAbove, the expectation is taken with respect to the sampled prompt {$(x_i, y_i)_{i\\in[N]}$} and the query $x_{N+1}$. Notably, when the underlying distribution for the prompt and query are defined by Assumption 1, this loss function is nonconvex with respect to W. Such nonconvexity makes the optimization hard to solve without further conditions such as PL condition or KL condition [Bierstone and Milman, 1988, Karimi et al., 2020]. We leave the proof of nonconvexity in Appendix E.\nWe shall consider the behavior of gradient descent on the single-layer attention architecture w.r.t. the loss function in Eq. (2.5). The parameters are updated as follows:\n$W^{k+1}$ \u2190 $W^{k}$ \u2212 $\\eta \\nabla_W L(W^{k})$. (2.6)\nWe shall consider the following initialization for the gradient descent:\nAssumption 2 (Initialization). Let $\\sigma$ > 0 be a parameter. We assume the following initialization:\n$W^0$ = $\\begin{bmatrix} \\sigma I_{(d+1)\\times(d+1)} & O_{d+1} \\ O_{d+1} & -\\sigma \\end{bmatrix}$,\nHere the parameter $\\sigma$ is similar to masking, which is widely applied in self-attention training process, and prevents the model from focusing on the zero-label for the query $x_{N+1}$, e.g. Vaswani et al. [2017], Baade et al. [2022], Chang et al. [2022]. The reason we take the zero initialization for non-diagonal entries will be made clear when we describe the proof in Section 4. However, from a higher view, it is because we want to keep the model focusing on the inner product between different $x_i$, which largely reduces the complexity of the dynamic system under gradient descent and makes it tractable. We leave the question of convergence under alternative random initialization schemes for future work."}, {"title": "Main Results", "content": "In this section, we summarize the convergence of training loss and testing error respectively. In Section 3.1, we discuss the convergence of training loss under gradient descent. Specifically, we prove that with a proper initialization constant $\\sigma$, gradient descent is able to minimize the loss function L(W) despite the nonconvexity. In Section 3.2, we further discuss the testing error of the trained transformer under distribution shift. Specifically, we consider a distribution $P_{test}$ for the prompt {$(x_i, y_i)_{i\\in[N]}$} \u222a {$x_{N+1}$}, which is different from the training data distribution $P_{prompt}$ \u2297 $P_{query}$, and discuss the difference between the trained transformer and 1-NN predictor under such distribution shift."}, {"title": "Convergence of Gradient Descent", "content": "First, we prove that under suitable initialization parameter $\\sigma$, the loss function will converge to zero under gradient descent.\nTheorem 1 (Convergence of Gradient Descent). Consider performing gradient descent of the softmax-attention transformer model $\\hat{y}_W(x_{N+1})$. Suppose the initialization satisfies Assumption 2 with $\\sigma$ > 2(max{log(Nd), \u2013 log (1 \u2013 (N\u221ad)), Ca(1 \u2013 )}), where $C_d$ = poly(d), and the number of context N > O(\u221adlog d), then L(Wk) converges to 0.\nWe leave the detailed proof in Appendix C. Theorem 1 shows that for the 1-NN data distribution, with a large enough initialization constant $\\sigma$, the training loss of the transformer converges to zero under gradient descent. Here $\\sigma$ plays a role similar to the masking techniques in the self-attention training training process, in which $\\sigma$ is often set as infinity or an extremely large number. Such a technique has been widely accepted and shown to greatly accelerate the training process Vaswani et al. [2017], Devlin et al. [2018], Dosovitskiy et al. [2020]. We also compare our results to existing"}, {"title": "Results for New Task under Distribution shift", "content": "In this section, we discuss the behavior of trained transformers under distribution shifts, i.e., how the model extrapolate beyond the training distribution. Following the definition in Garg et al. [2022], let us assume in the training process, the prompts {$(x_i, y_i)_{i\\in[N]}$} ~ $P_{prompt}^{train}$ and the query $x_{N+1}$ ~$P_{query}^{train}$. During inference, the prompts and queries are sampled from a new distribution $P_{prompt}^{test}$\u2297$P_{query}^{test}$. We study the behavior of the trained transformers under possible prompt and query shift, i.e. $P_{prompt}^{test}$  \u2260 $P_{prompt}^{train}$\u2297 $P_{query}^{train}$. Our studies show that, under some mild conditions, the behavior of the trained model is still similar to a 1-NN predictor even under a distribution shift. Before formally stating our result, let us introduce the following assumption on the testing distribution:\nAssumption 3 (Testing Distribution). We make the following assumption on $P^{test}$:\n(i) There exists a R \u2265 0 such that $|y_i|$ < R holds for all $y_i$ sampled from $P^{test}$.\n(ii) For all {$(x_i, y_i)_{i\\in[N]}$} \u222a {$x_{N+1}$} ~ $P^{test}$, we have $x_i$ \u2208 $S^{d\u22121}$ for all i \u2208 [N + 1].\nNote that Assumption 3 only requires the label $y_i$ is bounded and $x_i$ is supported on a sphere. We also remind the reader that we do not assume independence between different $x_i$ or {$(x_i)_{i\\in[N+1]}$} and {$(y_i)_{i\\in[N+1]}$. Now we are ready to summarize our result in the following theorem.\nTheorem 2 (Resemblance to 1-NN predictor under Distribution Shift). Suppose Assumption 1 and 3 hold for $P_{prompt}^{train}$\u2297$P_{query}^{train}$ and $P^{test}$. If we define\n$A_{\\delta}$ := {||$x_j$ - $x_{N+1}$||2 \u2265 ||$x_{i^*}$ - $x_{N+1}$||2 + $\\delta$ for all j \u2260 $i^*$ such that $y_j$ \u2260 $y_{i^*}$},\nthen, after K-iterations of gradient descent, we have\n$E_{\\{(x_i, y_i)}_{i\\in[N]},x_{N+1}\\} [(\\hat{y}_W^k(x_{N+1}) - y_{i^*})^2]$\u2264 O(inf{$R^2N^2K^{-poly(N,d)}\\delta}$ + $R^2 P^{test}(A_{\\delta})\\})$, here the expectation is taken w.r.t {$(x_i, y_i)_{i\\in[N]}$} \u222a {$x_{N+1}$} ~ $P^{test}$. Recall that $y_{i^*}$ is the 1-NN predictor of $x_{N+1}$, which we defined in Definition 1.\nWe leave the detailed proof in Appendix D. Let us discuss the implication of Theorem 2. The event $A_{\\delta}$ describes the situation when the query $x_{N+1}$ is located at an \"inner point\" away from its decision boundary, in which its distance to the nearest neighbor $x_{i^*}$ is strictly larger than all other points. Such a quantity is similar to the margin condition in classification theory in deep learning Bartlett et al. [2017] and k-NN literature Chaudhuri and Dasgupta [2014], where the optimal choice probability is strictly larger than all suboptimal choices. Specifically, if $P^{test}(A_{\\delta^*}) = 1$ for some $\\delta^*$ > 0, i.e., the query $x_{N+1}$ is strictly bounded away from the decision boundary almost surely, then the $L_2$ distance between $y_W^k$ and the 1-NN predictor will converge in a O($R^2K^{-poly(N,d)}\\delta^*$) even under a shifted distribution. We also introduce the following corollary, in which we show that when $y_i$ only takes value in a finite integer set, resembling a classification task, the trained transformer behaves like a 1-NN predictor under an additional rounding operation.\nCorollary 1 (Classfication of Trained Transformer). Suppose $y_i$ \u2208 [M] for some integer M \u2265 0 under $P^{test}$, then we have\n$P^{test}(Round (\\hat{y}_W^k(x_{N+1})) \u2260 y_{i^*})$\u2264 O(inf {$M^2N^2K^{-poly(N,d)}\\delta}$ + $M^2 P^{test}(A_{\\delta})\\})$.\nHere we define\nRound(t) := $y_i^*$ \u22c51[$|\t|<y_i*]$."}, {"title": "Sketch of Proof", "content": "In this section, we sketch the proof of Theorem 1 and highlight the techniques we used. The full proof is left to Appendix C.\nEquivalence to a Two-Dimensional Dynamic System. Recall that {$(x_i)_{i\\in[N+1]}$} and the first and second moment of {$(y_i)_{i\\in[N]}$} are uncorrelated. Utilizing this uncorrelation between {$(x_i)_{i\\in[N+1]}$} and {$(y_i)_{i\\in[N]}$}, we can eliminate the reliance of the gradient on {$(y_i)_{i\\in[N]}$} since we are considering a population loss. Moreover, utilizing the structure of the initialization, we can prove by induction that all $W_{ij}$ will remain zero except for $W_{11}$ and $W_{33}$. This shows that with a suitable initialization, the transformer model will only focus on the relationship between different tokens $x_i$ throughout the whole training process. Our findings can be summarized by the following lemma.\nLemma 1 (Closed-Form Gradient). With the initialization in Assumption 2, the gradient of L($W^k$) with respect to $W_{11}$ can be written in the following form for all k \u2265 0:\n$\\nabla_{W_{11}}L(W^k) = E [(\\sum_{i=1}^{N+1} x_i x^T_{N+1}) - x_{i^*} x^T_{N+1} (\\sum_{j=1}^{N+1} q_j(x)){y^2_{i^*}}] $  (4.1)\nwhere {$g(x)$}i\u2208[N] \u222a {$g(x)$} : R \u2192 R is a set of functions. Here the expectation is taken with respect to {$x_i$}i\u2208[N+1], with $x_{i^*}$ = argmin$x \\in {x_i}_{i \\in[N]} ||x - x_{N+1}||_2$ sampled i.i.d. from a uniform distribution on $S^{d\u22121}$. Moreover, we have $\\nabla_{W_{ij}}L(W^k$) = 0 for all (i, j) \u2208 [3] \u00d7 [3] and all k \u2265 0 except for $W_{11}$ and $W_{33}$.\nLemma 1 shows that we only need to consider $W_{11}$ and $W_{33}$ in our update since all other entries will remain zero during the whole learning process. Note that in Eq. (4.1), all nonlinearity comes from the inner product between $x_i x_{N+1}$ and $x_{i^*} x_{N+1}$. Recall that {$x_i$}i\u2208[N+1] are i.i.d. sampled from a uniform distribution supported on a d \u2212 1-dimensional sphere $S^{d\u22121}$, therefore, the distribution of {$x_i$}i\u2208[N+1] is rotational invariance, which means {$x_i$}i\u2208[N+1] $P_{x_i}$ = {$x_i$}i\u2208[N+1] $P_{Ux_i}$ for all orthogonal matrix U \u2208 $R^{d\u00d7d}$. Since the rotation of {$x_i$}i\u2208[N+1] does not change the inner products {$x_i x^T_j$}i\u2208[N] and {$x_i x^T_j$}i\u2208[N+1], from the structure of $\\nabla_{W_{11}}L(W^k$) illustrated by Eq. (4.1), we shall always have U$\\nabla_{W_{11}}L(W^k$)UT = $\\nabla_{W_{11}}L(W^k$), which shows $\\nabla_{W_{11}}L(W^k$) = ckId for some constant ck by simple algebra. We summarize our result in the following lemma.\nLemma 2 (Two-Dimensional System). With the initialization in Assumption 2, there exists two sets of real numbers {$g^t_1$}k\u22650 and {$g^t_2$}k\u22650, such that $W^k$ has the following form:\nWk = diag{$g^t_1$,...,$g^t_1$,0,-$g^t_2$}.\nd times"}]}