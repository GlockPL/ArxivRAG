{"title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers", "authors": ["Xuan Shen", "Zhao Song", "Yufa Zhou", "Bo Chen", "Yanyu Li", "Yifan Gong", "Kai Zhang", "Hao Tan", "Jason Kuen", "Henghui Ding", "Zhihao Shu", "Wei Niu", "Pu Zhao", "Yanzhi Wang", "Jiuxiang Gu"], "abstract": "Diffusion Transformers have emerged as the preeminent models for a wide array of generative tasks, demonstrating superior performance and efficacy across various applications. The promising results come at the cost of slow inference, as each denoising step requires running the whole transformer model with a large amount of parameters. In this paper, we show that performing the full computation of the model at each diffusion step is unnecessary, as some computations can be skipped by lazily reusing the results of previous steps. Furthermore, we show that the lower bound of similarity between outputs at consecutive steps is notably high, and this similarity can be linearly approximated using the inputs. To verify our demonstrations, we propose the LazyDiT, a lazy learning framework that efficiently leverages cached results from earlier steps to skip redundant computations. Specifically, we incorporate lazy learning layers into the model, effectively trained to maximize laziness, enabling dynamic skipping of redundant computations. Experimental results show that LazyDiT outperforms the DDIM sampler across multiple diffusion transformer models at various resolutions. Furthermore, we implement our method on mobile devices, achieving better performance than DDIM with similar latency.", "sections": [{"title": "1 Introduction", "content": "Diffusion models (Ho, Jain, and Abbeel 2020; Rombach et al. 2022a; Song et al. 2020; Song and Ermon 2019; Dhariwal and Nichol 2021; Zhan et al. 2024b) have become dominant in image generation research, attributable to their remarkable performance. U-Net (Ronneberger, Fischer, and Brox 2015) is a widely used backbone in diffusion models, while transformers (Vaswani et al. 2017) are increasingly proving to be a strong alternative. Compared to U-Net, transformer-based diffusion models have demonstrated superior performance in high-fidelity image generation (Peebles and Xie 2023; Bao et al. 2023), and their efficacy extends to video generation as well (Lu et al. 2023; Chen et al. 2023; Lab and etc. 2024; Zheng et al. 2024). This highlights the versatility and potential of transformers in advancing generative tasks across different media. Despite the notable scalability advantages of transformers, diffusion transformers face major efficiency challenges. The high deployment costs and the slow inference speeds create the significant barriers to their practical applications (Zhan et al. 2024a,c, 2021; Wu et al. 2022; Li et al. 2022; Yang et al. 2023a), which motivates us to explore their acceleration methods.\n\nThe increased sampling cost in diffusion models stems from two main components: the numerous timesteps required and the computational expense associated with each inference step. To improve sampling efficiency, existing methods generally fall into two categories: reducing the total number of sampling steps (Song, Meng, and Ermon 2020; Liu et al. 2022; Bao et al. 2022; Zhan et al. 2024b) or lowering the computational cost per step (Yang et al. 2023b; He et al. 2023). Several works (Yin et al. 2024; Luo et al. 2024; Salimans and Ho 2022) employ distillation techniques to reduce the number of sampling steps. Conversely, works (Li et al. 2023c; Kim et al. 2023; Fang, Ma, and Wang 2023; Li et al. 2023b) utilize compression techniques to streamline diffusion models. Recently, some studies have introduced caching mechanisms into the denoising process (Ma, Fang, and Wang 2024; Wimbauer et al. 2023) to accelerate the sampling. However, previous compression approaches of diffusion models have primarily focused on optimizing U-Net, leaving transformer-based models largely unexplored.\n\nLeveraging characteristic of uniquely structured, prior compression works (Zhang et al. 2024; Raposo et al. 2024; Fan, Grave, and Joulin 2019; Kong et al. 2022, 2023; Zhang et al. 2022; Li et al. 2023d; Zhao et al. 2024; Shen et al. 2024d,a,c,b, 2023b) have concentrated on techniques such as layer pruning and width pruning. However, we observe that removing certain layers results in a significant performance drop. This indicates the redundancy in diffusion transformers primarily occurs between sampling steps rather than the model architecture. This finding forms basis for exploring methods to reduce frequency of layer usage, aiming to decrease computational costs and accelerate the diffusion.\n\nIn this paper, we propose LazyDiT, a cache-based approach designed to dynamically reduce computational costs and accelerate the diffusion process. We begin by analyzing the output similarity between the current and previous steps, identifying that the lower bound of this similarity is notably high during the diffusion process. Then, we delve deeper into the similarity using a Taylor expansion around the current input, revealing that the similarity can be linearly approximated. Building on the theoretical analysis, we implement a lazy learning framework by introducing linear layers before each Multi-Head Self-Attention (MHSA) and pointwise feedforward (Feedforward) module. These added layers are trained with the proposed lazy loss to learn whether the subsequent module can be lazily bypassed by leveraging the previous step's cache. Compared to the DDIM sampler, extensive experiments demonstrate that our method achieves superior performance with similar computational costs. As shown in Figure 1, by lazily skipping 50% of the computations, our method achieves nearly the same performance as the original diffusion process. We also profile the latency of the diffusion process on mobile devices to offer a detailed comparison with the DDIM sampler. Our results show our superior image generation quality than DDIM with similar latency. Our main contributions are summarized as follows,\n\u2022 We explore the redundancy in diffusion process by evaluating the similarity between module outputs at consecutive steps, finding that the lower bound of the similarity is notably high.\n\u2022 We establish that the lazy skip strategy can be effectively learned through a linear layer based on the Taylor expansion of similarity.\n\u2022 We propose a lazy learning framework to optimize the diffusion process in transformer-based models by lazily bypassing computations using the previous step's cache.\n\u2022 Experiments show that the proposed method achieves better performance than DDIM sampler. We further implement our method on mobile devices, showing that our method is a promising solution for real-time generation."}, {"title": "2 Related Work", "content": "Transformer-Based Diffusion Models. Recent works such as GenVit (Yang et al. 2022), U-Vit (Bao et al. 2023), DiT (Peebles and Xie 2023), LlamaGen (Sun et al. 2024), and MAR (Li et al. 2024a) have incorporated transformers (Vaswani et al. 2017) into diffusion models, offering a different approach compared to the traditional U-Net architecture. GenViT incorporates the ViT (Dosovitskiy et al. 2021; Li et al. 2024e,d) architecture into DDPM, while U-ViT further enhances this approach by introducing long skip connections between shallow and deep layers. DiT demonstrates the scalability of diffusion transformers, and its architecture has been further utilized for text-to-video generation tasks, as explored in works (OpenAI 2024). LlamaGen introduces autoregressive models to image generation, verifying the effectiveness of the 'next-token prediction' in this domain. Thus, it is crucial to explore efficient designs for those large models to accelerate the diffusion process.\n\nAcceleration for Diffusion Models. High-quality image generation with diffusion models necessitates multiple sampling steps, leading to increased latency (Gong et al. 2024; Shen et al. 2023a). To enhance efficiency, DDIM (Song, Meng, and Ermon 2020) extends original DDPM to non-Markovian cases when DPM-Solver (Lu et al. 2022) advances the approximation of diffusion ODE solutions. Regarding the works that require fine-tuning, such as (Lin, Wang, and Yang 2024; Yin et al. 2024), they employ distillation techniques to effectively reduce the number of sampling steps. Additionally, reducing the computational workload for each diffusion step is a widely adopted, strategy to enhance the efficiency of the diffusion process. Various approaches have been explored, such as works (Fang, Ma, and Wang 2023; Castells et al. 2024; Wang et al. 2024a; Zhang et al. 2024) that adopt weight pruning techniques, works (He et al. 2023; Li et al. 2023b) that employ quantization techniques, and even works (Kim et al. 2023; Li et al. 2023c) that redesign the architecture of diffusion models."}, {"title": "3 Methodology", "content": "3.1 Preliminaries\n\nNotations. We use E[] to denote the expectation. We use 1n to denote a length-n vector where all the entries are ones. We use Xi,j to denote the j-th coordinate of xi \u2208 Rn. We use ||x||p to denote the lp norm of a vector x. We use || A|| to denote the spectral norm for a matrix A. We use a ob to denote the element-wise product of two vectors a, b. For a tensor X \u2208 RB\u00d7N\u00d7D and a matrix U \u2208 RD\u00d7d1, we define Y = XU \u2208 RB\u00d7N\u00d7d1. For a matrix V \u2208 Rd2\u00d7B and a tensor X \u2208 RB\u00d7N\u00d7D, we define Z = V \u2022 X \u2208 Rd2XNX D. For a square matrix A, we use tr[A] to denote the trace of A. For two matrices X, Y, the standard inner product between matrices is defined by (X, Y) := tr[XY]. We use U (a, b) to denote a uniform distribution. We use N(\u03bc, \u03c32) to denote a Gaussian distribution. We define cosine similarity as f(X,Y) = \\frac{tr[XY]}{ \\sqrt{tr[XY]} \\sqrt{tr[YY]}} for matrices X, Y.\n\nDiffusion Formulation. Diffusion models (Ho, Jain, and Abbeel 2020; Song et al. 2020) operate by transforming a sample x from its initial state within a real data distribution pp(x) into a noisier version through diffusion steps. For a diffusion model 60(\u00b7) with parameters 0, the training objective (Sohl-Dickstein et al. 2015) can be expressed as follows,\nmin \\mathbb{E}_{t\\sim U[0,1],x\\sim p_p(x),\\epsilon\\sim N(0,I)} ||\\epsilon_\\theta(t, z_t) - \\epsilon||^2,\nwhere t denotes the timestep; e denotes the ground-truth noise; Zt = at. x + \u03c3\u03c4\u00b7 \u20ac denotes the noisy data; at and \u03c3\u03c4 are the strengths of signal and noise.\n\nFor comparison purposes, this paper adopts Denoising Diffusion Implicit Models (DDIM) (Song, Meng, and Ermon 2020) as sampler. The iterative denoising process from timestep t to the previous timestep t' is described as follows,\nZ_{t'} = \\frac{a_{t'}}{a_t} (z_t - \\sigma_t \\epsilon_\\theta(t, z_t)) + \\sigma_{t'} \\cdot \\epsilon_\\theta(t, z_t),\nwhere zt is iteratively fed to e\u0189(\u00b7) until t' becomes 0.\n\nLatent Diffusion Models. The Latent Diffusion Model (LDM) (Rombach et al. 2022b) decreases computational demands and the number of steps with the latent space, which is obtained by encoding with a pre-trained variational autoencoder (VAE) (Sohl-Dickstein et al. 2015). Besides, the classifier-free guidance (CFG) (Ho and Salimans 2022) is adopted to improve quality as follows,\n\\epsilon_\\theta(t, z_t, c) = w \\cdot \\epsilon_\\theta(t, z_t, c) - (w - 1) \\cdot \\epsilon_\\theta(t, z_t, C_\\emptyset),\nwhere e(t, zt, Co) denotes unconditional prediction with null text; w denotes guidance scale which is used as control of conditional information and w > 1.\n\n3.2 Similarity Establishment\n\nLet B be the number of batches, N be the number of patches, D be the hidden dimension, T be the number of diffusion steps, and L be the number of model layers in diffusion transformers. Let f (\u00b7, \u00b7) : RB\u00d7N\u00d7D \u00d7 R\u00c9\u00d7N\u00d7D \u2192 [0, 1] be the function that estimate the similarity between two variables. Let FP(\u00b7) : RB\u00d7N\u00d7D \u2192 RB\u00d7N\u00d7D be the Multi-Head Self-Attention (MHSA) / pointwise feedforward (Feedforward) module at l-th layer where \u03a6 \u2208 {attn, feed} for l \u2208 [L]. We use normalized X \u2208 RB\u00d7N\u00d7D to denote the input hidden states with scaling factor at and shifting factor bt at timestep t in the l-th layer for t \u2208 [T],l \u2208 [L]. We denote the output at l-th layer and t-th timestep as Y.\n\nImpact of Scaling and Shifting. As progressive denoising proceeds, the input difference between, X-1 and X, grows. In contrast, the application of scaling and shifting transformations introduces an alternate problem formulation, potentially affecting the input distance in a manner that requires a different analytical approach. In detail, the diffusion transformer architecture incorporates scaling and shifting mechanisms in the computation of both the MHSA and Feedforward modules, utilizing the embeddings of the timestep, emd(t) \u2208 RD, and the condition, emd(c) \u2208 RD. We define yt = SiLU(emd(t) + emd(c)) \u2208 RD, with corresponding scaling and shifting factors defined as follows,\n\u2022 Scaling factor: at = Wi,a \u00b7 Yt + vi,a \u2208 RD;\n\u2022 Shifting factor: bt = W1,b \u00b7 Yt + 1,b \u2208 RD;\nwhere Wia, Wib \u2208 RD\u00d7D, Vla, Vlb \u2208 RD are the linear projection weight and bias, respectively.\n\nMeanwhile, we define broadcasted matrices to represent the scaling and shifting factors, ensuring the alignment with the implementation of diffusion transformers as follows,\n\u2022 Let At \u2208 RN\u00d7D be defined as the matrix that all rows are at, i.e. (At)i := at for i \u2208 [N], and At\u22121, Bt, Bt\u22121 can be defined as the same way.\n\nThen, we deliver the demonstration showing that there exist Yt, Yt-1 such that, after scaling and shifting, the distance between inputs Xt-1, Xt, defined in the Left Hand Side of the following Eq. (1), can be constrained within a small bound. Given at and bt are both linear transformation of yt, the problem reduces to demonstrating the existence of vectors at, bt, at-1, and bt\u22121 that satisfy following conditions,\n||(A_{t-1} \\odot X_{t-1} + B_{t-1}) - (A_t \\odot X_t + B_t)|| \\le \\eta, \\tag{1}\nwhere \u03b7 \u2208 (0,0.1). And Eq. (1) is equivalent as follows,\n||A \\odot X_{t-1} \\odot X_{t-1} + B \\odot X_t + C||_F \\le \\eta,\nwhere A := At\u22121, B := -At, C := Bt\u22121 - Bt.\n\nWe identify that there exists a, b and e such that Eq. (2) holds, and the detailed demonstration and explanation are included in Lemma 12 at Appendix C.2. Subsequently, we generate the following theorem,\n\nTheorem 1 (Scaling and shifting, informal version of Theorem 13 at Appendix C.2). There exist time-variant and condition-variant scalings and shiftings such that the distance between two inputs at consecutive steps for MHSA or Feedforward is bounded.\n\nSimilarity Lower Bound. To leverage the cache from the previous step, we begin by investigating the cache mechanism in transformer-based diffusion models. This analysis focuses on the similarity between the current output and the"}, {"title": "3.3 Lazy Learning", "content": "preceding one, providing insights into the efficacy of reusing cached information. One typical transformer block consists of two primary modules: MHSA module and Feedforward module. Both modules are computationally expensive, making them significant contributors to the overall processing cost. Thus, we aim to examine the output similarities between the current and previous steps for both modules. By identifying cases of high similarity, we can skip redundant computations, thereby reducing the overall computational cost. In practice, we employ the cosine similarity f(,) for the computation of similarity as follows,\nf(Y_{t-1}^l, Y_t^l) = \\frac{tr[(Y_{t-1}^l)^T Y_t^l]}{ \\sqrt{Y_{t-1}^l}_{F} \\sqrt{Y_t^l}_{F}} \\tag{3}\nInspired by the Lipschitz property (Trench 2013), we transform the similarity measure into a distance metric, defined as Dist := ||Y\u22121 - Y\u2081||, to simplify the analysis of similarity variations. According to Fact 7 in Appendix B.2, similarity function f(,) is further transformed as follows,\nf(Y_{t-1}^l, Y_t^l) = 1 - Dist/2.\nFor convenience, we further define the hidden states after scaling and shifting as Zit := At \u00a9 Xt + Bt. Meanwhile, building upon the Lemma H.5 of (Deng et al. 2023), we further derive the upper bounds for the distance Dist for either the MHSA or Feedforward modules as follows,\nDist \\leq C \\cdot ||Z_{t-1}^{l,t} - Z_{t}^{l,t}||. \\tag{4}\nwhere C is the Lipschitz constant related to the module.\n\nSubsequently, with Theorem 1, we integrate Eq. (1) and derive the bound of the similarity as follows,\nf(Y_{t-1}^l, Y_t^l) \\geq 1 - \\alpha,\nfor \u03b1 := O(C2\u03b7\u00b2) and \u03b7 is sufficiently small in practice.\nThus, we deliver Theorem 2 as below, which asserts that the lower bound of the output similarity between the two consecutive sampling steps is high.\n\nTheorem 2 (Similarity lower bound, informal version of Theorem 18 at Appendix C.4). The lower bound of the similarity f(Y1,Y) between the outputs at timestep t \u2212 1 and timestep t is high.\n\nLinear Layer Approximation. The similarity can be approximated using the inputs from either the current step Zit or previous one Zit\u22121, due to its mathematical symmetry according to Eq. (3). We then apply the Taylor expansion around Z as follows,\nf(Y_{t-1}^l, Y_t^l) = tr[J(Z_{l,t}^z) \\cdot [(-1)(0 + J \\cdot Z_{l,t} + O(1))],\\nwhere J is the Jacobian matrix.\n\nThrough Taylor expansion, we identify that there exists a Wi \u2208 RD\u00d7Dout along with Ze such that the similarity can be linearly approximated with certain error as follows,\n(W_i^T, Z) = f(Y_{t-1}^l, Y_t^l) + O(1),\nwhere the detailed proof is included in Appendix C.5 Eq.(9).\nThen, we generate the Theorem 3 as follows,"}, {"title": "4 Experimental Results", "content": "As illustrated in Figure 2, we incorporate lazy learning linear layers before each MHSA module and Feedforward module to learn the similarity. The MHSA module or Feedforward module is bypassed and replaced with the cached output from the previous step if the learned similarity is below 0.5. The input scale, input shift, output scale, and residual connections remain unchanged from the normal computation. The training details and the calculation of lazy ratio are outlined in the following paragraphs.\n\nTraining Forward. Assume we add linear layers with weights W \u2208 RD\u00d71 for each module F (\u00b7) at l-th layer in the model. For input hidden states X \u2208 RB\u00d7N\u00d7D for the module at l-th layer and t-th step, the similarity st\u2208 RB of the module is computed as follows,\ns_t = sigmoid((Z_{l,t}^W)^T \\cdot 1_D).\nWe then define the forward pass of the MHSA module or Feedforward module at l-th layer and t-th step with the input X during the training progress as follows,\nY_t^{\\Phi} = diag(1_B - s_t) \\cdot F_{l,t}^{\\Phi}(Z_{l,t}^{\\Phi}) + diag(s_t) Y_{t-1}^{l,\\Phi}.\nBackward Loss. Alongside the diffusion loss for a given timestep t during training, we introduce a lazy loss to encourage the model to be more lazy-relying more on cached computations rather than diligently executing the MHSA modules or Feedforward modules, as follows,\nC_{lazy} = p_{attn} \\frac{1}{LB} \\sum_{l=1}^{L} \\sum_{b=1}^{B} (1-(s_{b,t}^{attn})) + p_{feed} \\frac{1}{LB} \\sum_{l=1}^{L} \\sum_{b=1}^{B} (1-(s_{b,t}^{feed})), \\tag{5}\nwhere the pattn and pfeed denote the penalty ratio of MHSA module and Feedforward module, respectively.\n\nWe combine the lazy loss with diffusion loss and regulate pattn and pfeed to control the laziness (i.e., number of skips with cache) of sampling with diffusion transformers.\n\nAccelerate Sampling. After finishing the lazy learning with a few steps, we then accelerate the sampling during the diffusion process as follows,\nY_{l,t-1}^{{\\Phi}} = \\begin{cases} F_{l,t}^{\\Phi}(Z_{l,t}^{\\Phi}), & s_{l,t}^{{\\Phi}} < 0.5, \\ Y_{t-1}^{l,t-1}, & s_{l,t}^{{\\Phi}} > 0.5, \\end{cases}\nwhere \u03a6 \u2208 {attn, feed} can be either MHSA module or Feedforward module, and the skip occurs when sit > 0.5.\nThen, the lazy ratio \u0393\u03a6 \u2208 ZB of MSHA or Feedforward for B batches during sampling is be computed as follows,\n\\Gamma^{\\Phi} = \\frac{1}{LT} \\sum_{l=1}^{L} \\sum_{t=1}^{T} [s_{l,t} - 0.5]^2.\n4.1 Experiment Setup\n\nModel Family. We validate the effectiveness of our method on both the DiT (Peebles and Xie 2023) and LargeDiT (Zhang et al. 2023) model families. Specifically, our experiments utilize the officially provided models including DiT-XL/2 (256\u00d7256), DiT-XL/2 (512\u00d7512), Large-DiT-3B (256x256), and Large-DiT-7B (256x256).\n\nLazy Learning. We freeze the original model weights and introduce linear layers as lazy learning layers before each MHSA and Feedforward module at every diffusion step. For various sampling steps, these added layers are trained on the ImageNet dataset with 500 steps, with a learning rate of le-4 and using the AdamW optimizer. Following the training pipeline in DiT, we randomly drop some labels, assign a null token for classifier-free guidance, and set a global batch size of 256. The training is conducted on 8\u00d7NVIDIA A100 GPUs within 10 minutes.\n\nPenalty Regulation. We regulate the penalty ratios pattn and pfeed for MHSA and Feedforward in Eq. (5) from le-7 to le-2. Both penalty ratios are kept identical in our experiments to optimize performance, as explained by the ablation study results shown in the lower of Figure 5."}, {"title": "4.2 Results on ImageNet", "content": "We present the results generated with DiT officially released models compared to DDIM in Table 1. Full results with more model sizes and lazy ratios are included in Table 5 of Appendix A.1. Due to the addition of lazy learning layers, the computational cost of our method is slightly higher than that of DDIM. Our experiments demonstrate that our method can perform better than the DDIM on DiT models with 256x256 and 512\u00d7512 resolutions. Particularly, for sampling steps fewer than 10, our method demonstrates a clear advantage over DDIM at both resolutions, highlighting the promise of our approach. For larger models with 3B and 7B parameters, we present the results in Table 2. Compared to the DiT-XL/2 model with 676M parameters, Large-DiT models with a few billion parameters exhibit more redundancy during the diffusion process. Full results for Large-DiT models are included in Table 4 at Appendix A.1. Experiments demonstrate that at 50% lazy ratio, our method significantly outperforms the approach of directly reducing sampling steps with DDIM. We further visualize the images generation results in Figure 1. We also compare with other cache-based method Learn2Cache (Ma et al. 2024) which adopts input independent cache strategy and requires full training on ImageNet, the results are in Table 7 at Appendix A.4. For each sampling step, Learn2Cache only has one cache strategy, whereas our method outperforms it with less training cost, demonstrating both the effectiveness and the flexibility of our method."}, {"title": "4.3 Generation on Mobile", "content": "We present the latency profiling results on mobile devices in Table 3. Our method achieves better performance with less"}, {"title": "4.4 Ablation Study", "content": "Individual Laziness. We perform ablation studies on the laziness of MHSA and Feedforward modules separately by regulating the corresponding penalty ratios to determine the maximum applicable laziness for each, thereby exploring the redundancy within both components. We present the results generated with DDIM 20 steps on DiT-XL/2 (256x256) in the upper figure in Figure 5. The analysis indicates that the maximum applicable lazy ratio is 30% for MHSA and 20% for Feedforward modules. The identification reveals that applying laziness individually to either MHSA or Feedforward network is not the most effective lazy strategy, which motivates us to apply the laziness to both modules simultaneously in our experiments.\n\nLazy Strategy. To optimize laziness in both MHSA and Feedforward modules for optimal performance, we fix the laziness in one module and regulate the penalty ratio of the other, varying the lazy ratio from 0% to 40%. Specifically, we separately fix 30% lazy ratio to MHSA or 20% lazy ratio to Feedforward modules, and analyzed the model performance by regulating the lazy ratio of another module with DDIM 20 steps on DiT-XL/2 (256x256). The results, as presented in the lower figure of Figure 5, reveal that the model achieves optimal performance when the same lazy ratio is applied to both MHSA and Feedforward. Thus, we adopt the same penalty ratio for both modules in our experiments to achieve the best performance.\n\nLayer-wise Laziness. To investigate the layer-wise importance during the diffusion process, we examined the laziness"}, {"title": "5 Conclusion and Limitation", "content": "In this work, we introduce the LazyDiT framework, designed to accelerate the diffusion process in transformer-based diffusion models. Specifically, we first highlight redundancy in the diffusion process by showing that the lower bound of similarity between consecutive steps is notably high. Then, we design the lazy learning framework, which incorporates a lazy skip strategy inspired by the Taylor expansion of similarity. Experimental results validate the effectiveness of our method, indicating that it performs better than the DDIM sampler. We further implement our method on mobile devices, achieving better generation performance than DDIM sampler with lower latency. For the limitation, the paper also has some shortcomings, such as the additional computation overhead brought by the lazy learning layers."}, {"title": "Appendix", "content": "B Theoretical Preliminary\nIn this section, we provide the preliminary of our theoretical results.\nB.1 Notations\nFor two vectors x \u2208 Rn and y \u2208 Rn, we use (x, y) to denote the inner product between x, y, i.e., (x, y) = \u2211n=1 Xiyi. We use ei to denote a vector where only i-th coordinate is 1, and other entries are 0. For each a, b \u2208 R, we use a ob \u2208 Rn to denote the Hardamard product, i.e. the i-th entry of (a ob) is azbi for all i \u2208 [n]. We denote the i-th row of a matrix X as Xi. We use 1n to denote a length-n vector where all the entries are ones. We use xi,j to denote the j-th coordinate of xi \u2208 Rn. We use ||x||p to denote the lp norm of a vector x \u2208 Rn, i.e. ||x||1 := \u2211i=1 |xi|, ||x||2 := (n=1 x2)1/2, and ||x||\u221e := maxi\u2208[n] [Xi].\nFor k > n, for any matrix A \u2208 Rk\u00d7n, we use ||A|| to denote the spectral norm of A, i.e. ||A|| := supz\u2208Rn ||AX||2|||X||2.\nWe define || A||\u221e := maxi\u2208[m],j\u2208[n] |Ai,j| for A \u2208 Rm\u00d7n. For a square matrix A, we use tr[A] to denote the trace of A, i.e.,\ntr[A] = \u03a3=1 \u0391i,i. For two matrices X, Y, the standard inner product between matrices is defined by (X, Y) := tr[XTY].\nWe define ||X||F := (\u03a31 \u03a3=1X23)1/2 for a matrix X \u2208Rmxn.\nB.2 Facts\nIn this section, we provide several facts we use.\nFact 4. For a matrix A \u2208 IRmxn, we have\n||A||\u221e \u2264 ||A|| \u2264 \u221amn||A||\u221e.\nFact 5. For a matrix A \u2208 Rmxn, we have\n||A|| \u2264 ||A||F \u2264 \u221ak||A||,\nwhere k is the rank of A.\nFact 6 (Trace). We have the following properties of trace:\n\u2022 ||X||} = tr[XX] for matrix X \u2208 Rm\u00d7n.\n\u2022 tr[XY] = tr[XY] for matrices X,Y \u2208 Rm\u00d7n.\n\u2022 tr[ABC] = tr[CAB] = tr[BCA] for matrices A, B, C \u2208 Rm\u00d7m.\nFact 7. For matrices A, B \u2208 Rmxn, we have\n1-\\frac{||A \u2013 B||} \nWe can show\\frac{||A \u2013 B||} = ||A||}} + ||B||}} \u2013 2tr[A B].\nProof. We can show\n||A \u2013 B||} = tr[(A - B) (A - B)]\n= tr[ATA \u2013 AT B \u2013 BTA + B\u2122 B]\n= tr[AA] + tr[BB] \u2013 2tr[A B]\n= ||A||} + ||B||} - 2 tr[A B]\nwhere the first step follows from Fact 6, the second step follows from the basic algebra, the third and the last step follow from\nFact 6.\nFact 8 (Taylor expansion for matrix). Here, we provide the Taylor expansion for function f(\u00b7) : Rm\u00d7n \u2192 Rm\u00d7n. For matrices\nX \u2208 Rmxn, suppose we expand X around X0. We have\nf(X) = f(Xo) + J \u00b7 (X \u2013 Xo) + O(||X \u2013 Xo||\u00b2)\nwhere J is a Jacobian matrix.\nB.3 Definitions\nIn this section, we provide key definitions we use for the proofs. We define the attention module and feedforward module as\nfollows.\nDefinition 9 (Self-attention module). Given input matrix X \u2208 RN\u00d7D and weight matrices WQ,WK,Wv \u2208 RD\u00d7D, (which represent the query, key and value weight matrices separately), we have self-attention module at l-th layer defined by:\nF_{attn}^{l,t}(X) := D^{-1}AL^{T}, \\tag{NXNNXN NXD} \nwhere (1) V := XWv, (2) W := WoW, (3) A := exp(XWXT), and (4) D := diag(A \u00b7 1n).\nDefinition 10 (Feedforward module). Given input matrix X \u2208 RN\u00d7D, we define the feedforward module at l-th layer as\nF_{feed}^{l,t}(X) := XW_{feed}, \\tag{NXD DXD}\nwhere Wfeed \u2208 RD\u00d7D is the linear weight for the feedforward layer."}, {"title": "C Theoretical Results", "content": "C Theoretical Results\nFor simplicity of proofs, we assume the batch size B = 1. This will only affect the results by a constant factor of B. In this\nsection, we provide the theoretical part of our paper.\nC.1 More Related Work\nLazy Update. The concept of lazy update has emerged as a powerful strategy in addressing computational challenges. Lazy\nupdating defers certain computations or updates until they are absolutely necessary, which can significantly reduce computa-\ntional overhead in large-scale problems. This approach has applications to several fundamental theoretical frameworks such\nas Linear Programming (LP) and Semi-Definite Programming (SDP) (Anstreicher 2000; d'Aspremont et al. 2006; Amini and\nWainwright 2009; Diakonikolas et al. 2019; Dong, Hopkins, and Li 2019; Cohen, Lee, and Song 2021; Jambulapati, Li, and\nTian 2020; Jiang et al. 2020a; Gu and Song 2022; Song, Ye, and Zhang 2023). The application of lazy update strategies ex-\ntends beyond LP and SDP to other important areas in computer science and machine learning. Dynamic maintenance of data\nstructures and algorithms is one such area where lazy updates have proven valuable (Cohen, Lee, and Song 2021; Lee, Song,\nand Zhang 2019; Brand 2020; Jiang et al. 2020b; Brand et al. 2020; Jiang et al. 2020a,c; Song and Yu 2021; Dong, Lee, and\nYe 2023; van den Brand 2020; Huang et al. 2021; Gu and Song 202"}]}