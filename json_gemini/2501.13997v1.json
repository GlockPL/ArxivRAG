{"title": "Predictive Learning in Energy-based Models with Attractor Structures", "authors": ["Xingsi Dong", "Xiangyuan Peng", "Si Wu"], "abstract": "Predictive models are highly advanced in understanding the mechanisms of brain function. Recent advances in machine learning further underscore the power of prediction for optimal representation in learning. However, there remains a gap in creating a biologically plausible model that explains how the neural system achieves prediction. In this paper, we introduce a framework that employs an energy-based model (EBM) to capture the nuanced processes of predicting observation after action within the neural system, encompassing prediction, learning, and inference. We implement the EBM with a hierarchical structure and integrate a continuous attractor neural network for memory, constructing a biologically plausible model. In experimental evaluations, our model demonstrates efficacy across diverse scenarios. The range of actions includes eye movement, motion in environments, head turning, and static observation while the environment changes. Our model not only makes accurate predictions for environments it was trained on, but also provides reasonable predictions for unseen environments, matching the performances of machine learning methods in multiple tasks. We hope that this study contributes to a deep understanding of how the neural system performs prediction.", "sections": [{"title": "1. Introduction", "content": "To survive, humans need to interact with environment through actions, requiring an understanding of how these actions impact the surroundings. This involves building an internal model in the brain to represent the outside world (Knill & Pouget, 2004; Friston & Price, 2001). The success of large language models (LLMs) in understanding token-based worlds also indicates that predicting the next observation is a good objective in learning representations (Radford et al., 2017). However, as humans living in the physical world, our received observations are high-dimensional and diverse. This presents a challenge in understanding how the brain predicts the next observation.\nThe world model (Schmidhuber, 1990; LeCun, 2022) has laid out a basic framework for prediction. Recently, the machine learning society has made significant progress in predicting high-dimensional observations through planning in latent spaces (Ha & Schmidhuber, 2018; Hafner et al., 2019a; 2023; Nguyen et al., 2021). However, these models, not designed for explaining the neural system, lack consideration for biological realism (Chung et al., 2015), and they typically employ biologically implausible training algorithms such as backpropagation (BP) or backpropagation through time (BPTT). In the neuroscience society, there are ongoing efforts to model the hippocampal-entorhinal system as sequential generative models (Whittington et al., 2018; George et al., 2023). However, these approaches either employed a variational method, leading to still requiring BPTT, or assumed access to the underlying state of the world, which is not realistic to the neural system.\nEnergy-based models (EBMs) (Ackley et al., 1985) provide a framework for inference with sampling methods and learning with Hebb's rule. The variability of neuronal responses in the brain has been explained as Monte Carlo sampling (Hoyer & Hyv\u00e4rinen, 2002), which naturally accounts for the regular firing and other response properties of biological neurons (Haefner et al., 2016; Orb\u00e1n et al., 2016; Echeveste et al., 2020). Hebb's rule is a widely observed local learning rule in the neural system. A recent work (Dong & Wu, 2023) has shown that hierarchical EBMs are capable of learning complex probability distributions, suggesting their potential widespread applications in the brain.\nIn this paper, we propose a sequential generative model based on hierarchical EBMs to capture how the brain predicts the next observation after an action (Section 3). In our model (Section 4), a Markov chain of latent variables is employed, whose conditional probabilities following Gaussian distributions. This choice helps us bypass the computation of the partition function, leading to accelerated model convergence. Furthermore, the introduction of error neurons"}, {"title": "2. Related work", "content": "The world model (Schmidhuber, 1990; LeCun, 2022) laid out a framework for predicting observations following an agent's action. Recently, RSSM compresses observations through a variational autoencoder (VAE), then performs predictions in the compressed temporal space using temporal prediction models like RNNs (Chung et al., 2015; Hafner et al., 2019b), Transformers (Chen et al., 2022), S4 models (Samsami et al., 2024) or continuous Hopfield networks (Whittington et al., 2018). Our model adopts this RSSM framework but innovates by incorporating EBMs instead of VAEs and utilizing CANNs for temporal predictions. Our model aligns with biological plausibility, departing from less biologically realistic architectures and training methods.\nActive inference (Friston et al., 2017; Smith et al., 2022) is another framework which can predict the observation after an action. These works are unified under the free energy principle framework (Friston, 2010), modeling the prediction process as a hidden Markov model (HMM, a special case of the RSSM), and using the variational message passing algorithm for inference (Da Costa et al., 2020; Parr et al., 2019). We model the entire process as an RSSM with a temporal model (CANN) that can compress all previous states rather than reliance on the current state alone. Also our model employs a sampling algorithm, enabling online inference and learning without the need to know the entire sequence.\nPredictive coding networks (PCNs) (Rao & Ballard, 1999) can be viewed as an implementation of EBMs, with most current PCN works deal with static inputs (Salvatori et al., 2021; 2023; Millidge et al., 2022), do not involve temporal prediction of the next observation after actions. A recent study, ActPC (Ororbia & Mali, 2023), does introduce actions within the Markov process (a special case of HMM); however, it lacks an encoder-decoder structure, assuming an identity matrix mapping between observations and latent states. Our model integrates an EBM as the encoder-decoder part. Furthermore, while their approach utilizes a buffer to store observations directly, our model employs a CANN to efficiently compress all previous states."}, {"title": "3. Energy-based recurrent state space model", "content": "To build an internal model capturing the change of the environment during interaction, the brain needs to learn to predict the next observation after an action. We consider that the brain employs an energy-based RSSM as the intrinsic generative model for generating predictions. This section outlines the model framework encompassing the predicting, learning, and inference processes, with the detailed neural implementation presented in Section 4.\nProblem setup. Consider an action $a_t$ taken at time $t$. The brain anticipates the observation of the sensory neurons will receive after this action. According to the laws of physics, the world is Markovian, i.e., the next moment is solely determined by the previous moment. However, our observation $O_t$ and action $a_t$ reflect only a subset of the world, and we do not have the full knowledge of the world. To enhance the prediction, the brain can rely on past experiences. Denote"}, {"title": "4. A hierarchical neural network model", "content": "In this section, we propose a hierarchical neural network to implement the above generative model, and outline the specific dynamics involved in prediction, learning, and inference, as discussed in Section 3. Approximating the target distribution $P_{true}(o_t)$, which is diverse and complex, requires a good representation ability of the model. The hierarchical structure has been demonstrated to have strong expressive power and is widely adopted in the biological neural systems. Moreover, we employ a continuous attractor neural network (CANN) to model the memory process. All vectors below are column vectors, and all multiplications are matrix multiplications.\nA hierarchical generative model. Let $s \\in \\mathbb{R}^{n_0}$ be the observation variable. There are $L$ layers of neurons representing the latent variables $s^{0:L} = \\{s_t^0, s_t^1, ..., s_t^L\\}, s_t^l \\in \\mathbb{R}^{n_l}$. The joint distribution is a Markov chain,\n$p_\\theta(s^{0:L}_t|m_t) := p(s^L_t|m_t) \\prod_{l=0}^{L-1} p_\\theta(s^l_t|s^{l+1}_t)$.\nTo avoid calculating the derivation of the partition function in the likelihood function, we utilize the Gaussian distribution, whose partition function is constant,\n$p_\\theta(s^l_t|s^{l+1}_t) := \\mathcal{N}(s^l_t; \\Theta^l f(s^{l+1}_t), (A^l)^{-1}),$\nwhere $f(\\cdot)$ is the element-wise activation function and $A^l$ is the inverse of the covariance matrix called precision matrix. $\\Theta^l \\in \\mathbb{R}^{n_l \\times n_{l+1}}$ are parameters which determine the connectivity structure of the network.\nThe memory network. Attractor neural networks have been widely used as canonical models for elucidating the memory process in the neural system (Amari, 1972; Hopfield, 1982). Among these, CANNs excel in capturing continuous variables, as the underlying state of a temporal sequence is typically continuous. Therefore, we use the activity of CANN neurons at time $t$ as the memory $m_t$. The CANN, through its recurrent connections, forms a series of continuous attractors. This sequence of attractors constitutes a stable low-dimensional manifold, serving as the memory space. In spatially-related tasks, it is also referred to as a cognitive map (O'keefe & Nadel, 1979; Samsonovich & McNaughton, 1997). The CANN receives inputs from actions and from the last layer's latent neurons for prediction and memory update (see Appendix C for the CANN dynamics). When the CANN receives inputs from action $a_t$ and neurons $s^0_{t-1}$, it generates activity $m_t$, which gives rise the neuronal activity in the last layer according to,\n$p(s^L_t|m_t) := \\mathcal{N}(s^L_t; m_t, (A^L)^{-1}).$\nPrediction. At time $t$, the neural network first generates the prediction samples $\\hat{s}^{0:L}_t$ from layer $L$ to 0 according to,\n$\\hat{s}^L_t\\sim p(s^L_t|m_t), \\hat{s}^l_t \\sim p(s^l_t|\\hat{s}^{l+1}_t)$.\nWe use the Langevin dynamic to generate predictions,\n$\\tau_s \\frac{ds^l}{dt} = -\\nabla_{s^l} i p(s^l|\\hat{s}^{l+1}_t) + \\sqrt{2 \\tau_s} \\xi = -A^l \\hat{e}^l + \\sqrt{2 \\tau_s} \\xi,$\nwhere $\\hat{e}^l = s^l - \\Theta^l f(\\hat{s}^{l+1})$ and $\\hat{e}^L = s^L - m_t$ are the value represented by error neurons. We adopt the idea of predictive coding networks (Rao & Ballard, 1999; Whittington & Bogacz, 2017) by introducing error neurons, to satisfy Hebb's rule during learning. The connectivity diagram of neurons is depicted by the dashed box in Figure 2a.\nLearning & inference. After the model receives the observation $s^0_t \\sim P_{true}(s^0_t)$, for $s^l$ represented by neurons in layer one, the likelihood function is $p(s^l|s^{l+1})$ and the prior distribution is $p(s^l|\\hat{s}^{l+1}_t)$. Thus, the prediction bound $\\mathcal{L}^l$ can"}, {"title": "5. Experiment", "content": "We evaluate our model by selecting four types of action in different environments, including eye movement, motion in a virtual environment, motion and head-turning in a real environment, and static observation while the external world varies. To simulate the high-dimensional inputs received by the brain, all observations in our study are exclusively chosen to be visual inputs. We refer to the appendix for hyper parameters (Appendix E).\nEye movement refers to changing the direction of the eyeballs to obtain different visual inputs. It stands as the most frequent actions performed by humans, helping us gather as much visual information as possible. To avoid dizziness caused by rapid eye movement, neurons in the posterior parietal cortex encode stimuli that will be seen after planned eye movements (Cui & Andersen, 2011; Kuang et al., 2016). Additionally, experiments (Seung, 1996) suggest that neurons in the medial vestibular nucleus form a CANN to record eye direction.\nWe utilized the CIFAR-10 and Fashion-MNIST datasets to simulate the environments observed by the model. Each image in the dataset is divided into 4 \u00d7 4 patches, with each patch serving as an input for a single observation to mimic the human receptive field. After each eye movement, the next observation becomes the corresponding patch.\nDuring the learning phase (Figure 2a), we randomly generate a sequence of eye movement, and change the complete image every once in a while. We employed N = 16, 32, 64, 128 images for each model, resulting in a total of $N_{tol} = 16 \\times N$ possible observations. Rows 2-4 of Figure 3a demonstrate the generation results for images encountered during training, while the 5th row illustrates the generation results for unseen images. Figure 3c shows the mean squared error (MSE) between the prediction and the ground truth for different network structures (total number of neurons is the same, $L$ varies), which decreases with training epochs. To increase training efficiency, we used a batch size of 128, and roughly, the effectiveness of one epoch can be considered as the average over 128 time steps. Figure 3d depicts the decreases of loss $\\mathcal{L}^l$ for each layer in the $L=3$ model across training epochs. Here, the phenomenon of gradient vanishing is observed, and we plan to address this by introducing skip connections to deepen the network. Figure 3e displays the impact of network capacity on performance. When initializing memory with $K = 16$ patches, a higher number of neurons corresponds to improved model performance. However, when initializing with $K = 8$ patches, an excessive number of neurons increases the initialization space, posing a challenge and resulting in a decline in model performance.\nDuring the testing phase (Figure 3b), we start by randomly initializing the memory. We select $K$ patches from an image and perform random eye movement on these patches for prediction and inference without altering the network weights. After $2K$ steps, the obtained memory becomes the initialized memory. We then use this memory to envision each patch, generating the whole image. Although the CIFAR-10 dataset has higher dimensions than Fashion-MNIST, we found that the model performs better on CIFAR-10. This may be attributed to the fact that CIFAR-10, as a natural image dataset, exhibits higher correlations between patches, which is more favorable for the model's predictive capabilities. We also compared our model's test results on CIFAR-10 with the commonly used TransDreamer model (TDM) (Chen et al., 2022) for similar tasks in machine learning methods. The results show that our model achieved better performance with the same number of parameters (Table 1). In appendix D, we provide more details about the training process and clearer generated results.\nMotion and head-turning alter our spatial position and the orientation of our head, respectively. Experimental findings reveal neurons encoding position and head orientation in the brain exhibit structures akin to CANNs (Wills et al., 2005; Kim et al., 2017). Notably, place cells, particularly located in the hippocampus, are believed to be closely associated with spatial cognition and memory functions (Moser et al., 2015).\nIn our experiments, we employed an agent moving in four directions within the Deeplab map (Beattie et al., 2016), constructing a dataset for a virtual environment using observed images and action sequences. Additionally, we utilized the Google Street View Static API to capture images by continuously moving and rotating the viewpoint, creating a dataset for a real environment. Each environment was associated with datasets of varying sizes. The first and third rows of Figure 4a display observation sequences for the two datasets, while the corresponding action sequences are illustrated in the two rows of labels. During the training phase, each environment underwent continuous training using models with distinct structures. After a maximum of 100 epochs, all models achieved convergence. In the testing phase, for"}, {"title": "6. Discussion", "content": "We consider that the brain employs an EBM as an intrinsic generative model to predict the next observation after action. We utilize a hierarchical neural network to implement this process and incorporate a CANN as memory to compress past experiences. As a biologically plausible neural network, our model succeeds in various environments with different actions. This provides insight into how the brain builds an internal model capturing the dynamics of the environment. Moreover, our model achieves performances on par with machine learning methods, indicating that our framework has the potential for further development.\nUnlike previous machine learning works (Hafner et al., 2020) or predictive coding network approaches (Tang et al., 2023), our framework differs in that we first perform learning and then inference (see the order in Algorithm 1). In contrast to previous approaches that conduct learning after inference, this order in our framework leads to a distinct objective function. Our objective is expressed as $\\mathbb{E}_{p(s/m)} \\log p(o|s)$ (see Eq.(3)), whereas previous works often use $\\mathbb{E}_{p_{post}} \\log p(o|s)$. We experimented with the latter objective as well, but it resulted in poorer and less robust performance in our model. In fact, our approach of learning before inference is closer to the autoregressive models used in LLMs.\nIn the current model, when computing the gradient of the objective function with respect to the model parameters, we ignore the influence of parameters on memory (see Eq.(5)). This can be understood as a form of Truncated BPTT. Nevertheless, in experiments related to movement, we found that this does not affect our long-distance predictions.\nFuture work. Due to the Markovian nature of our generative model, our framework can seamlessly integrate with reinforcement learning (RL). By simply defining additional rewards for specific tasks, we can achieve model-based RL, creating a biologically plausible world model. In our current model, though direct access to the underlying state is not available, the utilization of CANNs effectively establishes a prior structure for the underlying state. We have not yet thoroughly investigated the impact of the environment dynamics on the CANN structure. The two for-loops in Algorithm 2 are executed sequentially. In neural systems, however, all neurons compute simultaneously. It has been demonstrated that by introducing a modulation function for error neurons, both for-loops can be unrolled to achieve parallel computation (Song et al., 2020). Nevertheless, when using a real continuously changing environment, we still need to carefully adjust the model's time constants to ensure it can maintain synchronized interaction with the real environment. We will explore these aspects in our future work."}, {"title": "A. Lower bound derivation 1", "content": "Firstly, we can prove that,\n$\\p(o|s) \\log p(o|s) - \\log p(o|s)$,\n$= [p(o|s) - 1] \\log p(o|s)$,\n$\\geq 0$\nThen, the mutual information between sample $o \\sim P_{true}(o)$ and $s$ under $p$ distribution is calculated as,\n$\\mathbb{E}_{o\\sim P_{true}(o)} I(o, s|m) = \\mathbb{E}_{o\\sim P_{true}(o)} \\mathbb{E}_{s,m\\sim p(o,s,m)} \\log \\frac{p(o, s, m)p(m)}{p(o,m)p(s,m)},$\n$= \\mathbb{E}_{o\\sim P_{true}(o)} \\mathbb{E}_{s,m\\sim p(o,s,m)} \\log \\frac{p(o, s, m)}{p(s,m)} + \\mathbb{E}_{o\\sim P_{true}(o)} \\mathbb{E}_{m\\sim p(m)} \\log \\frac{p(m)}{p(o,m)},$\n$= \\mathbb{E}_{o\\sim P_{true}(o)} \\mathbb{E}_{s,m\\sim p(o,s,m)} \\log p(o|s) + Const,$\n$\\mathbb{E}_{o\\sim P_{true}(o)} \\mathbb{E}_{m\\sim p(m)} \\mathbb{E}_{p(s|m)} p(o|s) \\log p(o|s),$\n$\\geq \\mathbb{E}_{m\\sim p(m)} \\mathbb{E}_{o\\sim P_{true}(o)} \\mathbb{E}_{p(s|m)} \\log p(o|s).$\nThe relation between $H$ and $L$ is written as,\n$\\mathcal{L} = H+ \\mathbb{E}_{o\\sim P_{true}(o)} D_{KL} [P(s|m)||P_{post}]$."}, {"title": "B. Lower bound derivation 2", "content": "The mutual information between $o$ and $s$ under $q$ distribution is calculated as,\n$I(o, s|m) = \\mathbb{E}_{o,s,m\\sim q(o,s,m)} \\log \\frac{q(o, s,m)q(m)}{q(o,m)q(s,m)},$\n$= \\mathbb{E}_{o,s,m\\sim q(o,s,m)} \\log \\frac{q(o, s,m)}{q(s,m)} + \\mathbb{E}_{o,m\\sim q(o,m)} \\log \\frac{q(m)}{q(o, m)},$\n$= \\mathbb{E}_{o,s,m\\sim q(o,s,m)} \\log q(o|s) + Const,$\n$\\mathbb{E}_{o,s,m\\sim q(o,s,m)} \\log q(o|s),$\n$> \\mathbb{E}_{o,s,m\\sim q(o,s,m)} \\log q(o|s) - \\mathbb{E}_{s\\sim q(s,m)} D_{KL} [q(o|s)||p(o|s)],$\n$= \\mathbb{E}_{o,m\\sim q(o,m)} \\mathbb{E}_{s\\sim q(s|o,m)} \\log p(o|s).$"}, {"title": "C. The CANN dynamics", "content": "We use $m_t \\in \\mathbb{R}^{N_L}$ to represent the firing rate of the CANN at time $t$, and $I_t \\in \\mathbb{R}^{N_L}$ to represent the total synaptic input to the neurons in CANN. According to the integral firing model, the firing rate can be approximated as,\n$m_t = H(I_t),$\nwhere $H(\\cdot)$ is a nonlinear function. The dynamics of $I_t$ are determined by its own relaxation, recurrent inputs from other neurons, neural adaptation $V_t$, and external inputs from $s_t$ and action neurons, as expressed by the following equation:\n$\\tau_I \\frac{dI_t}{dt} = -I_t + W m_t - V_t + s_t + a_t$\nHere, $\\tau_I$ represents the synaptic time constant, and $W$ denotes the recurrent neuronal connections. $W$ is a randomly generated low-rank matrix, where its norm is controlled by the hyperparameter $a = ||W||$. The dynamic of $V_t$ is written as,\n$\\tau_V \\frac{dV_t}{dt} = -V_t + \\beta m_t$\nwhere $\\tau_V$ is the adaptation time constant and $\\beta$ is a scalar controlling the adaptation strength."}, {"title": "D. Supplementary figures", "content": "Figure 5a illustrates the learning process of the eye movement experiment. Figure 5b presents clearer generated images, while Figures 5c and 5d depict the changes in MSE and layer losses during the model training process in the eye movement experiment. Figure 6 showcases the experimental results from static observations as the environment changes."}, {"title": "E. Hyper Parameters", "content": "Here are the hyperparameters used in the experiment. All programs are run on one NVIDIA RTX A6000, and we use JAX (CUDA 11) to accelerate the programs. For the experiments depicted in our figures, each one takes 5-20 minutes, with the best performance on the DeepLab and Google Street datasets requiring about 10 hours. The code will be open-sourced after publication.\nFor all stochastic differential equations, we employ the Euler method for simulation with a step size of $dt$. Both inference and learning after a single observation were conducted over a total simulation time of $T$. In other words, for a time step from $t$ to $t + 1$, the simulation occurs $T/dt$ times. The duration of operation for each layer is uniform. All models use the leaky_relu activation function denoted as $f(\\cdot)$. The parameter $\\tau_s, \\tau_I$ and $\\tau_V$ are set to 1 in all models.\nIn Tables 3 and 2, we used the corresponding parameter settings from the original texts. For the TransDreamer in Table 1, the parameter settings are as follows:\n\u2022 Attention head = 8, Dropout = 0.2, Hidden size = 128, Model size = 64, Layers = 3"}]}