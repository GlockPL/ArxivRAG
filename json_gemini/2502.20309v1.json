{"title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants", "authors": ["Franck Cappello", "Sandeep Madireddy", "Robert Underwood", "Neil Getty", "Nicholas Lee-Ping Chia", "Nesar Ramachandra", "Josh Nguyen", "Murat Ke\u00e7eli", "Tanwi Mallick", "Zilinghan Li", "Marieme Ngom", "Chenhui Zhang", "Angel Yanguas-Gil", "Evan Antoniuk", "Bhavya Kailkhura", "Minyang Tian", "Yufeng Du", "Yuan-Sen Ting", "Azton Wells", "Bogdan Nicolae", "Avinash Maurya", "M. Mustafa Rafique", "Eliu Huerta", "Bo Li", "Ian Foster", "Rick Stevens"], "abstract": "Recent advancements have positioned AI, and particularly Large Language Models (LLMs) as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants, but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology's state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) have greatly broadened conceptions of what AI may be able to accomplish in the near future. Models such as OpenAI's GPT 01 [1], Google's Gemini [2], and Anthropic's Claude [3] are transforming traditional natural language understanding (NLU) tasks like summarization, information extraction, translation, and classification with enhanced contextual depth and adaptability. They are also exhibiting promising potential beyond NLU, with measurable progress on tasks such as mathematical problem solving, multi-step reasoning, and symbolic logic and achieving significant milestones such as passing the Uniform BAR exam and medical licensing exams [4]. Such achievements highlight their potential to emulate abstraction, logical deduction, and domain-specific expertise. This evolution from NLU towards addressing complex, domain-specific challenges with minimal human guidance has propelled LLMs into a pivotal role for next-generation AI systems, positioning them as a cornerstone technology in the quest toward more general-purpose AI, and potentially, artificial general intelligence (AGI).\nBuilding on these advancements, scientists are now begin-ning to assess separately the suitability and potential impact of LLMs on a wide variety of specific tasks within specific fields of research and discovery [5]. This work has led to exciting demonstrations of LLMs as transformative tools for such tasks predicting molecular properties [6], uncovering genomic patterns [7], interpreting astrophysical data [8], solving mathematical problems [9], and even creating and manipulating tools for simulations and analysis [10].\nThese developments have also led scientists to envision the use of LLMs and transformers as research assistants that can not only automate individual research tasks but also engage with scientific problems in depth by taking advantage of growing multi-step reasoning skills that complement their expanding contextual understanding. This vision suggests a new holistic approach in which LLMs interface with relevant tools, operate (quasi-)autonomously on research challenges, identify"}, {"title": "II. RELATED WORK", "content": "Research on LLM evaluation encompasses various tech-niques relevant for the evaluation of LLMs as research as-sistants. Here we discuss that work and note gaps that our proposed methodology attempts to fill.\nA. Multiple-choice Question (MCQ) Benchmarks\nMCQ benchmarks offer a structured framework for as-sessing LLM performance across various domains. Notable examples are Massive Multitask Language Understanding (MMLU) [13] and MMLU-PRO [14], which evaluate general knowledge and reasoning in more than 50 subjects, including humanities, sciences and engineering. In the realm of mathe-matical reasoning, GSM8K [15], GSM1K [16], and MATH [9] are prominent. GSM8K and GSM1K addresses grade-school level problems, while MATH focuses on high-school and competition-level questions. Both benchmarks have been enhanced with multiple-choice adaptations to streamline eval-uation and minimize ambiguity in model outputs [17]. Other significant MCQ benchmarks include ARC [18], which tests scientific reasoning, and HellaSwag [19], which challenges models with complex commonsense reasoning scenarios that, while easy for humans, are especially hard for state-of-the-art models. In the field of chemistry, MCQ benchmarks include MoleculeQA [20], which comprises 61,574 MCQs, each with three distractors, focusing on factual information about molecules; MolPuzzle [21], a multimodal benchmark with over 23,000 question-answer pairs, structured with inter-linked sequential sub-tasks, each providing multiple choices; and ChemBench [22], with over 2700 questions, primarily MCQs. Few of the many other MCQ benchmarking efforts in the literature are science-domain focused and validated, and as LLM capabilities improve, there is an increasing need to generate more difficult questions and leverage synergies between domain expertize and LLM judges. The multi-domain benchmark AI4S that we present below is an attempt towards that end.\nB. Open Response Benchmarks\nWhile MCQ benchmarks restrict responses to predefined options, Open Response benchmarks require that LLMs pro-duce detailed, unconstrained outputs that can be evaluated for coherence, accuracy, and relevance. Notable examples include NarrativeQA [23], which challenges models to gen-erate summaries or interpret longer narratives, and HotpotQA [24], which demands multi-hop reasoning with synthesized answers derived from multiple sources. Similarly, HybridQA [25] combines textual and tabular data, requiring LLMs to provide coherent and comprehensive answers. For mathe-matical reasoning, GSM8K [15] and MATH [9] test model ability to solve complex, multi-step problems with free-form solutions. In chemistry, benchmarks have been developed to assess LLM open-response capabilities. ChemistryQA [26] comprises 4500 complex questions that require reasoning and calculations, evaluating model ability to generate detailed, accurate responses. ChemLLMBenchmark [27] consists of eight practical chemistry tasks that necessitate understanding, reasoning, and explanatory skills, with evaluations focusing on the quality and depth of model-generated answers. The open-domain TOMG-Bench [28] molecule generation benchmark comprises tasks such as molecule editing, optimization, and customized generation, each requiring models to produce specific molecular structures or modifications based on textual descriptions.\nRecent advancements have expanded the scope of open-response benchmarks to address specific evaluation challenges and domains. The Open-LLM-Leaderboard (OSQ-bench) [29] transitions from multiple-choice formats to open-style ques-tions, eliminating issues like selection bias and random guess-ing, while emphasizing models' ability to generate coherent, contextually accurate answers. FrontierMath [30] presents hundreds of exceptionally challenging mathematics problems that require models to generate detailed solutions, testing advanced reasoning and problem-solving skills. Similarly, Humanity's Last Exam [31] crowdsources complex questions from experts across fields to evaluate how closely LLMs ap-proximate expert-level capabilities, highlighting their potential and limitations in addressing real-world challenges.\nA difficulty with Open Response benchmarks is that eval-uating their responses is inherently challenging due to their unstructured nature, requiring time-intensive analysis, subjec-tive interpretation, and careful management of biases and data overload. These challenges are closely tied to uncertainty quantification (UQ), as the variability in interpretations and outcomes necessitates robust techniques to quantify and mit-igate uncertainty in the evaluation process, ensuring reliable and consistent insights. In addressing these challenges, it is also important to keep track of multiple model versions and to use them consistently [32], so as to enable reproducibility [33].\nC. Lab-Style Experiments\nLaboratory-style Experiments with LLMs involve controlled settings in which researchers systematically evaluate model performance on specific tasks, enabling precise measurement of capabilities and limitations. In chemistry, for instance, [34] developed Coscientist, which employs LLMs to plan and execute experimental procedures based on simple human prompts, and evaluated its performance by assigning it the task of identifying synthetic procedures for seven molecules of varying complexity. Similarly, [35] proposed an approach that combines LLMs with task and motion planning to trans-late natural language instructions into robot-executable plans, evaluating their system through simulations in a box-packing domain. In behavioral strategy research, [36] reproduced hu-man laboratory experiments using LLMs and compared their performance to human participants to analyze the extent to which LLMs can emulate human decision-making processes. These laboratory-style experiments provide valuable insights into applications and can inform the development of more advanced AI systems. However, comprehensive end-to-end evaluations of LLMs on scientific tasks, similar to human"}, {"title": "D. Field-Style Experiments", "content": "Field-style Experiments, also referred to as \u201cin-the-wild\u201d studies, involve observing and analyzing user interactions with LLM in real-world settings. This approach contrasts with controlled Lab-style Experiments by capturing non-predefined user interactions with LLMs, providing valuable insights into how LLMs perform across diverse, unstructured tasks. Recent studies have leveraged this methodology to assess various aspects of LLM performance. For instance, WildBench [37] is designed to evaluate LLMs using real-world user interactions, enabling a comprehensive analysis of model capabilities in practical scenarios. Similarly, HaluEval-Wild [38] was designed to evaluate hallucinations in LLMs by collecting challenging user queries from real-world inter-actions. Shen et al. [39] conducted a study characterizing and evaluating user-LLM interactions, providing insights into user behavior and model performance and highlight the importance of understanding user needs and expectations to improve LLM utility and user satisfaction. While such analyses are currently lacking in scientific domains, developing field-style experi-ments specifically for scientific research presents a significant opportunity to provide critical insights into how researchers interact with AI models, thereby enhancing the creation of AI assistants tailored to scientific inquiry."}, {"title": "E. Safety Evaluation", "content": "In addition to the general-purpose evaluations discussed above, comprehensive evaluations must rigorously assess alignment with ethical standards, robustness against jail-breaks, and adaptability to complex real-world scenarios [37], [40]. Safety evaluations are also cross-cutting :MCQs, Open-Response benchmarks, and Lab-style and Field-style Experi-ments. SafetyBench [41], for example, employs 11,435 MCQS in seven categories (e.g., bias, toxicity) to systematically test model adherence to ethical and safety standards in both English and Chinese. Similarly, SALAD-Bench [42] proposed 4000 MCQs, and part of the larger dataset structured into a detailed hierarchy of 6 domains, 16 tasks, and 66 categories. BigBench [43] has a subset of its tasks focused on safety evaluation in regards to toxicity, bias and truthfullness tat are MCQs. However, there has not been a focused MCQ-based safety evaluation benchmarks that take the nuances aspects of science, especially high consequential ones such as the the chemisty, biology, radiation, and nuclear (CBRN). We discuss one such effort on risks in chemistry in this work. Open-response evaluations, such as those in DecodingTrust [44] and TrustLLM [45], examine nuanced safety aspects, in-cluding hallucinations, privacy violations, and machine ethics. TrustLLM evaluates LLMs across six dimensions, including fairness and safety, using over 30 datasets to identify crit-ical safety gaps, while DecodingTrust introduces an eight-dimensional framework that probes issues like toxicity and ethical reasoning, with results published on a widely accessible leaderboard for transparency. However, such evaluations for scientific use cases are scarce [40] and thus provide an opportunity to develop them in the future.\nThe safety red-teaming methodologies can be effectively interpreted along the lines of lab-style and field-style exper-iments. In lab-style red-teaming, researchers interact directly with LLMs and systematically introduce adversarial prompts to identify vulnerabilities such as biases, hallucinations, or ethical compliance issues. For example, [46] present a frame-work for red-teaming experiments on LLMs by generating numerical questions and puzzles to evaluate the models' per-formance on elementary calculations and algebraic tasks. This approach provides detailed feedback at each step, allowing iterative improvements and a deeper understanding of model limitations. In contrast, field-style red-teaming assesses LLMs by analyzing large-scale human interactions in real-world environments. This method captures diverse and unpredictable user inputs, offering insights into how models perform \u201cin the wild.\" For instance, [47] discuss automating red-teaming by training a separate red team LLM with reinforcement learning to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. This large-scale approach identifies practical weaknesses and vulnerabilities that may not surface in controlled lab settings, contributing to model robustness across varied real-world scenarios. By employing both lab-style and field-style red-teaming strategies, researchers can comprehensively evaluate and enhance the safety, reliability, and ethical performance of LLMs across different contexts. That said, significant work still needs to be done in designing such experiments for safety and trust scenarios in science.\""}, {"title": "III. ESTABLISHING A METHODOLOGY TO EVALUATE LLMS AS RESEARCH ASSISTANTS", "content": "The overarching objective of using LLMs as research assis-tants is to accelerate the research process.\nIn Table II, we present seven tasks commonly performed by researchers to solve a scientific problem: (i) posing and formulating a research question; (ii) if needed, conduct-ing initial, preliminary experiments, observations, simulation, and/or database accesses to confirm the pertinence of the research question; (iii) performing literature search to identify related work; (iv) generating potentially multiple hypotheses (or research directions) to address the research question; (v) designing experiments, observations, and/or simulations to test the hypotheses; (vi) analyzing the resulting data to validate or invalidate the hypotheses; and (vii) writing a report about findings. Each of these seven tasks requires deep reason-ing, contextual understanding, and iterative problem solving. The scientific community needs confidence that LLMs are proficient in these tasks if they are to adopt them widely as research assistants. Thus a thorough and comprehensive evaluation methodology is required that can produce a rigorous assessment of LLM strengths and weaknesses in each task."}, {"title": "A. Domain-Specific MCQ Benchmarks", "content": "As discussed in the related work section, most LLM evalua-tions take the form of MCQ benchmarks, which are a subset of broader Q&A evaluations. However, while these benchmarks serve a useful purpose in quickly assessing the breadth of knowledge of LLMs, that cannot, by their very nature, assess the depth of reasoning, contextual understanding, and iterative problem-solving required in the various steps of the scientific research process [48], [49].\nTo address these limitations, it is critical to complement MCQ benchmarks with end-to-end evaluations in realistic contexts that assess both reasoning across multiple dimensions and the ability to plan and adapt across multi-step tasks. Therefore, we propose Evaluating AI models as scientific Research Assistants (EAIRA), a structured evaluation method-ology (Table I) that combines four techniques: two existing approaches that allow for quick and repeated assessment of the breadth of LLM abilities (MCQ Benchmarks and Open-Response Benchmarks) plus two new approaches for end-to-end evaluation of LLMs as scientific assistants (Lab-style Experiments and Field-style Experiments).\nMCQ benchmarks serve to test foundational knowledge and domain-specific expertise across diverse scientific fields. In addition to previously developed benchmarks, members of our team have developed three new MCQ benchmarks to address topic gaps in tasks for which MCQs are well suited. 1) Astro and 2) Climate are domain-specific benchmarks primarily generated through automated techniques to ensure the scalability of benchmark generation to serve new and evolving topics in science. 3) AI4S is a multi-domain \u201cAI for science\u201d MCQ benchmark, which integrates human ques-tion generation and validation with automated generation and validation methods to achieve a balanced, high-quality dataset that can be developed with moderate effort. By leveraging both human and automated techniques, we can assess the strengths and weaknesses of automated methods deployed in Astro and Climate in order to guide future research in improving their generation.\nOpen-response benchmarks serve to test more detailed knowledge and to generate open-ended responses or code that assesses a model's dynamic capability to handle unstructured, complex tasks, moving closer to reflecting the flexibility and adaptability required in real-world research scenarios while still facilitating a fast, automated evaluation. For this, our team has previously developed two benchmarks: SciCode, which assesses the ability of LLMs to develop code which is highly dependent on the knowledge of the context of a scientific domain to correctly implement, and ALDbench, which assesses the ability of LLMs to describe methods for synthesizing materials using atomic layer deposition.\nLab-style Experiments perform evaluation by domain ex-perts of AI models as they assist across all research tasks in real situations. This expert-reviewed method provides a comprehensive assessment of the relevance, effectiveness, and iterative improvement of a model over time (e.g., 4-12 hours per attempt). This technique goes beyond open response by using a human proctor to guide an expert to iteratively interact with the model in order to assess multi-stage planning and response to results in the context of a scientific domain. Because of the interactivity, it provides very granular feedback about what models are or are not capable of performing giving a unique insight into the strengths and weaknesses of models and possible paths for improvement. However, this capability comes at the cost of extensive effort by experts and proctors.\nField-style Experiments (inspired by, and adapting \u201cin-the-wild\u201d evaluations [39], [50] for the scientific context) ana-lyze automatically thousands of prompts, responses, and user behaviors during real-world interactions between researchers and AI assistants. This approach allows for large-scale capture and analysis of user needs, model strengths and weaknesses, and usage trends. It differs from lab-style experiments in that: 1) experts are not guided by a proctor, 2) analysis of interactions and feedback is automated, and 3) experiments can be conducted in the background as experts perform routine tasks (e.g., by capturing outputs of a site-wide LLM inference service or API proxy). These differences greatly improve the scalability of realistic end-to-end experiments but at the loss of the fine-grained granularity of the strengths and weaknesses detected by the lab-style approach.\nIn applying these four techniques, three key cross-cutting aspects must be considered: ethics, trust and safety; reliable uncertainty quantification; and scalable software infrastructure. These three aspects ensure that LLMs are \"aligned\" with human values, produce and qualify results correctly under uncertainty; and can be evaluated efficiently at scale. Trust and safety evaluations must address ethical alignment, defend against jailbreak attempts, and adapt to complex real-world contexts. Multiple-choice strategies [41] can identify biases, toxicity, and compliance gaps, while open-response tasks [44] can probe subtle issues such as hallucinations and machine ethics. Lab-style red-teaming [46] systematically challenges the model with adversarial prompts in controlled settings to expose stepwise weaknesses, while field-style red-teaming [47] tests the model's robustness amid unpredictable real-world inputs, unveiling vulnerabilities that may remain hidden in laboratory conditions. Reliable uncertainty quantification (UQ) is equally critical to establish trust in AI-driven sci-entific research assistants, as it systematically gauges model confidence and highlights potential inaccuracies [51]\u2013[53]. UQ insights guide targeted refinements in MCQ, open-response, and lab- or field-style evaluations, ensuring that areas of high uncertainty are addressed in scientific and real-world contexts [54], [55]. Together, robust safety evaluations and UQ foster transparency, accountability, and trust, facilitating the responsible integration of LLMs into critical scientific research [40], [56]. Finally, the scalable software infrastructure enables rapid evaluation to keep pace with rapid changes in the field of AI research. The framework needs to incorporate distributed task parallelism [57] and fast inference capabilities [58].\nThe following subsections present greater detail and results of the four techniques and three aspects of our evaluation methodology."}, {"title": "B. Scalable AI4S MCQ Benchmark", "content": "Domain-specific benchmarks are crucial for evaluating LLMs in specialized fields, as they address the limitations of general benchmarks that often fail to capture the complexities of domain-specific tasks. Without tailored benchmarks [8], LLMs risk over training on well-established datasets, leading to inflated performance that does not translate to real-world applicability. These benchmarks are essential for guiding tar-geted improvements, ensuring that models meet the specific demands of scientific research, and providing a baseline to understand their strengths and weaknesses. By capturing the nuanced challenges of individual domains, such benchmarks foster the effective and ethical deployment of LLMs, enabling their potential to accelerate discovery and innovation across disciplines. We now discuss the domain-specific benchmarking efforts that we conducted in Astronomy and Climate modeling.\n1) Astronomy: The Astronomy Benchmark [8] assesses LLM performance in a manner that reflects the interdisci-plinary nature of astronomy, testing both factual recall and the ability to connect insights across subfields. This benchmark was generated automatically using an LLM to compose MCQS from astronomy papers. To assemble a rich repository of scientific knowledge, we leveraged the Annual Review of Astronomy and Astrophysics (ARAA), a selective review journal renowned for its comprehensive overviews authored by leading experts.\nThe Nougat optical character recognition (OCR) tool was used to transcribe 885 ARAA articles over the years 1963 to 2023. Each transcribed paper was processed using Gemini-1.5-Pro, a long-context LLM capable of handling up to one million tokens, to generate five multiple-choice questions (MCQs) per paper. The questions were designed to be specific, yet independent of the article sections, with generalized answers and balanced options to avoid bias. This process yielded a total of 4425 MCQs covering diverse topics such as quasar density decline at high redshifts and subgrid feedback model calibration in simulations.\nThe overarching goal of the AI4S benchmark is to evaluate the knowledge extension of LLMs across many different scientific domains. In that respect, it is similar to GPQA [63]. However, AI4S design focuses on the quality and scalability of MCQ generation and validation. While GPQA has only 448 MCQs, our objective is to generate and validate thousands of MCQs and continuously add MCQs as LLMs progress in their capabilities.\nCurrent MCQ benchmarks, including GPQA, have two main limitations: 1) they are quickly saturated because of the fast progress of LLMs. 2) there is a high risk of contamination (benchmark included in the training sets) if the MCQs are made public. These two limitations arise from the static aspect of the current benchmarks. They are developed once and do not evolve. The current practice is to develop new versions [64] when the initial benchmark is saturated [65] and to open only a portion of the benchmark MCQs to avoid contamination.\nTo address the two limitations, we explore a novel approach to develop scalable generation and validation of MCQ bench-marks for the evaluation of LLMs capabilities: Automatic continuous Generation and validation of Increasingly Large MCQ benchmarks: AGIL.\nBuilding on the experience gained from the Astronomy and Climate domain-specific benchmarks, we are developing an AGIL process by which we combine manual and automated methods to generate and validate MCQs.\nWe present here our initial finding towards the creation of a 1000-MCQ AI4S benchmark that spans five scientific"}, {"title": "2) Climate", "content": "Gemini or Claude. An analysis of the cost per 0.1 M tokens showed that the cost for a desired performance can vary by more than three orders of magnitude across different models: see Figure 2 in [8].\nThe study also showed that open-weight models, though improving, lag behind proprietary ones, with older versions underperforming by as much as 30% on specialized tasks. Performance also varies significantly between English-focused and non-English-focused models, with the latter struggling in areas like theoretical astronomy and advanced instrumentation, reflecting gaps in training datasets. However, recent astronomy subfields, such as post-1990 advancements, exhibit narrower performance gaps which may be due to model's ability to handling historical context or older scientific consensus. These findings emphasize the importance of domain-specific bench-marks to assess not only performance in specialized tasks but also the performance-to-cost ratio crucial for user adoption in assisting with scientific research. This work also shows that performance varies across sub-fields.\nClimate and weather forecasting presents mul-tifaceted challenges that demand interdisciplinary knowledge and reasoning, making it an ideal testbed for evaluating LLM capabilities. However, existing benchmarks for climate science are limited [59]\u2013[61]. Thus we developed a Climate Benchmark, a set of MCQs focused on the urgent and complex domain of climate science. As the manual creation of such MCQs is labor intensive and climate scientists are already overcommitted, we employed automated methods to develop the Climate Benchmark. We adopted as our source material the Intergovernmental Panel on Climate Change (IPCC) reports [62], which are authoritative assessments synthesizing the latest scientific research on climate change, its impacts and potential solutions. These reports serve as a foundation for informed decision-making and international negotiations on climate action, highlighting the urgency of addressing the complex and interconnected challenges posed by changing climate. The reports are typically extensive, often exceeding 1000 pages, with each chapter and section addressing specific topics related to climate change. To generate questions on the various topics discussed in the report, we parsed the PDFs section-wise, using Nougat as was done for the Astronomy Benchmark. We then employed OpenAI's GPT4 to create one multiple-choice question, consisting of one correct answer and four distractors, for each section, with the prompt designed to create an MCQ based on the provided scientific text, ensuring that the question evaluates a broader understanding of climate science principles without referencing the specific report. This process resulted in a total of 752 questions on diverse topics, ranging from factors influencing vulnerability to climate change to primary strategies for risk reduction. This systematic approach ensured comprehensive coverage and alignment with the content of the IPCC reports.87.34%, demonstrating its effectiveness in handling complex climate-related tasks. GPT-40 was followed by Llama 3.8, which achieved an accuracy of 78.48%, and Phi 4, which scored 54.43%. This performance disparity highlights the need for continued refinement and optimization of models to bridge the gap in specialized applications.\nThe development of the Climate Benchmark provided useful insights into the creation and evaluation of MCQ datasets for scientific applications. The climate-focused MCQs, derived from IPCC reports, were designed to assess knowledge recall and decision-making, emphasizing accurate understanding of scientific concepts. While primarily testing factual knowledge, these questions establish a strong foundation for expanding into tasks that require more complex reasoning and application in climate science. However, the automatic generation of MCQs sometimes produced semantically similar questions that differed only slightly in phrasing or structure while testing the same core information. This observation highlights the need for robust evaluation mechanisms to eliminate redun-dancy and ensure diversity within the dataset. While automatic MCQ generation greatly accelerates the creation of benchmark questions, it cannot replace a rigorous evaluation process. A combination of LLM-based evaluators and human oversight is crucial for maintaining the benchmark's quality, relevance, and accuracy, ensuring that it meets the standards required for research-focused benchmarks."}, {"title": "C. Open Response Benchmarks", "content": "domains: Computer Science, Astrophysics, Climate, Physics, and Chemistry. Our goal with this first study is to assess key aspects of the AGIL approach: validity of the difficulty level, quality of the generated MCQs compared to GPQA ones, quality of the automatically generated MCQs compared to manually generated ones, and quality of the automatically validated MCQs.\nIn the AGIL approach (Figure 1), manual MCQ generation and validation by experts is critical to provide high-quality, domain-specific MCQs that serve as a gold standard for evaluating LLM performance and developing automatic MCQ generation and validation workflows. The goal of automated generation and validation (LLM as a judge technique [66]) by LLMs is to address the scalability issues of manual generation and validation while keeping their quality levels.\nFor the manual generation and validation of MCQs we organized hackathons which engaged 140 domain experts (PhDs and other research staff) 1. We offered the participants to generate MCQs from scientific papers of their choosing, including their own. The manually generated MCQs were crafted by using a purpose-built authoring interface (Appendix, Figure 9) that allowed contributors to test their questions on smaller models like Llama 3 before submission so as to ensure a baseline difficulty threshold. Manual validation used another specifically created form (see Appendix, Figure 10). The tool used for the MCQ generation and validation is available on github (https://github.com/auroraGPT-ANL/questions-ui)\nAutomated MCQ generation leveraged LLMs such as GPT-4 with domain-specific prompts to create MCQs from scientific1the exercises in this paper were done with volunteer Argonne employees, who understood that the goal of this effort is to identify opportunities to improve models. While the research team offered a rubric for notes and observations, the participants were free to use whatever rubric they preferred. The following is the general approach that was used.\npapers across fields such as climate science, physics, and chemistry, with validation guided by the LLM-as-a-Judge technique. Both the prompts used for automatic MCQ gen-eration and validation, as well as the rubrics for validation and reviews, were informed by the manual generation and validation, as well as by the experience gained during the domain-specific benchmark development discussed earlier.\nThis process generated 980 MCQs (720 manually and 260 automatically). Of 588 total manually generated reviews, 317 MCQs have been assessed so far, of which 254 were accepted. Acceptance was subject to the following criteria: appropriate difficulty, relevant, complete and correct answers and distrac-tors, controversial answers, mathematic requirements, as well as relevant skills and domain selection. The small percentage of accepted MCQs 25% illustrates the difficulty of generating and validating high-quality scientific MCQs.\nIn addition, domain experts categorized the accepted MCQs into easy, medium, and hard levels, capturing a spectrum of difficulty that mirrors real-world scientific challenges. This multi-level structure enables the AI4S benchmark to evaluate both the foundational and advanced capabilities of LLMs, offering an assessment of their strengths and limitations.\nTo evaluate the merits of our AGIL approach, we performed several tests to evaluate the quality of (i) the level classifica-tion, (ii) the AI4S benchmark compared to GPQA, (iii) the automatically generated MCQs, and (iv) the automatic MCQ validation.\nFor all tests, we used the STaR framework (see section VI) with five shots. The first row of the table Table III shows the overall performance of Llama 3 8B on the 254 MCQs. The resulting accuracy of 20% correspond to a random guess. The next three rows show results for MCQs grouped by their human-identified levels of difficulty. We see that the"}, {"title": "D. End-to-End Evaluations", "content": "Next we discuss the open-ended benchmarks. These are essential for evaluating the reasoning, creativity, and problem-solving abilities of LLMs, particularly in scientific domains. Unlike MCQs, which primarily test factual recall or single-step reasoning, open-ended tasks engage models with com-plex, real-world problems that require multi-step reasoning, synthesis of knowledge across domains, and adaptive problem-solving. For instance, while an MCQ might test a model's recall of a specific scientific fact, an open-ended task could require the model to design an experiment, analyze data, or propose solutions to unsolved research questions. This format aligns better with the exploratory nature of scientific inquiry, offering a more comprehensive assessment of a model's ca\u0440\u0430-bilities. However, existing open-ended benchmarks often fall short in capturing the depth and realism needed for scientific evaluations. Many rely on synthetic tasks that fail to reflect the intricacies of real-world scientific challenges, such as multi-disciplinary reasoning or generating accurate code for practical applications.\nAlthough MCQ benchmarks are effective in testing factual recall and reasoning within constrained formats, and open-ended benchmarks gauge the generation of detailed and flex-ible responses, these methods do not capture the iterative and complexity of scientific problem solving. End-to-end evaluations attempts to address this gap by assessing, in real situations, the models responses for assisting researchers in solving scientific problems. We propose two novel types of end-to-end methods in the context of scientific research: Lab-style and field-style experiments.\n1) Lab-style experiments: These experiments are de-signed to evaluate the capabilities of AI models to assist researchers in performing the typical tasks (Table I) to solve scientific problems. Note that, in real situations, these tasks are often repeated several times to solve problems. By cap-turing and evaluating all interactions between research and LLMs while attempting to solve a research problem, a lab-style experiment can capture accurately the complex reality of solving complex research problems. It can thus provides a unique perspective on the \"distance\" between the ideal scientific assistant and the current capabilities of AI models.\nThe setup for lab-style experiments involves defining a specific scientific problem and presenting it to multiple AI models for comparison. Each model is manually tasked with assisting in all the research tasks using the same prompts, en-suring consistency across evaluations. Prompts and responses are meticulously recorded, and domain experts analyze and"}, {"title": "1) SciCode - Scientific Code Generation Benchmark:", "content": "While open responses questions are versatile in capability", "67": "can be used to compare the semantics of LLM responses and reference answers. The third and most recent class is the LLM-as-a-judge methods which tackle the problem of evaluating LLM open responses when no reference answer is available. This approach currently has two variations. In the \"Pairwise comparison\" version", "Single answer grading": "ersion", "benefits": "consistency", "limitations": "position bias (first answer better in \u201cPairwise comparison\u201d), verbosity bias (longer answer better), self-enhancement bias (self-generated answer better) and limited capability to grade math and reasoning questions [66"}]}