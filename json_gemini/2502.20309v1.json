{"title": "EAIRA: Establishing a Methodology for Evaluating AI Models as Scientific Research Assistants", "authors": ["Franck Cappello", "Sandeep Madireddy", "Robert Underwood", "Neil Getty", "Nicholas Lee-Ping Chia", "Nesar Ramachandra", "Josh Nguyen", "Murat Ke\u00e7eli", "Tanwi Mallick", "Zilinghan Li", "Marieme Ngom", "Chenhui Zhang", "Angel Yanguas-Gil", "Evan Antoniuk", "Bhavya Kailkhura", "Minyang Tian", "Yufeng Du", "Yuan-Sen Ting", "Azton Wells", "Bogdan Nicolae", "Avinash Maurya", "M. Mustafa Rafique", "Eliu Huerta", "Bo Li", "Ian Foster", "Rick Stevens"], "abstract": "Recent advancements have positioned AI, and particularly Large Language Models (LLMs) as transformative tools for scientific research, capable of addressing complex tasks that require reasoning, problem-solving, and decision-making. Their exceptional capabilities suggest their potential as scientific research assistants, but also highlight the need for holistic, rigorous, and domain-specific evaluation to assess effectiveness in real-world scientific applications. This paper describes a multifaceted methodology for Evaluating AI models as scientific Research Assistants (EAIRA) developed at Argonne National Laboratory. This methodology incorporates four primary classes of evaluations. 1) Multiple Choice Questions to assess factual recall; 2) Open Response to evaluate advanced reasoning and problem-solving skills; 3) Lab-Style Experiments involving detailed analysis of capabilities as research assistants in controlled environments; and 4) Field-Style Experiments to capture researcher-LLM interactions at scale in a wide range of scientific domains and applications. These complementary methods enable a comprehensive analysis of LLM strengths and weaknesses with respect to their scientific knowledge, reasoning abilities, and adaptability. Recognizing the rapid pace of LLM advancements, we designed the methodology to evolve and adapt so as to ensure its continued relevance and applicability. This paper describes the methodology's state at the end of February 2025. Although developed within a subset of scientific domains, the methodology is designed to be generalizable to a wide range of scientific domains.", "sections": [{"title": "I. INTRODUCTION", "content": "Recent advances in Large Language Models (LLMs) have greatly broadened conceptions of what AI may be able to accomplish in the near future. Models such as OpenAI's GPT 01 [1], Google's Gemini [2], and Anthropic's Claude [3] are transforming traditional natural language understanding (NLU) tasks like summarization, information extraction, translation, and classification with enhanced contextual depth and adaptability. They are also exhibiting promising potential beyond NLU, with measurable progress on tasks such as mathematical problem solving, multi-step reasoning, and symbolic logic and achieving significant milestones such as passing the Uniform BAR exam and medical licensing exams [4]. Such achievements highlight their potential to emulate abstraction, logical deduction, and domain-specific expertise. This evolution from NLU towards addressing complex, domain-specific challenges with minimal human guidance has propelled LLMs into a pivotal role for next-generation AI systems, positioning them as a cornerstone technology in the quest toward more general-purpose AI, and potentially, artificial general intelligence (AGI).\nBuilding on these advancements, scientists are now beginning to assess separately the suitability and potential impact of LLMs on a wide variety of specific tasks within specific fields of research and discovery [5]. This work has led to exciting demonstrations of LLMs as transformative tools for such tasks predicting molecular properties [6], uncovering genomic patterns [7], interpreting astrophysical data [8], solving mathematical problems [9], and even creating and manipulating tools for simulations and analysis [10].\nThese developments have also led scientists to envision the use of LLMs and transformers as research assistants that can not only automate individual research tasks but also engage with scientific problems in depth by taking advantage of growing multi-step reasoning skills that complement their expanding contextual understanding. This vision suggests a new holistic approach in which LLMs interface with relevant tools, operate (quasi-)autonomously on research challenges, identify"}, {"title": "II. RELATED WORK", "content": "Research on LLM evaluation encompasses various techniques relevant for the evaluation of LLMs as research assistants. Here we discuss that work and note gaps that our proposed methodology attempts to fill.\n##### A. Multiple-choice Question (MCQ) Benchmarks\nMCQ benchmarks offer a structured framework for assessing LLM performance across various domains. Notable examples are Massive Multitask Language Understanding (MMLU) [13] and MMLU-PRO [14], which evaluate general knowledge and reasoning in more than 50 subjects, including humanities, sciences and engineering. In the realm of mathematical reasoning, GSM8K [15], GSM1K [16], and MATH [9] are prominent. GSM8K and GSM1K addresses grade-school level problems, while MATH focuses on high-school and competition-level questions. Both benchmarks have been enhanced with multiple-choice adaptations to streamline evaluation and minimize ambiguity in model outputs [17]. Other significant MCQ benchmarks include ARC [18], which tests scientific reasoning, and HellaSwag [19], which challenges models with complex commonsense reasoning scenarios that, while easy for humans, are especially hard for state-of-the-art models. In the field of chemistry, MCQ benchmarks include MoleculeQA [20], which comprises 61,574 MCQs, each with three distractors, focusing on factual information about molecules; MolPuzzle [21], a multimodal benchmark with over 23,000 question-answer pairs, structured with interlinked sequential sub-tasks, each providing multiple choices; and ChemBench [22], with over 2700 questions, primarily MCQs. Few of the many other MCQ benchmarking efforts in the literature are science-domain focused and validated, and as LLM capabilities improve, there is an increasing need to generate more difficult questions and leverage synergies between domain expertize and LLM judges. The multi-domain benchmark AI4S that we present below is an attempt towards that end.\n##### B. Open Response Benchmarks\nWhile MCQ benchmarks restrict responses to predefined options, Open Response benchmarks require that LLMs produce detailed, unconstrained outputs that can be evaluated for coherence, accuracy, and relevance. Notable examples include NarrativeQA [23], which challenges models to generate summaries or interpret longer narratives, and HotpotQA [24], which demands multi-hop reasoning with synthesized answers derived from multiple sources. Similarly, HybridQA [25] combines textual and tabular data, requiring LLMs to provide coherent and comprehensive answers. For mathematical reasoning, GSM8K [15] and MATH [9] test model ability to solve complex, multi-step problems with free-form solutions. In chemistry, benchmarks have been developed to assess LLM open-response capabilities. ChemistryQA [26] comprises 4500 complex questions that require reasoning and calculations, evaluating model ability to generate detailed, accurate responses. ChemLLMBenchmark [27] consists of eight practical chemistry tasks that necessitate understanding, reasoning, and explanatory skills, with evaluations focusing on the quality and depth of model-generated answers. The open-domain TOMG-Bench [28] molecule generation benchmark comprises tasks such as molecule editing, optimization, and customized generation, each requiring models to produce specific molecular structures or modifications based on textual descriptions.\nRecent advancements have expanded the scope of open-response benchmarks to address specific evaluation challenges and domains. The Open-LLM-Leaderboard (OSQ-bench) [29] transitions from multiple-choice formats to open-style questions, eliminating issues like selection bias and random guessing, while emphasizing models' ability to generate coherent, contextually accurate answers. FrontierMath [30] presents hundreds of exceptionally challenging mathematics problems that require models to generate detailed solutions, testing advanced reasoning and problem-solving skills. Similarly, Humanity's Last Exam [31] crowdsources complex questions from experts across fields to evaluate how closely LLMs approximate expert-level capabilities, highlighting their potential and limitations in addressing real-world challenges.\nA difficulty with Open Response benchmarks is that evaluating their responses is inherently challenging due to their unstructured nature, requiring time-intensive analysis, subjective interpretation, and careful management of biases and data overload. These challenges are closely tied to uncertainty quantification (UQ), as the variability in interpretations and outcomes necessitates robust techniques to quantify and mitigate uncertainty in the evaluation process, ensuring reliable and consistent insights. In addressing these challenges, it is also important to keep track of multiple model versions and to use them consistently [32], so as to enable reproducibility [33].\n##### C. Lab-Style Experiments\nLaboratory-style Experiments with LLMs involve controlled settings in which researchers systematically evaluate model performance on specific tasks, enabling precise measurement of capabilities and limitations. In chemistry, for instance, [34] developed Coscientist, which employs LLMs to plan and execute experimental procedures based on simple human prompts, and evaluated its performance by assigning it the task of identifying synthetic procedures for seven molecules of varying complexity. Similarly, [35] proposed an approach that combines LLMs with task and motion planning to translate natural language instructions into robot-executable plans, evaluating their system through simulations in a box-packing domain. In behavioral strategy research, [36] reproduced human laboratory experiments using LLMs and compared their performance to human participants to analyze the extent to which LLMs can emulate human decision-making processes. These laboratory-style experiments provide valuable insights into applications and can inform the development of more advanced AI systems. However, comprehensive end-to-end"}, {"title": "D. Field-Style Experiments", "content": "Field-style Experiments, also referred to as \u201cin-the-wild\u201d studies, involve observing and analyzing user interactions with LLM in real-world settings. This approach contrasts with controlled Lab-style Experiments by capturing non-predefined user interactions with LLMs, providing valuable insights into how LLMs perform across diverse, unstructured tasks. Recent studies have leveraged this methodology to assess various aspects of LLM performance. For instance, WildBench [37] is designed to evaluate LLMs using real-world user interactions, enabling a comprehensive analysis of model capabilities in practical scenarios. Similarly, HaluEval-Wild [38] was designed to evaluate hallucinations in LLMs by collecting challenging user queries from real-world interactions. Shen et al. [39] conducted a study characterizing and evaluating user-LLM interactions, providing insights into user behavior and model performance and highlight the importance of understanding user needs and expectations to improve LLM utility and user satisfaction. While such analyses are currently lacking in scientific domains, developing field-style experiments specifically for scientific research presents a significant opportunity to provide critical insights into how researchers interact with AI models, thereby enhancing the creation of AI assistants tailored to scientific inquiry."}, {"title": "E. Safety Evaluation", "content": "In addition to the general-purpose evaluations discussed above, comprehensive evaluations must rigorously assess alignment with ethical standards, robustness against jail-breaks, and adaptability to complex real-world scenarios [37], [40]. Safety evaluations are also cross-cutting :MCQs, Open-Response benchmarks, and Lab-style and Field-style Experiments. SafetyBench [41], for example, employs 11,435 MCQS in seven categories (e.g., bias, toxicity) to systematically test model adherence to ethical and safety standards in both English and Chinese. Similarly, SALAD-Bench [42] proposed 4000 MCQs, and part of the larger dataset structured into a detailed hierarchy of 6 domains, 16 tasks, and 66 categories. BigBench [43] has a subset of its tasks focused on safety evaluation in regards to toxicity, bias and truthfullness tat are MCQs. However, there has not been a focused MCQ-based safety evaluation benchmarks that take the nuances aspects of science, especially high consequential ones such as the the chemisty, biology, radiation, and nuclear (CBRN). We discuss one such effort on risks in chemistry in this work. Open-response evaluations, such as those in DecodingTrust [44] and TrustLLM [45], examine nuanced safety aspects, including hallucinations, privacy violations, and machine ethics. TrustLLM evaluates LLMs across six dimensions, including fairness and safety, using over 30 datasets to identify critical safety gaps, while DecodingTrust introduces an eight-dimensional framework that probes issues like toxicity and ethical reasoning, with results published on a widely accessible leaderboard for transparency. However, such evaluations for scientific use cases are scarce [40] and thus provide an opportunity to develop them in the future.\nThe safety red-teaming methodologies can be effectively interpreted along the lines of lab-style and field-style experiments. In lab-style red-teaming, researchers interact directly with LLMs and systematically introduce adversarial prompts to identify vulnerabilities such as biases, hallucinations, or ethical compliance issues. For example, [46] present a framework for red-teaming experiments on LLMs by generating numerical questions and puzzles to evaluate the models' performance on elementary calculations and algebraic tasks. This approach provides detailed feedback at each step, allowing iterative improvements and a deeper understanding of model limitations. In contrast, field-style red-teaming assesses LLMs by analyzing large-scale human interactions in real-world environments. This method captures diverse and unpredictable user inputs, offering insights into how models perform \u201cin the wild.\" For instance, [47] discuss automating red-teaming by training a separate red team LLM with reinforcement learning to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. This large-scale approach identifies practical weaknesses and vulnerabilities that may not surface in controlled lab settings, contributing to model robustness across varied real-world scenarios. By employing both lab-style and field-style red-teaming strategies, researchers can comprehensively evaluate and enhance the safety, reliability, and ethical performance of LLMs across different contexts. That said, significant work still needs to be done in designing such experiments for safety and trust scenarios in science.\""}, {"title": "III. ESTABLISHING A METHODOLOGY TO EVALUATE LLMS AS RESEARCH ASSISTANTS", "content": "The overarching objective of using LLMs as research assistants is to accelerate the research process.\nIn Table II, we present seven tasks commonly performed by researchers to solve a scientific problem: (i) posing and formulating a research question; (ii) if needed, conducting initial, preliminary experiments, observations, simulation, and/or database accesses to confirm the pertinence of the research question; (iii) performing literature search to identify related work; (iv) generating potentially multiple hypotheses (or research directions) to address the research question; (v) designing experiments, observations, and/or simulations to test the hypotheses; (vi) analyzing the resulting data to validate or invalidate the hypotheses; and (vii) writing a report about findings. Each of these seven tasks requires deep reasoning, contextual understanding, and iterative problem solving. The scientific community needs confidence that LLMs are proficient in these tasks if they are to adopt them widely as research assistants. Thus a thorough and comprehensive evaluation methodology is required that can produce a rigorous assessment of LLM strengths and weaknesses in each task."}, {"title": "A. Domain-Specific MCQ Benchmarks", "content": "Domain-specific benchmarks are crucial for evaluating LLMs in specialized fields, as they address the limitations of general benchmarks that often fail to capture the complexities of domain-specific tasks. Without tailored benchmarks [8], LLMs risk over training on well-established datasets, leading to inflated performance that does not translate to real-world applicability. These benchmarks are essential for guiding targeted improvements, ensuring that models meet the specific demands of scientific research, and providing a baseline to understand their strengths and weaknesses. By capturing the nuanced challenges of individual domains, such benchmarks foster the effective and ethical deployment of LLMs, enabling their potential to accelerate discovery and innovation across disciplines. We now discuss the domain-specific benchmarking efforts that we conducted in Astronomy and Climate modeling.\n1) Astronomy: The Astronomy Benchmark [8] assesses LLM performance in a manner that reflects the interdisciplinary nature of astronomy, testing both factual recall and the ability to connect insights across subfields. This benchmark was generated automatically using an LLM to compose MCQS from astronomy papers. To assemble a rich repository of scientific knowledge, we leveraged the Annual Review of Astronomy and Astrophysics (ARAA), a selective review journal renowned for its comprehensive overviews authored by leading experts.\nThe Nougat optical character recognition (OCR) tool was used to transcribe 885 ARAA articles over the years 1963 to 2023. Each transcribed paper was processed using Gemini-1.5-Pro, a long-context LLM capable of handling up to one million tokens, to generate five multiple-choice questions (MCQs) per paper. The questions were designed to be specific, yet independent of the article sections, with generalized answers and balanced options to avoid bias. This process yielded a total of 4425 MCQs covering diverse topics such as quasar density decline at high redshifts and subgrid feedback model calibration in simulations.\nThe Astronomy Benchmark has been then used to assess both the accuracy and computational cost of dozens of different closed and open LLM variants. This study revealed disparities in LLM performance across general-purpose and specialized tasks that highlight significant performance gaps and performance-to-cost ratios. While frontier models like Claude-3.5-Sonnet excelled in the Astronomy Benchmark with an accuracy of 85.0%, outperforming GPT-40 (80.4%) and Gemini-1.5-Pro (77.6%), these differences are less evident in general-purpose benchmarks such as MMLU [29]. A study of how Astronomy Benchmark performance varied with compute costs showed that, roughly speaking, each 3.5 percentage points increased accuracy was associated with 10-fold increase in price, within most given series of models such as GPT,"}, {"title": "B. Scalable AI4S MCQ Benchmark", "content": "The overarching goal of the AI4S benchmark is to evaluate the knowledge extension of LLMs across many different scientific domains. In that respect, it is similar to GPQA [63]. However, AI4S design focuses on the quality and scalability of MCQ generation and validation. While GPQA has only 448 MCQs, our objective is to generate and validate thousands of MCQs and continuously add MCQs as LLMs progress in their capabilities.\nCurrent MCQ benchmarks, including GPQA, have two main limitations: 1) they are quickly saturated because of the fast progress of LLMs. 2) there is a high risk of contamination (benchmark included in the training sets) if the MCQs are made public. These two limitations arise from the static aspect of the current benchmarks. They are developed once and do not evolve. The current practice is to develop new versions [64] when the initial benchmark is saturated [65] and to open only a portion of the benchmark MCQs to avoid contamination.\nTo address the two limitations, we explore a novel approach to develop scalable generation and validation of MCQ benchmarks for the evaluation of LLMs capabilities: Automatic continuous Generation and validation of Increasingly Large MCQ benchmarks: AGIL.\nBuilding on the experience gained from the Astronomy and Climate domain-specific benchmarks, we are developing an AGIL process by which we combine manual and automated methods to generate and validate MCQs.\nWe present here our initial finding towards the creation of a 1000-MCQ AI4S benchmark that spans five scientific"}, {"title": "C. Open Response Benchmarks", "content": "Next we discuss the open-ended benchmarks. These are essential for evaluating the reasoning, creativity, and problem-solving abilities of LLMs, particularly in scientific domains. Unlike MCQs, which primarily test factual recall or single-step reasoning, open-ended tasks engage models with complex, real-world problems that require multi-step reasoning, synthesis of knowledge across domains, and adaptive problem-solving. For instance, while an MCQ might test a model's recall of a specific scientific fact, an open-ended task could require the model to design an experiment, analyze data, or propose solutions to unsolved research questions. This format aligns better with the exploratory nature of scientific inquiry, offering a more comprehensive assessment of a model's capabilities. However, existing open-ended benchmarks often fall short in capturing the depth and realism needed for scientific evaluations. Many rely on synthetic tasks that fail to reflect the intricacies of real-world scientific challenges, such as multidisciplinary reasoning or generating accurate code for practical applications."}, {"title": "1) SciCode - Scientific Code Generation Benchmark:", "content": "The SciCode Benchmark is a set of manually curated coding problems designed to assess LLM capabilities for solving complex scientific coding problems across diverse domains. By providing tasks that reflect real-world challenges and require multi-step reasoning, SciCode allows models to be tested in contexts that align closely with the demands of scientific research [68]. SciCode includes problems across a range of scientific domains, including computational mechanics, quantum information, quantum chemistry, ecology, and molecular modeling. It consists of 80 main problems, decomposed into 338 intermediate steps, enabling a structured approach to assessing model capabilities. Solving each individual problem requires that an LLM implement multiple Python functions corresponding to subproblems and integrate those functions into a cohesive solution: see Figure 14. Each problem is accompanied by a gold-standard solution and multiple test cases so as to permit robust and reliable automatic evaluation.\nEach SciCode problem is meticulously annotated and verified by at least two senior researchers to ensure accuracy, and is drawn from real-world research tasks, maintaining relevance to practical applications. Problems are curated to avoid overlap with publicly available datasets and thus to test the deep scientific knowledge and analytical skills of LLMs by requiring the decomposition and integration of complex problems into comprehensive solutions. Additionally, SciCode allows for flexible evaluation of model capabilities in varied setups, enabling adjustments like providing background information or conditioning on previous solutions.\nThe SciCode Benchmark is configured to assess LLM capabilities to solve SciCode problems by using zero-shot prompts, maintaining a general approach while creating distinct prompts for various evaluation setups to guide the model on the tasks, as described in detail in [68]. The prompts remain consistent across models and fields, incorporating instructions for the main and sub-problems, as well as code from previous subproblems. We evaluated the coding capabilities of several state-of-the-art LLMs using the SciCode benchmark, focusing on three key aspects to assess their performance. First, the Impact of Scientific Background was analyzed by testing models in two modes: without background, to evaluate inherent scientific knowledge and reasoning, and with background, to focus on coding and instruction-following capabilities. The results showed significant performance improvements with background information, highlighting the limitations of current LLMs in scientific reasoning. Second, the comparison between Gold vs. Generated Solutions revealed insights into the challenges of realistic evaluations. While gold solutions accurately address each problem, generated solutions introduce error accumulation, creating a more practical and demanding evaluation scenario. Lastly, the assessment of Main vs. Subproblems provided a nuanced understanding of model performance. A main problem was considered solved only when all subproblem solutions and the integrated result were accurate. Additionally, SciCode's design allows independent evaluation of subproblems, enabling precise analysis of models' reasoning and coding abilities across discrete tasks. These evaluation dimensions underscore the benchmark's rigor in testing LLMs for real-world scientific applications.\nWe summarize the findings of our studies using several state-of-the-art models in Figure 2. These results show that SciCode is a difficult benchmark for current LLMs. Consistent with our observations on proprietary models, open-weight LLMs under test also showed their lack of capabilities in"}, {"title": "2) ALDbench - Materials Synthesis Benchmark:", "content": "An area that lacked relevant benchmarks is materials synthesis. This is particularly important for potential applications of LLMs in automated materials discovery or as Al research assistants. LLMs underpinning such capabilities need to exhibit both the ability to reason about specific processes (for instance to avoid unsafe conditions or transfer ideas across reactors and process conditions) and have a robust understanding of the literature (to build on existing process knowledge and avoid known dead ends).\nAs such capabilities appear hard to evaluate by using either MCQs or the statistical scorer or embedding approaches described earlier. we developed a new open-response benchmark ALDbench on materials synthesis, and in particular on a synthesis technique called atomic layer deposition [69]. Here we targeted a range of difficulty spanning from graduate level to PhD-level domain expert current with the state of the art. A model's ability to perform at a domain expert level is paramount whenever models are expected to assist in decision making processes that involve costly experiments. Beyond its applied interest in areas such as energy and microelectronics [70], this domain brings together multiple topics that are commonplace in chemistry-driven synthesis, including metal-organic and inorganic molecules, gas-surface kinetics and heterogeneous reactions, and gas phase transport. Evaluating LLM capabilities in this field can provide insights with wide applicability to other material synthesis techniques."}, {"title": "D. End-to-End Evaluations", "content": "Although MCQ benchmarks are effective in testing factual recall and reasoning within constrained formats, and open-ended benchmarks gauge the generation of detailed and flexible responses, these methods do not capture the iterative and complexity of scientific problem solving. End-to-end evaluations attempts to address this gap by assessing, in real situations, the models responses for assisting researchers in solving scientific problems. We propose two novel types of end-to-end methods in the context of scientific research: Lab-style and field-style experiments.\n##### 1) Lab-style experiments:\nThese experiments are designed to evaluate the capabilities of AI models to assist researchers in performing the typical tasks (Table I) to solve scientific problems. Note that, in real situations, these tasks are often repeated several times to solve problems. By capturing and evaluating all interactions between research and LLMs while attempting to solve a research problem, a lab-style experiment can capture accurately the complex reality of solving complex research problems. It can thus provides a unique perspective on the \"distance\" between the ideal scientific assistant and the current capabilities of AI models. The setup for lab-style experiments involves defining a specific scientific problem and presenting it to multiple AI models for comparison. Each model is manually tasked with assisting in all the research tasks using the same prompts, ensuring consistency across evaluations. Prompts and responses are meticulously recorded, and domain experts analyze and"}, {"title": "2) Field-style experiments:", "content": "This method takes inspiration from previous studies analyzing user-LLM interactions at scale [38], [39], [50]. The \u201cIn the wild\" method captures and analyzes all the interactions between volunteer users and AI models. This method provides additional critical information for the development and improvement of AI models for science: a precise understanding of researcher needs and requirements regarding AI assistants (e.g.: What task do researchers ask Al models to perform? What are their expectations regarding model responses? How frequently do researchers use AI models?); a deeper understanding of AI models strengths and weakness (by analyzing the thousands of prompts and responses); a window on the trends behind the use of AI models as research assistant (e.g., increased usage frequency, increased number of users, nature and distribution variations of the performed tasks); and tracking of AI model progress across generations. Ideally, this method will analyze online thousands of user-LLM interactions. Compared to the \u201cLab-style experiment\u201d method, users are not expected to evaluate LLM responses directly. Instead, evaluation is indirect, based on the study of the flow of prompts and responses. This method leverages user behavior as a signal to diagnose LLM failure modes [72]. For example, a user submitting rephrasing questions, providing feedback, or abandoning the interaction are signs of LLM weaknesses in understanding user intent. The flow can then be analyzed to diagnose potential sources of weakness [73]. Previous \u201cin-the-wild\u201d experiments focused on nonscientific domains. The Field-style experiment method adapts the \u201cin-the-wild\u201d approach to the scientific context by defining criteria and scoring specific to the scientific methodology.\nOn November 1, 2024, Argonne organized a JAM session that captured 180 conversations between Argonne researchers and Argo/O1-preview 1 Researchers were asked to bring to the JAM session evaluation a scientific problem that they would work on with Argo/O1-preview as a research assistant. At the end of the session, the researchers evaluated their experience according to five criteria: Novelty, Productivity, Solution, Strength, and Importance. (This approach is consistent with that followed in a much smaller study conducted at Los Alamos National Laboratory.) Five possible responses were proposed for each question, corresponding to a score of 1 to 5.\nThe scores produced by the Argonne researchers indicated that: 1) Importance: 82% of researchers consider that AI models such as Argo/O1-preview are \u201cvery important\u201d or \u201ccritical\u201d to their team's success, 2) Strength: at 59%, they consider that Al models significantly or noticeably improve productivity, 3) Productivity: 51% of researchers compare AI models such as Argo/O1-preview to PhD students or postdocs, 4) Solution quality: 50% of the researchers consider that AI models such as Argo/O1-preview produce exceptional or"}, {"title": "IV. RELIABILITY AND UQ OF EVALUATIONS", "content": "The success of LLMs in scientific domains, such as chemistry, biology, and physics, has been remarkable, but their trustworthiness as scientific assistants remains a significant concern. These models, including GPT, Claude, and Llama, are prone to generating unreliable or fabricated responses, often referred to as hallucinations [52]. Understanding and quantifying uncertainty in LLM outputs is essential to ensure safe, reliable, and informed decision making, particularly in scientific domains. Traditional uncertainty quantification (UQ) techniques, which rely on accessing internal model parameters [74], face challenges due to the black-box nature of modern LLMs like GPT-4, Claude 3, and Gemini, which are primarily accessible as API services. Recent research has focused on developing novel approaches to assess uncertainty directly from model outputs, such as semantic entropy [75], sampling-based methods, and aggregation techniques [51], [76]. These techniques aim to evaluate input sensitivity and output consistency, highlighting where models are most vulnerable. By improving transparency and trust, these UQ strategies play a crucial role in responsible AI deployment. Addressing these challenges is vital for leveraging LLMs in scientific applications, where errors can have substantial consequences. Moving forward, advancing UQ methods and enhancing LLM interpretability will be key to making these models safer and more robust in critical scientific and industrial domains.\nInspired by psychological assessments in which the same question is asked in different ways to test consistency, we propose a technique called Question Rephrasing [55] to quantify the uncertainty in LLM outputs. This approach involves rephrasing a given question while preserving its original semantic meaning and comparing LLM responses before and after rephrasing to assess input uncertainty. In addition, we adopt a sampling method that repeatedly queries an LLM with identical inputs to evaluate output uncertainty. We applied these methods to assess GPT-3.5 and GPT-4 performance on tasks in the chemistry domain, specifically property prediction and forward reaction prediction. Input uncertainty helps determine the LLM's sensitivity to variations in molecular representations (e.g., alternative SMILES notations), while output uncertainty evaluates the inherent variability in LLM predictions. These techniques allow us to systematically explore how robust and reliable LLMs are in handling different forms of input and producing consistent output. Below, we outline our approach:"}, {"title": "V. SAFETY EVALUATIONS", "content": "Safe and secure deployment of AI systems in scientific domains is paramount. As LLMs increasingly support critical applications in fields like biosecurity, cybersecurity, and chemistry, ensuring their safety and alignment is also essential to maintaining trustworthiness. Hence, it is critical to integrate into our proposed methodology to evaluate LLMs as research assistants rigorous safety and alignment evaluation techniques. To this end, we discuss below the CHEMRISK benchmark as one of our efforts in this direction."}, {"title": "VI. SCALABLE SOFTWARE INFRASTRUCTURE", "content": "As previous sections show, a comprehensive evaluation of LLMs as research assistants already requires the execution of many benchmarks for skills and safety assessments. We do not expect the evaluation workload to be reduced in the future. In contrast, as more capable LLMs appear, more research domains will be interested in using them, which will trigger the development of new evaluation benchmarks. This situation places ever-growing demands on software and computational infrastructure. Existing evaluation software platforms, such as HELM [77], EleutherAI's LM Evaluation Harness [78], and DecodingTrust [44], have made significant strides in this area, but exhibit certain limitations that impede comprehensive and scalable evaluations, particularly within high-performance computing (HPC) environments like those at Argonne National Laboratory. A critical shortcoming of current frameworks is their limited scalability and inefficiency in handling large-scale models. Many existing software platforms are not optimized for parallel processing across multiple GPUs or computing nodes, resulting in prolonged evaluation times and increased computational costs. This inefficiency becomes particularly problematic when assessing large LLMs that demand substantial computational resources. In addition, inconsistencies in evaluation methodologies and a lack of standardization further hinder comprehensive evaluations. The absence of consistent benchmarks and metrics across platforms and organizations complicates model comparisons, exacerbated by dataset biases, contamination, and the rapid evolution of LLMs outpacing evaluation strategies.\nTo address the challenges of scalable and comprehensive LLM evaluation, we are developing the Skills, Trust, and Reliability (STaR) evaluation framework, tailored for HPC systems at Argonne National Laboratory. STaR builds upon the general architecture of evaluation platforms, which typically involve a sequence of specifications (files or configuration flags) to instantiate controllers and manage communication through states. Central to these platforms are Runners, which act as top-level components orchestrating workflows that handle Scenarios-benchmarks comprising static datasets like Hellaswag or GSM8K, or dynamic scripts such as those in Chain-of-Thought Hub [79]. A Data Pre-Processor translates these Scenarios into formatted prompts, which are passed to Adapters interfacing with LLMs through libraries such as Hugging Face [80], vLLM [81], or OpenAI APIs. Executors like Slurm [82] or Ray [83] enable processing of prompts, and the results are aggregated into metrics, such as accuracy.\nExpanding on this general framework, STaR introduces a modular architecture comprising a data layer, prompting layer, model adapter, and result layer to streamline the evaluation process. The data layer ingests datasets, such as MMLU-Pro [14], and constructs evaluation instances, while the prompting layer generates standardized prompts using techniques such as few-shot and chain-of-thought reasoning [84]. The model adapter queries models in multiple modes, including locally loaded instances for smaller models, Parsl [57] for job bundling, and OpenAI-compatible inference backends (e.g., vLLM [81] and DeepSpeed FastGen [85]) for larger models deployed on HPC systems like Polaris and Aurora. The result layer aggregates responses, computes general and UQ metrics, and organizes results into comprehensive scores, providing nuanced insight into model performance.\nSTaR supports widely used benchmark libraries, including EleutherAI-Harness [78], DecodingTrust [44], Wildbench [37], and domain-specific benchmarks. It also integrates uncertainty quantification approaches [55] to enhance the reliability of evaluations. Designed for scalability, STaR incorporates data-parallel capabilities to distribute workloads across multiple GPUs and model-parallel solutions to handle large models exceeding single-GPU memory limits. It aims"}, {"title": "VII. CONCLUSIONS AND NEXT STEPS", "content": "As LLMs continue to expand the notions of what AI can accomplish, there remain two main challenges to address to enable the broad adoption of LLMs by the scientific community as research assistants: a holistic understanding of the capabilities of LLMs and a strong confidence in the results produced by them. To address this, our proposed methodology features four techniques: multiple choice questions, open-response questions, lab-style experiments, and field-style experiments which complement each other to form a comprehensive, rigorous, and realistic assessment of the capabilities of AI systems. Underneath the four approaches are three cross-cutting aspects, including trust and safety, reliable uncertainty quantification, and scalable software infrastructure which support our approach. In addition to proposing the holistic methodology, our team has advanced the state-of-the-art in each of the techniques and aspects.\nMultiple choice questions are a key technique to evaluate LLMs because of their ability to quickly assess a breadth of knowledge; our team extends beyond existing benchmarks with automatically-generated domain-specific MCQ benchmarks in Astronomy and Climate and the multi-domain AI4S Benchmark with both human and automatically curated with human reviews have revealed significant gaps in knowledge recall and reasoning in LLMs. We find that our new AI4S benchmark is more challenging for LLMs than benchmarks"}, {"title": "VIII. ACKNOWLEDGEMENTS", "content": "This material is based upon work supported by Laboratory Directed Research and Development (LDRD) funding from Argonne National Laboratory, provided by the Director, Office of Science, of the U.S. Department of Energy under Contract No. DEAC02-06CH11357. LLNL work was prepared under Contract DE-AC52- 07NA27344, supported by the LLNL-LDRD Program under Project No. 24-ERD-058, and authored by Lawrence Livermore National Security, LLC under Contract No. DE-AC52-07NA27344 with the U.S. Department of Energy. This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and"}]}