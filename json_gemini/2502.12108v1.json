{"title": "Using the Path of Least Resistance to Explain Deep Net-works", "authors": ["Sina Salek", "Joseph Enguehard"], "abstract": "Integrated Gradients (IG), a widely used axiomatic path-based attribution method, assigns importance scores to input features by integrating model gradients along a straight path from a baseline to the input. While effective in some cases, we show that straight paths can lead to flawed attributions. In this paper, we identify the cause of these misattributions and propose an alternative approach that treats the input space as a Riemannian manifold, computing attributions by integrating gradients along geodesics. We call this method Geodesic Integrated Gradients (GIG). \u03a4\u03bf approximate geodesic paths, we introduce two techniques: a k-Nearest Neighbours-based approach for smaller models and a Stochastic Variational Inference-based method for larger ones. Additionally, we propose a new axiom, Strong Completeness, extending the axioms satisfied by IG. We show that this property is desirable for attribution methods and that GIG is the only method that satisfies it. Through experiments on both synthetic and real-world data, we demonstrate that GIG outperforms existing explainability methods, including IG.", "sections": [{"title": "Introduction", "content": "The use of deep learning models has risen in many applications. With it, so too has the desire to understand why these models make certain predictions. These models are often referred to as \"opaque\", as it is difficult to discern the reasoning behind their predictions Marcus (2018). Additionally, deep learning models can inadvertently learn and perpetuate biases found in their training data Sap et al. (2019). To create fair and trustworthy algorithms, it is essential to be able to explain a model's output Das & Rad (2020).\nSome examples of the methods proposed to explain neural networks include Gradient SHAP Lundberg &\nLee (2017), Integrated Gradients Sundararajan et al. (2017) and Guided Integrated Gradients Kapishnikov et al. (2021).\nSignificant effort has been dedicated to designing explanation methods that satisfy certain desirable axioms. This is due to the lack of ground truth for evaluating them. The axioms can ensure that the explanations are principled. One of the most successful axiomatic methods is Integrated Gradients (IG) Sundararajan et al. (2017). Consider a function f : R \u2192 R, representing the neural network and an input vector x \u2208 Rn.\nFurthermore, consider a baseline input vector X \u2208 Rn (typically chosen such that the network gives baseline a near zero score). IG explains the network by quantifying how much of the difference f(x) - f(x) can be attributed to the ith dimension of x, xi.\nIntegrated Gradient gives attribution IGi to the ith dimension of the input by approximating the following path integral"}, {"title": null, "content": "IGi(x) = (xi - Xi) \u222b01 \u2202f(x(t))\u2202xi dt,\nwhere y(t) = x + t(x \u2013 x) is a straight path from the baseline to input. The claim of the creators of IG is that Eq. 1 tells us how the model got from predicting essentially nothing at to giving the prediction at x. Considering gradients represent the rate of change of functions, the above expression should tell us how scaling each feature along the path affects the increase in the network score for the predicted class."}, {"title": "Method", "content": "In section 1, we gave the intuition that using geodesic paths can correct the misattributions in IG that arise from integrating along straight paths. Let us now formalise this idea."}, {"title": "Geodesic distance formulation.", "content": "Let us define a neural network as a function f: R \u2192 R, where n is the dimension of the input space. Let us also define x a point in this input space. We denote the Jacobian of f at x as Jx.\nUsing Taylor's theorem, for a vector d with an infinitesimal norm: Ve, ||8|| \u2264 6, we have:\n||f(x + d) \u2212 f(x)|| \u2248 ||Jxd|| \u2248 8TJJxd\nUsing equation 4, we can now define a tangent space TxM of all d, equipped with a local inner product Gx:\n< \u03b4, \u03b4' >= \u03b4TGxd' = \u03b4TJ Jx \u03b4'\nAs a result, we can view the input space as a Riemannian manifold (Rn, G), where the Riemannian metric G is defined above. On this manifold, the length of a curve \u03b3(t) : [0, 1] \u2192 Rn is defined as:\nL(Y) = \u222b01\u221a<y(t), y(t) >y(t)dt\n= \u222b01||dt f (y(t)) y(t)|| dt,\nwhere y(t) is the derivative of y(t) with respect to t. The geodesic distance, denoted L*, between a and b is then defined as the minimum length among curves y such that y(0) = a and y(1) = b. We also call geodesic path the curve y* which minimises the length L. This path can be interpreted as the shortest path between a and b in the manifold.\nRemark 1 We can infer from Equation 6 that the geodesic path avoids as much as possible high-gradients regions. This is the main desired property of a path to be used for path-based attributions. Representing the path of least resistance, the geodesic path circumvents superficially high values of attributions."}, {"title": "Approximation of the geodesic with K Nearest Neighbours.", "content": "Computing the exact geodesic would require computing L on an infinite number of paths y, which is not possible in practice. However, several methods have been proposed to approximate this value. We draw from previous work (Yang et al., 2018; Chen et al., 2019) and present one with desirable characteristics.\nFirst, we compute the K Nearest Neighbours (kNN) algorithm on points between (and including) input and baseline. These points can be either sampled or generated. The geodesic distance between two neighbouring points, xi and xj, can be approximated by a straight path x\u2081 + t \u00d7 (xj \u2013 xi). We have the above approximation because for dense enough data, the euclidean distance between neighbouring points is a good approximation of the geodesic distance. This reflects the fact that a small region of a Riemannian manifold, called Riemann neighbourhood, is locally isometric to a Euclidean space2. So the geodesic distance between the two neighbouring points is approximated by:\nLij = \u222b01 ||dt f (xi + t x (xj - xi)) \u00d7 (xi - xj)|| dt\n= ||xi - Xj || \u222b01 ||dt f (xi + t \u00d7 (xj - x))|| dt\nEquation 7 corresponds to the original Integrated Gradients method, albeit with the norm. This integral can be approximated by a Riemannian sum similarly to Sundararajan et al. (2017):\nLj\u2248 ||xi \u2013 xj || \u03a3k=0m \u03a3 |\\df(xi + km \u00d7 (xj - Xi))||\nFor input-baseline pair, x and X, we can now see the set (x, x, xi) as a weighted graph, with the weights being the geodesic distances between two neighbours Lij. To compute the geodesic path between x and \u3121, we can use a shortest path algorithm, such as Dijkstra or A* with the euclidean distance as the heuristic.\nThe resulting Geodesic Integrated Gradients corresponds to the sum of the gradients along this shortest path:\nGeodesic IG; (x) = (Xi - Xi) \u222b01 \u2211k=0m \u2202f(xk + t \u00d7 (xk+1 xk))\u2202xk dt\nwhere xk are the points along the shortest path. The integrals in Equation 9 can also be approximated with Riemannian sums.\nThe gradients between each pair of neighbours can also be estimated in batches to speed up the attribution computation. Moreover, several inputs' attributions can be computed together, with similar speed as IG: if we want to compute the attribution of N inputs, with 10 interpolation steps and 5 nearest neighbours, the number of gradients to calculate is 10 \u00d7 5 \u00d7 N = 50N, which amounts to computing IG with 50 steps. This does not include the computation of the shortest path, which is for instance O(N2) for Dijkstra algorithm."}, {"title": "Assumption of the approximation.", "content": "Here we formalise the intuition that, for a pair of neighbours, the geodesic path between them is close to the euclidean one. Notice that the derivative of the neural network f is Lipschitz continuous,\nK\u2200x, y, ||Jx - Jy|| \u2264 K \u00d7 ||x - y||.\nEquation 10 is equivalent to the Hessian of f being bounded. Under this assumption, if two points x and y are close enough, the Jacobian of one point is approximately equal to the other: if ||x-y|| \u2264 6, then Jx \u2248 Jy. As a result, the length between x and y, for a curve \u03b3, is: L(y) \u2248 \u222b ||Jx|| dx \u2248 ||Jx|| \u222b\u2081 dx. Due to the triangular inequality, the shortest path y* is then a straight line, and we have: L*(x, y) \u2248 ||Jx|| \u00d7 ||x \u2212 y||.\nAs a result, under this assumption, if two points are close, the geodesic path can be approximated with a straight line. Note that even though we take the path between two neighbouring points to be a straight line, we do not assume that the Jacobian of the function between the two points is constant."}, {"title": "Handling disconnected graphs", "content": "An issue with the graph computed with the kNN algorithm is that it could be disconnected, in which case it could be impossible to compute a path between an input and a baseline. To alleviate this issue, we add so called \"bridges\" to the graph, as following: for each disconnected component, we add one link between them, specifically between two points of each component having the lowest euclidean distance."}, {"title": "Approximation of the geodesic with energy-based sampling.", "content": "While our kNN-based method is effective for explaining simpler models, its applicability diminishes as model complexity increases. In such cases, a prohibitively large number of samples is required between the baseline and the input to provide accurate estimates of the geodesic path. Even with relatively large number of samples, it is not trivial where on the manifold to sample the points to adequately capture the gradient landscape. Furthermore, once the points are sampled, searching the graph for the shortest path will be computationally too intensive. For such use-cases, in this subsection, we devise an energy-base sampling method as another approximation procedure.\nAs noted in Remark 1, we aim to sample from the shortest paths between two points in the input space while avoiding regions of high gradients. To achieve this, we deviate from the straight line to minimise the influence of high-gradient areas. This process can be approximated as follows: we begin with a straight-line path between the two points and define a potential energy function composed of two terms: a distance term to maintain proximity to the straight line and a curvature penalty term to push away from high gradient regions. Minimising this energy function approximates the geodesic path.\nFormally, the distance term is defined as d(x, y) := ||x - y||2, and the curvature term as c(x) := ||f(x)||2 where f represents the neural network. The total energy being minimised is\nE(\u03b3) = \u03a3i=1n d(vi,vi) \u2013 \u03b2c(vi),\nwhere y is the path, yo is the initial path, and \u1e9e controls the trade-off between distance and curvature.\nWith this energy function, one can use a suitable sampling method, such as Stochastic Variational Inference (SVI) or Hamiltonian Monte Carlo to sample points on the geodesic paths. Here we briefly describe the SVI optimisation, as this has a suitable balance of computational efficiency and accuracy.\nSVI provides a probabilistic framework for optimising paths between input and baseline points. To achieve this, it defines a probability distribution p(%) proportional to exp(-E(\u03b3)), where E(\u03b3) is our defined potential energy. Rather than directly sampling from this complex distribution, we introduce a simpler variational distribution q(\u03b3) parametrised by learnable means and scales. This guide distribution takes the form of a factorised normal distribution \u03a0; \u039d(\u03bc\u03af, \u03c3\u03af) over path deviations.\nThe optimisation proceeds by minimising the KL divergence between q(y) and the true posterior through maximisation of the Evidence Lower Bound. Critically, this allows us to learn optimal parameters for q() through gradient-based optimisation. The learned means \u03bc\u03b5 define the optimal path deviations from the initial straight-line path, while the scales o capture uncertainty in these deviations. This probabilistic approach naturally samples of the low-energy regions.\nWe apply this method in our computer vision experiments, demonstrating its efficacy in Section 3. However, for clarity, we visualise these paths on a simpler 2D half-moons example in Fig. 7. While the kNN method would typically be preferred for such simpler cases due to its ease of control, this example serves as an instructive illustration.\nOf course, using the SVI method comes with its own challenges. For example, in this case a suitable value of \u1e9e for the potential energy, as well as learning rate for the SVI algorithm itself needs to be chose. As for any standard machine learning training, these values can be chosen using hyperparameter tuning, as we discuss in section 3."}, {"title": "Axiomatic properties", "content": "Designing an effective attribution method is challenging, partly because there are often no ground-truth explanations. If a method assigns importance to certain features in a counterintuitive way, it can be difficult to determine whether the issue lies with the model, the data, or the attribution method itself."}, {"title": "Strong Completeness", "content": "Here we prove that Geodesic IG satisfies Axiom 2, Strong Completeness, and is the only path-based method that does so."}, {"title": "Theorem 1 (Strong Completeness)", "content": "Let f : R\" \u2192 R be continuously differentiable, and let \u3121, x \u2208 R\".\nGiven a smooth path\n\u03b3: [0,1] \u2192 R\", \u03b3(0) = x, \u03b3(1) = x,\ndefine its attributions as\nAi(x) = \u222b01 \u2202fOxi (y(t)) y(t) dt,\nand assume the Riemannian metric is given by Eq. 5. The length of a path is given by Eq. 6. Suppose that the geodesic path connecting and x exists. Then,\n\u03a3i=1n|Ai(x)| = |f(x) - f(x)|\nif and only if y is the geodesic path.\nSee Appendix A for the proof."}, {"title": "Symmetry preserving of Geodesic IG", "content": "The symmetry axiom is defined in the following way.\nAxiom 3 (Symmetry) Consider an input-baseline pair x and \u00e6, and a function f that is symmetric in dimensions i and j. If x = x; and X = Xj, then an attribution method is Symmetry-Preserving if A\u00bf(x) =\nAj(x), where An(x) is the attribution of xn.\n(Sundararajan et al., 2017, Theorem 1) shows that IG is the only path-based attribution method that satisfies symmetry for any function. However, as noted in Kapishnikov et al. (2021), while the straight path is the only one satisfying symmetry for any function, for a specific function, it may be possible to find other paths that also satisfy symmetry. Below, we demonstrate that Geodesic IG satisfies symmetry for Riemannian manifolds, and thus for the neural network functions we use when sampling the paths.\nLet the ith and jth dimensions of y(t) be yi(t) and yj(t) respectively and f be a function differentiable almost everywhere on t. Furthermore, take f to be symmetric with respect to xi and xj. If Yi(t) = yj(t) for all t \u2208 [0, 1], then we have\n||dtf(yi(t)) \u00d7 Vi(t)|| = ||dtf(yj(t)) \u00d7 \\j(t)||\nalmost everywhere on t. Therefore, the ith and jth components of Eq. 6 are equal. Furthermore, since Eq. 9 integrates along the path that is an approximation of Eq. 6, we have Geodesic IG\u2081 = Geodesic IGj.\nIndeed our geodesic paths satisfy vi(t) = yj(t) for all t \u2208 [0, 1] on the Riemannian manifolds. To see this, let us select a baseline and U a Riemann neighbourhood centred at X. Let us also define the geodesic path y such as y(0) = x. Further, define v(t) := \u03b3'(t), where y' is the derivative of \u03b3. Then, in the local coordinates system of the neighbourhood of any point, called normal coordinates, we have y(t) = (tv1(t), ..., tvn (t)).\nSince the function is symmetric in the ith and jth dimensions, we have vi and vj are the same everywhere. From this, we can see that Yi(t) = yj(t) for all t \u2208 [0, 1] and therefore Geodesic IG satisfies symmetry."}, {"title": "Experiments", "content": "To validate our method, we performed experiments on two datasets: one is the synthetic half-moons dataset, and the other is the real-world Pascal VOC 2012 dataset."}, {"title": "Experiments on the half-moons dataset", "content": "We use the half-moons dataset provided by Scikit learn (Pedregosa et al., 2011) to generate 10,000 points with a Gaussian noise of N(0,x), where x ranges between 0.05 and 0.65. The dataset is split into 8,000 training points and 2,000 testing ones. The model used is an MLP.\nWe evaluate each attribution method using an indicator of performance: the absence of artefacts that do not reflect the model's behaviour. To this end, we use purity, defined as follows.\nA well-trained model should classify approximately half of the data points as \u201cupper moon\u201d (class 1) and the other half as \"lower moon\u201d (class 0). Such a model should consider both features of each point important for classification into class 1. Therefore, for a good attribution method, A, we expect the top 50% of points-ranked by the quantity A(x) = \u2211i=0 |Ai(x)|, to be classified as 1, assuming the baseline is chosen as a point to which the network assigns a near-zero score. With this in mind, we define purity as\nPurity = 1N/2 \u2211x, A(x)\u2208Top 50% of all A argmax(f(x)),\nwhere N is the number of data points. We see that this is the average value of the predicted class labels for half of the points. From the above, we infer that, for a well-trained model, we prefer an attribution method that results in the purity close to 1. In contrast, a random attribution method in this case would result in the purity score of 0.5.\nIn this experiment, we compare the results of attributions from Geodesic IG with methods including Integrated Gradients, GradientShap, InputXGradients (Shrikumar et al., 2016), KernelShap (Lundberg & Lee, 2017), Occlusion (Zeiler & Fergus, 2014), and Guided IG (Kapishnikov et al., 2021)."}, {"title": "Experiments on the Pascal VOC 2012 dataset", "content": "To evaluate our method on a real-world dataset, we used the Pascal VOC 2012 dataset (Everingham et al.), which consists of labelled images. We trained a classification head on this dataset and integrated it with the pre-trained ConvNext model (Liu et al., 2022) from TorchVision to generate predictions for explanation."}, {"title": "Related Work", "content": "Approximating geodesic paths is a widely studied area of research, and many methods to do so have been developed. For a comprehensive survey on this subject, please refer to Crane et al. (2020).\nThe idea of using a kNN algorithm to avoid computing gradients on out of distribution data points has also been used in Enhanced Integrated Gradients Jha et al. (2020). However, this method creates a path which is model agnostic, as it does not necessarily avoid high gradients regions. As a result, it can lead to significant artefacts which do not reflect the model's behaviour. To support this argument, we provide an example where this method fails on the half-moons datasets. In Fig. 9, similar to the example in section 1, we see the Enhanced IG attributes different importance to the horizontal and vertical features of the half-moon data points, in a model that is flat everywhere, other than the decision boundary. Furthermore, in Fig. 9, we observer that the method violates Strong Completeness axiom, Axiom 2. This is expected, given Theorem 1.\nThe idea of adapting the path to avoid high gradient regions has be proposed by Kapishnikov et al. (2021), calling their method Guided Integrated Gradients. This method has a heuristic approach to finding such paths. As a result it does not guarantee to find paths of minimal accumulated gradients. In contrast, Geodesic Integrated Gradients offers a more principled approach by directly approximating the path of least resistance using a Riemannian manifold framework. As a result, as we see in Section 3, our method significantly outperforms Guided IG in terms of attribution accuracy."}, {"title": "Discussion", "content": "In this paper, we identified key limitations of path-based attribution methods such as Integrated Gradients (IG), particularly the artefacts that arise from ignoring the model's curvature. To address these issues, we introduced a novel path-based method, Geodesic IG, that integrates gradients along geodesic paths on a manifold defined by the model, rather than straight lines.\nBy avoiding regions of high gradient in the input space, Geodesic IG effectively mitigates these artefacts while preserving all the axioms established by Sundararajan et al. (2017). Additionally, we introduced a new axiom, Strong Completeness, which, when satisfied, prevents such misattributions. We proved that Geodesic IG is the only path-based method that satisfies this axiom. Through both theoretical analysis and empirical evaluation using metrics such as Comprehensiveness and Log-Odds-we demonstrated the advantages of our approach."}, {"title": null, "content": "To approximate geodesic paths, we proposed two methods: one based on k-Nearest Neighbour and another leveraging Stochastic Variational Inference. While these methods outperform existing alternatives, they also present challenges. One such challenge is computational cost, as discussed in Section 3. Another is the inherent noise in sampling-based geodesic approximations. Even though in our experiments we demonstrated noise reduction relative to the original IG, we believe further improvements can be achieved. A promising future direction is to solve the geodesic equation directly, which could reduce noise and improve accuracy. Additionally, depending on the chosen solution method, this approach may offer greater computational efficiency compared to the current reliance on SVI."}]}