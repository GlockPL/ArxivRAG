{"title": "Frequency and Generalisation of Periodic Activation Functions in Reinforcement Learning", "authors": ["Augustine N. Mavor-Parker", "Matthew J. Sargent", "Caswell Barry", "Lewis Griffin", "Clare Lyle"], "abstract": "Periodic activation functions, often referred to as learned Fourier features have been widely demonstrated to improve sample efficiency and stability in a variety of deep RL algorithms. Potentially incompatible hypotheses have been made about the source of these improvements. One is that periodic activations learn low frequency representations and as a result avoid overfitting to bootstrapped targets. Another is that periodic activations learn high frequency representations that are more expressive, allowing networks to quickly fit complex value functions. We analyse these claims empirically, finding that periodic representations consistently converge to high frequencies regardless of their initialisation frequency. We also find that while periodic activation functions improve sample efficiency, they exhibit worse generalization on states with added observation noise\u2014especially when compared to otherwise equivalent networks with ReLU activation functions. Finally, we show that weight decay regularization is able to partially offset the overfitting of periodic activation functions, delivering value functions that learn quickly while also generalizing.", "sections": [{"title": "1 Introduction", "content": "Deep learning has enabled reinforcement learning (RL) to conquer environments with large, high dimensional state spaces (e.g. Silver et al. (2016); Badia et al. (2020)). The architectures of deep RL agents have largely emulated innovations from supervised learning-a prime example being the original deep Q-networks (Silver et al., 2013), which used the GPU-optimized implementation of convolutional layers from AlexNet (Krizhevsky et al., 2012). However, design choices inherited from supervised learning do not always readily meet the demands of RL. For example, popular techniques such as weight decay (Salimans & Kingma, 2016) and momentum (Bengio et al., 2021) do not improve performance in RL as consistently as they do in the supervised learning contexts in which they were initially developed.\nOne important dimension along which RL differs from supervised learning is in the trade-off between gen- eralization and memorization, with RL algorithms often benefiting from localized features (Ghiassian et al., 2020) such as tile coding (Sherstov & Stone, 2005), which limit the degree to which the function approxi- mator can generalize updates on one state's predicted value to other distant states. While some degree of generalization is necessary for large problem scales, it is desirable to be able to calibrate the spectral bias of a function approximator to a particular problem instance. Fourier features, a popular representation for both supervised learning (Rahimi & Recht, 2007) as well as linear RL (Konidaris et al., 2011), present one means of achieving this objective, whereby the degree of generalization between training samples of the en- suing linear function approximator can be precisely specified by the frequency with which these features are initialized. Low-frequencies bias the learner towards smooth functions that generalize more between training samples, whereas high-frequencies will bias the learner towards non-smooth functions.\nPrior to the widespread adoption of deep RL, the Fourier features used in RL consisted of a vector of stacked Fourier decompositions of each state variable (where each state variable is decomposed into range of different frequencies), which are then fed to a linear function approximator (Konidaris et al., 2011). This approach is difficult to scale to high dimensional inputs as the number of features scales exponentially with the size of the input (see Brellmann et al. (2023) for work on scaling these traditional Fourier features to deep RL tasks). More recently, a more scalable approach has emerged for learning periodic representations, which adds periodic activation functions to neural network architectures (Sitzmann et al., 2020). These periodic activation functions can be viewed as a form of learned Fourier features, replacing the randomly generated, fixed weights of Rahimi & Recht (2007) with trainable parameters.\nLi & Pathak (2021) and Yang et al. (2022) applied periodic activations to deep RL, incorporating them into a neural network architecture trained to perform value function approximation. Yang et al. (2022) argue that the use of a periodic activation functions improves sample efficiency because it causes networks to learn high frequency representations that quickly fit high frequency details in value functions. Concurrently, Li & Pathak (2021) also employed periodic activations but with a different perspective. Li & Pathak (2021) state that periodic activations learn low frequency representations, which improves sample efficiency by reducing the amount of overfitting to noisy targets. It is not clear why both works see similar improvements on the same benchmark environments when Yang et al. (2022) suggests that high frequency representations are useful while Li et al. (2020) argues that low frequencies are beneficial.\nWe investigate how previous works with similar methodologies can arrive at these different conclusions."}, {"title": "2 Related Work", "content": "Periodic representations are widely applicable across different domains of machine learning research. In computer vision (Sitzmann et al., 2020; Tancik et al., 2020) Fourier-like positional encodings are used when constructing 3D scenes from 2D images (Mildenhall et al., 2021). For solving partial differential equations, neural Fourier operators learn representations that operate at a range of resolutions (Li et al., 2020). Ad- ditionally, geometric deep learning often operates in Fourier space (Cohen et al., 2018; Cobb et al., 2020). Even when not explicitly engineered into an architecture, Fourier features have emerged in transformer ar- chitectures (Nanda et al., 2023). Rahimi & Recht (2007) show that even random Fourier features can provide a useful approximation for kernel methods in supervised learning.\nIn RL, Fourier features were initiated by Konidaris et al. (2011), who learn a value function on top of a fixed Fourier decomposition of each individual variable in the state space. These classic Fourier features has proven to be useful for generalisation in spatial navigation tasks (Yu et al., 2020). Recently, Li & Pathak (2021) and Yang et al. (2022) developed a scalable approach for learning periodic features\u2014achieving state of the art results at the time of publication. A drawback of Fourier representations is that they continue to oscillate outside of their training distribution, leading to inaccurate extrapolation behaviour. More localized feature representations such as tile coding (Sherstov & Stone, 2005) can avoid this extrapolation behaviour. Ghiassian et al. (2020) leveraged localized feature representations to demonstrate the importance of fitting discontinuous local components of the value function for the overall performance and robustness of rein- forcement learning algorithms. While localized representations have found some success (Whiteson, 2010), they have not seen widespread adoption into neural network architectures to the same degree as periodic activations, whose use in neural networks goes back several decades (McCaughan, 1997).\nGeneralisation in deep reinforcement learning is a more nuanced property than in supervised learning due to its effect on the stability of RL algorithms (van Hasselt et al., 2018). Generalisation between observations in a single environment is a double-edged sword, bringing the potential of accelerating gradient-based opti- mization methods (Jacot et al., 2018) but also the risk of over-estimation and divergence of the bootstrapped training objective (Achiam et al., 2019). Many works studying generalisation in deep reinforcement learning typically focus on out-of-distribution transfer to new tasks, either drawn from some iid distribution (Cobbe et al., 2019), or obtained by changing the reward (Dayan, 1993; Kulkarni et al., 2016) or some latent factor of the environment transition dynamics. For a comprehensive review of generalisation in reinforcement learn- ing, we refer to the work of Kirk et al. (2023). Our interest will be in the relationship between low-frequency representations and generalization within a single environment, a connection studied in the RL context by Lyle et al. (2022). Relevant to the conclusions of this work, periodic representations in mice have been shown to shift to lower frequencies in states of high levels of uncertainty (Barry et al., 2007)."}, {"title": "3 Reinforcement Learning with Periodic Activations", "content": "Reinforcement learning is formalised in the framework of Markov decision processes, which are a tuple M = (S, A, R, P, \u03b3) where S is the set of all states an agent can experience; A is the set of actions an agent can take; R: S \u00d7 A \u2192 R is a reward function that gives rewards to the agent; \u03b3\u2208 [0, 1] is the discount factor that controls how preferential near term rewards are to long term rewards and P : S \u00d7 R \u00d7 S \u00d7 A \u2192 [0, 1] is the transition function, where P(s',r,s,a) = P(St = s', Rt = r|St\u22121 = s, At\u22121 = a), for St \u2208 S, At \u2208 A and t indexes the timestep (Sutton & Barto, 2018, p. 47). This work focuses on episodic reinforcement learning where an agent's goal is to maximize its expected return G = \u0395\u03c0[\u2211k=0\u03b3krt+k+1] in a given episode, where T is the length of the episode and \u03c0: S \u2192 A is a policy that controls how the agent selects its actions.\nSample efficient neural networks for continuous control are off-policy and actor-critic based, meaning they learn a policy function and an action-value function. Action-value functions Q: S \u00d7 A \u2192 R predict the expected future return given the current state and action Q(s, a) = En [Gt|St = s, At = a] (Sutton & Barto, 2018, p. 58). However, learned Fourier features have only been shown to significantly improve performance when used within the value functions of actor-critic architectures (Li & Pathak, 2021; Yang et al., 2022), and hence we focus our experiments on value learning."}, {"title": "3.1 Representation Frequency", "content": "Throughout this work we describe representations as having a frequency. To avoid ambiguity we define the frequency of representations here.\nDefinition 3.1. Given a dataset of network inputs D and layer weights W, the representation frequency f of the ith neuron is the maximum activation minus the minimum activation, divided by 2\u03c0. This definition corresponding to the number of wavelengths covered by the ith activation over the dataset D.\n$f_i = \\frac{\\max_{x \\in D} [(Wx)_i] - \\min_{x \\in D} [(Wx)_i]}{2\\pi}$ (1)\nIntuitively, one can plot the activation of a given neuron for different network inputs, which traces out either a sine or cosine wave depending on the choice of periodic activation function. As weight values become larger, the number of periods covered a periodic activation increases. Our definition follow this intuition, counting how many wavelengths the activation oscillates through in the over a dataset of inputs.\nNext, we describe the two different flavours of learned Fourier feature architectures from the literature that we call learned Fourier features (Yang et al., 2022) and concatenated learned Fourier features (Li & Pathak, 2021). It is important to note these are equivalent to traditional deep RL architectures, except that they replace activation functions with a sinusoid and/or cosine activation. They do not perform a Fourier decomposition of each state variable as is done by (Konidaris et al., 2011)."}, {"title": "3.2 Learned Fourier Features", "content": "The learned Fourier features (LFF) of Yang et al. (2022) learn a value function that has a multi-layer perceptron architecture. The first layer of learned Fourier feature networks take the following form F = sin(Wx + b), where W and b are weights and biases and x is the layer input. Then, layers of additional weights and biases are stacked on top of the initial Fourier layer these subsequent layers have ReLU activations. We denote the ReLU layers as L, meaning an N-layer learned Fourier feature value function be written as Q(s, a) = LN(LN-1(...(F\u2081(s, a))), where the state vector s and the action vector a are concatenated into one large vector x, which is the input to the learned Fourier feature layer. Yang et al. (2022) initialise weights from a normal distribution and their biases from a uniform distribution, which using their notation, is formulated as the following.\n$W_{ij} \\sim N(0,\\frac{\\pi^2 \\beta}{d})$ (2)\n$b_j \\sim U(-\\pi, \\pi)$ (3)\nWhere \u03b2 is the initial bandwidth and d is the dimension of the layer input. Larger weight values Wij mean higher frequency oscillations in the output neurons as you move through the state space, while smaller weights mean lower frequency oscillations. The bias bj can be thought of as controlling the phase of the output neurons in the first layer, translating the representation along the output oscillations of each individual neuron. Yang et al. (2022) hypothesise that this architecture allows networks to learn high frequency representations, which prevents the underfitting of value functions that may happen if the first layer uses a ReLU activation."}, {"title": "3.3 Concatenated Learned Fourier Features", "content": "Li & Pathak (2021) concatenate the feature input alongside periodic representations in the first layer of their networks. In addition, Li & Pathak (2021) use both cosine and sine activations and concatenate these features into one large vector, which additionally includes the raw state input.\n$C_F = (sin(Wx), cos(Wx), x)$ (4)\nWhere the comma denotes concatenation of sin(Wx), cos(Wx) and x into one larger vector. Note that Li & Pathak (2021) do not include a bias term in their layer architecture. The rest of the architecture follows the same pattern as learned Fourier features with ReLU layers following the initial layer as follows: Q(s,a) = LN(LN-1(...(CF1(s,a))). The motivation of Li & Pathak (2021) was to avoid overfitting to noise in bootstrapped targets when fitting the value function. It was hypothesised that concatenated learned Fourier features would learn smooth representations, while the concatenated x input would retain the high frequency information for the rest of the network."}, {"title": "4 Experiments", "content": "Here we empirically investigate the representations of periodic activation functions introduced in section 3 focusing on DeepMind control (Tassa et al., 2018), a standard benchmark for continuous control consisting of a variety of agent morphologies (quadrupeds, fingers and humanoids) paired with reward functions that encourage skills like running or walking. We focus on control from proprioceptive observations. The action space is a continuous vector of forces to apply to different joints. The full set of environments used and hyperparameters used can be found in appendix A.2. Our code builds upon the jaxrl repository (Bradbury et al., 2018; Kostrikov, 2021) with reference to the original repositories of Li et al. (2020) and Yang et al. (2022).\nLi & Pathak (2021) and Yang et al. (2022) agree that periodic activation functions are most useful for off- policy algorithms. Furthermore, ablations have shown periodic activations are only beneficial when added to the critic function of actor-critic architectures Li et al. (2020); Yang et al. (2022). As a result, we experiment with periodic activation functions embedded within critic networks only, which themselves are embedded in a soft-actor critic learning algorithm (Haarnoja et al., 2018).\nFollowing Yang et al. (2022), we use a learned Fourier feature critic with a first layer width equal to 40 times the size of the input, followed by two ReLU activated hidden layers of width 1024. The concatenated learned Fourier features critic has an initial layer with the same total width. The ReLU critic has an equivalent architecture except with a ReLU used instead of a periodic activation. All agents have the same actor architecture (details in appendix A.2.2)."}, {"title": "4.1 Representation Frequency and In-Distribution Performance", "content": "We analyse representation frequency on three environments highlighted by Yang et al. (2022): hopper-hop, walker-run and quadruped-run (we only focus on 3 environments initially due to computational limits, in later sections we broaden our analysis to all 8 environments tested by Yang et al. (2022)). We reproduce the results of both Yang et al. (2022) and Li & Pathak (2021), showing that both approaches are more sample efficient than ReLU architectures on these initial environments. In figure 2a, we evaluate performance at 100k frames of experience for the walker-run environment at variety of scales \u03b2 (roughly when the improved performance of learned Fourier is most apparent). We measure performance for a range of different initialisation scales B (see equation 2).\nFor low initial scales, learned Fourier features and concatenated learned Fourier features outperform the ReLU baseline (in the low sample limit shown in figure 2a). The performance of the Fourier feature archi- tectures is relatively consistent for low initial \u03b2. If very high scale initialisations are used (\u03b2 > 0.03, roughly"}, {"title": "4.1.1 Representation Frequencies After Training", "content": "We measure weight scales \u03b2 after training by assuming the weight distribution is a diagonal Gaussian, and then computing its standard deviation to extract \u03b2 (see equation 2). Previously, it was hypothesised that lower initial \u03b2's would lead to lower frequency representations after training, which could improve generalization (Li & Pathak, 2021).\nWeight distributions generally rise to similar scales after training regardless of their initialisa- tion. In figure 2b, we find that despite differing initialisation scales (shown by the black dotted line), weight distributions generally rise to similar scales (approximately \u03b2 \u2248 100.3 \u2248 2.0). This suggests that initialisation scale is not as important as previously thought for determining final scales (Yang et al., 2022). The exception being for large scale initialisations, where it seems that if representation frequency is initialised too high, it is not able to accurately predict the values of state-action pairs (see the far right hand side of figure 2a and 2b). This is interesting given work on learning dynamics (Gulcehre et al., 2022; Lyle et al., 2022), which shows the experiences of agents early in training are consequential for an agent's final performance. Our results suggest that periodic activation functions learn high frequency terminal value functions but also that early in training low frequency representations are may be crucial.\nFor a more intuitive measure of representation frequency, we count the number of wavelengths covered for each neuron for a given batch, which is equivalent to definition 3.1. We plot the distribution of these frequencies for all neurons in figure 2c. A large portion are in bins greater than 0.25 cycles, which we consider to be high frequency-as the number of wavelengths covered becomes greater than for the data collected by the agent, which is roughly similar to a ReLU activation. We also plot two different distributions of cycles from two different runs, a low frequency initialisation and a high frequency initialisation. The frequency distribution is strikingly similar for a large initial \u03b2 and a small initial \u03b2, further suggesting that initial scale is not consequential for the final scales learned periodic activations.\nIn summary, learned Fourier features and concatenated learned Fourier features generally rise to similar weight scales after training. Additional analysis of representation frequency distributions show that these large weight scales induce high frequency oscillations in the nonlinearity over the training distribution. In light of prior works suggesting a connection between low frequency components of a function and general- ization (Rahaman et al., 2019), it is reasonable to expect that the frequency of these representations might have significant implications on the generalization behaviour of the network. In the next section, we explore"}, {"title": "4.2 Generalization Properties of Learned Fourier Features", "content": "We have shown that periodic activation functions consistently learn high frequency representations. While this improves sample efficiency, it also suggests that learned Fourier feature agents could be overfitting. Following the literature on generalization in RL (Dulac-Arnold et al., 2021), we benchmark generalization by perturbing state observations with Gaussian noise. We only perturb the observations at test time. No training is performed on observations with added noise. As we have shown learned Fourier features and concatenated learned Fourier features learn representations with similar frequencies, we only include the simpler learned Fourier features in our analysis.\nNoise levels were tuned by hand for each environment, as the environments have different observation distri- butions (see appendix 7 for noise scales). We tuned observation noise to 3 different levels per environment: low noise where both ReLU and Fourier features can deploy their policy from training without degradation; medium noise where there is a noticeable impact on performance but the policies learned during training are still useful when compared to an untrained policy (typically with a medium amout of noise there is also a discrepancy in the generalisation performance of Fourier features and ReLU architectures); and lastly high noise, where all policies cannot do better than an untrained agent. In the medium noise regime, learned"}, {"title": "4.3 Why do Fourier Representations Struggle to Generalize?", "content": "The relationship between the frequency of learned fourier features and network generalization is perhaps best understood through the lens of feature geometry evolution. Prior work on the expressivity and initialization of neural networks has identified the importance of the effect of a layer on the cosine similarity between the representations of two different inputs prior to and after its application, formalized by the Q/C-maps of Poole et al. (2016). Layers which decorrelate their inputs (i.e. which reduce the cosine similarity) tend to produce networks which are trainable but generalize poorly, while layers which increase the cosine similarity of inputs on average allow the network to generalize more aggressively, potentially at the expense of convergence rates as discussed in greater detail by Martens et al. (2021). Due to their initially low frequency, learned Fourier features begin training with a high cosine similarity before and after activation (see figure 4b). However, this value declines as the frequency of the learned fourier features increases over training. The ReLU representations on the other hand see their cosine similarity decrease more slowly than the Fourier features.\nA consequence of this phenomenon is that the representation will become more sensitive to perturbations in the input with respect to the Euclidean distance as well. Concretely, we hypothesise that small changes in the input observations will cause larger changes in the post-activation representations than ReLU activa- tions. We find this to be true empirically in table 1, where the Euclidean distance between post activation representations before and after noise is larger for learned Fourier feature representations than for ReLU representations. A qualitative visualisation of this is shown in figure 4c, where ReLU points are generally correlated before and after noise, while the Fourier representations are more brittle, with dramatic shifts before and after noise.\nLastly, recent works have quantified the expressivity of neural representations by computing their effective rank-including recent work on classical Fourier features that perform a fixed Fourier decomposition of states Konidaris et al. (2011); Brellmann et al. (2023). Low-rank representations are \u201cimplicitly underparameter- ized\" which may help generalization but decrease expressiveness Kumar et al. (2020); Gulcehre et al. (2022). We compute the effective rank of representations from replay buffer samples at the end of training for both ReLU and learned Fourier feature representations in table 1. We report our full results on generalisation in A.5."}, {"title": "4.4 Can the Generalization of Fourier Features be improved?", "content": "The previous sections suggest that learned Fourier representations fail to generalize because they learn high frequency representations. Here we introduce a weight decay term in the loss function, which effectively discourages frequencies from growing too large. For both ReLU and periodic activations, we find that a weight decay of 0.1 was optimal. In figure 5, we plot how performance changes over training on the three environments from section 4.2. We find the weight decay has a positive effect on performance for both Fourier representations and ReLU representations. This effect on performance is enough to enable learned Fourier features to be comparable to the performance of the ReLU representations when aggregating results over the"}, {"title": "5 Conclusion", "content": "We have shown that unconstrained learned Fourier features tend to learn high frequency representations on continuous control tasks. As a result, we hypothesise, and then show empirically, that the high frequency rep- resentations do not generalize well. We test generalization by perturbing observation with noise as suggested by (Dulac-Arnold et al., 2021). We provide empirical explanations for why high frequency representations are brittle, showing that small changes in observations leads to large changes in Fourier representations. Lastly, we consider using weight decay as a means to discourage frequency from becoming excessively high-this is able to partially offset the generalization challenges faced by learned Fourier representations."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Supervised Learning Warmup", "content": "Following previous literature on learned Fourier features Yang et al. (2022), we qualitatively observe the performance of learned Fourier features on the mountain car environment Moore (1990). We first learn a ground truth value function with dynamic programming, using an implementation provided by Yang et al. (2022). Then we try to fit a MLP with ReLU activations and sinusoidal activations to the ground truth value function of the zeroth action, with a checkerboard pattern as the train/test split. We optimise both networks with Adam Kingma & Ba (2014) with a learning rate of 10-4. Each network has 2 hidden layers of width 400. The learned Fourier feature network has sinusoids throughout to highlight the properties of sinusoidal layers. We use an initial \u03b2 = 10/\u03c0. Note this experiment is used to exemplify the potential pitfalls of sinusoidal activations, it has not been tuned to achieve the best possible test error."}, {"title": "A.2 Deepmind Control Hyperparameters", "content": "Below we describe in detail the different hyperparameters used by our learned Fourier features agents."}, {"title": "A.2.1 Soft Actor-Critic hyperparameters", "content": ""}, {"title": "A.2.2 ReLU architecture", "content": ""}, {"title": "A.2.3 Learned Fourier Feature Architecture", "content": "The actor architecture is equivalent to the actor in table 4."}, {"title": "A.2.4 Concatenated Learned Fourier Feature Architecture", "content": "Note the concatenated learned Fourier feature architecture uses a concatenation of a the inputs projected with a sinusoidal activation applied, the inputs projected with a cosine applied and then also just the raw inputs (see equation 4). The actor architecture is again equivalent to the actor in table 4."}, {"title": "A.3 Further Results on Measuring Learned Fourier Frequencies", "content": ""}, {"title": "A.4 Tuning Observation Noise", "content": "We tuned observation noise for the different environments to reach low, medium and high levels of disruption on the performance of agents during training. Below is a table of the values of the standard deviation of the isotropic Gaussian noise added to each observations for the different environments."}, {"title": "A.5 Full Generalisation Properties results", "content": "Table 8: Generalisation properties for all methods averaged over all environments and normalised by the equivalent ReLU quantity."}, {"title": "A.5.1 Full Deepmind Control Results", "content": "We plot the generalisation results for all environments for low, medium and high levels of noise (as described in table 7)."}, {"title": "A.5.2 Low Noise Observation Pertubations", "content": "Figure 11: Full evaluating curves for all 8 environments plotted for a low amount of noise (see table 7). Here the benefits of ReLU activations over LFF representations are still present in one environment (quadruped- walk), however even with this low level of noise ReLU activations begin to outperform or match Fourier feature representations."}, {"title": "A.5.3 Medium Noise Observation Pertubations", "content": "Figure 12: In the medium noise setting (see table 7) the learned Fourier features network performs worse on 4/8 environments than ReLU networks (cheetah-run, hopper-hop, walker-run, quadruped-walk and quadruped-run)."}, {"title": "A.5.4 High Noise Observation Pertubations", "content": "Figure 13: In the high noise regime, all algorithms exhibit poor performance on all 8 environments."}]}